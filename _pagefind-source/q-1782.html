<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Design a beginner-level Azure data ingestion task: In a fintech scenario, ingest daily transaction rows from an on-premises SQL Server into Azure Data Lake Storage Gen2 as Parquet files using Azure Data Factory. Include steps to handle date partitioning, security, and reliable runs. What would your pipeline look like, and what minimal code or configuration would you provide?</title>
  <meta name="description" content="">
</head>
<body>
  <article data-pagefind-body>
    <h1 data-pagefind-meta="title">Design a beginner-level Azure data ingestion task: In a fintech scenario, ingest daily transaction rows from an on-premises SQL Server into Azure Data Lake Storage Gen2 as Parquet files using Azure Data Factory. Include steps to handle date partitioning, security, and reliable runs. What would your pipeline look like, and what minimal code or configuration would you provide?</h1>
    
    <div data-pagefind-filter="channel">azure-data-engineer</div>
    <div data-pagefind-filter="difficulty">beginner</div>
    <div data-pagefind-filter="topic">General</div>
    
    <div data-pagefind-meta="channel" data-pagefind-meta-value="azure-data-engineer"></div>
    <div data-pagefind-meta="difficulty" data-pagefind-meta-value="beginner"></div>
    <div data-pagefind-meta="id" data-pagefind-meta-value="q-1782"></div>
    
    <main>
      <section class="answer">
        
      </section>
      
      
      <section class="tags">
        <span>azure-data-engineer</span>
      </section>
      
      
      
      <section class="companies" data-pagefind-filter="company">
        <span>Citadel</span> <span>Coinbase</span>
      </section>
      
    </main>
  </article>
</body>
</html>