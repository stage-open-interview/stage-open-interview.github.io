{"questions":[{"id":"aws-sap-accelerate-migration-1768242622781-0","question":"A large on‑premises SAP‑like ERP workload must be migrated to AWS with minimal downtime. You plan to lift‑and‑shift but want automated testing and quick cutover. Which approach best achieves this in the AWS ecosystem?","answer":"[{\"id\":\"a\",\"text\":\"Migrate by manual VM replication and schedule go‑live after validation\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use AWS Application Migration Service (MGN) to replicate, test, and perform a controlled cutover\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Rebuild the ERP on AWS using managed SAP services from scratch\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Ship backups via AWS Snowball and restore on AWS after migration\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct answer is **B**: Use AWS Application Migration Service (MGN) to replicate, test, and perform a controlled cutover. This approach provides automated validation of the target environment and a structured cutover plan, minimizing downtime compared with manual VM replication or rebuilding from scratch.\n\n## Why Other Options Are Wrong\n- Option A: Manual VM replication lacks automated testing and controlled cutover, increasing risk and downtime.\n- Option C: Rebuilding from scratch defeats the goal of rapid migration and introduces avoidable risk and delay.\n- Option D: Snowball handles bulk data transfer but does not address ongoing replication, testing, or cutover orchestration.\n\n## Key Concepts\n- AWS Application Migration Service (MGN) enables automated lift‑and‑shift with test and cutover capabilities.\n- End‑to‑end migration orchestration reduces downtime and risk.\n\n## Real-World Application\n- Use MGN to continuously replicate on‑prem VMs to AWS, run automated validation tests, then perform a controlled cutover within a maintenance window.","diagram":null,"difficulty":"intermediate","tags":["AWS","Migration","MGN","Lift-and-Shift","certification-mcq","domain-weight-20"],"channel":"aws-sap","subChannel":"accelerate-migration","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T18:30:22.784Z","createdAt":"2026-01-12 18:30:23"},{"id":"aws-sap-accelerate-migration-1768242622781-1","question":"During a multi‑tier SAP‑like application migration with near‑zero downtime, which AWS approach best supports continuous replication and controlled cutover for the entire stack?","answer":"[{\"id\":\"a\",\"text\":\"Use AWS DMS to replicate the database and perform a cutover\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use AWS Application Migration Service (MGN) to replicate, test, and perform a controlled cutover\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Rebuild the database and application on AWS from scratch to minimize risk\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use AWS Snowball to transfer data and re-create the environment on AWS\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct answer is **B**: Use AWS Application Migration Service (MGN) to replicate, test, and perform a controlled cutover. MGN enables end‑to‑end replication and validation of the target environment, supporting near‑zero downtime when combined with planned cutover processes.\n\n## Why Other Options Are Wrong\n- Option A: DMS focuses on database replication and may require additional steps for full‑stack testing and cutover orchestration.\n- Option C: Rebuilding from scratch omits the near‑zero downtime objective and adds unnecessary risk.\n- Option D: Snowball is for bulk data transfer; it does not provide ongoing replication or cutover orchestration.\n\n## Key Concepts\n- AWS Application Migration Service (MGN) supports end‑to‑end migration with test/cutover.\n- End‑to‑end orchestration minimizes downtime during migrations.\n\n## Real-World Application\n- Employ MGN to continuously replicate the stack to AWS, perform automated tests, and execute a staged cutover during a maintenance window.","diagram":null,"difficulty":"intermediate","tags":["AWS","Migration","MGN","Cutover","certification-mcq","domain-weight-20"],"channel":"aws-sap","subChannel":"accelerate-migration","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T18:30:23.273Z","createdAt":"2026-01-12 18:30:23"},{"id":"aws-sap-accelerate-migration-1768242622781-2","question":"You are migrating containerized microservices from on‑prem to AWS. To minimize operational burden and maximize modernization, which deployment model best aligns with Accelerated Migration practices?","answer":"[{\"id\":\"a\",\"text\":\"Move to EC2 instances and run Docker without orchestration\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Deploy to AWS Fargate with EKS for serverless containers\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Install Kubernetes on‑prem and mirror to AWS manually\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Run Docker containers on EC2 without orchestration\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct answer is **B**: Deploy to AWS Fargate with EKS for serverless containers. This combination reduces operational overhead (no manage‑servers or clusters) and accelerates modernization by leveraging managed Kubernetes with serverless compute.\n\n## Why Other Options Are Wrong\n- Option A: Lifts and shifts to VMs with no orchestration increases maintenance burden and slows modernization.\n- Option C: On‑prem Kubernetes mirrored to AWS adds unnecessary complexity and delays.\n- Option D: Running containers without orchestration misses many benefits of Kubernetes and serverless compute.\n\n## Key Concepts\n- Kubernetes on AWS (EKS) with Fargate reduces management overhead.\n- Serverless containers accelerate modernization and scalability.\n\n## Real-World Application\n- Migrate microservices to EKS with Fargate profiles, enabling automated scaling and simplified operations.","diagram":null,"difficulty":"intermediate","tags":["AWS","EKS","Fargate","Containers","certification-mcq","domain-weight-20"],"channel":"aws-sap","subChannel":"accelerate-migration","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T18:30:23.761Z","createdAt":"2026-01-12 18:30:23"},{"id":"aws-sap-accelerate-migration-1768242622781-3","question":"To accelerate migration of analytics workloads from on‑prem to AWS data lake architecture, which service best enables secure data ingestion, catalog, and governance across S3 data lake?","answer":"[{\"id\":\"a\",\"text\":\"AWS Data Migration Service\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"AWS Glue\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"AWS Lake Formation\",\"isCorrect\":true},{\"id\":\"d\",\"text\":\"Amazon QuickSight\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct answer is **C**: AWS Lake Formation. It centralizes data governance, manages permissions, and cataloging for data stored in S3, accelerating analytics readiness and ensuring secure access.\n\n## Why Other Options Are Wrong\n- Option A: DMS focuses on data migration, not governance or cataloging across the lake.\n- Option B: Glue helps with ETL and catalog, but Lake Formation adds centralized governance and fine‑grained access controls for analytics workloads.\n- Option D: QuickSight is a BI tool, not a data governance or cataloging platform.\n\n## Key Concepts\n- Lake Formation provides centralized data governance for S3 data lakes.\n- Integration with Glue and IAM enables controlled data access and cataloging.\n\n## Real-World Application\n- Use Lake Formation to define data permissions, register data sources, and build a secure analytics layer on top of S3.","diagram":null,"difficulty":"intermediate","tags":["AWS","Lake Formation","S3","Analytics","certification-mcq","domain-weight-20"],"channel":"aws-sap","subChannel":"accelerate-migration","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T18:30:23.932Z","createdAt":"2026-01-12 18:30:24"},{"id":"aws-sap-accelerate-migration-1768242622781-4","question":"During migration of SAP workloads with strict security and governance requirements, which approach best ensures centralized control over permissions and compliance across multiple accounts?","answer":"[{\"id\":\"a\",\"text\":\"Create individual IAM users with administrative privileges per team\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use AWS Organizations with SCPs and standardized IAM roles\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Rely on on‑prem Active Directory authentication and bypass AWS IAM\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use only temporary credentials from STS without defined roles\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct answer is **B**: Use AWS Organizations with SCPs and standardized IAM roles. This enables centralized policy management, consistent least‑privilege permissions across accounts, and easier governance during migration.\n\n## Why Other Options Are Wrong\n- Option A: Admin privileges per team create broad blast radii and governance gaps.\n- Option C: Relying on on‑prem AD bypasses AWS IAM controls and introduces inconsistencies.\n- Option D: STS temporary credentials are useful but require defined roles and policies; without a structured framework, governance is weaker.\n\n## Key Concepts\n- AWS Organizations for cross‑account governance.\n- Service Control Policies (SCPs) enforce allowed actions at scale.\n\n## Real-World Application\n- Implement a multi‑account strategy with SCPs, standardized IAM roles, and centralized auditing to meet compliance during migration.","diagram":null,"difficulty":"intermediate","tags":["AWS","Organizations","IAM","Governance","certification-mcq","domain-weight-20"],"channel":"aws-sap","subChannel":"accelerate-migration","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T18:30:24.106Z","createdAt":"2026-01-12 18:30:24"},{"id":"aws-sap-continuous-improvement-1768228087580-0","question":"Your legacy web application runs in an Auto Scaling group on EC2 instances with fluctuating load. You want to continuously optimize costs by rightsizing the instance types over time. Which approach best supports this goal?","answer":"[{\"id\":\"a\",\"text\":\"Manually monitor usage and periodically resize instances to the next larger size\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Enable AWS Compute Optimizer to generate rightsizing recommendations and implement changes\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Migrate to a single larger instance to handle spikes\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use Spot Instances exclusively to cut costs\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct because AWS Compute Optimizer analyzes historical utilization and suggests rightsizing changes for EC2 instances, helping you reduce costs without sacrificing performance.\n\n## Why Other Options Are Wrong\n- Option A relies on manual, periodic resizing and is not a continuous, scalable approach.\n- Option C risks overprovisioning and can degrade fault tolerance during spikes.\n- Option D may save costs but compromises availability and reliability in steady-state workloads.\n\n## Key Concepts\n- AWS Compute Optimizer\n- Rightsizing and cost optimization\n- EC2 Auto Scaling alignment with instance types\n\n## Real-World Application\n- Run Compute Optimizer, review recommendations, update your ASG launch templates, and monitor metrics to validate performance and cost impact.","diagram":null,"difficulty":"intermediate","tags":["AWS Compute Optimizer","EC2 Auto Scaling","Rightsizing","Cost Optimization","certification-mcq","domain-weight-25"],"channel":"aws-sap","subChannel":"continuous-improvement","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T14:28:07.581Z","createdAt":"2026-01-12 14:28:07"},{"id":"aws-sap-continuous-improvement-1768228087580-1","question":"An ECS service deployed in a production environment is critical to customers. You want to minimize the risk of faulty releases and support a gradual traffic shift with automatic rollback if issues are detected. Which deployment strategy best achieves this?","answer":"[{\"id\":\"a\",\"text\":\"Blue/green deployment with Route 53 failover\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Canary deployment using AWS CodeDeploy for ECS with weighted traffic shift\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Red/black deployment using CloudFormation change sets\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Deploying a new version with no traffic governance\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct because canary deployments with CodeDeploy allow you to gradually route a small percentage of traffic to the new version, observe metrics, and automatically rollback if issues arise.\n\n## Why Other Options Are Wrong\n- Option A provides blue/green but Route 53 is less integrated with ECS canaries and may require full cutover.\n- Option C is not a standard ECS canary pattern and lacks built-in automated rollback.\n- Option D exposes customers to risk by deploying without traffic gating.\n\n## Key Concepts\n- Canary deployment\n- AWS CodeDeploy for ECS\n- Traffic shifting and automated rollback\n\n## Real-World Application\n- Configure a small canary weight (e.g., 10%), monitor latency and error rates, then gradually increase if healthy or rollback automatically if metrics deteriorate.","diagram":null,"difficulty":"intermediate","tags":["AWS CodeDeploy","ECS","Canary Deployments","Traffic Shaping","certification-mcq","domain-weight-25"],"channel":"aws-sap","subChannel":"continuous-improvement","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T14:28:08.069Z","createdAt":"2026-01-12 14:28:08"},{"id":"aws-sap-continuous-improvement-1768228087580-2","question":"An aging web application runs on EC2 behind an ALB and occasionally an instance in a single availability zone becomes unhealthy. You want automated remediation to restore service quickly with minimal manual intervention. Which approach best achieves this?","answer":"[{\"id\":\"a\",\"text\":\"Increase Auto Scaling group capacity to trigger more instances\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Create an EventBridge rule to trigger a Lambda function that restarts the web service via Systems Manager Run Command\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Disable alarms to avoid incident noise\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Migrate to a different region to avoid AZ failures\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct because EventBridge can trigger a Lambda that uses SSM Run Command to restart the service on the affected instance, enabling automated remediation without manual steps.\n\n## Why Other Options Are Wrong\n- Option A may help scale but does not target rapid remediation of a single unhealthy instance.\n- Option C removes visibility and is poor practice for reliability.\n- Option D is disruptive and does not address the root cause in a single AZ.\n\n## Key Concepts\n- EventBridge (CloudWatch Events)\n- AWS Lambda\n- Systems Manager Run Command for remote actions\n\n## Real-World Application\n- Rule triggers on health-change events, Lambda runs a remediation script to restart the web service and collect diagnostics.","diagram":null,"difficulty":"intermediate","tags":["EventBridge","Lambda","SSM","Automation","certification-mcq","domain-weight-25"],"channel":"aws-sap","subChannel":"continuous-improvement","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T14:28:08.523Z","createdAt":"2026-01-12 14:28:08"},{"id":"aws-sap-continuous-improvement-1768228087580-3","question":"A DynamoDB table experiences high read latency under heavy traffic. You want to improve read performance without changing application code. Which AWS feature should you add?","answer":"[{\"id\":\"a\",\"text\":\"Add read replicas for DynamoDB\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Add a DynamoDB Accelerator (DAX) cluster\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Move to RDS with a caching layer\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Place Elasticache in front of DynamoDB without code changes\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct because DynamoDB Accelerator (DAX) provides a managed in-memory cache for DynamoDB reads, reducing latency for read-heavy workloads without significant code changes.\n\n## Why Other Options Are Wrong\n- Option A DynamoDB does not support read replicas like relational databases.\n- Option C introduces a relational database and requires more changes and tuning.\n- Option D Elasticache in front of DynamoDB would require architectural changes and additional cache invalidation logic; DAX is purpose-built for DynamoDB.\n\n## Key Concepts\n- DynamoDB Accelerator (DAX)\n- In-memory caching for DynamoDB\n- Latency reduction for read-heavy workloads\n\n## Real-World Application\n- Deploy a DAX cluster in the same region, point reads to DAX, monitor latency improvements, and adjust read patterns as traffic grows.","diagram":null,"difficulty":"intermediate","tags":["DAX","DynamoDB","Caching","Performance","certification-mcq","domain-weight-25"],"channel":"aws-sap","subChannel":"continuous-improvement","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T14:28:08.687Z","createdAt":"2026-01-12 14:28:08"},{"id":"aws-sap-continuous-improvement-1768228087580-4","question":"Across multiple AWS accounts, you need centralized logging for audits and debugging with scalable data retention. Which approach best achieves this while minimizing ongoing operational effort?","answer":"[{\"id\":\"a\",\"text\":\"Enable CloudWatch Logs in each account and export to a centralized S3 bucket manually\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use AWS Organizations with centralized CloudWatch Logs and subscription filters to a central account\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Use CloudTrail logs only in each account for governance\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use a third-party SIEM with no AWS-native integration\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct because AWS Organizations enables centralized management, and cross-account CloudWatch Logs with subscription filters allow automatic aggregation into a central account or log analytics platform, reducing operational overhead.\n\n## Why Other Options Are Wrong\n- Option A introduces manual effort and lacks automatic aggregation and scaling.\n- Option C focuses only on API activity logs and misses full resource configuration visibility across accounts.\n- Option D adds external dependency and integration work without AWS-native benefits.\n\n## Key Concepts\n- AWS Organizations\n- Cross-account CloudWatch Logs\n- Centralized logging and auditing\n\n## Real-World Application\n- Configure an aggregator log group in the central account, attach resource-based policies, and set up subscription filters to stream logs from member accounts to the central store or OpenSearch.","diagram":null,"difficulty":"intermediate","tags":["CloudWatch","AWS Organizations","Cross-Account Logging","Audit","certification-mcq","domain-weight-25"],"channel":"aws-sap","subChannel":"continuous-improvement","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T14:28:08.850Z","createdAt":"2026-01-12 14:28:08"},{"id":"aws-sap-design-new-solutions-1768173684932-0","question":"Your company plans a multi-region SAP S/4HANA deployment on AWS to support 2,000 concurrent users with an RPO of 5 minutes and an RTO of 15 minutes. Which design best meets these objectives while minimizing downtime during region failover?","answer":"[{\"id\":\"a\",\"text\":\"Deploy SAP HANA in two AWS regions with synchronous SAP HANA System Replication (HSR), ALB in each region, Route 53 health checks, and cross-region EBS snapshots for backups.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Deploy SAP HANA in two regions with asynchronous replication and rely on DNS failover.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Deploy a single SAP HANA cluster in one region with zone-redundant HA and daily backups to S3.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use a third-party DR provider to replicate SAP HANA to a separate cloud.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA design with synchronous SAP HANA System Replication across two regions provides near-zero RPO and quick failover, meeting the 5-minute RPO and 15-minute RTO when combined with Route 53 failover and cross-region EBS backups.\n\n## Why Other Options Are Wrong\n- B uses asynchronous replication, which typically yields higher RPO and longer failover times, not meeting the 5-minute RPO.\n- C confines the deployment to a single region, eliminating cross-region DR and risking longer downtime during regional outages.\n- D adds external complexity and latency without AWS-native optimization.\n\n## Key Concepts\n- SAP HANA System Replication (HSR)\n- Cross-region DR in AWS\n- RPO/RTO definitions\n- Route 53 health checks and failover\n\n## Real-World Application\n- Implement HSR between SAP HANA instances across regions, configure health checks, and automate DNS failover. Regularly test failover drills and update DR runbooks.","diagram":null,"difficulty":"intermediate","tags":["AWS","SAP HANA","S3","EBS","Route53","HSR","certification-mcq","domain-weight-29"],"channel":"aws-sap","subChannel":"design-new-solutions","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T23:21:24.933Z","createdAt":"2026-01-11 23:21:25"},{"id":"aws-sap-design-new-solutions-1768173684932-1","question":"To meet compliance requiring encryption at rest for SAP backups and in transit, which design is most appropriate?","answer":"[{\"id\":\"a\",\"text\":\"Use AWS KMS customer-managed keys (CMKs) for EBS and S3; enable server-side encryption with KMS (SSE-KMS) on S3; enforce TLS 1.2+ for all data in transit; assign IAM roles with least privilege.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Use SSE-S3 for backups and enable TLS, but do not use KMS.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Store keys in Secrets Manager and use envelope encryption for backups.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use external hardware security module (HSM) and manage keys outside AWS.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nUsing AWS KMS CMKs for EBS and S3 with SSE-KMS and TLS 1.2+ ensures encryption at rest and in transit with centralized key management and auditability.\n\n## Why Other Options Are Wrong\n- B lacks KMS-based encryption for S3 backups, reducing control and auditability.\n- C Secrets Manager stores secrets, not encryption keys for at-rest encryption of backups; envelope encryption alone does not guarantee proper key management for S3/EBS without KMS.\n- D adds unnecessary complexity; AWS KMS provides managed, auditable encryption suitable for this scenario.\n\n## Key Concepts\n- SSE-KMS and KMS CMKs\n- TLS 1.2+ for in-transit encryption\n- IAM least-privilege access\n\n## Real-World Application\n- Create a CMK in KMS, enable SSE-KMS on S3 and EBS using that key, enforce TLS on all endpoints, and implement rotation and access controls.","diagram":null,"difficulty":"intermediate","tags":["AWS","KMS","SSE-KMS","S3","EBS","TLS","certification-mcq","domain-weight-29"],"channel":"aws-sap","subChannel":"design-new-solutions","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T23:21:25.427Z","createdAt":"2026-01-11 23:21:25"},{"id":"aws-sap-design-new-solutions-1768173684932-2","question":"Your organization is planning a scalable data analytics platform on AWS using S3, AWS Glue, and Lake Formation. Which design best ensures data governance, access control, and cost-efficient analytics?","answer":"[{\"id\":\"a\",\"text\":\"Use a centralized data lake in S3 with AWS Lake Formation for fine-grained access control, catalog data with Glue, and use Athena or Redshift Spectrum for analytics; implement data classifications and lifecycle policies.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Store data in S3 without Lake Formation and rely solely on IAM policies for access control.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use DynamoDB as a metadata store and run analytics directly from S3 without cataloging.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Replace AWS services with an on-prem data lake and use VPN for access.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA centralized data lake in S3 with Lake Formation provides fine-grained access control and governance, while Glue catalogs metadata and enables analytics via Athena or Redshift Spectrum, with data lifecycle policies to control costs.\n\n## Why Other Options Are Wrong\n- B lacks centralized governance; IAM policies alone are insufficient for fine-grained access and data cataloging.\n- C DynamoDB as a metadata store is not standard for data lakes, and skipping a catalog impairs discoverability and governance.\n- D introduces unnecessary complexity and reduces cloud-native scalability.\n\n## Key Concepts\n- AWS Lake Formation\n- AWS Glue Data Catalog\n- S3 data lake governance and lifecycle\n- Analytics options: Athena, Redshift Spectrum\n\n## Real-World Application\n- Enable Lake Formation, register S3 data, define permissions, build a Glue catalog, and set up analytics workloads with cost controls.","diagram":null,"difficulty":"intermediate","tags":["AWS","Lake Formation","Glue","S3","Athena","Redshift Spectrum","certification-mcq","domain-weight-29"],"channel":"aws-sap","subChannel":"design-new-solutions","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T23:21:25.956Z","createdAt":"2026-01-11 23:21:26"},{"id":"aws-sap-design-new-solutions-1768285982628-0","question":"To design a globally distributed application with sub-second latency for users in all regions and minimal data loss during regional outages, which combination provides true active-active data and fastest failover?","answer":"[{\"id\":\"a\",\"text\":\"Deploy in two regions with RDS Multi-AZ across regions and Route 53 failover; writes only to the primary region\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use DynamoDB Global Tables for multi-region writes, S3 Cross-Region Replication for assets, and AWS Global Accelerator to route traffic to healthy regions with automatic failover\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Use RDS Cross-Region Read Replicas with Route 53 latency routing\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Maintain separate isolated deployments in each region and switch DNS manually during outage\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B provides true active-active data and fast failover by combining DynamoDB Global Tables for multi-region writes, S3 Cross-Region Replication for assets, and AWS Global Accelerator for consistent routing to healthy regions. This setup minimizes data loss (RPO near zero for DynamoDB via Global Tables) and reduces latency with Global Accelerator.\n\n## Why Other Options Are Wrong\n- Option A: RDS Multi-AZ across regions does not support true active-active writes across regions; writes still go to a primary in one region, causing potential data lag during failover.\n- Option C: RDS Cross-Region Read Replicas are read-only; they do not support multi-master writes across regions.\n- Option D: Manual DNS switch and isolated deployments introduce longer recovery time and potential data inconsistency.\n\n## Key Concepts\n- DynamoDB Global Tables enables multi-region, multi-master writes.\n- S3 Cross-Region Replication replicates objects across regions.\n- AWS Global Accelerator optimizes global traffic and provides health-check based failover.\n\n## Real-World Application\n- Use this pattern for SaaS platforms needing low-latency access and strong data durability across continents, with automated failover to healthy regions.","diagram":null,"difficulty":"intermediate","tags":["AWS","DynamoDB","S3","GlobalAccelerator","certification-mcq","domain-weight-29"],"channel":"aws-sap","subChannel":"design-new-solutions","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T06:33:02.629Z","createdAt":"2026-01-13 06:33:02"},{"id":"aws-sap-design-new-solutions-1768285982628-1","question":"Your team runs a Kubernetes-based analytics platform across three AZs; you need shared, scalable, low-latency file storage accessible by all pods. Which storage choice best fits?","answer":"[{\"id\":\"a\",\"text\":\"Per-node EBS volumes with NFS-based shared mounting\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Amazon EFS with the CSI driver mounted in all Kubernetes pods\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"S3-based object storage mounted as a filesystem\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"FSx for Windows File Server\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct because EFS provides a scalable, shared, POSIX-compliant filesystem accessible concurrently by many pods across multiple AZs and integrates with the EFS CSI driver for Kubernetes.\n\n## Why Other Options Are Wrong\n- Option A: Per-node EBS cannot be shared across nodes; it is limited to a single AZ and node.\n- Option C: S3 is object storage, not a POSIX filesystem; mounting requires non-POSIX semantics.\n- Option D: FSx for Windows File Server is a Windows-based file system not ideal for Linux-based Kubernetes workloads.\n\n## Key Concepts\n- EFS provides shared storage for Linux workloads.\n- CSI driver enables dynamic provisioning in Kubernetes.\n- Multi-AZ scalability and throughput.\n\n## Real-World Application\n- Use this pattern to enable analytics jobs that need shared data access across worker pods with consistent latency.","diagram":null,"difficulty":"intermediate","tags":["AWS","EFS","Kubernetes","EKS","CSI","certification-mcq","domain-weight-29"],"channel":"aws-sap","subChannel":"design-new-solutions","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T06:33:03.025Z","createdAt":"2026-01-13 06:33:03"},{"id":"aws-sap-design-new-solutions-1768285982628-2","question":"You manage 10 AWS accounts with strict compliance requirements; you want centralized governance with least privilege per account. What is the most effective approach?","answer":"[{\"id\":\"a\",\"text\":\"Create identical IAM policies in every account and rotate keys every 90 days\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use AWS Organizations with SCPs to implement guardrails across accounts, plus per-account IAM roles with least privilege and centralized logging via CloudTrail and Security Hub\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Use a single master admin account to manage all permissions across accounts\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Rely solely on developer-provided access controls in each account\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct because AWS Organizations with SCPs provides centralized guardrails that enforce least privilege constraints across all accounts, while per-account roles ensure granular permissions and centralized logging via CloudTrail and Security Hub ensures visibility.\n\n## Why Other Options Are Wrong\n- Option A: Putting identical policies in every account without centralized governance increases drift and reduces central oversight.\n- Option C: A single master admin account creates a single point of failure and violates least-privilege design.\n- Option D: Relying only on per-account controls leads to inconsistent security postures and blind spots.\n\n## Key Concepts\n- AWS Organizations and SCPs for governance\n- Least privilege via per-account IAM roles\n- Centralized monitoring via CloudTrail/Security Hub\n\n## Real-World Application\n- Use this pattern to enforce compliance across dozens of accounts with auditable security controls and uniform policy boundaries.","diagram":null,"difficulty":"intermediate","tags":["AWS","Organizations","SCPs","SecurityHub","CloudTrail","certification-mcq","domain-weight-29"],"channel":"aws-sap","subChannel":"design-new-solutions","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T06:33:03.385Z","createdAt":"2026-01-13 06:33:03"},{"id":"aws-sap-design-new-solutions-1768285982628-3","question":"A on-premises MySQL database needs to migrate to AWS RDS with minimal downtime. Which approach is best?","answer":"[{\"id\":\"a\",\"text\":\"Use AWS DMS to continuously replicate changes and perform a cutover when ready\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Take a full backup, restore to RDS, and schedule downtime\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use periodic batch ETL jobs to sync entire data daily\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Ship physical disks to AWS and restore offline\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A is correct because AWS Database Migration Service DMS can continuously replicate changes from the on-premises MySQL to RDS, enabling a near-zero downtime cutover when you switch endpoints.\n\n## Why Other Options Are Wrong\n- Option B introduces downtime and risks data drift if not fully synchronized at the moment of cutover.\n- Option C relies on batch ETL, which cannot guarantee real-time sync and can cause long migration windows.\n- Option D is impractical and slow for most modern migrations and not suitable for a production cutover.\n\n## Key Concepts\n- DMS for continuous replication during migration\n- Minimal downtime cutover strategies\n\n## Real-World Application\n- Use DMS with a pre-cutover validation to ensure a smooth switchover for production databases.","diagram":null,"difficulty":"intermediate","tags":["AWS","DMS","RDS","Migration","certification-mcq","domain-weight-29"],"channel":"aws-sap","subChannel":"design-new-solutions","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T06:33:03.510Z","createdAt":"2026-01-13 06:33:03"},{"id":"aws-sap-design-new-solutions-1768285982628-4","question":"You require low-latency, high-throughput hybrid connectivity between on-prem data center and AWS across multiple regions. Which networking design is most robust?","answer":"[{\"id\":\"a\",\"text\":\"VPN over the Internet with no dedicated links\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use AWS Direct Connect with private VIFs in multiple regions, attached to a central Transit Gateway, and optional DX Gateway to connect on-prem to multiple regions, with VPN as backup\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Public internet-based VPN only\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Only Direct Connect in a single region\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct because Direct Connect with private VIFs in multiple regions attached to a central Transi t Gateway (and optional Direct Connect Gateway) provides low-latency, deterministic network performance with a robust multi-region, hybrid architecture; VPN serves as a failover path.\n\n## Why Other Options Are Wrong\n- Option A lacks dedicated bandwidth guarantees and cross-region coherence.\n- Option C relies solely on the public Internet, which introduces variability and higher latency.\n- Option D ignores multi-region resilience and cross-region reachability.\n\n## Key Concepts\n- Direct Connect private VIFs\n- Transit Gateway as a hub for multi-region connectivity\n- DX Gateway for cross-region on-prem connections\n\n## Real-World Application\n- Use this design for workloads requiring predictable latency, such as real-time analytics and hybrid DR scenarios.","diagram":null,"difficulty":"intermediate","tags":["AWS","DirectConnect","TransitGateway","DXGateway","HybridNetworking","certification-mcq","domain-weight-29"],"channel":"aws-sap","subChannel":"design-new-solutions","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T06:33:03.636Z","createdAt":"2026-01-13 06:33:03"},{"id":"aws-sap-design-solutions-1768213502907-0","question":"A global enterprise adopts a new multi-account landing zone and requires a centralized security baseline with default deny for all actions not explicitly allowed, while enabling teams to create their own resources without elevated privileges. Which combination delivers this with minimal friction?","answer":"[{\"id\":\"a\",\"text\":\"Centralize IAM in the management account and rely on manual cross-account access\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use AWS Organizations SCPs alone to limit actions across accounts\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Deploy AWS Control Tower with Account Factory and prebuilt guardrails\",\"isCorrect\":true},{\"id\":\"d\",\"text\":\"Create per-account IAM admins and permissions boundaries manually\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption C. AWS Control Tower with Account Factory and built-in guardrails enforces a centralized security baseline and enables delegated account provisioning, providing default-deny posture via guardrails and allowed actions via preconfigured permissions. This delivers governance with minimal friction.\n\n## Why Other Options Are Wrong\n- A: Centralizing IAM in the management account and cross-account access still lacks automated guardrails and scalable baseline enforcement across all accounts.\n- B: SCPs alone won't enable team resource creation with minimal friction; not enough guardrails across accounts.\n- D: Manual per-account admins and boundaries create security drift and are not scalable across many accounts.\n\n## Key Concepts\n- Multi-account landing zone\n- Guardrails and SCPs\n- Account Factory and automated provisioning\n- Delegated administration\n\n## Real-World Application\n- Organizations implement landing zones with Control Tower to scale governance across dozens of accounts while enabling teams to create resources within allowed boundaries.","diagram":null,"difficulty":"intermediate","tags":["AWS","Control Tower","Organizations","SCP","Landing Zone","Governance","certification-mcq","domain-weight-26"],"channel":"aws-sap","subChannel":"design-solutions","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T10:25:02.908Z","createdAt":"2026-01-12 10:25:03"},{"id":"aws-sap-design-solutions-1768213502907-1","question":"Your company uses 60 AWS accounts under a single Organization. You need centralized auditing and security findings with scalable compliance reporting. Which architecture best achieves this?","answer":"[{\"id\":\"a\",\"text\":\"Enable CloudTrail in each account with a separate S3 bucket in each account\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Create a dedicated logging account, enable organization-wide CloudTrail, and aggregate findings in Security Hub in the logging account\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Enable GuardDuty in each account and forward findings via SNS\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use IAM Access Analyzer across accounts to compile findings\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B. Centralizing logging in a dedicated account with Organization-wide CloudTrail and Security Hub aggregation provides scalable, auditable, and compliant visibility across many accounts. This approach minimizes management overhead and supports centralized monitoring.\n\n## Why Other Options Are Wrong\n- A: Centralized logging is not achieved; logs remain split across accounts and require cross-account access setup.\n- C: GuardDuty is reactive per account; aggregating findings manually is complex and not scalable.\n- D: Access Analyzer helps with permissions, not comprehensive security findings aggregation or centralized auditing.\n\n## Key Concepts\n- AWS Organizations\n- CloudTrail multi-account logging\n- Security Hub aggregation\n- Centralized governance\n\n## Real-World Application\n- Enterprises standardize compliance reporting by centralizing audit trails and security findings in a single logging account for internal and external audits.","diagram":null,"difficulty":"intermediate","tags":["AWS","CloudTrail","SecurityHub","Organizations","CentralizedLogging","certification-mcq","domain-weight-26"],"channel":"aws-sap","subChannel":"design-solutions","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T10:25:03.427Z","createdAt":"2026-01-12 10:25:03"},{"id":"aws-sap-design-solutions-1768213502907-2","question":"To enable safe sharing of network resources across accounts, you want a centralized services VPC and allow member accounts to access its services while maintaining their own VPC isolation. Which approach best achieves this?","answer":"[{\"id\":\"a\",\"text\":\"Create VPC peering connections from each member account to the shared services VPC\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use AWS RAM to share the shared services VPC's subnets with member accounts (VPC sharing)\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Create a Transit Gateway and attach all accounts\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Expose services via the public internet with controlled ACLs\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B. VPC sharing via AWS RAM allows multiple accounts to use a centralized set of subnets in the shared VPC while maintaining their own VPC isolation, reducing complexity and cross-account networking overhead.\n\n## Why Other Options Are Wrong\n- A: VPC peering requires many peering connections as accounts grow, which is not scalable.\n- C: Transit Gateway adds cost and management overhead; while viable, it’s not as scalable for shared services without separate VPCs.\n- D: Public exposure undermines isolation and security baselines.\n\n## Key Concepts\n- AWS RAM\n- VPC sharing\n- Centralized services VPC\n- Cross-account network design\n\n## Real-World Application\n- Large orgs share authentication or logging endpoints from a central VPC across dozens of accounts with controlled access.","diagram":null,"difficulty":"intermediate","tags":["AWS","RAM","VPCSharing","NetworkGovernance","SharedServices","certification-mcq","domain-weight-26"],"channel":"aws-sap","subChannel":"design-solutions","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T10:25:03.962Z","createdAt":"2026-01-12 10:25:04"},{"id":"aws-sap-design-solutions-1768259863881-0","question":"An organization uses AWS Organizations with multiple accounts and wants scalable governance and cost control. Which approach provides centralized governance while enabling independent account operations?","answer":"[{\"id\":\"a\",\"text\":\"Create a single IAM role that is allowed to manage all accounts\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use AWS Organizations service control policies with OU-based guardrails applied across accounts\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Implement per-account VPC CIDR ranges to isolate resources\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Consolidate all resources into a single account and manage centrally\",\"isCorrect\":false}]","explanation":"## Correct Answer\nUse AWS Organizations service control policies with OU-based guardrails applied across accounts\n\n## Why Other Options Are Wrong\n- Create a single IAM role that is allowed to manage all accounts: IAM roles do not enforce organization-wide governance and would create excessive breadth of access per account.\n- Implement per-account VPC CIDR ranges: Networking design aids segmentation but does not provide centralized governance or guardrails.\n- Consolidate all resources into a single account: Increases blast radius and reduces isolation between teams.\n\n## Key Concepts\n- AWS Organizations\n- Service Control Policies (SCPs)\n- Organizational Units (OUs)\n\n## Real-World Application\nImplement an OU hierarchy (e.g., Root > Security > Production > Dev) and apply SCPs to constrain permissions and services per OU, enabling centralized governance while allowing teams to operate in their own accounts.","diagram":null,"difficulty":"intermediate","tags":["AWS Organizations","SCPs","Governance","Cross-Account","CloudTrail","Cost Management","certification-mcq","domain-weight-26"],"channel":"aws-sap","subChannel":"design-solutions","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T23:17:43.881Z","createdAt":"2026-01-12 23:17:44"},{"id":"aws-sap-design-solutions-1768259863881-1","question":"In a multi-account Terraform deployment, teams manage separate state files in their respective accounts. What approach provides safe, scalable state management across accounts with strong isolation and history?","answer":"[{\"id\":\"a\",\"text\":\"Per-account local state files stored in each repository\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use a centralized remote backend (S3 with DynamoDB locking) and separate workspaces per account\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use Terraform Cloud with separate workspaces per account and VCS integration\",\"isCorrect\":true},{\"id\":\"d\",\"text\":\"Manually manage state in a shared Git repository\",\"isCorrect\":false}]","explanation":"## Correct Answer\nUse Terraform Cloud with separate workspaces per account and VCS integration\n\n## Why Other Options Are Wrong\n- Per-account local state files: Local state lacks central governance, hard to audit, and prone to drift.\n- Centralized S3 backend with separate workspaces: While workable, it lacks a single, auditable source of truth and central access controls for multi-team governance.\n- Manual state in a shared Git repo: Prone to race conditions and corruption; not designed for Terraform state management.\n\n## Key Concepts\n- Terraform Cloud / Terraform Enterprise\n- Remote state isolation\n- VCS integration and audit history\n\n## Real-World Application\nAdopt Terraform Cloud with per-account workspaces, enforcing role-based access, remote state locking, and full change history, while integrating with your code review workflow.","diagram":null,"difficulty":"intermediate","tags":["Terraform","Terraform Cloud","Remote State","Multi-Account","CI/CD","certification-mcq","domain-weight-26"],"channel":"aws-sap","subChannel":"design-solutions","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T23:17:44.285Z","createdAt":"2026-01-12 23:17:44"},{"id":"aws-sap-design-solutions-1768259863881-2","question":"A data processing platform deployed on EKS spans two regions. To minimize RTO/RPO during regional outages, which DR pattern should you implement?","answer":"[{\"id\":\"a\",\"text\":\"Multi-region active-active EKS clusters with cross-region data replication\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Active-passive standby cluster in a secondary region with automated failover and cross-region data replication\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Failover to a single regional disaster recovery site with manual failover\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Rely on intra-region availability zones and backups only\",\"isCorrect\":false}]","explanation":"## Correct Answer\nActive-passive standby cluster in a secondary region with automated failover and cross-region data replication\n\n## Why Other Options Are Wrong\n- Multi-region active-active EKS with cross-region data replication: Complex to maintain consistency for stateful components; higher operational complexity and cost.\n- Failover to a single regional disaster recovery site with manual failover: Increases RTO and potential human error; not ideal for rapid recovery.\n- Rely on intra-region AZs and backups only: Does not address cross-region availability requirements.\n\n## Key Concepts\n- Disaster Recovery patterns (active-active vs active-passive)\n- EKS multi-region considerations\n- Data replication and failover automation\n\n## Real-World Application\nDesign DR scripts and automated failover for the standby region, ensure cross-region replication for critical data stores, and regularly test failover drills.","diagram":null,"difficulty":"intermediate","tags":["EKS","Kubernetes","DR","Cross-Region","Route 53","S3","Data Replication","certification-mcq","domain-weight-26"],"channel":"aws-sap","subChannel":"design-solutions","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T23:17:44.677Z","createdAt":"2026-01-12 23:17:44"},{"id":"aws-sap-design-solutions-1768259863881-3","question":"You want to centralize security events and logs from all AWS accounts into a single logging account with proper retention and compliance controls. Which approach is best?","answer":"[{\"id\":\"a\",\"text\":\"Enable CloudWatch Logs in each account and use a central aggregator account with cross-account log streaming\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Keep logs local in each account and periodically export to a central S3 bucket without streaming\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use S3 to store logs in each account and aggregate manually\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Disable logging to save costs and rely on incident response only\",\"isCorrect\":false}]","explanation":"## Correct Answer\nEnable CloudWatch Logs in each account and use a central aggregator account with cross-account log streaming\n\n## Why Other Options Are Wrong\n- Centralized streaming is not implemented; manual export introduces gaps and delays.\n- Manual aggregation in S3 without streaming loses real-time insights and increases operational burden.\n- Disabling logging is not acceptable for compliance and security visibility.\n\n## Key Concepts\n- CloudWatch Logs cross-account access\n- Centralized logging account\n- Retention and compliance controls (log retention policies, IAM roles)\n\n## Real-World Application\nConfigure a central logging account, create cross-account log streams from each member account to a central log group, apply retention policies, and enable alerting on critical events.","diagram":null,"difficulty":"intermediate","tags":["CloudWatch","CloudWatch Logs","EventBridge","Cross-Account","Security","certification-mcq","domain-weight-26"],"channel":"aws-sap","subChannel":"design-solutions","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T23:17:44.817Z","createdAt":"2026-01-12 23:17:44"},{"id":"aws-sap-design-solutions-1768259863881-4","question":"To enforce consistent network security across multiple VPCs and accounts, you want centralized firewall policies with per-VPC deployment and stateful inspection. Which design is most appropriate?","answer":"[{\"id\":\"a\",\"text\":\"Deploy per-VPC security groups and rely on VPC peering to share rules\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use AWS Network Firewall with centralized policies and per-VPC deployment\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Use a single global security group across all VPCs\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Rely solely on NACLs in each VPC for all traffic control\",\"isCorrect\":false}]","explanation":"## Correct Answer\nUse AWS Network Firewall with centralized policies and per-VPC deployment\n\n## Why Other Options Are Wrong\n- Per-VPC security groups and VPC peering: Peering does not centralize policy management and can complicate rule consistency.\n- A single global security group: Security groups are scoped to a VPC; cannot be shared globally across VPCs.\n- NACLs alone: NACLs are stateless and provide limited, coarse control; they do not offer stateful inspection or centralized management.\n\n## Key Concepts\n- AWS Network Firewall\n- Centralized policy management\n- Stateful inspection\n\n## Real-World Application\nDefine firewall policies centrally, deploy them per VPC via Network Firewall rulesets, and automate policy drift checks to ensure consistency across accounts.","diagram":null,"difficulty":"intermediate","tags":["AWS Network Firewall","Security","VPC","Cross-Account","Governance","certification-mcq","domain-weight-26"],"channel":"aws-sap","subChannel":"design-solutions","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T23:17:44.959Z","createdAt":"2026-01-12 23:17:45"},{"id":"q-1005","question":"You operate SAP S/4HANA on AWS with analytics in Snowflake and must implement a data masking/tokenization pipeline so analytics do not expose PII. Design end-to-end data flow, masking rules by field, latency (<5 minutes), and governance using KMS/IAM. Include auditing, rollback, and failover considerations?","answer":"Design a masking/tokenization pipeline from SAP S/4HANA to Snowflake: implement a CDC/streaming layer to capture changes, apply field-level masking rules (PII, sensitive IDs), store masked data in Sno","explanation":"## Why This Is Asked\nEvaluates practical data protection across SAP on AWS with Snowflake analytics, balancing real-time access and compliance.\n\n## Key Concepts\n- SAP S/4HANA data ingestion and masking\n- Field-level masking rules for PII\n- Snowflake masking policies and data sharing\n- AWS KMS key management and IAM governance\n- Auditing, rollback, and failover strategies\n\n## Code Example\n```javascript\n// Example masking function\nfunction maskPII(record) {\n  const r = {...record};\n  if (r.email) r.email = r.email.replace(/(.{2}).+(@.*)/, '$1***$2');\n  if (r.phone) r.phone = r.phone.replace(/(\\d{3})\\d{4}(\\d{3})/, '$1***$2');\n  if (r.ssn) r.ssn = '***-**-' + r.ssn.slice(-4);\n  return r;\n}\n```\n\n## Follow-up Questions\n- How would you validate masking without leaking data in logs?\n- How would you rotate keys and propagate policy changes across Snowflake without downtime?","diagram":"flowchart TD\n  A[SAP S/4HANA on AWS] --> B[Masking Layer]\n  A --> C[Snowflake Analytics]\n  B --> C\n  C --> D[Auditing & Compliance]\n  D --> E[Rollback & Failover]","difficulty":"intermediate","tags":["aws-sap"],"channel":"aws-sap","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Oracle","Snap","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T18:50:40.464Z","createdAt":"2026-01-12T18:50:40.464Z"},{"id":"q-1011","question":"How would you implement an automated cross-region DR for SAP HANA on AWS? Use SAP HANA System Replication with Region A as primary and Region B as hot standby, orchestrated by AWS Step Functions; back up to EBS/S3 with Data Lifecycle Manager and cross-region KMS keys; ensure automated DR tests, rollback playbooks, and meet RPO <5 minutes, RTO <15 minutes?","answer":"Implement cross-region DR for SAP HANA on AWS: use SAP HANA System Replication with Region A as primary and Region B as hot standby, orchestrated by AWS Step Functions; back up to EBS/S3 with Data Lif","explanation":"## Why This Is Asked\nTests advanced DR design for SAP on AWS, leveraging SAP replication and native AWS orchestration to minimize downtime.\n\n## Key Concepts\n- SAP HANA System Replication across regions\n- AWS Step Functions orchestration\n- EBS/S3 backups with Data Lifecycle Manager (DLM)\n- Cross-region KMS key management\n- Automated DR testing and rollback planning\n\n## Code Example\n```javascript\n// AWS SDK v3 example to start a DR test workflow\nconst { SFNClient, StartExecutionCommand } = require('@aws-sdk/client-sfn');\nconst c = new SFNClient({ region: 'us-east-1' });\nconst cmd = new StartExecutionCommand({ stateMachineArn: 'arn:aws:states:us-east-1:123456789012:stateMachine:SAP_DR_Test', input: JSON.stringify({ test: true }) });\nc.send(cmd).then(console.log).catch(console.error);\n```\n\n## Follow-up Questions\n- How would you validate RPO during a live failover?\n- What monitoring would you add to detect replication lag across regions?\n- How would you perform a controlled rollback if the DR test reveals issues?","diagram":null,"difficulty":"advanced","tags":["aws-sap"],"channel":"aws-sap","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["PayPal","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T19:23:27.790Z","createdAt":"2026-01-12T19:23:27.790Z"},{"id":"q-1059","question":"Design an advanced SAP S/4HANA on AWS architecture using SAP HANA MDC on EC2 across 3 AZs, with analytics separated into a data lake. Propose a rolling OS/kernel/SAP patching and upgrade strategy that preserves near-zero downtime, enables automated cross-region DR testing, and guarantees RPO <2 minutes and RTO <5 minutes; specify automation, services, failure modes, and rollback?","answer":"Use SAP HANA MDC across 3 AZs, with analytics in a separate data lake. Patch OS/kernel with AWS Systems Manager Automation and a rolling upgrade plan, tightly coupling SAP kernel patching with HANA ba","explanation":"## Why This Is Asked\n\nThis question probes real-world orchestration of patch windows, SAP MDC, cross-region DR, and automation, ensuring RPO/RTO, including rollback and failover plan across multiple AWS services.\n\n## Key Concepts\n\n- SAP HANA MDC across multi-AZ\n- Rolling OS/kernel/SAP patching\n- Cross-region DR with AWS Step Functions\n- DR testing automation\n- Data security with KMS, IAM, Route 53\n\n## Code Example\n\n```javascript\n// Sample Step Functions state machine (pseudo)\n{\n  \"Comment\": \"SAP patch + DR flow\",\n  \"StartAt\": \"RollPatch\",\n  \"States\": {\n    \"RollPatch\": {\"Type\": \"Task\", \"Next\": \"ValidatePatch\"},\n    \"ValidatePatch\": {\"Type\": \"Choice\", \"Choices\": [{\"Variable\": \"$.patchOk\",\"BooleanEquals\": true,\"Next\":\"InitiateFailover\"}], \"Default\":\"Rollback\"},\n    \"InitiateFailover\": {\"Type\":\"Task\",\"Next\":\"RunDRTest\"},\n    \"RunDRTest\": {\"Type\":\"Task\",\"End\":true}\n  }\n}\n```\n\n## Follow-up Questions\n\n- How would you monitor and alert if DR test fails mid-run?\n- What considerations exist for maintaining data consistency during rolling patches?","diagram":"flowchart TD\n  MDC[SAP HANA MDC] --> DR[DR Orchestration]\n  MDC --> Patch[Rolling Patching]\n  DR --> Test[Automated DR Test]\n  Test --> Rollback[Rollback to Snapshot]","difficulty":"advanced","tags":["aws-sap"],"channel":"aws-sap","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Hashicorp","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T21:21:11.072Z","createdAt":"2026-01-12T21:21:11.072Z"},{"id":"q-1179","question":"Design an automated, auditable patching workflow for SAP S/4HANA on EC2 across multiple AWS accounts and regions. Use AWS Systems Manager Patch Manager to patch OS and SAP kernel updates with rolling upgrades, pre/post checks, and automated rollback if SLA drift occurs. Include governance, approvals, testing, and validation of success before go-live?","answer":"Leverage a cross-account patching pipeline: define SSM patch baselines and patch groups for SAP-enabled EC2s, coordinate OS and SAP kernel updates with rolling upgrades across AZs, and gate via CodePi","explanation":"## Why This Is Asked\n\nAssesses real-world, multi-account patch governance for SAP on AWS, including OS and SAP kernel updates, downtime control, and rollback in production.\n\n## Key Concepts\n\n- AWS Systems Manager Patch Manager baselines, patch groups, and maintenance windows\n- SAP kernel patching workflow and compatibility testing\n- Cross-account and cross-region orchestration, approvals, and auditing\n- Automated rollback, health checks, and SLA validation\n\n## Code Example\n\n```javascript\n// Pseudo-code: start patch workflow via SDK when approvals granted\nconst startPatchWorkflow = async (config) => {\n  // initialize CodePipeline or Step Functions with patch plan\n  // trigger OS patches, then SAP kernel patches, with rolling upgrades\n  // attach pre/post health checks and rollback hooks\n};\n```\n\n## Follow-up Questions\n\n- How would you test rollback and failover in a dry-run?\n- What metrics and alarms confirm patch success without SAP downtime?","diagram":null,"difficulty":"advanced","tags":["aws-sap"],"channel":"aws-sap","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Citadel","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T03:42:02.594Z","createdAt":"2026-01-13T03:42:02.594Z"},{"id":"q-1186","question":"Design a multi-account SAP S/4HANA on AWS with SAP HANA MDC across 3 AZs and a cross-region DR setup. Route SAP system and security logs to a centralized, cross-account S3 data lake with Object Lock (WORM) and cross-region replication. Use AWS Glue/Data Catalog and Lake Formation for lineage and access control; enforce least privilege with SCPs. Automate DR tests and integrity checks?","answer":"Design a multi-account SAP S/4HANA on AWS with MDC HANA across 3 AZs and cross-region DR. Route SAP system and security logs to a centralized, cross-account S3 data lake with WORM using Object Lock, r","explanation":"## Why This Is Asked\nA realistic multi-account, cross-region SAP deployment with immutable audit trails tests both control-plane security and data-plane resiliency under compliance constraints.\n\n## Key Concepts\n- SAP S/4HANA with MDC on AWS\n- S3 Object Lock for immutability\n- Cross-account Lake Formation and Glue Data Catalog\n- Service Control Policies and least privilege\n- Automated DR testing with end-to-end integrity checks\n\n## Code Example\n```bash\naws s3api put-object-lock-configuration --bucket sap-logs --object-lock-configuration '{\"ObjectLockEnabled\":\"Enabled\",\"Rule\":{\"DefaultRetention\":{\"Mode\":\"COMPLIANCE\",\"Days\":3650}}}' --region us-east-1\n```\n\n## Follow-up Questions\n- How would you monitor policy drift across accounts?\n- How would you validate restoration integrity from the immutable lake?","diagram":"flowchart TD\nA[SAP S/4HANA] --> B[HANA MDC across 3 AZs]\nA --> C[Cross-region DR]\nB --> D[Logs to S3 Lake]\nD --> E[DR region replication]","difficulty":"advanced","tags":["aws-sap"],"channel":"aws-sap","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","Lyft","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T04:38:36.918Z","createdAt":"2026-01-13T04:38:36.918Z"},{"id":"q-673","question":"You manage a small SAP NetWeaver footprint on AWS using EC2 for the app tier and a separate DB tier on HANA. Describe a practical single-region HA and backup plan to meet an RPO of 15 minutes and an RTO of 60 minutes. Include chosen services, EC2 sizing approach, storage strategy (EBS/S3), backup schedule, and a cost-conscious trade-off you’d consider?","answer":"I would design a two-AZ SAP NetWeaver stack with a synchronous HANA replication primary/standby, app tier in an Auto Scaling group behind a load balancer, and daily full backups with 15-minute log bac","explanation":"## Why This Is Asked\nTests practical HA/backup planning for SAP on AWS, balancing RPO/RTO within a single region while considering cost.\n\n## Key Concepts\n- SAP on AWS HA patterns: cross-AZ, synchronous replication, application DB separation\n- Storage: EBS for volumes, S3 for backups, lifecycle policies\n- Automation: CloudFormation/Infra as code, CloudWatch alarms, Route 53 health checks\n- Cost trade-offs: standby sizing vs on-demand spin-up, reserved instances, spot usage\n\n## Code Example\n```yaml\n# Minimal CloudFormation skeleton for SAP web tier ASG with EBS-backed volumes\nResources:\n  SAPWebASG:\n    Type: AWS::AutoScaling::AutoScalingGroup\n    Properties:\n      MinSize: 2\n      MaxSize: 4\n      LaunchConfigurationName: !Ref SAPLaunchCfg\n```\n\n## Follow-up Questions\n- How often would you test DR and what metrics matter?\n- What changes for cross-region DR would you consider?","diagram":"flowchart TD\n  A[Request] --> B[Define SAP workload]\n  B --> C[Select EC2 sizing & storage]\n  C --> D[Plan HA/DR within region]\n  D --> E[Automate with IaC]\n  E --> F[Monitoring & backup]","difficulty":"beginner","tags":["aws-sap"],"channel":"aws-sap","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Microsoft","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T14:45:06.666Z","createdAt":"2026-01-11T14:45:06.670Z"},{"id":"q-900","question":"Scenario: You manage a single-region SAP NetWeaver deployment on AWS with a separate SAP HANA DB on EC2. You need a beginner-friendly, cost-conscious maintenance workflow that automates OS patching and SAP kernel upgrades using only AWS native services, with minimal downtime. Outline the steps, services, and a sample two-hour weekly maintenance window, including how you validate success and perform rollback?","answer":"Use AWS SSM Patch Manager to stage OS patches during a weekly 2-hour window, pausing SAP services, patching both app and DB tiers, then perform a blue/green SAP kernel upgrade via Systems Manager Auto","explanation":"## Why This Is Asked\nTests practical maintenance automation with AWS-native tooling, not theory.\n\n## Key Concepts\n- Systems Manager Patch Manager\n- Systems Manager Automation blue/green workflows\n- SAP maintenance impact and rollback\n\n## Code Example\n```javascript\n// AWS Automation skeleton (illustrative)\nconst patchDoc = {\n  mainSteps: []\n}\n```\n\n## Follow-up Questions\n- How would you extend this for multi-AZ clusters?\n- How would you verify kernel compatibility before patch rollout?","diagram":"flowchart TD\n  A[Maintenance Trigger] --> B[Pause SAP]\n  B --> C[Patch OS via SSM]\n  C --> D[Patch SAP kernel (blue/green)]\n  D --> E[Resume SAP services]\n  E --> F[Health checks & alarms]","difficulty":"beginner","tags":["aws-sap"],"channel":"aws-sap","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Hashicorp","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T14:39:02.676Z","createdAt":"2026-01-12T14:39:02.677Z"}],"subChannels":["accelerate-migration","continuous-improvement","design-new-solutions","design-solutions","general"],"companies":["Airbnb","Apple","Citadel","Cloudflare","Hashicorp","Instacart","Lyft","Microsoft","Oracle","PayPal","Plaid","Robinhood","Snap","Snowflake","Stripe","Uber","Zoom"],"stats":{"total":33,"beginner":2,"intermediate":27,"advanced":4,"newThisWeek":33}}