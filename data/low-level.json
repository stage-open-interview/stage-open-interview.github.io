{"questions":[{"id":"q-684","question":"Design a fixed-size ring buffer in C that stores bytes. Capacity N is a power of two (e.g., 1024). Show how to compute the next index using a mask (idx & (N-1)), and explain full vs empty detection using only head and tail counters. Provide enqueue and dequeue logic for a single-producer/single-consumer scenario?","answer":"Use two indices: head and tail. Let mask = N-1. Enqueue: if ((head - tail) == N) return 0; data[head & mask] = value; head++; Dequeue: if (head == tail) return 0; *out = data[tail & mask]; tail++; Thi","explanation":"## Why This Is Asked\n\nTests understanding of a practical, low-level data structure and its wrap-around behavior, plus efficient empty/full checks in a simple producer/consumer path.\n\n## Key Concepts\n\n- Ring buffers\n- Power-of-two masking\n- Empty/full checks with head/tail without extra state\n- SPSC memory ordering\n\n## Code Example\n\n```c\ntypedef struct {\n  size_t head, tail;\n  unsigned char data[N];\n} Ring;\n\nint enqueue(Ring *r, unsigned char value) {\n  if ((r->head - r->tail) == N) return 0; // full\n  r->data[r->head & (N - 1)] = value;\n  r->head++;\n  return 1;\n}\nint dequeue(Ring *r, unsigned char *out) {\n  if (r->head == r->tail) return 0; // empty\n  *out = r->data[r->tail & (N - 1)];\n  r->tail++;\n  return 1;\n}\n```\n\n## Follow-up Questions\n\n- How would you modify for multi-producer/multi-consumer?\n- What memory-ordering considerations arise on weakly-ordered architectures?","diagram":"flowchart TD\n  A[Head/Tail] --> B[Mask: (N-1)]\n  B --> C[Enqueue path]\n  B --> D[Dequeue path]\n  C --> E[Increment Head]\n  D --> F[Increment Tail]\n  E --> G{Full?}\n  F --> H{Empty?}","difficulty":"beginner","tags":["low-level"],"channel":"low-level","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Hugging Face","Instacart"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T15:59:22.270Z","createdAt":"2026-01-11T15:59:22.270Z"},{"id":"q-692","question":"Design a lock-free ring buffer that supports multiple producers and multiple consumers with bounded capacity. Provide enqueue/dequeue pseudo-code, explain how you avoid ABA, how memory reclamation is handled (hazard pointers or epochs), and why it scales under high contention. Include caveats on cache lines and false sharing. How would you validate under stress?","answer":"Use a power-of-two ring buffer with per-slot sequence numbers and separate atomic head/tail indices. Each slot has a 64-bit sequence: low 32 bits = turn number, high 32 bits = slot index. Producers: read tail, compute slot, check sequence matches expected turn, CAS tail to reserve, write item, increment sequence. Consumers: read head, compute slot, check sequence matches expected turn, CAS head to advance, read item, increment sequence. ABA avoided by monotonically increasing sequence numbers (turn * capacity + index). Memory reclamation via epoch-based reclamation: threads enter critical section, publish retired nodes, exit when global epoch advances. Cache line padding: align head/tail indices and slot arrays to separate cache lines (64-byte boundaries) to avoid false sharing between producers and consumers.","explanation":"## Why This Is Asked\n\nTests understanding of low-level concurrency, lock-free design under real contention.\n\n## Key Concepts\n\n- MPMC queue design with sequence numbers\n- ABA protection via monotonic sequences\n- Memory reclamation (hazard pointers, epochs)\n- Cache friendliness and false sharing prevention\n\n## Code Example\n\n```c\n// Pseudo-code\nstruct Slot {\n    atomic_uint64_t seq;\n    void* data;\n};\n\nbool enqueue(void* item) {\n    uint64_t pos = tail.load();\n    Slot* slot = &buffer[pos & mask];\n    uint64_t seq = slot->seq.load();\n    if ((seq & mask) != (pos & mask)) return false;\n    if (!tail.compare_exchange(pos, pos + 1)) return false;\n    slot->data = item;\n    slot->seq.store(pos + 1);\n    return true;\n}\n\nvoid* dequeue() {\n    uint64_t pos = head.load();\n    Slot* slot = &buffer[pos & mask];\n    uint64_t seq = slot->seq.load();\n    if ((seq & mask) != (pos & mask)) return nullptr;\n    if (!head.compare_exchange(pos, pos + 1)) return nullptr;\n    void* item = slot->data;\n    slot->seq.store(pos + capacity + 1);\n    return item;\n}\n```\n\n## Follow-up Questions\n\n- How would you detect and recover from stalls under hot paths?\n- Compare hazard pointers vs epochs for this use-case.\n- What happens under backpressure when the buffer is full?","diagram":null,"difficulty":"advanced","tags":["low-level"],"channel":"low-level","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Databricks","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T03:54:20.888Z","createdAt":"2026-01-11T16:24:48.553Z"},{"id":"q-694","question":"Design a cache-friendly, per-thread deque work-stealer for a multi-core task executor. Each worker maintains a fixed-size ring buffer for bottom push/pop; thieves steal from the top of other workers via CAS on a top index with a version counter. Explain ABA avoidance, memory ordering, and padding to avoid false sharing. Provide precise pseudo-code and a realistic burst workload scenario?","answer":"Propose per-thread deques with a bottom push/pop and a shared top for steals. Local path uses a fixed-size, cache-aligned ring; steals CAS on top with a version tag to prevent ABA; pad structures to p","explanation":"## Why This Is Asked\nTests ability to design scalable, non-blocking schedulers and reason about ABA, memory ordering, and cache effects.\n\n## Key Concepts\n- Work-stealing deques\n- ABA avoidance with version counters\n- Cache-line padding to avoid false sharing\n- Memory order: acquire/release\n- Backoff and fairness under bursty loads\n\n## Code Example\n```javascript\n// Pseudo-code: per-thread deque with local and steal paths\nclass Deque {\n  constructor(cap) { /* ... */ }\n  pushBottom(x) { /* O(1) with cache-friendly indices */ }\n  popBottom() { /* ... */ }\n  stealTop(owner) { /* CAS on top with version, then claim slot */ }\n}\n```\n\n## Follow-up Questions\n- How would you test tail latency under bursty arrival rates?\n- How would you adapt the design for multi-socket NUMA and cache-coherent interconnects?\n- How would you extend to prioritized tasks or dynamic resizing of ring buffers?","diagram":"flowchart TD\n  A[Worker i deque] --> B[Push/Pop bottom (local)]\n  A --> C[Steal from top of another worker]\n  D[Top pointer with version tag] --> E[ABA avoidance via CAS/version]\n  B --> F[Local fast path]\n  C --> G[Successful steal -> task transfer]","difficulty":"advanced","tags":["low-level"],"channel":"low-level","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T17:16:13.498Z","createdAt":"2026-01-11T17:16:13.498Z"},{"id":"q-707","question":"Design a crash‑consistent, multi‑producer/multi‑consumer ring buffer backed by persistent memory PMEM. How would you ensure last enqueued item durability across power loss, implement recovery, and validate correctness? Provide concise enqueue/dequeue pseudocode with proper flush/fence ordering and discuss failure scenarios?","answer":"Use a crash‑safe PMEM ring buffer with two‑phase publish: write the data to the next slot, flush the data, then publish by updating a durable tail with a release store and a fence (sfence/clwb). Use p","explanation":"## Why This Is Asked\n\nTests ability to design crash‑consistent data paths in persistent memory, a common production issue when power failures occur during in‑flight operations.\n\n## Key Concepts\n\n- Durability guarantees for PMEM writes\n- Memory ordering: release/acquire, fences, cacheline flushes\n- Recovery via redo log and slot sequence counters\n- ABA avoidance in a concurrent ring\n- Testing with fault injection and crash replay\n\n## Code Example\n\n```c\n// Pseudo-code: enqueue/dequeue with PMEM flush/fence\n// Slot: { data, seq, valid }\nvoid enqueue(T v){\n  size_t t = tail.fetch_add(1);\n  Slot *s = &ring[t % CAP];\n  s->data = v; // write data\n  clwb(&s->data, sizeof(T));\n  fence(); // ensure data persists\n  s->seq = t; // publish with durable tail update (release)\n  clwb(&s->seq, sizeof(size_t));\n  fence();\n  // publish tail already happens via atomic tail update\n}\n\nbool dequeue(T *out){\n  size_t h0 = head.load();\n  Slot *s = &ring[h0 % CAP];\n  if (s->seq != h0) return false;\n  *out = s->data;\n  s->valid = false;\n  clwb(&s->valid, sizeof(bool));\n  fence();\n  head.fetch_add(1);\n  return true;\n}\n```\n\n## Follow-up Questions\n\n- How would you handle power loss during the publish vs data write steps?\n- How would you validate recovery correctness across multiple restart scenarios?","diagram":"flowchart TD\n  A[Producer Enqueue] --> B[Write Slot Data] \n  B --> C[Flush Data] \n  C --> D[Publish Tail] \n  D --> E[Consumer Dequeue]","difficulty":"advanced","tags":["low-level"],"channel":"low-level","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Coinbase","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T18:29:56.756Z","createdAt":"2026-01-11T18:29:56.756Z"},{"id":"q-712","question":"Design a NUMA-aware in-memory index with per-node shards and a lock-free cross-node coordinator. Provide insertion and lookup with minimal locking, specify data layout (shards, padding, key/value encodings), and memory-order guarantees (fences, atomic ops). Describe a deadlock-free shard rebalancing protocol under high contention and outline tests with realistic Snowflake/Twitter-scale workloads?","answer":"Per-node shards with a global coordinator. Lock-free hot path: atomic upserts into per-node arrays using CAS, with 64-byte padding; memory order: acquire/release fences around reads/writes; cross-node","explanation":"## Why This Is Asked\nTests understanding of NUMA-aware design, lock-free data paths, and safe shard rebalancing under contention in large-scale systems.\n\n## Key Concepts\n- NUMA affinity and per-node shard layouts\n- Lock-free synchronization with CAS and memory fences\n- Cache-line padding to prevent false sharing\n- Epoch-based or versioned rebalance without blocking\n\n## Code Example\n```cpp\n// Pseudo core: per-node shard insert using CAS\nstruct Shard { std::atomic<Key> key; std::atomic<Value> val; char pad[64]; };\nvoid insert(Shard* s, Key k, Value v){ auto old = s->key.load(std::memory_order_acquire); if(old==k) { s->val.store(v, std::memory_order_release); return; } // CAS loop omitted }\n```\n\n## Follow-up Questions\n- How would you validate correctness under cross-node contention?\n- What are failure modes under memory-order relaxations?","diagram":null,"difficulty":"advanced","tags":["low-level"],"channel":"low-level","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Snowflake","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T19:16:51.595Z","createdAt":"2026-01-11T19:16:51.595Z"},{"id":"q-723","question":"Design a software-based **TLB** for a 4-core 64-bit system with 4KiB pages. Each core has a private 128-entry **TLB** and a global page-table invalidation path. Provide lookup/refill pseudo-code, discuss eviction strategy (LRU vs. random), synchronization via **memory fences**, and cross-core shootdowns. Include a test under memory pressure and explain validation?","answer":"Private per-core 128-entry TLB, 4KiB pages, 4-way set-associative. On miss, walk multi-level page tables, then install using atomic ops. Use a scalable eviction (pseudo-LRU). Invalidate entries across","explanation":"## Why This Is Asked\n\nA software TLB with per-core caches and cross-core invalidations tests low-level thinking about coherence, consistency, and performance under contention. It also probes trade-offs between eviction, barriers, and hardware hints.\n\n## Key Concepts\n\n- TLB structure: per-core caches, set-associative layout\n- Page-table walk costs and caching gains\n- Cross-core invalidation/shootdown mechanisms\n- Memory fences and atomic updates\n- Validation under memory pressure and realistic workloads\n\n## Code Example\n\n```javascript\n// Pseudo-code: TLB lookup and refill\nfunction tlbLookup(vpn){\n  const idx = vpn % 128;\n  const e = tlb[idx];\n  if (e && e.vpn === vpn && e.valid) return e.pte;\n  return null;\n}\nfunction tlbRefill(vpn, pte){\n  const idx = vpn % 128;\n  tlb[idx] = { vpn, pte, valid: true, ts: Date.now() };\n}\n```\n\n## Follow-up Questions\n\n- How would you scale to larger pages or userfaults?\n- How would you validate correctness under concurrent refills and shootdowns?","diagram":"flowchart TD\n  A[Core] --> B[TLB miss]\n  B --> C[Page walk]\n  C --> D[Install entry]\n  D --> E[Shoot down other cores]\n  E --> F[Return to caller]","difficulty":"advanced","tags":["low-level"],"channel":"low-level","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","Oracle","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T20:24:01.827Z","createdAt":"2026-01-11T20:24:01.827Z"},{"id":"q-725","question":"Design a crash-consistent, in-memory index for a 4-byte key, 8-byte value store on an 8-core Linux machine. Use per-core log-structured segments and a Bloom filter; describe durable append ordering, startup recovery by replaying per-core logs, and validation under power-loss scenarios?","answer":"Per-core logs minimize contention; mutations append to local segments, then a durable epoch fence persists metadata. Recovery replays per-core segments to rebuild the index; Bloom filters speed lookup","explanation":"## Why This Is Asked\n\nThis question probes experience with crash-consistent in-memory structures, per-core data locality, and durable metadata synchronization under power failures—critical in low-latency financial/ads platforms.\n\n## Key Concepts\n\n- Per-core log-structured index and Bloom filter for fast lookups\n- Durable append order via epoch fences and fsync/msync semantics\n- Recovery by replaying per-core logs in a deterministic order\n- False sharing minimization and memory-mapped file durability\n\n## Code Example\n\n```javascript\n// Pseudo replay of per-core logs to rebuild index\nfunction replay_logs(coreLogs) {\n  for (const core of coreLogs) {\n    for (const entry of core.segments) {\n      apply(entry);\n    }\n  }\n}\n```\n\n## Follow-up Questions\n\n- How do you handle Bloom filter false positives during recovery?\n- What are the failure modes during corruption of a core log segment, and how do you detect them?\n- How would you validate performance under steady-state and crash-recovery cycles?","diagram":"flowchart TD\n  A[Mutate] --> B[Append to Core Log]\n  B --> C[Durable Epoch Fence]\n  C --> D[Query index with Bloom filter]\n  D --> E[Recovery uses per-core logs]","difficulty":"advanced","tags":["low-level"],"channel":"low-level","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","LinkedIn","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T21:16:32.089Z","createdAt":"2026-01-11T21:16:32.089Z"},{"id":"q-734","question":"Design an epoch-based reclamation scheme for a lock-free stack in a 64-bit, multi-threaded user-space library. Each thread publishes its local epoch; a global clock advances periodically. On pop, move the node to a retire-list with its epoch and reclaim only after all threads have observed an epoch older than that node. Include a stress test with high contention?","answer":"Use epoch-based reclamation for a lock-free stack. Each thread publishes a local epoch; a global epoch advances periodically. On pop, retire the node with its epoch and reclaim only after all threads ","explanation":"## Why This Is Asked\nThis question probes practical low-level memory management under concurrency, including safe reclamation without locks.\n\n## Key Concepts\n- Lock-free data structures\n- Epoch-based reclamation\n- Memory ordering and fences\n- ABA avoidance\n\n## Code Example\n```javascript\n// Minimal epoch-based reclamation sketch\nclass Node { constructor(val) { this.val = val; this.next = null; } }\n\n// Global state\nlet globalEpoch = 0;\nlet threadEpochs = new Map();\n\n// Publish epoch from a thread\nfunction publishEpoch(epoch) { /* store per-thread epoch */ }\n\n// Retire a node\nfunction retire(node, epoch) { /* add to retire list with epoch */ }\n\n// Reclaim loop\nfunction advanceEpoch() { globalEpoch++; }\n```\n\n## Follow-up Questions\n- How would you detect and prevent ABA in this scheme?\n- How would you tune epoch advancement to avoid memory starvation under bursty workloads?\n","diagram":"flowchart TD\n  A[Publish epoch] --> B[Update stack head with CAS]\n  B --> C[Retire node into epoch list]\n  C --> D[Global epoch advances]\n  D --> E[Reclaim when all seen < retire epoch]","difficulty":"intermediate","tags":["low-level"],"channel":"low-level","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Lyft","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T22:18:50.970Z","createdAt":"2026-01-11T22:18:50.970Z"},{"id":"q-744","question":"In a 2-socket x86-64 server with MESI coherence, design a lock-free, multi-producer single-consumer ring buffer in shared memory for a 10 Gbps network path. Use per-slot sequence numbers to avoid ABA, with a fixed size N=1<<16 and 64-byte payload slots. Provide slot layout, push/pop pseudo-code with memory fences, discuss wrap-around and backpressure, and outline a minimal microbenchmark to validate throughput and data integrity under contention?","answer":"Implement a fixed-size ring (N=1<<16) in shared memory. Each slot stores a 64-bit seq and 64-byte payload. Producers write payload, increment seq, mfence, then CAS head. Consumer spins on tail, valida","explanation":"## Why This Is Asked\nTests lock-free IPC, memory ordering, and cache-coherence handling on NUMA systems under contention.\n\n## Key Concepts\n- Lock-free ring buffers with per-slot sequence numbers to avoid ABA\n- Memory ordering on x86-64, mfence usage, cache-line padding\n- Backpressure handling and wrap-around semantics\n\n## Code Example\n```c\ntypedef struct { volatile uint64_t seq; uint8_t data[64]; char pad[56]; } Slot;\ntypedef struct { Slot slots[N]; _Atomic uint64_t head; _Atomic uint64_t tail; } Ring;\n```\n\n```c\n// Push (producer)\nuint64_t pos; do { pos = atomic_read(&ring->head); if (slot_full(pos)) continue; } while(!atomic_compare_exchange(&ring->head, pos, pos+1));\nr ing->slots[pos & (N-1)].data = payload; ring->slots[pos & (N-1)].seq = pos; mfence();\n```\n\n## Follow-up Questions\n- How would you extend to multi-consumer or multi-ported rings?\n- What are the performance pitfalls and mitigations (false sharing, cache thrashing)?","diagram":"flowchart TD\n  A[Producers] --> B[Shared Ring Slots]\n  B --> C[Single Consumer]\n  C --> D[Tail Advancement]\n  E[Memory Fences] --> F[Orderly Visibility]","difficulty":"intermediate","tags":["low-level"],"channel":"low-level","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","Tesla","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T23:19:01.045Z","createdAt":"2026-01-11T23:19:01.045Z"},{"id":"q-752","question":"Design a crash-safe, persistent ring buffer in NVRAM for a 2-socket NUMA system with a PCIe NIC. Capacity N=1<<18, 128-byte payloads, per-slot 64-bit sequence to prevent ABA. Slot: [payload|seq|meta]. Enqueue: publish payload, fence, then update seq. Dequeue: verify seq before consume. Durability via per-slot commit log: flush payload and seq, then epoch commit. On crash, recover by replaying committed epochs and validating seq monotonicity. Provide a minimal microbenchmark to validate throughput and correctness under power loss?","answer":"Design a crash-safe, persistent ring buffer in NVRAM for a 2-socket NUMA system with a PCIe NIC. N=1<<18, 128-byte payloads, per-slot 64-bit sequence to avoid ABA. Slot: payload|seq|meta. Enqueue: pub","explanation":"## Why This Is Asked\nTests understanding of crash-consistent data structures, non-volatile memory ordering, and recovery.\n\n## Key Concepts\n- Persistent memory semantics, fences, epoch-based durability\n- ABA avoidance with per-slot seq, cache-line layout, minimal recovery log\n- Validation under power loss using replay consistency checks\n\n## Code Example\n\n```javascript\n// Placeholder: interview design discussion, not implementable here\n```\n\n## Follow-up Questions\n- How would you adapt for multi-consumer scenarios?\n- What failure modes break the design and how to mitigate?","diagram":"flowchart TD\n  Producer --> Enqueue\n  Enqueue --> Commit\n  Commit --> Recovery\n  Recovery --> Consumer","difficulty":"advanced","tags":["low-level"],"channel":"low-level","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","IBM","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T01:30:58.766Z","createdAt":"2026-01-12T01:30:58.766Z"},{"id":"q-760","question":"On a dual-socket NUMA server, design a cross-NUMA, zero-copy ring buffer for a 40 Gbps path between two processes where producers live on socket A and a consumer on socket B. Use per-slot 64-bit sequence numbers to prevent ABA, 128-byte payload slots, and capacity 1<<18 with 64-byte alignment. Provide slot layout, enqueue/dequeue pseudo-code with memory fences and cache-line padding, wrap-around and backpressure handling, and outline a minimal microbenchmark to validate throughput and data integrity under cross-socket contention?","answer":"Cross-NUMA, two-process ring for 40 Gbps on a dual-socket server. Slot: 128 bytes; capacity 1<<18; 64-byte alignment; per-slot 64-bit sequence for ABA-avoidance. NUMA-aware allocation on local node. E","explanation":"## Why This Is Asked\nTests cross-NUMA IPC design with ABA avoidance, memory ordering, and cache-line discipline.\n\n## Key Concepts\n- Cross-NUMA memory allocation and cache coherence\n- Per-slot sequencing to prevent ABA\n- Acquire/release semantics and non-temporal stores\n- Backpressure and wrap-around handling\n\n## Code Example\n```javascript\n// Pseudo-code outline\n```\n## Follow-up Questions\n- How would you extend to N producers and M consumers with fairness guarantees?\n- What failure modes matter under NUMA remapping or node failure?","diagram":null,"difficulty":"advanced","tags":["low-level"],"channel":"low-level","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T03:49:06.047Z","createdAt":"2026-01-12T03:49:06.047Z"},{"id":"q-772","question":"Design a cache-friendly fixed-size object pool for 128-byte blocks used by a multi-threaded producer-consumer path. Implement init, alloc, and free in C for a pool of 1<<20 blocks, each 128 bytes and 64-byte aligned. Use per-thread caches to reduce contention and a global lock-free free-list for overflow. Explain how you avoid false sharing, show the slot layout with a freelist pointer, and outline a microbenchmark to measure throughput and tail latency under contention?","answer":"Per-thread arenas hold a small freelist of 256 blocks, each 128B aligned to 64B. Allocation pops from the thread-local freelist; if empty, refill from a shared global pool via CAS-based lock-free stac","explanation":"## Why This Is Asked\nThis task probes practical memory pool design under contention, cache alignment, and per-thread vs global sharing strategies.\n\n## Key Concepts\n- Cache locality and false sharing\n- Per-thread arenas vs global pool\n- Lock-free stack and CAS\n- Alignment and memory fences\n\n## Code Example\n```c\ntypedef struct Block {\n  struct Block *next;\n  uint8_t data[120];\n} Block;\n#endif\n``` \n\n```c\n// Pseudo: per-thread arena and global pool refill\n```\n\n## Follow-up Questions\n- How do you handle pool exhaustion?\n- How would you adapt for variable block sizes?","diagram":"flowchart TD\nA[Thread] --> B[Alloc from TLS]\nB --> C{Empty?}\nC -- Yes --> D[Refill from Global]\nC -- No --> E[Return Block]\nD --> E","difficulty":"beginner","tags":["low-level"],"channel":"low-level","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T04:52:07.495Z","createdAt":"2026-01-12T04:52:07.495Z"},{"id":"q-778","question":"Write a C function sum_prefetch that sums N 64-bit integers from an aligned array of length n using software prefetching to hide memory latency. Use 4-way unrolling and __builtin_prefetch to bring data 256 elements ahead. Ensure correctness for any length. Propose a microbenchmark plan to compare with a naive loop and discuss cache-line utilization and false sharing concerns?","answer":"Use four accumulators (a,b,c,d). Loop i+=4; prefetch arr[i+256]; accumulate a+=arr[i], b+=arr[i+1], c+=arr[i+2], d+=arr[i+3]. Finalize by adding a,b,c,d. Ensure arr is 64-byte aligned and compile with","explanation":"## Why This Is Asked\nTests practical low-level optimization skills: cache behavior, prefetching, and reliable counting.\n\n## Key Concepts\n- Cache lines and spatial locality\n- Software prefetching with __builtin_prefetch\n- Loop unrolling and multiple accumulators\n- Alignment and portability\n\n## Code Example\n```c\nuint64_t sum_prefetch(const uint64_t *arr, size_t n){\n    uint64_t a=0,b=0,c=0,d=0;\n    size_t i=0;\n    for(; i+3<n; i+=4){\n        __builtin_prefetch(&arr[i+256],0,1);\n        a += arr[i];\n        b += arr[i+1];\n        c += arr[i+2];\n        d += arr[i+3];\n    }\n    for(; i<n; ++i) a += arr[i];\n    return a+b+c+d;\n}\n``` \n\n## Follow-up Questions\n- How would you adapt this for streaming data vs. random access?\n- What changes if array length is not multiple of four?","diagram":null,"difficulty":"beginner","tags":["low-level"],"channel":"low-level","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Tesla","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T05:30:51.288Z","createdAt":"2026-01-12T05:30:51.288Z"},{"id":"q-784","question":"Design a deterministic, time-sliced barrier for a three-stage streaming pipeline on a 2-socket x86-64 system. Each stage runs on a fixed subset of cores; implement a barrier that advances phases only after every core finishes its assigned slice within a bounded time. Explain how to ensure bounded latency under cache-line contention, preserve MESI coherence, and prevent starvation. Provide pseudo-code for enter_barrier for a core and discuss validation?","answer":"Implement a per-core epoch barrier with fixed 64-cycle slices. Each core writes to its per-core slot and then performs an acquire on a shared phase counter. When all slots are written, a master increm","explanation":"## Why This Is Asked\n\nTests knowledge of deterministic synchronization and bounded-latency barriers in NUMA systems.\n\n## Key Concepts\n\n- Deterministic barriers for streaming pipelines\n- Time-sliced execution and fairness\n- Cache-line padding to avoid false sharing\n- MESI coherence implications on cross-socket barriers\n- Validation of worst-case latency under contention\n\n## Code Example\n\n```javascript\n// Pseudo-code for enter_barrier(coreId, total)\nfunction enter_barrier(coreId, total) {\n  // write completion\n  barrierSlots[coreId].done = true;\n  // spin until all done\n  while (!allDone()) {}\n  // reset for next phase\n  if (coreId == 0) phase++;\n  barrierSlots[coreId].done = false;\n}\n```\n\n## Follow-up Questions\n\n- How would you handle dynamic thread affinity changes?\n- How would you measure and bound worst-case latency across sockets?","diagram":null,"difficulty":"advanced","tags":["low-level"],"channel":"low-level","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T06:37:19.978Z","createdAt":"2026-01-12T06:37:19.978Z"},{"id":"q-794","question":"Design and implement a cache-friendly tiled matrix multiply for two large double-precision matrices stored in row-major order on a two-socket NUMA system. Propose a tile size of 32x32, provide C code for the tiled kernel with boundary handling, explain how to maximize L1/L2 reuse, NUMA locality (first-touch), avoid false sharing, and an inline 4-wide inner-loop unrolling. Include a microbenchmark plan comparing to a naive triple-nested loop and how you would measure throughput and cache behavior?","answer":"Implement a cache-friendly tiled matmul: 32x32 tiles, 4-wide inner-loop unrolling, and first-touch NUMA affinity to localize A, B, C. Use restrict pointers, align data to 64-byte cache lines, prefetch","explanation":"## Why This Is Asked\n\nTests hands-on understanding of memory hierarchy, NUMA locality, and practical optimization trade-offs in a real kernel.\n\n## Key Concepts\n\n- Cache tiling for L1/L2 reuse\n- NUMA first-touch locality on multi-socket systems\n- Boundary handling for non-multiples of tile size\n- Loop unrolling and prefetching strategies\n- False-sharing avoidance via per-tile accumulation\n\n## Code Example\n\n```javascript\n// Pseudo-C kernel illustrating 32x32 tiling\nvoid matmul_tiled(int N, const double *A, const double *B, double *C){\n  for(int ii=0; ii<N; ii+=32){\n    for(int jj=0; jj<N; jj+=32){\n      for(int kk=0; kk<N; kk+=32){\n        for(int i=ii; i< (ii+32) && i<N; ++i){\n          for(int j=jj; j<(jj+32) && j<N; ++j){\n            double sum = 0.0;\n            for(int k=kk; k<(kk+32) && k<N; ++k){\n              sum += A[i*N+k] * B[k*N+j];\n            }\n            C[i*N+j] += sum;\n          }\n        }\n      }\n    }\n  }\n}\n```\n\n## Follow-up Questions\n\n- How would you validate correctness for non-multiple tile sizes?\n- How would you adapt to AVX2/AVX-512 and data layout changes to boost throughput?","diagram":"flowchart TD\n  A[Start] --> B[Tiled Loops]\n  B --> C[Compute Tile]\n  C --> D[Edge Tiles]\n  D --> E[Benchmark & Validate]","difficulty":"intermediate","tags":["low-level"],"channel":"low-level","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","LinkedIn"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T07:29:17.327Z","createdAt":"2026-01-12T07:29:17.327Z"},{"id":"q-800","question":"Design a per-core, lock-free memory allocator for a shared-memory, NUMA-aware object store. Each core maintains a 2 MB local heap; allocations first attempt local allocation, with a fast cross-core path using atomic hand-offs; reclaimed memory is managed via hazard pointers and epoch-based reclamation. Provide allocate/free APIs, a sketch of the free-list structure, and a microbenchmark plan that shows fragmentation under steady-state load?","answer":"Per-core free lists with a bump-pointer local allocator inside a 2 MB heap per core. Align to 64 bytes; local allocations are fast paths, cross-core requests use an atomic hand-off ring to a target co","explanation":"## Why This Is Asked\nTests core memory management, concurrency primitives, NUMA awareness, and safe reclamation in high-throughput systems.\n\n## Key Concepts\n- Per-core locality\n- Lock-free reclaim (hazard pointers, epochs)\n- Cross-core handoff patterns\n- Fragmentation and cache-line alignment\n\n## Code Example\n```c\n// Pseudo allocator core logic\ntypedef struct Block Block;\nvoid* alloc(size_t n);\nvoid free(void* p);\n```\n\n## Follow-up Questions\n- How would you measure false sharing?\n- How would you adapt if cores may be hot-swapped or deactivated?","diagram":"flowchart TD\n  A[Request] --> B{Local?}\n  B -- Yes --> C[Alloc from local heap]\n  B -- No --> D[Enqueue cross-core handoff]\n  D --> E[Target core alloc]\n  E --> F[Return]\n","difficulty":"advanced","tags":["low-level"],"channel":"low-level","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","MongoDB","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T08:31:26.187Z","createdAt":"2026-01-12T08:31:26.187Z"},{"id":"q-810","question":"Design a crash-consistent, bounded, multi-producer/multi-consumer queue backed by non-volatile memory. It must survive power loss, use per-slot sequence numbers to avoid ABA, provide push/pop pseudo-code with proper memory fences, and support epoch-based reclamation to avoid hazard pointers. Outline recovery on boot and a microbenchmark plan?","answer":"A ring in NVRAM with N=1<<16 slots; head/tail durable counters; each slot carries a 64-byte payload and a sequence nonce. Enqueue writes payload, then updates sequence and persists with fences. Dequeu","explanation":"## Why This Is Asked\nTackles crash-consistency, non-volatile memory ordering, ABA avoidance, and safe memory reclamation in a high-concurrency data path.\n\n## Key Concepts\n- NVRAM durability and flush ordering (clwb/clflush, sfence)\n- Per-slot sequence numbers to prevent ABA\n- Epoch-based reclamation vs. hazard pointers\n- Recovery: replay tail, prune uncommitted entries\n\n## Code Example\n```c\n// simplified sketch showing write order and fences\nvoid enqueue(Ring *r, void *payload){\n  uint64_t t = fetch_and_add(&r->tail, 1);\n  Slot *s = &r->slots[t & (N-1)];\n  memcpy(s->payload, payload, 64);\n  __sync_synchronize(); // fence before seq\n  s->seq = t<<1 | 1; // mark committed\n  // persist s and tail\n}\n```\n\n## Follow-up Questions\n- How would you implement recovery after power loss?\n- How do you validate correctness under concurrent stress?","diagram":null,"difficulty":"advanced","tags":["low-level"],"channel":"low-level","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Plaid","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T09:35:21.464Z","createdAt":"2026-01-12T09:35:21.464Z"},{"id":"q-816","question":"Design a crash-consistent, persistent ring buffer for a multi-producer/multi-consumer event stream backed by non-volatile memory, with 1<<18 slots of 128-byte payloads; explain ABA avoidance via per-slot sequence numbers, two-phase publish with durable commit, and required flush/barrier order; describe recovery and a microbenchmark plan under simulated power loss?","answer":"Propose a crash-consistent, persistent ring buffer for NVRAM with multi-producer, multi-consumer access. Use 1<<18 slots, 128-byte payloads. ABA avoided via per-slot sequence numbers; two-phase publis","explanation":"## Why This Is Asked\nAsks about crash-consistency in a realistic persistent queue used in trading/telemetry.\n\n## Key Concepts\n- Crash-consistency, NVRAM, per-slot sequence numbers, flush barriers, recovery.\n\n## Code Example\n```javascript\n// pseudo enqueue/dequeue\n```\n\n## Follow-up Questions\n- How would you test durability under power loss and memory reordering? \n- How would you extend to multi-consumer fairness without stalls?","diagram":null,"difficulty":"advanced","tags":["low-level"],"channel":"low-level","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Robinhood","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T10:26:21.458Z","createdAt":"2026-01-12T10:26:21.458Z"},{"id":"q-826","question":"Design and implement a tiny spinlock in C11 for a shared memory region. Provide lock() and unlock() using stdatomic.h primitives, ensuring memory_order_acquire on successful lock and memory_order_release on unlock. Include a minimal two-thread test contending for the lock and explain your backoff/yield strategy and fairness limitations?","answer":"```c\n#include <stdatomic.h>\n#include <sched.h>\n\ntypedef struct { atomic_flag f; } spinlock_t;\n#define SPINLOCK_INIT { ATOMIC_FLAG_INIT }\n\nstatic inline void spin_lock(spinlock_t *s){\n  while (atomic_f","explanation":"## Why This Is Asked\nTests practical use of C11 atomics, memory ordering, and contention handling in low-level code. ## Key Concepts\n- C11 atomic primitives (atomic_flag)\n- memory_order_acquire/release semantics\n- busy-wait with backoff/yield to reduce bus traffic\n- fairness and starvation considerations ## Code Example\n```c\n// above code snippet\n```\n## Follow-up Questions\n- How would you extend this to a fair queueing spinlock?\n- What benchmarks would you run to profile contention impact?","diagram":"flowchart TD\n  Start(Start) --> LockRequest[Lock request]\n  LockRequest --> Acquired{Acquired?}\n  Acquired -->|Yes| Crit[Critical Section]\n  Crit --> Unlock[Unlock]\n  Unlock --> End[End]","difficulty":"beginner","tags":["low-level"],"channel":"low-level","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Google","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T11:22:40.506Z","createdAt":"2026-01-12T11:22:40.506Z"}],"subChannels":["general"],"companies":["Adobe","Airbnb","Amazon","Anthropic","Bloomberg","Coinbase","Databricks","Discord","Goldman Sachs","Google","Hugging Face","IBM","Instacart","LinkedIn","Lyft","MongoDB","NVIDIA","Netflix","OpenAI","Oracle","Plaid","Robinhood","Slack","Snap","Snowflake","Stripe","Tesla","Twitter","Two Sigma","Uber"],"stats":{"total":19,"beginner":4,"intermediate":3,"advanced":12,"newThisWeek":19}}