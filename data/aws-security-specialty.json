{"questions":[{"id":"aws-security-specialty-data-protection-1768216951582-0","question":"You have an S3 bucket storing sensitive data and you are using SSE-KMS with a customer-managed CMK. To meet compliance, you want the CMK to rotate automatically without manual re-encryption. Which approach best achieves automatic rotation?","answer":"[{\"id\":\"a\",\"text\":\"Enable automatic rotation on the CMK, which rotates the symmetric CMK on a yearly basis.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Create a new CMK every year and re-encrypt existing objects with the new key.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Rely on AWS-managed CMK rotation and do nothing else.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Disable rotation to simplify key management and rotate manually quarterly.\",\"isCorrect\":false}]","explanation":"## Correct Answer\n- Option A is correct: Enabling automatic rotation on a symmetric CMK rotates the CMK annually for new encryptions and re-encryptions as needed.\n\n## Why Other Options Are Wrong\n- Option B: Manual rotation is error-prone and requires re-encrypting existing data; automation is preferred.\n- Option C: AWS-managed keys rotate according to AWS schedules, not necessarily your policy.\n- Option D: Disabling rotation increases risk and is not a best practice.\n\n## Key Concepts\n- AWS KMS automatic key rotation\n- SSE-KMS for S3 encryption\n- Compliance requirements for key rotation\n\n## Real-World Application\n- In production, enable automatic rotation on customer-managed CMKs used for SSE-KMS to reduce operational overhead and meet regulatory rotation requirements.","diagram":null,"difficulty":"intermediate","tags":["AWS","KMS","S3","Encryption","DataProtection","certification-mcq","domain-weight-18"],"channel":"aws-security-specialty","subChannel":"data-protection","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T11:22:31.583Z","createdAt":"2026-01-12 11:22:31"},{"id":"aws-security-specialty-data-protection-1768216951582-1","question":"You need to share encrypted data with a partner AWS account, allowing decrypt with your CMK without sharing credentials. Which approach is appropriate?","answer":"[{\"id\":\"a\",\"text\":\"Add a cross-account KMS key policy that grants decrypt permissions to the partner's account.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Create an IAM user in the partner account and export credentials.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use an S3 bucket policy to allow access to the partner's account.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Share the CMK material directly.\",\"isCorrect\":false}]","explanation":"## Correct Answer\n- Option A is correct: A cross-account KMS key policy grants decrypt permissions to the partner while keeping control of the CMK in your account.\n\n## Why Other Options Are Wrong\n- Option B: Creating an IAM user in the partner account would require sharing credentials and is not appropriate for cryptographic operations.\n- Option C: S3 bucket policy alone does not grant decrypt permissions on KMS keys.\n- Option D: Sharing CMK material is not allowed; key material must stay in the owning account.\n\n## Key Concepts\n- KMS key policy\n- Cross-account access\n- Decrypt permissions\n\n## Real-World Application\n- Enables secure data sharing with partners without exposing credentials or broad access.","diagram":null,"difficulty":"intermediate","tags":["AWS","KMS","IAM","Cross-Account","DataProtection","certification-mcq","domain-weight-18"],"channel":"aws-security-specialty","subChannel":"data-protection","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T11:22:31.928Z","createdAt":"2026-01-12 11:22:32"},{"id":"aws-security-specialty-data-protection-1768216951582-2","question":"An RDS MySQL instance stores PII; you want column-level encryption for specific columns. RDS does not provide native per-column encryption. Which approach is the most practical to achieve this protection?","answer":"[{\"id\":\"a\",\"text\":\"Enable built-in column-level encryption in RDS.\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Encrypt sensitive columns at the application layer before sending to RDS (client-side encryption).\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Enable Transparent Data Encryption (TDE) on RDS MySQL.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Store encrypted values in Secrets Manager and decrypt in the application.\",\"isCorrect\":false}]","explanation":"## Correct Answer\n- Option B is correct: RDS does not offer native column-level encryption; encrypting sensitive fields at the application layer ensures per-column protection before storage.\n\n## Why Other Options Are Wrong\n- Option A: RDS MySQL does not provide native column-level encryption.\n- Option C: TDE is not available for MySQL on RDS.\n- Option D: Secrets Manager is for storing secrets, not for encrypting arbitrary column data within the database.\n\n## Key Concepts\n- Client-side encryption\n- Application-layer crypto\n- Data protection in databases\n\n## Real-World Application\n- Integrate encryption routines in the data-access layer to protect sensitive fields before persisting to RDS; this complements at-rest encryption provided by RDS.","diagram":null,"difficulty":"intermediate","tags":["AWS","RDS","Encryption","KMS","DataProtection","certification-mcq","domain-weight-18"],"channel":"aws-security-specialty","subChannel":"data-protection","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T11:22:32.280Z","createdAt":"2026-01-12 11:22:32"},{"id":"q-853","question":"In a multi-account AWS setup, centralize a logs bucket in Account A that stores CloudTrail and VPC Flow Logs from Accounts B and C. All objects must be encrypted at rest with a CMK in Account A that rotates automatically. Design the KMS key policy, bucket policy, and cross-account IAM roles to allow Account B/C services to encrypt, while preventing decryption except via a centralized IAM role in Account A. Include how you would validate encryption, rotation status, and auditability?","answer":"Design a cross-account CMK rotation with two layers: a centralized KMS key policy granting Encrypt/Decrypt to per-account Roles (B/C) via Grants, and a Decrypt role in Account A for review. Enforce SS","explanation":"## Why This Is Asked\nDemonstrates cross-account CMK control, least privilege, and auditability for centralized logs.\n\n## Key Concepts\n- Cross-account KMS key policies and grants\n- SSE-KMS with automatic rotation\n- Centralized log bucket policy and access controls\n- CloudTrail/VPC Flow Logs integration and IAM roles\n\n## Code Example\n```javascript\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\"AWS\": [\"arn:aws:iam::AccountB:role/LogsWriter\",\n                                \"arn:aws:iam::AccountC:role/LogsWriter\"]},\n      \"Action\": [\"kms:Encrypt\", \"kms:GenerateDataKey*\", \"kms:ReEncrypt*\"],\n      \"Resource\": \"*\"\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\"AWS\": \"arn:aws:iam::AccountA:role/KMSDecryptRole\"},\n      \"Action\": [\"kms:Decrypt\"],\n      \"Resource\": \"*\"\n    }\n  ]\n}\n```\n\n## Follow-up Questions\n- How would you rotate the CMK without downtime?\n- How do you validate access without exposing keys publicly?\n","diagram":null,"difficulty":"intermediate","tags":["aws-security-specialty"],"channel":"aws-security-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","IBM"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:36:36.217Z","createdAt":"2026-01-12T13:36:36.217Z"},{"id":"aws-security-specialty-infrastructure-security-1768148611649-0","question":"To meet compliance for sensitive data stored in S3, you want server-side encryption with a customer-managed CMK and enforce that all objects are encrypted at rest using that key. Which combination of actions achieves this?","answer":"[{\"id\":\"a\",\"text\":\"Enable SSE-KMS with a customer-managed key on the bucket, enforce encryption by requiring the specific KMS key in a bucket policy, and enable automatic key rotation on the CMK\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Enable SSE-KMS with an AWS managed key and enforce encryption by requiring that key in a bucket policy\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Enable SSE-S3 encryption and enforce it via a bucket policy to require encryption on all uploads\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Disable encryption and rely on IAM to prevent access\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A is correct because you explicitly use a customer-managed CMK (CMK) with SSE-KMS, enforce its use via a bucket policy, and enable automatic rotation for key lifecycle discipline.\n\n## Why Other Options Are Wrong\n- Option B uses an AWS managed key, which does not satisfy the requirement for a customer-managed CMK.\n- Option C uses SSE-S3, which encrypts with a service-managed key, not a customer-managed CMK.\n- Option D removes encryption entirely, which fails compliance and does not meet data protection requirements.\n\n## Key Concepts\n- SSE-KMS with a customer-managed CMK\n- Bucket policies enforcing encryption with a specific KMS key\n- KMS key rotation and lifecycle management\n\n## Real-World Application\nUsed to protect sensitive data lakes or archives in S3 where regulatory controls require customer-managed key control and enforceable encryption at rest.\n","diagram":null,"difficulty":"intermediate","tags":["AWS S3","AWS KMS","AWS IAM","Encryption","certification-mcq","domain-weight-20"],"channel":"aws-security-specialty","subChannel":"infrastructure-security","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T16:23:31.650Z","createdAt":"2026-01-11 16:23:31"},{"id":"aws-security-specialty-infrastructure-security-1768148611649-1","question":"To detect and preserve evidence of privileged activity across AWS accounts, you want centralized logging with tamper-evident logs and multi-region delivery. Which configuration best achieves this?","answer":"[{\"id\":\"a\",\"text\":\"Enable CloudTrail in all regions for all accounts, deliver logs to a centralized S3 bucket in a dedicated security account, enable multi-region logging, and enable log file integrity validation\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Enable CloudTrail in a single account and rely on CloudWatch Logs for auditing\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Enable VPC Flow Logs for all VPCs and store in S3\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Enable CloudTrail but disable log file validation\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A is correct because it provides comprehensive, tamper-evident, multi-region, multi-account logging with a centralized sink and log integrity validation, which protects against tampering and ensures availability for forensics.\n\n## Why Other Options Are Wrong\n- Option B limits visibility to a single account and omits multi-account centralized collection, reducing the chance of blind spots.\n- Option C logs network traffic (VPC Flow Logs) but does not capture management plane API activity (AWS Console/CLI) which is critical for privileged activity auditing.\n- Option D omits log integrity validation, making logs more susceptible to tampering and less trustworthy for investigations.\n\n## Key Concepts\n- CloudTrail multi-region, multi-account configuration\n- Centralized log sink in a separate security account\n- CloudTrail log file integrity validation\n\n## Real-World Application\nUsed in enterprises with multiple AWS accounts to ensure auditable, tamper-evident trails for security investigations and compliance audits.\n","diagram":null,"difficulty":"intermediate","tags":["AWS CloudTrail","AWS S3","AWS IAM","AWS Organizations","certification-mcq","domain-weight-20"],"channel":"aws-security-specialty","subChannel":"infrastructure-security","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T16:23:32.014Z","createdAt":"2026-01-11 16:23:32"},{"id":"aws-security-specialty-infrastructure-security-1768148611649-2","question":"A production workload in a VPC must access only specific AWS services over private connectivity, with no Internet access. Which configuration best achieves this while preserving least privilege?","answer":"[{\"id\":\"a\",\"text\":\"Create VPC endpoints for the allowed AWS services, apply per-service endpoint policies to restrict actions, and block all Internet access via security groups and NACLs\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Use a NAT gateway with an outbound allowlist to reach only whitelisted services\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Route all traffic to an Internet Gateway and rely on security groups to block unknown destinations\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use VPC peering to an external network and filter traffic at the peer\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A is correct because VPC endpoints provide private connectivity to specified AWS services, and endpoint policies can restrict actions to least privilege. Coupled with blocking Internet access at the subnet level, this ensures only approved services are reachable without Internet exposure.\n\n## Why Other Options Are Wrong\n- Option B is possible but relies on NAT; it still routes traffic through a NAT device and complicates enforcement of least privilege for AWS service APIs.\n- Option C reintroduces Internet-bound traffic via an Internet Gateway, defeating the private connectivity requirement.\n- Option D introduces external networks and potential data exfiltration paths; it does not provide the controlled, private access to AWS services inside the AWS backbone.\n\n## Key Concepts\n- VPC Endpoints (Interface and Gateway) for private service access\n- Endpoint policies for least-privilege access\n- Egress control without Internet exposure\n\n## Real-World Application\nEssential for workloads handling sensitive data that must avoid Internet exposure while still needing access to AWS services like S3, DynamoDB, or Secrets Manager.\n","diagram":null,"difficulty":"intermediate","tags":["AWS VPC","AWS VPC Endpoints","Terraform","Kubernetes","AWS IAM","certification-mcq","domain-weight-20"],"channel":"aws-security-specialty","subChannel":"infrastructure-security","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T16:23:32.358Z","createdAt":"2026-01-11 16:23:32"},{"id":"aws-security-specialty-security-logging-1768189691059-0","question":"You are centralizing logs from multiple AWS accounts for an enterprise security monitoring program. Which configuration provides tamper-evident, centralized, multi-account logging with real-time alerting and long-term storage?","answer":"[{\"id\":\"a\",\"text\":\"Create a separate CloudTrail trail in each account and deliver logs to a central S3 bucket using cross-account permissions, and enable CloudWatch Logs for each account to generate alerts.\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Create a single AWS Organizations organization trail with multi-region logging, enable log file validation, deliver logs to a centralized S3 bucket in a dedicated log-archive account, and enable CloudWatch Logs integration and EventBridge rules for real-time alerts.\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Rely only on VPC Flow Logs and store them in S3 for later review, then configure CloudWatch alarms based on network metrics.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Enable a CloudTrail trail in the management account and deliver logs to a central S3 bucket using a bucket policy, assuming cross-account access via roles.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct option is B. An organization trail centralizes logging across all member accounts, provides multi-region coverage, enables log file validation for tamper evidence, and delivers logs to a centralized S3 bucket in a dedicated log archive account. Integrating CloudWatch Logs and EventBridge enables real-time alerts and scalable incident response.\n\n## Why Other Options Are Wrong\n- A: While you can centralize logs with per-account trails, this approach is more complex to manage at scale and does not inherently provide a single tamper-evident, multi-region trail or centralized long-term storage.\n- C: VPC Flow Logs capture network traffic, not administrative or API activity, so they don’t provide the comprehensive security logging required.\n- D: A single management account trail does not automatically cover all member accounts; organization trails are the supported pattern for multi-account centralized logging.\n\n## Key Concepts\n- AWS Organizations organization trails\n- Multi-region CloudTrail logging\n- Log file validation for tamper evidence\n- Centralized S3 log storage\n- CloudWatch Logs and EventBridge for real-time alerts\n\n## Real-World Application\nUsed in enterprises to achieve unified visibility across many accounts, enabling rapid detection and response to security events across the entire AWS environment.","diagram":null,"difficulty":"intermediate","tags":["AWS CloudTrail","AWS Organizations","Amazon S3","CloudWatch","certification-mcq","domain-weight-18"],"channel":"aws-security-specialty","subChannel":"security-logging","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T03:48:11.061Z","createdAt":"2026-01-12 03:48:11"},{"id":"aws-security-specialty-security-logging-1768189691059-1","question":"You suspect unauthorized data exfiltration to an S3 bucket and want near real-time alerts on object-level API activity from unknown IP addresses. Which combination is the most appropriate to achieve this?","answer":"[{\"id\":\"a\",\"text\":\"Enable CloudTrail data events for the bucket, stream logs to CloudWatch Logs, and create a CloudWatch metric filter and Alarm for PutObject events from unknown IP addresses; optionally use EventBridge to forward to a SIEM.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Enable VPC Flow Logs for the VPC containing the application and rely on CloudTrail management events for object operations.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Enable S3 server access logs and set up a CloudWatch alarm on the access log bucket for PutObject events.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Enable CloudTrail data events for the bucket but do not enable CloudWatch integration; rely on CloudTrail console alerts instead.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA. Enabling CloudTrail data events for the bucket captures object-level API activity (e.g., PutObject, GetObject). Streaming those logs to CloudWatch Logs allows you to build metric filters and alarms that trigger when events originate from unknown or unexpected IP addresses, providing near real-time alerts. Integrating with EventBridge enables further routing to SIEM or notification systems.\n\n## Why Other Options Are Wrong\n- B: VPC Flow Logs monitor network traffic at the IP level, not S3 object-level API activity, so they won’t detect S3 data events effectively.\n- C: S3 server access logs are written after the fact and can have delays; they don’t provide real-time alerting for object-level API activity.\n- D: Without CloudWatch integration, you lose real-time alerting capability and fail to quickly detect exfiltration attempts.\n\n## Key Concepts\n- CloudTrail data events vs management events\n- CloudWatch Logs for real-time alerting\n- IP-based anomaly detection in logs\n- EventBridge integration for automated responses\n\n## Real-World Application\nSupports rapid detection of data exfiltration attempts to S3 by alerting security teams in near real time and enabling quick containment.","diagram":null,"difficulty":"intermediate","tags":["AWS CloudTrail","Amazon S3","CloudWatch","EventBridge","certification-mcq","domain-weight-18"],"channel":"aws-security-specialty","subChannel":"security-logging","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T03:48:11.472Z","createdAt":"2026-01-12 03:48:11"},{"id":"aws-security-specialty-security-logging-1768189691059-2","question":"During an incident response exercise, you need to verify that CloudTrail logs from all accounts in your organization have not been tampered with and are tamper-evident, while also ensuring long-term integrity for investigations. Which configuration provides strongest integrity guarantees?","answer":"[{\"id\":\"a\",\"text\":\"Enable CloudTrail Organization trails with log file validation enabled, deliver logs to a centralized S3 bucket, enable versioning on the bucket, and consider enabling S3 Object Lock for retention.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Disable log file validation but enable data events; rely on encryption to protect logs.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Store CloudTrail logs in a single S3 bucket in the master account with no versioning.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use only CloudWatch Logs encryption; CloudTrail integrity is not required.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA. CloudTrail Organization trails with log file validation provide tamper-evidence for log files, while delivering to a centralized S3 bucket ensures consistency across accounts. Enabling bucket versioning adds protection against accidental overwrites, and S3 Object Lock can enforce retention to prevent tampering or deletion, giving robust long-term integrity for investigations.\n\n## Why Other Options Are Wrong\n- B: Disabling log file validation removes cryptographic assurance of log integrity; data events alone do not guarantee tamper evidence.\n- C: A single bucket in the master account without versioning exposes logs to overwrites and loss of historical integrity across accounts.\n- D: Encrypting CloudWatch Logs protects data at rest but does not provide tamper evidence or cross-account integrity guarantees for CloudTrail logs.\n\n## Key Concepts\n- CloudTrail log file validation\n- Organization trails for multi-account coverage\n- S3 versioning and Object Lock for retention and immutability\n- Centralized logging architecture\n\n## Real-World Application\nEnsures defensible, auditable logs across an organization for incident response and forensics, even if some accounts are compromised.","diagram":null,"difficulty":"intermediate","tags":["AWS CloudTrail","AWS Organizations","Amazon S3","S3 Object Lock","certification-mcq","domain-weight-18"],"channel":"aws-security-specialty","subChannel":"security-logging","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T03:48:11.876Z","createdAt":"2026-01-12 03:48:11"}],"subChannels":["data-protection","general","infrastructure-security","security-logging"],"companies":["Citadel","IBM"],"stats":{"total":10,"beginner":0,"intermediate":10,"advanced":0,"newThisWeek":10}}