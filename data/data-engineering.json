{"questions":[{"id":"q-457","question":"You need to process 10GB of CSV files daily and load them into a PostgreSQL database. The files contain user activity logs with timestamps, user IDs, and event types. How would you design an efficient ETL pipeline using Python?","answer":"Use pandas with chunking for memory efficiency: `pd.read_csv('file.csv', chunksize=10000)`. Process each chunk, validate data types, convert timestamps with `pd.to_datetime()`, and use `psycopg2.extras.execute_batch()` for bulk inserts.","explanation":"## Key Components\n\n- **Memory Management**: Chunk large files to avoid memory issues\n- **Data Validation**: Check for missing values, correct data types\n- **Bulk Operations**: Use batch inserts instead of row-by-row\n- **Error Handling**: Log failed records for retry\n\n## Implementation Strategy\n\n```python\nimport pandas as pd\nimport psycopg2\nfrom psycopg2.extras import execute_batch\n\ndef process_csv_chunk(chunk):\n    # Clean and validate data\n    chunk['timestamp'] = pd.to_datetime(chunk['timestamp'])\n    chunk.dropna(inplace=True)\n    return chunk\n\ndef load_to_db(chunks):\n    conn = psycopg2.connect(database_url)\n    cursor = conn.cursor()\n    \n    for chunk in chunks:\n        processed_chunk = process_csv_chunk(chunk)\n        execute_batch(cursor, INSERT_QUERY, processed_chunk.values.tolist())\n    \n    conn.commit()\n    cursor.close()\n    conn.close()\n```","diagram":"flowchart TD\n  A[CSV Files] --> B[Chunk Processing]\n  B --> C[Data Validation]\n  C --> D[Type Conversion]\n  D --> E[Batch Insert]\n  E --> F[PostgreSQL]\n  B --> G[Error Logging]\n  G --> H[Retry Queue]","difficulty":"beginner","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Discord","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":["etl","chunking","pandas","memory efficiency","data validation","psycopg2"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2026-01-09T08:56:39.590Z","createdAt":"2025-12-26 12:51:05"},{"id":"q-488","question":"You're building a real-time analytics pipeline for a food delivery app. How would you design a data pipeline to process 1M events/day with 5-minute latency, considering data quality, schema evolution, and cost optimization?","answer":"Use Kafka + Flink for stream processing with exactly-once semantics. Implement schema registry for evolution, use Debezium CDC for change capture. Store raw events in S3, processed data in Redshift. Use schema validation, data quality checks, and dead letter queues for reliability. Optimize costs with spot instances, S3 Intelligent Tiering, and Redshift RA3 nodes. Monitor with CloudWatch metrics and data freshness alerts.","explanation":"## Architecture\n- **Ingestion**: Apache Kafka with 3 partitions, replication factor 3\n- **Processing**: Apache Flink for windowed aggregations and joins with exactly-once semantics\n- **Storage**: Raw events in S3 (Parquet format), processed data in Redshift\n\n## Data Quality & Schema Evolution\n- Schema validation using Confluent Schema Registry\n- Data quality checks with Great Expectations\n- Dead letter queue for failed events\n- Debezium CDC for reliable change data capture\n\n## Cost Optimization\n- Spot instances for Flink cluster\n- S3 Intelligent Tiering for raw data\n- Redshift RA3 nodes for compute-storage separation\n\n## Monitoring\n- CloudWatch metrics for pipeline health\n- Data freshness alerts","diagram":"flowchart TD\n  A[Mobile Events] --> B[Kafka Broker]\n  B --> C[Flink Processing]\n  C --> D[Data Quality Check]\n  D --> E[S3 Raw Storage]\n  D --> F[Redshift Analytics]\n  F --> G[Bi Dashboard]\n  C --> H[Dead Letter Queue]","difficulty":"intermediate","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Google","Instacart"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-08T11:58:29.130Z","createdAt":"2025-12-26 12:51:06"},{"id":"q-518","question":"You have a 10GB CSV file with user activity logs that needs to be processed daily. The file contains user_id, timestamp, action_type, and metadata. How would you design a data pipeline to efficiently process this file and load it into a data warehouse?","answer":"Use a distributed processing framework like Apache Spark or AWS Glue. Split the CSV into partitions, process in parallel, apply schema validation and data cleaning, then load into the warehouse using bulk insert operations.","explanation":"## Key Considerations\n- **File Size**: 10GB requires distributed processing for efficient handling\n- **Schema**: Define proper data types and constraints to ensure data quality\n- **Performance**: Implement partitioning and parallel processing for scalability\n\n## Pipeline Architecture\n- **Ingestion**: Store in S3 or similar object storage for scalable access\n- **Processing**: Use Spark/PySpark for distributed computation across multiple nodes\n- **Transformation**: Apply data cleaning, validation, and enrichment logic\n- **Loading**: Perform bulk insert operations to the warehouse (Snowflake, BigQuery)\n\n## Implementation Steps\n- Read CSV with proper schema definition to optimize parsing\n- Handle malformed records with comprehensive error logging\n- Apply business rules and data quality checks\n- Load processed data into warehouse using efficient bulk loading methods","diagram":"flowchart TD\n  A[10GB CSV] --> B[Spark Cluster]\n  B --> C[Data Validation]\n  C --> D[Transformation]\n  D --> E[Data Warehouse]\n  C --> F[Error Logging]","difficulty":"beginner","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Airbnb","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":["apache spark","aws glue","distributed processing","data warehouse","schema validation","data cleaning"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2026-01-08T11:42:43.700Z","createdAt":"2025-12-26 12:51:06"},{"id":"q-571","question":"How would you design a data pipeline to process 1M ride events per minute from Uber's real-time streaming system?","answer":"Design a data pipeline using Apache Kafka to ingest 1M ride events per minute with partitioning by ride_id, Apache Flink for real-time stream processing with windowed aggregations, and write optimized Parquet files to S3 partitioned by date and hour for efficient querying.","explanation":"## Architecture\n- **Ingestion**: Kafka with 3x replication, partitioned by geographic region\n- **Processing**: Flink with exactly-once semantics, 1-minute tumbling windows\n- **Storage**: S3 with Parquet format, compressed with Snappy\n\n## Key Considerations\n- **Scalability**: Auto-scale consumer groups based on lag metrics\n- **Fault tolerance**: Checkpointing to HDFS every 30 seconds\n- **Data quality**: Schema validation and duplicate detection\n\n## Code Example\n```python\nfrom pyflink.datastream import StreamExecutionEnvironment\nfrom pyflink.table import StreamTableEnvironment\nfrom pyflink.datastream.connectors import FlinkKafkaConsumer\nfrom pyflink.datastream.formats.json import JsonRowDeserializationSchema\n\n# Setup Flink environment\nenv = StreamExecutionEnvironment.get_execution_environment()\nenv.set_parallelism(12)  # Scale based on throughput\nenv.enable_checkpointing(30000)  # 30s checkpointing\n\n# Kafka consumer configuration\nkafka_props = {\n    'bootstrap.servers': 'kafka-broker-1:9092,kafka-broker-2:9092,kafka-broker-3:9092',\n    'group.id': 'ride-events-processor',\n    'auto.offset.reset': 'latest'\n}\n\n# Define ride event schema\ntype_info = Types.ROW_NAMED(\n    ['ride_id', 'timestamp', 'lat', 'lon', 'driver_id', 'rider_id', 'fare'],\n    [Types.STRING(), Types.SQL_TIMESTAMP(), Types.DOUBLE(), Types.DOUBLE(), \n     Types.STRING(), Types.STRING(), Types.DOUBLE()]\n)\n\n# Create Kafka source\nkafka_source = FlinkKafkaConsumer(\n    topics='ride-events',\n    deserialization_schema=JsonRowDeserializationSchema.builder()\n        .type_info(type_info)\n        .build(),\n    properties=kafka_props\n)\n\n# Process ride events with 1-minute windows\nride_stream = env.add_source(kafka_source) \\\n    .key_by(lambda x: x[2]) \\\n    .window(TumblingEventTimeWindows.of(Time.minutes(1))) \\\n    .aggregate(\n        aggregate_function=RideMetricsAggregator(),\n        window_function=MetricsWindowFunction()\n    )\n\n# Write to S3 in Parquet format\nride_stream.add_sink(\n    StreamingFileSink.for_bulk_format(\n        's3://uber-ride-events/processed/',\n        ParquetBulkWriter.for_schema(type_info)\n    ).build()\n)\n\n# Execute pipeline\nenv.execute('uber-ride-events-pipeline')\n```\n\n## Monitoring\n- Consumer lag alerts\n- Processing latency metrics\n- Data completeness checks","diagram":"flowchart TD\n  A[Ride Events] --> B[Kafka Cluster]\n  B --> C[Flink Processing]\n  C --> D[S3 Parquet Files]\n  C --> E[Real-time Dashboard]\n  D --> F[Analytics DB]","difficulty":"beginner","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Twitter","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":["apache kafka","apache flink","stream processing","partitioning","parquet","windowed aggregations"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2026-01-07T03:44:27.623Z","createdAt":"2025-12-27T01:12:24.585Z"},{"id":"q-885","question":"You operate a multi-tenant SaaS analytics platform ingesting per-tenant event streams from Kafka into Snowflake. Each tenant has different event schemas that can evolve independently. Design a data pipeline to enforce per-tenant data contracts, support late-arriving events, and minimize schema drift while controlling storage costs. Include schema versioning, validation, and deployment safety steps?","answer":"Design a per-tenant schema registry with versioned contracts and a streaming processor that validates each event against its tenant’s current contract, routing to tenant-specific Snowflake partitions ","explanation":"## Why This Is Asked\nTests ability to model per-tenant contracts and schema evolution in a streaming pipeline and to handle late-arriving data without breaking consumers.\n\n## Key Concepts\n- Per-tenant data contracts and a registry\n- Schema evolution policies and compatibility checks\n- Late-arrival handling with watermarking\n- Cost-aware storage isolation per tenant\n\n## Code Example\n```javascript\n// Pseudo-code illustrating per-tenant validation\nfunction validateEvent(event, tenant) {\n  const contract = registry.getContract(tenant, event.version);\n  return contract ? contract.validate(event) : false;\n}\n```\n\n## Follow-up Questions\n- How would you test schema drift detection?\n- How would you roll back a bad schema change without data loss?","diagram":null,"difficulty":"intermediate","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["NVIDIA","Snowflake","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T14:23:12.167Z","createdAt":"2026-01-12T14:23:12.167Z"},{"id":"q-915","question":"You ingest 200k newline-delimited JSON app events daily into S3. Each event has event_id, user_id, timestamp, event_type, and attributes. Design a beginner-friendly pipeline to deduplicate by event_id, hash user_id for privacy, validate required fields, and write Parquet data partitioned by date in a data lake. Address simple schema drift and testing?","answer":"Ingest daily JSON lines, deduplicate by event_id, hash user_id with SHA-256, validate required fields, and write Parquet data partitioned by date in a data lake. Include a simple schema-drift strategy","explanation":"## Why This Is Asked\n\nThis question tests practical basics of building a reliable, beginner-friendly data pipeline: ingestion from S3, deduplication, privacy via hashing, basic schema drift handling, and testing. It avoids company-specific traps while focusing on core ETL behavior.\n\n## Key Concepts\n\n- Ingestion from object storage and newline-delimited JSON\n- Deduplication by stable key event_id\n- Privacy: deterministic hashing of user_id\n- Schema drift: optional fields with defaults\n- Parquet partitioning by date; lightweight validation\n\n## Code Example\n\n```python\n# Pseudo-code sketch for deduplication\nimport json, hashlib\n\ndef process(lines):\n    seen = set()\n    for line in lines:\n        e = json.loads(line)\n        if 'event_id' not in e or 'user_id' not in e or 'timestamp' not in e:\n            continue\n        if e['event_id'] in seen:\n            continue\n        seen.add(e['event_id'])\n        yield {\n            'event_id': e['event_id'],\n            'timestamp': e['timestamp'],\n            'user_id_hashed': hashlib.sha256(e['user_id'].encode()).hexdigest(),\n            'event_type': e.get('event_type'),\n            'attributes': e.get('attributes', {})\n        }\n```\n\n## Follow-up Questions\n\n- How would you test idempotency for reprocessing files?\n- How would you monitor data quality and schema drift over time?","diagram":"flowchart TD\n  A[Source: S3] --> B[Ingest] \n  B --> C[Deduplicate by event_id] \n  C --> D[Transform] \n  D --> E[Partitioned Parquet Lake]","difficulty":"beginner","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","MongoDB","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T15:27:56.228Z","createdAt":"2026-01-12T15:27:56.228Z"},{"id":"q-176","question":"How would you design a data pipeline that handles both batch and streaming workloads for real-time analytics?","answer":"I would implement a Lambda architecture that combines both batch and streaming processing. The batch layer would handle historical data processing for accuracy, while the speed layer would process real-time data streams for immediate insights. Both layers would be integrated through a unified serving layer that merges results and provides a comprehensive view.","explanation":"## Why Asked\nThis question evaluates your understanding of modern data architecture patterns and your ability to design systems that handle multiple processing paradigms effectively.\n\n## Key Concepts\nLambda architecture, batch processing, stream processing, data consistency, real-time analytics, fault tolerance, scalability\n\n## Code Example\n```\n// Stream processing example (Apache Flink)\nDataStream<Event> stream = env.addSource(kafkaSource);\nstream.window(TumblingProcessingTimeWindows.of(Time.minutes(5)))\n      .aggregate(new CountAggregate())\n      .addSink(sink);\n```","diagram":"flowchart TD\n    A[Data Source] --> B[Batch Layer]\n    A --> C[Speed Layer]\n    B --> D[Batch View]\n    C --> E[Real-time View]\n    D --> F[Serving Layer]\n    E --> F\n    F --> G[Analytics/Queries]","difficulty":"beginner","tags":["streaming","kafka"],"channel":"data-engineering","subChannel":"streaming","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google","Meta","Netflix","Uber"],"eli5":"Imagine you have two ways to make lemonade at your lemonade stand! One way is making big batches in the morning (that's your batch processing) - you mix everything perfectly and it tastes great. The other way is making fresh cups one by one as friends arrive (that's your streaming) - super fast but maybe not as perfect. The smart trick is having both! You serve the fresh cups right away for instant refreshment, but you also have your perfect batch ready for when someone wants the best-tasting lemonade. Your brain keeps track of both - you know who got fresh cups now and who will get the perfect batch later. That way, everyone gets lemonade exactly when they need it, and you can tell your parents exactly how much you sold today!","relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-29T08:40:26.305Z","createdAt":"2025-12-26 12:51:06"},{"id":"q-222","question":"How would you design a Kafka Streams application to handle exactly-once processing with stateful aggregations while maintaining sub-second latency during peak loads of 100K events/sec?","answer":"Configure EOS_ALPHA with processing.guarantee=exactly_once_v2, use RocksDB state stores with changelog compaction, enable standby replicas, tune num.stream.threads=cores*2, set cache.max.bytes.buffering=10MB, and monitor consumer lag with Prometheus metrics.","explanation":"## Architecture\n**Exactly-once semantics**: EOS_ALPHA with transactional producers ensures atomic state updates and output commits\n**State management**: RocksDB local state + compacted changelog topics for fast recovery\n**Performance tuning**: Optimize thread pool, buffer sizes, and batch processing for sub-second latency\n\n## NFRs & Calculations\n**Throughput**: 100K events/sec ÷ 4 cores = 25K events/thread/sec\n**Latency**: Target <500ms with 100ms batch intervals\n**Storage**: 1GB state store ÷ 10MB cache = 100 cache entries\n**Recovery**: Standby replicas enable <30s failover\n\n## Key Configurations\n```properties\nprocessing.guarantee=exactly_once_v2\nnum.standby.replicas=1\ncache.max.bytes.buffering=10485760\ncommit.interval.ms=100\n```\n\n## Monitoring & Error Handling\n**Metrics**: consumer-lag, stream-latency, state-size\n**Alerts**: lag > 1000 events, latency > 1s\n**Recovery**: Automatic state restoration from changelog with incremental backups","diagram":"flowchart LR\n    A[Producer] --> B[Kafka Topic]\n    B --> C[Kafka Streams App]\n    C --> D[State Store]\n    C --> E[Standby Replica]\n    D --> F[Compact Topic]\n    E --> F\n    C --> G[Output Topic]\n    G --> H[Consumer]\n    I[Traffic Spike] --> C\n    C --> J[Adaptive Processing]\n    J --> K[Scale Out]","difficulty":"advanced","tags":["kafka","flink","kinesis"],"channel":"data-engineering","subChannel":"streaming","sourceUrl":null,"videos":{"longVideo":"https://www.youtube.com/watch?v=Z_gCv4Uum44"},"companies":["Amazon","Confluent","Netflix","Stripe","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-26T16:34:11.088Z","createdAt":"2025-12-26 12:51:07"},{"id":"q-248","question":"How would you implement exactly-once processing in a data pipeline when both source (Kafka) and sink (database) can fail, ensuring no duplicate data or data loss during network partitions and system crashes?","answer":"Implement Kafka transactions with idempotent producers (enable.idempotence=true), use database transaction IDs for deduplication, commit offsets only after successful DB commit, and configure EOS=ALWAYS for exactly-once semantics. Include retry logic with exponential backoff and dead-letter queue handling.","explanation":"## Core Implementation\n\n**Kafka Configuration:**\n```java\nProperties props = new Properties();\nprops.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);\nprops.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, \"pipeline-\" + UUID.randomUUID());\nprops.put(ProducerConfig.ACKS_CONFIG, \"all\");\nprops.put(ProducerConfig.RETRIES_CONFIG, Integer.MAX_VALUE);\n```\n\n**Transaction Pattern:**\n```java\n// Initialize transaction\nproducer.initTransactions();\n\ntry {\n    // Begin Kafka transaction\n    producer.beginTransaction();\n    \n    // Process and send to Kafka\n    ProducerRecord<String, String> record = new ProducerRecord<>(topic, key, value);\n    producer.send(record);\n    \n    // Database operation with transaction ID\n    String txId = UUID.randomUUID().toString();\n    jdbcTemplate.update(\"INSERT INTO data_table VALUES (?, ?, ?)\", \n        id, value, txId);\n    \n    // Commit offset only after DB success\n    producer.sendOffsetsToTransaction(offsets, consumer);\n    producer.commitTransaction();\n    \n} catch (Exception e) {\n    producer.abortTransaction();\n    // Retry logic with exponential backoff\n}\n```\n\n## Failure Recovery Strategies\n\n**Database Deduplication:**\n- Unique constraint on transaction_id column\n- INSERT IGNORE or ON CONFLICT DO NOTHING\n- Periodic cleanup of processed transaction IDs\n\n**Offset Management:**\n- Manual offset commits after successful processing\n- Store offsets in database for consistency\n- Use consumer group coordination for failover\n\n**Error Handling:**\n- Circuit breaker pattern for database failures\n- Dead-letter queue for unprocessable messages\n- Monitoring and alerting for transaction failures\n\n## Real-World Considerations\n\n**Performance Trade-offs:**\n- EOS=ALLS adds ~20% latency overhead\n- Increased memory usage for transaction state\n- Requires careful broker configuration (min.insync.replicas=2)\n\n**Edge Cases:**\n- Network partitions during commit phase\n- Broker leadership changes mid-transaction\n- Database connection pool exhaustion\n\n**Monitoring:**\n- Track transaction abort rates\n- Monitor consumer lag during failures\n- Alert on duplicate detection events","diagram":"graph TD\n    A[Kafka Topic] --> B[Consumer Poll]\n    B --> C[Begin Transaction]\n    C --> D[Process Records]\n    D --> E[Database UPSERT with TxID]\n    E --> F{Success?}\n    F -->|Yes| G[Send Offsets to Transaction]\n    F -->|No| H[Abort Transaction]\n    G --> I[Commit Transaction]\n    H --> J[Retry Processing]\n    I --> K[Next Poll]\n    J --> B","difficulty":"intermediate","tags":["dag","orchestration","scheduling"],"channel":"data-engineering","subChannel":"streaming","sourceUrl":null,"videos":null,"companies":["Amazon","Google","Meta","Microsoft","Netflix","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-26T16:38:30.418Z","createdAt":"2025-12-26 12:51:07"}],"subChannels":["general","streaming"],"companies":["Adobe","Airbnb","Amazon","Anthropic","Confluent","Discord","Google","Instacart","Meta","Microsoft","MongoDB","NVIDIA","Netflix","Oracle","Snap","Snowflake","Stripe","Twitter","Uber"],"stats":{"total":9,"beginner":5,"intermediate":3,"advanced":1,"newThisWeek":2}}