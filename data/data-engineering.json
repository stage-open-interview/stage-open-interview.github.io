{"questions":[{"id":"q-1077","question":"Design a data ingestion and processing pipeline for a global ride-hailing platform that ingests 5 TB/day of operational events from multiple regional Kafka topics and batch feeds. Requirements: idempotent upserts into a table (Iceberg/Delta), handle late-arriving events, schema evolution, and partition pruning by country/date. Compare Flink vs Spark for streaming, and outline testing, monitoring, and data quality checks?","answer":"Propose a hybrid pipeline: streaming with Flink writing to Iceberg, using idempotent upserts on (region, event_id), watermarking for late data, and Iceberg schema evolution; batch reconciliation with ","explanation":"## Why This Is Asked\nThis question probes end-to-end data engineering design, covering multi-source ingest, schema evolution, late data handling, and cost-aware architecture at scale.\n\n## Key Concepts\n- Multi-source ingestion, idempotent upserts, upsert semantics\n- Schema evolution, partition pruning, Iceberg/Delta\n- Late data handling, watermarking, out-of-order events\n- Streaming vs batch trade-offs, observability\n\n## Code Example\n```javascript\n// Example pseudo outline (not executable)\nfunction ingest(){ /* ... */ }\n```\n\n## Follow-up Questions\n- How would you test idempotence across regional sources?\n- What metrics and dashboards would you establish for data quality and SLA.\n","diagram":"flowchart TD\n  A[Regional Kafka Topics] --> B[Flink Streaming]\n  B --> C[Iceberg Table]\n  C --> D[Spark Batch Reconciliation]\n  D --> E[Monitoring & Quality]\n","difficulty":"advanced","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","LinkedIn","Lyft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T21:35:02.996Z","createdAt":"2026-01-12T21:35:02.996Z"},{"id":"q-1140","question":"Given daily 1 GB of web logs in JSON lines stored on S3 with fields user_id, timestamp, path, status, and optional referrer, design a beginner-friendly pipeline (Python or Node.js) that deduplicates by timestamp+user_id+path, validates required fields, normalizes timestamp to UTC, and writes date-partitioned Parquet to a data lake; include basic tests and monitoring?","answer":"Use a small streaming script (Python or Node.js) to read daily JSONL from S3, deduplicate by (timestamp, user_id, path), validate required fields (user_id, timestamp, path, status), coerce timestamp t","explanation":"## Why This Is Asked\nThis question tests practical, beginner-friendly construction of a data pipeline with common tasks: deduplication, validation, timestamp handling, and partitioned Parquet, plus testing and monitoring.\n\n## Key Concepts\n- JSON Lines parsing\n- Idempotent deduplication\n- Data quality checks (required fields, types)\n- Timestamp normalization to UTC\n- Parquet writing and date partitioning\n- Basic testing and observability\n\n## Code Example\n```javascript\nconst fs = require('fs');\nconst readline = require('readline');\n(async () => {\n  const rl = readline.createInterface({ input: fs.createReadStream('logs.jsonl'), crlfDelay: Infinity });\n  const seen = new Set();\n  for await (const line of rl) {\n    const obj = JSON.parse(line);\n    if (!obj.user_id || !obj.timestamp || !obj.path || !obj.status) continue;\n    const key = `${obj.timestamp}|${obj.user_id}|${obj.path}`;\n    if (seen.has(key)) continue;\n    seen.add(key);\n    // normalization and persistence would occur here\n  }\n})();\n```\n\n## Follow-up Questions\n- How would you scale this for multi-region logs?\n- What metrics and alerts would you add for production reliability?","diagram":"flowchart TD\n  Ingest[Ingest daily JSONL from S3] --> Dedup[Dedupe by timestamp+user_id+path]\n  Dedup --> Validate[Validate required fields]\n  Validate --> Normalize[Normalize timestamp to UTC]\n  Normalize --> Persist[Persist Parquet to date-partitioned path]\n  Persist --> Test[Basic tests and monitoring]","difficulty":"beginner","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","NVIDIA","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T01:27:30.496Z","createdAt":"2026-01-13T01:27:30.496Z"},{"id":"q-1272","question":"Design a data pipeline to ingest 2M GPU telemetry events per minute from a global fleet of AI training clusters into a data lake and a feature store. Events include host_id, region, timestamp, metric_type, and value. Requirements: immutable raw Parquet storage partitioned by region/hour; near-real-time metrics and anomaly alerts (1–2 minute latency) via a streaming engine; idempotent upserts into a feature store; schema evolution handling; late-arriving data; cost-aware storage/compute; monitoring and tests; compare Spark vs Flink for streaming components?","answer":"Architect a pipeline ingesting ~2M GPU telemetry events per minute from global clusters. Use Kafka -> Flink (or Spark Structured Streaming) to compute 1–2 minute rolling metrics and an anomaly score; ","explanation":"## Why This Is Asked\nThis question probes end-to-end data pipeline design for a hardware/AI-ops context, focusing on high-throughput ingestion, real-time analytics, schema evolution, late data, and cost—areas Apple/NVIDIA encounter in telemetry, MLops, and GPU monitoring.\n\n## Key Concepts\n- High-throughput streaming, watermarking, late data\n- Immutable raw storage and partitioning\n- Idempotent upserts in a feature store (Iceberg/Delta)\n- Schema evolution and data quality checks\n- Cost optimization (tiered storage, compute)\n\n## Code Example\n```javascript\n// Pseudo-code for anomaly score snippet\nfunction score(series) {\n  const mean = mean(series)\n  const std = stdDev(series)\n  return (series[-1] - mean) / std\n}\n```\n\n## Follow-up Questions\n- How would you test schema evolution without downtime?\n- How would you measure latency and data quality in production?","diagram":"flowchart TD\n  Kafka[(Kafka)]\n  Stream[(Streaming Engine)]\n  Raw[(Raw Parquet Lake)]\n  Features[(Feature Store)]\n  Alerts[(Alerts)]\n  Late[(Late Data)]\n  Kafka --> Stream\n  Stream --> Raw\n  Stream --> Features\n  Raw --> Features\n  Features --> Alerts\n  Late -.-> Stream","difficulty":"intermediate","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T07:38:05.936Z","createdAt":"2026-01-13T07:38:05.936Z"},{"id":"q-1498","question":"Design a global data ingestion and governance pipeline for a real-time ad-tech platform that processes 200k events/sec from 3 cloud regions into a centralized lakehouse. Each event has event_id, tenant_id, timestamp, event_type, and payload. Requirements: enforce data contracts via a central registry (schema + compatibility rules), support schema evolution with automatic catalog updates, and ensure multi-tenant data isolation and access control. Implement partitioning by tenant/date, handle late-arriving data within a 1–2 minute SLA, ensure data lineage and quality checks, and provide rollback semantics for contracts. Compare Iceberg vs Delta as the storage layer, outline testing/monitoring, and describe concrete example schemas and contract definitions. Include how you'd validate end-to-end?","answer":"I would implement a contract-first pipeline using a central schema registry (Avro/JSON), publish backward/forward-compatible schemas, and isolate data per tenant in Lakehouse partitions tenant/date. I","explanation":"## Why This Is Asked\nTests ability to design end-to-end, contract-driven lakehouse pipelines with multi-tenant governance, schema evolution, and robust observability.\n\n## Key Concepts\n- Data contracts and registry (Avro/JSON schemas, compatibility rules)\n- Multi-tenant isolation and fine-grained access control\n- Schema evolution with catalog updates and versioning\n- Late-arriving data handling and watermarking\n- Data lineage, quality checks, and rollback semantics\n- Iceberg vs Delta trade-offs at scale\n\n## Code Example\n```json\n{\n  \"type\": \"record\",\n  \"name\": \"AdEvent\",\n  \"namespace\": \"com.accel.adtech\",\n  \"fields\": [\n    {\"name\": \"event_id\", \"type\": \"string\"},\n    {\"name\": \"tenant_id\", \"type\": \"string\"},\n    {\"name\": \"timestamp\", \"type\": {\"type\": \"long\", \"logicalType\": \"timestamp-millis\"}},\n    {\"name\": \"event_type\", \"type\": \"string\"},\n    {\"name\": \"payload\", \"type\": \"string\"}\n  ]\n}\n```\n\n## Follow-up Questions\n- How would you validate a new schema version in canary before rollout?\n- How would you enforce per-tenant access to the lakehouse while preserving analytics flexibility?","diagram":"flowchart TD\n  A[Raw events] --> B[Schema Registry]\n  B --> C[Catalog (Iceberg/Delta)]\n  C --> D[Tenant/date partitions]\n  D --> E[Access / BI / ML]","difficulty":"advanced","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Scale Ai","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T19:32:55.649Z","createdAt":"2026-01-13T19:32:55.650Z"},{"id":"q-1692","question":"In a social app generating 100 GB/day of newline-delimited JSON events across 3 regions stored in S3, design a beginner-friendly batch pipeline that validates fields (event_id, user_id, timestamp, event_type), derives date, deduplicates by event_id, hashes user_id for privacy, and writes Parquet partitioned by date to a data lake. Compute a daily data-quality score (0-1) based on missing/invalid fields and store it in a metadata table. Outline tooling, testing, and monitoring?","answer":"Use a Spark batch job that reads 100 GB/day of newline-delimited JSON from S3, validates fields (event_id, user_id, timestamp, event_type), derives date, deduplicates by event_id, hashes user_id with ","explanation":"## Why This Is Asked\nAssesses ability to build an end-to-end batch pipeline with data validation, deduplication, privacy, and basic quality metrics. It also touches data lake organization and basic monitoring.\n\n## Key Concepts\n- Batch processing with Spark\n- Schema validation and deduplication\n- Privacy via sha-256 hashing\n- Parquet partitioning by date\n- Data-quality scoring and metadata tracking\n\n## Code Example\n```javascript\n# Pseudo-PySpark outline\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import sha2, col, to_date\nspark = SparkSession.builder.getOrCreate()\ndf = spark.read.json(\"s3://bucket/events/2025-*/\")\ndf = df.filter(\"event_id IS NOT NULL AND user_id IS NOT NULL\")\ndf = df.dropDuplicates([\"event_id\"])\ndf = df.withColumn(\"user_id_hash\", sha2(col(\"user_id\"), 256))\ndf = df.withColumn(\"date\", to_date(col(\"timestamp\")))\ndf.write.partitionBy(\"date\").parquet(\"s3://bucket/processed/\")\n```\n\n## Follow-up Questions\n- How would you adjust for late-arriving data or schema drift?\n- What tests would you add to validate deduplication and quality scoring?","diagram":"flowchart TD\n  A[Input data] --> B[Validation]\n  B --> C[Deduplicate by event_id]\n  C --> D[Hash user_id]\n  D --> E[Write Parquet partitioned by date]\n  E --> F[Daily quality score in metadata table]","difficulty":"beginner","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","PayPal","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T07:01:59.981Z","createdAt":"2026-01-14T07:01:59.982Z"},{"id":"q-1701","question":"Ingest 3 TB/day of regional event data from Kafka (EU/US/APAC) with user_id, session_id, event_type, timestamp, and attributes. Design a privacy-first analytics pipeline: per-region salt pseudonymization of user_id before cross-region joins, idempotent upserts into Apache Iceberg, event-time processing with 15-minute tumbling windows and 1-hour late data, and immutable audits and data contracts. Compare Spark Structured Streaming vs Flink for the streaming layer, and specify GDPR masking after aggregation?","answer":"Per-region salts for user_id, then SHA-256 hash to pseudonymize before cross-region joins; write idempotent upserts to Iceberg; use 15-minute tumbling windows with 1-hour allowed lateness; maintain im","explanation":"## Why This Is Asked\n\nThis question tests privacy-first, scalable analytics across regions with data contracts.\n\n## Key Concepts\n\n- Per-region pseudonymization\n- Iceberg upserts\n- Event-time windows and lateness handling\n- Immutable audits and data contracts\n- Spark vs Flink trade-offs for streaming\n\n## Code Example\n\n```javascript\n// Region-aware salt hash (Node.js)\nconst crypto = require('crypto');\nfunction saltedHash(region, id, salts){\n  const salt = salts[region] ?? salts.default;\n  return crypto.createHash('sha256').update(id + salt).digest('hex');\n}\n```\n\n## Follow-up Questions\n\n- How would you test idempotency and audit coverage?\n- How would you handle a new region with a different privacy policy?","diagram":"flowchart TD\n  Ingest[Ingest regional Kafka topics] --> Mask[Per-region salt masking of user_id]\n  Mask --> Upsert[Iceberg upserts in lakehouse]\n  Upsert --> Audit[Audit logs & data contracts]\n  Audit --> Monitor[Monitoring & testing]","difficulty":"advanced","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Lyft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T07:37:37.148Z","createdAt":"2026-01-14T07:37:37.149Z"},{"id":"q-1928","question":"Design a cost-aware real-time ingestion pipeline that processes 50 TB/day of clickstream data from multiple sources (Kafka topics and batch feeds) into a lakehouse. Ensure idempotent upserts into an Iceberg/Delta table, handle late-arriving events, and support schema evolution with partition pruning by date and region. Compare Spark Structured Streaming vs Flink for the path and outline data quality checks, testing, and monitoring strategies?","answer":"Design a pipeline ingesting 50 TB/day of clickstream data from Kafka topics and batch feeds into a lakehouse. Use an idempotent upsert into Iceberg/Delta, with watermarking for late events, and schema","explanation":"## Why This Is Asked\nTests ability to design scalable, cost-aware streaming pipelines with exactly-once semantics, late-arriving data handling, and schema evolution in a lakehouse.\n\n## Key Concepts\n- Ingestion from heterogeneous sources and batch feeds\n- Idempotent upserts into Iceberg/Delta\n- Watermarking and late-arriving data handling\n- Schema evolution and registry integration\n- Partition pruning by date/region; cost-aware resource planning\n- Testing, monitoring, and data quality checks\n- Trade-offs Spark vs Flink\n\n## Code Example\n```javascript\n// Pseudo: upsert function shape for events\nfunction upsertEvent(event){\n  // compute dedup key, apply to Iceberg/Delta table with primary key\n  // handle schema evolution via registry\n}\n```\n\n## Follow-up Questions\n- How would you implement exactly-once guarantees across Kafka and batch feeds?\n- How would you validate data quality and detect drift in this setup?\n- What monitoring metrics and SLAs would you track for latency and data freshness?","diagram":"flowchart TD\n  A[Sources: Kafka + Batch] --> B[Ingestion]\n  B --> C[Processing: watermarking & upsert]\n  C --> D[Lakehouse: Iceberg/Delta]\n  D --> E[Monitoring & Quality Checks]\n","difficulty":"intermediate","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["NVIDIA","Robinhood","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T17:43:26.111Z","createdAt":"2026-01-14T17:43:26.111Z"},{"id":"q-1965","question":"Design a GDPR/CCPA-compliant, multi-source data pipeline for an e-commerce analytics platform. Ingest CDC from OLTP databases plus batch feeds, preserve raw data immutably, and enable per-tenant data isolation. Implement lineage via a metadata catalog, apply privacy techniques (PII redaction and differential privacy for aggregates), and support schema evolution. Compare Flink vs Spark for streaming, outline tests, monitoring, and cost implications across regions?","answer":"Ingest CDC from OLTP and batch feeds into an immutable lake with per-tenant isolation and a metadata catalog for lineage. Redact PII and implement differential privacy for aggregates; store raw Parque","explanation":"## Why This Is Asked\nTests ability to design privacy-conscious, governance-aware pipelines across regions with real-world constraints.\n\n## Key Concepts\n- GDPR/CCPA data handling\n- CDC and batch ingestion\n- Immutable storage and Iceberg/Delta\n- Metadata catalogs and lineage\n- Data masking and differential privacy\n- Per-tenant isolation and cost-aware regional replication\n- Flink vs Spark for streaming\n\n## Code Example\n```javascript\n// pseudo-code sketch for DP on aggregates\n```\n\n## Follow-up Questions\n- How would you test schema evolution compatibility across regions?\n- What failure modes affect privacy and how would you mitigate?\n","diagram":null,"difficulty":"advanced","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","LinkedIn","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T18:59:13.741Z","createdAt":"2026-01-14T18:59:13.742Z"},{"id":"q-1971","question":"You run a product analytics pipeline for a global iOS/Android app (telemetry: user_id, device_id, ts, event, properties). Data must be ingested from a MongoDB Atlas source into a lakehouse on S3 using Apache Iceberg. Design an end-to-end pipeline that supports idempotent upserts, late-arriving events, and schema evolution, while enforcing PII masking and GDPR data deletion requests. Compare using Flink vs Spark for streaming, outline testing, monitoring, and data-quality checks?","answer":"Design: Ingest MongoDB Atlas via Change Streams into a durable sink (Kafka). Use a streaming engine (Flink or Spark Structured Streaming) to materialize an Iceberg table on S3 with upserts keyed by (u","explanation":"## Why This Is Asked\nThis question probes practical data engineering design for a privacy-conscious, scalable lakehouse pipeline that can handle cross-platform ingestion from MongoDB Atlas, with robust correctness (idempotence, late data, schema evolution) and trade-offs between Flink and Spark. It also touches testing, monitoring, and data quality.\n\n## Key Concepts\n- Change Data Capture from MongoDB Atlas (Change Streams)\n- Upserts with Iceberg on S3\n- Watermarks and lateness handling\n- Schema evolution in Iceberg and compatibility\n- PII masking and GDPR deletions in streaming\n- Flink vs Spark trade-offs; testing and monitoring\n\n## Code Example\n```javascript\n// Pseudocode outline for CDC -> Kafka -> Iceberg\n```\n\n## Follow-up Questions\n- How would you implement GDPR delete propagation across components?\n- How would you validate idempotence and late-arrival correctness?","diagram":"flowchart TD\n  A[MongoDB Atlas] --> B[Change Streams]\n  B --> C[Kafka]\n  C --> D[Flink/Spark]\n  D --> E[Iceberg on S3]\n  E --> F[Analytics/BI]","difficulty":"intermediate","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T19:27:27.822Z","createdAt":"2026-01-14T19:27:27.825Z"},{"id":"q-2272","question":"Design a global telemetry ingest pipeline for 8 regional Kafka clusters, totaling 20 TB/day of JSON events. Build an end-to-end flow into a lakehouse using Iceberg, ensuring idempotent upserts, late-arriving events, and schema evolution for nested JSON; enforce per-country data sovereignty and GDPR deletion requests. Compare Spark Structured Streaming vs Flink, and outline testing, monitoring, and data-quality checks?","answer":"Design a regional-first ingest into Iceberg lakehouses partitioned by country/date, using exactly-once streaming (Flink preferred; Spark for batch). Achieve idempotent upserts via MERGE INTO, support ","explanation":"## Why This Is Asked\nThis question probes cross-region data ingestion, schema evolution for nested data, and regulatory compliance at scale.\n\n## Key Concepts\n- Exactly-once streaming across multi-region sources\n- Iceberg MERGE Upserts and schema evolution\n- Data sovereignty, GDPR deletions, field masking\n- lineage, monitoring, data quality checks\n- Flink vs Spark trade-offs at scale\n\n## Code Example\n```sql\nMERGE INTO lake.country_data AS t\nUSING staged AS s\nON t.id = s.id\nWHEN MATCHED THEN UPDATE SET t.data = s.data\nWHEN NOT MATCHED THEN INSERT (id, data) VALUES (s.id, s.data)\n```\n\n## Follow-up Questions\n- How would you test late-arriving data and out-of-order events?\n- How to enforce per-country sovereignty in partitioning and retention?\n- How would GDPR deletion requests be propagated through the pipeline and sinks?","diagram":"flowchart TD\n  S[8 regional Kafka clusters] --> I[Ingestion Layer]\n  I --> P[Streaming Processor (Flink)]\n  P --> L[Iceberg Lakehouse (country/date partitions)]\n  L --> Governance[Governance: lineage, privacy, GDPR tombstones]","difficulty":"advanced","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T09:54:51.819Z","createdAt":"2026-01-15T09:54:51.819Z"},{"id":"q-2433","question":"Design a multi-tenant data ingestion pipeline that streams 40 TB/day of event data from dozens of publishers, each with its own schema and retention policy. Explain how you enforce per-tenant data contracts, support dynamic schema evolution, guarantee idempotent upserts into a lakehouse, handle late-arriving events and GDPR delete requests, and implement cross-region replication with cost controls. Compare Spark Structured Streaming vs Flink for this workload and outline testing and monitoring?","answer":"Implement a per-tenant schema registry and data contracts; stream 40 TB/day via Flink (exactly-once) or Spark, upsert into Iceberg partitioned by tenant/date; manage late events with watermarks and id","explanation":"## Why This Is Asked\nThis question probes multi-tenant ingestion, dynamic schemas, and governance at scale.\n\n## Key Concepts\n- Per-tenant data contracts and schema registry\n- Exactly-once streaming with idempotent upserts into Iceberg\n- Late-arriving events, watermarks, tombstones for deletes\n- Cross-region replication, cost controls, data lineage\n- Spark vs Flink trade-offs (backpressure, CDC, state management)\n\n## Code Example\n```javascript\n// Placeholder: pseudo-implementation sketch\nfunction validate(record, contract) { return schema.matches(record); }\n```\n\n## Follow-up Questions\n- How would you implement per-tenant quotas and isolation?\n- How would you test schema evolution without breaking tenants?","diagram":null,"difficulty":"advanced","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T17:51:08.942Z","createdAt":"2026-01-15T17:51:08.942Z"},{"id":"q-2460","question":"Design an end-to-end data pipeline for a multi-tenant ride-hailing platform where city-level feature stores power online fraud scoring and offline model training. Ingest 1) trip events, 2) driver status, 3) external pricing data; ensure per-city isolation, schema evolution, and GDPR deletion. Compare Spark Structured Streaming vs Flink for streaming, outline tests and monitoring?","answer":"Ingest via Kafka into a Spark Structured Streaming job or Flink, write features to an Iceberg-backed feature store partitioned by city. Provide online features in Redis for low latency; offline featur","explanation":"## Why This Is Asked\nAssesses ability to design multi-tenant pipelines, real-time features, and governance across regions.\n\n## Key Concepts\n- Multi-tenant feature store with city partitioning\n- Real-time vs offline feature separation\n- Schema evolution and data quality drift detection\n- GDPR delete propagation and data sovereignty\n- Testing with synthetic data and end-to-end pipelines\n\n## Code Example\n```python\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\n# Read from Kafka\ndf = spark.readStream.format(\"kafka\").option(\"kafka.bootstrap.servers\",\"kafka:9092\").option(\"subscribe\",\"trip_events\").load()\n# Simple feature extraction\nfeatures = df.selectExpr(\"CAST(value AS STRING) as json\").select(\"json\").where(\"json IS NOT NULL\")\n# Write to Iceberg feature store\nfeatures.writeStream.format(\"iceberg\").option(\"table\",\"feature_store.cityA.trips_features\").start()\n```\n\n## Follow-up Questions\n- How would you implement drift detection thresholds for features?\n- How to validate GDPR deletion propagation across online/offline stores?\n- How would you scale to thousands of cities while preserving isolation?\n- What metrics and alerts would you include for data quality and latency?","diagram":"flowchart TD\n  A[Ingest] --> B[Feature Store]\n  B --> C[Online Store]\n  B --> D[Offline Store]\n  C --> E[Serving Layer]\n  D --> F[Model Training]","difficulty":"intermediate","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","Lyft","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T19:02:31.364Z","createdAt":"2026-01-15T19:02:31.364Z"},{"id":"q-2564","question":"You're building a cross-region analytics platform ingesting 200M events/day into a lakehouse. Design a data-contract–driven pipeline with a central schema registry enforcing compatibility, add automated data quality, lineage, and privacy masking for PII, and ensure BI dashboards and model training never see raw sensitive fields. Compare Spark vs Flink for streaming governance and outline testing and monitoring?","answer":"Implement a data-contract-driven architecture with a centralized schema registry enforcing backward and forward compatibility. Ingest events using Flink for near real-time processing into a lakehouse, publishing masked views for BI and ML while maintaining strict access controls. Deploy automated data quality checks, comprehensive lineage tracking, and PII masking at the ingestion layer. Leverage Spark for batch transformations and complex analytics, while utilizing Flink for streaming governance with lower latency requirements. Establish robust testing frameworks including schema validation, data quality assertions, and end-to-end pipeline monitoring across all regions.","explanation":"## Why This Is Asked\n\nThis question assesses practical expertise in data governance, cross-region pipeline architecture, and technology selection. It evaluates a candidate's ability to design data contracts, enforce privacy controls, and provide observable lineage across streaming and batch workloads while balancing Spark vs Flink trade-offs.\n\n## Key Concepts\n\n- Data contracts and schema registry for compatibility enforcement\n- Privacy masking and access controls for PII protection\n- Data lineage and metadata catalog integration\n- Streaming (Flink) vs batch (Spark) processing considerations","diagram":null,"difficulty":"intermediate","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Bloomberg","Instacart"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T05:22:19.374Z","createdAt":"2026-01-15T22:53:48.446Z"},{"id":"q-2730","question":"You're building a multi-tenant retail analytics pipeline. Ingest JSON logs of user sessions from regional storefronts: {user_id, session_id, region, items: [{sku, qty, price}], event_ts, revenue, marketing}. Data lands in S3 daily and must feed a lakehouse (Iceberg) with near-real-time revenue per SKU by region. Requirements: per-tenant masking, GDPR deletion, schema evolution, idempotent upserts, late-arriving events, and robust testing/monitoring. Describe the end-to-end design, data contracts, and trade-offs between Flink and Spark for streaming?","answer":"Proposed pipeline: regional JSON events ingested via Kafka, parsed in Flink with event-time processing and 10-min lateness, written to Iceberg upserts (revenue by SKU-region-date; dims: product, regio","explanation":"## Why This Is Asked\nExplores end-to-end multi-tenant data governance, privacy enforcement, and the practical trade-offs of streaming engines in a lakehouse.\n\n## Key Concepts\n- Event-time processing and late data handling\n- Upserts in Iceberg with schema evolution\n- PII masking and GDPR deletion pipelines\n- Data contracts and data quality testing\n- Engine trade-offs: Flink vs Spark for stateful streams\n\n## Code Example\n```javascript\n// Flink streaming job skeleton (Java-like)\n```\n\n## Follow-up Questions\n- How would you test schema drift and GDPR deletion across partitions?\n- What would you monitor to catch data leakage between tenants?","diagram":"flowchart TD\n  Ingest[Ingest Events] --> Parse[Parse & Enrich]\n  Parse --> Mask[Mask PII]\n  Mask --> Upsert[Iceberg Upserts]\n  Upsert --> GDPR[GDPR Delete & Purge]\n  GDPR --> Monitor[Monitoring & Quality Checks]","difficulty":"intermediate","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Salesforce","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T09:42:13.301Z","createdAt":"2026-01-16T09:42:13.301Z"},{"id":"q-2762","question":"You're given a daily 200–500 MB CSV of user events with columns: user_id, event_time, event_type, payload. Design a beginner-friendly pipeline to load it into a Parquet lakehouse, partitioned by event_date, deduplicated on user_id+event_time, and with a small audit log for rows with invalid timestamps. Outline steps and a minimal PySpark snippet to start?","answer":"Use a PySpark batch pipeline: read the daily CSV, parse event_time with to_timestamp, extract event_date, deduplicate by user_id and event_time, and write to Parquet partitioned by event_date. Maintai","explanation":"## Why This Is Asked\nTests basic ETL design, simple validation, and idempotent daily loads using familiar tools.\n\n## Key Concepts\n- PySpark basics: read CSV, to_timestamp, to_date, dropDuplicates, partitionBy\n- Data quality: timestamp parsing and invalid-row auditing\n- Idempotent daily loads: deterministic dedup before write\n\n## Code Example\n```python\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import to_timestamp, to_date, col\n\nspark = SparkSession.builder.appName(\"DailyIngest\").getOrCreate()\n\npath = \"s3://bucket/daily/events_YYYYMMDD.csv\"\ndf = spark.read.csv(path, header=True, inferSchema=True)\ndf = df.withColumn(\"ts\", to_timestamp(col(\"event_time\"), \"yyyy-MM-dd'T'HH:mm:ss.SSSX\"))\nvalid = df.filter(col(\"ts\").isNotNull())\nvalid = valid.withColumn(\"event_date\", to_date(col(\"ts\")))\n\n# Deduplicate and write\nvalid.dropDuplicates([\"user_id\", \"ts\"]).write.partitionBy(\"event_date\").mode(\"append\").parquet(\"s3://bucket/warehouse/events\")\n\n# Audit invalid timestamps\ninvalid = df.filter(col(\"ts\").isNull())\ninvalid.write.csv(\"s3://bucket/audit/invalid_rows.csv\", header=True)\n```\n\n## Follow-up Questions\n- How would you test idempotence for daily re-runs?\n- How would you monitor data quality and failures in production?","diagram":"flowchart TD\n  Ingest[Ingest daily CSV] --> Validate[Parse and validate timestamps]\n  Validate --> Dedup[Deduplicate by user_id + event_time]\n  Dedup --> Write[Write partitioned Parquet by event_date]\n  Validate --> Audit[Log invalid timestamps]","difficulty":"beginner","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","LinkedIn","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T10:50:49.591Z","createdAt":"2026-01-16T10:50:49.592Z"},{"id":"q-2789","question":"You are onboarding a partner providing a daily JSON event feed (fields: user_id, event_ts, event_type, payload). Design a beginner-friendly batch ETL to normalize into a flat table, store Parquet in a lake partitioned by date, and perform idempotent upserts into the warehouse. Include basic data quality: non-null fields, timestamp sanity (within 90 days), and malformed-record handling. Compare batch vs streaming ingestion and outline tests?","answer":"Propose a Spark batch pipeline: read daily 50k JSON events, infer schema, flatten payload, enforce user_id and event_ts presence, drop bad rows, and filter out future timestamps. Write Parquet to lake","explanation":"## Why This Is Asked\nIntroduces batch ingestion with validation and idempotent upserts, a common beginner task for real-world data warehouses.\n\n## Key Concepts\n- Batch ETL basics: extract, transform, load\n- Data quality: non-null checks, timestamp sanity, malformed record handling\n- Parquet partitioning by date for query locality\n- Idempotent upserts via MERGE into warehouse tables\n\n## Code Example\n```javascript\n// Pseudo-code: batch ETL outline\nconst events = loadJSONLines('daily/input.json');\nconst flat = events.map(flattenEvent).filter(isValidEvent);\nwriteParquet(flat, `lake/partition_date=${date}`, {mode:'overwrite'});\nupsertTable('warehouse.events', `lake/partition_date=${date}`);\n```\n\n## Follow-up Questions\n- How would you handle schema drift between daily feeds?\n- What are simple testing strategies for data quality and idempotence?","diagram":"flowchart TD\n  A[Ingest daily feed] --> B[Parse & flatten to flat schema]\n  B --> C[Validate quality & filter malformed]\n  C --> D[Write Parquet partitioned by date]\n  D --> E[MERGE into warehouse (idempotent upserts)]\n  E --> F[Run tests & monitoring]","difficulty":"beginner","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Scale Ai","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T11:46:19.657Z","createdAt":"2026-01-16T11:46:19.657Z"},{"id":"q-2884","question":"Ingest streaming IoT telemetry from a global fleet where devices send json envelopes with id, ts, region, metrics, and an optional base64-encoded payload. Build an end-to-end pipeline into a lakehouse that decodes payloads, handles schema evolution, late events, and per-region masking, and supports GDPR delete requests. Compare Spark Structured Streaming vs Flink for this workload and outline testing/monitoring strategies?","answer":"Architect a streaming IoT telemetry pipeline: decode optional base64 payloads, apply schema evolution via a registry, and upsert into Iceberg; enforce region-based masking and GDPR delete hooks; imple","explanation":"## Why This Is Asked\nThis question probes practical streaming design for IoT data, including binary payloads, dynamic schemas, regional privacy rules, and lakehouse upserts, plus trade-offs between engines.\n\n## Key Concepts\n- Streaming ingestion, event-time, watermarking\n- Schema evolution with a registry and conformance checks\n- Data masking by region and GDPR delete integration\n- Upserts into Iceberg/lakehouse with idempotent sinks\n- Testing, observability, and fault-injection strategies\n\n## Code Example\n```javascript\n// Decode and parse base64 payload in a streaming map function\nconst payload = Buffer.from(record.base64Payload, 'base64').toString('utf-8');\nconst payloadObj = JSON.parse(payload);\n```\n\n## Follow-up Questions\n- How would you implement GDPR deletes across regions without data loss?\n- How would you validate schema evolution compatibility in production?","diagram":"flowchart TD\n  A[Ingest: Kafka/Kinesis] --> B[Parse envelope]\n  B --> C[Decode base64 payload]\n  C --> D[Schema validation/evolution via registry]\n  D --> E[Region-based masking & GDPR hooks]\n  E --> F[Upsert to Iceberg lakehouse]\n  F --> G[Observability & testing]","difficulty":"intermediate","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Netflix","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T15:55:34.786Z","createdAt":"2026-01-16T15:55:34.786Z"},{"id":"q-2984","question":"You receive a nightly CSV feed customers.csv with fields customer_id, signup_date, region, tier, and email_verified. A customer can appear on multiple nights with updated fields. Design a beginner-friendly pipeline to upsert into a canonical table customers_dim, ensure late changes are captured, implement basic data-quality checks, and describe how you'd test it?","answer":"Load nightly customers.csv into a staging table, then MERGE into customers_dim on customer_id using the latest row per id (based on signup_date or an updated_at column). Update non-key fields; insert ","explanation":"## Why This Is Asked\n\nAssesses ability to design a simple, reliable upsert pipeline with late-arriving updates and basic data quality checks at a beginner level.\n\n## Key Concepts\n\n- Upserts via MERGE or equivalent\n- Staging area and idempotency\n- Data quality checks: type safety, null checks, value constraints\n- Testing: unit tests with synthetic data, end-to-end smoke tests\n\n## Code Example\n\n```sql\n-- Pseudo SQL illustrating the merge into customers_dim\nMERGE INTO customers_dim AS d\nUSING staging AS s\nON d.customer_id = s.customer_id\nWHEN MATCHED THEN\n  UPDATE SET d.region = s.region,\n             d.tier = s.tier,\n             d.email_verified = s.email_verified,\n             d.signup_date = s.signup_date\nWHEN NOT MATCHED THEN\n  INSERT (customer_id, signup_date, region, tier, email_verified)\n  VALUES (s.customer_id, s.signup_date, s.region, s.tier, s.email_verified);\n```\n\n## Follow-up Questions\n\n- How would you handle schema evolution for new columns?\n- How would you test idempotence across multiple nightly runs?","diagram":"flowchart TD\n  A[Ingest nightly CSV] --> B[Staging Table]\n  B --> C[Merge into customers_dim]\n  C --> D[Quality Checks]\n  D --> E[Downstream Metrics]","difficulty":"beginner","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Meta","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T19:47:14.961Z","createdAt":"2026-01-16T19:47:14.962Z"},{"id":"q-3087","question":"You're handed a monthly 5GB JSON Lines file of e-commerce orders: {order_id, user_id, product_id, price, qty, timestamp, status}. Design a beginner-friendly batch ETL to deduplicate by order_id, normalize timestamp, fill missing price with 0 and status with 'new', and store as partitioned Parquet by year-month in a data lake. Explain validation, idempotence, and tests; compare Spark vs Pandas for this workload?","answer":"Use PySpark with a defined schema, read the JSON Lines file, cast the timestamp field to proper datetime format, fill missing price values with 0 and status with 'new', deduplicate by order_id using dropDuplicates(['order_id']), and write to Parquet partitioned by year-month using partitionBy('year', 'month') with overwrite mode for idempotence.","explanation":"## Why This Is Asked\n\nThis question evaluates practical batch ETL thinking for beginners, focusing on data quality, deduplication, and idempotent write operations.\n\n## Key Concepts\n\n- Batch ETL with schema enforcement\n- Deduplication on a key field\n- Time-based partitioning for efficient querying\n- Idempotent writes via partition overwrite\n\n## Code Example\n\n```python\nfrom pyspark.sql import SparkSession, functions as F\nfrom pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType\n\nspark = SparkSession.builder.getOrCreate()\nschema = StructType([\n  StructField('order_id', StringType(), nullable=False),\n  StructField('user_id', StringType(), nullable=True),\n  StructField('product_id', StringType(), nullable=True),\n  StructField('price', DoubleType(), nullable=True),\n  StructField('qty', IntegerType(), nullable=True),\n  StructField('timestamp', StringType(), nullable=True),\n  StructField('status', StringType(), nullable=True)\n])\n\n# Read JSON Lines with schema\ndf = spark.read.schema(schema).json('orders.jsonl')\n\n# Transform: normalize timestamp, fill missing values, deduplicate\ndf = df.withColumn('timestamp', F.to_timestamp('timestamp'))\n     .fillna({'price': 0, 'status': 'new'})\n     .withColumn('year', F.year('timestamp'))\n     .withColumn('month', F.month('timestamp'))\n     .dropDuplicates(['order_id'])\n\n# Write partitioned Parquet\n(df.write\n   .mode('overwrite')\n   .partitionBy('year', 'month')\n   .parquet('data_lake/orders/'))\n```\n\n## Validation Strategy\n\n- Schema validation on read\n- Null checks before fill operations\n- Record count verification pre/post deduplication\n- Partition existence validation on write\n\n## Idempotence\n\nPartition overwrite ensures re-running produces identical results. Deduplication guarantees single record per order_id regardless of input duplicates.\n\n## Tests\n\n- Unit: test transform functions with sample data\n- Integration: end-to-end pipeline with test JSON Lines file\n- Performance: validate Spark cluster handles 5GB efficiently\n\n## Spark vs Pandas\n\n**Spark**: Better for 5GB+ data, distributed processing, built-in partitioning, handles out-of-memory. **Pandas**: Simpler syntax for small data (<1GB), single-machine only, manual chunking required for large files. Spark is preferred for production batch ETL at this scale.","diagram":"flowchart TD\n  A[JSON Lines] --> B[Schema & Cast]\n  B --> C[Deduplicate]\n  C --> D[Partitioned Parquet Write]\n  D --> E[Commit final path]\n  E --> F[Quality checks]","difficulty":"beginner","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","MongoDB","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T05:04:20.773Z","createdAt":"2026-01-17T02:11:38.439Z"},{"id":"q-3181","question":"Design a global customer-analytics pipeline for a multi-region marketplace. Region-specific Kafka streams deliver events: user_id, session_id, region, event_type, timestamp, and attributes. Build a unified, near-real-time profile in an Iceberg lakehouse with per-region privacy, GDPR deletion, and schema evolution. Describe data contracts, idempotent upserts, late-arriving events, and trade-offs between Spark Structured Streaming and Flink; include testing and monitoring plan?","answer":"Adopt a Flink streaming pipeline feeding Iceberg. De-duplicate with a stable ingestion_id; upsert profiles by user_id using MERGE INTO. Enforce per-region privacy by masking PII and honoring GDPR dele","explanation":"## Why This Is Asked\nEvaluates ability to design cross-region streaming with privacy, schema evolution, and upserts.\n\n## Key Concepts\n- Data contracts, idempotent upserts, late-arriving events, GDPR deletion, per-region masking\n- Iceberg MERGE semantics, Flink vs Spark trade-offs, testing\n\n## Code Example\n```sql\nMERGE INTO iceberg.table AS t\nUSING staging AS s\nON t.user_id = s.user_id\nWHEN MATCHED THEN UPDATE SET ...\nWHEN NOT MATCHED THEN INSERT ...\n```\n\n## Follow-up Questions\n- How would you test idempotency and privacy masking at scale?\n- How would you monitor data quality across regions?","diagram":"flowchart TD\n  Kafka[Regional Kafka] --> Streaming[Flink]--> Iceberg[Iceberg Lakehouse]\n  Iceberg --> Profile[Unified User Profile]\n  GDPR[GDPR Deletions] --> Iceberg\n  Monitor[Monitoring & Alerts] --> Iceberg","difficulty":"intermediate","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Instacart","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T05:41:50.722Z","createdAt":"2026-01-17T05:41:50.722Z"},{"id":"q-3189","question":"You operate a streaming ingestion system that consumes 50k JSON events per second from mobile devices across regions. Events have nested payloads with optional fields and must be transformed into a star schema in a lakehouse (Iceberg) with schema evolution. Data arrives late (up to 2 hours). Describe end-to-end design, including idempotent upserts, per-record data contracts, and per-record lineage. Compare Spark Structured Streaming vs Flink for processing and outline testing/monitoring?","answer":"Prefer Flink: ingest from Kafka with event-time processing and 2h lateness. Validate against a schema registry, parse nested JSON, upsert into Iceberg star schema using (device_id, event_id) as PK. Em","explanation":"## Why This Is Asked\n\nTests practical streaming design at scale: late data, schema evolution, upserts into a lakehouse, data contracts, and lineage, plus engine trade-offs.\n\n## Key Concepts\n\n- Upserts into Iceberg with PK\n- Event-time processing and lateness\n- Nested JSON and data contracts\n- Data lineage and observability\n\n## Code Example\n\n```javascript\n// Pseudo-code: upsert path to Iceberg\n```\n\n## Follow-up Questions\n\n- How would you version data contracts for evolving fields?\n- How do you validate lineage correctness in prod?","diagram":"flowchart TD\n  Kafka --> Flink\n  Flink --> Iceberg\n  Iceberg --> DataMart\n  DataMart --> Lineage","difficulty":"intermediate","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Discord","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T06:41:37.218Z","createdAt":"2026-01-17T06:41:37.218Z"},{"id":"q-3312","question":"Design an end-to-end cross-region streaming pipeline ingesting 2.5M events/day from mobile apps into a lakehouse. Build a real-time feature store with time-bounded features, support late arrivals up to 10 minutes, and enforce per-region residency and masking. Compare Spark Structured Streaming vs Flink for the pipeline, and outline testing, monitoring, and data-contract guarantees?","answer":"Design a cross-region streaming pipeline that builds a time-bounded feature store from 2.5M events/day. Include late arrivals (up to 10 minutes), per-region residency and masking, and idempotent featu","explanation":"## Why This Is Asked\nTests ability to design end-to-end streaming pipelines with real-world constraints like region residency, privacy masking, and feature store integration; evaluates trade-offs between engines.\n\n## Key Concepts\n- Streaming ingestion and watermarking\n- Feature store versioning and idempotent upserts\n- Late-arrival handling and data contracts\n- Data residency and privacy controls\n- Testing and observability\n\n## Code Example\n```python\n# Pseudo upsert for a feature row\ndef upsert_feature(key, feature, store):\n    existing = store.get(key)\n    if existing is None or feature.version > existing.version:\n        store.put(key, feature)\n```\n\n## Follow-up Questions\n- How would you test data contracts for late events while ensuring training reproducibility?\n- Which metrics define latency targets across regions and how would you enforce backpressure?","diagram":"flowchart TD\n  A[Ingest Events] --> B[Resolve Region Residency]\n  B --> C[Mask PII & Apply Privacy Rules]\n  C --> D[Compute Time-Bounded Features]\n  D --> E[Store in Feature Lakehouse]\n  E --> F[Serve to Models]","difficulty":"advanced","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Netflix","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T11:24:44.071Z","createdAt":"2026-01-17T11:24:44.071Z"},{"id":"q-3441","question":"Ingest 2 TB/day mobile app events in JSON with nested optional fields into an Iceberg lakehouse. Timestamps arrive in multiple time zones. Design end-to-end pipeline that normalizes times to UTC, flattens key nested fields into a flat schema, and upserts with per-record lineage. Include schema validation, handling of missing fields, and monitoring; compare Spark Structured Streaming vs Flink for this workload?","answer":"Design end-to-end: read 2 TB/day JSON events, normalize timestamps to UTC, flatten selected nested fields, and upsert into Iceberg with per-record lineage. Use a data contract to validate optional fie","explanation":"## Why This Is Asked\nThis angle tests time zone normalization, per-record lineage, and practical upsert challenges not covered by prior questions.\n\n## Key Concepts\n- Iceberg upserts, schema evolution\n- Nested JSON flattening, data contracts\n- Timezone normalization, idempotency, lineage\n\n## Code Example\n```javascript\n// Placeholder illustrative example: normalize timestamp and flatten nested fields\nfunction normalize(record) {\n  // pseudo-code\n}\n```\n\n## Follow-up Questions\n- How would you implement per-record lineage storage? \n- How do you validate schema evolution and backward compatibility? \n- How would you monitor data quality and alert on schema drift? ","diagram":"flowchart TD\n  Ingest[Ingest JSON] --> Normalize[Normalize Timestamps to UTC]\n  Normalize --> Flatten[Flatten Nested Fields]\n  Flatten --> Upsert[Idempotent Upsert into Iceberg]\n  Upsert --> Lineage[Per-Record Lineage]\n  Lineage --> Monitor[Monitoring & Alerts]","difficulty":"beginner","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T16:39:39.317Z","createdAt":"2026-01-17T16:39:39.317Z"},{"id":"q-3545","question":"Design a real-time user-event pipeline ingesting multi-region nested JSON into a lakehouse (Iceberg). Each event includes user_id, event_ts, app_version, and a nested payload with optional fields that can evolve. Build an end-to-end flow with per-record data contracts, idempotent upserts, and per-record lineage; support late data, GDPR deletion, and maintain an online feature store for scoring with sub-100ms reads. Compare Spark Structured Streaming vs Flink for the streaming path, and outline testing and monitoring?","answer":"Ingest multi-region Kafka into Iceberg with a central schema registry. Enforce per-record contracts via runtime schema validation; implement idempotent upserts using event_id; handle late data with ev","explanation":"## Why This Is Asked\n\nTests ability to design a scalable, observable streaming pipeline that enforces contracts, handles schema evolution, and provides low-latency serving while satisfying privacy needs.\n\n## Key Concepts\n\n- Data contracts and runtime schema validation\n- Idempotent upserts and event_id management\n- Late-arrival handling with event-time semantics\n- GDPR deletions and tombstone strategies\n- Online feature store integration and serving latency targets\n- Spark vs Flink trade-offs for streaming\n- Observability: tests, monitoring, data quality checks\n\n## Code Example\n\n```python\ndef validate_event(event):\n    # Ensure required fields exist and types are correct\n    if 'user_id' not in event or 'event_ts' not in event:\n        return False\n    # Optional nested fields can be validated if present\n    return True\n```\n\n## Follow-up Questions\n\n- How would you test this pipeline with synthetic regional data and drift scenarios?\n- What rollback and backfill strategies would you implement for schema changes and GDPR deletions?","diagram":null,"difficulty":"intermediate","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Netflix","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T20:43:46.945Z","createdAt":"2026-01-17T20:43:46.945Z"},{"id":"q-3693","question":"Ingest 3 TB/day of JSON telemetry from 30 regional devices into an Iceberg lakehouse. Events include nested payloads with optional fields. Build a streaming pipeline that upserts using an idempotent upsert_id (device_id+event_id), enforces per-record data contracts with a schema registry, and records lineage. Handle late data (12h) and tombstones for deletes. Compare Spark Structured Streaming vs Flink and outline tests/monitoring?","answer":"Derive upsert_id from device_id+event_id and upsert into Iceberg via MERGE for idempotence. Validate each record against a schema registry (Avro/JSON) and emit per-record lineage to a lineage store. I","explanation":"## Why This Is Asked\nAssesses practical streaming design, data contracts, and lineage in a lakehouse; focuses on idempotence, schema evolution, late data, and operational visibility.\n\n## Key Concepts\n- Data contracts and schema evolution\n- Idempotent upserts in lakehouse (Iceberg)\n- Per-record lineage and data provenance\n- Late-arrival handling and tombstones\n\n## Code Example\n```javascript\n// Pseudo-Spark SQL MERGE for upsert into Iceberg\nspark.sql(`MERGE INTO iceberg.table AS t\n  USING updates AS s\n  ON t.upsert_id = s.upsert_id\n  WHEN MATCHED THEN UPDATE SET *\n  WHEN NOT MATCHED THEN INSERT *`)\n```\n\n## Follow-up Questions\n- How would you test schema evolution with non-breaking and breaking changes?\n- How would you monitor end-to-end latency and lineage quality?","diagram":"flowchart TD\n  Ingest[Ingest Events] --> Validate[Validate Contracts]\n  Validate --> Upsert[Upsert to Iceberg]\n  Upsert --> Lineage[Record Lineage]\n  Upsert --> Late[Late Data Handling]\n  Late --> Iceberg[Iceberg (Schema Evolution)]\n  Iceberg --> Monitor[Monitoring/Alerts]","difficulty":"beginner","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T05:42:30.295Z","createdAt":"2026-01-18T05:42:30.295Z"},{"id":"q-3821","question":"You're building a real-time graph analytics platform that ingests edge events from IoT devices: fields include device_id, neighbor_id, ts, tenant_id, and edge_attrs (nested, evolving). Data lands per-tenant in object storage and must feed a lakehouse (Iceberg) with near-real-time graph views. Requirements: per-tenant isolation, late-arriving edges up to 12 hours, idempotent upserts for edge state, schema evolution for edge_attrs, and end-to-end data contracts with lineage. Compare Flink vs Spark for streaming, and outline testing, monitoring, and GDPR-delete handling?","answer":"Ingest per-tenant edges with Flink, write idempotent upserts to Iceberg using a composite key (tenant_id, device_id, neighbor_id) for edge_attrs. Allow late arrivals up to 12 hours with constrained la","explanation":"## Why This Is Asked\n\nAssesses ability to design scalable, multi-tenant streaming graph pipelines with strict data contracts, lineage, and compliance controls in modern lakehouse setups.\n\n## Key Concepts\n\n- Real-time streaming patterns: watermarking, lateness windows, fault tolerance\n- Idempotent upserts in Iceberg with composite keys (tenant/device/neighbor)\n- Schema evolution for nested attributes and backward compatibility\n- Per-record lineage and auditability; GDPR delete handling via tombstones\n- Multi-tenant isolation, access controls, and governance\n\n## Code Example\n\n```javascript\n// Pseudo upsert for edge state\nfunction upsertEdge(existing, incoming) {\n  const key = `${incoming.tenant_id}:${incoming.device_id}:${incoming.neighbor_id}`;\n  if (!existing || existing.version < incoming.version) return incoming;\n  return existing;\n}\n```\n\n## Follow-up Questions\n\n- How would you test late-arrival handling across tenants?\n- What monitoring would you implement to detect cross-tenant data leakage or drift?\n","diagram":null,"difficulty":"advanced","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","Plaid","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T10:44:58.693Z","createdAt":"2026-01-18T10:44:58.693Z"},{"id":"q-3827","question":"Design a global real-time fraud-detection pipeline for a fintech with three regional streams (Kafka and MQTT) producing JSON/Parquet. Build a lakehouse (Iceberg) with bronze/silver/gold layers, ensure per-record lineage and region-based data isolation, and GDPR deletion. Implement idempotent upserts and late-data handling with schema evolution. Compare Spark Structured Streaming vs Flink for the stream path and outline testing, monitoring, and rollback strategies?","answer":"Design a 3-layer lakehouse: bronze raw events, silver canonical schema with evolution, gold features/alerts. Use region-scoped namespaces and per-record lineage; implement GDPR tombstones and region-b","explanation":"## Why This Is Asked\n\nTests ability to design a cross-region streaming data platform with strict governance, lineage, and regulatory compliance. It also probes trade-offs between two leading engines in real-time pipelines.\n\n## Key Concepts\n\n- Multi-source streaming ingestion (Kafka, MQTT)\n- Iceberg lakehouse with bronze/silver/gold\n- Per-record lineage and region-based isolation\n- GDPR deletion and tombstones\n- Idempotent upserts, late data handling, schema evolution\n- Spark vs Flink trade-offs and testing\n\n## Code Example\n\n```javascript\n// Pseudo: upsert pattern using unique key with a sink table\n```\n\n## Follow-up Questions\n\n- How would you validate schema evolution without breaking downstream queries?\n- What monitoring metrics and alerting would you implement for data drift?\n","diagram":null,"difficulty":"advanced","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","IBM","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T11:24:46.050Z","createdAt":"2026-01-18T11:24:46.051Z"},{"id":"q-4164","question":"Design a data mesh for ML features: ingest millions of signals from diverse sources, publish stable feature views to a central feature store, enforce per-source data contracts, implement drift detection and lineage to models, and honor GDPR deletions. Explain schema evolution, near-zero downtime feature updates, and a testing/monitoring plan?","answer":"Adopt a data mesh with a central feature store (Feast-like) backed by Iceberg tables. Enforce per-source contracts via a schema registry, publish versioned feature views, and emit lineage to models. I","explanation":"## Why This Is Asked\nThis question probes data-mesh ESL depth, feature-store design, data contracts, drift, and privacy controls in ML pipelines.\n\n## Key Concepts\n- Data mesh and centralized feature store\n- Per-source contracts and schema registry\n- Feature versioning and drift detection\n- GDPR deletion propagation\n- Testing and monitoring for quality and lineage\n\n## Code Example\n```yaml\n# Pseudo-config (not executable)\nsources:\n  payments:\n    contract: v1\n    retention_days: 365\nfeatures:\n  user_value_score_v1:\n    type: double\n    ttl_days: 7\n```\n\n## Follow-up Questions\n- How would you test drift detection thresholds and alerting?\n- What strategies ensure GDPR deletions propagate across feature versions?","diagram":"flowchart TD\nA[Publish per-source contracts] --> B[Central Feature Store]\nB --> C[Model training and inference]\nD[Drift detection] --> E[Alerts]\nE --> F[GDPR delete propagation]","difficulty":"advanced","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Adobe","Meta","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T05:52:39.661Z","createdAt":"2026-01-19T05:52:39.661Z"},{"id":"q-4246","question":"Design a policy-driven data masking and lineage system for streaming user data into a data lakehouse. Data arrives from regional sources with PII; implement dynamic masking based on consent and region, propagate masking decisions downstream, and ensure end-to-end lineage and a reproducible testable rollback. Include data contracts, schema evolution, and GDPR delete handling. Compare Spark vs Flink for the streaming leg and outline testing/monitoring?","answer":"Build a policy-driven masking layer in the streaming path, sourcing consent and region from a policy service. Apply per-record field masking before writing to Iceberg, tag each row with policy_id for ","explanation":"## Why This Is Asked\nAssesses ability to design privacy-aware pipelines with per-record masking and full lineage in a lakehouse, plus real-world testing and compliance.\n\n## Key Concepts\n- Policy-driven masking; consent/regional governance\n- Per-record lineage propagation; OpenLineage-compatible\n- Data contracts; schema evolution; GDPR delete handling\n- Streaming choice: Flink vs Spark trade-offs\n\n## Code Example\n```python\ndef apply_mask(record, policy):\n    if policy.get('mask_email'):\n        record['email'] = mask_email(record['email'])\n    return record\n```\n\n## Follow-up Questions\n- How would you test end-to-end masking correctness across regions?\n- How would you backfill masking decisions if consent changes retroactively?","diagram":null,"difficulty":"intermediate","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Google","Meta","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T09:59:56.306Z","createdAt":"2026-01-19T09:59:56.306Z"},{"id":"q-4395","question":"Design a multi-region CDC ingest for a payments platform: region adapters expose fields (payment_id, user_id, region, timestamp, event_type, metadata) with differing schemas; implement a canonical Avro, a changelog for schema evolution, and an end-to-end pipeline that upserts into Iceberg lakehouse via Flink (or Beam on Dataflow). Ensure per-event lineage, GDPR delete handling, and test/monitor strategies; justify trade-offs between Flink and Beam?","answer":"Ingest with a canonical Avro and region adapters; publish a region-specific changelog; sink via Flink exactly-once upserts to Iceberg; record per-event lineage in a catalog; support GDPR deletes via t","explanation":"## Why This Is Asked\nTests ability to design cross-region, schema-evolving streaming ecosystems with strict compliance, lineage, and cost considerations.\n\n## Key Concepts\n- CDC and region adapters\n- Canonical schema and schema evolution\n- Iceberg lakehouse upserts\n- Per-event lineage and data catalog\n- GDPR delete handling and tombstones\n\n## Code Example\n```javascript\n// Pseudo: enable checkpointing for exactly-once semantics in Flink\nval env = StreamExecutionEnvironment.getExecutionEnvironment()\nenv.enableCheckpointing(60000)\n```\n\n## Follow-up Questions\n- How would you test schema evolution without downtime?\n- What metrics indicate coastline drift and data-skew across regions?","diagram":"flowchart TD\n A[Region Ingest] --> B[CDC Changelog]\n B --> C[Kafka]\n C --> D[Flink Upsert to Iceberg]\n D --> E[Iceberg Lakehouse]\n E --> F[Data Catalog lineage]","difficulty":"advanced","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Amazon","Square","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T16:59:43.002Z","createdAt":"2026-01-19T16:59:43.002Z"},{"id":"q-4440","question":"Design and implement a CDC-based data pipeline that captures changes (insert/update/delete) from a PostgreSQL source at 50k rows/sec, streams to a data lake using Apache Iceberg, ensures idempotent upserts, per-record lineage, and automatic schema drift handling; compare Debezium + Spark Structured Streaming vs Flink for sink upserts, and describe testing and monitoring strategies?","answer":"Use a CDC source (Debezium) to capture inserts/updates/deletes from PostgreSQL at 50k rows/sec, emit an envelope with id, ts, op, and before/after, then upsert into Iceberg via Flink (exactly-once) us","explanation":"## Why This Is Asked\n\nTests ability to design CDC-driven pipelines with upserts in Iceberg, including deletes, lineage, and schema drift.\n\n## Key Concepts\n\n- CDC (Debezium)\n- Iceberg MERGE semantics\n- Exactly-once streaming\n- Data contracts and lineage\n- Monitoring and testing\n\n## Code Example\n\n```sql\nMERGE INTO iceberg_table AS t\nUSING staged_changes AS s\nON t.id = s.id\nWHEN MATCHED AND s.op = 'd' THEN DELETE\nWHEN MATCHED THEN UPDATE SET t.col1 = s.new_val, t.updated_at = s.ts\nWHEN NOT MATCHED THEN INSERT (id, col1, updated_at) VALUES (s.id, s.new_val, s.ts);\n```\n\n## Follow-up Questions\n\n- How would you test idempotence under retries?\n- How would you handle schema drift with evolving columns?\n","diagram":"flowchart TD\n  A[CDC Source: PostgreSQL via Debezium] --> B[Envelope + op]\n  B --> C[Iceberg Sink (MERGE INTO)]\n  C --> D[Data Lake]\n  B --> E[Lineage Channel]\n  E --> F[Audit/Monitoring]","difficulty":"intermediate","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Bloomberg","MongoDB","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T19:02:55.506Z","createdAt":"2026-01-19T19:02:55.506Z"},{"id":"q-4448","question":"You're building a geo-distributed ride-hailing analytics platform across 50 cities. Ingest GPS traces (vehicle_id, rider_id, ts, lat, lon), events (type: trip_start, trip_end), and telemetry. 2 TB/day of JSON. Requirements: per-city data isolation, GDPR deletion, schema evolution, idempotent upserts, late data up to 30 minutes, per-record data contracts, and robust data-quality gates. Propose end-to-end design: streaming ingestion, lakehouse (Iceberg), partitioning by city, and per-record lineage. Compare Spark Structured Streaming vs Flink for processing, and outline testing/monitoring strategies including contract tests?","answer":"Partition by city; ingest JSON events to a lakehouse (Iceberg) with idempotent upserts; enforce per-record data contracts via a schema registry (Avro/JSON) and support schema evolution; handle late da","explanation":"## Why This Is Asked\nTests handling of multi-city data sovereignty, strict data contracts, and production readiness for evolving schemas at scale. It probes thinking on idempotent upserts, late-arriving data, GDPR deletion, and governance alongside trade-offs between Spark and Flink.\n\n## Key Concepts\n- City-level partitioning and data isolation\n- Data contracts via schema registry and schema evolution\n- Lakehouse upserts with Iceberg and idempotency guarantees\n- Late data handling with watermarks and allowed lateness\n- GDPR deletion via tombstones and per-record lineage\n- Spark vs Flink trade-offs for latency and throughput\n- Data-quality gates and contract testing\n\n## Code Example\n```javascript\n// Pseudo contract check for incoming event against schema registry\nfunction validateEvent(event, schema) {\n  // validate required fields and types; fail fast on mismatch\n  // this is illustrative; integrate with a real registry in production\n  if (!event.vehicle_id || !event.ts) return false;\n  return true;\n}\n```\n\n## Follow-up Questions\n- How would you implement per-city data masking while preserving analytics?\n- How would you test schema evolution with backward/forward compatibility?","diagram":"flowchart TD\n  Ingest[Ingest streams] --> Process[Stream Processing]\n  Process --> Lake[Lakehouse (Iceberg)]\n  Lake --> BI[Analytics]\n  Ingest --> Govern[Governance & Deletion]","difficulty":"advanced","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Airbnb","Scale Ai","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T19:30:45.823Z","createdAt":"2026-01-19T19:30:45.823Z"},{"id":"q-4519","question":"You operate nightly ingestion of 30 regional vendor CSV feeds, each with up to 60 columns, some added over time. Data must land in a lakehouse (Iceberg) with per-record lineage and idempotent upserts, while dynamic schema evolution handles new columns. How would you design an end-to-end pipeline that enforces per-record data contracts, buffers late rows (up to 24h), isolates data by region, and validates data quality before load? Include simple monitoring and a concrete MERGE example?","answer":"Implement regional batch jobs that ingest CSV files into a staging Iceberg table with schema evolution capabilities, then upsert into the canonical table using MERGE on the primary key. Maintain per-record lineage in a dedicated LINEAGE table tracking source file, timestamp, and processing metadata. Configure a 24-hour buffer window for handling late-arriving data, validate all records against defined data contracts before staging, and enforce regional isolation through partitioned storage. Monitor job success rates, record counts, and data quality metrics with automated alerts.","explanation":"## Why This Is Asked\nTests practical design capabilities for batch-fed, multi-source data pipelines with evolving schemas, upserts, and lineage tracking. This scenario assesses intermediate-level understanding of data engineering patterns including data contracts, late data handling, and production monitoring.\n\n## Key Concepts\n- Regional batch processing with isolation\n- Iceberg schema evolution and idempotent upserts\n- Per-record lineage and contract enforcement\n- Late data buffering and quality validation\n- Production monitoring and observability\n\n## Code Example\n```sql\nMERGE INTO lake.target AS t\nUSING lake.staging AS s\nON t.primary_key = s.primary_key\nWHEN MATCHED THEN UPDATE SET *\nWHEN NOT MATCHED THEN INSERT *;\n```","diagram":"flowchart TD\n  Ingest[Regional CSV Feeds] --> Stage[Stage in Iceberg (staging)]\n  Stage --> Upsert[Upsert into Canonical Iceberg]\n  Upsert --> Lineage[Per-record Lineage]\n  Lineage --> Monitor[Quality Monitoring]","difficulty":"beginner","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Google","Instacart"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T07:13:04.669Z","createdAt":"2026-01-19T22:00:38.656Z"},{"id":"q-457","question":"You need to process 10GB of CSV files daily and load them into a PostgreSQL database. The files contain user activity logs with timestamps, user IDs, and event types. How would you design an efficient ETL pipeline using Python?","answer":"Use pandas with chunking for memory efficiency: `pd.read_csv('file.csv', chunksize=10000)`. Process each chunk, validate data types, convert timestamps with `pd.to_datetime()`, and use `psycopg2.extras.execute_batch()` for bulk inserts.","explanation":"## Key Components\n\n- **Memory Management**: Chunk large files to avoid memory issues\n- **Data Validation**: Check for missing values, correct data types\n- **Bulk Operations**: Use batch inserts instead of row-by-row\n- **Error Handling**: Log failed records for retry\n\n## Implementation Strategy\n\n```python\nimport pandas as pd\nimport psycopg2\nfrom psycopg2.extras import execute_batch\n\ndef process_csv_chunk(chunk):\n    # Clean and validate data\n    chunk['timestamp'] = pd.to_datetime(chunk['timestamp'])\n    chunk.dropna(inplace=True)\n    return chunk\n\ndef load_to_db(chunks):\n    conn = psycopg2.connect(database_url)\n    cursor = conn.cursor()\n    \n    for chunk in chunks:\n        processed_chunk = process_csv_chunk(chunk)\n        execute_batch(cursor, INSERT_QUERY, processed_chunk.values.tolist())\n    \n    conn.commit()\n    cursor.close()\n    conn.close()\n```","diagram":"flowchart TD\n  A[CSV Files] --> B[Chunk Processing]\n  B --> C[Data Validation]\n  C --> D[Type Conversion]\n  D --> E[Batch Insert]\n  E --> F[PostgreSQL]\n  B --> G[Error Logging]\n  G --> H[Retry Queue]","difficulty":"beginner","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Discord","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":["etl","chunking","pandas","memory efficiency","data validation","psycopg2"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2026-01-09T08:56:39.590Z","createdAt":"2025-12-26 12:51:05"},{"id":"q-4590","question":"You're building a real-time ad-tech analytics pipeline with impression and click events from multiple CDNs; data must be region-isolated, support late events up to 15 minutes, and GDPR deletions, with Iceberg as the lakehouse. Design end-to-end ingestion, processing, and storage, plus lineage and testing; compare Spark vs Flink trade-offs?","answer":"Implement region-scoped Kafka topics with Flink for low-latency upserts into Iceberg tables partitioned by region and date. Enforce per-record data contracts using Avro schemas with evolution rules, apply watermarking with 15-minute lateness tolerance, and handle GDPR deletions through Iceberg's row-level delete capabilities combined with metadata-based lineage tracking.","explanation":"## Why This Is Asked\nThis question evaluates practical streaming architecture design incorporating data contracts, late data handling, and privacy controls across geographic regions. It assesses understanding of Spark versus Flink trade-offs and implementation of idempotent upserts with comprehensive lineage tracking.\n\n## Key Concepts\n- Real-time ingestion with upserts to Iceberg lakehouse\n- Region-based partitioning and GDPR compliance\n- Data contracts enforced via Avro schemas\n- Watermarking and lateness tolerance management\n- Event lineage through unique IDs and metadata tracking\n\n## Code Example\n```scala\n// Flink job skeleton\nval env = StreamExecutionEnvironment.getExecutionEnvironment\nval stream = env.fromSource(KafkaSource.builder","diagram":null,"difficulty":"intermediate","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Bloomberg","Google","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T05:56:44.094Z","createdAt":"2026-01-20T02:47:51.795Z"},{"id":"q-4689","question":"Design a beginner-friendly ingestion pipeline for a mobile app emitting JSON events (open, screen_views) at ~60k/min across regions. Ingest into a lakehouse with per-record data contracts and idempotent upserts. Flatten nested payloads, support late events up to 15 minutes, and produce daily active users by country/version. Compare Airflow vs Dagster for orchestration and outline tests and monitoring?","answer":"Use Spark Structured Streaming with a strict JSON schema to ingest 60k/min across regions, enforce per-record contracts, and upsert into Iceberg via MERGE on event_id. Flatten payloads, apply a 15-min","explanation":"## Why This Is Asked\nTests practical ingestion design, idempotent upserts to Iceberg, late-data handling, and simple downstream analytics. It also probes tooling choices for orchestration.\n\n## Key Concepts\n- Strict JSON schema to enforce per-record contracts\n- Idempotent upserts using event_id as key\n- Watermarks and allowed lateness to handle late events\n- Flattening nested payloads for downstream analytics\n- MERGE INTO for Iceberg writes\n- Windowed daily DAU by country/version\n- Dagster vs Airflow for testing, logging, and observability\n\n## Code Example\n```python\n# PySpark-like pseudocode (simplified)\ndf = spark.readStream.format(\"json\").load(\"/events\")\nstaged = df.selectExpr(\"event_id\", \"region\", \"payload.*\")\nstaged.createOrReplaceTempView(\"staging\")\n\nspark.sql(\"\"\"\nMERGE INTO iceberg.db.events AS t\nUSING staging AS s\nON t.event_id = s.event_id\nWHEN MATCHED THEN UPDATE SET t.region = s.region, t.payload = s.payload\nWHEN NOT MATCHED THEN INSERT (event_id, region, payload) VALUES (s.event_id, s.region, s.payload)\n\"\"\")\n```\n\n## Follow-up Questions\n- How would you test idempotency and late-arrival handling end-to-end?\n- What observability would you add to detect schema drift and data-skew issues?","diagram":"flowchart TD\n  A[Ingest JSON events] --> B[Flatten payloads]\n  B --> C[MERGE into Iceberg]\n  C --> D[Compute DAU window]\n  D --> E[Store results]","difficulty":"beginner","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Coinbase","DoorDash","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T07:56:44.304Z","createdAt":"2026-01-20T07:56:44.304Z"},{"id":"q-4718","question":"You're building a geo-distributed rideshare telemetry pipeline across 24 regions. Ingest JSON events with nested metrics; land into Iceberg lakehouse with per-record lineage, and perform idempotent upserts on (vehicle_id, event_ts). Support dynamic schema evolution and late data up to 10 minutes. Describe end-to-end ingestion, region isolation, data contracts, a concrete MERGE example; compare Spark vs Flink for streaming and outline monitoring?","answer":"Design a region-scoped Iceberg lakehouse with per-record lineage and idempotent upserts on (vehicle_id, event_ts). Enforce contracts via a Schema Registry and Avro schemas; buffer late events up to 10","explanation":"## Why This Is Asked\n\nThis question probes ability to design a scalable, region-isolated streaming ingestion with evolving schemas, lineage, and upserts. It tests familiarity with Iceberg, schema management, and the trade-offs between Spark and Flink in production.\n\n## Key Concepts\n\n- Iceberg lakehouse + MERGE/UPSERT\n- Per-record lineage and data contracts via Schema Registry\n- Region isolation and partitioning\n- Late-arriving data handling and buffering\n- Observability: lineage tables, data quality gates\n\n## Code Example\n\n```sql\nMERGE INTO iceberg.ride_events AS target\nUSING staged_events AS source\nON target.vehicle_id = source.vehicle_id AND target.event_ts = source.event_ts\nWHEN MATCHED THEN UPDATE SET ...\nWHEN NOT MATCHED THEN INSERT ...\n```\n\n## Follow-up Questions\n\n- How would you implement per-region isolation in storage and compute?\n- What tests would you add for schema evolution and upserts?","diagram":null,"difficulty":"intermediate","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Bloomberg","Lyft","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T09:48:27.330Z","createdAt":"2026-01-20T09:48:27.330Z"},{"id":"q-4812","question":"Ingesting 100 TB/day of Parquet into an Iceberg lakehouse across regions, design a cost-aware data lifecycle with per-tenant retention, dynamic tiering to hot/warm/cold storage, and a policy engine that preserves Iceberg time travel; how would you ensure sub-60s dashboard reads, encryption at rest, per-tenant masking, and scalable partition pruning for cost control?","answer":"Use Iceberg partition pruning and metadata caching to read only relevant partitions; drive tiering with a policy engine (OPA) tagging data by tenant and age, moving files to hot S3 for recent data, wa","explanation":"## Why This Is Asked\nTests ability to design scalable data lifecycle with multi-tenant governance and cost control; integrates Iceberg features, storage tiering, policy-driven retention, and query-layer masking.\n\n## Key Concepts\n- Iceberg time travel via snapshots\n- Dynamic data tiering (hot/warm/cold)\n- Per-tenant data masking\n- Encryption at rest (KMS)\n- Partition pruning and metadata caching\n\n## Code Example\n```javascript\nfunction selectTier(tenant, ageDays) {\n  const retention = (tenantRetention[tenant] || 365);\n  if (ageDays < 30) return 'hot';\n  if (ageDays < retention) return 'warm';\n  return 'cold';\n}\n```\n\n## Follow-up Questions\n- How would you test data freshness and mask correctness across tiers?\n- How would you validate time travel remains accurate after tiering moves?","diagram":"flowchart TD\n  A[Ingest 100 TB/day] --> B[Iceberg Lakehouse]\n  B --> C[Tier Policy Engine]\n  C --> D[Hot/Warm/Cold Storage]\n  D --> E[Time Travel via Snapshots]\n  E --> F[Masked Tenant Views]\n  F --> G[Dashboard Reads <60s]","difficulty":"advanced","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Microsoft","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T14:43:15.367Z","createdAt":"2026-01-20T14:43:15.367Z"},{"id":"q-488","question":"You're building a real-time analytics pipeline for a food delivery app. How would you design a data pipeline to process 1M events/day with 5-minute latency, considering data quality, schema evolution, and cost optimization?","answer":"Use Kafka + Flink for stream processing with exactly-once semantics. Implement schema registry for evolution, use Debezium CDC for change capture. Store raw events in S3, processed data in Redshift. Use schema validation, data quality checks, and dead letter queues for reliability. Optimize costs with spot instances, S3 Intelligent Tiering, and Redshift RA3 nodes. Monitor with CloudWatch metrics and data freshness alerts.","explanation":"## Architecture\n- **Ingestion**: Apache Kafka with 3 partitions, replication factor 3\n- **Processing**: Apache Flink for windowed aggregations and joins with exactly-once semantics\n- **Storage**: Raw events in S3 (Parquet format), processed data in Redshift\n\n## Data Quality & Schema Evolution\n- Schema validation using Confluent Schema Registry\n- Data quality checks with Great Expectations\n- Dead letter queue for failed events\n- Debezium CDC for reliable change data capture\n\n## Cost Optimization\n- Spot instances for Flink cluster\n- S3 Intelligent Tiering for raw data\n- Redshift RA3 nodes for compute-storage separation\n\n## Monitoring\n- CloudWatch metrics for pipeline health\n- Data freshness alerts","diagram":"flowchart TD\n  A[Mobile Events] --> B[Kafka Broker]\n  B --> C[Flink Processing]\n  C --> D[Data Quality Check]\n  D --> E[S3 Raw Storage]\n  D --> F[Redshift Analytics]\n  F --> G[Bi Dashboard]\n  C --> H[Dead Letter Queue]","difficulty":"intermediate","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Google","Instacart"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-08T11:58:29.130Z","createdAt":"2025-12-26 12:51:06"},{"id":"q-4887","question":"Design a beginner-friendly batch ingestion pipeline for daily Parquet files arriving from three partners (via S3). Each file contains customer events with fields: customer_id, event_ts, product_id, and a nested metadata field. Over time, new optional fields may be added. Describe end‑to‑end processing to flatten, enforce per-record contracts, upsert into an Iceberg lakehouse partitioned by date, and emit per‑file lineage. Include a simple test plan and a concrete MERGE example?","answer":"Ingest daily Parquet files from three partners, flatten nested metadata, enforce per-record contracts (customer_id, event_ts, product_id required; sensible defaults for optional fields), and upsert in","explanation":"Why This Is Asked\n- Assesses ability to design batch ingestion with schema evolution, data contracts, and lineage.\n- Exercises practical steps for flattening, validation, and idempotent upserts into a lakehouse.\n- Includes testing strategy and a concrete MERGE example.\n\nKey Concepts\n- Batch ETL for Parquet and multi-partner ingestion\n- Per-record data contracts and defaults for optional fields\n- Flattening nested fields to flat schema\n- Iceberg schema evolution and MERGE-based upserts\n- Per-file lineage via a manifest/log table\n- Simple test plan and basic monitoring\n\nCode Example\n```python\n# Pseudo-code: validate and normalize a record before MERGE\ndef validate(record):\n    if 'customer_id' not in record or 'event_ts' not in record or 'product_id' not in record:\n        return False\n    if not isinstance(record['customer_id'], int):\n        return False\n    # event_ts as ISO string or int epoch\n    return True\n\ndef normalize(record):\n    record.setdefault('country', 'UNKNOWN')\n    record['metadata'] = flatten(record.get('metadata', {}))\n    return record\n```\n\nFollow-up Questions\n- How would you handle late-arriving files and ensure idempotent MERGE semantics?\n- How would you validate schema evolution with Iceberg (e.g., new fields) and perform backward-compatible reads?","diagram":"flowchart TD\n  A[Ingest Parquet files] --> B[Flatten nested fields]\n  B --> C[Validate per-record contract]\n  C --> D[Upsert to Iceberg (MERGE)]\n  D --> E[Emit per-file lineage]","difficulty":"beginner","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":null,"companies":["IBM","MongoDB","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T17:45:10.641Z","createdAt":"2026-01-20T17:45:10.641Z"},{"id":"q-4979","question":"Ingest daily JSONL batches of video view events from a CDN into a lakehouse. Each event has required fields: view_id, user_id, video_id, timestamp; optional: device_type, country. Design a beginner-friendly end-to-end ETL to validate per-record contracts, perform idempotent upserts by view_id, and support schema evolution for new fields. Partition by date, handle late-arriving data via staging, and include basic tests and monitoring?","answer":"Use a Spark batch ETL that reads daily JSONL batches from S3, enforces a per-record contract (view_id, user_id, video_id, timestamp required; device_type, country optional), deduplicates by view_id, and performs idempotent upserts into an Iceberg table partitioned by date. The pipeline includes a staging layer for late-arriving data, supports schema evolution for new fields, and incorporates basic validation tests with monitoring metrics.","explanation":"## Why This Is Asked\nTests practical batch ETL design with data contracts, idempotent operations, and Iceberg schema evolution in a realistic, beginner-friendly scenario.\n\n## Key Concepts\n- JSONL batch ingestion and per-record validation\n- Deduplication by natural key with idempotent upserts\n- Iceberg table schema evolution capabilities\n- Date-based partitioning and staging for data safety\n- Essential testing patterns and monitoring metrics\n\n## Code Example\n```python\n# PySpark implementation sketch\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, to_date\n\nspark = SparkSession.builder.getOrCreate()\n\n# Read daily batch\ndf = spark.read.json(\"s3://cdn-batch/video-views/20260101/*.jsonl\")\n\n# Validate required fields\nvalidated = df.filter(\n    col(\"view_id\").isNotNull() & \n    col(\"user_id\").isNotNull() & \n    col(\"video_id\").isNotNull() & \n    col(\"timestamp\").isNotNull()\n)\n\n# Add date partition and deduplicate\nfinal_df = validated.withColumn(\"date\", to_date(col(\"timestamp\")))\\\n    .dropDuplicates([\"view_id\"])\n```","diagram":"flowchart TD\n  CDN[CDN JSONL Batch] --> Staging[Staging Area]\n  Staging --> Validation[Contract Validation]\n  Validation --> Dedup[Deduplicate by view_id]\n  Dedup --> Iceberg[Iceberg Table (partition date)]\n  Iceberg --> Monitoring[Monitoring & Alerts]","difficulty":"beginner","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Bloomberg","DoorDash","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T04:35:47.401Z","createdAt":"2026-01-20T22:33:25.661Z"},{"id":"q-5078","question":"Design a beginner-friendly data ingestion pipeline for a fleet of autonomous devices issuing 800 events/sec per region as nested JSON. Ingest into a lakehouse (Iceberg) with per-record data contracts (JSON Schema), redaction of PII at the edge before load, and per-record lineage. Support additive schema evolution and late arrivals up to 15 minutes. Outline end-to-end flow, data quality checks, monitoring, and a simple idempotent upsert strategy?","answer":"Edge devices redact PII before sending; use JSON Schema contracts to validate each event; stream into a staging area and upsert into Iceberg using MERGE with event_id as the key; emit per-record linea","explanation":"## Why This Is Asked\n\nTests ability to design an end-to-end ingestion with privacy, lineage, schema evolution, and late data handling, at a beginner-friendly level.\n\n## Key Concepts\n\n- JSON Schema data contracts\n- Edge redaction of PII\n- Per-record lineage/audit\n- Iceberg upserts (MERGE) and additive schema evolution\n- Late-arrival handling with watermark\n\n## Code Example\n\n```sql\nMERGE INTO iceberg_table AS t\nUSING staging AS s\nON t.event_id = s.event_id\nWHEN MATCHED THEN UPDATE SET ...\nWHEN NOT MATCHED THEN INSERT (...);\n```\n\n## Follow-up Questions\n\n- How would you validate additive-only schema changes across regions?\n- How would you simulate late events during testing and measure latency?","diagram":"flowchart TD\n  A[Edge Device] --> B[Ingest & Validate]\n  B --> C[PII Redaction]\n  C --> D[Streaming Ingest]\n  D --> E[Staging -> Iceberg]\n  E --> F[Idempotent Upsert (MERGE)]\n  F --> G[Audit Lineage]\n  G --> H[Monitoring]","difficulty":"beginner","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":null,"companies":["NVIDIA","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T05:39:03.373Z","createdAt":"2026-01-21T05:39:03.373Z"},{"id":"q-518","question":"You have a 10GB CSV file with user activity logs that needs to be processed daily. The file contains user_id, timestamp, action_type, and metadata. How would you design a data pipeline to efficiently process this file and load it into a data warehouse?","answer":"Use a distributed processing framework like Apache Spark or AWS Glue. Split the CSV into partitions, process in parallel, apply schema validation and data cleaning, then load into the warehouse using bulk insert operations.","explanation":"## Key Considerations\n- **File Size**: 10GB requires distributed processing for efficient handling\n- **Schema**: Define proper data types and constraints to ensure data quality\n- **Performance**: Implement partitioning and parallel processing for scalability\n\n## Pipeline Architecture\n- **Ingestion**: Store in S3 or similar object storage for scalable access\n- **Processing**: Use Spark/PySpark for distributed computation across multiple nodes\n- **Transformation**: Apply data cleaning, validation, and enrichment logic\n- **Loading**: Perform bulk insert operations to the warehouse (Snowflake, BigQuery)\n\n## Implementation Steps\n- Read CSV with proper schema definition to optimize parsing\n- Handle malformed records with comprehensive error logging\n- Apply business rules and data quality checks\n- Load processed data into warehouse using efficient bulk loading methods","diagram":"flowchart TD\n  A[10GB CSV] --> B[Spark Cluster]\n  B --> C[Data Validation]\n  C --> D[Transformation]\n  D --> E[Data Warehouse]\n  C --> F[Error Logging]","difficulty":"beginner","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Airbnb","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":["apache spark","aws glue","distributed processing","data warehouse","schema validation","data cleaning"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2026-01-08T11:42:43.700Z","createdAt":"2025-12-26 12:51:06"},{"id":"q-5539","question":"Design an advanced cross-region ingestion and analytics pipeline for a multi-tenant SaaS platform where regional JSON events (customer_id, region, event_ts, pii: {name, email, address}, metrics) must be loaded into a central Iceberg lakehouse with per-record contracts, regional retention, and GDPR deletion support. Explain contracts, schema evolution, streaming upserts, late-arriving data, masking, lineage, and testing; compare centralized ingestion vs federated access?","answer":"Use a central Iceberg lakehouse with regional staging and a schema registry to enforce per-record contracts. Ingest JSON via Kafka with exactly-once, upsert into Iceberg using MERGE; handle late arriv","explanation":"## Why This Is Asked\nTests ability to design cross-region governance, data contracts, and privacy-preserving lakehouse pipelines under real-world constraints (sovereignty, GDPR, data lineage).\n\n## Key Concepts\n- Data contracts and schema evolution\n- Lakehouse upserts with late-arriving data\n- PII masking and retention policies\n- Data cataloging and lineage\n- Architecture trade-offs: centralized ingest vs federated access\n\n## Code Example\n```sql\nMERGE INTO iceberg_table AS t\nUSING staged AS s\nON t.key = s.key\nWHEN MATCHED THEN UPDATE SET ...\nWHEN NOT MATCHED THEN INSERT (...);\n```\n\n## Follow-up Questions\n- How would you validate schema evolution across regions?\n- How would you test GDPR deletion propagation?\n","diagram":null,"difficulty":"advanced","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Apple","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T05:38:13.870Z","createdAt":"2026-01-22T05:38:13.870Z"},{"id":"q-5595","question":"Design a regional, multi-tenant data pipeline where 60 regional feeds arrive daily as Parquet with evolving schemas. Store in Iceberg with per-record lineage, enforce per-record contracts, and isolate by region. Add a quarantine path for records failing contracts and a policy-driven masking engine to encrypt PII before load. Include testing, monitoring, and a concrete MERGE example?","answer":"Implement regional, multi-tenant ingestion of 60 daily Parquet feeds into Iceberg with per-record lineage. Enforce dynamic contracts at write (types, non-null PKs, regional constraints); divert failin","explanation":"## Why This Is Asked\nAssesses ability to design end-to-end, scalable data pipelines with per-record contracts, region isolation, quarantine flows, and masking policies, plus testing and monitoring strategies.\n\n## Key Concepts\n- Per-record contracts and data quality gates\n- Policy-driven data masking/encryption for PII\n- Quarantine path for contract-failure records\n- Iceberg upserts, lineage, and region-based isolation\n\n## Code Example\n```sql\nMERGE INTO iceberg_db.region.sales AS t\nUSING staging.region_a.sales AS s\nON t.id = s.id\nWHEN MATCHED THEN UPDATE SET t.amount = s.amount, t.last_seen = s.last_seen\nWHEN NOT MATCHED THEN INSERT (id, amount, last_seen) VALUES (s.id, s.amount, s.last_seen);\n```\n\n## Follow-up Questions\n- How would you test contract drift and quarantining routing at scale?\n- How would you measure masking policy performance and data latency?","diagram":null,"difficulty":"intermediate","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Bloomberg","Plaid","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T07:48:02.799Z","createdAt":"2026-01-22T07:48:02.799Z"},{"id":"q-5650","question":"You need to design an end-to-end CDC ingestion pipeline from PostgreSQL to an Iceberg-backed lakehouse for a multi-tenant system. Debezium events flow into Kafka and are upserted into per-tenant Iceberg tables. Explain idempotent MERGE strategies, late-arrival handling, schema evolution, per-record lineage, masking, and an ordering/latency trade-off between Spark Structured Streaming and Flink?","answer":"CDC from PostgreSQL via Debezium to Kafka; route by tenant_id to Iceberg per-tenant tables. Use MERGE INTO with (tenant_id, record_id) as the key for idempotent upserts. Handle late data with a capped","explanation":"## Why This Is Asked\nTests experience with multi-tenant data, CDC pipelines, and lakehouse semantics. It probes end-to-end thinking: data routing, upserts, schema evolution, and compliance.\n\n## Key Concepts\n- CDC ingestion and Debezium integration across tenants\n- Idempotent upserts using composite keys in Iceberg MERGE\n- Late-arrival handling with event-time watermarks\n- Schema evolution, per-record lineage, and masking for privacy\n- Trade-offs: Spark Structured Streaming vs Flink for throughput/latency\n\n## Code Example\n```sql\nMERGE INTO tenantA.sales AS t\nUSING staged AS s\nON t.tenant_id = s.tenant_id AND t.record_id = s.record_id\nWHEN MATCHED THEN UPDATE SET t.amount = s.amount, t.updated_at = s.updated_at\nWHEN NOT MATCHED THEN INSERT (tenant_id, record_id, amount, updated_at) VALUES (s.tenant_id, s.record_id, s.amount, s.updated_at);\n```\n\n## Follow-up Questions\n- How would you implement per-record masking and RBAC across tenants?\n- What monitoring would you add to detect drift and failed upserts across tenants?","diagram":null,"difficulty":"advanced","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Instacart","Meta","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T10:05:10.159Z","createdAt":"2026-01-22T10:05:10.159Z"},{"id":"q-5688","question":"In a data marketplace, vendors emit JSON records with tenant_id, dataset_id, record_id, ts, and payload (nested fields). Ingest into a shared Iceberg lakehouse with per-tenant isolation; enforce per-record data contracts; support dynamic schema evolution; handle late data up to 60 minutes; apply privacy masking for PII before load; ensure idempotent upserts and per-record lineage; propose contract registry, streaming framework choices, and a concrete MERGE example?","answer":"Design a registry-driven contract per (tenant_id, dataset_id). Use Spark/Flink to parse streaming JSON, validate against schemas, and push to Iceberg with 1) per-tenant isolation, 2) schema evolution ","explanation":"Why This Is Asked\n- Tests contract-driven data contracts and per-tenant isolation in a vendor marketplace.\n- Probes handling of schema drift and privacy masking before lakehouse load.\n- Assesses upsert semantics, lineage, and end-to-end monitoring.\n\nKey Concepts\n- Data contracts, schema evolution, per-tenant isolation\n- Privacy masking, GDPR-style redactions, idempotent upserts\n- Late data handling, streaming vs batch boundaries, lineage\n\nCode Example\n```javascript\n// Pseudo-pipeline skeleton showing contract check, masking, and upsert\n```\n\nFollow-up Questions\n- How would you test contract validity across tenants? What tooling?\n- How would you model lineage end-to-end for a single record across stages?","diagram":"flowchart TD\n  Vendor[Vendor Stream] --> Ingest[Streaming Ingest]\n  Ingest --> Validate[Contract Validation]\n  Validate --> Mask[Mask PII & Redact]\n  Mask --> Upsert[Iceberg Upsert]\n  Upsert --> Query[Analytical Queries]","difficulty":"intermediate","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Airbnb","Apple","Databricks"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T11:35:36.195Z","createdAt":"2026-01-22T11:35:36.195Z"},{"id":"q-571","question":"How would you design a data pipeline to process 1M ride events per minute from Uber's real-time streaming system?","answer":"Design a data pipeline using Apache Kafka to ingest 1M ride events per minute with partitioning by ride_id, Apache Flink for real-time stream processing with windowed aggregations, and write optimized Parquet files to S3 partitioned by date and hour for efficient querying.","explanation":"## Architecture\n- **Ingestion**: Kafka with 3x replication, partitioned by geographic region\n- **Processing**: Flink with exactly-once semantics, 1-minute tumbling windows\n- **Storage**: S3 with Parquet format, compressed with Snappy\n\n## Key Considerations\n- **Scalability**: Auto-scale consumer groups based on lag metrics\n- **Fault tolerance**: Checkpointing to HDFS every 30 seconds\n- **Data quality**: Schema validation and duplicate detection\n\n## Code Example\n```python\nfrom pyflink.datastream import StreamExecutionEnvironment\nfrom pyflink.table import StreamTableEnvironment\nfrom pyflink.datastream.connectors import FlinkKafkaConsumer\nfrom pyflink.datastream.formats.json import JsonRowDeserializationSchema\n\n# Setup Flink environment\nenv = StreamExecutionEnvironment.get_execution_environment()\nenv.set_parallelism(12)  # Scale based on throughput\nenv.enable_checkpointing(30000)  # 30s checkpointing\n\n# Kafka consumer configuration\nkafka_props = {\n    'bootstrap.servers': 'kafka-broker-1:9092,kafka-broker-2:9092,kafka-broker-3:9092',\n    'group.id': 'ride-events-processor',\n    'auto.offset.reset': 'latest'\n}\n\n# Define ride event schema\ntype_info = Types.ROW_NAMED(\n    ['ride_id', 'timestamp', 'lat', 'lon', 'driver_id', 'rider_id', 'fare'],\n    [Types.STRING(), Types.SQL_TIMESTAMP(), Types.DOUBLE(), Types.DOUBLE(), \n     Types.STRING(), Types.STRING(), Types.DOUBLE()]\n)\n\n# Create Kafka source\nkafka_source = FlinkKafkaConsumer(\n    topics='ride-events',\n    deserialization_schema=JsonRowDeserializationSchema.builder()\n        .type_info(type_info)\n        .build(),\n    properties=kafka_props\n)\n\n# Process ride events with 1-minute windows\nride_stream = env.add_source(kafka_source) \\\n    .key_by(lambda x: x[2]) \\\n    .window(TumblingEventTimeWindows.of(Time.minutes(1))) \\\n    .aggregate(\n        aggregate_function=RideMetricsAggregator(),\n        window_function=MetricsWindowFunction()\n    )\n\n# Write to S3 in Parquet format\nride_stream.add_sink(\n    StreamingFileSink.for_bulk_format(\n        's3://uber-ride-events/processed/',\n        ParquetBulkWriter.for_schema(type_info)\n    ).build()\n)\n\n# Execute pipeline\nenv.execute('uber-ride-events-pipeline')\n```\n\n## Monitoring\n- Consumer lag alerts\n- Processing latency metrics\n- Data completeness checks","diagram":"flowchart TD\n  A[Ride Events] --> B[Kafka Cluster]\n  B --> C[Flink Processing]\n  C --> D[S3 Parquet Files]\n  C --> E[Real-time Dashboard]\n  D --> F[Analytics DB]","difficulty":"beginner","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Twitter","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":["apache kafka","apache flink","stream processing","partitioning","parquet","windowed aggregations"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2026-01-07T03:44:27.623Z","createdAt":"2025-12-27T01:12:24.585Z"},{"id":"q-5778","question":"You receive a daily 25GB CSV feed from a partner with columns: user_id, event_time (ISO 8601), product_id, amount; some rows have missing user_id or negative amount, and a new optional column discount may appear over time. Design an end-to-end ingestion pipeline into a lakehouse that performs per-record data contracts, deduplicates by (user_id, event_time), and writes idempotently to a daily partitioned Iceberg table. Handle late arrivals up to 6 hours, implement schema evolution for new columns, and provide basic monitoring and tests. Compare Spark Structured Streaming vs Flink for this workload?","answer":"Approach: read the partner CSV with Spark Structured Streaming using a strict schema and per-record contracts (required fields, types, value ranges). Discard or quarantine invalid rows. Deduplicate on","explanation":"## Why This Is Asked\nTests ability to design a robust, beginner-friendly batch-to-lakehouse pipeline with data contracts, deduplication, and schema evolution, plus a practical tooling comparison.\n\n## Key Concepts\n- Data contracts and validation; dedup/upsert; late-arrival handling; schema evolution; monitoring.\n\n## Code Example\n```javascript\n// Implementation outline not provided here\n```\n\n## Follow-up Questions\n- How would you implement end-to-end tests for data quality and schema drift?\n- Which pipeline components would you instrument for auditing?","diagram":"flowchart TD\n  A[Partner CSV] --> B[Schema Validation]\n  B --> C{Valid}\n  C -->|Yes| D[Deduplicate & Upsert to Iceberg]\n  C -->|No| E[Quarantine]\n  D --> F[Partitioned Iceberg Table]\n  F --> G[Monitoring & Alerts]","difficulty":"beginner","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Hashicorp","LinkedIn"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T16:09:27.615Z","createdAt":"2026-01-22T16:09:27.615Z"},{"id":"q-5800","question":"Design a streaming ingestion and lakehouse architecture to ingest per-tenant telemetry and commerce data for Tesla, Instacart, and Hugging Face. Each tenant has unique data contracts, privacy masking, and GDPR deletion needs. Provide runtime data contracts, per-tenant masking, schema evolution, and governance via a policy engine; compare Spark vs Flink; outline tests, monitoring, and include a MERGE/UPSERT example and a simple data-flow diagram?","answer":"Propose a streaming data contracts-driven ingestion for three tenants (Tesla, Instacart, Hugging Face) into a lakehouse. Include per-tenant privacy masking, dynamic schema evolution, idempotent upsert","explanation":"## Why This Is Asked\n\nTests ability to design multi-tenant data contracts, runtime validation, privacy masking, and governance in a real-time setting. Also probes trade-offs between Spark and Flink, plus testing strategies and observability.\n\n## Key Concepts\n\n- Runtime data contracts per tenant\n- Per-tenant masking/redaction strategies\n- GDPR deletion and data sovereignty\n- Schema evolution in lakehouse (Iceberg/Parquet)\n- Policy engine governance (e.g., OPA)\n- Spark Structured Streaming vs Flink trade-offs\n- Testing, monitoring, data quality\n\n## Code Example\n\n```javascript\n// Pseudo runtime contract check\nfunction validateRecord(record, contract) {\n  for (const field of Object.keys(contract.fields)) {\n    if (!record.hasOwnProperty(field) && contract.fields[field].required) {\n      throw new Error(`Missing required field ${field}`);\n    }\n  }\n  // redaction example\n  if (contract.mask) {\n    for (const f of contract.mask) {\n      if (record[f]) record[f] = \"REDACTED\";\n    }\n  }\n  return record;\n}\n```\n\n## Follow-up Questions\n\n- How would you test per-tenant masking guarantees in CI/CD?\n- How would GDPR deletion requests propagate through streaming pipelines and downstream consumers?","diagram":"flowchart TD\n  A[Ingest Streams] --> B[Validate Per-Tenant Contracts]\n  B --> C[Mask/Redact PII per Tenant]\n  C --> D[Lakehouse Write (Iceberg)]\n  D --> E[Analytics & ML]\n  E --> F[Observability & Governance]","difficulty":"advanced","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Hugging Face","Instacart","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T17:09:55.729Z","createdAt":"2026-01-22T17:09:55.729Z"},{"id":"q-5823","question":"Design an end-to-end IoT telemetry pipeline for 100k devices emitting JSON with nested sensor readings and optional PII in metadata. Data lands in a lakehouse (Iceberg) with per-device isolation, dynamic schema evolution, and per-record data contracts. Ingest via streaming (Kafka -> Spark/Flink), tolerate late data up to 2 hours, implement privacy masking (redact owner_id, precise_location) before write, and capture per-record lineage. Propose the transformation, upsert strategy (MERGE INTO), testing plan, and monitoring?","answer":"Propose a streaming pipeline using Kafka -> Spark Structured Streaming, write to Iceberg with per-device partitioning, mask PII via UDFs before write, apply schema evolution via Iceberg, buffer late d","explanation":"## Why This Is Asked\n\nTests ability to design end-to-end data contracts with privacy controls and lineage in a lakehouse, plus handling nested JSON, late data, and dynamic schema evolution in a real-world IoT context.\n\n## Key Concepts\n\n- Streaming ingestion (Kafka) with Spark/Flink\n- Iceberg dynamic schema evolution and per-device partitioning\n- Row-level privacy masking before load\n- Per-record lineage via a metadata table\n- MERGE INTO upserts and idempotency\n- Contract tests and data quality gates\n\n## Code Example\n\n```javascript\n// Pseudocode for Iceberg MERGE\nMERGE INTO iceberg_db.telemetry AS t\nUSING staged AS s\nON t.device_id = s.device_id AND t.event_time = s.event_time\nWHEN MATCHED THEN UPDATE SET t.readings = s.readings, t.metadata = s.metadata\nWHEN NOT MATCHED THEN INSERT (device_id, event_time, readings, metadata) VALUES (s.device_id, s.event_time, s.readings, s.metadata)\n```\n\n## Follow-up Questions\n\n- How would you verify lineage correctness and masking in production with synthetic data?\n- How would schema drift in nested readings be detected and handled without downtime?","diagram":null,"difficulty":"intermediate","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Lyft","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T18:04:59.226Z","createdAt":"2026-01-22T18:04:59.226Z"},{"id":"q-5833","question":"Design a minimal end-to-end pipeline for a daily JSON log from a regional partner with fields user_id, session_id, event_type, and a nested session { device, country }. Flatten into a Lakehouse star schema (dim_user, dim_session, dim_device, dim_country, fact_events). Ensure idempotent daily upserts, a per-record data contract, and a unit-test idea for parsing. Include validation rules and basic monitoring considerations?","answer":"Use a deterministic key = hash(user_id||session_id). Parse JSON, flatten nested session into dim_device and dim_country; keep dim_user by user_id. Write to Delta Lake with MERGE on the key for the dim","explanation":"## Why This Is Asked\nTests practical ETL for nested JSON, star-schema mapping, and idempotent upserts at beginner scale. It also checks data contracts and basic testing.\n\n## Key Concepts\n- Flattening nested JSON to star schema\n- Idempotent upserts with MERGE\n- Per-record data contracts and validations\n- Lightweight monitoring ideas (row counts, schema drift)\n\n## Code Example\n```javascript\n// pseudo: parse JSON, flatten, upsert example\n```\n\n## Follow-up Questions\n- How would you test late-arriving records? \n- What basic monitoring metrics would you surface?","diagram":"flowchart TD\n  A[Ingest daily JSON] --> B[Parse & Flatten]\n  B --> C[Dim_User]\n  B --> D[Dim_Session]\n  B --> E[Dim_Device]\n  B --> F[Dim_Country]\n  B --> G[Fact_Events]\n  G --> H[MERGE Upserts]","difficulty":"beginner","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Databricks","LinkedIn","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T18:52:43.264Z","createdAt":"2026-01-22T18:52:43.264Z"},{"id":"q-5972","question":"Design an end-to-end daily ingestion for 3 regional CSV feeds into a lakehouse with a simple star schema. Include per-record contracts, idempotent upserts via MERGE, and per-record lineage. Handle late-arriving rows up to 6 hours. Provide a practical plan using Spark (or Pandas for small data) and outline testing and monitoring?","answer":"Ingest three regional CSV files daily into a staging table, then perform idempotent upserts into a star schema using MERGE operations. Enforce per-record contracts through required schema validation and record_version tracking; compute record_hash for comprehensive lineage tracking.","explanation":"## Why This Is Asked\nThis question evaluates practical implementation of data contracts, idempotent upserts, and lineage tracking in a beginner-friendly batch pipeline. It introduces per-record lineage and simple schema evolution across multiple data feeds, contrasting batch versus streaming architectural considerations.\n\n## Key Concepts\n- Batch ingestion with staging tables\n- Idempotent upserts via MERGE operations\n- Per-record contracts and lineage tracking\n- Late-arrival handling with 6-hour window\n- Simple schema evolution (nullable new fields)\n- Testing and monitoring fundamentals","diagram":null,"difficulty":"beginner","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":null,"companies":["IBM","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T05:02:20.979Z","createdAt":"2026-01-23T02:25:36.001Z"},{"id":"q-6139","question":"Design a beginner-friendly data ingestion pipeline that streams 5,000 JSON events per minute from a mobile app into a lakehouse, transforming into a star schema with idempotent upserts. Add end-to-end observability: propagate correlation IDs via OpenTelemetry, collect per-record latency and error rates, and surface a simple dashboard. Explain how you’d implement this across a producer, a processor (your choice Spark or Flink), and a sink, and how you’d validate contracts and lineage?","answer":"Attach trace context in the producer, use Flink for streaming with idempotent MERGE into Iceberg, validate events against a JSON schema, and write a lineage row per record. Propagate trace IDs through","explanation":"## Why This Is Asked\nThis question probes practical end-to-end ingestion design with observability, data contracts, and lineage at beginner scale. It also prompts engine choice reasoning and test strategies.\n\n## Key Concepts\n- Data contracts and schema validation\n- Idempotent upserts and deduplication\n- End-to-end observability with OpenTelemetry\n- Data lineage and per-record metadata\n- Engine trade-offs (Spark vs Flink) at small scale\n\n## Code Example\n```javascript\n// Example: attach trace context to a message before sending\nconst span = tracer.startSpan(\"ingest\");\nmsg.traceId = span.context().traceId;\n```\n\n## Follow-up Questions\n- How would you validate idempotence with replayed events?\n- How would you test schema evolution and ensure backward compatibility?","diagram":"flowchart TD\n  A[Producer] --> B[Processor (Flink/Spark)]\n  B --> C[Lakehouse (Iceberg)]\n  B --> D[Lineage Store]\n  C --> E[Dashboards / Alerts]","difficulty":"beginner","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Apple","Hashicorp"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T10:38:42.032Z","createdAt":"2026-01-23T10:38:42.032Z"},{"id":"q-6194","question":"Design a multi-tenant streaming ingestion pipeline for 1M events/min from mobile apps with evolving JSON schemas. Each tenant has retention/privacy rules. Build a lakehouse (Iceberg) with per-tenant data contracts, idempotent upserts, and per-tenant masking, plus dynamic tiering to hot storage and cold archival. Include GDPR delete handling (tombstones), testing, and a comparison of Spark vs Flink for processing?","answer":"Use per-tenant contracts at ingest and Iceberg schema evolution keyed by tenant_id. Ingest via Kafka, apply idempotent upserts into the lakehouse, mask PII per tenant, and emit deletion tombstones for","explanation":"## Why This Is Asked\n\nAssesses ability to design scalable, compliant data pipelines that handle multi- tenancy, evolving schemas, deletion requests, and cost‑aware storage.\n\n## Key Concepts\n\n- Multi-tenant data contracts and privacy rules\n- Iceberg schema evolution and partitioning by tenant\n- Idempotent upserts and delete tombstones\n- Dynamic tiering across hot/cold storage\n- GDPR deletes and audit logs\n- Testing: synthetic data, replay, schema drift tests\n- Spark vs Flink trade-offs in streaming pipelines\n\n## Code Example\n\n```scala\n// Pseudo-code: route event to Iceberg upsert with tenant_id\ndef upsertEvent(event: JsonEvent, tenant: String): Unit = {\n  val masked = maskForTenant(event, tenant)\n  iceberg.upsert(masked, tenant)\n  tierManager.maybeMove(masked, tenant)\n}\n```\n\n## Follow-up Questions\n\n- How would you implement tenant-level masking configuration? \n- How would you verify GDPR delete propagation across archives and dashboards?","diagram":null,"difficulty":"advanced","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Google","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T13:37:30.002Z","createdAt":"2026-01-23T13:37:30.002Z"},{"id":"q-6217","question":"Design an end-to-end data-product governance pipeline where 4 data sources publish curated datasets with per-source contracts (schema, data quality gates, and lineage). Ingest into Iceberg with versioned schemas and region isolation. Build a central schema registry, enforce contracts at the edge, and support idempotent upserts with SCD2 history. Include a MERGE example and a contract-failure rollback plan?","answer":"Define a data-product contract per source (schema, data quality gates, lineage). Store contracts in a central registry and pin a catalog version. Validate at ingest, gate updates behind views, and emi","explanation":"## Why This Is Asked\n\nData governance for multiple data sources with contracts ensures safety, compatibility, and faster onboarding of teams at scale.\n\n## Key Concepts\n\n- Data contracts and schema registry\n- Versioned Iceberg schemas and per-source isolation\n- Idempotent upserts and SCD2 history\n- Contract testing and rollback strategies\n\n## Code Example\n\n```javascript\n// Pseudo-contract check: validate record against contract version\nfunction validate(record, contract) {\n  // check required fields\n  for (const f of contract.required) {\n    if (!(f in record)) return false;\n  }\n  // type checks\n  for (const [k, t] of Object.entries(contract.types)) {\n    if (typeof record[k] !== t) return false;\n  }\n  return true;\n}\n```\n\n## Follow-up Questions\n\n- How would you detect contract drift across sources, and what actions would you trigger?\n- How would you handle data already ingested when a contract is updated—rollback, versioning, or retroactive validation?","diagram":null,"difficulty":"intermediate","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Netflix","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T14:52:14.391Z","createdAt":"2026-01-23T14:52:14.391Z"},{"id":"q-6285","question":"Ingest 100k protobuf events/sec from mobile devices across 5 regions. Events are nested and evolve with new types. Design an end-to-end streaming pipeline to Iceberg with per-record contracts, region isolation, and idempotent MERGE upserts; handle late data up to 15 minutes; emit per-event lineage; include basic data-quality gates and schema evolution. Compare Spark Structured Streaming vs Flink and outline testing/monitoring?","answer":"Architect a streaming pipeline: decode protobuf to a columnar format, partition by region in Iceberg, and enforce per-record contracts via a versioned schema registry. Upsert via MERGE for idempotency","explanation":"## Why This Is Asked\n\nTests ability to design a real-time pipeline with evolving schemas, per-record contracts, and region isolation, plus practical trade-offs between Spark and Flink and concrete testing/monitoring plans.\n\n## Key Concepts\n\n- Protobuf schema evolution and versioned registry\n- Iceberg MERGE upserts for idempotency\n- Region isolation and per-record contracts\n- Watermarks and late data handling\n- Per-event lineage and audit logging\n- Data quality gates and contract tests\n\n## Code Example\n\n```sql\nMERGE INTO lakehouse.events AS t\nUSING staged_events AS s\nON t.event_id = s.event_id\nWHEN MATCHED THEN UPDATE SET t.payload = s.payload, t.version = s.version\nWHEN NOT MATCHED THEN INSERT (event_id, region, payload, version) VALUES (s.event_id, s.region, s.payload, s.version)\n```\n\n## Follow-up Questions\n\n- How would you implement a versioned protobuf schema registry in practice?\n- How would you validate lineage continuity across regional partitions during schema evolution?","diagram":"flowchart TD\n  Ingest[Ingest protobuf events] --> Decode[Decode to columnar format]\n  Decode --> Route[Route by region to Iceberg partitions]\n  Route --> Upsert[MERGE upserts with idempotency]\n  Upsert --> Lineage[Lineage/audit log]\n  Upsert --> Quality[Data quality gates]","difficulty":"intermediate","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Discord","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T17:45:08.305Z","createdAt":"2026-01-23T17:45:08.305Z"},{"id":"q-6308","question":"Design a global, multi-source streaming to a lakehouse feature store for ML models used in fraud detection across 5 regions. Data arrives as JSON events with nested fields and optional metrics. Implement per-source data contracts, tombstone support for GDPR deletions, time-to-live (TTL) policies, and late-arriving data up to 3 minutes. Explain idempotent upserts, schema evolution, partitioning, and drift monitoring. Compare Spark Structured Streaming vs Flink for ingestion path and outline testing, observability, and rollback strategy?","answer":"Adopt a lakehouse feature store with per-source schemas in a registry. Ingest with Flink for sub-1s latency and exactly-once semantics; use MERGE-like upserts and tombstones for GDPR deletions; enforc","explanation":"## Why This Is Asked\nThe question probes design for real-time ML feature governance, cross-region data contracts, and GDPR support at scale, plus a pragmatic choice between engines.\n\n## Key Concepts\n- Feature store + schema registry\n- Upserts with tombstones for deletions\n- Event-time processing, late data handling, TTLs\n- Data contracts, per-source RBAC, drift monitoring\n- Ingest choice: Flink vs Spark, observability, rollback\n\n## Code Example\n```javascript\n// Pseudo-SQL for MERGE with tombstone\nMERGE INTO feature_store AS f\nUSING updates AS u\nON f.key = u.key AND f.version = u.version\nWHEN MATCHED AND u.delete_flag = true THEN DELETE\nWHEN NOT MATCHED THEN INSERT (key, version, features...) VALUES (...)\n```\n\n## Follow-up Questions\n- How would you test data-contract violations and GDPR deletions end-to-end?\n- How would you implement rollback and backfill if a schema evolves unexpectedly?","diagram":"flowchart TD\n  A[Sources] --> B[Schema Registry]\n  B --> C[Ingestion Layer]\n  C --> D[Lakehouse Feature Store]\n  D --> E[Serving/Models]","difficulty":"advanced","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Amazon","Google","Hugging Face"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T18:54:03.700Z","createdAt":"2026-01-23T18:54:03.700Z"},{"id":"q-6386","question":"Design a real-time feature store for fraud detection. Ingest event streams (user_id, device_id, event_time, features), compute rolling aggregates (1m, 5m, 24h), and serve using a feature-time clock. Ensure training/serving parity, TTL-based invalidation, and offline/online consistency. Compare Redis vs Pinot for the online store and outline testing strategies (drift, canaries, data-quality checks)?","answer":"Implement a real-time feature store for fraud detection that ingests event streams (user_id, device_id, event_time, features), computes rolling aggregates across multiple time windows (1-minute, 5-minute, 24-hour), and serves features through a feature-time clock mechanism.","explanation":"## Why This Is Asked\nThis question tests the ability to design a real-time feature store architecture while addressing critical concerns such as training/serving parity, TTL-based invalidation, and offline/online consistency. It also evaluates understanding of technology trade-offs between Redis and Pinot for low-latency serving, Apache Iceberg for offline training data, and comprehensive testing strategies including drift detection, canary deployments, and data-quality validation.\n\n## Key Concepts\n- Real-time feature store architecture and design patterns\n- Feature-time clock implementation for ensuring temporal consistency\n- Training/serving parity in machine learning pipelines\n- Online store selection: Redis vs Apache Pinot trade-offs\n- Offline storage with Apache Iceberg for training data\n- TTL-based invalidation and data lifecycle management\n- Testing strategies: drift detection, canary deployments, data-quality checks\n\n## Code Example\n```python\n# Pseudo-code: update rolling features in streaming pipeline\ndef update_rolling_features(event_stream):\n    for event in event_stream:\n        user_id = event['user_id']\n        device_id = event['device_id']\n        event_time = event['event_time']\n        features = event['features']\n        \n        # Compute rolling aggregates for different time windows\n        rolling_1m = compute_rolling_aggregate(user_id, features, window='1m', event_time)\n        rolling_5m = compute_rolling_aggregate(user_id, features, window='5m', event_time)\n        rolling_24h = compute_rolling_aggregate(user_id, features, window='24h', event_time)\n        \n        # Update feature store with feature-time clock\n        feature_store.update_features(\n            entity_id=user_id,\n            features={\n                'rolling_1m': rolling_1m,\n                'rolling_5m': rolling_5m,\n                'rolling_24h': rolling_24h\n            },\n            timestamp=event_time\n        )\n```","diagram":"flowchart TD\n  A[Ingest event streams] --> B[Compute rolling features]\n  B --> C[Online store (Redis/Pinot)]\n  B --> D[Offline store (Iceberg)]\n  C --> E[ML training/serving]\n  D --> E","difficulty":"advanced","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":null,"companies":["DoorDash","Robinhood","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T04:32:18.804Z","createdAt":"2026-01-23T21:49:42.068Z"},{"id":"q-6505","question":"You run a multi-tenant real-time feature store on a lakehouse (Iceberg) used by several ML models. Data sources evolve schemas and models need deterministic joins by (tenant_id, user_id, timestamp). How would you implement feature versioning, per-record contracts, and idempotent upserts, while ensuring offline-online consistency and a plan for drift detection and backfills?","answer":"Maintain a versioned feature registry and Iceberg tables partitioned by tenant. Ingested features are merged with an upsert on (tenant_id, user_id, feature_name, version, event_time_bucket) to ensure ","explanation":"## Why This Is Asked\nAssesses ability to design a multi-tenant ML feature store with evolving schemas, integrity guarantees, and drift/backfill strategies.\n\n## Key Concepts\n- Feature versioning and schema registry\n- Idempotent upserts in Iceberg\n- Deterministic joins across tenants\n- Drift detection and backfills\n\n## Code Example\n```sql\nMERGE INTO iceberg.feature_store t\nUSING staging_features s\nON t.tenant_id = s.tenant_id AND t.user_id = s.user_id AND t.feature_name = s.feature_name AND t.version = s.version\nWHEN MATCHED THEN UPDATE SET t.value = s.value, t.event_time = s.event_time\nWHEN NOT MATCHED THEN INSERT (tenant_id, user_id, feature_name, version, value, event_time) VALUES (...);\n```\n\n## Follow-up Questions\n- How would you test contracts and drift across versions in CI/CD?\n- How would you expose online-offline consistency SLAs to model developers?","diagram":null,"difficulty":"intermediate","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Meta","Oracle","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T05:49:15.675Z","createdAt":"2026-01-24T05:49:15.675Z"},{"id":"q-6549","question":"In a multi-region telemetry pipeline ingesting 40k events/sec from Kafka into an Iceberg lakehouse, enforce per-record contracts, region isolation, and schema evolution with late data up to 8 minutes. Design end-to-end using a streaming engine, idempotent upserts, per-record lineage, a schema registry, and a testing/rollback plan. What would you implement and why?","answer":"Use per-region Kafka topics, a Flink streaming job with exactly-once checkpoints, and Iceberg table upserts for idempotent writes. Centralize per-record contracts via a Schema Registry; encode lineage","explanation":"## Why This Is Asked\n\nEvaluates practical capability to design scalable, region-isolated streaming pipelines with strict data contracts, lineage, and robust handling of late and potentially corrupt data.\n\n## Key Concepts\n\n- Exactly-once streaming (Flink/Kafka)\n- Iceberg upserts and schema evolution\n- Schema Registry and per-record contracts\n- Per-record lineage tracking\n- Late-arrival handling and data-quality gates\n\n## Code Example\n\n```python\ndef validate_event(event, contract):\n    required = contract['required']\n    for f in required:\n        if f not in event or event[f] is None:\n            return False\n    for k, t in contract['types'].items():\n        if k in event and not isinstance(event[k], t):\n            return False\n    return True\n```\n\n## Follow-up Questions\n\n- How would you implement per-record lineage across regional partitions?\n- What are the failure modes and how would you test rollback by time travel?","diagram":null,"difficulty":"intermediate","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Discord","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T07:52:35.532Z","createdAt":"2026-01-24T07:52:35.532Z"},{"id":"q-6658","question":"You're building a global privacy-preserving analytics product. Ingest CRM and order data from three regions (customer_id, region, order_id, product_id, amount, timestamp, consent_flag) into a lakehouse (Delta or Iceberg) with per-record lineage and region isolation. Implement dynamic masking policies that evolve over time, ensure GDPR delete propagation, and enforce near real-time dashboards. Describe end-to-end design, testing strategy, and provide a concrete masking policy example plus a MERGE/delete approach?","answer":"Use Unity Catalog + Delta Lake with region-scoped catalogs; define column-level masking and access policies. Example: mask customer_id by salted hash for non-privileged roles; redact amount for non-fi","explanation":"## Why This Is Asked\nAssesses ability to design privacy-aware, lineage-rich data pipelines with evolving policies and automated deletions.\n\n## Key Concepts\n- Data masking policies and roles\n- Per-record lineage and region isolation\n- GDPR delete propagation\n- Policy evolution and testing\n\n## Code Example\n```sql\n-- Example masking policy (illustrative)\nCREATE MASKING POLICY mask_customer_id AS (cid STRING) RETURNS STRING\n  RETURNS IF current_user() IN ('DATA_SCIENCE') THEN cid ELSE SHA2(CONCAT(cid, 'salt'), 256) END\n```\n\n## Follow-up Questions\n- How would you test masking policy changes without affecting production?\n- How would you handle lineage updates when a GDPR delete is processed after dashboards are materialized?","diagram":"flowchart TD\n  A[Ingest] --> B[Masking & Policies]\n  B --> C[Lakehouse (Delta/Iceberg)]\n  C --> D[Publish Dashboards]\n  D --> E[GDPR Delete Propagation]","difficulty":"intermediate","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Databricks","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T11:52:25.033Z","createdAt":"2026-01-24T11:52:25.033Z"},{"id":"q-6688","question":"Design an end-to-end ingestion for a regional mobile gaming service that emits 25k JSON events per minute. Each event includes mandatory fields event_id, user_id, ts, event_type and an optional attr payload. Build a beginner-friendly pipeline that ingests from Kafka, validates per-record contracts, enforces schema evolution in an Iceberg lakehouse, and performs idempotent upserts by event_id. Include per-record lineage and late-arrival handling (up to 15 minutes)?","answer":"Use Spark Structured Streaming to read Kafka JSON, validate with a lightweight contract (event_id, user_id, ts, event_type required; attrs optional), and MERGE into an Iceberg table on event_id to ups","explanation":"## Why This Is Asked\nTests ability to design a practical ingestion with per-record contracts, idempotent upserts, and lineage at a beginner level.\n\n## Key Concepts\n- JSON schema validation\n- Upserts with Iceberg and MERGE\n- Per-record lineage and provenance\n- Late-arrival handling and watermarks\n- Schema evolution\n\n## Code Example\n```sql\nMERGE INTO iceberg_db.events AS t\nUSING staged_events AS s\nON t.event_id = s.event_id\nWHEN MATCHED THEN UPDATE SET\n  t.user_id = s.user_id,\n  t.ts = s.ts,\n  t.event_type = s.event_type,\n  t.attrs = s.attrs,\n  t.lineage_source = s.lineage_source\nWHEN NOT MATCHED THEN INSERT (event_id, user_id, ts, event_type, attrs, lineage_source)\nVALUES (s.event_id, s.user_id, s.ts, s.event_type, s.attrs, s.lineage_source);\n```\n\n## Follow-up Questions\n- How would you test schema evolution with a new optional field?\n- How would you implement per-record lineage when the source can be multi-region?","diagram":"flowchart TD\n  A[Ingest JSON] --> B[Validate Contract]\n  B --> C[Upsert Iceberg via MERGE]\n  C --> D[Record Lineage]\n  D --> E[Monitoring & Alerts]","difficulty":"beginner","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Apple","Citadel"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T13:38:06.972Z","createdAt":"2026-01-24T13:38:06.972Z"},{"id":"q-6747","question":"In a data-mesh across three BUs, each publishes JSON events to a central lakehouse. Design an end-to-end pipeline that isolates data by BU, enforces per-record contracts via a shared schema registry and policy engine, supports schema evolution, and performs idempotent upserts into Iceberg. Handle late-arriving data (up to 6h) and capture per-record lineage. Compare Spark vs Flink for this workload and propose monitoring?","answer":"Architect BU-isolated Iceberg tables; contracts bound to a central schema registry; enforce masking and retention with a policy engine; stream in Flink (exactly-once) and upsert into Iceberg; support ","explanation":"## Why This Is Asked\\n\\nTests ability to design multi-tenant, governed data pipelines with lineage, contracts, and cross-tool trade-offs.\\n\\n## Key Concepts\\n- Data contracts, schema evolution, Iceberg\\n- Per-record lineage, policy engine for masking/retention\\n- Late-arriving data handling, BU isolation\\n- Spark vs Flink trade-offs\\n\\n## Code Example\\n\\n```javascript\\n// Pseudo-contract test skeleton\\nconst contract = loadContract('BU_A');\\nassert(matchRecordSchema(contract, rec));\\n```\\n\\n## Follow-up Questions\\n- How would you test contract drift over time?\\n- What would you monitor for data quality and lineage completeness?","diagram":null,"difficulty":"intermediate","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Amazon","Anthropic","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T15:40:00.377Z","createdAt":"2026-01-24T15:40:00.377Z"},{"id":"q-6760","question":"Design an end-to-end ingestion and analytics pipeline for 20 TB/day of nested JSON events from 100 clinics across jurisdictions. Each event contains patient data with optional fields. Build a streaming path (Kafka -> Flink) into a lakehouse (Iceberg) with per-tenant data contracts, dynamic schema evolution, and per-country residency. Implement idempotent upserts into a dimensional model, support late events up to 30 minutes, and a field-level masking policy that varies by tenant and region. Compare Spark Structured Streaming vs Flink for this workload and outline testing, monitoring, and data-quality checks?","answer":"Use a registry of per-tenant JSON schemas for validation, enforce exactly-once via Flink, and map events to a star schema in Iceberg with MERGE-based upserts. Route data regionally for residency, and ","explanation":"## Why This Is Asked\n\nAssesses ability to design an end-to-end, multi-tenant streaming pipeline with dynamic schema, privacy controls, and region-based data residency. Also tests choice of engine (Flink vs Spark) and practical testing/monitoring strategies.\n\n## Key Concepts\n\n- Per-tenant JSON schema registry and runtime validation\n- Dynamic schema evolution and compatibility checks in Iceberg\n- Idempotent upserts into a dimensional/star model\n- Region-aware data routing for residency and GDPR-like deletions\n- Field-level masking rules by tenant and jurisdiction\n- Late-arriving events, watermarking, and backfill handling\n- Observability, data quality metrics, and end-to-end testing\n\n## Code Example\n\n```java\n// Pseudocode: validate against tenant schema, then upsert to Iceberg with masking\nDataStream<Event> events = KafkaSource.read(\"clinics-topic\");\nDataStream<Event> validated = events\n  .flatMap(new SchemaValidator(schemaRegistry));\nDataStream<Event> masked = validated\n  .map(e -> maskingEngine.mask(e));\nmasked\n  .keyBy(e -> e.tenantId)\n  .process(new IcebergUpsertProcess());\n```\n\n## Follow-up Questions\n\n- How would you test masking rules across tenants and regions?\n- How do you handle schema drift when two tenants diverge in fields?\n- What monitoring and alerting would you build for cross-region data consistency?","diagram":"flowchart TD\n  A[Kafka Ingest (20 TB/day)] --> B[Flink Streaming]\n  B --> C[Iceberg Lakehouse: Star Schema]\n  B --> D[Policy Engine: PII Masking by Tenant/Region]\n  A --> E[Region Router]\n  E --> C","difficulty":"advanced","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Goldman Sachs","MongoDB","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T16:43:21.759Z","createdAt":"2026-01-24T16:43:21.759Z"},{"id":"q-6922","question":"Ingest a daily JSONL feed containing user records with PII fields (email, phone). Build a beginner-friendly pipeline that redacts PII at ingest using tokenization, stores tokenized values for analytics, and upserts into a lakehouse dimension table while preserving per-record lineage in a metadata store. Ensure schema evolution for new fields and missing values. Outline tests, monitoring, and a simple SQL example for tokenization and lineage?","answer":"Implement a daily Python streaming job that ingests JSONL files containing user records with PII fields, redacts sensitive data (email, phone) using salted hash tokenization, persists both redacted fields and tokens for analytics purposes, performs upserts into a lakehouse dimension table while maintaining per-record lineage in a dedicated metadata store, and supports schema evolution for new fields and missing values.","explanation":"## Why This Is Asked\n\nThis question evaluates the ability to design a practical end-to-end data ingestion pipeline with proper PII handling and lineage tracking at a beginner level, while incorporating schema evolution and comprehensive testing considerations.\n\n## Key Concepts\n\n- PII redaction and tokenization\n- Lakehouse architecture and upserts\n- Per-record lineage tracking\n- Schema evolution handling\n- Testing and monitoring strategies\n\n## Code Example\n\n```python\nimport hashlib\n\ndef tokenize(value, salt=\"random_salt\"):\n    \"\"\"Generate salted hash token for PII values\"\"\"\n    return hashlib.sha256((str(value) + salt).encode()).hexdigest()\n```\n\n## Follow-up Questions\n\n- How would you test tokenization quality and collision risk?\n- How would you handle schema evolution when new PII fields are introduced?\n- What monitoring metrics would you track for pipeline health?\n- How would you ensure data consistency during upsert operations?","diagram":"flowchart TD\n  Ingest[Ingest JSONL] --> Redact[Redact/Tokenize PII]\n  Upsert[Upsert into lakehouse]\n  Lineage[Record lineage to metadata store]\n  Schema[Schema evolution for new fields]","difficulty":"beginner","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Anthropic","Scale Ai","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T05:52:18.025Z","createdAt":"2026-01-24T22:56:16.937Z"},{"id":"q-7011","question":"How would you roll out a backward-compatible schema change in an Iceberg-backed lakehouse used by multiple BI models when adding a new metric field to a widely consumed table, ensuring zero-downtime, data contracts, and rollback options?","answer":"Introduce the new metric as a nullable column with a default; create a compatibility view mapping old schema to new; deploy a canary BI model to read the view first; use Iceberg time travel to compare","explanation":"## Why This Is Asked\nThis question probes practical schema rollout in a lakehouse, balancing compatibility with dashboards and minimal downtime.\n\n## Key Concepts\n- Backward-compatible schema changes and nullability\n- Iceberg schema evolution and compatibility views\n- Canary rollout, data contracts, and rollout safety\n- Time travel, snapshot validation, and rollback plan\n- Impact on BI models and dashboards\n\n## Code Example\n```sql\n-- Add a nullable new metric to the Iceberg table\nALTER TABLE events ADD COLUMN new_metric DECIMAL(10,2) NULL;\n```\n\n## Follow-up Questions\n- How would you validate compatibility across all BI models during the canary phase?\n- What monitoring would confirm a successful rollout and when would you rollback?","diagram":"flowchart TD\n  A[New Schema] --> B[Canary Read]\n  B --> C{All models pass?}\n  C -- Yes --> D[Switch ETLs to new schema]\n  C -- No --> E[Rollback to old snapshot]","difficulty":"beginner","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Airbnb","NVIDIA","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T05:43:26.903Z","createdAt":"2026-01-25T05:43:26.903Z"},{"id":"q-7028","question":"Design a real-time feature store for ML inference at 100k lookups/sec, ingesting 3 streaming sources with evolving schemas. Specify data contracts, TTL-based pruning, feature versioning, and cross-region lineage. Explain data model (online/offline stores), ingestion, schema evolution, drift detection, and monitoring. Compare Flink vs Spark for streaming and outline tests and rollback strategies?","answer":"Use a dual online/offline feature store: online store for sub-10ms lookups, offline Iceberg for batch recomputation. Enforce data contracts via a schema registry; version features and enable TTL pruni","explanation":"## Why This Is Asked\n\nThis question probes real-time feature store design, data contracts, TTL, drift, and cross-region lineage, plus trade-offs between engines and data stores. It tests practical thinking about monitoring, testing, and rollback in production.\n\n## Key Concepts\n\n- Real-time feature store architecture (online vs offline)\n- Data contracts and schema evolution\n- TTL pruning and data retention\n- Drift detection and monitoring\n- Cross-region data governance and lineage\n- Ingestion engines and rollback semantics\n\n## Code Example\n\n```python\n# Pseudo-setup showing contract enforcement and TTL\ndef upsert_feature(store, feature_key, value, ttl_seconds):\n    # write to online store with TTL\n    pass\n```\n\n## Follow-up Questions\n\n- How would you implement TTL pruning in Iceberg efficiently?\n- How would you validate drift and trigger rollback when drift exceeds threshold?","diagram":"flowchart TD\n  S(Source A) --> D[Data Contracts]\n  D --> E(Ingestion)\n  E --> O(Online Store)\n  E --> F(Offline Store Iceberg)\n  O --> M(Model Serve)\n  F --> M\n  M --> R(Drift Feedback)\n  R --> D","difficulty":"advanced","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Hashicorp","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T06:46:02.575Z","createdAt":"2026-01-25T06:46:02.575Z"},{"id":"q-7157","question":"Ingest 2 regional streaming JSON feeds into an Iceberg lakehouse. Enforce per-record contracts: device_id, event_ts, metric non-null; optional fields allowed. Route non-conforming records to a bad_data sink. Support schema evolution for added fields. Use idempotent upserts into a star schema and handle late data up to 2 hours with watermarking. Compare Spark Structured Streaming vs Flink for this use case and outline a minimal test plan?","answer":"Outline a beginner pipeline ingesting 2 regional JSON streams into Iceberg. Enforce per-record contract: device_id, event_ts, metric non-null; optional fields allowed. Route bad records to a separate ","explanation":"## Why This Is Asked\nTests understanding of data contracts, bad-data routing, and lakehouse upserts in a streaming context. It also probes practical tradeoffs between Spark and Flink and basic testing, all at a beginner level.\n\n## Key Concepts\n- Data contracts and validation\n- Bad-data sink routing\n- Iceberg schema evolution\n- Idempotent upserts with MERGE\n- Late data handling with watermarks\n- Spark vs Flink tradeoffs\n- Basic test planning\n\n## Code Example\n```python\ndef is_valid(rec):\n    return rec.get(\"device_id\") is not None and rec.get(\"event_ts\") is not None and rec.get(\"metric\") is not None\n```\n\n## Follow-up Questions\n- How would you implement a basic CI data contract test for sample records?\n- How would you monitor bad_data throughput and adjust thresholds?","diagram":"flowchart TD\n  A[Ingest (Regional Streams)] --> B[Validate per-record contract]\n  B --> C{Valid?}\n  C -- Yes --> D[Write to golden Iceberg tables (STAR)]\n  C -- No --> E[Write to bad_data sink]\n  D --> F[Upsert with MERGE]\n  F --> G[Late data handling with watermark]\n  G --> H[Monitoring]","difficulty":"beginner","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Databricks","Snap","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T11:36:46.853Z","createdAt":"2026-01-25T11:36:46.853Z"},{"id":"q-7242","question":"Design a nightly batch ingestion from three regional CSVs into an Iceberg lakehouse. Enforce per-record contracts: customer_id (string, non-null), event_time (timestamp, non-null), revenue (decimal(12,2), non-null), region (string, non-null); optional: campaign_id, device_type. Implement deterministic masking for customer_id (SHA-256) and redact emails if present. Support schema evolution, idempotent upserts into a star schema, and late data up to 6 hours with a watermark. Briefly compare Spark batch vs Flink batch and outline tests?","answer":"Read three regional CSVs nightly, stage them, then MERGE into the Iceberg lakehouse star schema. Enforce contracts on non-nulls; mask customer_id with SHA-256; redact emails. Use Iceberg schema evolut","explanation":"## Why This Is Asked\nThis question tests practical batch ingestion with data contracts, privacy masking, and schema evolution in Iceberg, plus the trade-offs between Spark and Flink batch. It emphasizes testability and governance considerations.\n\n## Key Concepts\n- Data contracts and nullability\n- Deterministic masking (SHA-256) and data redaction\n- Iceberg schema evolution and MERGE upserts\n- Late-arrival handling with watermarking\n- Spark batch vs Flink batch trade-offs\n\n## Code Example\n```javascript\n// placeholder pseudocode for validation and masking\n```\n\n## Follow-up Questions\n- How would you validate schema evolution against existing partitions?\n- How would you monitor late data percentage and alert on drift?\n","diagram":null,"difficulty":"beginner","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Apple","IBM"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T14:57:34.474Z","createdAt":"2026-01-25T14:57:34.474Z"},{"id":"q-7556","question":"Design a multi-tenant regional data ingestion pipeline for a lakehouse (Iceberg) where 3 regions feed data with evolving schemas. Enforce per-record contracts at ingestion, support late rows up to 12h, isolate by region/tenant, and implement GDPR deletes without full dataset rewrites. Include a region-based change feed to downstream features, discuss testing/monitoring, and compare Spark vs Flink for this workload?","answer":"Enforce per-region data contracts via a schema registry and pre-ingest validation; buffer late rows for up to 12h; use Iceberg MERGE for idempotent upserts and region-tenant deletes to honor GDPR, tri","explanation":"## Why This Is Asked\n\nTests governance, multi-tenant isolation, and end-to-end latency in a realistic data lake setup with evolving schemas.\n\n## Key Concepts\n\n- Contract-first ingestion and region-tenant isolation\n- Iceberg MERGE for upserts and GDPR deletes\n- Change data feed to downstream feature store\n- Late data buffering and data quality checks\n- Spark vs Flink trade-offs for stateful streaming\n\n## Code Example\n\n```sql\nMERGE INTO lakehouse.region_42.events AS t\nUSING staged_events AS s\nON t.event_id = s.event_id\nWHEN MATCHED AND s.delete_flag = TRUE THEN DELETE\nWHEN MATCHED THEN UPDATE SET t.payload = s.payload, t.updated_at = current_timestamp()\nWHEN NOT MATCHED THEN INSERT (event_id, region, tenant_id, payload, event_time) VALUES (s.event_id, s.region, s.tenant_id, s.payload, s.event_time);\n```\n\n```python\n# Contract validation (pseudocode)\ndef validate(record, schema):\n  for field, typ in schema.items():\n    if field not in record or not isinstance(record[field], typ):\n      return False\n  return True\n```\n\n## Follow-up Questions\n\n- How would you test contract evolution without breaking backward compatibility?\n- What metrics and alerts would you deploy for GDPR delete latency and data quality?","diagram":"flowchart TD\n  A[Ingest] --> B[Contract Validation]\n  B --> C{Late Data?}\n  C -->|Yes| D[Buffer & Windowing]\n  C -->|No| E[Load to Iceberg by region/tenant]\n  E --> F[Publish to Feature Store]\n  F --> G[Monitor & lineage]","difficulty":"intermediate","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Citadel","Plaid","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T07:37:22.918Z","createdAt":"2026-01-26T07:37:22.918Z"},{"id":"q-7620","question":"You receive three regional CSV feeds with differing schemas. Design a batch pipeline to harmonize into a single Iceberg lakehouse, enforcing a fixed contract: user_id (string), event_ts (timestamp), event_type (string) required; optional value (double). Normalize to a canonical schema, route nonconforming rows to /bad_data, deduplicate by (user_id, event_ts, event_type), and upsert into Iceberg. Support schema evolution via nullable new fields. Include a minimal test plan?","answer":"Batch pipeline (PySpark + Iceberg) ingesting three regional CSV feeds into a single Iceberg lakehouse. Enforce contract: required user_id (string), event_ts (timestamp), event_type (string); optional ","explanation":"## Why This Is Asked\nFresh angle: handles drift across multiple CSV feeds, enforces a contract, and practices idempotent upserts in a lakehouse.\n\n## Key Concepts\n- Batch ingestion, schema normalization, per-record contracts, dedup/upsert, data quality sinks, Iceberg schema evolution.\n\n## Code Example\n```python\n# PySpark sketch\ncanonical = df.selectExpr(\"cast(user_id as string) as user_id\",\n  \"cast(event_ts as timestamp) as event_ts\",\n  \"cast(event_type as string) as event_type\",\n  \"cast(value as double) as value\")\ngood = canonical.filter(\"user_id is not null and event_ts is not null and event_type is not null\")\nbad = canonical.filter(\"user_id is null or event_ts is null or event_type is null\")\n# upsert example (pseudo)\nspark.sql(\"MERGE INTO lakehouse.table t USING good s ON t.user_id=s.user_id AND t.event_ts=s.event_ts AND t.event_type=s.event_type WHEN MATCHED THEN UPDATE SET * WHEN NOT MATCHED THEN INSERT *\")\n```\n\n## Follow-up Questions\n- How would you test schema evolution behavior? How to validate bad_data routing?\n- What monitoring would you add for data quality and latency?","diagram":null,"difficulty":"beginner","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":null,"companies":["IBM","Meta","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T10:03:13.139Z","createdAt":"2026-01-26T10:03:13.139Z"},{"id":"q-7718","question":"Nightly batch ingestion of 8 regional JSON feeds (~2–5 GB). Each event has record_id, user_id, region, event_ts, event_type, and an optional payload. Design a pipeline to load into an Iceberg lakehouse as a central fact_events table using idempotent upserts on record_id, enforce per-record contracts, route non-conforming rows to region-specific bad_data, and support schema evolution for new fields. Include a concise test plan and a simple row-level lineage approach?","answer":"Use a Spark batch job to read JSON with a defined schema, validate per-record contracts (non-null fields, region in allowed list, event_ts parsable), write to Iceberg via MERGE on record_id for idempo","explanation":"## Why This Is Asked\nThis tests batch data quality, idempotent upserts, and schema evolution—fundamentals for reliable lakehouse pipelines.\n\n## Key Concepts\n- Batch validation of per-record contracts\n- Idempotent MERGE upserts into Iceberg\n- Region-scoped bad_data sinks\n- Schema evolution in Iceberg\n- Row-level lineage and simple observability\n\n## Code Example\n```python\n# PySpark sketch\n# pseudocode illustrating structure\n```\n","diagram":"flowchart TD\n  S[Regional JSON feeds] --> V[Validate per-record contracts]\n  V --> U[Upsert to Iceberg fact_events]\n  V --> B[Bad_data sink per region]\n  U --> L[Row-level lineage]","difficulty":"beginner","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Netflix","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T15:01:14.725Z","createdAt":"2026-01-26T15:01:14.725Z"},{"id":"q-7745","question":"Scenario: Global telemetry ingests JSON events from 200k devices/sec across regions. In addition to a raw event lake, design a versioned feature store on top of Iceberg that materializes simple features (count per device in last 5m, mean value, recency) and keeps per-record lineage to raw events. Requirements: dynamic feature definitions with schema evolution, per-record data contracts, late data handling up to 10 minutes, idempotent upserts, TTL pruning, and per-region data isolation. Include a concrete MERGE example and a high-level monitoring plan; compare Spark Structured Streaming vs Flink for this workload?","answer":"Design a two-tier pipeline: a raw event lake in Iceberg partitioned by region and device, plus a versioned feature store built on Iceberg. Ingest streaming events, compute windowed features (5m device","explanation":"## Why This Is Asked\n\nThis question probes feature-store design, per-region isolation, dynamic schema evolution, and lineage in a high-throughput streaming context. It also evaluates understanding of idempotent writes, late data handling, TTL pruning, and practical trade-offs between Spark and Flink.\n\n## Key Concepts\n\n- Versioned feature definitions and schema evolution\n- Iceberg-backed lakehouse for raw and feature data\n- Per-record data contracts and lineage tracing\n- Late data handling with TTL pruning\n- Exactly-once semantics and state backend trade-offs (Spark vs Flink)\n\n## Code Example\n\n```sql\nMERGE INTO iceberg_schema.feature_store AS f\nUSING (SELECT device_id, feature_name, value, version FROM staged_features) AS s\nON f.device_id = s.device_id AND f.feature_name = s.feature_name AND f.version = s.version\nWHEN MATCHED THEN UPDATE SET value = s.value, last_updated = CURRENT_TIMESTAMP\nWHEN NOT MATCHED THEN INSERT (device_id, feature_name, value, version, last_updated) VALUES (s.device_id, s.feature_name, s.value, s.version, CURRENT_TIMESTAMP);\n```\n\n## Follow-up Questions\n\n- How would you implement per-region TTL and privacy masking in this setup?\n- What monitoring metrics would you surface for feature freshness and lineage integrity?\n","diagram":"flowchart TD\n  A[Ingested Events] --> B[Raw Iceberg Lake]\n  B --> C[Versioned Feature Store]\n  C --> D[Serving Layer/BI]\n  A --> E[Quality &lineage Checks]","difficulty":"intermediate","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Snap","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T15:55:50.010Z","createdAt":"2026-01-26T15:55:50.010Z"},{"id":"q-7907","question":"Design a pipeline that serves as both a data warehouse and an ML feature store. Ingest batch and streaming signals from 60 partners, partition data by region, and store both raw data and computed features in Iceberg. Implement a versioned feature registry, backfill-friendly feature definitions, and drift alerts to detect distribution shifts. Describe parity between batch and streaming features, and provide a concrete feature example: user_average_purchase_by_region. How would you test and monitor this system?","answer":"Implement a versioned feature registry and store both raw data and features in Iceberg, partitioned by region. Use backfill-safe feature definitions and schema-evolution controls, with streaming and b","explanation":"## Why This Is Asked\nTests ability to design production-grade ML data pipelines that unify storage, feature computation, and governance.\n\n## Key Concepts\n- Versioned feature registry; Iceberg raw and feature layers; drift detection; backfill-friendly schema evolution; batch/stream parity.\n\n## Code Example\n```python\n# Pseudo\ndef upsert_features(df, version):\n    df = df.withColumn(\"feature_version\", lit(version))\n    df.writeTo(\"iceberg.db.features\").using(\"iceberg\").append()\n```\n\n## Follow-up Questions\n- How would you test feature drift across regions?\n- How would you roll back a registry version with active streaming jobs?","diagram":null,"difficulty":"intermediate","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Google","LinkedIn","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T22:40:17.908Z","createdAt":"2026-01-26T22:40:17.908Z"},{"id":"q-7958","question":"Design a multi-tenant ML feature store ingesting both batch CSVs and streaming Kafka topics into an Iceberg-based lakehouse. Each tenant has strict data contracts, PII masking, and per-tenant isolation. Implement feature versioning, immutable changelogs, and backfill support while preserving lineage for reproducible model training. How would you architect the end-to-end pipeline and governance?","answer":"Propose per-tenant namespaces in Iceberg, a unified ingestion path (batch + streaming) with an observable changelog, policy-driven PII masking at ingress, and a central feature registry that versions ","explanation":"## Why This Is Asked\nThis question tests ability to design scalable, governance-aware data pipelines for ML features, balancing isolation and reproducibility.\n\n## Key Concepts\n- Multi-tenant isolation in lakehouse\n- Feature versioning and immutable changelogs\n- PII masking and data contracts at ingest\n- Backfill strategies and lineage tracking\n\n## Code Example\n```javascript\n// Pseudo Spark snippet for writing a versioned feature to Iceberg\ndf.write\n  .format('iceberg')\n  .option('table', 'tenantA.features_v1')\n  .mode('append')\n  .save()\n```\n\n## Follow-up Questions\n- How would you test backfill correctness across schema evolution?\n- How do you monitor per-tenant data quality and mask policy violations?","diagram":"flowchart TD\n  Ingest[Ingest Data] --> Mask[PII Masking]\n  Mask --> Validate[Contract Validation]\n  Validate --> Store[Store per-tenant Iceberg]\n  Store --> Registry[Feature Registry & Versioning]\n  Registry --> Backfill[Backfill & Recompute]\n  Store --> Lineage[Lineage Capture]\n  Lineage --> Model[Model Training Reproducibility]","difficulty":"advanced","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Netflix","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T02:42:49.477Z","createdAt":"2026-01-27T02:42:49.477Z"},{"id":"q-885","question":"You operate a multi-tenant SaaS analytics platform ingesting per-tenant event streams from Kafka into Snowflake. Each tenant has different event schemas that can evolve independently. Design a data pipeline to enforce per-tenant data contracts, support late-arriving events, and minimize schema drift while controlling storage costs. Include schema versioning, validation, and deployment safety steps?","answer":"Design a per-tenant schema registry with versioned contracts and a streaming processor that validates each event against its tenant’s current contract, routing to tenant-specific Snowflake partitions ","explanation":"## Why This Is Asked\nTests ability to model per-tenant contracts and schema evolution in a streaming pipeline and to handle late-arriving data without breaking consumers.\n\n## Key Concepts\n- Per-tenant data contracts and a registry\n- Schema evolution policies and compatibility checks\n- Late-arrival handling with watermarking\n- Cost-aware storage isolation per tenant\n\n## Code Example\n```javascript\n// Pseudo-code illustrating per-tenant validation\nfunction validateEvent(event, tenant) {\n  const contract = registry.getContract(tenant, event.version);\n  return contract ? contract.validate(event) : false;\n}\n```\n\n## Follow-up Questions\n- How would you test schema drift detection?\n- How would you roll back a bad schema change without data loss?","diagram":null,"difficulty":"intermediate","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["NVIDIA","Snowflake","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T14:23:12.167Z","createdAt":"2026-01-12T14:23:12.167Z"},{"id":"q-915","question":"You ingest 200k newline-delimited JSON app events daily into S3. Each event has event_id, user_id, timestamp, event_type, and attributes. Design a beginner-friendly pipeline to deduplicate by event_id, hash user_id for privacy, validate required fields, and write Parquet data partitioned by date in a data lake. Address simple schema drift and testing?","answer":"Ingest daily JSON lines, deduplicate by event_id, hash user_id with SHA-256, validate required fields, and write Parquet data partitioned by date in a data lake. Include a simple schema-drift strategy","explanation":"## Why This Is Asked\n\nThis question tests practical basics of building a reliable, beginner-friendly data pipeline: ingestion from S3, deduplication, privacy via hashing, basic schema drift handling, and testing. It avoids company-specific traps while focusing on core ETL behavior.\n\n## Key Concepts\n\n- Ingestion from object storage and newline-delimited JSON\n- Deduplication by stable key event_id\n- Privacy: deterministic hashing of user_id\n- Schema drift: optional fields with defaults\n- Parquet partitioning by date; lightweight validation\n\n## Code Example\n\n```python\n# Pseudo-code sketch for deduplication\nimport json, hashlib\n\ndef process(lines):\n    seen = set()\n    for line in lines:\n        e = json.loads(line)\n        if 'event_id' not in e or 'user_id' not in e or 'timestamp' not in e:\n            continue\n        if e['event_id'] in seen:\n            continue\n        seen.add(e['event_id'])\n        yield {\n            'event_id': e['event_id'],\n            'timestamp': e['timestamp'],\n            'user_id_hashed': hashlib.sha256(e['user_id'].encode()).hexdigest(),\n            'event_type': e.get('event_type'),\n            'attributes': e.get('attributes', {})\n        }\n```\n\n## Follow-up Questions\n\n- How would you test idempotency for reprocessing files?\n- How would you monitor data quality and schema drift over time?","diagram":"flowchart TD\n  A[Source: S3] --> B[Ingest] \n  B --> C[Deduplicate by event_id] \n  C --> D[Transform] \n  D --> E[Partitioned Parquet Lake]","difficulty":"beginner","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","MongoDB","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T15:27:56.228Z","createdAt":"2026-01-12T15:27:56.228Z"},{"id":"q-993","question":"Design a global, multi-tenant data ingestion system for a ride-hailing platform with streams for billing, trips, safety, and promotions. Each tenant defines a data contract; schemas evolve independently; late-arriving events up to 15 minutes must be accepted. Describe architecture using Apache Kafka, Schema Registry (Avro/JSON), an Iceberg/Delta lake sink, and a streaming processor (Flink/Spark). Include data model, schema evolution, backfill handling, testing, and observability?","answer":"Design a multi-tenant ingestion: enforce per-tenant contracts via a central Schema Registry (Avro) with backward/forward compatibility, publish to dedicated Kafka topics per stream, process with Flink","explanation":"## Why This Is Asked\nAssesses ability to build scalable, contract-driven ingestion across tenants with late data, schema evolution, and strong observability.\n\n## Key Concepts\n- Per-tenant data contracts and registry-based schema governance\n- Backward/forward compatibility and evolution strategy\n- Late-arriving data handling with event-time processing and watermarks\n- Exactly-once semantics, idempotent writes, and upserts\n- Observability: lineage, data freshness SLIs, drift alerts\n- Backfill testing and controlled replay plans\n\n## Code Example\n```json\n{\n  \"type\": \"record\",\n  \"name\": \"TripEvent\",\n  \"namespace\": \"com.example\",\n  \"fields\": [\n    {\"name\": \"tenant_id\", \"type\": \"string\"},\n    {\"name\": \"event_time\", \"type\": {\"type\": \"long\", \"logicalType\": \"timestamp-millis\"}},\n    {\"name\": \"payload\", \"type\": {\"type\": \"map\", \"values\": \"string\"}}\n  ]\n}\n```\n\n## Follow-up Questions\n- How would you test schema drift across tenants without affecting throughput?\n- How would you design rollback and backfill workflows for a failed batch?","diagram":null,"difficulty":"advanced","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","DoorDash","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T18:37:56.085Z","createdAt":"2026-01-12T18:37:56.085Z"},{"id":"q-176","question":"How would you design a data pipeline that handles both batch and streaming workloads for real-time analytics?","answer":"I would implement a Lambda architecture that combines both batch and streaming processing. The batch layer would handle historical data processing for accuracy, while the speed layer would process real-time data streams for immediate insights. Both layers would be integrated through a unified serving layer that merges results and provides a comprehensive view.","explanation":"## Why Asked\nThis question evaluates your understanding of modern data architecture patterns and your ability to design systems that handle multiple processing paradigms effectively.\n\n## Key Concepts\nLambda architecture, batch processing, stream processing, data consistency, real-time analytics, fault tolerance, scalability\n\n## Code Example\n```\n// Stream processing example (Apache Flink)\nDataStream<Event> stream = env.addSource(kafkaSource);\nstream.window(TumblingProcessingTimeWindows.of(Time.minutes(5)))\n      .aggregate(new CountAggregate())\n      .addSink(sink);\n```","diagram":"flowchart TD\n    A[Data Source] --> B[Batch Layer]\n    A --> C[Speed Layer]\n    B --> D[Batch View]\n    C --> E[Real-time View]\n    D --> F[Serving Layer]\n    E --> F\n    F --> G[Analytics/Queries]","difficulty":"beginner","tags":["streaming","kafka"],"channel":"data-engineering","subChannel":"streaming","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google","Meta","Netflix","Uber"],"eli5":"Imagine you have two ways to make lemonade at your lemonade stand! One way is making big batches in the morning (that's your batch processing) - you mix everything perfectly and it tastes great. The other way is making fresh cups one by one as friends arrive (that's your streaming) - super fast but maybe not as perfect. The smart trick is having both! You serve the fresh cups right away for instant refreshment, but you also have your perfect batch ready for when someone wants the best-tasting lemonade. Your brain keeps track of both - you know who got fresh cups now and who will get the perfect batch later. That way, everyone gets lemonade exactly when they need it, and you can tell your parents exactly how much you sold today!","relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-29T08:40:26.305Z","createdAt":"2025-12-26 12:51:06"},{"id":"q-222","question":"How would you design a Kafka Streams application to handle exactly-once processing with stateful aggregations while maintaining sub-second latency during peak loads of 100K events/sec?","answer":"Configure EOS_ALPHA with processing.guarantee=exactly_once_v2, use RocksDB state stores with changelog compaction, enable standby replicas, tune num.stream.threads=cores*2, set cache.max.bytes.buffering=10MB, and monitor consumer lag with Prometheus metrics.","explanation":"## Architecture\n**Exactly-once semantics**: EOS_ALPHA with transactional producers ensures atomic state updates and output commits\n**State management**: RocksDB local state + compacted changelog topics for fast recovery\n**Performance tuning**: Optimize thread pool, buffer sizes, and batch processing for sub-second latency\n\n## NFRs & Calculations\n**Throughput**: 100K events/sec ÷ 4 cores = 25K events/thread/sec\n**Latency**: Target <500ms with 100ms batch intervals\n**Storage**: 1GB state store ÷ 10MB cache = 100 cache entries\n**Recovery**: Standby replicas enable <30s failover\n\n## Key Configurations\n```properties\nprocessing.guarantee=exactly_once_v2\nnum.standby.replicas=1\ncache.max.bytes.buffering=10485760\ncommit.interval.ms=100\n```\n\n## Monitoring & Error Handling\n**Metrics**: consumer-lag, stream-latency, state-size\n**Alerts**: lag > 1000 events, latency > 1s\n**Recovery**: Automatic state restoration from changelog with incremental backups","diagram":"flowchart LR\n    A[Producer] --> B[Kafka Topic]\n    B --> C[Kafka Streams App]\n    C --> D[State Store]\n    C --> E[Standby Replica]\n    D --> F[Compact Topic]\n    E --> F\n    C --> G[Output Topic]\n    G --> H[Consumer]\n    I[Traffic Spike] --> C\n    C --> J[Adaptive Processing]\n    J --> K[Scale Out]","difficulty":"advanced","tags":["kafka","flink","kinesis"],"channel":"data-engineering","subChannel":"streaming","sourceUrl":null,"videos":{"longVideo":"https://www.youtube.com/watch?v=Z_gCv4Uum44"},"companies":["Amazon","Confluent","Netflix","Stripe","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-26T16:34:11.088Z","createdAt":"2025-12-26 12:51:07"},{"id":"q-248","question":"How would you implement exactly-once processing in a data pipeline when both source (Kafka) and sink (database) can fail, ensuring no duplicate data or data loss during network partitions and system crashes?","answer":"Implement Kafka transactions with idempotent producers (enable.idempotence=true), use database transaction IDs for deduplication, commit offsets only after successful DB commit, and configure EOS=ALWAYS for exactly-once semantics. Include retry logic with exponential backoff and dead-letter queue handling.","explanation":"## Core Implementation\n\n**Kafka Configuration:**\n```java\nProperties props = new Properties();\nprops.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);\nprops.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, \"pipeline-\" + UUID.randomUUID());\nprops.put(ProducerConfig.ACKS_CONFIG, \"all\");\nprops.put(ProducerConfig.RETRIES_CONFIG, Integer.MAX_VALUE);\n```\n\n**Transaction Pattern:**\n```java\n// Initialize transaction\nproducer.initTransactions();\n\ntry {\n    // Begin Kafka transaction\n    producer.beginTransaction();\n    \n    // Process and send to Kafka\n    ProducerRecord<String, String> record = new ProducerRecord<>(topic, key, value);\n    producer.send(record);\n    \n    // Database operation with transaction ID\n    String txId = UUID.randomUUID().toString();\n    jdbcTemplate.update(\"INSERT INTO data_table VALUES (?, ?, ?)\", \n        id, value, txId);\n    \n    // Commit offset only after DB success\n    producer.sendOffsetsToTransaction(offsets, consumer);\n    producer.commitTransaction();\n    \n} catch (Exception e) {\n    producer.abortTransaction();\n    // Retry logic with exponential backoff\n}\n```\n\n## Failure Recovery Strategies\n\n**Database Deduplication:**\n- Unique constraint on transaction_id column\n- INSERT IGNORE or ON CONFLICT DO NOTHING\n- Periodic cleanup of processed transaction IDs\n\n**Offset Management:**\n- Manual offset commits after successful processing\n- Store offsets in database for consistency\n- Use consumer group coordination for failover\n\n**Error Handling:**\n- Circuit breaker pattern for database failures\n- Dead-letter queue for unprocessable messages\n- Monitoring and alerting for transaction failures\n\n## Real-World Considerations\n\n**Performance Trade-offs:**\n- EOS=ALLS adds ~20% latency overhead\n- Increased memory usage for transaction state\n- Requires careful broker configuration (min.insync.replicas=2)\n\n**Edge Cases:**\n- Network partitions during commit phase\n- Broker leadership changes mid-transaction\n- Database connection pool exhaustion\n\n**Monitoring:**\n- Track transaction abort rates\n- Monitor consumer lag during failures\n- Alert on duplicate detection events","diagram":"graph TD\n    A[Kafka Topic] --> B[Consumer Poll]\n    B --> C[Begin Transaction]\n    C --> D[Process Records]\n    D --> E[Database UPSERT with TxID]\n    E --> F{Success?}\n    F -->|Yes| G[Send Offsets to Transaction]\n    F -->|No| H[Abort Transaction]\n    G --> I[Commit Transaction]\n    H --> J[Retry Processing]\n    I --> K[Next Poll]\n    J --> B","difficulty":"intermediate","tags":["dag","orchestration","scheduling"],"channel":"data-engineering","subChannel":"streaming","sourceUrl":null,"videos":null,"companies":["Amazon","Google","Meta","Microsoft","Netflix","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-26T16:38:30.418Z","createdAt":"2025-12-26 12:51:07"}],"subChannels":["general","streaming"],"companies":["Adobe","Airbnb","Amazon","Anthropic","Apple","Bloomberg","Citadel","Coinbase","Confluent","Databricks","Discord","DoorDash","Goldman Sachs","Google","Hashicorp","Hugging Face","IBM","Instacart","LinkedIn","Lyft","Meta","Microsoft","MongoDB","NVIDIA","Netflix","OpenAI","Oracle","PayPal","Plaid","Robinhood","Salesforce","Scale Ai","Snap","Snowflake","Square","Stripe","Tesla","Twitter","Two Sigma","Uber","Zoom"],"stats":{"total":82,"beginner":29,"intermediate":29,"advanced":24,"newThisWeek":38}}