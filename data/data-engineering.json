{"questions":[{"id":"q-1077","question":"Design a data ingestion and processing pipeline for a global ride-hailing platform that ingests 5 TB/day of operational events from multiple regional Kafka topics and batch feeds. Requirements: idempotent upserts into a table (Iceberg/Delta), handle late-arriving events, schema evolution, and partition pruning by country/date. Compare Flink vs Spark for streaming, and outline testing, monitoring, and data quality checks?","answer":"Propose a hybrid pipeline: streaming with Flink writing to Iceberg, using idempotent upserts on (region, event_id), watermarking for late data, and Iceberg schema evolution; batch reconciliation with ","explanation":"## Why This Is Asked\nThis question probes end-to-end data engineering design, covering multi-source ingest, schema evolution, late data handling, and cost-aware architecture at scale.\n\n## Key Concepts\n- Multi-source ingestion, idempotent upserts, upsert semantics\n- Schema evolution, partition pruning, Iceberg/Delta\n- Late data handling, watermarking, out-of-order events\n- Streaming vs batch trade-offs, observability\n\n## Code Example\n```javascript\n// Example pseudo outline (not executable)\nfunction ingest(){ /* ... */ }\n```\n\n## Follow-up Questions\n- How would you test idempotence across regional sources?\n- What metrics and dashboards would you establish for data quality and SLA.\n","diagram":"flowchart TD\n  A[Regional Kafka Topics] --> B[Flink Streaming]\n  B --> C[Iceberg Table]\n  C --> D[Spark Batch Reconciliation]\n  D --> E[Monitoring & Quality]\n","difficulty":"advanced","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","LinkedIn","Lyft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T21:35:02.996Z","createdAt":"2026-01-12T21:35:02.996Z"},{"id":"q-1140","question":"Given daily 1 GB of web logs in JSON lines stored on S3 with fields user_id, timestamp, path, status, and optional referrer, design a beginner-friendly pipeline (Python or Node.js) that deduplicates by timestamp+user_id+path, validates required fields, normalizes timestamp to UTC, and writes date-partitioned Parquet to a data lake; include basic tests and monitoring?","answer":"Use a small streaming script (Python or Node.js) to read daily JSONL from S3, deduplicate by (timestamp, user_id, path), validate required fields (user_id, timestamp, path, status), coerce timestamp t","explanation":"## Why This Is Asked\nThis question tests practical, beginner-friendly construction of a data pipeline with common tasks: deduplication, validation, timestamp handling, and partitioned Parquet, plus testing and monitoring.\n\n## Key Concepts\n- JSON Lines parsing\n- Idempotent deduplication\n- Data quality checks (required fields, types)\n- Timestamp normalization to UTC\n- Parquet writing and date partitioning\n- Basic testing and observability\n\n## Code Example\n```javascript\nconst fs = require('fs');\nconst readline = require('readline');\n(async () => {\n  const rl = readline.createInterface({ input: fs.createReadStream('logs.jsonl'), crlfDelay: Infinity });\n  const seen = new Set();\n  for await (const line of rl) {\n    const obj = JSON.parse(line);\n    if (!obj.user_id || !obj.timestamp || !obj.path || !obj.status) continue;\n    const key = `${obj.timestamp}|${obj.user_id}|${obj.path}`;\n    if (seen.has(key)) continue;\n    seen.add(key);\n    // normalization and persistence would occur here\n  }\n})();\n```\n\n## Follow-up Questions\n- How would you scale this for multi-region logs?\n- What metrics and alerts would you add for production reliability?","diagram":"flowchart TD\n  Ingest[Ingest daily JSONL from S3] --> Dedup[Dedupe by timestamp+user_id+path]\n  Dedup --> Validate[Validate required fields]\n  Validate --> Normalize[Normalize timestamp to UTC]\n  Normalize --> Persist[Persist Parquet to date-partitioned path]\n  Persist --> Test[Basic tests and monitoring]","difficulty":"beginner","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","NVIDIA","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T01:27:30.496Z","createdAt":"2026-01-13T01:27:30.496Z"},{"id":"q-1272","question":"Design a data pipeline to ingest 2M GPU telemetry events per minute from a global fleet of AI training clusters into a data lake and a feature store. Events include host_id, region, timestamp, metric_type, and value. Requirements: immutable raw Parquet storage partitioned by region/hour; near-real-time metrics and anomaly alerts (1–2 minute latency) via a streaming engine; idempotent upserts into a feature store; schema evolution handling; late-arriving data; cost-aware storage/compute; monitoring and tests; compare Spark vs Flink for streaming components?","answer":"Architect a pipeline ingesting ~2M GPU telemetry events per minute from global clusters. Use Kafka -> Flink (or Spark Structured Streaming) to compute 1–2 minute rolling metrics and an anomaly score; ","explanation":"## Why This Is Asked\nThis question probes end-to-end data pipeline design for a hardware/AI-ops context, focusing on high-throughput ingestion, real-time analytics, schema evolution, late data, and cost—areas Apple/NVIDIA encounter in telemetry, MLops, and GPU monitoring.\n\n## Key Concepts\n- High-throughput streaming, watermarking, late data\n- Immutable raw storage and partitioning\n- Idempotent upserts in a feature store (Iceberg/Delta)\n- Schema evolution and data quality checks\n- Cost optimization (tiered storage, compute)\n\n## Code Example\n```javascript\n// Pseudo-code for anomaly score snippet\nfunction score(series) {\n  const mean = mean(series)\n  const std = stdDev(series)\n  return (series[-1] - mean) / std\n}\n```\n\n## Follow-up Questions\n- How would you test schema evolution without downtime?\n- How would you measure latency and data quality in production?","diagram":"flowchart TD\n  Kafka[(Kafka)]\n  Stream[(Streaming Engine)]\n  Raw[(Raw Parquet Lake)]\n  Features[(Feature Store)]\n  Alerts[(Alerts)]\n  Late[(Late Data)]\n  Kafka --> Stream\n  Stream --> Raw\n  Stream --> Features\n  Raw --> Features\n  Features --> Alerts\n  Late -.-> Stream","difficulty":"intermediate","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T07:38:05.936Z","createdAt":"2026-01-13T07:38:05.936Z"},{"id":"q-1498","question":"Design a global data ingestion and governance pipeline for a real-time ad-tech platform that processes 200k events/sec from 3 cloud regions into a centralized lakehouse. Each event has event_id, tenant_id, timestamp, event_type, and payload. Requirements: enforce data contracts via a central registry (schema + compatibility rules), support schema evolution with automatic catalog updates, and ensure multi-tenant data isolation and access control. Implement partitioning by tenant/date, handle late-arriving data within a 1–2 minute SLA, ensure data lineage and quality checks, and provide rollback semantics for contracts. Compare Iceberg vs Delta as the storage layer, outline testing/monitoring, and describe concrete example schemas and contract definitions. Include how you'd validate end-to-end?","answer":"I would implement a contract-first pipeline using a central schema registry (Avro/JSON), publish backward/forward-compatible schemas, and isolate data per tenant in Lakehouse partitions tenant/date. I","explanation":"## Why This Is Asked\nTests ability to design end-to-end, contract-driven lakehouse pipelines with multi-tenant governance, schema evolution, and robust observability.\n\n## Key Concepts\n- Data contracts and registry (Avro/JSON schemas, compatibility rules)\n- Multi-tenant isolation and fine-grained access control\n- Schema evolution with catalog updates and versioning\n- Late-arriving data handling and watermarking\n- Data lineage, quality checks, and rollback semantics\n- Iceberg vs Delta trade-offs at scale\n\n## Code Example\n```json\n{\n  \"type\": \"record\",\n  \"name\": \"AdEvent\",\n  \"namespace\": \"com.accel.adtech\",\n  \"fields\": [\n    {\"name\": \"event_id\", \"type\": \"string\"},\n    {\"name\": \"tenant_id\", \"type\": \"string\"},\n    {\"name\": \"timestamp\", \"type\": {\"type\": \"long\", \"logicalType\": \"timestamp-millis\"}},\n    {\"name\": \"event_type\", \"type\": \"string\"},\n    {\"name\": \"payload\", \"type\": \"string\"}\n  ]\n}\n```\n\n## Follow-up Questions\n- How would you validate a new schema version in canary before rollout?\n- How would you enforce per-tenant access to the lakehouse while preserving analytics flexibility?","diagram":"flowchart TD\n  A[Raw events] --> B[Schema Registry]\n  B --> C[Catalog (Iceberg/Delta)]\n  C --> D[Tenant/date partitions]\n  D --> E[Access / BI / ML]","difficulty":"advanced","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Scale Ai","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T19:32:55.649Z","createdAt":"2026-01-13T19:32:55.650Z"},{"id":"q-1692","question":"In a social app generating 100 GB/day of newline-delimited JSON events across 3 regions stored in S3, design a beginner-friendly batch pipeline that validates fields (event_id, user_id, timestamp, event_type), derives date, deduplicates by event_id, hashes user_id for privacy, and writes Parquet partitioned by date to a data lake. Compute a daily data-quality score (0-1) based on missing/invalid fields and store it in a metadata table. Outline tooling, testing, and monitoring?","answer":"Use a Spark batch job that reads 100 GB/day of newline-delimited JSON from S3, validates fields (event_id, user_id, timestamp, event_type), derives date, deduplicates by event_id, hashes user_id with ","explanation":"## Why This Is Asked\nAssesses ability to build an end-to-end batch pipeline with data validation, deduplication, privacy, and basic quality metrics. It also touches data lake organization and basic monitoring.\n\n## Key Concepts\n- Batch processing with Spark\n- Schema validation and deduplication\n- Privacy via sha-256 hashing\n- Parquet partitioning by date\n- Data-quality scoring and metadata tracking\n\n## Code Example\n```javascript\n# Pseudo-PySpark outline\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import sha2, col, to_date\nspark = SparkSession.builder.getOrCreate()\ndf = spark.read.json(\"s3://bucket/events/2025-*/\")\ndf = df.filter(\"event_id IS NOT NULL AND user_id IS NOT NULL\")\ndf = df.dropDuplicates([\"event_id\"])\ndf = df.withColumn(\"user_id_hash\", sha2(col(\"user_id\"), 256))\ndf = df.withColumn(\"date\", to_date(col(\"timestamp\")))\ndf.write.partitionBy(\"date\").parquet(\"s3://bucket/processed/\")\n```\n\n## Follow-up Questions\n- How would you adjust for late-arriving data or schema drift?\n- What tests would you add to validate deduplication and quality scoring?","diagram":"flowchart TD\n  A[Input data] --> B[Validation]\n  B --> C[Deduplicate by event_id]\n  C --> D[Hash user_id]\n  D --> E[Write Parquet partitioned by date]\n  E --> F[Daily quality score in metadata table]","difficulty":"beginner","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","PayPal","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T07:01:59.981Z","createdAt":"2026-01-14T07:01:59.982Z"},{"id":"q-1701","question":"Ingest 3 TB/day of regional event data from Kafka (EU/US/APAC) with user_id, session_id, event_type, timestamp, and attributes. Design a privacy-first analytics pipeline: per-region salt pseudonymization of user_id before cross-region joins, idempotent upserts into Apache Iceberg, event-time processing with 15-minute tumbling windows and 1-hour late data, and immutable audits and data contracts. Compare Spark Structured Streaming vs Flink for the streaming layer, and specify GDPR masking after aggregation?","answer":"Per-region salts for user_id, then SHA-256 hash to pseudonymize before cross-region joins; write idempotent upserts to Iceberg; use 15-minute tumbling windows with 1-hour allowed lateness; maintain im","explanation":"## Why This Is Asked\n\nThis question tests privacy-first, scalable analytics across regions with data contracts.\n\n## Key Concepts\n\n- Per-region pseudonymization\n- Iceberg upserts\n- Event-time windows and lateness handling\n- Immutable audits and data contracts\n- Spark vs Flink trade-offs for streaming\n\n## Code Example\n\n```javascript\n// Region-aware salt hash (Node.js)\nconst crypto = require('crypto');\nfunction saltedHash(region, id, salts){\n  const salt = salts[region] ?? salts.default;\n  return crypto.createHash('sha256').update(id + salt).digest('hex');\n}\n```\n\n## Follow-up Questions\n\n- How would you test idempotency and audit coverage?\n- How would you handle a new region with a different privacy policy?","diagram":"flowchart TD\n  Ingest[Ingest regional Kafka topics] --> Mask[Per-region salt masking of user_id]\n  Mask --> Upsert[Iceberg upserts in lakehouse]\n  Upsert --> Audit[Audit logs & data contracts]\n  Audit --> Monitor[Monitoring & testing]","difficulty":"advanced","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Lyft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T07:37:37.148Z","createdAt":"2026-01-14T07:37:37.149Z"},{"id":"q-1928","question":"Design a cost-aware real-time ingestion pipeline that processes 50 TB/day of clickstream data from multiple sources (Kafka topics and batch feeds) into a lakehouse. Ensure idempotent upserts into an Iceberg/Delta table, handle late-arriving events, and support schema evolution with partition pruning by date and region. Compare Spark Structured Streaming vs Flink for the path and outline data quality checks, testing, and monitoring strategies?","answer":"Design a pipeline ingesting 50 TB/day of clickstream data from Kafka topics and batch feeds into a lakehouse. Use an idempotent upsert into Iceberg/Delta, with watermarking for late events, and schema","explanation":"## Why This Is Asked\nTests ability to design scalable, cost-aware streaming pipelines with exactly-once semantics, late-arriving data handling, and schema evolution in a lakehouse.\n\n## Key Concepts\n- Ingestion from heterogeneous sources and batch feeds\n- Idempotent upserts into Iceberg/Delta\n- Watermarking and late-arriving data handling\n- Schema evolution and registry integration\n- Partition pruning by date/region; cost-aware resource planning\n- Testing, monitoring, and data quality checks\n- Trade-offs Spark vs Flink\n\n## Code Example\n```javascript\n// Pseudo: upsert function shape for events\nfunction upsertEvent(event){\n  // compute dedup key, apply to Iceberg/Delta table with primary key\n  // handle schema evolution via registry\n}\n```\n\n## Follow-up Questions\n- How would you implement exactly-once guarantees across Kafka and batch feeds?\n- How would you validate data quality and detect drift in this setup?\n- What monitoring metrics and SLAs would you track for latency and data freshness?","diagram":"flowchart TD\n  A[Sources: Kafka + Batch] --> B[Ingestion]\n  B --> C[Processing: watermarking & upsert]\n  C --> D[Lakehouse: Iceberg/Delta]\n  D --> E[Monitoring & Quality Checks]\n","difficulty":"intermediate","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["NVIDIA","Robinhood","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T17:43:26.111Z","createdAt":"2026-01-14T17:43:26.111Z"},{"id":"q-1965","question":"Design a GDPR/CCPA-compliant, multi-source data pipeline for an e-commerce analytics platform. Ingest CDC from OLTP databases plus batch feeds, preserve raw data immutably, and enable per-tenant data isolation. Implement lineage via a metadata catalog, apply privacy techniques (PII redaction and differential privacy for aggregates), and support schema evolution. Compare Flink vs Spark for streaming, outline tests, monitoring, and cost implications across regions?","answer":"Ingest CDC from OLTP and batch feeds into an immutable lake with per-tenant isolation and a metadata catalog for lineage. Redact PII and implement differential privacy for aggregates; store raw Parque","explanation":"## Why This Is Asked\nTests ability to design privacy-conscious, governance-aware pipelines across regions with real-world constraints.\n\n## Key Concepts\n- GDPR/CCPA data handling\n- CDC and batch ingestion\n- Immutable storage and Iceberg/Delta\n- Metadata catalogs and lineage\n- Data masking and differential privacy\n- Per-tenant isolation and cost-aware regional replication\n- Flink vs Spark for streaming\n\n## Code Example\n```javascript\n// pseudo-code sketch for DP on aggregates\n```\n\n## Follow-up Questions\n- How would you test schema evolution compatibility across regions?\n- What failure modes affect privacy and how would you mitigate?\n","diagram":null,"difficulty":"advanced","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","LinkedIn","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T18:59:13.741Z","createdAt":"2026-01-14T18:59:13.742Z"},{"id":"q-1971","question":"You run a product analytics pipeline for a global iOS/Android app (telemetry: user_id, device_id, ts, event, properties). Data must be ingested from a MongoDB Atlas source into a lakehouse on S3 using Apache Iceberg. Design an end-to-end pipeline that supports idempotent upserts, late-arriving events, and schema evolution, while enforcing PII masking and GDPR data deletion requests. Compare using Flink vs Spark for streaming, outline testing, monitoring, and data-quality checks?","answer":"Design: Ingest MongoDB Atlas via Change Streams into a durable sink (Kafka). Use a streaming engine (Flink or Spark Structured Streaming) to materialize an Iceberg table on S3 with upserts keyed by (u","explanation":"## Why This Is Asked\nThis question probes practical data engineering design for a privacy-conscious, scalable lakehouse pipeline that can handle cross-platform ingestion from MongoDB Atlas, with robust correctness (idempotence, late data, schema evolution) and trade-offs between Flink and Spark. It also touches testing, monitoring, and data quality.\n\n## Key Concepts\n- Change Data Capture from MongoDB Atlas (Change Streams)\n- Upserts with Iceberg on S3\n- Watermarks and lateness handling\n- Schema evolution in Iceberg and compatibility\n- PII masking and GDPR deletions in streaming\n- Flink vs Spark trade-offs; testing and monitoring\n\n## Code Example\n```javascript\n// Pseudocode outline for CDC -> Kafka -> Iceberg\n```\n\n## Follow-up Questions\n- How would you implement GDPR delete propagation across components?\n- How would you validate idempotence and late-arrival correctness?","diagram":"flowchart TD\n  A[MongoDB Atlas] --> B[Change Streams]\n  B --> C[Kafka]\n  C --> D[Flink/Spark]\n  D --> E[Iceberg on S3]\n  E --> F[Analytics/BI]","difficulty":"intermediate","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T19:27:27.822Z","createdAt":"2026-01-14T19:27:27.825Z"},{"id":"q-2272","question":"Design a global telemetry ingest pipeline for 8 regional Kafka clusters, totaling 20 TB/day of JSON events. Build an end-to-end flow into a lakehouse using Iceberg, ensuring idempotent upserts, late-arriving events, and schema evolution for nested JSON; enforce per-country data sovereignty and GDPR deletion requests. Compare Spark Structured Streaming vs Flink, and outline testing, monitoring, and data-quality checks?","answer":"Design a regional-first ingest into Iceberg lakehouses partitioned by country/date, using exactly-once streaming (Flink preferred; Spark for batch). Achieve idempotent upserts via MERGE INTO, support ","explanation":"## Why This Is Asked\nThis question probes cross-region data ingestion, schema evolution for nested data, and regulatory compliance at scale.\n\n## Key Concepts\n- Exactly-once streaming across multi-region sources\n- Iceberg MERGE Upserts and schema evolution\n- Data sovereignty, GDPR deletions, field masking\n- lineage, monitoring, data quality checks\n- Flink vs Spark trade-offs at scale\n\n## Code Example\n```sql\nMERGE INTO lake.country_data AS t\nUSING staged AS s\nON t.id = s.id\nWHEN MATCHED THEN UPDATE SET t.data = s.data\nWHEN NOT MATCHED THEN INSERT (id, data) VALUES (s.id, s.data)\n```\n\n## Follow-up Questions\n- How would you test late-arriving data and out-of-order events?\n- How to enforce per-country sovereignty in partitioning and retention?\n- How would GDPR deletion requests be propagated through the pipeline and sinks?","diagram":"flowchart TD\n  S[8 regional Kafka clusters] --> I[Ingestion Layer]\n  I --> P[Streaming Processor (Flink)]\n  P --> L[Iceberg Lakehouse (country/date partitions)]\n  L --> Governance[Governance: lineage, privacy, GDPR tombstones]","difficulty":"advanced","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T09:54:51.819Z","createdAt":"2026-01-15T09:54:51.819Z"},{"id":"q-2433","question":"Design a multi-tenant data ingestion pipeline that streams 40 TB/day of event data from dozens of publishers, each with its own schema and retention policy. Explain how you enforce per-tenant data contracts, support dynamic schema evolution, guarantee idempotent upserts into a lakehouse, handle late-arriving events and GDPR delete requests, and implement cross-region replication with cost controls. Compare Spark Structured Streaming vs Flink for this workload and outline testing and monitoring?","answer":"Implement a per-tenant schema registry and data contracts; stream 40 TB/day via Flink (exactly-once) or Spark, upsert into Iceberg partitioned by tenant/date; manage late events with watermarks and id","explanation":"## Why This Is Asked\nThis question probes multi-tenant ingestion, dynamic schemas, and governance at scale.\n\n## Key Concepts\n- Per-tenant data contracts and schema registry\n- Exactly-once streaming with idempotent upserts into Iceberg\n- Late-arriving events, watermarks, tombstones for deletes\n- Cross-region replication, cost controls, data lineage\n- Spark vs Flink trade-offs (backpressure, CDC, state management)\n\n## Code Example\n```javascript\n// Placeholder: pseudo-implementation sketch\nfunction validate(record, contract) { return schema.matches(record); }\n```\n\n## Follow-up Questions\n- How would you implement per-tenant quotas and isolation?\n- How would you test schema evolution without breaking tenants?","diagram":null,"difficulty":"advanced","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T17:51:08.942Z","createdAt":"2026-01-15T17:51:08.942Z"},{"id":"q-2460","question":"Design an end-to-end data pipeline for a multi-tenant ride-hailing platform where city-level feature stores power online fraud scoring and offline model training. Ingest 1) trip events, 2) driver status, 3) external pricing data; ensure per-city isolation, schema evolution, and GDPR deletion. Compare Spark Structured Streaming vs Flink for streaming, outline tests and monitoring?","answer":"Ingest via Kafka into a Spark Structured Streaming job or Flink, write features to an Iceberg-backed feature store partitioned by city. Provide online features in Redis for low latency; offline featur","explanation":"## Why This Is Asked\nAssesses ability to design multi-tenant pipelines, real-time features, and governance across regions.\n\n## Key Concepts\n- Multi-tenant feature store with city partitioning\n- Real-time vs offline feature separation\n- Schema evolution and data quality drift detection\n- GDPR delete propagation and data sovereignty\n- Testing with synthetic data and end-to-end pipelines\n\n## Code Example\n```python\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\n# Read from Kafka\ndf = spark.readStream.format(\"kafka\").option(\"kafka.bootstrap.servers\",\"kafka:9092\").option(\"subscribe\",\"trip_events\").load()\n# Simple feature extraction\nfeatures = df.selectExpr(\"CAST(value AS STRING) as json\").select(\"json\").where(\"json IS NOT NULL\")\n# Write to Iceberg feature store\nfeatures.writeStream.format(\"iceberg\").option(\"table\",\"feature_store.cityA.trips_features\").start()\n```\n\n## Follow-up Questions\n- How would you implement drift detection thresholds for features?\n- How to validate GDPR deletion propagation across online/offline stores?\n- How would you scale to thousands of cities while preserving isolation?\n- What metrics and alerts would you include for data quality and latency?","diagram":"flowchart TD\n  A[Ingest] --> B[Feature Store]\n  B --> C[Online Store]\n  B --> D[Offline Store]\n  C --> E[Serving Layer]\n  D --> F[Model Training]","difficulty":"intermediate","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","Lyft","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T19:02:31.364Z","createdAt":"2026-01-15T19:02:31.364Z"},{"id":"q-2564","question":"You're building a cross-region analytics platform ingesting 200M events/day into a lakehouse. Design a data-contract–driven pipeline with a central schema registry enforcing compatibility, add automated data quality, lineage, and privacy masking for PII, and ensure BI dashboards and model training never see raw sensitive fields. Compare Spark vs Flink for streaming governance and outline testing and monitoring?","answer":"Implement a data-contract-driven architecture with a centralized schema registry enforcing backward and forward compatibility. Ingest events using Flink for near real-time processing into a lakehouse, publishing masked views for BI and ML while maintaining strict access controls. Deploy automated data quality checks, comprehensive lineage tracking, and PII masking at the ingestion layer. Leverage Spark for batch transformations and complex analytics, while utilizing Flink for streaming governance with lower latency requirements. Establish robust testing frameworks including schema validation, data quality assertions, and end-to-end pipeline monitoring across all regions.","explanation":"## Why This Is Asked\n\nThis question assesses practical expertise in data governance, cross-region pipeline architecture, and technology selection. It evaluates a candidate's ability to design data contracts, enforce privacy controls, and provide observable lineage across streaming and batch workloads while balancing Spark vs Flink trade-offs.\n\n## Key Concepts\n\n- Data contracts and schema registry for compatibility enforcement\n- Privacy masking and access controls for PII protection\n- Data lineage and metadata catalog integration\n- Streaming (Flink) vs batch (Spark) processing considerations","diagram":null,"difficulty":"intermediate","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Bloomberg","Instacart"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T05:22:19.374Z","createdAt":"2026-01-15T22:53:48.446Z"},{"id":"q-457","question":"You need to process 10GB of CSV files daily and load them into a PostgreSQL database. The files contain user activity logs with timestamps, user IDs, and event types. How would you design an efficient ETL pipeline using Python?","answer":"Use pandas with chunking for memory efficiency: `pd.read_csv('file.csv', chunksize=10000)`. Process each chunk, validate data types, convert timestamps with `pd.to_datetime()`, and use `psycopg2.extras.execute_batch()` for bulk inserts.","explanation":"## Key Components\n\n- **Memory Management**: Chunk large files to avoid memory issues\n- **Data Validation**: Check for missing values, correct data types\n- **Bulk Operations**: Use batch inserts instead of row-by-row\n- **Error Handling**: Log failed records for retry\n\n## Implementation Strategy\n\n```python\nimport pandas as pd\nimport psycopg2\nfrom psycopg2.extras import execute_batch\n\ndef process_csv_chunk(chunk):\n    # Clean and validate data\n    chunk['timestamp'] = pd.to_datetime(chunk['timestamp'])\n    chunk.dropna(inplace=True)\n    return chunk\n\ndef load_to_db(chunks):\n    conn = psycopg2.connect(database_url)\n    cursor = conn.cursor()\n    \n    for chunk in chunks:\n        processed_chunk = process_csv_chunk(chunk)\n        execute_batch(cursor, INSERT_QUERY, processed_chunk.values.tolist())\n    \n    conn.commit()\n    cursor.close()\n    conn.close()\n```","diagram":"flowchart TD\n  A[CSV Files] --> B[Chunk Processing]\n  B --> C[Data Validation]\n  C --> D[Type Conversion]\n  D --> E[Batch Insert]\n  E --> F[PostgreSQL]\n  B --> G[Error Logging]\n  G --> H[Retry Queue]","difficulty":"beginner","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Discord","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":["etl","chunking","pandas","memory efficiency","data validation","psycopg2"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2026-01-09T08:56:39.590Z","createdAt":"2025-12-26 12:51:05"},{"id":"q-488","question":"You're building a real-time analytics pipeline for a food delivery app. How would you design a data pipeline to process 1M events/day with 5-minute latency, considering data quality, schema evolution, and cost optimization?","answer":"Use Kafka + Flink for stream processing with exactly-once semantics. Implement schema registry for evolution, use Debezium CDC for change capture. Store raw events in S3, processed data in Redshift. Use schema validation, data quality checks, and dead letter queues for reliability. Optimize costs with spot instances, S3 Intelligent Tiering, and Redshift RA3 nodes. Monitor with CloudWatch metrics and data freshness alerts.","explanation":"## Architecture\n- **Ingestion**: Apache Kafka with 3 partitions, replication factor 3\n- **Processing**: Apache Flink for windowed aggregations and joins with exactly-once semantics\n- **Storage**: Raw events in S3 (Parquet format), processed data in Redshift\n\n## Data Quality & Schema Evolution\n- Schema validation using Confluent Schema Registry\n- Data quality checks with Great Expectations\n- Dead letter queue for failed events\n- Debezium CDC for reliable change data capture\n\n## Cost Optimization\n- Spot instances for Flink cluster\n- S3 Intelligent Tiering for raw data\n- Redshift RA3 nodes for compute-storage separation\n\n## Monitoring\n- CloudWatch metrics for pipeline health\n- Data freshness alerts","diagram":"flowchart TD\n  A[Mobile Events] --> B[Kafka Broker]\n  B --> C[Flink Processing]\n  C --> D[Data Quality Check]\n  D --> E[S3 Raw Storage]\n  D --> F[Redshift Analytics]\n  F --> G[Bi Dashboard]\n  C --> H[Dead Letter Queue]","difficulty":"intermediate","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Google","Instacart"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-08T11:58:29.130Z","createdAt":"2025-12-26 12:51:06"},{"id":"q-518","question":"You have a 10GB CSV file with user activity logs that needs to be processed daily. The file contains user_id, timestamp, action_type, and metadata. How would you design a data pipeline to efficiently process this file and load it into a data warehouse?","answer":"Use a distributed processing framework like Apache Spark or AWS Glue. Split the CSV into partitions, process in parallel, apply schema validation and data cleaning, then load into the warehouse using bulk insert operations.","explanation":"## Key Considerations\n- **File Size**: 10GB requires distributed processing for efficient handling\n- **Schema**: Define proper data types and constraints to ensure data quality\n- **Performance**: Implement partitioning and parallel processing for scalability\n\n## Pipeline Architecture\n- **Ingestion**: Store in S3 or similar object storage for scalable access\n- **Processing**: Use Spark/PySpark for distributed computation across multiple nodes\n- **Transformation**: Apply data cleaning, validation, and enrichment logic\n- **Loading**: Perform bulk insert operations to the warehouse (Snowflake, BigQuery)\n\n## Implementation Steps\n- Read CSV with proper schema definition to optimize parsing\n- Handle malformed records with comprehensive error logging\n- Apply business rules and data quality checks\n- Load processed data into warehouse using efficient bulk loading methods","diagram":"flowchart TD\n  A[10GB CSV] --> B[Spark Cluster]\n  B --> C[Data Validation]\n  C --> D[Transformation]\n  D --> E[Data Warehouse]\n  C --> F[Error Logging]","difficulty":"beginner","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Airbnb","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":["apache spark","aws glue","distributed processing","data warehouse","schema validation","data cleaning"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2026-01-08T11:42:43.700Z","createdAt":"2025-12-26 12:51:06"},{"id":"q-571","question":"How would you design a data pipeline to process 1M ride events per minute from Uber's real-time streaming system?","answer":"Design a data pipeline using Apache Kafka to ingest 1M ride events per minute with partitioning by ride_id, Apache Flink for real-time stream processing with windowed aggregations, and write optimized Parquet files to S3 partitioned by date and hour for efficient querying.","explanation":"## Architecture\n- **Ingestion**: Kafka with 3x replication, partitioned by geographic region\n- **Processing**: Flink with exactly-once semantics, 1-minute tumbling windows\n- **Storage**: S3 with Parquet format, compressed with Snappy\n\n## Key Considerations\n- **Scalability**: Auto-scale consumer groups based on lag metrics\n- **Fault tolerance**: Checkpointing to HDFS every 30 seconds\n- **Data quality**: Schema validation and duplicate detection\n\n## Code Example\n```python\nfrom pyflink.datastream import StreamExecutionEnvironment\nfrom pyflink.table import StreamTableEnvironment\nfrom pyflink.datastream.connectors import FlinkKafkaConsumer\nfrom pyflink.datastream.formats.json import JsonRowDeserializationSchema\n\n# Setup Flink environment\nenv = StreamExecutionEnvironment.get_execution_environment()\nenv.set_parallelism(12)  # Scale based on throughput\nenv.enable_checkpointing(30000)  # 30s checkpointing\n\n# Kafka consumer configuration\nkafka_props = {\n    'bootstrap.servers': 'kafka-broker-1:9092,kafka-broker-2:9092,kafka-broker-3:9092',\n    'group.id': 'ride-events-processor',\n    'auto.offset.reset': 'latest'\n}\n\n# Define ride event schema\ntype_info = Types.ROW_NAMED(\n    ['ride_id', 'timestamp', 'lat', 'lon', 'driver_id', 'rider_id', 'fare'],\n    [Types.STRING(), Types.SQL_TIMESTAMP(), Types.DOUBLE(), Types.DOUBLE(), \n     Types.STRING(), Types.STRING(), Types.DOUBLE()]\n)\n\n# Create Kafka source\nkafka_source = FlinkKafkaConsumer(\n    topics='ride-events',\n    deserialization_schema=JsonRowDeserializationSchema.builder()\n        .type_info(type_info)\n        .build(),\n    properties=kafka_props\n)\n\n# Process ride events with 1-minute windows\nride_stream = env.add_source(kafka_source) \\\n    .key_by(lambda x: x[2]) \\\n    .window(TumblingEventTimeWindows.of(Time.minutes(1))) \\\n    .aggregate(\n        aggregate_function=RideMetricsAggregator(),\n        window_function=MetricsWindowFunction()\n    )\n\n# Write to S3 in Parquet format\nride_stream.add_sink(\n    StreamingFileSink.for_bulk_format(\n        's3://uber-ride-events/processed/',\n        ParquetBulkWriter.for_schema(type_info)\n    ).build()\n)\n\n# Execute pipeline\nenv.execute('uber-ride-events-pipeline')\n```\n\n## Monitoring\n- Consumer lag alerts\n- Processing latency metrics\n- Data completeness checks","diagram":"flowchart TD\n  A[Ride Events] --> B[Kafka Cluster]\n  B --> C[Flink Processing]\n  C --> D[S3 Parquet Files]\n  C --> E[Real-time Dashboard]\n  D --> F[Analytics DB]","difficulty":"beginner","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Twitter","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":["apache kafka","apache flink","stream processing","partitioning","parquet","windowed aggregations"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2026-01-07T03:44:27.623Z","createdAt":"2025-12-27T01:12:24.585Z"},{"id":"q-885","question":"You operate a multi-tenant SaaS analytics platform ingesting per-tenant event streams from Kafka into Snowflake. Each tenant has different event schemas that can evolve independently. Design a data pipeline to enforce per-tenant data contracts, support late-arriving events, and minimize schema drift while controlling storage costs. Include schema versioning, validation, and deployment safety steps?","answer":"Design a per-tenant schema registry with versioned contracts and a streaming processor that validates each event against its tenant’s current contract, routing to tenant-specific Snowflake partitions ","explanation":"## Why This Is Asked\nTests ability to model per-tenant contracts and schema evolution in a streaming pipeline and to handle late-arriving data without breaking consumers.\n\n## Key Concepts\n- Per-tenant data contracts and a registry\n- Schema evolution policies and compatibility checks\n- Late-arrival handling with watermarking\n- Cost-aware storage isolation per tenant\n\n## Code Example\n```javascript\n// Pseudo-code illustrating per-tenant validation\nfunction validateEvent(event, tenant) {\n  const contract = registry.getContract(tenant, event.version);\n  return contract ? contract.validate(event) : false;\n}\n```\n\n## Follow-up Questions\n- How would you test schema drift detection?\n- How would you roll back a bad schema change without data loss?","diagram":null,"difficulty":"intermediate","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["NVIDIA","Snowflake","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T14:23:12.167Z","createdAt":"2026-01-12T14:23:12.167Z"},{"id":"q-915","question":"You ingest 200k newline-delimited JSON app events daily into S3. Each event has event_id, user_id, timestamp, event_type, and attributes. Design a beginner-friendly pipeline to deduplicate by event_id, hash user_id for privacy, validate required fields, and write Parquet data partitioned by date in a data lake. Address simple schema drift and testing?","answer":"Ingest daily JSON lines, deduplicate by event_id, hash user_id with SHA-256, validate required fields, and write Parquet data partitioned by date in a data lake. Include a simple schema-drift strategy","explanation":"## Why This Is Asked\n\nThis question tests practical basics of building a reliable, beginner-friendly data pipeline: ingestion from S3, deduplication, privacy via hashing, basic schema drift handling, and testing. It avoids company-specific traps while focusing on core ETL behavior.\n\n## Key Concepts\n\n- Ingestion from object storage and newline-delimited JSON\n- Deduplication by stable key event_id\n- Privacy: deterministic hashing of user_id\n- Schema drift: optional fields with defaults\n- Parquet partitioning by date; lightweight validation\n\n## Code Example\n\n```python\n# Pseudo-code sketch for deduplication\nimport json, hashlib\n\ndef process(lines):\n    seen = set()\n    for line in lines:\n        e = json.loads(line)\n        if 'event_id' not in e or 'user_id' not in e or 'timestamp' not in e:\n            continue\n        if e['event_id'] in seen:\n            continue\n        seen.add(e['event_id'])\n        yield {\n            'event_id': e['event_id'],\n            'timestamp': e['timestamp'],\n            'user_id_hashed': hashlib.sha256(e['user_id'].encode()).hexdigest(),\n            'event_type': e.get('event_type'),\n            'attributes': e.get('attributes', {})\n        }\n```\n\n## Follow-up Questions\n\n- How would you test idempotency for reprocessing files?\n- How would you monitor data quality and schema drift over time?","diagram":"flowchart TD\n  A[Source: S3] --> B[Ingest] \n  B --> C[Deduplicate by event_id] \n  C --> D[Transform] \n  D --> E[Partitioned Parquet Lake]","difficulty":"beginner","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","MongoDB","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T15:27:56.228Z","createdAt":"2026-01-12T15:27:56.228Z"},{"id":"q-993","question":"Design a global, multi-tenant data ingestion system for a ride-hailing platform with streams for billing, trips, safety, and promotions. Each tenant defines a data contract; schemas evolve independently; late-arriving events up to 15 minutes must be accepted. Describe architecture using Apache Kafka, Schema Registry (Avro/JSON), an Iceberg/Delta lake sink, and a streaming processor (Flink/Spark). Include data model, schema evolution, backfill handling, testing, and observability?","answer":"Design a multi-tenant ingestion: enforce per-tenant contracts via a central Schema Registry (Avro) with backward/forward compatibility, publish to dedicated Kafka topics per stream, process with Flink","explanation":"## Why This Is Asked\nAssesses ability to build scalable, contract-driven ingestion across tenants with late data, schema evolution, and strong observability.\n\n## Key Concepts\n- Per-tenant data contracts and registry-based schema governance\n- Backward/forward compatibility and evolution strategy\n- Late-arriving data handling with event-time processing and watermarks\n- Exactly-once semantics, idempotent writes, and upserts\n- Observability: lineage, data freshness SLIs, drift alerts\n- Backfill testing and controlled replay plans\n\n## Code Example\n```json\n{\n  \"type\": \"record\",\n  \"name\": \"TripEvent\",\n  \"namespace\": \"com.example\",\n  \"fields\": [\n    {\"name\": \"tenant_id\", \"type\": \"string\"},\n    {\"name\": \"event_time\", \"type\": {\"type\": \"long\", \"logicalType\": \"timestamp-millis\"}},\n    {\"name\": \"payload\", \"type\": {\"type\": \"map\", \"values\": \"string\"}}\n  ]\n}\n```\n\n## Follow-up Questions\n- How would you test schema drift across tenants without affecting throughput?\n- How would you design rollback and backfill workflows for a failed batch?","diagram":null,"difficulty":"advanced","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","DoorDash","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T18:37:56.085Z","createdAt":"2026-01-12T18:37:56.085Z"},{"id":"q-176","question":"How would you design a data pipeline that handles both batch and streaming workloads for real-time analytics?","answer":"I would implement a Lambda architecture that combines both batch and streaming processing. The batch layer would handle historical data processing for accuracy, while the speed layer would process real-time data streams for immediate insights. Both layers would be integrated through a unified serving layer that merges results and provides a comprehensive view.","explanation":"## Why Asked\nThis question evaluates your understanding of modern data architecture patterns and your ability to design systems that handle multiple processing paradigms effectively.\n\n## Key Concepts\nLambda architecture, batch processing, stream processing, data consistency, real-time analytics, fault tolerance, scalability\n\n## Code Example\n```\n// Stream processing example (Apache Flink)\nDataStream<Event> stream = env.addSource(kafkaSource);\nstream.window(TumblingProcessingTimeWindows.of(Time.minutes(5)))\n      .aggregate(new CountAggregate())\n      .addSink(sink);\n```","diagram":"flowchart TD\n    A[Data Source] --> B[Batch Layer]\n    A --> C[Speed Layer]\n    B --> D[Batch View]\n    C --> E[Real-time View]\n    D --> F[Serving Layer]\n    E --> F\n    F --> G[Analytics/Queries]","difficulty":"beginner","tags":["streaming","kafka"],"channel":"data-engineering","subChannel":"streaming","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google","Meta","Netflix","Uber"],"eli5":"Imagine you have two ways to make lemonade at your lemonade stand! One way is making big batches in the morning (that's your batch processing) - you mix everything perfectly and it tastes great. The other way is making fresh cups one by one as friends arrive (that's your streaming) - super fast but maybe not as perfect. The smart trick is having both! You serve the fresh cups right away for instant refreshment, but you also have your perfect batch ready for when someone wants the best-tasting lemonade. Your brain keeps track of both - you know who got fresh cups now and who will get the perfect batch later. That way, everyone gets lemonade exactly when they need it, and you can tell your parents exactly how much you sold today!","relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-29T08:40:26.305Z","createdAt":"2025-12-26 12:51:06"},{"id":"q-222","question":"How would you design a Kafka Streams application to handle exactly-once processing with stateful aggregations while maintaining sub-second latency during peak loads of 100K events/sec?","answer":"Configure EOS_ALPHA with processing.guarantee=exactly_once_v2, use RocksDB state stores with changelog compaction, enable standby replicas, tune num.stream.threads=cores*2, set cache.max.bytes.buffering=10MB, and monitor consumer lag with Prometheus metrics.","explanation":"## Architecture\n**Exactly-once semantics**: EOS_ALPHA with transactional producers ensures atomic state updates and output commits\n**State management**: RocksDB local state + compacted changelog topics for fast recovery\n**Performance tuning**: Optimize thread pool, buffer sizes, and batch processing for sub-second latency\n\n## NFRs & Calculations\n**Throughput**: 100K events/sec ÷ 4 cores = 25K events/thread/sec\n**Latency**: Target <500ms with 100ms batch intervals\n**Storage**: 1GB state store ÷ 10MB cache = 100 cache entries\n**Recovery**: Standby replicas enable <30s failover\n\n## Key Configurations\n```properties\nprocessing.guarantee=exactly_once_v2\nnum.standby.replicas=1\ncache.max.bytes.buffering=10485760\ncommit.interval.ms=100\n```\n\n## Monitoring & Error Handling\n**Metrics**: consumer-lag, stream-latency, state-size\n**Alerts**: lag > 1000 events, latency > 1s\n**Recovery**: Automatic state restoration from changelog with incremental backups","diagram":"flowchart LR\n    A[Producer] --> B[Kafka Topic]\n    B --> C[Kafka Streams App]\n    C --> D[State Store]\n    C --> E[Standby Replica]\n    D --> F[Compact Topic]\n    E --> F\n    C --> G[Output Topic]\n    G --> H[Consumer]\n    I[Traffic Spike] --> C\n    C --> J[Adaptive Processing]\n    J --> K[Scale Out]","difficulty":"advanced","tags":["kafka","flink","kinesis"],"channel":"data-engineering","subChannel":"streaming","sourceUrl":null,"videos":{"longVideo":"https://www.youtube.com/watch?v=Z_gCv4Uum44"},"companies":["Amazon","Confluent","Netflix","Stripe","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-26T16:34:11.088Z","createdAt":"2025-12-26 12:51:07"},{"id":"q-248","question":"How would you implement exactly-once processing in a data pipeline when both source (Kafka) and sink (database) can fail, ensuring no duplicate data or data loss during network partitions and system crashes?","answer":"Implement Kafka transactions with idempotent producers (enable.idempotence=true), use database transaction IDs for deduplication, commit offsets only after successful DB commit, and configure EOS=ALWAYS for exactly-once semantics. Include retry logic with exponential backoff and dead-letter queue handling.","explanation":"## Core Implementation\n\n**Kafka Configuration:**\n```java\nProperties props = new Properties();\nprops.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);\nprops.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, \"pipeline-\" + UUID.randomUUID());\nprops.put(ProducerConfig.ACKS_CONFIG, \"all\");\nprops.put(ProducerConfig.RETRIES_CONFIG, Integer.MAX_VALUE);\n```\n\n**Transaction Pattern:**\n```java\n// Initialize transaction\nproducer.initTransactions();\n\ntry {\n    // Begin Kafka transaction\n    producer.beginTransaction();\n    \n    // Process and send to Kafka\n    ProducerRecord<String, String> record = new ProducerRecord<>(topic, key, value);\n    producer.send(record);\n    \n    // Database operation with transaction ID\n    String txId = UUID.randomUUID().toString();\n    jdbcTemplate.update(\"INSERT INTO data_table VALUES (?, ?, ?)\", \n        id, value, txId);\n    \n    // Commit offset only after DB success\n    producer.sendOffsetsToTransaction(offsets, consumer);\n    producer.commitTransaction();\n    \n} catch (Exception e) {\n    producer.abortTransaction();\n    // Retry logic with exponential backoff\n}\n```\n\n## Failure Recovery Strategies\n\n**Database Deduplication:**\n- Unique constraint on transaction_id column\n- INSERT IGNORE or ON CONFLICT DO NOTHING\n- Periodic cleanup of processed transaction IDs\n\n**Offset Management:**\n- Manual offset commits after successful processing\n- Store offsets in database for consistency\n- Use consumer group coordination for failover\n\n**Error Handling:**\n- Circuit breaker pattern for database failures\n- Dead-letter queue for unprocessable messages\n- Monitoring and alerting for transaction failures\n\n## Real-World Considerations\n\n**Performance Trade-offs:**\n- EOS=ALLS adds ~20% latency overhead\n- Increased memory usage for transaction state\n- Requires careful broker configuration (min.insync.replicas=2)\n\n**Edge Cases:**\n- Network partitions during commit phase\n- Broker leadership changes mid-transaction\n- Database connection pool exhaustion\n\n**Monitoring:**\n- Track transaction abort rates\n- Monitor consumer lag during failures\n- Alert on duplicate detection events","diagram":"graph TD\n    A[Kafka Topic] --> B[Consumer Poll]\n    B --> C[Begin Transaction]\n    C --> D[Process Records]\n    D --> E[Database UPSERT with TxID]\n    E --> F{Success?}\n    F -->|Yes| G[Send Offsets to Transaction]\n    F -->|No| H[Abort Transaction]\n    G --> I[Commit Transaction]\n    H --> J[Retry Processing]\n    I --> K[Next Poll]\n    J --> B","difficulty":"intermediate","tags":["dag","orchestration","scheduling"],"channel":"data-engineering","subChannel":"streaming","sourceUrl":null,"videos":null,"companies":["Amazon","Google","Meta","Microsoft","Netflix","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-26T16:38:30.418Z","createdAt":"2025-12-26 12:51:07"}],"subChannels":["general","streaming"],"companies":["Adobe","Airbnb","Amazon","Anthropic","Apple","Bloomberg","Coinbase","Confluent","Discord","DoorDash","Google","Instacart","LinkedIn","Lyft","Meta","Microsoft","MongoDB","NVIDIA","Netflix","Oracle","PayPal","Robinhood","Salesforce","Scale Ai","Snap","Snowflake","Square","Stripe","Tesla","Twitter","Two Sigma","Uber"],"stats":{"total":23,"beginner":7,"intermediate":8,"advanced":8,"newThisWeek":16}}