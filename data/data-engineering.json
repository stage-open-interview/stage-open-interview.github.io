{"questions":[{"id":"q-457","question":"You need to process 10GB of CSV files daily and load them into a PostgreSQL database. The files contain user activity logs with timestamps, user IDs, and event types. How would you design an efficient ETL pipeline using Python?","answer":"Use pandas with chunking for memory efficiency: `pd.read_csv('file.csv', chunksize=10000)`. Process each chunk, validate data types, convert timestamps with `pd.to_datetime()`, and use `psycopg2.extra","explanation":"## Key Components\n\n- **Memory Management**: Chunk large files to avoid memory issues\n- **Data Validation**: Check for missing values, correct data types\n- **Bulk Operations**: Use batch inserts instead of row-by-row\n- **Error Handling**: Log failed records for retry\n\n## Implementation Strategy\n\n```python\nimport pandas as pd\nimport psycopg2\nfrom psycopg2.extras import execute_batch\n\ndef process_csv_chunk(chunk):\n    # Clean and validate data\n    chunk['timestamp'] = pd.to_datetime(chunk['timestamp'])\n    chunk.dropna(inplace=True)\n    return chunk\n\ndef load_to_db(chunks):\n    conn = psycopg2.connect(db_url)\n    cursor = conn.cursor()\n    \n    for chunk in chunks:\n        processed = process_csv_chunk(chunk)\n        execute_batch(cursor, insert_query, processed.values)\n    \n    conn.commit()\n```\n\n## Performance Considerations\n\n- Use database indexes on frequently queried columns\n- Consider parallel processing for multiple files\n- Monitor memory usage and adjust chunk size accordingly","diagram":"flowchart TD\n  A[CSV Files] --> B[Chunk Processing]\n  B --> C[Data Validation]\n  C --> D[Type Conversion]\n  D --> E[Batch Insert]\n  E --> F[PostgreSQL]\n  B --> G[Error Logging]\n  G --> H[Retry Queue]","difficulty":"beginner","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Discord","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":["etl","chunking","pandas","memory efficiency","data validation","psycopg2"],"voiceSuitable":true,"lastUpdated":"2025-12-27T05:50:53.343Z","createdAt":"2025-12-26 12:51:05"},{"id":"q-488","question":"You're building a real-time analytics pipeline for a food delivery app. How would you design a data pipeline to process 1M events/day with 5-minute latency, considering data quality, schema evolution, and cost optimization?","answer":"Use Kafka + Flink for stream processing with exactly-once semantics. Implement schema registry for evolution, use Debezium CDC for change capture. Store raw events in S3, processed data in Redshift. U","explanation":"## Architecture\n- **Ingestion**: Apache Kafka with 3 partitions, replication factor 3\n- **Processing**: Apache Flink for windowed aggregations and joins\n- **Storage**: Raw events in S3 (Parquet), processed in Redshift\n\n## Data Quality\n- Schema validation using Confluent Schema Registry\n- Data quality checks with Great Expectations\n- Dead letter queue for failed events\n\n## Cost Optimization\n- Use spot instances for Flink cluster\n- S3 Intelligent Tiering for raw data\n- Redshift RA3 nodes for compute-storage separation\n\n## Monitoring\n- CloudWatch metrics for pipeline health\n- Data freshness alerts\n- Error rate monitoring","diagram":"flowchart TD\n  A[Mobile Events] --> B[Kafka Broker]\n  B --> C[Flink Processing]\n  C --> D[Data Quality Check]\n  D --> E[S3 Raw Storage]\n  D --> F[Redshift Analytics]\n  F --> G[Bi Dashboard]\n  C --> H[Dead Letter Queue]","difficulty":"intermediate","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Google","Instacart"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"lastUpdated":"2025-12-27T04:58:42.676Z","createdAt":"2025-12-26 12:51:06"},{"id":"q-518","question":"You have a 10GB CSV file with user activity logs that needs to be processed daily. The file contains user_id, timestamp, action_type, and metadata. How would you design a data pipeline to efficiently process this file and load it into a data warehouse?","answer":"Use a distributed processing framework like Apache Spark or AWS Glue. Split the CSV into partitions, process in parallel, apply schema validation and data cleaning, then load into the warehouse using ","explanation":"## Key Considerations\n- **File Size**: 10GB requires distributed processing\n- **Schema**: Define proper data types and constraints\n- **Performance**: Partitioning and parallel processing\n\n## Pipeline Architecture\n- **Ingestion**: S3 or similar object storage\n- **Processing**: Spark/PySpark for distributed computation\n- **Transformation**: Data cleaning, validation, enrichment\n- **Loading**: Bulk insert to warehouse (Snowflake, BigQuery)\n\n## Implementation Steps\n- Read CSV with proper schema definition\n- Handle malformed records with error logging\n- Apply business rules and data quality checks\n- Write to warehouse in optimized file formats (Parquet)","diagram":"flowchart TD\n  A[10GB CSV] --> B[Spark Cluster]\n  B --> C[Data Validation]\n  C --> D[Transformation]\n  D --> E[Data Warehouse]\n  C --> F[Error Logging]","difficulty":"beginner","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Airbnb","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":["apache spark","aws glue","distributed processing","data warehouse","schema validation","data cleaning"],"voiceSuitable":true,"lastUpdated":"2025-12-27T05:31:31.621Z","createdAt":"2025-12-26 12:51:06"},{"id":"q-571","question":"How would you design a data pipeline to process 1M ride events per minute from Uber's real-time streaming system?","answer":"Use Apache Kafka for ingestion with partitioning by ride_id, Apache Flink for stream processing with windowed aggregations, and write to Parquet files in S3 with partitioning by date/hour. Include mon","explanation":"## Architecture\n- **Ingestion**: Kafka with 3x replication, partitioned by geographic region\n- **Processing**: Flink with exactly-once semantics, 1-minute tumbling windows\n- **Storage**: S3 with Parquet format, compressed with Snappy\n\n## Key Considerations\n- **Scalability**: Auto-scale consumer groups based on lag metrics\n- **Fault tolerance**: Checkpointing to HDFS every 30 seconds\n- **Data quality**: Schema validation and duplicate detection\n\n## Monitoring\n- Consumer lag alerts\n- Processing latency metrics\n- Data completeness checks","diagram":"flowchart TD\n  A[Ride Events] --> B[Kafka Cluster]\n  B --> C[Flink Processing]\n  C --> D[S3 Parquet Files]\n  C --> E[Real-time Dashboard]\n  D --> F[Analytics DB]","difficulty":"beginner","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Twitter","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":["apache kafka","apache flink","stream processing","partitioning","parquet","windowed aggregations"],"voiceSuitable":true,"lastUpdated":"2025-12-27T05:31:53.703Z","createdAt":"2025-12-27T01:12:24.585Z"},{"id":"q-176","question":"How would you design a data pipeline that handles both batch and streaming workloads for real-time analytics?","answer":"Lambda architecture using batch layer for historical accuracy and speed layer for real-time insights, combined through serving layer.","explanation":"## Why Asked\nTests understanding of modern data architecture patterns and handling multiple processing paradigms\n## Key Concepts\nLambda architecture, batch processing, stream processing, data consistency, real-time analytics\n## Code Example\n```\n// Stream processing example (Apache Flink)\nDataStream<Event> stream = env.addSource(kafkaSource);\nstream.window(TumblingProcessingTimeWindows.of(Time.minutes(5)))\n      .aggregate(new CountAggregate())\n      .addSink(sink);\n```","diagram":"flowchart TD\n    A[Data Source] --> B[Batch Layer]\n    A --> C[Speed Layer]\n    B --> D[Batch View]\n    C --> E[Real-time View]\n    D --> F[Serving Layer]\n    E --> F\n    F --> G[Analytics/Queries]","difficulty":"beginner","tags":["streaming","kafka"],"channel":"data-engineering","subChannel":"streaming","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google","Meta","Netflix","Uber"],"eli5":"Imagine you have two ways to make lemonade at your lemonade stand! One way is making big batches in the morning (that's your batch processing) - you mix everything perfectly and it tastes great. The other way is making fresh cups one by one as friends arrive (that's your streaming) - super fast but maybe not as perfect. The smart trick is having both! You serve the fresh cups right away for instant refreshment, but you also have your perfect batch ready for when someone wants the best-tasting lemonade. Your brain keeps track of both - you know who got fresh cups now and who will get the perfect batch later. That way, everyone gets lemonade exactly when they need it, and you can tell your parents exactly how much you sold today!","relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"lastUpdated":"2025-12-25T12:48:05.934Z","createdAt":"2025-12-26 12:51:06"},{"id":"q-222","question":"How would you design a Kafka Streams application to handle exactly-once processing with stateful aggregations while maintaining sub-second latency during peak loads of 100K events/sec?","answer":"Configure EOS_ALPHA with processing.guarantee=exactly_once_v2, use RocksDB state stores with changelog compaction, enable standby replicas, tune num.stream.threads=cores*2, set cache.max.bytes.buffering=10MB, and monitor consumer lag with Prometheus metrics.","explanation":"## Architecture\n**Exactly-once semantics**: EOS_ALPHA with transactional producers ensures atomic state updates and output commits\n**State management**: RocksDB local state + compacted changelog topics for fast recovery\n**Performance tuning**: Optimize thread pool, buffer sizes, and batch processing for sub-second latency\n\n## NFRs & Calculations\n**Throughput**: 100K events/sec รท 4 cores = 25K events/thread/sec\n**Latency**: Target <500ms with 100ms batch intervals\n**Storage**: 1GB state store รท 10MB cache = 100 cache entries\n**Recovery**: Standby replicas enable <30s failover\n\n## Key Configurations\n```properties\nprocessing.guarantee=exactly_once_v2\nnum.standby.replicas=1\ncache.max.bytes.buffering=10485760\ncommit.interval.ms=100\n```\n\n## Monitoring & Error Handling\n**Metrics**: consumer-lag, stream-latency, state-size\n**Alerts**: lag > 1000 events, latency > 1s\n**Recovery**: Automatic state restoration from changelog with incremental backups","diagram":"flowchart LR\n    A[Producer] --> B[Kafka Topic]\n    B --> C[Kafka Streams App]\n    C --> D[State Store]\n    C --> E[Standby Replica]\n    D --> F[Compact Topic]\n    E --> F\n    C --> G[Output Topic]\n    G --> H[Consumer]\n    I[Traffic Spike] --> C\n    C --> J[Adaptive Processing]\n    J --> K[Scale Out]","difficulty":"advanced","tags":["kafka","flink","kinesis"],"channel":"data-engineering","subChannel":"streaming","sourceUrl":null,"videos":{"longVideo":"https://www.youtube.com/watch?v=Z_gCv4Uum44"},"companies":["Amazon","Confluent","Netflix","Stripe","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"lastUpdated":"2025-12-26T16:34:11.088Z","createdAt":"2025-12-26 12:51:07"},{"id":"q-248","question":"How would you implement exactly-once processing in a data pipeline when both source (Kafka) and sink (database) can fail, ensuring no duplicate data or data loss during network partitions and system crashes?","answer":"Implement Kafka transactions with idempotent producers (enable.idempotence=true), use database transaction IDs for deduplication, commit offsets only after successful DB commit, and configure EOS=ALWAYS for exactly-once semantics. Include retry logic with exponential backoff and dead-letter queue handling.","explanation":"## Core Implementation\n\n**Kafka Configuration:**\n```java\nProperties props = new Properties();\nprops.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);\nprops.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, \"pipeline-\" + UUID.randomUUID());\nprops.put(ProducerConfig.ACKS_CONFIG, \"all\");\nprops.put(ProducerConfig.RETRIES_CONFIG, Integer.MAX_VALUE);\n```\n\n**Transaction Pattern:**\n```java\n// Initialize transaction\nproducer.initTransactions();\n\ntry {\n    // Begin Kafka transaction\n    producer.beginTransaction();\n    \n    // Process and send to Kafka\n    ProducerRecord<String, String> record = new ProducerRecord<>(topic, key, value);\n    producer.send(record);\n    \n    // Database operation with transaction ID\n    String txId = UUID.randomUUID().toString();\n    jdbcTemplate.update(\"INSERT INTO data_table VALUES (?, ?, ?)\", \n        id, value, txId);\n    \n    // Commit offset only after DB success\n    producer.sendOffsetsToTransaction(offsets, consumer);\n    producer.commitTransaction();\n    \n} catch (Exception e) {\n    producer.abortTransaction();\n    // Retry logic with exponential backoff\n}\n```\n\n## Failure Recovery Strategies\n\n**Database Deduplication:**\n- Unique constraint on transaction_id column\n- INSERT IGNORE or ON CONFLICT DO NOTHING\n- Periodic cleanup of processed transaction IDs\n\n**Offset Management:**\n- Manual offset commits after successful processing\n- Store offsets in database for consistency\n- Use consumer group coordination for failover\n\n**Error Handling:**\n- Circuit breaker pattern for database failures\n- Dead-letter queue for unprocessable messages\n- Monitoring and alerting for transaction failures\n\n## Real-World Considerations\n\n**Performance Trade-offs:**\n- EOS=ALLS adds ~20% latency overhead\n- Increased memory usage for transaction state\n- Requires careful broker configuration (min.insync.replicas=2)\n\n**Edge Cases:**\n- Network partitions during commit phase\n- Broker leadership changes mid-transaction\n- Database connection pool exhaustion\n\n**Monitoring:**\n- Track transaction abort rates\n- Monitor consumer lag during failures\n- Alert on duplicate detection events","diagram":"graph TD\n    A[Kafka Topic] --> B[Consumer Poll]\n    B --> C[Begin Transaction]\n    C --> D[Process Records]\n    D --> E[Database UPSERT with TxID]\n    E --> F{Success?}\n    F -->|Yes| G[Send Offsets to Transaction]\n    F -->|No| H[Abort Transaction]\n    G --> I[Commit Transaction]\n    H --> J[Retry Processing]\n    I --> K[Next Poll]\n    J --> B","difficulty":"intermediate","tags":["dag","orchestration","scheduling"],"channel":"data-engineering","subChannel":"streaming","sourceUrl":null,"videos":null,"companies":["Amazon","Google","Meta","Microsoft","Netflix","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"lastUpdated":"2025-12-26T16:38:30.418Z","createdAt":"2025-12-26 12:51:07"}],"subChannels":["general","streaming"],"companies":["Adobe","Airbnb","Amazon","Anthropic","Confluent","Discord","Google","Instacart","Meta","Microsoft","MongoDB","Netflix","Oracle","Snowflake","Stripe","Twitter","Uber"],"stats":{"total":7,"beginner":4,"intermediate":2,"advanced":1,"newThisWeek":7}}