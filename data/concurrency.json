{"questions":[{"id":"q-1080","question":"In a log-processing pipeline, multiple producers enqueue log entries into a bounded, rate-limited work queue. Implement a beginner-friendly token-bucket rate limiter that allows enqueuing only when a token is available; a background timer refills tokens at a fixed rate up to a max. Producers block when tokens==0; workers process items from the queue. Provide a concrete Go/Python/Java solution and discuss testing?","answer":"An ideal answer demonstrates a bounded queue plus a token bucket. A mutex guards an int tokens and a CV; a ticker periodically adds tokens (capped). Each producer blocks until tokens>0, then decrement","explanation":"## Why This Is Asked\nThis explores practical synchronization for backpressure, a common real-world constraint in streaming services.\n\n## Key Concepts\n- Token bucket rate limiter with bounded queue\n- Mutexes, condition variables, and wakeups\n- Backpressure, fairness, and bounded memory\n\n## Code Example\n```javascript\n// Implementation code here\n```\n\n## Follow-up Questions\n- How would you test for bursty producers and ensure no starvation?\n- How would you adapt for multiple rate limits per producer or per client?","diagram":"flowchart TD\n  A[Producers] --> B[Bounded Queue]\n  B --> C[Workers]\n  subgraph TokenBucket\n    D[Tokens] -->|refilled| E[Producer Wake]\n  end","difficulty":"beginner","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Lyft","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T21:36:23.203Z","createdAt":"2026-01-12T21:36:23.203Z"},{"id":"q-1204","question":"In a real-time notification system with thousands of tenants, each tenant's events must be delivered to their own handler in order, while cross-tenant processing happens in parallel. Design a bounded, multi-queue architecture with per-tenant in-order guarantees, backpressure, and graceful shutdown. Compare two backpressure strategies (per-tenant token buckets vs. global credits) and discuss testing and failure scenarios. Provide runnable sketch in Go or Rust?","answer":"Use per-tenant bounded queues, fed by a central bounded dispatch buffer. Each tenant has a single consumer to preserve per-tenant order; cross-tenant work proceeds in parallel. Implement backpressure ","explanation":"## Why This Is Asked\n\nEvaluates the ability to design concurrency-aware systems that preserve per-tenant ordering while enabling cross-tenant parallelism, and to reason about backpressure strategies and graceful shutdowns.\n\n## Key Concepts\n\n- Per-tenant queues with in-order delivery\n- Bounded buffers and backpressure strategies\n- Cancellation/graceful shutdown and failure handling\n- Testing under saturation and late-arrival conditions\n\n## Code Example\n\n```go\n// A minimal sketch of structures (not a full impl)\ntype Event struct{ Tenant string; Payload []byte; TS int64 }\n\ntype TenantQueue struct{ Ch chan Event }\n\ntype Dispatcher struct {\n  Delegates map[string]*TenantQueue\n  GlobalBuf chan Event\n}\n```\n\n## Follow-up Questions\n\n- How would you detect and mitigate tenant-level starvation under skewed traffic?\n- How would you handle dynamic tenant churn (adding/removing tenants) without pausing processing?","diagram":null,"difficulty":"intermediate","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Scale Ai","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T04:51:31.188Z","createdAt":"2026-01-13T04:51:31.188Z"},{"id":"q-2080","question":"In a real-time chat system, multiple producers enqueue messages into per-room bounded queues and multiple consumers deliver to connected clients. Design a beginner-friendly concurrency solution (Go, Java, or Python) that guarantees producers block when a room's queue is full and consumers block when the queue is empty, while preserving per-room isolation and simple backpressure. Compare a mutex/condition-based queue versus a channel-based approach for bounded queues?","answer":"Implement a per-room bounded queue with blocking semantics: producers block when the queue is full, consumers block when empty. In Go, use a buffered channel per room; in Python/Java, use a bounded queue with mutex/condition variables for blocking behavior.","explanation":"## Why This Is Asked\n\nThis question evaluates practical concurrency design for real-time systems, focusing on bounded queues, blocking behavior, and per-room isolation—core patterns at companies like HashiCorp and Discord.\n\n## Key Concepts\n\n- Bounded queues with blocking semantics\n- Per-room isolation and backpressure\n- Channel-based versus lock-based synchronization\n- Correct testing across producers/consumers\n\n## Code Example\n\n```go\n// Go sketch: per-room bounded channel\ntype Room struct {\n  Msgs chan Message\n}\nfunc NewRoom(cap int) *Room { return &Room{Msgs: make(chan Message, cap)} }\n```\n\n```python\n# Python sketch: per-room bounded queue with locks/conditions\nfrom threading import Condition, Lock\n\nclass Room:\n    def __init__(self, capacity):\n        self.capacity = capacity\n        self.queue = []\n        self.lock = Lock()\n        self.not_full = Condition(self.lock)\n        self.not_empty = Condition(self.lock)\n    \n    def enqueue(self, message):\n        with self.not_full:\n            while len(self.queue) >= self.capacity:\n                self.not_full.wait()\n            self.queue.append(message)\n            self.not_empty.notify()\n    \n    def dequeue(self):\n        with self.not_empty:\n            while not self.queue:\n                self.not_empty.wait()\n            message = self.queue.pop(0)\n            self.not_full.notify()\n            return message\n```\n\n## Comparison: Mutex/Condition vs Channel-Based\n\n**Mutex/Condition (Python/Java):**\n- More explicit control over queue state\n- Easier to implement custom behavior (timeouts, priorities)\n- Higher code complexity and error-prone\n- Manual management of condition variables\n\n**Channel-Based (Go):**\n- Simpler, idiomatic Go concurrency\n- Built-in blocking semantics\n- Less error-prone, compiler-enforced usage\n- Limited flexibility for custom queue behavior\n\nBoth approaches provide the same blocking guarantees, but channels offer cleaner code for standard bounded queue patterns.","diagram":null,"difficulty":"beginner","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Hashicorp"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T04:27:12.373Z","createdAt":"2026-01-14T23:33:30.390Z"},{"id":"q-2329","question":"Design a bounded, concurrent router for a real-time analytics platform ingesting events from thousands of clients. Each client's events must be processed in arrival order, while events from different clients may be processed in parallel. The router should implement per-client backpressure and support clients joining/leaving on the fly. Describe two concrete implementations: (A) per-client queues with a central scheduler using locks, (B) per-client lock-free queues with a shared ready-list. Include correctness, failure handling, and a practical test plan?","answer":"Use N per-client FIFO queues and a fixed worker pool. Each client has a credit counter; producers enqueue only when credits>0, decrementing atomically. (A) Lock-based: guard each queue with a mutex; a","explanation":"## Why This Is Asked\nTests ability to design per-client ordering with global parallelism and backpressure, plus two concrete implementations and correctness reasoning.\n\n## Key Concepts\n- Per-client FIFO queues with bounded capacity\n- Backpressure via per-client credits\n- Locking vs. lock-free queue design\n- Dynamic client join/leave handling and fairness\n\n## Code Example\n```javascript\n// Skeleton illustrating per-client queues and a central scheduler (pseudo)\nclass Router {\n  constructor(capacity){/* ... */}\n  enqueue(clientId, item){/* ... */}\n  dequeue(workerId){/* ... */}\n}\n```\n\n## Follow-up Questions\n- How would you test ordering guarantees under bursty traffic?\n- How do you detect/handle starvation and ensure fairness across clients?\n- What failure scenarios require replay or idempotency guarantees?","diagram":null,"difficulty":"intermediate","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Instacart","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T13:05:58.777Z","createdAt":"2026-01-15T13:05:58.777Z"},{"id":"q-2542","question":"Design a two-stage, bounded-concurrency pipeline for real-time video frames from multiple cameras. Stage 1 decodes frames (CPU-bound) using a fixed thread pool; Stage 2 runs an AI-based enhancer asynchronously. Ensure per-frame ordering across all cameras, backpressure to keep queues bounded, support dynamic worker scaling and clean shutdown with cancellation, and provide a runnable minimal sketch in a language of your choice?","answer":"Implement two bounded queues: Q1 for decoding (N1 workers) and Q2 for enhancement (N2 workers). Attach per-frame sequence numbers to enforce global in-order delivery to Stage 2. Stage 1 runs on a fixed thread pool; Stage 2 uses async workers with dynamic scaling. Use semaphores for backpressure, cancellation tokens for shutdown, and a sequencer to maintain ordering across all cameras.","explanation":"## Why This Is Asked\n\nThis question probes real-world patterns for multi-stage pipelines, ordering guarantees, backpressure, dynamic scaling, and graceful shutdown across components. It highlights trade-offs between CPU-bound decoding and async I/O-bound processing, and tests ability to reason about cross-stage coordination.\n\n## Key Concepts\n\n- Bounded queues and backpressure\n- Ordering guarantees across concurrent stages\n- Thread pools vs async workers\n- Cancellation tokens and error propagation\n- Dynamic worker scaling\n\n## Code Example\n\n```javascript\n// Implementation code here\n```\n\n## Follow-up Considerations\n\nThe solution should address how to handle frame drops during overload scenarios, monitor queue depths for scaling decisions, and ensure proper resource cleanup during shutdown.","diagram":null,"difficulty":"advanced","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Google","Hugging Face"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T05:33:35.924Z","createdAt":"2026-01-15T22:30:36.615Z"},{"id":"q-2606","question":"Design an in-process, bounded-concurrency event bus for a streaming service that ingests events from hundreds of producers (per-video id) and delivers to a pool of workers while guaranteeing per-video in-order processing, minimal latency, and backpressure signaling to producers when queues fill. Include a runnable sketch in Go or Rust and discuss testing?","answer":"Partitioned ring buffers per video ID with one consumer per partition, managed by a global scheduler for hot partition balancing. Producers append with per-ID sequence numbers to preserve in-order delivery. Backpressure is implemented through bounded buffers that block producers when full, using condition variables or atomic counters for efficient signaling.","explanation":"## Why This Is Asked\nThis question evaluates your ability to design a practical, low-latency, bounded-concurrency event bus with per-key ordering and backpressure. It tests your understanding of concurrency primitives, scheduling strategies, and failure modes in distributed systems.\n\n## Key Concepts\n- Partitioning for data locality and parallelism\n- Per-key ordering with sequence numbers\n- Bounded buffers and backpressure strategies\n- Lock-free vs locking trade-offs; wakeups and shutdown\n\n## Code Example\n```javascript\n// Skeleton illustrating partitioned queues\nclass Partition {\n  constructor(","diagram":null,"difficulty":"intermediate","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Amazon","LinkedIn"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T05:02:33.098Z","createdAt":"2026-01-16T02:36:29.090Z"},{"id":"q-2658","question":"Design a bounded, fair, multi-producer/multi-consumer system for real-time order matching in a crypto exchange. Many producers submit limit orders; multiple workers attempt matches against an in-memory order book. Guarantee bounded memory, backpressure, and starvation-free fairness across producers (e.g., per-producer quotas or weighted fairness). Describe data structures, synchronization, and a minimal runnable Java snippet showing enqueue and a worker loop?","answer":"Implement per-producer bounded queues fed into a rotating, weighted fair dispatcher that pushes into a fixed-size ring buffer consumed by worker threads. Use a single-order-book lock for matching, wit","explanation":"## Why This Is Asked\nTests ability to design high-throughput, fair concurrency with bounded memory in a critical system like exchanges or dispatchers.\n\n## Key Concepts\n- Bounded queues with backpressure\n- Multi-producer/multi-consumer fairness\n- Dispatcher scheduling policies (weighted fair queuing)\n- Shared-order-book safety and lock granularity\n\n## Code Example\n```java\nimport java.util.concurrent.*;\nimport java.util.*;\n\npublic class BoundedFairDispatcher {\n  static class Order { final String id; Order(String id){ this.id = id; } }\n  private final int perProducerCap;\n  private final Map<String,BlockingQueue<Order>> queues = new ConcurrentHashMap<>();\n  private final BlockingQueue<Order> ring;\n  private volatile boolean shutdown = false;\n  public BoundedFairDispatcher(int perProducerCap, int ringCap){ this.perProducerCap = perProducerCap; this.ring = new ArrayBlockingQueue<>(ringCap); }\n  public void enqueue(String pid, Order o) throws InterruptedException {\n    BlockingQueue<Order> q = queues.computeIfAbsent(pid, k -> new ArrayBlockingQueue<>(perProducerCap));\n    q.put(o); // producer blocks if quota reached\n  }\n  public void dispatcherLoop() throws InterruptedException {\n    while (!shutdown) {\n      for (BlockingQueue<Order> q : queues.values()) {\n        Order o = q.poll();\n        if (o != null) ring.put(o);\n      }\n      // simple backoff to avoid busy spin\n      Thread.yield();\n    }\n  }\n  public void shutdown() { shutdown = true; }\n}\n```\n\n## Follow-up Questions\n- How would you scale to multiple dispatchers and partition rings?\n- How would you test fairness under highly skewed producer rates?","diagram":null,"difficulty":"advanced","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","DoorDash","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T05:43:13.395Z","createdAt":"2026-01-16T05:43:13.395Z"},{"id":"q-2804","question":"In a real-time analytics pipeline, millions of event streams (streamId) arrive; each stream must be processed in-order, but streams can be processed in parallel. Design a bounded per-stream queueing layer with a shared worker pool. Include backpressure when a queue fills, handling of late/out-of-order events, and graceful recovery after worker failure. Provide a runnable sketch in Go or Rust?","answer":"Use per-stream bounded queues and a fixed worker pool. Producers enqueue into their stream's queue; if full, apply backpressure by blocking or signaling upstream. Use per-stream sequence numbers and a","explanation":"## Why This Is Asked\n\nIn real-time analytics pipelines, streams arrive continuously. Achieving per-stream in-order processing while allowing global parallelism is a non-trivial balance requiring bounded buffers, backpressure, and robust failure handling.\n\n## Key Concepts\n\n- Per-stream in-order processing\n- Bounded queues\n- Backpressure\n- Out-of-order handling\n- Graceful shutdown & failure recovery\n- Locks vs lock-free trade-offs\n\n## Code Example\n\n```go\n// Implementation sketch\ntype Event struct{ StreamID int; Seq int; Payload []byte }\n```\n\n## Follow-up Questions\n\n- How would you test backpressure and ordering?\n- How would you scale to thousands of streams and dynamic creation?","diagram":null,"difficulty":"intermediate","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T13:09:01.409Z","createdAt":"2026-01-16T13:09:01.409Z"},{"id":"q-2834","question":"Three producers generate events tagged by user id and time; a single in-memory sink aggregates per-user activity over a 1-second sliding window and emits a summary to a consumer. Implement a beginner-friendly concurrency solution in Go, Python, or Java that guarantees thread-safe window updates, handles late events (up to 200ms), and ensures the sink doesn't drop events under low backpressure. Provide runnable sketch and tests?","answer":"Use a per-user rolling window guarded by a mutex with an in-memory map, plus a periodic sweeper to drop old events and emit counts. Alternatively funnel events through a single worker with a bounded c","explanation":"## Why This Is Asked\n\nTests practical concurrency patterns around per-key state, backpressure, and late-arrival handling beyond basic queues.\n\n## Key Concepts\n\n- Per-key state management with thread safety\n- Sliding windows and late-event handling\n- Backpressure and testability between mutex-based and channel-based designs\n\n## Code Example\n\n```go\npackage main\n\nimport (\n  \"time\"\n  \"sync\"\n)\n\ntype Event struct { User string; TS time.Time }\ntype Summary struct { User string; Count int }\n\nfunc boundWindowWorker(in <-chan Event, out chan<- Summary, window time.Duration) {\n  mu := sync.Mutex{}\n  perUser := make(map[string][]time.Time)\n  tick := time.NewTicker(1 * time.Second)\n  defer tick.Stop()\n\n  for {\n    select {\n    case e := <-in:\n      mu.Lock()\n      arr := perUser[e.User]\n      arr = append(arr, e.TS)\n      cutoff := e.TS.Add(-window)\n      i := 0\n      for i < len(arr) && arr[i].Before(cutoff) {\n        i++\n      }\n      perUser[e.User] = arr[i:]\n      mu.Unlock()\n    case t := <-tick.C:\n      _ = t\n      mu.Lock()\n      for u, arr := range perUser {\n        out <- Summary{User: u, Count: len(arr)}\n      }\n      mu.Unlock()\n    }\n  }\n}\n```\n\n## Follow-up Questions\n\n- How would you test late arrivals and ensure no data loss under backpressure?\n- Compare mutex-based versus single-worker channel designs in terms of latency and fairness.","diagram":null,"difficulty":"beginner","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","PayPal","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T14:09:24.445Z","createdAt":"2026-01-16T14:09:24.445Z"},{"id":"q-2876","question":"Design a dynamic DAG-based task scheduler with runtime-submitted tasks and dependencies; implement a fixed-size worker pool with local queues, support cross-worker work-stealing, and ensure dependents wake atomically when prerequisites finish; bound the global queue to enforce backpressure; discuss deadlock avoidance and fairness under high contention?","answer":"Represent tasks as nodes with in-degrees and adjacency lists. Each worker owns a local queue; when a task’s in-degree becomes 0, enqueue to a worker. On completion, atomically decrement children and e","explanation":"## Why This Is Asked\n\nTests practical concurrency design for dynamic workloads, runtime DAGs, backpressure, and fairness in a multi-threaded runtime common in real systems.\n\n## Key Concepts\n\n- DAG scheduling with in-degree tracking\n- Local queues and cross-worker work-stealing\n- Bounded global queue for backpressure\n- Deadlock and livelock avoidance, fairness under contention\n\n## Code Example\n\n```go\n// Skeleton illustrating the core ideas\ntype Task struct {\n  id   string\n  deps int32\n  // children references omitted for brevity\n}\n\n// On submit: if in-degree becomes 0 -> enqueue to a worker\n// On completion: atomically decrement children; enqueue when 0\n```\n\n## Follow-up Questions\n\n- How would you test for starvation or livelock in this scheduler?\n- How would you extend to support task priorities or IO-bound vs CPU-bound tasks?","diagram":"flowchart TD\nA[Submit Task] --> B[Increment In-degree]\nB --> C{In-degree == 0?}\nC -- Yes --> D[Enqueue to Local Queue]\nC -- No --> E[Wait for Dependencies]\nD --> F[Worker Executes Task]\nF --> G[Release to Children]\nG --> H{New Ready Tasks}\nH -- Enqueue --> D","difficulty":"intermediate","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","Netflix","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T15:45:21.678Z","createdAt":"2026-01-16T15:45:21.678Z"},{"id":"q-2910","question":"Design a concurrent job queue for an image-processing service: tasks arrive with tenant_id, priority, and deadline; implement a bounded, multi-queue system where workers always pull from the currently highest-priority non-empty tenant queue, but aging prevents any tenant from starving. Include backpressure on the global queue, cross-tenant fairness, and a test plan for liveness under bursty load. Provide concrete structures and trade-offs?","answer":"I would implement per-tenant queues plus a shared min-heap keyed by (priority, age). Workers pick the highest-priority head, applying aging to avoid starvation. A global semaphore enforces bound and c","explanation":"## Why This Is Asked\n- Evaluates design of a concurrent, bounded queue with per-tenant fairness under contention.\n- Probes data structures for priority scheduling and starvation prevention.\n- Checks strategies for backpressure and testing approaches.\n\n## Key Concepts\n- Bounded queues, per-tenant fairness, priority scheduling\n- Backpressure, wake-ups, and liveness guarantees\n- Testing under bursty workloads and contention\n\n## Code Example\n```javascript\n// Pseudocode sketch of data structures\nclass TenantQueue { constructor() { this.q = []; this.lock = new SpinLock(); } }\nclass GlobalScheduler {\n  enqueue(task){ /* bounded token + per-tenant push */ }\n  dequeue(){ /* pick highest-priority non-empty queue with aging */ }\n}\n```\n\n## Follow-up Questions\n- How would you implement aging to avoid starvation without starving high-priority tasks?\n- How do you verify liveness under bursty load? ","diagram":"flowchart TD\nA[Submit Task] --> B[Place in Tenant Queue]\nB --> C[Global Scheduler Picks by Priority]\nC --> D[Worker Executes]\nD --> E[Dequeue and Reclaim Credits]\n","difficulty":"intermediate","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T16:59:49.399Z","createdAt":"2026-01-16T16:59:49.400Z"},{"id":"q-2995","question":"Design a speculative execution layer for a real-time collaborative editor where user actions arrive as a DAG of edits with dependencies. Implement a multithreaded executor that runs independent edits in parallel, defers dependents until prerequisites finish, and detects conflicts on the same position to rollback. Include a bounded replay queue to cap work and a rollback protocol. Provide a runnable sketch in Go or Rust and discuss guarantees?","answer":"Leverage per-document DAG frontier; each edit has id, deps, and position. Schedule ready edits on a worker pool; detect conflicts by (position, client-version). On conflict, rollback to last committed","explanation":"## Why This Is Asked\nThis question probes the ability to design speculative, dependency-aware execution for real-time collaboration, a realistic pattern in chat editors and SaaS platforms.\n\n## Key Concepts\n- Directed acyclic graph scheduling\n- Speculative execution with rollback\n- Conflict detection on shared document positions\n- Bounded replay queue and deterministic commit ordering\n\n## Code Example\n```rust\n// Skeleton sketch of scheduling loop (high level)\nstruct Edit { id: u64, deps: Vec<u64>, pos: usize, content: String }\n```\n\n## Follow-up Questions\n- How would you test race conditions and simulate latency?\n- How would you scale the replay buffer across many documents?","diagram":null,"difficulty":"intermediate","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Discord","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T20:36:42.992Z","createdAt":"2026-01-16T20:36:42.993Z"},{"id":"q-3132","question":"Design an in-memory, concurrent stream join operator for two input streams A and B. Each event has a key and a timestamp. Implement a per-key sliding-window equi-join with multiple worker threads that can process different keys in parallel, but guarantee in-order output per key. Bound memory usage with eviction, and apply backpressure when downstream slows. Compare locking vs lock-free implementations and outline testing strategies and failure scenarios?","answer":"Strategy: partition by key hash; per-partition workers hold per-key sliding windows in bounded maps. Join when both sides have a matching key within the window; emit with per-key sequence for in-order","explanation":"## Why This Is Asked\n\nAssess ability to design a concurrent, stateful streaming component with per-key ordering and backpressure across workers.\n\n## Key Concepts\n\n- per-key sliding windows\n- bounded memory with eviction\n- output-order guarantees\n- backpressure mechanisms\n- locking vs lock-free trade-offs\n- test strategies and failure modes\n\n## Code Example\n\n```javascript\n// Pseudo-code sketch of partitioning and per-key join\n```\n\n## Follow-up Questions\n\n- How would you handle hot keys and dynamic partitions?\n- What metrics and tests ensure liveness under backpressure?","diagram":"flowchart TD\n  A[Producer A] --> P[Partitioner]\n  B[Producer B] --> P\n  P --> W[Worker Pool]\n  W --> O[Output]\n  O --> D[Downstream]\n  D -- backpressure--> O","difficulty":"intermediate","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","LinkedIn","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T04:12:57.227Z","createdAt":"2026-01-17T04:12:57.227Z"},{"id":"q-3217","question":"Design a per-session ordered event dispatcher for a real-time multiplayer game server. Each game session has its own FIFO queue; multiple producers can enqueue events across sessions, and a fixed-size pool of worker threads processes events. Requirements: (1) preserve strict per-session order; (2) allow cross-session work-stealing for load balancing; (3) enforce a global bounded queue with backpressure to producers; (4) support dynamic session creation/destruction with zero deadlock and graceful shutdown; (5) discuss memory reclamation and safety?","answer":"Leverage a bounded global queue feeding a fixed worker pool, with per-session FIFOs. Producers enqueue to the global queue; workers pull sessions, draining in-order. For load balancing, allow stealing","explanation":"## Why This Is Asked\nThis question probes the candidate's ability to design a concurrent, low-latency dispatcher that preserves per-session ordering while scaling across cores. It also exercises lifecycle management and safe shutdown.\n\n### Key Concepts\n- Bounded backpressure and global queues\n- Per-session FIFO with in-order processing\n- Work stealing across sessions without violating order\n- Dynamic session lifecycle and memory reclamation\n\n### Code Example\n```rust\n// Pseudo Rust sketch: session queues and stealing interface\nstruct SessionQueue { /* ... */ }\nimpl SessionQueue {\n  fn enqueue(&self, evt: Event);\n  fn dequeue(&self) -> Option<Event>;\n}\n```\n\n### Follow-up Questions\n- How would you extend to prioritize latency-sensitive events within a session?\n- How would you detect starvation and ensure fairness across sessions?","diagram":null,"difficulty":"advanced","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Google","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T07:30:21.545Z","createdAt":"2026-01-17T07:30:21.546Z"},{"id":"q-3306","question":"Design a bounded, deadline-aware task scheduler for a live image-processing pipeline. Producers enqueue tasks labeled CPU-bound or IO-bound with deadlines; implement a two-tier priority with earliest deadline first and aging to prevent starvation. Use per-worker local queues with work-stealing, and a global backpressure mechanism when backlog grows. Ensure deadlines are respected and test bursts with latency metrics?","answer":"Use a bounded global queue guarded by a counting semaphore to apply backpressure. Dequeue tasks into per-worker local deques; workers steal from neighbors to balance. Each task carries a deadline and ","explanation":"## Why This Is Asked\nEvaluates a candidate's ability to design a concurrent scheduler that respects deadlines under high contention, while balancing throughput via per-worker queues and backpressure. It also probes testing strategies for bursty workloads and fairness.\n\n## Key Concepts\n- Bounded queues and backpressure\n- Deadline-aware scheduling (EDF) with aging\n- Per-worker local queues and work-stealing\n- Task typing (CPU vs IO) and affinity considerations\n- Correctness under contention and starvation avoidance\n\n## Code Example\n```javascript\n// Skeleton: Task with deadline and type\nclass Task { constructor(id, deadline, type, payload) { this.id = id; this.deadline = deadline; this.type = type; this.payload = payload; } }\nclass Scheduler {\n  // core components: boundedGlobalQueue, perWorkerQueues, steal logic, aging\n}\n```\n\n## Follow-up Questions\n- How would you validate absence of starvation under worst-case burst scenarios?\n- How would you adapt the design for multi-host deployment with cross-node backpressure?","diagram":"flowchart TD\n  P[Producer] --> G[GlobalBoundedQueue]\n  G --> W[WorkerLocalQueues]\n  W --> C[Completed]\n  Back[Backpressure] --> G","difficulty":"intermediate","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Meta","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T10:42:19.393Z","createdAt":"2026-01-17T10:42:19.393Z"},{"id":"q-3453","question":"Design a beginner-friendly concurrency setup in Go for a real-time notification service where many producers emit payloads that must be delivered to users in per-user order. Implement a global bounded queue and per-user FIFO channels so that each user's messages are processed in order, while a small worker pool handles delivery. Explain how you enforce backpressure, avoid starvation, and test ordering and race conditions?","answer":"Use a global buffered channel as backpressure, and maintain a per-user FIFO channel to guarantee order. A single dispatcher reads from the global queue and forwards each payload to the user’s dedicate","explanation":"## Why This Is Asked\nTests practical micro-concurrency patterns, including backpressure, per-key ordering, and fairness—common in large-scale services.\n\n## Key Concepts\n- Bounded queues\n- Per-key sequencing\n- Dispatcher + worker pool\n- Fairness and deadlock avoidance\n\n## Code Example\n```go\n// skeleton illustrating structures and flow (omitted for brevity)\n```\n\n## Follow-up Questions\n- How would you scale the worker pool if the per-user queues grow unbounded? \n- How would you test for starvation under skewed load?","diagram":null,"difficulty":"beginner","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T16:49:22.802Z","createdAt":"2026-01-17T16:49:22.802Z"},{"id":"q-3483","question":"In a real-time video analytics service, cameras stream frames that must be processed in per-camera order while frames from different cameras can run concurrently. Design a bounded, backpressured dispatch with a central queue and per-camera FIFO queues, plus a small worker pool. Explain ordering, starvation avoidance, and failure handling. Provide a runnable asyncio Python sketch demonstrating enqueue, dispatcher, queues, and graceful shutdown?","answer":"Use a central bounded queue with maxsize N and a dispatcher that routes frames to per-camera FIFO queues. Each camera has a dedicated worker set that preserves order per camera, while frames from diff","explanation":"## Why This Is Asked\n\nReal-time video analytics require strict per-camera ordering with parallelism across cameras; this tests architectural decisions for bounded dispatchers, backpressure, and robust failure handling.\n\n## Key Concepts\n\n- Per-camera FIFO with a global bound\n- Dispatcher design and backpressure handling\n- Failure isolation and graceful shutdown\n- Starvation avoidance and latency considerations\n\n## Code Example\n\n```python\nimport asyncio\nfrom collections import defaultdict\n\nCENTRAL_MAX = 100\nPER_CAM_MAX = 20\n\nasync def dispatcher(central, queues):\n    while True:\n        cam, frame = await central.get()\n        if cam is None:\n            for q in queues.values():\n                await q.put((None, None))\n            return\n        await queues[cam].put(frame)\n\nasync def worker(queue, cam):\n    while True:\n        frame = await queue.get()\n        if frame is None:\n            break\n        # process(frame)  # placeholder for actual work\n        queue.task_done()\n```\n\n## Follow-up Questions\n\n- How would you monitor backpressure and latency in production?\n- How would you scale to hundreds of cameras with churn while preserving guarantees?","diagram":"flowchart TD\n  P[Producers] --> C[Central bounded queue]\n  C --> D[Dispatcher]\n  D --> Q[Per-camera queues]\n  Q --> W[Workers]","difficulty":"advanced","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Robinhood","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T18:41:29.810Z","createdAt":"2026-01-17T18:41:29.810Z"},{"id":"q-3770","question":"Scenario: A telemetry inlet ingests events from numerous IoT devices. Each device’s events must be processed in arrival order, while events from different devices are handled concurrently by a small worker pool. Design a beginner-friendly Go solution using a bounded global queue and per-device FIFO channels to preserve per-device order. Add deduplication for repeated (deviceId,seq) pairs, implement backpressure, and ensure fairness when bursts occur. Provide a runnable sketch and brief testing plan?","answer":"Go solution: a bounded global channel acts as a backpressure point. A dispatcher reads events and forwards them into per-device FIFO channels (one buffered channel per device). A small worker pool dra","explanation":"## Why This Is Asked\n\nNew angle: adds per-device dedup and burst fairness, common in telemetry IoT workloads. Tests should cover ordering, backpressure, and race conditions, mirroring real-world constraints at data-intensive shops.\n\n## Key Concepts\n\n- Bounded global queue enforces backpressure.\n- Per-device FIFO channels preserve per-device order under concurrency.\n- Deduplication by (device,seq) prevents duplicate work.\n- Round-robin worker drain provides fairness during bursts.\n\n## Code Example\n\n```go\npackage main\n\ntype Event struct {\n  Device string\n  Seq    int\n  Data   string\n}\n\n// global bounded queue\nvar globalQueue = make(chan Event, 128)\n\nvar (\n  mu     sync.Mutex\n  queues = map[string]chan Event{}\n  seen   = map[string]struct{}{}\n)\n\nfunc getQueue(id string) chan Event {\n  mu.Lock()\n  defer mu.Unlock()\n  q, ok := queues[id]\n  if !ok {\n    q = make(chan Event, 32)\n    queues[id] = q\n  }\n  return q\n}\n\nfunc dispatcher() {\n  for e := range globalQueue {\n    key := e.Device + \":\" + strconv.Itoa(e.Seq)\n    mu.Lock()\n    if _, ok := seen[key]; ok {\n      mu.Unlock()\n      continue\n    }\n    seen[key] = struct{}{}\n    ch := getQueue(e.Device)\n    mu.Unlock()\n    ch <- e\n  }\n}\n\n// workers would range over device queues in round-robin\n```\n\n## Follow-up Questions\n\n- How would you test ordering, dedup, and backpressure under bursty loads?\n- How would you scale this to dynamic worker counts and multiple brokers while preserving per-device ordering?","diagram":"flowchart TD\n  P[Producers] --> Q[GlobalBoundedQueue]\n  Q --> D[Dispatcher]\n  D --> DEV_A[DeviceQueueA]\n  D --> DEV_B[DeviceQueueB]\n  DEV_A --> W1[Worker]\n  DEV_B --> W2[Worker]\n  %% rounds out to multiple devices with a shared worker pool","difficulty":"beginner","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Tesla","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T08:50:01.209Z","createdAt":"2026-01-18T08:50:01.209Z"},{"id":"q-3797","question":"In a GPU-accelerated image-processing pipeline, dozens of producers generate work units with different priorities and data dependencies, and a pool of workers dispatch kernels. Design a bounded, multi-producer multi-consumer queue with backpressure that supports per-item priority without starving high-priority tasks. Explain data structures, memory ordering, and how you avoid ABA and deadlocks. Include a runnable minimal sketch (Rust or C++) showing enqueue/dequeue semantics and a basic test strategy with synthetic load?","answer":"Design a bounded MPMC queue built as a ring buffer with atomic head/tail indices and per-slot state. Use memory_order_release/acquire, padding to avoid false sharing, and a backoff strategy for conten","explanation":"## Why This Is Asked\n\nTests advanced concurrency design for bounded queues, backpressure, and fairness in a GPU/telemetry-like pipeline.\n\n## Key Concepts\n\n- Lock-free MPMC queues\n- memory ordering\n- backpressure strategies\n- starvation prevention with priority lanes\n- deadlock-free shutdown and testing\n\n## Code Example\n\n```cpp\n// skeleton of bounded MPMC queue\n#include <atomic>\n#include <vector>\ntemplate<class T>\nclass BoundedMPMC {\n  size_t const CAP;\n  std::vector<std::atomic<T*>> slots;\n  std::atomic<size_t> head{0}, tail{0};\npublic:\n  bool enqueue(const T& v);\n  bool dequeue(T& v);\n};\n```\n\n## Follow-up Questions\n\n- How would you integrate per-item priority without starving low-priority tasks?\n- How would you test for memory ordering issues and race conditions at scale?","diagram":null,"difficulty":"advanced","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","NVIDIA","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T09:43:27.985Z","createdAt":"2026-01-18T09:43:27.985Z"},{"id":"q-3855","question":"Design a streaming dataflow for real-time analytics that processes events into fixed 1-second windows. Implement per-window bounded queues, a dynamic worker pool, and a watermark-driven cancellation protocol: late events crossing the watermark cancel the window and abort pending tasks. Guarantee in-window output order and no deadlocks under backpressure. Explain testing strategies and trade-offs?","answer":"Propose a streaming dataflow with per-window bounded queues and a dynamic worker pool. Map events to fixed 1s windows; maintain a watermark. If a late event arrives past the watermark, cancel the corr","explanation":"## Why This Is Asked\n\nInterviewers seek practical experience with streaming systems, time-windowing, and cancellation under backpressure. A watermark-driven cancellation tests correctness under out-of-order data and ensures resources aren’t wasted on irrelevant windows.\n\n## Key Concepts\n\n- Watermarks and late data handling\n- Per-window bounded queues and dynamic worker pools\n- Cancellation propagation and deadlock avoidance\n- Testing strategies for timing, backpressure, and failure modes\n\n## Code Example\n\n```javascript\n// Skeleton windowing structure\nclass Window {\n  constructor(ts) {\n    this.ts = ts;\n    this.tasks = [];\n    this.cancelled = false;\n  }\n  addTask(t) {\n    if (this.cancelled) throw new Error('window cancelled');\n    this.tasks.push(t);\n  }\n  complete() {\n    // emit aggregated result\n  }\n}\n```\n\n## Follow-up Questions\n\n- How would you measure fairness across windows under bursty input?\n- How would you test cancellation propagation and deadlock scenarios?","diagram":null,"difficulty":"intermediate","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Instacart","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T13:00:02.391Z","createdAt":"2026-01-18T13:00:02.391Z"},{"id":"q-3994","question":"Design a fault-tolerant, multi-region task queue for a real-time data-enrichment service serving many tenants. Each tenant has a cap and can enqueue interdependent tasks. Propose a bounded, distributed queue with backpressure, delivering at least once while achieving exactly-once processing for idempotent handlers, plus per-tenant ordering and a recovery plan. Include a test strategy?","answer":"To meet scale, implement a bounded, per-tenant, multi-region queue backed by a durable store with region-local buffers. Use a per-task sequence ID and idempotent handlers to realize exactly-once proce","explanation":"## Why This Is Asked\n\nTests ability to design a distributed queue with per-tenant backpressure, cross-region resilience, and strong delivery guarantees in a multi-region deployment.\n\n## Key Concepts\n\n- Bounded, per-tenant queues and regional buffering\n- Exactly-once vs at-least-once semantics with idempotence\n- Cross-region failure handling and replay\n- Ordering guarantees and recovery testing\n\n## Code Example\n\n```javascript\n// Pseudo sketch: per-tenant queue API with sequence IDs and dedup store\nclass Task { constructor(id, tenant, seq, deps) { ... } }\nclass DurableStore { enqueue(task); ack(seq); replay(tenant); }\n```\n\n## Follow-up Questions\n\n- How would you test region failover and replay guarantees?\n- How do you prevent head-of-line blocking across tenants while preserving ordering?\n- What metrics signal backpressure effectiveness and replay impact?","diagram":null,"difficulty":"advanced","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google","Hashicorp"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T18:51:55.631Z","createdAt":"2026-01-18T18:51:55.632Z"},{"id":"q-4066","question":"Design a backpressure-aware streaming engine for real-time analytics: streams partitioned by sensor_id; each partition has a bounded queue and a 5s tumbling window; a watermark advances to emit aggregates; late data goes to a late-path buffer for a grace period; exactly-once via incremental checkpointing of partition offsets and window state; discuss latency-throughput trade-offs and testing strategy?","answer":"Design per-partition bounded queues with backpressure signaling upstream; implement 5s tumbling windows with a monotonic watermark; route late events to a buffer for a grace period and still allow eve","explanation":"## Why This Is Asked\n\nTests understanding of streaming architectures under load, including partitioning, backpressure, and correctness guarantees (exactly-once) in the presence of late data and failures.\n\n## Key Concepts\n\n- Backpressure and bounded queues\n- Partitioning by key for ordering guarantees\n- Watermarks and late-data handling\n- Windowed aggregations and state management\n- Exactly-once semantics via checkpointing and replay\n- Test strategies for bursts, late data, and crash recovery\n\n## Code Example\n\n```javascript\n// Pseudo-code: per-partition window with watermark and late path\nclass PartitionProcessor {\n  constructor(limit) { this.queue = []; this.limit = limit; this.watermark = 0; this.windowState = {}; }\n  enqueue(event) {\n    if (this.queue.length >= this.limit) return false; this.queue.push(event); return true;\n  }\n  tick(now) {\n    // advance watermark and emit if window complete\n    while (this.queue.length && this.queue[0].ts <= this.watermark) {\n      const e = this.queue.shift(); // late data path logic here\n      // update windowState for sensor_id with e.value\n    }\n  }\n}\n```\n\n## Follow-up Questions\n\n- How would you handle highly skewed partitions where one sensor_id dominates load?\n- What testing strategy would you use to validate exactly-once guarantees during crash recovery?","diagram":"flowchart TD\n  A[Producers] --> B[Partitioned Queues (sensor_id)]\n  B --> C[5s Windows]\n  C --> D[Aggregator/State]\n  D --> E[Checkpoints]\n  E --> F[Downstream Consumers]","difficulty":"intermediate","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","DoorDash"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T22:34:09.513Z","createdAt":"2026-01-18T22:34:09.514Z"},{"id":"q-682","question":"In a service handling image uploads, each file triggers a resize and thumbnail generation pipeline. Design a bounded producer-consumer queue in Python using asyncio. Use a Queue with maxsize, a fixed number of worker coroutines, and backpressure so producers await when full. Include clean shutdown and error handling. Provide a runnable minimal example showing enqueue, worker loop, and cancellation?","answer":"Use an asyncio.Queue(maxsize=128) with 4 worker coroutines. Producers await queue.put(task) to apply backpressure. Each worker pops tasks, processes image transforms, then calls queue.task_done(). On ","explanation":"## Why This Is Asked\n\nAssesses practical concurrency skills: bounded queues, worker pools, backpressure, and robust shutdown in a real feature like image processing.\n\n## Key Concepts\n\n- asyncio.Queue(maxsize) and backpressure\n- fixed-size worker pool\n- graceful shutdown and cancellation\n- error handling and observability\n\n## Code Example\n\n```python\n# Minimal outline of the pattern (not a full solution)\nimport asyncio\n\nclass Task:\n    def __init__(self, data): self.data = data\n\nasync def worker(q):\n    while True:\n        t = await q.get()\n        try:\n            # process t\n            pass\n        finally:\n            q.task_done()\n\nasync def main():\n    q = asyncio.Queue(maxsize=128)\n    workers = [asyncio.create_task(worker(q)) for _ in range(4)]\n    # enqueue tasks\n    for item in range(10):\n        await q.put(Task(item))\n    await q.join()\n    for w in workers: w.cancel()\n\nasyncio.run(main())\n```\n\n## Follow-up Questions\n\n- How would you adjust for burst traffic without starving producers?\n- How can you monitor queue depth and worker latency in production?","diagram":null,"difficulty":"beginner","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Lyft","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T15:58:30.388Z","createdAt":"2026-01-11T15:58:30.388Z"},{"id":"q-687","question":"Write a small Python snippet: create a shared counter initialized to 0, spawn 4 threads that increment it 1000 times each, using a Lock to protect the increment. After joining, print the final value. Explain what happens if the lock is removed and how atomicity is ensured. What value do you expect and why?","answer":"Use a Lock around the increment: with lock: counter += 1 in each thread. Spawn 4 threads, each loops 1000 times. Final value should be 4000. Without the lock, race conditions may yield a final value l","explanation":"## Why This Is Asked\n\nThis checks understanding of race conditions, mutual exclusion, and practical use of locks in a tiny concurrency exercise.\n\n## Key Concepts\n\n- Race conditions and protection via locks\n- Atomicity in read-modify-write sequences\n- Trade-offs of locks vs lock-free structures\n\n## Code Example\n\n```python\nimport threading\ncounter = 0\nlock = threading.Lock()\ndef worker():\n    global counter\n    for _ in range(1000):\n        with lock:\n            counter += 1\nthreads = [threading.Thread(target=worker) for _ in range(4)]\nfor t in threads: t.start()\nfor t in threads: t.join()\nprint(counter)\n```\n\n## Follow-up Questions\n\n- What happens if a thread raises an exception inside the critical section?\n- How would you test this under CPU-bound vs I/O-bound workloads?","diagram":"flowchart TD\n  A[Start] --> B[Create 4 threads]\n  B --> C[Each thread increments 1000x]\n  C --> D[Acquire lock for increment]\n  D --> E[Increment counter]\n  E --> F[Join threads]\n  F --> G[Print final value]\n  G --> H[End]","difficulty":"beginner","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Tesla","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T16:22:26.189Z","createdAt":"2026-01-11T16:22:26.189Z"},{"id":"q-696","question":"Context: in a real-time analytics pipeline for a video-conference system, dozens of producers emit events into a shared queue and multiple workers consume them. Implement a bounded, multi-producer, multi-consumer queue with capacity 1024. It must not drop messages while not full, block producers when full, support concurrent consumers, and support a clean shutdown. Describe API, invariants, and a simple synchronization strategy?","answer":"Use a fixed 1024-slot circular buffer guarded by a ReentrantLock with two Conditions: notFull and notEmpty. Producers await notFull; consumers await notEmpty. Update head, tail, and count under lock; ","explanation":"## Why This Is Asked\n- Tests practical concurrency design under load with backpressure and shutdown semantics. **Expects** concrete data structures and synchronization choices.\n\n## Key Concepts\n- **Thread-safety**, **bounded buffers**, and **backpressure**\n- Correct handling of spurious wakeups\n- Liveness guarantees and clean shutdown\n\n## Code Example\n\n```java\nclass BoundedMPMCQueue<T> {\n  private final Object[] buf; private int head=0, tail=0, count=0;\n  private final ReentrantLock lock = new ReentrantLock();\n  private final Condition notFull = lock.newCondition();\n  private final Condition notEmpty = lock.newCondition();\n  private volatile boolean shutdown = false;\n\n  public BoundedMPMCQueue(int capacity){ buf = new Object[capacity]; }\n\n  public void enqueue(T item) throws InterruptedException {\n    lock.lock();\n    try {\n      while (count == buf.length && !shutdown) notFull.await();\n      if (shutdown) throw new InterruptedException();\n      buf[tail] = item; tail = (tail+1) % buf.length; count++;\n      notEmpty.signalAll();\n    } finally { lock.unlock(); }\n  }\n\n  public T dequeue() throws InterruptedException {\n    lock.lock();\n    try {\n      while (count == 0 && !shutdown) notEmpty.await();\n      if (shutdown && count == 0) return null;\n      @SuppressWarnings(\"unchecked\") T item = (T) buf[head]; buf[head] = null; head = (head+1) % buf.length; count--;\n      notFull.signalAll();\n      return item;\n    } finally { lock.unlock(); }\n  }\n\n  public void shutdown() {\n    lock.lock(); try { shutdown = true; notFull.signalAll(); notEmpty.signalAll(); } finally { lock.unlock(); }\n  }\n}\n```\n\n## Follow-up Questions\n- How would you test for fairness and avoid starvation?\n- How would you adapt this to multiple machines or queue reuse across restarts?","diagram":null,"difficulty":"intermediate","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T17:17:04.688Z","createdAt":"2026-01-11T17:17:04.688Z"},{"id":"q-708","question":"Design a bounded, concurrent, multi-priority work scheduler for a rendering service: 3 priority levels (0 highest). Producers enqueue tasks into per-priority queues with backpressure; workers drain from the highest non-empty queue (0→1→2). Include aging to prevent starvation and a graceful shutdown sentinel. Provide a runnable minimal Java example?","answer":"Implement a bounded, multi-priority scheduler with three per-priority queues (0 high). Producers call offer with a timeout to apply backpressure. Workers poll queues in priority order (0, then 1, then","explanation":"## Why This Is Asked\nTests ability to design correct, robust concurrency under backpressure with priority aging and graceful shutdown.\n\n## Key Concepts\n- Bounded queues and backpressure\n- Priority-aware scheduling\n- Fairness via aging\n- Graceful shutdown and cancellation\n\n## Code Example\n```java\nimport java.util.concurrent.*;\n\nclass WorkItem { final int priority; final Runnable task; WorkItem(int p, Runnable t){priority=p;this.task=t;} void run(){task.run();} }\n\npublic class MultiPriorityScheduler {\n  private final BlockingQueue<WorkItem>[] queues = new BlockingQueue[3];\n  private volatile boolean running = true;\n\n  @SuppressWarnings(\"unchecked\")\n  public MultiPriorityScheduler(int cap){\n    for(int i=0;i<3;i++) queues[i] = new ArrayBlockingQueue<>(cap);\n  }\n\n  public boolean submit(WorkItem w, long timeout, TimeUnit unit) throws InterruptedException {\n    return queues[w.priority].offer(w, timeout, unit);\n  }\n\n  public void start(int workerCount){\n    for(int i=0;i<workerCount;i++){\n      new Thread(() -> {\n        while(running){\n          WorkItem w=null;\n          for(int p=0;p<3;p++){\n            w = queues[p].poll(50, TimeUnit.MILLISECONDS);\n            if (w != null){ w.run(); break; }\n          }\n        }\n      }).start();\n    }\n  }\n\n  public void stop(){ running = false; }\n}\n```\n\n## Follow-up Questions\n- How would you implement aging with constant time bounds?\n- How would you adapt this for multiple machines?","diagram":"flowchart TD\n  P[Producer] --> Q0[Queue0]\n  P --> Q1[Queue1]\n  P --> Q2[Queue2]\n  W[Worker] --> Q0\n  W --> Q1\n  W --> Q2\n  Q0 -- has task --> W\n  Q1 -- has task --> W\n  Q2 -- has task --> W","difficulty":"advanced","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Salesforce","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T18:31:21.490Z","createdAt":"2026-01-11T18:31:21.490Z"},{"id":"q-713","question":"Implement a thread-safe bounded queue using a fixed circular buffer with mutex protection and condition variables for coordinating producers and consumers. How would you handle enqueue/dequeue operations and implement graceful shutdown when the queue is no longer accepting new tasks?","answer":"Use a mutex to protect shared state (head, tail, count) and two condition variables: not_full for producers to wait when the buffer is full, and not_empty for consumers to wait when empty. During shutdown, signal all waiting threads and return special values to indicate termination. For enqueue: acquire mutex, wait while full, add item, update tail, signal not_empty, release mutex. For dequeue: acquire mutex, wait while empty and not shutdown, remove item, update head, signal not_full, release mutex. Shutdown sets flag, signals both condition variables, and operations return None/null to indicate termination.","explanation":"## Why This Is Asked\nThis question tests fundamental concurrency concepts including mutex synchronization, condition variables for thread coordination, and graceful shutdown patterns - all essential for building reliable concurrent systems.\n\n## Key Concepts\n- **Mutex**: Protects shared state (head, tail, count, buffer) from race conditions\n- **Condition Variables**: not_full blocks producers when buffer is full, not_empty blocks consumers when empty\n- **Circular Buffer**: Efficient fixed-size storage using modulo arithmetic for wrap-around\n- **Shutdown Handling**: Clean termination by signaling all waiting threads\n\n## Implementation Details\n```python\nimport threading\nimport time\n\nclass BoundedQueue:\n    def __init__(self, capacity):\n        self.capacity = capacity\n        self.buffer = [None] * capacity\n        self.head = 0  # dequeue position\n        self.tail = 0  # enqueue position\n        self.count = 0\n        self.mutex = threading.Lock()\n        self.not_full = threading.Condition(self.mutex)\n        self.not_empty = threading.Condition(self.mutex)\n        self.shutdown = False\n    \n    def enqueue(self, item):\n        with self.not_full:\n            while self.count == self.capacity and not self.shutdown:\n                self.not_full.wait()\n            \n            if self.shutdown:\n                return False  # Indicate shutdown\n            \n            self.buffer[self.tail] = item\n            self.tail = (self.tail + 1) % self.capacity\n            self.count += 1\n            \n            self.not_empty.notify()\n            return True\n    \n    def dequeue(self):\n        with self.not_empty:\n            while self.count == 0 and not self.shutdown:\n                self.not_empty.wait()\n            \n            if self.shutdown and self.count == 0:\n                return None  # Indicate shutdown\n            \n            item = self.buffer[self.head]\n            self.buffer[self.head] = None  # Clear reference\n            self.head = (self.head + 1) % self.capacity\n            self.count -= 1\n            \n            self.not_full.notify()\n            return item\n    \n    def graceful_shutdown(self):\n        with self.mutex:\n            self.shutdown = True\n            self.not_full.notify_all()\n            self.not_empty.notify_all()\n```\n\n## Shutdown Behavior\n- **Producers**: enqueue() returns False when shutdown is active\n- **Consumers**: dequeue() returns None when queue is empty AND shutdown is active\n- **Signal Broadcasting**: notify_all() wakes all waiting threads for clean exit\n- **Idempotent**: Multiple shutdown calls are safe\n\n## Usage Pattern\n```python\n# Producer thread\ndef producer(queue):\n    for i in range(100):\n        if not queue.enqueue(i):\n            print(\"Producer shutting down\")\n            break\n\n# Consumer thread  \ndef consumer(queue):\n    while True:\n        item = queue.dequeue()\n        if item is None:\n            print(\"Consumer shutting down\")\n            break\n        process(item)\n\n# Graceful shutdown\nqueue.graceful_shutdown()\n```","diagram":null,"difficulty":"beginner","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Cloudflare","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":["mutex protection","condition variables","circular buffer","thread coordination","graceful shutdown","bounded queue","producers consumers","shared state","race conditions","modulo arithmetic","waiting threads","special values"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2026-01-18T04:59:04.221Z","createdAt":"2026-01-11T19:17:18.492Z"},{"id":"q-720","question":"Design a bounded worker pool for a high-throughput API gateway that queues work with backpressure. Use a lock-free ring buffer, N workers, and blocking enqueue when full. Include per-task cancellation, timeouts, and metrics. Compare Go, Rust, and Java trade-offs and explain how you’d debug data races and starvation under burst traffic?","answer":"Implement a bounded worker pool with a fixed number of workers and a lock-free ring buffer. Block enqueue when full to enforce backpressure; use per-task cancellation via context and per-task timeouts","explanation":"## Why This Is Asked\n\nAssesses practical understanding of bounded concurrency, backpressure strategies, and race debugging in high-throughput services. The candidate should discuss data-race mitigation, perf instrumentation, and cross-language trade-offs relevant to modern back-end systems like those at Square and Hugging Face.\n\n## Key Concepts\n\n- Bounded queues and backpressure\n- Lock-free ring buffers with atomic indices\n- Task cancellation and timeouts\n- Metrics: p50/p95 latency, queue depth, throughput\n- Race detection and starvation debugging\n- Language trade-offs: Go, Rust, Java\n\n## Code Example\n\n```go\npackage main\n\ntype Task struct{ /*...*/ }\n\ntype RingBuffer struct {\n  buf  []Task\n  head uint64\n  tail uint64\n  mask uint64\n}\n\nfunc (r *RingBuffer) Push(t Task) bool { /* lock-free push */ }\nfunc (r *RingBuffer) Pop() (Task, bool) { /* lock-free pop */ }\n```\n\n## Follow-up Questions\n\n- How would you handle backpressure when producers vastly outpace consumers?\n- What diagnostics would you run to detect and fix data races in the hot path?","diagram":"flowchart TD\n  A[Submit Task] --> B{Queue Full?}\n  B -- Yes --> C[Block Enqueue]\n  B -- No --> D[Enqueue Task]\n  D --> E[Worker Picks Up]\n  E --> F[Process Task]\n  F --> G[Task Complete]\n","difficulty":"intermediate","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T20:22:26.591Z","createdAt":"2026-01-11T20:22:26.591Z"},{"id":"q-726","question":"Design a concurrent event broker for a live chat service that guarantees per-user in-order delivery while allowing parallel processing across users, using a bounded buffer; describe backpressure handling when the buffer fills, and compare locking versus lock-free approaches in your language of choice?","answer":"Design uses per-user FIFO queues feeding a shared dispatcher with a fixed-size worker pool. Each user has a bounded queue; producers enqueue non-blockingly and back off or drop messages when full. Dis","explanation":"## Why This Is Asked\n\nReal-world chat backends require per-user ordering with high throughput and minimal tail latency. This question probes ability to design scalable concurrency primitives, reason about backpressure, and compare locking vs lock-free trade-offs.\n\n## Key Concepts\n\n- Per-user FIFO ordering\n- Bounded buffers and backpressure\n- Lock granularity vs lock-free structures\n- Throughput vs latency under saturation\n- Testing strategies for tails and fairness\n\n## Code Example\n\n```go\ntype Message struct { UserID int; Payload []byte }\nfunc enqueue(q chan Message, m Message) bool {\n  select { case q <- m: return true; default: return false }\n}\n```\n\n## Follow-up Questions\n\n- How would you extend for fairness across many users with bursty traffic?\n- What backpressure policy would you adjust in production and why?","diagram":null,"difficulty":"intermediate","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Microsoft","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T21:17:03.063Z","createdAt":"2026-01-11T21:17:03.063Z"},{"id":"q-735","question":"Design a concurrency-safe per-channel message queue for a Discord-like chat service: multiple producers push messages, a single consumer persists them to storage; implement bounded capacity, preserve per-channel order, and handle backpressure. Which synchronization primitives would you use and how would you test edge cases?","answer":"Use a per-channel bounded queue (ring buffer) guarded by a mutex, with two condition variables: notFull and notEmpty. Producers block on full; a single consumer dequeues in arrival order and persists ","explanation":"## Why This Is Asked\nConcurrency is central to both Discord-like chat and Snowflake-like storage. This question probes ability to map a real production pattern (per-channel queues) to concrete synchronization, backpressure, and ordering guarantees.\n\n## Key Concepts\n- Producer-consumer pattern\n- Bounded buffers and backpressure\n- Mutexes and condition variables\n- Per-channel ordering and fault handling\n\n## Code Example\n\n```javascript\nclass BoundedQueue {\n  constructor(capacity) {\n    this.capacity = capacity;\n    this.buf = new Array(capacity);\n    this.head = 0;\n    this.tail = 0;\n    this.size = 0;\n    this.mux = new Mutex();\n    this.notFull = new ConditionVariable();\n    this.notEmpty = new ConditionVariable();\n  }\n  enqueue(item) {\n    // acquire mutex; wait while size === capacity; store at tail; tail=(tail+1)%capacity; size++; notEmpty.signal();\n  }\n  dequeue() {\n    // acquire mutex; wait while size === 0; read from head; head=(head+1)%capacity; size--; notFull.signal();\n  }\n}\n```\n\n## Follow-up Questions\n- How would you test backpressure and ordering across bursty producers?\n- How would you extend to multiple consumers while preserving per-channel order?","diagram":null,"difficulty":"beginner","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T22:19:16.416Z","createdAt":"2026-01-11T22:19:16.416Z"},{"id":"q-748","question":"You're building a real-time analytics pipeline with many shards. Propose a concurrent, bounded path that preserves per-shard in-order processing while enabling cross-shard parallelism. Design a data structure and protocol (enqueuing, routing, backpressure, and shutdown). Provide a runnable sketch in Go or Rust showing enqueue, per-shard consumer loop, and graceful shutdown?","answer":"Route by shard to per-shard fixed-size ring buffers; each shard has a single in-order consumer. Producers hash the shard key and enqueue; if any buffer is full, block until space frees (backpressure).","explanation":"## Why This Is Asked\nTests ability to design scalable, shard-aware, in-order processing with backpressure and clean shutdown in a high-throughput concurrency context.\n\n## Key Concepts\n- Bounded queues\n- Per-shard ordering\n- Backpressure\n- Lock-free synchronization\n- Graceful shutdown\n\n## Code Example\n```go\n// skeleton concurrent ring buffer for a shard\ntype Ring[T any] struct { /* ... */ }\nfunc (r *Ring[T]) Enqueue(v T) bool { /* ... */ }\nfunc (r *Ring[T]) Dequeue() (T, bool) { /* ... */ }\n```\n\n## Follow-up Questions\n- How would you handle dynamic shard rebalancing?\n- How to detect and recover from a stuck consumer?","diagram":"flowchart TD\n  P[Producers] -->|route by shard| S1[Shard 1 Buffer] --> C1[Shard 1 Consumer]\n  P --> S2[Shard 2 Buffer] --> C2[Shard 2 Consumer]\n  Shutdown --> C1\n  Shutdown --> C2","difficulty":"advanced","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["NVIDIA","Snap","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T23:21:27.457Z","createdAt":"2026-01-11T23:21:27.457Z"},{"id":"q-756","question":"Implement a concurrent task-graph executor with dependencies. Tasks form a DAG; a task runs only after all its prerequisites complete. Use a bounded in-flight task pool, per-worker work stealing, and deadlock/backpressure handling. Provide a runnable Go or Rust sketch showing submission, ready-state transitions, worker loop, and graceful shutdown?","answer":"Use a dependency counter per task and a bounded global ready queue; each worker pops from its local deque and steals from others when idle. On task completion, decrement indegree of dependents and enq","explanation":"## Why This Is Asked\nTests understanding of DAG-based scheduling, readiness, backpressure, and graceful shutdown in a concurrent context.\n\n## Key Concepts\n- DAG validation and cycle detection\n- Readiness counters and per-task in-degree\n- Work-stealing and per-worker caches\n- Bounded in-flight with backpressure\n- Safe shutdown and replay considerations\n\n## Code Example\n```javascript\n// Implementation sketch in JS (conceptual)\nclass Task { constructor(id, deps, exec){ this.id=id; this.deps=deps; this.exec=exec; } }\nclass DAGExecutor { constructor(maxInFlight){ this.maxInFlight=maxInFlight; /* ... */ } submit(tasks){ /* validate DAG, init indegrees */ } run(){ /* start workers, process ready tasks, steal, and exit on shutdown */ } shutdown(){ /* signal and wait */ } }\n```\n\n## Follow-up Questions\n- How would you extend for dynamic dependency changes?\n- How would you add deterministic replay logging for debugging?","diagram":"flowchart TD\n  A[Submit Task] --> B[Build Indegree]\n  B --> C{Ready?}\n  C -->|Yes| D[Enqueue to Ready Pool]\n  C -->|No| E[Wait for Dependencies]","difficulty":"intermediate","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T01:33:36.133Z","createdAt":"2026-01-12T01:33:36.134Z"},{"id":"q-759","question":"In a multi-threaded microservice, there is a shared in-memory counter for total processed events. Provide a concrete, beginner-friendly approach to implement a thread-safe increment using language primitives (Java, Go, or Python) and discuss the trade-offs between lock-based vs lock-free solutions when scaling across cores?","answer":"Use language-supported atomic operations or a minimal lock to guard the counter. For Java: counter is AtomicLong; call incrementAndGet after each event. For Go: use atomic.AddUint64; for CPython: prot","explanation":"## Why This Is Asked\n\nInterview context explanation.\n\n## Key Concepts\n\n- Atomic operations\n- Memory visibility\n- Contention and scalability\n- Language constructs (AtomicLong, atomic.Add, threading.Lock)\n\n## Code Example\n\n```javascript\n// Java-like atomic usage (illustrative)\nclass AtomicLong {\n  constructor(n=0){ this.v = n; }\n  incrementAndGet(){ return ++this.v; }\n}\n```\n\n## Follow-up Questions\n\n- How would you test for race conditions in this setup?\n- What would you change if there are multiple processes instead of threads?","diagram":null,"difficulty":"beginner","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Coinbase","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T03:48:34.876Z","createdAt":"2026-01-12T03:48:34.876Z"},{"id":"q-773","question":"In a Go HTTP server, you maintain a per-user quota counter in memory. Multiple requests for different users should advance concurrently, but updates to the same user must be serialized. Design a striped lock (a fixed set of mutexes) to guard a map[string]int, map userID to a bucket via hashing, and explain how you minimize contention, handle hash collisions, and ensure correctness when entries may be evicted?","answer":"Use a striped lock: a fixed-size slice of mutexes guards a map[string]int quotas. Hash userID to bucket index, lock bucket, read-modify-write, unlock. This lowers contention vs a global lock. Collisio","explanation":"## Why This Is Asked\n\nTests understanding of fine-grained synchronization and practical trade-offs for high-concurrency in-memory state.\n\n## Key Concepts\n\n- Striped locking for reduced contention\n- Lock granularity vs global lock\n- Handling hash collisions with buckets\n- Eviction strategies and correctness\n\n## Code Example\n\n```javascript\n// Go-like pseudocode demonstrating striped locks\ntype Striped struct {\n  locks []sync.Mutex\n  quotas map[string]int\n}\nfunc NewStriped(n int) *Striped { ... }\nfunc (s *Striped) Inc(id string) { idx := hash(id) % len(s.locks); s.locks[idx].Lock(); defer s.locks[idx].Unlock(); s.quotas[id]++ }\n```\n\n## Follow-up Questions\n\n- What happens if bucket count is too small? how to resize safely?\n- How would you extend to support read-heavy workloads (readers-writer locks)?","diagram":"flowchart TD\n  A[Request arrives] --> B[Hash userID to bucket]\n  B --> C[Acquire bucket lock]\n  C --> D[Read/Increment quota]\n  D --> E[Release lock]\n  E --> F[Respond]","difficulty":"beginner","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","NVIDIA","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T04:54:27.848Z","createdAt":"2026-01-12T04:54:27.848Z"},{"id":"q-780","question":"Implement a lock-free, bounded multi-producer, multi-consumer pipeline with per-symbol partitions and fixed-capacity queues. Producers must acquire credits to enqueue; downstream workers release credits on completion. Explain memory visibility, false sharing avoidance, and a clean shutdown. How do you guarantee in-order per partition and exactly-once processing on failure?","answer":"Propose a per-symbol partitioned, bounded ring-buffer with credit-based flow. Producers atomically decrement a per-partition credit counter to enqueue, avoiding locks; consumers advance a per-partitio","explanation":"## Why This Is Asked\nThis checks ability to design lock-free structures, backpressure, and fault-tolerant guarantees across partitions in high-throughput services.\n\n## Key Concepts\n- Lock-free queues and per-partition backpressure\n- Memory visibility, false sharing avoidance, cache alignment\n- Clean shutdown and exactly-once semantics\n\n## Code Example\n```go\n// skeleton illustrating credit-based enqueue\n```\n\n## Follow-up Questions\n- How would you test correctness under bursty traffic\n- How would you extend to cross-node coordination with minimal latency","diagram":null,"difficulty":"advanced","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Microsoft","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T05:31:56.180Z","createdAt":"2026-01-12T05:31:56.180Z"},{"id":"q-788","question":"In a streaming data pipeline, multiple producers generate frames and a bounded buffer sits between the producer stage and a consumer stage. Implement a backpressure-enabled producer-consumer pair in JavaScript (Node.js) using an async bounded queue that supports multiple producers and multiple consumers. It must guarantee no data loss, bounded memory, and graceful shutdown. How would you test under varying production/consumption rates?","answer":"Use a bounded queue with capacity N; producers await when full and consumers await when empty to enforce backpressure and bounded memory. For shutdown, push a sentinel or close the queue so workers ex","explanation":"## Why This Is Asked\nTests understanding of backpressure, bounded buffers, and graceful shutdown in a concurrent environment using asynchronous primitives.\n\n## Key Concepts\n- Bounded buffers and backpressure\n- Multiple producers and multiple consumers\n- Graceful shutdown signaling\n- Correct testing under varying rates\n\n## Code Example\n```javascript\nclass BoundedQueue {\n  constructor(capacity) {\n    this.capacity = capacity\n    this.queue = []\n    this.waitPuts = []\n    this.waitGets = []\n  }\n  async push(item) {\n    if (this.queue.length < this.capacity) {\n      this.queue.push(item)\n      if (this.waitGets.length) this.waitGets.shift()(this.queue.shift())\n      return\n    }\n    await new Promise(res => this.waitPuts.push(res))\n    return this.push(item)\n  }\n  async pop() {\n    if (this.queue.length) {\n      const item = this.queue.shift()\n      if (this.waitPuts.length) this.waitPuts.shift()\n      return item\n    }\n    return new Promise(resolve => this.waitGets.push(resolve))\n  }\n}\n// Usage sketch: create producers/consumers using q.push(q.pop())\n```\n\n## Follow-up Questions\n- How would you adapt for a single consumer vs multiple consumers?\n- What strategies exist if you must drop data when the buffer is full, and how would you test them?","diagram":null,"difficulty":"beginner","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T06:42:23.564Z","createdAt":"2026-01-12T06:42:23.564Z"},{"id":"q-795","question":"Design a dynamic, concurrency-safe worker pool in Go for a real-time analytics pipeline. Each task type has its own queue; enforce backpressure with bounded channels and implement a central scaler that adjusts worker counts based on observed tail latency and throughput. Provide a runnable skeleton showing enqueue, worker loops, and graceful shutdown?","answer":"Use per-type bounded channels to enforce backpressure, a dispatcher routing tasks to each type queue, and a central scaler that monitors tail latency and throughput to adjust worker counts via atomic ","explanation":"## Why This Is Asked\n\nAdvanced concurrency design focusing on dynamic scaling and per-type queues, avoiding contention and starvation in a real-time analytics pipeline.\n\n## Key Concepts\n\n- Per-type queues and backpressure\n- Dynamic worker scaling with latency feedback\n- Safe shutdown and fault tolerance\n- Context cancellation and coordination\n\n## Code Example\n\n```go\npackage main\nimport (\n  \"context\"\n  \"fmt\"\n  \"sync\"\n  \"time\"\n)\ntype Task struct{ Type string; Payload interface{} }\nfunc main() {\n  ctx, cancel := context.WithCancel(context.Background())\n  defer cancel()\n\n  queues := map[string]chan Task{\n    \"CPU\": make(chan Task, 100),\n    \"IO\":  make(chan Task, 100),\n  }\n\n  var wg sync.WaitGroup\n  // start workers\n  for t := range queues {\n    for i := 0; i < 2; i++ {\n      wg.Add(1)\n      go worker(ctx, t, queues[t], &wg)\n    }\n  }\n\n  // simple enqueue\n  queues[\"CPU\"] <- Task{Type: \"CPU\"}\n  queues[\"IO\"] <- Task{Type: \"IO\"}\n\n  time.Sleep(100 * time.Millisecond)\n  cancel()\n  for _, ch := range queues { close(ch) }\n  wg.Wait()\n}\nfunc worker(ctx context.Context, t string, ch <-chan Task, wg *sync.WaitGroup) {\n  defer wg.Done()\n  for {\n    select {\n    case <-ctx.Done():\n      return\n    case task, ok := <-ch:\n      if !ok { return }\n      time.Sleep(10 * time.Millisecond)\n      fmt.Println(\"done\", t)\n    }\n  }\n}\n```\n\n## Follow-up Questions\n\n- How would you test starvation scenarios and balance fairness?\n- How would you extend to dozens of task types and maintain stable latency?\n","diagram":null,"difficulty":"advanced","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Meta","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T07:29:49.914Z","createdAt":"2026-01-12T07:29:49.914Z"},{"id":"q-804","question":"You're building a video-frame ingestion service where frames from thousands of users arrive on a shared input channel. Design a bounded, concurrent dispatcher that routes frames to per-user in-order queues, enabling parallel processing across users. Provide backpressure handling when the global buffer fills, a strategy for reordering out-of-order frames, and a graceful shutdown. Sketch the core data structures and synchronization in Rust or C++ and explain trade-offs?","answer":"Approach: a bounded global input buffer fans frames to per-user in-order queues. Each frame carries user_id and seq. A per-user sequencer buffers out-of-order frames in a small heap until gaps close, ","explanation":"## Why This Is Asked\nExplores practical concurrency challenges in real-time ingestion: per-key in-order delivery, bounded buffers, reordering, and safe shutdown.\n\n## Key Concepts\n- Bounded buffers and backpressure\n- Per-user in-order delivery with out-of-order buffering\n- Per-key sequencing and memory visibility\n- Graceful shutdown and drain guarantees\n\n## Code Example\n```rust\nstruct Frame { user_id: u64, seq: u64, payload: Vec<u8> }\nstruct PerUserQueue { next_seq: u64, pending: std::collections::BinaryHeap<Frame> }\n```\n\n## Follow-up Questions\n- How would you test under jitter and loss?\n- How would you scale to 100k active users with limited memory?\n","diagram":"flowchart TD\n  A[Shared Input] --> B[Route by user_id]\n  B --> C[Per-user Queues]\n  C --> D[Worker Threads]\n  A --> E[Backpressure Monitor]","difficulty":"intermediate","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","Netflix","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T08:34:55.743Z","createdAt":"2026-01-12T08:34:55.743Z"},{"id":"q-811","question":"In a real-time analytics service, multiple producers enqueue data into a fixed-size circular buffer consumed by multiple workers. Design a beginner-friendly concurrency solution in Java, Go, or Python that guarantees producers block when the buffer is full and consumers block when empty, while maintaining correctness under concurrent access; compare lock-based vs channel-based approaches for this bounded buffer?","answer":"Use a bounded buffer API Put(item)/Get(). In Go, a buffered channel size N blocks producers when full and blocks consumers when empty. In Java, ArrayBlockingQueue<N> with put/take; Python: queue.Queue","explanation":"## Why This Is Asked\nInterviewers assess practical concurrency intuition with bounded buffers, a common real-time pattern.\n\n## Key Concepts\n- Bounded buffers\n- Producer-consumer synchronization\n- Blocking semantics\n- Trade-offs: channels vs locks vs lock-free\n\n## Code Example\n```java\nArrayBlockingQueue<Integer> q = new ArrayBlockingQueue<>(N);\nq.put(item); // blocks if full\nint x = q.take(); // blocks if empty\n```\n\n## Follow-up Questions\n- How would you handle graceful shutdown?\n- How would you introduce fairness guarantees or timeouts?","diagram":null,"difficulty":"beginner","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Oracle","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T09:35:47.460Z","createdAt":"2026-01-12T09:35:47.460Z"},{"id":"q-818","question":"In a build pipeline, N compile tasks run in parallel and must all finish before the linker runs. Design a barrier that blocks each task at barrier.wait() until all N tasks have arrived, then releases them to proceed. Implement two beginner-friendly approaches in Go (or Java/Python): (A) locks/condition variables; (B) channels/futures. Ensure the barrier is reusable for repeated builds, avoids deadlocks, and explain correctness and trade-offs?","answer":"Two approaches: (A) Locks/Cond: track count, target N, and a generation value. Each worker increments count and waits while count < N; the last increments generation and broadcasts; all wake, verify g","explanation":"## Why This Is Asked\nTests practical synchronization primitives and reuse. It checks correctness under concurrent arrival and wake-up.\n\n## Key Concepts\n- Barrier, generation, spurious wakeups, reuse\n- Deadlock avoidance, fairness, memory usage\n\n## Code Example\n```go\n// barrier skeleton in Go\n```\n\n## Follow-up Questions\n- How would you test for fairness across rounds?\n- How would you adapt for mixed local and remote workers?","diagram":null,"difficulty":"beginner","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T10:27:36.782Z","createdAt":"2026-01-12T10:27:36.782Z"},{"id":"q-825","question":"In a real-time chat moderation pipeline for a platform like Discord/LinkedIn, design a dynamic Go worker pool that scales from minWorkers to maxWorkers based on a bounded task queue. The queue should backpressure producers, guarantee per-user fairness, support graceful shutdown, and tolerate occasional misbehaving workers (timeouts). Provide runnable code showing enqueue, worker loop, and scaler?","answer":"Use a bounded queue channel and a scalable worker pool. Start with minWorkers; a scaler grows workers when queue length crosses a high watermark and shrinks when it falls below a low watermark, bounde","explanation":"## Why This Is Asked\n\nTests ability to design a scalable, backpressured worker pool for real-time workloads, with fairness, fault tolerance, and clean shutdown—exactly the kind of pattern seen in large chat platforms.\n\n## Key Concepts\n\n- Bounded queue with backpressure\n- Dynamic worker scaling (min/max bounds)\n- Fairness across users (distribution by UserID)\n- Graceful shutdown and worker timeout handling\n\n## Code Example\n\n```go\npackage main\n\nimport (\n  \"context\"\n  \"fmt\"\n  \"hash/fnv\"\n  \"sync\"\n  \"time\"\n)\n\ntype Task struct {\n  UserID  string\n  Payload string\n}\n\nfunc worker(ctx context.Context, id int, tasks <-chan Task, wg *sync.WaitGroup) {\n  defer wg.Done()\n  for {\n    select {\n    case t := <-tasks:\n      // simulate processing\n      _ = t\n      time.Sleep(5 * time.Millisecond)\n      fmt.Printf(\"worker %d processed %s\\n\", id, t.Payload)\n    case <-ctx.Done():\n      return\n    }\n  }\n}\n\nfunc main() {\n  minW, maxW := 2, 6\n  queueCap := 64\n  queue := make(chan Task, queueCap)\n  var wg sync.WaitGroup\n  var cancels []context.CancelFunc\n\n  // start min workers\n  for i := 0; i < minW; i++ {\n    ctx, cancel := context.WithCancel(context.Background())\n    cancels = append(cancels, cancel)\n    wg.Add(1)\n    go worker(ctx, i, queue, &wg)\n  }\n\n  // scaler\n  go func() {\n    ticker := time.NewTicker(100 * time.Millisecond)\n    defer ticker.Stop()\n    for range ticker.C {\n      l := len(queue)\n      if l > 32 && len(cancels) < maxW {\n        // scale up\n        ctx, cancel := context.WithCancel(context.Background())\n        cancels = append(cancels, cancel)\n        w := len(cancels) - 1\n        wg.Add(1)\n        go worker(ctx, w, queue, &wg)\n      } else if l < 8 && len(cancels) > minW {\n        // scale down\n        cancel := cancels[len(cancels)-1]\n        cancels = cancels[:len(cancels)-1]\n        cancel()\n      }\n    }\n  }()\n\n  // producer: enqueue tasks\n  for i := 0; i < 1000; i++ {\n    t := Task{UserID: fmt.Sprintf(\"user-%d\", i%20), Payload: fmt.Sprintf(\"payload-%d\", i)}\n    queue <- t // blocks when full (backpressure)\n  }\n\n  // graceful shutdown\n  time.Sleep(1 * time.Second)\n  for _, c := range cancels {\n    c()\n  }\n  close(queue)\n  wg.Wait()\n}\n```\n\n## Follow-up Questions\n\n- How would you test scaling behavior under bursty traffic and ensure latency SLOs are met?\n- How would you extend fairness for thousands of users without starving rare users?","diagram":"flowchart TD\n  P[Producer] --> Q[Task Queue]\n  Q --> W[Worker Pool]\n  W --> M[Moderation Action]\n  subgraph Scale\n    SScaler[Scaler] --> W\n  end","difficulty":"advanced","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","LinkedIn"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T11:21:45.375Z","createdAt":"2026-01-12T11:21:45.375Z"},{"id":"q-835","question":"Design a multi-tenant dispatcher for a streaming event bus. Producers from each tenant push events into per-tenant input channels; a central dispatcher interleaves tenants fairly, enforces per-tenant rate limits using a token-bucket, and enforces a maximum in-flight events per tenant. Implement in Go using channels; ensure backpressure propagates to producers when quotas or in-flight limits are reached, and support graceful shutdown. Include a runnable minimal example with: per-tenant limiter, dispatcher loop, producer(s), and cancellation?","answer":"Implement per-tenant token buckets and in-flight limits, with a central fairness-driven dispatcher. Each tenant has a bucket (capacity, refill rate) and a per-tenant in-flight cap. The dispatcher non-","explanation":"## Why This Is Asked\nEvaluates ability to design a scalable, fair, multi-tenant concurrency mechanism with backpressure, not just a single queue.\n\n## Key Concepts\n- Per-tenant token bucket with capacity and refill\n- Per-tenant in-flight limits\n- Fair scheduling to prevent starvation\n- Backpressure propagation to producers\n- Graceful shutdown via context cancellation\n\n## Code Example\n\n```go\npackage main\n\nimport (\n  \"context\"\n  \"fmt\"\n  \"sync\"\n  \"time\"\n)\n\ntype Event struct{ Payload string }\n\ntype Bucket struct {\n  mu     sync.Mutex\n  tokens int\n  cap    int\n  refill int\n}\n\nfunc (b *Bucket) TryTake() bool {\n  b.mu.Lock()\n  defer b.mu.Unlock()\n  if b.tokens > 0 {\n    b.tokens--\n    return true\n  }\n  return false\n}\n\nfunc (b *Bucket) Refund(n int) {\n  b.mu.Lock()\n  b.tokens += n\n  if b.tokens > b.cap {\n    b.tokens = b.cap\n  }\n  b.mu.Unlock()\n}\n\nfunc (b *Bucket) StartRefill(interval time.Duration) {\n  go func() {\n    ticker := time.NewTicker(interval)\n    for range ticker.C {\n      b.mu.Lock()\n      if b.tokens < b.cap {\n        b.tokens += b.refill\n        if b.tokens > b.cap { b.tokens = b.cap }\n      }\n      b.mu.Unlock()\n    }\n  }()\n}\n\ntype Tenant struct {\n  id           string\n  in           chan Event\n  bucket       *Bucket\n  inFlight     int\n  maxInFlight  int\n}\n\nfunc main() {\n  ctx, cancel := context.WithCancel(context.Background())\n  defer cancel()\n\n  tA := &Tenant{id: \"A\", in: make(chan Event, 32), bucket: &Bucket{tokens: 10, cap: 10, refill: 1}, maxInFlight: 3}\n  tB := &Tenant{id: \"B\", in: make(chan Event, 32), bucket: &Bucket{tokens: 8, cap: 8, refill: 1}, maxInFlight: 2}\n  tA.bucket.StartRefill(120 * time.Millisecond)\n  tB.bucket.StartRefill(120 * time.Millisecond)\n\n  tenants := []*Tenant{tA, tB}\n\n  // Simple dispatcher\n  go func() {\n    for {\n      select {\n      case <-ctx.Done():\n        return\n      default:\n      }\n      for _, t := range tenants {\n        if t.inFlight >= t.maxInFlight {\n          continue\n        }\n        if t.bucket.TryTake() {\n          select {\n          case e := <-t.in:\n            t.inFlight++\n            go func(ev Event, tt *Tenant) {\n              time.Sleep(50 * time.Millisecond)\n              tt.inFlight--\n              tt.bucket.Refund(1)\n              fmt.Printf(\"Tenant %s processed %s\\n\", tt.id, ev.Payload)\n            }(e, t)\n          default:\n            t.bucket.Refund(1)\n          }\n        }\n      }\n      time.Sleep(1 * time.Millisecond)\n    }\n  }()\n\n  // Producers with backpressure\n  go func() {\n    for i := 0; i < 100; i++ {\n      tA.in <- Event{Payload: fmt.Sprintf(\"A-%d\", i)}\n      time.Sleep(20 * time.Millisecond)\n    }\n  }()\n  go func() {\n    for i := 0; i < 60; i++ {\n      tB.in <- Event{Payload: fmt.Sprintf(\"B-%d\", i)}\n      time.Sleep(30 * time.Millisecond)\n    }\n  }()\n\n  // Run for a while then shutdown\n  time.Sleep(5 * time.Second)\n  cancel()\n  time.Sleep(200 * time.Millisecond)\n}\n```\n\n## Follow-up Questions\n- How would you adapt the design to N tenants with dynamic quotas?\n- How would you measure and optimize tail latency under bursty traffic?","diagram":null,"difficulty":"advanced","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","NVIDIA","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T12:48:08.710Z","createdAt":"2026-01-12T12:48:08.710Z"}],"subChannels":["general"],"companies":["Adobe","Airbnb","Amazon","Apple","Bloomberg","Citadel","Cloudflare","Coinbase","Databricks","Discord","DoorDash","Google","Hashicorp","Hugging Face","Instacart","LinkedIn","Lyft","Meta","Microsoft","MongoDB","NVIDIA","Netflix","OpenAI","Oracle","PayPal","Plaid","Robinhood","Salesforce","Scale Ai","Slack","Snap","Snowflake","Square","Stripe","Tesla","Twitter","Two Sigma","Zoom"],"stats":{"total":42,"beginner":14,"intermediate":16,"advanced":12,"newThisWeek":31}}