{"questions":[{"id":"q-1080","question":"In a log-processing pipeline, multiple producers enqueue log entries into a bounded, rate-limited work queue. Implement a beginner-friendly token-bucket rate limiter that allows enqueuing only when a token is available; a background timer refills tokens at a fixed rate up to a max. Producers block when tokens==0; workers process items from the queue. Provide a concrete Go/Python/Java solution and discuss testing?","answer":"An ideal answer demonstrates a bounded queue plus a token bucket. A mutex guards an int tokens and a CV; a ticker periodically adds tokens (capped). Each producer blocks until tokens>0, then decrement","explanation":"## Why This Is Asked\nThis explores practical synchronization for backpressure, a common real-world constraint in streaming services.\n\n## Key Concepts\n- Token bucket rate limiter with bounded queue\n- Mutexes, condition variables, and wakeups\n- Backpressure, fairness, and bounded memory\n\n## Code Example\n```javascript\n// Implementation code here\n```\n\n## Follow-up Questions\n- How would you test for bursty producers and ensure no starvation?\n- How would you adapt for multiple rate limits per producer or per client?","diagram":"flowchart TD\n  A[Producers] --> B[Bounded Queue]\n  B --> C[Workers]\n  subgraph TokenBucket\n    D[Tokens] -->|refilled| E[Producer Wake]\n  end","difficulty":"beginner","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Lyft","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T21:36:23.203Z","createdAt":"2026-01-12T21:36:23.203Z"},{"id":"q-1204","question":"In a real-time notification system with thousands of tenants, each tenant's events must be delivered to their own handler in order, while cross-tenant processing happens in parallel. Design a bounded, multi-queue architecture with per-tenant in-order guarantees, backpressure, and graceful shutdown. Compare two backpressure strategies (per-tenant token buckets vs. global credits) and discuss testing and failure scenarios. Provide runnable sketch in Go or Rust?","answer":"Use per-tenant bounded queues, fed by a central bounded dispatch buffer. Each tenant has a single consumer to preserve per-tenant order; cross-tenant work proceeds in parallel. Implement backpressure ","explanation":"## Why This Is Asked\n\nEvaluates the ability to design concurrency-aware systems that preserve per-tenant ordering while enabling cross-tenant parallelism, and to reason about backpressure strategies and graceful shutdowns.\n\n## Key Concepts\n\n- Per-tenant queues with in-order delivery\n- Bounded buffers and backpressure strategies\n- Cancellation/graceful shutdown and failure handling\n- Testing under saturation and late-arrival conditions\n\n## Code Example\n\n```go\n// A minimal sketch of structures (not a full impl)\ntype Event struct{ Tenant string; Payload []byte; TS int64 }\n\ntype TenantQueue struct{ Ch chan Event }\n\ntype Dispatcher struct {\n  Delegates map[string]*TenantQueue\n  GlobalBuf chan Event\n}\n```\n\n## Follow-up Questions\n\n- How would you detect and mitigate tenant-level starvation under skewed traffic?\n- How would you handle dynamic tenant churn (adding/removing tenants) without pausing processing?","diagram":null,"difficulty":"intermediate","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Scale Ai","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T04:51:31.188Z","createdAt":"2026-01-13T04:51:31.188Z"},{"id":"q-2080","question":"In a real-time chat system, multiple producers enqueue messages into per-room bounded queues and multiple consumers deliver to connected clients. Design a beginner-friendly concurrency solution (Go, Java, or Python) that guarantees producers block when a room's queue is full and consumers block when the queue is empty, while preserving per-room isolation and simple backpressure. Compare a mutex/condition-based queue versus a channel-based approach for bounded queues?","answer":"Implement a per-room bounded queue with blocking semantics: producers block when the queue is full, consumers block when empty. In Go, use a buffered channel per room; in Python/Java, use a bounded queue with mutex/condition variables for blocking behavior.","explanation":"## Why This Is Asked\n\nThis question evaluates practical concurrency design for real-time systems, focusing on bounded queues, blocking behavior, and per-room isolation—core patterns at companies like HashiCorp and Discord.\n\n## Key Concepts\n\n- Bounded queues with blocking semantics\n- Per-room isolation and backpressure\n- Channel-based versus lock-based synchronization\n- Correct testing across producers/consumers\n\n## Code Example\n\n```go\n// Go sketch: per-room bounded channel\ntype Room struct {\n  Msgs chan Message\n}\nfunc NewRoom(cap int) *Room { return &Room{Msgs: make(chan Message, cap)} }\n```\n\n```python\n# Python sketch: per-room bounded queue with locks/conditions\nfrom threading import Condition, Lock\n\nclass Room:\n    def __init__(self, capacity):\n        self.capacity = capacity\n        self.queue = []\n        self.lock = Lock()\n        self.not_full = Condition(self.lock)\n        self.not_empty = Condition(self.lock)\n    \n    def enqueue(self, message):\n        with self.not_full:\n            while len(self.queue) >= self.capacity:\n                self.not_full.wait()\n            self.queue.append(message)\n            self.not_empty.notify()\n    \n    def dequeue(self):\n        with self.not_empty:\n            while not self.queue:\n                self.not_empty.wait()\n            message = self.queue.pop(0)\n            self.not_full.notify()\n            return message\n```\n\n## Comparison: Mutex/Condition vs Channel-Based\n\n**Mutex/Condition (Python/Java):**\n- More explicit control over queue state\n- Easier to implement custom behavior (timeouts, priorities)\n- Higher code complexity and error-prone\n- Manual management of condition variables\n\n**Channel-Based (Go):**\n- Simpler, idiomatic Go concurrency\n- Built-in blocking semantics\n- Less error-prone, compiler-enforced usage\n- Limited flexibility for custom queue behavior\n\nBoth approaches provide the same blocking guarantees, but channels offer cleaner code for standard bounded queue patterns.","diagram":null,"difficulty":"beginner","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Hashicorp"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T04:27:12.373Z","createdAt":"2026-01-14T23:33:30.390Z"},{"id":"q-2329","question":"Design a bounded, concurrent router for a real-time analytics platform ingesting events from thousands of clients. Each client's events must be processed in arrival order, while events from different clients may be processed in parallel. The router should implement per-client backpressure and support clients joining/leaving on the fly. Describe two concrete implementations: (A) per-client queues with a central scheduler using locks, (B) per-client lock-free queues with a shared ready-list. Include correctness, failure handling, and a practical test plan?","answer":"Use N per-client FIFO queues and a fixed worker pool. Each client has a credit counter; producers enqueue only when credits>0, decrementing atomically. (A) Lock-based: guard each queue with a mutex; a","explanation":"## Why This Is Asked\nTests ability to design per-client ordering with global parallelism and backpressure, plus two concrete implementations and correctness reasoning.\n\n## Key Concepts\n- Per-client FIFO queues with bounded capacity\n- Backpressure via per-client credits\n- Locking vs. lock-free queue design\n- Dynamic client join/leave handling and fairness\n\n## Code Example\n```javascript\n// Skeleton illustrating per-client queues and a central scheduler (pseudo)\nclass Router {\n  constructor(capacity){/* ... */}\n  enqueue(clientId, item){/* ... */}\n  dequeue(workerId){/* ... */}\n}\n```\n\n## Follow-up Questions\n- How would you test ordering guarantees under bursty traffic?\n- How do you detect/handle starvation and ensure fairness across clients?\n- What failure scenarios require replay or idempotency guarantees?","diagram":null,"difficulty":"intermediate","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Instacart","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T13:05:58.777Z","createdAt":"2026-01-15T13:05:58.777Z"},{"id":"q-2542","question":"Design a two-stage, bounded-concurrency pipeline for real-time video frames from multiple cameras. Stage 1 decodes frames (CPU-bound) using a fixed thread pool; Stage 2 runs an AI-based enhancer asynchronously. Ensure per-frame ordering across all cameras, backpressure to keep queues bounded, support dynamic worker scaling and clean shutdown with cancellation, and provide a runnable minimal sketch in a language of your choice?","answer":"Implement two bounded queues: Q1 for decoding (N1 workers) and Q2 for enhancement (N2 workers). Attach per-frame sequence numbers to enforce global in-order delivery to Stage 2. Stage 1 runs on a fixed thread pool; Stage 2 uses async workers with dynamic scaling. Use semaphores for backpressure, cancellation tokens for shutdown, and a sequencer to maintain ordering across all cameras.","explanation":"## Why This Is Asked\n\nThis question probes real-world patterns for multi-stage pipelines, ordering guarantees, backpressure, dynamic scaling, and graceful shutdown across components. It highlights trade-offs between CPU-bound decoding and async I/O-bound processing, and tests ability to reason about cross-stage coordination.\n\n## Key Concepts\n\n- Bounded queues and backpressure\n- Ordering guarantees across concurrent stages\n- Thread pools vs async workers\n- Cancellation tokens and error propagation\n- Dynamic worker scaling\n\n## Code Example\n\n```javascript\n// Implementation code here\n```\n\n## Follow-up Considerations\n\nThe solution should address how to handle frame drops during overload scenarios, monitor queue depths for scaling decisions, and ensure proper resource cleanup during shutdown.","diagram":null,"difficulty":"advanced","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Google","Hugging Face"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T05:33:35.924Z","createdAt":"2026-01-15T22:30:36.615Z"},{"id":"q-2606","question":"Design an in-process, bounded-concurrency event bus for a streaming service that ingests events from hundreds of producers (per-video id) and delivers to a pool of workers while guaranteeing per-video in-order processing, minimal latency, and backpressure signaling to producers when queues fill. Include a runnable sketch in Go or Rust and discuss testing?","answer":"Partitioned ring buffers per video ID with one consumer per partition, managed by a global scheduler for hot partition balancing. Producers append with per-ID sequence numbers to preserve in-order delivery. Backpressure is implemented through bounded buffers that block producers when full, using condition variables or atomic counters for efficient signaling.","explanation":"## Why This Is Asked\nThis question evaluates your ability to design a practical, low-latency, bounded-concurrency event bus with per-key ordering and backpressure. It tests your understanding of concurrency primitives, scheduling strategies, and failure modes in distributed systems.\n\n## Key Concepts\n- Partitioning for data locality and parallelism\n- Per-key ordering with sequence numbers\n- Bounded buffers and backpressure strategies\n- Lock-free vs locking trade-offs; wakeups and shutdown\n\n## Code Example\n```javascript\n// Skeleton illustrating partitioned queues\nclass Partition {\n  constructor(","diagram":null,"difficulty":"intermediate","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Amazon","LinkedIn"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T05:02:33.098Z","createdAt":"2026-01-16T02:36:29.090Z"},{"id":"q-2658","question":"Design a bounded, fair, multi-producer/multi-consumer system for real-time order matching in a crypto exchange. Many producers submit limit orders; multiple workers attempt matches against an in-memory order book. Guarantee bounded memory, backpressure, and starvation-free fairness across producers (e.g., per-producer quotas or weighted fairness). Describe data structures, synchronization, and a minimal runnable Java snippet showing enqueue and a worker loop?","answer":"Implement per-producer bounded queues fed into a rotating, weighted fair dispatcher that pushes into a fixed-size ring buffer consumed by worker threads. Use a single-order-book lock for matching, wit","explanation":"## Why This Is Asked\nTests ability to design high-throughput, fair concurrency with bounded memory in a critical system like exchanges or dispatchers.\n\n## Key Concepts\n- Bounded queues with backpressure\n- Multi-producer/multi-consumer fairness\n- Dispatcher scheduling policies (weighted fair queuing)\n- Shared-order-book safety and lock granularity\n\n## Code Example\n```java\nimport java.util.concurrent.*;\nimport java.util.*;\n\npublic class BoundedFairDispatcher {\n  static class Order { final String id; Order(String id){ this.id = id; } }\n  private final int perProducerCap;\n  private final Map<String,BlockingQueue<Order>> queues = new ConcurrentHashMap<>();\n  private final BlockingQueue<Order> ring;\n  private volatile boolean shutdown = false;\n  public BoundedFairDispatcher(int perProducerCap, int ringCap){ this.perProducerCap = perProducerCap; this.ring = new ArrayBlockingQueue<>(ringCap); }\n  public void enqueue(String pid, Order o) throws InterruptedException {\n    BlockingQueue<Order> q = queues.computeIfAbsent(pid, k -> new ArrayBlockingQueue<>(perProducerCap));\n    q.put(o); // producer blocks if quota reached\n  }\n  public void dispatcherLoop() throws InterruptedException {\n    while (!shutdown) {\n      for (BlockingQueue<Order> q : queues.values()) {\n        Order o = q.poll();\n        if (o != null) ring.put(o);\n      }\n      // simple backoff to avoid busy spin\n      Thread.yield();\n    }\n  }\n  public void shutdown() { shutdown = true; }\n}\n```\n\n## Follow-up Questions\n- How would you scale to multiple dispatchers and partition rings?\n- How would you test fairness under highly skewed producer rates?","diagram":null,"difficulty":"advanced","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","DoorDash","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T05:43:13.395Z","createdAt":"2026-01-16T05:43:13.395Z"},{"id":"q-2804","question":"In a real-time analytics pipeline, millions of event streams (streamId) arrive; each stream must be processed in-order, but streams can be processed in parallel. Design a bounded per-stream queueing layer with a shared worker pool. Include backpressure when a queue fills, handling of late/out-of-order events, and graceful recovery after worker failure. Provide a runnable sketch in Go or Rust?","answer":"Use per-stream bounded queues and a fixed worker pool. Producers enqueue into their stream's queue; if full, apply backpressure by blocking or signaling upstream. Use per-stream sequence numbers and a","explanation":"## Why This Is Asked\n\nIn real-time analytics pipelines, streams arrive continuously. Achieving per-stream in-order processing while allowing global parallelism is a non-trivial balance requiring bounded buffers, backpressure, and robust failure handling.\n\n## Key Concepts\n\n- Per-stream in-order processing\n- Bounded queues\n- Backpressure\n- Out-of-order handling\n- Graceful shutdown & failure recovery\n- Locks vs lock-free trade-offs\n\n## Code Example\n\n```go\n// Implementation sketch\ntype Event struct{ StreamID int; Seq int; Payload []byte }\n```\n\n## Follow-up Questions\n\n- How would you test backpressure and ordering?\n- How would you scale to thousands of streams and dynamic creation?","diagram":null,"difficulty":"intermediate","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T13:09:01.409Z","createdAt":"2026-01-16T13:09:01.409Z"},{"id":"q-2834","question":"Three producers generate events tagged by user id and time; a single in-memory sink aggregates per-user activity over a 1-second sliding window and emits a summary to a consumer. Implement a beginner-friendly concurrency solution in Go, Python, or Java that guarantees thread-safe window updates, handles late events (up to 200ms), and ensures the sink doesn't drop events under low backpressure. Provide runnable sketch and tests?","answer":"Use a per-user rolling window guarded by a mutex with an in-memory map, plus a periodic sweeper to drop old events and emit counts. Alternatively funnel events through a single worker with a bounded c","explanation":"## Why This Is Asked\n\nTests practical concurrency patterns around per-key state, backpressure, and late-arrival handling beyond basic queues.\n\n## Key Concepts\n\n- Per-key state management with thread safety\n- Sliding windows and late-event handling\n- Backpressure and testability between mutex-based and channel-based designs\n\n## Code Example\n\n```go\npackage main\n\nimport (\n  \"time\"\n  \"sync\"\n)\n\ntype Event struct { User string; TS time.Time }\ntype Summary struct { User string; Count int }\n\nfunc boundWindowWorker(in <-chan Event, out chan<- Summary, window time.Duration) {\n  mu := sync.Mutex{}\n  perUser := make(map[string][]time.Time)\n  tick := time.NewTicker(1 * time.Second)\n  defer tick.Stop()\n\n  for {\n    select {\n    case e := <-in:\n      mu.Lock()\n      arr := perUser[e.User]\n      arr = append(arr, e.TS)\n      cutoff := e.TS.Add(-window)\n      i := 0\n      for i < len(arr) && arr[i].Before(cutoff) {\n        i++\n      }\n      perUser[e.User] = arr[i:]\n      mu.Unlock()\n    case t := <-tick.C:\n      _ = t\n      mu.Lock()\n      for u, arr := range perUser {\n        out <- Summary{User: u, Count: len(arr)}\n      }\n      mu.Unlock()\n    }\n  }\n}\n```\n\n## Follow-up Questions\n\n- How would you test late arrivals and ensure no data loss under backpressure?\n- Compare mutex-based versus single-worker channel designs in terms of latency and fairness.","diagram":null,"difficulty":"beginner","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","PayPal","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T14:09:24.445Z","createdAt":"2026-01-16T14:09:24.445Z"},{"id":"q-2876","question":"Design a dynamic DAG-based task scheduler with runtime-submitted tasks and dependencies; implement a fixed-size worker pool with local queues, support cross-worker work-stealing, and ensure dependents wake atomically when prerequisites finish; bound the global queue to enforce backpressure; discuss deadlock avoidance and fairness under high contention?","answer":"Represent tasks as nodes with in-degrees and adjacency lists. Each worker owns a local queue; when a task’s in-degree becomes 0, enqueue to a worker. On completion, atomically decrement children and e","explanation":"## Why This Is Asked\n\nTests practical concurrency design for dynamic workloads, runtime DAGs, backpressure, and fairness in a multi-threaded runtime common in real systems.\n\n## Key Concepts\n\n- DAG scheduling with in-degree tracking\n- Local queues and cross-worker work-stealing\n- Bounded global queue for backpressure\n- Deadlock and livelock avoidance, fairness under contention\n\n## Code Example\n\n```go\n// Skeleton illustrating the core ideas\ntype Task struct {\n  id   string\n  deps int32\n  // children references omitted for brevity\n}\n\n// On submit: if in-degree becomes 0 -> enqueue to a worker\n// On completion: atomically decrement children; enqueue when 0\n```\n\n## Follow-up Questions\n\n- How would you test for starvation or livelock in this scheduler?\n- How would you extend to support task priorities or IO-bound vs CPU-bound tasks?","diagram":"flowchart TD\nA[Submit Task] --> B[Increment In-degree]\nB --> C{In-degree == 0?}\nC -- Yes --> D[Enqueue to Local Queue]\nC -- No --> E[Wait for Dependencies]\nD --> F[Worker Executes Task]\nF --> G[Release to Children]\nG --> H{New Ready Tasks}\nH -- Enqueue --> D","difficulty":"intermediate","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","Netflix","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T15:45:21.678Z","createdAt":"2026-01-16T15:45:21.678Z"},{"id":"q-2910","question":"Design a concurrent job queue for an image-processing service: tasks arrive with tenant_id, priority, and deadline; implement a bounded, multi-queue system where workers always pull from the currently highest-priority non-empty tenant queue, but aging prevents any tenant from starving. Include backpressure on the global queue, cross-tenant fairness, and a test plan for liveness under bursty load. Provide concrete structures and trade-offs?","answer":"I would implement per-tenant queues plus a shared min-heap keyed by (priority, age). Workers pick the highest-priority head, applying aging to avoid starvation. A global semaphore enforces bound and c","explanation":"## Why This Is Asked\n- Evaluates design of a concurrent, bounded queue with per-tenant fairness under contention.\n- Probes data structures for priority scheduling and starvation prevention.\n- Checks strategies for backpressure and testing approaches.\n\n## Key Concepts\n- Bounded queues, per-tenant fairness, priority scheduling\n- Backpressure, wake-ups, and liveness guarantees\n- Testing under bursty workloads and contention\n\n## Code Example\n```javascript\n// Pseudocode sketch of data structures\nclass TenantQueue { constructor() { this.q = []; this.lock = new SpinLock(); } }\nclass GlobalScheduler {\n  enqueue(task){ /* bounded token + per-tenant push */ }\n  dequeue(){ /* pick highest-priority non-empty queue with aging */ }\n}\n```\n\n## Follow-up Questions\n- How would you implement aging to avoid starvation without starving high-priority tasks?\n- How do you verify liveness under bursty load? ","diagram":"flowchart TD\nA[Submit Task] --> B[Place in Tenant Queue]\nB --> C[Global Scheduler Picks by Priority]\nC --> D[Worker Executes]\nD --> E[Dequeue and Reclaim Credits]\n","difficulty":"intermediate","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T16:59:49.399Z","createdAt":"2026-01-16T16:59:49.400Z"},{"id":"q-2995","question":"Design a speculative execution layer for a real-time collaborative editor where user actions arrive as a DAG of edits with dependencies. Implement a multithreaded executor that runs independent edits in parallel, defers dependents until prerequisites finish, and detects conflicts on the same position to rollback. Include a bounded replay queue to cap work and a rollback protocol. Provide a runnable sketch in Go or Rust and discuss guarantees?","answer":"Leverage per-document DAG frontier; each edit has id, deps, and position. Schedule ready edits on a worker pool; detect conflicts by (position, client-version). On conflict, rollback to last committed","explanation":"## Why This Is Asked\nThis question probes the ability to design speculative, dependency-aware execution for real-time collaboration, a realistic pattern in chat editors and SaaS platforms.\n\n## Key Concepts\n- Directed acyclic graph scheduling\n- Speculative execution with rollback\n- Conflict detection on shared document positions\n- Bounded replay queue and deterministic commit ordering\n\n## Code Example\n```rust\n// Skeleton sketch of scheduling loop (high level)\nstruct Edit { id: u64, deps: Vec<u64>, pos: usize, content: String }\n```\n\n## Follow-up Questions\n- How would you test race conditions and simulate latency?\n- How would you scale the replay buffer across many documents?","diagram":null,"difficulty":"intermediate","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Discord","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T20:36:42.992Z","createdAt":"2026-01-16T20:36:42.993Z"},{"id":"q-3132","question":"Design an in-memory, concurrent stream join operator for two input streams A and B. Each event has a key and a timestamp. Implement a per-key sliding-window equi-join with multiple worker threads that can process different keys in parallel, but guarantee in-order output per key. Bound memory usage with eviction, and apply backpressure when downstream slows. Compare locking vs lock-free implementations and outline testing strategies and failure scenarios?","answer":"Strategy: partition by key hash; per-partition workers hold per-key sliding windows in bounded maps. Join when both sides have a matching key within the window; emit with per-key sequence for in-order","explanation":"## Why This Is Asked\n\nAssess ability to design a concurrent, stateful streaming component with per-key ordering and backpressure across workers.\n\n## Key Concepts\n\n- per-key sliding windows\n- bounded memory with eviction\n- output-order guarantees\n- backpressure mechanisms\n- locking vs lock-free trade-offs\n- test strategies and failure modes\n\n## Code Example\n\n```javascript\n// Pseudo-code sketch of partitioning and per-key join\n```\n\n## Follow-up Questions\n\n- How would you handle hot keys and dynamic partitions?\n- What metrics and tests ensure liveness under backpressure?","diagram":"flowchart TD\n  A[Producer A] --> P[Partitioner]\n  B[Producer B] --> P\n  P --> W[Worker Pool]\n  W --> O[Output]\n  O --> D[Downstream]\n  D -- backpressure--> O","difficulty":"intermediate","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","LinkedIn","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T04:12:57.227Z","createdAt":"2026-01-17T04:12:57.227Z"},{"id":"q-3217","question":"Design a per-session ordered event dispatcher for a real-time multiplayer game server. Each game session has its own FIFO queue; multiple producers can enqueue events across sessions, and a fixed-size pool of worker threads processes events. Requirements: (1) preserve strict per-session order; (2) allow cross-session work-stealing for load balancing; (3) enforce a global bounded queue with backpressure to producers; (4) support dynamic session creation/destruction with zero deadlock and graceful shutdown; (5) discuss memory reclamation and safety?","answer":"Leverage a bounded global queue feeding a fixed worker pool, with per-session FIFOs. Producers enqueue to the global queue; workers pull sessions, draining in-order. For load balancing, allow stealing","explanation":"## Why This Is Asked\nThis question probes the candidate's ability to design a concurrent, low-latency dispatcher that preserves per-session ordering while scaling across cores. It also exercises lifecycle management and safe shutdown.\n\n### Key Concepts\n- Bounded backpressure and global queues\n- Per-session FIFO with in-order processing\n- Work stealing across sessions without violating order\n- Dynamic session lifecycle and memory reclamation\n\n### Code Example\n```rust\n// Pseudo Rust sketch: session queues and stealing interface\nstruct SessionQueue { /* ... */ }\nimpl SessionQueue {\n  fn enqueue(&self, evt: Event);\n  fn dequeue(&self) -> Option<Event>;\n}\n```\n\n### Follow-up Questions\n- How would you extend to prioritize latency-sensitive events within a session?\n- How would you detect starvation and ensure fairness across sessions?","diagram":null,"difficulty":"advanced","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Google","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T07:30:21.545Z","createdAt":"2026-01-17T07:30:21.546Z"},{"id":"q-3306","question":"Design a bounded, deadline-aware task scheduler for a live image-processing pipeline. Producers enqueue tasks labeled CPU-bound or IO-bound with deadlines; implement a two-tier priority with earliest deadline first and aging to prevent starvation. Use per-worker local queues with work-stealing, and a global backpressure mechanism when backlog grows. Ensure deadlines are respected and test bursts with latency metrics?","answer":"Use a bounded global queue guarded by a counting semaphore to apply backpressure. Dequeue tasks into per-worker local deques; workers steal from neighbors to balance. Each task carries a deadline and ","explanation":"## Why This Is Asked\nEvaluates a candidate's ability to design a concurrent scheduler that respects deadlines under high contention, while balancing throughput via per-worker queues and backpressure. It also probes testing strategies for bursty workloads and fairness.\n\n## Key Concepts\n- Bounded queues and backpressure\n- Deadline-aware scheduling (EDF) with aging\n- Per-worker local queues and work-stealing\n- Task typing (CPU vs IO) and affinity considerations\n- Correctness under contention and starvation avoidance\n\n## Code Example\n```javascript\n// Skeleton: Task with deadline and type\nclass Task { constructor(id, deadline, type, payload) { this.id = id; this.deadline = deadline; this.type = type; this.payload = payload; } }\nclass Scheduler {\n  // core components: boundedGlobalQueue, perWorkerQueues, steal logic, aging\n}\n```\n\n## Follow-up Questions\n- How would you validate absence of starvation under worst-case burst scenarios?\n- How would you adapt the design for multi-host deployment with cross-node backpressure?","diagram":"flowchart TD\n  P[Producer] --> G[GlobalBoundedQueue]\n  G --> W[WorkerLocalQueues]\n  W --> C[Completed]\n  Back[Backpressure] --> G","difficulty":"intermediate","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Meta","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T10:42:19.393Z","createdAt":"2026-01-17T10:42:19.393Z"},{"id":"q-3453","question":"Design a beginner-friendly concurrency setup in Go for a real-time notification service where many producers emit payloads that must be delivered to users in per-user order. Implement a global bounded queue and per-user FIFO channels so that each user's messages are processed in order, while a small worker pool handles delivery. Explain how you enforce backpressure, avoid starvation, and test ordering and race conditions?","answer":"Use a global buffered channel as backpressure, and maintain a per-user FIFO channel to guarantee order. A single dispatcher reads from the global queue and forwards each payload to the user’s dedicate","explanation":"## Why This Is Asked\nTests practical micro-concurrency patterns, including backpressure, per-key ordering, and fairness—common in large-scale services.\n\n## Key Concepts\n- Bounded queues\n- Per-key sequencing\n- Dispatcher + worker pool\n- Fairness and deadlock avoidance\n\n## Code Example\n```go\n// skeleton illustrating structures and flow (omitted for brevity)\n```\n\n## Follow-up Questions\n- How would you scale the worker pool if the per-user queues grow unbounded? \n- How would you test for starvation under skewed load?","diagram":null,"difficulty":"beginner","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T16:49:22.802Z","createdAt":"2026-01-17T16:49:22.802Z"},{"id":"q-3483","question":"In a real-time video analytics service, cameras stream frames that must be processed in per-camera order while frames from different cameras can run concurrently. Design a bounded, backpressured dispatch with a central queue and per-camera FIFO queues, plus a small worker pool. Explain ordering, starvation avoidance, and failure handling. Provide a runnable asyncio Python sketch demonstrating enqueue, dispatcher, queues, and graceful shutdown?","answer":"Use a central bounded queue with maxsize N and a dispatcher that routes frames to per-camera FIFO queues. Each camera has a dedicated worker set that preserves order per camera, while frames from diff","explanation":"## Why This Is Asked\n\nReal-time video analytics require strict per-camera ordering with parallelism across cameras; this tests architectural decisions for bounded dispatchers, backpressure, and robust failure handling.\n\n## Key Concepts\n\n- Per-camera FIFO with a global bound\n- Dispatcher design and backpressure handling\n- Failure isolation and graceful shutdown\n- Starvation avoidance and latency considerations\n\n## Code Example\n\n```python\nimport asyncio\nfrom collections import defaultdict\n\nCENTRAL_MAX = 100\nPER_CAM_MAX = 20\n\nasync def dispatcher(central, queues):\n    while True:\n        cam, frame = await central.get()\n        if cam is None:\n            for q in queues.values():\n                await q.put((None, None))\n            return\n        await queues[cam].put(frame)\n\nasync def worker(queue, cam):\n    while True:\n        frame = await queue.get()\n        if frame is None:\n            break\n        # process(frame)  # placeholder for actual work\n        queue.task_done()\n```\n\n## Follow-up Questions\n\n- How would you monitor backpressure and latency in production?\n- How would you scale to hundreds of cameras with churn while preserving guarantees?","diagram":"flowchart TD\n  P[Producers] --> C[Central bounded queue]\n  C --> D[Dispatcher]\n  D --> Q[Per-camera queues]\n  Q --> W[Workers]","difficulty":"advanced","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Robinhood","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T18:41:29.810Z","createdAt":"2026-01-17T18:41:29.810Z"},{"id":"q-3770","question":"Scenario: A telemetry inlet ingests events from numerous IoT devices. Each device’s events must be processed in arrival order, while events from different devices are handled concurrently by a small worker pool. Design a beginner-friendly Go solution using a bounded global queue and per-device FIFO channels to preserve per-device order. Add deduplication for repeated (deviceId,seq) pairs, implement backpressure, and ensure fairness when bursts occur. Provide a runnable sketch and brief testing plan?","answer":"Go solution: a bounded global channel acts as a backpressure point. A dispatcher reads events and forwards them into per-device FIFO channels (one buffered channel per device). A small worker pool dra","explanation":"## Why This Is Asked\n\nNew angle: adds per-device dedup and burst fairness, common in telemetry IoT workloads. Tests should cover ordering, backpressure, and race conditions, mirroring real-world constraints at data-intensive shops.\n\n## Key Concepts\n\n- Bounded global queue enforces backpressure.\n- Per-device FIFO channels preserve per-device order under concurrency.\n- Deduplication by (device,seq) prevents duplicate work.\n- Round-robin worker drain provides fairness during bursts.\n\n## Code Example\n\n```go\npackage main\n\ntype Event struct {\n  Device string\n  Seq    int\n  Data   string\n}\n\n// global bounded queue\nvar globalQueue = make(chan Event, 128)\n\nvar (\n  mu     sync.Mutex\n  queues = map[string]chan Event{}\n  seen   = map[string]struct{}{}\n)\n\nfunc getQueue(id string) chan Event {\n  mu.Lock()\n  defer mu.Unlock()\n  q, ok := queues[id]\n  if !ok {\n    q = make(chan Event, 32)\n    queues[id] = q\n  }\n  return q\n}\n\nfunc dispatcher() {\n  for e := range globalQueue {\n    key := e.Device + \":\" + strconv.Itoa(e.Seq)\n    mu.Lock()\n    if _, ok := seen[key]; ok {\n      mu.Unlock()\n      continue\n    }\n    seen[key] = struct{}{}\n    ch := getQueue(e.Device)\n    mu.Unlock()\n    ch <- e\n  }\n}\n\n// workers would range over device queues in round-robin\n```\n\n## Follow-up Questions\n\n- How would you test ordering, dedup, and backpressure under bursty loads?\n- How would you scale this to dynamic worker counts and multiple brokers while preserving per-device ordering?","diagram":"flowchart TD\n  P[Producers] --> Q[GlobalBoundedQueue]\n  Q --> D[Dispatcher]\n  D --> DEV_A[DeviceQueueA]\n  D --> DEV_B[DeviceQueueB]\n  DEV_A --> W1[Worker]\n  DEV_B --> W2[Worker]\n  %% rounds out to multiple devices with a shared worker pool","difficulty":"beginner","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Tesla","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T08:50:01.209Z","createdAt":"2026-01-18T08:50:01.209Z"},{"id":"q-3797","question":"In a GPU-accelerated image-processing pipeline, dozens of producers generate work units with different priorities and data dependencies, and a pool of workers dispatch kernels. Design a bounded, multi-producer multi-consumer queue with backpressure that supports per-item priority without starving high-priority tasks. Explain data structures, memory ordering, and how you avoid ABA and deadlocks. Include a runnable minimal sketch (Rust or C++) showing enqueue/dequeue semantics and a basic test strategy with synthetic load?","answer":"Design a bounded MPMC queue built as a ring buffer with atomic head/tail indices and per-slot state. Use memory_order_release/acquire, padding to avoid false sharing, and a backoff strategy for conten","explanation":"## Why This Is Asked\n\nTests advanced concurrency design for bounded queues, backpressure, and fairness in a GPU/telemetry-like pipeline.\n\n## Key Concepts\n\n- Lock-free MPMC queues\n- memory ordering\n- backpressure strategies\n- starvation prevention with priority lanes\n- deadlock-free shutdown and testing\n\n## Code Example\n\n```cpp\n// skeleton of bounded MPMC queue\n#include <atomic>\n#include <vector>\ntemplate<class T>\nclass BoundedMPMC {\n  size_t const CAP;\n  std::vector<std::atomic<T*>> slots;\n  std::atomic<size_t> head{0}, tail{0};\npublic:\n  bool enqueue(const T& v);\n  bool dequeue(T& v);\n};\n```\n\n## Follow-up Questions\n\n- How would you integrate per-item priority without starving low-priority tasks?\n- How would you test for memory ordering issues and race conditions at scale?","diagram":null,"difficulty":"advanced","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","NVIDIA","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T09:43:27.985Z","createdAt":"2026-01-18T09:43:27.985Z"},{"id":"q-3855","question":"Design a streaming dataflow for real-time analytics that processes events into fixed 1-second windows. Implement per-window bounded queues, a dynamic worker pool, and a watermark-driven cancellation protocol: late events crossing the watermark cancel the window and abort pending tasks. Guarantee in-window output order and no deadlocks under backpressure. Explain testing strategies and trade-offs?","answer":"Propose a streaming dataflow with per-window bounded queues and a dynamic worker pool. Map events to fixed 1s windows; maintain a watermark. If a late event arrives past the watermark, cancel the corr","explanation":"## Why This Is Asked\n\nInterviewers seek practical experience with streaming systems, time-windowing, and cancellation under backpressure. A watermark-driven cancellation tests correctness under out-of-order data and ensures resources aren’t wasted on irrelevant windows.\n\n## Key Concepts\n\n- Watermarks and late data handling\n- Per-window bounded queues and dynamic worker pools\n- Cancellation propagation and deadlock avoidance\n- Testing strategies for timing, backpressure, and failure modes\n\n## Code Example\n\n```javascript\n// Skeleton windowing structure\nclass Window {\n  constructor(ts) {\n    this.ts = ts;\n    this.tasks = [];\n    this.cancelled = false;\n  }\n  addTask(t) {\n    if (this.cancelled) throw new Error('window cancelled');\n    this.tasks.push(t);\n  }\n  complete() {\n    // emit aggregated result\n  }\n}\n```\n\n## Follow-up Questions\n\n- How would you measure fairness across windows under bursty input?\n- How would you test cancellation propagation and deadlock scenarios?","diagram":null,"difficulty":"intermediate","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Instacart","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T13:00:02.391Z","createdAt":"2026-01-18T13:00:02.391Z"},{"id":"q-3994","question":"Design a fault-tolerant, multi-region task queue for a real-time data-enrichment service serving many tenants. Each tenant has a cap and can enqueue interdependent tasks. Propose a bounded, distributed queue with backpressure, delivering at least once while achieving exactly-once processing for idempotent handlers, plus per-tenant ordering and a recovery plan. Include a test strategy?","answer":"To meet scale, implement a bounded, per-tenant, multi-region queue backed by a durable store with region-local buffers. Use a per-task sequence ID and idempotent handlers to realize exactly-once proce","explanation":"## Why This Is Asked\n\nTests ability to design a distributed queue with per-tenant backpressure, cross-region resilience, and strong delivery guarantees in a multi-region deployment.\n\n## Key Concepts\n\n- Bounded, per-tenant queues and regional buffering\n- Exactly-once vs at-least-once semantics with idempotence\n- Cross-region failure handling and replay\n- Ordering guarantees and recovery testing\n\n## Code Example\n\n```javascript\n// Pseudo sketch: per-tenant queue API with sequence IDs and dedup store\nclass Task { constructor(id, tenant, seq, deps) { ... } }\nclass DurableStore { enqueue(task); ack(seq); replay(tenant); }\n```\n\n## Follow-up Questions\n\n- How would you test region failover and replay guarantees?\n- How do you prevent head-of-line blocking across tenants while preserving ordering?\n- What metrics signal backpressure effectiveness and replay impact?","diagram":null,"difficulty":"advanced","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google","Hashicorp"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T18:51:55.631Z","createdAt":"2026-01-18T18:51:55.632Z"},{"id":"q-4066","question":"Design a backpressure-aware streaming engine for real-time analytics: streams partitioned by sensor_id; each partition has a bounded queue and a 5s tumbling window; a watermark advances to emit aggregates; late data goes to a late-path buffer for a grace period; exactly-once via incremental checkpointing of partition offsets and window state; discuss latency-throughput trade-offs and testing strategy?","answer":"Implement per-partition bounded queues with backpressure signaling upstream; use 5-second tumbling windows with a monotonic watermark for time advancement; route late events to a buffer during the grace period; ensure exactly-once semantics through incremental checkpointing of partition offsets and window state; balance latency and throughput by tuning batch sizes and watermark intervals.","explanation":"## Why This Is Asked\n\nTests understanding of streaming architectures under load, including partitioning, backpressure, and correctness guarantees (exactly-once) in the presence of late data and failures.\n\n## Key Concepts\n\n- Backpressure and bounded queues\n- Partitioning by key for ordering guarantees\n- Watermarks and late-data handling\n- Windowed aggregations and state management\n- Exactly-once semantics via checkpointing and replay\n- Test strategies for bursts, late data, and crash recovery\n\n## Code Example\n\n```javascript\n// Pseudo-code: per-partition window with watermark and late path\nclass PartitionProcessor {\n  constructor(sensorId, queueCapacity = 1000) {\n    this.sensorId = sensorId;\n    this.queue = new BoundedQueue(queueCapacity);\n    this.window = new TumblingWindow(5000); // 5s window\n    this.watermark = new MonotonicWatermark();\n    this.lateBuffer = new LateDataBuffer();\n  }\n  \n  process(event) {\n    if (event.timestamp < this.watermark.get()) {\n      this.lateBuffer.add(event);\n      return;\n    }\n    \n    if (this.queue.isFull()) {\n      this.signalBackpressure();\n      return false;\n    }\n    \n    this.queue.add(event);\n    this.watermark.update(event.timestamp);\n    this.window.add(event);\n    \n    if (this.window.isReady()) {\n      this.emitAggregates();\n      this.window.reset();\n    }\n  }\n}\n```","diagram":"flowchart TD\n  A[Producers] --> B[Partitioned Queues (sensor_id)]\n  B --> C[5s Windows]\n  C --> D[Aggregator/State]\n  D --> E[Checkpoints]\n  E --> F[Downstream Consumers]","difficulty":"intermediate","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","DoorDash"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T05:56:09.575Z","createdAt":"2026-01-18T22:34:09.514Z"},{"id":"q-4144","question":"Design a beginner-friendly concurrency pattern for a bounded multi-producer, multi-consumer router: many producers publish tasks tagged by resource_id into a shared bounded global queue. Implement per-resource FIFO buffers and a small worker pool that processes tasks from those buffers, while guaranteeing per-resource order and fair progress between resources. Explain backpressure, starvation avoidance, and a simple test plan to verify ordering under bursty loads?","answer":"Implement a bounded global queue and per-resource FIFO buffers. A dispatcher uses round-robin to route incoming tasks by resource_id into small per-resource queues. A fixed-size worker pool drains the","explanation":"## Why This Is Asked\n\nThis checks understanding of bounded queues, fairness, and per-key ordering with simple primitives.\n\n## Key Concepts\n\n- Bounded queues and backpressure\n- Per-resource FIFO ordering\n- Fair scheduling (round-robin)\n- Lightweight testing of ordering under concurrency\n\n## Code Example\n\n```javascript\n// Pseudo: dispatcher routes tasks to per-resource queues; workers drain in round-robin\n```\n\n## Follow-up Questions\n\n- How would you extend to dynamic resource sets?\n- How would you detect and mitigate starvation if a resource is rarely active?","diagram":"flowchart TD\n  A[Producers] --> B[GlobalBoundedQueue]\n  B --> C{Dispatch}\n  C --> D[ResourceQueue1]\n  C --> E[ResourceQueue2]\n  D --> F[WorkerPool]\n  E --> F\n  F --> G[Deliver]","difficulty":"beginner","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","Microsoft","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T04:49:32.485Z","createdAt":"2026-01-19T04:49:32.485Z"},{"id":"q-4222","question":"You're building a real-time analytics ingestion service for a multi-tenant SaaS platform. Each incoming event carries tenant_id and a local sequence number. You must preserve per-tenant in-order semantics across keys while allowing full parallelism across tenants. Design a bounded, multi-queue scheduler with per-tenant queues, a fixed-size worker pool, and backpressure when buffers saturate. Describe data structures, synchronization approach (locks vs lock-free), deadlock avoidance, and how to achieve exactly-once processing on failure. Provide runnable sketches in Go or Rust and outline a testing plan?","answer":"Map tenant_id to a per-tenant queue and a fixed-size worker pool fed by a bounded global queue. Enqueue under a tenant lock to preserve order; workers pull events by tenant sequence to maintain per-te","explanation":"## Why This Is Asked\n\nReal-world ingestion needs per-tenant ordering with cross-tenant parallelism. Bounded buffers prevent unbounded memory growth and backpressure must be predictable under bursts. Candidate must reason about data structures, synchronization, and recovery guarantees.\n\n## Key Concepts\n\n- Per-tenant ordering with cross-tenant parallelism\n- Bounded, multi-queue scheduler and worker pool\n- Lock-based vs lock-free trade-offs\n- Deadlock avoidance and backpressure strategies\n- Exactly-once semantics via durable commits and idempotent handlers\n\n## Code Example\n\n```go\ntype Event struct {\n  TenantID string\n  Seq      int64\n  Payload  []byte\n}\n\ntype TenantQueue struct {\n  mu      sync.Mutex\n  nextSeq int64\n  q       []Event\n}\n\ntype Scheduler struct {\n  tenants map[string]*TenantQueue\n  global  chan *Event\n  workers int\n}\n```\n\n## Follow-up Questions\n\n- How would you test failure recovery and restart scenarios to guarantee exactly-once processing?\n- Discuss scaling the worker pool when tenants exhibit vastly different ingress rates and how to detect starvation.","diagram":null,"difficulty":"intermediate","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T09:12:32.940Z","createdAt":"2026-01-19T09:12:32.940Z"},{"id":"q-4292","question":"You are building a real-time notification system with a bounded, multi-producer, multi-consumer queue. Producers label tasks as critical or normal. The queue capacity is fixed; producers block when full. A worker pool must fetch tasks in a fair, per type round robin to avoid starvation. Provide a runnable sketch in Go (or Python/Java) and discuss backpressure, cancellation, and testing for latency and task ordering?","answer":"Use two bounded channels, one per task type (critical, normal). A small front-end fan-in goroutine enqueues tasks into their channel; workers select in a round-robin fashion to prevent starvation. Bac","explanation":"## Why This Is Asked\n\nTests practical concurrency basics: bounded queues, backpressure, fairness, and cancellation under load.\n\n## Key Concepts\n\n- Bounded buffers and backpressure\n- Multi-producer, multi-consumer coordination\n- Fair scheduling (round-robin per type)\n- Cancellation and testing\n\n## Code Example\n\n```javascript\n// skeleton showing two channels and a round-robin fetch loop\n```\n\n## Follow-up Questions\n\n- How would you extend to add per-task deadlines?\n- How would you verify ordering guarantees under bursty traffic?","diagram":"flowchart TD\n  Frontend[Enqueue frontend] --> CritQueue[critical channel]\n  Frontend --> NormQueue[normal channel]\n  subgraph WorkerPool\n    W1[Worker] --> CritQueue\n    W2[Worker] --> NormQueue\n  end","difficulty":"beginner","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","Scale Ai","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T11:38:57.290Z","createdAt":"2026-01-19T11:38:57.290Z"},{"id":"q-4482","question":"In a real-time task processing system, multiple producers enqueue jobs into a global bounded queue with capacity C. A fixed pool of workers consumes jobs. Extend a beginner concurrency pattern to support two priority levels: high and normal. High-priority jobs must be served before normal ones, while preserving FIFO within each priority. Producers should block when the queue is full. Implement backpressure signaling and outline a concrete test plan to verify ordering and backpressure?","answer":"Use two bounded queues (High and Normal) sharing capacity C. Enqueue blocks via a counting semaphore representing total items. A single dispatcher drains High first, then Normal, preserving FIFO withi","explanation":"## Why This Is Asked\nTests practical handling of multiple producers with prioritized fairness and backpressure in a simple, implementable pattern.\n\n## Key Concepts\n- Bounded queues and backpressure\n- Priority with FIFO guarantees\n- Semaphore-based flow control\n- Testing ordering and deadlock avoidance\n\n## Code Example\n```javascript\n// Pseudo-code illustrating two queues with a dispatcher\nclass QueuePair {\n  constructor(capacity) { /* ... */ }\n  enqueueHigh(item) { /* block if full */ }\n  enqueueNormal(item) { /* block if full */ }\n  dispatch() { /* drain High then Normal */ }\n}\n```\n\n## Follow-up Questions\n- How would you adapt this for dynamic worker scaling?\n- How would you detect and recover from dispatcher starvation?","diagram":"flowchart TD\n  A[Producers] --> B[HighQueue]\n  A --> C[NormalQueue]\n  D[Dispatcher] --> E[WorkerPool]\n  B --> F[Drain High]\n  C --> F\n  F --> E","difficulty":"beginner","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T20:43:48.220Z","createdAt":"2026-01-19T20:43:48.220Z"},{"id":"q-4617","question":"Design a beginner-friendly concurrency pattern for a real-time ingest system where producers submit messages tagged by dynamic topics. Each topic must preserve per-topic in-order delivery, while a bounded global queue and a small worker pool process items across topics. Topics can be created or removed at runtime; ensure backpressure signals to producers when quotas are reached and gracefully drain or archive messages on topic removal. Explain test plan for per-topic ordering under churn and for topic lifecycle events?","answer":"Use a per-topic FIFO queue and a global bounded capacity, e.g., a semaphore. A dispatcher pulls from non-empty topic queues and assigns to a small pool of workers. Topic creation/removal protected by ","explanation":"## Why This Is Asked\n\nTests practical concurrency handling: per-topic ordering, dynamic topic lifecycle, and backpressure. It also probes test design for nondeterminism.\n\n## Key Concepts\n\n- Per-topic FIFO queues with dynamic creation\n- Global bounded capacity with backpressure\n- Dispatcher fairness across topics\n- Safe topic removal draining/archiving\n\n## Code Example\n\n```javascript\n// sketch showing dispatcher loop using async queues\n```\n\n## Follow-up Questions\n\n- How would you ensure fairness if one topic keeps sending?\n- How would you test end-to-end ordering guarantees under bursty traffic?","diagram":null,"difficulty":"beginner","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Goldman Sachs","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T04:36:39.412Z","createdAt":"2026-01-20T04:36:39.412Z"},{"id":"q-4669","question":"In a real-time ingestion service, dozens of producers emit per-tenant events into a fixed-size global queue. Implement a bounded, multi-tenant queue that guarantees per-tenant in-order processing and fair sharing while allowing dynamic partition rebalancing without pausing producers. Provide a runnable minimal sketch in Go or Rust showing enqueue/dequeue and a basic test plan for backpressure, ordering, and rebalancing?","answer":"Two-level queue: a bounded global ring buffer plus per-tenant FIFO subqueues. Each event includes tenantId and a monotonic sequence. Producers push to the ring; a dispatcher forwards to the tenant’s q","explanation":"## Why This Is Asked\nTests ability to design a bounded, multi-tenant pipeline with ordering guarantees, backpressure, and dynamic rebalancing, reflecting real-world multi-tenant systems.\n\n## Key Concepts\n- Bounded queues and backpressure\n- Per-tenant in-order processing\n- Dynamic partitioning and rebalancing\n- Memory visibility and race conditions in concurrent code\n\n## Code Example\n```go\n// Minimal sketch: global bounded channel + per-tenant queues\npackage main\nimport (\n  \"fmt\"\n)\n\ntype Event struct{ Tenant string; Seq uint64; Data string }\n\nfunc main() {\n  global := make(chan Event, 8) // bounded global queue\n  tenants := map[string]chan Event{\n    \"tA\": make(chan Event, 4),\n    \"tB\": make(chan Event, 4),\n  }\n  // enqueue example (producer)\n  go func(){ global <- Event{Tenant:\"tA\", Seq:1, Data:\"x\"} }()\n  // dispatcher (simplified)\n  go func(){ for e := range global { tenants[e.Tenant] <- e } }()\n  // worker (consumer)\n  go func(){ for e := range tenants[\"tA\"] { fmt.Println(\"work\", e) } }()\n}\n```\n\n## Follow-up Questions\n- How would you test for starvation during bursty tenant loads?\n- How would you ensure monotonic sequencing across partition rebalancing?","diagram":null,"difficulty":"advanced","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Netflix","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T07:38:13.610Z","createdAt":"2026-01-20T07:38:13.610Z"},{"id":"q-4770","question":"In a real-time video channel encoding pipeline with thousands of channels, producers enqueue per-channel tasks into a bounded global queue and per-channel local queues. Design a backpressure-aware, low-latency scheduler (Rust; crossbeam or Tokio) that preserves per-channel order, avoids starvation, and scales to many cores. Include a runnable sketch showing enqueue, worker pull, and backpressure?","answer":"Propose a global bounded queue plus per-channel local deques. Workers drain local queues first, then fetch from the global queue with a rotating index and aging to prevent starvation. Block producers ","explanation":"## Why This Is Asked\nTests the ability to design a scalable, low-latency scheduler that preserves per-channel ordering under heavy bursty load and multi-core contention. It probes backpressure, fairness, and memory-safety across queues and worker pools in a real-world streaming context.\n\n## Key Concepts\n- Bounded global queue + per-channel deques\n- Per-channel ordering with fair scheduling\n- Backpressure signaling and blocking producers\n- Lock-free / low-contention structures; ABA avoidance\n- Bursty workload testing and tail latency metrics\n\n## Code Example\n```rust\nuse crossbeam_queue::SegQueue;\nuse std::collections::VecDeque;\nuse std::sync::Arc;\nstruct Task { channel_id: usize, payload: Vec<u8> }\nstruct ChannelState { q: VecDeque<Task>, backoff: usize }\nstruct Scheduler { global: Arc<SegQueue<Task>>, channels: Vec<ChannelState> }\nimpl Scheduler {\n  fn enqueue(&self, t: Task) { /* block if full; push to channel queue */ }\n  fn worker_pull(&self, id: usize) -> Option<Task> { /* drain local, then global with aging */ }\n}\n```\n\n## Follow-up Questions\n- How would you prevent starvation for low-rate channels while high-rate channels stay busy?\n- How would you adapt the design for heterogeneous workers with different encoding costs?","diagram":null,"difficulty":"advanced","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T11:43:56.406Z","createdAt":"2026-01-20T11:43:56.406Z"},{"id":"q-4793","question":"Design a bounded, multi-producer, multi-consumer processing system for streaming analytics. Each event has a topic id; events within the same topic must be processed in order, but topics may run in parallel. Implement a dynamic worker pool that can grow/shrink at runtime and rebalance tasks across topics while preserving per-topic order. Include backpressure signaling when the global queue is full, fairness across topics, and a graceful shutdown that drains in-flight items per topic?","answer":"Design a bounded global queue feeding per-topic FIFO queues. Producers tag events with topic IDs and block when full. A dispatcher assigns work to workers while preserving per-topic order, using a top","explanation":"Why This Is Asked\nTests ability to preserve per-key order under contention while enabling global throughput and dynamic resource scaling. It also probes backpressure handling, fairness across many topics, and safe shutdowns.\n\nKey Concepts\n- Bounded queues and backpressure\n- Per-topic FIFO ordering\n- Dynamic worker pool with runtime reconfiguration\n- Fair scheduling across topics and avoidance of starvation\n- Graceful shutdown that drains in-flight tasks per topic\n\nCode Example\n```javascript\n// Pseudo-structure for dispatcher (high level)\nclass Dispatcher {\n  constructor(globalQueue, topicQueues, workers) { ... }\n  schedule() {\n    // pick next topic with ready item, assign to a free worker\n  }\n  resizeWorkers(n) { ... } // adjust pool size\n}\n```\n\nFollow-up Questions\n- How would you handle a bursty topic that monopolizes workers?\n- How would you test latency and fairness across dozens/hundreds of topics?","diagram":"flowchart TD\n  P[Producers] --> G[GlobalBoundedQueue]\n  G --> TA[TopicQueue: Topic A]\n  G --> TB[TopicQueue: Topic B]\n  TA --> W[Worker]\n  TB --> W\n  W --> F[Finished]\n  F --> D[Backpressure Signaled if full]","difficulty":"intermediate","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","IBM"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T13:16:17.147Z","createdAt":"2026-01-20T13:16:17.147Z"},{"id":"q-4867","question":"In a high-frequency risk-checking pipeline, many trading engines emit per-instrument events that must be processed in instrument-local order by a pool of workers. Design a bounded multi-producer, multi-consumer queue in Rust (Tokio) with per-instrument FIFOs and global backpressure. Include how to register new instruments, graceful shutdown, and a runnable minimal example showing enqueue, dispatch, and exit?","answer":"A two-layer design: per-instrument bounded FIFOs and a global bounded dispatcher. Producers push into an instrument-specific Tokio MPSC with cap; when full, backpressure stalls enqueue. Dispatcher pol","explanation":"## Why This Is Asked\nThis question probes mastery of advanced concurrency patterns, backpressure, and safe shutdown in a low-latency pipeline.\n\n## Key Concepts\n- Per-instrument FIFO queues\n- Global bounded capacity and backpressure\n- Round-robin or fair scheduling\n- Safe startup/shutdown and dynamic registration\n- Idempotent processing for retries\n\n## Code Example\n```rust\n// skeleton showing per-instrument queues and a dispatcher\n```\n\n## Follow-up Questions\n- How would you test backpressure under bursty load?\n- How would you simulate replays or late events to ensure idempotence?","diagram":"flowchart TD\n  P[Producers] --> D[Dispatcher]\n  D --> QA[Instrument A Queue]\n  D --> QB[Instrument B Queue]\n  QA --> W[Workers]\n  QB --> W","difficulty":"advanced","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["PayPal","Robinhood","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T16:56:12.502Z","createdAt":"2026-01-20T16:56:12.502Z"},{"id":"q-4923","question":"In a high-throughput streaming service, multiple producers publish events across distinct topics to a shared bounded buffer. Design a dispatcher in Rust that routes events to per-topic worker pools using a single ring buffer. Include per-topic offsets, backpressure when full, fair scheduling, and a clean shutdown. Provide a runnable minimal sketch and testing guidance?","answer":"Use a single bounded ring buffer (size N) shared by all topics. Maintain per-topic consumer cursors and a single producer index with atomic enqueue. Producers await when full; workers poll per-topic o","explanation":"## Why This Is Asked\n\nTests ability to design a low-latency, multi-topic dispatcher with bounded resources, backpressure, and fairness.\n\n## Key Concepts\n\n- Bounded buffers, ring buffers, per-topic cursors\n- Atomics, memory ordering, CAS\n- Backpressure, starvation avoidance, shutdown\n\n## Code Example\n\n```rust\n// Placeholder sketch\n```\n\n## Follow-up Questions\n\n- How would you handle dynamic topic addition?\n- How would you verify fairness under bursty load?","diagram":null,"difficulty":"advanced","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","LinkedIn"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T19:16:51.677Z","createdAt":"2026-01-20T19:16:51.677Z"},{"id":"q-5410","question":"In a real-time event ingestion path for a multi-tenant analytics platform, producers publish per-topic events into a bounded global ring buffer. A pool of workers must deliver events in per-topic FIFO order while interleaving topics for throughput; design a bounded, two-level queueing system that guarantees backpressure, avoids starvation, and remains live under bursts. Describe data structures, synchronization (atomics, memory ordering, or locks), failure modes, testing strategy, and show a runnable minimal sketch in C++ that enqueues into a bounded global ring and dispatches per-topic FIFO?","answer":"Two-level bounded queue: a fixed-size global ring buffer for backpressure and per-topic FIFO queues feeding workers. Producers claim a slot, append to topic queue; a compact scheduler interleaves topi","explanation":"## Why This Is Asked\nThe scenario mirrors high-throughput services (e.g., payments, telemetry) where multiple producers emit per-topic events and workers must honor per-topic order while applying global backpressure. Designing a two-level queue reveals understanding of memory ordering, contention, and failure modes under load.\n\n## Key Concepts\n- Bounded queues and backpressure guarantees\n- Per-topic FIFO ordering within a global scheduler\n- Lock-free vs fine-grained locking trade-offs\n- Memory ordering pitfalls and ABA avoidance\n- Testing: burst traffic, latency, starvation, failure modes\n\n## Code Example\n```cpp\n// Minimal sketch: two-level queues\n#include <atomic>\n#include <vector>\n#include <queue>\n#include <unordered_map>\n#include <mutex>\n#include <string>\n\nstruct Slot { std::string data; bool used; };\n\nclass Ring {\n  std::vector<Slot> buf;\n  std::atomic<size_t> head, tail;\npublic:\n  Ring(size_t n): buf(n), head(0), tail(0) {}\n  bool try_enqueue(const std::string& s){\n    size_t h = head.load(std::memory_order_relaxed);\n    size_t t = tail.load(std::memory_order_acquire);\n    if ((t - h) >= buf.size()) return false;\n    buf[t % buf.size()] = {s,true};\n    tail.store(t+1, std::memory_order_release);\n    return true;\n  }\n  bool try_dequeue(std::string& out){\n    size_t h = head.load(std::memory_order_acquire);\n    size_t t = tail.load(std::memory_order_relaxed);\n    if (h==t) return false;\n    out = buf[h % buf.size()].data;\n    head.store(h+1, std::memory_order_release);\n    return true;\n  }\n};\n\nclass Dispatcher {\n  Ring ring;\n  std::unordered_map<std::string, std::queue<std::string>> topicQueues;\n  std::mutex m;\npublic:\n  Dispatcher(size_t n): ring(n) {}\n  bool enqueue(const std::string& topic, const std::string& data){\n    if (!ring.try_enqueue(topic + \":\" + data)) return false;\n    std::lock_guard<std::mutex> lg(m);\n    topicQueues[topic].push(data);\n    return true;\n  }\n  bool dequeue(std::string& outTopicData){\n    std::lock_guard<std::mutex> lg(m);\n    for (auto &kv : topicQueues){\n      if (!kv.second.empty()){\n        outTopicData = kv.first + \":\" + kv.second.front();\n        kv.second.pop();\n        return true;\n      }\n    }\n    return false;\n  }\n};\n```\n\n## Follow-up Questions\n- How would you scale this across multiple machines and maintain per-topic ordering?\n- How would you test for head-of-line blocking and backpressure stability under bursty loads?","diagram":null,"difficulty":"advanced","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","OpenAI","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T21:08:56.924Z","createdAt":"2026-01-21T21:08:56.924Z"},{"id":"q-5444","question":"In a live video encoding service, dozens of producers submit encoding jobs tagged by tenant_id to a bounded system. Design a hierarchical, bounded queue with per-tenant queues feeding a global worker pool. Each tenant has a max concurrent jobs quota; the global pool has a total cap. Explain enqueue/dequeue, backpressure propagation, starvation avoidance, and how you would test correctness and latency under burst traffic. Provide a language-agnostic sketch?","answer":"Two-tier queue: each tenant has a bounded local queue (e.g., 4 in-flight). A global semaphore caps total in-flight jobs (e.g., 16). Producers enqueue to their tenant queue; a scheduler dispatches when capacity permits, enforcing per-tenant quotas and global limits.","explanation":"## Why This Is Asked\nTests multi-tenant concurrency, backpressure, and fairness at scale, relevant to Stripe/Netflix encoding/processing pipelines.\n\n## Key Concepts\n- Hierarchical queues, per-tenant quotas\n- Global in-flight cap, backpressure propagation\n- Fairness vs. throughput, starvation avoidance\n- Observability and test strategies under burst traffic\n\n## Code Example\n\n```pseudo\n// pseudo: enqueue then dispatch respecting quotas\nfor prod in producers:\n  tenant = prod.tenant\n  if tenant.queue.size < TENANT_QMAX and global.inflight < GLOBAL_MAX:\n     tenant.queue.push(prod)\n     global.inflight++\n     dispatch_to_worker_pool(prod)\n```\n\n## Implementation Details\n\n**Enqueue Flow:**\n1. Check tenant queue capacity\n2. Check global in-flight limit\n3. If both available, enqueue and increment counters\n4. Else apply backpressure (reject or block producer)\n\n**Dequeue Flow:**\n1. Worker completes job → decrement global counter\n2. Scheduler selects next job using fair scheduling (e.g., round-robin across tenant queues)\n3. Dispatch selected job to available worker\n\n**Backpressure Propagation:**\n- Tenant queue full → reject new jobs for that tenant\n- Global pool full → reject all new jobs\n- Implement exponential backoff or circuit breaker patterns\n\n**Starvation Avoidance:**\n- Weighted fair queuing: prioritize tenants with lower utilization\n- Minimum guaranteed slots per tenant\n- Aging mechanism: boost priority for waiting jobs\n\n**Testing Strategy:**\n- Load testing with burst patterns (Poisson arrival)\n- Chaos testing: simulate worker failures\n- Metrics: queue depths, latency percentiles, fairness index\n- Property-based testing: invariant verification\n\n**Observability:**\n- Per-tenant queue depths and wait times\n- Global pool utilization\n- Backpressure events and rejection rates\n- Worker throughput and job completion times","diagram":null,"difficulty":"advanced","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Netflix","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T06:00:04.105Z","createdAt":"2026-01-21T22:42:01.694Z"},{"id":"q-5801","question":"You’re building a high-throughput streaming service for financial trades. Multiple producers enqueue trades into a bounded buffer; a small pool of workers applies risk checks and routes trades to downstream systems. Design a concurrency solution that guarantees per-instrument total ordering, backpressure, and fault-tolerance with at-least-once semantics. Provide a runnable sketch in Rust or Java showing enqueue, per-instrument dispatch, and recovery via a durable log?","answer":"Use a two-tier model: a global bounded queue to backpressure producers and per-instrument FIFO queues consumed by a small fixed worker pool. Route each trade to its instrument queue, guarded by a mono","explanation":"## Why This Is Asked\nThis probes multi-tier concurrency, per-key ordering, backpressure, and crash-recovery trade-offs in production-grade streaming.\n\n## Key Concepts\n- Bounded backpressure and throughput\n- Per-instrument ordering via per-key queues and sequence numbers\n- Durable logs (WAL) and idempotent handlers\n- Crash recovery and replay semantics\n- Testing strategies under burst\n\n## Code Example\n```rust\n// Minimal sketch of routing trades to per-instrument queues with sequence checks\nuse std::collections::HashMap;\nstruct Trade { instrument: String, seq: u64 }\nfn main() { /* enqueue, route, and replay logic placeholder */ }\n```\n\n## Follow-up Questions\n- How would you test per-instrument ordering under bursty load?\n- How would you measure trade-loss vs duplicate-delivery in failure scenarios?","diagram":"flowchart TD\n  A[Producers] --> B[GlobalBoundedQueue]\n  B --> C{InstrumentQueues}\n  C --> D[Workers] --> E[Downstream]\n  B --> F[WAL]\n  F --> G[Recovery]\n","difficulty":"advanced","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","MongoDB","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T17:10:45.875Z","createdAt":"2026-01-22T17:10:45.876Z"},{"id":"q-5847","question":"Design a beginner-friendly concurrent workflow for a video-processing service: hundreds of producers submit video render jobs tagged by videoId. A bounded global queue accepts jobs; ensure per-video in-order processing while keeping a small worker pool. Propose a concrete Go or Python asyncio design that guarantees per-video FIFO, backpressure, and test plan to verify ordering and throughput?","answer":"Implement a bounded global queue and a per-video sequencer. Use a small worker pool for transcoding. For each videoId maintain a dedicated channel; the router forwards jobs from the global queue to th","explanation":"## Why This Is Asked\n\nConveys a concrete concurrency problem with per-key ordering and backpressure that beginners can approach yet still demonstrates deeper understanding.\n\n## Key Concepts\n\n- Concurrency control\n- Per-key ordering\n- Bounded queues\n- Backpressure\n- Testing strategies\n\n## Code Example\n\n```go\npackage main\n\ntype Job struct { VideoID string; Payload []byte }\n\nfunc main() {\n  // scaffold: inbound bounded channel, per-video queues, and a small worker pool\n}\n```\n\n## Follow-up Questions\n\n- How would you scale to millions of videoIds without unbounded goroutines?\n- How would you verify ordering guarantees under bursty input?\n","diagram":null,"difficulty":"beginner","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T19:09:36.760Z","createdAt":"2026-01-22T19:09:36.760Z"},{"id":"q-5904","question":"Design a concurrent per-shard event processor for a real-time analytics pipeline: events arrive from multiple producers into N shards, each with a bounded queue and dedicated workers. Implement backpressure to prevent overflow, fairness across shards, and strict per-shard ordering. How would you verify ordering, backpressure, and starvation avoidance with realistic unit and integration tests?","answer":"Partition events by shard; each shard maintains a bounded queue with a dedicated worker to preserve strict per-shard ordering. Map producers to shards using consistent hashing, and implement backpressure through per-shard capacity limits that cause producers to block when queues are full.","explanation":"## Why This Is Asked\n\nThis question tests a realistic concurrency pattern involving partitioned streams with local ordering guarantees, bounded buffers, and backpressure mechanisms. It also evaluates understanding of cross-partition fairness and practical testing strategies for distributed systems.\n\n## Key Concepts\n\n- Partitioned concurrency and data locality\n- Bounded buffers and backpressure propagation\n- In-order processing within partitions\n- Cross-partition fairness and starvation prevention\n- Realistic unit and integration testing approaches\n\n## Code Example\n\n```go\npackage main\n\ntype Event struct {\n    ID      string\n    ShardID int\n    Data    []byte\n}\n\ntype ShardProcessor struct {\n    queue   chan Event\n    worker  func(Event)\n    done    chan struct{}\n}\n\ntype ConcurrentProcessor struct {\n    shards    []*ShardProcessor\n    shardCount int\n}\n\nfunc NewConcurrentProcessor(shardCount int, queueSize int, worker func(Event)) *ConcurrentProcessor {\n    processor := &ConcurrentProcessor{\n        shards:     make([]*ShardProcessor, shardCount),\n        shardCount: shardCount,\n    }\n    \n    for i := 0; i < shardCount; i++ {\n        processor.shards[i] = &ShardProcessor{\n            queue:  make(chan Event, queueSize),\n            worker: worker,\n            done:   make(chan struct{}),\n        }\n        go processor.shards[i].process()\n    }\n    \n    return processor\n}\n\nfunc (p *ConcurrentProcessor) getShardID(eventID string) int {\n    hash := fnv.New32a()\n    hash.Write([]byte(eventID))\n    return int(hash.Sum32()) % p.shardCount\n}\n\nfunc (p *ConcurrentProcessor) Process(event Event) error {\n    shardID := p.getShardID(event.ID)\n    event.ShardID = shardID\n    \n    select {\n    case p.shards[shardID].queue <- event:\n        return nil\n    default:\n        return errors.New(\"shard queue full - backpressure applied\")\n    }\n}\n\nfunc (s *ShardProcessor) process() {\n    for {\n        select {\n        case event := <-s.queue:\n            s.worker(event)\n        case <-s.done:\n            return\n        }\n    }\n}\n\nfunc (p *ConcurrentProcessor) Close() {\n    for _, shard := range p.shards {\n        close(shard.done)\n    }\n}\n```\n\n## Testing Strategy\n\n### Unit Tests\n- Verify per-shard ordering with sequential events\n- Test backpressure when queues reach capacity\n- Validate consistent hashing distributes events evenly\n\n### Integration Tests\n- Simulate multiple producers with varying rates\n- Monitor queue depths and processing latency\n- Verify no shard starvation under uneven load\n\n### Property-Based Tests\n- Generate random event streams and verify invariants\n- Test edge cases: empty queues, rapid producer bursts, worker failures","diagram":null,"difficulty":"intermediate","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","PayPal","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T05:56:41.653Z","createdAt":"2026-01-22T21:43:39.668Z"},{"id":"q-6036","question":"In a real-time chat platform: dozens of chat rooms publish messages to a central router. Each room's messages must be delivered in the order published, while a global cap on in-flight messages bounds memory. Design a beginner-friendly concurrency solution (Go or Python asyncio) with: a global bounded queue, per-room FIFO buffers, a small worker pool, and a mechanism to retire inactive rooms to reclaim resources. Explain backpressure, ordering, and a simple test plan?","answer":"Design a central bounded channel for in-flight messages, a map of per-room FIFO buffers, and a small worker pool. Producers enqueue into the room buffer, a router forwards to the global bound, and wor","explanation":"## Why This Is Asked\n\nAssesses practical concurrency skills: bounded resources, per-key ordering, and resource reclamation.\n\n## Key Concepts\n\n- bounded channels and backpressure\n- per-key FIFO ordering\n- inactivity-based retirement to avoid leaks\n- testing nondeterminism and ordering\n\n## Code Example\n\n```go\npackage main\ntype Message struct {\n  Room string\n  ID   int\n  Body string\n}\n```\n\n## Follow-up Questions\n\n- How would you handle sudden bursty traffic across many rooms?\n- How would you verify ordering guarantees under concurrent producers?","diagram":null,"difficulty":"beginner","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","Salesforce","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T05:52:34.642Z","createdAt":"2026-01-23T05:52:34.642Z"},{"id":"q-6204","question":"Intermediate: In a real-time event processor, N producers emit events into a bounded global queue of size Q. A pool of M workers consumes events. Each event has a key (e.g., user_id). Guarantee per-key in-order processing across all producers while allowing parallel processing for different keys. Implement per-key FIFO buffers with a maximum size K and a global backpressure mechanism when the queue is full. Decide overflow policy (drop oldest, drop new, or reject) and justify?","answer":"Two-layer: a global bounded queue (size Q) and per-key FIFO buffers with cap K. A dispatcher routes next event for a key to an available worker, preserving in-order delivery per key. Use a semaphore t","explanation":"## Why This Is Asked\nTests ability to design multithreaded pipelines with per-key ordering and bounded buffers, a common real-time constraint in payment/telemetry systems. It also probes overflow policies and fairness under contention.\n\n## Key Concepts\n- Per-key in-order processing\n- Bounded buffers and global backpressure\n- Overflow policy and starvation considerations\n\n## Code Example\n```javascript\n// Dispatcher sketch: map<Key, FIFO>; global semaphore for Q\n```\n\n## Follow-up Questions\n- How would you extend to dynamically adjust K or Q under load?\n- How would you verify ordering and backpressure with realistic workloads?","diagram":"flowchart TD\n  A[Producers] --> B[Global Queue]\n  B --> C[Dispatcher]\n  C --> D[Worker Pool]\n  D --> E[Per-key Buffers]","difficulty":"intermediate","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Coinbase","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T14:34:48.735Z","createdAt":"2026-01-23T14:34:48.735Z"},{"id":"q-6247","question":"In a streaming analytics system, dozens of data shards emit aggregation tasks that must be processed within strict latency budgets. Design a bounded, hierarchical queue: a global bounded backlog that feeds per-shard FIFO queues, with a fixed worker pool. Enforce backpressure to producers when full, isolate shards to prevent starvation, and implement a fairness policy (e.g., weighted round-robin or tokens) to allocate processing time fairly across shards. Include a runnable minimal sketch in Rust or C++ showing enqueue and dequeue semantics and a basic test strategy to verify latency and ordering?","answer":"Two-tier scheduler: a global bounded queue feeding per-shard FIFO queues with a fixed worker pool. Producers backpressure when global is full. Scheduler uses weighted round-robin to allocate CPU fairl","explanation":"## Why This Is Asked\n\nTests ability to design a two-level concurrency control that scales, maintains per-shard ordering, and exposes backpressure and fairness under bursty traffic.\n\n## Key Concepts\n\n- Bounded global queue with per‑shard FIFO queues\n- Backpressure strategies and producer throttling\n- Fair scheduling across shards (weighted RR or token buckets)\n- Graceful shutdown, error propagation, testability\n\n## Code Example\n\n```rust\nuse std::sync::mpsc::{sync_channel, SyncSender, Receiver};\nuse std::thread;\n\nfn main() {\n  // Global bounded queue: (shard_id, value)\n  let (global_tx, global_rx) = sync_channel::<(usize, i32)>(8);\n\n  // Per-shard queues\n  let (shard1_tx, shard1_rx) = sync_channel::<i32>(4);\n  let (shard2_tx, shard2_rx) = sync_channel::<i32>(4);\n\n  // Dispatcher: routes by shard\n  thread::spawn(move || {\n    while let Ok((sid, v)) = global_rx.recv() {\n      if sid == 1 { shard1_tx.send(v).unwrap(); }\n      else { shard2_tx.send(v).unwrap(); }\n    }\n  });\n\n  // Simple workers: pull from shard queues\n  let _w1 = thread::spawn(move || {\n    while let Ok(x) = shard1_rx.recv() { /* process x preserving shard1 order */ }\n  });\n  let _w2 = thread::spawn(move || {\n    while let Ok(x) = shard2_rx.recv() { /* process x preserving shard2 order */ }\n  });\n\n  // enqueue example\n  global_tx.send((1, 123)).unwrap();\n  global_tx.send((2, 456)).unwrap();\n\n  // graceful shutdown omitted for brevity\n}\n```\n\n## Follow-up Questions\n\n- How to extend to N shards and ensure global fairness without starving any shard?\n- How to measure tail latency and verify ordering guarantees under burst?","diagram":"flowchart TD\n  G[GlobalBoundedQueue] --> S1[Shard1Queue]\n  G --> S2[Shard2Queue]\n  S1 --> W[WorkerPool]\n  S2 --> W\n  W --> O[Output]","difficulty":"advanced","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Hashicorp","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T16:10:58.768Z","createdAt":"2026-01-23T16:10:58.768Z"},{"id":"q-6366","question":"Design a real-time event router for a chat platform that guarantees per-room FIFO processing while enabling parallelism across rooms. N producers enqueue messages into a bounded global ring buffer of size C. Implement a lock-free multi-producer/multi-consumer router using per-room local queues inside workers and a global scheduler to balance backpressure. On overflow, apply a room-aware eviction: remove the oldest message from the least recently active room to preserve freshness for active rooms while preserving per-room order. Provide concrete data structures and outline a practical test plan validating per-room ordering, backpressure behavior, and starvation avoidance?","answer":"Approach: a bounded global ring buffer with per-room FIFO queues inside workers; use CAS-based handoffs to avoid locks; maintain an LRU of active rooms to guide eviction. Overflow discards oldest mess","explanation":"## Why This Is Asked\nEvaluates backpressure handling, lock-free coordination, and keeping per-room order under high throughput.\n\n## Key Concepts\n- Lock-free MPMC queues, per-room FIFO, global scheduler\n- Bounded buffer, eviction policy, starvation avoidance\n\n## Code Example\n```javascript\n// Pseudocode sketch: enqueue -> route to per-room queue; eviction logic\n```\n\n## Follow-up Questions\n- How to ensure memory safety with recycling and ABA issues?\n- How would you instrument tests for livelock or unfair eviction?","diagram":"flowchart TD\n  P[N Producers] --> Q[Global Ring Buffer (size C)]\n  Q --> S[Scheduler]\n  S --> W[Worker Threads with Per-Room Queues]\n  W --> B[Backend Delivery]","difficulty":"intermediate","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Discord","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T20:55:41.232Z","createdAt":"2026-01-23T20:55:41.232Z"},{"id":"q-6535","question":"In a real-time multi-producer task queue, each task has a unique ID and must be processed exactly once. Producers submit tasks concurrently; workers may crash and tasks can retry. Design a beginner-friendly concurrency solution in Go or Python asyncio with a bounded global queue, a per-task dedup buffer, a small worker pool, and a crash-recovery protocol. Explain idempotence, backoff, and a concrete test plan?","answer":"Use a durable in-flight map keyed by taskID to track status; mark inflight on enqueue, complete on success, and persist to a durable log for crash recovery. Workers pull from a bounded channel; before","explanation":"## Why This Is Asked\nTests knowledge of exactly-once semantics and crash recovery in a beginner-friendly setting.\n\n## Key Concepts\n- Bounded queues, dedup buffers, idempotent work\n- Crash recovery via durable logs\n- Backoff strategies and simple retry limits\n\n## Code Example\n```javascript\n// Outline: pseudo Go/async code sketch\n```\n\n## Follow-up Questions\n- How would you implement durable storage for in-flight tasks?\n- How would you test recovery under power failure?","diagram":null,"difficulty":"beginner","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","Netflix","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T07:16:37.684Z","createdAt":"2026-01-24T07:16:37.684Z"},{"id":"q-6544","question":"Design a multi-tenant, bounded-concurrency task processor: N tenants push events into a shared queue of capacity Q. Each tenant’s events must be processed in per-tenant order, but tenants can run in parallel. Implement per-tenant FIFO buffers of size K, a dynamic cadence scheduling window per tenant (timer-wheel) to control throughput, and a fairness/backpressure policy that prevents starvation under burst traffic. Describe data structures, scheduling rules, and a minimal test plan?","answer":"Use per-tenant FIFO buffers of size K feeding a global bounded queue Q. Each tenant gets a dynamic cadence window controlled by a timer-wheel, with tokens that throttle bursty tenants but allow fast t","explanation":"## Why This Is Asked\n\nTests understanding of advanced scheduling, per-tenant ordering, backpressure, and fairness under bursty multi-tenant workloads.\n\n## Key Concepts\n\n- Per-tenant FIFO with bounded buffers\n- Dynamic cadence scheduling using a timer-wheel\n- Token/credit-based fairness and backpressure\n- Work-stealing for load balance\n\n## Code Example\n\n```javascript\n// Skeleton only: data structures and flow\nclass TenantBuffer { constructor(id, K) { this.id=id; this.q=[]; this.K=K; } enqueue(t){ if(this.q.length < this.K) this.q.push(t); else throw 'Buffer full'; } dequeue(){ return this.q.shift(); } }\nclass Scheduler { constructor(Q){ this.global= []; } /* placeholder */ }\n```\n\n## Follow-up Questions\n\n- How would you detect and prevent starvation if one tenant is always producing? \n- How would you test ordering guarantees under skewed workloads?\n","diagram":"flowchart TD\n  A[Producers per tenant] --> B[Per-tenant FIFO buffers (K)]\n  B --> C[Global bounded queue (Q)]\n  C --> D[Worker pool (M)]\n  D --> E[Cross-tenant work-stealing]\n  B --- F[Cadence timer-wheel per tenant] --> C","difficulty":"intermediate","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["PayPal","Snap","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T07:33:53.176Z","createdAt":"2026-01-24T07:33:53.176Z"},{"id":"q-6592","question":"In a distributed telemetry pipeline for a fleet of delivery robots, producers emit events with robot_id, seq, and payload. Design a concurrency strategy to process events with per-robot in-order handling, a bounded in-flight cap, and exactly-once persistence to an external store. Requirements: 1) bounded global queue with capacity, 2) per-robot buffers to preserve order, 3) a small worker pool to apply side effects, 4) handle late/duplicate events, 5) provide a runnable sketch in Go or Rust and a test plan for ordering, liveness, and recovery. How would you implement this?","answer":"Use a bounded global queue with capacity K and per-robot buffers. Incoming events route to per-robot buffers in sequence order; workers fetch in-order from each buffer. Track next_seq per robot and pe","explanation":"## Why This Is Asked\nTests ability to design scalable, low-latency ingestion with strict per-entity ordering and strong delivery guarantees, under memory pressure. It probes backpressure handling, out-of-order recycling, and failure recovery.\n\n## Key Concepts\n- Bounded queues and backpressure\n- Per-entity in-order processing\n- Exactly-once semantics via commit logs and idempotent writes\n- Late/duplicate event handling and recovery strategies\n\n## Code Example\n```rust\n// Minimal sketch: per-robot buffers + next_seq tracking\n```\n\n## Follow-up Questions\n- How would you scale the worker pool under bursty traffic?\n- How would you test failure scenarios (partial outages, replay) and monitor liveness?","diagram":"flowchart TD\n  P[Producers] --> Q[Global Queue (bounded)]\n  Q --> W[Worker Pool]\n  Q --> B[Per-Robot Buffers]\n  B --> W\n  W --> S[External Store]\n","difficulty":"advanced","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Microsoft","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T09:31:31.156Z","createdAt":"2026-01-24T09:31:31.156Z"},{"id":"q-6638","question":"Design a high-throughput event processor where dozens of producers publish events across many tenants. A single bounded global queue feeds a small worker pool. Each tenant’s events must be delivered in-per-tenant order, while events across tenants may interleave. Propose a concrete Go or Rust design using per-tenant FIFOs and a central arbiter, with backpressure, fault tolerance, and a test strategy to verify ordering and throughput under burst?","answer":"Use a fixed-size bounded ring buffer for the global queue and per-tenant FIFO subqueues. An arbiter assigns workers to the oldest event from a tenant, guaranteeing per-tenant order while preserving in","explanation":"## Why This Is Asked\n\nThis task probes the ability to design a scalable, low-latency concurrency system with strict per-tenant ordering and global backpressure. It touches race-condition prevention, memory visibility, and fault tolerance under bursty load.\n\n## Key Concepts\n\n- Bounded global queue and per-tenant FIFOs\n- Arbiter that preserves per-tenant order\n- Backpressure, liveness, and fault tolerance\n- Sequencing, exactly-once vs at-least-once semantics\n\n## Code Example\n\n```go\npackage main\n\ntype Event struct {\n  Tenant string\n  Seq    uint64\n  Payload []byte\n}\n\n// Skeleton: ring buffer + per-tenant queues + arbiter\ntype RingBuffer struct { /* ... */ }\ntype Arbiter struct {\n  // map[Tenant]string -> TenantQueue\n}\n\nfunc (rb *RingBuffer) Enqueue(e Event) bool { /* bounded enqueue */ }\nfunc (a *Arbiter) Dispatch() *Event { /* pick oldest ready event per tenant, assign to worker */ }\n```\n\n## Follow-up Questions\n\n- How to handle worker failure or slow tenants without starving others?\n- How would you tune backpressure thresholds under different burst patterns?\n- What tests would you run to verify ordering guarantees and detect subtle race conditions?","diagram":"flowchart TD\n  P1(Producers) --> GQ[Global bound queue]\n  GQ --> A[Arbiter]\n  A --> W[Worker Pool]\n  subgraph Tenants\n    T1[Tenant FIFO 1]\n    T2[Tenant FIFO 2]\n  end\n  W --> C[Consumer]\n","difficulty":"advanced","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T10:55:29.056Z","createdAt":"2026-01-24T10:55:29.056Z"},{"id":"q-6653","question":"Design an intermediate concurrency interview problem: implement a three-stage streaming pipeline with bounded buffers: Ingress with N producers emits keyed records into a first queue; Transformer stage with M workers applies stateless processing; Aggregator stage with K workers computes per-key aggregates. Guarantee per-key in-order processing across all stages under backpressure, support dynamic worker resizing, and define a clear policy for late or out-of-order messages. Provide data structures and a concrete test plan?","answer":"Use a three-stage bounded pipeline with per-key sequencing. Maintain a per-key sequence counter and an in-flight buffer for that key in each stage; upstream producers block when their key’s buffer is ","explanation":"## Why This Is Asked\n\nTests understanding of multi-stage concurrency, per-key ordering across stages, and robust backpressure propagation. Requires designing data structures that preserve ordering while allowing parallelism and handling dynamic worker changes.\n\n## Key Concepts\n\n- Multi-stage bounded pipelines with per-key sequencing\n- Cross-stage backpressure and credit-based flow control\n- Late/out-of-order handling with per-key versioning\n- Dynamic worker resizing and fairness under load\n\n## Code Example\n\n```javascript\nclass PerKeySequencer {\n  constructor() {\n    this.map = new Map();\n  }\n  next(key, seq) {\n    const prev = this.map.get(key) || Promise.resolve();\n    const nextP = prev.then(() => seq);\n    this.map.set(key, nextP);\n    return nextP;\n  }\n}\n```\n\n## Follow-up Questions\n\n- How would you test backpressure propagation across stages under bursty traffic?\n- How would you extend this to support exactly-once semantics with idempotent downstream aggregations?","diagram":null,"difficulty":"intermediate","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Scale Ai","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T11:43:12.543Z","createdAt":"2026-01-24T11:43:12.543Z"},{"id":"q-6689","question":"Design a per-topic multi-stage streaming pipeline with dynamic worker pools and per-topic in-order guarantees. Producers publish topic-labeled messages to a bounded input queue. Workers route messages through a topic-specific Dispatcher, Processor, and Sink chain, each with bounded buffers. Ensure per-topic ordering, allow resizing the worker pool per topic, and backpressure that prevents starvation for hot topics. Include data structures, synchronization, and a test plan for late messages and topic churn?","answer":"Design a per-topic multi-stage streaming pipeline with dynamic worker pools and per-topic in-order guarantees. Producers publish topic-labeled messages to a bounded input queue. Workers route messages","explanation":"## Why This Is Asked\nTests ability to design modular, scalable concurrency with per-topic isolation, dynamic scaling, and robust backpressure.\n\n## Key Concepts\n- Per-topic ordering and isolation\n- Dynamic worker resizing and backpressure\n- Bounded buffers and cross-stage synchronization\n- Late/out-of-order message handling and churn resilience\n\n## Code Example\n```javascript\n// Pseudocode sketch of core state per topic\ntype TopicState = {\n  dispatchQ: FIFOQueue<Message>;\n  procQ: FIFOQueue<Message>;\n  sinkQ: FIFOQueue<Message>;\n  nextSeq: number;\n  inFlight: number;\n  workers: number;\n}\n```\n\n## Follow-up Questions\n- How would you test hot vs cold topic starvation under dynamic resizing?\n- How do you guarantee lock-free progress under contention while preserving order?","diagram":"flowchart TD\n  P[Producers] --> D[Dispatcher: per-topic queues]\n  D --> W[Worker pools: per-topic]\n  W --> S[Sink]\n  subgraph per-topic buffers\n    D --> QD[DispatchQ]\n    W --> QP[ProcQ]\n    W --> QS[SinkQ]\n  end","difficulty":"intermediate","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Hashicorp"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T13:39:10.433Z","createdAt":"2026-01-24T13:39:10.434Z"},{"id":"q-6750","question":"Design a concurrent in-memory rate-limiter router for a fleet of microservices. N producers emit requests labeled by service_key; a bounded global queue feeds M workers. Implement per-key token buckets with independent refill rates and a global budget to cap total in-flight permits. Support dynamic key creation, backpressure signaling when the global queue is full, and a clear burst-idle policy. Provide concrete data structures and a test plan?","answer":"Implement a per-key token-bucket rate limiter with a global budget. Each service_key has its own bucket refilled at its rate; total tokens across keys capped by a shared budget. Use a concurrent map w","explanation":"## Why This Is Asked\nTests integration of per-key throttling with system-wide backpressure, plus dynamic key handling under contention.\n\n## Key Concepts\n- Per-key rate limiting with independent refills\n- Global budget for overall control\n- Lock-free or low-contention data structures\n- Backpressure signaling and verification\n\n## Code Example\n```javascript\n// sketch\n```\n\n## Follow-up Questions\n- How would you adapt to multi-region deployments with shard budgets?\n- How would you validate fairness and monotonicity under bursty traffic?\n","diagram":"flowchart TD\n  A[Producers] --> B[Bounded Global Queue]\n  B --> C[M Workers]\n  C --> D[Downstream Store]\n  E[Global Budget] --> B","difficulty":"intermediate","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","LinkedIn","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T15:42:46.050Z","createdAt":"2026-01-24T15:42:46.050Z"},{"id":"q-682","question":"In a service handling image uploads, each file triggers a resize and thumbnail generation pipeline. Design a bounded producer-consumer queue in Python using asyncio. Use a Queue with maxsize, a fixed number of worker coroutines, and backpressure so producers await when full. Include clean shutdown and error handling. Provide a runnable minimal example showing enqueue, worker loop, and cancellation?","answer":"Use an asyncio.Queue(maxsize=128) with 4 worker coroutines. Producers await queue.put(task) to apply backpressure. Each worker pops tasks, processes image transforms, then calls queue.task_done(). On ","explanation":"## Why This Is Asked\n\nAssesses practical concurrency skills: bounded queues, worker pools, backpressure, and robust shutdown in a real feature like image processing.\n\n## Key Concepts\n\n- asyncio.Queue(maxsize) and backpressure\n- fixed-size worker pool\n- graceful shutdown and cancellation\n- error handling and observability\n\n## Code Example\n\n```python\n# Minimal outline of the pattern (not a full solution)\nimport asyncio\n\nclass Task:\n    def __init__(self, data): self.data = data\n\nasync def worker(q):\n    while True:\n        t = await q.get()\n        try:\n            # process t\n            pass\n        finally:\n            q.task_done()\n\nasync def main():\n    q = asyncio.Queue(maxsize=128)\n    workers = [asyncio.create_task(worker(q)) for _ in range(4)]\n    # enqueue tasks\n    for item in range(10):\n        await q.put(Task(item))\n    await q.join()\n    for w in workers: w.cancel()\n\nasyncio.run(main())\n```\n\n## Follow-up Questions\n\n- How would you adjust for burst traffic without starving producers?\n- How can you monitor queue depth and worker latency in production?","diagram":null,"difficulty":"beginner","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Lyft","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T15:58:30.388Z","createdAt":"2026-01-11T15:58:30.388Z"},{"id":"q-687","question":"Write a small Python snippet: create a shared counter initialized to 0, spawn 4 threads that increment it 1000 times each, using a Lock to protect the increment. After joining, print the final value. Explain what happens if the lock is removed and how atomicity is ensured. What value do you expect and why?","answer":"Use a Lock around the increment: with lock: counter += 1 in each thread. Spawn 4 threads, each loops 1000 times. Final value should be 4000. Without the lock, race conditions may yield a final value l","explanation":"## Why This Is Asked\n\nThis checks understanding of race conditions, mutual exclusion, and practical use of locks in a tiny concurrency exercise.\n\n## Key Concepts\n\n- Race conditions and protection via locks\n- Atomicity in read-modify-write sequences\n- Trade-offs of locks vs lock-free structures\n\n## Code Example\n\n```python\nimport threading\ncounter = 0\nlock = threading.Lock()\ndef worker():\n    global counter\n    for _ in range(1000):\n        with lock:\n            counter += 1\nthreads = [threading.Thread(target=worker) for _ in range(4)]\nfor t in threads: t.start()\nfor t in threads: t.join()\nprint(counter)\n```\n\n## Follow-up Questions\n\n- What happens if a thread raises an exception inside the critical section?\n- How would you test this under CPU-bound vs I/O-bound workloads?","diagram":"flowchart TD\n  A[Start] --> B[Create 4 threads]\n  B --> C[Each thread increments 1000x]\n  C --> D[Acquire lock for increment]\n  D --> E[Increment counter]\n  E --> F[Join threads]\n  F --> G[Print final value]\n  G --> H[End]","difficulty":"beginner","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Tesla","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T16:22:26.189Z","createdAt":"2026-01-11T16:22:26.189Z"},{"id":"q-6884","question":"In a real-time telemetry pipeline for multi-region IoT devices, design a bounded, region-scoped concurrency model that preserves per-region event order while capping memory usage. Describe data structures, backpressure signaling, and region lifecycle, and provide a runnable prototype outline in Go or Python showing enqueue, per-region dispatch, backpressure, and graceful shutdown. What tests would you add to verify ordering and backpressure?","answer":"Implement region-scoped FIFO queues with a global bounded semaphore for memory control. Each region maintains its own ordered queue; a fixed worker pool drains region queues in round-robin fashion to preserve per-region event ordering while ensuring fairness across regions. Backpressure propagates through quota exhaustion, triggering Retry-After headers to upstream producers. Regions enter retirement after configurable inactivity periods and reactivate automatically on new events.","explanation":"## Why This Is Asked\nTests multi-region concurrency, backpressure propagation, and dynamic lifecycle management in realistic telemetry pipelines.\n\n## Key Concepts\n- Region-scoped FIFO queues maintaining strict ordering\n- Global bounded capacity with per-region quotas\n- Fairness across regions via round-robin draining\n- Backpressure signaling with exponential backoff\n- Idle-region retirement and automatic reactivation\n\n## Code Example\n```javascript\n// sketch: region queue management and worker coordination\n```\n\n## Follow-up Questions\n- How would you measure end-to-end latency under burst conditions?\n- What monitoring metrics would you track for system health?\n- How would you handle region-specific priority requirements?","diagram":"flowchart TD\n  ProducerA --> RegionAQueue\n  ProducerB --> RegionBQueue\n  RegionAQueue --> WorkerPoolA\n  RegionBQueue --> WorkerPoolB\n  RegionAQueue --> Downstream\n  RegionBQueue --> Downstream","difficulty":"advanced","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","Salesforce","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T06:17:07.964Z","createdAt":"2026-01-24T21:36:47.113Z"},{"id":"q-696","question":"Context: in a real-time analytics pipeline for a video-conference system, dozens of producers emit events into a shared queue and multiple workers consume them. Implement a bounded, multi-producer, multi-consumer queue with capacity 1024. It must not drop messages while not full, block producers when full, support concurrent consumers, and support a clean shutdown. Describe API, invariants, and a simple synchronization strategy?","answer":"Use a fixed 1024-slot circular buffer guarded by a ReentrantLock with two Conditions: notFull and notEmpty. Producers await notFull; consumers await notEmpty. Update head, tail, and count under lock; ","explanation":"## Why This Is Asked\n- Tests practical concurrency design under load with backpressure and shutdown semantics. **Expects** concrete data structures and synchronization choices.\n\n## Key Concepts\n- **Thread-safety**, **bounded buffers**, and **backpressure**\n- Correct handling of spurious wakeups\n- Liveness guarantees and clean shutdown\n\n## Code Example\n\n```java\nclass BoundedMPMCQueue<T> {\n  private final Object[] buf; private int head=0, tail=0, count=0;\n  private final ReentrantLock lock = new ReentrantLock();\n  private final Condition notFull = lock.newCondition();\n  private final Condition notEmpty = lock.newCondition();\n  private volatile boolean shutdown = false;\n\n  public BoundedMPMCQueue(int capacity){ buf = new Object[capacity]; }\n\n  public void enqueue(T item) throws InterruptedException {\n    lock.lock();\n    try {\n      while (count == buf.length && !shutdown) notFull.await();\n      if (shutdown) throw new InterruptedException();\n      buf[tail] = item; tail = (tail+1) % buf.length; count++;\n      notEmpty.signalAll();\n    } finally { lock.unlock(); }\n  }\n\n  public T dequeue() throws InterruptedException {\n    lock.lock();\n    try {\n      while (count == 0 && !shutdown) notEmpty.await();\n      if (shutdown && count == 0) return null;\n      @SuppressWarnings(\"unchecked\") T item = (T) buf[head]; buf[head] = null; head = (head+1) % buf.length; count--;\n      notFull.signalAll();\n      return item;\n    } finally { lock.unlock(); }\n  }\n\n  public void shutdown() {\n    lock.lock(); try { shutdown = true; notFull.signalAll(); notEmpty.signalAll(); } finally { lock.unlock(); }\n  }\n}\n```\n\n## Follow-up Questions\n- How would you test for fairness and avoid starvation?\n- How would you adapt this to multiple machines or queue reuse across restarts?","diagram":null,"difficulty":"intermediate","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T17:17:04.688Z","createdAt":"2026-01-11T17:17:04.688Z"},{"id":"q-708","question":"Design a bounded, concurrent, multi-priority work scheduler for a rendering service: 3 priority levels (0 highest). Producers enqueue tasks into per-priority queues with backpressure; workers drain from the highest non-empty queue (0→1→2). Include aging to prevent starvation and a graceful shutdown sentinel. Provide a runnable minimal Java example?","answer":"Implement a bounded, multi-priority scheduler with three per-priority queues (0 high). Producers call offer with a timeout to apply backpressure. Workers poll queues in priority order (0, then 1, then","explanation":"## Why This Is Asked\nTests ability to design correct, robust concurrency under backpressure with priority aging and graceful shutdown.\n\n## Key Concepts\n- Bounded queues and backpressure\n- Priority-aware scheduling\n- Fairness via aging\n- Graceful shutdown and cancellation\n\n## Code Example\n```java\nimport java.util.concurrent.*;\n\nclass WorkItem { final int priority; final Runnable task; WorkItem(int p, Runnable t){priority=p;this.task=t;} void run(){task.run();} }\n\npublic class MultiPriorityScheduler {\n  private final BlockingQueue<WorkItem>[] queues = new BlockingQueue[3];\n  private volatile boolean running = true;\n\n  @SuppressWarnings(\"unchecked\")\n  public MultiPriorityScheduler(int cap){\n    for(int i=0;i<3;i++) queues[i] = new ArrayBlockingQueue<>(cap);\n  }\n\n  public boolean submit(WorkItem w, long timeout, TimeUnit unit) throws InterruptedException {\n    return queues[w.priority].offer(w, timeout, unit);\n  }\n\n  public void start(int workerCount){\n    for(int i=0;i<workerCount;i++){\n      new Thread(() -> {\n        while(running){\n          WorkItem w=null;\n          for(int p=0;p<3;p++){\n            w = queues[p].poll(50, TimeUnit.MILLISECONDS);\n            if (w != null){ w.run(); break; }\n          }\n        }\n      }).start();\n    }\n  }\n\n  public void stop(){ running = false; }\n}\n```\n\n## Follow-up Questions\n- How would you implement aging with constant time bounds?\n- How would you adapt this for multiple machines?","diagram":"flowchart TD\n  P[Producer] --> Q0[Queue0]\n  P --> Q1[Queue1]\n  P --> Q2[Queue2]\n  W[Worker] --> Q0\n  W --> Q1\n  W --> Q2\n  Q0 -- has task --> W\n  Q1 -- has task --> W\n  Q2 -- has task --> W","difficulty":"advanced","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Salesforce","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T18:31:21.490Z","createdAt":"2026-01-11T18:31:21.490Z"},{"id":"q-7089","question":"Design a latency-sensitive, per-tenant fair scheduler for a multi-tenant streaming job system: N tenants, each with a dynamic quota and an independent FIFO queue; M workers pull jobs from any tenant queue, preserving per-tenant ordering, enforcing quotas via token buckets, and applying backpressure to tenants when global queue fills. How would you implement data structures, dynamic tenant changes, and a concrete test plan?","answer":"Design a two-layer scheduler: per-tenant FIFO queues guarded by a token-bucket quota, and a central arbiter that assigns M workers via deficit-round-robin to preserve per-tenant order while ensuring f","explanation":"## Why This Is Asked\n\nAssesses ability to design a low-latency, fair concurrency system with per-tenant isolation, dynamic tenant management, and practical backpressure.\n\n## Key Concepts\n\n- Per-tenant FIFO ordering\n- Token-bucket quotas and dynamic tuning\n- Deficit-round-robin scheduling for fairness\n- Global spill/backpressure buffer\n- Dynamic tenant registry and safe rebalancing\n\n## Code Example\n\n```javascript\n// Pseudocode sketch\nclass TenantQueue { constructor(id){...} enqueue(t){...} }\nclass Arbiter { /* assigns slots to tenants */ }\n```\n\n## Follow-up Questions\n\n- How to detect deadlocks under skew and recover?\n- How to test with bursty tenant loads and dynamic churn?","diagram":null,"difficulty":"intermediate","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T08:51:09.957Z","createdAt":"2026-01-25T08:51:09.957Z"},{"id":"q-7101","question":"Design a fault-tolerant, bounded telemetry router: partition by region, with per-device FIFO buffers feeding a small worker pool. Use a central bounded queue to backpressure producers; ensure in-order processing per device via sequence counters and per-device reorder buffers. On regional outage, spill to durable regional store and throttle producers with backpressure. Auto-scale workers from 1 to K based on queue depth and latency?","answer":"Use region-local per-device FIFOs feeding a small worker pool. A central bounded queue enforces backpressure; per-device order is preserved with sequence numbers and a local reorder buffer. Regional o","explanation":"## Why This Is Asked\nAssesses thinking about multi-region concurrency, ordering guarantees, fault tolerance, and dynamic scaling.\n\n## Key Concepts\n- Bounded queues and backpressure\n- Per-device FIFO ordering with sequence counters\n- Regional spill-to-durable storage and failure handling\n- Auto-scaling and observability\n\n## Code Example\n```go\n// Skeleton\ntype Event struct { region string; device string; seq uint64; data []byte }\ntype Router struct {\n  central chan Event\n  regionBuffers map[string]map[string]chan Event\n  nextSeq map[string]uint64\n}\n```\n\n## Follow-up Questions\n- How would you test ordering under regional outages?\n- How would you ensure idempotence and exactly-once processing across regions?","diagram":"flowchart TD\n  A[Producers] --> B[Region Buffers]\n  B --> C[Central Bounded Queue]\n  C --> D[Workers]\n  D --> E[Outputs]","difficulty":"advanced","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Square","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T09:32:33.748Z","createdAt":"2026-01-25T09:32:33.748Z"},{"id":"q-713","question":"Implement a thread-safe bounded queue using a fixed circular buffer with mutex protection and condition variables for coordinating producers and consumers. How would you handle enqueue/dequeue operations and implement graceful shutdown when the queue is no longer accepting new tasks?","answer":"Use a mutex to protect shared state (head, tail, count) and two condition variables: not_full for producers to wait when the buffer is full, and not_empty for consumers to wait when empty. During shutdown, signal all waiting threads and return special values to indicate termination. For enqueue: acquire mutex, wait while full, add item, update tail, signal not_empty, release mutex. For dequeue: acquire mutex, wait while empty and not shutdown, remove item, update head, signal not_full, release mutex. Shutdown sets flag, signals both condition variables, and operations return None/null to indicate termination.","explanation":"## Why This Is Asked\nThis question tests fundamental concurrency concepts including mutex synchronization, condition variables for thread coordination, and graceful shutdown patterns - all essential for building reliable concurrent systems.\n\n## Key Concepts\n- **Mutex**: Protects shared state (head, tail, count, buffer) from race conditions\n- **Condition Variables**: not_full blocks producers when buffer is full, not_empty blocks consumers when empty\n- **Circular Buffer**: Efficient fixed-size storage using modulo arithmetic for wrap-around\n- **Shutdown Handling**: Clean termination by signaling all waiting threads\n\n## Implementation Details\n```python\nimport threading\nimport time\n\nclass BoundedQueue:\n    def __init__(self, capacity):\n        self.capacity = capacity\n        self.buffer = [None] * capacity\n        self.head = 0  # dequeue position\n        self.tail = 0  # enqueue position\n        self.count = 0\n        self.mutex = threading.Lock()\n        self.not_full = threading.Condition(self.mutex)\n        self.not_empty = threading.Condition(self.mutex)\n        self.shutdown = False\n    \n    def enqueue(self, item):\n        with self.not_full:\n            while self.count == self.capacity and not self.shutdown:\n                self.not_full.wait()\n            \n            if self.shutdown:\n                return False  # Indicate shutdown\n            \n            self.buffer[self.tail] = item\n            self.tail = (self.tail + 1) % self.capacity\n            self.count += 1\n            \n            self.not_empty.notify()\n            return True\n    \n    def dequeue(self):\n        with self.not_empty:\n            while self.count == 0 and not self.shutdown:\n                self.not_empty.wait()\n            \n            if self.shutdown and self.count == 0:\n                return None  # Indicate shutdown\n            \n            item = self.buffer[self.head]\n            self.buffer[self.head] = None  # Clear reference\n            self.head = (self.head + 1) % self.capacity\n            self.count -= 1\n            \n            self.not_full.notify()\n            return item\n    \n    def graceful_shutdown(self):\n        with self.mutex:\n            self.shutdown = True\n            self.not_full.notify_all()\n            self.not_empty.notify_all()\n```\n\n## Shutdown Behavior\n- **Producers**: enqueue() returns False when shutdown is active\n- **Consumers**: dequeue() returns None when queue is empty AND shutdown is active\n- **Signal Broadcasting**: notify_all() wakes all waiting threads for clean exit\n- **Idempotent**: Multiple shutdown calls are safe\n\n## Usage Pattern\n```python\n# Producer thread\ndef producer(queue):\n    for i in range(100):\n        if not queue.enqueue(i):\n            print(\"Producer shutting down\")\n            break\n\n# Consumer thread  \ndef consumer(queue):\n    while True:\n        item = queue.dequeue()\n        if item is None:\n            print(\"Consumer shutting down\")\n            break\n        process(item)\n\n# Graceful shutdown\nqueue.graceful_shutdown()\n```","diagram":null,"difficulty":"beginner","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Cloudflare","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":["mutex protection","condition variables","circular buffer","thread coordination","graceful shutdown","bounded queue","producers consumers","shared state","race conditions","modulo arithmetic","waiting threads","special values"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2026-01-18T04:59:04.221Z","createdAt":"2026-01-11T19:17:18.492Z"},{"id":"q-7130","question":"In a CI/CD runner, hundreds of jobs from different teams submit tasks that form a DAG of artifacts. Design a bounded, multi-tenant work queue with per-tenant FIFO buffers and a dependency scheduler that enforces in-degree readiness. Include backpressure signaling, cancellation, and a minimal runnable sketch in Go or Rust?","answer":"Design a bounded, multi-tenant work queue with per-tenant FIFO buffers and a DAG scheduler. Use a global semaphore to cap in-flight tasks; track in-degree and a ready-set. Workers pull ready tasks fro","explanation":"## Why This Is Asked\nTests ability to design a concurrent system with bounded resources, fairness across tenants, and dependency-aware scheduling. It probes practical reasoning about backpressure, cancellation, and testability in large-scale CI-like workloads.\n\n## Key Concepts\n- Bounded queues and semaphores for backpressure\n- DAG scheduling with in-degree tracking\n- Per-tenant FIFO buffers and fair work distribution\n- Task cancellation, timeouts, and test strategies\n\n## Code Example\n```javascript\n// Pseudo sketch: structures for Task, Scheduler, and Enqueue/Dequeue\n```\n\n## Follow-up Questions\n- How would you detect and mitigate starvation across tenants?\n- What metrics would you collect to diagnose stalls in the DAG?\n- How would you scale the design to thousands of tenants and millions of tasks?","diagram":null,"difficulty":"advanced","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","Lyft","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T10:35:21.306Z","createdAt":"2026-01-25T10:35:21.306Z"},{"id":"q-7151","question":"Design a beginner-friendly concurrency coordinator for a web crawler: many producers submit domain-URL tasks concurrently. Use a bounded global queue and per-domain FIFO buffers. A small worker pool fetches pages, while a per-domain rate limiter enforces politeness (e.g., 2 requests/sec with burst). Explain backpressure, avoid cross-domain starvation, and outline a practical test plan for per-domain ordering, rate-limiter behavior, and failure recovery?","answer":"I’d implement: a bounded global channel (size 200); domain queues protected by a mutex map; a round-robin dispatcher pulling one task from each domain queue and submitting to workers if the domain rat","explanation":"## Why This Is Asked\n\nTests a beginner's grasp of concurrency basics in Go or Python asyncio: bounded queues, per-key FIFO, rate limiting, and simple scheduling; invites trade-offs like fairness vs latency.\n\n## Key Concepts\n\n- Bounded queues and backpressure\n- Per-domain FIFO ordering\n- Lightweight rate limiting (token bucket)\n- Fair scheduling and starvation avoidance\n\n## Code Example\n\n```go\n// skeleton: per-domain token bucket limiter\ntype Bucket struct{ ... }\nfunc (b *Bucket) Allow() bool { ... }\n```\n\n## Follow-up Questions\n\n- How would you adapt this if some domains frequently burst, others stay idle?\n- How would you test for deadlocks or starvation under high concurrency?","diagram":"flowchart TD\n  A[Producers] --> B[GlobalQueue]\n  B --> C[Dispatcher]\n  C --> D[DomainQueue: domain1]\n  C --> E[DomainQueue: domain2]\n  D --> F[Workers]\n  E --> F","difficulty":"beginner","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Instacart","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T11:29:23.787Z","createdAt":"2026-01-25T11:29:23.787Z"},{"id":"q-7186","question":"Design a real-time pipeline for per-user events: N producers emit events labeled user_id into bounded per-user queues of size B. A dispatcher routes available events to M workers while preserving per-user order across all workers. Support dynamic worker resizing, implement a backpressure policy when any per-user queue is full, and specify how late/out-of-order events are detected and handled?","answer":"Per-user bounded queues (user_id -> ring buffer size B) with next_expected_seq[user]. A dispatcher marks ready users; M workers consume in-order sequences across keys. Support dynamic resizing by reba","explanation":"## Why This Is Asked\nAssesses ability to design per-key ordering with backpressure and dynamic scaling in a real-time system.\n\n## Key Concepts\n- Per-key state, bounded buffers, backpressure\n- Dynamic worker resizing and work redistribution\n- Late or out-of-order event handling and DLQ metrics\n\n## Code Example\n```javascript\n// Event model sketch\nclass Event { constructor(user, seq, payload) { this.user = user; this.seq = seq; this.payload = payload; } }\n```\n\n## Follow-up Questions\n- How would you quantify fairness when backpressure hits many users?\n- How would you test ordering guarantees under simulated network delays?","diagram":null,"difficulty":"intermediate","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Plaid","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T13:16:16.531Z","createdAt":"2026-01-25T13:16:16.531Z"},{"id":"q-720","question":"Design a bounded worker pool for a high-throughput API gateway that queues work with backpressure. Use a lock-free ring buffer, N workers, and blocking enqueue when full. Include per-task cancellation, timeouts, and metrics. Compare Go, Rust, and Java trade-offs and explain how you’d debug data races and starvation under burst traffic?","answer":"Implement a bounded worker pool with a fixed number of workers and a lock-free ring buffer. Block enqueue when full to enforce backpressure; use per-task cancellation via context and per-task timeouts","explanation":"## Why This Is Asked\n\nAssesses practical understanding of bounded concurrency, backpressure strategies, and race debugging in high-throughput services. The candidate should discuss data-race mitigation, perf instrumentation, and cross-language trade-offs relevant to modern back-end systems like those at Square and Hugging Face.\n\n## Key Concepts\n\n- Bounded queues and backpressure\n- Lock-free ring buffers with atomic indices\n- Task cancellation and timeouts\n- Metrics: p50/p95 latency, queue depth, throughput\n- Race detection and starvation debugging\n- Language trade-offs: Go, Rust, Java\n\n## Code Example\n\n```go\npackage main\n\ntype Task struct{ /*...*/ }\n\ntype RingBuffer struct {\n  buf  []Task\n  head uint64\n  tail uint64\n  mask uint64\n}\n\nfunc (r *RingBuffer) Push(t Task) bool { /* lock-free push */ }\nfunc (r *RingBuffer) Pop() (Task, bool) { /* lock-free pop */ }\n```\n\n## Follow-up Questions\n\n- How would you handle backpressure when producers vastly outpace consumers?\n- What diagnostics would you run to detect and fix data races in the hot path?","diagram":"flowchart TD\n  A[Submit Task] --> B{Queue Full?}\n  B -- Yes --> C[Block Enqueue]\n  B -- No --> D[Enqueue Task]\n  D --> E[Worker Picks Up]\n  E --> F[Process Task]\n  F --> G[Task Complete]\n","difficulty":"intermediate","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T20:22:26.591Z","createdAt":"2026-01-11T20:22:26.591Z"},{"id":"q-726","question":"Design a concurrent event broker for a live chat service that guarantees per-user in-order delivery while allowing parallel processing across users, using a bounded buffer; describe backpressure handling when the buffer fills, and compare locking versus lock-free approaches in your language of choice?","answer":"Design uses per-user FIFO queues feeding a shared dispatcher with a fixed-size worker pool. Each user has a bounded queue; producers enqueue non-blockingly and back off or drop messages when full. Dis","explanation":"## Why This Is Asked\n\nReal-world chat backends require per-user ordering with high throughput and minimal tail latency. This question probes ability to design scalable concurrency primitives, reason about backpressure, and compare locking vs lock-free trade-offs.\n\n## Key Concepts\n\n- Per-user FIFO ordering\n- Bounded buffers and backpressure\n- Lock granularity vs lock-free structures\n- Throughput vs latency under saturation\n- Testing strategies for tails and fairness\n\n## Code Example\n\n```go\ntype Message struct { UserID int; Payload []byte }\nfunc enqueue(q chan Message, m Message) bool {\n  select { case q <- m: return true; default: return false }\n}\n```\n\n## Follow-up Questions\n\n- How would you extend for fairness across many users with bursty traffic?\n- What backpressure policy would you adjust in production and why?","diagram":null,"difficulty":"intermediate","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Microsoft","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T21:17:03.063Z","createdAt":"2026-01-11T21:17:03.063Z"},{"id":"q-7308","question":"You run a real-time data ingestion service. Many producers publish tasks with a deadline timestamp and a payload size. A bounded global queue holds tasks; a scheduler pops tasks by earliest deadline and assigns them to a small worker pool for processing. Design a beginner-friendly Go or Python asyncio solution that enforces backpressure, explores deadline fairness, and includes a concrete test plan for late tasks, jitter, and queue fullness?","answer":"Use a bounded queue and a dedicated deadline scheduler (earliest-deadline-first) feeding a small worker pool; implement with Go channels or asyncio queues. Backpressure via bounded size; tie-break wit","explanation":"## Why This Is Asked\n\nThis asks for a practical scheduling pattern beyond per-domain or per-user queues, focusing on deadline-driven work with simple primitives.\n\n## Key Concepts\n\n- Bounded queues\n- Deadline-aware scheduling (earliest deadline first)\n- Backpressure\n- Fairness among tasks with equal deadlines\n\n## Code Example\n\n```javascript\n// sketch: bounded queue + EDF scheduler\n```\n\n## Follow-up Questions\n\n- How to test fairness when deadlines tie?\n- How adapt to clock drift or skew?\n- How would you extend for multiple priority levels and jitter?","diagram":null,"difficulty":"beginner","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Robinhood","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T17:38:43.284Z","createdAt":"2026-01-25T17:38:43.284Z"},{"id":"q-7348","question":"Design a beginner-friendly concurrency task queue for a monitoring system: multiple agents publish health checks concurrently; a bounded global queue feeds a small worker pool that writes results to a local store. Implement per-agent rate-limiting and a graceful shutdown that drains in-flight tasks without losing results. Explain backpressure, ordering, and a minimal test plan?","answer":"Use a bounded global queue with per-agent rate-limiters (token bucket). Each agent maintains a small FIFO buffer and a dispatcher enqueues to the global queue; a small worker pool dequeues and writes ","explanation":"## Why This Is Asked\n\nThis question tests practical concurrency patterns that map to real-world monitoring systems: bounded queues, per-source rate limiting, graceful shutdowns, and recoverability.\n\n## Key Concepts\n\n- Bounded queues and backpressure\n- Per-agent FIFO buffers\n- Rate limiting and fairness\n- Graceful shutdown and recovery\n\n## Code Example\n\n```javascript\n// Pseudocode: dispatcher + workers + rate limiter\n```\n\n## Follow-up Questions\n\n- How would you modify for bursty health checks?\n- How would you simulate failures and ensure no data loss?","diagram":"flowchart TD\n  A[Agents publish health checks] --> B[Bounded global queue]\n  B --> C[Worker pool]\n  C --> D[Local store]\n  subgraph Per-Agent buffers\n    F[Agent FIFO] -->|enqueue| B\n  end","difficulty":"beginner","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Apple","IBM"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T19:38:55.494Z","createdAt":"2026-01-25T19:38:55.494Z"},{"id":"q-735","question":"Design a concurrency-safe per-channel message queue for a Discord-like chat service: multiple producers push messages, a single consumer persists them to storage; implement bounded capacity, preserve per-channel order, and handle backpressure. Which synchronization primitives would you use and how would you test edge cases?","answer":"Use a per-channel bounded queue (ring buffer) guarded by a mutex, with two condition variables: notFull and notEmpty. Producers block on full; a single consumer dequeues in arrival order and persists ","explanation":"## Why This Is Asked\nConcurrency is central to both Discord-like chat and Snowflake-like storage. This question probes ability to map a real production pattern (per-channel queues) to concrete synchronization, backpressure, and ordering guarantees.\n\n## Key Concepts\n- Producer-consumer pattern\n- Bounded buffers and backpressure\n- Mutexes and condition variables\n- Per-channel ordering and fault handling\n\n## Code Example\n\n```javascript\nclass BoundedQueue {\n  constructor(capacity) {\n    this.capacity = capacity;\n    this.buf = new Array(capacity);\n    this.head = 0;\n    this.tail = 0;\n    this.size = 0;\n    this.mux = new Mutex();\n    this.notFull = new ConditionVariable();\n    this.notEmpty = new ConditionVariable();\n  }\n  enqueue(item) {\n    // acquire mutex; wait while size === capacity; store at tail; tail=(tail+1)%capacity; size++; notEmpty.signal();\n  }\n  dequeue() {\n    // acquire mutex; wait while size === 0; read from head; head=(head+1)%capacity; size--; notFull.signal();\n  }\n}\n```\n\n## Follow-up Questions\n- How would you test backpressure and ordering across bursty producers?\n- How would you extend to multiple consumers while preserving per-channel order?","diagram":null,"difficulty":"beginner","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T22:19:16.416Z","createdAt":"2026-01-11T22:19:16.416Z"},{"id":"q-7400","question":"Design a concurrency strategy for a real-time trading analytics engine that ingests streams from 10k tickers, computes moving-window aggregates, and emits per-ticker streams to downstream consumers. Propose a bounded work queue, per-ticker partitioned state, dynamic worker pool resizing, and robust backpressure signaling to prevent hot-ticker starvation. Include failure recovery, observability, and a minimal runnable example?","answer":"Implement a bounded work queue with approximately 1,000 capacity, a dynamically resizable worker pool, and per-ticker partitioned state to compute moving-window aggregates. When the queue reaches capacity, producers block to apply backpressure. Workers scale up or down based on queue depth and processing latency. Each ticker's state is maintained in a separate shard to ensure isolation and prevent hot-ticker starvation. Include checkpointing for failure recovery and comprehensive metrics for observability.","explanation":"## Why This Is Asked\nThis tests the ability to design a low-latency, scalable streaming system that maintains strict ordering and fairness across thousands of concurrent data streams.\n\n## Key Concepts\n- Bounded queues for flow control and backpressure management\n- Dynamic worker pool sizing based on load metrics\n- Per-key state partitioning to isolate hot spots\n- Windowed aggregation with time-based sliding windows\n- Fault tolerance through checkpointing and idempotent updates\n- Observability via metrics, tracing, and SLO monitoring\n\n## Code Example\n```python\n# Skeleton: bounded queue + per-ti","diagram":null,"difficulty":"advanced","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Goldman Sachs","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T06:51:16.833Z","createdAt":"2026-01-25T21:37:41.818Z"},{"id":"q-7432","question":"Design a beginner-friendly concurrency event processor: multiple services emit events tagged by type A, B, or C at varying rates. Use a bounded global queue and per-type FIFO buffers. Some types require system-wide in-order processing; others can run in parallel up to a per-type limit. Explain backpressure, cross-type fairness, and provide a concrete test plan for ordering, throughput, and starvation scenarios?","answer":"Implement a bounded global queue that feeds into separate per-type FIFO buffers. For types requiring system-wide in-order processing, enforce sequential processing within each type buffer and route to a shared worker pool with type-specific constraints. Apply backpressure through queue size monitoring and adaptive throttling mechanisms. Ensure cross-type fairness using weighted round-robin scheduling and prevent starvation by implementing minimum allocation guarantees for each event type.","explanation":"## Why This Is Asked\n\nThis question evaluates understanding of multi-queue coordination, backpressure mechanisms, fairness algorithms, and comprehensive testing strategies in a beginner-friendly context, specifically focusing on per-type ordering requirements and handling varying event rates.\n\n## Key Concepts\n\n- Bounded queues and backpressure implementation\n- Per-type FIFO buffers for event segregation\n- In-order versus parallel processing requirements\n- Cross-type fairness and resource allocation\n- Testing strategies: ordering verification, throughput measurement, starvation prevention\n\n## Core Implementation\n\nThe solution uses a bounded global queue as the entry point for all events, which then routes them to type-specific FIFO buffers. This two-level approach provides both global flow control and type-specific processing guarantees. For order-critical types, events are processed sequentially within their buffer, while other types can leverage parallel workers up to their configured limits.\n\n## Backpressure Management\n\nBackpressure is implemented through queue depth monitoring and adaptive throttling. When any queue approaches capacity, the system reduces acceptance rates from high-volume producers while maintaining minimum throughput for lower-volume types. This prevents memory exhaustion while maintaining system responsiveness.\n\n## Fairness and Starvation Prevention\n\nCross-type fairness is achieved through weighted round-robin scheduling, ensuring no single type dominates the worker pool. Minimum allocation guarantees prevent starvation by reserving processing capacity for each type, even during high-load conditions from other types.","diagram":"flowchart TD\n  E[Event] --> D[Dispatcher]\n  D --> T1[Type A Buffer]\n  D --> T2[Type B Buffer]\n  D --> T3[Type C Buffer]\n  T1 --> W[Worker Pool]\n  T2 --> W\n  T3 --> W","difficulty":"beginner","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","LinkedIn","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T06:03:22.304Z","createdAt":"2026-01-25T22:46:41.136Z"},{"id":"q-748","question":"You're building a real-time analytics pipeline with many shards. Propose a concurrent, bounded path that preserves per-shard in-order processing while enabling cross-shard parallelism. Design a data structure and protocol (enqueuing, routing, backpressure, and shutdown). Provide a runnable sketch in Go or Rust showing enqueue, per-shard consumer loop, and graceful shutdown?","answer":"Route by shard to per-shard fixed-size ring buffers; each shard has a single in-order consumer. Producers hash the shard key and enqueue; if any buffer is full, block until space frees (backpressure).","explanation":"## Why This Is Asked\nTests ability to design scalable, shard-aware, in-order processing with backpressure and clean shutdown in a high-throughput concurrency context.\n\n## Key Concepts\n- Bounded queues\n- Per-shard ordering\n- Backpressure\n- Lock-free synchronization\n- Graceful shutdown\n\n## Code Example\n```go\n// skeleton concurrent ring buffer for a shard\ntype Ring[T any] struct { /* ... */ }\nfunc (r *Ring[T]) Enqueue(v T) bool { /* ... */ }\nfunc (r *Ring[T]) Dequeue() (T, bool) { /* ... */ }\n```\n\n## Follow-up Questions\n- How would you handle dynamic shard rebalancing?\n- How to detect and recover from a stuck consumer?","diagram":"flowchart TD\n  P[Producers] -->|route by shard| S1[Shard 1 Buffer] --> C1[Shard 1 Consumer]\n  P --> S2[Shard 2 Buffer] --> C2[Shard 2 Consumer]\n  Shutdown --> C1\n  Shutdown --> C2","difficulty":"advanced","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["NVIDIA","Snap","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T23:21:27.457Z","createdAt":"2026-01-11T23:21:27.457Z"},{"id":"q-756","question":"Implement a concurrent task-graph executor with dependencies. Tasks form a DAG; a task runs only after all its prerequisites complete. Use a bounded in-flight task pool, per-worker work stealing, and deadlock/backpressure handling. Provide a runnable Go or Rust sketch showing submission, ready-state transitions, worker loop, and graceful shutdown?","answer":"Use a dependency counter per task and a bounded global ready queue; each worker pops from its local deque and steals from others when idle. On task completion, decrement indegree of dependents and enq","explanation":"## Why This Is Asked\nTests understanding of DAG-based scheduling, readiness, backpressure, and graceful shutdown in a concurrent context.\n\n## Key Concepts\n- DAG validation and cycle detection\n- Readiness counters and per-task in-degree\n- Work-stealing and per-worker caches\n- Bounded in-flight with backpressure\n- Safe shutdown and replay considerations\n\n## Code Example\n```javascript\n// Implementation sketch in JS (conceptual)\nclass Task { constructor(id, deps, exec){ this.id=id; this.deps=deps; this.exec=exec; } }\nclass DAGExecutor { constructor(maxInFlight){ this.maxInFlight=maxInFlight; /* ... */ } submit(tasks){ /* validate DAG, init indegrees */ } run(){ /* start workers, process ready tasks, steal, and exit on shutdown */ } shutdown(){ /* signal and wait */ } }\n```\n\n## Follow-up Questions\n- How would you extend for dynamic dependency changes?\n- How would you add deterministic replay logging for debugging?","diagram":"flowchart TD\n  A[Submit Task] --> B[Build Indegree]\n  B --> C{Ready?}\n  C -->|Yes| D[Enqueue to Ready Pool]\n  C -->|No| E[Wait for Dependencies]","difficulty":"intermediate","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T01:33:36.133Z","createdAt":"2026-01-12T01:33:36.134Z"},{"id":"q-759","question":"In a multi-threaded microservice, there is a shared in-memory counter for total processed events. Provide a concrete, beginner-friendly approach to implement a thread-safe increment using language primitives (Java, Go, or Python) and discuss the trade-offs between lock-based vs lock-free solutions when scaling across cores?","answer":"Use language-supported atomic operations or a minimal lock to guard the counter. For Java: counter is AtomicLong; call incrementAndGet after each event. For Go: use atomic.AddUint64; for CPython: prot","explanation":"## Why This Is Asked\n\nInterview context explanation.\n\n## Key Concepts\n\n- Atomic operations\n- Memory visibility\n- Contention and scalability\n- Language constructs (AtomicLong, atomic.Add, threading.Lock)\n\n## Code Example\n\n```javascript\n// Java-like atomic usage (illustrative)\nclass AtomicLong {\n  constructor(n=0){ this.v = n; }\n  incrementAndGet(){ return ++this.v; }\n}\n```\n\n## Follow-up Questions\n\n- How would you test for race conditions in this setup?\n- What would you change if there are multiple processes instead of threads?","diagram":null,"difficulty":"beginner","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Coinbase","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T03:48:34.876Z","createdAt":"2026-01-12T03:48:34.876Z"},{"id":"q-7624","question":"Design a beginner-friendly concurrency pattern for a feature flag evaluation service: many microservices request flag values by key; a bounded global queue feeds a small worker pool that fetches from a remote flag store on a cache miss. Implement per-key request coalescing so concurrent requests for the same key trigger a single remote fetch; ensure fairness, backpressure, and a practical test plan?","answer":"Use per-key coalescing (singleflight). Maintain a map from key to an in-flight future. On a cache miss, if no in-flight for that key, start a remote fetch and store its future; otherwise await the exi","explanation":"## Why This Is Asked\n\nTests practical concurrency patterns: dedup across concurrent requests, bounded queues, and cache integration in a real-world service.\n\n## Key Concepts\n\n- Per-key deduplication (singleflight)\n- Bounded queues and backpressure\n- Cache coherence and eviction\n- Failure handling and retries\n\n## Code Example\n\n```javascript\nconst inflight = new Map();\nasync function fetchFlag(key){\n  if (inflight.has(key)) return inflight.get(key);\n  const p = remoteFetch(key).finally(()=>inflight.delete(key));\n  inflight.set(key,p);\n  return p;\n}\n```\n\n## Follow-up Questions\n\n- How would you handle remote fetch failures?\n- How would you extend to support priority keys or TTL-based eviction?","diagram":"flowchart TD\n  A[Request] --> B[CheckCache]\n  B --> C{Miss?}\n  C -- Yes --> D[StartFetch(key)]\n  C -- No --> E[ReturnCached]\n  D --> F[UpdateCache]\n  F --> G[Respond]\n  G --> H[Done]","difficulty":"beginner","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Plaid","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T10:07:45.973Z","createdAt":"2026-01-26T10:07:45.973Z"},{"id":"q-7692","question":"Design a bounded, multi-tenant task router for a real-time telemetry ingest service. Producers submit per-tenant event batches; implement a global queue with max capacity N, plus per-tenant FIFO buffers. Build a fair, backpressure-aware scheduler that prevents starvation, supports dynamic tenant creation, and a small worker pool. Provide core data structures and a runnable minimal example?","answer":"Propose a bounded, multi-tenant task router: a global queue with capacity N, plus per-tenant FIFO buffers. Implement a fair scheduler that prevents starvation under bursts by aging: waiting tenants ga","explanation":"## Why This Is Asked\nTests practical design of multi-tenant concurrency with backpressure and fairness under bursty loads.\n\n## Key Concepts\n- Bounded queues and backpressure\n- Per-tenant FIFO and aging-based fairness\n- Deadlock avoidance and clean shutdown\n- Dynamic tenant creation and worker orchestration\n\n## Code Example\n```python\n# Skeleton\nclass Router:\n    pass\n```\n\n## Follow-up Questions\n- How would you adapt to heterogeneous task sizes?\n- How to measure and guarantee latency bounds?","diagram":"flowchart TD\n  A[Producers] --> B[GlobalQueue(Max N)]\n  B --> C[WorkerPool]\n  subgraph Tenants\n    D[Tenant A] --> E[PerTenantFIFO]\n    F[Tenant B] --> G[PerTenantFIFO]\n  end\n  E --> C\n  G --> C","difficulty":"advanced","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Anthropic","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T13:34:03.867Z","createdAt":"2026-01-26T13:34:03.867Z"},{"id":"q-773","question":"In a Go HTTP server, you maintain a per-user quota counter in memory. Multiple requests for different users should advance concurrently, but updates to the same user must be serialized. Design a striped lock (a fixed set of mutexes) to guard a map[string]int, map userID to a bucket via hashing, and explain how you minimize contention, handle hash collisions, and ensure correctness when entries may be evicted?","answer":"Use a striped lock: a fixed-size slice of mutexes guards a map[string]int quotas. Hash userID to bucket index, lock bucket, read-modify-write, unlock. This lowers contention vs a global lock. Collisio","explanation":"## Why This Is Asked\n\nTests understanding of fine-grained synchronization and practical trade-offs for high-concurrency in-memory state.\n\n## Key Concepts\n\n- Striped locking for reduced contention\n- Lock granularity vs global lock\n- Handling hash collisions with buckets\n- Eviction strategies and correctness\n\n## Code Example\n\n```javascript\n// Go-like pseudocode demonstrating striped locks\ntype Striped struct {\n  locks []sync.Mutex\n  quotas map[string]int\n}\nfunc NewStriped(n int) *Striped { ... }\nfunc (s *Striped) Inc(id string) { idx := hash(id) % len(s.locks); s.locks[idx].Lock(); defer s.locks[idx].Unlock(); s.quotas[id]++ }\n```\n\n## Follow-up Questions\n\n- What happens if bucket count is too small? how to resize safely?\n- How would you extend to support read-heavy workloads (readers-writer locks)?","diagram":"flowchart TD\n  A[Request arrives] --> B[Hash userID to bucket]\n  B --> C[Acquire bucket lock]\n  C --> D[Read/Increment quota]\n  D --> E[Release lock]\n  E --> F[Respond]","difficulty":"beginner","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","NVIDIA","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T04:54:27.848Z","createdAt":"2026-01-12T04:54:27.848Z"},{"id":"q-7781","question":"Design a scalable, multi-tenant log processing system: hundreds of producers generate log events tagged by tenant_id. A bounded global queue feeds M workers. Each tenant must have in-order processing, but tenants vary in event rate; implement dynamic per-tenant backpressure with aging to prevent starvation, and a global policy to upgrade or degrade tenant priority during bursts. Provide concrete data structures, API sketch, and a runnable minimal example in Python or Go?","answer":"Use a global bounded queue with a pool of workers, plus per-tenant FIFO buffers and a distributor that enforces in-order processing for each tenant. Add aging to increase priority for stalled tenants ","explanation":"## Why This Is Asked\n\nAssess ability to design multi-tenant concurrency with ordering guarantees, dynamic priorities, and backpressure under bursty load. Requires robust data structures, race-condition awareness, and a clear test plan.\n\n## Key Concepts\n\n- Bounded queues and backpressure\n- Per-tenant FIFO ordering\n- Dynamic aging to prevent starvation\n- Scheduling under bursty traffic and failure modes\n\n## Code Example\n\n```python\n# sketch of distributor with per-tenant buffers\nfrom collections import defaultdict, deque\n\nclass Distributor:\n    def __init__(self, global_q):\n        self.global_q = global_q\n        self.tenant_queues = defaultdict(deque)\n        self.active = {}\n\n    def enqueue(self, tenant_id, event):\n        self.tenant_queues[tenant_id].append(event)\n        self.try_dispatch(tenant_id)\n\n    def try_dispatch(self, tenant_id):\n        if tenant_id in self.active:\n            return\n        if self.tenant_queues[tenant_id]:\n            self.active[tenant_id] = True\n            # simulate processing next in-order event for this tenant\n            event = self.tenant_queues[tenant_id].popleft()\n            self.global_q.put((tenant_id, event))\n            del self.active[tenant_id]\n```\n\n## Follow-up Questions\n\n- How would you test ordering guarantees across tenants under burst traffic?\n- How would you extend to failure and replay scenarios?","diagram":null,"difficulty":"advanced","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","MongoDB","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T17:43:26.024Z","createdAt":"2026-01-26T17:43:26.024Z"},{"id":"q-780","question":"Implement a lock-free, bounded multi-producer, multi-consumer pipeline with per-symbol partitions and fixed-capacity queues. Producers must acquire credits to enqueue; downstream workers release credits on completion. Explain memory visibility, false sharing avoidance, and a clean shutdown. How do you guarantee in-order per partition and exactly-once processing on failure?","answer":"Propose a per-symbol partitioned, bounded ring-buffer with credit-based flow. Producers atomically decrement a per-partition credit counter to enqueue, avoiding locks; consumers advance a per-partitio","explanation":"## Why This Is Asked\nThis checks ability to design lock-free structures, backpressure, and fault-tolerant guarantees across partitions in high-throughput services.\n\n## Key Concepts\n- Lock-free queues and per-partition backpressure\n- Memory visibility, false sharing avoidance, cache alignment\n- Clean shutdown and exactly-once semantics\n\n## Code Example\n```go\n// skeleton illustrating credit-based enqueue\n```\n\n## Follow-up Questions\n- How would you test correctness under bursty traffic\n- How would you extend to cross-node coordination with minimal latency","diagram":null,"difficulty":"advanced","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Microsoft","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T05:31:56.180Z","createdAt":"2026-01-12T05:31:56.180Z"},{"id":"q-7805","question":"Build a real-time per-user join between two streams. Stream A emits (user_id, tsA) and Stream B emits (user_id, tsB, amount). A bounded inbound queue feeds M workers that output joined records when both sides have arrived within a 60s watermark. Must preserve per-user in-order emission, handle late data with a policy, and support dynamic worker resizing with backpressure?","answer":"Use a per-user state with bounded FIFO buffers for A and B and a 60s watermark. Emit when both sides exist, preserving order by the latest timestamp. Drop/purge late records per policy to bound memory","explanation":"## Why This Is Asked\nProbe practical streaming joins, backpressure, and dynamic scaling under bounded resources.\n\n## Key Concepts\n- Per-key buffering, watermarks, and in-order guarantees\n- Join semantics in streaming under backpressure\n- Backpressure propagation and dynamic worker resizing\n\n## Code Example\n```javascript\n// Pseudo: per-user buffers, watermark, and join\n```\n\n## Follow-up Questions\n- How would you implement late data handling policies? \n- How would you test ordering under burst traffic and worker resizing?","diagram":null,"difficulty":"intermediate","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T18:56:27.024Z","createdAt":"2026-01-26T18:56:27.025Z"},{"id":"q-7831","question":"Design an intermediate concurrency problem: a real-time telemetry processor with per-device time-windowed aggregates. N producers emit {device_id, ts, value} readings; readings may arrive out of order and late by up to L seconds. Implement a bounded per-device window manager that computes min, max, sum, count in fixed-size windows [start, end). Use M workers to maintain per-device buffers, advance a watermark, and emit aggregates when windows close. Enforce memory bounds, allow dynamic worker resizing, and define late-data policy with at-most-once vs at-least-once semantics. Provide data structures and a concrete test plan?","answer":"Use per-device buffers keyed by device_id, each storing windows with running aggregates. Maintain a global watermark; when window_end <= watermark, flush and drop. Cap total buffered windows; on cap, ","explanation":"## Why This Is Asked\nTests understanding of event-time processing, bounded-memory windowing, and backpressure in a concurrent setting. It also probes dynamic worker resizing and late-data strategies.\n\n## Key Concepts\n- Event-time windowing with per-device state\n- Bounded buffers and memory accounting\n- Watermarks to trigger emission\n- Dynamic worker scaling and backpressure\n- Late-data handling and exactly-once vs at-least-once semantics\n\n## Code Example\n```javascript\n// Pseudo-structure: perDeviceWindows: Map<device_id, Map<WindowKey, WindowState>>\n// WindowKey could be (start, end); WindowState tracks sum, min, max, count, etc.\n```\n\n## Follow-up Questions\n- How would you validate correctness with late arrivals and clock skew?\n- What metrics would you expose to operators to tune window size and cap?","diagram":"flowchart TD\n  P[Producers] --> WM[Window Manager]\n  WM --> E[Emit Aggregates]\n  WM --> BP[Backpressure Controller]\n  BP --> P","difficulty":"intermediate","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Hugging Face","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T19:40:30.815Z","createdAt":"2026-01-26T19:40:30.815Z"},{"id":"q-788","question":"In a streaming data pipeline, multiple producers generate frames and a bounded buffer sits between the producer stage and a consumer stage. Implement a backpressure-enabled producer-consumer pair in JavaScript (Node.js) using an async bounded queue that supports multiple producers and multiple consumers. It must guarantee no data loss, bounded memory, and graceful shutdown. How would you test under varying production/consumption rates?","answer":"Use a bounded queue with capacity N; producers await when full and consumers await when empty to enforce backpressure and bounded memory. For shutdown, push a sentinel or close the queue so workers ex","explanation":"## Why This Is Asked\nTests understanding of backpressure, bounded buffers, and graceful shutdown in a concurrent environment using asynchronous primitives.\n\n## Key Concepts\n- Bounded buffers and backpressure\n- Multiple producers and multiple consumers\n- Graceful shutdown signaling\n- Correct testing under varying rates\n\n## Code Example\n```javascript\nclass BoundedQueue {\n  constructor(capacity) {\n    this.capacity = capacity\n    this.queue = []\n    this.waitPuts = []\n    this.waitGets = []\n  }\n  async push(item) {\n    if (this.queue.length < this.capacity) {\n      this.queue.push(item)\n      if (this.waitGets.length) this.waitGets.shift()(this.queue.shift())\n      return\n    }\n    await new Promise(res => this.waitPuts.push(res))\n    return this.push(item)\n  }\n  async pop() {\n    if (this.queue.length) {\n      const item = this.queue.shift()\n      if (this.waitPuts.length) this.waitPuts.shift()\n      return item\n    }\n    return new Promise(resolve => this.waitGets.push(resolve))\n  }\n}\n// Usage sketch: create producers/consumers using q.push(q.pop())\n```\n\n## Follow-up Questions\n- How would you adapt for a single consumer vs multiple consumers?\n- What strategies exist if you must drop data when the buffer is full, and how would you test them?","diagram":null,"difficulty":"beginner","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T06:42:23.564Z","createdAt":"2026-01-12T06:42:23.564Z"},{"id":"q-7885","question":"Design a beginner-friendly concurrency dispatcher for a multi-tenant financial alert system: many producers from different tenants emit alert events that must be delivered to downstream processors in per-tenant FIFO order. Use a bounded global queue and per-tenant buffers with a small worker pool. Add a simple dynamic quota that throttles tenants when latency rises, and implement a spillback retry with exponential backoff. Explain fairness, ordering, and a minimal test plan?","answer":"Design uses a bounded global queue with per-tenant FIFO buffers and a small worker pool. Schedule via round-robin across tenants with a per-tenant token bucket; throttle when latency rises by delaying token acquisition and implementing spillback retry with exponential backoff.","explanation":"## Why This Is Asked\nExposes understanding of bounded queues, backpressure, per-tenant ordering, and simple dynamic throttling in a realistic setting.\n\n## Key Concepts\n- Bounded queues and backpressure\n- Per-tenant FIFO guarantees\n- Lightweight dynamic quotas based on latency\n- Spillback retries with backoff and idempotence concerns\n\n## Code Example\n```javascript\n// pseudo sketch: queue creation and round-robin scheduler\n```\n\n## Follow-up Questions\n- How would you simulate latency bursts in tests?\n- How would you prove fairness under varying tenant loads?","diagram":"flowchart TD\n  P[Producers] --> Q[GlobalBoundedQueue]\n  Q --> S[Dispatcher]\n  S --> W[WorkerPool]\n  S --> T1[Tenant A Buffer]\n  S --> T2[Tenant B Buffer]\n  T1 --> W\n  T2 --> W","difficulty":"beginner","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T06:40:37.604Z","createdAt":"2026-01-26T21:40:37.411Z"},{"id":"q-795","question":"Design a dynamic, concurrency-safe worker pool in Go for a real-time analytics pipeline. Each task type has its own queue; enforce backpressure with bounded channels and implement a central scaler that adjusts worker counts based on observed tail latency and throughput. Provide a runnable skeleton showing enqueue, worker loops, and graceful shutdown?","answer":"Use per-type bounded channels to enforce backpressure, a dispatcher routing tasks to each type queue, and a central scaler that monitors tail latency and throughput to adjust worker counts via atomic ","explanation":"## Why This Is Asked\n\nAdvanced concurrency design focusing on dynamic scaling and per-type queues, avoiding contention and starvation in a real-time analytics pipeline.\n\n## Key Concepts\n\n- Per-type queues and backpressure\n- Dynamic worker scaling with latency feedback\n- Safe shutdown and fault tolerance\n- Context cancellation and coordination\n\n## Code Example\n\n```go\npackage main\nimport (\n  \"context\"\n  \"fmt\"\n  \"sync\"\n  \"time\"\n)\ntype Task struct{ Type string; Payload interface{} }\nfunc main() {\n  ctx, cancel := context.WithCancel(context.Background())\n  defer cancel()\n\n  queues := map[string]chan Task{\n    \"CPU\": make(chan Task, 100),\n    \"IO\":  make(chan Task, 100),\n  }\n\n  var wg sync.WaitGroup\n  // start workers\n  for t := range queues {\n    for i := 0; i < 2; i++ {\n      wg.Add(1)\n      go worker(ctx, t, queues[t], &wg)\n    }\n  }\n\n  // simple enqueue\n  queues[\"CPU\"] <- Task{Type: \"CPU\"}\n  queues[\"IO\"] <- Task{Type: \"IO\"}\n\n  time.Sleep(100 * time.Millisecond)\n  cancel()\n  for _, ch := range queues { close(ch) }\n  wg.Wait()\n}\nfunc worker(ctx context.Context, t string, ch <-chan Task, wg *sync.WaitGroup) {\n  defer wg.Done()\n  for {\n    select {\n    case <-ctx.Done():\n      return\n    case task, ok := <-ch:\n      if !ok { return }\n      time.Sleep(10 * time.Millisecond)\n      fmt.Println(\"done\", t)\n    }\n  }\n}\n```\n\n## Follow-up Questions\n\n- How would you test starvation scenarios and balance fairness?\n- How would you extend to dozens of task types and maintain stable latency?\n","diagram":null,"difficulty":"advanced","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Meta","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T07:29:49.914Z","createdAt":"2026-01-12T07:29:49.914Z"},{"id":"q-8037","question":"Design a beginner-friendly concurrency queue for a real-time chat moderation system: messages arrive concurrently from thousands of chat rooms. A bounded global queue feeds a small worker pool that runs moderation checks. Each chat room's messages must be processed in order, but messages across rooms may be interleaved. Propose a concrete design (data structures, routing, per-room buffers, backpressure) and outline a practical test plan to verify per-room ordering, room throttling, and graceful shutdown?","answer":"Use a bounded global queue (size Q) and per-room FIFO queues. A router assigns incoming messages to the corresponding room buffer; a small worker pool pulls from room buffers in round-robin, preservin","explanation":"## Why This Is Asked\nThis question probes practical concurrency design where per-key ordering matters, and backpressure affects producers and throughput.\n\n## Key Concepts\n- Bounded queues and backpressure\n- Per-key (per-room) FIFO ordering\n- Work stealing vs round-robin drain\n- Per-room rate limiting and graceful shutdown testing\n\n## Code Example\n```python\nfrom collections import defaultdict, deque\nclass Router:\n    def __init__(self, global_limit):\n        self.global_q = deque()\n        self.room_buffers = defaultdict(deque)\n        self.rate = defaultdict(lambda: TokenBucket(allowance=1, bucket=5))\n    # methods to enqueue, route, and drain...\n```\n\n## Follow-up Questions\n- How would you scale to 100x load while preserving ordering guarantees per room?\n- How would you detect and recover from worker crashes without dropping messages?\n","diagram":"flowchart TD\n  A[Producers] --> B[GlobalQueue]\n  B --> C[Router / Room Buffers]\n  C --> D[Worker Pool]\n  D --> E[Moderation Output]","difficulty":"beginner","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Cloudflare","Hugging Face"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T06:58:54.555Z","createdAt":"2026-01-27T06:58:54.555Z"},{"id":"q-804","question":"You're building a video-frame ingestion service where frames from thousands of users arrive on a shared input channel. Design a bounded, concurrent dispatcher that routes frames to per-user in-order queues, enabling parallel processing across users. Provide backpressure handling when the global buffer fills, a strategy for reordering out-of-order frames, and a graceful shutdown. Sketch the core data structures and synchronization in Rust or C++ and explain trade-offs?","answer":"Approach: a bounded global input buffer fans frames to per-user in-order queues. Each frame carries user_id and seq. A per-user sequencer buffers out-of-order frames in a small heap until gaps close, ","explanation":"## Why This Is Asked\nExplores practical concurrency challenges in real-time ingestion: per-key in-order delivery, bounded buffers, reordering, and safe shutdown.\n\n## Key Concepts\n- Bounded buffers and backpressure\n- Per-user in-order delivery with out-of-order buffering\n- Per-key sequencing and memory visibility\n- Graceful shutdown and drain guarantees\n\n## Code Example\n```rust\nstruct Frame { user_id: u64, seq: u64, payload: Vec<u8> }\nstruct PerUserQueue { next_seq: u64, pending: std::collections::BinaryHeap<Frame> }\n```\n\n## Follow-up Questions\n- How would you test under jitter and loss?\n- How would you scale to 100k active users with limited memory?\n","diagram":"flowchart TD\n  A[Shared Input] --> B[Route by user_id]\n  B --> C[Per-user Queues]\n  C --> D[Worker Threads]\n  A --> E[Backpressure Monitor]","difficulty":"intermediate","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","Netflix","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T08:34:55.743Z","createdAt":"2026-01-12T08:34:55.743Z"},{"id":"q-8107","question":"Design a concurrent per-vehicle telemetry processor. N producers emit events with vehicle_id and timestamp into bounded per-vehicle buffers of size B. M workers consume from all buffers, preserving per-vehicle in-order delivery across producers. Support dynamic worker resizing, per-vehicle backpressure, and a lateness policy: drop events with timestamp older than T relative to processing time; otherwise enqueue. Provide data structures and a concrete test plan?","answer":"Propose per-vehicle bounded FIFOs of size B and a shared dispatcher. Producers append to the vehicle queue; workers fetch the heads, maintaining per-vehicle order with per-vehicle sequence numbers. Ba","explanation":"## Why This Is Asked\nThis problem probes practical concurrency control in streaming telemetry, combining per-key ordering with backpressure and dynamic scaling.\n\n## Key Concepts\n- Per-vehicle ordering, bounded buffers\n- Backpressure signaling and dynamic worker pools\n- Late-event policy and exactly-once considerations\n\n## Code Example\n```javascript\n// Skeleton illustrating queues and dispatcher\n```\n\n## Follow-up Questions\n- How would you implement exactly-once semantics with failure recovery?\n- What diagnostics would you add to observe tail latency per vehicle?","diagram":"flowchart TD\n  P[Producers] --> D(Dispatcher)\n  D --> W[Workers]\n  W --> Q[Per-vehicle Buffers]\n  Q --> Pn[Processed]","difficulty":"intermediate","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Hugging Face","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T09:59:24.828Z","createdAt":"2026-01-27T09:59:24.828Z"},{"id":"q-811","question":"In a real-time analytics service, multiple producers enqueue data into a fixed-size circular buffer consumed by multiple workers. Design a beginner-friendly concurrency solution in Java, Go, or Python that guarantees producers block when the buffer is full and consumers block when empty, while maintaining correctness under concurrent access; compare lock-based vs channel-based approaches for this bounded buffer?","answer":"Use a bounded buffer API Put(item)/Get(). In Go, a buffered channel size N blocks producers when full and blocks consumers when empty. In Java, ArrayBlockingQueue<N> with put/take; Python: queue.Queue","explanation":"## Why This Is Asked\nInterviewers assess practical concurrency intuition with bounded buffers, a common real-time pattern.\n\n## Key Concepts\n- Bounded buffers\n- Producer-consumer synchronization\n- Blocking semantics\n- Trade-offs: channels vs locks vs lock-free\n\n## Code Example\n```java\nArrayBlockingQueue<Integer> q = new ArrayBlockingQueue<>(N);\nq.put(item); // blocks if full\nint x = q.take(); // blocks if empty\n```\n\n## Follow-up Questions\n- How would you handle graceful shutdown?\n- How would you introduce fairness guarantees or timeouts?","diagram":null,"difficulty":"beginner","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Oracle","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T09:35:47.460Z","createdAt":"2026-01-12T09:35:47.460Z"},{"id":"q-8147","question":"Design a beginner-friendly concurrent job scheduler for a build system: dozens of compilers produce CPU-bound compilation tasks and test runners (IO-bound) submitted concurrently. Use a bounded global queue and two worker pools (CPU vs IO). Route tasks by type, ensure each task runs exactly once, and implement simple fair sharing to prevent starvation. Provide a minimal test plan for routing, contention, and correctness?","answer":"Two bounded queues (CPU-bound, IO-bound) fed from a single dispatcher; each job has a unique id and is enqueued only once. Two worker pools pull from their queues. When both queues have work, apply a ","explanation":"## Why This Is Asked\n\nThis task probes practical concurrency skills: building a lightweight scheduler that handles mixed workloads, backpressure, and starvation safety without heavy abstractions.\n\n## Key Concepts\n\n- Bounded queues and backpressure\n- Separate pools for CPU vs IO tasks\n- Idempotent enqueues and in-flight tracking\n- Simple fairness to prevent starvation\n\n## Code Example\n\n```python\n# Minimal sketch of routing and dedup\nfrom queue import Queue\ncpu_q = Queue(maxsize=100)\nio_q = Queue(maxsize=200)\nin_flight = set()\n\ndef submit(task_id, kind):\n    if task_id in in_flight:\n        return False\n    in_flight.add(task_id)\n    (cpu_q if kind == 'cpu' else io_q).put(task_id)\n    return True\n```\n\n## Follow-up Questions\n\n- How would you tune queue sizes and the fairness policy for bursty workloads?\n- How would you detect and recover from stalled workers?","diagram":"flowchart TD\n  A[Producers] --> B[GlobalDispatcher]\n  B --> C[CPU Queue]\n  B --> D[IO Queue]\n  C --> E[CPU Workers]\n  D --> F[IO Workers]","difficulty":"beginner","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T11:33:27.049Z","createdAt":"2026-01-27T11:33:27.049Z"},{"id":"q-818","question":"In a build pipeline, N compile tasks run in parallel and must all finish before the linker runs. Design a barrier that blocks each task at barrier.wait() until all N tasks have arrived, then releases them to proceed. Implement two beginner-friendly approaches in Go (or Java/Python): (A) locks/condition variables; (B) channels/futures. Ensure the barrier is reusable for repeated builds, avoids deadlocks, and explain correctness and trade-offs?","answer":"Two approaches: (A) Locks/Cond: track count, target N, and a generation value. Each worker increments count and waits while count < N; the last increments generation and broadcasts; all wake, verify g","explanation":"## Why This Is Asked\nTests practical synchronization primitives and reuse. It checks correctness under concurrent arrival and wake-up.\n\n## Key Concepts\n- Barrier, generation, spurious wakeups, reuse\n- Deadlock avoidance, fairness, memory usage\n\n## Code Example\n```go\n// barrier skeleton in Go\n```\n\n## Follow-up Questions\n- How would you test for fairness across rounds?\n- How would you adapt for mixed local and remote workers?","diagram":null,"difficulty":"beginner","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T10:27:36.782Z","createdAt":"2026-01-12T10:27:36.782Z"},{"id":"q-8215","question":"Design a real-time fraud-processing task scheduler with many per-account streams. Use a global bounded queue and per-account FIFO buffers. Each task has a deadline; implement a deadline-aware scheduler that prioritizes high-SLA tasks, preempts or stalls lower-SLA tasks as deadlines approach, and applies backpressure when the global queue is full. Provide runnable Go pseudocode and a concrete test plan?","answer":"Design a deadline-aware scheduler with a global bounded queue and per-account FIFO buffers; ensure high-SLA streams are prioritized, allow preemption or stall of lower-SLA tasks as deadlines near, and","explanation":"## Why This Is Asked\n\nExamines real-time scheduling with strict deadlines, multi-tenant fairness, and dynamic backpressure under load—common at fraud detection systems.\n\n## Key Concepts\n\n- Deadline-aware scheduling\n- Global vs per-account queues\n- Preemption and backpressure\n- SLA-focused testing and cancellation handling\n\n## Code Example\n\n```go\ntype Task struct {\n  ID       string\n  Account  string\n  Deadline time.Time\n  Payload  interface{}\n  Cancel   <-chan struct{}\n}\ntype Scheduler struct {\n  Global   chan *Task\n  PerAcct  map[string]chan *Task\n}\nfunc (s *Scheduler) Run() {\n  // simplified: pick next by earliest deadline from ready queues\n}\n```\n\n## Follow-up Questions\n\n- How would you test for starvation under mixed SLA values?\n- How would you extend to dynamic priority boosts for urgent tasks?","diagram":"flowchart TD\n  A[Producers] --> B[GlobalQueue]\n  B --> C[Scheduler]\n  C --> D[Workers]\n  D --> E[Results]","difficulty":"advanced","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","MongoDB","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T15:02:58.034Z","createdAt":"2026-01-27T15:02:58.034Z"},{"id":"q-8235","question":"Design a beginner-friendly concurrency scheduler for a log-forwarding service in a multi-tenant cloud app: dozens of services publish logs concurrently; a bounded global queue feeds a small worker pool, with per-tenant buffering and a simple fairness policy to avoid any single tenant starving others. Explain backpressure, fairness, and a test plan for per-tenant throughput, ordering within each tenant, and failure recovery?","answer":"Implement a bounded global queue and per-tenant buffers; workers pull in round-robin across active tenants. Each tenant buffer preserves order; on overflow, apply backpressure to producers (block subm","explanation":"## Why This Is Asked\\n\\nThis angle tests practical, beginner-friendly concurrency design under multi-tenant fairness and backpressure.\\n\\n## Key Concepts\\n\\n- Bounded global queue and per-tenant buffers\\n- Round-robin fairness and backpressure\\n- Per-tenant ordering and crash recovery\\n\\n## Code Example\\n\\n```python\\n# Sketch\\nclass Scheduler:\\n    def __init__(self, global_cap, tenants):\\n        self.global = asyncio.Queue(maxsize=global_cap)\\n        self.tenant_bufs = {t: asyncio.Queue(maxsize=10) for t in tenants}\\n    async def schedule(self):\\n        while True:\\n            for t, buf in self.tenant_bufs.items():\\n                if not buf.empty():\\n                    job = await buf.get()\\n                    await self.global.put((t, job))\\n```\\n\\n## Follow-up Questions\\n\\n- How would you test per-tenant throughput and ordering under bursty tenants?\\n- How would you adapt if tenants have different SLA requirements?","diagram":null,"difficulty":"beginner","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Cloudflare","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T16:09:32.316Z","createdAt":"2026-01-27T16:09:32.316Z"},{"id":"q-825","question":"In a real-time chat moderation pipeline for a platform like Discord/LinkedIn, design a dynamic Go worker pool that scales from minWorkers to maxWorkers based on a bounded task queue. The queue should backpressure producers, guarantee per-user fairness, support graceful shutdown, and tolerate occasional misbehaving workers (timeouts). Provide runnable code showing enqueue, worker loop, and scaler?","answer":"Use a bounded queue channel and a scalable worker pool. Start with minWorkers; a scaler grows workers when queue length crosses a high watermark and shrinks when it falls below a low watermark, bounde","explanation":"## Why This Is Asked\n\nTests ability to design a scalable, backpressured worker pool for real-time workloads, with fairness, fault tolerance, and clean shutdown—exactly the kind of pattern seen in large chat platforms.\n\n## Key Concepts\n\n- Bounded queue with backpressure\n- Dynamic worker scaling (min/max bounds)\n- Fairness across users (distribution by UserID)\n- Graceful shutdown and worker timeout handling\n\n## Code Example\n\n```go\npackage main\n\nimport (\n  \"context\"\n  \"fmt\"\n  \"hash/fnv\"\n  \"sync\"\n  \"time\"\n)\n\ntype Task struct {\n  UserID  string\n  Payload string\n}\n\nfunc worker(ctx context.Context, id int, tasks <-chan Task, wg *sync.WaitGroup) {\n  defer wg.Done()\n  for {\n    select {\n    case t := <-tasks:\n      // simulate processing\n      _ = t\n      time.Sleep(5 * time.Millisecond)\n      fmt.Printf(\"worker %d processed %s\\n\", id, t.Payload)\n    case <-ctx.Done():\n      return\n    }\n  }\n}\n\nfunc main() {\n  minW, maxW := 2, 6\n  queueCap := 64\n  queue := make(chan Task, queueCap)\n  var wg sync.WaitGroup\n  var cancels []context.CancelFunc\n\n  // start min workers\n  for i := 0; i < minW; i++ {\n    ctx, cancel := context.WithCancel(context.Background())\n    cancels = append(cancels, cancel)\n    wg.Add(1)\n    go worker(ctx, i, queue, &wg)\n  }\n\n  // scaler\n  go func() {\n    ticker := time.NewTicker(100 * time.Millisecond)\n    defer ticker.Stop()\n    for range ticker.C {\n      l := len(queue)\n      if l > 32 && len(cancels) < maxW {\n        // scale up\n        ctx, cancel := context.WithCancel(context.Background())\n        cancels = append(cancels, cancel)\n        w := len(cancels) - 1\n        wg.Add(1)\n        go worker(ctx, w, queue, &wg)\n      } else if l < 8 && len(cancels) > minW {\n        // scale down\n        cancel := cancels[len(cancels)-1]\n        cancels = cancels[:len(cancels)-1]\n        cancel()\n      }\n    }\n  }()\n\n  // producer: enqueue tasks\n  for i := 0; i < 1000; i++ {\n    t := Task{UserID: fmt.Sprintf(\"user-%d\", i%20), Payload: fmt.Sprintf(\"payload-%d\", i)}\n    queue <- t // blocks when full (backpressure)\n  }\n\n  // graceful shutdown\n  time.Sleep(1 * time.Second)\n  for _, c := range cancels {\n    c()\n  }\n  close(queue)\n  wg.Wait()\n}\n```\n\n## Follow-up Questions\n\n- How would you test scaling behavior under bursty traffic and ensure latency SLOs are met?\n- How would you extend fairness for thousands of users without starving rare users?","diagram":"flowchart TD\n  P[Producer] --> Q[Task Queue]\n  Q --> W[Worker Pool]\n  W --> M[Moderation Action]\n  subgraph Scale\n    SScaler[Scaler] --> W\n  end","difficulty":"advanced","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","LinkedIn"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T11:21:45.375Z","createdAt":"2026-01-12T11:21:45.375Z"},{"id":"q-835","question":"Design a multi-tenant dispatcher for a streaming event bus. Producers from each tenant push events into per-tenant input channels; a central dispatcher interleaves tenants fairly, enforces per-tenant rate limits using a token-bucket, and enforces a maximum in-flight events per tenant. Implement in Go using channels; ensure backpressure propagates to producers when quotas or in-flight limits are reached, and support graceful shutdown. Include a runnable minimal example with: per-tenant limiter, dispatcher loop, producer(s), and cancellation?","answer":"Implement per-tenant token buckets and in-flight limits, with a central fairness-driven dispatcher. Each tenant has a bucket (capacity, refill rate) and a per-tenant in-flight cap. The dispatcher non-","explanation":"## Why This Is Asked\nEvaluates ability to design a scalable, fair, multi-tenant concurrency mechanism with backpressure, not just a single queue.\n\n## Key Concepts\n- Per-tenant token bucket with capacity and refill\n- Per-tenant in-flight limits\n- Fair scheduling to prevent starvation\n- Backpressure propagation to producers\n- Graceful shutdown via context cancellation\n\n## Code Example\n\n```go\npackage main\n\nimport (\n  \"context\"\n  \"fmt\"\n  \"sync\"\n  \"time\"\n)\n\ntype Event struct{ Payload string }\n\ntype Bucket struct {\n  mu     sync.Mutex\n  tokens int\n  cap    int\n  refill int\n}\n\nfunc (b *Bucket) TryTake() bool {\n  b.mu.Lock()\n  defer b.mu.Unlock()\n  if b.tokens > 0 {\n    b.tokens--\n    return true\n  }\n  return false\n}\n\nfunc (b *Bucket) Refund(n int) {\n  b.mu.Lock()\n  b.tokens += n\n  if b.tokens > b.cap {\n    b.tokens = b.cap\n  }\n  b.mu.Unlock()\n}\n\nfunc (b *Bucket) StartRefill(interval time.Duration) {\n  go func() {\n    ticker := time.NewTicker(interval)\n    for range ticker.C {\n      b.mu.Lock()\n      if b.tokens < b.cap {\n        b.tokens += b.refill\n        if b.tokens > b.cap { b.tokens = b.cap }\n      }\n      b.mu.Unlock()\n    }\n  }()\n}\n\ntype Tenant struct {\n  id           string\n  in           chan Event\n  bucket       *Bucket\n  inFlight     int\n  maxInFlight  int\n}\n\nfunc main() {\n  ctx, cancel := context.WithCancel(context.Background())\n  defer cancel()\n\n  tA := &Tenant{id: \"A\", in: make(chan Event, 32), bucket: &Bucket{tokens: 10, cap: 10, refill: 1}, maxInFlight: 3}\n  tB := &Tenant{id: \"B\", in: make(chan Event, 32), bucket: &Bucket{tokens: 8, cap: 8, refill: 1}, maxInFlight: 2}\n  tA.bucket.StartRefill(120 * time.Millisecond)\n  tB.bucket.StartRefill(120 * time.Millisecond)\n\n  tenants := []*Tenant{tA, tB}\n\n  // Simple dispatcher\n  go func() {\n    for {\n      select {\n      case <-ctx.Done():\n        return\n      default:\n      }\n      for _, t := range tenants {\n        if t.inFlight >= t.maxInFlight {\n          continue\n        }\n        if t.bucket.TryTake() {\n          select {\n          case e := <-t.in:\n            t.inFlight++\n            go func(ev Event, tt *Tenant) {\n              time.Sleep(50 * time.Millisecond)\n              tt.inFlight--\n              tt.bucket.Refund(1)\n              fmt.Printf(\"Tenant %s processed %s\\n\", tt.id, ev.Payload)\n            }(e, t)\n          default:\n            t.bucket.Refund(1)\n          }\n        }\n      }\n      time.Sleep(1 * time.Millisecond)\n    }\n  }()\n\n  // Producers with backpressure\n  go func() {\n    for i := 0; i < 100; i++ {\n      tA.in <- Event{Payload: fmt.Sprintf(\"A-%d\", i)}\n      time.Sleep(20 * time.Millisecond)\n    }\n  }()\n  go func() {\n    for i := 0; i < 60; i++ {\n      tB.in <- Event{Payload: fmt.Sprintf(\"B-%d\", i)}\n      time.Sleep(30 * time.Millisecond)\n    }\n  }()\n\n  // Run for a while then shutdown\n  time.Sleep(5 * time.Second)\n  cancel()\n  time.Sleep(200 * time.Millisecond)\n}\n```\n\n## Follow-up Questions\n- How would you adapt the design to N tenants with dynamic quotas?\n- How would you measure and optimize tail latency under bursty traffic?","diagram":null,"difficulty":"advanced","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","NVIDIA","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T12:48:08.710Z","createdAt":"2026-01-12T12:48:08.710Z"},{"id":"q-8375","question":"Design a high-throughput bounded ring-buffer queue for a real-time analytics backend: many producers enqueue events into a fixed-size in-memory ring, a small worker pool dequeues for processing, and per-partition ordering must be preserved; apply backpressure when full, support graceful shutdown, and spill overflow to disk. Describe data structures, synchronization, and a test plan?","answer":"Implement a fixed-size circular ring buffer with atomic head/tail indices and per-partition sequencing. Use compare-and-swap (CAS) to advance tail on enqueue and head on dequeue; preserve per-partition order by embedding a partition identifier and sequence number in each entry. When the buffer reaches capacity, apply backpressure by blocking or rejecting new entries until space becomes available. Support graceful shutdown using atomic flags and condition variables to signal workers to finish processing remaining items before exit. For overflow scenarios, implement a spill-to-disk mechanism that writes excess entries to persistent storage when the in-memory buffer is full, with a recovery path to reload spilled items during startup.","explanation":"## Why This Is Asked\nTests the ability to design a bounded, low-latency queue with per-partition ordering, backpressure, and graceful shutdown. It also probes fault-tolerance concepts like spill-to-disk for handling overflow scenarios.\n\n## Key Concepts\n- Bounded ring buffer with atomic indices for lock-free operations\n- Per-partition ordering guarantees using sequence numbers\n- Lock-free or low-lock synchronization with proper memory ordering\n- Backpressure strategies to prevent resource exhaustion\n- Graceful shutdown coordination with worker notification\n- Spill-to-disk path for overflow handling\n\n## Code Example\n```go\n// Simplified data structures (illustrative)\ntype Entry struct {\n  PartitionID int\n  Sequence   uint64\n  Data       []byte\n}\n\ntype RingBuffer struct {\n  Buffer    []Entry\n  Capacity   int64\n  Head       int64  // Consumer position\n  Tail       int64  // Producer position\n  Mask       int64  // Capacity-1 for modulo operations\n  Full       bool\n  Shutdown    int32  // Atomic shutdown flag\n}\n\nfunc (rb *RingBuffer) Enqueue(entry Entry) error {\n  for {\n    head := atomic.LoadInt64(&rb.Head)\n    tail := atomic.LoadInt64(&rb.Head)\n    \n    if (tail-head) >= rb.Capacity {\n      // Buffer full, apply backpressure or spill to disk\n      return rb.spillToDisk(entry)\n    }\n    \n    // Attempt to advance tail using CAS\n    newTail := (tail + 1) & rb.Mask\n    if atomic.CompareAndSwapInt64(&rb.Tail, tail, newTail) {\n      rb.Buffer[tail] = entry\n      return nil\n    }\n  }\n}\n\nfunc (rb *RingBuffer) Dequeue() (Entry, bool) {\n  for {\n    if atomic.LoadInt32(&rb.Shutdown) == 1 {\n      return Entry{}, false\n    }\n    \n    head := atomic.LoadInt64(&rb.Head)\n    tail := atomic.LoadInt64(&rb.Tail)\n    \n    if head == tail {\n      return Entry{}, false  // Empty\n    }\n    \n    // Attempt to advance head using CAS\n    newHead := (head + 1) & rb.Mask\n    if atomic.CompareAndSwapInt64(&rb.Head, head, newHead) {\n      return rb.Buffer[head], true\n    }\n  }\n}\n```\n\n## Test Plan\n1. **Unit Tests**: Verify enqueue/dequeue correctness, boundary conditions, and CAS operations\n2. **Concurrency Tests**: Stress test with multiple producers/consumers under high load\n3. **Ordering Tests**: Validate per-partition sequence preservation\n4. **Backpressure Tests**: Confirm proper blocking/rejection when buffer is full\n5. **Shutdown Tests**: Verify graceful termination with all items processed\n6. **Spill-to-Disk Tests**: Test overflow handling and recovery scenarios\n7. **Performance Tests**: Measure throughput and latency under various load conditions\n8. **Fault Injection**: Test behavior during failures and recovery","diagram":null,"difficulty":"advanced","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Lyft","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-28T05:19:38.577Z","createdAt":"2026-01-27T21:52:11.686Z"},{"id":"q-8499","question":"Design a beginner-friendly concurrent event queue for a real-time analytics system: hundreds of producers push events tagged with a priority (high, normal, low) into a bounded global queue. Implement a simple scheduler that always serves high-priority events first, but guarantees aging so normal and low-priority events are not starved (e.g., promotion after a fixed wait). Use a small worker pool to process events and measure throughput and latency. Provide a concrete implementation approach in Go or Python asyncio and a test plan?","answer":"Three bounded queues by priority (high, normal, low). Scheduler pops from high if non-empty, else normal, else low; aging promotes waited items to the next higher priority after a fixed timeout (e.g.,","explanation":"## Why This Is Asked\nThis probes practical concurrency design: multi-priority queues, bounded backpressure, and aging to prevent starvation in a real-time setting.\n\n## Key Concepts\n- Bounded priority queues\n- Scheduler fairness via aging\n- Worker pool basics and backpressure\n- Observability and basic test planning\n\n## Code Example\n```python\n# Basic sketch of a 3-queue scheduler\nfrom queue import Queue\nqueues = {'high': Queue(32), 'normal': Queue(64), 'low': Queue(64)}\n\ndef process(item):\n    pass\n\ndef schedule():\n    while True:\n        if not queues['high'].empty():\n            item = queues['high'].get()\n        elif not queues['normal'].empty():\n            item = queues['normal'].get()\n        else:\n            item = queues['low'].get()\n        process(item)\n\n# Aging: promote items after wait (pseudo)\n```\n\n## Follow-up Questions\n- How would you instrument aging and backpressure in metrics?\n- Which race conditions could arise and how would you test them?","diagram":null,"difficulty":"beginner","tags":["concurrency"],"channel":"concurrency","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-28T05:55:59.143Z","createdAt":"2026-01-28T05:55:59.143Z"}],"subChannels":["general"],"companies":["Adobe","Airbnb","Amazon","Anthropic","Apple","Bloomberg","Citadel","Cloudflare","Coinbase","Databricks","Discord","DoorDash","Goldman Sachs","Google","Hashicorp","Hugging Face","IBM","Instacart","LinkedIn","Lyft","Meta","Microsoft","MongoDB","NVIDIA","Netflix","OpenAI","Oracle","PayPal","Plaid","Robinhood","Salesforce","Scale Ai","Slack","Snap","Snowflake","Square","Stripe","Tesla","Twitter","Two Sigma","Uber","Zoom"],"stats":{"total":91,"beginner":31,"intermediate":30,"advanced":30,"newThisWeek":39}}