{"questions":[{"id":"terraform-associate-iac-concepts-1768193518666-0","question":"You are organizing a multi-team Terraform project with shared modules and a centralized remote state backend. Which approach best ensures consistent provider versions, standardized backends, and safe remote state sharing across teams?","answer":"[{\"id\":\"a\",\"text\":\"Create local copies of modules for each team and configure separate backends and provider versions per team\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Publish all resources in a single monolithic configuration with no modules and a shared backend\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Publish shared modules to a centralized module registry and configure a single remote backend with versioned providers, using a shared backend and workspace for remote state\",\"isCorrect\":true},{\"id\":\"d\",\"text\":\"Duplicate infrastructure code per environment with separate state files and no module reuse\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption C is correct. By publishing shared modules to a centralized registry and configuring a single remote backend with pinned provider versions, teams reuse validated configurations and share state safely, reducing drift and conflicts.\n\n## Why Other Options Are Wrong\n- A: Lacks centralization and module reuse, leading to drift and inconsistent state handling.\n- B: Removes modularity and shared state governance, increasing maintenance burden.\n- D: Duplicates code across environments and teams, causing divergence and governance problems.\n\n## Key Concepts\n- Module Registry and reuse\n- Backend configuration and remote state\n- required_providers and version pinning\n- State locking and collaboration\n\n## Real-World Application\nA large organization standardizes infrastructure by exposing common modules via a registry and using a shared remote backend to coordinate across teams during apply operations.","diagram":null,"difficulty":"intermediate","tags":["Terraform","AWS","Module Registry","Remote-State","State-Locking","certification-mcq","domain-weight-15"],"channel":"terraform-associate","subChannel":"iac-concepts","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T04:51:58.667Z","createdAt":"2026-01-12 04:51:59"},{"id":"terraform-associate-iac-concepts-1768193518666-1","question":"You are managing an AWS S3 data lake bucket via Terraform and want to prevent accidental deletion while still allowing legitimate destroy operations when explicitly requested. Which Terraform feature best achieves this?","answer":"[{\"id\":\"a\",\"text\":\"Use a lifecycle block with prevent_destroy = true\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Set force_destroy = true on the aws_s3_bucket resource\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Attach an IAM policy that denies s3:DeleteBucket for the Terraform role\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use ignore_changes on the bucket's force_destroy attribute\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA is correct because a lifecycle block with prevent_destroy = true prevents Terraform from destroying the bucket during apply unless the block is removed or modified. This protects against accidental deletions.\n\n## Why Other Options Are Wrong\n- B would force destruction regardless of contents, which defeats safety.\n- C could be bypassed by using credentials with delete permissions or manual overrides; it does not guarantee plan-time safety.\n- D ignore_changes does not prevent delete operations and is not a safety mechanism for destructive actions.\n\n## Key Concepts\n- Lifecycle blocks\n- prevent_destroy\n- Destruction safety in Terraform\n\n## Real-World Application\nOperators protect critical data assets from accidental deletion by enforcing a lifecycle safeguard, requiring an explicit action to disable the protection before deletion.","diagram":null,"difficulty":"intermediate","tags":["Terraform","AWS","S3","IAM","certification-mcq","domain-weight-15"],"channel":"terraform-associate","subChannel":"iac-concepts","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T04:51:59.083Z","createdAt":"2026-01-12 04:51:59"},{"id":"terraform-associate-iac-concepts-1768193518666-2","question":"A Terraform configuration creates a VM and a bootstrap script executed via a null_resource using local-exec. The script must run only after the VM and its network resources exist. Which pattern ensures the bootstrap runs in the correct order?","answer":"[{\"id\":\"a\",\"text\":\"Use depends_on to explicitly reference the VM and network resources on the null_resource\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Use a separate apply with -target to run the bootstrap after the VM creation\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Place the bootstrap script in the same resource block to create an implicit dependency\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use a separate module and trigger it with a separate Terraform workspace\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA is correct because explicit depends_on declarations ensure the null_resource executes only after the VM and network resources are created, establishing a clear ordering for the bootstrap.\n\n## Why Other Options Are Wrong\n- B relies on targeted applies which can be brittle and do not guarantee ordering within a single plan.\n- C cannot establish a reliable dependency since the script is a separate provisioner and not referenced directly.\n- D introduces complexity and does not guarantee ordering across workspaces.\n\n## Key Concepts\n- Depends_on and resource ordering\n- Provisioners (local-exec) caveats\n- Implicit vs explicit dependencies\n\n## Real-World Application\nEnsures bootstrap steps run only after required infrastructure components exist, preventing failures during bootstrap.","diagram":null,"difficulty":"intermediate","tags":["Terraform","AWS","EC2","Provisioners","Local-exec","certification-mcq","domain-weight-15"],"channel":"terraform-associate","subChannel":"iac-concepts","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T04:51:59.468Z","createdAt":"2026-01-12 04:51:59"},{"id":"terraform-associate-terraform-basics-1768155950528-0","question":"In a team environment, you need to ensure safe concurrent Terraform operations across multiple developers and CI runs. Which backend configuration best prevents conflicting state writes?","answer":"[{\"id\":\"a\",\"text\":\"Use the local backend so state is kept in the repository alongside configuration\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use a remote backend with S3 as the state store and DynamoDB for state locking\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Use a remote backend with Consul as storage but without a locking mechanism\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use Terraform Cloud as the backend with default locking behavior\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct answer is B. Using a remote backend with S3 and DynamoDB enables persistent state storage and locking, which prevents concurrent writes from racing each other in multiple runs. \n\n## Why Other Options Are Wrong\n- A: Local backend stores state on disk and is not suitable for collaboration or locking. \n- C: Consul backend exists but does not provide DynamoDB-style locking, which undermines concurrent safety. \n- D: Terraform Cloud is a valid remote backend option but the question specifies a classic AWS-backed setup with explicit locking via DynamoDB; Terraform Cloud is a different service and may introduce different operational considerations. \n\n## Key Concepts\n- Remote state with locking\n- Backends (S3) and locking mechanisms (DynamoDB)\n\n## Real-World Application\n- When multiple engineers and CI pipelines apply changes, this pattern prevents corrupt state by ensuring only one operation can write at a time.","diagram":null,"difficulty":"intermediate","tags":["Terraform","AWS","S3","DynamoDB","Backends","CI/CD","certification-mcq","domain-weight-20"],"channel":"terraform-associate","subChannel":"terraform-basics","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T18:25:50.531Z","createdAt":"2026-01-11 18:25:50"},{"id":"terraform-associate-terraform-basics-1768155950528-1","question":"You have a child module that outputs a database password along with the database endpoint. To avoid leaking the password in CLI output and logs, which approach should you take?","answer":"[{\"id\":\"a\",\"text\":\"Expose the password directly via a standard output value\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Mark the password output as sensitive = true\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Store the password in the repository so it is versioned\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use a separate data source to fetch the password at apply time\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct answer is B. Marking the output as sensitive hides the value from default CLI displays and plan output, reducing the risk of accidentally leaking secrets. \n\n## Why Other Options Are Wrong\n- A: Exposing the password defeats purpose of masking and is insecure. \n- C: Storing passwords in the repository risks credential leakage and accidental exposure. \n- D: A data source cannot inherently mask the password when surfaced as an output and adds unnecessary complexity; sensitive outputs are the standard approach for secret values output by modules. \n\n## Key Concepts\n- Outputs can be marked sensitive to prevent display\n- Secrets should not be logged or displayed by default\n\n## Real-World Application\n- In CI pipelines, marking outputs as sensitive ensures secrets like DB passwords are not printed in logs while still allowing dependent modules to consume the value programmatically.","diagram":null,"difficulty":"intermediate","tags":["Terraform","Security","Secrets","AWS","RDS","certification-mcq","domain-weight-20"],"channel":"terraform-associate","subChannel":"terraform-basics","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T18:25:50.895Z","createdAt":"2026-01-11 18:25:51"},{"id":"terraform-associate-terraform-basics-1768155950528-2","question":"You manage two Kubernetes clusters (prod and dev) on AWS EKS and want to manage resources in both clusters from a single Terraform configuration. Which approach is most appropriate?","answer":"[{\"id\":\"a\",\"text\":\"Create two entirely separate Terraform configurations for the two clusters\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Define multiple Kubernetes providers with different aliases and specify provider = kubernetes.prod or kubernetes.dev in resources\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Use a single Kubernetes provider and dynamically switch the cluster using a runtime variable in each resource\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use the same provider for both clusters and rely on separate namespaces to segregate resources\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct answer is B. Terraform supports multiple provider configurations with aliases. You can define separate Kubernetes providers for prod and dev clusters and assign them to resources using provider = kubernetes.prod or provider = kubernetes.dev. \n\n```javascript\nprovider \"kubernetes\" {\n  alias = \"prod\"\n  host = var.prod_host\n  client_certificate = var.prod_cert\n  token = var.prod_token\n}\nprovider \"kubernetes\" {\n  alias = \"dev\"\n  host = var.dev_host\n  client_certificate = var.dev_cert\n  token = var.dev_token\n}\nresource \"kubernetes_namespace\" \"prod_ns\" {\n  provider = kubernetes.prod\n  metadata { name = \"production\" }\n}\nresource \"kubernetes_namespace\" \"dev_ns\" {\n  provider = kubernetes.dev\n  metadata { name = \"development\" }\n}\n```\n\n## Why Other Options Are Wrong\n- A: Two separate configurations are valid but increases maintenance and duplication; a single config with aliases is more scalable. \n- C: A single provider cannot target multiple clusters simultaneously without prefixes/aliases. \n- D: Namespaces do not provide isolation at the provider level and wonâ€™t allow per-cluster API interactions. \n\n## Key Concepts\n- Provider aliases for multi-cluster management\n- Resource-level provider assignment\n\n## Real-World Application\n- Simplifies multi-cluster governance by centralizing configuration while keeping prod/dev isolated.","diagram":null,"difficulty":"intermediate","tags":["Terraform","Kubernetes","EKS","AWS","Provider Aliases","certification-mcq","domain-weight-20"],"channel":"terraform-associate","subChannel":"terraform-basics","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T18:25:51.250Z","createdAt":"2026-01-11 18:25:51"}],"subChannels":["iac-concepts","terraform-basics"],"companies":[],"stats":{"total":6,"beginner":0,"intermediate":6,"advanced":0,"newThisWeek":6}}