{"questions":[{"id":"q-1009","question":"Scenario: a single repo provisions per-tenant AWS resources for many tenants. To isolate state without workspaces, use a shared S3 backend with a DynamoDB lock table and a tenant-scoped key. Describe the backend config (bucket, region, dynamodb_table, key pattern per tenant), how you detect drift across tenants, and CI gating for dev-to-prod promotions (plan -out, tests, approve, apply)?","answer":"Configure a shared S3 backend with per-tenant keys and a single DynamoDB lock table. Backend config example: bucket=tf-backend, region=us-east-1, dynamodb_table=tf-locks, key=tenants/${TENANT_ID}/terr","explanation":"## Why This Is Asked\nTests mastery of multi-tenant state isolation, drift detection, and CI-gated promotions using Terraform remote backend.\n\n## Key Concepts\n- Remote state isolation without workspaces\n- Backend config per-tenant key patterns\n- Centralized locking with DynamoDB\n- Drift detection via plan and refresh\n- CI gating with plan/out, tests, approvals\n\n## Code Example\n```javascript\n# backend-configs/dev.tfbackend\nbucket = \"tf-backend\"\nregion = \"us-east-1\"\ndynamodb_table = \"tf-locks\"\nkey = \"tenants/${TENANT_ID}/terraform.tfstate\"\n```\n\n## Follow-up Questions\n- How would you enforce per-tenant role-based access to the backend?\n- How do you handle tenant onboarding/offboarding when state files move?","diagram":null,"difficulty":"advanced","tags":["terraform-associate"],"channel":"terraform-associate","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Coinbase","Databricks"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T19:21:33.339Z","createdAt":"2026-01-12T19:21:33.339Z"},{"id":"q-1154","question":"In a Terraform module that provisions an AWS RDS instance, operators sometimes alter maintenance_window directly in AWS, creating drift. You want Terraform to ignore external changes to maintenance_window but still apply code-driven updates (e.g., allocated_storage). How would you implement this using a lifecycle block? Provide a minimal aws_db_instance snippet showing ignore_changes for maintenance_window and outline tradeoffs?","answer":"Use: lifecycle { ignore_changes = [maintenance_window] }. In the aws_db_instance resource, keep allocated_storage driven by config. Caveats: other fields drift remains visible only if not ignored; if ","explanation":"## Why This Is Asked\n\nShows practical drift handling with Terraform state; tests understanding of lifecycle rules and their impact on operations.\n\n## Key Concepts\n\n- Drift and state\n- lifecycle ignore_changes\n- Resource attribute scoping\n\n## Code Example\n\n```javascript\nresource \"aws_db_instance\" \"example\" {\n  identifier         = \"example\"\n  engine             = \"mysql\"\n  instance_class     = \"db.t3.micro\"\n  allocated_storage  = 20\n  maintenance_window = \"sun:23:45-sun:00:15\"\n\n  lifecycle {\n    ignore_changes = [maintenance_window]\n  }\n}\n```\n\n## Follow-up Questions\n\n- What would happen if maintenance_window is changed in code while an apply is in progress?\n- How would you revert drift if needed later?","diagram":null,"difficulty":"intermediate","tags":["terraform-associate"],"channel":"terraform-associate","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Salesforce","Scale Ai","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T01:37:52.444Z","createdAt":"2026-01-13T01:37:52.444Z"},{"id":"q-1172","question":"In AWS us-east-1, you must provision 3 private subnets in a single VPC across 3 AZs using one module. Define a map variable with AZs and CIDRs, create the subnets with for_each, attach appropriate tags, and output the subnet IDs. Then show how to reference these IDs in a resource that requires subnet_id (eg NAT Gateway) and justify for_each vs count?","answer":"Use a map variable for subnets keyed by AZ, create aws_subnet with for_each, set az and cidr_block from the map, and TAGS. Output the IDs for reuse. Example: create a NAT gateway per subnet by iterati","explanation":"## Why This Is Asked\nAssesses practical use of for_each with maps, resource interdependencies, and stable identities across environments. Delivers hands-on pattern candidates expect in production.\n\n## Key Concepts\n- for_each with maps provides stable keys and predictable IDs\n- referencing attributes from one resource in another (subnet_id in NAT GW)\n- tagging and AZ assignment per subnet\n- outputs to surface IDs for downstream resources\n- when to prefer for_each vs count (stable keys vs positional indexing)\n\n## Code Example\n```javascript\nvariable \"subnets\" {\n  type = map(object({ cidr_block = string; az = string }))\n  default = {\n    az1 = { cidr_block = \"10.0.1.0/24\", az = \"us-east-1a\" },\n    az2 = { cidr_block = \"10.0.2.0/24\", az = \"us-east-1b\" },\n    az3 = { cidr_block = \"10.0.3.0/24\", az = \"us-east-1c\" }\n  }\n}\n\nresource \"aws_subnet\" \"private\" {\n  for_each = var.subnets\n  vpc_id            = var.vpc_id\n  cidr_block        = each.value.cidr_block\n  availability_zone = each.value.az\n  tags = {\n    Name = \"private-${each.key}\"\n  }\n}\n\noutput \"private_subnet_ids\" {\n  value = [for s in aws_subnet.private : s.id]\n}\n```\n\n```javascript\nresource \"aws_nat_gateway\" \"ngw\" {\n  for_each = aws_subnet.private\n  allocation_id = aws_eip.ngw[each.key].id\n  subnet_id     = aws_subnet.private[each.key].id\n}\n```\n\n## Follow-up Questions\n- When would you choose for_each over count in real projects?\n- How would you migrate from count-based to for_each-based resources without breaking state?\n- How would you validate AZ distribution and CIDR correctness after apply?","diagram":"flowchart TD\n  S[Subnets] --> A1[Subnet us-east-1a]\n  S --> A2[Subnet us-east-1b]\n  S --> A3[Subnet us-east-1c]\n  A1 --> NGW[Per-Subnet NAT Gateway]","difficulty":"beginner","tags":["terraform-associate"],"channel":"terraform-associate","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Netflix","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T03:37:27.209Z","createdAt":"2026-01-13T03:37:27.209Z"},{"id":"q-1202","question":"Scenario: you must bring under Terraform management a set of existing AWS S3 buckets across teams. Some buckets already exist and must be imported; you will manage encryption and versioning with a single module using for_each over a bucket map. How would you import, structure, and promote changes safely via CI?","answer":"Import existing buckets into state by address, e.g. terraform import 'aws_s3_bucket.bucket[\"team-a\"]' team-a-bucket; define a map of buckets with per-bucket flags and use for_each to create resources;","explanation":"## Why This Is Asked\n\nThis checks practical import workflows and multi-resource management with for_each, keeping data in sync and enabling safe CI-driven promotions.\n\n## Key Concepts\n\n- terraform import with for_each\n- map of per-bucket settings\n- dynamic blocks for conditional blocks\n- drift reconciliation and idempotency\n- CI/CD validation and promotion\n\n## Code Example\n\n```javascript\nvariable \"buckets\" {\n  type = map(object({\n    encryption = bool\n    versioning = bool\n  }))\n}\nresource \"aws_s3_bucket\" \"bucket\" {\n  for_each = var.buckets\n  bucket = each.key\n  acl    = \"private\"\n\n  dynamic \"versioning\" {\n    for_each = each.value.versioning ? [1] : []\n    content {\n      enabled = true\n    }\n  }\n\n  dynamic \"server_side_encryption_configuration\" {\n    for_each = each.value.encryption ? [1] : []\n    content {\n      rule {\n        apply_server_side_encryption_by_default {\n          sse_algorithm = \"AES256\"\n        }\n      }\n    }\n  }\n}\n```\n\n```javascript\nterraform import 'aws_s3_bucket.bucket[\"team-a\"]' team-a-bucket\n```\n\n## Follow-up Questions\n\n- How would you handle a bucket rename in the map without recreating?\n- How would you test encryption changes in CI without touching prod data?\n","diagram":null,"difficulty":"advanced","tags":["terraform-associate"],"channel":"terraform-associate","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","NVIDIA","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T04:49:50.394Z","createdAt":"2026-01-13T04:49:50.394Z"},{"id":"q-1338","question":"Scenario: You maintain a Terraform repo that provisions an AWS RDS primary in us-east-1 and an optional cross-region read replica in us-west-2 controlled by a feature flag var.enable_replica. If the flag is false, Terraform should destroy the replica on apply. How would you implement the cross-region provider, conditional resource creation, and safe destruction without leaving dangling resources? Provide code patterns for the provider alias and the replica resource?","answer":"Define two AWS providers with aliases (primary us-east-1 and replica us-west-2). Create the replica resource with count = var.enable_replica ? 1 : 0 and assign provider = aws.replica; declare depends_","explanation":"## Why This Is Asked\nThis question tests practical capabilities to manage cross-region resources with provider aliases and conditional creation, ensuring safe destruction when a feature flag disables the replica, and understanding of Terraform lifecycle to avoid downtime.\n\n## Key Concepts\n- provider aliases\n- conditional resource creation with count\n- cross-region dependencies\n- lifecycle create_before_destroy\n- drift and state management\n\n## Code Example\n```javascript\nprovider \"aws\" {\n  region = \"us-east-1\"\n  alias  = \"primary\"\n}\nprovider \"aws\" {\n  region = \"us-west-2\"\n  alias  = \"replica\"\n}\n\nresource \"aws_db_instance\" \"primary\" {\n  provider = aws.primary\n  # ...\n}\n\nresource \"aws_db_instance\" \"replica\" {\n  provider   = aws.replica\n  count      = var.enable_replica ? 1 : 0\n  depends_on = [aws_db_instance.primary]\n  lifecycle {\n    create_before_destroy = true\n  }\n}\n```\n\n## Follow-up Questions\n- What are the failover considerations for cross-region replicas?\n- How would you test this pattern in CI and guard against accidental destruction?","diagram":null,"difficulty":"intermediate","tags":["terraform-associate"],"channel":"terraform-associate","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","NVIDIA","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T11:44:40.993Z","createdAt":"2026-01-13T11:44:40.993Z"},{"id":"q-864","question":"In Terraform, you need to manage two AWS accounts in a single repo: prod (provider aws.prod) and audit (provider aws.audit). Create an S3 bucket in prod and a cross-account IAM policy in audit that grants read access to that bucket. How do you configure aliased providers, reference the prod bucket ARN from the audit module, and enforce deterministic apply order (e.g., data fetch before policy) with minimal risk? Include a minimal config outline?","answer":"Define two providers with aliases (aws.prod and aws.audit). Create the bucket using aws.prod, then fetch its ARN from prod via data.aws_s3_bucket in the audit config and craft an audit IAM policy that","explanation":"## Why This Is Asked\n\nThis question tests multi-account Terraform patterns: provider aliases, cross-account data sources, and deterministic apply order.\n\n## Key Concepts\n\n- Aliased providers\n- Cross-account data sources\n- Data referencing across providers\n- depends_on for ordering\n- Credentials management via profiles and assume_role\n\n## Code Example\n\n```hcl\nprovider \"aws\" {\n  alias   = \"prod\"\n  region  = \"us-east-1\"\n  profile = \"prod\"\n}\nprovider \"aws\" {\n  alias   = \"audit\"\n  region  = \"us-east-1\"\n  profile = \"audit\"\n}\n\ndata \"aws_caller_identity\" \"prod\" {\n  provider = aws.prod\n}\n\ndata \"aws_s3_bucket\" \"prod_bucket\" {\n  provider = aws.prod\n  bucket   = \"my-prod-bucket\"\n}\n\ndata \"aws_iam_policy_document\" \"audit\" {\n  provider = aws.audit\n  statement {\n    actions   = [\"s3:GetObject\"]\n    resources = [\"${data.aws_s3_bucket.prod_bucket.arn}/*\"]\n    principals {\n      type        = \"AWS\"\n      identifiers = [\"arn:aws:iam::${data.aws_caller_identity.prod.account_id}:root\"]\n    }\n  }\n}\n\nresource \"aws_s3_bucket\" \"audit_bucket\" {\n  provider = aws.audit\n  bucket   = \"audit-bucket\"\n}\n\nresource \"aws_iam_policy\" \"audit_policy\" {\n  provider = aws.audit\n  name     = \"CrossAccountGetObject\"\n  policy   = data.aws_iam_policy_document.audit.json\n}\n```\n\n## Follow-up Questions\n\n- How would you rotate credentials for the aliased providers safely?\n- How would you test this cross-account permission in a CI workflow?","diagram":null,"difficulty":"intermediate","tags":["terraform-associate"],"channel":"terraform-associate","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Snap","Snowflake","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T13:45:51.426Z","createdAt":"2026-01-12T13:45:51.426Z"},{"id":"q-960","question":"In a single Terraform repo that provisions prod, staging, and dev AWS environments, how would you configure a single S3 backend to isolate state for each environment without using separate Terraform workspaces? Provide the exact backend config (bucket, region, dynamodb_lock_table, key per env) and explain how you would promote changes from dev to prod, including drift handling and CI plan/apply steps?","answer":"Use one S3 bucket with environment-scoped keys and per-env locking: dev/terraform.tfstate with tf-lock-dev, staging/terraform.tfstate with tf-lock-staging, prod/terraform.tfstate with tf-lock-prod. Do","explanation":"## Why This Is Asked\n\nTests practical backend configuration and multi-environment isolation without relying on workspaces, which can obscure state provenance. It also covers promotion workflows and drift handling in CI.\n\n## Key Concepts\n\n- Terraform backends and per-environment state isolation\n- State locking with DynamoDB per environment\n- Promotion pipelines and drift considerations in CI/CD\n\n## Code Example\n\n```hcl\nterraform {\n  backend \"s3\" {\n    bucket         = \"tf-state-bucket\"\n    key            = \"dev/terraform.tfstate\"\n    region         = \"us-east-1\"\n    dynamodb_table = \"tf-lock-dev\"\n    encrypt        = true\n  }\n}\n```\n\n```hcl\nterraform {\n  backend \"s3\" {\n    bucket         = \"tf-state-bucket\"\n    key            = \"prod/terraform.tfstate\"\n    region         = \"us-east-1\"\n    dynamodb_table = \"tf-lock-prod\"\n    encrypt        = true\n  }\n}\n```\n\n## Follow-up Questions\n\n- How would you handle a state drift detected in prod after a manual change?\n- What are the trade-offs of using a single bucket with per-env keys vs separate buckets per env?","diagram":null,"difficulty":"intermediate","tags":["terraform-associate"],"channel":"terraform-associate","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Google","Hashicorp"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T17:21:52.897Z","createdAt":"2026-01-12T17:21:52.897Z"}],"subChannels":["general"],"companies":["Amazon","Bloomberg","Coinbase","Databricks","Google","Hashicorp","Hugging Face","NVIDIA","Netflix","Oracle","Robinhood","Salesforce","Scale Ai","Snap","Snowflake","Square","Tesla"],"stats":{"total":7,"beginner":1,"intermediate":4,"advanced":2,"newThisWeek":7}}