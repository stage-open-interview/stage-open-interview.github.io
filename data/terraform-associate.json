{"questions":[{"id":"q-864","question":"In Terraform, you need to manage two AWS accounts in a single repo: prod (provider aws.prod) and audit (provider aws.audit). Create an S3 bucket in prod and a cross-account IAM policy in audit that grants read access to that bucket. How do you configure aliased providers, reference the prod bucket ARN from the audit module, and enforce deterministic apply order (e.g., data fetch before policy) with minimal risk? Include a minimal config outline?","answer":"Define two providers with aliases (aws.prod and aws.audit). Create the bucket using aws.prod, then fetch its ARN from prod via data.aws_s3_bucket in the audit config and craft an audit IAM policy that","explanation":"## Why This Is Asked\n\nThis question tests multi-account Terraform patterns: provider aliases, cross-account data sources, and deterministic apply order.\n\n## Key Concepts\n\n- Aliased providers\n- Cross-account data sources\n- Data referencing across providers\n- depends_on for ordering\n- Credentials management via profiles and assume_role\n\n## Code Example\n\n```hcl\nprovider \"aws\" {\n  alias   = \"prod\"\n  region  = \"us-east-1\"\n  profile = \"prod\"\n}\nprovider \"aws\" {\n  alias   = \"audit\"\n  region  = \"us-east-1\"\n  profile = \"audit\"\n}\n\ndata \"aws_caller_identity\" \"prod\" {\n  provider = aws.prod\n}\n\ndata \"aws_s3_bucket\" \"prod_bucket\" {\n  provider = aws.prod\n  bucket   = \"my-prod-bucket\"\n}\n\ndata \"aws_iam_policy_document\" \"audit\" {\n  provider = aws.audit\n  statement {\n    actions   = [\"s3:GetObject\"]\n    resources = [\"${data.aws_s3_bucket.prod_bucket.arn}/*\"]\n    principals {\n      type        = \"AWS\"\n      identifiers = [\"arn:aws:iam::${data.aws_caller_identity.prod.account_id}:root\"]\n    }\n  }\n}\n\nresource \"aws_s3_bucket\" \"audit_bucket\" {\n  provider = aws.audit\n  bucket   = \"audit-bucket\"\n}\n\nresource \"aws_iam_policy\" \"audit_policy\" {\n  provider = aws.audit\n  name     = \"CrossAccountGetObject\"\n  policy   = data.aws_iam_policy_document.audit.json\n}\n```\n\n## Follow-up Questions\n\n- How would you rotate credentials for the aliased providers safely?\n- How would you test this cross-account permission in a CI workflow?","diagram":null,"difficulty":"intermediate","tags":["terraform-associate"],"channel":"terraform-associate","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Snap","Snowflake","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:45:51.426Z","createdAt":"2026-01-12T13:45:51.426Z"},{"id":"terraform-associate-iac-concepts-1768193518666-0","question":"You are organizing a multi-team Terraform project with shared modules and a centralized remote state backend. Which approach best ensures consistent provider versions, standardized backends, and safe remote state sharing across teams?","answer":"[{\"id\":\"a\",\"text\":\"Create local copies of modules for each team and configure separate backends and provider versions per team\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Publish all resources in a single monolithic configuration with no modules and a shared backend\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Publish shared modules to a centralized module registry and configure a single remote backend with versioned providers, using a shared backend and workspace for remote state\",\"isCorrect\":true},{\"id\":\"d\",\"text\":\"Duplicate infrastructure code per environment with separate state files and no module reuse\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption C is correct. By publishing shared modules to a centralized registry and configuring a single remote backend with pinned provider versions, teams reuse validated configurations and share state safely, reducing drift and conflicts.\n\n## Why Other Options Are Wrong\n- A: Lacks centralization and module reuse, leading to drift and inconsistent state handling.\n- B: Removes modularity and shared state governance, increasing maintenance burden.\n- D: Duplicates code across environments and teams, causing divergence and governance problems.\n\n## Key Concepts\n- Module Registry and reuse\n- Backend configuration and remote state\n- required_providers and version pinning\n- State locking and collaboration\n\n## Real-World Application\nA large organization standardizes infrastructure by exposing common modules via a registry and using a shared remote backend to coordinate across teams during apply operations.","diagram":null,"difficulty":"intermediate","tags":["Terraform","AWS","Module Registry","Remote-State","State-Locking","certification-mcq","domain-weight-15"],"channel":"terraform-associate","subChannel":"iac-concepts","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T04:51:58.667Z","createdAt":"2026-01-12 04:51:59"},{"id":"terraform-associate-iac-concepts-1768193518666-1","question":"You are managing an AWS S3 data lake bucket via Terraform and want to prevent accidental deletion while still allowing legitimate destroy operations when explicitly requested. Which Terraform feature best achieves this?","answer":"[{\"id\":\"a\",\"text\":\"Use a lifecycle block with prevent_destroy = true\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Set force_destroy = true on the aws_s3_bucket resource\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Attach an IAM policy that denies s3:DeleteBucket for the Terraform role\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use ignore_changes on the bucket's force_destroy attribute\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA is correct because a lifecycle block with prevent_destroy = true prevents Terraform from destroying the bucket during apply unless the block is removed or modified. This protects against accidental deletions.\n\n## Why Other Options Are Wrong\n- B would force destruction regardless of contents, which defeats safety.\n- C could be bypassed by using credentials with delete permissions or manual overrides; it does not guarantee plan-time safety.\n- D ignore_changes does not prevent delete operations and is not a safety mechanism for destructive actions.\n\n## Key Concepts\n- Lifecycle blocks\n- prevent_destroy\n- Destruction safety in Terraform\n\n## Real-World Application\nOperators protect critical data assets from accidental deletion by enforcing a lifecycle safeguard, requiring an explicit action to disable the protection before deletion.","diagram":null,"difficulty":"intermediate","tags":["Terraform","AWS","S3","IAM","certification-mcq","domain-weight-15"],"channel":"terraform-associate","subChannel":"iac-concepts","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T04:51:59.083Z","createdAt":"2026-01-12 04:51:59"},{"id":"terraform-associate-iac-concepts-1768193518666-2","question":"A Terraform configuration creates a VM and a bootstrap script executed via a null_resource using local-exec. The script must run only after the VM and its network resources exist. Which pattern ensures the bootstrap runs in the correct order?","answer":"[{\"id\":\"a\",\"text\":\"Use depends_on to explicitly reference the VM and network resources on the null_resource\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Use a separate apply with -target to run the bootstrap after the VM creation\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Place the bootstrap script in the same resource block to create an implicit dependency\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use a separate module and trigger it with a separate Terraform workspace\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA is correct because explicit depends_on declarations ensure the null_resource executes only after the VM and network resources are created, establishing a clear ordering for the bootstrap.\n\n## Why Other Options Are Wrong\n- B relies on targeted applies which can be brittle and do not guarantee ordering within a single plan.\n- C cannot establish a reliable dependency since the script is a separate provisioner and not referenced directly.\n- D introduces complexity and does not guarantee ordering across workspaces.\n\n## Key Concepts\n- Depends_on and resource ordering\n- Provisioners (local-exec) caveats\n- Implicit vs explicit dependencies\n\n## Real-World Application\nEnsures bootstrap steps run only after required infrastructure components exist, preventing failures during bootstrap.","diagram":null,"difficulty":"intermediate","tags":["Terraform","AWS","EC2","Provisioners","Local-exec","certification-mcq","domain-weight-15"],"channel":"terraform-associate","subChannel":"iac-concepts","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T04:51:59.468Z","createdAt":"2026-01-12 04:51:59"},{"id":"terraform-associate-terraform-basics-1768155950528-0","question":"In a team environment, you need to ensure safe concurrent Terraform operations across multiple developers and CI runs. Which backend configuration best prevents conflicting state writes?","answer":"[{\"id\":\"a\",\"text\":\"Use the local backend so state is kept in the repository alongside configuration\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use a remote backend with S3 as the state store and DynamoDB for state locking\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Use a remote backend with Consul as storage but without a locking mechanism\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use Terraform Cloud as the backend with default locking behavior\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct answer is B. Using a remote backend with S3 and DynamoDB enables persistent state storage and locking, which prevents concurrent writes from racing each other in multiple runs. \n\n## Why Other Options Are Wrong\n- A: Local backend stores state on disk and is not suitable for collaboration or locking. \n- C: Consul backend exists but does not provide DynamoDB-style locking, which undermines concurrent safety. \n- D: Terraform Cloud is a valid remote backend option but the question specifies a classic AWS-backed setup with explicit locking via DynamoDB; Terraform Cloud is a different service and may introduce different operational considerations. \n\n## Key Concepts\n- Remote state with locking\n- Backends (S3) and locking mechanisms (DynamoDB)\n\n## Real-World Application\n- When multiple engineers and CI pipelines apply changes, this pattern prevents corrupt state by ensuring only one operation can write at a time.","diagram":null,"difficulty":"intermediate","tags":["Terraform","AWS","S3","DynamoDB","Backends","CI/CD","certification-mcq","domain-weight-20"],"channel":"terraform-associate","subChannel":"terraform-basics","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T18:25:50.531Z","createdAt":"2026-01-11 18:25:50"},{"id":"terraform-associate-terraform-basics-1768155950528-1","question":"You have a child module that outputs a database password along with the database endpoint. To avoid leaking the password in CLI output and logs, which approach should you take?","answer":"[{\"id\":\"a\",\"text\":\"Expose the password directly via a standard output value\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Mark the password output as sensitive = true\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Store the password in the repository so it is versioned\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use a separate data source to fetch the password at apply time\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct answer is B. Marking the output as sensitive hides the value from default CLI displays and plan output, reducing the risk of accidentally leaking secrets. \n\n## Why Other Options Are Wrong\n- A: Exposing the password defeats purpose of masking and is insecure. \n- C: Storing passwords in the repository risks credential leakage and accidental exposure. \n- D: A data source cannot inherently mask the password when surfaced as an output and adds unnecessary complexity; sensitive outputs are the standard approach for secret values output by modules. \n\n## Key Concepts\n- Outputs can be marked sensitive to prevent display\n- Secrets should not be logged or displayed by default\n\n## Real-World Application\n- In CI pipelines, marking outputs as sensitive ensures secrets like DB passwords are not printed in logs while still allowing dependent modules to consume the value programmatically.","diagram":null,"difficulty":"intermediate","tags":["Terraform","Security","Secrets","AWS","RDS","certification-mcq","domain-weight-20"],"channel":"terraform-associate","subChannel":"terraform-basics","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T18:25:50.895Z","createdAt":"2026-01-11 18:25:51"},{"id":"terraform-associate-terraform-basics-1768155950528-2","question":"You manage two Kubernetes clusters (prod and dev) on AWS EKS and want to manage resources in both clusters from a single Terraform configuration. Which approach is most appropriate?","answer":"[{\"id\":\"a\",\"text\":\"Create two entirely separate Terraform configurations for the two clusters\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Define multiple Kubernetes providers with different aliases and specify provider = kubernetes.prod or kubernetes.dev in resources\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Use a single Kubernetes provider and dynamically switch the cluster using a runtime variable in each resource\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use the same provider for both clusters and rely on separate namespaces to segregate resources\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct answer is B. Terraform supports multiple provider configurations with aliases. You can define separate Kubernetes providers for prod and dev clusters and assign them to resources using provider = kubernetes.prod or provider = kubernetes.dev. \n\n```javascript\nprovider \"kubernetes\" {\n  alias = \"prod\"\n  host = var.prod_host\n  client_certificate = var.prod_cert\n  token = var.prod_token\n}\nprovider \"kubernetes\" {\n  alias = \"dev\"\n  host = var.dev_host\n  client_certificate = var.dev_cert\n  token = var.dev_token\n}\nresource \"kubernetes_namespace\" \"prod_ns\" {\n  provider = kubernetes.prod\n  metadata { name = \"production\" }\n}\nresource \"kubernetes_namespace\" \"dev_ns\" {\n  provider = kubernetes.dev\n  metadata { name = \"development\" }\n}\n```\n\n## Why Other Options Are Wrong\n- A: Two separate configurations are valid but increases maintenance and duplication; a single config with aliases is more scalable. \n- C: A single provider cannot target multiple clusters simultaneously without prefixes/aliases. \n- D: Namespaces do not provide isolation at the provider level and wonâ€™t allow per-cluster API interactions. \n\n## Key Concepts\n- Provider aliases for multi-cluster management\n- Resource-level provider assignment\n\n## Real-World Application\n- Simplifies multi-cluster governance by centralizing configuration while keeping prod/dev isolated.","diagram":null,"difficulty":"intermediate","tags":["Terraform","Kubernetes","EKS","AWS","Provider Aliases","certification-mcq","domain-weight-20"],"channel":"terraform-associate","subChannel":"terraform-basics","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T18:25:51.250Z","createdAt":"2026-01-11 18:25:51"},{"id":"terraform-associate-terraform-state-1768224155868-0","question":"Which backend attribute enables concurrency control for Terraform remote state in an S3 backend?","answer":"[{\"id\":\"a\",\"text\":\"region\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"dynamodb_table\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"bucket\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"encrypt\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct. The dynamodb_table attribute configures a DynamoDB table to manage state locks, enabling concurrency control for remote state.\n\n## Why Other Options Are Wrong\n- A: region specifies the AWS region for the backend but does not provide locking.\n- C: bucket only defines the S3 bucket location, not locking behavior.\n- D: encrypt controls at-rest encryption, not locking.\n\n## Key Concepts\n- Remote state locking with DynamoDB\n- Backend configuration for S3\n\n## Real-World Application\n- In teams, enabling DynamoDB locking prevents conflicting Terraform runs on the same state.","diagram":null,"difficulty":"intermediate","tags":["Terraform","State","AWS","S3","DynamoDB","Remote State","certification-mcq","domain-weight-15"],"channel":"terraform-associate","subChannel":"terraform-state","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:22:35.870Z","createdAt":"2026-01-12 13:22:36"},{"id":"terraform-associate-terraform-state-1768224155868-1","question":"You are migrating a local Terraform state to a remote backend. Which command best migrates the local state to the new backend?","answer":"[{\"id\":\"a\",\"text\":\"terraform init -migrate-state\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"terraform init\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"terraform apply\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"terraform state push\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A is correct. Terraform init -migrate-state migrates an existing local state to the newly configured remote backend.\n\n## Why Other Options Are Wrong\n- B: terraform init initializes backends but does not migrate local state by itself.\n- C: terraform apply executes changes, not backend migration.\n- D: terraform state push is not the standard migration flow for moving local state to a backend and is deprecated in newer workflows.\n\n## Key Concepts\n- Migrating local to remote state\n- Backend initialization\n\n## Real-World Application\n- When reconfiguring to a remote backend after initial development.","diagram":null,"difficulty":"intermediate","tags":["Terraform","State","Backend","Migration","certification-mcq","domain-weight-15"],"channel":"terraform-associate","subChannel":"terraform-state","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:22:36.410Z","createdAt":"2026-01-12 13:22:36"},{"id":"terraform-associate-terraform-state-1768224155868-2","question":"A remote state stored in S3 becomes corrupted due to a crash during a write. What is the safest way to recover without affecting live resources?","answer":"[{\"id\":\"a\",\"text\":\"Restore the state file from a previous version in S3\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Run terraform state rm on all resources\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Manually edit the JSON state file\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Run terraform apply to re-create resources\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A is correct. If S3 versioning is enabled, restore a previous good state version to recover without modifying live infrastructure.\n\n## Why Other Options Are Wrong\n- B: Removing resources from state can orphan real resources and lose track.\n- C: Manually editing the state file is error-prone and unsafe.\n- D: Applying may fail or cause conflicts if the state is corrupted.\n\n## Key Concepts\n- State corruption recovery\n- S3 versioning and backup\n\n## Real-World Application\n- Regularly back up and version Terraform state to recover from corruption quickly.","diagram":null,"difficulty":"intermediate","tags":["Terraform","State","AWS","S3","Backups","certification-mcq","domain-weight-15"],"channel":"terraform-associate","subChannel":"terraform-state","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:22:36.928Z","createdAt":"2026-01-12 13:22:37"},{"id":"terraform-associate-terraform-state-1768224155868-3","question":"To manage an existing AWS VPC that was created outside Terraform, which action brings the resource into Terraform state?","answer":"[{\"id\":\"a\",\"text\":\"terraform import aws_vpc.example_vpc vpc-123456\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"terraform apply\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"terraform refresh\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"terraform state list\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A is correct. terraform import associates the existing AWS VPC resource with a Terraform resource in the state, enabling management.\n\n## Why Other Options Are Wrong\n- B: apply would attempt to create/modify resources per configuration, not import.\n- C: refresh is deprecated and does not import resources into state.\n- D: state list only lists resources known to the state; it does not perform import.\n\n## Key Concepts\n- Importing existing resources into state\n- Managing externally created infrastructure\n\n## Real-World Application\n- Onboarding existing environments into Terraform management.","diagram":null,"difficulty":"intermediate","tags":["Terraform","State","AWS","Import","certification-mcq","domain-weight-15"],"channel":"terraform-associate","subChannel":"terraform-state","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:22:37.108Z","createdAt":"2026-01-12 13:22:37"},{"id":"terraform-associate-terraform-state-1768224155868-4","question":"If a DynamoDB lock prevents another apply due to an unclean shutdown, which command forcibly unlocks the state lock when you are sure no one else is applying?","answer":"[{\"id\":\"a\",\"text\":\"terraform force-unlock LOCK_ID\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"terraform unlock\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"terraform state unlock\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"terraform apply -force-lock\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A is correct. terraform force-unlock LOCK_ID releases the lock forcibly when you are certain no other process is running.\n\n## Why Other Options Are Wrong\n- B: There is no terraform unlock command.\n- C: No such subcommand; state unlock does not exist.\n- D: There is no apply flag -force-lock; locking is managed by DynamoDB.\n\n## Key Concepts\n- Force unlocking remote state\n- Lock management with DynamoDB\n\n## Real-World Application\n- Resolving stalled Terraform runs after outages without impacting live resources.","diagram":null,"difficulty":"intermediate","tags":["Terraform","State","AWS","DynamoDB","Locking","certification-mcq","domain-weight-15"],"channel":"terraform-associate","subChannel":"terraform-state","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:22:37.290Z","createdAt":"2026-01-12 13:22:37"}],"subChannels":["general","iac-concepts","terraform-basics","terraform-state"],"companies":["Snap","Snowflake","Square"],"stats":{"total":12,"beginner":0,"intermediate":12,"advanced":0,"newThisWeek":12}}