{"questions":[{"id":"azure-administrator-deploy-manage-compute-1768199787880-0","question":"You manage a web application running on a VM scale set behind an internal load balancer. You need to deploy a new OS image update with zero downtime and ensure upgrades are rolled across instances without taking all instances offline. Which VM scale set configuration best supports this requirement?","answer":"[{\"id\":\"a\",\"text\":\"Upgrade policy: Manual mode with manual upgrades\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Upgrade policy: Automatic mode with rolling upgrades\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Use an Availability Set to distribute instances\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Deploy to a single zone without zone redundancy\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct. VM scale sets with upgradePolicy mode set to Automatic enable the platform to roll out updates gradually across instances, reducing downtime and preserving availability during image upgrades. \n\n## Why Other Options Are Wrong\n- Option A: Manual upgrades require explicit intervention and can lead to downtime if not carefully coordinated. \n- Option C: An Availability Set distributes across fault and update domains but does not provide built-in rolling upgrades for OS image changes. \n- Option D: A single-zone deployment eliminates zone redundancy and does not support safe rolling upgrades, increasing risk during maintenance. \n\n## Key Concepts\n- VM Scale Sets support upgradePolicy with mode Automatic or Manual \n- Rolling upgrades minimize downtime during OS image updates \n- Automatic upgrades coordinate across instances to maintain availability\n\n## Real-World Application\n- Use this pattern when you need to maintain high availability during OS updates across a fleet of VMs behind a load balancer.","diagram":null,"difficulty":"intermediate","tags":["Azure","VMSS","AZ-104","Terraform","Kubernetes","AWS","certification-mcq","domain-weight-20"],"channel":"azure-administrator","subChannel":"deploy-manage-compute","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T06:36:27.881Z","createdAt":"2026-01-12 06:36:28"},{"id":"azure-administrator-deploy-manage-compute-1768199787880-1","question":"You need to automate post-provision configuration on each VM instance in a VM scale set, such as installing agents and applying a baseline configuration. Which Azure extension is designed to run a script on every VM after provisioning?","answer":"[{\"id\":\"a\",\"text\":\"Custom Script Extension\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Microsoft Monitoring Agent\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"VM Access Extension\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Desired State Configuration (DSC) Extension\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A is correct. The Custom Script Extension runs a script on each VM in the scale set after provisioning to install software or apply configuration. \n\n## Why Other Options Are Wrong\n- Option B: Microsoft Monitoring Agent collects telemetry, not intended for post-provision configuration. \n- Option C: VM Access Extension provides remote access, not mass configuration. \n- Option D: DSC Extension can enforce configuration state but is more complex; the question asks for a script-based post-provision run, which the Custom Script Extension is designed for. \n\n## Key Concepts\n- Custom Script Extension enables agentless, script-based provisioning across VMSS\n- Extensions are the primary mechanism for post-provision configuration on Azure VMs\n\n## Real-World Application\n- Use this for rapid baseline setup across all instances (e.g., install monitoring, agents, and initial configuration) after VMSS provisioning.","diagram":null,"difficulty":"intermediate","tags":["Azure","VMSS","AZ-104","Terraform","Kubernetes","AWS","certification-mcq","domain-weight-20"],"channel":"azure-administrator","subChannel":"deploy-manage-compute","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T06:36:28.401Z","createdAt":"2026-01-12 06:36:28"},{"id":"azure-administrator-deploy-manage-compute-1768199787880-2","question":"To protect a VM-based backend against data-center (zone) outages in a region that supports Availability Zones, which deployment option should you choose?","answer":"[{\"id\":\"a\",\"text\":\"Deploy a single VM in one zone behind an internal load balancer\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use an Availability Set to distribute across fault and update domains\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use a VM Scale Set configured for zone-redundant deployment (Availability Zones)\",\"isCorrect\":true},{\"id\":\"d\",\"text\":\"Deploy to App Service with regional replication\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption C is correct. Zone-redundant VM Scale Sets distribute instances across Availability Zones, providing resilience against a single zone outage. \n\n## Why Other Options Are Wrong\n- Option A: A single VM in one zone does not provide zone resilience. \n- Option B: Availability Sets protect within a data center but not across zones. \n- Option D: App Service is not a VM-based compute resource and does not directly cover VM-scale redundancy in this context. \n\n## Key Concepts\n- Availability Zones provide zone-level failure isolation \n- VM Scale Sets can be configured for zone-redundant deployment\n\n## Real-World Application\n- Use this approach for mission-critical workloads requiring high availability across data-center outages.","diagram":null,"difficulty":"intermediate","tags":["Azure","VMSS","AZ-104","Terraform","Kubernetes","AWS","certification-mcq","domain-weight-20"],"channel":"azure-administrator","subChannel":"deploy-manage-compute","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T06:36:28.921Z","createdAt":"2026-01-12 06:36:29"},{"id":"azure-administrator-deploy-manage-resources-1768286305260-0","question":"You plan to deploy a Windows VM from an image and need to execute a setup script immediately after provisioning. Which mechanism should you use to run the script during provisioning?","answer":"[{\"id\":\"a\",\"text\":\"Run Command after provisioning\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Custom Script Extension\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Desired State Configuration extension\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Azure Automation DSC\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct. The Custom Script Extension runs a script as part of the VM provisioning process (Windows or Linux) and is the standard approach for bootstrap tasks. Run Command can execute a script after provisioning but is not tied to the initial provisioning flow. The DSC extension and Azure Automation DSC are primarily for desired state configuration and more complex management scenarios than a simple bootstrap script.\n\n## Why Other Options Are Wrong\n- Option A: Run Command can run commands after provisioning but isn't automatically invoked during initial provisioning and may require orchestration.\n- Option C: DSC extension is intended for desired state configuration, not a one-off bootstrap script.\n- Option D: DSC (Azure Automation DSC) serves similar purposes but adds unnecessary complexity for a simple provisioning task.\n\n## Key Concepts\n- VM extensions enable post-provision configuration; Custom Script Extension runs scripts during provisioning.\n- Idempotent provisioning ensures predictable state across deployments.\n- DSC is for declarative configuration, which is heavier for bootstrap tasks.\n\n## Real-World Application\n- In CI/CD pipelines, use the Custom Script Extension to bootstrap software after VM creation, ensuring consistent environments across all environments.","diagram":null,"difficulty":"intermediate","tags":["AZ-104","Azure","AKS","Terraform","EC2","S3","Kubernetes","TrafficManager","FrontDoor","certification-mcq","domain-weight-10"],"channel":"azure-administrator","subChannel":"deploy-manage-resources","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T06:38:25.261Z","createdAt":"2026-01-13 06:38:25"},{"id":"azure-administrator-deploy-manage-resources-1768286305260-1","question":"You have a web app deployed in two regions and need to route user requests to the nearest healthy regional endpoint with automatic failover. Which Azure service is most appropriate for this requirement?","answer":"[{\"id\":\"a\",\"text\":\"Application Gateway\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Azure Traffic Manager\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Azure Front Door\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Load Balancer\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct. Azure Traffic Manager provides global DNS-based routing to endpoints in multiple regions and can direct users to the nearest healthy endpoint using the Performance routing method. Application Gateway and Load Balancer are regional services, and Front Door is a global delivery service but primarily used for edge routing with features like WAF; for nearest-endpoint routing across regions, Traffic Manager is the appropriate choice.\n\n## Why Other Options Are Wrong\n- Option A: Application Gateway is regional and does not route traffic across regions.\n- Option C: Front Door offers global delivery but routing to the nearest region is not its primary mode; it focuses on web application delivery and edge features.\n- Option D: Load Balancer is regional and cannot perform global cross-region routing.\n\n## Key Concepts\n- DNS-based global traffic management with Traffic Manager.\n- Performance routing uses latency to select endpoints.\n\n## Real-World Application\n- Use Traffic Manager to gracefully distribute traffic for a multi-region deployment and achieve rapid failover without client-side changes.","diagram":null,"difficulty":"intermediate","tags":["AZ-104","Azure","AKS","Terraform","EC2","S3","Kubernetes","TrafficManager","FrontDoor","certification-mcq","domain-weight-10"],"channel":"azure-administrator","subChannel":"deploy-manage-resources","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T06:38:25.607Z","createdAt":"2026-01-13 06:38:25"},{"id":"azure-administrator-deploy-manage-resources-1768286305260-2","question":"To automatically enforce a required tag on all resources across a subscription and apply a default value when the tag is missing, which Azure feature and configuration should you use?","answer":"[{\"id\":\"a\",\"text\":\"Azure Policy with Deny effect\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Azure Policy with AuditIfNotExists\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Azure Policy with Append effect\",\"isCorrect\":true},{\"id\":\"d\",\"text\":\"Azure Blueprint with pre-assignments\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption C is correct. Azure Policy can use the Append effect to automatically add a missing tag (or tag value) to resources during creation or update, enforcing a default tag across the subscription without blocking deployment. Deny would block non-compliant resources, AuditOnly would only log non-compliance, and Blueprints are guidance for environment provisioning rather than automatic tag enrichment.\n\n## Why Other Options Are Wrong\n- Option A: Deny blocks non-compliant resources instead of automatically adding the tag.\n- Option B: AuditIfNotExists logs non-compliance but does not automatically apply tags.\n- Option D: Blueprints describe resource configurations but do not automatically modify resources to add missing tags.\n\n## Key Concepts\n- Azure Policy Append effect for automatic tagging.\n- Policy-based governance for tag standards.\n\n## Real-World Application\n- Enforce consistent cost allocation and reporting by auto-tagging new resources with Department and CostCenter values.","diagram":null,"difficulty":"intermediate","tags":["AZ-104","Azure","AKS","Terraform","EC2","S3","Kubernetes","TrafficManager","FrontDoor","certification-mcq","domain-weight-10"],"channel":"azure-administrator","subChannel":"deploy-manage-resources","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T06:38:25.947Z","createdAt":"2026-01-13 06:38:25"},{"id":"azure-administrator-deploy-manage-resources-1768286305260-3","question":"You manage an Azure SQL Database fleet across two regions and want seamless automatic failover for the entire group of databases. Which feature provides this capability?","answer":"[{\"id\":\"a\",\"text\":\"Active Geo-Replication\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Auto-Failover Groups\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Geo-Restore\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Failover Cluster Instance\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct. Auto-Failover Groups allow automatic failover of a group of databases to a paired region, providing coordinated failover for multiple databases. Active Geo-Replication is per-database and typically requires manual failover coordination. Geo-Restore is a point-in-time restore capability, and Failover Cluster Instance applies to on-prem or VM-based SQL Server scenarios.\n\n## Why Other Options Are Wrong\n- Option A: Active Geo-Replication is per-database and usually requires manual failover orchestration for multiple databases.\n- Option C: Geo-Restore performs point-in-time restore and does not provide automatic failover for a database group.\n- Option D: Failover Cluster Instance targets on-premises or VM-based SQL Server configurations, not managed SQL Database failover groups.\n\n## Key Concepts\n- Auto-Failover Groups for coordinated, automatic cross-region failover.\n- Differences between per-database replication and multi-database group failover.\n\n## Real-World Application\n- Use auto-failover groups to minimize downtime for mission-critical dashboards spanning regions.","diagram":null,"difficulty":"intermediate","tags":["AZ-104","Azure","AKS","Terraform","EC2","S3","Kubernetes","TrafficManager","FrontDoor","certification-mcq","domain-weight-10"],"channel":"azure-administrator","subChannel":"deploy-manage-resources","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T06:38:26.073Z","createdAt":"2026-01-13 06:38:26"},{"id":"azure-administrator-deploy-manage-resources-1768286305260-4","question":"A virtual machine requires access to a Key Vault to retrieve a secret at runtime without storing credentials in code. Which mechanism provides this securely and without credential management in the app?","answer":"[{\"id\":\"a\",\"text\":\"Service principal with client secret\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"SAS token\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Managed identity (system-assigned or user-assigned)\",\"isCorrect\":true},{\"id\":\"d\",\"text\":\"Access keys for Key Vault\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption C is correct. Using a managed identity, the VM can obtain tokens from Azure AD to access Key Vault securely without embedding credentials in code. Service principals and client secrets require credential management, SAS tokens are for storage services, and Key Vault access keys do not apply as a credential mechanism for app code.\n\n## Why Other Options Are Wrong\n- Option A: Service principals with client secrets require credential rotation and secure storage, adding risk.\n- Option B: SAS tokens are not used for Key Vault access control.\n- Option D: Key Vault does not expose access keys in the way storage accounts do.\n\n## Key Concepts\n- Managed identities for Azure resources.\n- Least-privilege access to Key Vault.\n\n## Real-World Application\n- Enable a VM or app to fetch database connection strings or API keys from Key Vault at runtime without hard-coded secrets.","diagram":null,"difficulty":"intermediate","tags":["AZ-104","Azure","AKS","Terraform","EC2","S3","Kubernetes","TrafficManager","FrontDoor","certification-mcq","domain-weight-10"],"channel":"azure-administrator","subChannel":"deploy-manage-resources","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T06:38:26.202Z","createdAt":"2026-01-13 06:38:26"},{"id":"q-1036","question":"Scenario: A new external vendor needs read-only access to a single Azure AD-secured web portal (SaaS app) via B2B. Create a guest user, assign them to a dedicated App Role of the portal, enforce MFA and device-compliant access with a Conditional Access policy restricted to the office IP range, and outline post-grant validation and rollback?","answer":"Invite the vendor via B2B guest, assign them to the portal's App Role, and enforce a Conditional Access policy that requires MFA and restricts sign-ins to the office IP range for compliant devices. Va","explanation":"## Why This Is Asked\nThis question tests practical knowledge of onboarding external collaborators using Azure AD B2B, scoping permissions with App Roles, and enforcing security with Conditional Access (MFA, IP restrictions, device compliance). It also examines validation and rollback discipline.\n\n## Key Concepts\n- Azure AD B2B guest users\n- App Roles in App Registration\n- Conditional Access with MFA and location-based controls\n- Device compliance (Intune)\n- Validation and rollback procedures\n\n## Code Example\n```powershell\n# Placeholder example for App Role assignment (requires Graph)\n# Add-AzureADUserAppRoleAssignment -ObjectId $guest.ObjectId -ResourceId $portalAppId -RoleId $appRoleId\n```\n\n## Follow-up Questions\n- How would you monitor CA policy impact and audit logs?\n- What are risks with IP restrictions for remote workers?","diagram":"flowchart TD\n  A[Guest invited] --> B[Assign App Role]\n  B --> C[CA: MFA + Office IP + Compliant device]\n  C --> D[Access Portal]\n  D --> E[Validation]\n  E --> F[Rollback]","difficulty":"beginner","tags":["azure-administrator"],"channel":"azure-administrator","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","NVIDIA","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T20:24:25.046Z","createdAt":"2026-01-12T20:24:25.046Z"},{"id":"q-1109","question":"How would you enable Self-Service Password Reset (SSPR) for a 20-user Azure AD group, enforce MFA during resets, and ensure auditable reset events with a simple rollback plan? Include licensing notes, enrollment flow, verification steps, and failure handling?","answer":"Enable SSPR for the target group in Entra ID, require MFA during password resets, and limit methods to authenticator app and phone. Mandate users to register at first sign-in, enable audit logs and al","explanation":"## Why This Is Asked\nSSPR is a common, beginner-friendly way to offload password management while enforcing security.\n\n## Key Concepts\n- Entra ID SSPR scope and licensing\n- MFA enforcement during resets\n- Authentication methods and registration flows\n- Audit logs and alerts for reset events\n- Safe rollback and admin overrides\n\n## Code Example\n```powershell\n# Placeholder: enable SSPR policy for a group (commands vary by module)\n# Example only; use Graph/PowerShell modules in production\n```\n\n## Follow-up Questions\n- What are trade-offs of broad SSPR rollout vs scoped groups?\n- How would you test SSPR in a non-prod tenant without affecting users?","diagram":null,"difficulty":"beginner","tags":["azure-administrator"],"channel":"azure-administrator","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T23:18:00.473Z","createdAt":"2026-01-12T23:18:00.473Z"},{"id":"q-1191","question":"Scenario: A data platform CI/CD pipeline deploys to Data Lake Gen2, Synapse, and a storage account across three subscriptions using a non-interactive service principal. Design a secure, rotating, auditable auth model: App Registration with certificate-based OAuth, Key Vault-stored certs rotated every 30 days, per-resource RBAC with least privilege, automatic revocation on build failure, and validation steps. Include how you would test access during runs and how to roll back changes if needed?","answer":"Use a dedicated App Registration bound to a certificate-based service principal stored in Key Vault; rotate certificates monthly. Grant across-subscription RBAC: Data Lake Gen2 Data Contributor, Synap","explanation":"## Why This Is Asked\nTests ability to design secure, auditable cross-subscription access for automated pipelines, emphasizing certificate-based auth, secret rotation, and least privilege.\n\n## Key Concepts\n- Certificate-based service principals and Key Vault storage/rotation\n- Cross-subscription RBAC with least privilege per resource\n- Auditing via Azure AD and Key Vault logs\n- Validation through token issuance and dry-run deployments\n- Rollback: revoke credentials and remove RBAC\n\n## Code Example\n```bash\n# Pseudo-Azure CLI: create app, cert, and assign roles (illustrative)\naz ad app create --display-name ci-cd-pipeline\n# create & upload cert to Key Vault, configure SPN to use cert\n# assign roles in each subscription\n# set up rotation schedule to rotate cert every 30 days\n```\n\n## Follow-up Questions\n- How would you monitor for orphaned service principals?\n- How would you handle rotation failures and ensure no pipeline downtime?","diagram":"flowchart TD\nA(App Registration) --> B(Certificate in Key Vault)\nA --> C(Service Principal across 3 subscriptions)\nB --> D(Certificate Rotation: 30 days)\nC --> E(RBAC per resource: Data Lake, Synapse, Blob)\nE --> F(Token Issuance)\nF --> G(Dry-run verification)\nG --> H(Audit Logs & Rollback if fail)","difficulty":"advanced","tags":["azure-administrator"],"channel":"azure-administrator","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Google","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T04:41:48.755Z","createdAt":"2026-01-13T04:41:48.755Z"},{"id":"q-1304","question":"Scenario: a startup with 5–15 users is moving to Azure AD for the first time. You need to enable basic identity services: (a) create users and groups, (b) provide SSO for a SaaS app using SAML, (c) enforce MFA, (d) automatically assign Office 365 licenses via group membership, and (e) enable self-service password reset for all users. Outline an end-to-end setup plan and a minimal validation checklist prior to go-live?","answer":"Create an Azure AD tenant with a simple user/group model, add a test user and a test licensing group, configure SAML-based SSO for the SaaS app via Enterprise Applications, enforce MFA with a basic Co","explanation":"## Why This Is Asked\n\nTests translating onboarding needs into concrete Azure AD setup using beginner-friendly, tangible steps.\n\n## Key Concepts\n\n- Azure AD tenant bootstrapping\n- SAML-based SSO via Enterprise Applications\n- Conditional Access for MFA\n- Group-based license assignment\n- Self-Service Password Reset (SSPR)\n\n## Code Example\n\n```powershell\n# Note: Requires AzureAD/MS Graph modules\nConnect-AzureAD\nNew-AzureADUser -UserPrincipalName testuser@contoso.com -DisplayName \"Test User\" -PasswordProfile (New-Object -TypeName Microsoft.Open.AzureAD.Model.PasswordProfile -Property @{Password=\"P@ssw0rd!\"; ForceChangePasswordNextLogin=$true})\n$grp = New-AzureADGroup -DisplayName \"TestLicGroup\" -MailEnabled $false -SecurityEnabled $true\nAdd-AzureADGroupMember -ObjectId $grp.ObjectId -RefObjectId (Get-AzureADUser -ObjectId testuser@contoso.com).ObjectId\n# License assignment would be via Graph; placeholder for structure\n```\n\n## Follow-up Questions\n\n- How would you verify license propagation after assignment?\n- How would you handle MFA device enrollment for new users during onboarding?","diagram":"flowchart TD\n  Start(Start) --> Create[Create users & groups]\n  Create --> SSO[Configure SSO for SaaS app (SAML)]\n  SSO --> MFA[Enable MFA via CA]\n  MFA --> Lic[License by group]\n  Lic --> SSPR[Enable SSPR]\n  SSPR --> Validate[Validation checklist]","difficulty":"beginner","tags":["azure-administrator"],"channel":"azure-administrator","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","IBM","Lyft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T08:51:22.267Z","createdAt":"2026-01-13T08:51:22.267Z"},{"id":"q-855","question":"Your organization needs a temporary access workflow: grant a contractor read/write access to a single Blob Storage container for 30 days using Azure AD groups and RBAC, and automatically revoke access at day 30. How would you implement this end-to-end?","answer":"Create an Azure AD group for the contractor, grant that group the Storage Blob Data Contributor role scoped to the container, and configure expiry via Privileged Identity Management or an Access Packa","explanation":"## Why This Is Asked\n\nTests practical understanding of identity-led access control in Azure, focusing on provisioning, scope, and automated revocation for temporary access.\n\n## Key Concepts\n\n- Azure AD groups and role assignments at container scope\n- Built-in Storage Blob Data Contributor role\n- Expiry mechanisms: Privileged Identity Management (PIM) or Access Packages\n- Automated revocation and auditing of access\n\n## Code Example\n\n```javascript\n// PIM/Access Package configuration would be defined via Azure Portal or CLI scripts\n```\n\n## Follow-up Questions\n\n- How would you monitor and alert on expiring access?\n- What are potential pitfall if the container has shared access signatures (SAS)?","diagram":"flowchart TD\n  A[Start] --> B[Create AAD group for contractor]\n  B --> C[Assign Storage Blob Data Contributor role to container]\n  C --> D[Configure expiry (30 days) via PIM or Access Package]\n  D --> E[Automatic revocation on expiry]\n  E --> F[Audit and verify access]","difficulty":"beginner","tags":["azure-administrator"],"channel":"azure-administrator","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","IBM","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:37:57.417Z","createdAt":"2026-01-12T13:37:57.417Z"},{"id":"q-897","question":"Scenario: A contractor needs access to a single Azure AD‑secured app (CI/CD portal) for 14 days. Outline an end‑to‑end approach using a dedicated Azure AD security group, app RBAC, and an Access Review to auto‑revoke access at day 14. Include how you would validate access during the window and after expiry, with minimal automation and no custom scripts?","answer":"Create a dedicated security group for the app, add the contractor, assign the app RBAC role (e.g., App Contributor) to that group scoped to the app, and configure an access review to revoke on expiry ","explanation":"## Why This Is Asked\nTests practical use of Azure AD basics to enforce time‑bound access with governance, a foundational skill for any Azure Administrator.\n\n## Key Concepts\n- Azure AD Security Groups\n- Resource/app RBAC assignment\n- Access Reviews for expiry\n- Audit logs and sign‑in validation\n\n## Code Example\n```javascript\n// Azure CLI snippets (replace IDs with real values)\naz ad group create --display-name AppTempAccess --mail-nickname AppTempAccess\naz ad group member add --group AppTempAccess --member-id <contractor-id>\naz role assignment create --assignee <group-id> --role \"Contributor\" --scope \"/subscriptions/xxx/resourceGroups/rg-prod/providers/Microsoft.Web/sites/ci-cd-portal\"\n```\n\n## Follow-up Questions\n- Which granular role would you assign and why?\n- How would you test expiry scenarios if the contractor reappears later?\n","diagram":"flowchart TD\n  Q[Question] --> G[Create App Access Group]\n  G --> A[Assign App RBAC to Group]\n  A --> R[Configure Access Review for 14 days]\n  R --> V[Validate Grant and Revocation in logs]","difficulty":"beginner","tags":["azure-administrator"],"channel":"azure-administrator","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Twitter","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T14:37:09.020Z","createdAt":"2026-01-12T14:37:09.020Z"},{"id":"q-929","question":"Scenario: A multinational bank needs external data scientists to access multiple Azure resources (Data Lake Gen2, Synapse, and a storage account) for a 6-week analytics project. Propose an end-to-end access model using Azure AD Entitlement Management, Access Reviews, B2B collaboration, and Privileged Identity Management (PIM) for Just-In-Time role activations, with cross-subscription considerations, least privilege, and auditability. Include how you enforce MFA, token lifetimes, revocation at end, and validation steps?","answer":"Use Azure AD Entitlement Management to grant a 6-week access package for Data Lake Gen2, Synapse, and a storage account to external collaborators via B2B guests; enforce least privilege with scoped RB","explanation":"## Why This Is Asked\nThis question probes experience with cross-tenant, time-bound access using Entitlement Management, B2B, PIM, and cross-subscription governance, plus observability.\n\n## Key Concepts\n- Entitlement Management and Access Packages\n- B2B guest access and conditional access\n- Privileged Identity Management (PIM) for JIT\n- Least-privilege RBAC across subscriptions\n- MFA, token lifetimes, and session controls\n- Access Reviews and automatic revocation\n- Auditing and telemetry\n\n## Code Example\n```javascript\n// Conceptual: temporary role activation (illustrative)\nconst request = {\n  assignee: \"<guest-object-id>\",\n  role: \"Storage Blob Data Contributor\",\n  scope: \"/subscriptions/{sub}/ResourceGroups/{rg}/providers/Microsoft.Storage/storageAccounts/{sa}\",\n  duration: \"PT6W\"\n};\n```\n\n## Follow-up Questions\n- How would you handle revocation if the vendor misses a milestone?\n- What telemetry signals would indicate misuse during the window?","diagram":"flowchart TD\n  A[Request] --> B[Assign Entitlement Package (6w)]\n  B --> C[PIM JIT Activation + MFA]\n  C --> D[Scoped RBAC across Data Lake, Synapse, Storage]\n  D --> E[Access Review @ week 6]\n  E --> F[Automatic Revocation & Audit]","difficulty":"advanced","tags":["azure-administrator"],"channel":"azure-administrator","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Netflix","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T15:39:39.688Z","createdAt":"2026-01-12T15:39:39.688Z"},{"id":"q-942","question":"Scenario: A vendor must run nightly ingestion pipelines against Data Lake Gen2, a storage account, and Synapse in a shared Azure AD tenant for 10 days using a single service principal. Propose an end-to-end access model using an App Registration with scoped RBAC, Just-In-Time activation (PIM) for the service principal, Entitlement Management or Access Reviews, and automatic revocation at expiry. Include how you would validate access during the window and after expiry with minimal automation and no custom scripts?","answer":"Use a dedicated App Registration to obtain a service principal with scoped RBAC: Data Lake Gen2 read, Storage blob data reader, and Synapse workspace user scope. Enable Just‑In‑Time (PIM) activation f","explanation":"## Why This Is Asked\nThis question tests practical design of ephemeral service-principal access across multiple resources with governance and auditing.\n\n## Key Concepts\n- App Registration and service principals with scoped RBAC\n- Just-In-Time activation (PIM) for service principals\n- Entitlement Management and Access Reviews for the app\n- Conditional Access MFA and device compliance\n\n## Code Example\n\n```bash\n# Example policy snippet (non-executable)\n```\n\n## Follow-up Questions\n- How would you handle token lifetimes and rotation for the SP?\n- How would you monitor and validate revocation after expiry?","diagram":"flowchart TD\n  A[Vendor needs access] --> B[App Registration SP]\n  B --> C{PIM Activation}\n  C --> D[Time-limited access]\n  D --> E[Access Validation]\n  E --> F[Expiry Revocation]","difficulty":"advanced","tags":["azure-administrator"],"channel":"azure-administrator","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T16:32:08.373Z","createdAt":"2026-01-12T16:32:08.373Z"},{"id":"azure-administrator-implement-networking-1768228340275-0","question":"You have two VNets in different Azure regions and want private connectivity with minimal latency for an application that requires cross-region data sharing. Which connection method should you use?","answer":"[{\"id\":\"a\",\"text\":\"VNet peering within the same region\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Global VNet Peering\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"VPN Gateway (VNet-to-VNet) over the public internet\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"ExpressRoute private peering to on-prem\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct answer is Option B: Global VNet Peering.\n\nGlobal VNet Peering provides private, low-latency connectivity between VNets across regions using the Azure backbone, without traversing the public Internet.\n\n## Why Other Options Are Wrong\n- Option A: VNet Peering in the same region does not connect VNets across regions.\n- Option C: VPN Gateway VNet-to-VNet over the Internet would route traffic over the public internet, increasing latency and exposing traffic.\n- Option D: ExpressRoute private peering connects to on-prem networks, not directly between two cross-region VNets.\n\n## Key Concepts\n- Global VNet Peering enables cross-region VNet connectivity via private IPs.\n- Traffic remains on the Azure backbone with low latency.\n\n## Real-World Application\n- Use Global VNet Peering to link staging and production VNets across regions for disaster recovery and latency-sensitive apps without moving traffic to the public internet.","diagram":null,"difficulty":"intermediate","tags":["Azure Virtual Network","Global VNet Peering","NSG","Private Endpoints","Azure Firewall","AWS VPC","Kubernetes","Terraform","certification-mcq","domain-weight-20"],"channel":"azure-administrator","subChannel":"implement-networking","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T14:32:20.276Z","createdAt":"2026-01-12 14:32:20"},{"id":"azure-administrator-implement-networking-1768228340275-1","question":"You need to block outbound Internet access from a subnet but still permit access to a specific private endpoint for a storage account. What configuration achieves this with the least impact on other traffic?","answer":"[{\"id\":\"a\",\"text\":\"Apply an outbound Deny 0.0.0.0/0 with a higher priority than any allow rules, and add an explicit allow rule for the private endpoint’s private IP\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Create an NSG with a blanket Deny rule for all traffic, then add an allow rule for the private endpoint’s private IP\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Remove all NSG rules so traffic is unrestricted\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use Azure Firewall to DNAT all outbound traffic to the private endpoint\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A is correct.\n\nBy placing a Deny 0.0.0.0/0 rule with a higher priority (lower number) than the allows, you block general Internet egress while explicitly permitting traffic to the private endpoint IP address of the storage account.\n\n## Why Other Options Are Wrong\n- Option B: A blanket Deny followed by an allow to the private IP could still be misconfigured if priorities aren’t correct; it’s effectively the same concept as A but less precise in phrasing.\n- Option C: Removing NSG rules removes necessary protections and does not guarantee private endpoint access.\n- Option D: DNAT fails to provide the precise, selective access needed and adds unnecessary complexity.\n\n## Key Concepts\n- NSG outbound rules and priorities determine egress behavior.\n- Private endpoints resolve through private IPs within the VNet.\n\n## Real-World Application\n- This pattern is common when restricting general Internet access but preserving private connectivity to specific Azure services via Private Endpoints.","diagram":null,"difficulty":"intermediate","tags":["Azure Virtual Network","NSG","Private Endpoints","Azure Storage","Terraform","AWS VPC","Kubernetes","certification-mcq","domain-weight-20"],"channel":"azure-administrator","subChannel":"implement-networking","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T14:32:20.778Z","createdAt":"2026-01-12 14:32:21"},{"id":"azure-administrator-implement-networking-1768228340275-2","question":"In a hub-and-spoke topology, you want spoke VNets to use the hub's VPN gateway for on-prem connectivity. Which configuration step ensures this traffic uses the hub gateway for on-prem connections?","answer":"[{\"id\":\"a\",\"text\":\"Enable UseRemoteGateways on the peering from each spoke to the hub and Enable AllowGatewayTransit on the hub’s peering to the on-prem network\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Enable UseRemoteGateways on the hub-to-spoke peering only\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Disable gateway transit on all peerings\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use ExpressRoute for all connections between spokes and on-prem\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A is correct.\n\nTo allow spokes to reach on-prem via the hub's gateway, you enable UseRemoteGateways on the spoke-to-hub peerings and you enable AllowGatewayTransit on the hub-to-on-prem peering. This routes spoke traffic through the hub gateway for on-prem connectivity while keeping spokes isolated from each other.\n\n## Why Other Options Are Wrong\n- Option B: UseRemoteGateways on the hub-to-spoke peering is not the correct direction for spokes to use the hub gateway.\n- Option C: Disabling gateway transit prevents the hub gateway from serving as the transit for on-prem connectivity.\n- Option D: ExpressRoute is an alternative but not the required configuration for this pattern and is unnecessary for hub-based gateway transit.\n\n## Key Concepts\n- UseRemoteGateways and AllowGatewayTransit are the keys to hub-spoke gateway transit.\n\n## Real-World Application\n- This pattern is typical in enterprise MNCs where a central hub handles on-prem connectivity while spokes access resources independently.","diagram":null,"difficulty":"intermediate","tags":["Azure Virtual Network","Hub-Spoke","Gateway Transit","NSG","ExpressRoute","AWS VPC","Kubernetes","certification-mcq","domain-weight-20"],"channel":"azure-administrator","subChannel":"implement-networking","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T14:32:21.245Z","createdAt":"2026-01-12 14:32:21"},{"id":"azure-administrator-implement-networking-1768228340275-3","question":"You need to access a private endpoint for an Azure SQL Database from a VNet using a private IP, and ensure DNS resolution resolves to the private IP within the VNet. What resource must you create or modify?","answer":"[{\"id\":\"a\",\"text\":\"Create a Private DNS Zone for privatelink.database.windows.net and link it to the VNet\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Create a Public DNS Zone and export DNS records to the on-prem network\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Enable DNS resolution via NSG rules only\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use a Web Application Firewall to rewrite DNS records\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A is correct.\n\nA Private DNS Zone (such as privatelink.database.windows.net) must be created and linked to the VNet to ensure DNS queries resolve to the private endpoint's IP rather than the public endpoint.\n\n## Why Other Options Are Wrong\n- Option B: A Public DNS Zone does not provide private resolution for the private endpoint.\n- Option C: DNS resolution cannot be fully achieved through NSG rules alone.\n- Option D: A Web Application Firewall has no role in DNS resolution for private endpoints.\n\n## Key Concepts\n- Private DNS Zones map privatelink.* domains to private IPs.\n- Linking the zone to the VNet enables internal resolution of private endpoints.\n\n## Real-World Application\n- Essential for secure, private connectivity to Azure services via Private Endpoints while keeping resolution isolated to the VNet.","diagram":null,"difficulty":"intermediate","tags":["Azure Private DNS","Private Endpoints","Azure SQL Database","privatelink","Kubernetes","Terraform","AWS VPC","certification-mcq","domain-weight-20"],"channel":"azure-administrator","subChannel":"implement-networking","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T14:32:21.405Z","createdAt":"2026-01-12 14:32:21"},{"id":"azure-administrator-implement-networking-1768228340275-4","question":"In a hub-and-spoke architecture with an Azure Firewall in the hub, how do you ensure outbound Internet traffic from spoke subnets passes through the firewall for inspection?","answer":"[{\"id\":\"a\",\"text\":\"Create a route table with 0.0.0.0/0 next hop as the firewall’s private IP and associate it with the spoke subnets\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Rely on system routes; no additional configuration is needed\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Only use NSG rules to filter outbound traffic\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Configure a standard Internet-facing load balancer to mediate egress\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A is correct.\n\nTo force spoke subnets to route outbound Internet traffic through the hub firewall, create a User-Defined Route (UDR) with 0.0.0.0/0 pointing to the Azure Firewall’s private IP and associate this route table with the spoke subnets. This ensures traffic is inspected by the firewall and SNAT works correctly for responses.\n\n## Why Other Options Are Wrong\n- Option B: System routes alone do not force traffic through the firewall.\n- Option C: NSGs control allow/deny per packet but do not redirect traffic to a centralized inspection point.\n- Option D: A standard load balancer does not provide centralized egress inspection like an Azure Firewall does.\n\n## Key Concepts\n- UDRs direct traffic to a virtual appliance for inspection.\n- Azure Firewall provides centralized egress filtering and SNAT.\n\n## Real-World Application\n- Common in enterprise networks to enforce policy and monitoring for outbound Internet access from multiple spoke networks.","diagram":null,"difficulty":"intermediate","tags":["Azure Virtual Network","Azure Firewall","Hub-Spoke","UDR","Terraform","AWS VPC","Kubernetes","certification-mcq","domain-weight-20"],"channel":"azure-administrator","subChannel":"implement-networking","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T14:32:21.566Z","createdAt":"2026-01-12 14:32:21"},{"id":"azure-administrator-implement-storage-1768246015176-0","question":"You have a storage account with a blob container that stores large long-term data. You want to automatically move blobs older than 90 days to the Archive tier to save costs. Which feature should you configure?","answer":"[{\"id\":\"a\",\"text\":\"Enable a Lifecycle Management policy to transition to Archive\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Enable blob soft delete and set a 90 day retention\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Set the access tier to Archive on the container level only\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Copy blobs to a separate Archive storage account manually\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA. Lifecycle Management policy can automatically transition blobs to Archive based on age; no manual copying required.\n\n## Why Other Options Are Wrong\n- B: Soft delete is for deletions, not archiving. It does not move data to Archive.\n- C: The Archive tier is a per-blob/per-item decision; there is no container-level automatic Archive setting without a policy.\n- D: Manual copy is operationally heavy and defeats automation.\n\n## Key Concepts\n- Lifecycle Management policies\n- Blob storage tiers: hot, cool, archive\n\n## Real-World Application\n- Use lifecycle rules to reduce storage costs for rarely accessed data without manual intervention.","diagram":null,"difficulty":"intermediate","tags":["Azure","BlobStorage","LifecycleManagement","Terraform","CloudStorage","certification-mcq","domain-weight-15"],"channel":"azure-administrator","subChannel":"implement-storage","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T19:26:55.177Z","createdAt":"2026-01-12 19:26:55"},{"id":"azure-administrator-implement-storage-1768246015176-1","question":"Your organization wants to protect against accidental blob deletions for 30 days. You need to enable blob soft delete and configure a 30-day retention. Which action will achieve this?","answer":"[{\"id\":\"a\",\"text\":\"Enable blob soft delete with retention days set to 30\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Enable immutable blob storage with a 30-day WORM policy\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Enable blob versioning only\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use a Shared Access Signature with restricted permissions\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA. Blob soft delete with a 30-day retention protects against accidental deletions and allows recovery within the retention window.\n\n## Why Other Options Are Wrong\n- B: Immutable blob storage (WORM) is for regulatory immutability, not for occasional recoveries within a rolling window.\n- C: Versioning helps recover previous blob versions, but soft delete provides recovery from deletions within the retention period.\n- D: SAS controls access but does not protect against accidental deletions.\n\n## Key Concepts\n- Soft delete retention policies\n- Blob versioning vs soft delete\n\n## Real-World Application\n- Enables rapid recovery from human error or corrupted data while maintaining regulatory/compliance needs.","diagram":null,"difficulty":"intermediate","tags":["Azure","BlobStorage","SoftDelete","Terraform","DataProtection","certification-mcq","domain-weight-15"],"channel":"azure-administrator","subChannel":"implement-storage","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T19:26:55.708Z","createdAt":"2026-01-12 19:26:56"},{"id":"azure-administrator-implement-storage-1768246015176-2","question":"You are selecting a replication option for a mission-critical storage account used by customers worldwide. You want automatic failover to a paired region with read access during failover. Which replication option should you choose?","answer":"[{\"id\":\"a\",\"text\":\"LRS\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"ZRS\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"GRS with read-access enabled (RA-GRS)\",\"isCorrect\":true},{\"id\":\"d\",\"text\":\"GZRS\",\"isCorrect\":false}]","explanation":"## Correct Answer\nC. GRS with read-access enabled (RA-GRS) provides geo-redundant replication to a paired region and supports read access during failover.\n\n## Why Other Options Are Wrong\n- A: LRS keeps data in a single region with no geo-redundancy.\n- B: ZRS provides zone-redundant storage within the region, not cross-region failover.\n- D: GZRS is geo-zone redundant but RA-GRS specifically enables read access in the paired region during failover.\n\n## Key Concepts\n- Georedundant replication (GRS/RA-GRS)\n- Read access during failover (RA-GRS)\n\n## Real-World Application\n- Ensures customers can access data from the secondary region during regional outages.","diagram":null,"difficulty":"intermediate","tags":["Azure","GeoRedundantStorage","RA-GRS","Terraform","CloudStorage","certification-mcq","domain-weight-15"],"channel":"azure-administrator","subChannel":"implement-storage","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T19:26:56.234Z","createdAt":"2026-01-12 19:26:56"},{"id":"azure-administrator-implement-storage-1768246015176-3","question":"A third-party needs temporary read-only access to a specific blob for 48 hours. You want to grant restricted access without sharing your account keys. Which mechanism should you use?","answer":"[{\"id\":\"a\",\"text\":\"Shared Access Signature with read permission and expiry\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Account key with write permission\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Make the blob public for 48 hours\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use a virtual network service endpoint with public access enabled\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA. A Shared Access Signature (SAS) with read permission and a defined expiry provides granular, time-limited access without exposing account keys.\n\n## Why Other Options Are Wrong\n- B: Sharing account keys grants broad access and is not secure for temporary needs.\n- C: Making the blob public exposes data openly and is not time-limited.\n- D: Service endpoints control network access, not object-level permissions or temporary access tokens.\n\n## Key Concepts\n- Shared Access Signatures (SAS)\n- Least-privilege access control\n\n## Real-World Application\n- Vendor demonstrations or customer data sharing with strict time windows.","diagram":null,"difficulty":"intermediate","tags":["Azure","SAS","BlobStorage","Terraform","AccessControl","certification-mcq","domain-weight-15"],"channel":"azure-administrator","subChannel":"implement-storage","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T19:26:56.410Z","createdAt":"2026-01-12 19:26:56"},{"id":"azure-administrator-implement-storage-1768246015176-4","question":"To prevent any public internet exposure of a production Storage Account, you plan to restrict all public network access and rely on private endpoints from your virtual networks. Which configuration combination achieves this goal?","answer":"[{\"id\":\"a\",\"text\":\"Enable public network access and create a Private Endpoint\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Disable public network access and create Private Endpoint\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Use IP-based firewall rules only\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Rely on SAS tokens for security and leave public access enabled\",\"isCorrect\":false}]","explanation":"## Correct Answer\nB. Disabling public network access and using Private Endpoint connections ensures storage traffic stays within the virtual network and is not exposed to the public internet.\n\n## Why Other Options Are Wrong\n- A: Public access remains enabled, defeating the private-endpoint objective.\n- C: IP-based firewall alone does not cover all access paths and is less comprehensive than Private Endpoints.\n- D: SAS tokens do not prevent public exposure; they only govern access permissions.\n\n## Key Concepts\n- Private Endpoints\n- Private Link/VNet integration\n\n## Real-World Application\n- Required for compliance and to minimize public surface area in production environments.","diagram":null,"difficulty":"intermediate","tags":["Azure","PrivateEndpoint","VNet","Terraform","CloudNetworking","certification-mcq","domain-weight-15"],"channel":"azure-administrator","subChannel":"implement-storage","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T19:26:56.589Z","createdAt":"2026-01-12 19:26:56"},{"id":"azure-administrator-manage-identities-1768162892632-0","question":"An administrator needs to grant a development team access to a production resource group for two weeks, requiring approval and MFA upon activation. Which Azure AD feature should you enable to fulfill this requirement?","answer":"[{\"id\":\"a\",\"text\":\"Create a static group and assign roles manually\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Enable Privileged Identity Management (PIM) with eligible role assignments, approval workflow, and MFA on activation\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Use dynamic access tokens and conditional access\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use access reviews to periodically revoke access\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct because Privileged Identity Management (PIM) provides Just-In-Time (JIT) access for privileged roles, requiring approval workflows and MFA on activation, with an expiry window that revokes access automatically after the set duration.\n\n## Why Other Options Are Wrong\n- A: Static groups grant permanent access and do not support built-in approval or time-bound activation.\n- C: There is no concept of general dynamic access tokens with MFA on activation; this option conflates capabilities.\n- D: Access reviews manage recertification of existing access, not the granting of ephemeral elevated access.\n\n## Key Concepts\n- Privileged Identity Management (PIM)\n- Just-In-Time access\n- MFA enforcement on activation\n- Role-based access control (RBAC)\n\n## Real-World Application\nUsed when on-boarding a short-term privileged task (e.g., hotfix) where the developer needs elevated rights temporarily and auditability is required.","diagram":null,"difficulty":"intermediate","tags":["AzureAD","PIM","MFA","RBAC","IdentityGovernance","AKS","Terraform","certification-mcq","domain-weight-20"],"channel":"azure-administrator","subChannel":"manage-identities","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T20:21:32.634Z","createdAt":"2026-01-11 20:21:33"},{"id":"azure-administrator-manage-identities-1768162892632-1","question":"You need to grant a group of external contractors access to a collection of Azure resources for a limited period, with an approval workflow and automatic revocation after the period ends. Which Identity Governance feature should you use?","answer":"[{\"id\":\"a\",\"text\":\"Access Reviews\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Entitlement Management with Access Packages\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Privileged Identity Management (PIM)\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Conditional Access\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct because Entitlement Management with Access Packages enables packaging resources and roles for a group of users, with lifecycle controls, approval workflows, and automatic revocation as part of access package expiration.\n\n## Why Other Options Are Wrong\n- A: Access Reviews are for periodic recertification of existing access, not provisioning a new package with expiration.\n- C: PIM focuses on elevated privileges for individual users, not packaging access for a group.\n- D: Conditional Access enforces sign-in conditions, not lifecycle-managed access provisioning.\n\n## Key Concepts\n- Identity Governance\n- Entitlement Management\n- Access Packages\n- Lifecycle management\n\n## Real-World Application\nA project team of external contractors receives an Access Package granting required resources for 30 days; upon expiration, access is revoked automatically.","diagram":null,"difficulty":"intermediate","tags":["AzureAD","EntitlementManagement","AccessPackages","ExternalUsers","RBAC","Terraform","certification-mcq","domain-weight-20"],"channel":"azure-administrator","subChannel":"manage-identities","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T20:21:33.141Z","createdAt":"2026-01-11 20:21:33"},{"id":"azure-administrator-manage-identities-1768162892632-2","question":"To automatically manage membership-based access to Azure resources based on user attributes, which feature should you implement?","answer":"[{\"id\":\"a\",\"text\":\"Static groups\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Dynamic membership rules for Azure AD groups\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Access Reviews\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Entitlement Management\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct because dynamic membership rules automatically update group membership based on user attributes (e.g., department, title), which then drives group-based RBAC to Azure resources.\n\n## Why Other Options Are Wrong\n- A: Static groups require manual edits and do not respond to user attribute changes.\n- C: Access Reviews manage ongoing access, not automatic membership changes.\n- D: Entitlement Management packages access but does not provide dynamic membership for groups.\n\n## Key Concepts\n- Dynamic groups\n- Azure AD attributes\n- Group-based RBAC\n- ABAC concepts\n\n## Real-World Application\nAutomatically provisioning access for a department by updating group membership when employees change roles.","diagram":null,"difficulty":"intermediate","tags":["AzureAD","DynamicGroups","RBAC","ABAC","Kubernetes","Terraform","certification-mcq","domain-weight-20"],"channel":"azure-administrator","subChannel":"manage-identities","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T20:21:33.686Z","createdAt":"2026-01-11 20:21:33"},{"id":"azure-administrator-monitor-backup-1768260176934-0","question":"You have a VM scale set with 6 instances. You want to trigger an alert when the average CPU percentage across all instances exceeds 80% for 10 consecutive minutes. Which approach is correct?","answer":"[{\"id\":\"a\",\"text\":\"Create a metric alert on Percentage CPU scoped to the VM scale set, with aggregation set to Average, condition Greater than 80%, and a 10-minute evaluation window.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Create a log analytics alert using a Kusto query on Perf to detect average CPU > 80% across all VMs over a 10-minute window.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Create an activity log alert for VM restart events when CPU spikes.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Create a dashboard tile that shows the current CPU average and manually alert.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct answer is **A** because Azure Monitor metric alerts support multi resource scope and aggregation, allowing you to compute the average Percentage CPU across all instances in a VM scale set and trigger when the threshold is exceeded for the defined duration. \n\n## Why Other Options Are Wrong\n- Option **B** describes a log analytics query approach which adds complexity and potential delay; metric alerts are the proper mechanism for real-time thresholding across multiple instances. \n- Option **C** monitors VM restart events, not CPU utilization, so it won't satisfy the requirement. \n- Option **D** would not produce automated alerts and relies on manual intervention. \n\n## Key Concepts\n- Azure Monitor metric alerts\n- Multi-resource scope and aggregation\n- VM Scale Sets and cross-instance metrics\n\n## Real-World Application\n- Use this pattern to detect sustained CPU pressure across an entire scale set and trigger automated response or paging to on-call engineers.","diagram":null,"difficulty":"intermediate","tags":["Azure Monitor","Kubernetes","Terraform","AWS","Automation","AKS","certification-mcq","domain-weight-15"],"channel":"azure-administrator","subChannel":"monitor-backup","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T23:22:56.935Z","createdAt":"2026-01-12 23:22:57"},{"id":"azure-administrator-monitor-backup-1768260176934-1","question":"You need a cross-resource dashboard showing CPU, memory, and latency across App Service, virtual machines, and AKS. Which Azure Monitor feature is best suited to this?","answer":"[{\"id\":\"a\",\"text\":\"Create a single Azure Dashboard with separate tiles for each resource.\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use Azure Monitor Workbooks to combine multiple data sources into a single, customizable view.\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Create a Log Analytics dashboard with Kusto queries.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use Application Insights for all resources.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct answer is **B** because Azure Monitor Workbooks provide a flexible, cross-resource view that can integrate metrics from App Service, VMs, and AKS in a single, interactive canvas. \n\n## Why Other Options Are Wrong\n- Option **A** may display data but lacks the integrated, queryable, and shareable capabilities of Workbooks for multi-resource scenarios. \n- Option **C** is possible but Workbooks offer a more polished, interactive experience without requiring separate per-resource queries. \n- Option **D** focuses on app telemetry and is not suited for infrastructure and service metrics across multiple resource types. \n\n## Key Concepts\n- Azure Monitor Workbooks\n- Cross-resource telemetry integration\n- Multi-resource dashboards\n\n## Real-World Application\n- Use Workbooks to provide much-needed single-pane-of-glass visibility for ops teams managing hybrid workloads.","diagram":null,"difficulty":"intermediate","tags":["Azure Monitor","Kubernetes","Terraform","AWS","AKS","certification-mcq","domain-weight-15"],"channel":"azure-administrator","subChannel":"monitor-backup","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T23:22:57.345Z","createdAt":"2026-01-12 23:22:57"},{"id":"azure-administrator-monitor-backup-1768260176934-2","question":"Backup operations occasionally fail due to transient network issues. You want to automatically retry the failed backup job and notify on persistent failures. Which solution best achieves this?","answer":"[{\"id\":\"a\",\"text\":\"Create an Azure Automation Runbook that triggers a backup retry in response to a failure alert.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Create a Log Analytics alert that requires a manual retry from the portal.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Configure the backup policy to automatically retry failed jobs.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use a monitoring dashboard to visually identify failures and assign manual remediation.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct option is **A** because you can automate remediation by triggering an Azure Automation Runbook to retry the failed backup job when a failure alert fires.\n\n## Why Other Options Are Wrong\n- Option **B** relies on manual intervention rather than automation. \n- Option **C** implies built-in automatic retry which may not be available or reliable for transient network issues. \n- Option **D** provides visibility but does not automate remediation. \n\n## Key Concepts\n- Azure Backup and Recovery Services Vault\n- Azure Automation Runbooks\n- Alert-driven remediation\n\n## Real-World Application\n- Implementing automated retries reduces MTTR for backup failures and improves compliance with recovery objectives.","diagram":null,"difficulty":"intermediate","tags":["Azure Monitor","Terraform","Kubernetes","AWS","Automation","certification-mcq","domain-weight-15"],"channel":"azure-administrator","subChannel":"monitor-backup","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T23:22:57.756Z","createdAt":"2026-01-12 23:22:57"},{"id":"azure-administrator-monitor-backup-1768260176934-3","question":"To alert on CPU spikes only during business hours (Mon-Fri 08:00-18:00), which approach is most practical?","answer":"[{\"id\":\"a\",\"text\":\"Create a metric alert with a schedule-based evaluation window restricting to business hours.\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Create a Log Analytics alert with a Kusto query that filters data to 08:00-18:00 on weekdays.\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Use Application Insights to apply time-based sampling during business hours.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use a dashboard alert that only shows data during business hours.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct option is **B** because you can implement time-bound alerting in a cross-resource context by using a Log Analytics alert with a Kusto query that filters TimeGenerated to business hours on weekdays. \n\n## Why Other Options Are Wrong\n- Option **A** does not support such precise time-based gating in metric alerts. \n- Option **C** is not suitable for infrastructure metrics across resources. \n- Option **D** only affects visualization and does not trigger alerts. \n\n## Key Concepts\n- Log Analytics and Kusto queries\n- Time-based filtering in alerts\n- Cross-resource monitoring\n\n## Real-World Application\n- Reduces alert fatigue by only notifying during critical business hours when operators are available.","diagram":null,"difficulty":"intermediate","tags":["Azure Monitor","Kubernetes","Terraform","AWS","Automation","certification-mcq","domain-weight-15"],"channel":"azure-administrator","subChannel":"monitor-backup","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T23:22:57.895Z","createdAt":"2026-01-12 23:22:57"},{"id":"azure-administrator-monitor-backup-1768260176934-4","question":"To ensure all resources in a resource group emit diagnostic logs to a shared Log Analytics workspace and automatically remediate missing configurations, which approach is most effective?","answer":"[{\"id\":\"a\",\"text\":\"Implement an Azure Policy with DeployIfNotExists to enforce Diagnostic Settings on all resources.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Manually verify diagnostic settings quarterly and apply changes as needed.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Publish diagnostic data to an Event Hub and use a separate process to ingest to Log Analytics.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Rely on default diagnostic settings and rely on resource-level alerts.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct option is **A** because Azure Policy with DeployIfNotExists can automatically deploy diagnostic settings to all resources that lack them, ensuring consistent logging. \n\n## Why Other Options Are Wrong\n- Option **B** is prone to human error and delays. \n- Option **C** adds complexity without automatic remediation. \n- Option **D** does not guarantee diagnostic data is collected from all resources. \n\n## Key Concepts\n- Azure Policy and DeployIfNotExists\n- Diagnostic Settings in Azure Monitor\n- Centralized logging and governance\n\n## Real-World Application\n- Automates enforcement of observability across a fleet of resources, improving incident response and compliance.","diagram":null,"difficulty":"intermediate","tags":["Azure Monitor","Kubernetes","Terraform","AWS","Automation","certification-mcq","domain-weight-15"],"channel":"azure-administrator","subChannel":"monitor-backup","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T23:22:58.037Z","createdAt":"2026-01-12 23:22:58"}],"subChannels":["deploy-manage-compute","deploy-manage-resources","general","implement-networking","implement-storage","manage-identities","monitor-backup"],"companies":["Airbnb","Citadel","Databricks","Discord","DoorDash","Goldman Sachs","Google","IBM","Instacart","Lyft","MongoDB","NVIDIA","Netflix","PayPal","Stripe","Twitter","Two Sigma"],"stats":{"total":34,"beginner":5,"intermediate":26,"advanced":3,"newThisWeek":34}}