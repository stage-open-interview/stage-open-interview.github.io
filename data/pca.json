{"questions":[{"id":"q-1012","question":"In a churn prediction problem, you have 20k customers and 500 features (mix of binary indicators and continuous metrics). PCA will be used before a logistic regression model to predict churn. Describe an end-to-end plan to (1) handle missing values and mixed data types, (2) scale features appropriately, (3) choose the number of components with cross-validated downstream performance, (4) interpret the top loadings for business insight, and (5) guard against leakage and overfitting in a production pipeline?","answer":"Use a pipeline: impute missing values per type (continuous: median, binary/categorical: most frequent). Encode binaries as 0/1 or via one-hot for all categories. Scale to zero mean and unit variance, ","explanation":"## Why This Is Asked\nTests end-to-end handling of mixed data in PCA and how to tie PCA to a downstream model with proper validation.\n\n## Key Concepts\n- Mixed data PCA preprocessing; imputation strategies; scaling; component selection via CV-AUC; loading interpretation.\n\n## Code Example\n```javascript\n// Pseudo-code: sklearn-like pipeline (illustrative)\nconst pipeline = [\n  {step: 'imputer', strategy: 'median'},\n  {step: 'scaler', type: 'StandardScaler'},\n  {step: 'pca', components: 0.95},\n  {step: 'classifier', type: 'LogisticRegression'}\n];\n```\n\n## Follow-up Questions\n- How would you handle highly imbalanced churn? alternatives to AUC tie.\n- How would you validate the production pipeline to avoid leakage?\n","diagram":"flowchart TD\n  A[Gather Data] --> B[Impute & Scale]\n  B --> C[PCA]\n  C --> D[Train Logistic Regression]\n  D --> E[Validate & Deploy]","difficulty":"beginner","tags":["pca"],"channel":"pca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","NVIDIA","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T19:26:31.167Z","createdAt":"2026-01-12T19:26:31.167Z"},{"id":"q-1068","question":"Design an online robust PCA for a streaming fraud-detection pipeline: 50k events/sec, 1000 features with missing values. Describe incremental component updates, robust outlier handling (robust PCA or GoDec variants), missing-data strategy, drift monitoring (eigenvalue gaps, loadings stability, score distribution), and how you’d validate downstream classifier performance under tight memory/time constraints?","answer":"Propose online robust PCA: maintain a compact basis updated with mini-batches via incremental SVD or GoDec-like low-rank + sparse decomposition. Use masked updates for missing data or pre-impute; moni","explanation":"## Why This Is Asked\n\nTests ability to reason about online, robust dimensionality reduction in production-like streaming environment; assesses drift handling, missing data strategies, and resource constraints, plus linkage to downstream models.\n\n## Key Concepts\n\n- Online/incremental PCA\n- Robust PCA / low-rank plus sparse\n- Streaming missing-data handling\n- Drift detection and model refresh\n- Downstream model validation in constrained settings\n\n## Code Example\n\n```python\n# Pseudocode\nipca = IncrementalPCA(n_components=50)\nfor batch in stream():\n    X = mask_impute(batch)  # handle missing\n    ipca.partial_fit(X)\n```\n\n## Follow-up Questions\n\n- How would you trigger a re-training and what data would you use?\n- How do you ensure component spaces remain aligned after refresh?","diagram":null,"difficulty":"advanced","tags":["pca"],"channel":"pca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Oracle","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T21:29:25.385Z","createdAt":"2026-01-12T21:29:25.385Z"},{"id":"q-1104","question":"In a production streaming recommender system, you maintain an incremental PCA basis on 1M users and 3k features, updated hourly. Design an online robust PCA pipeline that adapts to non-stationary covariances, handles missing data, and detects concept drift. Describe algorithm choices (incremental/robust variants, forgetting factors), when to re-train vs update, how to align with past loadings, validation of downstream models after projection, and monitoring for numerical stability?","answer":"Use incremental PCA with exponential forgetting to adapt to non-stationary covariances, plus a robust RPCA layer to separate low-rank structure from sparse outliers. Handle missing data with online EM","explanation":"## Why This Is Asked\nTests designing an online, robust PCA system under drift, missing data, and scale, plus practical validation for downstream models in production.\n\n## Key Concepts\n- Incremental PCA, robust PCA variants, online EM for missing data, concept drift, forgetting factors, drift detection triggers, online A/B validation, numerical stability checks.\n\n## Code Example\n```python\n# Pseudocode: online_update(X_new, U, S, Vt, forgetting=0.98):\n# 1) impute X_new via current subspace U\n# 2) update covariance with forgetting\n# 3) perform RPCA split (low-rank U S Vt, sparse Z)\n# 4) update U,S,Vt accordingly\n```\n\n## Follow-up Questions\n- How would you set drift thresholds and forgetting factors in a live system?\n- What metrics would you monitor beyond recall/precision to detect instability?","diagram":"flowchart TD\n  A[Streaming Data] --> B[Incremental PCA]\n  B --> C{Drift Detected?}\n  C -->| yes | D[Refresh Basis]\n  C -->| no | E[Continue Update]\n  D --> F[Validate downstream]\n  E --> F","difficulty":"advanced","tags":["pca"],"channel":"pca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Goldman Sachs","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T22:35:13.691Z","createdAt":"2026-01-12T22:35:13.691Z"},{"id":"q-1128","question":"You have 50k samples, 1k gene-expression features with many missing values. Design an end-to-end Sparse PCA pipeline to produce 40 interpretable components. How would you handle missing data, choose sparsity vs components, validate downstream models, and assess stability and biological coherence of loadings across folds?","answer":"Leverage Sparse PCA (SPCA) to enforce interpretable loadings (L1 penalty) and handle missing data via EM-SPCA or prior imputation (MICE). Run a grid over n_components and sparsity lambda, selecting by","explanation":"## Why This Is Asked\nEvaluates ability to fuse interpretability with predictive utility in high-dimensional, incomplete data, a common genomics scenario.\n\n## Key Concepts\n- Sparse PCA (SPCA) vs standard PCA\n- Missing data handling in unsupervised steps\n- Hyperparameter tuning via cross-validation\n- Stability selection across folds\n- Biological interpretability of loadings (gene sets, pathways)\n\n## Code Example\n```javascript\n// Pseudo-code: fit sparse PCA with lambda and n_components\nconst model = fitSparsePCA(X, { nComponents: 40, lambda: 0.1 });\nconst scores = model.transform(X);\nconst loadings = model.loadings;\n```\n\n## Follow-up Questions\n- How would you validate loadings for new data to guard against drift?\n- Which metrics would you monitor for both sparsity and predictive performance during CV?","diagram":null,"difficulty":"intermediate","tags":["pca"],"channel":"pca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Twitter","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T23:33:03.262Z","createdAt":"2026-01-12T23:33:03.262Z"},{"id":"q-1153","question":"Design a PPCA-based dimensionality reduction pipeline for real-time telemetry data: 1B feature vectors daily, 500 real-valued features with missing values and skew, to feed a downstream anomaly detector. Explain fitting PPCA with EM, selecting k via BIC on a rolling window, comparing to standard PCA, handling streaming updates with forgetting factors, and validating in production?","answer":"Fit PPCA with EM to handle missing data and obtain component uncertainties. Pick k via BIC on a representative held-out window; compare to standard PCA on reconstruction error and downstream anomaly d","explanation":"## Why This Is Asked\n\nTests practical application of probabilistic PCA in realistic, noisy, and streaming environments. Evaluates model selection, missing-data handling, and integration with downstream tasks under non-ideal data.\n\n## Key Concepts\n\n- Probabilistic PCA (PPCA) and EM algorithm\n- Missing data handling in PCA frameworks\n- Model selection with BIC in a streaming context\n- Streaming/incremental updates and forgetting factors\n- Evaluation: reconstruction error vs downstream detector performance\n\n## Code Example\n\n```python\n# Pseudo-code: E-step for PPCA\nZ_hat = W.T @ x\n# M-step updates for W, psi (noise variance)\n# Iterate until convergence\n```\n\n## Follow-up Questions\n\n- How would you detect and react to drift in loadings over time?\n- How would you handle non-Gaussian noise or heavy tails in the data?\n- When might kernel PCA or a sparse PPCA be preferable over PPCA?","diagram":"flowchart TD\n  A[Ingest data] --> B[PPCA with EM]\n  B --> C{Select k via BIC on window}\n  C --> D[Project data to k components]\n  D --> E[Feed to anomaly detector]\n  E --> F[Monitor drift & reconstruction error]","difficulty":"intermediate","tags":["pca"],"channel":"pca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Salesforce","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T01:37:11.263Z","createdAt":"2026-01-13T01:37:11.263Z"},{"id":"q-1175","question":"In a factory IoT setting, 20 devices stream 40 features each (numeric, with occasional missing values). You want a beginner-friendly PCA-based anomaly detector on the edge. Describe how you would handle missing values, decide the number of components, and translate top loadings into actionable maintenance signals for operators, while keeping the model lightweight on-device?","answer":"Impute missing values (featurewise mean) before scaling. Standardize numeric features, fit PCA on historical edge data, select k by explained variance (elbow ~85–90%). Use incremental PCA (or Oja) for","explanation":"## Why This Is Asked\nTests practical edge PCA use with missing data, component selection, and interpretability in a real-time constraint.\n\n## Key Concepts\n- Edge deployment, incremental PCA, missing-value handling, variance-based component selection, interpretability of loadings.\n- Trade-offs: memory, drift thresholds, per-device calibration.\n\n## Code Example\n```javascript\n// Pseudo-outline for incremental PCA steps\n```\n\n## Follow-up Questions\n- How would you validate detector performance with imbalanced anomalies?\n- How to adapt thresholds per device over time?","diagram":"flowchart TD\n  A[Data stream] --> B[Impute missing]\n  B --> C[Standardize]\n  C --> D[PCA (k components)]\n  D --> E[Compute anomaly score]\n  E --> F[Raise alert]","difficulty":"beginner","tags":["pca"],"channel":"pca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T03:40:05.022Z","createdAt":"2026-01-13T03:40:05.022Z"},{"id":"q-1218","question":"You have a 40-feature numeric customer-survey dataset with some missing values and skewed distributions. You want a beginner-friendly PCA-based feature set for a churn-classification model. Describe preprocessing steps (imputation, transformations, outlier handling), how to choose the number of components, and how to translate top loadings into concrete business signals for a dashboard while keeping the pipeline lightweight?","answer":"Impute with mean, apply a skew-aware transform (e.g., Yeo-Johnson), and cap outliers at 1st/99th percentiles. Standardize, then fit PCA on train data. Pick k so cumulative explained variance ≥ 90% (or","explanation":"## Why This Is Asked\nTests practical PCA workflow: preprocessing for real data, component selection, and translating math to business signals with a lightweight pipeline.\n\n## Key Concepts\n- Preprocessing for PCA (imputation, transforms, outlier handling)\n- Component selection via explained variance / elbow\n- Interpretability of loadings for dashboards\n- Lightweight pipelines suitable for production\n\n## Code Example\n```javascript\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import PowerTransformer, StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import Pipeline\n\npipe = Pipeline([\n  ('imputer', SimpleImputer(strategy='mean')),\n  ('pt', PowerTransformer(method='yeo-johnson')),\n  ('scaler', StandardScaler()),\n  ('pca', PCA(n_components=0.9, random_state=0))\n])\n```\n\n## Follow-up Questions\n- How would you explain components to a non-technical stakeholder?\n- How would you monitor loadings stability over time in a production dashboard?","diagram":"flowchart TD\n  A[Dataset: 40 features] --> B[Preprocess: impute, transform, clip]\n  B --> C[Standardize]\n  C --> D[PCA]\n  D --> E[Select components by explained variance]\n  E --> F[Interpret loadings -> signals]\n  F --> G[Dashboard-ready features]","difficulty":"beginner","tags":["pca"],"channel":"pca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T05:31:11.260Z","createdAt":"2026-01-13T05:31:11.260Z"},{"id":"q-1249","question":"You're building a real-time risk-scoring system for cross-border payments. Data arrives as numeric features with occasional missing values and a few graph-derived signals, streaming at high velocity. You need an incremental PCA that adapts to non-stationary distributions and yields 40 components. Describe how you would: (a) choose/update the number of components under drift, (b) perform online imputation without data leakage, (c) keep loadings interpretable for dashboards, (d) coordinate PCA updates with downstream models to control drift, and (e) design a robust rollback strategy with governance in production?","answer":"Use IncrementalPCA with 40 components and a sliding window forgetting factor to adapt to drift. Impute online with masked updates to avoid leakage. Keep loadings interpretable by anchoring to a stable","explanation":"## Why This Is Asked\n\nThis question probes the candidate's ability to design an online PCA pipeline that adapts to drift while preserving interpretability and governance in production.\n\n## Key Concepts\n\n- Incremental PCA with forgetting factor and windowing\n- Drift detection for loadings and explained variance\n- Online imputation with masking to prevent leakage\n- Loadings stability and interpretability for dashboards\n- Model governance: rollback, blue/green, feature toggles\n\n## Code Example\n\n```python\n# Skeleton for IncrementalPCA with partial_fit\nfrom sklearn.decomposition import IncrementalPCA\nipca = IncrementalPCA(n_components=40)\nfor batch in data_stream():\n    batch = impute(batch)  # online imputation\n    ipca.partial_fit(batch)\n```\n\n## Follow-up Questions\n\n- How would you quantify drift in loadings across windows?\n- What rollback criteria would you implement for production rollouts?","diagram":null,"difficulty":"advanced","tags":["pca"],"channel":"pca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Coinbase","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T06:43:19.686Z","createdAt":"2026-01-13T06:43:19.686Z"},{"id":"q-1288","question":"Design a daily-updated PCA-based representation for streaming telemetry with 300 features per vector, many missing values and sparse signals. Outline preprocessing, choice of incremental PCA approach (IPCA vs randomized SVD), when to refresh the basis, how to align new loadings with the existing basis, and how to validate the downstream anomaly detector after dimensionality reduction. Include concrete knobs (batch size, forgetting factor, drift thresholds)?","answer":"IPCA with daily batch of 50k vectors, 300 features. Impute missing via training feature means; center and scale with running stats. Forgetting factor 0.98. Refresh basis when mean principal-angle of t","explanation":"## Why This Is Asked\nTests practical, production-ready design for incremental PCA on streaming data with missingness and drift, including preprocessing, basis management, and end-to-end validation.\n\n## Key Concepts\n- Incremental PCA variants (IPCA vs randomized SVD)\n- Drift detection via principal angles and explained variance\n- Basis alignment (Procrustes) to preserve downstream weights\n- Rolling holdout validation and rollback strategy\n\n## Code Example\n```python\n# Python sketch\nfrom sklearn.decomposition import IncrementalPCA\nipca = IncrementalPCA(n_components=10, batch_size=5000)\nfor batch in stream():\n    X = preprocess(batch)\n    ipca.partial_fit(X)\n```\n\n## Follow-up Questions\n- How would you handle features that evolve (new features) over time?\n- What alternative drift metrics would you consider besides principal angles?\n- How would you monitor and alert if reconstruction error spikes occur?","diagram":"flowchart TD\n  A[Ingest daily batch] --> B[Preprocess]\n  B --> C[PCA update]\n  C --> D{Drift check}\n  D -->|Yes| E[Refresh basis]\n  D -->|No| F[Use existing basis]\n  E --> G[Align loadings]\n  F --> G\n  G --> H[Validate on holdout]","difficulty":"intermediate","tags":["pca"],"channel":"pca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T08:33:32.612Z","createdAt":"2026-01-13T08:33:32.612Z"},{"id":"q-1306","question":"Imagine a factory IoT setup where edge devices stream 60 numeric features (with occasional missing values). You need a beginner-friendly, PCA-based anomaly detector that runs on-device. Outline a concrete pipeline: (a) how to handle missing values, (b) how to standardize and fit PCA (including incremental options), (c) how to decide the number of components for robust anomaly signals, and (d) how to translate top loadings into actionable operator cues on a dashboard or device indicator?","answer":"Use SimpleImputer with mean strategy, scale features with StandardScaler, fit PCA on historical clean data; choose components by explained variance threshold (e.g., 95%) and stability via bootstrap; m","explanation":"## Why This Is Asked\n\nTests practical PCA deployment in a constrained, real-world setting: missing data, on-device computation, and translating math into actionable signals.\n\n## Key Concepts\n\n- Data imputation and standardization for PCA suitability\n- Incremental PCA for streaming data\n- Selecting components via explained variance and stability checks\n- Interpreting loadings to concrete operator cues\n\n## Code Example\n\n```python\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import IncrementalPCA\nfrom sklearn.pipeline import Pipeline\n\npipe = Pipeline([\n  ('imputer', SimpleImputer(strategy='mean')),\n  ('scaler', StandardScaler()),\n  ('ipca', IncrementalPCA(n_components=8))\n])\n```\n\n## Follow-up Questions\n\n- How would you monitor drift in component loadings post-deployment?\n- How would you validate the detector with synthetic anomalies and real sensors intermittently failing?","diagram":"flowchart TD\n  A[60 Features Edge Stream] --> B[Impute & Scale]\n  B --> C[PCA (n_components)]\n  C --> D[Anomaly Score & Loadings]\n  D --> E[Operator Cues (dashboard/device)]","difficulty":"beginner","tags":["pca"],"channel":"pca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","Tesla","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T10:33:18.858Z","createdAt":"2026-01-13T10:33:18.860Z"},{"id":"q-837","question":"In a dataset with 120k samples and 200 features, after standardizing, you fit PCA and observe 95% variance explained by the first 8 components. How would you validate using PCA for downstream linear regression, decide the number of components, and interpret the top loadings? Consider missing values and large-scale data in your answer?","answer":"Use explained variance, scree, and cross-validated reconstruction error to pick k; for large data, use IncrementalPCA and scale features. Validate by nested CV: train a linear regressor on the k compo","explanation":"## Why This Is Asked\nAssesses practical PCA decision-making, including selection of components, preprocessing, and how to validate in a regression context.\n\n## Key Concepts\n- Explained variance and scree plots\n- Cross-validated reconstruction error\n- Incremental PCA for scale\n- Loadings interpretation and mapping to features\n- Handling missing data in PCA\n\n## Code Example\n```python\nfrom sklearn.decomposition import IncrementalPCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\n\n# pipeline sketch\nfrom sklearn.pipeline import Pipeline\npipe = Pipeline([\n  ('scaler', StandardScaler()),\n  ('ipca', IncrementalPCA(n_components=8)),\n  ('lr', LinearRegression())\n])\n# fit and evaluate with cross-validation\n```\n\n## Follow-up Questions\n- How would you handle categorical features with PCA?\n- What trade-offs arise with Sparse PCA vs dense components?","diagram":"flowchart TD\n  A[Standardize data] --> B[PCA fit]\n  B --> C[Select k by explained variance]\n  C --> D[Nested CV with regressor on k components]\n  D --> E[Compare to full-feature model]\n  E --> F[Interpret top loadings]","difficulty":"intermediate","tags":["pca"],"channel":"pca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:21:56.218Z","createdAt":"2026-01-12T13:21:56.218Z"},{"id":"q-934","question":"Suppose a streaming analytics pipeline ingests 10k new vectors daily, each with 180 features, and PCA was computed on historical data. Describe an end-to-end approach to decide when to refresh the PCA vs keep the existing basis, how to measure component drift, how to align new loadings with the old basis, how to handle missing values in streaming data, and how to validate downstream models after dimensionality reduction?","answer":"Implement online PCA (incremental SVD) to update the basis with daily data. For drift, compute per-component explained variance change and loadings cosine similarity; refresh if >0.1 drift on 2+ compo","explanation":"## Why This Is Asked\n\nTests online PCA deployment, drift handling, and downstream impact in streaming contexts.\n\n## Key Concepts\n\n- Online/Incremental PCA\n- Drift detection and component alignment\n- Missing value handling in streams\n- Validation of downstream models after dimensionality reduction\n\n## Code Example\n\n```python\nimport numpy as np\nfrom scipy.linalg import svd\ndef procrustes(X, Y):\n    muX = X.mean(0); muY = Y.mean(0)\n    X0 = X - muX; Y0 = Y - muY\n    U, s, Vt = svd(np.dot(X0.T, Y0))\n    R = np.dot(U, Vt)\n    return Y0.dot(R)\n```\n\n## Follow-up Questions\n\n- How to quantify drift across batches with unequal sizes?\n- How to choose thresholds without overfitting to a single dataset?","diagram":"flowchart TD\n  A[Daily Batch] --> B[Update Basis]\n  B --> C{Drift Detected?}\n  C -- Yes --> D[Refresh Basis]\n  C -- No --> E[Align Loadings]\n  E --> F[Impute Missing]\n  F --> G[Validate Downstream]\n  G --> H[Monitor & Iterate]","difficulty":"intermediate","tags":["pca"],"channel":"pca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Discord","Hashicorp"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T15:43:43.295Z","createdAt":"2026-01-12T15:43:43.295Z"}],"subChannels":["general"],"companies":["Adobe","Airbnb","Amazon","Apple","Citadel","Cloudflare","Coinbase","Databricks","Discord","DoorDash","Goldman Sachs","Google","Hashicorp","Meta","MongoDB","NVIDIA","Oracle","Plaid","Salesforce","Snowflake","Tesla","Twitter","Two Sigma","Uber","Zoom"],"stats":{"total":12,"beginner":4,"intermediate":5,"advanced":3,"newThisWeek":12}}