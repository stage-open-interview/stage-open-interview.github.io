{"questions":[{"id":"q-1125","question":"You're building an incremental dbt model analytics.daily_revenue_by_product sourced from staging.sales_raw. Data can arrive late up to 2 days. How would you implement incremental upserts, reprocess late days without disturbing older data, and validate with tests and a snapshot of product price history? Provide a minimal SQL snippet illustrating the approach?","answer":"Implement an incremental model with unique_key=['day','product_id','region']; reprocess the last 2 days during incremental runs to absorb late data; use MERGE/upsert where supported, else a two-phase ","explanation":"## Why This Is Asked\nTests practical incremental logic and late-arrival handling; checks upsert patterns, tests, and snapshots.\n\n## Key Concepts\n- Incremental materialization with unique keys\n- Late-arrival handling window\n- Snapshot for price history\n- Basic tests for data quality\n\n## Code Example\n```sql\n-- models/analytics/daily_revenue_by_product.sql\n{{ config(materialized='incremental', unique_key=['day','product_id','region']) }}\n\nwith s as (\n  select date_trunc('day', order_date) as day,\n         product_id,\n         region,\n         sum(quantity * price) as revenue\n  from {{ ref('staging_sales_raw') }}\n  group by 1,2,3\n)\n\nselect day, product_id, region, revenue\nfrom s\n{% if is_incremental() %}\n  -- reprocess last 2 days to capture late data\n  where day >= (select max(day) from {{ this }}) - interval '2 day'\n{% endif %}\n```\n\n## Follow-up Questions\n- How would you adapt this for Redshift vs Snowflake?\n- How would you test the snapshot consistency across environments?","diagram":null,"difficulty":"beginner","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Hashicorp"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T23:31:34.187Z","createdAt":"2026-01-12T23:31:34.187Z"},{"id":"q-1165","question":"You're designing a dbt analytics pipeline for a ride-sharing platform. The raw events are in `staging.events_raw` with columns like `ride_id`, `city`, `ride_type`, `currency`, `amount`, `occurred_at`, and data can arrive up to 48 hours late. Build a beginner-friendly incremental model `analytics.daily_revenue` that computes USD revenue per day by `date`, `city`, and `ride_type`. Use a daily `pricing.exchange_rates` table to convert from local currency to USD. Describe how you'd implement incremental upserts, late-data handling (2-day window), schema-drift guards, and validation with tests and a snapshot. Also outline tests for late refunds and missing rates?","answer":"Use an incremental model with is_incremental(). Join staging events to pricing.exchange_rates on date and currency, then aggregate revenue in USD by date, city, ride_type. Implement a 2-day late-data ","explanation":"## Why This Is Asked\nThe question probes practical incremental modeling with late data, currency conversion, and drift safeguards—core dbt skills at junior to mid-beginner level.\n\n## Key Concepts\n- Incremental models and upserts\n- Currency conversion via reference table\n- Late-data window handling\n- Schema-drift guards and tests\n- Snapshots for drift detection\n\n## Code Example\n```sql\n-- dbt model: analytics/daily_revenue.sql\nwith events as (\n  select\n    date_trunc('day', occurred_at) as date,\n    city,\n    ride_type,\n    currency,\n    amount\n  from {{ source('staging','events_raw') }}\n  where occurred_at <= (current_timestamp() - interval '2 days')\n),\nrates as (\n  select date, currency, rate\n  from {{ source('pricing','exchange_rates') }}\n)\nselect\n  e.date,\n  e.city,\n  e.ride_type,\n  sum(e.amount * coalesce(r.rate, 1.0)) as revenue_usd\nfrom events e\nleft join rates r\n  on r.date = e.date and r.currency = e.currency\ngroup by 1,2,3\n```\n\n## Follow-up Questions\n- How would you handle gaps in exchange_rates (missing rates)?\n- How would you validate late refunds affecting revenue?\n- How would you test for time zone consistency across cities?","diagram":null,"difficulty":"beginner","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T03:31:16.675Z","createdAt":"2026-01-13T03:31:16.675Z"},{"id":"q-1257","question":"Design a drift-aware dbt workflow across dev/staging/prod in Snowflake. How would you detect schema drift between sources and models using snapshots, tests, and sources, enforce a drift threshold, and automatically fail CI when drift exceeds the threshold? Describe the macro, tests, and alert/rollback mechanism, plus how you version thresholds?","answer":"Implement a dbt macro drift_check that compares per-column metadata (data type, nullability, default, and max length) between a source table and its corresponding model, plus a runtime comparison of r","explanation":"## Why This Is Asked\nReal-world pipelines require automated detection of schema drift to prevent deployment of broken analytics. This question tests practical macro design, test coverage, and CI/CD integration.\n\n## Key Concepts\n- dbt snapshots and sources for metadata\n- Per-column drift checks (type, nullability, max length)\n- Threshold-driven gating in CI/CD and rollback policies\n- Canary prod lineage validation and alerting\n\n## Code Example\n```jinja\n{% macro drift_check(source_schema, source_table, model_schema, model_table, thresholds) -%}\n-- pseudo-logic: compare metadata and basic stats, emit drift_count if > thresholds\n{%- endmacro %}\n```\n\n## Follow-up Questions\n- How would you scale per-table thresholds across hundreds of tables?\n- How would you distinguish benign drift (e.g., new optional column) from breaking drift?","diagram":null,"difficulty":"advanced","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Google","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T07:23:31.079Z","createdAt":"2026-01-13T07:23:31.079Z"},{"id":"q-1361","question":"Design a beginner-friendly incremental dbt model in Snowflake for a SaaS analytics pipeline: raw events live at staging.event_logs with user_id, event_type (signup, login, purchase), country, occurred_at. Build analytics.daily_metrics that counts distinct active users by day and event_type, enriched by dimensions.countries. Explain incremental logic, late data handling (7 days), schema drift guards, and validation with tests and a snapshot. Also outline an Exposure for the dashboard with lineage?","answer":"Build analytics.daily_metrics as an incremental Snowflake model that counts distinct user_id by day and event_type from staging.event_logs, enriched via a join to dimensions.countries on country. Upse","explanation":"## Why This Is Asked\nThis question probes practical dbt incremental modeling, enrichment, and the linkage to dashboards via exposures.\n\n## Key Concepts\n- Incremental models with upserts in Snowflake\n- Late-arriving data handling with a 7-day window\n- Dimensional enrichment via dimensions.countries\n- Schema-drift guards and tests\n- Snapshots for user cohorts\n- Exposures and lineage visualization\n\n## Code Example\n```sql\nwith src as (\n  select distinct user_id, date(occurred_at) as day, event_type, country\n  from {{ ref('staging.event_logs') }}\n  where occurred_at >= current_timestamp() - interval '7 day'\n)\nselect day, event_type, country, count(distinct user_id) as active_users\nfrom src\ngroup by 1,2,3\n```\n\n## Follow-up Questions\n- How would you test referential integrity between analytics.daily_metrics and dimensions.countries?\n- How would you expose this model in a dashboard and ensure lineage is accurate?","diagram":null,"difficulty":"beginner","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Snowflake","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T13:15:09.731Z","createdAt":"2026-01-13T13:15:09.731Z"},{"id":"q-1523","question":"Design an incremental dbt model analytics.daily_engagement for a global multi-tenant app. Raw events sit in staging.user_events with tenant_id, user_id, email, event_type, occurred_at, privacy_flag. Build per-tenant daily distinct-user counts by event_type, redacting emails when privacy_flag is true. Describe incremental strategy, late data window, schema-drift guards, tests and snapshots, and how to expose lineage for dashboards?","answer":"Use a (tenant_id, date( occurred_at ), event_type) keyed incremental model. Compute distinct user_id from staging.user_events, redact email via a macro: if privacy_flag then 'REDACTED' else email. Lat","explanation":"## Why This Is Asked\nTests ability to design privacy-aware, tenant-scoped analytics with late-arriving data. Emphasizes incremental logic, governance, and lineage.\n\n## Key Concepts\n- Incremental models with composite keys (tenant_id, date, event_type)\n- Field-level redaction controlled by privacy_flag via a macro\n- Late-arriving data handled through a short lookback MERGE\n- Schema drift guards using sources, tests, and snapshots\n- Exposures to reflect lineage in dashboards\n\n## Code Example\n```sql\n-- macros/redact_email.sql\n{% macro redact_email(email, privacy_flag) -%}\n  {% if privacy_flag -%}\n    {{ return('REDACTED') }}\n  {% else -%}\n    {{ return(email) }}\n  {% endif -%}\n{% endmacro %}\n```\n\n## Follow-up Questions\n- How would you test the redact_email macro across combinations of privacy_flag and null emails?\n- How would you scale this approach to thousands of tenants with varying data freshness SLAs?","diagram":null,"difficulty":"intermediate","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","IBM","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T20:40:54.485Z","createdAt":"2026-01-13T20:40:54.486Z"},{"id":"q-1560","question":"In a dbt project, you ingest raw events into `staging.web_events` with columns: `event_id`, `user_id`, `event_type`, `country`, `occurred_at` (UTC). Build a beginner-friendly incremental model **analytics.daily_active_users** that reports the count of distinct `user_id`s per day, by `country` and `event_type`. Data can arrive up to 2 days late. Describe your incremental logic, how you handle late data with a rolling window, how to guard against schema drift, and how you validate with tests. Also draft an Exposure for a dashboard showing daily active users by country and event_type, including lineage?","answer":"Use an incremental model that computes daily active users by date(occurred_at) in UTC, grouped by country and event_type, then upserts into analytics.daily_active_users. For late data, retain a 2-day rolling window and reprocess the last 2 days on each run. Guard against schema drift with explicit column definitions and dbt's source freshness checks. Validate with unique user tests, null checks, and row count expectations.","explanation":"## Why This Is Asked\nAssesses practical dbt incremental modeling with late data handling and exposure design.\n\n## Key Concepts\n- Incremental model by day\n- Late-arrival window (2 days)\n- Schema drift guards\n- Tests and a dashboard exposure with lineage\n\n## Code Example\n```sql\nSELECT\n  date_trunc('day', occurred_at) AS day,\n  country,\n  event_type,\n  COUNT(DISTINCT user_id) AS active_users\nFROM {{ ref('staging.web_events') }}\nGROUP BY 1,2,3\n```\n\n## Follow-up Questions\n- How would you handle time zones for users across regions?\n- How would you adapt for evolving event_type vocabularies?","diagram":null,"difficulty":"beginner","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","DoorDash","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T06:19:05.797Z","createdAt":"2026-01-13T21:45:41.380Z"},{"id":"q-1590","question":"Design a two-model incremental dbt flow in Snowflake: 1) analytics.first_session derives first_session_at per user and seeds analytics.users(user_id, first_session_at, country_code, cohort). 2) analytics.daily_metrics counts daily active users by day and event_type, enriched with dimensions.countries. Include late-data handling (3 days), schema-drift guards, tests (not_null, unique), a snapshot, and expose lineage to dashboards?","answer":"Approach: Implement a two-model incremental dbt flow in Snowflake. First, analytics.first_session computes min(occurred_at) per user as first_session_at and seeds analytics.users with user_id, first_session_at, country_code, and cohort. Second, analytics.daily_metrics runs incrementally with a 3-day late-data watermark, counting daily active users by day and event_type while joining dimensions.countries for enrichment. Include schema-drift guards, not_null/unique tests, and a snapshot for slowly changing fields.","explanation":"## Why This Is Asked\nTests ability to design robust incremental pipelines, handle late data, and maintain lineage.\n\n## Key Concepts\n- Incremental models and ref() usage\n- First-session derivation and cohort calculation\n- Late-data watermark and idempotent upserts\n- Schema drift guards and tests; snapshots for slowly changing fields\n- Dashboards exposure and lineage\n\n## Code Example\n```sql\n-- first_session model (simplified)\nwith e as (\n  select user_id, occurred_at, country_code\n  from {{ source('raw','events_log') }}\n)\nselect user_id, min(occurred_at) as first_session_at, country_code,\n      \"","diagram":"flowchart TD\n  A[raw.events_log] --> B[analytics.first_session]\n  B --> C[analytics.users]\n  C --> D[analytics.daily_metrics]\n  E[dimensions.countries] --> C","difficulty":"intermediate","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T05:35:22.438Z","createdAt":"2026-01-13T22:55:10.734Z"},{"id":"q-1604","question":"In a dbt pipeline powering a fintech analytics platform, raw events arrive at staging.fin_events with customer_id, txn_id, amount, currency, status, event_ts. Design an incremental analytics.transactions model that deduplicates by txn_id across sources, handles late events within a 2-day window, validates currency via a macro-based set per source, guards against schema drift with a dynamic mapping table, and snapshots customers on their first txn for dashboard lineage. Include tests and performance considerations?","answer":"I would implement an incremental analytics.transactions model with unique_key txn_id, deduplicating across sources, handling late events within a 2-day watermark window, validating currency via a custom macro per source, guarding against schema drift with a dynamic mapping table, and snapshotting customers on their first transaction for dashboard lineage.","explanation":"## Why This Is Asked\n\nTests advanced dbt patterns: deduplication, late-arrival handling, schema drift detection, and data lineage.\n\n## Key Concepts\n\n- Incremental merge with per-source deduplication on txn_id\n- Watermark-based late-arrival handling (2 days)\n- Macro-based currency validation per source\n- Dynamic schema drift guards via mapping tables\n- Snapshot on customers for first transaction lineage\n- Tests: not_null, unique, relationships\n- Performance: clustering, pruning, and date-based partitioning\n\n## Code Example\n\n```sql\n-- dbt incremental model sketch\n{{ config(materialized='incremental', unique_key='txn_id') }}\n\nWITH deduped_events AS (\n  SELECT *,\n         ROW_NUMBER() OVER (\n           PARTITION BY txn_id, source \n           ORDER BY event_ts DESC\n         ) AS rn\n  FROM {{ ref('staging_fin_events') }}\n  WHERE event_ts >= DATEADD('day', -2, CURRENT_TIMESTAMP)\n),\n\nvalidated_events AS (\n  SELECT *\n  FROM deduped_events\n  WHERE rn = 1\n    AND {{ validate_currency(source, currency) }}\n),\n\nfinal_transactions AS (\n  SELECT *\n  FROM validated_events\n  WHERE {{ schema_drift_guard() }}\n)\n\nSELECT * FROM final_transactions\n```\n\n## Implementation Notes\n\n- Use `dbt snapshot` for customer first-transaction tracking\n- Implement currency validation macro with source-specific rules\n- Create mapping table for dynamic schema drift detection\n- Add comprehensive tests for data quality and performance","diagram":null,"difficulty":"advanced","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T05:31:38.231Z","createdAt":"2026-01-14T02:32:20.530Z"},{"id":"q-1714","question":"In a beginner dbt project for a media site, staging.events_logs(user_id, event_type, occurred_at, region_code) feeds analytics.daily_engagement that counts distinct users per date and event_type, enriched by dimensions.regions on region_code. Build the incremental model with late data window (3 days), include schema-drift guards, and tests (not_null on user_id, occurred_at; unique on (user_id, occurred_at, event_type)). Add a data-contract macro/test ensuring staging.events_logs contains required fields and types, and a snapshot on users to capture region changes. Explain approach, tests, and macro design?","answer":"I’d implement analytics.daily_engagement as an incremental model that aggregates distinct users per date and event_type, joining dimensions.regions on region_code. Late data is allowed in a 3-day wind","explanation":"## Why This Is Asked\nThis question probes practical dbt workflow: incremental logic, late-arrival handling, schema drift guardrails, data-contract testing, and snapshot usage. It covers core dbt skills in a realistic, beginner-friendly way.\n\n## Key Concepts\n- Incremental models and late data windows\n- Schema drift guards and tests\n- Data-contracts via macros/tests\n- Snapshots for slowly changing attributes\n\n## Code Example\n```javascript\n// Pseudo-contract test skeleton (dbt-style macro would be SQL/Jinja in practice)\nfunction assertContract(stagingColumns, required) {\n  const missing = required.filter(r => !stagingColumns.includes(r));\n  if (missing.length) throw new Error(\"Missing: \" + missing.join(\", \"));\n}\n```\n\n## Follow-up Questions\n- How would you extend the contract to handle optional fields?\n- How would you validate contracts across multiple environments (dev/stage/prod)?","diagram":null,"difficulty":"beginner","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Cloudflare","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T07:49:20.068Z","createdAt":"2026-01-14T07:49:20.068Z"},{"id":"q-1741","question":"You're building a multi-tenant dbt analytics pipeline on Snowflake. Raw events live in staging.events with tenant_id, user_id, event_type, occurred_at. You plan per-tenant analytics schemas (analytics.{tenant}). Design an incremental analytics.daily_metrics per tenant that counts distinct active users by day and event_type, with late data up to 2 days, joining dimensions.tenants and dimensions.countries, including schema-drift guards, tests (not_null, unique), and a snapshot for user_first_seen per tenant. Explain approach for cross-tenant lineage and tenant-scoped materializations?","answer":"Per-tenant schemas in Snowflake: implement analytics.daily_metrics as an incremental model scoped to analytics.${tenant}. daily by (day, tenant_id, event_type). Late data window: 2 days; use MERGE int","explanation":"## Why This Is Asked\nTests the ability to architect multi-tenant dbt pipelines with strict isolation (per-tenant schemas), robust late-data handling, and governance via tests and snapshots. It also probes strategies for cross-tenant lineage in dashboards. \n\n## Key Concepts\n- Multi-tenant isolation with per-tenant schemas\n- Incremental modeling with late-arrival handling\n- Schema drift guards and robust tests\n- Snapshots for slowly changing dimensions per tenant\n- Clear lineage exposure for dashboards across tenants\n\n## Code Example\n````sql\n-- analytics.{{tenant}}.daily_metrics.sql\nwith src as (\n  select tenant_id, user_id, event_type, date(occurred_at) as day\n  from {{ source('raw','events') }}\n)\nselect day, tenant_id, event_type, count(distinct user_id) as active_users\nfrom src\ngroup by day, tenant_id, event_type\n````\n\n## Follow-up Questions\n- How would you automate tenancy onboarding to new schemas without gluing code?\n- How do you test for cross-tenant join correctness without data leakage?","diagram":null,"difficulty":"advanced","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T09:01:41.446Z","createdAt":"2026-01-14T09:01:41.447Z"},{"id":"q-1757","question":"Design a dbt analytics layer for three domains: core_events (user actions), memberships (plans), and referrals. Data arrives late (up to 2 days). Propose an architecture that ensures: 1) incremental models with controlled late data backfills, 2) deterministic surrogate keys for cross-domain joins, 3) schema-drift guards via custom tests/macros, 4) automated docs with complete lineage, 5) dashboard-friendly exposure with stable lineage during backfills, and 6) performance considerations for Snowflake or BigQuery (partitioning, clustering). Include a concrete incremental SQL snippet?","answer":"Propose staging per domain, a single grain, incremental models with a 2-day late-arrival window, a surrogate key like domain_key + occurred_at + id for cross-domain joins, a macro-based cross-domain r","explanation":"## Why This Is Asked\nTests end-to-end dbt mastery across multi-domain conformance, late data handling, and lineage stability during backfills. It also probes custom macros for data quality and cross-domain integrity, plus performance considerations for two leading warehouses.\n\n## Key Concepts\n- Incremental models with late-arrival windows\n- Cross-domain surrogate keys and referential integrity\n- Schema drift guards via tests and macros\n- Automated docs with complete lineage\n- Backfill strategy ensuring stable dashboards\n- Performance tuning for Snowflake vs BigQuery (partitioning, clustering)\n\n## Code Example\n```sql\n-- models/analytics/fact_core_events_incremental.sql\n{{ config(materialized='incremental') }}\n\nwith src as (\n  select\n    user_id as domain_user_id,\n    event_type,\n    occurred_at,\n    domain\n  from {{ source('core_events', 'events') }}\n  where occurred_at >= dateadd(day, -2, current_date())\n)\n\nselect\n  domain_user_id,\n  domain,\n  max(occurred_at) as occurred_at,\n  count(*) as event_count\nfrom src\ngroup by domain_user_id, domain\n{% if is_incremental() %}\nwhere occurred_at > (select max(occurred_at) from {{ this }})\n{% endif %}\n```\n\n## Follow-up Questions\n- How would you validate cross-domain joins under backfills?\n- What tests would you add to guard against schema drift across domains?","diagram":null,"difficulty":"advanced","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","MongoDB","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T09:44:23.172Z","createdAt":"2026-01-14T09:44:23.172Z"},{"id":"q-1808","question":"Design an intermediate dbt workflow for a fintech analytics pipeline on Snowflake. Raw events are in staging.transactions (transaction_id, user_id, amount, currency, occurred_at, status) and staging.users (user_id, country_code, account_status). Build an incremental analytics.daily_finance that sums total_amount and transaction_count by day, currency, country_code, and status, with late data support up to 2 days. Add analytics.users_snapshot as a Type 2 surrogate for changes in country_code/account_status. Expose lineage and discuss a data-contract macro to validate source schemas and auto-generate tests?","answer":"Use an incremental model analytics.daily_finance that upserts sum(amount) and transaction_count by day, currency, country_code, and status; allow late data within a 2-day window via a MERGE-based upse","explanation":"## Why This Is Asked\nTests resilience to late data, SCD Type 2, and data contracts. It also probes lineage and macro-level test generation.\n\n## Key Concepts\n- Incremental upserts in dbt\n- Type 2 SCD for users\n- Late data handling with a 2-day window\n- Data-contract macro to auto-create tests\n- dbt docs lineage\n\n## Code Example\n```sql\n-- Example dbt model skeleton\nwith src as (\n  select * from {{ ref('staging_transactions') }}\n), u as (\n  select * from {{ ref('staging_users') }}\n)\nselect\n  date_trunc('day', occurred_at) as day,\n  currency,\n  country_code,\n  status,\n  sum(amount) as total_amount,\n  count(*) as transaction_count\nfrom src s\njoin u on s.user_id = u.user_id\ngroup by 1,2,3,4\n```\n\n## Follow-up Questions\n- How would you test for nulls and duplicates?\n- How would you adapt this for multi-tenant isolation?\n- How to surface lineage in dashboards and docs?","diagram":"flowchart TD\n  A[staging.transactions] --> B[analytics.daily_finance]\n  C[staging.users] --> D[analytics.users_snapshot]\n  B --> E[dashboard.financials]\n  B --> F[docs/lineage]","difficulty":"intermediate","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","LinkedIn"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T11:42:53.754Z","createdAt":"2026-01-14T11:42:53.754Z"},{"id":"q-1823","question":"Scenario: In a beginner dbt project for a gig-economy platform, raw events arrive in staging.event_logs with user_id, session_id, event_type (visit, click, conversion), occurred_at, and region_code. Build an incremental model analytics.daily_events that counts events by day and event_type, enriched by dims.regions on region_code. Use a 2-day late data window, guard against schema drift with not_null and unique tests, and create a data-contract macro to verify staging.event_logs contains the required columns and types before run. What approach would you take?","answer":"Implement an incremental analytics.daily_events that aggregates counts by day, event_type, and region_code from staging.event_logs joined to dims.regions on region_code. Use a 2-day late window, enfor","explanation":"Why This Is Asked\n- Tests incremental modeling with late data handling and schema-drift guards.\n- Evaluates macro usage for pre-flight data contracts and test coverage.\n\nKey Concepts\n- Incremental models in dbt\n- Late data window techniques\n- Schema-drift guards (not_null, unique)\n- Data-contract macros and pre-flight validation\n- Joining with lightweight dimensions for enrichment\n\nCode Example\n```sql\n-- analytics/daily_events.sql\nwith src as (\n  select user_id, session_id, event_type, occurred_at, region_code\n  from {{ source('staging', 'event_logs') }}\n), base as (\n  select\n    date_trunc('day', occurred_at) as day,\n    event_type,\n    region_code,\n    count(*) as event_count\n  from src\n  group by 1,2,3\n)\nselect * from base\n{% if is_incremental() %}\nwhere occurred_at >= (select max(occurred_at) from {{ this }})\n{% endif %}\n```\n\n```sql\n-- macros/data_contract.sql\n{% macro data_contract(table, required_cols) -%}\n  -- Pseudo-implementation: validates presence and types of required_cols on table\n  {# In practice, iterate required_cols and raise if missing #}\n{%- endmacro %}\n```\n\nFollow-up Questions\n- How would you extend this for multiple regions with partition pruning?\n- How would you test macro robustness across schema changes?","diagram":null,"difficulty":"beginner","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T13:05:52.833Z","createdAt":"2026-01-14T13:05:52.833Z"},{"id":"q-1909","question":"Design a per-tenant incremental dbt flow in Snowflake that computes analytics.{tenant}.daily_event_summary from staging.events (tenant_id, user_id, event_type, occurred_at, country_code, record_hash). Include enrichment from dimensions.countries, and produce a per-tenant daily count of distinct users by event_type. Implement late data handling (2 days), schema-drift guards, tests (not_null, unique), and a snapshot for analytics.{tenant}.users to capture cohort changes. Explain how you ensure cross-tenant lineage and isolation?","answer":"Plan: implement an incremental model analytics.{tenant}.daily_event_summary with a composite key (tenant_id, day, event_type, user_id). Use MERGE-like upserts via dbt incremental to accommodate late d","explanation":"Why This Is Asked\n- Tests practical dbt incremental patterns with multi-tenant isolation.\n- Covers late data handling and schema drift guards.\n\nKey Concepts\n- Incremental model with composite key; DBT incremental behavior in Snowflake.\n- Tenant isolation through per-tenant schemas; cross-tenant lineage in docs.\n- Snapshots for slowly changing cohort labels; tests for data quality.\n\nCode Example\n```sql\n-- dbt model: analytics/{tenant}/daily_event_summary.sql\nwith src as (\n  select tenant_id, user_id, event_type, date(occurred_at) as day, country_code\n  from {{ ref('staging_events') }}\n  where occurred_at >= dateadd(day, -2, current_date())\n), enriched as (\n  select s.tenant_id, s.user_id, s.event_type, s.day, c.country_name, c.continent\n  from src s\n  left join {{ ref('dimensions__countries') }} c on c.country_code = s.country_code\n)\nselect tenant_id, day, event_type, count(distinct user_id) as active_users\nfrom enriched\ngroup by 1,2,3\n```\n\nFollow-up Questions\n- How would you adapt this to handle event_type changes post-ingest?\n- How do you validate per-tenant isolation in dbt docs?","diagram":null,"difficulty":"intermediate","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","Netflix","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T16:53:50.128Z","createdAt":"2026-01-14T16:53:50.129Z"},{"id":"q-1988","question":"Design an incremental, per-tenant analytics.daily_metrics model in Snowflake with dbt. Staging.events has tenant_id, user_id, event_type, occurred_at, platform, revenue. Build analytics.{tenant}.daily_metrics counting distinct users per day by event_type, with late data tolerance of 2 days. Add a per-tenant feature flag in dimensions.tenants (revenue_enabled) and a macro to include revenue only when true. Enforce isolation via per-tenant schemas, add schema-drift guards, tests (not_null, unique), and a snapshot for analytics.{tenant}.users. Explain cross-tenant lineage and dashboard exposure?","answer":"Outline a macro-driven approach: a per-tenant flag revenue_enabled in dimensions.tenants, read by a macro revenue_included(tenant) to conditionally include revenue in analytics.{tenant}.daily_metrics.","explanation":"## Why This Is Asked\nTests ability to design complex per-tenant dbt pipelines with dynamic schemas and feature flags.\n\n## Key Concepts\n- per-tenant schema isolation\n- dbt macros for feature flags\n- incremental late-data handling\n- snapshots for cohort changes\n- tests and lineage\n\n## Code Example\n```sql\n-- analytics/{tenant}/daily_metrics.sql (conceptual)\nselect\n  '{{ tenant }}' as tenant_id,\n  date(occurred_at) as day,\n  event_type,\n  count(distinct user_id) as active_users,\n  sum(case when revenue_enabled then revenue else 0 end) as revenue\nfrom {{ ref('staging__events') }} e\njoin {{ ref('dimensions__tenants') }} t on t.tenant_id = e.tenant_id\nwhere occurred_at >= dateadd(day,-2,current_date())\ngroup by 1,2,3\n```\n\n## Follow-up Questions\n- How would you observe and test cross-tenant lineage changes when tenants are added/dropped?\n- How would you scale the macro to support tenants with different time zones?","diagram":null,"difficulty":"intermediate","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T19:43:21.230Z","createdAt":"2026-01-14T19:43:21.230Z"},{"id":"q-2013","question":"Design a per-tenant incremental analytics model in Snowflake that materializes analytics.{tenant}.daily_event_engagement from staging.events (tenant_id, user_id, event_type, occurred_at). Enrich with dimensions.regions on country_code. Produce daily counts by event_type and region_name. Add a 3-day late data window, schema-drift guards, tests (not_null, unique), and a snapshot analytics.{tenant}.customers for cohort changes. Explain tenant isolation and cross-tenant lineage via macro-generated per-tenant schemas?","answer":"Implement an incremental per-tenant model that writes to analytics.{tenant}.daily_event_engagement using staging.events, joined to dimensions.regions by country_code to group by region_name. Aggregate","explanation":"Why This Is Asked\n- Tests ability to design per-tenant dbt models with dynamic schemas and macro-driven generation.\n- Evaluates cross-tenant isolation and lineage strategies.\n- Assesses handling of late data and schema drift.\n\nKey Concepts\n- Incremental per-tenant modeling in Snowflake\n- Macros for tenant-scoped schema generation\n- Cross-tenant lineage and isolation controls\n- Data quality tests and snapshots for cohorts\n\nCode Example\n```jinja\n{% macro tenant_daily_engagement(tenant_id) %}\nselect\n  '{{ tenant_id }}' as tenant_id,\n  cast(date(occurred_at) as date) as day,\n  event_type,\n  region_name,\n  count(distinct user_id) as active_users\nfrom {{ source('staging','events') }}\nwhere tenant_id = '{{ tenant_id }}'\ngroup by 1,2,3,4\n{% endmacro %}\n```\n\nFollow-up Questions\n- How would you handle a tenant with missing region mapping?\n- How would you monitor and adapt the late-data window across tenants?","diagram":"flowchart TD\n A(Source: staging.events) --> B(Join: regions)\n B --> C(Martialization: analytics.{tenant}.daily_event_engagement)\n C --> D(Snapshot: analytics.{tenant}.customers)\n D --> E(Dashboards/Lineage)","difficulty":"intermediate","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","LinkedIn"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T20:50:59.166Z","createdAt":"2026-01-14T20:50:59.166Z"},{"id":"q-2097","question":"Design a per-tenant weekly retention pipeline in Snowflake using dbt where raw events live in staging.events with tenant_id, user_id, first_seen_at, occurred_at, and an events.users table. Create analytics.tenant_retention (tenant_id, cohort_week, retention_users, total_users) that computes weekly retention by cohort (first_seen_week) with incremental loading, and late data handling up to 4 days. Also implement analytics.global_retention that aggregates per-tenant retention across tenants with tenant_dim country/plan, ensuring cross-tenant lineage and isolation. Include schema-drift guards, tests (not_null, unique), and a snapshot of analytics.tenants to track cohort definitions. Explain how you ensure isolation and lineage?","answer":"Implement a per-tenant weekly retention pipeline in analytics.tenant_retention that cohorts users by their first_seen_at week and computes weekly active users from staging.events per tenant_id. Use MERGE statements for idempotent incremental loading with a 4-day lookback window for late data handling, and implement schema drift guards. Create analytics.global_retention to aggregate retention metrics across tenants with tenant_dim country/plan attributes. Include a snapshot of analytics.tenants for cohort definition tracking and comprehensive tests (not_null, unique).","explanation":"## Why This Is Asked\nTests ability to design cross-tenant, incremental transformations with late data handling and quality checks in a dbt/Snowflake context.\n\n## Key Concepts\n- Multi-tenant isolation and lineage\n- Incremental logic with late data window\n- Schema drift guards and tests\n- Snapshots and cross-tenant aggregates\n\n## Code Example\n```sql\n-- Pseudocode for MERGE-based incremental\nMERGE INTO analytics.tenant_retention t\nUSING (\n  SELECT tenant_id, cohort_week, \n         COUNT(DISTINCT user_id) AS retention_users, \n         COUNT(DISTINCT user_id) AS total_users \n  FROM staging.events \n```","diagram":null,"difficulty":"intermediate","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T05:02:01.147Z","createdAt":"2026-01-15T02:12:59.856Z"},{"id":"q-2133","question":"Scenario: A new multi-tenant event feed named staging.stream_events with columns tenant_id, user_id, event_type, occurred_at, country_code, payload (JSON). Task: implement a dbt incremental model analytics.daily_user_events that, for each day, tenant_id, and event_type, returns the count of distinct users. Enrich with dimensions.countries on country_code. Create a macro to parse payload JSON extracting device and app_version; fallback defaults if missing. Implement late data tolerance of 2 days (i.e., if occurred_at within last 2 days, process in daily batch). Add tests: not_null on tenant_id, user_id, occurred_at; unique on (tenant_id, user_id, occurred_at, event_type). Add a snapshot for analytics.users to capture cohort-country changes. Provide strategy for cross-tenant lineage and isolation in a single schema (no per-tenant schemas)?","answer":"Design a dbt incremental analytics.daily_user_events: group by tenant_id, date(occurred_at), event_type; count distinct user_id. Enrich using left join dimensions.countries on country_code; use a smal","explanation":"## Why This Is Asked\nTests a beginner in building incremental per-tenant analytics with data enrichment, JSON parsing, and late-arrival handling. It also probes macro design for robust JSON extraction and basic data contracts.\n\n## Key Concepts\n- dbt incremental models and late-arrival windows\n- JSON payload parsing via macros\n- Data enrichment with dimension tables\n- Basic data quality tests and a snapshot for evolving users\n- Cross-tenant lineage in a single analytics schema\n\n## Code Example\n```sql\n-- macro: parse_payload.sql\n{% macro parse_payload(payload) %}\n  CASE WHEN JSON_EXTRACT_PATH_TEXT({{ payload }}, 'device') IS NOT NULL\n       THEN JSON_EXTRACT_PATH_TEXT({{ payload }}, 'device')\n       ELSE 'unknown'\n  END AS device,\n  COALESCE(JSON_EXTRACT_PATH_TEXT({{ payload }}, 'app_version'), 'unknown') AS app_version\n{% endmacro %}\n```\n\n```sql\n-- analytics.daily_user_events model (simplified)\nselect\n  tenant_id,\n  date_trunc('day', occurred_at) as day,\n  event_type,\n  count(distinct user_id) as user_count\nfrom {{ ref('staging_stream_events') }} as s\nleft join {{ ref('dimensions_countries') }} as c\n  on s.country_code = c.country_code\ngroup by 1, 2, 3\n```\n\n## Follow-up Questions\n- How would you test for schema drift between staging and analytics models?\n- How would you extend this to thousands of tenants while preserving performance?","diagram":"flowchart TD\n  S[staging.stream_events] --> A[analytics.daily_user_events]\n  A --> D[dashboard by tenant/event_type]\n  S --> M[parse payload via macro]\n  A --> U[analytics.users snapshot]","difficulty":"beginner","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","NVIDIA","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T04:16:24.757Z","createdAt":"2026-01-15T04:16:24.757Z"},{"id":"q-2173","question":"Design a per-tenant incremental dbt model for a SaaS analytics pipeline on Snowflake. Source staging.user_events (tenant_id, user_id, event_type, event_timestamp, revenue, lifecycle_stage). Build analytics.{tenant}.daily_cohort to compute daily cohorts by lifecycle_stage and event_type with a 2-day late-data window, upserting via MERGE. Add schema-drift guards, tests (not_null, unique), and a per-tenant last_seen_users snapshot. How do you enforce cross-tenant isolation and lineage?","answer":"Build analytics.{tenant}.daily_cohort as an incremental model sourced from staging.user_events. Compute daily cohorts by lifecycle_stage and event_type with a 2-day late-data window, upserting via MER","explanation":"## Why This Is Asked\nThis probes per-tenant incremental design, late data handling, data quality, and lineage isolation in a realistic SaaS context.\n\n## Key Concepts\n- Incremental modeling with per-tenant schemas\n- Macros for tenant isolation and dynamic schema resolution\n- Late data window and MERGE upserts\n- Schema drift guards and tests (not_null, unique)\n- Snapshots for cohort dynamics and last_seen_users\n- Clear lineage via sources and refs\n\n## Code Example\n```sql\n-- skeleton showing late data handling and tenant scoping\nwith events as (\n  select * from {{ source('staging','user_events') }}\n  where event_timestamp < current_timestamp - interval '2 days'\n)\nselect\n  tenant_id,\n  date(event_timestamp) as cohort_date,\n  event_type,\n  lifecycle_stage,\n  count(distinct user_id) as user_cnt\nfrom events\ngroup by 1,2,3,4\n```\n\n## Follow-up Questions\n- How would you test tenant isolation in CI?\n- How would you adapt this for new event_type dimensions without breaking existing tenants?","diagram":null,"difficulty":"intermediate","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T05:48:29.145Z","createdAt":"2026-01-15T05:48:29.145Z"},{"id":"q-2315","question":"Design a beginner-friendly per-tenant daily_session_summary in Snowflake/dbt. Source: staging.sessions(tenant_id, user_id, session_start). Build analytics.{tenant}.daily_session_summary counting distinct users per tenant per day, with enrichment from dimensions.tenants (plan, region). Implement late data tolerance of 1 day, schema-drift guards, tests (not_null, unique). Include a data-contract macro to validate staging.sessions fields/types and a snapshot on analytics.{tenant}.users. Explain how you ensure cross-tenant isolation and lineage in dashboards?","answer":"Implement a per-tenant daily session summary in dbt: an incremental model analytics.{tenant}.daily_session_summary using a tenant-scoped macro to reference staging.sessions. Define unique key as (tena","explanation":"Why This Is Asked\nAssesses ability to design an end-to-end per-tenant metric with incremental logic, data contracts, and snapshots, plus isolation and lineage considerations.\n\nKey Concepts\n- Per-tenant isolation and lineage\n- Incremental models and late data tolerance\n- Data-contract macros and schema-drift tests\n- Snapshots to capture cohort changes\n\nCode Example\n```sql\nwith s as (\n  select tenant_id, date_trunc('day', session_start) as session_date, user_id\n  from {{ ref('staging_sessions') }}\n  where session_start is not null\n)\nselect tenant_id, session_date, count(distinct user_id) as active_users\nfrom s\ngroup by 1,2\n```\n\nFollow-up Questions\n- How would you validate that late sessions arrive within the 1-day window?\n- How would you handle tenants added after initial deployment (new analytics.{tenant} schemas)?","diagram":null,"difficulty":"beginner","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Microsoft","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T11:42:15.446Z","createdAt":"2026-01-15T11:42:15.446Z"},{"id":"q-2403","question":"Beginner dbt task: from staging.events (event_id, user_id, event_type, occurred_at, region_code, delete_flag) and dimensions.regions, implement an incremental model analytics.daily_event_summary that counts distinct users per day, event_type, country_code (via region_code). Exclude deleted events by default; provide a macro to toggle inclusion for audits. Add tests not_null(event_id, occurred_at) and unique(event_id)?","answer":"Implement analytics.daily_event_summary as an incremental Snowflake model: aggregate date_trunc('day', occurred_at), event_type, country_code (via regions) with count(distinct user_id). Use a macro de","explanation":"## Why This Is Asked\nTests a candidate's ability to implement a simple, robust incremental model with data quality checks and a governance hook (macro) for audits. Emphasizes joins, aggregation, and proper filtering.\n\n## Key Concepts\n- Incremental modeling in dbt\n- Basic joins to enrich with dimensions.regions\n- Simple data governance via macros for optional audit paths\n- Tests: not_null and unique\n\n## Code Example\n```sql\n-- analytics/daily_event_summary.sql\nwith src as (\n  select\n    date_trunc('day', occurred_at) as day,\n    e.event_type,\n    r.country_code,\n    count(distinct e.user_id) as active_users\n  from {{ source('staging','events') }} e\n  join {{ ref('dimensions_regions') }} r on e.region_code = r.region_code\n  where {{ delete_filter(false) }}\n  group by 1,2,3\n)\nselect * from src;\n```\n\n```sql\n-- macros/delete_filter.sql\n{% macro delete_filter(include_deleted=false) -%}\n  {% if include_deleted %}\n    1=1\n  {% else %}\n    delete_flag = false\n  {% endif %}\n{%- endmacro %}\n```\n\n## Follow-up Questions\n- How would you test the macro across multiple models?\n- How would you adapt this to handle late-arriving updates in staging?\n","diagram":"flowchart TD\n  A[staging.events] --> B[analytics.daily_event_summary]\n  B --> C[dashboards/consumption]\n  A --> D[dimensions.regions]\n  D --> B","difficulty":"beginner","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T16:53:24.215Z","createdAt":"2026-01-15T16:53:24.215Z"},{"id":"q-2442","question":"In a dbt project, given staging.events(user_id, event_type, occurred_at, country_code) and staging.users(user_id, created_at, country_code), implement an incremental model analytics.daily_engagement that counts distinct users per day by event_type and country_code, enriched by dimensions.countries. Include 1-day late data tolerance, schema-drift guards, tests (not_null, unique), and a snapshot on analytics.users to track country changes. Describe lineage and isolation notes?","answer":"Create an incremental analytics.daily_engagement that counts distinct user_id per day, grouped by event_type and country_code, joining staging.events with staging.users and dimensions.countries. Allow","explanation":"## Why This Is Asked\n\nAssess ability to design a beginner-friendly incremental model, enforce data quality, and capture slowly changing dimensions with snapshots.\n\n## Key Concepts\n\n- Incremental modeling in dbt for daily aggregates\n- Data quality tests: not_null and unique\n- Late-arriving data handling (1-day tolerance)\n- Snapshot usage for slowly changing dimensions\n- Referential integrity with dimensions.countries and cross-model lineage\n\n## Code Example\n\n```sql\nwith src as (\n  select\n    e.user_id,\n    date( e.occurred_at ) as day,\n    e.event_type,\n    e.country_code\n  from {{ ref('staging_events') }} e\n)\nselect\n  day,\n  event_type,\n  country_code,\n  count(distinct user_id) as user_cnt\nfrom src\ngroup by 1,2,3\n```\n\n## Follow-up Questions\n\n- How would you test that country_code exists in dimensions.countries?\n- How would you incorporate a 1-day late data window into your model without double-counting users?","diagram":"flowchart TD\n  S1[staging.events] --> EN[analytics.daily_engagement]\n  U1[staging.users] --> EN\n  EN --> C[dimensions.countries]\n  EN --> SN[analytics.users_snapshot]","difficulty":"beginner","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T17:59:36.342Z","createdAt":"2026-01-15T17:59:36.342Z"},{"id":"q-2502","question":"Design a contract-validated, multi-tenant revenue analytics flow in dbt on Snowflake. Raw events live in staging.{tenant}.events (tenant_id, user_id, event_type, amount, currency, occurred_at). Build a incremental analytics.{tenant}.daily_revenue that sums revenue per day/tenant/currency, with 2-day late data tolerance. Create contracts.tenants describing required fields and a macro that fails tests when missing fields. Add per-tenant analytics.{tenant}.users snapshot for status changes. Enforce per-tenant schemas, schema-drift guards, and tests (not_null, unique). Explain cross-tenant lineage and performance considerations?","answer":"Implement a per-tenant incremental MERGE from staging.{tenant}.events into analytics.{tenant}.daily_revenue (key: tenant_id, date, currency) with a 2-day late data window and record_hash for idempoten","explanation":"## Why This Is Asked\nAssesses ability to design contract-driven, multi-tenant dbt pipelines with late data handling and governance.\n\n## Key Concepts\n- Incremental models and MERGE in Snowflake\n- Data contracts and custom tests\n- Per-tenant schema isolation and lineage\n- Snapshots for slowly changing user state\n- Schema drift guards and test coverage\n\n## Code Example\n```sql\n-- example placeholder: actual implementation in repo\n```\n\n## Follow-up Questions\n- How would you automate contract drift detection across tenants?\n- How do you evolve contracts without breaking dashboards?\n","diagram":"flowchart TD\n  S[staging.{tenant}.events] --> A[incremental daily_revenue]\n  A --> R[analytics.{tenant}.daily_revenue]\n  C[contracts.tenants] --> M[contract-enforcer macro]\n  U[analytics.{tenant}.users Snapshot] --> L[lineage across tenants]","difficulty":"advanced","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Microsoft","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T20:44:34.530Z","createdAt":"2026-01-15T20:44:34.531Z"},{"id":"q-2685","question":"Design a beginner dbt task to build an incremental per-tenant analytics.daily_events model in Snowflake from staging.events(tenant_id, user_id, event_type, occurred_at). Include enrichment from dimensions.event_types, a 2-day late-data window, per-tenant isolation via schemas, and schema-drift guards. Add tests (not_null, unique) and a snapshot for analytics.{tenant}.users; also implement a simple data-contract macro to validate staging.events columns/types and a test using it?","answer":"Propose an incremental per-tenant daily_events model that slices by day, joins staging.events with dimensions.event_types for category enrichment, and uses per-tenant schemas to enforce isolation. Imp","explanation":"## Why This Is Asked\nTests coverage for multi-tenant isolation, data contracts, and incremental modeling in a beginner context. It also introduces a lightweight data-quality macro beyond basic tests.\n\n## Key Concepts\n- dbt incremental models in Snowflake\n- per-tenant schema isolation\n- data contracts via custom macros\n- tests: not_null, unique; snapshots for cohort tracking\n\n## Code Example\n```javascript\n-- Example macro to validate staging.events columns\n{% macro validate_staging_events(cols) %}\n  {# implementation details #}\n{% endmacro %}\n\n-- Example incremental model sketch\nwith staged as (\n  select tenant_id, user_id, event_type, occurred_at,\n         date_trunc('day', occurred_at) as day\n  from {{ source('staging','events') }}\n  where occurred_at > (current_date() - interval '2 days')\n), enriched as (\n  select s.*, e.category\n  from staged s\n  left join {{ ref('dimensions.event_types') }} e\n    on s.event_type = e.event_type\n)\nselect tenant_id, day, event_type, count(*) as event_count\nfrom enriched\ngroup by 1,2,3\n{% if is_incremental() %}\nwhere day > (select max(day) from {{ this }})\n{% endif %}\n```\n\n## Follow-up Questions\n- How would you handle late data beyond 2 days and retractions?\n- How would you test for schema drift across tenants without cross-tenant leakage?","diagram":"flowchart TD\n  A[staging.events] --> B[dimensions.event_types enrichment]\n  B --> C[analytics.{tenant}.daily_events]\n  C --> D[tests & schema drift guards]\n  D --> E[analytics.{tenant}.users snapshot]","difficulty":"beginner","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Instacart","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T06:57:40.373Z","createdAt":"2026-01-16T06:57:40.373Z"},{"id":"q-2765","question":"In a beginner dbt project for multi-tenant analytics, build an incremental model analytics.{tenant}.hourly_engagement over staging.events(tenant_id, user_id, event_type, occurred_at). Implement a 2-hour late data window, per-tenant schema isolation, and tests not_null (tenant_id, user_id, occurred_at, event_type) plus unique on (tenant_id, occurred_at, user_id, event_type). Include a small macro to provision analytics.{tenant} schemas and a snapshot analytics.{tenant}.users to capture cohort changes. Explain how to surface tenant lineage in dashboards?","answer":"Implement analytics.{tenant}.hourly_engagement as an incremental model over staging.events (tenant_id, user_id, event_type, occurred_at). Partition by hour and apply a 2-hour late window. Tests: not_n","explanation":"## Why This Is Asked\nAssesses ability to design a robust, beginner-friendly, multi-tenant dbt workflow: incremental hourly data, late-arrival tolerance, and data quality via tests; plus practical isolation via per-tenant schemas and a cohort-tracking snapshot.\n\n## Key Concepts\n- Incremental models and late-arrival handling\n- Per-tenant schema isolation\n- dbt tests: not_null and unique constraints\n- Snapshots for cohort tracking\n- Macros for tenant provisioning\n- Tenant-level dashboard lineage\n\n## Code Example\n```sql\n-- analytics/{tenant}.hourly_engagement incremental model skeleton\nwith s as (\n  select tenant_id, user_id, event_type, occurred_at\n  from {{ source('staging','events') }}\n  where tenant_id = '{{ this.schema }}' -- conceptual; actual tenant param binding varies by project\n)\nselect tenant_id,\n       date_trunc('hour', occurred_at) as occurred_at_hour,\n       event_type,\n       count(distinct user_id) as user_count\nfrom s\nwhere occurred_at > (select max(occurred_at) - interval '2 hours' from analytics.{tenant}.hourly_engagement)\ngroup by 1,2,3\n```\n\n```sql\n-- macro example (conceptual)\n{% macro ensure_tenant_schema(tenant) %}\n  {% set schema = 'analytics.' ~ tenant %}\n  {% do run_query('CREATE SCHEMA IF NOT EXISTS ' ~ schema) %}\n{% endmacro %}\n```\n\n## Follow-up Questions\n- How would you adapt late-window logic for tenants with different data arrival patterns?\n- What governance or monitoring would you add to catch schema drift across tenants?\n- How would you automate tenant onboarding and remove a tenant’s data safely?","diagram":null,"difficulty":"beginner","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Scale Ai","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T10:54:43.153Z","createdAt":"2026-01-16T10:54:43.153Z"},{"id":"q-2797","question":"In a multi-tenant dbt analytics stack on Snowflake, raw events sit in staging.events with tenant_id, user_id, event_type, occurred_at. Propose a per-tenant drift-guard plan: a canary analytics.canary_{tenant} surfacing current schema, fields, and max_version; staged per-tenant views; and an incremental analytics.tenant.daily_metrics by day, event_type, tenant_id using MERGE for upserts. Include tests (not_null, unique, foreign_key), a snapshot analytics.tenants_cohort, and isolation/lineage via dbt sources and Snowflake row-level policies. Provide skeleton SQL and tradeoffs?","answer":"Design a per-tenant drift-guard canary: analytics.canary_{tenant} surfaces current schema, fields, and max_version. Use staged per-tenant views, incrementally build daily_metrics with MERGE by (tenant","explanation":"## Why This Is Asked\nAssesses ability to design robust, multi-tenant dbt pipelines with schema drift guards, data quality tests, and clear data lineage in Snowflake. It also evaluates how you model per-tenant snapshots and maintain isolation for BI dashboards.\n\n## Key Concepts\n- Incremental daily metrics per tenant\n- Canary drift guard per-tenant\n- Snapshots for tenant cohorts\n- Data quality tests: not_null, unique, foreign_key\n- Cross-tenant lineage and isolation via dbt sources and row-level policies\n\n## Code Example\n```sql\n-- skeleton: create drift canary per tenant\nCREATE VIEW analytics.canary_${tenant} AS\nSELECT tenant_id, MAX(schema_version) AS max_version, ARRAY_AGG(field) AS fields\nFROM staging.events\nGROUP BY tenant_id;\n```\n\n## Follow-up Questions\n- How would you detect drift programmatically and gate promotions?\n- How would you scale this to 1,000 tenants and monitor drift cost-effectively?","diagram":null,"difficulty":"intermediate","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Discord","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T13:04:07.133Z","createdAt":"2026-01-16T13:04:07.134Z"},{"id":"q-2840","question":"You’re building a beginner dbt flow for multi-tenant analytics. With staging.events(tenant_id, user_id, event_type, occurred_at) and a tenants.csv seed containing tenant_id and risk_multiplier, design analytics.{tenant}.daily_event_score that sums events per day by type, applies the per-tenant risk_multiplier, tolerates 1 day late data, and includes a per-tenant users snapshot and basic tests. How would you implement this?","answer":"Use a per-tenant incremental model analytics.{tenant}.daily_event_score built on staging.events, joined to seeds.tenants for multiplier. Bucket occurred_at to day, group by tenant_id, day, event_type;","explanation":"## Why This Is Asked\nTests ability to design a practical, per-tenant incremental flow using seeds for parameters, with data freshness, tests, and a snapshot to track user cohorts.\n\n## Key Concepts\n- Incremental models per tenant\n- Seeds for tenant metadata\n- Per-tenant schemas/isolation\n- Data contracts with tests and snapshots\n- Late data handling and lineage\n\n## Code Example\n```sql\n-- analytics/{tenant}.daily_event_score.sql\nwith e as (\n  select * from {{ ref('staging__events') }}\n)\nselect\n  tenant_id,\n  date_trunc('day', occurred_at) as day,\n  event_type,\n  count(distinct user_id) as user_count,\n  count(distinct user_id) * t.multiplier as score\nfrom e\njoin {{ ref('tenants_seed') }} t on t.tenant_id = e.tenant_id\ngroup by 1,2,3\n```\n\n## Follow-up Questions\n- How would you validate late data beyond the 1-day window, and alert on anomalies?\n- How would you handle tenants missing a multiplier in the seed and prevent failures?","diagram":null,"difficulty":"beginner","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T14:35:02.039Z","createdAt":"2026-01-16T14:35:02.040Z"},{"id":"q-2882","question":"How would you design a multi-tenant dbt pipeline in Snowflake where staging.events (tenant_id, user_id, event_type, occurred_at, event_properties) feeds analytics.{tenant}.daily_metrics incrementally to count distinct active users by day and event_type, with late data up to 2 days via MERGE upserts and a per-tenant surrogate key? Include enrichment from dimensions.tenants (region, plan), a users cohort snapshot, tenant isolation via namespaced models, and a central metadata table for cross-tenant lineage; outline tests and validations?","answer":"Leverage per-tenant namespaced incremental models with a MERGE-based upsert on analytics.{tenant}.daily_metrics, keyed by (tenant_id, day, event_type, user_id). Use a surrogate daily_key to enable ide","explanation":"## Why This Is Asked\nAssess ability to architect scalable, isolated multi-tenant pipelines with late data handling, schema drift, and lineage.\n\n## Key Concepts\n- Namespaced (tenant-scoped) dbt models for isolation\n- Incremental MERGE upserts handling late data\n- Surrogate keys to guarantee idempotency\n- Central metadata table to preserve cross-tenant lineage\n- Cohort snapshotting and robust tests (not_null, unique, snapshot)\n\n## Code Example\n```sql\n-- Pseudo MERGE for daily_metrics per tenant\nMERGE INTO analytics.{tenant}.daily_metrics AS t\nUSING staging.events AS s\nON (t.tenant_id = s.tenant_id AND t.day = DATE(s.occurred_at) AND t.event_type = s.event_type AND t.user_id = s.user_id)\nWHEN MATCHED THEN UPDATE SET ...\nWHEN NOT MATCHED THEN INSERT (...);\n```\n\n## Follow-up Questions\n- How would you validate cross-tenant lineage in dashboards?\n- How would you detect and remediate schema drift in this setup?","diagram":"flowchart TD\n  A[staging.events] --> B[analytics.{tenant}.daily_metrics]\n  B --> C[dimensions.tenants]\n  B --> D[analytics.{tenant}.users_snapshot]\n  E[central_metadata] --> B","difficulty":"intermediate","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Meta","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T15:52:31.328Z","createdAt":"2026-01-16T15:52:31.329Z"},{"id":"q-2901","question":"Design a multi-tenant dbt pipeline that creates per-tenant-per-region analytics schemas (analytics.{tenant}_{region}). From staging.events (tenant_id, region_id, user_id, event_type, occurred_at, ingestion_id), implement an incremental analytics.{tenant}_{region}.hourly_metrics counting distinct users by event_type per hour, with late data handling of 1 day. Add a tenant-region users snapshot for cohort changes and a tenancy-isolation macro to block cross-region joins. Include tests (not_null, unique) and cross-tenant lineage validation?","answer":"Describe a per-tenant-per-region dbt flow creating schemas analytics.{tenant}_{region} with an incremental hourly_metrics model over staging.events(tenant_id, region_id, user_id, event_type, occurred_","explanation":"## Why This Is Asked\nTests ability to enforce tenancy boundaries in a dynamic multi-tenant, multi-region dbt setup, including late-arriving data and per-tenant snapshots.\n\n## Key Concepts\n- Dynamic per-tenant-per-region schemas analytics.{tenant}_{region}\n- Incremental hourly_metrics from staging.events\n- Deduplication via ingestion_id with ROW_NUMBER windowing\n- Late data window of 1 day; schema-drift guards via tests\n- Tenancy isolation macro preventing cross-region joins\n- Snapshots for analytics.{tenant}_{region}.users and lineage checks\n\n## Code Example\n```sql\n-- models/analytics/{{tenant}}_{{region}}/hourly_metrics.sql\nwith src as (\n  select * from {{ ref('staging_events') }}\n  where tenant_id = '{{ tenant }}' and region_id = '{{ region }}'\n),\ndedup as (\n  select *,\n         row_number() over (partition by ingestion_id order by occurred_at desc) as rn\n  from src\n)\nselect\n  date_trunc('hour', occurred_at) as hour,\n  event_type,\n  count(distinct user_id) as active_users\nfrom dedup\nwhere rn = 1\ngroup by 1,2\n```\n\n## Follow-up Questions\n- How would you validate cross-region lineage in the catalog?\n- What strategies ensure resilience if a region is degraded?","diagram":"flowchart TD\n  A[staging_events] --> B[hourly_metrics (analytics.{tenant}_{region})]\n  B --> C[users_snapshot (analytics.{tenant}_{region})]\n  C --> D[tenancy_isolation_macro]\n  D --> E[catalog & lineage validation]","difficulty":"intermediate","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Microsoft","NVIDIA","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T16:53:11.372Z","createdAt":"2026-01-16T16:53:11.372Z"},{"id":"q-2933","question":"Design a multi-tenant dbt flow in Snowflake that computes analytics.{tenant}.hourly_metrics from staging.events_raw (tenant_id, user_id, event_type, occurred_at, record_hash). Implement an incremental analytics.{tenant}.hourly_metrics by hour_start and event_type with active_users = count(distinct user_id); support 1-day late data; add dynamic schema-drift guards and tests (not_null on hour_start, tenant_id; unique on (tenant_id, hour_start, event_type)); include analytics.{tenant}.users_cohort snapshot for first_session and country changes; ensure per-tenant isolation via schemas and a global analytics.lineage table. Outline macro-driven tests and tenant-scoped materializations?","answer":"Per-tenant schemas analytics.{tenant}.hourly_metrics and analytics.{tenant}.users_cohort; an incremental hourly_metrics model bucketed by hour_start and event_type, with active_users as count(distinct","explanation":"## Why This Is Asked\nTests ability to design a scalable multi-tenant dbt flow with hourly granularity, late data handling, schema drift guards, and governance artifacts.\n\n## Key Concepts\n- Tenant isolation via per-tenant schemas\n- Incremental models with hourly bucketing\n- Late data window and watermark strategy\n- Schema drift guards implemented as reusable macros\n- Tests (not_null, unique) and a cohort snapshot\n- Data lineage and exposed dashboards\n\n## Code Example\n```javascript\n-- dbt hourly_metrics model (simplified)\nwith src as (\n  select tenant_id, user_id, event_type, occurred_at\n  from {{ source('staging', 'events_raw') }}\n), hourly as (\n  select\n    date_trunc('hour', occurred_at) as hour_start,\n    event_type,\n    tenant_id,\n    count(distinct user_id) as active_users\n  from src\n  group by 1,2,3\n)\nselect * from hourly\n```\n\n## Follow-up Questions\n- How would you test for late-arriving data skew across tenants?\n- How would you automate per-tenant schema drift guards?","diagram":null,"difficulty":"intermediate","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Microsoft","NVIDIA","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T17:54:25.699Z","createdAt":"2026-01-16T17:54:25.699Z"},{"id":"q-2940","question":"Design a beginner dbt exercise in Snowflake to build analytics.{tenant}.daily_event_summary from staging.events (tenant_id, user_id, event_type, occurred_at) enriched with dimensions.tenants (retention_group). Implement an incremental model per day that counts distinct users by event_type, with a 2-day late data window. Add tests (not_null on occurred_at,event_type,tenant_id; unique on (tenant_id, occurred_at, event_type)). Create a data-contract macro that validates staging.events fields and types before run and describe how to enforce per-tenant isolation and lineage?","answer":"Implement an incremental per-tenant daily_event_summary from staging.events (tenant_id, user_id, event_type, occurred_at) enriched by dimensions.tenants (retention_group) in Snowflake. Compute daily u","explanation":"## Why This Is Asked\nTests incremental logic, late data handling, data contracts, and tenant isolation via per-tenant artifacts.\n\n## Key Concepts\n- Incremental models\n- Late data windows\n- Data-contract macro\n- Quality tests\n- Tenant isolation patterns\n\n## Code Example\n```sql\n-- example dbt model snippet (pseudo)\nwith s as (\n  select tenant_id, user_id, event_type, date(occurred_at) as day\n  from {{ ref('staging_events') }}\n)\nselect tenant_id, day, event_type, count(distinct user_id) as users\nfrom s\ngroup by 1,2,3\n```\n\n## Follow-up Questions\n- How would you extend to 3 days late data?\n- How would you implement cross-tenant lineage mapping?\n","diagram":"flowchart TD\n  A[staging.events] --> B[analytics.{tenant}.daily_event_summary]\n  B --> C[dimensions.tenants enrichment]\n  B --> D[tests & guards]\n  B --> E[late data window]","difficulty":"beginner","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T18:46:01.513Z","createdAt":"2026-01-16T18:46:01.513Z"},{"id":"q-3016","question":"Design an advanced, privacy-aware per-tenant dbt flow. Tenants table stores privacy_level. Create a macro that SHA256-hashes user_id to analytics.{tenant}.daily_engagement.hashed_user_id only when privacy_level=high; otherwise pass-through. Build analytics.{tenant}.daily_engagement incrementally by date and action with 2-day late data; snapshot analytics.{tenant}.users for cohort and country changes. Enforce per-tenant schemas, schema-drift tests, and not_null/unique constraints; expose a cross-tenant lineage view and privacy-aware dashboards?","answer":"I would implement a privacy-aware, per-tenant dbt flow with dynamic data masking based on tenant privacy settings. The solution includes a macro that conditionally SHA256-hashes user_id to analytics.{tenant}.daily_engagement.hashed_user_id when privacy_level=high, otherwise passes through the original user_id. The daily_engagement model would be built incrementally by date and action with a 2-day late data window, while analytics.{tenant}.users would be snapshot for tracking cohort and country changes. The architecture enforces per-tenant schemas, schema-drift tests, and not_null/unique constraints, with cross-tenant lineage views and privacy-aware dashboards for comprehensive data governance.","explanation":"## Why This Is Asked\nTests ability to implement privacy-aware per-tenant data pipelines with dynamic masking, per-tenant schema isolation, and robust data quality controls.\n\n## Key Concepts\n- dbt macros and Jinja logic for per-tenant behavior\n- Conditional data masking with hashing (SHA256)\n- Incremental models with late data handling\n- Per-tenant schemas and strict tests for isolation\n- Snapshotting and lineage exposure\n\n## Code Example\n```sql\n-- Macro skeleton for privacy-aware hashing\n{% macro hash_user_for_tenant(privacy_level, user_id) %}\n  {% if privacy_level == 'high' %}\n    SHA2(CONCAT('{{ user_id }}'), 256)\n  {% else %}\n    {{ user_id }}\n  {% endif %}\n{% endmacro %}\n```\n\n## Implementation Approach\n1. Create tenant-aware macro for conditional hashing\n2. Build incremental daily_engagement with late data handling\n3. Implement snapshot for users tracking changes\n4. Add per-tenant schema tests and constraints\n5. Create cross-tenant lineage views and dashboards","diagram":null,"difficulty":"advanced","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","Oracle","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T06:03:27.792Z","createdAt":"2026-01-16T21:35:14.434Z"},{"id":"q-3275","question":"In a multi-tenant dbt deployment on Snowflake serving analytics.{tenant}.hourly_metrics from staging.events_raw (tenant_id, user_id, event_type, occurred_at, record_hash), implement a per-tenant, incremental materialization by hour_start and event_type with late data handling of 1 day. Add a tenant-filter macro that enforces isolation by injecting tenant_id filters into every model. Describe isolation, tests, and a macro skeleton with usage and a lineage approach for dashboards?","answer":"Implement a tenant-aware macro that injects tenant_id filters into all models, enabling a single dbt project to serve analytics.{tenant} schemas with strict isolation. Build analytics.{tenant}.hourly_","explanation":"## Why This Is Asked\n\nReal-world multi-tenant isolation in a single dbt project with dynamic tenant scoping. The question tests macro design, incremental logic, late data handling, and governance around schema drift.\n\n## Key Concepts\n\n- Macro-driven tenant isolation in dbt\n- Incremental by (hour_start, event_type)\n- Late data handling (1 day)\n- Schema drift guards and tests (not_null, unique)\n- Snapshots and global lineage for dashboards\n\n## Code Example\n\n```sql\n-- Macro skeleton\n{% macro tenant_filter() %}\n  {% if var('tenant_id') is defined %}\n    AND tenant_id = '{{ var(\"tenant_id\") }}'\n  {% endif %}\n{% endmacro %}\n\n-- Example usage\nSELECT\n  hour_start,\n  event_type,\n  COUNT(*) AS cnt\nFROM analytics.{tenant}.hourly_metrics\nWHERE 1=1\n  {{ tenant_filter() }}\nGROUP BY 1,2\n```\n\n## Follow-up Questions\n\n- How would you test the macro across tenants?\n- How would you detect cross-tenant leakage in production?\n","diagram":null,"difficulty":"intermediate","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Databricks","LinkedIn"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T09:39:24.600Z","createdAt":"2026-01-17T09:39:24.600Z"},{"id":"q-3337","question":"Design an intermediate dbt flow in Snowflake to derive analytics.{tenant}.hourly_metrics from staging.events_json (tenant_id, occurred_at, payload VARIANT) with event_type extracted from payload.eventType (and fallback to payload.type) and indexed by hour_start and event_type. Implement an incremental model that handles 2-day late data, adds schema-drift guards, and tests (not_null on hour_start, tenant_id, event_type; unique on (tenant_id, hour_start, event_type)). Include a per-tenant analytics.{tenant}.users snapshot from payload.user.* for cohort analysis. Outline tenant isolation via per-tenant schemas, a global lineage, and a macro for cross-tenant extraction of event_type?","answer":"Implement a per-tenant incremental hourly_metrics model that extracts event_type from payload.eventType (fallback to payload.type) and aggregates active_users by tenant_id/hour_start/event_type. Handl","explanation":"## Why This Is Asked\nTests ability to work with semi-structured data (JSON VARIANT) in Snowflake, extract consistent event_type across tenants, and build incremental models with late-arrival data. It also probes macro design for per-tenant isolation and lineage visibility in dashboards, plus snapshots for cohort analysis.\n\n## Key Concepts\n- Snowflake VARIANT parsing and lateral flatten for nested payloads\n- Incremental dbt by hour_start and event_type with late data handling\n- Schema-drift guards and tests (not_null, unique)\n- Snapshots for per-tenant user cohorts from payload.user.*\n- Tenant isolation via per-tenant schemas and a macro to enforce filters\n\n## Code Example\n```javascript\n-- dbt SQL for hourly_metrics (skeleton)\nwith src as (\n  select\n    tenant_id,\n    date_trunc('hour', occurred_at) as hour_start,\n    coalesce((payload:eventType)::string, (payload:type)::string) as event_type,\n    (payload:userId) as user_id\n  from {{ source('staging','events_json') }}\n  where occurred_at >= dateadd(day, -2, current_timestamp())\n)\n, agg as (\n  select\n    tenant_id,\n    hour_start,\n    lower(trim(event_type)) as event_type,\n    count(distinct user_id) as active_users\n  from src\n  group by 1,2,3\n)\nselect * from agg\n```\n\n## Follow-up Questions\n- How would you test the cross-tenant event_type extraction macro for schema drift?\n- How would you adapt the approach if payloads vary significantly by tenant (e.g., nested fields differ per tenant)?","diagram":"flowchart TD\n  A[staging.events_json] --> B[parse event_type from payload]\n  B --> C[analytics.{tenant}.hourly_metrics]\n  C --> D[analytics.{tenant}.users snapshot]\n  D --> E[dashboards and lineage]","difficulty":"intermediate","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Lyft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T12:57:31.485Z","createdAt":"2026-01-17T12:57:31.485Z"},{"id":"q-3377","question":"Design a beginner dbt flow to compute analytics.{tenant}.daily_user_event_count from staging.events (tenant_id, user_id, event_type, occurred_at). Use per-tenant schemas for isolation. Build an incremental model keyed by day and event_type, counting distinct users. Implement 1-day late data tolerance. Create a data-contract macro validating required fields and that event_type values are whitelisted per-tenant via dimensions.tenants.event_whitelist. Add tests: not_null on tenant_id, user_id, occurred_at, event_type; unique on (tenant_id, occurred_at_date, event_type). Include a snapshot analytics.{tenant}.users to track first_seen/last_seen. Explain cross-tenant lineage and how to add a new tenant without touching existing tenants?","answer":"I would implement a tenant-aware macro to render analytics.{tenant}.daily_user_event_count from staging.events, with an incremental key (tenant_id, date(occurred_at), event_type). Late data 1 day via ","explanation":"Why asked: tests enforce practical dbt discipline—contracts, macros, and per-tenant governance. Key ideas: incremental, per-tenant isolation via schemas; data-contracts with whitelists; late-data handling; and a users snapshot for SCD-like state. Trade-offs: macro complexity vs governance; scalable tenant onboarding via metadata.","diagram":null,"difficulty":"beginner","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","NVIDIA","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T13:51:36.043Z","createdAt":"2026-01-17T13:51:36.043Z"},{"id":"q-3432","question":"Describe an automated, tenant-aware dbt workflow that discovers tenants from a tenants table, creates per-tenant schemas, and materializes analytics.{tenant}.interactions incrementally from staging.events (tenant_id, user_id, interaction_type, occurred_at). Include late data tolerance of 2 days, per-tenant lineage via a global analytics.lineage table, and macro-driven schema-drift guards plus tests (not_null on occurred_at and user_id; unique on (tenant_id, occurred_at, user_id, interaction_type)); explain how isolation is enforced and how you would validate changes across tenants?","answer":"Leverage a tenants registry to auto-generate per-tenant analytics schemas and an incremental model analytics.{tenant}.interactions from staging.events. Late data tolerance: 2 days. Use a tenant-aware ","explanation":"## Why This Is Asked\n\nTests ability to architect an automated, tenant-aware dbt workflow that generates per-tenant models, ensures data isolation, and enforces governance via lineage.\n\n## Key Concepts\n\n- tenant registry-driven model generation with dbt macros\n- per-tenant schemas for isolation\n- late-data handling and drift guards\n- dynamic tests tied to tenant flags\n- global analytics.lineage for provenance\n\n```javascript\n// Pseudocode sketch of a tenant-scaffold macro\nfunction generateTenantModel(tenant){\n  const schema = `analytics_${tenant}`;\n  return `create schema if not exists ${schema}; create table ${schema}.interactions ...`;\n}\n```\n\n## Follow-up Questions\n\n- How would you detect cross-tenant data leakage?\n- How would you monitor drift and automatically regen models per new tenant?","diagram":"flowchart TD\n  A[Tenant Registry] --> B[Model Generator]\n  B --> C[analytics_{tenant} Schemas]\n  C --> D[analytics.{tenant}.interactions]\n  D --> E[analytics.lineage]","difficulty":"advanced","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Plaid","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T15:43:18.856Z","createdAt":"2026-01-17T15:43:18.857Z"},{"id":"q-3470","question":"Design a beginner dbt flow to compute per-tenant daily metrics from staging.events that include an is_deleted flag for tombstones. Build analytics.{tenant}.daily_metrics by day_start (date of occurred_at) and event_type for rows where is_deleted = false; also create a per-tenant analytics.{tenant}.users snapshot with last_seen. Ensure incremental processing with tenant isolation (per-tenant schemas), plus not_null and unique tests and a data-contract macro. Explain tombstone handling and lineage?","answer":"To implement: use day_start = date_trunc('day', occurred_at); aggregate active_users = count(distinct user_id) where is_deleted = false; implement incremental by tenant schema; create analytics.{tenan","explanation":"## Why This Is Asked\n\nTests understanding of beginner-friendly dbt patterns in a multi-tenant context, focusing on incremental flows, simple tombstone logic, and data contracts.\n\n## Key Concepts\n\n- Multi-tenant isolation via per-tenant schemas\n- Incremental models and day-based partitioning\n- Tombstones with is_deleted and its impact on active_users\n- Snapshot usage for users with last_seen\n- Data-contract macros and basic tests (not_null, unique)\n\n## Code Example\n\n```sql\n-- Incremental daily_metrics (illustrative)\nwith src as (\n  select tenant_id, user_id, event_type, occurred_at, is_deleted\n  from staging_events\n)\nselect\n  tenant_id,\n  date_trunc('day', occurred_at) as day_start,\n  event_type,\n  count(distinct case when is_deleted = false then user_id end) as active_users\nfrom src\nwhere is_deleted = false\ngroup by 1,2,3\n```\n\n```sql\n-- Snapshot concept (last_seen)\nselect\n  tenant_id,\n  user_id,\n  max(occurred_at) as last_seen\nfrom staging_events\ngroup by 1,2\n```\n\n## Follow-up Questions\n\n- How would you handle late tombstones that retroactively remove users from prior days?\n- How would you validate and monitor per-tenant lineage across dbt models?","diagram":null,"difficulty":"beginner","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Microsoft","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T17:36:26.931Z","createdAt":"2026-01-17T17:36:26.931Z"},{"id":"q-3627","question":"Design a beginner dbt workflow to build analytics.country_event_summary per tenant. Source: staging.events(tenant_id, user_id, event_type, occurred_at, country_code). Create analytics.{tenant}.country_event_summary incrementally by day, counting distinct users per event_type and country_code. Enrich with dimensions.countries. Implement late data handling for up to 2 days. Add basic tests: not_null on tenant_id, day, country_code, event_type; unique on (tenant_id, day, event_type, country_code, user_id). Include a simple snapshot analytics.{tenant}.users to capture last_known_country and explain how you verify cross-tenant isolation and lineage?","answer":"Build an incremental model `analytics.{tenant}.country_event_summary` from `staging.events`, grouping by `tenant_id`, `day`, `event_type`, and `country_code` while counting distinct `user_id`. Join with `dimensions.countries` for enrichment, implement late data handling with a 2-day lookback window, and add data quality tests including not_null constraints on `tenant_id`, `day`, `country_code`, `event_type` and unique constraints on `(tenant_id, day, event_type, country_code, user_id)`. Create a simple snapshot `analytics.{tenant}.users` to capture `last_known_country` for slowly changing dimensions.","explanation":"## Why This Is Asked\n\nTests fundamentals of incremental modeling, late data handling, basic data quality tests, snapshots for slowly changing dimensions, and lineage/isolation in a multi-tenant dbt setup. It also reinforces practical enrichment with a dimension table.\n\n## Key Concepts\n\n- Incremental dbt models by day\n- Late data tolerance (2 days)\n- Data quality tests (not_null, unique)\n- Dimension enrichment (dimensions.countries)\n- Snapshots for slowly changing attributes (last_known_country)\n- Cross-tenant lineage and isolation via schemas and docs\n\n## Code Example\n\n```sql\n-- illustrative example","diagram":"flowchart TD\n  A[staging.events] --> B[analytics.{tenant}.country_event_summary]\n  C[dimensions.countries] --> B\n  B --> D[analytics.{tenant}.users_snapshot]\n  B --> E[dbt docs lineage]","difficulty":"beginner","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","IBM","LinkedIn"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T05:07:54.387Z","createdAt":"2026-01-18T02:30:25.090Z"},{"id":"q-3669","question":"Design a multi-tenant dbt pipeline for a Discord/Snap like platform that computes analytics.{tenant}.event_summary from staging.events (tenant_id, user_id, event_type, occurred_at, country). Build an incremental model by hour and event_type with 1 day late data, tenant isolation via per tenant schemas, and a tenant-filter macro; add dynamic schema drift guards and tests (not_null on hour_start, tenant_id, event_type; unique on (tenant_id, hour_start, event_type)); include per-tenant analytics.{tenant}.users snapshot, analytics.lineage, and analytics.{tenant}.alerts for anomalies; outline testing and alerting strategy?","answer":"Propose a per-tenant isolation with analytics.{tenant} schemas, an hourly incremental by hour_start and event_type, 1-day late data, macro to inject tenant filters, dynamic schema drift guards and tes","explanation":"## Why This Is Asked\n\nTests ability to design scalable multi-tenant dbt flows with strict isolation, drift guards, lineage, and alerting. Must show pragmatic handling of late data, per-tenant materializations, and automated tests.\n\n## Key Concepts\n\n- Multi-tenant isolation via per-tenant schemas\n- Incremental models by hour_start and event_type\n- Late data handling (1 day)\n- Tenant-filter macro for consistent isolation\n- Dynamic schema drift guards and tests (not_null, unique)\n- Snapshots for user cohorts per tenant\n- Central lineage exposure for dashboards\n- Per-tenant alerts for anomalies\n\n## Code Example\n\n```jinja\n{% macro tenant_filter(table) %}\n  {% if var('tenant_id') is defined %}\n    SELECT * FROM {{ table }} WHERE tenant_id = '{{ var('tenant_id') }}'\n  {% else %}\n    SELECT * FROM {{ table }}\n  {% endif %}\n{% endmacro %}\n```\n\n```sql\n-- sample: ensure a per-tenant incremental by hour_start\nwith source as (\n  select tenant_id, user_id, event_type, date_trunc('hour', occurred_at) as hour_start, country\n  from {{ ref('staging__events') }}\n  where tenant_id = '{{ var('tenant_id', '') }}'\n)\nselect * from source\n```\n\n## Follow-up Questions\n\n- How would you scale this for thousands of tenants with varying data volumes?\n- How would you automate schema-drift guard updates and test coverage across tenants?","diagram":"flowchart TD\n  S[staging.events] --> E[analytics.{tenant}.event_summary]\n  E --> L[analytics.lineage]\n  E --> A[analytics.{tenant}.alerts]\n  U[analytics.{tenant}.users] --> L","difficulty":"intermediate","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T04:20:02.856Z","createdAt":"2026-01-18T04:20:02.856Z"},{"id":"q-3755","question":"Design a beginner-friendly multi-tenant dbt exercise in Snowflake: for each tenant, build analytics.{tenant}.daily_summary from staging.events (tenant_id, user_id, event_type, occurred_at) enriched by dimensions.countries, producing daily counts by event_type per tenant. Implement a 2-day late data window, per-tenant isolation via schemas, and tests: not_null on tenant_id, occurred_at, event_type; unique on (tenant_id, occurred_at, event_type); a per-tenant data_contract that country_code exists in countries; and a cross-tenant leakage test ensuring analytics.{tenant} only reads its own data. Outline approach and tests?","answer":"Implement a tenant-scoped daily_summary as an incremental analytics.{tenant}.daily_summary built from staging.events (tenant_id, user_id, event_type, occurred_at) joined to dimensions.countries, with ","explanation":"## Why This Is Asked\n\nAssesses practical dbt setup for multi-tenant isolation, late data handling, and data quality tests.\n\n## Key Concepts\n\n- Tenant-scoped schemas\n- Incremental daily aggregation\n- Data contracts and cross-tenant leakage tests\n- Late data window and watermarking\n\n## Code Example\n\n```javascript\n-- Pseudo dbt macro usage snippet\n```\n\n## Follow-up Questions\n\n- How would you automate tenant onboarding in this pattern?\n- How would you monitor data freshness across tenants?","diagram":null,"difficulty":"beginner","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Snowflake","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T08:39:33.108Z","createdAt":"2026-01-18T08:39:33.109Z"},{"id":"q-3815","question":"Design an intermediate dbt workflow on Snowflake for a federated analytics layer with per-tenant schemas (tenant_<id>). Staging.events_raw has (tenant_id, user_id, event_type, occurred_at, payload VARIANT). Build analytics.hourly_metrics as an incremental model by hour_start and event_type across tenants, resolving schemas via a macro. Include 1-day late data, schema-drift guards, tests (not_null on hour_start, tenant_id; unique on tenant_id/hour_start/event_type), and a global analytics.lineage table; describe per-tenant isolation and dashboard exposure?","answer":"Leverage a single incremental model analytics.hourly_metrics across tenants, using a macro to resolve tenant_schema(tenant_id) for source and destination. Compute hour_start as date_trunc('hour', occu","explanation":"## Why This Is Asked\n\nAssesses multi-tenant dbt design, dynamic schema routing, and cross-tenant governance on Snowflake; tests ability to implement late data handling and lineage.\n\n## Key Concepts\n\n- Federated multi-tenant patterns in dbt\n- Macro-based dynamic tenant_schema resolution\n- Incremental models by hour_start and event_type\n- Late data handling, schema drift guards, and data leakage prevention\n- Global lineage table and per-tenant isolation\n\n## Code Example\n\n```sql\n-- Example macro (simplified)\n{% macro tenant_schema(tenant_id) %}\n  -- resolves to tenant_<id> schema\n  {{ return('analytics.tenant_' ~ tenant_id) }}\n{% endmacro %}\n```\n\n## Follow-up Questions\n\n- How would you test for cross-tenant data leakage?\n- How would onboarding new tenants affect schema and lineage guarantees?","diagram":"flowchart TD\n  S[staging.events_raw]\n  H[analytics.hourly_metrics]\n  L[analytics.lineage]\n  S --> H\n  H --> L","difficulty":"intermediate","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T10:38:47.652Z","createdAt":"2026-01-18T10:38:47.652Z"},{"id":"q-3849","question":"In a multi-tenant dbt deployment on Snowflake, implement an anomaly detection layer: analytics.{tenant}.anomalies derived from staging.events_raw (tenant_id, user_id, event_type, occurred_at, value). Build a per-tenant rolling z-score per event_type, daily incremental, with 2-day late data, and add schema-drift guards and tests. Provide a tenant-scoped macro that injects filters into all models and a simple lineage snapshot for dashboards. How would you implement this end-to-end?","answer":"Design a per-tenant anomaly layer with a rolling 7- or 14-day z-score per event_type, compute per-tenant anomaly_flag, and incrementally materialize analytics.{tenant}.anomalies by day. Include 2-day ","explanation":"## Why This Is Asked\n\nTests ability to design per-tenant isolation, a practical anomaly-detection workflow in dbt, and macro-based scoping with lineage considerations.\n\n## Key Concepts\n\n- Per-tenant isolation via dynamic macros and schemas\n- Anomaly detection: rolling z-score per tenant_id and event_type\n- Incremental models with late-arriving data (2 days)\n- Schema drift guards and dbt tests (not_null, value, and flags)\n- Tenant-scoped macros for isolation; lineage snapshots for dashboards\n\n## Code Example\n\n```sql\n-- Skeleton macro to scope a model by tenant\n{% macro tenant_scope(relation, tenant_id) %}\n  SELECT * FROM {{ relation }} WHERE tenant_id = '{{ tenant_id }}'\n{% endmacro %}\n```\n\n```sql\n-- Pseudo-aggregation for anomaly score (illustrative)\nWITH base AS (\n  SELECT tenant_id, event_type, occurred_at, value,\n         AVG(value) OVER (PARTITION BY tenant_id, event_type ORDER BY occurred_at ROWS BETWEEN 13 PRECEDING AND CURRENT ROW) AS mean_14d,\n         STDDEV_SAMP(value) OVER (PARTITION BY tenant_id, event_type ORDER BY occurred_at ROWS BETWEEN 13 PRECEDING AND CURRENT ROW) AS sd_14d\n  FROM {{ source('staging','events_raw') }}\n)\nSELECT *, CASE WHEN sd_14d > 0 THEN (value - mean_14d) / sd_14d ELSE NULL END AS zscore\nFROM base\n```\n\n## Follow-up Questions\n\n- How would you test the rolling z-score across tenants with varying data scales?\n- What strategies would you use to scale the tenant-scoped macro as tenant count grows?","diagram":null,"difficulty":"intermediate","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","DoorDash"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T12:55:56.712Z","createdAt":"2026-01-18T12:55:56.713Z"},{"id":"q-3939","question":"Design a beginner dbt flow in Snowflake to compute analytics.{tenant}.daily_user_engagement from staging.events (tenant_id, user_id, event_type, occurred_at, country_code), enriched by dimensions.countries and dimensions.products; produce per-tenant daily counts by event_type and country, with 2-day late data handling, per-tenant schema isolation, schema-drift guards, and tests (not_null on tenant_id, user_id, occurred_at; unique on (tenant_id, occurred_at, user_id)); include a snapshot analytics.{tenant}.users to capture cohort changes and a data-contract macro validating staging fields; explain cross-tenant lineage and privacy considerations?","answer":"Propose a per-tenant incremental model analytics.{tenant}.daily_user_engagement, scoped to a tenant-specific schema. Build incrementally by day, joining staging.events with countries and products dime","explanation":"## Why This Is Asked\nTests ability to design per-tenant, incremental dbt flows with late data, tests, snapshots, and isolation, plus governance via data contracts.\n\n## Key Concepts\n- Incremental, tenant-scoped models in Snowflake\n- Late-arriving data handling (2 days)\n- Schema-drift guards and tests (not_null, unique)\n- Snapshots for cohort tracking\n- Data-contract macro to validate sources\n- Cross-tenant lineage and privacy considerations\n\n## Code Example\n```sql\n-- simplified incremental model sketch\nwith base as (\n  select tenant_id, user_id, event_type, occurred_at, country_code\n  from {{ source('staging','events') }}\n  where occurred_at >= dateadd(day, -2, current_date())\n)\nselect\n  tenant_id,\n  date(occurred_at) as day,\n  event_type,\n  country_code,\n  count(distinct user_id) as active_users\nfrom base\ngroup by 1,2,3,4\n```\n\n## Follow-up Questions\n- How would you implement the data-contract macro to validate staging fields before run?\n- What are the trade-offs of per-tenant schema isolation versus a shared schema with tenant_id partitioning?","diagram":null,"difficulty":"beginner","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","NVIDIA","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T16:35:05.564Z","createdAt":"2026-01-18T16:35:05.564Z"},{"id":"q-4033","question":"Design a per-tenant incremental model in Snowflake that computes analytics.{tenant}.retention_daily from staging.events (tenant_id, user_id, event_date, first_seen, country_code). Include a macro-driven approach to switch between per-tenant daily retention and rolling-7-day retention, handle late data up to 3 days, enforce per-tenant schema isolation, and add tests (not_null on tenant_id, user_id, event_date; unique on (tenant_id, user_id, event_date)). Also add a snapshot analytics.{tenant}.users_cohort for first_seen and country changes. Explain how you would implement cross-tenant lineage and per-tenant data quality checks?","answer":"Implement a per-tenant macro system that dynamically configures retention_window_days (1 or 7) and processing mode (daily or rolling). Build analytics.{tenant}.retention_daily incrementally from staging.events (tenant_id, user_id, event_date, first_seen, country_code) with a 3-day lookback window to handle late-arriving data. Enforce per-tenant schema isolation using Snowflake schemas and create analytics.{tenant}.users_cohort snapshot to track first_seen and country_code changes over time. Apply comprehensive data quality tests including not_null constraints on tenant_id, user_id, event_date and unique constraints on (tenant_id, user_id, event_date).","explanation":"## Why This Is Asked\nAssesses practical multi-tenant dbt design capabilities including macro-driven per-tenant logic, incremental processing with late data handling, and governance artifacts.\n\n## Key Concepts\n- Macros for tenant-specific configuration and logic switching\n- Incremental models with late-arriving data handling\n- Per-tenant schemas and snapshots for data isolation\n- Data quality tests and cross-tenant lineage tracking\n\n## Code Example\n```javascript\n// Placeholder implementation\n```\n\n## Follow-up Questions\n- How would you test macro correctness across tenants with different retention windows?\n- What strategies would you use for cross-tenant lineage tracking?\n- How would you implement per-tenant data quality checks at scale?","diagram":null,"difficulty":"advanced","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Hashicorp","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T07:07:22.919Z","createdAt":"2026-01-18T20:45:06.202Z"},{"id":"q-4103","question":"Design a multi-tenant incremental dbt flow in Snowflake that computes analytics.{tenant}.monthly_cost from staging.sales_events (tenant_id, cost_cents, currency, billed_at, event_hash). Build analytics.{tenant}.monthly_cost by (tenant_id, month_start) with 30-day late_arrival tolerance; implement currency conversion using a per-tenant rates table; handle null currencies. Provide schema-drift guards and tests (not_null on month_start and tenant_id; unique on (tenant_id, month_start)); add a tenant-scoping macro routing models to per-tenant schemas and a global analytics.lineage. Describe testing strategy and cross-tenant isolation?","answer":"Design a multi-tenant incremental dbt flow in Snowflake that computes analytics.{tenant}.monthly_cost from staging.sales_events (tenant_id, cost_cents, currency, billed_at, event_hash). Build analytics.{tenant}.monthly_cost by (tenant_id, month_start) with 30-day late-arrival tolerance; implement currency conversion using a per-tenant rates table; handle null currencies. Provide schema-drift guards and tests (not_null on month_start and tenant_id; unique on (tenant_id, month_start)); add a tenant-scoping macro routing models to per-tenant schemas and a global analytics.lineage. Describe testing strategy and cross-tenant isolation?","explanation":"## Why This Is Asked\nTests ability to build robust, tenant-aware dbt pipelines that handle late data, currency conversion, and schema drift while preserving cross-tenant isolation.\n\n## Key Concepts\n- Per-tenant schemas with macro-based routing\n- Incremental processing by month_start with late-arrival tolerance\n- Currency normalization via per-tenant rates table\n- Schema-drift guards and comprehensive data quality tests\n- Global lineage table for cross-tenant analytics and dashboards\n\n## Code Example\n```sql\n-- macro skeleton for tenant scoping\n{% macro tenant_model(model_name, tenant) -%}\n  {{\n\n```","diagram":"flowchart TD\n  A[staging.sales_events] --> B[per-tenant analytics.{tenant}.monthly_cost]\n  B --> C[per-tenant schema routing via macro]\n  C --> D[global analytics.lineage]\n  D --> E[dashboards/consumers]","difficulty":"intermediate","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Amazon","Anthropic"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T05:32:07.326Z","createdAt":"2026-01-18T23:46:35.648Z"},{"id":"q-4274","question":"In a multi-tenant Snowflake dbt project, staging.events has tenant_id, user_id, event_type, occurred_at. Create an incremental analytics.{tenant}.daily_events that counts events per tenant/day by event_type from staging.events. Implement 1-day late data, per-tenant schemas via a macro, and a lightweight analytics.lineage table. Tests: not_null on key columns and unique on (tenant_id, day, event_type). How would you ensure isolation and maintainability?","answer":"Design an incremental per-tenant model analytics.{tenant}.daily_events from staging.events, aggregating by tenant_id, date(occurred_at), event_type. Allow 1-day late data. Use a macro to render per-te","explanation":"## Why This Is Asked\n\nThis question probes ability to implement a simple, tenant-aware incremental model with light data-quality checks, while introducing macros and a lineage artifact to address maintainability and isolation—common beginner tasks in real dbt workflows at scale.\n\n## Key Concepts\n\n- Incremental models with late data handling\n- Tenant-scoped schemas and macros\n- Basic data-contract tests\n- Lightweight lineage tracking for cross-tenant visibility\n\n## Code Example\n\n```sql\n-- Example incremental model outline\nwith src as (\n  select tenant_id, cast(date(occurred_at) as date) as day, event_type\n  from {{ source('staging', 'events') }}\n)\nselect tenant_id, day, event_type, count(*) as events\nfrom src\ngroup by tenant_id, day, event_type\n{% if is_incremental() %}\n  where day < (select max(day) from analytics.lineage where tenant_id = tenant_id)\n{% endif %}\n```\n\n## Follow-up Questions\n\n- How would you implement a generic tests macro to enforce required fields for all tenants?\n- How would you scale the lineage table as tenants are added?","diagram":null,"difficulty":"beginner","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","MongoDB","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T11:03:06.052Z","createdAt":"2026-01-19T11:03:06.052Z"},{"id":"q-4488","question":"Design a beginner dbt workflow in Snowflake to compute analytics.{tenant}.daily_user_activity from staging.events (tenant_id, user_id, action, occurred_at, country_code). Normalize occurred_at to UTC, bucket by day, and count distinct users per action. Include 1-day late data, per-tenant schema isolation, tests (not_null on keys, unique on (tenant_id, date, action)); add a data-contract macro to validate staging.events schema; snapshot analytics.{tenant}.users for country churn, and outline per-tenant lineage into analytics.lineage?","answer":"Implement an incremental model analytics.{tenant}.daily_user_activity using staging.events per tenant, UTC-normalize occurred_at to date, and aggregate distinct user_id by action. Add tests: not_null ","explanation":"## Why This Is Asked\nInterview context explanation.\n\n## Key Concepts\n- Incremental models and unique keys per tenant\n- Timezone normalization (UTC) and date bucketing\n- Data contracts and schema tests\n\n## Code Example\n```javascript\n{{ config(materialized='incremental', unique_key='(tenant_id, date, action)') }}\nwith s as (\n  select tenant_id, user_id, action, occurred_at, country_code\n  from {{ source('staging','events') }}\n)\nselect\n  tenant_id,\n  date_trunc('day', occurred_at AT TIME ZONE 'UTC')::date as date,\n  action,\n  count(distinct user_id) as user_count\nfrom s\ngroup by 1,2,3\n{% if is_incremental() %}\n  where occurred_at > (select max(occurred_at) from {{ this }})\n{% endif %}\n```\n\n## Follow-up Questions\n- How would you unit-test the data-contract macro to ensure required fields exist?\n- How would you validate that per-tenant snapshots reflect churn in analytics.{tenant}.users over time?","diagram":null,"difficulty":"beginner","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Snowflake","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T20:49:54.514Z","createdAt":"2026-01-19T20:49:54.514Z"},{"id":"q-4686","question":"In a beginner dbt project for multi-tenant analytics on Snowflake, staging.events(tenant_id, user_id, event_type, occurred_at, record_hash) feeds analytics.{tenant}.daily_metrics. Propose an incremental approach that uses a per-tenant schema, handles 2-day late data, and uses a MERGE-based dedup strategy on (tenant_id, day, event_type) while preserving idempotence. Include a macro for tests and a snapshot of analytics.{tenant}.users. Explain cross-tenant lineage and error handling?","answer":"Propose a per-tenant incremental daily_metrics using MERGE-based upserts on (tenant_id, day, event_type) to deduplicate by record_hash, with 2-day late data. Enforce per-tenant isolation via schemas, ","explanation":"## Why This Is Asked\nTests ability to design per-tenant isolation with dbt incremental loads, handle late data, and implement dedup via MERGE. Also checks macro-driven tests and snapshots for user state.\n\n## Key Concepts\n- Incremental models with unique keys per tenant\n- Snowflake MERGE for idempotent upserts\n- Per-tenant schema isolation\n- Late data tolerance (2 days)\n- Snapshots for users to capture cohort changes\n- Macro-driven tests and data contracts for governance\n\n## Code Example\n```javascript\n// Pseudo dbt macro sketch for per-tenant MERGE upsert\n{% macro upsert_daily_metrics(tenant_id, day) -%}\nMERGE INTO analytics.{{ tenant_id }}.daily_metrics AS t\nUSING (\n  SELECT tenant_id, DATE(occurred_at) AS day, event_type,\n         COUNT(DISTINCT user_id) AS active_users, MAX(record_hash) AS max_hash\n  FROM {{ ref('staging_events') }}\n  WHERE tenant_id = '{{ tenant_id }}' AND occurred_at >= DATE('{{ day }}') - INTERVAL '2 days'\n  GROUP BY 1,2,3\n) AS s\nON (t.tenant_id = s.tenant_id AND t.day = s.day AND t.event_type = s.event_type)\nWHEN MATCHED THEN UPDATE SET active_users = s.active_users\nWHEN NOT MATCHED THEN INSERT (tenant_id, day, event_type, active_users) VALUES (s.tenant_id, s.day, s.event_type, s.active_users);\n{% endmacro %}\n```\n\n## Follow-up Questions\n- How would you validate idempotence across repeated runs?\n- How would you audit and monitor tenant isolation and lineage?","diagram":"flowchart TD\n  A[staging.events] --> B[analytics.{tenant}.daily_metrics]\n  B --> C[analytics.lineage?]\n  D[tenants] --> E[isolation via schemas]\n  F[dbt tests] --> G[data contracts]","difficulty":"beginner","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Snap","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T07:54:23.561Z","createdAt":"2026-01-20T07:54:23.562Z"},{"id":"q-4747","question":"Design a per-tenant, incremental dbt flow in Snowflake that computes analytics.{tenant}.hourly_engagement from staging.tenant_events (tenant_id, user_id, action, occurred_at). Build via a macro that creates analytics_{{tenant}} schemas and analytics_{{tenant}}.hourly_engagement, plus a global analytics.lineage table to track dependencies. Include 1-day late data, schema-drift guards, and tests (not_null on keys, unique on (tenant_id, hour_start, user_id)). Also snapshot analytics.{tenant}.users for churn. Explain how you would validate cross-tenant isolation in CI and prevent bleed?","answer":"Automate a per-tenant, incremental dbt flow: a macro creates analytics_{{tenant}} schemas and builds analytics_{{tenant}}.hourly_engagement from staging_{{tenant}}_events (tenant_id, user_id, action, ","explanation":"## Why This Is Asked\nTests macro-driven tenant isolation, lineage governance, and late-arrival handling in a scalable dbt fintech analytics pipeline.\n\n## Key Concepts\n- Per-tenant macros and schemas\n- Incremental models with hourly windows\n- Late data tolerance and drift guards\n- Global lineage table with tenant scoping\n- Snapshot for churn and tests for isolation\n\n## Code Example\n```javascript\n// pseudo dbt macro example\n```\n\n## Follow-up Questions\n- How would you test cross-tenant lineage changes in CI?\n- What are failure modes if a tenant schema migrates to a new cluster?","diagram":null,"difficulty":"advanced","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Google","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T10:44:11.440Z","createdAt":"2026-01-20T10:44:11.440Z"},{"id":"q-4859","question":"Design a multi-tenant dbt workflow on Snowflake to compute analytics.{tenant}.daily_engagement from staging.tenant_events, enforcing per-tenant isolation with a runtime macro that injects tenant filters and exposes per-tenant views in a shared schema. Include PII masking, a lineage audit table updated post-run, 1-day late data, and CI tests for isolation and drift guards. Outline macro skeletons and test ideas?","answer":"Implement a tenant_context var and a tenant_filter(ctx) macro that appends tenant_id = {{ var('TENANT') }} to every model, expose analytics.public.engagement_<tenant> views, apply Snowflake masking on","explanation":"## Why This Is Asked\n\nAssesses practical multi-tenant isolation, dynamic scoping, and governance in dbt/Snowflake with runtime filtering and shared schemas.\n\n## Key Concepts\n\n- Tenant-scoped runtime filtering via dbt macros\n- Shared schema with per-tenant views\n- Snowflake masking policies on PII\n- Post-run hooks for lineage auditing\n- Late data handling (1 day) and drift guards\n- CI tests ensuring isolation\n\n## Code Example\n\n```javascript\n{% macro tenant_filter() -%}\n  {% if var('TENANT') %}\n    tenant_id = '{{ var('TENANT') }}'\n  {% else %}\n    1=0\n  {% endif %}\n{%- endmacro %}\n```\n\n## Follow-up Questions\n\n- How would you test macro correctness and prevent leaks?\n- How would you scale to many tenants without O(n) views?","diagram":null,"difficulty":"intermediate","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Robinhood","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T16:47:49.945Z","createdAt":"2026-01-20T16:47:49.945Z"},{"id":"q-4894","question":"Design a per-tenant churn-risk dbt pipeline in Snowflake. From staging.tenant_actions (tenant_id, user_id, action, occurred_at) and staging.tenant_users (tenant_id, user_id, signup_date), build an incremental model that computes analytics.{tenant}.churn_risk using features like last_seen_days, days_since_signup, and average_session_minutes (derived from event streams). Create a per-tenant analytics.{tenant}.schema and a global analytics.lineage table. Include 3-day late data handling, not_null on keys, and a min_score guard. Outline tests, a simple macro for tenant onboarding, and CI checks to prevent cross-tenant leakage?","answer":"Implement an incremental dbt model per tenant that derives churn_risk from tenant_actions and tenant_users. Upsert analytics.{tenant}.churn_risk by tenant_id and user_id, compute features (last_seen_d","explanation":"## Why This Is Asked\nTests ability to architect multi-tenant churn analytics with late data, isolation, and governance beyond basic models.\n\n## Key Concepts\n- Per-tenant schemas and global lineage\n- Incremental upserts with MERGE in Snowflake\n- Feature engineering for churn signals\n- Late-arrival handling and schema-drift guards\n- CI governance for tenant onboarding\n\n## Code Example\n```javascript\n// dbt model sketch: churn_risk.sql\nwith staging as (\n  select tenant_id, user_id, max(occurred_at) as last_seen\n  from {{ ref('staging__tenant_actions') }}\n  group by 1,2\n)\nselect t.tenant_id, t.user_id,\n  datediff('day', signup_date, last_seen) as last_seen_days,\n  datediff('day', signup_date, current_date()) as signup_age_days,\n  avg_session_minutes\nfrom staging s\njoin {{ ref('staging__tenant_users') }} u on s.tenant_id = u.tenant_id and s.user_id = u.user_id\n```\n\n## Follow-up Questions\n- How would you test cross-tenant isolation and lineage drift in CI?\n- How would you onboard a new tenant without downtime to analytics schemas?","diagram":"flowchart TD\nA[staging.tenant_actions] --> B[dbt churn_risk model]\nB --> C[analytics.{tenant}.churn_risk]\nB --> D[analytics.lineage]\nA2[staging.tenant_users] --> B","difficulty":"advanced","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T17:51:30.201Z","createdAt":"2026-01-20T17:51:30.201Z"},{"id":"q-4939","question":"Design a multi-tenant Snowflake dbt flow that computes analytics.{tenant}.hourly_metrics from staging.events_raw (tenant_id, user_id, event_type, occurred_at, email, phone, record_hash) and enforces per-tenant data redaction for PII. Implement a macro redaction_policy(tenant_id) that reads a central tenant_config(redact_pii) and injects masking for email/phone when enabled. Include tests ensuring redacted fields are masked and that tenant lineage remains intact. How would you implement end-to-end?","answer":"Implement a macro redaction_policy(tenant_id) that reads a central tenant_config(redact_pii). If true, return masking expressions for email and phone; otherwise pass-through. In analytics.{tenant}.hou","explanation":"This question probes practical privacy controls in a multi-tenant dbt setup. The macro should gracefully handle missing config by defaulting to non-redaction, and the tests must cover both opt-in and opt-out tenants. Use a centralized tenant_config table with tenant_id as key; lineage should be updated to reflect redacted fields. Consider Snowflake masking policies as a second line of defense for data at rest. Performance-wise, apply redaction at projection, not late in CTEs, to minimize recomputation.","diagram":null,"difficulty":"intermediate","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Google","LinkedIn"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T20:29:26.113Z","createdAt":"2026-01-20T20:29:26.114Z"},{"id":"q-4976","question":"Design a per-tenant, time-versioned hourly metric pipeline in Snowflake using dbt. Staging.events(tenant_id, user_id, event_type, occurred_at) feeds analytics.{tenant}.hourly_metrics. Implement an incremental model that stores historical versions via effective_from/effective_to and supports late data up to 2 days. Build a macro to generate per-tenant schemas and a global analytics.lineage, plus tests ensuring non-null keys and non-overlapping validity intervals. Also outline a rollback-by-timestamp mechanism and CI validation strategy?","answer":"Implement per-tenant hourly metrics with time-versioning: compute hour_start, store effective_from/effective_to, and adjust overlaps when late data arrives (≤2 days). Use a macro to generate analytics schemas and ensure data integrity through proper versioning.","explanation":"Why This Is Asked\nThis question probes time-versioned, multi-tenant dbt design with late data handling, drift resilience, and lineage governance. It also tests how to implement safe rollback paths in CI environments.\n\nKey Concepts\n- Time-versioned rows with effective_from/effective_to to support late data\n- Incremental per-tenant materializations and dynamic schema generation\n- Global lineage table and strict interval non-overlaps for correctness\n- Rollback-by-timestamp and CI validation for data freshness and safety\n\nCode Example\n```sql\n-- Pseudo: merge with versioning for a given tenant\nMERGE INTO analytics.${tenant}.hourly_metrics AS target\nUSING (\n    SELECT \n        tenant_id,\n        DATE_TRUNC('hour', occurred_at) AS hour_start,\n        event_type,\n        COUNT(*) AS metric_value,\n        CURRENT_TIMESTAMP() AS effective_from,\n        NULL AS effective_to\n    FROM staging.events\n    WHERE occurred_at >= DATEADD('day', -2, CURRENT_TIMESTAMP())\n    GROUP BY tenant_id, hour_start, event_type\n) AS source\nON target.tenant_id = source.tenant_id \n   AND target.hour_start = source.hour_start\n   AND target.event_type = source.event_type\n   AND target.effective_to IS NULL\nWHEN MATCHED THEN UPDATE SET\n    target.effective_to = source.effective_from\nWHEN NOT MATCHED THEN INSERT (tenant_id, hour_start, event_type, metric_value, effective_from, effective_to)\nVALUES (source.tenant_id, source.hour_start, source.event_type, source.metric_value, source.effective_from, source.effective_to);\n```","diagram":null,"difficulty":"intermediate","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-21T04:36:14.790Z","createdAt":"2026-01-20T21:59:36.892Z"},{"id":"q-4978","question":"Design a multi-tenant dbt flow in Snowflake that computes per-tenant daily revenue from staging.sales and staging.refunds. Create a macro that materializes analytics_{tenant}.daily_revenue and a global analytics.lineage; support 1-day late data, idempotent upserts, and tests (not_null, unique on tenant_id/day/order_id, revenue >= 0). Include per-tenant customers snapshot and outline CI isolation and backfill strategy?","answer":"Use a macro to materialize analytics_{{tenant}}.daily_revenue by incrementally merging staging.sales and staging.refunds per day. Build a global analytics.lineage to track dependencies. Backfill with tenant-specific date ranges and validate isolation in CI using separate schemas.","explanation":"Why This Is Asked\nTests multi-tenant data modeling, incremental logic, and data safety in production dbt.\n\nKey Concepts\n- Macro-driven per-tenant materializations\n- Snowflake MERGE upserts for daily revenue\n- Global lineage for auditable dependencies\n- Late-arrival data handling and schema guards\n- Cross-tenant isolation validated in CI\n\nCode Example\n```sql\n{% macro revenue_per_tenant(tenant) %}\nselect\n  date_trunc('day', occurred_at) as day,\n  tenant_id,\n  order_id,\n  sum(amount - coalesce(discount, 0)) as revenue\nfrom {{ ref('staging_sales') }} s\nleft join {{ ref('staging_refunds') }} r\n  on```","diagram":"flowchart TD\n  A[staging.sales] --> B[per-tenant daily_revenue]\n  C[staging.refunds] --> B\n  B --> D[analytics_{tenant}].daily_revenue\n  D --> E[analytics.lineage]\n  F[customers snapshot] --> G[churn metrics]\n  E --> H[BI dashboards]\n  style A fill:#f9f,stroke:#333\n  style B fill:#bbf,stroke:#333\n  style D fill:#9f9,stroke:#333","difficulty":"advanced","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Scale Ai","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-21T06:00:00.109Z","createdAt":"2026-01-20T22:31:21.290Z"},{"id":"q-5003","question":"Design a beginner dbt flow in Snowflake to compute analytics.sales.daily_user_spend from staging.orders (order_id, user_id, amount, created_at) and staging.users (user_id). Aggregate by user_id and date(created_at) to compute total_spend and order_count. Allow 1-day late data; tests: not_null on user_id and order_date; unique on (user_id, order_date). Snapshot analytics.sales.users for first_order_date. Explain local run and CI isolation?","answer":"Implement analytics.sales.daily_user_spend as an incremental model keyed by (user_id, order_date), joining staging.orders with staging.users as needed. Compute total_spend = sum(amount) and order_count = count(*). Configure a 1-day lookback window to handle late-arriving data. Add not_null tests on user_id and order_date, plus a unique test on (user_id, order_date). Create analytics.sales.users snapshot to capture first_order_date for user auditing.","explanation":"## Why This Is Asked\nThis tests building a small, practical dbt flow with incremental logic, late data tolerance, and basic quality checks. It also covers simple snapshot usage to audit user history and CI isolation for environments.\n\n## Key Concepts\n- Incremental materializations in dbt\n- Late-arriving data handling with date window\n- Basic tests: not_null and unique\n- Snapshots for slowly changing data\n- Environment isolation via per-env targets/schemas\n\n## Code Example\n```sql\n-- dbt model: analytics.sales.daily_user_spend\nwith orders as (\n  select \n    order_id, \n    user_id, \n    amount, \n    created_at::date as order_date\n  from {{ ref('staging.orders') }}\n),\nfinal as (\n  select \n    user_id,\n    order_date,\n    sum(amount) as total_spend,\n    count(*) as order_count\n  from orders\n  where order_date >= dateadd(day, -1, '{{ var('run_date') }}')\n  group by user_id, order_date\n)\nselect * from final\n```\n\n## Local vs CI Isolation\nLocal runs use dev schema/target; CI uses separate schema with clean state to prevent cross-environment contamination.","diagram":null,"difficulty":"beginner","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Hugging Face","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-21T05:43:29.835Z","createdAt":"2026-01-20T23:33:26.732Z"},{"id":"q-5080","question":"Design an on-demand, per-tenant export flow in a dbt + Snowflake setup that generates daily export tables analytics_export.{tenant}.daily_export for a given date range. Describe a macro-driven model that materializes exports incrementally by date, uses Snowflake streams for CDC, enforces tenant isolation, and includes tests (row_count within bounds, not_null keys, and tenant_id integrity). Explain how you would validate correctness and security in CI?","answer":"Propose a per-tenant, on-demand export path that materializes analytics_export.{tenant}.daily_export for a specified date range. Describe a macro-driven Snowflake model that creates daily export table","explanation":"## Why This Is Asked\nThis question probes practical multi-tenant export design, combining dbt macros, Snowflake CDC, and tests.\n\n## Key Concepts\n- Macro-driven models for on-demand objects\n- Snowflake streams and tasks for incremental exports\n- Tenants isolation and governance\n- Data quality via tests: not_null, unique, row_count bounds\n- CI validation and security controls\n\n## Code Example\n```sql\n-- macro skeleton\n{% macro export_per_tenant(tenant_id, start_date, end_date) %}\n  -- create analytics_export.{tenant_id}.daily_export\n  -- incremental by date, attach to stream\n{% endmacro %}\n```\n\n## Follow-up Questions\n- How to scale to 100s of tenants without hitting catalog limits?\n- How to enforce least privilege in Snowflake roles for exports?","diagram":"flowchart TD\n  A[dbt macro] --> B[Export table per tenant]\n  B --> C[Incremental by date]\n  C --> D[Snowflake streams (CDC)]\n  D --> E[CI tests & docs]\n  E --> F[Secure access controls]","difficulty":"intermediate","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","Square","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T05:40:20.387Z","createdAt":"2026-01-21T05:40:20.387Z"},{"id":"q-5161","question":"Design a dbt-driven federated analytics layer on Snowflake that enforces per-tenant isolation with Row Access Policies, and uses a macro to auto-create analytics_<tenant> schemas from staging events. Build analytics_<tenant>.hourly_metrics incrementally by hour_start, tenant_id, event_type from staging_events_<tenant>. Include 1-day late data, 7-day retention, and a global analytics.lineage table. Add tests for cross-tenant access (no cross-tenant reads) and a tenant-cost ledger. Explain deployment and lineage auditing?","answer":"Macro-driven per-tenant schema creation; Row Access Policies on tenant_id for isolation; incremental by hour_start, tenant_id, event_type using staging_events_<tenant>; global lineage analytics.lineag","explanation":"## Why This Is Asked\n\nInterview context explanation.\n\n## Key Concepts\n\n- Row Access Policies in Snowflake\n- Macro generation for tenants\n- Incremental models by hour_start\n- Global lineage table and tests for cross-tenant isolation\n\n## Code Example\n\n```sql\n-- skeleton policy and macro usage\nCREATE ROW ACCESS POLICY rap_tenant ON TABLE analytics_{{tenant}}.hourly_metrics USING (tenant_id = CURRENT_TENANT());\n```\n\n## Follow-up Questions\n\n- How would you automate tenant onboarding/offboarding with this macro?\n- How would you audit lineage changes in CI/CD?","diagram":"flowchart TD\n  A[Staging events per tenant] --> B[Macro creates analytics_<tenant> schema]\n  B --> C[Incremental analytics_<tenant>.hourly_metrics by hour_start]\n  C --> D[analytics.lineage table]\n  D --> E[Cross-tenant isolation tests]","difficulty":"intermediate","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","LinkedIn","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T08:53:21.838Z","createdAt":"2026-01-21T08:53:21.838Z"},{"id":"q-5249","question":"Design a beginner multi-tenant dbt task: compute daily DAU per tenant from staging.tenant_users and staging.tenant_engagement, creating analytics_{{tenant}}.daily_metrics and a per-tenant analytics_{{tenant}}.anomaly flag via a simple z-score. Use a macro to generate per-tenant schemas and a global analytics.lineage. Include 1-day late data handling, schema drift guards, tests (not_null on keys, unique on (tenant_id, day)), and a analytics.{tenant}.users snapshot. How would you validate cross-tenant isolation in CI?","answer":"Use a per-tenant macro to create analytics_{{tenant}} schemas. Build analytics_{{tenant}}.daily_metrics by tenant_id/day from staging.tenant_users/engagement; compute DAU and a z-score anomaly flag in","explanation":"## Why This Is Asked\nAssess familiarity with dbt macros, per-tenant data organization, incremental modeling, and data-quality checks in a beginner context. It also touches CI validation for cross-tenant isolation.\n\n## Key Concepts\n- dbt macros and templates for tenant scoping\n- per-tenant schemas vs global lineage\n- incremental daily metrics and simple anomaly flag logic\n- tests for data quality and uniqueness; snapshots for audit\n- CI checks for cross-tenant isolation via lineage\n\n## Code Example\n```sql\n-- macro example sketch\n{% macro make_tenant_schema(name) %}\n  create schema analytics_{{ name }};\n{% endmacro %}\n```\n\n## Follow-up Questions\n- How would you scale this macro for hundreds of tenants?\n- How would you test for cross-tenant bleed in CI beyond lineage checks?","diagram":null,"difficulty":"beginner","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Meta","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T13:13:28.120Z","createdAt":"2026-01-21T13:13:28.120Z"},{"id":"q-5388","question":"Design a per-tenant incremental dbt flow in Snowflake that computes analytics.{tenant}.hourly_session_summary from staging.session_events (tenant_id, session_id, user_id, action, occurred_at). Use a macro to create analytics_{tenant} schemas and a global analytics.lineage. Include 1-day late data, schema-drift guards, tests (not_null on hour_start, tenant_id; unique on (tenant_id, hour_start, session_id)). Add a cross-tenant alert when total active_sessions per hour across tenants breaches a threshold and describe CI for isolation?","answer":"Implement a per-tenant macro-driven schema analytics_{tenant} with an hourly_session_summary model incremental by hour_start; source from staging.session_events. Support 1-day late data, include schem","explanation":"## Why This Is Asked\n\nTests the ability to design a scalable, governance-minded multi-tenant pipeline with macro-driven schema generation, incremental models, late-data handling, and cross-tenant alerting. It also probes how isolation and lineage are verified in CI.\n\n## Key Concepts\n\n- Macro-driven per-tenant schemas (analytics_{tenant}) and a global lineage table\n- Incremental models by hour_start with 1-day late data window\n- Schema-drift guards and robust tests (not_null, unique)\n- Cross-tenant alerting fed into analytics.alerts and dashboards\n- CI validation for isolation and bleed prevention\n\n## Code Example\n\n```javascript\n// Macro sketch (dbt/Jinja-like syntax in JavaScript-style block)\n{% macro analytics_schema(tenant) %}\nanalytics_${tenant}\n{% endmacro %}\n```\n\n## Follow-up Questions\n\n- How would you simulate a bleed scenario in CI and prevent it?\n- What changes would you make to scale to hundreds of tenants without slowing runs?","diagram":null,"difficulty":"intermediate","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Oracle","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T20:01:29.019Z","createdAt":"2026-01-21T20:01:29.019Z"},{"id":"q-5468","question":"Design a multi-tenant dbt workflow in Snowflake to compute analytics.{tenant}.hourly_conversion_funnel from staging.clicks (tenant_id, user_id, funnel_step, occurred_at, revenue). Use a macro to auto-create analytics_{tenant} schema and analytics_{tenant}.hourly_conversion_funnel, plus a central analytics.lineage registry. Support 2-hour late data; enforce schema drift guards; tests: not_null on (tenant_id, hour_start, user_id, funnel_step) and unique on (tenant_id, hour_start, user_id, funnel_step). Snapshot analytics.{tenant}.customers for churn. Add CI checks for cross-tenant isolation via RLS and dynamic tenant scoping; describe bleed scenarios and fixes?","answer":"Implement a per-tenant incremental model that ingests staging.clicks into analytics_{tenant}.hourly_conversion_funnel via a macro that creates analytics_{tenant} schema, plus a global analytics.lineage registry for dependency tracking.","explanation":"## Why This Is Asked\n\nAssesses ability to design scalable multi-tenant dbt workflows with macro-driven per-tenant schemas, incremental loads, and a centralized lineage registry. Also tests late-data handling, schema drift protection, and robust cross-tenant isolation via CI and policy controls.\n\n## Key Concepts\n\n- dbt macros for per-tenant schema creation\n- Incremental loads with late-arriving data windows\n- Global lineage registry and dependency tracking\n- Row-level security and CI-based bleed prevention\n\n## Code Example\n\n```sql\n-- pseudo macro usage\n{% macro create_tenant_schema(tenant) %}\n-- creates analytics_{tenant} schema if not exists\n{% endmacro %}\n\n-- incremental model with 2-hour late data window\n{{ create_tenant_schema(var('tenant')) }}\n```\n\n## Implementation Notes\n\n- Schema drift guards via dbt tests and CI validation\n- RLS policies enforce tenant isolation\n- Dynamic tenant scoping prevents cross-tenant data bleed","diagram":"flowchart TD\n  A[Ingest: staging.clicks] --> B[analytics_{tenant}.hourly_conversion_funnel]\n  B --> C[analytics.lineage_registry]\n  B --> D[analytics.{tenant}.customers snapshot]\n  C --> E[Cross-tenant isolation checks in CI]","difficulty":"advanced","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T04:35:26.123Z","createdAt":"2026-01-21T23:39:13.262Z"},{"id":"q-5783","question":"Given staging.events(tenant_id, user_id, event, occurred_at), design a beginner dbt exercise for per-tenant daily engagement. Use a macro to create analytics_{{tenant}}.daily_engagement and a global analytics.lineage. Include 1-day late data, schema-drift guards, tests (not_null on tenant_id, user_id, day_start; unique on tenant_id/day_start/user_id), and a CI check ensuring analytics_{{tenant}}.daily_engagement reads only from its tenant. How would you validate isolation in CI?","answer":"Build a tenant_schema macro that creates analytics_{{tenant}} namespaces and a daily_engagement model materialized incrementally by day_start. Gate late data with occurred_at <= current_date - 1. Add ","explanation":"## Why This Is Asked\nTests macro-driven per-tenant scoping, incremental daily rollups, and a basic CI guard for data isolation—foundational for multi-tenant analytics.\n\n## Key Concepts\n- dbt macros for tenant-scoped schemas\n- incremental models by day_start\n- 1-day late data handling\n- tests for not_null and uniqueness\n- simple CI checks for tenant isolation and lineage\n\n## Code Example\n```javascript\n-- dbt macro (pseudo-sql) to create per-tenant schema\n{% macro tenant_schema(tenant) -%}\n  create schema if not exists analytics_{{- tenant -}};\n{% endmacro %}\n```\n\n```javascript\n-- incremental daily_engagement model skeleton\nSELECT\n  tenant_id,\n  user_id,\n  DATE(occurred_at) AS day_start,\n  COUNT(*) AS engagement_count\nFROM {{ ref('staging_events') }}\nWHERE occurred_at <= CURRENT_DATE() - INTERVAL '1 day'\nGROUP BY 1,2,3\n{% if is_incremental() %}\n  HAVING day_start = (SELECT MAX(day_start) FROM {{ this }})\n{% endif %}\n```\n\n## Follow-up Questions\n- How would you extend this to handle multiple tenants with different holidays?\n- What additional tests would you add to guard against cross-tenant joins in dashboards?","diagram":"flowchart TD\n  Q[Question] --> M[Macro creates per-tenant schemas]\n  M --> I[Incremental daily_engagement by day_start]\n  I --> T[Tests: not_null, unique]\n  T --> CI[CI isolation check]\n  CI --> L[Lineage table present]","difficulty":"beginner","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["PayPal","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T16:46:23.512Z","createdAt":"2026-01-22T16:46:23.512Z"},{"id":"q-5824","question":"In a multi-tenant Snowflake dbt project, with staging.events(tenant_id, user_id, event, occurred_at), design an advanced per-tenant incremental flow that computes analytics_{{tenant}}.daily_engagement and analytics_{{tenant}}.cohorts via a macro creating per-tenant schemas analytics_{{tenant}} and a global analytics.lineage. Include late data handling (3 days), schema-drift guards, tests (not_null on keys, unique on (tenant_id, cohort_id, day)), and a cross-tenant leakage CI check. Explain how to ensure isolation and rollback if cohort bucketing changes?","answer":"Architect a per-tenant incremental flow in Snowflake using a macro to create analytics_{{tenant}} schemas and analytics_{{tenant}}.daily_engagement plus analytics.lineage. Compute analytics_{{tenant}}","explanation":"## Why This Is Asked\nTests ability to design per-tenant isolation with dynamic schema creation, robust late-data handling, and lineage tracking. It probes macro design, incremental logic, and end-to-end validation in CI.\n\n## Key Concepts\n- Per-tenant schema generation via macros\n- Incremental models with late-arriving data\n- Cohort derivation and per-tenant analytics\n- Cross-tenant isolation tests and leakage detection\n- Schema drift guards and robust tests\n\n## Code Example\n\n```sql\n-- example: daily_engagement (simplified)\n{{ config(materialized='incremental') }}\nSELECT tenant_id, user_id, DATE(occ occurred_at) AS day, COUNT(*) AS engagements\nFROM {{ source('staging','events') }}\nWHERE occurred_at >= CURRENT_DATE - INTERVAL '3' DAY\nGROUP BY 1,2,3\n```\n\n```sql\n-- example: cohorts (simplified)\nSELECT tenant_id, user_id, MIN(occurred_at) AS first_seen\nFROM {{ ref('staging_events') }}\nGROUP BY tenant_id, user_id\n```\n\n## Follow-up Questions\n- How would you implement a CI check to verify no cross-tenant data leakage across tenants?\n- What rollback strategy would you adopt if cohort bucketing changes reassigns users between cohorts?","diagram":null,"difficulty":"advanced","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Cloudflare","Hugging Face"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T18:05:36.367Z","createdAt":"2026-01-22T18:05:36.367Z"},{"id":"q-5861","question":"Design a multi-tenant dbt workflow in Snowflake that computes analytics.{tenant}.weekly_engagement_score from staging.tenant_events (tenant_id, user_id, action, occurred_at, device). Use a macro to create per-tenant analytics_{tenant} schemas and a global analytics.lineage, with 3-day late data, dynamic schema-drift guards, and tests (not_null on keys; unique on (tenant_id, week_start, user_id)). Include a per-tenant churn snapshot and CI checks for cross-tenant isolation?","answer":"Propose a macro-driven per-tenant schema, incremental weekly_engagement_score by week_start with action weights, 3-day late data, lineage updates, and tests for not_null keys and uniqueness. Snapshot ","explanation":"## Why This Is Asked\n\nAssesses ability to architect multi-tenant dbt flows with per-tenant isolation, lineage, and late-data handling beyond simple aggregations.\n\n## Key Concepts\n\n- dbt macros for tenant scaffolding\n- per-tenant schemas and global lineage\n- incremental by week_start with late data windows\n- schema drift guards and data quality tests\n- CI checks for cross-tenant isolation\n\n## Code Example\n\n```javascript\n// pseudo macro skeleton illustrating tenant-scoped schema creation\nfunction analyticsTenantSchema(tenant) {\n  return `analytics_${tenant}`;\n}\n```\n\n## Follow-up Questions\n\n- How would you extend to support changing action weights per tenant?\n- How would you expose lineage metadata to dashboards while preserving isolation?","diagram":"flowchart TD\n  A[Start] --> B[Materialize weekly_engagement_score per tenant]\n  B --> C[Update analytics.lineage]\n  C --> D[Late data window (3 days)]\n  D --> E[Run CI isolation checks]\n  E --> F[Snapshot analytics.{tenant}.users]\n  F --> G[Dashboard access per tenant]","difficulty":"intermediate","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T19:40:22.069Z","createdAt":"2026-01-22T19:40:22.069Z"},{"id":"q-5908","question":"Design a multi-tenant dbt workflow in Snowflake that supports dynamic per-tenant enrichment: tenants provide an enrichment payload as a JSON field with extra fields (e.g., country_code, plan, region). Build a macro that creates analytics_{tenant}.hourly_engagement tables and a central analytics.lineage, applying schema drift guards. Implement an anomaly detector that writes to analytics.alerts when cross-tenant data leakage is detected, and add tests for not_null on keys, unique constraints, and a per-tenant leakage test. Describe CI checks to enforce isolation and prevent bleed?","answer":"Implement a tenant-aware enrichment macro that generates analytics_{tenant}.hourly_engagement tables from staging_events, with a central analytics.lineage table tracking cross-tenant dependencies. The macro validates enrichment payloads (JSON) against defined schemas, applying Snowflake's VARIANT handling for dynamic fields while enforcing schema drift guards. Build an anomaly detection model that monitors query patterns and data access patterns, writing violations to analytics.alerts when cross-tenant data leakage is detected. Implement comprehensive testing including not_null constraints on tenant keys, unique constraints on composite identifiers, and a custom per-tenant leakage test that validates isolation boundaries. Establish CI checks that enforce tenant isolation through automated lineage validation, schema drift detection, and cross-tenant access pattern analysis.","explanation":"## Why This Is Asked\nTests ability to design dynamic, scalable tenant isolation in dbt with real-time enrichment and governance.\n\n## Key Concepts\n- dbt macros for per-tenant scaffolding\n- Snowflake VARIANT vs flattened extra fields\n- Cross-tenant leakage detection and analytics.alerts\n- Schema drift guards, not_null and unique checks\n- CI validation of isolation and lineage integrity\n\n## Code Example\n```jinja\n{% macro analytics_schema(tenant) -%}\n  analytics_{{ tenant }}\n{%- endmacro %}\n\n{% macro enrich_hourly(tenant, src) -%}\nSELECT\n  t.tenant_id,\n  t.user_id,\n  date_trunc('hour', t.occurred_at) as engagement_hour,\n  t.event_type,\n  t.properties,\n  e.enrichment_payload::VARIANT as enrichment,\n  e.country_code,\n  e.plan,\n  e.region\nFROM {{ src }} t\nLEFT JOIN tenant_enrichments e \n  ON t.tenant_id = e.tenant_id\nWHERE t.tenant_id = '{{ tenant }}'\n{%- endmacro %}\n```\n\n## Implementation Notes\n- Use Snowflake's OBJECT_KEYS() to validate enrichment schema\n- Implement row-level security policies for tenant isolation\n- Set up automated anomaly detection using query history patterns\n- Create custom dbt tests for per-tenant data leakage validation","diagram":null,"difficulty":"intermediate","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T05:55:17.407Z","createdAt":"2026-01-22T21:47:45.456Z"},{"id":"q-6026","question":"Scenario: you operate a multi-tenant analytics platform on Snowflake. Tenants select a region and cost tier; implement a dbt macro that provisions analytics_{tenant} schemas in the chosen region and a tenant-scoped warehouse, and materializes analytics.{tenant}.hourly_engagement incrementally. Build a central analytics.lineage to track dependencies, support 1-day late data, guard against schema drift, and add tests (not_null on tenant_id, hour_start, user_id; unique on (tenant_id, hour_start, user_id)). Include CI gates to enforce tenant isolation and deny cross-region bleed?","answer":"To implement, define a macro that, given tenant_id and region, creates analytics_{tenant} schema and configures dbt to target a per-tenant warehouse from a regional map. Use an incremental model for a","explanation":"## Why This Is Asked\nTests real-world multi-tenant isolation and cross-region concerns, with per-tenant resource provisioning and lineage.\n\n## Key Concepts\n- Per-tenant schema and warehouse targeting\n- Macro-driven metamodel for dynamic tenants\n- Incremental with late data handling\n- Schema drift guards and lineage tracking\n- CI gates for isolation\n\n## Code Example\n```javascript\n-- pseudo fragment illustrating macro usage\nCREATE OR REPLACE SCHEMA analytics_{{tenant}};\nALTER WAREHOUSE IF EXISTS w_{{region}} SET WAREHOUSE = w_{{region}};\n```\n\n## Follow-up Questions\n- How would you test cross-tenant leaks in CI?\n- How would you monitor cost and performance across regions?","diagram":null,"difficulty":"intermediate","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","Meta","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T05:43:01.416Z","createdAt":"2026-01-23T05:43:01.416Z"},{"id":"q-6078","question":"Design a per-tenant incremental dbt flow in Snowflake to compute analytics.{tenant}.hourly_engagement from staging.tenant_events (tenant_id, user_id, action, occurred_at). Use a macro to create analytics_{{tenant}} schemas and analytics_{{tenant}}.hourly_engagement, plus a global analytics.lineage. Enforce 1-day late data, schema drift guards, and tests. Add a masking policy on PII in analytics.{tenant} and a CI check to prevent cross-tenant bleed; explain how you validate isolation and lineage?","answer":"To address this, implement a tenant-scoped incremental pipeline. A macro creates analytics_{{tenant}} schemas and per-tenant hourly_engagement tables. Use Snowflake masking policies on PII in analytic","explanation":"## Why This Is Asked\nTests practical, macro-driven multi-tenant dbt design with data masking, lineage, and CI isolation checks.\n\n## Key Concepts\n- Tenant-scoped macros and per-tenant schemas\n- Snowflake masking policies for PII\n- Incremental by hour with late data handling\n- Global lineage table and cross-tenant isolation tests\n\n## Code Example\n```sql\n-- macro skeleton\n{% macro analytics_tenant_schema(tenant) %}\ncreate schema if not exists analytics_{{tenant}};\n{% endmacro %}\n```\n\n## Follow-up Questions\n- How would you test cross-tenant bleed in CI?\n- How to scale masking policy management as tenants grow?\n","diagram":null,"difficulty":"intermediate","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["PayPal","Snap","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T07:50:35.630Z","createdAt":"2026-01-23T07:50:35.630Z"},{"id":"q-6215","question":"NEW ANGLE: Tenant registry driven per-tenant error analytics. Given a seeds table tenants(tenant_id, status) and staging.errors(tenant_id, error_code, occurred_at, stacktrace), design a beginner dbt flow that uses a macro to create analytics_{{tenant}}.daily_error_events with daily counts by error_code. Include 1-day late data, not_null and unique tests, and a global analytics.lineage. Add a CI check that analytics_{{tenant}}.daily_error_events exists for all tenants and that new tenants in the registry automatically appear. Explain how you'd validate cross-tenant isolation?","answer":"Plan: Create a tenant macro that reads tenants seed, creates analytics_{{tenant}}.daily_error_events and analytics.lineage per tenant, materializes per-day by day_start using occurred_at, handles 1-da","explanation":"## Why This Is Asked\nTests practical dbt skills for multi-tenant analytics, macro usage, and CI validation in a beginner-friendly scenario.\n\n## Key Concepts\n- dbt macros generating per-tenant objects from a seed\n- handling 1-day late data in incremental workloads\n- tests: not_null and unique for day_start/tenant_id/error_code\n- cross-tenant isolation validation in CI\n- lineage tracking for tenant-scoped dependencies\n\n## Code Example\n```javascript\n-- dbt macro sketch\n{% macro daily_error_events_sql(tenant) -%}\nselect\n  date_trunc('day', occurred_at) as day_start,\n  tenant_id,\n  error_code,\n  count(*) as error_count\nfrom {{ source('staging','errors') }}\nwhere tenant_id = '{{ tenant }}'\ngroup by 1,2,3\n{% endmacro %}\n```\n\n## Follow-up Questions\n- How would you test macro-generated objects for a new tenant without running the full pipeline?\n- How would you handle a tenant removal or churn from the seed while preserving historical analytics?","diagram":null,"difficulty":"beginner","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Hashicorp","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T14:47:50.777Z","createdAt":"2026-01-23T14:47:50.778Z"},{"id":"q-6361","question":"Design a beginner dbt exercise to generate per-tenant daily_event_summary and a per-tenant data_quality score using a macro that creates analytics_{{tenant}}.daily_event_summary (incremental by day) and analytics_{{tenant}}.quality (score 0-100 from not_null checks and late data). Include 1-day late data window, tests (not_null on tenant_id, day_start, event_type; unique on tenant_id/day_start/event_type). CI should verify analytics_{{tenant}}.daily_event_summary exists for all tenants and that new tenants auto-create. How would you validate cross-tenant isolation by filtering by tenant_id and checking lineage?","answer":"Create a macro that materializes analytics_{{tenant}}.daily_event_summary incrementally by day from staging.events (tenant_id, event_type, occurred_at). Add analytics_{{tenant}}.quality computed from ","explanation":"## Why This Is Asked\n\nTests a beginner-friendly dbt flow with macros, per-tenant schemas, incremental models, and basic data quality scoring. Also exercises CI automation to auto-create new tenants and verify isolation.\n\n## Key Concepts\n\n- dbt macros for tenant-scoped tables\n- incremental models by day\n- simple data_quality scoring\n- tests: not_null and unique\n- CI checks and automatic tenant provisioning\n- data lineage visibility across tenants\n\n## Code Example\n\n```sql\n-- micro-example: incremental by day for analytics_{{tenant}}.daily_event_summary\nwith src as (\n  select tenant_id, date_trunc('day', occurred_at) as day_start, event_type, count(*) as cnt\n  from {{ ref('staging.events') }}\n  group by 1,2,3\n)\nselect * from src\n```\n\n## Follow-up Questions\n\n- How would you handle a tenant deletion in this model?\n- How would you scale the data_quality computation to more metrics without increasing build time?","diagram":"flowchart TD\n  A[Seeds: tenants] --> B(Staging: events)\n  B --> C(Macro: analytics_{{tenant}}.daily_event_summary (incremental))\n  C --> D(Global: analytics.lineage)\n  E(CI: tenant autos-creation) --> C","difficulty":"beginner","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","NVIDIA","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T20:49:47.592Z","createdAt":"2026-01-23T20:49:47.592Z"},{"id":"q-6457","question":"NEW ANGLE: Beginner dbt flow for per-tenant consent auditing. Given seeds.tenants(tenant_id, status) and staging.consent_events(tenant_id, user_id, consent_type, consent_ts), design a macro-driven flow that creates analytics_{{tenant}}.consent_audit with daily counts by consent_type. Include 1-day late data, not_null tests for tenant_id, user_id, consent_type, and a unique on (tenant_id, consent_type, day_start). Add a global analytics.lineage and CI that every tenant has analytics_{{tenant}}.consent_audit and auto-creates for new tenants. Explain cross-tenant isolation validation?","answer":"Create a macro that materializes analytics_{{tenant}}.consent_audit from staging.consent_events, rounded to day_start with daily counts per consent_type. Use an incremental load with a 1-day late window to capture delayed events. The macro dynamically generates tenant-specific models with proper partitioning and clustering for performance. Implement dbt tests including not_null for tenant_id, user_id, consent_type, and unique constraint on (tenant_id, consent_type, day_start). The global analytics.lineage table tracks all tenant models with creation timestamps and schema versions.","explanation":"## Why This Is Asked\nTests macro-driven multi-tenant object creation, incremental processing, and data-quality controls at beginner level.\n\n## Key Concepts\n- dbt macro scaffolding per-tenant models\n- incremental loads with late data windows\n- not_null and unique tests\n- lineage tracking and CI per-tenant validation\n- cross-tenant isolation checks\n\n## Code Example\n```sql\n-- example macro usage (conceptual)\n{% macro per_tenant_consent_audit(tenant) %}\nselect '{{ tenant }}'::text as tenant_id, \n       consent_type, \n       date_trunc('day', consent_ts) as day_start, \n       count(*) as cnt\nfrom s\n```","diagram":null,"difficulty":"beginner","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Google","Hashicorp"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T04:39:56.396Z","createdAt":"2026-01-24T02:30:54.164Z"},{"id":"q-6496","question":"Given seeds tenants(tenant_id, mask_policy) and staging.user_activity(tenant_id, user_id, action, username, email, occurred_at), design a per-tenant dbt flow that masks PII in analytics_{{tenant}}.masked_activity using a macro to generate per-tenant schemas and a global analytics.lineage. Include 24h late data, schema drift guards, and tests (not_null on keys, unique on tenant_id/occurred_at/user_id). Explain how you'd validate cross-tenant isolation in CI and prevent bleed?","answer":"Implement a per-tenant masking dbt flow: use a macro mask_pii(policy, field) that returns redaction, hashing, or partial masking based on mask_policy per tenant. Generate analytics_{{tenant}}.masked_a","explanation":"## Why This Is Asked\nTests ability to design per-tenant masking, macro-driven schema generation, and lineage tracking in dbt, while enforcing isolation and late data.\n\n## Key Concepts\n- Macro-driven per-tenant schemas\n- Policy-based PII masking (redact, hash, partial)\n- Global lineage table and dependency tracking\n- Late-arrival handling and tenant isolation CI\n\n## Code Example\n\n```sql\n-- macro: mask_pii\n{% macro mask_pii(policy, field) -%}\n  {% if policy == 'redact' -%}\n    NULL\n  {% elif policy == 'hash' -%}\n    SHA2(CONCAT(COALESCE('{{tenant_id}}', 'global'), {{field}}), 256)\n  {% else -%}\n    {{ field }}\n  {% endif %}\n{% endmacro %}\n```\n\n## Follow-up Questions\n- How would you test masking across tenants with different policies?\n- How would you extend lineage to support multi-tenant drift?","diagram":null,"difficulty":"advanced","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Hashicorp","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T05:32:36.848Z","createdAt":"2026-01-24T05:32:36.848Z"},{"id":"q-6519","question":"Design a per-tenant incremental dbt flow in Snowflake that computes analytics.{tenant}.hourly_revenue from staging.tenant_sales (tenant_id, sale_id, amount, currency, occurred_at). Use a shared exchange_rates table (currency, rate_to_usd, as_of) to convert to USD. Build via a macro creating analytics_{{tenant}} schemas and analytics_{{tenant}}.hourly_revenue_usd, plus a global analytics.lineage table. Include 2-day late data, schema drift guards, tests (not_null on keys, unique on (tenant_id, hour_start, sale_id, currency)). Explain how you would implement per-tenant retention policies and ensure no cross-tenant bleed?","answer":"Implement a macro that creates analytics_{{tenant}}.hourly_revenue_usd via an incremental model joining staging.tenant_sales to exchange_rates on currency and occurred_at, computing hour_start with da","explanation":"Why This Is Asked\n- Tests macro-driven per-tenant schema creation, currency conversion, and cross-tenant isolation in a practical setting.\n\nKey Concepts\n- dbt macros, incremental models, Snowflake, currency conversion, lineage, per-tenant retention.\n\nCode Example\n```javascript\n{% macro hourly_revenue_usd_sql(tenant) -%}\nselect\n  '{{' + tenant + '}}' as tenant_id,\n  date_trunc('hour', occurred_at) as hour_start,\n  sale_id,\n  currency,\n  sum(amount * rate_to_usd) as revenue_usd\nfrom {{ ref('staging_tenant_sales') }} s\njoin {{ ref('exchange_rates') }} e\n  on e.currency = s.currency and e.as_of <= s.occurred_at\ngroup by 1,2,3,4\n{% endmacro %}\n```\n\nFollow-up Questions\n- How would you handle missing exchange rates for a currency? \n- How would you validate per-tenant retention enforcement in CI?","diagram":null,"difficulty":"intermediate","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Microsoft","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T06:47:32.159Z","createdAt":"2026-01-24T06:47:32.159Z"},{"id":"q-6767","question":"NEW ANGLE: Privacy-focused per-tenant analytics. Given seeds.tenants(tenant_id, privacy_class) and staging.user_events(tenant_id, user_id, event, pii_email, occurred_at), design a beginner dbt flow that uses a macro to create analytics_{{tenant}}.daily_privacy_events with counts by event by day, while hashing pii_email per tenant (central macro using SHA256) and emitting hashed_user_id instead of raw IDs. Include 1-day late data, tests not_null/unique, a global analytics.lineage, and a CI check ensuring per-tenant tables exist and new tenants auto-create. How would you validate cross-tenant isolation and hashing consistency?","answer":"Use a macro to create analytics_{{tenant}}.daily_privacy_events with daily event counts, hashing pii_email via SHA256 per tenant and emitting hashed_user_id. Build incrementally from staging.user_even","explanation":"## Why This Is Asked\nAssesses practical dbt macro usage, per-tenant isolation, and simple privacy controls in a beginner task. It also tests incremental modeling, tests, and CI validation without requiring deep production-scale infra.\n\n## Key Concepts\n- dbt macros for tenant-scoped objects\n- incremental models with late-arrival data\n- data masking via hashing (per-tenant salt concept)\n- tests for data quality and uniqueness\n- cross-tenant isolation checks in CI\n\n## Code Example\n```sql\n-- sample macro usage snippet (conceptual, not full model)\nselect\n  tenant_id,\n  date(occurred_at) as day,\n  event,\n  sha2(pii_email, 256) as hashed_email\nfrom {{ ref('staging__user_events') }}\n```\n\n## Follow-up Questions\n- How would you manage salt-per-tenant for hashing to avoid identical hashes across tenants?\n- How would you extend tests to detect accidental leakage of pii fields in non-privacy tables?","diagram":null,"difficulty":"beginner","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Oracle","Robinhood","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T16:52:38.963Z","createdAt":"2026-01-24T16:52:38.963Z"},{"id":"q-6819","question":"Design an incremental dbt flow that ingests staging.raw_events (tenant_id, user_id, event_type, occurred_at, record_hash) and upserts into analytics.{tenant}.daily_user_state using MERGE, supporting 1-day late data. Add a macro to create analytics_{{tenant}} schemas and a global analytics.lineage. Enrich with analytics.{tenant}.users snapshot. Include tests: not_null on keys, unique (tenant_id, user_id, day). Explain CI validation for cross-tenant isolation and lineage?","answer":"Propose an incremental dbt flow: MERGE upsert from staging.raw_events (...) into analytics.{tenant}.daily_user_state, with day derived from occurred_at and 1-day late data. Use a macro to create analy","explanation":"## Why This Is Asked\nReasoning about multi-tenant data handling, late data, and lineage isolation under real constraints.\n\n## Key Concepts\n- dbt macros; per-tenant schemas; MERGE upserts; late-arriving data; snapshots; lineage table; CI checks for isolation.\n\n## Code Example\n```javascript\n// Pseudo-dsn creating MERGE SQL in a macro-like fashion\ndef upsert_sql(tenant):\n  return f'''MERGE INTO analytics.{tenant}.daily_user_state AS t\nUSING staging.{tenant}.raw_events AS s\nON t.tenant_id = s.tenant_id AND t.user_id = s.user_id AND t.day = DATE_TRUNC('day', s.occurred_at)\nWHEN MATCHED THEN UPDATE SET event_type = s.event_type, occurred_at = s.occurred_at, record_hash = s.record_hash\nWHEN NOT MATCHED THEN INSERT (tenant_id, user_id, day, event_type, occurred_at, record_hash) VALUES (s.tenant_id, s.user_id, DATE_TRUNC('day', s.occurred_at), s.event_type, s.occurred_at, s.record_hash);'''\n```\n\n## Follow-up Questions\n- How would you test idempotency across re-runs?\n- How would you monitor cross-tenant data bleed in CI?\n","diagram":null,"difficulty":"intermediate","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T18:52:14.156Z","createdAt":"2026-01-24T18:52:14.156Z"},{"id":"q-6924","question":"In a multi-tenant dbt analytics project deployed to Snowflake, design a region-aware, per-tenant daily funnel model that infers user progression across signup -> activated -> purchased from staging.events (tenant_id, user_id, event, occurred_at). Create a macro to materialize analytics_{{tenant}}.daily_funnel and a global analytics.lineage table. Enforce 2-day late data, normalize times to the tenant's timezone, include schema-drift guards, and tests (not_null on keys, unique on tenant_id/day/user_id/stage). Also snapshot analytics.{{tenant}}.users. Explain CI checks to validate cross-tenant isolation and lineage?","answer":"Design a per-tenant, region-aware funnel in Snowflake: a macro creates analytics_{{tenant}}.daily_funnel by joining staging.events for tenant_id, normalizing occurred_at to the tenant's timezone, and applying a 2-day late-data window. The macro generates tenant-specific schemas with timezone-aware timestamp conversion, implements schema-drift guards through column validation, and enforces data quality with not_null constraints on keys and unique constraints on tenant_id/day/user_id/stage combinations. A global analytics.lineage table tracks all tenant objects, while analytics.{{tenant}}.users snapshots capture user cohorts for longitudinal analysis.","explanation":"## Why This Is Asked\nTests ability to design region-aware, multi-tenant pipelines with per-tenant schemas, robust late-data handling, and stringent data quality checks. Emphasizes governance, isolation, and lineage.\n\n## Key Concepts\n- Multi-tenant data isolation and dynamic schemas\n- Timezone normalization per tenant and region awareness\n- Incremental models with late-arriving data windows\n- Macros for per-tenant object generation and global lineage\n- Data quality tests and snapshotting for user cohorts\n\n## Code Example\n```sql\n-- Macro sketch (dbt/Jinja)\n{% macro daily_funnel_sql(tenant) %}\nWITH tenant_events AS (\n  SELECT \n    tenant_id,\n    user_id,\n    event,\n    CONVERT_TIMEZONE(\n      (SELECT timezone FROM tenant_config WHERE tenant_id = '{{tenant}}'),\n      occurred_at\n    ) AS local_occurred_at\n  FROM staging.events\n  WHERE tenant_id = '{{tenant}}'\n    AND occurred_at >= DATEADD(day, -2, CURRENT_DATE())\n),\nfunnel_stages AS (\n  SELECT \n    tenant_id,\n    user_id,\n    DATE(local_occurred_at) AS funnel_day,\n    CASE \n      WHEN event = 'signup' THEN 'signup'\n      WHEN event = 'activated' THEN 'activated'\n      WHEN event = 'purchased' THEN 'purchased'\n    END AS stage\n  FROM tenant_events\n  WHERE event IN ('signup', 'activated', 'purchased')\n)\nSELECT * FROM funnel_stages\n{% endmacro %}\n```\n\n## CI Validation\nCross-tenant isolation checks validate that tenant-specific schemas contain only their data, while lineage verification ensures all tenant objects are properly tracked in analytics.lineage with correct metadata.","diagram":null,"difficulty":"advanced","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","Netflix","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T05:52:02.114Z","createdAt":"2026-01-24T22:58:02.885Z"},{"id":"q-6996","question":"Design a beginner dbt flow for per-tenant funnel analytics. Use a macro to create analytics_{{tenant}}.daily_funnel with (day_start, stage, user_count) from staging.funnel_events(tenant_id, user_id, stage, occurred_at) covering signup/view/add_to_cart/purchase. Build an incremental model with a 1-day late window, tests not_null on tenant_id/day_start/stage and unique on (tenant_id, day_start, stage). Create a global analytics.lineage. Add CI to ensure analytics_{{tenant}}.daily_funnel exists for all tenants and auto-creates for new tenants. Explain cross-tenant isolation validation in CI?","answer":"Implement a macro that creates analytics_{{tenant}}.daily_funnel with day_start, stage, user_count. Build an incremental model over staging.funnel_events to produce daily counts for signup, view, add_","explanation":"## Why This Is Asked\n\nGauges macro-driven per-tenant analytics, incremental models, late data handling, tests, lineage, and CI automation for multi-tenant isolation.\n\n## Key Concepts\n\n- dbt macros for per-tenant objects\n- incremental models with late data window\n- tests: not_null, unique\n- global analytics.lineage\n- CI for tenant provisioning and isolation\n\n## Code Example\n\n```javascript\n{% macro daily_funnel_sql(tenant) -%}\nselect '{{tenant}}' as tenant_id, date_trunc('day', occurred_at) as day_start, stage,\n       count(distinct user_id) as user_count\nfrom {{ source('staging','funnel_events') }}\nwhere tenant_id = '{{tenant}}'\ngroup by 1,2,3\n{%- endmacro %}\n```\n\n## Follow-up Questions\n\n- How would you test that a new tenant triggers creation of analytics_{{tenant}}.daily_funnel?\n- How would you scale the macro to handle missing stage values?","diagram":null,"difficulty":"beginner","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Square","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T04:32:44.891Z","createdAt":"2026-01-25T04:32:44.891Z"},{"id":"q-7119","question":"Design a multi-tenant dbt flow in Snowflake where a macro creates analytics_{{tenant}} schemas and analytics_{{tenant}}.hourly_engagement and analytics_{{tenant}}.hourly_engagement_funnel from staging.events (tenant_id, user_id, action, occurred_at, country). Implement incremental by hour_start; compute a 24h rolling baseline per tenant (mean/std) and flag spikes where current > mean + 3*std. Include a global analytics.lineage; ensure 1-day late data, schema-drift guards, and tests; snapshot analytics_{{tenant}}.users for churn. CI should auto-create tenants and validate isolation by tenant_id filters. How would you implement and test this across tenants?","answer":"Leverage a tenant-scoped macro to create analytics_{{tenant}} schemas and analytics_{{tenant}}.hourly_engagement_funnel from staging.events, plus a global analytics.lineage. Implement incremental by h","explanation":"## Why This Is Asked\nTests ability to architect multi-tenant dbt with per-tenant schemas, dynamic macros, and cross-tenant governance. It probes incremental logic, anomaly detection, lineage tracking, and CI guarantees for new tenants.\n\n## Key Concepts\n- Tenant-scoped objects via macros\n- Incremental by hour_start\n- Rolling baseline and spike detection\n- Schema drift guards and tests\n- Cross-tenant isolation and lineage\n\n## Code Example\n```javascript\n// Pseudocode: macro and models would live in dbt/macros/multitenant.sql\n```\n\n## Follow-up Questions\n- How would you monitor CI performance when tenants scale?\n- How would you handle missing tenants in the registry?","diagram":"flowchart TD\n  A[Tenant Registry] --> B[Generate analytics_{{tenant}} schema via macro]\n  B --> C[Compute analytics_{{tenant}}.hourly_engagement and _funnel]\n  C --> D[Incremental by hour_start]\n  D --> E[Compute 24h baseline per tenant and spike flag]\n  E --> F[Populate analytics.lineage]\n  F --> G[Snapshot analytics_{{tenant}}.users for churn]\n  G --> H[CI auto-create tenants and validate isolation]","difficulty":"intermediate","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","MongoDB","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T09:46:13.731Z","createdAt":"2026-01-25T09:46:13.731Z"},{"id":"q-7146","question":"Design a beginner dbt flow for time-zone aware per-tenant daily metrics? Given seeds tenants(tenant_id, tz) and staging.events(tenant_id, action, occurred_at), create a macro that builds analytics_{{tenant}}.daily_metrics by converting occurred_at to the tenant's timezone and aggregating counts by day and action with a 1-day lag. Include not_null and unique tests, a global analytics.lineage, and a CI check that daily_metrics exists for all tenants and auto-creates for new tenants. How would you validate cross-tenant isolation and timezone correctness?","answer":"Implement a macro that builds analytics_{{tenant}}.daily_metrics by converting occurred_at to the tenant's timezone and aggregating counts by day and action with a 1-day lag from staging.events; add t","explanation":"## Why This Is Asked\nTimezone-aware per-tenant analytics addresses real-world needs when tenants operate in different regions. This tests fundamental dbt skills (macros, incremental models, tests) while introducing cross-tenant isolation and validation of timezone boundaries.\n\n## Key Concepts\n- dbt macros and per-tenant schemas\n- timezone handling in SQL\n- 1-day late data window\n- data lineage and CI for tenant auto-creation\n\n## Code Example\n```javascript\n-- macro example (macros/daily_metrics.sql)\n{% macro daily_metrics_sql(tenant_id) %}\nSELECT\n  DATE(CONVERT_TIMEZONE(tz, occurred_at)) AS day,\n  action,\n  COUNT(*) AS cnt\nFROM {{ ref('staging__events') }}\nJOIN {{ ref('tenants') }} t ON t.tenant_id = staging__events.tenant_id\nWHERE staging__events.tenant_id = '{{ tenant_id }}'\nGROUP BY 1, 2\n{% endmacro %}\n```\n\n## Follow-up Questions\n- How would you test a tenant changing timezone?\n- How would you handle missing tz values or late events?\n","diagram":null,"difficulty":"beginner","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T10:51:45.627Z","createdAt":"2026-01-25T10:51:45.627Z"},{"id":"q-7269","question":"Your team runs a multi-tenant dbt project in Snowflake. Propose an intermediate, macro-driven workflow to produce per-tenant hourly_engagement from staging.tenant_events, with 1-day late data, dynamic schema-drift guards, and a global analytics_lineage. Include auto-provisioning when a new tenant appears in a central tenants registry, and per-tenant data masking for analytics_{tenant}.user_profiles. Outline tests and CI checks for isolation?","answer":"Build a macro that on a new tenant creates analytics_{tenant} schemas and hourly_engagement incremental models, plus a Snowflake stream for staging.tenant_events. Use a global analytics_lineage table ","explanation":"## Why This Is Asked\nTests ability to design scalable, tenant-isolated dbt workflows with dynamic provisioning, data masking, and lineage tracking.\n\n## Key Concepts\n- Macro-driven per-tenant schema provisioning\n- Incremental hourly metrics with late data handling\n- Snowflake streams for near-real-time ingestion\n- Global lineage and tenant registry-driven auto-provisioning\n- Data masking for PII and robust isolation tests\n\n## Code Example\n```javascript\n// Example macro sketch (non-functional placeholder for concept)\nmacro analytics_tenant_setup(tenant) {\n  create schema analytics_#{tenant}\n  create table analytics_#{tenant}.hourly_engagement as select * from staging.tenant_events where tenant_id = #{tenant}\n}\n```\n\n## Follow-up Questions\n- How would you test cross-tenant data leakage risk in CI?\n- What are the trade-offs of masking at DB vs. at query layer for analytics?\n- How would you handle schema drift auto-remediation across tenants?","diagram":"flowchart TD\n  A[New tenant registered] --> B[Provision analytics_{tenant} schema]\n  B --> C[Incremental hourly_engagement]\n  C --> D[Update analytics_lineage]\n  D --> E[CI tests & auto-provisioning]","difficulty":"intermediate","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Microsoft","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T15:54:25.005Z","createdAt":"2026-01-25T15:54:25.006Z"},{"id":"q-7312","question":"Design a per-tenant, incremental dbt flow in Snowflake to compute analytics.{tenant}.anomaly_summary from staging.events (tenant_id, user_id, event_type, value, occurred_at, predicted_anomaly_score). Build via a macro that creates analytics_{tenant} schemas and analytics_{tenant}.anomaly_summary, plus a global analytics.lineage table. Include 1-day late data, schema-drift guards, tests (not_null on keys, not_null on predicted_anomaly_score, unique on (tenant_id, day_start, event_type)), and a per-tenant users snapshot. Explain cross-tenant isolation validation and anomaly lineage auditing?","answer":"Describe a per-tenant incremental flow that creates analytics_{tenant} schemas and analytics_{tenant}.anomaly_summary from staging.events, including predicted_anomaly_score. Use a macro for schema cre","explanation":"## Why This Is Asked\nThis angle tests multi-tenant data governance when ML-influenced signals drive analytics. It emphasizes dynamic schema creation, robust lineage, and isolation checks in CI.\n\n## Key Concepts\n- Macro-driven per-tenant schemas and tables\n- Incremental daily anomalySummary with ML scores\n- Global analytics.lineage for cross-tenant tracing\n- Schema drift guards and data-quality tests\n\n## Code Example\n```jinja\n{% macro create_anomaly_schema(tenant) %}\n  {% set tbl = 'analytics_' ~ tenant ~ '.anomaly_summary' %}\n  -- dbt macro to initialize tenant schema objects\n{% endmacro %}\n```\n\n## Follow-up Questions\n- How would you validate cross-tenant isolation in CI?\n- How would you monitor anomaly_score drift across tenants?","diagram":null,"difficulty":"intermediate","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","OpenAI","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T17:46:23.840Z","createdAt":"2026-01-25T17:46:23.840Z"},{"id":"q-7518","question":"NEW ANGLE: Privacy-driven per-tenant dbt flow. Given a tenants table (tenant_id, privacy_tier) and staging.events(tenant_id, user_id, event_type, email, ip_address, occurred_at), design a macro-driven analytics_{{tenant}}.hourly_events that enforces privacy rules by redacting or hashing PII for non-public tenants. Load 1-day late data, include tests (not_null on keys, unique (tenant_id, hour_start, event_type)); snapshot analytics_{{tenant}}.users; and a global analytics.lineage. Explain how this scales to 100s of tenants and how CI validates cross-tenant isolation and privacy compliance?","answer":"Design a macro-driven per-tenant dbt flow that reads staging.events (tenant_id, user_id, event_type, email, ip_address, occurred_at) and loads analytics_{{tenant}}.hourly_events. Enforce privacy: if p","explanation":"## Why This Is Asked\n\nTests a realistic, privacy‑aware multi‑tenant pipeline. Requires macro generation of tenant-scoped models, conditional data masking, and governance hooks (lineage, snapshots) under CI.\n\n## Key Concepts\n\n- dbt macros for tenant-scoped models\n- per-tenant privacy rules (PII redaction/hash)\n- incremental loading with a 1-day late window\n- tests: not_null, unique keys\n- snapshots and lineage governance\n- CI validation across many tenants\n\n## Code Example\n\n```javascript\n// Pseudo macro sketch (dbt uses Jinja in SQL)\n{% macro redact_fields(field, privacy_tier) %}\n  {% if privacy_tier == 'public' %}\n    {{ field }}\n  {% else %}\n    SHA2({{ field }}, 256)\n  {% endif %}\n{% endmacro %}\n```\n\n## Follow-up Questions\n\n- How would you monitor privacy violations in production and adjust masking rules?\n- How would you scale testing and CI for 100s of tenants without slowing commits?","diagram":null,"difficulty":"intermediate","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Lyft","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T05:43:26.839Z","createdAt":"2026-01-26T05:43:26.839Z"},{"id":"q-7642","question":"Design a beginner dbt flow to generate per-tenant daily revenue analytics using a macro that creates analytics_{{tenant}}.revenue from staging.transactions(tenant_id, customer_id, amount, occurred_at). Hash customer_id for privacy, compute day_start as date_trunc('day', occurred_at), aggregate revenue per tenant/day, and include 1-day late data. Add tests: not_null on tenant_id, day_start, anonymized_customer_id; unique on (tenant_id, day_start, anonymized_customer_id). Build a global analytics.lineage and CI checks that analytics_{{tenant}}.revenue exists for all tenants and that new tenants auto-create. How would you validate cross-tenant isolation and privacy in CI?","answer":"Implement a single macro that, for each tenant, creates analytics_{{tenant}}.revenue by day_start from staging.transactions, hashing customer_id via SHA2(256) to anonymize. Use an incremental model wi","explanation":"## Why This Is Asked\n\nGauges privacy-aware, tenant-scoped dbt patterns beyond standard per-tenant notebooks. Tests both data quality and isolation, with a macro that scales tenants automatically.\n\n## Key Concepts\n\n- Tenant-scoped macros and per-tenant schemas\n- Hashing sensitive keys for privacy (SHA2-256)\n- Incremental models with 1-day late data\n- Not-null and unique tests per tenant\n- Global lineage tracking\n- CI validation for new tenants and cross-tenant isolation\n\n## Code Example\n\n```javascript\n-- macro to build per-tenant revenue\n{% macro build_revenue(tenant) %}\n  select\n    tenant_id,\n    date_trunc('day', occurred_at) as day_start,\n    sha2(cast(customer_id as string), 256) as anonymized_customer_id,\n    sum(amount) as revenue\n  from {{ ref('staging_transactions') }}\n  where tenant_id = '{{ tenant }}'\n  group by 1,2,3\n{% endmacro %}\n```\n\n```javascript\n-- sample tests (conceptual)\nselect * from analytics_{{tenant}}.revenue where tenant_id is null;\nselect count(*) as c from analytics_{{tenant}}.revenue group by tenant_id, day_start, anonymized_customer_id having count(*) > 1;\n```\n\n## Follow-up Questions\n\n- How would you test masking effectiveness and detect potential leakage?\n- How would you handle currency variations across tenants in this flow?","diagram":null,"difficulty":"beginner","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Plaid","Salesforce","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T10:56:48.480Z","createdAt":"2026-01-26T10:56:48.480Z"},{"id":"q-7741","question":"Design a per-tenant incremental dbt flow that ingests semi-structured JSON events from staging.events_json (tenant_id, event_type, payload, occurred_at) into analytics_{{tenant}}.hourly_events by hour_start. Create a macro to extract required fields from payload (e.g., user_id, action) into analytics_{{tenant}}.hourly_events_derived. Support 1-day late data, add schema-drift guards, and tests (not_null on hour_start, tenant_id; unique on (tenant_id, hour_start, event_type)). Maintain a global analytics.lineage and CI checks for cross-tenant isolation?","answer":"Outline a per-tenant incremental flow: build analytics_{{tenant}}.hourly_events incrementally by hour_start from staging.events_json, parse payload JSON to derive user_id and action into analytics_{{t","explanation":"## Why This Is Asked\nTests knowledge of per-tenant isolation, incremental dbt, and JSON payload parsing within Snowflake. It also probes cross-tenant lineage governance and CI validation.\n\n## Key Concepts\n- dbt incremental models by tenant\n- macros for tenant-scoped objects\n- JSON payload extraction in Snowflake\n- schema drift guards and rigorous tests\n- lineage tracking and CI validation\n\n## Code Example\n```javascript\n-- Pseudo dbt macro sketch for extracting payload fields\n{% macro derive_payload_fields() %}\nSELECT\n  tenant_id,\n  event_type,\n  occurred_at,\n  payload:user_id::string as user_id,\n  payload:action::string as action\nFROM {{ source('staging','events_json') }}\n{% endmacro %}\n```\n\n## Follow-up Questions\n- How would you evolve the flow to support new payload fields without breaking tests?\n- How would you test cross-tenant isolation in CI across multiple tenants?","diagram":null,"difficulty":"intermediate","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","OpenAI","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T15:50:41.588Z","createdAt":"2026-01-26T15:50:41.588Z"},{"id":"q-7888","question":"Design a per-tenant cohort dbt flow in Snowflake using a macro that boots analytics_{{tenant}}.session_cohort from staging.sessions (tenant_id, session_id, started_at, ended_at, device_type) and analytics.dim_devices. Produce per-tenant weekly cohorts (first_session_week, device_type) and churn flags. Implement incremental by week, allow 2-day late data, guard against schema drift, include tests (not_null on tenant_id, cohort_week, first_session, churn; unique on (tenant_id, cohort_week, device_type)); add a tenant registry seeds table tenants(tenant_id, created_at) to auto-create analytics_{{tenant}} schemas via macro; maintain a global analytics.lineage table. Provide CI steps to verify bootstrap of new tenants and per-tenant isolation by filtering on tenant_id. Also discuss trade-offs of per-tenant schemas vs single schema with tenant_id?","answer":"Design a per-tenant cohort dbt flow in Snowflake using a macro that boots analytics_{{tenant}}.session_cohort from staging.sessions (tenant_id, session_id, started_at, ended_at, device_type) and analytics.dim_devices. Produce per-tenant weekly cohorts (first_session_week, device_type) and churn flags. Implement incremental by week, allow 2-day late data, guard against schema drift, include tests (not_null on tenant_id, cohort_week, first_session, churn; unique on (tenant_id, cohort_week, device_type)); add a tenant registry seeds table tenants(tenant_id, created_at) to auto-create analytics_{{tenant}} schemas via macro; maintain a global analytics.lineage table. Provide CI steps to verify bootstrap of new tenants and per-tenant isolation by filtering on tenant_id. Also discuss trade-offs of per-tenant schemas vs single schema with tenant_id.","explanation":"## Why This Is Asked\nAssesses macro-driven tenant bootstrapping, multi-tenant isolation, and cohort analytics with late-data handling, plus lineage and CI validation.\n\n## Key Concepts\n- Macro-driven schema creation per tenant\n- Incremental weekly cohort modeling\n- Cross-tenant isolation via tenant_id scoping\n- Schema drift guards and tests\n- Data lineage table analytics.lineage\n\n## Code Example\n```sql\n-- Pseudo macro for schema bootstrap\n{% macro bootstrap_tenant(tenant_id) %}\n  -- creates analytics_{{tenant_id}} schemas and tables\n{% endmacro %}\n```\n\n## Follow-up Questions\n- How would you test tenant isolation in CI?\n- What monitoring would you implement for schema drift?\n- How do you handle tenant deprovisioning?","diagram":"flowchart TD\n  A[tenants seeds] --> B[Bootstrap analytics_{{tenant}} schema]\n  B --> C[Build analytics_{{tenant}}.session_cohort]\n  C --> D[Join with analytics.dim_devices]\n  D --> E[Compute first_session_week, churn]\n  E --> F[Incremental weekly load]\n  F --> G[Update analytics.lineage]\n  G --> H[CI tests for isolation]","difficulty":"intermediate","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","IBM","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T06:37:16.151Z","createdAt":"2026-01-26T21:43:08.921Z"},{"id":"q-7904","question":"Registry-driven multi-tenant dbt workflow: design a macro that auto-creates analytics_{{tenant}}.hourly_user_engagement from staging.tenant_events and a global analytics.lineage. Include 1-day late data, schema_drift guards, and tests (not_null: tenant_id, hour_start, user_id; unique on (tenant_id, hour_start, user_id)). Add CI gates to skip inactive tenants to avoid bleed, with clear rollback/reactivation steps?","answer":"Design a registry-driven macro that automatically creates analytics_{{tenant}}.hourly_user_engagement models from staging.tenant_events and a global analytics.lineage table. The solution must incorporate 1-day late data handling, schema_drift guards, and comprehensive tests including not_null constraints on tenant_id, hour_start, and user_id, plus uniqueness on the composite key (tenant_id, hour_start, user_id). Implement CI gates to skip inactive tenants and prevent data bleed, with clearly documented rollback and reactivation procedures.","explanation":"## Why This Is Asked\nTests the ability to design scalable, governance-friendly multi-tenant dbt workflows that dynamically adapt to active/inactive tenants without requiring redeployments or risking data contamination.\n\n## Key Concepts\n- Registry-driven tenancy patterns\n- Dynamic macro generation\n- Incremental model processing\n- Data governance and isolation\n- CI automation and rollback strategies\n\n## Code Example\n```sql\n-- dbt macro skeleton to generate tenant models\n{% macro generate_tenant_models(tenant_id) %}\n  {# implement macro that creates analytics_{{tenant_id}}.hourly_user_engagement #}\n{% endmacro %}\n```","diagram":"flowchart TD\n  A[Tenants Registry] --> B[Analytics per-tenant schema analytics_{{tenant}}]\n  B --> C[analytics.lineage]\n  A --> D[CI gating: skip inactive tenants]\n  D --> E[Rollback/reactivation]","difficulty":"advanced","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","IBM","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T06:17:41.915Z","createdAt":"2026-01-26T22:37:09.350Z"},{"id":"q-7981","question":"In Snowflake with dbt, design a per-tenant masked engagement flow from staging.tenant_events to analytics_{{tenant}}.masked_engagement using a macro that masks user_id for non-full-access roles. Include 1-day late data, schema drift guards, tests (not_null on keys, unique on (tenant_id, day_start, masked_user_id)), a global analytics.lineage, and CI that validates per-tenant views exist and isolates data?","answer":"Architect a per-tenant masked engagement flow by defining a Snowflake masking policy on user_id and exposing analytics_{{tenant}}.masked_engagement via a dbt macro analytics_build_view(tenant). The ma","explanation":"Why This Is Asked: Probes multi-tenant security and macro-driven dbt design. Key Concepts: dbt macros, Snowflake masking policies, per-tenant views, late data handling, lineage, tests, and CI isolation checks. Code Example: Implement masking policy and a macro-driven view generator. Follow-up Questions: 1) How to simulate roles in CI? 2) How to extend to additional sensitive fields?","diagram":"flowchart TD\n  A[staging.tenant_events] --> B[analytics_build_view(tenant)]\n  B --> C[analytics_{{tenant}}.masked_engagement]\n  C --> D[analytics.lineage]\n  C --> E[CI tests]","difficulty":"advanced","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Google","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T04:17:14.999Z","createdAt":"2026-01-27T04:17:14.999Z"},{"id":"q-8032","question":"Design a beginner dbt exercise to compute per-tenant weekly retention by cohort. With staging.signups(tenant_id, user_id, signed_up_at) and staging.active_days(tenant_id, user_id, day, active), build analytics_{{tenant}}.weekly_cohort_retention that groups cohorts by week_start(signed_up_at) and measures retention by days_since_signup (0-6, 7-13). Use a macro to create per-tenant schemas analytics_{{tenant}} and a global analytics.lineage. Include 1-day late data, not_null tests, and CI ensuring per-tenant tables exist and data stays isolated?","answer":"Implement a dbt macro that materializes analytics_{{tenant}}.weekly_cohort_retention as an incremental table from staging.signups and staging.active_days, computing week_start and days_since_signup, a","explanation":"## Why This Is Asked\nTests per-tenant cohort retention design, macro scaffolding, and CI for isolation, a realistic starter for multi-tenant analytics.\n\n## Key Concepts\n- dbt macros for per-tenant objects\n- incremental models and late-arriving data\n- cohort retention calculations\n- cross-tenant isolation and lineage\n\n## Code Example\n```sql\n-- macro example (pseudo)\n{% macro analytics_tenant_weekly_retention() %}\n  SELECT\n    tenant_id,\n    week_start(signed_up_at) AS week_start,\n    COUNT(DISTINCT user_id) AS retained_users\n  FROM {{ ref('staging_signups') }} s\n  JOIN {{ ref('staging_active_days') }} a USING (tenant_id, user_id)\n  WHERE a.active = TRUE\n  GROUP BY tenant_id, week_start\n{% endmacro %}\n```\n\n## Follow-up Questions\n- How would you adjust for tenants with sparse data?\n- How would you test cross-tenant bleed in CI?","diagram":null,"difficulty":"beginner","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Lyft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T06:53:39.294Z","createdAt":"2026-01-27T06:53:39.295Z"},{"id":"q-8164","question":"Design a tenant-aware data quality and security flow in Snowflake + dbt. Create a macro to build analytics_{{tenant}}.hourly_engagement from staging.tenant_events, plus a global analytics.lineage. Implement per-tenant row-level security via Snowflake policies tied to a tenants registry, ensuring cross-tenant isolation. Include 1-day late data, schema-drift guards, and tests (not_null on keys, unique on (tenant_id, hour_start, user_id)). Add CI to verify policies exist for all tenants and auto-provision on registry changes?","answer":"Macro-driven per-tenant schemas and hourly_engagement from staging; per-tenant ROW ACCESS POLICIES tied to a tenants registry enforce isolation; 1-day late data with a Snowflake TASK; tests cover not_","explanation":"## Why This Is Asked\n\nAssesses ability to design scalable, secure, multi-tenant dbt pipelines with automated provisioning and governance.\n\n## Key Concepts\n\n- Tenant-aware schemas and macros\n- Snowflake ROW ACCESS POLICIES linked to a runtime registry\n- Late-arrival data handling and drift guards\n- Global lineage and CI validation\n\n## Code Example\n\n```sql\n-- Macro sketch (dbt) to create tenant schema and table\n{% macro create_tenant_schema(tenant) -%}\ncreate schema if not exists analytics_{{ tenant }};\ncreate table analytics_{{ tenant }}.hourly_engagement (...);\n{% endmacro %}\n```\n\n```sql\n-- Policy sketch (Snowflake)\nCREATE OR REPLACE ROW ACCESS POLICY tenant_rls ON analytics_{{tenant}}.hourly_engagement USING (tenant_id = current_tenant_id());\n```\n\n## Follow-up Questions\n\n- How would you test cross-tenant leakage in CI?\n- How would you scale policy management for hundreds of tenants?","diagram":"flowchart TD\n  A[tenants registry] --> B[ROW ACCESS POLICIES per tenant]\n  B --> C[analytics_{{tenant}}.hourly_engagement]\n  A --> D[analytics.lineage]\n  E[staging.tenant_events] --> B\n","difficulty":"advanced","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Cloudflare"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T11:53:21.096Z","createdAt":"2026-01-27T11:53:21.096Z"},{"id":"q-8281","question":"Design a multi-tenant dbt flow in Snowflake for computing daily_user_engagement per tenant from staging.tenant_events (tenant_id, user_id, event_type, occurred_at, session_id, record_hash). Create via a macro analytics_{{tenant}}.daily_user_engagement, incremental by day_start. Include 1-day late data, dynamic schema drift guards, and tests (not_null on keys; unique on (tenant_id, day_start, user_id)). Build a global analytics.lineage table to track dependencies. Explain dedup logic for late/duplicate events and how you validate cross-tenant isolation in CI?","answer":"Use a per-tenant macro to create analytics_{{tenant}}.daily_user_engagement and backfill with a Snowflake stream on staging.tenant_events; perform an incremental MERGE by day_start with a dedup key (t","explanation":"## Why This Is Asked\nThis question probes practical dbt multi-tenant design: per-tenant schemas, incremental builds, late data handling, and data lineage. It also tests dedup logic and CI validation for isolation.\n\n## Key Concepts\n- Multi-tenant isolation and per-tenant schemas\n- Incremental dbt with day_start partitioning\n- Late data handling and process_offset\n- Deduplication using record_hash with MERGE\n- Schema drift guards and tests\n- Data lineage tracking in analytics.lineage\n\n## Code Example\n```sql\n-- Pseudo MERGE for daily engagement\nMERGE INTO analytics_{{tenant}}.daily_user_engagement AS target\nUSING (\n  SELECT tenant_id, user_id, day_start, event_type, MAX(record_hash) AS hash\n  FROM {{ref('staging.tenant_events')}}\n  WHERE occurred_at >= DATEADD(day, -1, CURRENT_DATE())\n  GROUP BY tenant_id, user_id, day_start, event_type\n) AS src\nON (target.tenant_id = src.tenant_id AND target.day_start = src.day_start AND target.user_id = src.user_id)\nWHEN MATCHED THEN UPDATE SET event_type = src.event_type\nWHEN NOT MATCHED THEN INSERT (...);\n```\n\n## Follow-up Questions\n- How would you monitor late-arriving events across tenants in CI?\n- What strategies optimize macro test coverage for new tenants?","diagram":null,"difficulty":"intermediate","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","NVIDIA","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T17:57:14.529Z","createdAt":"2026-01-27T17:57:14.530Z"},{"id":"q-8287","question":"Design a per-tenant incremental dbt flow in Snowflake that computes analytics_{{tenant}}.hourly_engagement from staging.tenant_events (tenant_id, user_id, action, occurred_at). Implement an idempotent MERGE upsert that accommodates 2 hours of late data, uses a run_id to avoid duplicates, and a macro that auto-creates analytics_{{tenant}} schemas plus a global analytics.lineage. Add schema-drift guards and tests (not_null on keys, unique on (tenant_id, hour_start, user_id)); include a CI check that per-tenant tables exist and results are reproducible on rerun?","answer":"I would implement a per-tenant incremental model with a run_id and an idempotent MERGE into analytics_{{tenant}}.hourly_engagement from staging.tenant_events, keyed by (tenant_id, hour_start, user_id)","explanation":"## Why This Is Asked\nTests the ability to design robust, tenant‑isolated, idempotent dbt flows with late data and lineage tracking; evaluates schema drift guards and CI reproducibility.\n\n## Key Concepts\n- dbt incremental strategies and MERGE\n- tenant‑scoped schemas and macro generation\n- data lineage and cross‑tenant isolation\n- late data handling and schema drift tests\n- CI validation and reproducibility\n\n## Code Example\n```javascript\n// Macro skeleton: tenant schema name\n{% macro tenant_schema(tenant) %}\nanalytics_{{- tenant -}}.hourly_engagement\n{% endmacro %}\n\n// Simple idempotence key\nfunction key(tenant, hour, user) { return `${tenant}::${hour}::${user}`; }\n```\n\n## Follow-up Questions\n- How would you test partial failures during MERGE?\n- How would you detect and rollback a bad tenant schema migration?","diagram":null,"difficulty":"intermediate","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","Netflix","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T18:53:59.468Z","createdAt":"2026-01-27T18:53:59.468Z"},{"id":"q-8429","question":"In Snowflake with dbt, given staging.events(tenant_id, user_id, event, occurred_at) and seeds/tenants(tenant_id, time_zone), design a beginner dbt flow that builds analytics_{{tenant}}.daily_engagement using occurred_at converted to tenant local date via a macro that creates analytics_{{tenant}} schemas and a global analytics.lineage. Include 1-day late data, schema-drift guards, tests not_null on keys, unique (tenant_id, day_start, user_id), and a CI check that per-tenant analytics exist and no cross-tenant bleed in time zones. Explain how you'd validate isolation?","answer":"I'll design a tenant-aware dbt flow that builds per-tenant analytics with proper time zone conversion and isolation safeguards. The solution uses a macro that loops through active tenants, creates dedicated analytics_{{tenant}} schemas, and builds daily_engagement tables by converting occurred_at to each tenant's local time zone using CONVERT_TIMEZONE. The flow includes 1-day late data handling, schema drift guards, and comprehensive testing with not_null constraints on keys and unique constraints on (tenant_id, day_start, user_id). A global analytics.lineage table tracks data lineage across all tenants while CI checks ensure per-tenant analytics exist and prevent cross-tenant time zone bleed.","explanation":"## Why This Is Asked\n\nThis question tests critical skills in multi-tenant data architecture: time zone handling, dynamic modeling, and isolation validation. It's essential for SaaS companies where tenant data must remain properly segmented while supporting local time contexts.\n\n## Key Concepts\n\n- Per-tenant time zone conversion using CONVERT_TIMEZONE\n- Macro-driven schema generation (analytics_{{tenant}}.*)\n- Schema drift protection with data type guards\n- Global lineage tracking with tenant metadata\n- CI validation for tenant parity and isolation\n\n## Code Example\n\n```sql\n{% macro tenant_model(tenant_id, time_zone) %}\nSELECT\n  '{{ tenant_id }}' AS tenant_id,\n  user_id,\n  DATE_TRUNC('day', CONVERT_TIMEZONE('{{ time_zone }}', occurred_at)) AS day_start,\n  event\nFROM {{ ref('staging_events') }}\nWHERE tenant_id = '{{ tenant_id }}'\n  AND occurred_at >= DATEADD(day, -1, CURRENT_TIMESTAMP())\n{% endmacro %}\n```\n\n## Validation Approach\n\nValidate isolation through: 1) Per-tenant schema separation ensuring data segregation, 2) Time zone conversion testing confirming accurate local date mapping, 3) Unique constraint verification preventing cross-tenant contamination, and 4) CI checks validating tenant completeness and schema consistency across all active tenants.","diagram":null,"difficulty":"beginner","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","DoorDash","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-28T04:44:33.212Z","createdAt":"2026-01-27T23:55:10.163Z"},{"id":"q-848","question":"You're designing a dbt analytics pipeline for an e-commerce platform. The 'staging.events_raw' table ingests page views and purchases and is partitioned by day. Data arrives both on time and as late as 24 hours. Design a practical incremental dbt model 'analytics.daily_sales' that aggregates revenue by day, category, and customer_segment. Explain how you'd implement incremental logic, handle late data, guard against schema drift, and validate with tests and snapshots?","answer":"Use an incremental model keyed by day, category, and customer_segment. Compute revenue as sum(price*quantity) from staging.events_raw. Upsert into analytics.daily_sales via MERGE on (day, category, cu","explanation":"## Why This Is Asked\n\nAssess ability to design scalable, fault-tolerant dbt pipelines handling late data and schema drift.\n\n## Key Concepts\n\n- Incremental models and idempotent MERGE\n- Late-arrival handling and backfill windows\n- Schema drift detection via tests and snapshots\n- Performance at scale and CI validation\n\n## Code Example\n\n```sql\n-- Example incremental sql sketch\nWITH s AS (\n  SELECT day, category, customer_segment,\n         SUM(price * quantity) AS revenue\n  FROM {{ ref('staging__events_raw') }}\n  WHERE day >= (SELECT MIN(day) FROM analytics.daily_sales)\n  GROUP BY day, category, customer_segment\n)\nMERGE INTO analytics.daily_sales AS t\nUSING s AS s\nON t.day = s.day AND t.category = s.category AND t.customer_segment = s.customer_segment\nWHEN MATCHED THEN UPDATE SET revenue = s.revenue\nWHEN NOT MATCHED THEN INSERT (day, category, customer_segment, revenue) VALUES (s.day, s.category, s.customer_segment, s.revenue);\n```\n\n## Follow-up Questions\n\n- How would you test for idempotency and late-arrival correctness?\n- How would you adapt this for a lakehouse (Parquet) or streaming source?","diagram":null,"difficulty":"advanced","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Anthropic","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:30:20.377Z","createdAt":"2026-01-12T13:30:20.377Z"},{"id":"q-8490","question":"In Snowflake with dbt, design a cross-tenant daily_metrics engine that materializes analytics_{{tenant}}.daily_metrics from staging.tenant_events (tenant_id, user_id, event_type, occurred_at, revenue) and a customers table. Build an incremental by day_start with 2-day late data, include schema drift guards, tests (not_null on keys, unique on (tenant_id, day_start)), and a per-tenant customers snapshot to compute churn. Add a global analytics.lineage table and CI cross-tenant validation for isolation and lineage?","answer":"Implement a macro that creates analytics_{{tenant}}.daily_metrics as an incremental by day_start from staging.tenant_events (tenant_id, user_id, event_type, occurred_at, revenue) and a customers snaps","explanation":"## Why This Is Asked\nTests real-world dbt patterns for multi-tenant isolation, incremental pipelines, late data handling, and data lineage governance.\n\n## Key Concepts\n- Macro-driven tenant model generation\n- Incremental by day_start with late data window\n- Schema drift guards and validation tests\n- Per-tenant churn via customer snapshot\n- Global lineage table and CI checks for isolation\n\n## Code Example\n```javascript\n// pseudo-macro sketch (dbt macros/text only)\n{% macro daily_metrics_sql(tenant) %}\n-- returns analytics_{{tenant}}.daily_metrics SQL\n{% endmacro %}\n```\n\n## Follow-up Questions\n- How would you detect and recover from cross-tenant data bleed in CI?\n- How would you adapt this for different data freshness SLAs per tenant?","diagram":null,"difficulty":"intermediate","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["OpenAI","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-28T05:45:02.756Z","createdAt":"2026-01-28T05:45:02.756Z"},{"id":"q-972","question":"In a dbt analytics project deployed to Snowflake, three tenants share a common model but data is isolated by schema prefixes (tenant_a_, tenant_b_, tenant_c_). Describe how you would structure tenancy-aware tests and a test runner to prevent cross-tenant data leakage during PR runs. Include how you configure sources, seeds, and per-tenant test filtering, and how you verify isolation in CI without touching production data?","answer":"A tenancy-aware dbt run parameterizes the target schema per tenant, isolates seeds and sources with tenant prefixes, and uses a tenancy-aware test macro to scope checks. In CI, create ephemeral tenant","explanation":"## Why This Is Asked\nThis question probes how a candidate ensures strict data isolation in a multi-tenant dbt setup during CI, a common real-world issue.\n\n## Key Concepts\n- Tenancy-aware testing across schemas\n- Per-tenant seeds/sources isolation\n- Test filtering macros and CI ephemeral environments\n- Cross-tenant leakage detection without touching prod\n\n## Code Example\n```javascript\n-- macros/tenancy_filters.sql\n{% macro tenant_test_filter(tenant) -%}\n  tenant_id = '{{ tenant }}'\n{%- endmacro %}\n```\n\n```javascript\n-- tests/tenancy_isolation_test.sql\nselect * from {{ ref('customers') }}\nwhere {{ tenant_test_filter('tenant_a') }}\n```\n\n## Follow-up Questions\n- How would you extend this for dynamic tenant onboarding?\n- How do you validate that physical storage quotas per tenant are respected?","diagram":"flowchart TD\n  PR[Pull Request] --> TESTS[Run tenancy-aware tests]\n  TESTS --> PASS{All tenant tests pass?}\n  PASS --> MERGE[Merge PR]\n  PASS --> REBUILD[Rebuild docs if needed]\n  FAIL --> NOTIFY[Notify engineer]","difficulty":"advanced","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Lyft","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T17:35:20.447Z","createdAt":"2026-01-12T17:35:20.447Z"}],"subChannels":["general"],"companies":["Adobe","Airbnb","Amazon","Anthropic","Apple","Bloomberg","Citadel","Cloudflare","Coinbase","Databricks","Discord","DoorDash","Goldman Sachs","Google","Hashicorp","Hugging Face","IBM","Instacart","LinkedIn","Lyft","Meta","Microsoft","MongoDB","NVIDIA","Netflix","OpenAI","Oracle","PayPal","Plaid","Robinhood","Salesforce","Scale Ai","Slack","Snap","Snowflake","Square","Stripe","Tesla","Twitter","Two Sigma","Uber","Zoom"],"stats":{"total":93,"beginner":34,"intermediate":39,"advanced":20,"newThisWeek":35}}