{"questions":[{"id":"dbt-analytics-engineer-data-modeling-1768210358285-0","question":"A retail company wants to preserve historical changes to customer segments (e.g., Silver, Gold) tied to a customer profile. The daily feed updates the master customer table with new segment values. Which approach best implements Slowly Changing Dimensions Type 2 in dbt to retain history and enable time-travel in BI dashboards?","answer":"[{\"id\":\"a\",\"text\":\"Use dbt snapshots on the customer_dim to capture history and output a history-enabled table with start_date and end_date.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Use a dedicated ETL job that appends to a separate history table on every change, without using dbt snapshots.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Rely on the source system's CDC and refresh the entire dimension table daily, discarding history on restatement.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Overwrite the dimension table with only the latest segment value, losing history.\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nUse dbt snapshots on the customer_dim to capture history and output a history-enabled table with start_date and end_date.\n\n## Why Other Options Are Wrong\n\n- Option B is incorrect because it bypasses dbt snapshots and risks maintaining history in an unmanaged, inconsistent way.\n- Option C is incorrect because relying on CDC without built-in versioning can miss past states and breaks the single source of truth for history.\n- Option D is incorrect because overwriting loses history, preventing time-travel in BI dashboards.\n\n## Key Concepts\n\n- Slowly Changing Dimensions Type 2\n- dbt snapshots\n- History tables\n- Data lineage\n\n## Real-World Application\n\nWhen a customer moves from Silver to Gold, a snapshot records a new historical row with its own validity window, enabling dashboards to show the customer segment at any point in time.","diagram":null,"difficulty":"intermediate","tags":["dbt","data-modeling","data-warehousing","aws-redshift","eks","terraform","certification-mcq","domain-weight-25"],"channel":"dbt-analytics-engineer","subChannel":"data-modeling","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T09:32:38.286Z","createdAt":"2026-01-12 09:32:38"},{"id":"dbt-analytics-engineer-data-modeling-1768210358285-1","question":"In a data model where a Customer can have multiple preferences (for example, email, SMS, push) and you want to avoid denormalization while maintaining a clean many-to-many relationship, which modeling approach should you use in dbt?","answer":"[{\"id\":\"a\",\"text\":\"Create a bridge table customer_preferences with customer_id and preference_id and join in queries.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Duplicate customer rows for each preference in the customer_dim.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Store preferences as a JSON array in customer_dim and parse in queries.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Add a new column on customer_dim with a list of preferences as a string.\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nCreate a bridge table customer_preferences with customer_id and preference_id and join in queries.\n\n## Why Other Options Are Wrong\n\n- Option B duplicating customer rows increases dimensionality, harms readability, and complicates joins.\n- Option C storing as JSON reduces queryability and hurts standard BI tooling compatibility.\n- Option D storing a list in a single column complicates filtering and analytics and violates normalization principles.\n\n## Key Concepts\n\n- Many-to-many relationships\n- Bridge (link) tables\n- Normalized dimensional modeling\n\n## Real-World Application\n\nAllows analytics to answer questions like which customers have multiple preferences without duplicating customer records across facts.","diagram":null,"difficulty":"intermediate","tags":["dbt","data-modeling","data-warehousing","aws-redshift","terraform","eks","certification-mcq","domain-weight-25"],"channel":"dbt-analytics-engineer","subChannel":"data-modeling","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T09:32:38.823Z","createdAt":"2026-01-12 09:32:39"},{"id":"dbt-analytics-engineer-data-modeling-1768210358285-2","question":"You need to enforce durable surrogate keys in a dimensional model built with dbt across incremental loads and late-arriving data. Which practice provides stable surrogate keys that can be reconstructed if needed?","answer":"[{\"id\":\"a\",\"text\":\"Use an auto-increment surrogate key in the target data warehouse.\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use a deterministic surrogate key derived from hashing the business key to ensure stability.\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Use a composite surrogate key built from all attributes of the dimension row.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Generate surrogate keys in the source and pass-through to the target.\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nUse a deterministic surrogate key derived from hashing the business key to ensure stability across incremental loads and late-arriving data. This provides a durable, reproducible key that does not depend on load order or data arrival timing.\n\n## Why Other Options Are Wrong\n\n- Option A is incorrect because auto-increment keys can shift when data is reloaded or merged from multiple sources, breaking reproducibility.\n- Option C is incorrect because a composite key from all attributes is heavy and fragile to changes in non-key attributes.\n- Option D is incorrect because generating keys in the source reduces control and can lead to mismatches if source isnâ€™t fully trusted or replicated.\n\n## Key Concepts\n\n- Surrogate keys\n- Deterministic hashing for keys\n- Data lineage and reproducibility\n\n## Real-World Application\n\nHashing the business key (plus a version indicator if needed) yields stable surrogate keys that remain consistent across full or partial reloads, enabling reliable slowly changing dimension handling.","diagram":null,"difficulty":"intermediate","tags":["dbt","data-modeling","data-warehousing","aws-redshift","terraform","eks","certification-mcq","domain-weight-25"],"channel":"dbt-analytics-engineer","subChannel":"data-modeling","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T09:32:39.359Z","createdAt":"2026-01-12 09:32:39"},{"id":"dbt-analytics-engineer-dbt-fundamentals-1768170078445-0","question":"In a dbt project, you maintain a large fact table with an incremental model keyed by order_id. New data fills daily, but some existing orders have updates. Which approach best ensures both new inserts and updates are reflected without full refresh?","answer":"[{\"id\":\"a\",\"text\":\"Implement a MERGE-based upsert inside the incremental block to handle updates and inserts\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Rely on dbt's default incremental behavior using only simple insert for new rows\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use a full_refresh on every run to guarantee correctness\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use a separate snapshot model to track changes\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nA. Implement a MERGE-based upsert inside the incremental block to handle updates and inserts.\n\n## Why Other Options Are Wrong\n\n- B: The default incremental behavior inserts only new rows and does not handle updates, which can lead to duplicates or stale data.\n- C: Full refresh rebuilds the entire table every run, negating the benefits of incremental loads and consuming unnecessary compute.\n- D: Snapshots track history for slowly changing dimensions and are not a substitute for upsert logic in incremental fact tables.\n\n## Key Concepts\n\n- Incremental models can perform upserts with MERGE to handle updates.\n- is_incremental() pattern guides conditional SQL in dbt.\n\n## Real-World Application\n\n- When feeding daily operational data into large fact tables, implementing a MERGE-based upsert in the incremental path ensures data accuracy while avoiding costly full reloads.","diagram":null,"difficulty":"intermediate","tags":["dbt","AWS","Snowflake","Terraform","Kubernetes","certification-mcq","domain-weight-25"],"channel":"dbt-analytics-engineer","subChannel":"dbt-fundamentals","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T22:21:18.446Z","createdAt":"2026-01-11 22:21:18"},{"id":"dbt-analytics-engineer-dbt-fundamentals-1768170078445-1","question":"You want to enforce referential integrity between a fact table and a date dimension in dbt. You need to prevent orphaned records. Which approach is most appropriate in dbt?","answer":"[{\"id\":\"a\",\"text\":\"Create a relationships test on the foreign key to the date_dim table\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Use a freshness test on the date dimension to ensure it's up to date\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Create a snapshot to capture historical changes\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use a seed to load fixed date_dim data\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nA. Create a relationships test on the foreign key to the date_dim table.\n\n## Why Other Options Are Wrong\n\n- B: Freshness tests ensure the data source is up-to-date, not referential integrity between tables.\n- C: Snapshots track history in slowly changing dimensions, not enforce foreign-key referential integrity.\n- D: Seeds populate static data but do not validate relationships between existing tables.\n\n## Key Concepts\n\n- dbt tests can enforce referential integrity via relationships tests.\n- Relationships tests help catch orphaned records during CI.\n\n## Real-World Application\n\n- Adding a relationships test helps guard against data quality regressions when new foreign-key relationships are introduced or when date_dim is updated.","diagram":null,"difficulty":"intermediate","tags":["dbt","AWS","Redshift","Kubernetes","Terraform","certification-mcq","domain-weight-25"],"channel":"dbt-analytics-engineer","subChannel":"dbt-fundamentals","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T22:21:18.936Z","createdAt":"2026-01-11 22:21:19"},{"id":"dbt-analytics-engineer-dbt-fundamentals-1768170078445-2","question":"During deployment to Snowflake, you want incremental models to load only new or changed rows rather than rewriting entire tables to optimize compute. Which strategy best achieves this in dbt?","answer":"[{\"id\":\"a\",\"text\":\"Implement upsert logic in the incremental block using MERGE statements\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Force full_refresh on every deployment to ensure consistency\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Convert incremental models to views to leverage dynamic querying\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use dbt test to enforce incremental semantics during runs\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nA. Implement upsert logic in the incremental block using MERGE statements.\n\n## Why Other Options Are Wrong\n\n- B: Full_refresh negates the benefits of incremental loads and is compute-intensive.\n- C: Views always recompute on every query and do not persist transformed data like incremental tables.\n- D: Tests validate data quality but do not control how data is written during runs.\n\n## Key Concepts\n\n- MERGE-based upserts in incremental dbt models for upserts.\n- Snowflake supports MERGE in dbt incremental implementations.\n\n## Real-World Application\n\n- In a Snowflake-based analytics stack, using MERGE in incremental paths lets you add new rows and update existing ones efficiently, reducing compute and improving freshness.","diagram":null,"difficulty":"intermediate","tags":["dbt","Snowflake","AWS","Kubernetes","Terraform","certification-mcq","domain-weight-25"],"channel":"dbt-analytics-engineer","subChannel":"dbt-fundamentals","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T22:21:19.427Z","createdAt":"2026-01-11 22:21:19"}],"subChannels":["data-modeling","dbt-fundamentals"],"companies":[],"stats":{"total":6,"beginner":0,"intermediate":6,"advanced":0,"newThisWeek":6}}