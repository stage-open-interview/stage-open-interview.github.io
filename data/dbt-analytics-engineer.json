{"questions":[{"id":"q-1125","question":"You're building an incremental dbt model analytics.daily_revenue_by_product sourced from staging.sales_raw. Data can arrive late up to 2 days. How would you implement incremental upserts, reprocess late days without disturbing older data, and validate with tests and a snapshot of product price history? Provide a minimal SQL snippet illustrating the approach?","answer":"Implement an incremental model with unique_key=['day','product_id','region']; reprocess the last 2 days during incremental runs to absorb late data; use MERGE/upsert where supported, else a two-phase ","explanation":"## Why This Is Asked\nTests practical incremental logic and late-arrival handling; checks upsert patterns, tests, and snapshots.\n\n## Key Concepts\n- Incremental materialization with unique keys\n- Late-arrival handling window\n- Snapshot for price history\n- Basic tests for data quality\n\n## Code Example\n```sql\n-- models/analytics/daily_revenue_by_product.sql\n{{ config(materialized='incremental', unique_key=['day','product_id','region']) }}\n\nwith s as (\n  select date_trunc('day', order_date) as day,\n         product_id,\n         region,\n         sum(quantity * price) as revenue\n  from {{ ref('staging_sales_raw') }}\n  group by 1,2,3\n)\n\nselect day, product_id, region, revenue\nfrom s\n{% if is_incremental() %}\n  -- reprocess last 2 days to capture late data\n  where day >= (select max(day) from {{ this }}) - interval '2 day'\n{% endif %}\n```\n\n## Follow-up Questions\n- How would you adapt this for Redshift vs Snowflake?\n- How would you test the snapshot consistency across environments?","diagram":null,"difficulty":"beginner","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Hashicorp"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T23:31:34.187Z","createdAt":"2026-01-12T23:31:34.187Z"},{"id":"q-1165","question":"You're designing a dbt analytics pipeline for a ride-sharing platform. The raw events are in `staging.events_raw` with columns like `ride_id`, `city`, `ride_type`, `currency`, `amount`, `occurred_at`, and data can arrive up to 48 hours late. Build a beginner-friendly incremental model `analytics.daily_revenue` that computes USD revenue per day by `date`, `city`, and `ride_type`. Use a daily `pricing.exchange_rates` table to convert from local currency to USD. Describe how you'd implement incremental upserts, late-data handling (2-day window), schema-drift guards, and validation with tests and a snapshot. Also outline tests for late refunds and missing rates?","answer":"Use an incremental model with is_incremental(). Join staging events to pricing.exchange_rates on date and currency, then aggregate revenue in USD by date, city, ride_type. Implement a 2-day late-data ","explanation":"## Why This Is Asked\nThe question probes practical incremental modeling with late data, currency conversion, and drift safeguardsâ€”core dbt skills at junior to mid-beginner level.\n\n## Key Concepts\n- Incremental models and upserts\n- Currency conversion via reference table\n- Late-data window handling\n- Schema-drift guards and tests\n- Snapshots for drift detection\n\n## Code Example\n```sql\n-- dbt model: analytics/daily_revenue.sql\nwith events as (\n  select\n    date_trunc('day', occurred_at) as date,\n    city,\n    ride_type,\n    currency,\n    amount\n  from {{ source('staging','events_raw') }}\n  where occurred_at <= (current_timestamp() - interval '2 days')\n),\nrates as (\n  select date, currency, rate\n  from {{ source('pricing','exchange_rates') }}\n)\nselect\n  e.date,\n  e.city,\n  e.ride_type,\n  sum(e.amount * coalesce(r.rate, 1.0)) as revenue_usd\nfrom events e\nleft join rates r\n  on r.date = e.date and r.currency = e.currency\ngroup by 1,2,3\n```\n\n## Follow-up Questions\n- How would you handle gaps in exchange_rates (missing rates)?\n- How would you validate late refunds affecting revenue?\n- How would you test for time zone consistency across cities?","diagram":null,"difficulty":"beginner","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T03:31:16.675Z","createdAt":"2026-01-13T03:31:16.675Z"},{"id":"q-1257","question":"Design a drift-aware dbt workflow across dev/staging/prod in Snowflake. How would you detect schema drift between sources and models using snapshots, tests, and sources, enforce a drift threshold, and automatically fail CI when drift exceeds the threshold? Describe the macro, tests, and alert/rollback mechanism, plus how you version thresholds?","answer":"Implement a dbt macro drift_check that compares per-column metadata (data type, nullability, default, and max length) between a source table and its corresponding model, plus a runtime comparison of r","explanation":"## Why This Is Asked\nReal-world pipelines require automated detection of schema drift to prevent deployment of broken analytics. This question tests practical macro design, test coverage, and CI/CD integration.\n\n## Key Concepts\n- dbt snapshots and sources for metadata\n- Per-column drift checks (type, nullability, max length)\n- Threshold-driven gating in CI/CD and rollback policies\n- Canary prod lineage validation and alerting\n\n## Code Example\n```jinja\n{% macro drift_check(source_schema, source_table, model_schema, model_table, thresholds) -%}\n-- pseudo-logic: compare metadata and basic stats, emit drift_count if > thresholds\n{%- endmacro %}\n```\n\n## Follow-up Questions\n- How would you scale per-table thresholds across hundreds of tables?\n- How would you distinguish benign drift (e.g., new optional column) from breaking drift?","diagram":null,"difficulty":"advanced","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Google","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T07:23:31.079Z","createdAt":"2026-01-13T07:23:31.079Z"},{"id":"q-848","question":"You're designing a dbt analytics pipeline for an e-commerce platform. The 'staging.events_raw' table ingests page views and purchases and is partitioned by day. Data arrives both on time and as late as 24 hours. Design a practical incremental dbt model 'analytics.daily_sales' that aggregates revenue by day, category, and customer_segment. Explain how you'd implement incremental logic, handle late data, guard against schema drift, and validate with tests and snapshots?","answer":"Use an incremental model keyed by day, category, and customer_segment. Compute revenue as sum(price*quantity) from staging.events_raw. Upsert into analytics.daily_sales via MERGE on (day, category, cu","explanation":"## Why This Is Asked\n\nAssess ability to design scalable, fault-tolerant dbt pipelines handling late data and schema drift.\n\n## Key Concepts\n\n- Incremental models and idempotent MERGE\n- Late-arrival handling and backfill windows\n- Schema drift detection via tests and snapshots\n- Performance at scale and CI validation\n\n## Code Example\n\n```sql\n-- Example incremental sql sketch\nWITH s AS (\n  SELECT day, category, customer_segment,\n         SUM(price * quantity) AS revenue\n  FROM {{ ref('staging__events_raw') }}\n  WHERE day >= (SELECT MIN(day) FROM analytics.daily_sales)\n  GROUP BY day, category, customer_segment\n)\nMERGE INTO analytics.daily_sales AS t\nUSING s AS s\nON t.day = s.day AND t.category = s.category AND t.customer_segment = s.customer_segment\nWHEN MATCHED THEN UPDATE SET revenue = s.revenue\nWHEN NOT MATCHED THEN INSERT (day, category, customer_segment, revenue) VALUES (s.day, s.category, s.customer_segment, s.revenue);\n```\n\n## Follow-up Questions\n\n- How would you test for idempotency and late-arrival correctness?\n- How would you adapt this for a lakehouse (Parquet) or streaming source?","diagram":null,"difficulty":"advanced","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Anthropic","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T13:30:20.377Z","createdAt":"2026-01-12T13:30:20.377Z"},{"id":"q-972","question":"In a dbt analytics project deployed to Snowflake, three tenants share a common model but data is isolated by schema prefixes (tenant_a_, tenant_b_, tenant_c_). Describe how you would structure tenancy-aware tests and a test runner to prevent cross-tenant data leakage during PR runs. Include how you configure sources, seeds, and per-tenant test filtering, and how you verify isolation in CI without touching production data?","answer":"A tenancy-aware dbt run parameterizes the target schema per tenant, isolates seeds and sources with tenant prefixes, and uses a tenancy-aware test macro to scope checks. In CI, create ephemeral tenant","explanation":"## Why This Is Asked\nThis question probes how a candidate ensures strict data isolation in a multi-tenant dbt setup during CI, a common real-world issue.\n\n## Key Concepts\n- Tenancy-aware testing across schemas\n- Per-tenant seeds/sources isolation\n- Test filtering macros and CI ephemeral environments\n- Cross-tenant leakage detection without touching prod\n\n## Code Example\n```javascript\n-- macros/tenancy_filters.sql\n{% macro tenant_test_filter(tenant) -%}\n  tenant_id = '{{ tenant }}'\n{%- endmacro %}\n```\n\n```javascript\n-- tests/tenancy_isolation_test.sql\nselect * from {{ ref('customers') }}\nwhere {{ tenant_test_filter('tenant_a') }}\n```\n\n## Follow-up Questions\n- How would you extend this for dynamic tenant onboarding?\n- How do you validate that physical storage quotas per tenant are respected?","diagram":"flowchart TD\n  PR[Pull Request] --> TESTS[Run tenancy-aware tests]\n  TESTS --> PASS{All tenant tests pass?}\n  PASS --> MERGE[Merge PR]\n  PASS --> REBUILD[Rebuild docs if needed]\n  FAIL --> NOTIFY[Notify engineer]","difficulty":"advanced","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Lyft","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T17:35:20.447Z","createdAt":"2026-01-12T17:35:20.447Z"}],"subChannels":["general"],"companies":["Amazon","Anthropic","Apple","DoorDash","Goldman Sachs","Google","Hashicorp","Lyft","Meta","OpenAI","PayPal","Tesla"],"stats":{"total":5,"beginner":2,"intermediate":0,"advanced":3,"newThisWeek":5}}