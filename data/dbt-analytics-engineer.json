{"questions":[{"id":"dbt-analytics-engineer-data-modeling-1768210358285-0","question":"A retail company wants to preserve historical changes to customer segments (e.g., Silver, Gold) tied to a customer profile. The daily feed updates the master customer table with new segment values. Which approach best implements Slowly Changing Dimensions Type 2 in dbt to retain history and enable time-travel in BI dashboards?","answer":"[{\"id\":\"a\",\"text\":\"Use dbt snapshots on the customer_dim to capture history and output a history-enabled table with start_date and end_date.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Use a dedicated ETL job that appends to a separate history table on every change, without using dbt snapshots.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Rely on the source system's CDC and refresh the entire dimension table daily, discarding history on restatement.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Overwrite the dimension table with only the latest segment value, losing history.\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nUse dbt snapshots on the customer_dim to capture history and output a history-enabled table with start_date and end_date.\n\n## Why Other Options Are Wrong\n\n- Option B is incorrect because it bypasses dbt snapshots and risks maintaining history in an unmanaged, inconsistent way.\n- Option C is incorrect because relying on CDC without built-in versioning can miss past states and breaks the single source of truth for history.\n- Option D is incorrect because overwriting loses history, preventing time-travel in BI dashboards.\n\n## Key Concepts\n\n- Slowly Changing Dimensions Type 2\n- dbt snapshots\n- History tables\n- Data lineage\n\n## Real-World Application\n\nWhen a customer moves from Silver to Gold, a snapshot records a new historical row with its own validity window, enabling dashboards to show the customer segment at any point in time.","diagram":null,"difficulty":"intermediate","tags":["dbt","data-modeling","data-warehousing","aws-redshift","eks","terraform","certification-mcq","domain-weight-25"],"channel":"dbt-analytics-engineer","subChannel":"data-modeling","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T09:32:38.286Z","createdAt":"2026-01-12 09:32:38"},{"id":"dbt-analytics-engineer-data-modeling-1768210358285-1","question":"In a data model where a Customer can have multiple preferences (for example, email, SMS, push) and you want to avoid denormalization while maintaining a clean many-to-many relationship, which modeling approach should you use in dbt?","answer":"[{\"id\":\"a\",\"text\":\"Create a bridge table customer_preferences with customer_id and preference_id and join in queries.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Duplicate customer rows for each preference in the customer_dim.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Store preferences as a JSON array in customer_dim and parse in queries.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Add a new column on customer_dim with a list of preferences as a string.\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nCreate a bridge table customer_preferences with customer_id and preference_id and join in queries.\n\n## Why Other Options Are Wrong\n\n- Option B duplicating customer rows increases dimensionality, harms readability, and complicates joins.\n- Option C storing as JSON reduces queryability and hurts standard BI tooling compatibility.\n- Option D storing a list in a single column complicates filtering and analytics and violates normalization principles.\n\n## Key Concepts\n\n- Many-to-many relationships\n- Bridge (link) tables\n- Normalized dimensional modeling\n\n## Real-World Application\n\nAllows analytics to answer questions like which customers have multiple preferences without duplicating customer records across facts.","diagram":null,"difficulty":"intermediate","tags":["dbt","data-modeling","data-warehousing","aws-redshift","terraform","eks","certification-mcq","domain-weight-25"],"channel":"dbt-analytics-engineer","subChannel":"data-modeling","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T09:32:38.823Z","createdAt":"2026-01-12 09:32:39"},{"id":"dbt-analytics-engineer-data-modeling-1768210358285-2","question":"You need to enforce durable surrogate keys in a dimensional model built with dbt across incremental loads and late-arriving data. Which practice provides stable surrogate keys that can be reconstructed if needed?","answer":"[{\"id\":\"a\",\"text\":\"Use an auto-increment surrogate key in the target data warehouse.\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use a deterministic surrogate key derived from hashing the business key to ensure stability.\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Use a composite surrogate key built from all attributes of the dimension row.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Generate surrogate keys in the source and pass-through to the target.\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nUse a deterministic surrogate key derived from hashing the business key to ensure stability across incremental loads and late-arriving data. This provides a durable, reproducible key that does not depend on load order or data arrival timing.\n\n## Why Other Options Are Wrong\n\n- Option A is incorrect because auto-increment keys can shift when data is reloaded or merged from multiple sources, breaking reproducibility.\n- Option C is incorrect because a composite key from all attributes is heavy and fragile to changes in non-key attributes.\n- Option D is incorrect because generating keys in the source reduces control and can lead to mismatches if source isn’t fully trusted or replicated.\n\n## Key Concepts\n\n- Surrogate keys\n- Deterministic hashing for keys\n- Data lineage and reproducibility\n\n## Real-World Application\n\nHashing the business key (plus a version indicator if needed) yields stable surrogate keys that remain consistent across full or partial reloads, enabling reliable slowly changing dimension handling.","diagram":null,"difficulty":"intermediate","tags":["dbt","data-modeling","data-warehousing","aws-redshift","terraform","eks","certification-mcq","domain-weight-25"],"channel":"dbt-analytics-engineer","subChannel":"data-modeling","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T09:32:39.359Z","createdAt":"2026-01-12 09:32:39"},{"id":"dbt-analytics-engineer-dbt-fundamentals-1768170078445-0","question":"In a dbt project, you maintain a large fact table with an incremental model keyed by order_id. New data fills daily, but some existing orders have updates. Which approach best ensures both new inserts and updates are reflected without full refresh?","answer":"[{\"id\":\"a\",\"text\":\"Implement a MERGE-based upsert inside the incremental block to handle updates and inserts\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Rely on dbt's default incremental behavior using only simple insert for new rows\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use a full_refresh on every run to guarantee correctness\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use a separate snapshot model to track changes\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nA. Implement a MERGE-based upsert inside the incremental block to handle updates and inserts.\n\n## Why Other Options Are Wrong\n\n- B: The default incremental behavior inserts only new rows and does not handle updates, which can lead to duplicates or stale data.\n- C: Full refresh rebuilds the entire table every run, negating the benefits of incremental loads and consuming unnecessary compute.\n- D: Snapshots track history for slowly changing dimensions and are not a substitute for upsert logic in incremental fact tables.\n\n## Key Concepts\n\n- Incremental models can perform upserts with MERGE to handle updates.\n- is_incremental() pattern guides conditional SQL in dbt.\n\n## Real-World Application\n\n- When feeding daily operational data into large fact tables, implementing a MERGE-based upsert in the incremental path ensures data accuracy while avoiding costly full reloads.","diagram":null,"difficulty":"intermediate","tags":["dbt","AWS","Snowflake","Terraform","Kubernetes","certification-mcq","domain-weight-25"],"channel":"dbt-analytics-engineer","subChannel":"dbt-fundamentals","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T22:21:18.446Z","createdAt":"2026-01-11 22:21:18"},{"id":"dbt-analytics-engineer-dbt-fundamentals-1768170078445-1","question":"You want to enforce referential integrity between a fact table and a date dimension in dbt. You need to prevent orphaned records. Which approach is most appropriate in dbt?","answer":"[{\"id\":\"a\",\"text\":\"Create a relationships test on the foreign key to the date_dim table\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Use a freshness test on the date dimension to ensure it's up to date\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Create a snapshot to capture historical changes\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use a seed to load fixed date_dim data\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nA. Create a relationships test on the foreign key to the date_dim table.\n\n## Why Other Options Are Wrong\n\n- B: Freshness tests ensure the data source is up-to-date, not referential integrity between tables.\n- C: Snapshots track history in slowly changing dimensions, not enforce foreign-key referential integrity.\n- D: Seeds populate static data but do not validate relationships between existing tables.\n\n## Key Concepts\n\n- dbt tests can enforce referential integrity via relationships tests.\n- Relationships tests help catch orphaned records during CI.\n\n## Real-World Application\n\n- Adding a relationships test helps guard against data quality regressions when new foreign-key relationships are introduced or when date_dim is updated.","diagram":null,"difficulty":"intermediate","tags":["dbt","AWS","Redshift","Kubernetes","Terraform","certification-mcq","domain-weight-25"],"channel":"dbt-analytics-engineer","subChannel":"dbt-fundamentals","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T22:21:18.936Z","createdAt":"2026-01-11 22:21:19"},{"id":"dbt-analytics-engineer-dbt-fundamentals-1768170078445-2","question":"During deployment to Snowflake, you want incremental models to load only new or changed rows rather than rewriting entire tables to optimize compute. Which strategy best achieves this in dbt?","answer":"[{\"id\":\"a\",\"text\":\"Implement upsert logic in the incremental block using MERGE statements\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Force full_refresh on every deployment to ensure consistency\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Convert incremental models to views to leverage dynamic querying\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use dbt test to enforce incremental semantics during runs\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nA. Implement upsert logic in the incremental block using MERGE statements.\n\n## Why Other Options Are Wrong\n\n- B: Full_refresh negates the benefits of incremental loads and is compute-intensive.\n- C: Views always recompute on every query and do not persist transformed data like incremental tables.\n- D: Tests validate data quality but do not control how data is written during runs.\n\n## Key Concepts\n\n- MERGE-based upserts in incremental dbt models for upserts.\n- Snowflake supports MERGE in dbt incremental implementations.\n\n## Real-World Application\n\n- In a Snowflake-based analytics stack, using MERGE in incremental paths lets you add new rows and update existing ones efficiently, reducing compute and improving freshness.","diagram":null,"difficulty":"intermediate","tags":["dbt","Snowflake","AWS","Kubernetes","Terraform","certification-mcq","domain-weight-25"],"channel":"dbt-analytics-engineer","subChannel":"dbt-fundamentals","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T22:21:19.427Z","createdAt":"2026-01-11 22:21:19"},{"id":"q-848","question":"You're designing a dbt analytics pipeline for an e-commerce platform. The 'staging.events_raw' table ingests page views and purchases and is partitioned by day. Data arrives both on time and as late as 24 hours. Design a practical incremental dbt model 'analytics.daily_sales' that aggregates revenue by day, category, and customer_segment. Explain how you'd implement incremental logic, handle late data, guard against schema drift, and validate with tests and snapshots?","answer":"Use an incremental model keyed by day, category, and customer_segment. Compute revenue as sum(price*quantity) from staging.events_raw. Upsert into analytics.daily_sales via MERGE on (day, category, cu","explanation":"## Why This Is Asked\n\nAssess ability to design scalable, fault-tolerant dbt pipelines handling late data and schema drift.\n\n## Key Concepts\n\n- Incremental models and idempotent MERGE\n- Late-arrival handling and backfill windows\n- Schema drift detection via tests and snapshots\n- Performance at scale and CI validation\n\n## Code Example\n\n```sql\n-- Example incremental sql sketch\nWITH s AS (\n  SELECT day, category, customer_segment,\n         SUM(price * quantity) AS revenue\n  FROM {{ ref('staging__events_raw') }}\n  WHERE day >= (SELECT MIN(day) FROM analytics.daily_sales)\n  GROUP BY day, category, customer_segment\n)\nMERGE INTO analytics.daily_sales AS t\nUSING s AS s\nON t.day = s.day AND t.category = s.category AND t.customer_segment = s.customer_segment\nWHEN MATCHED THEN UPDATE SET revenue = s.revenue\nWHEN NOT MATCHED THEN INSERT (day, category, customer_segment, revenue) VALUES (s.day, s.category, s.customer_segment, s.revenue);\n```\n\n## Follow-up Questions\n\n- How would you test for idempotency and late-arrival correctness?\n- How would you adapt this for a lakehouse (Parquet) or streaming source?","diagram":null,"difficulty":"advanced","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Anthropic","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:30:20.377Z","createdAt":"2026-01-12T13:30:20.377Z"},{"id":"dbt-analytics-engineer-testing-documentation-1768231420091-0","question":"In a dbt analytics pipeline deployed on AWS Redshift with S3 staging, a data quality test fails in CI/CD. To obtain robust, deterministic results without impacting production, which approach should you adopt?","answer":"[{\"id\":\"a\",\"text\":\"Run tests against production data in a single run\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use a separate ephemeral test schema with a known test dataset and seed data, isolating tests from production\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Disable tests in CI and rely on manual QA\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Run only schema tests (unique and not null) and ignore data tests\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe recommended approach is to use a separate ephemeral test schema with a known test dataset and seed data, isolating tests to avoid touching production.\n\n## Why Other Options Are Wrong\n- Option A: Testing against production data can corrupt or alter production and may not be reproducible in CI.\n- Option C: Disabling tests eliminates automated quality checks and increases risk.\n- Option D: Schema tests cover structure but do not validate data quality and business rules.\n\n## Key Concepts\n- Ephemeral test schemas\n- Seeded test data for determinism\n- dbt test scope and isolation\n\n## Real-World Application\nIn practice, configure your Redshift or other warehouse environment to provision a temporary test schema, load seeds with `dbt seed`, run `dbt test`, and drop the test schema after the run to avoid any production impact. For example:\n\n```bash\ndbt seed\ndbt run --models tag:quality\ndbt test\n```\n","diagram":null,"difficulty":"intermediate","tags":["dbt","AWS","Redshift","S3","CI/CD","Testing","certification-mcq","domain-weight-20"],"channel":"dbt-analytics-engineer","subChannel":"testing-documentation","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T15:23:40.094Z","createdAt":"2026-01-12 15:23:40"},{"id":"dbt-analytics-engineer-testing-documentation-1768231420091-1","question":"You want to auto-generate and publish documentation that includes dbt test results after pull requests. Which CI/CD approach ensures the docs site is rebuilt with latest test results?","answer":"[{\"id\":\"a\",\"text\":\"Run docs generation once per release and manually update test results in docs\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"In CI, run dbt test to produce run_results.json, publish a markdown test-summary into the docs site, and run dbt docs generate on PRs\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Rely on static docs without test results\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Create a separate docs site outside the repo and push results manually\",\"isCorrect\":false}]","explanation":"## Correct Answer\nIn CI, run `dbt test` to produce run_results.json, publish a markdown test-summary into the docs site, and run `dbt docs generate` on PRs so docs reflect latest test results automatically.\n\n## Why Other Options Are Wrong\n- Option A: Manual updates cause drift and slow feedback loops.\n- Option C: Static docs omit the current test state, reducing trust.\n- Option D: External/docs-site duplication creates sync challenges and extra maintenance.\n\n## Key Concepts\n- dbt docs generate\n- run_results.json from dbt test\n- CI/CD automation for docs\n\n## Real-World Application\nAdd a CI step that executes `dbt test`, extracts a concise markdown summary from `target/run_results.json`, updates docs, then runs `dbt docs generate` to refresh the docs site within a PR workflow. Example commands:\n\n```bash\ndbt test --models +quality\n# script to convert run_results.json to a markdown summary\n\ndbt docs generate\n```\n","diagram":null,"difficulty":"intermediate","tags":["dbt","AWS","Documentation","CI/CD","S3","Kubernetes","certification-mcq","domain-weight-20"],"channel":"dbt-analytics-engineer","subChannel":"testing-documentation","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T15:23:40.461Z","createdAt":"2026-01-12 15:23:40"},{"id":"dbt-analytics-engineer-testing-documentation-1768231420091-2","question":"A newly added seed table is used by several models. To ensure tests are reproducible across environments, what is the best practice?","answer":"[{\"id\":\"a\",\"text\":\"Use a dynamic seed loaded at runtime\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Commit seeds to git and load via dbt seed in CI to populate a known dataset\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Generate seeds via Faker in each environment\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Do not seed; rely on source data\",\"isCorrect\":false}]","explanation":"## Correct Answer\nCommit seeds to git and load via `dbt seed` in CI to ensure a known, version-controlled dataset is used for tests across environments.\n\n## Why Other Options Are Wrong\n- Option A: Dynamic seeds lead to nondeterministic tests.\n- Option C: Faker-generated seeds vary by environment, breaking reproducibility.\n- Option D: Relying solely on source data can introduce environmental variance and non-repeatable tests.\n\n## Key Concepts\n- dbt seed\n- Versioned seed data\n- CI deterministic testing\n\n## Real-World Application\nKeep a `seeds/` directory in your repo and run `dbt seed` as part of your CI pipeline before tests. This ensures every environment uses identical seed data. Example:\n\n```bash\ngit add seeds/*\ndbt seed\n```\n","diagram":null,"difficulty":"intermediate","tags":["dbt","AWS","Seed Data","Terraform","CI/CD","certification-mcq","domain-weight-20"],"channel":"dbt-analytics-engineer","subChannel":"testing-documentation","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T15:23:40.823Z","createdAt":"2026-01-12 15:23:40"},{"id":"dbt-analytics-engineer-testing-documentation-1768231420091-3","question":"When testing dbt models across multiple environments (dev, staging, prod), which practice ensures consistent test results across all environments?","answer":"[{\"id\":\"a\",\"text\":\"Use random seed values per environment\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use deterministic seed values and a shared test dataset across all environments\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Run tests only in development and assume rest\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Mirror prod data exactly in all environments\",\"isCorrect\":false}]","explanation":"## Correct Answer\nUse deterministic seed values and a shared test dataset across all environments to ensure repeatable results.\n\n## Why Other Options Are Wrong\n- Option A: Random seeds introduce nondeterminism, breaking repeatability.\n- Option C: Incomplete testing across environments leaves risk unverified.\n- Option D: Mirroring prod data can be impractical and may violate data governance; seeds offer a controlled baseline.\n\n## Key Concepts\n- Deterministic data seeds\n- Environment parity\n- Reproducible tests\n\n## Real-World Application\nIn your CI/CD pipeline, load a deterministic seed dataset into each environment before tests and use the same test queries across envs. Example:\n\n```bash\ndbt seed --models seed_common\ndbt test\n```\n","diagram":null,"difficulty":"intermediate","tags":["dbt","AWS","Kubernetes","CI/CD","Testing","certification-mcq","domain-weight-20"],"channel":"dbt-analytics-engineer","subChannel":"testing-documentation","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T15:23:40.954Z","createdAt":"2026-01-12 15:23:41"},{"id":"dbt-analytics-engineer-testing-documentation-1768231420091-4","question":"To measure how well your dbt project's documentation covers its models and tests, which metric best indicates documentation coverage?","answer":"[{\"id\":\"a\",\"text\":\"The total number of models listed in the docs site\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"The ratio of documented models and tests to total models\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"The time it takes to generate docs\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"The number of tests failing in CI\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe ratio of documented models and tests to total models best indicates documentation coverage.\n\n## Why Other Options Are Wrong\n- Option A: Listing models without context or tests doesn’t reflect coverage quality.\n- Option C: Generation time measures performance, not content coverage.\n- Option D: Test failures measure data quality, not documentation coverage.\n\n## Key Concepts\n- Documentation coverage metrics\n- Model-test mapping\n- Documentation quality\n\n## Real-World Application\nImplement a coverage dashboard that computes documented models and tests against total models, updating automatically with PRs. Example snippet:\n\n```markdown\nCoverage = documented_models / total_models\n```\n","diagram":null,"difficulty":"intermediate","tags":["dbt","AWS","Documentation","Kubernetes","Terraform","certification-mcq","domain-weight-20"],"channel":"dbt-analytics-engineer","subChannel":"testing-documentation","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T15:23:41.100Z","createdAt":"2026-01-12 15:23:41"}],"subChannels":["data-modeling","dbt-fundamentals","general","testing-documentation"],"companies":["Amazon","Anthropic","Tesla"],"stats":{"total":12,"beginner":0,"intermediate":11,"advanced":1,"newThisWeek":12}}