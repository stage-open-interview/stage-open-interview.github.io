{"questions":[{"id":"q-1125","question":"You're building an incremental dbt model analytics.daily_revenue_by_product sourced from staging.sales_raw. Data can arrive late up to 2 days. How would you implement incremental upserts, reprocess late days without disturbing older data, and validate with tests and a snapshot of product price history? Provide a minimal SQL snippet illustrating the approach?","answer":"Implement an incremental model with unique_key=['day','product_id','region']; reprocess the last 2 days during incremental runs to absorb late data; use MERGE/upsert where supported, else a two-phase ","explanation":"## Why This Is Asked\nTests practical incremental logic and late-arrival handling; checks upsert patterns, tests, and snapshots.\n\n## Key Concepts\n- Incremental materialization with unique keys\n- Late-arrival handling window\n- Snapshot for price history\n- Basic tests for data quality\n\n## Code Example\n```sql\n-- models/analytics/daily_revenue_by_product.sql\n{{ config(materialized='incremental', unique_key=['day','product_id','region']) }}\n\nwith s as (\n  select date_trunc('day', order_date) as day,\n         product_id,\n         region,\n         sum(quantity * price) as revenue\n  from {{ ref('staging_sales_raw') }}\n  group by 1,2,3\n)\n\nselect day, product_id, region, revenue\nfrom s\n{% if is_incremental() %}\n  -- reprocess last 2 days to capture late data\n  where day >= (select max(day) from {{ this }}) - interval '2 day'\n{% endif %}\n```\n\n## Follow-up Questions\n- How would you adapt this for Redshift vs Snowflake?\n- How would you test the snapshot consistency across environments?","diagram":null,"difficulty":"beginner","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Hashicorp"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T23:31:34.187Z","createdAt":"2026-01-12T23:31:34.187Z"},{"id":"q-1165","question":"You're designing a dbt analytics pipeline for a ride-sharing platform. The raw events are in `staging.events_raw` with columns like `ride_id`, `city`, `ride_type`, `currency`, `amount`, `occurred_at`, and data can arrive up to 48 hours late. Build a beginner-friendly incremental model `analytics.daily_revenue` that computes USD revenue per day by `date`, `city`, and `ride_type`. Use a daily `pricing.exchange_rates` table to convert from local currency to USD. Describe how you'd implement incremental upserts, late-data handling (2-day window), schema-drift guards, and validation with tests and a snapshot. Also outline tests for late refunds and missing rates?","answer":"Use an incremental model with is_incremental(). Join staging events to pricing.exchange_rates on date and currency, then aggregate revenue in USD by date, city, ride_type. Implement a 2-day late-data ","explanation":"## Why This Is Asked\nThe question probes practical incremental modeling with late data, currency conversion, and drift safeguards—core dbt skills at junior to mid-beginner level.\n\n## Key Concepts\n- Incremental models and upserts\n- Currency conversion via reference table\n- Late-data window handling\n- Schema-drift guards and tests\n- Snapshots for drift detection\n\n## Code Example\n```sql\n-- dbt model: analytics/daily_revenue.sql\nwith events as (\n  select\n    date_trunc('day', occurred_at) as date,\n    city,\n    ride_type,\n    currency,\n    amount\n  from {{ source('staging','events_raw') }}\n  where occurred_at <= (current_timestamp() - interval '2 days')\n),\nrates as (\n  select date, currency, rate\n  from {{ source('pricing','exchange_rates') }}\n)\nselect\n  e.date,\n  e.city,\n  e.ride_type,\n  sum(e.amount * coalesce(r.rate, 1.0)) as revenue_usd\nfrom events e\nleft join rates r\n  on r.date = e.date and r.currency = e.currency\ngroup by 1,2,3\n```\n\n## Follow-up Questions\n- How would you handle gaps in exchange_rates (missing rates)?\n- How would you validate late refunds affecting revenue?\n- How would you test for time zone consistency across cities?","diagram":null,"difficulty":"beginner","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T03:31:16.675Z","createdAt":"2026-01-13T03:31:16.675Z"},{"id":"q-1257","question":"Design a drift-aware dbt workflow across dev/staging/prod in Snowflake. How would you detect schema drift between sources and models using snapshots, tests, and sources, enforce a drift threshold, and automatically fail CI when drift exceeds the threshold? Describe the macro, tests, and alert/rollback mechanism, plus how you version thresholds?","answer":"Implement a dbt macro drift_check that compares per-column metadata (data type, nullability, default, and max length) between a source table and its corresponding model, plus a runtime comparison of r","explanation":"## Why This Is Asked\nReal-world pipelines require automated detection of schema drift to prevent deployment of broken analytics. This question tests practical macro design, test coverage, and CI/CD integration.\n\n## Key Concepts\n- dbt snapshots and sources for metadata\n- Per-column drift checks (type, nullability, max length)\n- Threshold-driven gating in CI/CD and rollback policies\n- Canary prod lineage validation and alerting\n\n## Code Example\n```jinja\n{% macro drift_check(source_schema, source_table, model_schema, model_table, thresholds) -%}\n-- pseudo-logic: compare metadata and basic stats, emit drift_count if > thresholds\n{%- endmacro %}\n```\n\n## Follow-up Questions\n- How would you scale per-table thresholds across hundreds of tables?\n- How would you distinguish benign drift (e.g., new optional column) from breaking drift?","diagram":null,"difficulty":"advanced","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Google","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T07:23:31.079Z","createdAt":"2026-01-13T07:23:31.079Z"},{"id":"q-1361","question":"Design a beginner-friendly incremental dbt model in Snowflake for a SaaS analytics pipeline: raw events live at staging.event_logs with user_id, event_type (signup, login, purchase), country, occurred_at. Build analytics.daily_metrics that counts distinct active users by day and event_type, enriched by dimensions.countries. Explain incremental logic, late data handling (7 days), schema drift guards, and validation with tests and a snapshot. Also outline an Exposure for the dashboard with lineage?","answer":"Build analytics.daily_metrics as an incremental Snowflake model that counts distinct user_id by day and event_type from staging.event_logs, enriched via a join to dimensions.countries on country. Upse","explanation":"## Why This Is Asked\nThis question probes practical dbt incremental modeling, enrichment, and the linkage to dashboards via exposures.\n\n## Key Concepts\n- Incremental models with upserts in Snowflake\n- Late-arriving data handling with a 7-day window\n- Dimensional enrichment via dimensions.countries\n- Schema-drift guards and tests\n- Snapshots for user cohorts\n- Exposures and lineage visualization\n\n## Code Example\n```sql\nwith src as (\n  select distinct user_id, date(occurred_at) as day, event_type, country\n  from {{ ref('staging.event_logs') }}\n  where occurred_at >= current_timestamp() - interval '7 day'\n)\nselect day, event_type, country, count(distinct user_id) as active_users\nfrom src\ngroup by 1,2,3\n```\n\n## Follow-up Questions\n- How would you test referential integrity between analytics.daily_metrics and dimensions.countries?\n- How would you expose this model in a dashboard and ensure lineage is accurate?","diagram":null,"difficulty":"beginner","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Snowflake","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T13:15:09.731Z","createdAt":"2026-01-13T13:15:09.731Z"},{"id":"q-1523","question":"Design an incremental dbt model analytics.daily_engagement for a global multi-tenant app. Raw events sit in staging.user_events with tenant_id, user_id, email, event_type, occurred_at, privacy_flag. Build per-tenant daily distinct-user counts by event_type, redacting emails when privacy_flag is true. Describe incremental strategy, late data window, schema-drift guards, tests and snapshots, and how to expose lineage for dashboards?","answer":"Use a (tenant_id, date( occurred_at ), event_type) keyed incremental model. Compute distinct user_id from staging.user_events, redact email via a macro: if privacy_flag then 'REDACTED' else email. Lat","explanation":"## Why This Is Asked\nTests ability to design privacy-aware, tenant-scoped analytics with late-arriving data. Emphasizes incremental logic, governance, and lineage.\n\n## Key Concepts\n- Incremental models with composite keys (tenant_id, date, event_type)\n- Field-level redaction controlled by privacy_flag via a macro\n- Late-arriving data handled through a short lookback MERGE\n- Schema drift guards using sources, tests, and snapshots\n- Exposures to reflect lineage in dashboards\n\n## Code Example\n```sql\n-- macros/redact_email.sql\n{% macro redact_email(email, privacy_flag) -%}\n  {% if privacy_flag -%}\n    {{ return('REDACTED') }}\n  {% else -%}\n    {{ return(email) }}\n  {% endif -%}\n{% endmacro %}\n```\n\n## Follow-up Questions\n- How would you test the redact_email macro across combinations of privacy_flag and null emails?\n- How would you scale this approach to thousands of tenants with varying data freshness SLAs?","diagram":null,"difficulty":"intermediate","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","IBM","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T20:40:54.485Z","createdAt":"2026-01-13T20:40:54.486Z"},{"id":"q-1560","question":"In a dbt project, you ingest raw events into `staging.web_events` with columns: `event_id`, `user_id`, `event_type`, `country`, `occurred_at` (UTC). Build a beginner-friendly incremental model **analytics.daily_active_users** that reports the count of distinct `user_id`s per day, by `country` and `event_type`. Data can arrive up to 2 days late. Describe your incremental logic, how you handle late data with a rolling window, how to guard against schema drift, and how you validate with tests. Also draft an Exposure for a dashboard showing daily active users by country and event_type, including lineage?","answer":"Use an incremental model that computes daily active users by date(occurred_at) in UTC, grouped by country and event_type, then upserts into analytics.daily_active_users. For late data, retain a 2-day rolling window and reprocess the last 2 days on each run. Guard against schema drift with explicit column definitions and dbt's source freshness checks. Validate with unique user tests, null checks, and row count expectations.","explanation":"## Why This Is Asked\nAssesses practical dbt incremental modeling with late data handling and exposure design.\n\n## Key Concepts\n- Incremental model by day\n- Late-arrival window (2 days)\n- Schema drift guards\n- Tests and a dashboard exposure with lineage\n\n## Code Example\n```sql\nSELECT\n  date_trunc('day', occurred_at) AS day,\n  country,\n  event_type,\n  COUNT(DISTINCT user_id) AS active_users\nFROM {{ ref('staging.web_events') }}\nGROUP BY 1,2,3\n```\n\n## Follow-up Questions\n- How would you handle time zones for users across regions?\n- How would you adapt for evolving event_type vocabularies?","diagram":null,"difficulty":"beginner","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","DoorDash","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T06:19:05.797Z","createdAt":"2026-01-13T21:45:41.380Z"},{"id":"q-1590","question":"Design a two-model incremental dbt flow in Snowflake: 1) analytics.first_session derives first_session_at per user and seeds analytics.users(user_id, first_session_at, country_code, cohort). 2) analytics.daily_metrics counts daily active users by day and event_type, enriched with dimensions.countries. Include late-data handling (3 days), schema-drift guards, tests (not_null, unique), a snapshot, and expose lineage to dashboards?","answer":"Approach: Implement a two-model incremental dbt flow in Snowflake. First, analytics.first_session computes min(occurred_at) per user as first_session_at and seeds analytics.users with user_id, first_session_at, country_code, and cohort. Second, analytics.daily_metrics runs incrementally with a 3-day late-data watermark, counting daily active users by day and event_type while joining dimensions.countries for enrichment. Include schema-drift guards, not_null/unique tests, and a snapshot for slowly changing fields.","explanation":"## Why This Is Asked\nTests ability to design robust incremental pipelines, handle late data, and maintain lineage.\n\n## Key Concepts\n- Incremental models and ref() usage\n- First-session derivation and cohort calculation\n- Late-data watermark and idempotent upserts\n- Schema drift guards and tests; snapshots for slowly changing fields\n- Dashboards exposure and lineage\n\n## Code Example\n```sql\n-- first_session model (simplified)\nwith e as (\n  select user_id, occurred_at, country_code\n  from {{ source('raw','events_log') }}\n)\nselect user_id, min(occurred_at) as first_session_at, country_code,\n      \"","diagram":"flowchart TD\n  A[raw.events_log] --> B[analytics.first_session]\n  B --> C[analytics.users]\n  C --> D[analytics.daily_metrics]\n  E[dimensions.countries] --> C","difficulty":"intermediate","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T05:35:22.438Z","createdAt":"2026-01-13T22:55:10.734Z"},{"id":"q-1604","question":"In a dbt pipeline powering a fintech analytics platform, raw events arrive at staging.fin_events with customer_id, txn_id, amount, currency, status, event_ts. Design an incremental analytics.transactions model that deduplicates by txn_id across sources, handles late events within a 2-day window, validates currency via a macro-based set per source, guards against schema drift with a dynamic mapping table, and snapshots customers on their first txn for dashboard lineage. Include tests and performance considerations?","answer":"I would implement an incremental analytics.transactions model with unique_key txn_id, deduplicating across sources, handling late events within a 2-day watermark window, validating currency via a custom macro per source, guarding against schema drift with a dynamic mapping table, and snapshotting customers on their first transaction for dashboard lineage.","explanation":"## Why This Is Asked\n\nTests advanced dbt patterns: deduplication, late-arrival handling, schema drift detection, and data lineage.\n\n## Key Concepts\n\n- Incremental merge with per-source deduplication on txn_id\n- Watermark-based late-arrival handling (2 days)\n- Macro-based currency validation per source\n- Dynamic schema drift guards via mapping tables\n- Snapshot on customers for first transaction lineage\n- Tests: not_null, unique, relationships\n- Performance: clustering, pruning, and date-based partitioning\n\n## Code Example\n\n```sql\n-- dbt incremental model sketch\n{{ config(materialized='incremental', unique_key='txn_id') }}\n\nWITH deduped_events AS (\n  SELECT *,\n         ROW_NUMBER() OVER (\n           PARTITION BY txn_id, source \n           ORDER BY event_ts DESC\n         ) AS rn\n  FROM {{ ref('staging_fin_events') }}\n  WHERE event_ts >= DATEADD('day', -2, CURRENT_TIMESTAMP)\n),\n\nvalidated_events AS (\n  SELECT *\n  FROM deduped_events\n  WHERE rn = 1\n    AND {{ validate_currency(source, currency) }}\n),\n\nfinal_transactions AS (\n  SELECT *\n  FROM validated_events\n  WHERE {{ schema_drift_guard() }}\n)\n\nSELECT * FROM final_transactions\n```\n\n## Implementation Notes\n\n- Use `dbt snapshot` for customer first-transaction tracking\n- Implement currency validation macro with source-specific rules\n- Create mapping table for dynamic schema drift detection\n- Add comprehensive tests for data quality and performance","diagram":null,"difficulty":"advanced","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T05:31:38.231Z","createdAt":"2026-01-14T02:32:20.530Z"},{"id":"q-1714","question":"In a beginner dbt project for a media site, staging.events_logs(user_id, event_type, occurred_at, region_code) feeds analytics.daily_engagement that counts distinct users per date and event_type, enriched by dimensions.regions on region_code. Build the incremental model with late data window (3 days), include schema-drift guards, and tests (not_null on user_id, occurred_at; unique on (user_id, occurred_at, event_type)). Add a data-contract macro/test ensuring staging.events_logs contains required fields and types, and a snapshot on users to capture region changes. Explain approach, tests, and macro design?","answer":"I’d implement analytics.daily_engagement as an incremental model that aggregates distinct users per date and event_type, joining dimensions.regions on region_code. Late data is allowed in a 3-day wind","explanation":"## Why This Is Asked\nThis question probes practical dbt workflow: incremental logic, late-arrival handling, schema drift guardrails, data-contract testing, and snapshot usage. It covers core dbt skills in a realistic, beginner-friendly way.\n\n## Key Concepts\n- Incremental models and late data windows\n- Schema drift guards and tests\n- Data-contracts via macros/tests\n- Snapshots for slowly changing attributes\n\n## Code Example\n```javascript\n// Pseudo-contract test skeleton (dbt-style macro would be SQL/Jinja in practice)\nfunction assertContract(stagingColumns, required) {\n  const missing = required.filter(r => !stagingColumns.includes(r));\n  if (missing.length) throw new Error(\"Missing: \" + missing.join(\", \"));\n}\n```\n\n## Follow-up Questions\n- How would you extend the contract to handle optional fields?\n- How would you validate contracts across multiple environments (dev/stage/prod)?","diagram":null,"difficulty":"beginner","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Cloudflare","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T07:49:20.068Z","createdAt":"2026-01-14T07:49:20.068Z"},{"id":"q-1741","question":"You're building a multi-tenant dbt analytics pipeline on Snowflake. Raw events live in staging.events with tenant_id, user_id, event_type, occurred_at. You plan per-tenant analytics schemas (analytics.{tenant}). Design an incremental analytics.daily_metrics per tenant that counts distinct active users by day and event_type, with late data up to 2 days, joining dimensions.tenants and dimensions.countries, including schema-drift guards, tests (not_null, unique), and a snapshot for user_first_seen per tenant. Explain approach for cross-tenant lineage and tenant-scoped materializations?","answer":"Per-tenant schemas in Snowflake: implement analytics.daily_metrics as an incremental model scoped to analytics.${tenant}. daily by (day, tenant_id, event_type). Late data window: 2 days; use MERGE int","explanation":"## Why This Is Asked\nTests the ability to architect multi-tenant dbt pipelines with strict isolation (per-tenant schemas), robust late-data handling, and governance via tests and snapshots. It also probes strategies for cross-tenant lineage in dashboards. \n\n## Key Concepts\n- Multi-tenant isolation with per-tenant schemas\n- Incremental modeling with late-arrival handling\n- Schema drift guards and robust tests\n- Snapshots for slowly changing dimensions per tenant\n- Clear lineage exposure for dashboards across tenants\n\n## Code Example\n````sql\n-- analytics.{{tenant}}.daily_metrics.sql\nwith src as (\n  select tenant_id, user_id, event_type, date(occurred_at) as day\n  from {{ source('raw','events') }}\n)\nselect day, tenant_id, event_type, count(distinct user_id) as active_users\nfrom src\ngroup by day, tenant_id, event_type\n````\n\n## Follow-up Questions\n- How would you automate tenancy onboarding to new schemas without gluing code?\n- How do you test for cross-tenant join correctness without data leakage?","diagram":null,"difficulty":"advanced","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T09:01:41.446Z","createdAt":"2026-01-14T09:01:41.447Z"},{"id":"q-1757","question":"Design a dbt analytics layer for three domains: core_events (user actions), memberships (plans), and referrals. Data arrives late (up to 2 days). Propose an architecture that ensures: 1) incremental models with controlled late data backfills, 2) deterministic surrogate keys for cross-domain joins, 3) schema-drift guards via custom tests/macros, 4) automated docs with complete lineage, 5) dashboard-friendly exposure with stable lineage during backfills, and 6) performance considerations for Snowflake or BigQuery (partitioning, clustering). Include a concrete incremental SQL snippet?","answer":"Propose staging per domain, a single grain, incremental models with a 2-day late-arrival window, a surrogate key like domain_key + occurred_at + id for cross-domain joins, a macro-based cross-domain r","explanation":"## Why This Is Asked\nTests end-to-end dbt mastery across multi-domain conformance, late data handling, and lineage stability during backfills. It also probes custom macros for data quality and cross-domain integrity, plus performance considerations for two leading warehouses.\n\n## Key Concepts\n- Incremental models with late-arrival windows\n- Cross-domain surrogate keys and referential integrity\n- Schema drift guards via tests and macros\n- Automated docs with complete lineage\n- Backfill strategy ensuring stable dashboards\n- Performance tuning for Snowflake vs BigQuery (partitioning, clustering)\n\n## Code Example\n```sql\n-- models/analytics/fact_core_events_incremental.sql\n{{ config(materialized='incremental') }}\n\nwith src as (\n  select\n    user_id as domain_user_id,\n    event_type,\n    occurred_at,\n    domain\n  from {{ source('core_events', 'events') }}\n  where occurred_at >= dateadd(day, -2, current_date())\n)\n\nselect\n  domain_user_id,\n  domain,\n  max(occurred_at) as occurred_at,\n  count(*) as event_count\nfrom src\ngroup by domain_user_id, domain\n{% if is_incremental() %}\nwhere occurred_at > (select max(occurred_at) from {{ this }})\n{% endif %}\n```\n\n## Follow-up Questions\n- How would you validate cross-domain joins under backfills?\n- What tests would you add to guard against schema drift across domains?","diagram":null,"difficulty":"advanced","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","MongoDB","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T09:44:23.172Z","createdAt":"2026-01-14T09:44:23.172Z"},{"id":"q-1808","question":"Design an intermediate dbt workflow for a fintech analytics pipeline on Snowflake. Raw events are in staging.transactions (transaction_id, user_id, amount, currency, occurred_at, status) and staging.users (user_id, country_code, account_status). Build an incremental analytics.daily_finance that sums total_amount and transaction_count by day, currency, country_code, and status, with late data support up to 2 days. Add analytics.users_snapshot as a Type 2 surrogate for changes in country_code/account_status. Expose lineage and discuss a data-contract macro to validate source schemas and auto-generate tests?","answer":"Use an incremental model analytics.daily_finance that upserts sum(amount) and transaction_count by day, currency, country_code, and status; allow late data within a 2-day window via a MERGE-based upse","explanation":"## Why This Is Asked\nTests resilience to late data, SCD Type 2, and data contracts. It also probes lineage and macro-level test generation.\n\n## Key Concepts\n- Incremental upserts in dbt\n- Type 2 SCD for users\n- Late data handling with a 2-day window\n- Data-contract macro to auto-create tests\n- dbt docs lineage\n\n## Code Example\n```sql\n-- Example dbt model skeleton\nwith src as (\n  select * from {{ ref('staging_transactions') }}\n), u as (\n  select * from {{ ref('staging_users') }}\n)\nselect\n  date_trunc('day', occurred_at) as day,\n  currency,\n  country_code,\n  status,\n  sum(amount) as total_amount,\n  count(*) as transaction_count\nfrom src s\njoin u on s.user_id = u.user_id\ngroup by 1,2,3,4\n```\n\n## Follow-up Questions\n- How would you test for nulls and duplicates?\n- How would you adapt this for multi-tenant isolation?\n- How to surface lineage in dashboards and docs?","diagram":"flowchart TD\n  A[staging.transactions] --> B[analytics.daily_finance]\n  C[staging.users] --> D[analytics.users_snapshot]\n  B --> E[dashboard.financials]\n  B --> F[docs/lineage]","difficulty":"intermediate","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","LinkedIn"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T11:42:53.754Z","createdAt":"2026-01-14T11:42:53.754Z"},{"id":"q-1823","question":"Scenario: In a beginner dbt project for a gig-economy platform, raw events arrive in staging.event_logs with user_id, session_id, event_type (visit, click, conversion), occurred_at, and region_code. Build an incremental model analytics.daily_events that counts events by day and event_type, enriched by dims.regions on region_code. Use a 2-day late data window, guard against schema drift with not_null and unique tests, and create a data-contract macro to verify staging.event_logs contains the required columns and types before run. What approach would you take?","answer":"Implement an incremental analytics.daily_events that aggregates counts by day, event_type, and region_code from staging.event_logs joined to dims.regions on region_code. Use a 2-day late window, enfor","explanation":"Why This Is Asked\n- Tests incremental modeling with late data handling and schema-drift guards.\n- Evaluates macro usage for pre-flight data contracts and test coverage.\n\nKey Concepts\n- Incremental models in dbt\n- Late data window techniques\n- Schema-drift guards (not_null, unique)\n- Data-contract macros and pre-flight validation\n- Joining with lightweight dimensions for enrichment\n\nCode Example\n```sql\n-- analytics/daily_events.sql\nwith src as (\n  select user_id, session_id, event_type, occurred_at, region_code\n  from {{ source('staging', 'event_logs') }}\n), base as (\n  select\n    date_trunc('day', occurred_at) as day,\n    event_type,\n    region_code,\n    count(*) as event_count\n  from src\n  group by 1,2,3\n)\nselect * from base\n{% if is_incremental() %}\nwhere occurred_at >= (select max(occurred_at) from {{ this }})\n{% endif %}\n```\n\n```sql\n-- macros/data_contract.sql\n{% macro data_contract(table, required_cols) -%}\n  -- Pseudo-implementation: validates presence and types of required_cols on table\n  {# In practice, iterate required_cols and raise if missing #}\n{%- endmacro %}\n```\n\nFollow-up Questions\n- How would you extend this for multiple regions with partition pruning?\n- How would you test macro robustness across schema changes?","diagram":null,"difficulty":"beginner","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T13:05:52.833Z","createdAt":"2026-01-14T13:05:52.833Z"},{"id":"q-1909","question":"Design a per-tenant incremental dbt flow in Snowflake that computes analytics.{tenant}.daily_event_summary from staging.events (tenant_id, user_id, event_type, occurred_at, country_code, record_hash). Include enrichment from dimensions.countries, and produce a per-tenant daily count of distinct users by event_type. Implement late data handling (2 days), schema-drift guards, tests (not_null, unique), and a snapshot for analytics.{tenant}.users to capture cohort changes. Explain how you ensure cross-tenant lineage and isolation?","answer":"Plan: implement an incremental model analytics.{tenant}.daily_event_summary with a composite key (tenant_id, day, event_type, user_id). Use MERGE-like upserts via dbt incremental to accommodate late d","explanation":"Why This Is Asked\n- Tests practical dbt incremental patterns with multi-tenant isolation.\n- Covers late data handling and schema drift guards.\n\nKey Concepts\n- Incremental model with composite key; DBT incremental behavior in Snowflake.\n- Tenant isolation through per-tenant schemas; cross-tenant lineage in docs.\n- Snapshots for slowly changing cohort labels; tests for data quality.\n\nCode Example\n```sql\n-- dbt model: analytics/{tenant}/daily_event_summary.sql\nwith src as (\n  select tenant_id, user_id, event_type, date(occurred_at) as day, country_code\n  from {{ ref('staging_events') }}\n  where occurred_at >= dateadd(day, -2, current_date())\n), enriched as (\n  select s.tenant_id, s.user_id, s.event_type, s.day, c.country_name, c.continent\n  from src s\n  left join {{ ref('dimensions__countries') }} c on c.country_code = s.country_code\n)\nselect tenant_id, day, event_type, count(distinct user_id) as active_users\nfrom enriched\ngroup by 1,2,3\n```\n\nFollow-up Questions\n- How would you adapt this to handle event_type changes post-ingest?\n- How do you validate per-tenant isolation in dbt docs?","diagram":null,"difficulty":"intermediate","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","Netflix","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T16:53:50.128Z","createdAt":"2026-01-14T16:53:50.129Z"},{"id":"q-1988","question":"Design an incremental, per-tenant analytics.daily_metrics model in Snowflake with dbt. Staging.events has tenant_id, user_id, event_type, occurred_at, platform, revenue. Build analytics.{tenant}.daily_metrics counting distinct users per day by event_type, with late data tolerance of 2 days. Add a per-tenant feature flag in dimensions.tenants (revenue_enabled) and a macro to include revenue only when true. Enforce isolation via per-tenant schemas, add schema-drift guards, tests (not_null, unique), and a snapshot for analytics.{tenant}.users. Explain cross-tenant lineage and dashboard exposure?","answer":"Outline a macro-driven approach: a per-tenant flag revenue_enabled in dimensions.tenants, read by a macro revenue_included(tenant) to conditionally include revenue in analytics.{tenant}.daily_metrics.","explanation":"## Why This Is Asked\nTests ability to design complex per-tenant dbt pipelines with dynamic schemas and feature flags.\n\n## Key Concepts\n- per-tenant schema isolation\n- dbt macros for feature flags\n- incremental late-data handling\n- snapshots for cohort changes\n- tests and lineage\n\n## Code Example\n```sql\n-- analytics/{tenant}/daily_metrics.sql (conceptual)\nselect\n  '{{ tenant }}' as tenant_id,\n  date(occurred_at) as day,\n  event_type,\n  count(distinct user_id) as active_users,\n  sum(case when revenue_enabled then revenue else 0 end) as revenue\nfrom {{ ref('staging__events') }} e\njoin {{ ref('dimensions__tenants') }} t on t.tenant_id = e.tenant_id\nwhere occurred_at >= dateadd(day,-2,current_date())\ngroup by 1,2,3\n```\n\n## Follow-up Questions\n- How would you observe and test cross-tenant lineage changes when tenants are added/dropped?\n- How would you scale the macro to support tenants with different time zones?","diagram":null,"difficulty":"intermediate","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T19:43:21.230Z","createdAt":"2026-01-14T19:43:21.230Z"},{"id":"q-2013","question":"Design a per-tenant incremental analytics model in Snowflake that materializes analytics.{tenant}.daily_event_engagement from staging.events (tenant_id, user_id, event_type, occurred_at). Enrich with dimensions.regions on country_code. Produce daily counts by event_type and region_name. Add a 3-day late data window, schema-drift guards, tests (not_null, unique), and a snapshot analytics.{tenant}.customers for cohort changes. Explain tenant isolation and cross-tenant lineage via macro-generated per-tenant schemas?","answer":"Implement an incremental per-tenant model that writes to analytics.{tenant}.daily_event_engagement using staging.events, joined to dimensions.regions by country_code to group by region_name. Aggregate","explanation":"Why This Is Asked\n- Tests ability to design per-tenant dbt models with dynamic schemas and macro-driven generation.\n- Evaluates cross-tenant isolation and lineage strategies.\n- Assesses handling of late data and schema drift.\n\nKey Concepts\n- Incremental per-tenant modeling in Snowflake\n- Macros for tenant-scoped schema generation\n- Cross-tenant lineage and isolation controls\n- Data quality tests and snapshots for cohorts\n\nCode Example\n```jinja\n{% macro tenant_daily_engagement(tenant_id) %}\nselect\n  '{{ tenant_id }}' as tenant_id,\n  cast(date(occurred_at) as date) as day,\n  event_type,\n  region_name,\n  count(distinct user_id) as active_users\nfrom {{ source('staging','events') }}\nwhere tenant_id = '{{ tenant_id }}'\ngroup by 1,2,3,4\n{% endmacro %}\n```\n\nFollow-up Questions\n- How would you handle a tenant with missing region mapping?\n- How would you monitor and adapt the late-data window across tenants?","diagram":"flowchart TD\n A(Source: staging.events) --> B(Join: regions)\n B --> C(Martialization: analytics.{tenant}.daily_event_engagement)\n C --> D(Snapshot: analytics.{tenant}.customers)\n D --> E(Dashboards/Lineage)","difficulty":"intermediate","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","LinkedIn"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T20:50:59.166Z","createdAt":"2026-01-14T20:50:59.166Z"},{"id":"q-2097","question":"Design a per-tenant weekly retention pipeline in Snowflake using dbt where raw events live in staging.events with tenant_id, user_id, first_seen_at, occurred_at, and an events.users table. Create analytics.tenant_retention (tenant_id, cohort_week, retention_users, total_users) that computes weekly retention by cohort (first_seen_week) with incremental loading, and late data handling up to 4 days. Also implement analytics.global_retention that aggregates per-tenant retention across tenants with tenant_dim country/plan, ensuring cross-tenant lineage and isolation. Include schema-drift guards, tests (not_null, unique), and a snapshot of analytics.tenants to track cohort definitions. Explain how you ensure isolation and lineage?","answer":"Implement a per-tenant weekly retention pipeline in analytics.tenant_retention that cohorts users by their first_seen_at week and computes weekly active users from staging.events per tenant_id. Use MERGE statements for idempotent incremental loading with a 4-day lookback window for late data handling, and implement schema drift guards. Create analytics.global_retention to aggregate retention metrics across tenants with tenant_dim country/plan attributes. Include a snapshot of analytics.tenants for cohort definition tracking and comprehensive tests (not_null, unique).","explanation":"## Why This Is Asked\nTests ability to design cross-tenant, incremental transformations with late data handling and quality checks in a dbt/Snowflake context.\n\n## Key Concepts\n- Multi-tenant isolation and lineage\n- Incremental logic with late data window\n- Schema drift guards and tests\n- Snapshots and cross-tenant aggregates\n\n## Code Example\n```sql\n-- Pseudocode for MERGE-based incremental\nMERGE INTO analytics.tenant_retention t\nUSING (\n  SELECT tenant_id, cohort_week, \n         COUNT(DISTINCT user_id) AS retention_users, \n         COUNT(DISTINCT user_id) AS total_users \n  FROM staging.events \n```","diagram":null,"difficulty":"intermediate","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T05:02:01.147Z","createdAt":"2026-01-15T02:12:59.856Z"},{"id":"q-2133","question":"Scenario: A new multi-tenant event feed named staging.stream_events with columns tenant_id, user_id, event_type, occurred_at, country_code, payload (JSON). Task: implement a dbt incremental model analytics.daily_user_events that, for each day, tenant_id, and event_type, returns the count of distinct users. Enrich with dimensions.countries on country_code. Create a macro to parse payload JSON extracting device and app_version; fallback defaults if missing. Implement late data tolerance of 2 days (i.e., if occurred_at within last 2 days, process in daily batch). Add tests: not_null on tenant_id, user_id, occurred_at; unique on (tenant_id, user_id, occurred_at, event_type). Add a snapshot for analytics.users to capture cohort-country changes. Provide strategy for cross-tenant lineage and isolation in a single schema (no per-tenant schemas)?","answer":"Design a dbt incremental analytics.daily_user_events: group by tenant_id, date(occurred_at), event_type; count distinct user_id. Enrich using left join dimensions.countries on country_code; use a smal","explanation":"## Why This Is Asked\nTests a beginner in building incremental per-tenant analytics with data enrichment, JSON parsing, and late-arrival handling. It also probes macro design for robust JSON extraction and basic data contracts.\n\n## Key Concepts\n- dbt incremental models and late-arrival windows\n- JSON payload parsing via macros\n- Data enrichment with dimension tables\n- Basic data quality tests and a snapshot for evolving users\n- Cross-tenant lineage in a single analytics schema\n\n## Code Example\n```sql\n-- macro: parse_payload.sql\n{% macro parse_payload(payload) %}\n  CASE WHEN JSON_EXTRACT_PATH_TEXT({{ payload }}, 'device') IS NOT NULL\n       THEN JSON_EXTRACT_PATH_TEXT({{ payload }}, 'device')\n       ELSE 'unknown'\n  END AS device,\n  COALESCE(JSON_EXTRACT_PATH_TEXT({{ payload }}, 'app_version'), 'unknown') AS app_version\n{% endmacro %}\n```\n\n```sql\n-- analytics.daily_user_events model (simplified)\nselect\n  tenant_id,\n  date_trunc('day', occurred_at) as day,\n  event_type,\n  count(distinct user_id) as user_count\nfrom {{ ref('staging_stream_events') }} as s\nleft join {{ ref('dimensions_countries') }} as c\n  on s.country_code = c.country_code\ngroup by 1, 2, 3\n```\n\n## Follow-up Questions\n- How would you test for schema drift between staging and analytics models?\n- How would you extend this to thousands of tenants while preserving performance?","diagram":"flowchart TD\n  S[staging.stream_events] --> A[analytics.daily_user_events]\n  A --> D[dashboard by tenant/event_type]\n  S --> M[parse payload via macro]\n  A --> U[analytics.users snapshot]","difficulty":"beginner","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","NVIDIA","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T04:16:24.757Z","createdAt":"2026-01-15T04:16:24.757Z"},{"id":"q-2173","question":"Design a per-tenant incremental dbt model for a SaaS analytics pipeline on Snowflake. Source staging.user_events (tenant_id, user_id, event_type, event_timestamp, revenue, lifecycle_stage). Build analytics.{tenant}.daily_cohort to compute daily cohorts by lifecycle_stage and event_type with a 2-day late-data window, upserting via MERGE. Add schema-drift guards, tests (not_null, unique), and a per-tenant last_seen_users snapshot. How do you enforce cross-tenant isolation and lineage?","answer":"Build analytics.{tenant}.daily_cohort as an incremental model sourced from staging.user_events. Compute daily cohorts by lifecycle_stage and event_type with a 2-day late-data window, upserting via MER","explanation":"## Why This Is Asked\nThis probes per-tenant incremental design, late data handling, data quality, and lineage isolation in a realistic SaaS context.\n\n## Key Concepts\n- Incremental modeling with per-tenant schemas\n- Macros for tenant isolation and dynamic schema resolution\n- Late data window and MERGE upserts\n- Schema drift guards and tests (not_null, unique)\n- Snapshots for cohort dynamics and last_seen_users\n- Clear lineage via sources and refs\n\n## Code Example\n```sql\n-- skeleton showing late data handling and tenant scoping\nwith events as (\n  select * from {{ source('staging','user_events') }}\n  where event_timestamp < current_timestamp - interval '2 days'\n)\nselect\n  tenant_id,\n  date(event_timestamp) as cohort_date,\n  event_type,\n  lifecycle_stage,\n  count(distinct user_id) as user_cnt\nfrom events\ngroup by 1,2,3,4\n```\n\n## Follow-up Questions\n- How would you test tenant isolation in CI?\n- How would you adapt this for new event_type dimensions without breaking existing tenants?","diagram":null,"difficulty":"intermediate","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T05:48:29.145Z","createdAt":"2026-01-15T05:48:29.145Z"},{"id":"q-2315","question":"Design a beginner-friendly per-tenant daily_session_summary in Snowflake/dbt. Source: staging.sessions(tenant_id, user_id, session_start). Build analytics.{tenant}.daily_session_summary counting distinct users per tenant per day, with enrichment from dimensions.tenants (plan, region). Implement late data tolerance of 1 day, schema-drift guards, tests (not_null, unique). Include a data-contract macro to validate staging.sessions fields/types and a snapshot on analytics.{tenant}.users. Explain how you ensure cross-tenant isolation and lineage in dashboards?","answer":"Implement a per-tenant daily session summary in dbt: an incremental model analytics.{tenant}.daily_session_summary using a tenant-scoped macro to reference staging.sessions. Define unique key as (tena","explanation":"Why This Is Asked\nAssesses ability to design an end-to-end per-tenant metric with incremental logic, data contracts, and snapshots, plus isolation and lineage considerations.\n\nKey Concepts\n- Per-tenant isolation and lineage\n- Incremental models and late data tolerance\n- Data-contract macros and schema-drift tests\n- Snapshots to capture cohort changes\n\nCode Example\n```sql\nwith s as (\n  select tenant_id, date_trunc('day', session_start) as session_date, user_id\n  from {{ ref('staging_sessions') }}\n  where session_start is not null\n)\nselect tenant_id, session_date, count(distinct user_id) as active_users\nfrom s\ngroup by 1,2\n```\n\nFollow-up Questions\n- How would you validate that late sessions arrive within the 1-day window?\n- How would you handle tenants added after initial deployment (new analytics.{tenant} schemas)?","diagram":null,"difficulty":"beginner","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Microsoft","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T11:42:15.446Z","createdAt":"2026-01-15T11:42:15.446Z"},{"id":"q-2403","question":"Beginner dbt task: from staging.events (event_id, user_id, event_type, occurred_at, region_code, delete_flag) and dimensions.regions, implement an incremental model analytics.daily_event_summary that counts distinct users per day, event_type, country_code (via region_code). Exclude deleted events by default; provide a macro to toggle inclusion for audits. Add tests not_null(event_id, occurred_at) and unique(event_id)?","answer":"Implement analytics.daily_event_summary as an incremental Snowflake model: aggregate date_trunc('day', occurred_at), event_type, country_code (via regions) with count(distinct user_id). Use a macro de","explanation":"## Why This Is Asked\nTests a candidate's ability to implement a simple, robust incremental model with data quality checks and a governance hook (macro) for audits. Emphasizes joins, aggregation, and proper filtering.\n\n## Key Concepts\n- Incremental modeling in dbt\n- Basic joins to enrich with dimensions.regions\n- Simple data governance via macros for optional audit paths\n- Tests: not_null and unique\n\n## Code Example\n```sql\n-- analytics/daily_event_summary.sql\nwith src as (\n  select\n    date_trunc('day', occurred_at) as day,\n    e.event_type,\n    r.country_code,\n    count(distinct e.user_id) as active_users\n  from {{ source('staging','events') }} e\n  join {{ ref('dimensions_regions') }} r on e.region_code = r.region_code\n  where {{ delete_filter(false) }}\n  group by 1,2,3\n)\nselect * from src;\n```\n\n```sql\n-- macros/delete_filter.sql\n{% macro delete_filter(include_deleted=false) -%}\n  {% if include_deleted %}\n    1=1\n  {% else %}\n    delete_flag = false\n  {% endif %}\n{%- endmacro %}\n```\n\n## Follow-up Questions\n- How would you test the macro across multiple models?\n- How would you adapt this to handle late-arriving updates in staging?\n","diagram":"flowchart TD\n  A[staging.events] --> B[analytics.daily_event_summary]\n  B --> C[dashboards/consumption]\n  A --> D[dimensions.regions]\n  D --> B","difficulty":"beginner","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T16:53:24.215Z","createdAt":"2026-01-15T16:53:24.215Z"},{"id":"q-2442","question":"In a dbt project, given staging.events(user_id, event_type, occurred_at, country_code) and staging.users(user_id, created_at, country_code), implement an incremental model analytics.daily_engagement that counts distinct users per day by event_type and country_code, enriched by dimensions.countries. Include 1-day late data tolerance, schema-drift guards, tests (not_null, unique), and a snapshot on analytics.users to track country changes. Describe lineage and isolation notes?","answer":"Create an incremental analytics.daily_engagement that counts distinct user_id per day, grouped by event_type and country_code, joining staging.events with staging.users and dimensions.countries. Allow","explanation":"## Why This Is Asked\n\nAssess ability to design a beginner-friendly incremental model, enforce data quality, and capture slowly changing dimensions with snapshots.\n\n## Key Concepts\n\n- Incremental modeling in dbt for daily aggregates\n- Data quality tests: not_null and unique\n- Late-arriving data handling (1-day tolerance)\n- Snapshot usage for slowly changing dimensions\n- Referential integrity with dimensions.countries and cross-model lineage\n\n## Code Example\n\n```sql\nwith src as (\n  select\n    e.user_id,\n    date( e.occurred_at ) as day,\n    e.event_type,\n    e.country_code\n  from {{ ref('staging_events') }} e\n)\nselect\n  day,\n  event_type,\n  country_code,\n  count(distinct user_id) as user_cnt\nfrom src\ngroup by 1,2,3\n```\n\n## Follow-up Questions\n\n- How would you test that country_code exists in dimensions.countries?\n- How would you incorporate a 1-day late data window into your model without double-counting users?","diagram":"flowchart TD\n  S1[staging.events] --> EN[analytics.daily_engagement]\n  U1[staging.users] --> EN\n  EN --> C[dimensions.countries]\n  EN --> SN[analytics.users_snapshot]","difficulty":"beginner","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T17:59:36.342Z","createdAt":"2026-01-15T17:59:36.342Z"},{"id":"q-2502","question":"Design a contract-validated, multi-tenant revenue analytics flow in dbt on Snowflake. Raw events live in staging.{tenant}.events (tenant_id, user_id, event_type, amount, currency, occurred_at). Build a incremental analytics.{tenant}.daily_revenue that sums revenue per day/tenant/currency, with 2-day late data tolerance. Create contracts.tenants describing required fields and a macro that fails tests when missing fields. Add per-tenant analytics.{tenant}.users snapshot for status changes. Enforce per-tenant schemas, schema-drift guards, and tests (not_null, unique). Explain cross-tenant lineage and performance considerations?","answer":"Implement a per-tenant incremental MERGE from staging.{tenant}.events into analytics.{tenant}.daily_revenue (key: tenant_id, date, currency) with a 2-day late data window and record_hash for idempoten","explanation":"## Why This Is Asked\nAssesses ability to design contract-driven, multi-tenant dbt pipelines with late data handling and governance.\n\n## Key Concepts\n- Incremental models and MERGE in Snowflake\n- Data contracts and custom tests\n- Per-tenant schema isolation and lineage\n- Snapshots for slowly changing user state\n- Schema drift guards and test coverage\n\n## Code Example\n```sql\n-- example placeholder: actual implementation in repo\n```\n\n## Follow-up Questions\n- How would you automate contract drift detection across tenants?\n- How do you evolve contracts without breaking dashboards?\n","diagram":"flowchart TD\n  S[staging.{tenant}.events] --> A[incremental daily_revenue]\n  A --> R[analytics.{tenant}.daily_revenue]\n  C[contracts.tenants] --> M[contract-enforcer macro]\n  U[analytics.{tenant}.users Snapshot] --> L[lineage across tenants]","difficulty":"advanced","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Microsoft","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T20:44:34.530Z","createdAt":"2026-01-15T20:44:34.531Z"},{"id":"q-2685","question":"Design a beginner dbt task to build an incremental per-tenant analytics.daily_events model in Snowflake from staging.events(tenant_id, user_id, event_type, occurred_at). Include enrichment from dimensions.event_types, a 2-day late-data window, per-tenant isolation via schemas, and schema-drift guards. Add tests (not_null, unique) and a snapshot for analytics.{tenant}.users; also implement a simple data-contract macro to validate staging.events columns/types and a test using it?","answer":"Propose an incremental per-tenant daily_events model that slices by day, joins staging.events with dimensions.event_types for category enrichment, and uses per-tenant schemas to enforce isolation. Imp","explanation":"## Why This Is Asked\nTests coverage for multi-tenant isolation, data contracts, and incremental modeling in a beginner context. It also introduces a lightweight data-quality macro beyond basic tests.\n\n## Key Concepts\n- dbt incremental models in Snowflake\n- per-tenant schema isolation\n- data contracts via custom macros\n- tests: not_null, unique; snapshots for cohort tracking\n\n## Code Example\n```javascript\n-- Example macro to validate staging.events columns\n{% macro validate_staging_events(cols) %}\n  {# implementation details #}\n{% endmacro %}\n\n-- Example incremental model sketch\nwith staged as (\n  select tenant_id, user_id, event_type, occurred_at,\n         date_trunc('day', occurred_at) as day\n  from {{ source('staging','events') }}\n  where occurred_at > (current_date() - interval '2 days')\n), enriched as (\n  select s.*, e.category\n  from staged s\n  left join {{ ref('dimensions.event_types') }} e\n    on s.event_type = e.event_type\n)\nselect tenant_id, day, event_type, count(*) as event_count\nfrom enriched\ngroup by 1,2,3\n{% if is_incremental() %}\nwhere day > (select max(day) from {{ this }})\n{% endif %}\n```\n\n## Follow-up Questions\n- How would you handle late data beyond 2 days and retractions?\n- How would you test for schema drift across tenants without cross-tenant leakage?","diagram":"flowchart TD\n  A[staging.events] --> B[dimensions.event_types enrichment]\n  B --> C[analytics.{tenant}.daily_events]\n  C --> D[tests & schema drift guards]\n  D --> E[analytics.{tenant}.users snapshot]","difficulty":"beginner","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Instacart","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T06:57:40.373Z","createdAt":"2026-01-16T06:57:40.373Z"},{"id":"q-848","question":"You're designing a dbt analytics pipeline for an e-commerce platform. The 'staging.events_raw' table ingests page views and purchases and is partitioned by day. Data arrives both on time and as late as 24 hours. Design a practical incremental dbt model 'analytics.daily_sales' that aggregates revenue by day, category, and customer_segment. Explain how you'd implement incremental logic, handle late data, guard against schema drift, and validate with tests and snapshots?","answer":"Use an incremental model keyed by day, category, and customer_segment. Compute revenue as sum(price*quantity) from staging.events_raw. Upsert into analytics.daily_sales via MERGE on (day, category, cu","explanation":"## Why This Is Asked\n\nAssess ability to design scalable, fault-tolerant dbt pipelines handling late data and schema drift.\n\n## Key Concepts\n\n- Incremental models and idempotent MERGE\n- Late-arrival handling and backfill windows\n- Schema drift detection via tests and snapshots\n- Performance at scale and CI validation\n\n## Code Example\n\n```sql\n-- Example incremental sql sketch\nWITH s AS (\n  SELECT day, category, customer_segment,\n         SUM(price * quantity) AS revenue\n  FROM {{ ref('staging__events_raw') }}\n  WHERE day >= (SELECT MIN(day) FROM analytics.daily_sales)\n  GROUP BY day, category, customer_segment\n)\nMERGE INTO analytics.daily_sales AS t\nUSING s AS s\nON t.day = s.day AND t.category = s.category AND t.customer_segment = s.customer_segment\nWHEN MATCHED THEN UPDATE SET revenue = s.revenue\nWHEN NOT MATCHED THEN INSERT (day, category, customer_segment, revenue) VALUES (s.day, s.category, s.customer_segment, s.revenue);\n```\n\n## Follow-up Questions\n\n- How would you test for idempotency and late-arrival correctness?\n- How would you adapt this for a lakehouse (Parquet) or streaming source?","diagram":null,"difficulty":"advanced","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Anthropic","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T13:30:20.377Z","createdAt":"2026-01-12T13:30:20.377Z"},{"id":"q-972","question":"In a dbt analytics project deployed to Snowflake, three tenants share a common model but data is isolated by schema prefixes (tenant_a_, tenant_b_, tenant_c_). Describe how you would structure tenancy-aware tests and a test runner to prevent cross-tenant data leakage during PR runs. Include how you configure sources, seeds, and per-tenant test filtering, and how you verify isolation in CI without touching production data?","answer":"A tenancy-aware dbt run parameterizes the target schema per tenant, isolates seeds and sources with tenant prefixes, and uses a tenancy-aware test macro to scope checks. In CI, create ephemeral tenant","explanation":"## Why This Is Asked\nThis question probes how a candidate ensures strict data isolation in a multi-tenant dbt setup during CI, a common real-world issue.\n\n## Key Concepts\n- Tenancy-aware testing across schemas\n- Per-tenant seeds/sources isolation\n- Test filtering macros and CI ephemeral environments\n- Cross-tenant leakage detection without touching prod\n\n## Code Example\n```javascript\n-- macros/tenancy_filters.sql\n{% macro tenant_test_filter(tenant) -%}\n  tenant_id = '{{ tenant }}'\n{%- endmacro %}\n```\n\n```javascript\n-- tests/tenancy_isolation_test.sql\nselect * from {{ ref('customers') }}\nwhere {{ tenant_test_filter('tenant_a') }}\n```\n\n## Follow-up Questions\n- How would you extend this for dynamic tenant onboarding?\n- How do you validate that physical storage quotas per tenant are respected?","diagram":"flowchart TD\n  PR[Pull Request] --> TESTS[Run tenancy-aware tests]\n  TESTS --> PASS{All tenant tests pass?}\n  PASS --> MERGE[Merge PR]\n  PASS --> REBUILD[Rebuild docs if needed]\n  FAIL --> NOTIFY[Notify engineer]","difficulty":"advanced","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Lyft","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T17:35:20.447Z","createdAt":"2026-01-12T17:35:20.447Z"}],"subChannels":["general"],"companies":["Amazon","Anthropic","Apple","Bloomberg","Citadel","Cloudflare","Discord","DoorDash","Goldman Sachs","Google","Hashicorp","Hugging Face","IBM","Instacart","LinkedIn","Lyft","Meta","Microsoft","MongoDB","NVIDIA","Netflix","OpenAI","Oracle","PayPal","Plaid","Salesforce","Slack","Snowflake","Stripe","Tesla","Twitter","Uber","Zoom"],"stats":{"total":26,"beginner":11,"intermediate":8,"advanced":7,"newThisWeek":26}}