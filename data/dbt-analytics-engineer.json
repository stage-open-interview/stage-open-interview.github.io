{"questions":[{"id":"q-1125","question":"You're building an incremental dbt model analytics.daily_revenue_by_product sourced from staging.sales_raw. Data can arrive late up to 2 days. How would you implement incremental upserts, reprocess late days without disturbing older data, and validate with tests and a snapshot of product price history? Provide a minimal SQL snippet illustrating the approach?","answer":"Implement an incremental model with unique_key=['day','product_id','region']; reprocess the last 2 days during incremental runs to absorb late data; use MERGE/upsert where supported, else a two-phase ","explanation":"## Why This Is Asked\nTests practical incremental logic and late-arrival handling; checks upsert patterns, tests, and snapshots.\n\n## Key Concepts\n- Incremental materialization with unique keys\n- Late-arrival handling window\n- Snapshot for price history\n- Basic tests for data quality\n\n## Code Example\n```sql\n-- models/analytics/daily_revenue_by_product.sql\n{{ config(materialized='incremental', unique_key=['day','product_id','region']) }}\n\nwith s as (\n  select date_trunc('day', order_date) as day,\n         product_id,\n         region,\n         sum(quantity * price) as revenue\n  from {{ ref('staging_sales_raw') }}\n  group by 1,2,3\n)\n\nselect day, product_id, region, revenue\nfrom s\n{% if is_incremental() %}\n  -- reprocess last 2 days to capture late data\n  where day >= (select max(day) from {{ this }}) - interval '2 day'\n{% endif %}\n```\n\n## Follow-up Questions\n- How would you adapt this for Redshift vs Snowflake?\n- How would you test the snapshot consistency across environments?","diagram":null,"difficulty":"beginner","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Hashicorp"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T23:31:34.187Z","createdAt":"2026-01-12T23:31:34.187Z"},{"id":"q-1165","question":"You're designing a dbt analytics pipeline for a ride-sharing platform. The raw events are in `staging.events_raw` with columns like `ride_id`, `city`, `ride_type`, `currency`, `amount`, `occurred_at`, and data can arrive up to 48 hours late. Build a beginner-friendly incremental model `analytics.daily_revenue` that computes USD revenue per day by `date`, `city`, and `ride_type`. Use a daily `pricing.exchange_rates` table to convert from local currency to USD. Describe how you'd implement incremental upserts, late-data handling (2-day window), schema-drift guards, and validation with tests and a snapshot. Also outline tests for late refunds and missing rates?","answer":"Use an incremental model with is_incremental(). Join staging events to pricing.exchange_rates on date and currency, then aggregate revenue in USD by date, city, ride_type. Implement a 2-day late-data ","explanation":"## Why This Is Asked\nThe question probes practical incremental modeling with late data, currency conversion, and drift safeguards—core dbt skills at junior to mid-beginner level.\n\n## Key Concepts\n- Incremental models and upserts\n- Currency conversion via reference table\n- Late-data window handling\n- Schema-drift guards and tests\n- Snapshots for drift detection\n\n## Code Example\n```sql\n-- dbt model: analytics/daily_revenue.sql\nwith events as (\n  select\n    date_trunc('day', occurred_at) as date,\n    city,\n    ride_type,\n    currency,\n    amount\n  from {{ source('staging','events_raw') }}\n  where occurred_at <= (current_timestamp() - interval '2 days')\n),\nrates as (\n  select date, currency, rate\n  from {{ source('pricing','exchange_rates') }}\n)\nselect\n  e.date,\n  e.city,\n  e.ride_type,\n  sum(e.amount * coalesce(r.rate, 1.0)) as revenue_usd\nfrom events e\nleft join rates r\n  on r.date = e.date and r.currency = e.currency\ngroup by 1,2,3\n```\n\n## Follow-up Questions\n- How would you handle gaps in exchange_rates (missing rates)?\n- How would you validate late refunds affecting revenue?\n- How would you test for time zone consistency across cities?","diagram":null,"difficulty":"beginner","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T03:31:16.675Z","createdAt":"2026-01-13T03:31:16.675Z"},{"id":"q-1257","question":"Design a drift-aware dbt workflow across dev/staging/prod in Snowflake. How would you detect schema drift between sources and models using snapshots, tests, and sources, enforce a drift threshold, and automatically fail CI when drift exceeds the threshold? Describe the macro, tests, and alert/rollback mechanism, plus how you version thresholds?","answer":"Implement a dbt macro drift_check that compares per-column metadata (data type, nullability, default, and max length) between a source table and its corresponding model, plus a runtime comparison of r","explanation":"## Why This Is Asked\nReal-world pipelines require automated detection of schema drift to prevent deployment of broken analytics. This question tests practical macro design, test coverage, and CI/CD integration.\n\n## Key Concepts\n- dbt snapshots and sources for metadata\n- Per-column drift checks (type, nullability, max length)\n- Threshold-driven gating in CI/CD and rollback policies\n- Canary prod lineage validation and alerting\n\n## Code Example\n```jinja\n{% macro drift_check(source_schema, source_table, model_schema, model_table, thresholds) -%}\n-- pseudo-logic: compare metadata and basic stats, emit drift_count if > thresholds\n{%- endmacro %}\n```\n\n## Follow-up Questions\n- How would you scale per-table thresholds across hundreds of tables?\n- How would you distinguish benign drift (e.g., new optional column) from breaking drift?","diagram":null,"difficulty":"advanced","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Google","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T07:23:31.079Z","createdAt":"2026-01-13T07:23:31.079Z"},{"id":"q-1361","question":"Design a beginner-friendly incremental dbt model in Snowflake for a SaaS analytics pipeline: raw events live at staging.event_logs with user_id, event_type (signup, login, purchase), country, occurred_at. Build analytics.daily_metrics that counts distinct active users by day and event_type, enriched by dimensions.countries. Explain incremental logic, late data handling (7 days), schema drift guards, and validation with tests and a snapshot. Also outline an Exposure for the dashboard with lineage?","answer":"Build analytics.daily_metrics as an incremental Snowflake model that counts distinct user_id by day and event_type from staging.event_logs, enriched via a join to dimensions.countries on country. Upse","explanation":"## Why This Is Asked\nThis question probes practical dbt incremental modeling, enrichment, and the linkage to dashboards via exposures.\n\n## Key Concepts\n- Incremental models with upserts in Snowflake\n- Late-arriving data handling with a 7-day window\n- Dimensional enrichment via dimensions.countries\n- Schema-drift guards and tests\n- Snapshots for user cohorts\n- Exposures and lineage visualization\n\n## Code Example\n```sql\nwith src as (\n  select distinct user_id, date(occurred_at) as day, event_type, country\n  from {{ ref('staging.event_logs') }}\n  where occurred_at >= current_timestamp() - interval '7 day'\n)\nselect day, event_type, country, count(distinct user_id) as active_users\nfrom src\ngroup by 1,2,3\n```\n\n## Follow-up Questions\n- How would you test referential integrity between analytics.daily_metrics and dimensions.countries?\n- How would you expose this model in a dashboard and ensure lineage is accurate?","diagram":null,"difficulty":"beginner","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Snowflake","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T13:15:09.731Z","createdAt":"2026-01-13T13:15:09.731Z"},{"id":"q-1523","question":"Design an incremental dbt model analytics.daily_engagement for a global multi-tenant app. Raw events sit in staging.user_events with tenant_id, user_id, email, event_type, occurred_at, privacy_flag. Build per-tenant daily distinct-user counts by event_type, redacting emails when privacy_flag is true. Describe incremental strategy, late data window, schema-drift guards, tests and snapshots, and how to expose lineage for dashboards?","answer":"Use a (tenant_id, date( occurred_at ), event_type) keyed incremental model. Compute distinct user_id from staging.user_events, redact email via a macro: if privacy_flag then 'REDACTED' else email. Lat","explanation":"## Why This Is Asked\nTests ability to design privacy-aware, tenant-scoped analytics with late-arriving data. Emphasizes incremental logic, governance, and lineage.\n\n## Key Concepts\n- Incremental models with composite keys (tenant_id, date, event_type)\n- Field-level redaction controlled by privacy_flag via a macro\n- Late-arriving data handled through a short lookback MERGE\n- Schema drift guards using sources, tests, and snapshots\n- Exposures to reflect lineage in dashboards\n\n## Code Example\n```sql\n-- macros/redact_email.sql\n{% macro redact_email(email, privacy_flag) -%}\n  {% if privacy_flag -%}\n    {{ return('REDACTED') }}\n  {% else -%}\n    {{ return(email) }}\n  {% endif -%}\n{% endmacro %}\n```\n\n## Follow-up Questions\n- How would you test the redact_email macro across combinations of privacy_flag and null emails?\n- How would you scale this approach to thousands of tenants with varying data freshness SLAs?","diagram":null,"difficulty":"intermediate","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","IBM","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T20:40:54.485Z","createdAt":"2026-01-13T20:40:54.486Z"},{"id":"q-1560","question":"In a dbt project, you ingest raw events into `staging.web_events` with columns: `event_id`, `user_id`, `event_type`, `country`, `occurred_at` (UTC). Build a beginner-friendly incremental model **analytics.daily_active_users** that reports the count of distinct `user_id`s per day, by `country` and `event_type`. Data can arrive up to 2 days late. Describe your incremental logic, how you handle late data with a rolling window, how to guard against schema drift, and how you validate with tests. Also draft an Exposure for a dashboard showing daily active users by country and event_type, including lineage?","answer":"Use an incremental model that computes daily active users by date(occurred_at) in UTC, grouped by country and event_type, then upserts into analytics.daily_active_users. For late data, retain a 2-day rolling window and reprocess the last 2 days on each run. Guard against schema drift with explicit column definitions and dbt's source freshness checks. Validate with unique user tests, null checks, and row count expectations.","explanation":"## Why This Is Asked\nAssesses practical dbt incremental modeling with late data handling and exposure design.\n\n## Key Concepts\n- Incremental model by day\n- Late-arrival window (2 days)\n- Schema drift guards\n- Tests and a dashboard exposure with lineage\n\n## Code Example\n```sql\nSELECT\n  date_trunc('day', occurred_at) AS day,\n  country,\n  event_type,\n  COUNT(DISTINCT user_id) AS active_users\nFROM {{ ref('staging.web_events') }}\nGROUP BY 1,2,3\n```\n\n## Follow-up Questions\n- How would you handle time zones for users across regions?\n- How would you adapt for evolving event_type vocabularies?","diagram":null,"difficulty":"beginner","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","DoorDash","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T06:19:05.797Z","createdAt":"2026-01-13T21:45:41.380Z"},{"id":"q-1590","question":"Design a two-model incremental dbt flow in Snowflake: 1) analytics.first_session derives first_session_at per user and seeds analytics.users(user_id, first_session_at, country_code, cohort). 2) analytics.daily_metrics counts daily active users by day and event_type, enriched with dimensions.countries. Include late-data handling (3 days), schema-drift guards, tests (not_null, unique), a snapshot, and expose lineage to dashboards?","answer":"Approach: Implement a two-model incremental dbt flow in Snowflake. First, analytics.first_session computes min(occurred_at) per user as first_session_at and seeds analytics.users with user_id, first_session_at, country_code, and cohort. Second, analytics.daily_metrics runs incrementally with a 3-day late-data watermark, counting daily active users by day and event_type while joining dimensions.countries for enrichment. Include schema-drift guards, not_null/unique tests, and a snapshot for slowly changing fields.","explanation":"## Why This Is Asked\nTests ability to design robust incremental pipelines, handle late data, and maintain lineage.\n\n## Key Concepts\n- Incremental models and ref() usage\n- First-session derivation and cohort calculation\n- Late-data watermark and idempotent upserts\n- Schema drift guards and tests; snapshots for slowly changing fields\n- Dashboards exposure and lineage\n\n## Code Example\n```sql\n-- first_session model (simplified)\nwith e as (\n  select user_id, occurred_at, country_code\n  from {{ source('raw','events_log') }}\n)\nselect user_id, min(occurred_at) as first_session_at, country_code,\n      \"","diagram":"flowchart TD\n  A[raw.events_log] --> B[analytics.first_session]\n  B --> C[analytics.users]\n  C --> D[analytics.daily_metrics]\n  E[dimensions.countries] --> C","difficulty":"intermediate","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T05:35:22.438Z","createdAt":"2026-01-13T22:55:10.734Z"},{"id":"q-1604","question":"In a dbt pipeline powering a fintech analytics platform, raw events arrive at staging.fin_events with customer_id, txn_id, amount, currency, status, event_ts. Design an incremental analytics.transactions model that deduplicates by txn_id across sources, handles late events within a 2-day window, validates currency via a macro-based set per source, guards against schema drift with a dynamic mapping table, and snapshots customers on their first txn for dashboard lineage. Include tests and performance considerations?","answer":"I would implement an incremental analytics.transactions model with unique_key txn_id, deduplicating across sources, handling late events within a 2-day watermark window, validating currency via a custom macro per source, guarding against schema drift with a dynamic mapping table, and snapshotting customers on their first transaction for dashboard lineage.","explanation":"## Why This Is Asked\n\nTests advanced dbt patterns: deduplication, late-arrival handling, schema drift detection, and data lineage.\n\n## Key Concepts\n\n- Incremental merge with per-source deduplication on txn_id\n- Watermark-based late-arrival handling (2 days)\n- Macro-based currency validation per source\n- Dynamic schema drift guards via mapping tables\n- Snapshot on customers for first transaction lineage\n- Tests: not_null, unique, relationships\n- Performance: clustering, pruning, and date-based partitioning\n\n## Code Example\n\n```sql\n-- dbt incremental model sketch\n{{ config(materialized='incremental', unique_key='txn_id') }}\n\nWITH deduped_events AS (\n  SELECT *,\n         ROW_NUMBER() OVER (\n           PARTITION BY txn_id, source \n           ORDER BY event_ts DESC\n         ) AS rn\n  FROM {{ ref('staging_fin_events') }}\n  WHERE event_ts >= DATEADD('day', -2, CURRENT_TIMESTAMP)\n),\n\nvalidated_events AS (\n  SELECT *\n  FROM deduped_events\n  WHERE rn = 1\n    AND {{ validate_currency(source, currency) }}\n),\n\nfinal_transactions AS (\n  SELECT *\n  FROM validated_events\n  WHERE {{ schema_drift_guard() }}\n)\n\nSELECT * FROM final_transactions\n```\n\n## Implementation Notes\n\n- Use `dbt snapshot` for customer first-transaction tracking\n- Implement currency validation macro with source-specific rules\n- Create mapping table for dynamic schema drift detection\n- Add comprehensive tests for data quality and performance","diagram":null,"difficulty":"advanced","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T05:31:38.231Z","createdAt":"2026-01-14T02:32:20.530Z"},{"id":"q-1714","question":"In a beginner dbt project for a media site, staging.events_logs(user_id, event_type, occurred_at, region_code) feeds analytics.daily_engagement that counts distinct users per date and event_type, enriched by dimensions.regions on region_code. Build the incremental model with late data window (3 days), include schema-drift guards, and tests (not_null on user_id, occurred_at; unique on (user_id, occurred_at, event_type)). Add a data-contract macro/test ensuring staging.events_logs contains required fields and types, and a snapshot on users to capture region changes. Explain approach, tests, and macro design?","answer":"I’d implement analytics.daily_engagement as an incremental model that aggregates distinct users per date and event_type, joining dimensions.regions on region_code. Late data is allowed in a 3-day wind","explanation":"## Why This Is Asked\nThis question probes practical dbt workflow: incremental logic, late-arrival handling, schema drift guardrails, data-contract testing, and snapshot usage. It covers core dbt skills in a realistic, beginner-friendly way.\n\n## Key Concepts\n- Incremental models and late data windows\n- Schema drift guards and tests\n- Data-contracts via macros/tests\n- Snapshots for slowly changing attributes\n\n## Code Example\n```javascript\n// Pseudo-contract test skeleton (dbt-style macro would be SQL/Jinja in practice)\nfunction assertContract(stagingColumns, required) {\n  const missing = required.filter(r => !stagingColumns.includes(r));\n  if (missing.length) throw new Error(\"Missing: \" + missing.join(\", \"));\n}\n```\n\n## Follow-up Questions\n- How would you extend the contract to handle optional fields?\n- How would you validate contracts across multiple environments (dev/stage/prod)?","diagram":null,"difficulty":"beginner","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Cloudflare","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T07:49:20.068Z","createdAt":"2026-01-14T07:49:20.068Z"},{"id":"q-1741","question":"You're building a multi-tenant dbt analytics pipeline on Snowflake. Raw events live in staging.events with tenant_id, user_id, event_type, occurred_at. You plan per-tenant analytics schemas (analytics.{tenant}). Design an incremental analytics.daily_metrics per tenant that counts distinct active users by day and event_type, with late data up to 2 days, joining dimensions.tenants and dimensions.countries, including schema-drift guards, tests (not_null, unique), and a snapshot for user_first_seen per tenant. Explain approach for cross-tenant lineage and tenant-scoped materializations?","answer":"Per-tenant schemas in Snowflake: implement analytics.daily_metrics as an incremental model scoped to analytics.${tenant}. daily by (day, tenant_id, event_type). Late data window: 2 days; use MERGE int","explanation":"## Why This Is Asked\nTests the ability to architect multi-tenant dbt pipelines with strict isolation (per-tenant schemas), robust late-data handling, and governance via tests and snapshots. It also probes strategies for cross-tenant lineage in dashboards. \n\n## Key Concepts\n- Multi-tenant isolation with per-tenant schemas\n- Incremental modeling with late-arrival handling\n- Schema drift guards and robust tests\n- Snapshots for slowly changing dimensions per tenant\n- Clear lineage exposure for dashboards across tenants\n\n## Code Example\n````sql\n-- analytics.{{tenant}}.daily_metrics.sql\nwith src as (\n  select tenant_id, user_id, event_type, date(occurred_at) as day\n  from {{ source('raw','events') }}\n)\nselect day, tenant_id, event_type, count(distinct user_id) as active_users\nfrom src\ngroup by day, tenant_id, event_type\n````\n\n## Follow-up Questions\n- How would you automate tenancy onboarding to new schemas without gluing code?\n- How do you test for cross-tenant join correctness without data leakage?","diagram":null,"difficulty":"advanced","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T09:01:41.446Z","createdAt":"2026-01-14T09:01:41.447Z"},{"id":"q-1757","question":"Design a dbt analytics layer for three domains: core_events (user actions), memberships (plans), and referrals. Data arrives late (up to 2 days). Propose an architecture that ensures: 1) incremental models with controlled late data backfills, 2) deterministic surrogate keys for cross-domain joins, 3) schema-drift guards via custom tests/macros, 4) automated docs with complete lineage, 5) dashboard-friendly exposure with stable lineage during backfills, and 6) performance considerations for Snowflake or BigQuery (partitioning, clustering). Include a concrete incremental SQL snippet?","answer":"Propose staging per domain, a single grain, incremental models with a 2-day late-arrival window, a surrogate key like domain_key + occurred_at + id for cross-domain joins, a macro-based cross-domain r","explanation":"## Why This Is Asked\nTests end-to-end dbt mastery across multi-domain conformance, late data handling, and lineage stability during backfills. It also probes custom macros for data quality and cross-domain integrity, plus performance considerations for two leading warehouses.\n\n## Key Concepts\n- Incremental models with late-arrival windows\n- Cross-domain surrogate keys and referential integrity\n- Schema drift guards via tests and macros\n- Automated docs with complete lineage\n- Backfill strategy ensuring stable dashboards\n- Performance tuning for Snowflake vs BigQuery (partitioning, clustering)\n\n## Code Example\n```sql\n-- models/analytics/fact_core_events_incremental.sql\n{{ config(materialized='incremental') }}\n\nwith src as (\n  select\n    user_id as domain_user_id,\n    event_type,\n    occurred_at,\n    domain\n  from {{ source('core_events', 'events') }}\n  where occurred_at >= dateadd(day, -2, current_date())\n)\n\nselect\n  domain_user_id,\n  domain,\n  max(occurred_at) as occurred_at,\n  count(*) as event_count\nfrom src\ngroup by domain_user_id, domain\n{% if is_incremental() %}\nwhere occurred_at > (select max(occurred_at) from {{ this }})\n{% endif %}\n```\n\n## Follow-up Questions\n- How would you validate cross-domain joins under backfills?\n- What tests would you add to guard against schema drift across domains?","diagram":null,"difficulty":"advanced","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","MongoDB","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T09:44:23.172Z","createdAt":"2026-01-14T09:44:23.172Z"},{"id":"q-848","question":"You're designing a dbt analytics pipeline for an e-commerce platform. The 'staging.events_raw' table ingests page views and purchases and is partitioned by day. Data arrives both on time and as late as 24 hours. Design a practical incremental dbt model 'analytics.daily_sales' that aggregates revenue by day, category, and customer_segment. Explain how you'd implement incremental logic, handle late data, guard against schema drift, and validate with tests and snapshots?","answer":"Use an incremental model keyed by day, category, and customer_segment. Compute revenue as sum(price*quantity) from staging.events_raw. Upsert into analytics.daily_sales via MERGE on (day, category, cu","explanation":"## Why This Is Asked\n\nAssess ability to design scalable, fault-tolerant dbt pipelines handling late data and schema drift.\n\n## Key Concepts\n\n- Incremental models and idempotent MERGE\n- Late-arrival handling and backfill windows\n- Schema drift detection via tests and snapshots\n- Performance at scale and CI validation\n\n## Code Example\n\n```sql\n-- Example incremental sql sketch\nWITH s AS (\n  SELECT day, category, customer_segment,\n         SUM(price * quantity) AS revenue\n  FROM {{ ref('staging__events_raw') }}\n  WHERE day >= (SELECT MIN(day) FROM analytics.daily_sales)\n  GROUP BY day, category, customer_segment\n)\nMERGE INTO analytics.daily_sales AS t\nUSING s AS s\nON t.day = s.day AND t.category = s.category AND t.customer_segment = s.customer_segment\nWHEN MATCHED THEN UPDATE SET revenue = s.revenue\nWHEN NOT MATCHED THEN INSERT (day, category, customer_segment, revenue) VALUES (s.day, s.category, s.customer_segment, s.revenue);\n```\n\n## Follow-up Questions\n\n- How would you test for idempotency and late-arrival correctness?\n- How would you adapt this for a lakehouse (Parquet) or streaming source?","diagram":null,"difficulty":"advanced","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Anthropic","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T13:30:20.377Z","createdAt":"2026-01-12T13:30:20.377Z"},{"id":"q-972","question":"In a dbt analytics project deployed to Snowflake, three tenants share a common model but data is isolated by schema prefixes (tenant_a_, tenant_b_, tenant_c_). Describe how you would structure tenancy-aware tests and a test runner to prevent cross-tenant data leakage during PR runs. Include how you configure sources, seeds, and per-tenant test filtering, and how you verify isolation in CI without touching production data?","answer":"A tenancy-aware dbt run parameterizes the target schema per tenant, isolates seeds and sources with tenant prefixes, and uses a tenancy-aware test macro to scope checks. In CI, create ephemeral tenant","explanation":"## Why This Is Asked\nThis question probes how a candidate ensures strict data isolation in a multi-tenant dbt setup during CI, a common real-world issue.\n\n## Key Concepts\n- Tenancy-aware testing across schemas\n- Per-tenant seeds/sources isolation\n- Test filtering macros and CI ephemeral environments\n- Cross-tenant leakage detection without touching prod\n\n## Code Example\n```javascript\n-- macros/tenancy_filters.sql\n{% macro tenant_test_filter(tenant) -%}\n  tenant_id = '{{ tenant }}'\n{%- endmacro %}\n```\n\n```javascript\n-- tests/tenancy_isolation_test.sql\nselect * from {{ ref('customers') }}\nwhere {{ tenant_test_filter('tenant_a') }}\n```\n\n## Follow-up Questions\n- How would you extend this for dynamic tenant onboarding?\n- How do you validate that physical storage quotas per tenant are respected?","diagram":"flowchart TD\n  PR[Pull Request] --> TESTS[Run tenancy-aware tests]\n  TESTS --> PASS{All tenant tests pass?}\n  PASS --> MERGE[Merge PR]\n  PASS --> REBUILD[Rebuild docs if needed]\n  FAIL --> NOTIFY[Notify engineer]","difficulty":"advanced","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Lyft","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T17:35:20.447Z","createdAt":"2026-01-12T17:35:20.447Z"}],"subChannels":["general"],"companies":["Amazon","Anthropic","Apple","Bloomberg","Cloudflare","DoorDash","Goldman Sachs","Google","Hashicorp","IBM","Lyft","Meta","MongoDB","NVIDIA","OpenAI","Oracle","PayPal","Slack","Snowflake","Stripe","Tesla","Zoom"],"stats":{"total":13,"beginner":5,"intermediate":2,"advanced":6,"newThisWeek":13}}