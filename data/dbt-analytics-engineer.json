{"questions":[{"id":"q-1125","question":"You're building an incremental dbt model analytics.daily_revenue_by_product sourced from staging.sales_raw. Data can arrive late up to 2 days. How would you implement incremental upserts, reprocess late days without disturbing older data, and validate with tests and a snapshot of product price history? Provide a minimal SQL snippet illustrating the approach?","answer":"Implement an incremental model with unique_key=['day','product_id','region']; reprocess the last 2 days during incremental runs to absorb late data; use MERGE/upsert where supported, else a two-phase ","explanation":"## Why This Is Asked\nTests practical incremental logic and late-arrival handling; checks upsert patterns, tests, and snapshots.\n\n## Key Concepts\n- Incremental materialization with unique keys\n- Late-arrival handling window\n- Snapshot for price history\n- Basic tests for data quality\n\n## Code Example\n```sql\n-- models/analytics/daily_revenue_by_product.sql\n{{ config(materialized='incremental', unique_key=['day','product_id','region']) }}\n\nwith s as (\n  select date_trunc('day', order_date) as day,\n         product_id,\n         region,\n         sum(quantity * price) as revenue\n  from {{ ref('staging_sales_raw') }}\n  group by 1,2,3\n)\n\nselect day, product_id, region, revenue\nfrom s\n{% if is_incremental() %}\n  -- reprocess last 2 days to capture late data\n  where day >= (select max(day) from {{ this }}) - interval '2 day'\n{% endif %}\n```\n\n## Follow-up Questions\n- How would you adapt this for Redshift vs Snowflake?\n- How would you test the snapshot consistency across environments?","diagram":null,"difficulty":"beginner","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Hashicorp"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T23:31:34.187Z","createdAt":"2026-01-12T23:31:34.187Z"},{"id":"q-1165","question":"You're designing a dbt analytics pipeline for a ride-sharing platform. The raw events are in `staging.events_raw` with columns like `ride_id`, `city`, `ride_type`, `currency`, `amount`, `occurred_at`, and data can arrive up to 48 hours late. Build a beginner-friendly incremental model `analytics.daily_revenue` that computes USD revenue per day by `date`, `city`, and `ride_type`. Use a daily `pricing.exchange_rates` table to convert from local currency to USD. Describe how you'd implement incremental upserts, late-data handling (2-day window), schema-drift guards, and validation with tests and a snapshot. Also outline tests for late refunds and missing rates?","answer":"Use an incremental model with is_incremental(). Join staging events to pricing.exchange_rates on date and currency, then aggregate revenue in USD by date, city, ride_type. Implement a 2-day late-data ","explanation":"## Why This Is Asked\nThe question probes practical incremental modeling with late data, currency conversion, and drift safeguards—core dbt skills at junior to mid-beginner level.\n\n## Key Concepts\n- Incremental models and upserts\n- Currency conversion via reference table\n- Late-data window handling\n- Schema-drift guards and tests\n- Snapshots for drift detection\n\n## Code Example\n```sql\n-- dbt model: analytics/daily_revenue.sql\nwith events as (\n  select\n    date_trunc('day', occurred_at) as date,\n    city,\n    ride_type,\n    currency,\n    amount\n  from {{ source('staging','events_raw') }}\n  where occurred_at <= (current_timestamp() - interval '2 days')\n),\nrates as (\n  select date, currency, rate\n  from {{ source('pricing','exchange_rates') }}\n)\nselect\n  e.date,\n  e.city,\n  e.ride_type,\n  sum(e.amount * coalesce(r.rate, 1.0)) as revenue_usd\nfrom events e\nleft join rates r\n  on r.date = e.date and r.currency = e.currency\ngroup by 1,2,3\n```\n\n## Follow-up Questions\n- How would you handle gaps in exchange_rates (missing rates)?\n- How would you validate late refunds affecting revenue?\n- How would you test for time zone consistency across cities?","diagram":null,"difficulty":"beginner","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T03:31:16.675Z","createdAt":"2026-01-13T03:31:16.675Z"},{"id":"q-1257","question":"Design a drift-aware dbt workflow across dev/staging/prod in Snowflake. How would you detect schema drift between sources and models using snapshots, tests, and sources, enforce a drift threshold, and automatically fail CI when drift exceeds the threshold? Describe the macro, tests, and alert/rollback mechanism, plus how you version thresholds?","answer":"Implement a dbt macro drift_check that compares per-column metadata (data type, nullability, default, and max length) between a source table and its corresponding model, plus a runtime comparison of r","explanation":"## Why This Is Asked\nReal-world pipelines require automated detection of schema drift to prevent deployment of broken analytics. This question tests practical macro design, test coverage, and CI/CD integration.\n\n## Key Concepts\n- dbt snapshots and sources for metadata\n- Per-column drift checks (type, nullability, max length)\n- Threshold-driven gating in CI/CD and rollback policies\n- Canary prod lineage validation and alerting\n\n## Code Example\n```jinja\n{% macro drift_check(source_schema, source_table, model_schema, model_table, thresholds) -%}\n-- pseudo-logic: compare metadata and basic stats, emit drift_count if > thresholds\n{%- endmacro %}\n```\n\n## Follow-up Questions\n- How would you scale per-table thresholds across hundreds of tables?\n- How would you distinguish benign drift (e.g., new optional column) from breaking drift?","diagram":null,"difficulty":"advanced","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Google","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T07:23:31.079Z","createdAt":"2026-01-13T07:23:31.079Z"},{"id":"q-1361","question":"Design a beginner-friendly incremental dbt model in Snowflake for a SaaS analytics pipeline: raw events live at staging.event_logs with user_id, event_type (signup, login, purchase), country, occurred_at. Build analytics.daily_metrics that counts distinct active users by day and event_type, enriched by dimensions.countries. Explain incremental logic, late data handling (7 days), schema drift guards, and validation with tests and a snapshot. Also outline an Exposure for the dashboard with lineage?","answer":"Build analytics.daily_metrics as an incremental Snowflake model that counts distinct user_id by day and event_type from staging.event_logs, enriched via a join to dimensions.countries on country. Upse","explanation":"## Why This Is Asked\nThis question probes practical dbt incremental modeling, enrichment, and the linkage to dashboards via exposures.\n\n## Key Concepts\n- Incremental models with upserts in Snowflake\n- Late-arriving data handling with a 7-day window\n- Dimensional enrichment via dimensions.countries\n- Schema-drift guards and tests\n- Snapshots for user cohorts\n- Exposures and lineage visualization\n\n## Code Example\n```sql\nwith src as (\n  select distinct user_id, date(occurred_at) as day, event_type, country\n  from {{ ref('staging.event_logs') }}\n  where occurred_at >= current_timestamp() - interval '7 day'\n)\nselect day, event_type, country, count(distinct user_id) as active_users\nfrom src\ngroup by 1,2,3\n```\n\n## Follow-up Questions\n- How would you test referential integrity between analytics.daily_metrics and dimensions.countries?\n- How would you expose this model in a dashboard and ensure lineage is accurate?","diagram":null,"difficulty":"beginner","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Snowflake","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T13:15:09.731Z","createdAt":"2026-01-13T13:15:09.731Z"},{"id":"q-1523","question":"Design an incremental dbt model analytics.daily_engagement for a global multi-tenant app. Raw events sit in staging.user_events with tenant_id, user_id, email, event_type, occurred_at, privacy_flag. Build per-tenant daily distinct-user counts by event_type, redacting emails when privacy_flag is true. Describe incremental strategy, late data window, schema-drift guards, tests and snapshots, and how to expose lineage for dashboards?","answer":"Use a (tenant_id, date( occurred_at ), event_type) keyed incremental model. Compute distinct user_id from staging.user_events, redact email via a macro: if privacy_flag then 'REDACTED' else email. Lat","explanation":"## Why This Is Asked\nTests ability to design privacy-aware, tenant-scoped analytics with late-arriving data. Emphasizes incremental logic, governance, and lineage.\n\n## Key Concepts\n- Incremental models with composite keys (tenant_id, date, event_type)\n- Field-level redaction controlled by privacy_flag via a macro\n- Late-arriving data handled through a short lookback MERGE\n- Schema drift guards using sources, tests, and snapshots\n- Exposures to reflect lineage in dashboards\n\n## Code Example\n```sql\n-- macros/redact_email.sql\n{% macro redact_email(email, privacy_flag) -%}\n  {% if privacy_flag -%}\n    {{ return('REDACTED') }}\n  {% else -%}\n    {{ return(email) }}\n  {% endif -%}\n{% endmacro %}\n```\n\n## Follow-up Questions\n- How would you test the redact_email macro across combinations of privacy_flag and null emails?\n- How would you scale this approach to thousands of tenants with varying data freshness SLAs?","diagram":null,"difficulty":"intermediate","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","IBM","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T20:40:54.485Z","createdAt":"2026-01-13T20:40:54.486Z"},{"id":"q-1560","question":"In a dbt project, you ingest raw events into `staging.web_events` with columns: `event_id`, `user_id`, `event_type`, `country`, `occurred_at` (UTC). Build a beginner-friendly incremental model **analytics.daily_active_users** that reports the count of distinct `user_id`s per day, by `country` and `event_type`. Data can arrive up to 2 days late. Describe your incremental logic, how you handle late data with a rolling window, how to guard against schema drift, and how you validate with tests. Also draft an Exposure for a dashboard showing daily active users by country and event_type, including lineage?","answer":"Use an incremental model that computes daily active users by date(occurred_at) in UTC, grouped by country and event_type, then upserts into analytics.daily_active_users. For late data, retain a 2-day rolling window and reprocess the last 2 days on each run. Guard against schema drift with explicit column definitions and dbt's source freshness checks. Validate with unique user tests, null checks, and row count expectations.","explanation":"## Why This Is Asked\nAssesses practical dbt incremental modeling with late data handling and exposure design.\n\n## Key Concepts\n- Incremental model by day\n- Late-arrival window (2 days)\n- Schema drift guards\n- Tests and a dashboard exposure with lineage\n\n## Code Example\n```sql\nSELECT\n  date_trunc('day', occurred_at) AS day,\n  country,\n  event_type,\n  COUNT(DISTINCT user_id) AS active_users\nFROM {{ ref('staging.web_events') }}\nGROUP BY 1,2,3\n```\n\n## Follow-up Questions\n- How would you handle time zones for users across regions?\n- How would you adapt for evolving event_type vocabularies?","diagram":null,"difficulty":"beginner","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","DoorDash","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T06:19:05.797Z","createdAt":"2026-01-13T21:45:41.380Z"},{"id":"q-1590","question":"Design a two-model incremental dbt flow in Snowflake: 1) analytics.first_session derives first_session_at per user and seeds analytics.users(user_id, first_session_at, country_code, cohort). 2) analytics.daily_metrics counts daily active users by day and event_type, enriched with dimensions.countries. Include late-data handling (3 days), schema-drift guards, tests (not_null, unique), a snapshot, and expose lineage to dashboards?","answer":"Approach: Implement a two-model incremental dbt flow in Snowflake. First, analytics.first_session computes min(occurred_at) per user as first_session_at and seeds analytics.users with user_id, first_session_at, country_code, and cohort. Second, analytics.daily_metrics runs incrementally with a 3-day late-data watermark, counting daily active users by day and event_type while joining dimensions.countries for enrichment. Include schema-drift guards, not_null/unique tests, and a snapshot for slowly changing fields.","explanation":"## Why This Is Asked\nTests ability to design robust incremental pipelines, handle late data, and maintain lineage.\n\n## Key Concepts\n- Incremental models and ref() usage\n- First-session derivation and cohort calculation\n- Late-data watermark and idempotent upserts\n- Schema drift guards and tests; snapshots for slowly changing fields\n- Dashboards exposure and lineage\n\n## Code Example\n```sql\n-- first_session model (simplified)\nwith e as (\n  select user_id, occurred_at, country_code\n  from {{ source('raw','events_log') }}\n)\nselect user_id, min(occurred_at) as first_session_at, country_code,\n      \"","diagram":"flowchart TD\n  A[raw.events_log] --> B[analytics.first_session]\n  B --> C[analytics.users]\n  C --> D[analytics.daily_metrics]\n  E[dimensions.countries] --> C","difficulty":"intermediate","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T05:35:22.438Z","createdAt":"2026-01-13T22:55:10.734Z"},{"id":"q-1604","question":"In a dbt pipeline powering a fintech analytics platform, raw events arrive at staging.fin_events with customer_id, txn_id, amount, currency, status, event_ts. Design an incremental analytics.transactions model that deduplicates by txn_id across sources, handles late events within a 2-day window, validates currency via a macro-based set per source, guards against schema drift with a dynamic mapping table, and snapshots customers on their first txn for dashboard lineage. Include tests and performance considerations?","answer":"I would implement an incremental analytics.transactions model with unique_key txn_id, deduplicating across sources, handling late events within a 2-day watermark window, validating currency via a custom macro per source, guarding against schema drift with a dynamic mapping table, and snapshotting customers on their first transaction for dashboard lineage.","explanation":"## Why This Is Asked\n\nTests advanced dbt patterns: deduplication, late-arrival handling, schema drift detection, and data lineage.\n\n## Key Concepts\n\n- Incremental merge with per-source deduplication on txn_id\n- Watermark-based late-arrival handling (2 days)\n- Macro-based currency validation per source\n- Dynamic schema drift guards via mapping tables\n- Snapshot on customers for first transaction lineage\n- Tests: not_null, unique, relationships\n- Performance: clustering, pruning, and date-based partitioning\n\n## Code Example\n\n```sql\n-- dbt incremental model sketch\n{{ config(materialized='incremental', unique_key='txn_id') }}\n\nWITH deduped_events AS (\n  SELECT *,\n         ROW_NUMBER() OVER (\n           PARTITION BY txn_id, source \n           ORDER BY event_ts DESC\n         ) AS rn\n  FROM {{ ref('staging_fin_events') }}\n  WHERE event_ts >= DATEADD('day', -2, CURRENT_TIMESTAMP)\n),\n\nvalidated_events AS (\n  SELECT *\n  FROM deduped_events\n  WHERE rn = 1\n    AND {{ validate_currency(source, currency) }}\n),\n\nfinal_transactions AS (\n  SELECT *\n  FROM validated_events\n  WHERE {{ schema_drift_guard() }}\n)\n\nSELECT * FROM final_transactions\n```\n\n## Implementation Notes\n\n- Use `dbt snapshot` for customer first-transaction tracking\n- Implement currency validation macro with source-specific rules\n- Create mapping table for dynamic schema drift detection\n- Add comprehensive tests for data quality and performance","diagram":null,"difficulty":"advanced","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T05:31:38.231Z","createdAt":"2026-01-14T02:32:20.530Z"},{"id":"q-1714","question":"In a beginner dbt project for a media site, staging.events_logs(user_id, event_type, occurred_at, region_code) feeds analytics.daily_engagement that counts distinct users per date and event_type, enriched by dimensions.regions on region_code. Build the incremental model with late data window (3 days), include schema-drift guards, and tests (not_null on user_id, occurred_at; unique on (user_id, occurred_at, event_type)). Add a data-contract macro/test ensuring staging.events_logs contains required fields and types, and a snapshot on users to capture region changes. Explain approach, tests, and macro design?","answer":"I’d implement analytics.daily_engagement as an incremental model that aggregates distinct users per date and event_type, joining dimensions.regions on region_code. Late data is allowed in a 3-day wind","explanation":"## Why This Is Asked\nThis question probes practical dbt workflow: incremental logic, late-arrival handling, schema drift guardrails, data-contract testing, and snapshot usage. It covers core dbt skills in a realistic, beginner-friendly way.\n\n## Key Concepts\n- Incremental models and late data windows\n- Schema drift guards and tests\n- Data-contracts via macros/tests\n- Snapshots for slowly changing attributes\n\n## Code Example\n```javascript\n// Pseudo-contract test skeleton (dbt-style macro would be SQL/Jinja in practice)\nfunction assertContract(stagingColumns, required) {\n  const missing = required.filter(r => !stagingColumns.includes(r));\n  if (missing.length) throw new Error(\"Missing: \" + missing.join(\", \"));\n}\n```\n\n## Follow-up Questions\n- How would you extend the contract to handle optional fields?\n- How would you validate contracts across multiple environments (dev/stage/prod)?","diagram":null,"difficulty":"beginner","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Cloudflare","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T07:49:20.068Z","createdAt":"2026-01-14T07:49:20.068Z"},{"id":"q-1741","question":"You're building a multi-tenant dbt analytics pipeline on Snowflake. Raw events live in staging.events with tenant_id, user_id, event_type, occurred_at. You plan per-tenant analytics schemas (analytics.{tenant}). Design an incremental analytics.daily_metrics per tenant that counts distinct active users by day and event_type, with late data up to 2 days, joining dimensions.tenants and dimensions.countries, including schema-drift guards, tests (not_null, unique), and a snapshot for user_first_seen per tenant. Explain approach for cross-tenant lineage and tenant-scoped materializations?","answer":"Per-tenant schemas in Snowflake: implement analytics.daily_metrics as an incremental model scoped to analytics.${tenant}. daily by (day, tenant_id, event_type). Late data window: 2 days; use MERGE int","explanation":"## Why This Is Asked\nTests the ability to architect multi-tenant dbt pipelines with strict isolation (per-tenant schemas), robust late-data handling, and governance via tests and snapshots. It also probes strategies for cross-tenant lineage in dashboards. \n\n## Key Concepts\n- Multi-tenant isolation with per-tenant schemas\n- Incremental modeling with late-arrival handling\n- Schema drift guards and robust tests\n- Snapshots for slowly changing dimensions per tenant\n- Clear lineage exposure for dashboards across tenants\n\n## Code Example\n````sql\n-- analytics.{{tenant}}.daily_metrics.sql\nwith src as (\n  select tenant_id, user_id, event_type, date(occurred_at) as day\n  from {{ source('raw','events') }}\n)\nselect day, tenant_id, event_type, count(distinct user_id) as active_users\nfrom src\ngroup by day, tenant_id, event_type\n````\n\n## Follow-up Questions\n- How would you automate tenancy onboarding to new schemas without gluing code?\n- How do you test for cross-tenant join correctness without data leakage?","diagram":null,"difficulty":"advanced","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T09:01:41.446Z","createdAt":"2026-01-14T09:01:41.447Z"},{"id":"q-1757","question":"Design a dbt analytics layer for three domains: core_events (user actions), memberships (plans), and referrals. Data arrives late (up to 2 days). Propose an architecture that ensures: 1) incremental models with controlled late data backfills, 2) deterministic surrogate keys for cross-domain joins, 3) schema-drift guards via custom tests/macros, 4) automated docs with complete lineage, 5) dashboard-friendly exposure with stable lineage during backfills, and 6) performance considerations for Snowflake or BigQuery (partitioning, clustering). Include a concrete incremental SQL snippet?","answer":"Propose staging per domain, a single grain, incremental models with a 2-day late-arrival window, a surrogate key like domain_key + occurred_at + id for cross-domain joins, a macro-based cross-domain r","explanation":"## Why This Is Asked\nTests end-to-end dbt mastery across multi-domain conformance, late data handling, and lineage stability during backfills. It also probes custom macros for data quality and cross-domain integrity, plus performance considerations for two leading warehouses.\n\n## Key Concepts\n- Incremental models with late-arrival windows\n- Cross-domain surrogate keys and referential integrity\n- Schema drift guards via tests and macros\n- Automated docs with complete lineage\n- Backfill strategy ensuring stable dashboards\n- Performance tuning for Snowflake vs BigQuery (partitioning, clustering)\n\n## Code Example\n```sql\n-- models/analytics/fact_core_events_incremental.sql\n{{ config(materialized='incremental') }}\n\nwith src as (\n  select\n    user_id as domain_user_id,\n    event_type,\n    occurred_at,\n    domain\n  from {{ source('core_events', 'events') }}\n  where occurred_at >= dateadd(day, -2, current_date())\n)\n\nselect\n  domain_user_id,\n  domain,\n  max(occurred_at) as occurred_at,\n  count(*) as event_count\nfrom src\ngroup by domain_user_id, domain\n{% if is_incremental() %}\nwhere occurred_at > (select max(occurred_at) from {{ this }})\n{% endif %}\n```\n\n## Follow-up Questions\n- How would you validate cross-domain joins under backfills?\n- What tests would you add to guard against schema drift across domains?","diagram":null,"difficulty":"advanced","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","MongoDB","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T09:44:23.172Z","createdAt":"2026-01-14T09:44:23.172Z"},{"id":"q-1808","question":"Design an intermediate dbt workflow for a fintech analytics pipeline on Snowflake. Raw events are in staging.transactions (transaction_id, user_id, amount, currency, occurred_at, status) and staging.users (user_id, country_code, account_status). Build an incremental analytics.daily_finance that sums total_amount and transaction_count by day, currency, country_code, and status, with late data support up to 2 days. Add analytics.users_snapshot as a Type 2 surrogate for changes in country_code/account_status. Expose lineage and discuss a data-contract macro to validate source schemas and auto-generate tests?","answer":"Use an incremental model analytics.daily_finance that upserts sum(amount) and transaction_count by day, currency, country_code, and status; allow late data within a 2-day window via a MERGE-based upse","explanation":"## Why This Is Asked\nTests resilience to late data, SCD Type 2, and data contracts. It also probes lineage and macro-level test generation.\n\n## Key Concepts\n- Incremental upserts in dbt\n- Type 2 SCD for users\n- Late data handling with a 2-day window\n- Data-contract macro to auto-create tests\n- dbt docs lineage\n\n## Code Example\n```sql\n-- Example dbt model skeleton\nwith src as (\n  select * from {{ ref('staging_transactions') }}\n), u as (\n  select * from {{ ref('staging_users') }}\n)\nselect\n  date_trunc('day', occurred_at) as day,\n  currency,\n  country_code,\n  status,\n  sum(amount) as total_amount,\n  count(*) as transaction_count\nfrom src s\njoin u on s.user_id = u.user_id\ngroup by 1,2,3,4\n```\n\n## Follow-up Questions\n- How would you test for nulls and duplicates?\n- How would you adapt this for multi-tenant isolation?\n- How to surface lineage in dashboards and docs?","diagram":"flowchart TD\n  A[staging.transactions] --> B[analytics.daily_finance]\n  C[staging.users] --> D[analytics.users_snapshot]\n  B --> E[dashboard.financials]\n  B --> F[docs/lineage]","difficulty":"intermediate","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","LinkedIn"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T11:42:53.754Z","createdAt":"2026-01-14T11:42:53.754Z"},{"id":"q-1823","question":"Scenario: In a beginner dbt project for a gig-economy platform, raw events arrive in staging.event_logs with user_id, session_id, event_type (visit, click, conversion), occurred_at, and region_code. Build an incremental model analytics.daily_events that counts events by day and event_type, enriched by dims.regions on region_code. Use a 2-day late data window, guard against schema drift with not_null and unique tests, and create a data-contract macro to verify staging.event_logs contains the required columns and types before run. What approach would you take?","answer":"Implement an incremental analytics.daily_events that aggregates counts by day, event_type, and region_code from staging.event_logs joined to dims.regions on region_code. Use a 2-day late window, enfor","explanation":"Why This Is Asked\n- Tests incremental modeling with late data handling and schema-drift guards.\n- Evaluates macro usage for pre-flight data contracts and test coverage.\n\nKey Concepts\n- Incremental models in dbt\n- Late data window techniques\n- Schema-drift guards (not_null, unique)\n- Data-contract macros and pre-flight validation\n- Joining with lightweight dimensions for enrichment\n\nCode Example\n```sql\n-- analytics/daily_events.sql\nwith src as (\n  select user_id, session_id, event_type, occurred_at, region_code\n  from {{ source('staging', 'event_logs') }}\n), base as (\n  select\n    date_trunc('day', occurred_at) as day,\n    event_type,\n    region_code,\n    count(*) as event_count\n  from src\n  group by 1,2,3\n)\nselect * from base\n{% if is_incremental() %}\nwhere occurred_at >= (select max(occurred_at) from {{ this }})\n{% endif %}\n```\n\n```sql\n-- macros/data_contract.sql\n{% macro data_contract(table, required_cols) -%}\n  -- Pseudo-implementation: validates presence and types of required_cols on table\n  {# In practice, iterate required_cols and raise if missing #}\n{%- endmacro %}\n```\n\nFollow-up Questions\n- How would you extend this for multiple regions with partition pruning?\n- How would you test macro robustness across schema changes?","diagram":null,"difficulty":"beginner","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T13:05:52.833Z","createdAt":"2026-01-14T13:05:52.833Z"},{"id":"q-1909","question":"Design a per-tenant incremental dbt flow in Snowflake that computes analytics.{tenant}.daily_event_summary from staging.events (tenant_id, user_id, event_type, occurred_at, country_code, record_hash). Include enrichment from dimensions.countries, and produce a per-tenant daily count of distinct users by event_type. Implement late data handling (2 days), schema-drift guards, tests (not_null, unique), and a snapshot for analytics.{tenant}.users to capture cohort changes. Explain how you ensure cross-tenant lineage and isolation?","answer":"Plan: implement an incremental model analytics.{tenant}.daily_event_summary with a composite key (tenant_id, day, event_type, user_id). Use MERGE-like upserts via dbt incremental to accommodate late d","explanation":"Why This Is Asked\n- Tests practical dbt incremental patterns with multi-tenant isolation.\n- Covers late data handling and schema drift guards.\n\nKey Concepts\n- Incremental model with composite key; DBT incremental behavior in Snowflake.\n- Tenant isolation through per-tenant schemas; cross-tenant lineage in docs.\n- Snapshots for slowly changing cohort labels; tests for data quality.\n\nCode Example\n```sql\n-- dbt model: analytics/{tenant}/daily_event_summary.sql\nwith src as (\n  select tenant_id, user_id, event_type, date(occurred_at) as day, country_code\n  from {{ ref('staging_events') }}\n  where occurred_at >= dateadd(day, -2, current_date())\n), enriched as (\n  select s.tenant_id, s.user_id, s.event_type, s.day, c.country_name, c.continent\n  from src s\n  left join {{ ref('dimensions__countries') }} c on c.country_code = s.country_code\n)\nselect tenant_id, day, event_type, count(distinct user_id) as active_users\nfrom enriched\ngroup by 1,2,3\n```\n\nFollow-up Questions\n- How would you adapt this to handle event_type changes post-ingest?\n- How do you validate per-tenant isolation in dbt docs?","diagram":null,"difficulty":"intermediate","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","Netflix","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T16:53:50.128Z","createdAt":"2026-01-14T16:53:50.129Z"},{"id":"q-1988","question":"Design an incremental, per-tenant analytics.daily_metrics model in Snowflake with dbt. Staging.events has tenant_id, user_id, event_type, occurred_at, platform, revenue. Build analytics.{tenant}.daily_metrics counting distinct users per day by event_type, with late data tolerance of 2 days. Add a per-tenant feature flag in dimensions.tenants (revenue_enabled) and a macro to include revenue only when true. Enforce isolation via per-tenant schemas, add schema-drift guards, tests (not_null, unique), and a snapshot for analytics.{tenant}.users. Explain cross-tenant lineage and dashboard exposure?","answer":"Outline a macro-driven approach: a per-tenant flag revenue_enabled in dimensions.tenants, read by a macro revenue_included(tenant) to conditionally include revenue in analytics.{tenant}.daily_metrics.","explanation":"## Why This Is Asked\nTests ability to design complex per-tenant dbt pipelines with dynamic schemas and feature flags.\n\n## Key Concepts\n- per-tenant schema isolation\n- dbt macros for feature flags\n- incremental late-data handling\n- snapshots for cohort changes\n- tests and lineage\n\n## Code Example\n```sql\n-- analytics/{tenant}/daily_metrics.sql (conceptual)\nselect\n  '{{ tenant }}' as tenant_id,\n  date(occurred_at) as day,\n  event_type,\n  count(distinct user_id) as active_users,\n  sum(case when revenue_enabled then revenue else 0 end) as revenue\nfrom {{ ref('staging__events') }} e\njoin {{ ref('dimensions__tenants') }} t on t.tenant_id = e.tenant_id\nwhere occurred_at >= dateadd(day,-2,current_date())\ngroup by 1,2,3\n```\n\n## Follow-up Questions\n- How would you observe and test cross-tenant lineage changes when tenants are added/dropped?\n- How would you scale the macro to support tenants with different time zones?","diagram":null,"difficulty":"intermediate","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T19:43:21.230Z","createdAt":"2026-01-14T19:43:21.230Z"},{"id":"q-2013","question":"Design a per-tenant incremental analytics model in Snowflake that materializes analytics.{tenant}.daily_event_engagement from staging.events (tenant_id, user_id, event_type, occurred_at). Enrich with dimensions.regions on country_code. Produce daily counts by event_type and region_name. Add a 3-day late data window, schema-drift guards, tests (not_null, unique), and a snapshot analytics.{tenant}.customers for cohort changes. Explain tenant isolation and cross-tenant lineage via macro-generated per-tenant schemas?","answer":"Implement an incremental per-tenant model that writes to analytics.{tenant}.daily_event_engagement using staging.events, joined to dimensions.regions by country_code to group by region_name. Aggregate","explanation":"Why This Is Asked\n- Tests ability to design per-tenant dbt models with dynamic schemas and macro-driven generation.\n- Evaluates cross-tenant isolation and lineage strategies.\n- Assesses handling of late data and schema drift.\n\nKey Concepts\n- Incremental per-tenant modeling in Snowflake\n- Macros for tenant-scoped schema generation\n- Cross-tenant lineage and isolation controls\n- Data quality tests and snapshots for cohorts\n\nCode Example\n```jinja\n{% macro tenant_daily_engagement(tenant_id) %}\nselect\n  '{{ tenant_id }}' as tenant_id,\n  cast(date(occurred_at) as date) as day,\n  event_type,\n  region_name,\n  count(distinct user_id) as active_users\nfrom {{ source('staging','events') }}\nwhere tenant_id = '{{ tenant_id }}'\ngroup by 1,2,3,4\n{% endmacro %}\n```\n\nFollow-up Questions\n- How would you handle a tenant with missing region mapping?\n- How would you monitor and adapt the late-data window across tenants?","diagram":"flowchart TD\n A(Source: staging.events) --> B(Join: regions)\n B --> C(Martialization: analytics.{tenant}.daily_event_engagement)\n C --> D(Snapshot: analytics.{tenant}.customers)\n D --> E(Dashboards/Lineage)","difficulty":"intermediate","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","LinkedIn"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T20:50:59.166Z","createdAt":"2026-01-14T20:50:59.166Z"},{"id":"q-2097","question":"Design a per-tenant weekly retention pipeline in Snowflake using dbt where raw events live in staging.events with tenant_id, user_id, first_seen_at, occurred_at, and an events.users table. Create analytics.tenant_retention (tenant_id, cohort_week, retention_users, total_users) that computes weekly retention by cohort (first_seen_week) with incremental loading, and late data handling up to 4 days. Also implement analytics.global_retention that aggregates per-tenant retention across tenants with tenant_dim country/plan, ensuring cross-tenant lineage and isolation. Include schema-drift guards, tests (not_null, unique), and a snapshot of analytics.tenants to track cohort definitions. Explain how you ensure isolation and lineage?","answer":"Implement a per-tenant weekly retention pipeline in analytics.tenant_retention that cohorts users by their first_seen_at week and computes weekly active users from staging.events per tenant_id. Use MERGE statements for idempotent incremental loading with a 4-day lookback window for late data handling, and implement schema drift guards. Create analytics.global_retention to aggregate retention metrics across tenants with tenant_dim country/plan attributes. Include a snapshot of analytics.tenants for cohort definition tracking and comprehensive tests (not_null, unique).","explanation":"## Why This Is Asked\nTests ability to design cross-tenant, incremental transformations with late data handling and quality checks in a dbt/Snowflake context.\n\n## Key Concepts\n- Multi-tenant isolation and lineage\n- Incremental logic with late data window\n- Schema drift guards and tests\n- Snapshots and cross-tenant aggregates\n\n## Code Example\n```sql\n-- Pseudocode for MERGE-based incremental\nMERGE INTO analytics.tenant_retention t\nUSING (\n  SELECT tenant_id, cohort_week, \n         COUNT(DISTINCT user_id) AS retention_users, \n         COUNT(DISTINCT user_id) AS total_users \n  FROM staging.events \n```","diagram":null,"difficulty":"intermediate","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T05:02:01.147Z","createdAt":"2026-01-15T02:12:59.856Z"},{"id":"q-2133","question":"Scenario: A new multi-tenant event feed named staging.stream_events with columns tenant_id, user_id, event_type, occurred_at, country_code, payload (JSON). Task: implement a dbt incremental model analytics.daily_user_events that, for each day, tenant_id, and event_type, returns the count of distinct users. Enrich with dimensions.countries on country_code. Create a macro to parse payload JSON extracting device and app_version; fallback defaults if missing. Implement late data tolerance of 2 days (i.e., if occurred_at within last 2 days, process in daily batch). Add tests: not_null on tenant_id, user_id, occurred_at; unique on (tenant_id, user_id, occurred_at, event_type). Add a snapshot for analytics.users to capture cohort-country changes. Provide strategy for cross-tenant lineage and isolation in a single schema (no per-tenant schemas)?","answer":"Design a dbt incremental analytics.daily_user_events: group by tenant_id, date(occurred_at), event_type; count distinct user_id. Enrich using left join dimensions.countries on country_code; use a smal","explanation":"## Why This Is Asked\nTests a beginner in building incremental per-tenant analytics with data enrichment, JSON parsing, and late-arrival handling. It also probes macro design for robust JSON extraction and basic data contracts.\n\n## Key Concepts\n- dbt incremental models and late-arrival windows\n- JSON payload parsing via macros\n- Data enrichment with dimension tables\n- Basic data quality tests and a snapshot for evolving users\n- Cross-tenant lineage in a single analytics schema\n\n## Code Example\n```sql\n-- macro: parse_payload.sql\n{% macro parse_payload(payload) %}\n  CASE WHEN JSON_EXTRACT_PATH_TEXT({{ payload }}, 'device') IS NOT NULL\n       THEN JSON_EXTRACT_PATH_TEXT({{ payload }}, 'device')\n       ELSE 'unknown'\n  END AS device,\n  COALESCE(JSON_EXTRACT_PATH_TEXT({{ payload }}, 'app_version'), 'unknown') AS app_version\n{% endmacro %}\n```\n\n```sql\n-- analytics.daily_user_events model (simplified)\nselect\n  tenant_id,\n  date_trunc('day', occurred_at) as day,\n  event_type,\n  count(distinct user_id) as user_count\nfrom {{ ref('staging_stream_events') }} as s\nleft join {{ ref('dimensions_countries') }} as c\n  on s.country_code = c.country_code\ngroup by 1, 2, 3\n```\n\n## Follow-up Questions\n- How would you test for schema drift between staging and analytics models?\n- How would you extend this to thousands of tenants while preserving performance?","diagram":"flowchart TD\n  S[staging.stream_events] --> A[analytics.daily_user_events]\n  A --> D[dashboard by tenant/event_type]\n  S --> M[parse payload via macro]\n  A --> U[analytics.users snapshot]","difficulty":"beginner","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","NVIDIA","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T04:16:24.757Z","createdAt":"2026-01-15T04:16:24.757Z"},{"id":"q-2173","question":"Design a per-tenant incremental dbt model for a SaaS analytics pipeline on Snowflake. Source staging.user_events (tenant_id, user_id, event_type, event_timestamp, revenue, lifecycle_stage). Build analytics.{tenant}.daily_cohort to compute daily cohorts by lifecycle_stage and event_type with a 2-day late-data window, upserting via MERGE. Add schema-drift guards, tests (not_null, unique), and a per-tenant last_seen_users snapshot. How do you enforce cross-tenant isolation and lineage?","answer":"Build analytics.{tenant}.daily_cohort as an incremental model sourced from staging.user_events. Compute daily cohorts by lifecycle_stage and event_type with a 2-day late-data window, upserting via MER","explanation":"## Why This Is Asked\nThis probes per-tenant incremental design, late data handling, data quality, and lineage isolation in a realistic SaaS context.\n\n## Key Concepts\n- Incremental modeling with per-tenant schemas\n- Macros for tenant isolation and dynamic schema resolution\n- Late data window and MERGE upserts\n- Schema drift guards and tests (not_null, unique)\n- Snapshots for cohort dynamics and last_seen_users\n- Clear lineage via sources and refs\n\n## Code Example\n```sql\n-- skeleton showing late data handling and tenant scoping\nwith events as (\n  select * from {{ source('staging','user_events') }}\n  where event_timestamp < current_timestamp - interval '2 days'\n)\nselect\n  tenant_id,\n  date(event_timestamp) as cohort_date,\n  event_type,\n  lifecycle_stage,\n  count(distinct user_id) as user_cnt\nfrom events\ngroup by 1,2,3,4\n```\n\n## Follow-up Questions\n- How would you test tenant isolation in CI?\n- How would you adapt this for new event_type dimensions without breaking existing tenants?","diagram":null,"difficulty":"intermediate","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T05:48:29.145Z","createdAt":"2026-01-15T05:48:29.145Z"},{"id":"q-2315","question":"Design a beginner-friendly per-tenant daily_session_summary in Snowflake/dbt. Source: staging.sessions(tenant_id, user_id, session_start). Build analytics.{tenant}.daily_session_summary counting distinct users per tenant per day, with enrichment from dimensions.tenants (plan, region). Implement late data tolerance of 1 day, schema-drift guards, tests (not_null, unique). Include a data-contract macro to validate staging.sessions fields/types and a snapshot on analytics.{tenant}.users. Explain how you ensure cross-tenant isolation and lineage in dashboards?","answer":"Implement a per-tenant daily session summary in dbt: an incremental model analytics.{tenant}.daily_session_summary using a tenant-scoped macro to reference staging.sessions. Define unique key as (tena","explanation":"Why This Is Asked\nAssesses ability to design an end-to-end per-tenant metric with incremental logic, data contracts, and snapshots, plus isolation and lineage considerations.\n\nKey Concepts\n- Per-tenant isolation and lineage\n- Incremental models and late data tolerance\n- Data-contract macros and schema-drift tests\n- Snapshots to capture cohort changes\n\nCode Example\n```sql\nwith s as (\n  select tenant_id, date_trunc('day', session_start) as session_date, user_id\n  from {{ ref('staging_sessions') }}\n  where session_start is not null\n)\nselect tenant_id, session_date, count(distinct user_id) as active_users\nfrom s\ngroup by 1,2\n```\n\nFollow-up Questions\n- How would you validate that late sessions arrive within the 1-day window?\n- How would you handle tenants added after initial deployment (new analytics.{tenant} schemas)?","diagram":null,"difficulty":"beginner","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Microsoft","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T11:42:15.446Z","createdAt":"2026-01-15T11:42:15.446Z"},{"id":"q-2403","question":"Beginner dbt task: from staging.events (event_id, user_id, event_type, occurred_at, region_code, delete_flag) and dimensions.regions, implement an incremental model analytics.daily_event_summary that counts distinct users per day, event_type, country_code (via region_code). Exclude deleted events by default; provide a macro to toggle inclusion for audits. Add tests not_null(event_id, occurred_at) and unique(event_id)?","answer":"Implement analytics.daily_event_summary as an incremental Snowflake model: aggregate date_trunc('day', occurred_at), event_type, country_code (via regions) with count(distinct user_id). Use a macro de","explanation":"## Why This Is Asked\nTests a candidate's ability to implement a simple, robust incremental model with data quality checks and a governance hook (macro) for audits. Emphasizes joins, aggregation, and proper filtering.\n\n## Key Concepts\n- Incremental modeling in dbt\n- Basic joins to enrich with dimensions.regions\n- Simple data governance via macros for optional audit paths\n- Tests: not_null and unique\n\n## Code Example\n```sql\n-- analytics/daily_event_summary.sql\nwith src as (\n  select\n    date_trunc('day', occurred_at) as day,\n    e.event_type,\n    r.country_code,\n    count(distinct e.user_id) as active_users\n  from {{ source('staging','events') }} e\n  join {{ ref('dimensions_regions') }} r on e.region_code = r.region_code\n  where {{ delete_filter(false) }}\n  group by 1,2,3\n)\nselect * from src;\n```\n\n```sql\n-- macros/delete_filter.sql\n{% macro delete_filter(include_deleted=false) -%}\n  {% if include_deleted %}\n    1=1\n  {% else %}\n    delete_flag = false\n  {% endif %}\n{%- endmacro %}\n```\n\n## Follow-up Questions\n- How would you test the macro across multiple models?\n- How would you adapt this to handle late-arriving updates in staging?\n","diagram":"flowchart TD\n  A[staging.events] --> B[analytics.daily_event_summary]\n  B --> C[dashboards/consumption]\n  A --> D[dimensions.regions]\n  D --> B","difficulty":"beginner","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T16:53:24.215Z","createdAt":"2026-01-15T16:53:24.215Z"},{"id":"q-2442","question":"In a dbt project, given staging.events(user_id, event_type, occurred_at, country_code) and staging.users(user_id, created_at, country_code), implement an incremental model analytics.daily_engagement that counts distinct users per day by event_type and country_code, enriched by dimensions.countries. Include 1-day late data tolerance, schema-drift guards, tests (not_null, unique), and a snapshot on analytics.users to track country changes. Describe lineage and isolation notes?","answer":"Create an incremental analytics.daily_engagement that counts distinct user_id per day, grouped by event_type and country_code, joining staging.events with staging.users and dimensions.countries. Allow","explanation":"## Why This Is Asked\n\nAssess ability to design a beginner-friendly incremental model, enforce data quality, and capture slowly changing dimensions with snapshots.\n\n## Key Concepts\n\n- Incremental modeling in dbt for daily aggregates\n- Data quality tests: not_null and unique\n- Late-arriving data handling (1-day tolerance)\n- Snapshot usage for slowly changing dimensions\n- Referential integrity with dimensions.countries and cross-model lineage\n\n## Code Example\n\n```sql\nwith src as (\n  select\n    e.user_id,\n    date( e.occurred_at ) as day,\n    e.event_type,\n    e.country_code\n  from {{ ref('staging_events') }} e\n)\nselect\n  day,\n  event_type,\n  country_code,\n  count(distinct user_id) as user_cnt\nfrom src\ngroup by 1,2,3\n```\n\n## Follow-up Questions\n\n- How would you test that country_code exists in dimensions.countries?\n- How would you incorporate a 1-day late data window into your model without double-counting users?","diagram":"flowchart TD\n  S1[staging.events] --> EN[analytics.daily_engagement]\n  U1[staging.users] --> EN\n  EN --> C[dimensions.countries]\n  EN --> SN[analytics.users_snapshot]","difficulty":"beginner","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T17:59:36.342Z","createdAt":"2026-01-15T17:59:36.342Z"},{"id":"q-2502","question":"Design a contract-validated, multi-tenant revenue analytics flow in dbt on Snowflake. Raw events live in staging.{tenant}.events (tenant_id, user_id, event_type, amount, currency, occurred_at). Build a incremental analytics.{tenant}.daily_revenue that sums revenue per day/tenant/currency, with 2-day late data tolerance. Create contracts.tenants describing required fields and a macro that fails tests when missing fields. Add per-tenant analytics.{tenant}.users snapshot for status changes. Enforce per-tenant schemas, schema-drift guards, and tests (not_null, unique). Explain cross-tenant lineage and performance considerations?","answer":"Implement a per-tenant incremental MERGE from staging.{tenant}.events into analytics.{tenant}.daily_revenue (key: tenant_id, date, currency) with a 2-day late data window and record_hash for idempoten","explanation":"## Why This Is Asked\nAssesses ability to design contract-driven, multi-tenant dbt pipelines with late data handling and governance.\n\n## Key Concepts\n- Incremental models and MERGE in Snowflake\n- Data contracts and custom tests\n- Per-tenant schema isolation and lineage\n- Snapshots for slowly changing user state\n- Schema drift guards and test coverage\n\n## Code Example\n```sql\n-- example placeholder: actual implementation in repo\n```\n\n## Follow-up Questions\n- How would you automate contract drift detection across tenants?\n- How do you evolve contracts without breaking dashboards?\n","diagram":"flowchart TD\n  S[staging.{tenant}.events] --> A[incremental daily_revenue]\n  A --> R[analytics.{tenant}.daily_revenue]\n  C[contracts.tenants] --> M[contract-enforcer macro]\n  U[analytics.{tenant}.users Snapshot] --> L[lineage across tenants]","difficulty":"advanced","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Microsoft","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T20:44:34.530Z","createdAt":"2026-01-15T20:44:34.531Z"},{"id":"q-2685","question":"Design a beginner dbt task to build an incremental per-tenant analytics.daily_events model in Snowflake from staging.events(tenant_id, user_id, event_type, occurred_at). Include enrichment from dimensions.event_types, a 2-day late-data window, per-tenant isolation via schemas, and schema-drift guards. Add tests (not_null, unique) and a snapshot for analytics.{tenant}.users; also implement a simple data-contract macro to validate staging.events columns/types and a test using it?","answer":"Propose an incremental per-tenant daily_events model that slices by day, joins staging.events with dimensions.event_types for category enrichment, and uses per-tenant schemas to enforce isolation. Imp","explanation":"## Why This Is Asked\nTests coverage for multi-tenant isolation, data contracts, and incremental modeling in a beginner context. It also introduces a lightweight data-quality macro beyond basic tests.\n\n## Key Concepts\n- dbt incremental models in Snowflake\n- per-tenant schema isolation\n- data contracts via custom macros\n- tests: not_null, unique; snapshots for cohort tracking\n\n## Code Example\n```javascript\n-- Example macro to validate staging.events columns\n{% macro validate_staging_events(cols) %}\n  {# implementation details #}\n{% endmacro %}\n\n-- Example incremental model sketch\nwith staged as (\n  select tenant_id, user_id, event_type, occurred_at,\n         date_trunc('day', occurred_at) as day\n  from {{ source('staging','events') }}\n  where occurred_at > (current_date() - interval '2 days')\n), enriched as (\n  select s.*, e.category\n  from staged s\n  left join {{ ref('dimensions.event_types') }} e\n    on s.event_type = e.event_type\n)\nselect tenant_id, day, event_type, count(*) as event_count\nfrom enriched\ngroup by 1,2,3\n{% if is_incremental() %}\nwhere day > (select max(day) from {{ this }})\n{% endif %}\n```\n\n## Follow-up Questions\n- How would you handle late data beyond 2 days and retractions?\n- How would you test for schema drift across tenants without cross-tenant leakage?","diagram":"flowchart TD\n  A[staging.events] --> B[dimensions.event_types enrichment]\n  B --> C[analytics.{tenant}.daily_events]\n  C --> D[tests & schema drift guards]\n  D --> E[analytics.{tenant}.users snapshot]","difficulty":"beginner","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Instacart","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T06:57:40.373Z","createdAt":"2026-01-16T06:57:40.373Z"},{"id":"q-2765","question":"In a beginner dbt project for multi-tenant analytics, build an incremental model analytics.{tenant}.hourly_engagement over staging.events(tenant_id, user_id, event_type, occurred_at). Implement a 2-hour late data window, per-tenant schema isolation, and tests not_null (tenant_id, user_id, occurred_at, event_type) plus unique on (tenant_id, occurred_at, user_id, event_type). Include a small macro to provision analytics.{tenant} schemas and a snapshot analytics.{tenant}.users to capture cohort changes. Explain how to surface tenant lineage in dashboards?","answer":"Implement analytics.{tenant}.hourly_engagement as an incremental model over staging.events (tenant_id, user_id, event_type, occurred_at). Partition by hour and apply a 2-hour late window. Tests: not_n","explanation":"## Why This Is Asked\nAssesses ability to design a robust, beginner-friendly, multi-tenant dbt workflow: incremental hourly data, late-arrival tolerance, and data quality via tests; plus practical isolation via per-tenant schemas and a cohort-tracking snapshot.\n\n## Key Concepts\n- Incremental models and late-arrival handling\n- Per-tenant schema isolation\n- dbt tests: not_null and unique constraints\n- Snapshots for cohort tracking\n- Macros for tenant provisioning\n- Tenant-level dashboard lineage\n\n## Code Example\n```sql\n-- analytics/{tenant}.hourly_engagement incremental model skeleton\nwith s as (\n  select tenant_id, user_id, event_type, occurred_at\n  from {{ source('staging','events') }}\n  where tenant_id = '{{ this.schema }}' -- conceptual; actual tenant param binding varies by project\n)\nselect tenant_id,\n       date_trunc('hour', occurred_at) as occurred_at_hour,\n       event_type,\n       count(distinct user_id) as user_count\nfrom s\nwhere occurred_at > (select max(occurred_at) - interval '2 hours' from analytics.{tenant}.hourly_engagement)\ngroup by 1,2,3\n```\n\n```sql\n-- macro example (conceptual)\n{% macro ensure_tenant_schema(tenant) %}\n  {% set schema = 'analytics.' ~ tenant %}\n  {% do run_query('CREATE SCHEMA IF NOT EXISTS ' ~ schema) %}\n{% endmacro %}\n```\n\n## Follow-up Questions\n- How would you adapt late-window logic for tenants with different data arrival patterns?\n- What governance or monitoring would you add to catch schema drift across tenants?\n- How would you automate tenant onboarding and remove a tenant’s data safely?","diagram":null,"difficulty":"beginner","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Scale Ai","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T10:54:43.153Z","createdAt":"2026-01-16T10:54:43.153Z"},{"id":"q-2797","question":"In a multi-tenant dbt analytics stack on Snowflake, raw events sit in staging.events with tenant_id, user_id, event_type, occurred_at. Propose a per-tenant drift-guard plan: a canary analytics.canary_{tenant} surfacing current schema, fields, and max_version; staged per-tenant views; and an incremental analytics.tenant.daily_metrics by day, event_type, tenant_id using MERGE for upserts. Include tests (not_null, unique, foreign_key), a snapshot analytics.tenants_cohort, and isolation/lineage via dbt sources and Snowflake row-level policies. Provide skeleton SQL and tradeoffs?","answer":"Design a per-tenant drift-guard canary: analytics.canary_{tenant} surfaces current schema, fields, and max_version. Use staged per-tenant views, incrementally build daily_metrics with MERGE by (tenant","explanation":"## Why This Is Asked\nAssesses ability to design robust, multi-tenant dbt pipelines with schema drift guards, data quality tests, and clear data lineage in Snowflake. It also evaluates how you model per-tenant snapshots and maintain isolation for BI dashboards.\n\n## Key Concepts\n- Incremental daily metrics per tenant\n- Canary drift guard per-tenant\n- Snapshots for tenant cohorts\n- Data quality tests: not_null, unique, foreign_key\n- Cross-tenant lineage and isolation via dbt sources and row-level policies\n\n## Code Example\n```sql\n-- skeleton: create drift canary per tenant\nCREATE VIEW analytics.canary_${tenant} AS\nSELECT tenant_id, MAX(schema_version) AS max_version, ARRAY_AGG(field) AS fields\nFROM staging.events\nGROUP BY tenant_id;\n```\n\n## Follow-up Questions\n- How would you detect drift programmatically and gate promotions?\n- How would you scale this to 1,000 tenants and monitor drift cost-effectively?","diagram":null,"difficulty":"intermediate","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Discord","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T13:04:07.133Z","createdAt":"2026-01-16T13:04:07.134Z"},{"id":"q-2840","question":"You’re building a beginner dbt flow for multi-tenant analytics. With staging.events(tenant_id, user_id, event_type, occurred_at) and a tenants.csv seed containing tenant_id and risk_multiplier, design analytics.{tenant}.daily_event_score that sums events per day by type, applies the per-tenant risk_multiplier, tolerates 1 day late data, and includes a per-tenant users snapshot and basic tests. How would you implement this?","answer":"Use a per-tenant incremental model analytics.{tenant}.daily_event_score built on staging.events, joined to seeds.tenants for multiplier. Bucket occurred_at to day, group by tenant_id, day, event_type;","explanation":"## Why This Is Asked\nTests ability to design a practical, per-tenant incremental flow using seeds for parameters, with data freshness, tests, and a snapshot to track user cohorts.\n\n## Key Concepts\n- Incremental models per tenant\n- Seeds for tenant metadata\n- Per-tenant schemas/isolation\n- Data contracts with tests and snapshots\n- Late data handling and lineage\n\n## Code Example\n```sql\n-- analytics/{tenant}.daily_event_score.sql\nwith e as (\n  select * from {{ ref('staging__events') }}\n)\nselect\n  tenant_id,\n  date_trunc('day', occurred_at) as day,\n  event_type,\n  count(distinct user_id) as user_count,\n  count(distinct user_id) * t.multiplier as score\nfrom e\njoin {{ ref('tenants_seed') }} t on t.tenant_id = e.tenant_id\ngroup by 1,2,3\n```\n\n## Follow-up Questions\n- How would you validate late data beyond the 1-day window, and alert on anomalies?\n- How would you handle tenants missing a multiplier in the seed and prevent failures?","diagram":null,"difficulty":"beginner","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T14:35:02.039Z","createdAt":"2026-01-16T14:35:02.040Z"},{"id":"q-2882","question":"How would you design a multi-tenant dbt pipeline in Snowflake where staging.events (tenant_id, user_id, event_type, occurred_at, event_properties) feeds analytics.{tenant}.daily_metrics incrementally to count distinct active users by day and event_type, with late data up to 2 days via MERGE upserts and a per-tenant surrogate key? Include enrichment from dimensions.tenants (region, plan), a users cohort snapshot, tenant isolation via namespaced models, and a central metadata table for cross-tenant lineage; outline tests and validations?","answer":"Leverage per-tenant namespaced incremental models with a MERGE-based upsert on analytics.{tenant}.daily_metrics, keyed by (tenant_id, day, event_type, user_id). Use a surrogate daily_key to enable ide","explanation":"## Why This Is Asked\nAssess ability to architect scalable, isolated multi-tenant pipelines with late data handling, schema drift, and lineage.\n\n## Key Concepts\n- Namespaced (tenant-scoped) dbt models for isolation\n- Incremental MERGE upserts handling late data\n- Surrogate keys to guarantee idempotency\n- Central metadata table to preserve cross-tenant lineage\n- Cohort snapshotting and robust tests (not_null, unique, snapshot)\n\n## Code Example\n```sql\n-- Pseudo MERGE for daily_metrics per tenant\nMERGE INTO analytics.{tenant}.daily_metrics AS t\nUSING staging.events AS s\nON (t.tenant_id = s.tenant_id AND t.day = DATE(s.occurred_at) AND t.event_type = s.event_type AND t.user_id = s.user_id)\nWHEN MATCHED THEN UPDATE SET ...\nWHEN NOT MATCHED THEN INSERT (...);\n```\n\n## Follow-up Questions\n- How would you validate cross-tenant lineage in dashboards?\n- How would you detect and remediate schema drift in this setup?","diagram":"flowchart TD\n  A[staging.events] --> B[analytics.{tenant}.daily_metrics]\n  B --> C[dimensions.tenants]\n  B --> D[analytics.{tenant}.users_snapshot]\n  E[central_metadata] --> B","difficulty":"intermediate","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Meta","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T15:52:31.328Z","createdAt":"2026-01-16T15:52:31.329Z"},{"id":"q-2901","question":"Design a multi-tenant dbt pipeline that creates per-tenant-per-region analytics schemas (analytics.{tenant}_{region}). From staging.events (tenant_id, region_id, user_id, event_type, occurred_at, ingestion_id), implement an incremental analytics.{tenant}_{region}.hourly_metrics counting distinct users by event_type per hour, with late data handling of 1 day. Add a tenant-region users snapshot for cohort changes and a tenancy-isolation macro to block cross-region joins. Include tests (not_null, unique) and cross-tenant lineage validation?","answer":"Describe a per-tenant-per-region dbt flow creating schemas analytics.{tenant}_{region} with an incremental hourly_metrics model over staging.events(tenant_id, region_id, user_id, event_type, occurred_","explanation":"## Why This Is Asked\nTests ability to enforce tenancy boundaries in a dynamic multi-tenant, multi-region dbt setup, including late-arriving data and per-tenant snapshots.\n\n## Key Concepts\n- Dynamic per-tenant-per-region schemas analytics.{tenant}_{region}\n- Incremental hourly_metrics from staging.events\n- Deduplication via ingestion_id with ROW_NUMBER windowing\n- Late data window of 1 day; schema-drift guards via tests\n- Tenancy isolation macro preventing cross-region joins\n- Snapshots for analytics.{tenant}_{region}.users and lineage checks\n\n## Code Example\n```sql\n-- models/analytics/{{tenant}}_{{region}}/hourly_metrics.sql\nwith src as (\n  select * from {{ ref('staging_events') }}\n  where tenant_id = '{{ tenant }}' and region_id = '{{ region }}'\n),\ndedup as (\n  select *,\n         row_number() over (partition by ingestion_id order by occurred_at desc) as rn\n  from src\n)\nselect\n  date_trunc('hour', occurred_at) as hour,\n  event_type,\n  count(distinct user_id) as active_users\nfrom dedup\nwhere rn = 1\ngroup by 1,2\n```\n\n## Follow-up Questions\n- How would you validate cross-region lineage in the catalog?\n- What strategies ensure resilience if a region is degraded?","diagram":"flowchart TD\n  A[staging_events] --> B[hourly_metrics (analytics.{tenant}_{region})]\n  B --> C[users_snapshot (analytics.{tenant}_{region})]\n  C --> D[tenancy_isolation_macro]\n  D --> E[catalog & lineage validation]","difficulty":"intermediate","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Microsoft","NVIDIA","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T16:53:11.372Z","createdAt":"2026-01-16T16:53:11.372Z"},{"id":"q-2933","question":"Design a multi-tenant dbt flow in Snowflake that computes analytics.{tenant}.hourly_metrics from staging.events_raw (tenant_id, user_id, event_type, occurred_at, record_hash). Implement an incremental analytics.{tenant}.hourly_metrics by hour_start and event_type with active_users = count(distinct user_id); support 1-day late data; add dynamic schema-drift guards and tests (not_null on hour_start, tenant_id; unique on (tenant_id, hour_start, event_type)); include analytics.{tenant}.users_cohort snapshot for first_session and country changes; ensure per-tenant isolation via schemas and a global analytics.lineage table. Outline macro-driven tests and tenant-scoped materializations?","answer":"Per-tenant schemas analytics.{tenant}.hourly_metrics and analytics.{tenant}.users_cohort; an incremental hourly_metrics model bucketed by hour_start and event_type, with active_users as count(distinct","explanation":"## Why This Is Asked\nTests ability to design a scalable multi-tenant dbt flow with hourly granularity, late data handling, schema drift guards, and governance artifacts.\n\n## Key Concepts\n- Tenant isolation via per-tenant schemas\n- Incremental models with hourly bucketing\n- Late data window and watermark strategy\n- Schema drift guards implemented as reusable macros\n- Tests (not_null, unique) and a cohort snapshot\n- Data lineage and exposed dashboards\n\n## Code Example\n```javascript\n-- dbt hourly_metrics model (simplified)\nwith src as (\n  select tenant_id, user_id, event_type, occurred_at\n  from {{ source('staging', 'events_raw') }}\n), hourly as (\n  select\n    date_trunc('hour', occurred_at) as hour_start,\n    event_type,\n    tenant_id,\n    count(distinct user_id) as active_users\n  from src\n  group by 1,2,3\n)\nselect * from hourly\n```\n\n## Follow-up Questions\n- How would you test for late-arriving data skew across tenants?\n- How would you automate per-tenant schema drift guards?","diagram":null,"difficulty":"intermediate","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Microsoft","NVIDIA","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T17:54:25.699Z","createdAt":"2026-01-16T17:54:25.699Z"},{"id":"q-2940","question":"Design a beginner dbt exercise in Snowflake to build analytics.{tenant}.daily_event_summary from staging.events (tenant_id, user_id, event_type, occurred_at) enriched with dimensions.tenants (retention_group). Implement an incremental model per day that counts distinct users by event_type, with a 2-day late data window. Add tests (not_null on occurred_at,event_type,tenant_id; unique on (tenant_id, occurred_at, event_type)). Create a data-contract macro that validates staging.events fields and types before run and describe how to enforce per-tenant isolation and lineage?","answer":"Implement an incremental per-tenant daily_event_summary from staging.events (tenant_id, user_id, event_type, occurred_at) enriched by dimensions.tenants (retention_group) in Snowflake. Compute daily u","explanation":"## Why This Is Asked\nTests incremental logic, late data handling, data contracts, and tenant isolation via per-tenant artifacts.\n\n## Key Concepts\n- Incremental models\n- Late data windows\n- Data-contract macro\n- Quality tests\n- Tenant isolation patterns\n\n## Code Example\n```sql\n-- example dbt model snippet (pseudo)\nwith s as (\n  select tenant_id, user_id, event_type, date(occurred_at) as day\n  from {{ ref('staging_events') }}\n)\nselect tenant_id, day, event_type, count(distinct user_id) as users\nfrom s\ngroup by 1,2,3\n```\n\n## Follow-up Questions\n- How would you extend to 3 days late data?\n- How would you implement cross-tenant lineage mapping?\n","diagram":"flowchart TD\n  A[staging.events] --> B[analytics.{tenant}.daily_event_summary]\n  B --> C[dimensions.tenants enrichment]\n  B --> D[tests & guards]\n  B --> E[late data window]","difficulty":"beginner","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T18:46:01.513Z","createdAt":"2026-01-16T18:46:01.513Z"},{"id":"q-3016","question":"Design an advanced, privacy-aware per-tenant dbt flow. Tenants table stores privacy_level. Create a macro that SHA256-hashes user_id to analytics.{tenant}.daily_engagement.hashed_user_id only when privacy_level=high; otherwise pass-through. Build analytics.{tenant}.daily_engagement incrementally by date and action with 2-day late data; snapshot analytics.{tenant}.users for cohort and country changes. Enforce per-tenant schemas, schema-drift tests, and not_null/unique constraints; expose a cross-tenant lineage view and privacy-aware dashboards?","answer":"I would implement a privacy-aware, per-tenant dbt flow with dynamic data masking based on tenant privacy settings. The solution includes a macro that conditionally SHA256-hashes user_id to analytics.{tenant}.daily_engagement.hashed_user_id when privacy_level=high, otherwise passes through the original user_id. The daily_engagement model would be built incrementally by date and action with a 2-day late data window, while analytics.{tenant}.users would be snapshot for tracking cohort and country changes. The architecture enforces per-tenant schemas, schema-drift tests, and not_null/unique constraints, with cross-tenant lineage views and privacy-aware dashboards for comprehensive data governance.","explanation":"## Why This Is Asked\nTests ability to implement privacy-aware per-tenant data pipelines with dynamic masking, per-tenant schema isolation, and robust data quality controls.\n\n## Key Concepts\n- dbt macros and Jinja logic for per-tenant behavior\n- Conditional data masking with hashing (SHA256)\n- Incremental models with late data handling\n- Per-tenant schemas and strict tests for isolation\n- Snapshotting and lineage exposure\n\n## Code Example\n```sql\n-- Macro skeleton for privacy-aware hashing\n{% macro hash_user_for_tenant(privacy_level, user_id) %}\n  {% if privacy_level == 'high' %}\n    SHA2(CONCAT('{{ user_id }}'), 256)\n  {% else %}\n    {{ user_id }}\n  {% endif %}\n{% endmacro %}\n```\n\n## Implementation Approach\n1. Create tenant-aware macro for conditional hashing\n2. Build incremental daily_engagement with late data handling\n3. Implement snapshot for users tracking changes\n4. Add per-tenant schema tests and constraints\n5. Create cross-tenant lineage views and dashboards","diagram":null,"difficulty":"advanced","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","Oracle","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T06:03:27.792Z","createdAt":"2026-01-16T21:35:14.434Z"},{"id":"q-3275","question":"In a multi-tenant dbt deployment on Snowflake serving analytics.{tenant}.hourly_metrics from staging.events_raw (tenant_id, user_id, event_type, occurred_at, record_hash), implement a per-tenant, incremental materialization by hour_start and event_type with late data handling of 1 day. Add a tenant-filter macro that enforces isolation by injecting tenant_id filters into every model. Describe isolation, tests, and a macro skeleton with usage and a lineage approach for dashboards?","answer":"Implement a tenant-aware macro that injects tenant_id filters into all models, enabling a single dbt project to serve analytics.{tenant} schemas with strict isolation. Build analytics.{tenant}.hourly_","explanation":"## Why This Is Asked\n\nReal-world multi-tenant isolation in a single dbt project with dynamic tenant scoping. The question tests macro design, incremental logic, late data handling, and governance around schema drift.\n\n## Key Concepts\n\n- Macro-driven tenant isolation in dbt\n- Incremental by (hour_start, event_type)\n- Late data handling (1 day)\n- Schema drift guards and tests (not_null, unique)\n- Snapshots and global lineage for dashboards\n\n## Code Example\n\n```sql\n-- Macro skeleton\n{% macro tenant_filter() %}\n  {% if var('tenant_id') is defined %}\n    AND tenant_id = '{{ var(\"tenant_id\") }}'\n  {% endif %}\n{% endmacro %}\n\n-- Example usage\nSELECT\n  hour_start,\n  event_type,\n  COUNT(*) AS cnt\nFROM analytics.{tenant}.hourly_metrics\nWHERE 1=1\n  {{ tenant_filter() }}\nGROUP BY 1,2\n```\n\n## Follow-up Questions\n\n- How would you test the macro across tenants?\n- How would you detect cross-tenant leakage in production?\n","diagram":null,"difficulty":"intermediate","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Databricks","LinkedIn"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T09:39:24.600Z","createdAt":"2026-01-17T09:39:24.600Z"},{"id":"q-3337","question":"Design an intermediate dbt flow in Snowflake to derive analytics.{tenant}.hourly_metrics from staging.events_json (tenant_id, occurred_at, payload VARIANT) with event_type extracted from payload.eventType (and fallback to payload.type) and indexed by hour_start and event_type. Implement an incremental model that handles 2-day late data, adds schema-drift guards, and tests (not_null on hour_start, tenant_id, event_type; unique on (tenant_id, hour_start, event_type)). Include a per-tenant analytics.{tenant}.users snapshot from payload.user.* for cohort analysis. Outline tenant isolation via per-tenant schemas, a global lineage, and a macro for cross-tenant extraction of event_type?","answer":"Implement a per-tenant incremental hourly_metrics model that extracts event_type from payload.eventType (fallback to payload.type) and aggregates active_users by tenant_id/hour_start/event_type. Handl","explanation":"## Why This Is Asked\nTests ability to work with semi-structured data (JSON VARIANT) in Snowflake, extract consistent event_type across tenants, and build incremental models with late-arrival data. It also probes macro design for per-tenant isolation and lineage visibility in dashboards, plus snapshots for cohort analysis.\n\n## Key Concepts\n- Snowflake VARIANT parsing and lateral flatten for nested payloads\n- Incremental dbt by hour_start and event_type with late data handling\n- Schema-drift guards and tests (not_null, unique)\n- Snapshots for per-tenant user cohorts from payload.user.*\n- Tenant isolation via per-tenant schemas and a macro to enforce filters\n\n## Code Example\n```javascript\n-- dbt SQL for hourly_metrics (skeleton)\nwith src as (\n  select\n    tenant_id,\n    date_trunc('hour', occurred_at) as hour_start,\n    coalesce((payload:eventType)::string, (payload:type)::string) as event_type,\n    (payload:userId) as user_id\n  from {{ source('staging','events_json') }}\n  where occurred_at >= dateadd(day, -2, current_timestamp())\n)\n, agg as (\n  select\n    tenant_id,\n    hour_start,\n    lower(trim(event_type)) as event_type,\n    count(distinct user_id) as active_users\n  from src\n  group by 1,2,3\n)\nselect * from agg\n```\n\n## Follow-up Questions\n- How would you test the cross-tenant event_type extraction macro for schema drift?\n- How would you adapt the approach if payloads vary significantly by tenant (e.g., nested fields differ per tenant)?","diagram":"flowchart TD\n  A[staging.events_json] --> B[parse event_type from payload]\n  B --> C[analytics.{tenant}.hourly_metrics]\n  C --> D[analytics.{tenant}.users snapshot]\n  D --> E[dashboards and lineage]","difficulty":"intermediate","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Lyft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T12:57:31.485Z","createdAt":"2026-01-17T12:57:31.485Z"},{"id":"q-3377","question":"Design a beginner dbt flow to compute analytics.{tenant}.daily_user_event_count from staging.events (tenant_id, user_id, event_type, occurred_at). Use per-tenant schemas for isolation. Build an incremental model keyed by day and event_type, counting distinct users. Implement 1-day late data tolerance. Create a data-contract macro validating required fields and that event_type values are whitelisted per-tenant via dimensions.tenants.event_whitelist. Add tests: not_null on tenant_id, user_id, occurred_at, event_type; unique on (tenant_id, occurred_at_date, event_type). Include a snapshot analytics.{tenant}.users to track first_seen/last_seen. Explain cross-tenant lineage and how to add a new tenant without touching existing tenants?","answer":"I would implement a tenant-aware macro to render analytics.{tenant}.daily_user_event_count from staging.events, with an incremental key (tenant_id, date(occurred_at), event_type). Late data 1 day via ","explanation":"Why asked: tests enforce practical dbt discipline—contracts, macros, and per-tenant governance. Key ideas: incremental, per-tenant isolation via schemas; data-contracts with whitelists; late-data handling; and a users snapshot for SCD-like state. Trade-offs: macro complexity vs governance; scalable tenant onboarding via metadata.","diagram":null,"difficulty":"beginner","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","NVIDIA","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T13:51:36.043Z","createdAt":"2026-01-17T13:51:36.043Z"},{"id":"q-3432","question":"Describe an automated, tenant-aware dbt workflow that discovers tenants from a tenants table, creates per-tenant schemas, and materializes analytics.{tenant}.interactions incrementally from staging.events (tenant_id, user_id, interaction_type, occurred_at). Include late data tolerance of 2 days, per-tenant lineage via a global analytics.lineage table, and macro-driven schema-drift guards plus tests (not_null on occurred_at and user_id; unique on (tenant_id, occurred_at, user_id, interaction_type)); explain how isolation is enforced and how you would validate changes across tenants?","answer":"Leverage a tenants registry to auto-generate per-tenant analytics schemas and an incremental model analytics.{tenant}.interactions from staging.events. Late data tolerance: 2 days. Use a tenant-aware ","explanation":"## Why This Is Asked\n\nTests ability to architect an automated, tenant-aware dbt workflow that generates per-tenant models, ensures data isolation, and enforces governance via lineage.\n\n## Key Concepts\n\n- tenant registry-driven model generation with dbt macros\n- per-tenant schemas for isolation\n- late-data handling and drift guards\n- dynamic tests tied to tenant flags\n- global analytics.lineage for provenance\n\n```javascript\n// Pseudocode sketch of a tenant-scaffold macro\nfunction generateTenantModel(tenant){\n  const schema = `analytics_${tenant}`;\n  return `create schema if not exists ${schema}; create table ${schema}.interactions ...`;\n}\n```\n\n## Follow-up Questions\n\n- How would you detect cross-tenant data leakage?\n- How would you monitor drift and automatically regen models per new tenant?","diagram":"flowchart TD\n  A[Tenant Registry] --> B[Model Generator]\n  B --> C[analytics_{tenant} Schemas]\n  C --> D[analytics.{tenant}.interactions]\n  D --> E[analytics.lineage]","difficulty":"advanced","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Plaid","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T15:43:18.856Z","createdAt":"2026-01-17T15:43:18.857Z"},{"id":"q-3470","question":"Design a beginner dbt flow to compute per-tenant daily metrics from staging.events that include an is_deleted flag for tombstones. Build analytics.{tenant}.daily_metrics by day_start (date of occurred_at) and event_type for rows where is_deleted = false; also create a per-tenant analytics.{tenant}.users snapshot with last_seen. Ensure incremental processing with tenant isolation (per-tenant schemas), plus not_null and unique tests and a data-contract macro. Explain tombstone handling and lineage?","answer":"To implement: use day_start = date_trunc('day', occurred_at); aggregate active_users = count(distinct user_id) where is_deleted = false; implement incremental by tenant schema; create analytics.{tenan","explanation":"## Why This Is Asked\n\nTests understanding of beginner-friendly dbt patterns in a multi-tenant context, focusing on incremental flows, simple tombstone logic, and data contracts.\n\n## Key Concepts\n\n- Multi-tenant isolation via per-tenant schemas\n- Incremental models and day-based partitioning\n- Tombstones with is_deleted and its impact on active_users\n- Snapshot usage for users with last_seen\n- Data-contract macros and basic tests (not_null, unique)\n\n## Code Example\n\n```sql\n-- Incremental daily_metrics (illustrative)\nwith src as (\n  select tenant_id, user_id, event_type, occurred_at, is_deleted\n  from staging_events\n)\nselect\n  tenant_id,\n  date_trunc('day', occurred_at) as day_start,\n  event_type,\n  count(distinct case when is_deleted = false then user_id end) as active_users\nfrom src\nwhere is_deleted = false\ngroup by 1,2,3\n```\n\n```sql\n-- Snapshot concept (last_seen)\nselect\n  tenant_id,\n  user_id,\n  max(occurred_at) as last_seen\nfrom staging_events\ngroup by 1,2\n```\n\n## Follow-up Questions\n\n- How would you handle late tombstones that retroactively remove users from prior days?\n- How would you validate and monitor per-tenant lineage across dbt models?","diagram":null,"difficulty":"beginner","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Microsoft","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T17:36:26.931Z","createdAt":"2026-01-17T17:36:26.931Z"},{"id":"q-3627","question":"Design a beginner dbt workflow to build analytics.country_event_summary per tenant. Source: staging.events(tenant_id, user_id, event_type, occurred_at, country_code). Create analytics.{tenant}.country_event_summary incrementally by day, counting distinct users per event_type and country_code. Enrich with dimensions.countries. Implement late data handling for up to 2 days. Add basic tests: not_null on tenant_id, day, country_code, event_type; unique on (tenant_id, day, event_type, country_code, user_id). Include a simple snapshot analytics.{tenant}.users to capture last_known_country and explain how you verify cross-tenant isolation and lineage?","answer":"Build an incremental model `analytics.{tenant}.country_event_summary` from `staging.events`, grouping by `tenant_id`, `day`, `event_type`, and `country_code` while counting distinct `user_id`. Join with `dimensions.countries` for enrichment, implement late data handling with a 2-day lookback window, and add data quality tests including not_null constraints on `tenant_id`, `day`, `country_code`, `event_type` and unique constraints on `(tenant_id, day, event_type, country_code, user_id)`. Create a simple snapshot `analytics.{tenant}.users` to capture `last_known_country` for slowly changing dimensions.","explanation":"## Why This Is Asked\n\nTests fundamentals of incremental modeling, late data handling, basic data quality tests, snapshots for slowly changing dimensions, and lineage/isolation in a multi-tenant dbt setup. It also reinforces practical enrichment with a dimension table.\n\n## Key Concepts\n\n- Incremental dbt models by day\n- Late data tolerance (2 days)\n- Data quality tests (not_null, unique)\n- Dimension enrichment (dimensions.countries)\n- Snapshots for slowly changing attributes (last_known_country)\n- Cross-tenant lineage and isolation via schemas and docs\n\n## Code Example\n\n```sql\n-- illustrative example","diagram":"flowchart TD\n  A[staging.events] --> B[analytics.{tenant}.country_event_summary]\n  C[dimensions.countries] --> B\n  B --> D[analytics.{tenant}.users_snapshot]\n  B --> E[dbt docs lineage]","difficulty":"beginner","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","IBM","LinkedIn"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T05:07:54.387Z","createdAt":"2026-01-18T02:30:25.090Z"},{"id":"q-3669","question":"Design a multi-tenant dbt pipeline for a Discord/Snap like platform that computes analytics.{tenant}.event_summary from staging.events (tenant_id, user_id, event_type, occurred_at, country). Build an incremental model by hour and event_type with 1 day late data, tenant isolation via per tenant schemas, and a tenant-filter macro; add dynamic schema drift guards and tests (not_null on hour_start, tenant_id, event_type; unique on (tenant_id, hour_start, event_type)); include per-tenant analytics.{tenant}.users snapshot, analytics.lineage, and analytics.{tenant}.alerts for anomalies; outline testing and alerting strategy?","answer":"Propose a per-tenant isolation with analytics.{tenant} schemas, an hourly incremental by hour_start and event_type, 1-day late data, macro to inject tenant filters, dynamic schema drift guards and tes","explanation":"## Why This Is Asked\n\nTests ability to design scalable multi-tenant dbt flows with strict isolation, drift guards, lineage, and alerting. Must show pragmatic handling of late data, per-tenant materializations, and automated tests.\n\n## Key Concepts\n\n- Multi-tenant isolation via per-tenant schemas\n- Incremental models by hour_start and event_type\n- Late data handling (1 day)\n- Tenant-filter macro for consistent isolation\n- Dynamic schema drift guards and tests (not_null, unique)\n- Snapshots for user cohorts per tenant\n- Central lineage exposure for dashboards\n- Per-tenant alerts for anomalies\n\n## Code Example\n\n```jinja\n{% macro tenant_filter(table) %}\n  {% if var('tenant_id') is defined %}\n    SELECT * FROM {{ table }} WHERE tenant_id = '{{ var('tenant_id') }}'\n  {% else %}\n    SELECT * FROM {{ table }}\n  {% endif %}\n{% endmacro %}\n```\n\n```sql\n-- sample: ensure a per-tenant incremental by hour_start\nwith source as (\n  select tenant_id, user_id, event_type, date_trunc('hour', occurred_at) as hour_start, country\n  from {{ ref('staging__events') }}\n  where tenant_id = '{{ var('tenant_id', '') }}'\n)\nselect * from source\n```\n\n## Follow-up Questions\n\n- How would you scale this for thousands of tenants with varying data volumes?\n- How would you automate schema-drift guard updates and test coverage across tenants?","diagram":"flowchart TD\n  S[staging.events] --> E[analytics.{tenant}.event_summary]\n  E --> L[analytics.lineage]\n  E --> A[analytics.{tenant}.alerts]\n  U[analytics.{tenant}.users] --> L","difficulty":"intermediate","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T04:20:02.856Z","createdAt":"2026-01-18T04:20:02.856Z"},{"id":"q-3755","question":"Design a beginner-friendly multi-tenant dbt exercise in Snowflake: for each tenant, build analytics.{tenant}.daily_summary from staging.events (tenant_id, user_id, event_type, occurred_at) enriched by dimensions.countries, producing daily counts by event_type per tenant. Implement a 2-day late data window, per-tenant isolation via schemas, and tests: not_null on tenant_id, occurred_at, event_type; unique on (tenant_id, occurred_at, event_type); a per-tenant data_contract that country_code exists in countries; and a cross-tenant leakage test ensuring analytics.{tenant} only reads its own data. Outline approach and tests?","answer":"Implement a tenant-scoped daily_summary as an incremental analytics.{tenant}.daily_summary built from staging.events (tenant_id, user_id, event_type, occurred_at) joined to dimensions.countries, with ","explanation":"## Why This Is Asked\n\nAssesses practical dbt setup for multi-tenant isolation, late data handling, and data quality tests.\n\n## Key Concepts\n\n- Tenant-scoped schemas\n- Incremental daily aggregation\n- Data contracts and cross-tenant leakage tests\n- Late data window and watermarking\n\n## Code Example\n\n```javascript\n-- Pseudo dbt macro usage snippet\n```\n\n## Follow-up Questions\n\n- How would you automate tenant onboarding in this pattern?\n- How would you monitor data freshness across tenants?","diagram":null,"difficulty":"beginner","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Snowflake","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T08:39:33.108Z","createdAt":"2026-01-18T08:39:33.109Z"},{"id":"q-3815","question":"Design an intermediate dbt workflow on Snowflake for a federated analytics layer with per-tenant schemas (tenant_<id>). Staging.events_raw has (tenant_id, user_id, event_type, occurred_at, payload VARIANT). Build analytics.hourly_metrics as an incremental model by hour_start and event_type across tenants, resolving schemas via a macro. Include 1-day late data, schema-drift guards, tests (not_null on hour_start, tenant_id; unique on tenant_id/hour_start/event_type), and a global analytics.lineage table; describe per-tenant isolation and dashboard exposure?","answer":"Leverage a single incremental model analytics.hourly_metrics across tenants, using a macro to resolve tenant_schema(tenant_id) for source and destination. Compute hour_start as date_trunc('hour', occu","explanation":"## Why This Is Asked\n\nAssesses multi-tenant dbt design, dynamic schema routing, and cross-tenant governance on Snowflake; tests ability to implement late data handling and lineage.\n\n## Key Concepts\n\n- Federated multi-tenant patterns in dbt\n- Macro-based dynamic tenant_schema resolution\n- Incremental models by hour_start and event_type\n- Late data handling, schema drift guards, and data leakage prevention\n- Global lineage table and per-tenant isolation\n\n## Code Example\n\n```sql\n-- Example macro (simplified)\n{% macro tenant_schema(tenant_id) %}\n  -- resolves to tenant_<id> schema\n  {{ return('analytics.tenant_' ~ tenant_id) }}\n{% endmacro %}\n```\n\n## Follow-up Questions\n\n- How would you test for cross-tenant data leakage?\n- How would onboarding new tenants affect schema and lineage guarantees?","diagram":"flowchart TD\n  S[staging.events_raw]\n  H[analytics.hourly_metrics]\n  L[analytics.lineage]\n  S --> H\n  H --> L","difficulty":"intermediate","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T10:38:47.652Z","createdAt":"2026-01-18T10:38:47.652Z"},{"id":"q-3849","question":"In a multi-tenant dbt deployment on Snowflake, implement an anomaly detection layer: analytics.{tenant}.anomalies derived from staging.events_raw (tenant_id, user_id, event_type, occurred_at, value). Build a per-tenant rolling z-score per event_type, daily incremental, with 2-day late data, and add schema-drift guards and tests. Provide a tenant-scoped macro that injects filters into all models and a simple lineage snapshot for dashboards. How would you implement this end-to-end?","answer":"Design a per-tenant anomaly layer with a rolling 7- or 14-day z-score per event_type, compute per-tenant anomaly_flag, and incrementally materialize analytics.{tenant}.anomalies by day. Include 2-day ","explanation":"## Why This Is Asked\n\nTests ability to design per-tenant isolation, a practical anomaly-detection workflow in dbt, and macro-based scoping with lineage considerations.\n\n## Key Concepts\n\n- Per-tenant isolation via dynamic macros and schemas\n- Anomaly detection: rolling z-score per tenant_id and event_type\n- Incremental models with late-arriving data (2 days)\n- Schema drift guards and dbt tests (not_null, value, and flags)\n- Tenant-scoped macros for isolation; lineage snapshots for dashboards\n\n## Code Example\n\n```sql\n-- Skeleton macro to scope a model by tenant\n{% macro tenant_scope(relation, tenant_id) %}\n  SELECT * FROM {{ relation }} WHERE tenant_id = '{{ tenant_id }}'\n{% endmacro %}\n```\n\n```sql\n-- Pseudo-aggregation for anomaly score (illustrative)\nWITH base AS (\n  SELECT tenant_id, event_type, occurred_at, value,\n         AVG(value) OVER (PARTITION BY tenant_id, event_type ORDER BY occurred_at ROWS BETWEEN 13 PRECEDING AND CURRENT ROW) AS mean_14d,\n         STDDEV_SAMP(value) OVER (PARTITION BY tenant_id, event_type ORDER BY occurred_at ROWS BETWEEN 13 PRECEDING AND CURRENT ROW) AS sd_14d\n  FROM {{ source('staging','events_raw') }}\n)\nSELECT *, CASE WHEN sd_14d > 0 THEN (value - mean_14d) / sd_14d ELSE NULL END AS zscore\nFROM base\n```\n\n## Follow-up Questions\n\n- How would you test the rolling z-score across tenants with varying data scales?\n- What strategies would you use to scale the tenant-scoped macro as tenant count grows?","diagram":null,"difficulty":"intermediate","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","DoorDash"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T12:55:56.712Z","createdAt":"2026-01-18T12:55:56.713Z"},{"id":"q-3939","question":"Design a beginner dbt flow in Snowflake to compute analytics.{tenant}.daily_user_engagement from staging.events (tenant_id, user_id, event_type, occurred_at, country_code), enriched by dimensions.countries and dimensions.products; produce per-tenant daily counts by event_type and country, with 2-day late data handling, per-tenant schema isolation, schema-drift guards, and tests (not_null on tenant_id, user_id, occurred_at; unique on (tenant_id, occurred_at, user_id)); include a snapshot analytics.{tenant}.users to capture cohort changes and a data-contract macro validating staging fields; explain cross-tenant lineage and privacy considerations?","answer":"Propose a per-tenant incremental model analytics.{tenant}.daily_user_engagement, scoped to a tenant-specific schema. Build incrementally by day, joining staging.events with countries and products dime","explanation":"## Why This Is Asked\nTests ability to design per-tenant, incremental dbt flows with late data, tests, snapshots, and isolation, plus governance via data contracts.\n\n## Key Concepts\n- Incremental, tenant-scoped models in Snowflake\n- Late-arriving data handling (2 days)\n- Schema-drift guards and tests (not_null, unique)\n- Snapshots for cohort tracking\n- Data-contract macro to validate sources\n- Cross-tenant lineage and privacy considerations\n\n## Code Example\n```sql\n-- simplified incremental model sketch\nwith base as (\n  select tenant_id, user_id, event_type, occurred_at, country_code\n  from {{ source('staging','events') }}\n  where occurred_at >= dateadd(day, -2, current_date())\n)\nselect\n  tenant_id,\n  date(occurred_at) as day,\n  event_type,\n  country_code,\n  count(distinct user_id) as active_users\nfrom base\ngroup by 1,2,3,4\n```\n\n## Follow-up Questions\n- How would you implement the data-contract macro to validate staging fields before run?\n- What are the trade-offs of per-tenant schema isolation versus a shared schema with tenant_id partitioning?","diagram":null,"difficulty":"beginner","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","NVIDIA","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T16:35:05.564Z","createdAt":"2026-01-18T16:35:05.564Z"},{"id":"q-4033","question":"Design a per-tenant incremental model in Snowflake that computes analytics.{tenant}.retention_daily from staging.events (tenant_id, user_id, event_date, first_seen, country_code). Include a macro-driven approach to switch between per-tenant daily retention and rolling-7-day retention, handle late data up to 3 days, enforce per-tenant schema isolation, and add tests (not_null on tenant_id, user_id, event_date; unique on (tenant_id, user_id, event_date)). Also add a snapshot analytics.{tenant}.users_cohort for first_seen and country changes. Explain how you would implement cross-tenant lineage and per-tenant data quality checks?","answer":"Implement a per-tenant macro system that dynamically configures retention_window_days (1 or 7) and processing mode (daily or rolling). Build analytics.{tenant}.retention_daily incrementally from staging.events (tenant_id, user_id, event_date, first_seen, country_code) with a 3-day lookback window to handle late-arriving data. Enforce per-tenant schema isolation using Snowflake schemas and create analytics.{tenant}.users_cohort snapshot to track first_seen and country_code changes over time. Apply comprehensive data quality tests including not_null constraints on tenant_id, user_id, event_date and unique constraints on (tenant_id, user_id, event_date).","explanation":"## Why This Is Asked\nAssesses practical multi-tenant dbt design capabilities including macro-driven per-tenant logic, incremental processing with late data handling, and governance artifacts.\n\n## Key Concepts\n- Macros for tenant-specific configuration and logic switching\n- Incremental models with late-arriving data handling\n- Per-tenant schemas and snapshots for data isolation\n- Data quality tests and cross-tenant lineage tracking\n\n## Code Example\n```javascript\n// Placeholder implementation\n```\n\n## Follow-up Questions\n- How would you test macro correctness across tenants with different retention windows?\n- What strategies would you use for cross-tenant lineage tracking?\n- How would you implement per-tenant data quality checks at scale?","diagram":null,"difficulty":"advanced","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Hashicorp","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-19T07:07:22.919Z","createdAt":"2026-01-18T20:45:06.202Z"},{"id":"q-4103","question":"Design a multi-tenant incremental dbt flow in Snowflake that computes analytics.{tenant}.monthly_cost from staging.sales_events (tenant_id, cost_cents, currency, billed_at, event_hash). Build analytics.{tenant}.monthly_cost by (tenant_id, month_start) with 30-day late_arrival tolerance; implement currency conversion using a per-tenant rates table; handle null currencies. Provide schema-drift guards and tests (not_null on month_start and tenant_id; unique on (tenant_id, month_start)); add a tenant-scoping macro routing models to per-tenant schemas and a global analytics.lineage. Describe testing strategy and cross-tenant isolation?","answer":"Design a multi-tenant incremental dbt flow in Snowflake that computes analytics.{tenant}.monthly_cost from staging.sales_events (tenant_id, cost_cents, currency, billed_at, event_hash). Build analytics.{tenant}.monthly_cost by (tenant_id, month_start) with 30-day late-arrival tolerance; implement currency conversion using a per-tenant rates table; handle null currencies. Provide schema-drift guards and tests (not_null on month_start and tenant_id; unique on (tenant_id, month_start)); add a tenant-scoping macro routing models to per-tenant schemas and a global analytics.lineage. Describe testing strategy and cross-tenant isolation?","explanation":"## Why This Is Asked\nTests ability to build robust, tenant-aware dbt pipelines that handle late data, currency conversion, and schema drift while preserving cross-tenant isolation.\n\n## Key Concepts\n- Per-tenant schemas with macro-based routing\n- Incremental processing by month_start with late-arrival tolerance\n- Currency normalization via per-tenant rates table\n- Schema-drift guards and comprehensive data quality tests\n- Global lineage table for cross-tenant analytics and dashboards\n\n## Code Example\n```sql\n-- macro skeleton for tenant scoping\n{% macro tenant_model(model_name, tenant) -%}\n  {{\n\n```","diagram":"flowchart TD\n  A[staging.sales_events] --> B[per-tenant analytics.{tenant}.monthly_cost]\n  B --> C[per-tenant schema routing via macro]\n  C --> D[global analytics.lineage]\n  D --> E[dashboards/consumers]","difficulty":"intermediate","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Amazon","Anthropic"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-19T05:32:07.326Z","createdAt":"2026-01-18T23:46:35.648Z"},{"id":"q-4274","question":"In a multi-tenant Snowflake dbt project, staging.events has tenant_id, user_id, event_type, occurred_at. Create an incremental analytics.{tenant}.daily_events that counts events per tenant/day by event_type from staging.events. Implement 1-day late data, per-tenant schemas via a macro, and a lightweight analytics.lineage table. Tests: not_null on key columns and unique on (tenant_id, day, event_type). How would you ensure isolation and maintainability?","answer":"Design an incremental per-tenant model analytics.{tenant}.daily_events from staging.events, aggregating by tenant_id, date(occurred_at), event_type. Allow 1-day late data. Use a macro to render per-te","explanation":"## Why This Is Asked\n\nThis question probes ability to implement a simple, tenant-aware incremental model with light data-quality checks, while introducing macros and a lineage artifact to address maintainability and isolation—common beginner tasks in real dbt workflows at scale.\n\n## Key Concepts\n\n- Incremental models with late data handling\n- Tenant-scoped schemas and macros\n- Basic data-contract tests\n- Lightweight lineage tracking for cross-tenant visibility\n\n## Code Example\n\n```sql\n-- Example incremental model outline\nwith src as (\n  select tenant_id, cast(date(occurred_at) as date) as day, event_type\n  from {{ source('staging', 'events') }}\n)\nselect tenant_id, day, event_type, count(*) as events\nfrom src\ngroup by tenant_id, day, event_type\n{% if is_incremental() %}\n  where day < (select max(day) from analytics.lineage where tenant_id = tenant_id)\n{% endif %}\n```\n\n## Follow-up Questions\n\n- How would you implement a generic tests macro to enforce required fields for all tenants?\n- How would you scale the lineage table as tenants are added?","diagram":null,"difficulty":"beginner","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","MongoDB","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-19T11:03:06.052Z","createdAt":"2026-01-19T11:03:06.052Z"},{"id":"q-4488","question":"Design a beginner dbt workflow in Snowflake to compute analytics.{tenant}.daily_user_activity from staging.events (tenant_id, user_id, action, occurred_at, country_code). Normalize occurred_at to UTC, bucket by day, and count distinct users per action. Include 1-day late data, per-tenant schema isolation, tests (not_null on keys, unique on (tenant_id, date, action)); add a data-contract macro to validate staging.events schema; snapshot analytics.{tenant}.users for country churn, and outline per-tenant lineage into analytics.lineage?","answer":"Implement an incremental model analytics.{tenant}.daily_user_activity using staging.events per tenant, UTC-normalize occurred_at to date, and aggregate distinct user_id by action. Add tests: not_null ","explanation":"## Why This Is Asked\nInterview context explanation.\n\n## Key Concepts\n- Incremental models and unique keys per tenant\n- Timezone normalization (UTC) and date bucketing\n- Data contracts and schema tests\n\n## Code Example\n```javascript\n{{ config(materialized='incremental', unique_key='(tenant_id, date, action)') }}\nwith s as (\n  select tenant_id, user_id, action, occurred_at, country_code\n  from {{ source('staging','events') }}\n)\nselect\n  tenant_id,\n  date_trunc('day', occurred_at AT TIME ZONE 'UTC')::date as date,\n  action,\n  count(distinct user_id) as user_count\nfrom s\ngroup by 1,2,3\n{% if is_incremental() %}\n  where occurred_at > (select max(occurred_at) from {{ this }})\n{% endif %}\n```\n\n## Follow-up Questions\n- How would you unit-test the data-contract macro to ensure required fields exist?\n- How would you validate that per-tenant snapshots reflect churn in analytics.{tenant}.users over time?","diagram":null,"difficulty":"beginner","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Snowflake","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-19T20:49:54.514Z","createdAt":"2026-01-19T20:49:54.514Z"},{"id":"q-4686","question":"In a beginner dbt project for multi-tenant analytics on Snowflake, staging.events(tenant_id, user_id, event_type, occurred_at, record_hash) feeds analytics.{tenant}.daily_metrics. Propose an incremental approach that uses a per-tenant schema, handles 2-day late data, and uses a MERGE-based dedup strategy on (tenant_id, day, event_type) while preserving idempotence. Include a macro for tests and a snapshot of analytics.{tenant}.users. Explain cross-tenant lineage and error handling?","answer":"Propose a per-tenant incremental daily_metrics using MERGE-based upserts on (tenant_id, day, event_type) to deduplicate by record_hash, with 2-day late data. Enforce per-tenant isolation via schemas, ","explanation":"## Why This Is Asked\nTests ability to design per-tenant isolation with dbt incremental loads, handle late data, and implement dedup via MERGE. Also checks macro-driven tests and snapshots for user state.\n\n## Key Concepts\n- Incremental models with unique keys per tenant\n- Snowflake MERGE for idempotent upserts\n- Per-tenant schema isolation\n- Late data tolerance (2 days)\n- Snapshots for users to capture cohort changes\n- Macro-driven tests and data contracts for governance\n\n## Code Example\n```javascript\n// Pseudo dbt macro sketch for per-tenant MERGE upsert\n{% macro upsert_daily_metrics(tenant_id, day) -%}\nMERGE INTO analytics.{{ tenant_id }}.daily_metrics AS t\nUSING (\n  SELECT tenant_id, DATE(occurred_at) AS day, event_type,\n         COUNT(DISTINCT user_id) AS active_users, MAX(record_hash) AS max_hash\n  FROM {{ ref('staging_events') }}\n  WHERE tenant_id = '{{ tenant_id }}' AND occurred_at >= DATE('{{ day }}') - INTERVAL '2 days'\n  GROUP BY 1,2,3\n) AS s\nON (t.tenant_id = s.tenant_id AND t.day = s.day AND t.event_type = s.event_type)\nWHEN MATCHED THEN UPDATE SET active_users = s.active_users\nWHEN NOT MATCHED THEN INSERT (tenant_id, day, event_type, active_users) VALUES (s.tenant_id, s.day, s.event_type, s.active_users);\n{% endmacro %}\n```\n\n## Follow-up Questions\n- How would you validate idempotence across repeated runs?\n- How would you audit and monitor tenant isolation and lineage?","diagram":"flowchart TD\n  A[staging.events] --> B[analytics.{tenant}.daily_metrics]\n  B --> C[analytics.lineage?]\n  D[tenants] --> E[isolation via schemas]\n  F[dbt tests] --> G[data contracts]","difficulty":"beginner","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Snap","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T07:54:23.561Z","createdAt":"2026-01-20T07:54:23.562Z"},{"id":"q-4747","question":"Design a per-tenant, incremental dbt flow in Snowflake that computes analytics.{tenant}.hourly_engagement from staging.tenant_events (tenant_id, user_id, action, occurred_at). Build via a macro that creates analytics_{{tenant}} schemas and analytics_{{tenant}}.hourly_engagement, plus a global analytics.lineage table to track dependencies. Include 1-day late data, schema-drift guards, and tests (not_null on keys, unique on (tenant_id, hour_start, user_id)). Also snapshot analytics.{tenant}.users for churn. Explain how you would validate cross-tenant isolation in CI and prevent bleed?","answer":"Automate a per-tenant, incremental dbt flow: a macro creates analytics_{{tenant}} schemas and builds analytics_{{tenant}}.hourly_engagement from staging_{{tenant}}_events (tenant_id, user_id, action, ","explanation":"## Why This Is Asked\nTests macro-driven tenant isolation, lineage governance, and late-arrival handling in a scalable dbt fintech analytics pipeline.\n\n## Key Concepts\n- Per-tenant macros and schemas\n- Incremental models with hourly windows\n- Late data tolerance and drift guards\n- Global lineage table with tenant scoping\n- Snapshot for churn and tests for isolation\n\n## Code Example\n```javascript\n// pseudo dbt macro example\n```\n\n## Follow-up Questions\n- How would you test cross-tenant lineage changes in CI?\n- What are failure modes if a tenant schema migrates to a new cluster?","diagram":null,"difficulty":"advanced","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Google","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T10:44:11.440Z","createdAt":"2026-01-20T10:44:11.440Z"},{"id":"q-848","question":"You're designing a dbt analytics pipeline for an e-commerce platform. The 'staging.events_raw' table ingests page views and purchases and is partitioned by day. Data arrives both on time and as late as 24 hours. Design a practical incremental dbt model 'analytics.daily_sales' that aggregates revenue by day, category, and customer_segment. Explain how you'd implement incremental logic, handle late data, guard against schema drift, and validate with tests and snapshots?","answer":"Use an incremental model keyed by day, category, and customer_segment. Compute revenue as sum(price*quantity) from staging.events_raw. Upsert into analytics.daily_sales via MERGE on (day, category, cu","explanation":"## Why This Is Asked\n\nAssess ability to design scalable, fault-tolerant dbt pipelines handling late data and schema drift.\n\n## Key Concepts\n\n- Incremental models and idempotent MERGE\n- Late-arrival handling and backfill windows\n- Schema drift detection via tests and snapshots\n- Performance at scale and CI validation\n\n## Code Example\n\n```sql\n-- Example incremental sql sketch\nWITH s AS (\n  SELECT day, category, customer_segment,\n         SUM(price * quantity) AS revenue\n  FROM {{ ref('staging__events_raw') }}\n  WHERE day >= (SELECT MIN(day) FROM analytics.daily_sales)\n  GROUP BY day, category, customer_segment\n)\nMERGE INTO analytics.daily_sales AS t\nUSING s AS s\nON t.day = s.day AND t.category = s.category AND t.customer_segment = s.customer_segment\nWHEN MATCHED THEN UPDATE SET revenue = s.revenue\nWHEN NOT MATCHED THEN INSERT (day, category, customer_segment, revenue) VALUES (s.day, s.category, s.customer_segment, s.revenue);\n```\n\n## Follow-up Questions\n\n- How would you test for idempotency and late-arrival correctness?\n- How would you adapt this for a lakehouse (Parquet) or streaming source?","diagram":null,"difficulty":"advanced","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Anthropic","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:30:20.377Z","createdAt":"2026-01-12T13:30:20.377Z"},{"id":"q-972","question":"In a dbt analytics project deployed to Snowflake, three tenants share a common model but data is isolated by schema prefixes (tenant_a_, tenant_b_, tenant_c_). Describe how you would structure tenancy-aware tests and a test runner to prevent cross-tenant data leakage during PR runs. Include how you configure sources, seeds, and per-tenant test filtering, and how you verify isolation in CI without touching production data?","answer":"A tenancy-aware dbt run parameterizes the target schema per tenant, isolates seeds and sources with tenant prefixes, and uses a tenancy-aware test macro to scope checks. In CI, create ephemeral tenant","explanation":"## Why This Is Asked\nThis question probes how a candidate ensures strict data isolation in a multi-tenant dbt setup during CI, a common real-world issue.\n\n## Key Concepts\n- Tenancy-aware testing across schemas\n- Per-tenant seeds/sources isolation\n- Test filtering macros and CI ephemeral environments\n- Cross-tenant leakage detection without touching prod\n\n## Code Example\n```javascript\n-- macros/tenancy_filters.sql\n{% macro tenant_test_filter(tenant) -%}\n  tenant_id = '{{ tenant }}'\n{%- endmacro %}\n```\n\n```javascript\n-- tests/tenancy_isolation_test.sql\nselect * from {{ ref('customers') }}\nwhere {{ tenant_test_filter('tenant_a') }}\n```\n\n## Follow-up Questions\n- How would you extend this for dynamic tenant onboarding?\n- How do you validate that physical storage quotas per tenant are respected?","diagram":"flowchart TD\n  PR[Pull Request] --> TESTS[Run tenancy-aware tests]\n  TESTS --> PASS{All tenant tests pass?}\n  PASS --> MERGE[Merge PR]\n  PASS --> REBUILD[Rebuild docs if needed]\n  FAIL --> NOTIFY[Notify engineer]","difficulty":"advanced","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Lyft","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T17:35:20.447Z","createdAt":"2026-01-12T17:35:20.447Z"}],"subChannels":["general"],"companies":["Adobe","Airbnb","Amazon","Anthropic","Apple","Bloomberg","Citadel","Cloudflare","Coinbase","Databricks","Discord","DoorDash","Goldman Sachs","Google","Hashicorp","Hugging Face","IBM","Instacart","LinkedIn","Lyft","Meta","Microsoft","MongoDB","NVIDIA","Netflix","OpenAI","Oracle","PayPal","Plaid","Robinhood","Salesforce","Scale Ai","Slack","Snap","Snowflake","Square","Stripe","Tesla","Twitter","Uber","Zoom"],"stats":{"total":51,"beginner":22,"intermediate":18,"advanced":11,"newThisWeek":45}}