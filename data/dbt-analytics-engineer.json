{"questions":[{"id":"dbt-analytics-engineer-advanced-features-1768253136238-0","question":"You need to preserve historical changes for a customers dimension using SCD Type 2. Which dbt feature is most appropriate to capture and query these historical changes?","answer":"[{\"id\":\"a\",\"text\":\"Use a snapshot to capture historical changes with a primary key\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Use an incremental model with a defined unique_key and a full-refresh for late data\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Build a separate history table outside dbt and join it to the current data\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Load past records with seeds to store historical rows\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA) Use a snapshot to capture historical changes with a primary key. Snapshots automatically track history for SCD Type 2 dimensions, whereas incremental alone only updates current rows and external history tables require maintenance.\n\n## Why Other Options Are Wrong\n- Option B: Incremental with a unique_key and full-refresh does not retain historical versions automatically.\n- Option C: Building a separate history table outside dbt adds manual maintenance and disconnects from dbt's lineage.\n- Option D: Seeds only load static data and don't capture history.\n\n## Key Concepts\n- Snapshots\n- SCD Type 2\n- History tracking\n\n## Real-World Application\n- Keeps a complete history of customer attributes for time-based reporting and trend analysis.","diagram":null,"difficulty":"intermediate","tags":["dbt","dbt-snapshots","Snowflake","AWS","certification-mcq","domain-weight-15"],"channel":"dbt-analytics-engineer","subChannel":"advanced-features","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T21:25:36.239Z","createdAt":"2026-01-12 21:25:36"},{"id":"dbt-analytics-engineer-advanced-features-1768253136238-1","question":"In dbt, you want to publish data products and document their downstream dependencies and owners for business users. Which dbt feature should you use?","answer":"[{\"id\":\"a\",\"text\":\"Seeds\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Exposures\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Snapshots\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Macros\",\"isCorrect\":false}]","explanation":"## Correct Answer\nB) Exposures. Exposures allow you to describe data products, owners, and downstream consumers within dbt, enabling clear governance and discoverability in your docs.\n\n## Why Other Options Are Wrong\n- Option A: Seeds are static data files used for initial data loading, not for publishing products or documenting ownership.\n- Option C: Snapshots track historical changes, not product governance or ownership metadata.\n- Option D: Macros are for code reuse and logic, not for documenting data products.\n\n## Key Concepts\n- Exposures\n- Data governance in dbt docs\n- Downstream relationships\n\n## Real-World Application\n- Business users can discover data products, understand dependencies, and know data owners directly from the data docs.","diagram":null,"difficulty":"intermediate","tags":["dbt","dbt-exposures","AWS","Redshift","certification-mcq","domain-weight-15"],"channel":"dbt-analytics-engineer","subChannel":"advanced-features","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T21:25:36.929Z","createdAt":"2026-01-12 21:25:37"},{"id":"dbt-analytics-engineer-advanced-features-1768253136238-2","question":"You need a reusable data quality rule that checks a numeric column is always between 0 and 1 across several models. How should you implement this in dbt?","answer":"[{\"id\":\"a\",\"text\":\"Use a not_null test with a range constraint\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Create a custom generic test macro and reuse it in multiple models\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Use a relationships test to check the range\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use a built-in interval test\",\"isCorrect\":false}]","explanation":"## Correct Answer\nB) Create a custom generic test macro and reuse it in multiple models. Generic tests let you parameterize constraints (like 0 <= value <= 1) and apply them across many models. A not_null test cannot enforce a numeric range; Relationships tests verify referential integrity, not value ranges; There is no built-in interval test in dbt core.\n\n## Why Other Options Are Wrong\n- Option A: Not_null cannot enforce a numeric range.\n- Option C: Relationships tests check referential integrity, not value ranges.\n- Option D: There is no standard built-in interval test in core dbt.\n\n## Key Concepts\n- Generic tests\n- Macros and reusability\n- Parameterized constraints\n\n## Real-World Application\n- Enforces consistent data quality across multiple models for probabilistic metrics that must stay within [0,1].","diagram":null,"difficulty":"intermediate","tags":["dbt","dbt-utils","Kubernetes","Terraform","certification-mcq","domain-weight-15"],"channel":"dbt-analytics-engineer","subChannel":"advanced-features","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T21:25:37.446Z","createdAt":"2026-01-12 21:25:37"},{"id":"dbt-analytics-engineer-advanced-features-1768253136238-3","question":"A fact table receives late-arriving data for the same keys. Which dbt pattern ensures you deduplicate and apply the latest values during incremental loads?","answer":"[{\"id\":\"a\",\"text\":\"Use incremental with a defined unique_key and run full-refresh for late data\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Always perform a full-table refresh for every run\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Materialize as a view to avoid duplicates\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use a snapshot to manage late-arriving data\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA) Use incremental with a defined unique_key and run full-refresh for late data. This approach allows deduplication and updating of existing keys when late-arriving rows arrive; a full refresh every run is expensive and unnecessary; a view does not deduplicate; snapshots are for historical changes, not incremental deduplication.\n\n## Why Other Options Are Wrong\n- Option B: Full refresh every run is costly and negates the benefits of incremental loads.\n- Option C: A view cannot deduplicate historical updates across runs.\n- Option D: Snapshots track history, not deduplication during incremental loads.\n\n## Key Concepts\n- Incremental models\n- unique_key configuration\n- Late-arriving data handling\n\n## Real-World Application\n- Ensures stable, deduplicated metrics in a high-ingest pipeline where records may arrive out of order.","diagram":null,"difficulty":"intermediate","tags":["dbt","dbt-incremental","AWS","BigQuery","certification-mcq","domain-weight-15"],"channel":"dbt-analytics-engineer","subChannel":"advanced-features","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T21:25:37.626Z","createdAt":"2026-01-12 21:25:37"},{"id":"dbt-analytics-engineer-advanced-features-1768253136238-4","question":"You need to set a temporary schema or database for the duration of a dbt run and revert afterward. Which mechanism should you use?","answer":"[{\"id\":\"a\",\"text\":\"Pre-hook on every model to set the session parameter\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"On-run-end hook to reset the environment\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"On-run-start hook to set the session parameter for the duration of the run\",\"isCorrect\":true},{\"id\":\"d\",\"text\":\"Use exposures to configure the runtime environment\",\"isCorrect\":false}]","explanation":"## Correct Answer\nC) On-run-start hook to set the session parameter for the duration of the run. on-run-start executes before models run, enabling temporary changes (like schema or search_path) that are automatically reverted at the end. Pre-hooks would require adding to every model; on-run-end can revert but must pair with on-run-start; exposures do not affect session parameters.\n\n## Why Other Options Are Wrong\n- Option A: Pre-hooks require modifying every model and are less centralized.\n- Option B: On-run-end alone cannot ensure the parameter is set for the entire run.\n- Option D: Exposures document data products, not runtime session changes.\n\n## Key Concepts\n- Hooks (on-run-start, on-run-end)\n- Session parameters and schema switching\n- Run-time isolation\n\n## Real-World Application\n- Isolates environments for testing or environment-specific pipelines without impacting other runs.","diagram":null,"difficulty":"intermediate","tags":["dbt","dbt-hooks","AWS","Redshift","certification-mcq","domain-weight-15"],"channel":"dbt-analytics-engineer","subChannel":"advanced-features","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T21:25:37.803Z","createdAt":"2026-01-12 21:25:37"},{"id":"dbt-analytics-engineer-data-modeling-1768210358285-0","question":"A retail company wants to preserve historical changes to customer segments (e.g., Silver, Gold) tied to a customer profile. The daily feed updates the master customer table with new segment values. Which approach best implements Slowly Changing Dimensions Type 2 in dbt to retain history and enable time-travel in BI dashboards?","answer":"[{\"id\":\"a\",\"text\":\"Use dbt snapshots on the customer_dim to capture history and output a history-enabled table with start_date and end_date.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Use a dedicated ETL job that appends to a separate history table on every change, without using dbt snapshots.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Rely on the source system's CDC and refresh the entire dimension table daily, discarding history on restatement.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Overwrite the dimension table with only the latest segment value, losing history.\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nUse dbt snapshots on the customer_dim to capture history and output a history-enabled table with start_date and end_date.\n\n## Why Other Options Are Wrong\n\n- Option B is incorrect because it bypasses dbt snapshots and risks maintaining history in an unmanaged, inconsistent way.\n- Option C is incorrect because relying on CDC without built-in versioning can miss past states and breaks the single source of truth for history.\n- Option D is incorrect because overwriting loses history, preventing time-travel in BI dashboards.\n\n## Key Concepts\n\n- Slowly Changing Dimensions Type 2\n- dbt snapshots\n- History tables\n- Data lineage\n\n## Real-World Application\n\nWhen a customer moves from Silver to Gold, a snapshot records a new historical row with its own validity window, enabling dashboards to show the customer segment at any point in time.","diagram":null,"difficulty":"intermediate","tags":["dbt","data-modeling","data-warehousing","aws-redshift","eks","terraform","certification-mcq","domain-weight-25"],"channel":"dbt-analytics-engineer","subChannel":"data-modeling","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T09:32:38.286Z","createdAt":"2026-01-12 09:32:38"},{"id":"dbt-analytics-engineer-data-modeling-1768210358285-1","question":"In a data model where a Customer can have multiple preferences (for example, email, SMS, push) and you want to avoid denormalization while maintaining a clean many-to-many relationship, which modeling approach should you use in dbt?","answer":"[{\"id\":\"a\",\"text\":\"Create a bridge table customer_preferences with customer_id and preference_id and join in queries.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Duplicate customer rows for each preference in the customer_dim.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Store preferences as a JSON array in customer_dim and parse in queries.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Add a new column on customer_dim with a list of preferences as a string.\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nCreate a bridge table customer_preferences with customer_id and preference_id and join in queries.\n\n## Why Other Options Are Wrong\n\n- Option B duplicating customer rows increases dimensionality, harms readability, and complicates joins.\n- Option C storing as JSON reduces queryability and hurts standard BI tooling compatibility.\n- Option D storing a list in a single column complicates filtering and analytics and violates normalization principles.\n\n## Key Concepts\n\n- Many-to-many relationships\n- Bridge (link) tables\n- Normalized dimensional modeling\n\n## Real-World Application\n\nAllows analytics to answer questions like which customers have multiple preferences without duplicating customer records across facts.","diagram":null,"difficulty":"intermediate","tags":["dbt","data-modeling","data-warehousing","aws-redshift","terraform","eks","certification-mcq","domain-weight-25"],"channel":"dbt-analytics-engineer","subChannel":"data-modeling","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T09:32:38.823Z","createdAt":"2026-01-12 09:32:39"},{"id":"dbt-analytics-engineer-data-modeling-1768210358285-2","question":"You need to enforce durable surrogate keys in a dimensional model built with dbt across incremental loads and late-arriving data. Which practice provides stable surrogate keys that can be reconstructed if needed?","answer":"[{\"id\":\"a\",\"text\":\"Use an auto-increment surrogate key in the target data warehouse.\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use a deterministic surrogate key derived from hashing the business key to ensure stability.\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Use a composite surrogate key built from all attributes of the dimension row.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Generate surrogate keys in the source and pass-through to the target.\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nUse a deterministic surrogate key derived from hashing the business key to ensure stability across incremental loads and late-arriving data. This provides a durable, reproducible key that does not depend on load order or data arrival timing.\n\n## Why Other Options Are Wrong\n\n- Option A is incorrect because auto-increment keys can shift when data is reloaded or merged from multiple sources, breaking reproducibility.\n- Option C is incorrect because a composite key from all attributes is heavy and fragile to changes in non-key attributes.\n- Option D is incorrect because generating keys in the source reduces control and can lead to mismatches if source isn’t fully trusted or replicated.\n\n## Key Concepts\n\n- Surrogate keys\n- Deterministic hashing for keys\n- Data lineage and reproducibility\n\n## Real-World Application\n\nHashing the business key (plus a version indicator if needed) yields stable surrogate keys that remain consistent across full or partial reloads, enabling reliable slowly changing dimension handling.","diagram":null,"difficulty":"intermediate","tags":["dbt","data-modeling","data-warehousing","aws-redshift","terraform","eks","certification-mcq","domain-weight-25"],"channel":"dbt-analytics-engineer","subChannel":"data-modeling","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T09:32:39.359Z","createdAt":"2026-01-12 09:32:39"},{"id":"dbt-analytics-engineer-data-modeling-1768293347975-0","question":"In a star schema with fact_sales and dimension tables, you want to preserve history for customer changes without updating existing fact rows. What modeling pattern should you implement and how should you wire the keys in the fact table?","answer":"[{\"id\":\"a\",\"text\":\"Rely on natural business keys in both dimension and fact tables and slowly update the facts on each change.\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Create a surrogate key for each dimension and store it as a foreign key in the fact table, while implementing SCD Type 2 in the dimensions.\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Use a single global surrogate key for all dimensions in the fact table.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Flatten all history into the fact table by duplicating dimension attributes.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B. Use a surrogate key for each dimension and store it as a foreign key in the fact table, while implementing SCD Type 2 in the dimensions to preserve history. This keeps facts lean and preserves dimensional history via slowly changing dimensions.\n\n## Why Other Options Are Wrong\n- Option A: Relies on natural business keys and updates facts; this loses historical context and can create FK churn.\n- Option C: A single global surrogate key cannot uniquely identify diverse dimensions for joins.\n- Option D: Duplicating dimension attributes in the fact breaks normalization and complicates history management.\n\n## Key Concepts\n- Surrogate keys in dimensions\n- SCD Type 2 history preservation\n- Star schema best practices\n\n## Real-World Application\n- You can implement this in dbt by creating per-dimension surrogate keys, storing them in the fact table, and modeling Type 2 history in the dimension tables, then referencing the surrogate keys in joins.","diagram":null,"difficulty":"intermediate","tags":["dbt","data-modeling","SCD-2","surrogate-keys","dbt-snapshots","Snowflake","BigQuery","Redshift","data-warehouse","ETL","certification-mcq","domain-weight-25"],"channel":"dbt-analytics-engineer","subChannel":"data-modeling","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T08:35:47.976Z","createdAt":"2026-01-13 08:35:48"},{"id":"dbt-analytics-engineer-data-modeling-1768293347975-1","question":"In a data model with multiple date fields such as order_date and ship_date, what modeling approach minimizes dimension growth while enabling consistent time-based analysis?","answer":"[{\"id\":\"a\",\"text\":\"Create separate date dimension tables for each date field\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use a single conformed date dimension and reference it from all facts\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Store date fields as strings in the fact table\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Create a distinct date dimension per fact\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B. Use a single conformed date dimension and reference it from all facts. This avoids proliferating date dimensions and enables consistent time-based analysis across multiple facts.\n\n## Why Other Options Are Wrong\n- Option A: Duplicates date logic per date field and increases maintenance.\n- Option C: Storing dates as strings impairs time intelligence and joins.\n- Option D: A separate date dimension per fact leads to fragmentation and poor join performance.\n\n## Key Concepts\n- Conformed dimensions\n- Shared date dimension\n- Time intelligence across facts\n\n## Real-World Application\n- In dbt, create a central date dimension (e.g., dim_date) and reference its surrogate key in all fact tables.","diagram":null,"difficulty":"intermediate","tags":["dbt","data-modeling","date-dimension","dim_date","Snowflake","BigQuery","Redshift","data-warehouse","certification-mcq","domain-weight-25"],"channel":"dbt-analytics-engineer","subChannel":"data-modeling","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T08:35:48.486Z","createdAt":"2026-01-13 08:35:48"},{"id":"dbt-analytics-engineer-data-modeling-1768293347975-2","question":"You are modeling product pricing history and need to query the current price as of a given date; which modeling approach is best?","answer":"[{\"id\":\"a\",\"text\":\"Maintain price history in a Type 2 dimension with start_date and end_date and a current flag, then query by as-of date\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Overwrite previous price with a Type 1 update\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Store price in the fact table and do not track history\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Store all price values in a single row as an array\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A. Model price history as a Type 2 dimension with start_date, end_date, and a current flag to enable as-of-date queries while preserving history.\n\n## Why Other Options Are Wrong\n- Option B: Type 1 overwrites history, losing past price context.\n- Option C: Pricing in the fact without history loses historical analysis ability.\n- Option D: An array is not a reliable way to query as-of-date prices and complicates joins.\n\n## Key Concepts\n- SCD Type 2 for pricing\n- Valid date-range attributes\n- As-of date querying\n\n## Real-World Application\n- dbt models can implement price dimension with versioned rows and a current flag, enabling accurate price at any date.","diagram":null,"difficulty":"intermediate","tags":["dbt","data-modeling","SCD-2","price-history","dim_price","Snowflake","BigQuery","Redshift","data-warehouse","certification-mcq","domain-weight-25"],"channel":"dbt-analytics-engineer","subChannel":"data-modeling","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T08:35:48.974Z","createdAt":"2026-01-13 08:35:49"},{"id":"dbt-analytics-engineer-data-modeling-1768293347975-3","question":"To enforce referential integrity across fact and dimension tables in dbt, which test should you implement?","answer":"[{\"id\":\"a\",\"text\":\"not_null test on foreign keys\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"unique test on foreign keys\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"relationships test linking the foreign key to the dimension key\",\"isCorrect\":true},{\"id\":\"d\",\"text\":\"accepted_values test on the dimension key\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption C. The relationships test validates that each foreign key in the fact table references a valid primary key in the corresponding dimension, enforcing referential integrity.\n\n## Why Other Options Are Wrong\n- Option A: not_null prevents nulls but does not verify the existence of a matching dimension row.\n- Option B: unique test on a foreign key is inappropriate and may violate valid many-to-one relationships.\n- Option D: accepted_values doesn't verify cross-table references.\n\n## Key Concepts\n- dbt relationships test\n- Referential integrity\n- Foreign keys validation\n\n## Real-World Application\n- Use relationships tests in dbt to catch orphaned FKs during CI checks.","diagram":null,"difficulty":"intermediate","tags":["dbt","data-modeling","relationships","foreign-keys","Snowflake","BigQuery","Redshift","data-warehouse","certification-mcq","domain-weight-25"],"channel":"dbt-analytics-engineer","subChannel":"data-modeling","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T08:35:49.134Z","createdAt":"2026-01-13 08:35:49"},{"id":"dbt-analytics-engineer-data-modeling-1768293347975-4","question":"Which dbt feature is ideal to capture and maintain changes in a slowly changing dimension over time?","answer":"[{\"id\":\"a\",\"text\":\"dbt snapshots\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"dbt seeds\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"dbt sources\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"dbt tests\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A. dbt snapshots are designed to capture and maintain history of slowly changing dimensions by recording changes over time.\n\n## Why Other Options Are Wrong\n- Option B: Seeds are static CSV-like inputs for loading reference data.\n- Option C: Sources define external data tables; they don't track history automatically.\n- Option D: Tests validate data quality but do not persist historical changes.\n\n## Key Concepts\n- SCD implementation in dbt\n- Db snapshots functionality\n- History tracking\n\n## Real-World Application\n- Use dbt snapshots to automatically version dimension rows when their business keys or attributes change.","diagram":null,"difficulty":"intermediate","tags":["dbt","data-modeling","snapshots","SCD-2","dbt-cloud","Snowflake","BigQuery","Redshift","data-warehouse","certification-mcq","domain-weight-25"],"channel":"dbt-analytics-engineer","subChannel":"data-modeling","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T08:35:49.299Z","createdAt":"2026-01-13 08:35:49"},{"id":"dbt-analytics-engineer-dbt-fundamentals-1768170078445-0","question":"In a dbt project, you maintain a large fact table with an incremental model keyed by order_id. New data fills daily, but some existing orders have updates. Which approach best ensures both new inserts and updates are reflected without full refresh?","answer":"[{\"id\":\"a\",\"text\":\"Implement a MERGE-based upsert inside the incremental block to handle updates and inserts\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Rely on dbt's default incremental behavior using only simple insert for new rows\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use a full_refresh on every run to guarantee correctness\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use a separate snapshot model to track changes\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nA. Implement a MERGE-based upsert inside the incremental block to handle updates and inserts.\n\n## Why Other Options Are Wrong\n\n- B: The default incremental behavior inserts only new rows and does not handle updates, which can lead to duplicates or stale data.\n- C: Full refresh rebuilds the entire table every run, negating the benefits of incremental loads and consuming unnecessary compute.\n- D: Snapshots track history for slowly changing dimensions and are not a substitute for upsert logic in incremental fact tables.\n\n## Key Concepts\n\n- Incremental models can perform upserts with MERGE to handle updates.\n- is_incremental() pattern guides conditional SQL in dbt.\n\n## Real-World Application\n\n- When feeding daily operational data into large fact tables, implementing a MERGE-based upsert in the incremental path ensures data accuracy while avoiding costly full reloads.","diagram":null,"difficulty":"intermediate","tags":["dbt","AWS","Snowflake","Terraform","Kubernetes","certification-mcq","domain-weight-25"],"channel":"dbt-analytics-engineer","subChannel":"dbt-fundamentals","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T22:21:18.446Z","createdAt":"2026-01-11 22:21:18"},{"id":"dbt-analytics-engineer-dbt-fundamentals-1768170078445-1","question":"You want to enforce referential integrity between a fact table and a date dimension in dbt. You need to prevent orphaned records. Which approach is most appropriate in dbt?","answer":"[{\"id\":\"a\",\"text\":\"Create a relationships test on the foreign key to the date_dim table\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Use a freshness test on the date dimension to ensure it's up to date\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Create a snapshot to capture historical changes\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use a seed to load fixed date_dim data\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nA. Create a relationships test on the foreign key to the date_dim table.\n\n## Why Other Options Are Wrong\n\n- B: Freshness tests ensure the data source is up-to-date, not referential integrity between tables.\n- C: Snapshots track history in slowly changing dimensions, not enforce foreign-key referential integrity.\n- D: Seeds populate static data but do not validate relationships between existing tables.\n\n## Key Concepts\n\n- dbt tests can enforce referential integrity via relationships tests.\n- Relationships tests help catch orphaned records during CI.\n\n## Real-World Application\n\n- Adding a relationships test helps guard against data quality regressions when new foreign-key relationships are introduced or when date_dim is updated.","diagram":null,"difficulty":"intermediate","tags":["dbt","AWS","Redshift","Kubernetes","Terraform","certification-mcq","domain-weight-25"],"channel":"dbt-analytics-engineer","subChannel":"dbt-fundamentals","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T22:21:18.936Z","createdAt":"2026-01-11 22:21:19"},{"id":"dbt-analytics-engineer-dbt-fundamentals-1768170078445-2","question":"During deployment to Snowflake, you want incremental models to load only new or changed rows rather than rewriting entire tables to optimize compute. Which strategy best achieves this in dbt?","answer":"[{\"id\":\"a\",\"text\":\"Implement upsert logic in the incremental block using MERGE statements\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Force full_refresh on every deployment to ensure consistency\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Convert incremental models to views to leverage dynamic querying\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use dbt test to enforce incremental semantics during runs\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nA. Implement upsert logic in the incremental block using MERGE statements.\n\n## Why Other Options Are Wrong\n\n- B: Full_refresh negates the benefits of incremental loads and is compute-intensive.\n- C: Views always recompute on every query and do not persist transformed data like incremental tables.\n- D: Tests validate data quality but do not control how data is written during runs.\n\n## Key Concepts\n\n- MERGE-based upserts in incremental dbt models for upserts.\n- Snowflake supports MERGE in dbt incremental implementations.\n\n## Real-World Application\n\n- In a Snowflake-based analytics stack, using MERGE in incremental paths lets you add new rows and update existing ones efficiently, reducing compute and improving freshness.","diagram":null,"difficulty":"intermediate","tags":["dbt","Snowflake","AWS","Kubernetes","Terraform","certification-mcq","domain-weight-25"],"channel":"dbt-analytics-engineer","subChannel":"dbt-fundamentals","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T22:21:19.427Z","createdAt":"2026-01-11 22:21:19"},{"id":"dbt-analytics-engineer-dbt-fundamentals-1768275249403-0","question":"You’re building an incremental model in dbt on Snowflake. You need to handle late-arriving data and prevent duplicates. Which approach aligns best with dbt best practices?","answer":"[{\"id\":\"a\",\"text\":\"Use incremental materialization with a unique_key defined so dbt uses MERGE to upsert on duplicates.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Always run a full-refresh to ensure correctness.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Rely on Snowflake constraints to deduplicate during inserts.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use a separate staging table and a Python snippet to deduplicate offline.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A is correct because when a unique_key is defined for an incremental model, dbt uses a MERGE-based upsert strategy on engines that support it (e.g., Snowflake). This handles late-arriving data and prevents duplicates.\n\n## Why Other Options Are Wrong\n- Option B: Full-refresh rebuilds the model every run and is costly; it does not address late-arriving data efficiently.\n- Option C: Snowflake constraints alone do not automatically deduplicate on insert; they enforce constraints but do not perform upserts.\n- Option D: Introducing a manual Python dedup pipeline adds complexity and breaks dbt’s single-source-of-truth workflow.\n\n## Key Concepts\n- Incremental models and unique_key upserts\n- MERGE-based upsert capability in adapters like Snowflake\n- Handling late-arriving data within dbt\n\n## Real-World Application\nTeams using Snowflake often rely on incremental models with unique keys to keep large fact tables up-to-date without reprocessing the entire dataset, ensuring data freshness with controlled compute costs.","diagram":null,"difficulty":"intermediate","tags":["dbt","dbt-fundamentals","Snowflake","Redshift","Kubernetes","Terraform","AWS","BigQuery","Airflow","dbt-cloud","certification-mcq","domain-weight-25"],"channel":"dbt-analytics-engineer","subChannel":"dbt-fundamentals","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T03:34:09.404Z","createdAt":"2026-01-13 03:34:09"},{"id":"dbt-analytics-engineer-dbt-fundamentals-1768275249403-1","question":"In a dbt project, you need to enforce referential integrity between orders and customers. Which dbt test enforces this at load time?","answer":"[{\"id\":\"a\",\"text\":\"not_null on orders.customer_id\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"unique on orders.customer_id\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"relationships test between orders.customer_id and dim_customers.id\",\"isCorrect\":true},{\"id\":\"d\",\"text\":\"accepted_values on orders.customer_id\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption C is correct because a relationships test validates that every value in orders.customer_id exists in dim_customers.id, enforcing referential integrity across the data warehouse.\n\n## Why Other Options Are Wrong\n- Option A: not_null only ensures presence of a value, not referential integrity.\n- Option B: unique ensures each customer_id is unique, not that it references a valid customer.\n- Option D: accepted_values restricts to a predefined set, which does not guarantee existence in the customers dimension.\n\n## Key Concepts\n- dbt relationships tests\n- Referential integrity between facts and dimensions\n- Test placement in schema.yml for models\n\n## Real-World Application\nThis test helps prevent orphaned records in orders and ensures downstream analytics aren’t built on invalid foreign keys.","diagram":null,"difficulty":"intermediate","tags":["dbt","dbt-fundamentals","Snowflake","BigQuery","Redshift","Airflow","Kubernetes","Terraform","AWS","Postgres","certification-mcq","domain-weight-25"],"channel":"dbt-analytics-engineer","subChannel":"dbt-fundamentals","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T03:34:09.916Z","createdAt":"2026-01-13 03:34:10"},{"id":"dbt-analytics-engineer-dbt-fundamentals-1768275249403-2","question":"You want to run only a subset of models that are tagged with 'staging' in your dbt project as part of a nightly pipeline. Which command best accomplishes this using selectors and tags?","answer":"[{\"id\":\"a\",\"text\":\"dbt run --models path:shared/models/*\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"dbt run --models tag:staging\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"dbt run --full-refresh --models staging\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"dbt run --models exclude:staging\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct because dbt selectors support filtering by tags (e.g., tag:staging) to run only models with that tag, which is ideal for targeted nightly runs.\n\n## Why Other Options Are Wrong\n- Option A: Uses a path-based filter, not a tag selector.\n- Option C: Combines full-refresh with a tag, which is unnecessary for subset execution and changes run semantics.\n- Option D: Excludes staging instead of selecting it.\n\n## Key Concepts\n- dbt selectors and tagging\n- Subsetting runs for nightly pipelines\n- Parallel execution considerations with targeted models\n\n## Real-World Application\nTeams use tag-based selectors to stabilize nightly pipelines by isolating staging models, reducing runtime and allowing safe validation before promoting to production paths.","diagram":null,"difficulty":"intermediate","tags":["dbt","dbt-fundamentals","Snowflake","BigQuery","Airflow","Kubernetes","Terraform","AWS","Docker","certification-mcq","domain-weight-25"],"channel":"dbt-analytics-engineer","subChannel":"dbt-fundamentals","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T03:34:10.459Z","createdAt":"2026-01-13 03:34:10"},{"id":"dbt-analytics-engineer-dbt-fundamentals-1768275249403-3","question":"dbt snapshots are used to track historical changes to a record in a source table. Which statement best describes their configuration?","answer":"[{\"id\":\"a\",\"text\":\"A snapshot requires a source table, a unique key, and a chosen strategy (check or timestamp) to detect changes.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Snapshots automatically deduplicate rows without any configuration.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Snapshots are materialized views that refresh nightly by default.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Snapshots replace the base table with a versioned copy on every run.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A is correct because dbt snapshots require a source, a primary key (unique key), and a change-detection strategy (check or timestamp) to capture history when changes occur.\n\n## Why Other Options Are Wrong\n- Option B: Snapshots do not automatically deduplicate; they track changes based on configuration.\n- Option C: Snapshots are not automatically created as materialized views; they produce separate tables with historical data.\n- Option D: Snapshots preserve historical rows rather than replacing the base table unless explicitly designed to do so.\n\n## Key Concepts\n- Snapshot configuration in dbt\n- Unique key and change-detection strategy\n- Historical data capture for slowly changing dimensions\n\n## Real-World Application\nUse snapshots to preserve historical changes in attribute dimensions (e.g., customer name/address) for audit trails and historical reporting.","diagram":null,"difficulty":"intermediate","tags":["dbt","dbt-fundamentals","Snowflake","BigQuery","Redshift","Airflow","Kubernetes","Terraform","AWS","Postgres","certification-mcq","domain-weight-25"],"channel":"dbt-analytics-engineer","subChannel":"dbt-fundamentals","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T03:34:10.640Z","createdAt":"2026-01-13 03:34:10"},{"id":"dbt-analytics-engineer-dbt-fundamentals-1768275249403-4","question":"To keep seed data consistent across environments, what is the recommended practice?","answer":"[{\"id\":\"a\",\"text\":\"Store seed data as CSV files and load via dbt seed with schema tests; run dbt seed --full-refresh when data changes.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Convert seeds into models and rely on data tests in the models.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Rely on database constraints to enforce seed integrity.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Skip tests for seeds to reduce pipeline overhead.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A is correct because seeds are loaded from CSVs via dbt seed; defining schema tests ensures the seed data remains valid across environments, and using --full-refresh reloads seed data when the source CSV changes.\n\n## Why Other Options Are Wrong\n- Option B: Seeds are best kept as seed files; converting to models adds unnecessary indirection.\n- Option C: Database constraints alone do not validate the seed contents in all environments.\n- Option D: Skipping tests for seeds reduces data integrity checks and is not best practice.\n\n## Key Concepts\n- dbt seed workflow\n- Schema tests on seeds\n- Full-refresh behavior for seeds\n\n## Real-World Application\nTeams rely on seed tests and controlled reloads to ensure reference data remains consistent across development, staging, and production environments.","diagram":null,"difficulty":"intermediate","tags":["dbt","dbt-fundamentals","Snowflake","BigQuery","Redshift","Airflow","Kubernetes","Terraform","AWS","S3","certification-mcq","domain-weight-25"],"channel":"dbt-analytics-engineer","subChannel":"dbt-fundamentals","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T03:34:10.821Z","createdAt":"2026-01-13 03:34:10"},{"id":"dbt-analytics-engineer-deployment-environments-1768239080794-0","question":"In a multi-environment deployment for a dbt project, you want to promote changes from development to staging to production using a single profiles.yml with environment targets. Which approach provides the most robust isolation and seamless promotion?","answer":"[{\"id\":\"a\",\"text\":\"Maintain a single profiles.yml with multiple targets (dev, staging, prod) and select --target per CI run\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Create separate profiles.yml files per environment and switch DBT_PROFILES_DIR in CI\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Store credentials in pipeline environment variables and reuse the same target\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Copy artifacts between environments manually as part of each release\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA. The recommended approach is to define per-environment targets within a single profiles.yml and select the appropriate target in CI using --target. This keeps configuration in one place and prevents drift across environments.\n\n## Why Other Options Are Wrong\n- B duplicates profile management and increases maintenance burden, risking drift.\n- C moves credentials into the pipeline, which can lead to credential leakage and inconsistent access controls.\n- D relies on manual artifact transfer, which is error-prone and non-reproducible.\n\n## Key Concepts\n- dbt profiles and targets\n- CI/CD promotion gates\n\n## Real-World Application\n- Used when designing a CI/CD pipeline to promote dbt changes across dev, staging, and prod with clear environment boundaries.","diagram":null,"difficulty":"intermediate","tags":["Terraform","AWS","dbt","CI/CD","certification-mcq","domain-weight-15"],"channel":"dbt-analytics-engineer","subChannel":"deployment-environments","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T17:31:20.795Z","createdAt":"2026-01-12 17:31:21"},{"id":"dbt-analytics-engineer-deployment-environments-1768239080794-1","question":"You are deploying a new data processing service to a Kubernetes cluster on AWS EKS. You want to roll it out with a canary strategy and progressive traffic shifting. Which deployment pattern should you use?","answer":"[{\"id\":\"a\",\"text\":\"Use a Deployment with a canary strategy and traffic splitting via a service mesh\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Use a Job for the new version and scale the old version down\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use a StatefulSet and rely on Kubernetes rolling update for traffic\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use a DaemonSet to run a copy on every node\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA. A Deployment with a canary strategy and traffic splitting (via Istio, App Mesh, or similar) enables progressive rollout and safe validation of the new version before full production.\n\n## Why Other Options Are Wrong\n- B is not suitable for traffic-shift canary patterns and would replace the old version abruptly.\n- C StatefulSets do not inherently support traffic splitting during updates and are not ideal for canaries.\n- D DaemonSets ensure a pod on every node, which is unrelated to canary traffic controls.\n\n## Key Concepts\n- Kubernetes canary deployments\n- Traffic splitting with service meshes\n\n## Real-World Application\n- Gradual rollout of a new data processing service in an EKS cluster with monitored metrics before full rollout.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","EKS","App Mesh","certification-mcq","domain-weight-15"],"channel":"dbt-analytics-engineer","subChannel":"deployment-environments","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T17:31:21.341Z","createdAt":"2026-01-12 17:31:21"},{"id":"dbt-analytics-engineer-deployment-environments-1768239080794-2","question":"For AWS infrastructure managed with Terraform, you need isolated state per environment to prevent drift and accidental cross-environment changes. Which pattern best achieves this without duplicating code?","answer":"[{\"id\":\"a\",\"text\":\"Use separate Terraform workspaces for each environment, with a shared configuration\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Use a single large tfvars file with all environment values\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use a single backend state file shared across environments\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Keep credentials embedded in the Terraform code\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA. Separate Terraform workspaces provide isolated state per environment while sharing the same configuration, reducing duplication and drift risk.\n\n## Why Other Options Are Wrong\n- B increases risk of accidental cross-environment changes and leakage of environment-specific values.\n- C couples environments through a single state, enabling drift and cross-environment impact.\n- D exposes credentials and sensitive data in code.\n\n## Key Concepts\n- Terraform workspaces\n- Environment isolation and remote backends\n\n## Real-World Application\n- Managing dev, staging, and prod infra with isolated state but a single configuration.","diagram":null,"difficulty":"intermediate","tags":["Terraform","AWS","S3","certification-mcq","domain-weight-15"],"channel":"dbt-analytics-engineer","subChannel":"deployment-environments","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T17:31:21.933Z","createdAt":"2026-01-12 17:31:22"},{"id":"dbt-analytics-engineer-deployment-environments-1768239080794-3","question":"In an AWS CodePipeline-based CI/CD for dbt artifacts, you want to prevent production deployment until staging validation passes. Which stage arrangement implements the correct gating?","answer":"[{\"id\":\"a\",\"text\":\"Stage for build, stage for unit tests, stage for staging deployment\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Stage for build, stage for unit tests, stage for staging deployment, stage for manual approval before production deployment\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Stage for build only\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Stage for canary testing in production\",\"isCorrect\":false}]","explanation":"## Correct Answer\nB. Include a manual approval stage between staging deployment and production deployment to gate production pushes behind validation results.\n\n## Why Other Options Are Wrong\n- A lacks an explicit gating step before production.\n- C provides no validation or promotion flow.\n- D deploys directly to production without staging checks.\n\n## Key Concepts\n- AWS CodePipeline stages\n- Manual approval gates\n\n## Real-World Application\n- Enforces policy-based promotion of dbt artifacts from staging to production.","diagram":null,"difficulty":"intermediate","tags":["AWS CodePipeline","CodeBuild","IAM","certification-mcq","domain-weight-15"],"channel":"dbt-analytics-engineer","subChannel":"deployment-environments","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T17:31:22.125Z","createdAt":"2026-01-12 17:31:22"},{"id":"dbt-analytics-engineer-deployment-environments-1768239080794-4","question":"In a Kubernetes-based deployment, you want environment-specific configuration loaded at runtime and not baked into container images to avoid drift between environments. Which approach achieves this?","answer":"[{\"id\":\"a\",\"text\":\"Bake environment variables into the container image\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use ConfigMaps and Secrets mounted into pods for runtime configuration\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Store static config files inside the image and mount them as read-only\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Maintain a separate Dockerfile per environment with in-image config changes\",\"isCorrect\":false}]","explanation":"## Correct Answer\nB. ConfigMaps and Secrets provide runtime, environment-specific configuration without embedding values in images, reducing drift and enabling easy updates.\n\n## Why Other Options Are Wrong\n- A embeds secrets and config into the image, increasing drift risk when envs change.\n- C fixes config inside the image, hindering environment-specific customization.\n- D duplicates images and increases maintenance without real benefit.\n\n## Key Concepts\n- Kubernetes ConfigMap and Secret usage\n- Runtime configuration vs image baked config\n\n## Real-World Application\n- Deploying apps across dev/stage/prod with distinct configuration without rebuilding images.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","ConfigMap","Secrets","certification-mcq","domain-weight-15"],"channel":"dbt-analytics-engineer","subChannel":"deployment-environments","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T17:31:22.305Z","createdAt":"2026-01-12 17:31:22"},{"id":"q-1125","question":"You're building an incremental dbt model analytics.daily_revenue_by_product sourced from staging.sales_raw. Data can arrive late up to 2 days. How would you implement incremental upserts, reprocess late days without disturbing older data, and validate with tests and a snapshot of product price history? Provide a minimal SQL snippet illustrating the approach?","answer":"Implement an incremental model with unique_key=['day','product_id','region']; reprocess the last 2 days during incremental runs to absorb late data; use MERGE/upsert where supported, else a two-phase ","explanation":"## Why This Is Asked\nTests practical incremental logic and late-arrival handling; checks upsert patterns, tests, and snapshots.\n\n## Key Concepts\n- Incremental materialization with unique keys\n- Late-arrival handling window\n- Snapshot for price history\n- Basic tests for data quality\n\n## Code Example\n```sql\n-- models/analytics/daily_revenue_by_product.sql\n{{ config(materialized='incremental', unique_key=['day','product_id','region']) }}\n\nwith s as (\n  select date_trunc('day', order_date) as day,\n         product_id,\n         region,\n         sum(quantity * price) as revenue\n  from {{ ref('staging_sales_raw') }}\n  group by 1,2,3\n)\n\nselect day, product_id, region, revenue\nfrom s\n{% if is_incremental() %}\n  -- reprocess last 2 days to capture late data\n  where day >= (select max(day) from {{ this }}) - interval '2 day'\n{% endif %}\n```\n\n## Follow-up Questions\n- How would you adapt this for Redshift vs Snowflake?\n- How would you test the snapshot consistency across environments?","diagram":null,"difficulty":"beginner","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Hashicorp"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T23:31:34.187Z","createdAt":"2026-01-12T23:31:34.187Z"},{"id":"q-1165","question":"You're designing a dbt analytics pipeline for a ride-sharing platform. The raw events are in `staging.events_raw` with columns like `ride_id`, `city`, `ride_type`, `currency`, `amount`, `occurred_at`, and data can arrive up to 48 hours late. Build a beginner-friendly incremental model `analytics.daily_revenue` that computes USD revenue per day by `date`, `city`, and `ride_type`. Use a daily `pricing.exchange_rates` table to convert from local currency to USD. Describe how you'd implement incremental upserts, late-data handling (2-day window), schema-drift guards, and validation with tests and a snapshot. Also outline tests for late refunds and missing rates?","answer":"Use an incremental model with is_incremental(). Join staging events to pricing.exchange_rates on date and currency, then aggregate revenue in USD by date, city, ride_type. Implement a 2-day late-data ","explanation":"## Why This Is Asked\nThe question probes practical incremental modeling with late data, currency conversion, and drift safeguards—core dbt skills at junior to mid-beginner level.\n\n## Key Concepts\n- Incremental models and upserts\n- Currency conversion via reference table\n- Late-data window handling\n- Schema-drift guards and tests\n- Snapshots for drift detection\n\n## Code Example\n```sql\n-- dbt model: analytics/daily_revenue.sql\nwith events as (\n  select\n    date_trunc('day', occurred_at) as date,\n    city,\n    ride_type,\n    currency,\n    amount\n  from {{ source('staging','events_raw') }}\n  where occurred_at <= (current_timestamp() - interval '2 days')\n),\nrates as (\n  select date, currency, rate\n  from {{ source('pricing','exchange_rates') }}\n)\nselect\n  e.date,\n  e.city,\n  e.ride_type,\n  sum(e.amount * coalesce(r.rate, 1.0)) as revenue_usd\nfrom events e\nleft join rates r\n  on r.date = e.date and r.currency = e.currency\ngroup by 1,2,3\n```\n\n## Follow-up Questions\n- How would you handle gaps in exchange_rates (missing rates)?\n- How would you validate late refunds affecting revenue?\n- How would you test for time zone consistency across cities?","diagram":null,"difficulty":"beginner","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T03:31:16.675Z","createdAt":"2026-01-13T03:31:16.675Z"},{"id":"q-1257","question":"Design a drift-aware dbt workflow across dev/staging/prod in Snowflake. How would you detect schema drift between sources and models using snapshots, tests, and sources, enforce a drift threshold, and automatically fail CI when drift exceeds the threshold? Describe the macro, tests, and alert/rollback mechanism, plus how you version thresholds?","answer":"Implement a dbt macro drift_check that compares per-column metadata (data type, nullability, default, and max length) between a source table and its corresponding model, plus a runtime comparison of r","explanation":"## Why This Is Asked\nReal-world pipelines require automated detection of schema drift to prevent deployment of broken analytics. This question tests practical macro design, test coverage, and CI/CD integration.\n\n## Key Concepts\n- dbt snapshots and sources for metadata\n- Per-column drift checks (type, nullability, max length)\n- Threshold-driven gating in CI/CD and rollback policies\n- Canary prod lineage validation and alerting\n\n## Code Example\n```jinja\n{% macro drift_check(source_schema, source_table, model_schema, model_table, thresholds) -%}\n-- pseudo-logic: compare metadata and basic stats, emit drift_count if > thresholds\n{%- endmacro %}\n```\n\n## Follow-up Questions\n- How would you scale per-table thresholds across hundreds of tables?\n- How would you distinguish benign drift (e.g., new optional column) from breaking drift?","diagram":null,"difficulty":"advanced","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Google","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T07:23:31.079Z","createdAt":"2026-01-13T07:23:31.079Z"},{"id":"q-848","question":"You're designing a dbt analytics pipeline for an e-commerce platform. The 'staging.events_raw' table ingests page views and purchases and is partitioned by day. Data arrives both on time and as late as 24 hours. Design a practical incremental dbt model 'analytics.daily_sales' that aggregates revenue by day, category, and customer_segment. Explain how you'd implement incremental logic, handle late data, guard against schema drift, and validate with tests and snapshots?","answer":"Use an incremental model keyed by day, category, and customer_segment. Compute revenue as sum(price*quantity) from staging.events_raw. Upsert into analytics.daily_sales via MERGE on (day, category, cu","explanation":"## Why This Is Asked\n\nAssess ability to design scalable, fault-tolerant dbt pipelines handling late data and schema drift.\n\n## Key Concepts\n\n- Incremental models and idempotent MERGE\n- Late-arrival handling and backfill windows\n- Schema drift detection via tests and snapshots\n- Performance at scale and CI validation\n\n## Code Example\n\n```sql\n-- Example incremental sql sketch\nWITH s AS (\n  SELECT day, category, customer_segment,\n         SUM(price * quantity) AS revenue\n  FROM {{ ref('staging__events_raw') }}\n  WHERE day >= (SELECT MIN(day) FROM analytics.daily_sales)\n  GROUP BY day, category, customer_segment\n)\nMERGE INTO analytics.daily_sales AS t\nUSING s AS s\nON t.day = s.day AND t.category = s.category AND t.customer_segment = s.customer_segment\nWHEN MATCHED THEN UPDATE SET revenue = s.revenue\nWHEN NOT MATCHED THEN INSERT (day, category, customer_segment, revenue) VALUES (s.day, s.category, s.customer_segment, s.revenue);\n```\n\n## Follow-up Questions\n\n- How would you test for idempotency and late-arrival correctness?\n- How would you adapt this for a lakehouse (Parquet) or streaming source?","diagram":null,"difficulty":"advanced","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Anthropic","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:30:20.377Z","createdAt":"2026-01-12T13:30:20.377Z"},{"id":"q-972","question":"In a dbt analytics project deployed to Snowflake, three tenants share a common model but data is isolated by schema prefixes (tenant_a_, tenant_b_, tenant_c_). Describe how you would structure tenancy-aware tests and a test runner to prevent cross-tenant data leakage during PR runs. Include how you configure sources, seeds, and per-tenant test filtering, and how you verify isolation in CI without touching production data?","answer":"A tenancy-aware dbt run parameterizes the target schema per tenant, isolates seeds and sources with tenant prefixes, and uses a tenancy-aware test macro to scope checks. In CI, create ephemeral tenant","explanation":"## Why This Is Asked\nThis question probes how a candidate ensures strict data isolation in a multi-tenant dbt setup during CI, a common real-world issue.\n\n## Key Concepts\n- Tenancy-aware testing across schemas\n- Per-tenant seeds/sources isolation\n- Test filtering macros and CI ephemeral environments\n- Cross-tenant leakage detection without touching prod\n\n## Code Example\n```javascript\n-- macros/tenancy_filters.sql\n{% macro tenant_test_filter(tenant) -%}\n  tenant_id = '{{ tenant }}'\n{%- endmacro %}\n```\n\n```javascript\n-- tests/tenancy_isolation_test.sql\nselect * from {{ ref('customers') }}\nwhere {{ tenant_test_filter('tenant_a') }}\n```\n\n## Follow-up Questions\n- How would you extend this for dynamic tenant onboarding?\n- How do you validate that physical storage quotas per tenant are respected?","diagram":"flowchart TD\n  PR[Pull Request] --> TESTS[Run tenancy-aware tests]\n  TESTS --> PASS{All tenant tests pass?}\n  PASS --> MERGE[Merge PR]\n  PASS --> REBUILD[Rebuild docs if needed]\n  FAIL --> NOTIFY[Notify engineer]","difficulty":"advanced","tags":["dbt-analytics-engineer"],"channel":"dbt-analytics-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Lyft","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T17:35:20.447Z","createdAt":"2026-01-12T17:35:20.447Z"},{"id":"dbt-analytics-engineer-testing-documentation-1768231420091-0","question":"In a dbt analytics pipeline deployed on AWS Redshift with S3 staging, a data quality test fails in CI/CD. To obtain robust, deterministic results without impacting production, which approach should you adopt?","answer":"[{\"id\":\"a\",\"text\":\"Run tests against production data in a single run\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use a separate ephemeral test schema with a known test dataset and seed data, isolating tests from production\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Disable tests in CI and rely on manual QA\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Run only schema tests (unique and not null) and ignore data tests\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe recommended approach is to use a separate ephemeral test schema with a known test dataset and seed data, isolating tests to avoid touching production.\n\n## Why Other Options Are Wrong\n- Option A: Testing against production data can corrupt or alter production and may not be reproducible in CI.\n- Option C: Disabling tests eliminates automated quality checks and increases risk.\n- Option D: Schema tests cover structure but do not validate data quality and business rules.\n\n## Key Concepts\n- Ephemeral test schemas\n- Seeded test data for determinism\n- dbt test scope and isolation\n\n## Real-World Application\nIn practice, configure your Redshift or other warehouse environment to provision a temporary test schema, load seeds with `dbt seed`, run `dbt test`, and drop the test schema after the run to avoid any production impact. For example:\n\n```bash\ndbt seed\ndbt run --models tag:quality\ndbt test\n```\n","diagram":null,"difficulty":"intermediate","tags":["dbt","AWS","Redshift","S3","CI/CD","Testing","certification-mcq","domain-weight-20"],"channel":"dbt-analytics-engineer","subChannel":"testing-documentation","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T15:23:40.094Z","createdAt":"2026-01-12 15:23:40"},{"id":"dbt-analytics-engineer-testing-documentation-1768231420091-1","question":"You want to auto-generate and publish documentation that includes dbt test results after pull requests. Which CI/CD approach ensures the docs site is rebuilt with latest test results?","answer":"[{\"id\":\"a\",\"text\":\"Run docs generation once per release and manually update test results in docs\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"In CI, run dbt test to produce run_results.json, publish a markdown test-summary into the docs site, and run dbt docs generate on PRs\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Rely on static docs without test results\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Create a separate docs site outside the repo and push results manually\",\"isCorrect\":false}]","explanation":"## Correct Answer\nIn CI, run `dbt test` to produce run_results.json, publish a markdown test-summary into the docs site, and run `dbt docs generate` on PRs so docs reflect latest test results automatically.\n\n## Why Other Options Are Wrong\n- Option A: Manual updates cause drift and slow feedback loops.\n- Option C: Static docs omit the current test state, reducing trust.\n- Option D: External/docs-site duplication creates sync challenges and extra maintenance.\n\n## Key Concepts\n- dbt docs generate\n- run_results.json from dbt test\n- CI/CD automation for docs\n\n## Real-World Application\nAdd a CI step that executes `dbt test`, extracts a concise markdown summary from `target/run_results.json`, updates docs, then runs `dbt docs generate` to refresh the docs site within a PR workflow. Example commands:\n\n```bash\ndbt test --models +quality\n# script to convert run_results.json to a markdown summary\n\ndbt docs generate\n```\n","diagram":null,"difficulty":"intermediate","tags":["dbt","AWS","Documentation","CI/CD","S3","Kubernetes","certification-mcq","domain-weight-20"],"channel":"dbt-analytics-engineer","subChannel":"testing-documentation","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T15:23:40.461Z","createdAt":"2026-01-12 15:23:40"},{"id":"dbt-analytics-engineer-testing-documentation-1768231420091-2","question":"A newly added seed table is used by several models. To ensure tests are reproducible across environments, what is the best practice?","answer":"[{\"id\":\"a\",\"text\":\"Use a dynamic seed loaded at runtime\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Commit seeds to git and load via dbt seed in CI to populate a known dataset\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Generate seeds via Faker in each environment\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Do not seed; rely on source data\",\"isCorrect\":false}]","explanation":"## Correct Answer\nCommit seeds to git and load via `dbt seed` in CI to ensure a known, version-controlled dataset is used for tests across environments.\n\n## Why Other Options Are Wrong\n- Option A: Dynamic seeds lead to nondeterministic tests.\n- Option C: Faker-generated seeds vary by environment, breaking reproducibility.\n- Option D: Relying solely on source data can introduce environmental variance and non-repeatable tests.\n\n## Key Concepts\n- dbt seed\n- Versioned seed data\n- CI deterministic testing\n\n## Real-World Application\nKeep a `seeds/` directory in your repo and run `dbt seed` as part of your CI pipeline before tests. This ensures every environment uses identical seed data. Example:\n\n```bash\ngit add seeds/*\ndbt seed\n```\n","diagram":null,"difficulty":"intermediate","tags":["dbt","AWS","Seed Data","Terraform","CI/CD","certification-mcq","domain-weight-20"],"channel":"dbt-analytics-engineer","subChannel":"testing-documentation","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T15:23:40.823Z","createdAt":"2026-01-12 15:23:40"},{"id":"dbt-analytics-engineer-testing-documentation-1768231420091-3","question":"When testing dbt models across multiple environments (dev, staging, prod), which practice ensures consistent test results across all environments?","answer":"[{\"id\":\"a\",\"text\":\"Use random seed values per environment\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use deterministic seed values and a shared test dataset across all environments\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Run tests only in development and assume rest\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Mirror prod data exactly in all environments\",\"isCorrect\":false}]","explanation":"## Correct Answer\nUse deterministic seed values and a shared test dataset across all environments to ensure repeatable results.\n\n## Why Other Options Are Wrong\n- Option A: Random seeds introduce nondeterminism, breaking repeatability.\n- Option C: Incomplete testing across environments leaves risk unverified.\n- Option D: Mirroring prod data can be impractical and may violate data governance; seeds offer a controlled baseline.\n\n## Key Concepts\n- Deterministic data seeds\n- Environment parity\n- Reproducible tests\n\n## Real-World Application\nIn your CI/CD pipeline, load a deterministic seed dataset into each environment before tests and use the same test queries across envs. Example:\n\n```bash\ndbt seed --models seed_common\ndbt test\n```\n","diagram":null,"difficulty":"intermediate","tags":["dbt","AWS","Kubernetes","CI/CD","Testing","certification-mcq","domain-weight-20"],"channel":"dbt-analytics-engineer","subChannel":"testing-documentation","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T15:23:40.954Z","createdAt":"2026-01-12 15:23:41"},{"id":"dbt-analytics-engineer-testing-documentation-1768231420091-4","question":"To measure how well your dbt project's documentation covers its models and tests, which metric best indicates documentation coverage?","answer":"[{\"id\":\"a\",\"text\":\"The total number of models listed in the docs site\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"The ratio of documented models and tests to total models\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"The time it takes to generate docs\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"The number of tests failing in CI\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe ratio of documented models and tests to total models best indicates documentation coverage.\n\n## Why Other Options Are Wrong\n- Option A: Listing models without context or tests doesn’t reflect coverage quality.\n- Option C: Generation time measures performance, not content coverage.\n- Option D: Test failures measure data quality, not documentation coverage.\n\n## Key Concepts\n- Documentation coverage metrics\n- Model-test mapping\n- Documentation quality\n\n## Real-World Application\nImplement a coverage dashboard that computes documented models and tests against total models, updating automatically with PRs. Example snippet:\n\n```markdown\nCoverage = documented_models / total_models\n```\n","diagram":null,"difficulty":"intermediate","tags":["dbt","AWS","Documentation","Kubernetes","Terraform","certification-mcq","domain-weight-20"],"channel":"dbt-analytics-engineer","subChannel":"testing-documentation","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T15:23:41.100Z","createdAt":"2026-01-12 15:23:41"}],"subChannels":["advanced-features","data-modeling","dbt-fundamentals","deployment-environments","general","testing-documentation"],"companies":["Amazon","Anthropic","Apple","DoorDash","Goldman Sachs","Google","Hashicorp","Lyft","Meta","OpenAI","PayPal","Tesla"],"stats":{"total":36,"beginner":2,"intermediate":31,"advanced":3,"newThisWeek":36}}