{"questions":[{"id":"azure-solutions-architect-design-data-storage-1768227830375-0","question":"A manufacturing company stores telemetry data in Azure Data Lake Storage Gen2. They have a mixture of hot, frequently accessed data and cold, rarely accessed historical data. They want to minimize cost while preserving query performance for analytics workloads. Which storage configuration should they implement?","answer":"[{\"id\":\"a\",\"text\":\"Store all data in the Hot tier of Azure Blob Storage with no lifecycle rules\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Apply a lifecycle policy that automatically moves infrequently accessed data to Cool or Archive tiers while keeping hot data in Hot\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Move data to Archive tier only and disable access to hot data\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use a separate on-premises storage solution for historical data\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption b is correct. Implementing a lifecycle policy that auto-migrates infrequently accessed data to Cool/Archive while keeping hot data in Hot delivers cost savings without sacrificing analytics performance. The other options are suboptimal: a overprovisioning all data in Hot increases cost; c sacrifices accessibility and performance for hot data; d introduces on-prem complexity and latency.","diagram":null,"difficulty":"intermediate","tags":["Azure","AzureBlobStorage","AzureDataLake","LifecycleManagement","CostOptimization","Terraform","AWS","Kubernetes","certification-mcq","domain-weight-20"],"channel":"azure-solutions-architect","subChannel":"design-data-storage","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T14:23:50.378Z","createdAt":"2026-01-12 14:23:50"},{"id":"azure-solutions-architect-design-data-storage-1768227830375-1","question":"A financial services company processes massive time-series sensor data that is appended and needs to be queried across multiple regions with low latency. They want scalable, cost-effective storage with fast analytics. Which approach is best?","answer":"[{\"id\":\"a\",\"text\":\"Store data in Azure Blob Storage in Hot tier without any processing layer\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Store data in Azure Data Lake Storage Gen2 with Parquet formatting and use Synapse Analytics with external tables\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Store data in Azure SQL Database as a time-series table\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Store data in Azure File Storage and mount via Azure Virtual Network\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption b is correct. Data Lake Storage Gen2 with Parquet offers columnar storage and efficient compression for time-series data, and using Synapse Analytics external tables enables scalable analytics across regions. The other options are less suitable: a lacks analytics optimization; c is not optimized for massive time-series analytics at scale; d adds unnecessary complexity and is not ideal for analytics workloads.","diagram":null,"difficulty":"intermediate","tags":["Azure","AzureDataLake","Parquet","SynapseAnalytics","TimeSeries","AWS","Kubernetes","certification-mcq","domain-weight-20"],"channel":"azure-solutions-architect","subChannel":"design-data-storage","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T14:23:50.877Z","createdAt":"2026-01-12 14:23:51"},{"id":"azure-solutions-architect-design-data-storage-1768227830375-2","question":"A regulated organization must ensure that certain blob data cannot be deleted for a fixed retention period (for example 7 years). They want to meet this requirement while still allowing lifecycle management for other data. Which solution should they implement?","answer":"[{\"id\":\"a\",\"text\":\"Enable a 7-year time-based retention immutability policy on the blob container\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Enable soft delete for 7 years on the blob storage\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use lifecycle policy to delete older data after 7 years\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Store the data in a relational database with audit trails\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption a is correct. A time-based immutability policy (blob immutability) enforces non-deletion for the specified period, satisfying regulatory retention. Soft delete (option b) allows recovery but does not guarantee immutability; lifecycle policies (option c) would delete data after 7 years; and using a database with audit trails (option d) does not provide immutable retention at the blob level.","diagram":null,"difficulty":"intermediate","tags":["Azure","BlobImmutability","RegulatoryRetention","Compliance","AWS","Terraform","Kubernetes","certification-mcq","domain-weight-20"],"channel":"azure-solutions-architect","subChannel":"design-data-storage","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T14:23:51.432Z","createdAt":"2026-01-12 14:23:51"},{"id":"azure-solutions-architect-design-data-storage-1768227830375-3","question":"You need to design cross-region disaster recovery for a mission-critical Azure SQL Database used by global applications. Which pattern provides automatic failover and recovery across regions with minimal downtime?","answer":"[{\"id\":\"a\",\"text\":\"Use geo-replication across regions with asynchronous commits\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use cross-region SQL Database read replicas with manual failover\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use auto-failover groups to replicate and failover across regions\",\"isCorrect\":true},{\"id\":\"d\",\"text\":\"Back up daily and restore in the DR region during outage\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption c is correct. Auto-failover groups provide coordinated, automatic failover and failback across regions for Azure SQL Database, minimizing downtime and RPO. Geo-replication (option a) is useful but failover is manual in many scenarios; read replicas with manual failover (option b) adds latency and operational risk; daily backups (option d) do not offer real-time DR readiness.","diagram":null,"difficulty":"intermediate","tags":["Azure","AzureSQL","AutoFailoverGroups","DisasterRecovery","AWS","Kubernetes","Terraform","certification-mcq","domain-weight-20"],"channel":"azure-solutions-architect","subChannel":"design-data-storage","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T14:23:51.597Z","createdAt":"2026-01-12 14:23:51"},{"id":"azure-solutions-architect-design-data-storage-1768227830375-4","question":"You are building a data lake of event logs in Azure Data Lake Storage Gen2. They want to optimize for query performance and cost. Which approach is most effective?","answer":"[{\"id\":\"a\",\"text\":\"Store as JSON files in a single directory and query them directly\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Store as Parquet files partitioned by date and service, with a data catalog to enable efficient queries\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Store as CSV files with daily dumps and scan them sequentially\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Store as Avro files in one large directory without partitioning\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption b is correct. Parquet is a columnar format that supports efficient encoding and compression, and partitioning by date/service enables partition pruning, dramatically improving query performance and cost. JSON (option a) and CSV (option c) are row-oriented and increase I/O; Avro (option d) without partitioning is less efficient for large-scale analytics.","diagram":null,"difficulty":"intermediate","tags":["Azure","AzureDataLake","Parquet","Partitioning","DataCatalog","AWS","Kubernetes","certification-mcq","domain-weight-20"],"channel":"azure-solutions-architect","subChannel":"design-data-storage","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T14:23:51.759Z","createdAt":"2026-01-12 14:23:51"},{"id":"azure-solutions-architect-design-identity-1768202888268-0","question":"You are designing an identity governance solution for a multi-tenant Azure environment and must grant temporary access to a sensitive Azure SQL Database for a contractor. The contractor should receive time-bound access, with approvals required, and all access should be auditable. Which combination of Azure AD Identity Governance features should you implement?","answer":"[{\"id\":\"a\",\"text\":\"Use Azure AD Entitlement Management to create an access package for the contractor, configure an approval workflow, and set an expiration period; require access reviews for the package.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Use Conditional Access to require MFA and device compliance for access to the database.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use Privileged Identity Management to assign a permanent role to the contractor.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use Access Reviews alone to periodically review group membership.\",\"isCorrect\":false}]","explanation":"## Correct Answer\n- Option A is correct because Azure AD Entitlement Management enables the creation of time-bound access packages with approved workflows, and expiration, providing an auditable trail for contractor access.\n\n## Why Other Options Are Wrong\n- Option B: Conditional Access controls authentication requirements but does not provide lifecycle management, approvals, or auditable access provisioning for contractors.\n- Option C: PIM is for privileged admin roles and does not provide time-bound contractor access with automated provisioning for regular data access.\n- Option D: Access Reviews alone do not provide the initial automated provisioning and expiration controls required for time-bound contractor access.\n\n## Key Concepts\n- Azure AD Entitlement Management\n- Access packages\n- Approval workflows\n- Access expiration and auditing\n\n## Real-World Application\n- Use Entitlement Management to onboard contractors with defined access packages, enforce approvals, and automatically revoke access when expired, ensuring compliance and traceability.","diagram":null,"difficulty":"intermediate","tags":["Azure","AzureAD","IdentityGovernance","AWS IAM","Kubernetes","Terraform","certification-mcq","domain-weight-25"],"channel":"azure-solutions-architect","subChannel":"design-identity","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T07:28:08.269Z","createdAt":"2026-01-12 07:28:08"},{"id":"azure-solutions-architect-design-identity-1768202888268-1","question":"A multinational company with B2B guest access across multiple Azure AD tenants wants scalable governance. Guests should receive time-limited, approved access to critical apps, with automated provisioning and de-provisioning when access expires, and periodic reviews of granted permissions. Which approach achieves this?","answer":"[{\"id\":\"a\",\"text\":\"Enable Azure AD Entitlement Management to create access packages for guest users, assign approvals, configure automatic expiration, and enable Access Reviews.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Create static Conditional Access policies to enforce MFA for guest sign-ins.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use Privileged Identity Management to assign admin roles to guest users.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Manually assign roles to guest users without lifecycle management.\",\"isCorrect\":false}]","explanation":"## Correct Answer\n- Option A is correct because Entitlement Management supports provisioning of access packages for guests with approval workflows, time-bound expiration, and built-in support for periodic reviews, providing scalable governance.\n\n## Why Other Options Are Wrong\n- Option B: MFA enforcement does not address lifecycle provisioning, expiration, or periodic reviews for guest access.\n- Option C: PIM focuses on privileged admin roles, not general guest access lifecycle and reviews.\n- Option D: Manual role assignment lacks automated provisioning, expiration, and auditable review processes.\n\n## Key Concepts\n- Entitlement Management\n- Access packages\n- Approval workflows\n- Access expiration and Access Reviews\n\n## Real-World Application\n- Scale guest access with lifecycle-managed packages and automatic de-provisioning, reducing risk and administrative overhead while maintaining auditability.","diagram":null,"difficulty":"intermediate","tags":["Azure","AzureAD","IdentityGovernance","AWS IAM","Kubernetes","Terraform","certification-mcq","domain-weight-25"],"channel":"azure-solutions-architect","subChannel":"design-identity","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T07:28:08.824Z","createdAt":"2026-01-12 07:28:09"},{"id":"azure-solutions-architect-design-identity-1768202888268-2","question":"You need a centralized, scalable monitoring solution for identity and access governance events across subscriptions. You want to detect and alert on unusual privileged access changes and tie them to a SIEM that can surface incidents. Which approach is most appropriate?","answer":"[{\"id\":\"a\",\"text\":\"Deploy Azure Monitor logs to a central workspace and configure log alerts for sign-ins.\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use Azure Information Protection to classify data and monitor access.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use Microsoft Sentinel with an Azure AD connector across subscriptions, enable identity-related detections, and integrate Privileged Identity Management events.\",\"isCorrect\":true},{\"id\":\"d\",\"text\":\"Use Defender for Cloud to monitor compliance.\",\"isCorrect\":false}]","explanation":"## Correct Answer\n- Option C is correct because Microsoft Sentinel (a cloud-native SIEM) can ingest identity signals from Azure AD across subscriptions, enable identity-related detections, and correlate Privileged Identity Management events for centralized incident response.\n\n## Why Other Options Are Wrong\n- Option A: While useful, Azure Monitor alone does not provide the identity-specific detections and centralized incident response workflows a SIEM offers.\n- Option B: Information Protection focuses on data classification and protection, not real-time identity governance monitoring.\n- Option D: Defender for Cloud focuses on cloud security posture and compliance, not comprehensive identity monitoring and SIEM integration.\n\n## Key Concepts\n- Microsoft Sentinel\n- Azure AD connector\n- Identity-related detections\n- Privileged Identity Management integration\n\n## Real-World Application\n- Centralize identity security telemetry in a SIEM to accelerate detection and response to privileged access abuse across the organization.","diagram":null,"difficulty":"intermediate","tags":["Azure","AzureAD","IdentityGovernance","AWS IAM","Kubernetes","Terraform","certification-mcq","domain-weight-25"],"channel":"azure-solutions-architect","subChannel":"design-identity","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T07:28:09.363Z","createdAt":"2026-01-12 07:28:09"},{"id":"azure-solutions-architect-design-infrastructure-1768162975592-0","question":"You are designing a multi-region web application in Azure with a stateless front end and globally distributed users. To minimize downtime during regional outages and keep user requests responsive, which design combination is most effective?","answer":"[{\"id\":\"a\",\"text\":\"Route traffic using Azure Traffic Manager in priority mode and rely on regional failover with a single-region database configuration\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Place the front end behind Azure Front Door with health-based failover and enable Cosmos DB multi-region writes to support low-latency reads and writes\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Use API Management as the gateway and implement client-side region detection while disabling cross-region replication\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Rely on DNS-based failover with a long TTL and perform manual cutover when an outage is detected\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct because Azure Front Door provides global HTTP/HTTPS routing with health probes and instant failover, while Cosmos DB multi-region writes enables low-latency reads and writes across regions, reducing RTO and RPO during regional outages.\n\n## Why Other Options Are Wrong\n- A: Traffic Manager in priority mode can route traffic, but without a global frontend like Front Door, failover is slower and DNS-based routing adds TTL-induced delays, increasing downtime.\n- C: API Management is a gateway for APIs, not a global load balancer; region-detection on clients and disabled cross-region replication increases complexity and latency, harming resilience.\n- D: DNS-based failover with long TTLs leads to high failover times and poor continuity during outages.\n\n## Key Concepts\n- Azure Front Door for global load balancing and instant failover\n- Cosmos DB multi-region writes for cross-region data distribution\n- Read/write locality and consistency choices affect latency and RPO/RTO\n\n## Real-World Application\nUsed in multi-region SaaS deployments to provide resilient user experiences during regional outages while maintaining responsive data access across geographies.","diagram":null,"difficulty":"intermediate","tags":["Azure","Frontend","CosmosDB","AWS-VPC","Kubernetes","Terraform","certification-mcq","domain-weight-30"],"channel":"azure-solutions-architect","subChannel":"design-infrastructure","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T20:22:55.593Z","createdAt":"2026-01-11 20:22:56"},{"id":"azure-solutions-architect-design-infrastructure-1768162975592-1","question":"Your organization operates an Azure SQL Database that must remain available during regional outages with minimal data loss and supports reporting workloads from a secondary region. Which feature should you implement?","answer":"[{\"id\":\"a\",\"text\":\"Geo-restore without automated failover\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Auto-failover group across primary and secondary regions with readable secondaries\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Manual cross-region replication without failover capability\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Backup and restore to the secondary region on every query\",\"isCorrect\":false}]","explanation":"## Correct Answer\nAuto-failover groups enable automatic failover to a configured secondary region and provide readable secondaries for off-load reporting, ensuring minimal downtime and data loss.\n\n## Why Other Options Are Wrong\n- a: Geo-restore provides point-in-time recovery but does not offer automatic failover or readable secondaries for ongoing workloads.\n- c: Manual cross-region replication without automated failover increases downtime and operator risk.\n- d: Backups/restores are too slow for real-time DR needs and are not suitable for ongoing reporting workloads.\n\n## Key Concepts\n- Azure SQL Database auto-failover groups\n- Readable secondaries and automatic failover\n- Cross-region disaster recovery planning\n\n## Real-World Application\nCommonly used for mission-critical databases where DR readiness and rapid failover are required, such as e-commerce or financial services platforms.","diagram":null,"difficulty":"intermediate","tags":["Azure","SQL","DR","AWS-VPC","Kubernetes","Terraform","certification-mcq","domain-weight-30"],"channel":"azure-solutions-architect","subChannel":"design-infrastructure","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T20:22:56.124Z","createdAt":"2026-01-11 20:22:56"},{"id":"azure-solutions-architect-design-infrastructure-1768162975592-2","question":"You need to grant developers access across multiple Azure subscriptions with least privilege and scalable governance, without creating per-subscription administrator accounts. Which approach best achieves this?","answer":"[{\"id\":\"a\",\"text\":\"Create global admin accounts and assign them to each subscription\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use Azure Lighthouse to delegate resource management across subscriptions with scoped RBAC\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Assign broad custom roles at the tenant level for all subscriptions\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use permanent service principals with full access in each subscription\",\"isCorrect\":false}]","explanation":"## Correct Answer\nAzure Lighthouse enables delegated resource management across multiple tenants and subscriptions, allowing you to grant least-privilege access to external partners or internal teams without elevating global privileges, and supports scoped RBAC for fine-grained control.\n\n## Why Other Options Are Wrong\n- a: Global admin accounts create broad, unsafe access across all subscriptions and do not scale well.\n- c: Tenant-wide broad roles violate least privilege and reduce accountability.\n- d: Permanent full-access service principals create long-term risk and governance challenges.\n\n## Key Concepts\n- Azure Lighthouse for cross-tenant/resource delegation\n- Scoped RBAC and least-privilege access\n- Governance and compliance in multi-subscription environments\n\n## Real-World Application\nCommon in MSP partnerships and large enterprises needing controlled cross-subscription operations with auditable access controls.","diagram":null,"difficulty":"intermediate","tags":["Azure","RBAC","Lighthouse","AWS-VPC","Kubernetes","Terraform","certification-mcq","domain-weight-30"],"channel":"azure-solutions-architect","subChannel":"design-infrastructure","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T20:22:56.657Z","createdAt":"2026-01-11 20:22:56"},{"id":"q-887","question":"Youâ€™re building a multi-tenant analytics platform on Azure for a consumer-brand SaaS product. Each tenant must have isolated data processing, with per-tenant data lake isolation, on-demand Spark/notebook compute that auto-suspends, and cost governance at the tenant level. Propose an architecture using Azure Data Lake Storage Gen2, Unity Catalog or RBAC, Synapse or Databricks, private endpoints, and auditing. How do you ensure data isolation, prevent cross-tenant leakage, and meet compliance while keeping ops simple?","answer":"Use per-tenant isolation via either separate Data Lake Storage Gen2 accounts or a single lake with strict namespaces and Unity Catalog RBAC. Provision on-demand Spark/notebook pools with auto-suspend,","explanation":"## Why This Is Asked\nAssess practical multi-tenant analytics architecture in Azure focusing on data isolation, cost governance, and compliance for a customer-facing platform.\n\n## Key Concepts\n- Data isolation models (per-tenant vs shared lake)\n- Unity Catalog/RBAC scoping by tenant\n- Auto-suspend compute and per-tenant budgets\n- Private Endpoints and encryption at rest per tenant\n- Centralized audit/log retention and compliance\n\n## Code Example\n```javascript\n// Placeholder IaC snippet illustrating per-tenant RBAC scope\nconst tenantScope = getTenantScope(\"tenantA\");\nrbac.assignRole(tenantScope, \"DataReader\");\n// This is illustrative; implementation will use your chosen IaC tool\n```\n\n## Follow-up Questions\n- How would you scale onboarding/offboarding tenants without downtime?\n- How would you validate no cross-tenant data leakage in tests?","diagram":"flowchart TD\n  Tenant[Tenant] --> Lake[Data Lake Gen2]\n  Tenant --> Compute[Notebook/Compute]\n  Lake --> UC[Unity Catalog RBAC]\n  Compute --> Private[Private Endpoints]\n  Compute --> AuditSink[Audit Sink] --> Compliance[Compliance & Retention]","difficulty":"intermediate","tags":["azure-solutions-architect"],"channel":"azure-solutions-architect","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","Microsoft","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T14:25:45.635Z","createdAt":"2026-01-12T14:25:45.635Z"}],"subChannels":["design-data-storage","design-identity","design-infrastructure","general"],"companies":["Instacart","Microsoft","Snap"],"stats":{"total":12,"beginner":0,"intermediate":12,"advanced":0,"newThisWeek":12}}