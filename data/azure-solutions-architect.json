{"questions":[{"id":"azure-solutions-architect-design-business-continuity-1768238784454-0","question":"You operate a multi-region web application with critical data, and a regional outage is possible. You need automated failover to a secondary region with minimal downtime and RPO under 15 minutes. Which Azure service best satisfies this requirement?","answer":"[{\"id\":\"a\",\"text\":\"Azure Availability Sets\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Azure Site Recovery\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Azure Backup\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Azure Traffic Manager\",\"isCorrect\":false}]","explanation":"## Correct Answer\nAzure Site Recovery\n\n## Why Other Options Are Wrong\n- Availability Sets provide VM-level fault domain isolation but do not orchestrate cross-region failover for entire workloads.\n- Azure Backup is for data protection/restore, not active workload failover and service orchestration.\n- Traffic Manager is DNS-based routing and does not automate regional workload failover or reduce RTO to minutes.\n\n## Key Concepts\n- Disaster Recover orchestration across regions\n- RTO/RPO targets and cross-region failover capabilities\n\n## Real-World Application\n- Implement DR drills using ASR to validate failover to a secondary region with automated orchestration and minimal downtime during outages.","diagram":null,"difficulty":"intermediate","tags":["Azure","AWS","Kubernetes","Terraform","Azure Site Recovery","certification-mcq","domain-weight-15"],"channel":"azure-solutions-architect","subChannel":"design-business-continuity","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T17:26:24.455Z","createdAt":"2026-01-12 17:26:25"},{"id":"azure-solutions-architect-design-business-continuity-1768238784454-1","question":"An organization runs Azure SQL Database in two regions and wants automatic failover of the entire database group to a secondary region with minimal manual intervention during a regional disruption. Which feature should you implement?","answer":"[{\"id\":\"a\",\"text\":\"Geo-Replication\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Failover Groups\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Auto-backup restore\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Geo-restore\",\"isCorrect\":false}]","explanation":"## Correct Answer\nFailover Groups\n\n## Why Other Options Are Wrong\n- Geo-Replication is per-database replication but does not provide automated group failover across multiple databases as a single unit.\n- Auto-backup restore covers point-in-time restore, not cross-region automatic failover for multiple databases.\n- Geo-restore is a restore from a secondary region, not a live failover mechanism for an entire group.\n\n## Key Concepts\n- Group-level failover for Azure SQL Databases\n- Cross-region DR orchestration\n\n## Real-World Application\n- Configure an auto-failover group to ensure all databases in the group fail over together during regional outages.","diagram":null,"difficulty":"intermediate","tags":["Azure","Azure SQL","Azure Site Recovery","AWS","Kubernetes","Terraform","certification-mcq","domain-weight-15"],"channel":"azure-solutions-architect","subChannel":"design-business-continuity","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T17:26:25.146Z","createdAt":"2026-01-12 17:26:25"},{"id":"azure-solutions-architect-design-business-continuity-1768238784454-2","question":"To deliver a globally distributed e-commerce experience with fast failover and resilience, you want to route user traffic to the nearest healthy region and switch traffic quickly in case of regional outages. Which Azure service should you choose?","answer":"[{\"id\":\"a\",\"text\":\"Azure Traffic Manager\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Azure Front Door\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Azure Load Balancer\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Azure Application Gateway\",\"isCorrect\":false}]","explanation":"## Correct Answer\nAzure Front Door\n\n## Why Other Options Are Wrong\n- Traffic Manager is DNS-based and does not provide edge-based routing with fast, application-aware failover at the edge.\n- Load Balancer operates at layer 4/7 but is regional and does not provide global routing across multiple regions.\n- Application Gateway is regional and focused on app-layer routing within a single region.\n\n## Key Concepts\n- Global, edge-based routing with health checks\n- Fast failover for multi-region deployments\n\n## Real-World Application\n- Deploy Front Door in front of your multi-region app to ensure users are directed to healthy regions with minimal latency and rapid failover during outages.","diagram":null,"difficulty":"intermediate","tags":["Azure","Azure Front Door","Azure Traffic Manager","AWS","Kubernetes","Terraform","certification-mcq","domain-weight-15"],"channel":"azure-solutions-architect","subChannel":"design-business-continuity","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T17:26:25.654Z","createdAt":"2026-01-12 17:26:25"},{"id":"azure-solutions-architect-design-business-continuity-1768238784454-3","question":"During DR planning, you want to validate recovery procedures without impacting production systems. Which DR testing capability provided by Azure Site Recovery should you use?","answer":"[{\"id\":\"a\",\"text\":\"Immediate failover\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Test Failover\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Planned Failover\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Replica failover\",\"isCorrect\":false}]","explanation":"## Correct Answer\nTest Failover\n\n## Why Other Options Are Wrong\n- Immediate failover disrupts production and is not suitable for non-disruptive testing.\n- Planned Failover is used for validating planned changes, but it still impacts production state if misused; Test Failover is specifically designed for DR drills without production impact.\n- Replica failover is not a standard DR test mode and does not guarantee a non-disruptive drill.\n\n## Key Concepts\n- Non-disruptive DR testing\n- DR runbooks and validation procedures\n\n## Real-World Application\n- Regularly perform test failovers to validate DR readiness without affecting live customers.","diagram":null,"difficulty":"intermediate","tags":["Azure","Azure Site Recovery","AWS","Kubernetes","Terraform","certification-mcq","domain-weight-15"],"channel":"azure-solutions-architect","subChannel":"design-business-continuity","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T17:26:25.845Z","createdAt":"2026-01-12 17:26:25"},{"id":"azure-solutions-architect-design-business-continuity-1768238784454-4","question":"To protect against ransomware and ensure data remains immutable, you need backups that cannot be altered for a defined retention period. Which storage option provides immutability (WORM) and supports air-gapped backups?","answer":"[{\"id\":\"a\",\"text\":\"Azure Backup vault with soft delete\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Azure Blob Storage immutable storage (WORM)\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Azure Archive long-term retention\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"On-premises offline backups via tape\",\"isCorrect\":false}]","explanation":"## Correct Answer\nAzure Blob Storage immutable storage (WORM)\n\n## Why Other Options Are Wrong\n- Soft delete in Azure Backup vault is reversible and does not guarantee immutability for the entire retention period.\n- Archive tier provides cost savings but does not inherently enforce immutability or air-gapped backups.\n- On-premises offline backups via tape are not an Azure-native immutable, cloud-based solution and complicate DR orchestration.\n\n## Key Concepts\n- Immutable storage (WORM) in Azure Blob Storage\n- Air-gapped or protected backups for ransomware resilience\n\n## Real-World Application\n- Enable Immutable Blob Storage on the backup container and enforce retention policies to meet compliance and ransomware recovery goals.","diagram":null,"difficulty":"intermediate","tags":["Azure","Azure Blob Storage","AWS","Kubernetes","Terraform","certification-mcq","domain-weight-15"],"channel":"azure-solutions-architect","subChannel":"design-business-continuity","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T17:26:26.024Z","createdAt":"2026-01-12 17:26:26"},{"id":"azure-solutions-architect-design-data-storage-1768227830375-0","question":"A manufacturing company stores telemetry data in Azure Data Lake Storage Gen2. They have a mixture of hot, frequently accessed data and cold, rarely accessed historical data. They want to minimize cost while preserving query performance for analytics workloads. Which storage configuration should they implement?","answer":"[{\"id\":\"a\",\"text\":\"Store all data in the Hot tier of Azure Blob Storage with no lifecycle rules\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Apply a lifecycle policy that automatically moves infrequently accessed data to Cool or Archive tiers while keeping hot data in Hot\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Move data to Archive tier only and disable access to hot data\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use a separate on-premises storage solution for historical data\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption b is correct. Implementing a lifecycle policy that auto-migrates infrequently accessed data to Cool/Archive while keeping hot data in Hot delivers cost savings without sacrificing analytics performance. The other options are suboptimal: a overprovisioning all data in Hot increases cost; c sacrifices accessibility and performance for hot data; d introduces on-prem complexity and latency.","diagram":null,"difficulty":"intermediate","tags":["Azure","AzureBlobStorage","AzureDataLake","LifecycleManagement","CostOptimization","Terraform","AWS","Kubernetes","certification-mcq","domain-weight-20"],"channel":"azure-solutions-architect","subChannel":"design-data-storage","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T14:23:50.378Z","createdAt":"2026-01-12 14:23:50"},{"id":"azure-solutions-architect-design-data-storage-1768227830375-1","question":"A financial services company processes massive time-series sensor data that is appended and needs to be queried across multiple regions with low latency. They want scalable, cost-effective storage with fast analytics. Which approach is best?","answer":"[{\"id\":\"a\",\"text\":\"Store data in Azure Blob Storage in Hot tier without any processing layer\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Store data in Azure Data Lake Storage Gen2 with Parquet formatting and use Synapse Analytics with external tables\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Store data in Azure SQL Database as a time-series table\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Store data in Azure File Storage and mount via Azure Virtual Network\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption b is correct. Data Lake Storage Gen2 with Parquet offers columnar storage and efficient compression for time-series data, and using Synapse Analytics external tables enables scalable analytics across regions. The other options are less suitable: a lacks analytics optimization; c is not optimized for massive time-series analytics at scale; d adds unnecessary complexity and is not ideal for analytics workloads.","diagram":null,"difficulty":"intermediate","tags":["Azure","AzureDataLake","Parquet","SynapseAnalytics","TimeSeries","AWS","Kubernetes","certification-mcq","domain-weight-20"],"channel":"azure-solutions-architect","subChannel":"design-data-storage","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T14:23:50.877Z","createdAt":"2026-01-12 14:23:51"},{"id":"azure-solutions-architect-design-data-storage-1768227830375-2","question":"A regulated organization must ensure that certain blob data cannot be deleted for a fixed retention period (for example 7 years). They want to meet this requirement while still allowing lifecycle management for other data. Which solution should they implement?","answer":"[{\"id\":\"a\",\"text\":\"Enable a 7-year time-based retention immutability policy on the blob container\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Enable soft delete for 7 years on the blob storage\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use lifecycle policy to delete older data after 7 years\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Store the data in a relational database with audit trails\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption a is correct. A time-based immutability policy (blob immutability) enforces non-deletion for the specified period, satisfying regulatory retention. Soft delete (option b) allows recovery but does not guarantee immutability; lifecycle policies (option c) would delete data after 7 years; and using a database with audit trails (option d) does not provide immutable retention at the blob level.","diagram":null,"difficulty":"intermediate","tags":["Azure","BlobImmutability","RegulatoryRetention","Compliance","AWS","Terraform","Kubernetes","certification-mcq","domain-weight-20"],"channel":"azure-solutions-architect","subChannel":"design-data-storage","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T14:23:51.432Z","createdAt":"2026-01-12 14:23:51"},{"id":"azure-solutions-architect-design-data-storage-1768227830375-3","question":"You need to design cross-region disaster recovery for a mission-critical Azure SQL Database used by global applications. Which pattern provides automatic failover and recovery across regions with minimal downtime?","answer":"[{\"id\":\"a\",\"text\":\"Use geo-replication across regions with asynchronous commits\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use cross-region SQL Database read replicas with manual failover\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use auto-failover groups to replicate and failover across regions\",\"isCorrect\":true},{\"id\":\"d\",\"text\":\"Back up daily and restore in the DR region during outage\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption c is correct. Auto-failover groups provide coordinated, automatic failover and failback across regions for Azure SQL Database, minimizing downtime and RPO. Geo-replication (option a) is useful but failover is manual in many scenarios; read replicas with manual failover (option b) adds latency and operational risk; daily backups (option d) do not offer real-time DR readiness.","diagram":null,"difficulty":"intermediate","tags":["Azure","AzureSQL","AutoFailoverGroups","DisasterRecovery","AWS","Kubernetes","Terraform","certification-mcq","domain-weight-20"],"channel":"azure-solutions-architect","subChannel":"design-data-storage","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T14:23:51.597Z","createdAt":"2026-01-12 14:23:51"},{"id":"azure-solutions-architect-design-data-storage-1768227830375-4","question":"You are building a data lake of event logs in Azure Data Lake Storage Gen2. They want to optimize for query performance and cost. Which approach is most effective?","answer":"[{\"id\":\"a\",\"text\":\"Store as JSON files in a single directory and query them directly\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Store as Parquet files partitioned by date and service, with a data catalog to enable efficient queries\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Store as CSV files with daily dumps and scan them sequentially\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Store as Avro files in one large directory without partitioning\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption b is correct. Parquet is a columnar format that supports efficient encoding and compression, and partitioning by date/service enables partition pruning, dramatically improving query performance and cost. JSON (option a) and CSV (option c) are row-oriented and increase I/O; Avro (option d) without partitioning is less efficient for large-scale analytics.","diagram":null,"difficulty":"intermediate","tags":["Azure","AzureDataLake","Parquet","Partitioning","DataCatalog","AWS","Kubernetes","certification-mcq","domain-weight-20"],"channel":"azure-solutions-architect","subChannel":"design-data-storage","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T14:23:51.759Z","createdAt":"2026-01-12 14:23:51"},{"id":"azure-solutions-architect-design-identity-1768202888268-0","question":"You are designing an identity governance solution for a multi-tenant Azure environment and must grant temporary access to a sensitive Azure SQL Database for a contractor. The contractor should receive time-bound access, with approvals required, and all access should be auditable. Which combination of Azure AD Identity Governance features should you implement?","answer":"[{\"id\":\"a\",\"text\":\"Use Azure AD Entitlement Management to create an access package for the contractor, configure an approval workflow, and set an expiration period; require access reviews for the package.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Use Conditional Access to require MFA and device compliance for access to the database.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use Privileged Identity Management to assign a permanent role to the contractor.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use Access Reviews alone to periodically review group membership.\",\"isCorrect\":false}]","explanation":"## Correct Answer\n- Option A is correct because Azure AD Entitlement Management enables the creation of time-bound access packages with approved workflows, and expiration, providing an auditable trail for contractor access.\n\n## Why Other Options Are Wrong\n- Option B: Conditional Access controls authentication requirements but does not provide lifecycle management, approvals, or auditable access provisioning for contractors.\n- Option C: PIM is for privileged admin roles and does not provide time-bound contractor access with automated provisioning for regular data access.\n- Option D: Access Reviews alone do not provide the initial automated provisioning and expiration controls required for time-bound contractor access.\n\n## Key Concepts\n- Azure AD Entitlement Management\n- Access packages\n- Approval workflows\n- Access expiration and auditing\n\n## Real-World Application\n- Use Entitlement Management to onboard contractors with defined access packages, enforce approvals, and automatically revoke access when expired, ensuring compliance and traceability.","diagram":null,"difficulty":"intermediate","tags":["Azure","AzureAD","IdentityGovernance","AWS IAM","Kubernetes","Terraform","certification-mcq","domain-weight-25"],"channel":"azure-solutions-architect","subChannel":"design-identity","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T07:28:08.269Z","createdAt":"2026-01-12 07:28:08"},{"id":"azure-solutions-architect-design-identity-1768202888268-1","question":"A multinational company with B2B guest access across multiple Azure AD tenants wants scalable governance. Guests should receive time-limited, approved access to critical apps, with automated provisioning and de-provisioning when access expires, and periodic reviews of granted permissions. Which approach achieves this?","answer":"[{\"id\":\"a\",\"text\":\"Enable Azure AD Entitlement Management to create access packages for guest users, assign approvals, configure automatic expiration, and enable Access Reviews.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Create static Conditional Access policies to enforce MFA for guest sign-ins.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use Privileged Identity Management to assign admin roles to guest users.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Manually assign roles to guest users without lifecycle management.\",\"isCorrect\":false}]","explanation":"## Correct Answer\n- Option A is correct because Entitlement Management supports provisioning of access packages for guests with approval workflows, time-bound expiration, and built-in support for periodic reviews, providing scalable governance.\n\n## Why Other Options Are Wrong\n- Option B: MFA enforcement does not address lifecycle provisioning, expiration, or periodic reviews for guest access.\n- Option C: PIM focuses on privileged admin roles, not general guest access lifecycle and reviews.\n- Option D: Manual role assignment lacks automated provisioning, expiration, and auditable review processes.\n\n## Key Concepts\n- Entitlement Management\n- Access packages\n- Approval workflows\n- Access expiration and Access Reviews\n\n## Real-World Application\n- Scale guest access with lifecycle-managed packages and automatic de-provisioning, reducing risk and administrative overhead while maintaining auditability.","diagram":null,"difficulty":"intermediate","tags":["Azure","AzureAD","IdentityGovernance","AWS IAM","Kubernetes","Terraform","certification-mcq","domain-weight-25"],"channel":"azure-solutions-architect","subChannel":"design-identity","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T07:28:08.824Z","createdAt":"2026-01-12 07:28:09"},{"id":"azure-solutions-architect-design-identity-1768202888268-2","question":"You need a centralized, scalable monitoring solution for identity and access governance events across subscriptions. You want to detect and alert on unusual privileged access changes and tie them to a SIEM that can surface incidents. Which approach is most appropriate?","answer":"[{\"id\":\"a\",\"text\":\"Deploy Azure Monitor logs to a central workspace and configure log alerts for sign-ins.\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use Azure Information Protection to classify data and monitor access.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use Microsoft Sentinel with an Azure AD connector across subscriptions, enable identity-related detections, and integrate Privileged Identity Management events.\",\"isCorrect\":true},{\"id\":\"d\",\"text\":\"Use Defender for Cloud to monitor compliance.\",\"isCorrect\":false}]","explanation":"## Correct Answer\n- Option C is correct because Microsoft Sentinel (a cloud-native SIEM) can ingest identity signals from Azure AD across subscriptions, enable identity-related detections, and correlate Privileged Identity Management events for centralized incident response.\n\n## Why Other Options Are Wrong\n- Option A: While useful, Azure Monitor alone does not provide the identity-specific detections and centralized incident response workflows a SIEM offers.\n- Option B: Information Protection focuses on data classification and protection, not real-time identity governance monitoring.\n- Option D: Defender for Cloud focuses on cloud security posture and compliance, not comprehensive identity monitoring and SIEM integration.\n\n## Key Concepts\n- Microsoft Sentinel\n- Azure AD connector\n- Identity-related detections\n- Privileged Identity Management integration\n\n## Real-World Application\n- Centralize identity security telemetry in a SIEM to accelerate detection and response to privileged access abuse across the organization.","diagram":null,"difficulty":"intermediate","tags":["Azure","AzureAD","IdentityGovernance","AWS IAM","Kubernetes","Terraform","certification-mcq","domain-weight-25"],"channel":"azure-solutions-architect","subChannel":"design-identity","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T07:28:09.363Z","createdAt":"2026-01-12 07:28:09"},{"id":"azure-solutions-architect-design-identity-1768274794495-0","question":"A global software company uses Azure AD B2B for partner access to a shared project. They require centralized access control, MFA, and periodic access reviews with automated expiry for external users across multiple subscriptions. Which design best fits?","answer":"[{\"id\":\"a\",\"text\":\"Leverage Azure AD B2B guest users with Conditional Access policies and automated Access Reviews.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Create separate Azure AD tenants for each partner and manage guest users via cross-tenant federation.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use hard-coded credentials in apps and share them with partners.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Grant all partners an admin role to simplify management.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct option is A. Using Azure AD B2B guest users with Conditional Access and Access Reviews centralizes external access management, enforces MFA at access time, and allows automated expiry via periodic access reviews across subscriptions.\n\n## Why Other Options Are Wrong\n- Option B: Creating separate tenants per partner introduces significant management overhead and breaks centralized governance across subscriptions.\n- Option C: Hard-coded credentials are insecure and violate least-privilege and credential management practices.\n- Option D: Granting admin roles to external partners expands risk and is not aligned with least-privilege principles.\n\n## Key Concepts\n- Azure AD B2B guest users\n- Conditional Access policies\n- Access Reviews for external users\n- Least privilege and credential hygiene\n\n## Real-World Application\nIn practice, configure a single external partner group in Azure AD, enforce MFA via Conditional Access, and enable Access Reviews with automatic expiry for guest users to maintain compliance without manual re-provisioning.","diagram":null,"difficulty":"intermediate","tags":["Azure","AzureAD","Identity","B2B","AWS_IAM","Kubernetes","Terraform","certification-mcq","domain-weight-25"],"channel":"azure-solutions-architect","subChannel":"design-identity","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T03:26:34.499Z","createdAt":"2026-01-13 03:26:34"},{"id":"azure-solutions-architect-design-identity-1768274794495-1","question":"Your organization wants to ensure encryption at rest and tagging compliance across all existing and future resources in multiple subscriptions. Some resources are currently non-compliant. To remediate non-compliant resources without impacting workloads, which approach is most appropriate?","answer":"[{\"id\":\"a\",\"text\":\"Apply a DeployIfNotExists policy that auto-deploys encryption where missing.\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use an AuditIfNotExists policy to identify non-compliant resources and create remediation tasks that enforce compliance.\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Deny non-compliant resource creations going forward and handle existing issues manually.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Manually annotate each resource and rely on developers to fix non-compliant resources.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct. Use AuditIfNotExists to detect non-compliant resources and leverage remediation tasks to automatically or semi-automatically bring them into compliance without downtime. This aligns with governance workflows and minimizes manual effort.\n\n## Why Other Options Are Wrong\n- Option A could automatically deploy encryption in some scenarios but may not cover all configurations and could have unintended side effects on existing resources.\n- Option C blocks new resources but does not address remediation of existing non-compliant resources promptly.\n- Option D is error-prone and lacks automation, increasing risk and operational burden.\n\n## Key Concepts\n- Azure Policy remediation tasks\n- AuditIfNotExists vs DeployIfNotExists\n- Governance and compliance automation\n\n## Real-World Application\nIn production, define an AuditIfNotExists policy for required encryption and tags, then create remediation tasks that apply the correct encryption and tags to non-compliant resources across subscriptions, ensuring compliance over time with minimal downtime.","diagram":null,"difficulty":"intermediate","tags":["Azure","AzurePolicy","Governance","Terraform","Kubernetes","AWS_IAM","certification-mcq","domain-weight-25"],"channel":"azure-solutions-architect","subChannel":"design-identity","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T03:26:35.079Z","createdAt":"2026-01-13 03:26:35"},{"id":"azure-solutions-architect-design-identity-1768274794495-2","question":"To monitor workloads across multiple subscriptions with centralized analysis and consistent data retention, which design best achieves this with minimal data transfer and manageable costs?","answer":"[{\"id\":\"a\",\"text\":\"Create separate Log Analytics workspaces for each subscription and aggregate data in external dashboards.\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Create a hub Log Analytics workspace in a central management group and route data using Data Collection Rules across subscriptions.\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Rely solely on metrics in each resource and omit logs to reduce costs.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use a single Application Insights instance per application to cover all subscriptions.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct. A hub Log Analytics workspace with Data Collection Rules (DCR) consolidates data from multiple subscriptions under a unified view while controlling ingestion and cost, enabling centralized monitoring.\n\n## Why Other Options Are Wrong\n- Option A adds complexity and reduces the centralization that aids governance and correlation.\n- Option C sacrifices valuable log data, limiting troubleshooting and security visibility.\n- Option D (Application Insights) is primarily for app telemetry and doesnâ€™t cover broader infrastructure monitoring across subscriptions.\n\n## Key Concepts\n- Log Analytics workspace hub pattern\n- Data Collection Rules (DCR)\n- Azure Monitor data strategy across subscriptions\n\n## Real-World Application\nIn practice, place a hub workspace at the management group root, define DCRs per data source, and route data from all subscriptions into the hub for unified dashboards, alerts, and cost-optimized retention policies.","diagram":null,"difficulty":"intermediate","tags":["Azure","AzureMonitor","LogAnalytics","DCR","ManagementGroups","Terraform","Kubernetes","certification-mcq","domain-weight-25"],"channel":"azure-solutions-architect","subChannel":"design-identity","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T03:26:35.626Z","createdAt":"2026-01-13 03:26:35"},{"id":"azure-solutions-architect-design-identity-1768274794495-3","question":"An organization requires just-in-time administrative access with approval for Azure AD administrative roles, MFA enforcement during activation, and periodic access reviews. Which combination best implements this requirement?","answer":"[{\"id\":\"a\",\"text\":\"Enable Privileged Identity Management with just-in-time activation, MFA for activation, and configure Access Reviews for eligible admins.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Grant permanent elevated roles and rely on annual password changes.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use Azure AD B2C for admin access and remove MFA requirements.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Store admin credentials in a key vault and rotate them monthly without approvals.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A is correct. Privileged Identity Management (PIM) provides just-in-time activation, enforces MFA during activation, and enables Access Reviews for ongoing governance of admin roles, aligning with least-privilege and compliance requirements.\n\n## Why Other Options Are Wrong\n- Option B creates permanent elevated access, increasing risk and deviating from least-privilege.\n- Option C uses a consumer-oriented identity service (B2C) not suited for privileged admin access and undermines security controls.\n- Option D relies on static credentials and lacks governance approvals, increasing risk of misuse.\n\n## Key Concepts\n- Azure AD Privileged Identity Management (PIM)\n- Just-in-time access\n- MFA on activation\n- Access Reviews for privileged roles\n\n## Real-World Application\nImplement PIM for production admins, require MFA and approval to activate, and run periodic reviews to ensure only supported users retain elevated rights, with automatic justification and audit logging for compliance evidence.","diagram":null,"difficulty":"intermediate","tags":["Azure","AzureAD","PIM","AccessReviews","RBAC","Terraform","Kubernetes","certification-mcq","domain-weight-25"],"channel":"azure-solutions-architect","subChannel":"design-identity","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T03:26:35.807Z","createdAt":"2026-01-13 03:26:35"},{"id":"azure-solutions-architect-design-identity-1768274794495-4","question":"To enforce organization-wide cost governance across multiple subscriptions and prevent unexpected spend, where should you place budgets and set alerts for maximum effectiveness?","answer":"[{\"id\":\"a\",\"text\":\"Create budgets at the subscription scope for each subscription independently.\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Create budgets at the management group scope to cover all subscriptions under the group and configure alerts.\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Rely on tags and manual reviews to track costs across resources.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Place budgets only at the resource group scope.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct. Budgets at the management group scope provide centralized cost governance across multiple subscriptions, enabling holistic alerts and more consistent enforcement.\n\n## Why Other Options Are Wrong\n- Option A distributes governance and makes cross-subscription cost correlation harder.\n- Option C relies on manual processes, which is error-prone and inefficient.\n- Option D limits visibility to a subset of resources, missing broader organizational spend.\n\n## Key Concepts\n- Azure Cost Management budgets\n- Management Group scope\n- Cross-subscription cost governance\n\n## Real-World Application\nIn practice, define a management group budget aligned to the organizational cost targets, enable alerts at appropriate thresholds, and review spend trends in the Cost Management dashboard to keep cost growth in check across all subscriptions.","diagram":null,"difficulty":"intermediate","tags":["Azure","CostManagement","Budgets","ManagementGroups","Terraform","Kubernetes","AWS_IAM","certification-mcq","domain-weight-25"],"channel":"azure-solutions-architect","subChannel":"design-identity","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T03:26:35.990Z","createdAt":"2026-01-13 03:26:36"},{"id":"azure-solutions-architect-design-infrastructure-1768162975592-0","question":"You are designing a multi-region web application in Azure with a stateless front end and globally distributed users. To minimize downtime during regional outages and keep user requests responsive, which design combination is most effective?","answer":"[{\"id\":\"a\",\"text\":\"Route traffic using Azure Traffic Manager in priority mode and rely on regional failover with a single-region database configuration\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Place the front end behind Azure Front Door with health-based failover and enable Cosmos DB multi-region writes to support low-latency reads and writes\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Use API Management as the gateway and implement client-side region detection while disabling cross-region replication\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Rely on DNS-based failover with a long TTL and perform manual cutover when an outage is detected\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct because Azure Front Door provides global HTTP/HTTPS routing with health probes and instant failover, while Cosmos DB multi-region writes enables low-latency reads and writes across regions, reducing RTO and RPO during regional outages.\n\n## Why Other Options Are Wrong\n- A: Traffic Manager in priority mode can route traffic, but without a global frontend like Front Door, failover is slower and DNS-based routing adds TTL-induced delays, increasing downtime.\n- C: API Management is a gateway for APIs, not a global load balancer; region-detection on clients and disabled cross-region replication increases complexity and latency, harming resilience.\n- D: DNS-based failover with long TTLs leads to high failover times and poor continuity during outages.\n\n## Key Concepts\n- Azure Front Door for global load balancing and instant failover\n- Cosmos DB multi-region writes for cross-region data distribution\n- Read/write locality and consistency choices affect latency and RPO/RTO\n\n## Real-World Application\nUsed in multi-region SaaS deployments to provide resilient user experiences during regional outages while maintaining responsive data access across geographies.","diagram":null,"difficulty":"intermediate","tags":["Azure","Frontend","CosmosDB","AWS-VPC","Kubernetes","Terraform","certification-mcq","domain-weight-30"],"channel":"azure-solutions-architect","subChannel":"design-infrastructure","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T20:22:55.593Z","createdAt":"2026-01-11 20:22:56"},{"id":"azure-solutions-architect-design-infrastructure-1768162975592-1","question":"Your organization operates an Azure SQL Database that must remain available during regional outages with minimal data loss and supports reporting workloads from a secondary region. Which feature should you implement?","answer":"[{\"id\":\"a\",\"text\":\"Geo-restore without automated failover\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Auto-failover group across primary and secondary regions with readable secondaries\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Manual cross-region replication without failover capability\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Backup and restore to the secondary region on every query\",\"isCorrect\":false}]","explanation":"## Correct Answer\nAuto-failover groups enable automatic failover to a configured secondary region and provide readable secondaries for off-load reporting, ensuring minimal downtime and data loss.\n\n## Why Other Options Are Wrong\n- a: Geo-restore provides point-in-time recovery but does not offer automatic failover or readable secondaries for ongoing workloads.\n- c: Manual cross-region replication without automated failover increases downtime and operator risk.\n- d: Backups/restores are too slow for real-time DR needs and are not suitable for ongoing reporting workloads.\n\n## Key Concepts\n- Azure SQL Database auto-failover groups\n- Readable secondaries and automatic failover\n- Cross-region disaster recovery planning\n\n## Real-World Application\nCommonly used for mission-critical databases where DR readiness and rapid failover are required, such as e-commerce or financial services platforms.","diagram":null,"difficulty":"intermediate","tags":["Azure","SQL","DR","AWS-VPC","Kubernetes","Terraform","certification-mcq","domain-weight-30"],"channel":"azure-solutions-architect","subChannel":"design-infrastructure","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T20:22:56.124Z","createdAt":"2026-01-11 20:22:56"},{"id":"azure-solutions-architect-design-infrastructure-1768162975592-2","question":"You need to grant developers access across multiple Azure subscriptions with least privilege and scalable governance, without creating per-subscription administrator accounts. Which approach best achieves this?","answer":"[{\"id\":\"a\",\"text\":\"Create global admin accounts and assign them to each subscription\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use Azure Lighthouse to delegate resource management across subscriptions with scoped RBAC\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Assign broad custom roles at the tenant level for all subscriptions\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use permanent service principals with full access in each subscription\",\"isCorrect\":false}]","explanation":"## Correct Answer\nAzure Lighthouse enables delegated resource management across multiple tenants and subscriptions, allowing you to grant least-privilege access to external partners or internal teams without elevating global privileges, and supports scoped RBAC for fine-grained control.\n\n## Why Other Options Are Wrong\n- a: Global admin accounts create broad, unsafe access across all subscriptions and do not scale well.\n- c: Tenant-wide broad roles violate least privilege and reduce accountability.\n- d: Permanent full-access service principals create long-term risk and governance challenges.\n\n## Key Concepts\n- Azure Lighthouse for cross-tenant/resource delegation\n- Scoped RBAC and least-privilege access\n- Governance and compliance in multi-subscription environments\n\n## Real-World Application\nCommon in MSP partnerships and large enterprises needing controlled cross-subscription operations with auditable access controls.","diagram":null,"difficulty":"intermediate","tags":["Azure","RBAC","Lighthouse","AWS-VPC","Kubernetes","Terraform","certification-mcq","domain-weight-30"],"channel":"azure-solutions-architect","subChannel":"design-infrastructure","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T20:22:56.657Z","createdAt":"2026-01-11 20:22:56"},{"id":"azure-solutions-architect-design-infrastructure-1768289246534-0","question":"You are designing a global, multi-region Azure network using a hub-and-spoke topology with a central on-premises gateway. You require private, high-throughput connectivity with a predictable SLA and minimal internet exposure for all traffic between on-prem and Azure. Which design choice best meets these requirements?","answer":"[{\"id\":\"a\",\"text\":\"Provision Azure ExpressRoute private peering for the hub VNet and connect the on-prem circuit, then VNet peering to spoke VNets.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Use a Site-to-Site VPN over the internet between the on-prem and the hub VNet, with VNet peering to spokes.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use Global VNet Peering between all VNets to share private IP addresses.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Publish all services to the public internet and expose them via an Internet-facing load balancer.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA. ExpressRoute private peering provides private, dedicated connectivity with a SLA and predictable latency, and hub-and-spoke topology with VNet peering extends private connectivity to spoke VNets.\n\n## Why Other Options Are Wrong\nB. VPN over the public internet adds exposure and typically higher and less predictable latency; acceptable as backup but not the chosen design.\nC. Global VNet Peering does not provide private on-prem connectivity and can complicate traffic routes.\nD. Exposing services publicly increases risk and violates private connectivity requirements.\n\n## Key Concepts\n- ExpressRoute private peering\n- Hub-and-spoke network topology\n- VNet peering for spokes\n- Private connectivity SLAs\n\n## Real-World Application\nUse ExpressRoute private peering to connect on-prem data centers to a central hub VNet and then use peering to connect regional spokes, ensuring private, reliable traffic between on-prem and Azure.","diagram":null,"difficulty":"intermediate","tags":["Azure","ExpressRoute","Networking","Terraform","AzurePrivateDNS","certification-mcq","domain-weight-30"],"channel":"azure-solutions-architect","subChannel":"design-infrastructure","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T07:27:26.535Z","createdAt":"2026-01-13 07:27:26"},{"id":"azure-solutions-architect-design-infrastructure-1768289246534-1","question":"You're deploying a private, multi-region AKS-based application. You need to ensure internal DNS resolution and private access to services across VNets in two regions. Which approach best achieves private DNS resolution and cross-region access?","answer":"[{\"id\":\"a\",\"text\":\"Create Azure Private DNS zones for the internal domain and link them to both VNets, ensure private endpoints or internal load balancers are used for services.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Use public DNS and rely on public IP addresses to reach services.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Rely on Azure Traffic Manager to route traffic to regional endpoints using public IPs.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use Azure Front Door with WAF to route private endpoints.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA. Private DNS zones linked to both VNets provide private name resolution across regions, and using private endpoints or internal load balancers keeps traffic within Azure network boundaries.\n\n## Why Other Options Are Wrong\nB. Public DNS/Public IPs expose internal services to the Internet and defeat private resolution.\nC. Traffic Manager operates at DNS level for public endpoints, not private DNS resolution for cross-region private connectivity.\nD. Front Door targets public edge delivery; not suitable for private cross-region internal DNS resolution.\n\n## Key Concepts\n- Azure Private DNS zones\n- VNet linking and private endpoints\n- AKS private access patterns\n\n## Real-World Application\nFor multi-region AKS deployments, bind services to private endpoints and unify DNS under Private DNS zones to ensure fast, private cross-region service discovery.","diagram":null,"difficulty":"intermediate","tags":["Azure","AKS","Terraform","Networking","PrivateDNS","certification-mcq","domain-weight-30"],"channel":"azure-solutions-architect","subChannel":"design-infrastructure","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T07:27:27.056Z","createdAt":"2026-01-13 07:27:27"},{"id":"azure-solutions-architect-design-infrastructure-1768289246534-2","question":"For disaster recovery of a VMware-based workload hosted in Azure, requiring near-zero data loss and automated failover between regional replicas, which Azure service best fits this requirement?","answer":"[{\"id\":\"a\",\"text\":\"Azure Site Recovery (ASR) to replicate and orchestrate failover to a secondary region.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Azure Backup to periodically snapshot and restore VMs in a different region.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Azure Monitor with alert-based remediation and manual failover.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"VMware Site Recovery Manager (SRM) deployed in Azure.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA. Azure Site Recovery provides automated replication, failover orchestration, and near-zero data loss for VMware-based workloads across regions.\n\n## Why Other Options Are Wrong\nB. Backups alone do not provide automated failover or near-zero RPO for continuous DR.\nC. Monitoring with alerts does not perform automatic failover of workloads.\nD. SRM is a VMware product that is not natively deployed as a service within Azure; ASR is the supported Azure service for DR orchestration.\n\n## Key Concepts\n- Replication and failover orchestration\n- VMware workloads in Azure (AVS/VMware on Azure) DR paths\n\n## Real-World Application\nImplement ASR to automatically replicate VMs from your primary region to a secondary region and enable one-click or automatic failover during regional outages.","diagram":null,"difficulty":"intermediate","tags":["Azure","ASR","VMware","DR","Terraform","certification-mcq","domain-weight-30"],"channel":"azure-solutions-architect","subChannel":"design-infrastructure","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T07:27:27.595Z","createdAt":"2026-01-13 07:27:27"},{"id":"azure-solutions-architect-design-migrations-1768249837297-0","question":"An on-premises SQL Server 2019 database of 500 GB needs to migrate to Azure with minimal downtime, targeting Azure SQL Managed Instance. Which migration approach best enables online data replication and a final cutover?","answer":"[{\"id\":\"a\",\"text\":\"Use Azure Site Recovery to replicate the SQL Server VM to an Azure IaaS VM and perform the cutover when ready\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use Azure Database Migration Service to perform an online migration with continuous data replication and a final cutover\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Take a full backup on-prem, restore to Azure SQL Database Managed Instance in a single downtime window\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Migrate data with Azure Data Factory in a batch transfer during maintenance window\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct option is B because Azure Database Migration Service (DMS) supports online migrations for SQL Server to Azure SQL Managed Instance with continuous data replication, enabling a final cutover with minimal downtime.\n\n## Why Other Options Are Wrong\n- A: ASR would move or replicate VMs; it does not migrate the database directly to Azure SQL Managed Instance and typically incurs higher downtime risk for DB state.\n- C: A full backup/restore is offline and does not support minimal downtime for large databases.\n- D: Data Factory handles ETL/ELT pipelines; it is not designed for live database migrations with minimal downtime.\n\n## Key Concepts\n- Azure Database Migration Service (DMS)\n- Online migration and continuous data replication\n- Cutover planning for minimal downtime\n\n## Real-World Application\n- Validate source compatibility, test migration window, and plan a rollback; coordinate with app teams for a synchronized cutover.","diagram":null,"difficulty":"intermediate","tags":["AzureMigrate","AzureDatabaseMigrationService","ExpressRoute","Terraform","AKS","AWSMigration","certification-mcq","domain-weight-10"],"channel":"azure-solutions-architect","subChannel":"design-migrations","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T20:30:37.298Z","createdAt":"2026-01-12 20:30:37"},{"id":"azure-solutions-architect-design-migrations-1768249837297-1","question":"Your organization has a three-tier on-premises application (web front-end, business logic, and database). You want to migrate to Azure with minimal downtime and first identify dependencies to plan migration order. Which toolchain provides dependency discovery and a guided migration plan for both application tiers and database?","answer":"[{\"id\":\"a\",\"text\":\"Use Azure Migrate to discover on-prem dependencies, visualize application topology, and then use Database Migration Service for the database migration\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Use Azure Site Recovery to migrate the VMs and rehost everything as IaaS\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use Database Migration Service alone without dependency discovery\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use manual inventory and scripting to map dependencies\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A is correct because Azure Migrate provides dependency visualization and discovery, helping plan an orderly migration of application tiers; DMS can then handle the database migration.\n\n## Why Other Options Are Wrong\n- B: ASR focuses on lifting and shifting VMs, not on planning migrations to PaaS components with dependency mapping.\n- C: DMS alone cannot map on-prem dependencies or plan migration order for multiple tiers.\n- D: Manual inventory is error-prone and less scalable for complex topologies.\n\n## Key Concepts\n- Azure Migrate Dependency Visualization\n- Application discovery and dependency mapping\n- Database Migration Service (DMS)\n\n## Real-World Application\n- Produces an actionable migration plan with staged cutover across web, app, and database tiers.","diagram":null,"difficulty":"intermediate","tags":["AzureMigrate","AzureDatabaseMigrationService","ExpressRoute","Terraform","AWSMigration","certification-mcq","domain-weight-10"],"channel":"azure-solutions-architect","subChannel":"design-migrations","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T20:30:37.683Z","createdAt":"2026-01-12 20:30:37"},{"id":"azure-solutions-architect-design-migrations-1768249837297-2","question":"During migration planning, you must ensure data is transmitted over private connectivity and remains encrypted in transit, aligning with compliance requirements. Which approach best satisfies this?","answer":"[{\"id\":\"a\",\"text\":\"Use Azure Migrate with ExpressRoute or VPN private connectivity and perform migrations with DMS\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Use the public Internet with TLS for all data transfers\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use FTP over TLS for all migration data transfers\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use Azure Data Factory to move data over the public Internet\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A is correct because private connectivity (ExpressRoute or VPN) ensures data remains within trusted networks and TLS encrypts data in transit; this closely aligns with governance and compliance needs during migrations using DMS.\n\n## Why Other Options Are Wrong\n- B: Public Internet increases exposure and may violate strict compliance requirements.\n- C: FTP over TLS is obsolete for modern migrations and not integrated into migration services.\n- D: Data Factory over public Internet does not guarantee private, policy-compliant transit for migrations.\n\n## Key Concepts\n- ExpressRoute/VPN, private connectivity\n- Encryption in transit\n- Migration orchestration with DMS\n\n## Real-World Application\n- Architect network design to meet regulatory demands and perform secure migrations.","diagram":null,"difficulty":"intermediate","tags":["AzureMigrate","ExpressRoute","AzureDatabaseMigrationService","Terraform","AWSMigration","certification-mcq","domain-weight-10"],"channel":"azure-solutions-architect","subChannel":"design-migrations","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T20:30:38.060Z","createdAt":"2026-01-12 20:30:38"},{"id":"azure-solutions-architect-design-migrations-1768249837297-3","question":"An on-premises PostgreSQL database needs to move to Azure Database for PostgreSQL - Flexible Server with minimal downtime. Which migration method should you choose?","answer":"[{\"id\":\"a\",\"text\":\"Use Azure Database Migration Service in online mode with change data capture\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Dump the database to a SQL file and import into Azure PostgreSQL with downtime\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use Data Factory to continuously copy data during the cutover window\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Run a manual logical backup and restore with no replication\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A is correct because Azure Database Migration Service supports online migrations for PostgreSQL with change data capture, enabling near-zero downtime.\n\n## Why Other Options Are Wrong\n- B: Offline dumps cause downtime and are unreliable for large databases.\n- C: Data Factory is not optimized for live PostgreSQL migrations with minimal downtime.\n- D: Manual backups without replication do not support minimal downtime transitions.\n\n## Key Concepts\n- DMS for PostgreSQL online migrations\n- Change Data Capture (CDC)/logical replication\n\n## Real-World Application\n- Plan cutover timing, validate replication lag, and monitor performance during migration.","diagram":null,"difficulty":"intermediate","tags":["AzureMigrate","AzureDatabaseMigrationService","PostgreSQL","Terraform","AWSMigration","certification-mcq","domain-weight-10"],"channel":"azure-solutions-architect","subChannel":"design-migrations","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T20:30:38.194Z","createdAt":"2026-01-12 20:30:38"},{"id":"azure-solutions-architect-design-migrations-1768249837297-4","question":"To satisfy data residency and governance requirements, where should you deploy the migration service and the target data store when migrating on-premises data to Azure?","answer":"[{\"id\":\"a\",\"text\":\"Deploy Azure Database Migration Service in the same region as the target database and migrate from on-prem to that region\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Deploy the migration service in a different region from the target and route data cross-region\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Deploy the migration service in any region; region does not affect data residency\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use cross-region data transfer and then replicate back to the target region\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A is correct because keeping the migration service in the same region as the target database helps satisfy data residency requirements and reduces cross-region data transfer latencies.\n\n## Why Other Options Are Wrong\n- B: Cross-region data transfer can violate residency requirements and introduce latency.\n- C: Region matters for residency; deploying in any region may breach controls.\n- D: Cross-region transfer adds unnecessary complexity and compliance risk.\n\n## Key Concepts\n- Data residency and regional deployment\n- Regional placement of DMS and data stores\n\n## Real-World Application\n- Align migration tooling with regulatory constraints and audit trails.","diagram":null,"difficulty":"intermediate","tags":["AzureMigrate","AzureDatabaseMigrationService","ExpressRoute","Terraform","AWSMigration","certification-mcq","domain-weight-10"],"channel":"azure-solutions-architect","subChannel":"design-migrations","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T20:30:38.328Z","createdAt":"2026-01-12 20:30:38"},{"id":"q-1046","question":"You're building a regulated, multi-tenant analytics platform on Azure that ingests IoT and application logs from customers across three continents. Customers demand regional data residency while analytics must be global for cross-tenant benchmarks. Propose a practical, cost-conscious architecture that enforces per-tenant data isolation (at rest and in transit), regional ingestion, geo-redundant storage, cross-region analytics, and auditable access control using Azure native services. Include data plane vs control plane separation, and show how you'd satisfy RPO/RTO targets and regulatory requirements?","answer":"Per-tenant ADLS Gen2 lakes with hierarchical namespaces and tenantId prefixes; region-bound landing zones with geo-replication (GRS) and Private Endpoints. Ingest via Event Hubs, process with Databric","explanation":"## Why This Is Asked\nThis question probes Azure-era architectural choices for data residency, isolation, and cross-region analytics under compliance constraints.\n\n## Key Concepts\n- Per-tenant data isolation in ADLS Gen2 with prefixing\n- Geo-redundant storage and Private Endpoints\n- Ingestion (Event Hubs), processing (Databricks), regional analytics (Synapse)\n- Governance (Purview, Entra ID RBAC) and auditability\n- Data plane vs control plane separation; DR/RTO/RPO planning\n\n## Code Example\n```javascript\n// Pseudocode: route tenant data to regional lake\nconst region = getRegionForTenant(tenantId);\nwriteToRegionLake(tenantId, payload, region);\n```\n\n## Follow-up Questions\n- How would you enforce data residency while enabling cross-tenant analytics?\n- What trade-offs exist with Synapse vs Databricks for cross-region workloads?","diagram":"flowchart TD\n  A[Ingest] --> B[Landing (Region)]\n  B --> C[Process (Databricks)]\n  C --> D[Publish (Regional Synapse)]\n  D --> E[Global Analytics (Aggregates)]\n  E --> F[Governance (Purview, RBAC)]","difficulty":"intermediate","tags":["azure-solutions-architect"],"channel":"azure-solutions-architect","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T20:32:24.621Z","createdAt":"2026-01-12T20:32:24.621Z"},{"id":"q-1181","question":"You operate a fintech SaaS platform serving tenants across US, EU, and APAC. Each tenant's data must reside regionally at rest, yet global analytics require anonymized cross-tenant insights. Describe an Azure-native architecture that (1) enforces per-tenant data isolation in storage and processing, (2) supports real-time ingestion of fraud/transaction events, (3) enables cross-region analytics without tenant leakage, (4) meets DR targets with RPO <15 minutes and RTO <5 minutes, and (5) provides end-to-end auditing and governance. Include components, data flows, trade-offs, and a concrete failover test plan?","answer":"Design an Azure-native, per-tenant isolated data pipeline for a fintech SaaS with tenants across US/EU/APAC. Ingest real-time fraud events via tenant-scoped Event Hubs, land regionally in ADLS Gen2, p","explanation":"## Why This Is Asked\nAssesses ability to design Azure-first, multi-region data architectures that isolate customer data, scale real-time ingestion, and satisfy strict DR and governance requirements for fintech customers.\n\n## Key Concepts\n- Per-tenant isolation in storage and compute (ADLS Gen2, Delta Lake, Unity Catalog RBAC)\n- Real-time ingestion using tenant-scoped Event Hubs\n- Cross-region analytics with anonymization and secure data sharing\n- DR strategy with RPO <15m and RTO <5m ( geo-redundant storage, cross-region replicas )\n- Auditing and governance via Purview and Azure Monitor; CMK in Key Vault\n\n## Code Example\n```json\n{\n  \"type\": \"Microsoft.EventHub/namespaces\",\n  \"apiVersion\": \"2021-06-01\",\n  \"name\": \"tenant-{tenantId}-namespace\",\n  \"location\": \"eastus\",\n  \"properties\": {}\n}\n```\n\n## Follow-up Questions\n- How would you test the DR failover to ensure <15m RPO in practice?\n- What are the security trade-offs between tenant-scoped Event Hubs and a shared analytics layer?\n- How would Purview classifications integrate with Unity Catalog RBAC for per-tenant auditing?","diagram":"flowchart TD\n  A[Ingest Events via Tenant-scoped Event Hubs] --> B[Regional ADLS Gen2 Data Lakes]\n  B --> C[Delta Lake Processing on Synapse/Databricks]\n  C --> D[Per-tenant RBAC via Unity Catalog]\n  D --> E[Global Analytics Layer (Anonymized Aggregates)]\n  E --> F[Cross-Region Replication & DR Copy]\n  F --> G[Failover Test & Validation]","difficulty":"advanced","tags":["azure-solutions-architect"],"channel":"azure-solutions-architect","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Plaid","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T03:42:59.465Z","createdAt":"2026-01-13T03:42:59.465Z"},{"id":"q-1305","question":"You manage a global healthcare analytics platform on Azure. Regulations require that **PHI** stays in-country while **non-PHI** can aggregate regionally. Propose an end-to-end data pipeline using **Azure Data Lake Storage Gen2**, **Data Factory**/Synapse, and **Purview** to enforce residency, enable regional analytics, and provide auditable data lineage and masking. Include encryption, governance, and failover strategies across regions?","answer":"Per-country PHI stored in country-specific ADLS Gen2 accounts with customer-managed keys in Key Vault; non-PHI data mirrored to regional analytics lakes for aggregated insights. Use Data Factory pipel","explanation":"## Why This Is Asked\nAssess the ability to design data residency with cross-border analytics, governance, and DR in Azure.\n\n## Key Concepts\n- Data residency and sovereignty\n- PHI masking and data classification\n- Per-country ADLS Gen2 storage with CMK\n- Regional analytics lakes and cross-region replication of non-PHI\n- Purview for lineage and data governance\n- Azure Policy and RBAC enforcement\n- DR with region pairs and automated data movement\n\n## Code Example\n```\n```json\n{\n  \"policyDefinition\": {\n    \"mode\": \"Indexed\",\n    \"policyRule\": {\n      \"if\": { \"field\": \"type\", \"equals\": \"Microsoft.Storage/storageAccounts\" },\n      \"then\": { \"effect\": \"deny\" }\n    }\n  }\n}\n```\n```\n\n## Follow-up Questions\n- How would you validate residency compliance during CI/CD?\n- How would you handle retention and cross-region analytics latency?\n","diagram":"flowchart TD\n  Ingest[Ingest Data] --> Route[Route PHI to in-country lake; non-PHI to regional lake]\n  Route --> Persist[Persist to ADLS Gen2 per region]\n  Persist --> Govern[Governance & lineage with Purview]\n  Govern --> Analyze[Analytics in-region]","difficulty":"advanced","tags":["azure-solutions-architect"],"channel":"azure-solutions-architect","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Instacart","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T08:51:58.189Z","createdAt":"2026-01-13T08:51:58.189Z"},{"id":"q-887","question":"Youâ€™re building a multi-tenant analytics platform on Azure for a consumer-brand SaaS product. Each tenant must have isolated data processing, with per-tenant data lake isolation, on-demand Spark/notebook compute that auto-suspends, and cost governance at the tenant level. Propose an architecture using Azure Data Lake Storage Gen2, Unity Catalog or RBAC, Synapse or Databricks, private endpoints, and auditing. How do you ensure data isolation, prevent cross-tenant leakage, and meet compliance while keeping ops simple?","answer":"Use per-tenant isolation via either separate Data Lake Storage Gen2 accounts or a single lake with strict namespaces and Unity Catalog RBAC. Provision on-demand Spark/notebook pools with auto-suspend,","explanation":"## Why This Is Asked\nAssess practical multi-tenant analytics architecture in Azure focusing on data isolation, cost governance, and compliance for a customer-facing platform.\n\n## Key Concepts\n- Data isolation models (per-tenant vs shared lake)\n- Unity Catalog/RBAC scoping by tenant\n- Auto-suspend compute and per-tenant budgets\n- Private Endpoints and encryption at rest per tenant\n- Centralized audit/log retention and compliance\n\n## Code Example\n```javascript\n// Placeholder IaC snippet illustrating per-tenant RBAC scope\nconst tenantScope = getTenantScope(\"tenantA\");\nrbac.assignRole(tenantScope, \"DataReader\");\n// This is illustrative; implementation will use your chosen IaC tool\n```\n\n## Follow-up Questions\n- How would you scale onboarding/offboarding tenants without downtime?\n- How would you validate no cross-tenant data leakage in tests?","diagram":"flowchart TD\n  Tenant[Tenant] --> Lake[Data Lake Gen2]\n  Tenant --> Compute[Notebook/Compute]\n  Lake --> UC[Unity Catalog RBAC]\n  Compute --> Private[Private Endpoints]\n  Compute --> AuditSink[Audit Sink] --> Compliance[Compliance & Retention]","difficulty":"intermediate","tags":["azure-solutions-architect"],"channel":"azure-solutions-architect","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","Microsoft","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T14:25:45.635Z","createdAt":"2026-01-12T14:25:45.635Z"}],"subChannels":["design-business-continuity","design-data-storage","design-identity","design-infrastructure","design-migrations","general"],"companies":["Databricks","Google","Instacart","Microsoft","OpenAI","Plaid","Snap"],"stats":{"total":33,"beginner":0,"intermediate":31,"advanced":2,"newThisWeek":33}}