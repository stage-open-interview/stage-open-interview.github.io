{"questions":[{"id":"q-1121","question":"Scenario: You operate a shared Kubernetes cluster serving multiple product teams. You must prevent cross-namespace data leakage and enforce least-privilege access while remaining auditable and scalable. Describe a concrete strategy using either OPA Gatekeeper or Kyverno for admission control (with at least two constraints), implement namespace RBAC boundaries, apply Calico NetworkPolicy for namespace isolation, and outline a monitoring/audit plan with tests and runbooks. Include example policies and a minimal test commands snippet?","answer":"Implement policy-as-code with admission controls using either OPA Gatekeeper or Kyverno, enforcing at least two constraints: (1) workloads must run in approved namespaces, (2) pods must not run as pri","explanation":"## Why This Is Asked\n\nEvaluates practical multi-tenant security controls, policy-as-code, and operational testing in Kubernetes.\n\n## Key Concepts\n\n- Admission control with Gatekeeper or Kyverno\n- Namespace RBAC scoping\n- Calico NetworkPolicy isolation\n- Auditing and runbooks\n- CI validation and drift checks\n\n## Code Example\n\n```javascript\napiVersion: constraints.gatekeeper.sh/v1beta1\nkind: K8sNSApproved\nmetadata:\n  name: ns-approved\nspec:\n  match:\n    namespaces: [\"approved-*\"]\n```\n\n```javascript\napiVersion: kyverno.io/v1\nkind: ClusterPolicy\nmetadata:\n  name: restrict-privileges\nspec:\n  rules:\n  - name: disallow-privileged\n    match:\n      resources:\n        kinds: [\"Pod\"]\n    validate:\n      message: \"Privileged containers are not allowed\"\n      pattern:\n        spec:\n          containers:\n          - securityContext:\n              allowPrivilegeEscalation: false\n              readOnlyRootFilesystem: true\n```\n\n```javascript\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: deny-all\nspec:\n  podSelector: {}\n  policyTypes: [\"Ingress\",\"Egress\"]\n  ingress: []\n  egress: []\n```\n\n## Follow-up Questions\n\n- How would you test these policies in CI?\n- How would you handle policy drift and remediation?\n","diagram":"flowchart TD\n  A[Namespaces] --> B[RBAC]\n  B --> C[AdmissionPolicy]\n  C --> D[NetworkPolicy]\n  D --> E[Auditing]","difficulty":"intermediate","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","IBM","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T23:29:20.472Z","createdAt":"2026-01-12T23:29:20.472Z"},{"id":"q-1130","question":"You're running a Kubernetes cluster for a web app. A Pod mounting hostPath and running as root was detected in dev. Outline a practical plan to enforce least privilege across namespaces (Baseline/Restricted) using a policy engine (Kyverno or OPA Gatekeeper) and show how you would validate enforcement without disrupting workloads. What steps and files would you use?","answer":"Implement a baseline Pod Security Standard across namespaces by enabling Kyverno policy or OPA Gatekeeper constraint. Enforce runAsNonRoot: true, readOnlyRootFilesystem: true, disallow hostPath, and d","explanation":"## Why This Is Asked\nA practical beginner-level check for implementing and validating security controls in Kubernetes using policy engines, focusing on least privilege and safe defaults.\n\n## Key Concepts\n- Pod Security Standards Baseline/Restricted\n- Policy engines: Kyverno, OPA Gatekeeper\n- Privilege escalation controls: runAsNonRoot, readOnlyRootFilesystem, capabilities\n- Testing methods: compliant vs non-compliant pods, namespace scoping\n- Change management and rollback\n\n## Code Example\n```javascript\n// Conceptual Kyverno policy (JS object for illustration)\nconst policy = {\n  apiVersion: 'kyverno/v1',\n  kind: 'ClusterPolicy',\n  metadata: { name: 'require-baseline-security' },\n  spec: {\n    rules: [\n      {\n        name: 'require-baseline',\n        match: { resources: { kinds: ['Pod'] }},\n        validate: {\n          message: 'Pods must set runAsNonRoot, readOnlyRootFilesystem and avoid hostPath',\n          pattern: {\n            spec: {\n              containers: { any: { securityContext: { runAsNonRoot: true, readOnlyRootFilesystem: true } }},\n              hostPath: { absent: {} }\n            }\n          }\n        }\n      }\n    ]\n  }\n}\n```\n\n## Follow-up Questions\n- How would you handle exceptions for legitimate hostPath usage?\n- How would you monitor for policy violations in production?","diagram":"flowchart TD\n  A[Define Baseline] --> B[Apply to Namespace]\n  B --> C[Test Compliant Pod]\n  C --> D[Test Violating Pod]\n  D --> E[Monitor & Iterate]","difficulty":"beginner","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T01:22:06.201Z","createdAt":"2026-01-13T01:22:06.201Z"},{"id":"q-1167","question":"Scenario: You operate a multi-cluster Kubernetes data platform (cloud+on‑prem) where a Spark job can access customer data. Design an end-to-end approach to detect, prevent, and respond to data exfiltration attempts from pods across clusters. Include policy design, telemetry signals, enforcement, and incident runbooks; discuss trade-offs?","answer":"Adopt a data-classification driven policy wired through OPA Gatekeeper and Kyverno, enforce egress with default-deny for pods without explicit allow, and centralize audit logs across clusters. Use lab","explanation":"## Why This Is Asked\nTests ability to design end-to-end controls across multi-cluster environments, not just single-cluster examples.\n\n## Key Concepts\n- Data classification and tagging in Kubernetes\n- OPA Gatekeeper and Kyverno policy frameworks\n- Egress controls with Calico/Cilium\n- Kubernetes audit logging and tamper-evident storage\n- Spark / data platform security\n\n## Code Example\n```javascript\n// Example policy sketch (pseudo)\nfunction policy(pod, action, dest){ /* evaluate against labels, namespaces, and allowlists */ }\n```\n\n## Follow-up Questions\n- How would you test policy drift at scale?\n- What telemetry would you collect to distinguish exfiltration from legitimate data movement?","diagram":"flowchart TD\n  A[Data classification] --> B[Policy evaluation]\n  B --> C{Allow or Deny}\n  C --> D[Enforcement point]\n  A --> E[Audit & telemetry]\n  D --> E","difficulty":"advanced","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Instacart"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T03:32:11.099Z","createdAt":"2026-01-13T03:32:11.099Z"},{"id":"q-1278","question":"Scenario: A fintech data platform runs a multi-tenant data lake on Kubernetes. Each data job uses per-job ServiceAccounts to access restricted cloud storage. A rogue pod tries to exfiltrate data via the bucket. Propose a security approach that binds each pod to a dedicated cloud IAM role (workload identity), enforces namespace-scoped permissions, and provides tamper-evident audit trails. Include detection and response for abnormal egress and a safe rotation plan. What trade-offs?","answer":"Bind each pod to a dedicated cloud IAM role via workload identity federation, with namespace-scoped permissions and least privilege. Disable instance metadata access, use private endpoints, and short-","explanation":"## Why This Is Asked\n\nThis question assesses practical implementation of workload identity, RBAC scoping, and robust audit/response in a fintech-like Kubernetes deployment.\n\n## Key Concepts\n\n- Workload Identity Federation\n- Least privilege RBAC and namespace isolation\n- IMDS access control and private endpoints\n- Immutable audit logs and egress monitoring\n- Credential rotation and incident response\n\n## Code Example\n\n```yaml\n# GKE Workload Identity binding example\napiVersion: v1\nkind: Pod\nmetadata:\n  name: data-job\n  annotations:\n    iam.gke.io/gcp-service-account: data-job-sa@PROJECT.iam.gserviceaccount.com\nspec:\n  serviceAccountName: data-job-sa\n```\n\n## Follow-up Questions\n\n- How would you test these controls end-to-end?\n- How would you rotate keys without restarting jobs?\n- How would you handle missed bindings or fallback scenarios?","diagram":null,"difficulty":"advanced","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T07:41:55.251Z","createdAt":"2026-01-13T07:41:55.251Z"},{"id":"q-1301","question":"You're debugging a Kubernetes deployment in a multi-tenant environment where one namespace's pods delay startup by several minutes. Provide a practical, beginner-friendly diagnostic flow focusing on pod events, init containers, image pulls, and config maps. List concrete kubectl commands you would run and how you’d determine the root cause?","answer":"Begin with: kubectl describe pod <pod> -n <ns> to surface events and init-container status. Inspect init containers with kubectl get pod <pod> -n <ns> -o jsonpath '{.status.initContainerStatuses[*].st","explanation":"## Why This Is Asked\n\nTests practical Kubernetes debugging skills, focusing on actionable steps, not theory. It checks familiarity with pod lifecycle, init containers, and how to correlate events with configuration objects during startup delays.\n\n## Key Concepts\n\n- Pod lifecycle and init containers\n- kubectl debugging commands (describe, get, logs, jsonpath)\n- Image pull behavior and identifiers\n- ConfigMaps and Secrets mounting in pods\n- Deployment rollout and readiness checks\n\n## Code Example\n\n```javascript\n// Example diagnostic sequence (pseudo-commands, replace <pod> and <ns> accordingly)\nkubectl describe pod <pod> -n <ns>\nkubectl get pod <pod> -n <ns> -o jsonpath '{.status.initContainerStatuses[*].state}'\nkubectl logs <pod> -c <init-container> -n <ns>\nkubectl describe deployment <deploy> -n <ns>\nkubectl get cm -n <ns>\nkubectl get secret -n <ns>\n```\n\n## Follow-up Questions\n\n- What would you check if the pod remains Pending after image pull completes?\n- How would you distinguish between a misconfigured readinessProbe vs a slow startup?","diagram":"flowchart TD\n  A[Start] --> B[Describe Pod] \n  B --> C[InitContainers Status] \n  C --> D[Check InitContainer Logs] \n  D --> E[Inspect Image Pull Policy/IDs] \n  E --> F[Review ConfigMaps/Secrets] \n  F --> G[Check Deployment Rollout]\n  G --> H[Root Cause Identified]","difficulty":"beginner","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Netflix","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T08:47:12.930Z","createdAt":"2026-01-13T08:47:12.930Z"},{"id":"q-920","question":"In a real-time chat service like Discord, deployed on Kubernetes with NVIDIA GPUs for video processing, you introduce a third-party plugin system that runs as WebAssembly modules to apply custom video filters. How would you design a secure plugin sandbox and runtime attestation to prevent leakage of streams or keys, ensure isolation from other plugins, and enable rapid rollback if a plugin behaves unexpectedly in production? Provide concrete approaches and trade-offs?","answer":"Adopt a per-plugin WebAssembly sandbox with strict memory caps (e.g., 32–64MB), syscall filtering (seccomp), and an isolated FS; require attestation using a signed manifest and a hardware-backed key f","explanation":"## Why This Is Asked\n\nThis question probes pragmatic security engineering for dynamic plugin ecosystems in real-time services with GPU workloads. It tests risk modeling, tooling choices, and trade-offs between performance and isolation.\n\n## Key Concepts\n\n- WebAssembly sandboxing\n- syscall filtering and memory limits\n- hardware-backed attestation and KMS\n- per-plugin encryption and data isolation\n- runtime observability and quick rollback\n\n## Code Example\n\n```javascript\n// Verify plugin signature before load\nconst sig = loadSignature(plugin);\nconst ok = verifySignature(sig, plugin.code, TRUSTED_PUBLIC_KEY);\nif (!ok) throw new Error(\"Invalid plugin\");\n```\n\n## Follow-up Questions\n\n- How would you test the sandbox boundaries and detect escapes?\n- What would you monitor to detect a compromised plugin, and how would you roll back safely?","diagram":"flowchart TD\n  A[Plugin Load] --> B{Attestation Passed?}\n  B -->|Yes| C[Isolate in WASM sandbox]\n  B -->|No| D[Reject]\n  C --> E[Run in separate process]\n  E --> F[Audit Logs to tamper-evident store]","difficulty":"intermediate","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","NVIDIA","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T15:31:52.573Z","createdAt":"2026-01-12T15:31:52.573Z"},{"id":"q-959","question":"Scenario: A service executes user-provided Python plugins inside a container. Design a concrete runtime hardening plan using Linux namespaces, a minimal seccomp profile, and capability bounding, ensuring plugins cannot access host files or network directly while preserving IPC with a controlled channel. Outline exact steps and validation tests?","answer":"Drop all capabilities, run in a dedicated user namespace, mount root as read-only, and bind‑mount only the plugin assets at /plugins. Use a strict seccomp profile that whitelists essential syscalls (r","explanation":"## Why This Is Asked\n\nTests practical security hardening with knowledge of namespaces, seccomp, and capabilities—core skills for roles that handle untrusted code.\n\n## Key Concepts\n\n- Linux namespaces (user/net)\n- Capability bounding and dropping all caps\n- Seccomp profiles with syscall whitelists\n- Read-only root filesystem and selective bind mounts\n- Inter-process communication (IPC) channel integrity\n\n## Code Example\n\n```javascript\n// Example: docker run sandbox flags (conceptual)\nconst cmd = [\n  'docker', 'run', '--rm',\n  '--cap-drop', 'ALL',\n  '--security-opt', 'seccomp=seccomp-profile.json',\n  '--read-only',\n  '--network', 'none',\n  '--mount', 'type=bind,source=/path/to/plugins,target=/plugins',\n  'python-plugin-runner'\n];\n```\n\n## Follow-up Questions\n\n- How would you test for privilege escalation attempts from within the plugin?\n- What trade-offs exist between security and plugin functionality, and how would you mitigate them?","diagram":"flowchart TD\n  A[User Plugin] -->|Runs in sandbox| B[Isolated Container]\n  B --> C{Seccomp Filters}\n  C --> D[Allowed syscalls]\n  C --> E[Blocked syscalls]\n  B --> F[IPC Channel to Host]\n  F --> G[Controlled IPC Endpoint]","difficulty":"beginner","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["PayPal","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T16:44:33.574Z","createdAt":"2026-01-12T16:44:33.574Z"},{"id":"q-967","question":"Scenario: You manage a microservice app deployed to Kubernetes with CI/CD; you need to prevent tampered container images. **Describe a practical, beginner-friendly plan** to implement image signing and verification using **cosign**, integrate it into a GitHub Actions workflow, and enforce verification at deployment (registry or admission webhook). Include concrete commands?","answer":"Plan: sign container images with cosign in CI, store keys in a cloud KMS, rotate monthly, and require signature verification before deployment. In CI: sign on push, verify before publish. Commands to ","explanation":"## Why This Is Asked\nTest practical understanding of software supply chain security with beginner-friendly tooling.\n\n## Key Concepts\n- Image signing with cosign\n- Key management and rotation\n- CI/CD integration (GitHub Actions)\n- Deployment-time verification (registry or admission webhook)\n- Basic audit logging\n\n## Code Example\n```bash\ncosign generate-key-pair\ncosign sign docker.io/org/app:tag --key cosign.key\ncosign verify docker.io/org/app:tag --key cosign.pub\n```\n\n## Follow-up Questions\n- How would you rotate signing keys with minimal downtime?\n- How would you handle private registries and secret key storage in CI?\n","diagram":null,"difficulty":"beginner","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","NVIDIA","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T17:28:30.376Z","createdAt":"2026-01-12T17:28:30.376Z"},{"id":"q-994","question":"Scenario: A Kubernetes-based ML platform serves multiple teams; outbound data exfiltration is a breach risk. Propose a concrete, end-to-end control plane approach to prevent unauthorized data egress using policy-as-code, Kubernetes NetworkPolicy, and a centralized egress gateway. Include a sample Rego policy for Gatekeeper that enforces a namespace label data-export=allowed and an annotation egress-proxy=https://proxy.internal, and outline testing and GitOps integration?","answer":"An example: I implement an OPA Gatekeeper deny rule requiring pods to carry label data-export=allowed and annotation egress-proxy=https://proxy.internal; Gatekeeper blocks otherwise. A Calico NetworkP","explanation":"## Why This Is Asked\n\nThis tests practical application of policy-as-code (OPA Gatekeeper), Kubernetes security controls (NetworkPolicy), and GitOps-driven deployment, plus testing and trade-offs.\n\n## Key Concepts\n\n- OPA Gatekeeper\n- Rego policies\n- Kubernetes NetworkPolicy / Calico egress\n- GitOps (ArgoCD)\n- End-to-end testing\n\n## Code Example\n\n```rego\npackage gatekeeper.sample\n\ndeny[{\"msg\":\"Outbound egress blocked\",\"ns\":ns}]\n{ input.review.kind == \"AdmissionReview\"; obj := input.review.object; ns := obj.metadata.namespace; not obj.metadata.labels[\"data-export\"] == \"allowed\"; not obj.metadata.annotations[\"egress-proxy\"] }\n```\n\n## Follow-up Questions\n\n- How would you test this in a CI pipeline?\n- What are potential bypass vectors and mitigations?","diagram":null,"difficulty":"intermediate","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["OpenAI","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T18:39:01.707Z","createdAt":"2026-01-12T18:39:01.707Z"}],"subChannels":["general"],"companies":["Amazon","Anthropic","Apple","Bloomberg","Discord","DoorDash","Google","IBM","Instacart","NVIDIA","Netflix","OpenAI","PayPal","Robinhood","Snap","Stripe","Tesla","Twitter","Zoom"],"stats":{"total":9,"beginner":4,"intermediate":3,"advanced":2,"newThisWeek":9}}