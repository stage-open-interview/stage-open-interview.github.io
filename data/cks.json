{"questions":[{"id":"cks-cluster-hardening-1768235109717-0","question":"A developer needs to deploy pods into a dedicated namespace but must not access cluster-wide resources. Which RBAC configuration provides the least-privilege, namespace-scoped deployment capability?","answer":"[{\"id\":\"a\",\"text\":\"ClusterRole with full admin rights bound via ClusterRoleBinding\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Role in the target namespace granting create, get, list, and watch on pods; bound with RoleBinding to the user\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"ClusterRole with get and list on pods; bound via RoleBinding to the target namespace\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Role in the target namespace granting all permissions on all resources; bound with RoleBinding\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B. A Namespace-scoped Role bound via RoleBinding provides the minimal, namespace-limited privileges required for deployment without granting cluster-wide access.\n\n## Why Other Options Are Wrong\n- Option A grants cluster-wide admin rights, violating least privilege.\n- Option C uses a ClusterRole with only read permissions, which does not permit pod creation.\n- Option D grants excessive permissions (all resources) within the namespace.\n\n## Key Concepts\n- RBAC in Kubernetes\n- Namespace-scoped access control\n- Role vs ClusterRole\n- RoleBinding vs ClusterRoleBinding\n\n## Real-World Application\nIn multi-tenant clusters, assign developers to per-namespace Roles to isolate access and minimize blast radius while enabling legitimate workload deployment.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","RBAC","ClusterHardening","certification-mcq","domain-weight-15"],"channel":"cks","subChannel":"cluster-hardening","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T16:25:09.720Z","createdAt":"2026-01-12 16:25:10"},{"id":"cks-cluster-hardening-1768235109717-1","question":"You want to enforce Pod Security Standards across all namespaces in Kubernetes 1.22+ with minimal per-namespace changes. Which approach is recommended?","answer":"[{\"id\":\"a\",\"text\":\"Deploy and enable PodSecurityPolicy (PSP) resources and bindings\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Enable Pod Security Admission and label every namespace with pod-security.kubernetes.io/enforce=restricted\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Rely on NetworkPolicy to enforce container hardening policies\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use a mutating webhook to inject non-root users into all containers\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B. Pod Security Admission (the modern approach) enforces Pod Security Standards, and labeling namespaces with enforce=restricted applies the default policy cluster-wide without PSP, which is deprecated.\n\n## Why Other Options Are Wrong\n- Option A is incorrect because PSP has been deprecated and removed in newer Kubernetes versions.\n- Option C does not enforce Pod Security Standards; it addresses network security, not pod security.\n- Option D, while possible, is not the recommended default approach and adds complexity; it also relies on mutating behavior rather than policy enforcement.\n\n## Key Concepts\n- Pod Security Standards (PSS)\n- Pod Security Admission\n- Namespace labeling for enforcement\n- PSP deprecation\n\n## Real-World Application\nAdopt Pod Security Admission with namespace labels to consistently enforce restricted pods across all namespaces, simplifying governance and reducing misconfigurations.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","PodSecurity","PSA","ClusterHardening","certification-mcq","domain-weight-15"],"channel":"cks","subChannel":"cluster-hardening","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T16:25:10.400Z","createdAt":"2026-01-12 16:25:10"},{"id":"cks-cluster-hardening-1768235109717-2","question":"To detect privilege escalation attempts and retain evidence from Kubernetes RBAC events, which configuration best ensures auditable RBAC activity?","answer":"[{\"id\":\"a\",\"text\":\"Rely on Kubernetes events without enabling audits\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Configure API server audit with an audit policy that includes authorization events, write logs to a file, and rotate logs\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Only collect logs from kubelet and ignore API server logs\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Store RBAC events exclusively in a local node diary\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B. API server audit with a policy that includes authorization events captures RBAC actions, and rotating logs ensures long-term evidence retention.\n\n## Why Other Options Are Wrong\n- Option A misses formal audit trails, reducing reliability for detecting privilege escalation.\n- Option C ignores API server events, which are the authoritative source for RBAC changes.\n- Option D describes an impractical, localized approach with poor reliability and retention.\n\n## Key Concepts\n- Kubernetes API server audit\n- RBAC event capture\n- Log rotation and centralization\n\n## Real-World Application\nImplement centralized audit logging (e.g., to a SIEM or cloud log service) to retain RBAC evidence for security investigations and compliance.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","Audit","RBAC","ClusterHardening","certification-mcq","domain-weight-15"],"channel":"cks","subChannel":"cluster-hardening","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T16:25:10.968Z","createdAt":"2026-01-12 16:25:11"},{"id":"cks-cluster-hardening-1768235109717-3","question":"To protect pod traffic and limit inter-pod communication, you implement Kubernetes NetworkPolicy with a default-deny posture. Which approach reflects best practice for restricting egress to only DNS and required internal services?","answer":"[{\"id\":\"a\",\"text\":\"Rely on the default CNI settings to block all traffic\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Create a default-deny NetworkPolicy in each namespace and add explicit allow rules for DNS and necessary internal services\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Apply a single cluster-wide NetworkPolicy that covers all namespaces\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Disable NetworkPolicy and rely solely on host-level firewalls\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B. A default-deny approach per namespace followed by explicit allow rules for DNS and required services is the recommended, predictable method for governing pod traffic.\n\n## Why Other Options Are Wrong\n- Option A assumes default CNI behavior is secure, which is not guaranteed and often too permissive.\n- Option C cluster-wide policies are not standard across all CNIs and may not apply uniformly; namespace scoping is required.\n- Option D removes Kubernetes-native network controls and shifts burden to hostfirewalls, reducing portability and increasing complexity.\n\n## Key Concepts\n- NetworkPolicy scoping (namespaced)\n- Default-deny as baseline\n- DNS and internal service accessibility\n\n## Real-World Application\nImplement per-namespace default-deny NetPolicies and craft precise egress/ingress rules to limit communication to only necessary destinations.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","NetworkPolicy","CNI","ClusterHardening","certification-mcq","domain-weight-15"],"channel":"cks","subChannel":"cluster-hardening","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T16:25:11.147Z","createdAt":"2026-01-12 16:25:11"},{"id":"cks-cluster-hardening-1768235109717-4","question":"To enforce that all container images are signed by a trusted authority before admission, which mechanism is most effective in a Kubernetes cluster?","answer":"[{\"id\":\"a\",\"text\":\"Rely on imagePullPolicy Always to fetch the latest image\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use an Admission Controller webhook (ImagePolicyWebhook) or OPA Gatekeeper policy to require images be signed (e.g., Cosign/Sigstore) before admission\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Depend on Notary v1 Notary signatures for runtime verification\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Store images in a private registry without signature verification\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B. An admission control policy (ImagePolicyWebhook or OPA Gatekeeper) can enforce image signing (e.g., Cosign/Sigstore) before pods are admitted, providing strong provenance controls.\n\n## Why Other Options Are Wrong\n- Option A only controls pulling behavior, not image integrity or provenance.\n- Option C Notary v1 is outdated and not universally integrated with Kubernetes admission controls.\n- Option D Even in a private registry, unsigned images could still be admitted; without signature verification, provenance is not guaranteed.\n\n## Key Concepts\n- Image signing and provenance\n- Admission controls (ImagePolicyWebhook, OPA Gatekeeper)\n- Cosign/Sigstore integration\n\n## Real-World Application\nImplement image signing policy to ensure only signed, trusted images are deployed, reducing supply-chain risk and preventing tampered images from running in production.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","ImageSigning","Cosign","Sigstore","ClusterHardening","certification-mcq","domain-weight-15"],"channel":"cks","subChannel":"cluster-hardening","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T16:25:11.339Z","createdAt":"2026-01-12 16:25:11"},{"id":"cks-cluster-setup-1768275350418-0","question":"You are provisioning a private 3-node Kubernetes cluster with kubeadm. You want the API server to be accessible only from the admin network. Which configuration best achieves this?","answer":"[{\"id\":\"a\",\"text\":\"Expose the API server on 0.0.0.0 and rely on TLS to protect traffic\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use firewall rules to restrict access and set --apiserver-advertise-address to a private IP\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Bind the APIServer to a public DNS entry and rely on IP filtering\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use NodePort 6443 and open security groups to admin networks\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct: configuring the API server with a private advertise address and restricting access via firewall rules confines API exposure to the admin network.\n\n## Why Other Options Are Wrong\n- A exposes the API server publicly and ignores network access control.\n- C binding to a public DNS entry does not inherently restrict who can reach the API server.\n- D using a NodePort for the API server expands the attack surface and is not appropriate for API server exposure.\n\n## Key Concepts\n- Kubernetes API server exposure\n- --apiserver-advertise-address\n- Firewall/NACL controls\n\n## Real-World Application\nIn production, limit API server access to trusted networks and monitor access logs to detect unusual activity.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","ClusterSetup","kubeadm","RBAC","EKS","Terraform","certification-mcq","domain-weight-10"],"channel":"cks","subChannel":"cluster-setup","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T03:35:50.418Z","createdAt":"2026-01-13 03:35:50"},{"id":"cks-cluster-setup-1768275350418-1","question":"In an air-gapped environment, which approach ensures kubeadm can bootstrap the cluster without internet access?","answer":"[{\"id\":\"a\",\"text\":\"Preload all required Kubernetes images into a local registry and configure kubeadm with --image-repository pointing to it\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Disable image pulling and reuse existing containers\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use Docker Hub directly during bootstrap\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Skip CA cert validation during TLS handshake\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A is correct: in an air-gapped environment, preloading required images into a local registry and pointing kubeadm to that registry ensures all images are available during bootstrap.\n\n## Why Other Options Are Wrong\n- B relies on existing containers but doesn't guarantee all required images are present or updated.\n- C requires internet access to pull images from Docker Hub, which is not allowed in air-gapped setups.\n- D disabling CA validation compromises security and is not a valid bootstrap approach.\n\n## Key Concepts\n- kubeadm image repository configuration\n- Local registry preloading\n- Offline bootstrap considerations\n\n## Real-World Application\nThis approach enables reproducible, auditable cluster bootstrapping in secured, isolated environments such as secured data centers or regulated facilities.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","ClusterSetup","kubeadm","Air-Gapped","Security","certification-mcq","domain-weight-10"],"channel":"cks","subChannel":"cluster-setup","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T03:35:50.946Z","createdAt":"2026-01-13 03:35:51"},{"id":"cks-cluster-setup-1768275350418-2","question":"During cluster bootstrap, you want to ensure etcd durability and secure communications between members. Which approach is best?","answer":"[{\"id\":\"a\",\"text\":\"Run a 3-member etcd cluster on the control-plane nodes with TLS mutual authentication\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Run etcd as a single node on the etcd-master node\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Store etcd data in a remote object store such as S3\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Disable encryption for etcd to simplify access\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A is correct: a three-member etcd cluster with TLS mutual authentication provides high availability and secure inter-member communication, which are best practices for production clusters.\n\n## Why Other Options Are Wrong\n- B creates a single point of failure and undermines durability.\n- C is not compatible with etcd's on-disk, consistent storage model and is not supported for etcd data.\n- D removing encryption exposes etcd data at rest and in transit to risk.\n\n## Key Concepts\n- etcd quorum and high availability\n- TLS mutual authentication between etcd members\n- Control-plane security\n\n## Real-World Application\nThis setup minimizes risk of data loss during node failures and protects sensitive cluster state from eavesdropping.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","ClusterSetup","etcd","Security","TLS","certification-mcq","domain-weight-10"],"channel":"cks","subChannel":"cluster-setup","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T03:35:51.498Z","createdAt":"2026-01-13 03:35:51"},{"id":"cks-cluster-setup-1768275350418-3","question":"You are preparing a production namespace and must enforce default deny and allow only specific egress to a backend service on port 8080 within the cluster. Which approach would you implement?","answer":"[{\"id\":\"a\",\"text\":\"Create a default-deny NetworkPolicy for the namespace and add a separate policy allowing egress to the backend on port 8080\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Disable NetworkPolicy and rely on host firewall\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Apply a cluster-wide allow-all policy and then block ports individually\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Enable Calico only for DNS traffic\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A is correct: starting from a default deny baseline and explicitly allowing only the required egress ensures least privilege and minimizes blast radius.\n\n## Why Other Options Are Wrong\n- B leaves the cluster open by default and relies on exterior controls that may be bypassed.\n- C is error-prone and creates a broad allow policy before narrowing it down.\n- D Calico DNS focus does not address namespace-wide egress controls required for security.\n\n## Key Concepts\n- NetworkPolicy default deny\n- Namespace-scoped policies\n- Egress rules and port filtering\n\n## Real-World Application\nThis pattern is standard for production workloads to minimize lateral movement and enforce explicit traffic flow.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","Networking","NetworkPolicy","CNI","Calico","ClusterSetup","certification-mcq","domain-weight-10"],"channel":"cks","subChannel":"cluster-setup","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T03:35:51.677Z","createdAt":"2026-01-13 03:35:51"},{"id":"cks-cluster-setup-1768275350418-4","question":"CI/CD pipelines require deployment permissions to a specific namespace. Which RBAC binding best practices achieve least privilege?","answer":"[{\"id\":\"a\",\"text\":\"Create a dedicated ServiceAccount in the target namespace and bind it to a Role with narrowly scoped permissions\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Bind the pipeline SA to cluster-admin via ClusterRoleBinding\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Create a single RoleBinding in the kube-system namespace for all pipelines\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Bind to a Role with edit permissions across all namespaces\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A is correct: using a dedicated ServiceAccount in the target namespace and binding it to a narrowly scoped Role achieves least privilege and minimizes the blast radius if credentials are compromised.\n\n## Why Other Options Are Wrong\n- B grants excessive permissions (cluster-admin) across the entire cluster.\n- C binds to kube-system and is not scoped to the pipeline's needs.\n- D grants edit across all namespaces, which is broader than necessary.\n\n## Key Concepts\n- RBAC best practices\n- Namespace-scoped Roles (Role, RoleBinding)\n- Least-privilege access for CI/CD\n\n## Real-World Application\nThis approach reduces risk of accidental or malicious deployment impacting other namespaces or cluster resources.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","RBAC","Security","CI/CD","ClusterSetup","certification-mcq","domain-weight-10"],"channel":"cks","subChannel":"cluster-setup","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T03:35:51.857Z","createdAt":"2026-01-13 03:35:51"},{"id":"q-1121","question":"Scenario: You operate a shared Kubernetes cluster serving multiple product teams. You must prevent cross-namespace data leakage and enforce least-privilege access while remaining auditable and scalable. Describe a concrete strategy using either OPA Gatekeeper or Kyverno for admission control (with at least two constraints), implement namespace RBAC boundaries, apply Calico NetworkPolicy for namespace isolation, and outline a monitoring/audit plan with tests and runbooks. Include example policies and a minimal test commands snippet?","answer":"Implement policy-as-code with admission controls using either OPA Gatekeeper or Kyverno, enforcing at least two constraints: (1) workloads must run in approved namespaces, (2) pods must not run as pri","explanation":"## Why This Is Asked\n\nEvaluates practical multi-tenant security controls, policy-as-code, and operational testing in Kubernetes.\n\n## Key Concepts\n\n- Admission control with Gatekeeper or Kyverno\n- Namespace RBAC scoping\n- Calico NetworkPolicy isolation\n- Auditing and runbooks\n- CI validation and drift checks\n\n## Code Example\n\n```javascript\napiVersion: constraints.gatekeeper.sh/v1beta1\nkind: K8sNSApproved\nmetadata:\n  name: ns-approved\nspec:\n  match:\n    namespaces: [\"approved-*\"]\n```\n\n```javascript\napiVersion: kyverno.io/v1\nkind: ClusterPolicy\nmetadata:\n  name: restrict-privileges\nspec:\n  rules:\n  - name: disallow-privileged\n    match:\n      resources:\n        kinds: [\"Pod\"]\n    validate:\n      message: \"Privileged containers are not allowed\"\n      pattern:\n        spec:\n          containers:\n          - securityContext:\n              allowPrivilegeEscalation: false\n              readOnlyRootFilesystem: true\n```\n\n```javascript\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: deny-all\nspec:\n  podSelector: {}\n  policyTypes: [\"Ingress\",\"Egress\"]\n  ingress: []\n  egress: []\n```\n\n## Follow-up Questions\n\n- How would you test these policies in CI?\n- How would you handle policy drift and remediation?\n","diagram":"flowchart TD\n  A[Namespaces] --> B[RBAC]\n  B --> C[AdmissionPolicy]\n  C --> D[NetworkPolicy]\n  D --> E[Auditing]","difficulty":"intermediate","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","IBM","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T23:29:20.472Z","createdAt":"2026-01-12T23:29:20.472Z"},{"id":"q-1130","question":"You're running a Kubernetes cluster for a web app. A Pod mounting hostPath and running as root was detected in dev. Outline a practical plan to enforce least privilege across namespaces (Baseline/Restricted) using a policy engine (Kyverno or OPA Gatekeeper) and show how you would validate enforcement without disrupting workloads. What steps and files would you use?","answer":"Implement a baseline Pod Security Standard across namespaces by enabling Kyverno policy or OPA Gatekeeper constraint. Enforce runAsNonRoot: true, readOnlyRootFilesystem: true, disallow hostPath, and d","explanation":"## Why This Is Asked\nA practical beginner-level check for implementing and validating security controls in Kubernetes using policy engines, focusing on least privilege and safe defaults.\n\n## Key Concepts\n- Pod Security Standards Baseline/Restricted\n- Policy engines: Kyverno, OPA Gatekeeper\n- Privilege escalation controls: runAsNonRoot, readOnlyRootFilesystem, capabilities\n- Testing methods: compliant vs non-compliant pods, namespace scoping\n- Change management and rollback\n\n## Code Example\n```javascript\n// Conceptual Kyverno policy (JS object for illustration)\nconst policy = {\n  apiVersion: 'kyverno/v1',\n  kind: 'ClusterPolicy',\n  metadata: { name: 'require-baseline-security' },\n  spec: {\n    rules: [\n      {\n        name: 'require-baseline',\n        match: { resources: { kinds: ['Pod'] }},\n        validate: {\n          message: 'Pods must set runAsNonRoot, readOnlyRootFilesystem and avoid hostPath',\n          pattern: {\n            spec: {\n              containers: { any: { securityContext: { runAsNonRoot: true, readOnlyRootFilesystem: true } }},\n              hostPath: { absent: {} }\n            }\n          }\n        }\n      }\n    ]\n  }\n}\n```\n\n## Follow-up Questions\n- How would you handle exceptions for legitimate hostPath usage?\n- How would you monitor for policy violations in production?","diagram":"flowchart TD\n  A[Define Baseline] --> B[Apply to Namespace]\n  B --> C[Test Compliant Pod]\n  C --> D[Test Violating Pod]\n  D --> E[Monitor & Iterate]","difficulty":"beginner","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T01:22:06.201Z","createdAt":"2026-01-13T01:22:06.201Z"},{"id":"q-1167","question":"Scenario: You operate a multi-cluster Kubernetes data platform (cloud+on‑prem) where a Spark job can access customer data. Design an end-to-end approach to detect, prevent, and respond to data exfiltration attempts from pods across clusters. Include policy design, telemetry signals, enforcement, and incident runbooks; discuss trade-offs?","answer":"Adopt a data-classification driven policy wired through OPA Gatekeeper and Kyverno, enforce egress with default-deny for pods without explicit allow, and centralize audit logs across clusters. Use lab","explanation":"## Why This Is Asked\nTests ability to design end-to-end controls across multi-cluster environments, not just single-cluster examples.\n\n## Key Concepts\n- Data classification and tagging in Kubernetes\n- OPA Gatekeeper and Kyverno policy frameworks\n- Egress controls with Calico/Cilium\n- Kubernetes audit logging and tamper-evident storage\n- Spark / data platform security\n\n## Code Example\n```javascript\n// Example policy sketch (pseudo)\nfunction policy(pod, action, dest){ /* evaluate against labels, namespaces, and allowlists */ }\n```\n\n## Follow-up Questions\n- How would you test policy drift at scale?\n- What telemetry would you collect to distinguish exfiltration from legitimate data movement?","diagram":"flowchart TD\n  A[Data classification] --> B[Policy evaluation]\n  B --> C{Allow or Deny}\n  C --> D[Enforcement point]\n  A --> E[Audit & telemetry]\n  D --> E","difficulty":"advanced","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Instacart"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T03:32:11.099Z","createdAt":"2026-01-13T03:32:11.099Z"},{"id":"q-1278","question":"Scenario: A fintech data platform runs a multi-tenant data lake on Kubernetes. Each data job uses per-job ServiceAccounts to access restricted cloud storage. A rogue pod tries to exfiltrate data via the bucket. Propose a security approach that binds each pod to a dedicated cloud IAM role (workload identity), enforces namespace-scoped permissions, and provides tamper-evident audit trails. Include detection and response for abnormal egress and a safe rotation plan. What trade-offs?","answer":"Bind each pod to a dedicated cloud IAM role via workload identity federation, with namespace-scoped permissions and least privilege. Disable instance metadata access, use private endpoints, and short-","explanation":"## Why This Is Asked\n\nThis question assesses practical implementation of workload identity, RBAC scoping, and robust audit/response in a fintech-like Kubernetes deployment.\n\n## Key Concepts\n\n- Workload Identity Federation\n- Least privilege RBAC and namespace isolation\n- IMDS access control and private endpoints\n- Immutable audit logs and egress monitoring\n- Credential rotation and incident response\n\n## Code Example\n\n```yaml\n# GKE Workload Identity binding example\napiVersion: v1\nkind: Pod\nmetadata:\n  name: data-job\n  annotations:\n    iam.gke.io/gcp-service-account: data-job-sa@PROJECT.iam.gserviceaccount.com\nspec:\n  serviceAccountName: data-job-sa\n```\n\n## Follow-up Questions\n\n- How would you test these controls end-to-end?\n- How would you rotate keys without restarting jobs?\n- How would you handle missed bindings or fallback scenarios?","diagram":null,"difficulty":"advanced","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T07:41:55.251Z","createdAt":"2026-01-13T07:41:55.251Z"},{"id":"q-1301","question":"You're debugging a Kubernetes deployment in a multi-tenant environment where one namespace's pods delay startup by several minutes. Provide a practical, beginner-friendly diagnostic flow focusing on pod events, init containers, image pulls, and config maps. List concrete kubectl commands you would run and how you’d determine the root cause?","answer":"Begin with: kubectl describe pod <pod> -n <ns> to surface events and init-container status. Inspect init containers with kubectl get pod <pod> -n <ns> -o jsonpath '{.status.initContainerStatuses[*].st","explanation":"## Why This Is Asked\n\nTests practical Kubernetes debugging skills, focusing on actionable steps, not theory. It checks familiarity with pod lifecycle, init containers, and how to correlate events with configuration objects during startup delays.\n\n## Key Concepts\n\n- Pod lifecycle and init containers\n- kubectl debugging commands (describe, get, logs, jsonpath)\n- Image pull behavior and identifiers\n- ConfigMaps and Secrets mounting in pods\n- Deployment rollout and readiness checks\n\n## Code Example\n\n```javascript\n// Example diagnostic sequence (pseudo-commands, replace <pod> and <ns> accordingly)\nkubectl describe pod <pod> -n <ns>\nkubectl get pod <pod> -n <ns> -o jsonpath '{.status.initContainerStatuses[*].state}'\nkubectl logs <pod> -c <init-container> -n <ns>\nkubectl describe deployment <deploy> -n <ns>\nkubectl get cm -n <ns>\nkubectl get secret -n <ns>\n```\n\n## Follow-up Questions\n\n- What would you check if the pod remains Pending after image pull completes?\n- How would you distinguish between a misconfigured readinessProbe vs a slow startup?","diagram":"flowchart TD\n  A[Start] --> B[Describe Pod] \n  B --> C[InitContainers Status] \n  C --> D[Check InitContainer Logs] \n  D --> E[Inspect Image Pull Policy/IDs] \n  E --> F[Review ConfigMaps/Secrets] \n  F --> G[Check Deployment Rollout]\n  G --> H[Root Cause Identified]","difficulty":"beginner","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Netflix","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T08:47:12.930Z","createdAt":"2026-01-13T08:47:12.930Z"},{"id":"q-920","question":"In a real-time chat service like Discord, deployed on Kubernetes with NVIDIA GPUs for video processing, you introduce a third-party plugin system that runs as WebAssembly modules to apply custom video filters. How would you design a secure plugin sandbox and runtime attestation to prevent leakage of streams or keys, ensure isolation from other plugins, and enable rapid rollback if a plugin behaves unexpectedly in production? Provide concrete approaches and trade-offs?","answer":"Adopt a per-plugin WebAssembly sandbox with strict memory caps (e.g., 32–64MB), syscall filtering (seccomp), and an isolated FS; require attestation using a signed manifest and a hardware-backed key f","explanation":"## Why This Is Asked\n\nThis question probes pragmatic security engineering for dynamic plugin ecosystems in real-time services with GPU workloads. It tests risk modeling, tooling choices, and trade-offs between performance and isolation.\n\n## Key Concepts\n\n- WebAssembly sandboxing\n- syscall filtering and memory limits\n- hardware-backed attestation and KMS\n- per-plugin encryption and data isolation\n- runtime observability and quick rollback\n\n## Code Example\n\n```javascript\n// Verify plugin signature before load\nconst sig = loadSignature(plugin);\nconst ok = verifySignature(sig, plugin.code, TRUSTED_PUBLIC_KEY);\nif (!ok) throw new Error(\"Invalid plugin\");\n```\n\n## Follow-up Questions\n\n- How would you test the sandbox boundaries and detect escapes?\n- What would you monitor to detect a compromised plugin, and how would you roll back safely?","diagram":"flowchart TD\n  A[Plugin Load] --> B{Attestation Passed?}\n  B -->|Yes| C[Isolate in WASM sandbox]\n  B -->|No| D[Reject]\n  C --> E[Run in separate process]\n  E --> F[Audit Logs to tamper-evident store]","difficulty":"intermediate","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","NVIDIA","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T15:31:52.573Z","createdAt":"2026-01-12T15:31:52.573Z"},{"id":"q-959","question":"Scenario: A service executes user-provided Python plugins inside a container. Design a concrete runtime hardening plan using Linux namespaces, a minimal seccomp profile, and capability bounding, ensuring plugins cannot access host files or network directly while preserving IPC with a controlled channel. Outline exact steps and validation tests?","answer":"Drop all capabilities, run in a dedicated user namespace, mount root as read-only, and bind‑mount only the plugin assets at /plugins. Use a strict seccomp profile that whitelists essential syscalls (r","explanation":"## Why This Is Asked\n\nTests practical security hardening with knowledge of namespaces, seccomp, and capabilities—core skills for roles that handle untrusted code.\n\n## Key Concepts\n\n- Linux namespaces (user/net)\n- Capability bounding and dropping all caps\n- Seccomp profiles with syscall whitelists\n- Read-only root filesystem and selective bind mounts\n- Inter-process communication (IPC) channel integrity\n\n## Code Example\n\n```javascript\n// Example: docker run sandbox flags (conceptual)\nconst cmd = [\n  'docker', 'run', '--rm',\n  '--cap-drop', 'ALL',\n  '--security-opt', 'seccomp=seccomp-profile.json',\n  '--read-only',\n  '--network', 'none',\n  '--mount', 'type=bind,source=/path/to/plugins,target=/plugins',\n  'python-plugin-runner'\n];\n```\n\n## Follow-up Questions\n\n- How would you test for privilege escalation attempts from within the plugin?\n- What trade-offs exist between security and plugin functionality, and how would you mitigate them?","diagram":"flowchart TD\n  A[User Plugin] -->|Runs in sandbox| B[Isolated Container]\n  B --> C{Seccomp Filters}\n  C --> D[Allowed syscalls]\n  C --> E[Blocked syscalls]\n  B --> F[IPC Channel to Host]\n  F --> G[Controlled IPC Endpoint]","difficulty":"beginner","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["PayPal","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T16:44:33.574Z","createdAt":"2026-01-12T16:44:33.574Z"},{"id":"q-967","question":"Scenario: You manage a microservice app deployed to Kubernetes with CI/CD; you need to prevent tampered container images. **Describe a practical, beginner-friendly plan** to implement image signing and verification using **cosign**, integrate it into a GitHub Actions workflow, and enforce verification at deployment (registry or admission webhook). Include concrete commands?","answer":"Plan: sign container images with cosign in CI, store keys in a cloud KMS, rotate monthly, and require signature verification before deployment. In CI: sign on push, verify before publish. Commands to ","explanation":"## Why This Is Asked\nTest practical understanding of software supply chain security with beginner-friendly tooling.\n\n## Key Concepts\n- Image signing with cosign\n- Key management and rotation\n- CI/CD integration (GitHub Actions)\n- Deployment-time verification (registry or admission webhook)\n- Basic audit logging\n\n## Code Example\n```bash\ncosign generate-key-pair\ncosign sign docker.io/org/app:tag --key cosign.key\ncosign verify docker.io/org/app:tag --key cosign.pub\n```\n\n## Follow-up Questions\n- How would you rotate signing keys with minimal downtime?\n- How would you handle private registries and secret key storage in CI?\n","diagram":null,"difficulty":"beginner","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","NVIDIA","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T17:28:30.376Z","createdAt":"2026-01-12T17:28:30.376Z"},{"id":"q-994","question":"Scenario: A Kubernetes-based ML platform serves multiple teams; outbound data exfiltration is a breach risk. Propose a concrete, end-to-end control plane approach to prevent unauthorized data egress using policy-as-code, Kubernetes NetworkPolicy, and a centralized egress gateway. Include a sample Rego policy for Gatekeeper that enforces a namespace label data-export=allowed and an annotation egress-proxy=https://proxy.internal, and outline testing and GitOps integration?","answer":"An example: I implement an OPA Gatekeeper deny rule requiring pods to carry label data-export=allowed and annotation egress-proxy=https://proxy.internal; Gatekeeper blocks otherwise. A Calico NetworkP","explanation":"## Why This Is Asked\n\nThis tests practical application of policy-as-code (OPA Gatekeeper), Kubernetes security controls (NetworkPolicy), and GitOps-driven deployment, plus testing and trade-offs.\n\n## Key Concepts\n\n- OPA Gatekeeper\n- Rego policies\n- Kubernetes NetworkPolicy / Calico egress\n- GitOps (ArgoCD)\n- End-to-end testing\n\n## Code Example\n\n```rego\npackage gatekeeper.sample\n\ndeny[{\"msg\":\"Outbound egress blocked\",\"ns\":ns}]\n{ input.review.kind == \"AdmissionReview\"; obj := input.review.object; ns := obj.metadata.namespace; not obj.metadata.labels[\"data-export\"] == \"allowed\"; not obj.metadata.annotations[\"egress-proxy\"] }\n```\n\n## Follow-up Questions\n\n- How would you test this in a CI pipeline?\n- What are potential bypass vectors and mitigations?","diagram":null,"difficulty":"intermediate","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["OpenAI","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T18:39:01.707Z","createdAt":"2026-01-12T18:39:01.707Z"},{"id":"cks-minimize-vulnerabilities-1768151890328-0","question":"An e-commerce platform runs microservices in Kubernetes with an Istio service mesh. A malicious actor gains access to a pod in the payments namespace and attempts to call the billing service to exfiltrate customer data. Which control most effectively minimizes blast radius in this scenario?","answer":"[{\"id\":\"a\",\"text\":\"Enforce Istio AuthorizationPolicy to restrict which services can be called by each workload\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Rely on Kubernetes NetworkPolicy alone to segment traffic\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Disable mTLS to enable traffic inspection at the gateway\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Place all services into a single namespace with broad RBAC\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\na. Enforce Istio AuthorizationPolicy to restrict which services can be called by each workload\n\n## Why Other Options Are Wrong\n\n- b) Relying on Kubernetes NetworkPolicy alone is insufficient for L7 service-to-service authorization in a mesh.\n- c) Disabling mTLS would weaken authentication and confidentiality and could worsen the breach.\n- d) Splitting the cluster into multiple clusters is heavy-handed and does not address the immediate risk from a compromised pod.\n\n## Key Concepts\n\n- Istio AuthorizationPolicy\n- Zero-trust security\n- Service-to-service access control\n\n## Real-World Application\n\n- Apply per-workload AuthorizationPolicy across all namespaces; combine with strict PeerAuthentication; validate traffic with Istio telemetry to detect violations.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","Istio","Security","Service Mesh","certification-mcq","domain-weight-20"],"channel":"cks","subChannel":"minimize-vulnerabilities","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T17:18:10.329Z","createdAt":"2026-01-11 17:18:10"},{"id":"cks-minimize-vulnerabilities-1768151890328-1","question":"A Kubernetes cluster stores database credentials in Kubernetes Secrets in etcd. Encryption at rest is not enabled, and backups contain plaintext secrets. Which remediation provides the strongest defense against credential leakage?","answer":"[{\"id\":\"a\",\"text\":\"Enable encryption at rest for Kubernetes Secrets using a cloud KMS and rotate existing credentials\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Switch Secrets to ConfigMaps to improve readability\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Place credentials in environment variables inside Pod specs\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Remove Secrets and inject credentials at runtime via prompts\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\na. Enable encryption at rest for Kubernetes Secrets using a cloud KMS and rotate existing credentials\n\n## Why Other Options Are Wrong\n\n- b) ConfigMaps are not suitable for storing secrets and do not provide encryption or access controls.\n- c) Environment variables in Pod specs can be exposed via process listings and logs.\n- d) Runtime prompts are insecure and not suitable for automated deployment pipelines.\n\n## Key Concepts\n\n- Kubernetes Secrets encryption at rest\n- KMS integration\n- Credential rotation\n\n## Real-World Application\n\n- Enable encryption at rest for Secrets; consider external secret stores (e.g., AWS Secrets Manager) and rotate credentials regularly to minimize leakage risk.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","AWS","KMS","Secrets","certification-mcq","domain-weight-20"],"channel":"cks","subChannel":"minimize-vulnerabilities","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T17:18:10.737Z","createdAt":"2026-01-11 17:18:11"},{"id":"cks-minimize-vulnerabilities-1768151890328-2","question":"During deployment, some services communicate over TLS, while development environments use self-signed certs and code disables certificate verification. Which approach best minimizes this vulnerability in production?","answer":"[{\"id\":\"a\",\"text\":\"Adopt a service mesh with mTLS enabled and use a trusted certificate authority with rotating certs; enforce peer authentication across all services\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Always disable TLS verification in the client to simplify calls\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Pin static certificates in code and rebuild for each rotation\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Remove TLS from internal calls and rely on network isolation\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\na. Adopt a service mesh with mTLS enabled and use a trusted certificate authority with rotating certs; enforce peer authentication across all services\n\n## Why Other Options Are Wrong\n\n- b) Disabling TLS verification creates MITM risks and undermines security.\n- c) Pinning static certificates is brittle and rotation-heavy; risks stale trust anchors.\n- d) Removing TLS from internal calls removes encryption and increases exposure; network isolation is insufficient alone.\n\n## Key Concepts\n\n- mTLS and certificate rotation\n- Istio/Envoy CA and PeerAuthentication\n- Service-to-service security\n\n## Real-World Application\n\n- Implement Istio or another service mesh to automatically manage short-lived certs; enforce strict peer verification and regular certificate rotations to minimize production risk.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","Istio","TLS","Security","certification-mcq","domain-weight-20"],"channel":"cks","subChannel":"minimize-vulnerabilities","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T17:18:11.192Z","createdAt":"2026-01-11 17:18:11"},{"id":"cks-monitoring-logging-1768221909292-0","question":"In a Kubernetes cluster, you want to detect and respond to runtime security threats such as privilege escalation and container breakout in near real time. Which combination best achieves this?","answer":"[{\"id\":\"a\",\"text\":\"Deploy Falco with custom rules for privilege escalation and route alerts to the security incident channel\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Rely on Kubernetes events from kubelet alone to trigger investigations\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Enable only Kubernetes API audit logs and store them offline for investigations\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Install a host-based IDS on the operating system without container runtime monitoring\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A is correct because Falco provides runtime security monitoring for Kubernetes and can detect privilege escalation and container breakout in near real time when paired with custom rules and real-time alerting.\n\n## Why Other Options Are Wrong\n- Option B is incorrect because Kubernetes events from kubelet offer limited runtime visibility and can miss syscall-level events and subtle container intrusions.\n- Option C is incorrect because API audit logs monitor API activity only and do not reveal or correlate runtime container behavior necessary for detecting compromises.\n- Option D is incorrect because host-based IDS without container runtime integration lacks visibility into containerized workloads and cannot reliably detect container-level anomalies.\n\n## Key Concepts\n- Runtime security tooling (e.g., Falco) for Kubernetes, with custom rules for common attack patterns\n- Real-time alerting and integration with incident response workflows\n\n## Real-World Application\nDeploy Falco as a DaemonSet, write rules for privilege escalation (e.g., shell access, unexpected exec), and integrate alerts with your SIEM or incident channel. Combine with container image provenance and, where possible, Kubernetes audit logs for a layered defense.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","Falco","Loki","Fluent Bit","Prometheus","Grafana","Runtime Security","certification-mcq","domain-weight-20"],"channel":"cks","subChannel":"monitoring-logging","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T12:45:09.293Z","createdAt":"2026-01-12 12:45:09"},{"id":"cks-monitoring-logging-1768221909292-1","question":"You operate a Kubernetes cluster and need to centralize logs and metrics to support security investigations. Which approach provides reliable log correlation across pods, namespaces, and containers?","answer":"[{\"id\":\"a\",\"text\":\"Ship container logs to Loki via Fluent Bit with Kubernetes metadata, and build Grafana dashboards that correlate by pod UID and container ID\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Collect only Prometheus metrics and forego logs\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use CloudWatch Logs only without structured Kubernetes metadata\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Retrieve logs using kubectl logs only and store them in a separate blob\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A is correct because Loki, when paired with Fluent Bit, preserves Kubernetes metadata (namespace, pod, container, UID) enabling precise cross-component log correlation, and Grafana dashboards can unite logs with metrics and traces for efficient investigations.\n\n## Why Other Options Are Wrong\n- Option B is incorrect because metrics alone do not provide the context needed to diagnose security incidents; logs are essential for root-cause analysis.\n- Option C is incorrect because CloudWatch Logs without Kubernetes metadata limits correlation across pods/nods, reducing investigative effectiveness in multi-cluster or on-prem environments.\n- Option D is incorrect because kubectl logs is manual, non-scalable, and loses historical context when stored in a separate blob without centralized indexing.\n\n## Key Concepts\n- Centralized observability stacks (logs, metrics, traces) with Kubernetes metadata\n- Loki + Fluent Bit integration for scalable log shipping\n\n## Real-World Application\nDeploy Fluent Bit as a DaemonSet to ship container logs to Loki, enrich with Kubernetes metadata, and create Grafana dashboards that link logs to Prometheus metrics for rapid security investigations.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","Loki","Fluent Bit","Grafana","Prometheus","Observability","Security","certification-mcq","domain-weight-20"],"channel":"cks","subChannel":"monitoring-logging","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T12:45:09.808Z","createdAt":"2026-01-12 12:45:10"},{"id":"cks-monitoring-logging-1768221909292-2","question":"To detect and prevent anomalous network behavior in real time within a Kubernetes cluster, which tooling approach is most effective?","answer":"[{\"id\":\"a\",\"text\":\"Use an eBPF-based CNI like Cilium to enforce and monitor policies and generate runtime security alerts\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Rely on Kubernetes NetworkPolicy objects alone and don't monitor runtime traffic\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use an IDS on a separate host with no integration into the cluster network\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use a static firewall rule set on the ingress controller only\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A is correct because eBPF-based CNIs like Cilium provide visibility into live network flows, allow enforcement of fine-grained policies at the kernel level, and generate runtime security alerts for anomalous traffic patterns.\n\n## Why Other Options Are Wrong\n- Option B is incorrect because static Kubernetes NetworkPolicy objects do not monitor runtime traffic or detect anomalies in real time.\n- Option C is incorrect because an external IDS on a separate host may miss in-cluster east-west traffic and lacks tight integration for immediate containment.\n- Option D is incorrect because ingress-only firewalls miss internal cluster traffic and lateral movement within the pod network.\n\n## Key Concepts\n- In-cluster runtime network visibility with eBPF (e.g., Cilium, Hubble)\n- Real-time policy enforcement and alerting for Kubernetes workloads\n\n## Real-World Application\nDeploy Cilium with its Hubble observability, define runtime network policies, and wire alerts to your incident response tooling to detect and block suspicious cross-pod communication in real time.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","Cilium","eBPF","Networking","Observability","Security","certification-mcq","domain-weight-20"],"channel":"cks","subChannel":"monitoring-logging","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T12:45:10.294Z","createdAt":"2026-01-12 12:45:10"},{"id":"cks-supply-chain-1768193408010-0","question":"A development team builds Docker images in CI for a microservice that runs on an EKS cluster. To enforce reproducible builds and trusted provenance, which approach should be implemented end-to-end?","answer":"[{\"id\":\"a\",\"text\":\"Sign the image with Cosign and publish attestations to Rekor; generate a CycloneDX SBOM; enforce in-cluster by admission controls.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Rely solely on static code analysis in CI for provenance.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use a private registry without attestations and disable image verification.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use a build with no signature and rely on KMS encryption only.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA. Sign the image with Cosign and publish attestations to Rekor; generate a CycloneDX SBOM; enforce in-cluster by admission controls.\n\n## Why Other Options Are Wrong\n- B: Static code analysis does not provide cryptographic provenance or tamper-evidence for artifacts.\n- C: A private registry without attestations lacks verifiable provenance and can undermine supply chain integrity.\n- D: Encryption alone protects confidentiality but does not provide artifact provenance or tamper-evidence.\n\n## Key Concepts\n- Image provenance with Sigstore Cosign and Rekor\n- SBOM generation with CycloneDX\n- Reproducible builds and attestations\n- In-cluster enforcement via admission controls (e.g., Webhook/OPA)\n\n## Real-World Application\n- Integrate Cosign signing in CI, publish attestations to Rekor, generate CycloneDX SBOMs, and implement a Kubernetes admission webhook or OPA policy to require valid signatures for deployments.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","EKS","Cosign","Rekor","SBOM","CycloneDX","OPA","certification-mcq","domain-weight-20"],"channel":"cks","subChannel":"supply-chain","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T04:50:08.011Z","createdAt":"2026-01-12 04:50:08"},{"id":"cks-supply-chain-1768193408010-1","question":"In a multi-language software project, a company wants a single source of truth SBOM across languages to support audits and vulnerability management. Which practice best achieves this?","answer":"[{\"id\":\"a\",\"text\":\"Generate CycloneDX SBOMs for each language and publish to a central SBOM registry with consistent metadata.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Generate a single SBOM only for the primary language and reuse for others.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Rely on license scanning tools as a substitute for SBOM.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Maintain SBOMs in local spreadsheets for each repo.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA. Generate CycloneDX SBOMs for each language and publish to a central SBOM registry with consistent metadata.\n\n## Why Other Options Are Wrong\n- B: Different languages have distinct dependencies; a single SBOM per language avoids gaps and ensures completeness.\n- C: License scanning does not provide a complete SBOM with component versions and transitive dependencies.\n- D: Spreadsheets are prone to drift and are not tamper-evident or machine-readable for audits.\n\n## Key Concepts\n- CycloneDX standard for SBOMs\n- Multi-language dependency management\n- Central SBOM registry for tamper-evidence\n\n## Real-World Application\n- Implement automated SBOM generation in CI for each language and push artifacts to a centralized SBOM catalog used by security and compliance teams.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","CycloneDX","SBOM","OPA","Multi-language","certification-mcq","domain-weight-20"],"channel":"cks","subChannel":"supply-chain","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T04:50:08.441Z","createdAt":"2026-01-12 04:50:08"},{"id":"cks-supply-chain-1768193408010-2","question":"To prevent supply chain compromise in a Kubernetes deployment, a policy enforces that all container images must be signed by a trusted signer before they can be deployed. Which implementation best enforces this policy in practice?","answer":"[{\"id\":\"a\",\"text\":\"Deploy a ValidatingAdmissionWebhook (e.g., using Sigstore or Cosign integration) with Gatekeeper/OPA policy that rejects unsigned images.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Rely on runtime security scanning and block unsigned images after deployment.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use image pull-through caching with no signature checks.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Allow developers to override policy with cluster-admin privileges.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA. Deploy a ValidatingAdmissionWebhook (e.g., using Sigstore or Cosign integration) with Gatekeeper/OPA policy that rejects unsigned images.\n\n## Why Other Options Are Wrong\n- B: Runtime checks after deployment miss early failure and can allow unsigned images to run in production.\n- C: Caching without signature checks defeats provenance guarantees.\n- D: Overriding policy with cluster-admin privileges undermines the control and increases risk.\n\n## Key Concepts\n- Image provenance enforcement via admission controls\n- Sigstore/Cosign signing with Rekor\n- Gatekeeper/OPA policies for Kubernetes\n\n## Real-World Application\n- Implement a ValidatingWebhookConfiguration and Gatekeeper policy requiring signatures; set fail-closed for unsigned artifacts and maintain an exceptions workflow for legitimate cases.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","Cosign","Sigstore","Gatekeeper","OPA","EKS","certification-mcq","domain-weight-20"],"channel":"cks","subChannel":"supply-chain","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T04:50:08.834Z","createdAt":"2026-01-12 04:50:08"},{"id":"cks-system-hardening-1768252906672-0","question":"Scenario: You manage a production Kubernetes cluster on AWS EKS that processes sensitive data. Which practice most effectively reduces blast radius if credentials are compromised?","answer":"[{\"id\":\"a\",\"text\":\"Enforce RBAC with least-privilege roles and separate admin vs. user accounts\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Enable anonymous access to the API server to simplify debugging\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Assign cluster-admin to all admins to simplify operations\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Disable API server admission controllers to reduce latency\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption a is correct. Enforcing RBAC with least-privilege roles and separating admin from normal user accounts minimizes blast radius if credentials are compromised because no user has more rights than necessary.\n\n## Why Other Options Are Wrong\n- Option b: Enabling anonymous API access expands risk and makes it trivial to perform actions without authentication.\n- Option c: Granting cluster-admin to all admins creates a single point of privilege escalation.\n- Option d: Disabling admission controllers removes enforcement of security policies, increasing attack surface.\n\n## Key Concepts\n- RBAC and least-privilege access\n- Separation of admin vs user roles\n\n## Real-World Application\n- Implement RoleBindings/ClusterRoleBindings with scoped permissions; use groups for admin access only where appropriate; regularly audit bindings.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","RBAC","PodSecurity","EKS","Terraform","certification-mcq","domain-weight-15"],"channel":"cks","subChannel":"system-hardening","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T21:21:46.673Z","createdAt":"2026-01-12 21:21:47"},{"id":"cks-system-hardening-1768252906672-1","question":"Scenario: In a multi-tenant Kubernetes cluster, which hardening measure most effectively reduces the risk of container breakout from a misbehaving pod?","answer":"[{\"id\":\"a\",\"text\":\"Enforce Pod Security Standards via Pod Security Admission with a restricted baseline\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Allow privileged containers to support flexibility\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Enable hostNetwork for easier networking\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Disable network policies to simplify traffic control\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption a is correct. Pod Security Standards enforce strict pod security policies, restricting capabilities and privileges that pods can request, which reduces the blast radius if a pod is compromised.\n\n## Why Other Options Are Wrong\n- Option b: Privileged containers greatly increase the risk of breakout.\n- Option c: hostNetwork can bypass namespace isolation and escalate reach.\n- Option d: Disabling network policies removes traffic controls that limit lateral movement.\n\n## Key Concepts\n- Pod Security Standards\n- Admission controls\n\n## Real-World Application\n- Apply restricted baseline to namespaces, monitor for policy violations, and adjust as needed for legitimate workloads.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","PodSecurity","EKS","Terraform","certification-mcq","domain-weight-15"],"channel":"cks","subChannel":"system-hardening","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T21:21:47.213Z","createdAt":"2026-01-12 21:21:47"},{"id":"cks-system-hardening-1768252906672-2","question":"Scenario: Your team needs to improve detection and response to potential data exfiltration via the Kubernetes API. Which logging approach best supports incident response?","answer":"[{\"id\":\"a\",\"text\":\"Enable centralized, tamper-evident Kubernetes Audit Logs with a remote sink\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Rely solely on container runtime logs for visibility\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Disable logs to save storage\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use only application logs without cluster-wide visibility\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption a is correct. Centralized, tamper-evident Kubernetes Audit Logs with a remote sink provides an immutable, audit-ready trail of API activity, which is essential for detecting suspicious API usage and guiding incident response.\n\n## Why Other Options Are Wrong\n- Option b: Runtime logs may miss API-level actions and are not tamper-evident.\n- Option c: Removing logs eliminates evidence and hinders forensics.\n- Option d: Application logs do not capture cluster-wide events or API calls.\n\n## Key Concepts\n- Kubernetes Audit Logging\n- Tamper-evident/log integrity\n- Centralized log aggregation\n\n## Real-World Application\n- Configure audit policy, export to CloudWatch/S3 or a SIEM with proper retention and access controls.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","Audit","EKS","CloudTrail","Terraform","certification-mcq","domain-weight-15"],"channel":"cks","subChannel":"system-hardening","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T21:21:47.750Z","createdAt":"2026-01-12 21:21:47"},{"id":"cks-system-hardening-1768252906672-3","question":"Scenario: You want to ensure container image trust across deployments in Kubernetes. Which practice best reduces supply-chain risk?","answer":"[{\"id\":\"a\",\"text\":\"Sign images with cosign and enforce runtime verification via imagePolicyWebhook\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Pull images from any registry without signatures\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use AlwaysPull with unverified base images\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Disable image scanning and vulnerability checks\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption a is correct. Signing images (e.g., with cosign or Notary) and enforcing runtime verification via imagePolicyWebhook ensures only trusted, signed images are deployed, reducing supply-chain risk.\n\n## Why Other Options Are Wrong\n- Option b: Unverified images from any registry can be tampered with.\n- Option c: AlwaysPull with unverified images allows untrusted content to run.\n- Option d: Skipping image scanning omits vulnerability and provenance checks.\n\n## Key Concepts\n- Image signing and verification\n- ImagePolicyWebhook / admission controls\n\n## Real-World Application\n- Integrate signing into CI/CD, configure admission webhook to block unsigned images, and maintain a catalog of approved registries.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","ImageSecurity","Cosign","EKS","Terraform","certification-mcq","domain-weight-15"],"channel":"cks","subChannel":"system-hardening","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T21:21:47.929Z","createdAt":"2026-01-12 21:21:48"},{"id":"cks-system-hardening-1768252906672-4","question":"Scenario: You must protect Kubernetes secrets at rest and ensure only authorized workloads can access them. Which approach best meets these requirements?","answer":"[{\"id\":\"a\",\"text\":\"Use external secret management (e.g., AWS Secrets Manager or HashiCorp Vault) integrated with Kubernetes CSI secret store and KMS encryption\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Store secrets in etcd unencrypted\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use Kubernetes Secrets encoded in base64 without encryption\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Write secrets to ConfigMaps\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption a is correct. External secret management with CSI secret store and KMS-encrypted storage provides strong at-rest protection, access controls, and seamless rotation for secrets used by workloads.\n\n## Why Other Options Are Wrong\n- Option b: Unencrypted etcd storage exposes secrets if etcd is compromised.\n- Option c: Base64 encoding is not encryption and can be decoded easily.\n- Option d: ConfigMaps are not intended for secrets and may be exposed in plaintext.\n\n## Key Concepts\n- External secret management\n- CSI secret store integration\n- KMS-based encryption\n\n## Real-World Application\n- Deploy Secrets Store CSI Driver, connect to AWS Secrets Manager or Vault, map secrets to pods with RBAC controls and short-lived access.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","SecretsManager","CSI","EKS","Terraform","KMS","certification-mcq","domain-weight-15"],"channel":"cks","subChannel":"system-hardening","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T21:21:48.110Z","createdAt":"2026-01-12 21:21:48"}],"subChannels":["cluster-hardening","cluster-setup","general","minimize-vulnerabilities","monitoring-logging","supply-chain","system-hardening"],"companies":["Amazon","Anthropic","Apple","Bloomberg","Discord","DoorDash","Google","IBM","Instacart","NVIDIA","Netflix","OpenAI","PayPal","Robinhood","Snap","Stripe","Tesla","Twitter","Zoom"],"stats":{"total":33,"beginner":4,"intermediate":27,"advanced":2,"newThisWeek":33}}