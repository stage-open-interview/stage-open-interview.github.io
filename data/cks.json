{"questions":[{"id":"q-1121","question":"Scenario: You operate a shared Kubernetes cluster serving multiple product teams. You must prevent cross-namespace data leakage and enforce least-privilege access while remaining auditable and scalable. Describe a concrete strategy using either OPA Gatekeeper or Kyverno for admission control (with at least two constraints), implement namespace RBAC boundaries, apply Calico NetworkPolicy for namespace isolation, and outline a monitoring/audit plan with tests and runbooks. Include example policies and a minimal test commands snippet?","answer":"Implement policy-as-code with admission controls using either OPA Gatekeeper or Kyverno, enforcing at least two constraints: (1) workloads must run in approved namespaces, (2) pods must not run as pri","explanation":"## Why This Is Asked\n\nEvaluates practical multi-tenant security controls, policy-as-code, and operational testing in Kubernetes.\n\n## Key Concepts\n\n- Admission control with Gatekeeper or Kyverno\n- Namespace RBAC scoping\n- Calico NetworkPolicy isolation\n- Auditing and runbooks\n- CI validation and drift checks\n\n## Code Example\n\n```javascript\napiVersion: constraints.gatekeeper.sh/v1beta1\nkind: K8sNSApproved\nmetadata:\n  name: ns-approved\nspec:\n  match:\n    namespaces: [\"approved-*\"]\n```\n\n```javascript\napiVersion: kyverno.io/v1\nkind: ClusterPolicy\nmetadata:\n  name: restrict-privileges\nspec:\n  rules:\n  - name: disallow-privileged\n    match:\n      resources:\n        kinds: [\"Pod\"]\n    validate:\n      message: \"Privileged containers are not allowed\"\n      pattern:\n        spec:\n          containers:\n          - securityContext:\n              allowPrivilegeEscalation: false\n              readOnlyRootFilesystem: true\n```\n\n```javascript\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: deny-all\nspec:\n  podSelector: {}\n  policyTypes: [\"Ingress\",\"Egress\"]\n  ingress: []\n  egress: []\n```\n\n## Follow-up Questions\n\n- How would you test these policies in CI?\n- How would you handle policy drift and remediation?\n","diagram":"flowchart TD\n  A[Namespaces] --> B[RBAC]\n  B --> C[AdmissionPolicy]\n  C --> D[NetworkPolicy]\n  D --> E[Auditing]","difficulty":"intermediate","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","IBM","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T23:29:20.472Z","createdAt":"2026-01-12T23:29:20.472Z"},{"id":"q-1130","question":"You're running a Kubernetes cluster for a web app. A Pod mounting hostPath and running as root was detected in dev. Outline a practical plan to enforce least privilege across namespaces (Baseline/Restricted) using a policy engine (Kyverno or OPA Gatekeeper) and show how you would validate enforcement without disrupting workloads. What steps and files would you use?","answer":"Implement a baseline Pod Security Standard across namespaces by enabling Kyverno policy or OPA Gatekeeper constraint. Enforce runAsNonRoot: true, readOnlyRootFilesystem: true, disallow hostPath, and d","explanation":"## Why This Is Asked\nA practical beginner-level check for implementing and validating security controls in Kubernetes using policy engines, focusing on least privilege and safe defaults.\n\n## Key Concepts\n- Pod Security Standards Baseline/Restricted\n- Policy engines: Kyverno, OPA Gatekeeper\n- Privilege escalation controls: runAsNonRoot, readOnlyRootFilesystem, capabilities\n- Testing methods: compliant vs non-compliant pods, namespace scoping\n- Change management and rollback\n\n## Code Example\n```javascript\n// Conceptual Kyverno policy (JS object for illustration)\nconst policy = {\n  apiVersion: 'kyverno/v1',\n  kind: 'ClusterPolicy',\n  metadata: { name: 'require-baseline-security' },\n  spec: {\n    rules: [\n      {\n        name: 'require-baseline',\n        match: { resources: { kinds: ['Pod'] }},\n        validate: {\n          message: 'Pods must set runAsNonRoot, readOnlyRootFilesystem and avoid hostPath',\n          pattern: {\n            spec: {\n              containers: { any: { securityContext: { runAsNonRoot: true, readOnlyRootFilesystem: true } }},\n              hostPath: { absent: {} }\n            }\n          }\n        }\n      }\n    ]\n  }\n}\n```\n\n## Follow-up Questions\n- How would you handle exceptions for legitimate hostPath usage?\n- How would you monitor for policy violations in production?","diagram":"flowchart TD\n  A[Define Baseline] --> B[Apply to Namespace]\n  B --> C[Test Compliant Pod]\n  C --> D[Test Violating Pod]\n  D --> E[Monitor & Iterate]","difficulty":"beginner","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T01:22:06.201Z","createdAt":"2026-01-13T01:22:06.201Z"},{"id":"q-1167","question":"Scenario: You operate a multi-cluster Kubernetes data platform (cloud+on‑prem) where a Spark job can access customer data. Design an end-to-end approach to detect, prevent, and respond to data exfiltration attempts from pods across clusters. Include policy design, telemetry signals, enforcement, and incident runbooks; discuss trade-offs?","answer":"Adopt a data-classification driven policy wired through OPA Gatekeeper and Kyverno, enforce egress with default-deny for pods without explicit allow, and centralize audit logs across clusters. Use lab","explanation":"## Why This Is Asked\nTests ability to design end-to-end controls across multi-cluster environments, not just single-cluster examples.\n\n## Key Concepts\n- Data classification and tagging in Kubernetes\n- OPA Gatekeeper and Kyverno policy frameworks\n- Egress controls with Calico/Cilium\n- Kubernetes audit logging and tamper-evident storage\n- Spark / data platform security\n\n## Code Example\n```javascript\n// Example policy sketch (pseudo)\nfunction policy(pod, action, dest){ /* evaluate against labels, namespaces, and allowlists */ }\n```\n\n## Follow-up Questions\n- How would you test policy drift at scale?\n- What telemetry would you collect to distinguish exfiltration from legitimate data movement?","diagram":"flowchart TD\n  A[Data classification] --> B[Policy evaluation]\n  B --> C{Allow or Deny}\n  C --> D[Enforcement point]\n  A --> E[Audit & telemetry]\n  D --> E","difficulty":"advanced","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Instacart"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T03:32:11.099Z","createdAt":"2026-01-13T03:32:11.099Z"},{"id":"q-1278","question":"Scenario: A fintech data platform runs a multi-tenant data lake on Kubernetes. Each data job uses per-job ServiceAccounts to access restricted cloud storage. A rogue pod tries to exfiltrate data via the bucket. Propose a security approach that binds each pod to a dedicated cloud IAM role (workload identity), enforces namespace-scoped permissions, and provides tamper-evident audit trails. Include detection and response for abnormal egress and a safe rotation plan. What trade-offs?","answer":"Bind each pod to a dedicated cloud IAM role via workload identity federation, with namespace-scoped permissions and least privilege. Disable instance metadata access, use private endpoints, and short-","explanation":"## Why This Is Asked\n\nThis question assesses practical implementation of workload identity, RBAC scoping, and robust audit/response in a fintech-like Kubernetes deployment.\n\n## Key Concepts\n\n- Workload Identity Federation\n- Least privilege RBAC and namespace isolation\n- IMDS access control and private endpoints\n- Immutable audit logs and egress monitoring\n- Credential rotation and incident response\n\n## Code Example\n\n```yaml\n# GKE Workload Identity binding example\napiVersion: v1\nkind: Pod\nmetadata:\n  name: data-job\n  annotations:\n    iam.gke.io/gcp-service-account: data-job-sa@PROJECT.iam.gserviceaccount.com\nspec:\n  serviceAccountName: data-job-sa\n```\n\n## Follow-up Questions\n\n- How would you test these controls end-to-end?\n- How would you rotate keys without restarting jobs?\n- How would you handle missed bindings or fallback scenarios?","diagram":null,"difficulty":"advanced","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T07:41:55.251Z","createdAt":"2026-01-13T07:41:55.251Z"},{"id":"q-1301","question":"You're debugging a Kubernetes deployment in a multi-tenant environment where one namespace's pods delay startup by several minutes. Provide a practical, beginner-friendly diagnostic flow focusing on pod events, init containers, image pulls, and config maps. List concrete kubectl commands you would run and how you’d determine the root cause?","answer":"Begin with: kubectl describe pod <pod> -n <ns> to surface events and init-container status. Inspect init containers with kubectl get pod <pod> -n <ns> -o jsonpath '{.status.initContainerStatuses[*].st","explanation":"## Why This Is Asked\n\nTests practical Kubernetes debugging skills, focusing on actionable steps, not theory. It checks familiarity with pod lifecycle, init containers, and how to correlate events with configuration objects during startup delays.\n\n## Key Concepts\n\n- Pod lifecycle and init containers\n- kubectl debugging commands (describe, get, logs, jsonpath)\n- Image pull behavior and identifiers\n- ConfigMaps and Secrets mounting in pods\n- Deployment rollout and readiness checks\n\n## Code Example\n\n```javascript\n// Example diagnostic sequence (pseudo-commands, replace <pod> and <ns> accordingly)\nkubectl describe pod <pod> -n <ns>\nkubectl get pod <pod> -n <ns> -o jsonpath '{.status.initContainerStatuses[*].state}'\nkubectl logs <pod> -c <init-container> -n <ns>\nkubectl describe deployment <deploy> -n <ns>\nkubectl get cm -n <ns>\nkubectl get secret -n <ns>\n```\n\n## Follow-up Questions\n\n- What would you check if the pod remains Pending after image pull completes?\n- How would you distinguish between a misconfigured readinessProbe vs a slow startup?","diagram":"flowchart TD\n  A[Start] --> B[Describe Pod] \n  B --> C[InitContainers Status] \n  C --> D[Check InitContainer Logs] \n  D --> E[Inspect Image Pull Policy/IDs] \n  E --> F[Review ConfigMaps/Secrets] \n  F --> G[Check Deployment Rollout]\n  G --> H[Root Cause Identified]","difficulty":"beginner","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Netflix","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T08:47:12.930Z","createdAt":"2026-01-13T08:47:12.930Z"},{"id":"q-1323","question":"In a Kubernetes cluster deploying an ML inference service, models and weights live in a private registry. Outline a practical plan to sign models with cosign, publish attestations, and enforce runtime verification so only attested models can be deployed via GitOps (Argo CD) and an enforcement policy (OPA Gatekeeper or Kyverno). Include concrete commands and sample configuration?","answer":"Sign each model blob with cosign sign-blob, generate and publish an attestation, and require attested provenance in deployment. Integrate into Argo CD via a policy check and enforce at admission with ","explanation":"## Why This Is Asked\nTests end-to-end security for ML artifacts, combining signing attestations, GitOps, and runtime policy in Kubernetes.\n\n## Key Concepts\n- Sigstore Cosign sign-blob attest and verify-blob\n- Model provenance and SBOM\n- GitOps with Argo CD\n- Admission policies: OPA Gatekeeper or Kyverno\n\n## Code Example\n```bash\n# Sign a model blob and create an attestation\ncosign sign-blob -key cosign.key models/model-v1.bin\ncosign attest -key cosign.key --predicate predicate.json models/model-v1.bin\ncosign verify-blob -key cosign.pub models/model-v1.bin\n```\n\n```yaml\n# Kyverno policy requiring attestation before deployment\napiVersion: kyverno.io/v1\nkind: ClusterPolicy\nmetadata:\n  name: require-model-attestation\nspec:\n  validationFailureAction: enforce\n  rules:\n  - name: attested-model-artifact\n    match:\n      resources:\n        kinds: [Deployment]\n    validate:\n      message: Attestation required for model artifact\n      pattern:\n        metadata.annotations.cosign_attested: \"true\"\n```\n\n```rego\npackage kubernetes.admission\n\ndeny[{\"msg\": msg}] {\n  input.request.kind.kind == \"Deployment\"\n  not input.request.object.metadata.annotations[\"cosign_attested\"] == \"true\"\n  msg = \"Model artifact must be attested with cosign before deployment\"\n}\n```\n\n## Follow-up Questions\n- How would you handle key rotation and revocation?\n- How would you test end-to-end in CI/CD and cluster?","diagram":null,"difficulty":"intermediate","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","PayPal","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T11:34:44.913Z","createdAt":"2026-01-13T11:34:44.913Z"},{"id":"q-1356","question":"You manage a Kubernetes cluster hosting regulated data across tenants. Design a practical end-to-end plan to enable at-rest encryption with envelope encryption using a cloud KMS, protect both API server data and etcd data, rotate keys safely, and prove compliance via CI checks. Include concrete commands and sample manifests?","answer":"Configure Kubernetes EncryptionConfig to use a cloud KMS as the envelope-encryption provider, encrypting secrets and etcd data. Rotate keys by adding a new KMS key, applying it via the config, and rol","explanation":"## Why This Is Asked\nTests practical mastery of data protection in Kubernetes, including key management, rotation strategies, and CI validation for compliance.\n\n## Key Concepts\n- Envelope encryption with Kubernetes EncryptionConfig\n- Cloud KMS integration for kms provider\n- etcd and secrets encryption\n- Key rotation and rolling updates\n- CI gates and audit logging\n\n## Code Example\n```yaml\napiVersion: v1\nkind: EncryptionConfig\nresources:\n- resources:\n  - secrets\n  providers:\n  - kms:\n      name: \"cloud-kms\"\n      endpoint: \"unix:///var/run/kms/kms.sock\"\n      cachesize: 1000\n  - aesgcm:\n      keys:\n        - name: \"default-key\"\n          secret: \"BASE64ENCODEDKEY==\"\n```\n\n## Follow-up Questions\n- How would you validate re-encryption during rotation without downtime?\n- What logging/auditing would you implement to detect key mis-rotation or misuse?","diagram":null,"difficulty":"intermediate","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T13:11:29.765Z","createdAt":"2026-01-13T13:11:29.765Z"},{"id":"q-1374","question":"Scenario: A multi-cloud platform runs Kubernetes on EKS and serverless runtimes; ensure only cosign-signed images with verifiable attestations can be deployed across all targets. Describe an end-to-end plan to enforce cross-registry provenance, automate Rekor attestations, and integrate with a GitOps workflow (Argo CD), including concrete commands and sample policy rules?","answer":"Sign images in CI with a KMS-backed cosign key, push to all registries, and attach a Rekor attestation. Enforce via a Gatekeeper/OPA policy that rejects unsigned images and a GitOps gate in Argo CD. C","explanation":"## Why This Is Asked\nTests understanding of cross-environment image provenance and automated attestation in a multi-cloud setup.\n\n## Key Concepts\n- Cross-registry signing and verification\n- Rekor attestations and intoto format\n- Policy-driven deployment gates (OPA/Gatekeeper, Argo CD gate)\n\n## Code Example\n```bash\n# Sign and verify (example for AWS KMS key)\ncosign sign --key 'kms://aws/kms/cosign' ghcr.io/org/service:tag\ncosign verify --key 'kms://aws/kms/cosign' ghcr.io/org/service:tag\ncosign attest ghcr.io/org/service:tag --predicate attest.json --type intoto\n```\n\n## Follow-up Questions\n- How would you rotate signing keys without breaking deployments?\n- How do you audit failed attestations across clusters?","diagram":"flowchart TD\n  A[Code Commit] --> B[CI Build]\n  B --> C[cosign Sign]\n  C --> D[Publish to Registries]\n  D --> E[Attach Attestations]\n  E --> F[Policy Enforcement]\n  F --> G[Deployment Across Environments]","difficulty":"intermediate","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Meta","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T14:37:12.766Z","createdAt":"2026-01-13T14:37:12.766Z"},{"id":"q-1398","question":"In a multi-tenant Kubernetes cluster with images stored in a private OCI registry, design an end-to-end workflow to sign images with cosign, publish attestations, and enforce that only attested images are deployed. Include concrete commands, CI hints, and an admission control policy snippet?","answer":"Sign each image with cosign and publish attestations to the private OCI registry; automate in CI by running 'cosign sign' and 'cosign attest' on tag pushes. Enforce with a Kyverno deny policy (check f","explanation":"## Why This Is Asked\nReal-world supply chain integrity for multi-tenant clusters requires verifiable attestation and automated enforcement.\n\n## Key Concepts\n- Sigstore cosign attestations\n- Private OCI registries\n- CI/CD integration (GitHub Actions)\n- Admission control (Kyverno) and OPA/rego\n\n## Code Example\n```yaml\n# Kyverno policy example (high level)\napiVersion: kyverno.io/v1\nkind: ClusterPolicy\nmetadata:\n  name: require-signed-images\nspec:\n  rules:\n  - name: deny-unsigned\n    match:\n      resources:\n        kinds: [\"Deployment\"]\n    validate:\n      message: \"Image must be cosign-attested\"\n      pattern:\n        spec:\n          template:\n            spec:\n              containers:\n              - image: \"*\"\n```\n\n```rego\npackage imageauth\ndefault allow = false\n\nallow {\n  input.request.kind.kind == \"Pod\"\n  # imagine a field that indicates attestation validity\n  input.review.request.object.spec.containers[_].imageAttested == true\n}\n```\n\n## Follow-up Questions\n- How would you handle base-image rotation and attestation key rotation?\n- How do you test the policy across multiple clusters and during CI/CD failures?\n- What performance trade-offs arise with frequent attestations and how to mitigate them?","diagram":"flowchart TD\n  A[CI Build] --> B[cosign sign]\n  B --> C[cosign attest]\n  C --> D[Push to registry]\n  D --> E[GitOps deploy]\n  E --> F[Admission checks]","difficulty":"intermediate","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Discord","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T15:40:02.857Z","createdAt":"2026-01-13T15:40:02.857Z"},{"id":"q-1444","question":"Scenario: A poly-cloud serverless stack runs AWS Lambda, GCP Cloud Functions, and Azure Functions. CI/CD signs each function package and its dependencies with cosign and publishes SLSA attestations to a central registry. Deployments must be allowed only if attested across all clouds. Describe an end-to-end plan, including concrete commands and sample configs for signing, attesting, and cross-cloud enforcement?","answer":"Sign each function image digest and its SBOM with cosign sign-blob or cosign sign, then publish an attestation via cosign attest to Rekor. Enforce across clouds with OPA/Kyverno policies that block de","explanation":"## Why This Is Asked\nTests cross-cloud supply chain security for serverless deployments, requiring practical steps to sign artifacts, publish attestations, and enforce those attestations across multiple cloud platforms.\n\n## Key Concepts\n- Sigstore cosign and SLSA attestations\n- Multi-cloud deployment and policy enforcement (OPA, Kyverno)\n- CI/CD automation with GitHub Actions\n\n## Code Example\n```yaml\n# GitHub Actions: sign and attest serverless image\nname: Sign and Attest Serverless Images\non:\n  push:\n    branches: [ main ]\njobs:\n  sign_attest:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - name: Setup cosign\n        uses: sigstore/cosign-installer@v1\n      - name: Build image\n        run: |\n          IMAGE=registry.example.com/function-${{ github.sha }}:latest\n          docker build -t $IMAGE .\n      - name: Sign image\n        run: cosign sign --key cosign.key $IMAGE\n      - name: Attest image\n        run: cosign attest --key cosign.key --predicate attest.json $IMAGE\n```\n\n```rego\n# Kyverno policy enforcing attestations (example)\npackage sigstore\n\ndefault allow = false\n\nallow {\n  input.attestations[_].predicate.type == \"type.googleapis.com/slsa.Provenance\"\n  input.image.docker_content_digest != \"\"\n}\n```\n\n## Follow-up Questions\n- How would you handle key rotation and revocation across clouds?\n- What metrics and dashboards would you add to monitor attestation health?","diagram":"flowchart TD\n  A[Digest] --> B[Sign Image]\n  B --> C[Publish Attestation]\n  C --> D[Policy Check]\n  D --> E[Deploy Across Clouds]","difficulty":"intermediate","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","LinkedIn","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T17:37:52.667Z","createdAt":"2026-01-13T17:37:52.667Z"},{"id":"q-1522","question":"Scenario: In a multi-tenant SaaS, a central migration service applies Flyway SQL scripts to dozens of PostgreSQL instances. How would you implement a concrete plan to sign each migration with cosign in CI, publish attestations to a registry, and enforce at runtime that only attested migrations are executed? Include concrete signing commands, attestation storage approach, and a sample enforcement policy (Kyverno/OPA) plus integration steps with Flyway?","answer":"Sign each migration in CI with cosign using a dedicated key, publish the attestation to a Sigstore-compatible registry, and enforce runtime verification before Flyway applies the script. Fail the job ","explanation":"## Why This Is Asked\nTests practical signing and runtime enforcement in migration workflows.\n\n## Key Concepts\n- Artifact signing with cosign\n- Attestations and trust roots\n- Runtime verification before DB changes\n- Policy tooling (Kyverno/OPA)\n\n## Code Example\n```yaml\n# Kyverno or OPA policy samples would be here in a real setup\n```\n\n## Follow-up Questions\n- How would you scale attestation storage across tenants?\n- How do you handle key rotation and revocation?","diagram":"flowchart TD\n  CI[CI/CD] --> Sign[Sign Migration]\n  Sign --> Attest[Publish Attestation]\n  Attest --> Runner[Migration Runner]\n  Runner --> DB[PostgreSQL DBs]","difficulty":"intermediate","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Slack","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T20:40:10.489Z","createdAt":"2026-01-13T20:40:10.489Z"},{"id":"q-1551","question":"In a Kubernetes-based data platform hosting a multi-tenant ML feature store exposed via a high-volume API, design a privacy-preserving, end-to-end audit trail to support forensics without exposing PII. Specify architecture, RBAC/ABAC controls, OpenTelemetry instrumentation, log pipeline (Fluentd/Loki), encryption, data redaction, retention, and how you’d run production-scale incident drills. What would you monitor first and why?","answer":"Instrument all feature-store accesses with OpenTelemetry; funnel logs/traces to per-tenant Loki sinks; redact PII before persistence; encrypt at rest with KMS; enforce ABAC via OPA and per-tenant RBAC.","explanation":"## Why This Is Asked\nThis question tests end-to-end design for privacy-preserving forensics on a multi-tenant data platform, focusing on practical tooling choices and production safety.\n\n## Key Concepts\n- OpenTelemetry instrumentation\n- Per-tenant isolation (Namespaces, ServiceAccounts)\n- Loki/NDS for immutable logs\n- Data redaction, KMS, mTLS\n- ABAC with OPA, RBAC policies\n- Incident drills and validation\n\n## Code Example\n```javascript\n// No executable code required in interview; design rationale only\n```\n\n## Follow-up Questions\n- How would you validate retention and deletion across tenants?","diagram":null,"difficulty":"advanced","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","LinkedIn","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T06:29:58.858Z","createdAt":"2026-01-13T21:39:15.067Z"},{"id":"q-1691","question":"Scenario: A Terraform-driven multi-tenant cloud platform provisions resources across clouds. You must sign every Terraform plan in CI with cosign, publish an attestation to a central registry, and enforce at runtime that only attested plans are applied by the GitOps flow. Describe an end-to-end approach with concrete signing commands, attestation storage layout, an sample OPA policy, and integration steps with the deployment pipeline?","answer":"CI: terraform plan -out=tfplan; terraform show -json tfplan > tfplan.json; cosign sign-blob tfplan.json ghcr.io/org/terraform-plans/tenant-A:plan-${CI_COMMIT_SHORT_SHA}; cosign attest ghcr.io/org/terr","explanation":"## Why This Is Asked\nTests the ability to extend supply chain security to IaC, operationalize attestation, and integrate with GitOps across Terraform and multiple clouds. It covers plan hashing, artifact signing, attestation publication, and policy enforcement, including drift handling and key rotation.\n\n## Key Concepts\n- IaC plan provenance (Terraform plan JSON)\n- cosign sign-blob and attest\n- Rekor/Sigstore attestation registry\n- Open Policy Agent (OPA) for gate policies\n- GitOps gating (Argo CD / Tekton)\n\n## Code Example\n```javascript\nconst fs = require('fs');\nconst crypto = require('crypto');\nconst plan = fs.readFileSync('tfplan.json');\nconsole.log('SHA256', crypto.createHash('sha256').update(plan).digest('hex'));\n```\n\n## Follow-up Questions\n- How would you handle re-signing if a plan changes post-attestation?\n- How to scale attestations across dozens of tenants with separate keys and registries?","diagram":null,"difficulty":"intermediate","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Netflix","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T07:01:14.464Z","createdAt":"2026-01-14T07:01:14.464Z"},{"id":"q-1734","question":"In a multi-cloud Terraform deployment across AWS, GCP, and Azure, a central registry hosts official modules. Propose a concrete plan to: - sign every module and its dependencies with cosign during CI, - publish attestations to a provenance registry, - enforce at plan/apply time that only attested modules are used via an OPA policy or equivalent gate, including concrete signing commands, storage layout for attestations, and a sample enforcement policy with integration steps?","answer":"CI signs and attests every Terraform module tarball using cosign, stores attestations in registry.example.com/terraform/attestations, and gates plans with an OPA policy requiring an attestation for al","explanation":"## Why This Is Asked\nEvaluates end-to-end IaC provenance, cross‑cloud module governance, and CI/CD gating.\n\n## Key Concepts\n- Terraform module provenance, cosign attestations, SBOMs\n- GitHub Actions or CI gates, OPA policy enforcement\n- Multi-cloud module registry integration and access control\n\n## Code Example\n```javascript\npackage terraform\n\ndefault allow = false\n\nallow {\n  input.module == m\n  data.attestations[m].signed == true\n}\n```\n\n## Follow-up Questions\n- How would you handle rotated signing keys across clouds?\n- What changes when using a module registry instead of a local path?","diagram":null,"difficulty":"intermediate","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Apple","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T08:56:28.436Z","createdAt":"2026-01-14T08:56:28.436Z"},{"id":"q-1846","question":"In a Tesla-scale Databricks lakehouse on AWS, with Unity Catalog and a Kubernetes data plane, contractors routinely export aggregated datasets to external S3 buckets. Design a detection and response mechanism that distinguishes legitimate exports from exfiltration attempts. Include telemetry (Unity Catalog audit logs, IAM activity, Delta table operations), thresholds, alerting, and an incident playbook?","answer":"Implement a multi-layer detection using Unity Catalog audit logs, IAM/STS activity, and Delta Lake metadata to flag outbound exports to external buckets beyond a whitelist. Enrich with VPC flow logs a","explanation":"## Why This Is Asked\n\nTests ability to design multi-source telemetry-driven detection for data exfiltration in complex cloud-native data platforms, balancing legitimate data workflows with security controls. Evaluates incident response readiness and depth of tooling integration.\n\n## Key Concepts\n\n- Unity Catalog audit logs and Delta Lake metadata\n- IAM/STS activity and rule-based access governance\n- External data egress controls and approval workflows\n- VPC flow logs, Kubernetes data plane telemetry, and export APIs\n- Baseline-based anomaly detection and auto-containment\n\n## Code Example\n\n```python\ndef is_suspicious(event, whitelist, THRESHOLD_GB):\n    if event.type != \"EXPORT\":\n        return False\n    if event.destination_bucket not in whitelist:\n        return True\n    if event.size_gb > THRESHOLD_GB:\n        return True\n    return False\n```\n\n## Follow-up Questions\n- How would you minimize false positives while maintaining network security?\n- What metrics would you monitor to tune thresholds over time?","diagram":"flowchart TD\n  A[Export Event] --> B{IsDestinationWhitelisted?}\n  B -- Yes --> C[CheckSizeThreshold]\n  B -- No --> D[Flag as Suspicious]\n  C -- IfAboveThreshold --> D\n  C -- IfBelowThreshold --> E[Allow with Monitoring]","difficulty":"advanced","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T14:33:35.974Z","createdAt":"2026-01-14T14:33:35.976Z"},{"id":"q-1964","question":"In a multi-tenant notebook service on Kubernetes, each notebook runs in a transient pod and pulls dependencies from a private registry. Design a concrete plan to sign the notebook artifact (notebook.ipynb) and its dependencies with cosign, publish SLSA attestations to a central registry, and enforce at runtime that only attested notebooks and dependencies are allowed to run. Include concrete signing commands, attestations storage, and a sample Kyverno/OPA policy plus integration steps with the notebook runner?","answer":"Sign notebook.ipynb and its dependencies with cosign, e.g. cosign sign notebook.ipynb --key cosign.key and cosign sign libs/*.whl --key cosign.key; publish SLSA attestations to a central registry; enf","explanation":"## Why This Is Asked\nTests practical attestation workflows in a notebook-centric, multi-tenant Kubernetes setup, including artifact signing, attestation storage, and runtime enforcement. \n\n## Key Concepts\n- Cosign signing of notebooks and dependencies\n- SLSA attestations and registry storage\n- Runtime policy enforcement (Kyverno/OPA)\n- Notebook runner integration and rollback considerations\n\n## Code Example\n```javascript\n// Example: building a simple policy check string to feed into OPA or Kyverno\nconst policy = `\npackage notebook.authz\n\ndefault allow = false\n\nallow {\n  input.attested == true\n}\n`;\n```\n\n## Follow-up Questions\n- How would you handle key rotation and revocation for cosign keys in this flow?\n- How would you test this end-to-end in CI/CD and in disaster recovery scenarios?","diagram":null,"difficulty":"intermediate","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Citadel","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T18:58:00.356Z","createdAt":"2026-01-14T18:58:00.356Z"},{"id":"q-2092","question":"In a Kubernetes-based data-analytics platform with two namespaces, dev and prod, design a practical RBAC and Pod Security setup for a new microservice 'data-processor' to ensure it can only read its own ConfigMaps and Secrets, runs as a non-root user, and cannot escalate privileges. Provide concrete manifest fragments (RBAC, ServiceAccount, RoleBinding, PodSecurityContext) and describe how to validate at admission time that all pods comply?","answer":"Create namespace-scoped RBAC with a dedicated ServiceAccount for data-processor in each namespace, a Role granting read access to ConfigMaps and Secrets, and a RoleBinding associating the ServiceAccount with the Deployment. Include PodSecurityContext to enforce non-root execution and restrict capabilities.","explanation":"## Why This Is Asked\n\nThis question evaluates practical understanding of Kubernetes security primitives including RBAC, ServiceAccount isolation, PodSecurityContext, and admission controls—essential skills for securing containerized workloads.\n\n## Key Concepts\n\n- **Namespace-scoped RBAC**: Role and RoleBinding for resource access control within specific namespaces\n- **ServiceAccount isolation**: Dedicated service accounts to separate pod identities and permissions\n- **PodSecurityContext**: Security constraints including runAsNonRoot, runAsUser, readOnlyRootFilesystem, and capability dropping\n- **Admission controls**: PodSecurity standards and OPA Gatekeeper for policy enforcement at admission time\n\n## Code Example\n\n```yaml\n# ServiceAccount (dev)\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: data-processor-sa\n  namespace: dev\n---\n# Role for read-only access to ConfigMaps and Secrets\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: data-processor-role\n  namespace: dev\nrules:\n- apiGroups: [\"\"]\n  resources: [\"configmaps\", \"secrets\"]\n  verbs: [\"get\", \"list\"]\n---\n# RoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: data-processor-binding\n  namespace: dev\nsubjects:\n- kind: ServiceAccount\n  name: data-processor-sa\n  namespace: dev\nroleRef:\n  kind: Role\n  name: data-processor-role\n  apiGroup: rbac.authorization.k8s.io\n```\n\n## Follow-up Questions\n\n- How would you test the RBAC permissions locally?\n- What changes are needed for the prod namespace?\n- How would you implement network policies alongside these security controls?","diagram":null,"difficulty":"beginner","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Databricks","DoorDash"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T04:14:12.430Z","createdAt":"2026-01-14T23:42:07.566Z"},{"id":"q-2128","question":"In a high-sensitivity environment spanning on-prem and cloud, design a tamper-evident Kubernetes audit logging and incident-response pipeline that preserves evidence from the API server, etcd, and kubelets while enabling automated triage and containment during a breach. Specify data formats, forwarding targets, non-repudiation measures, and failure modes?","answer":"Implement a multi-layered, tamper‑evident pipeline: enable API server and etcd audit logs, forward securely to an immutable object store (S3 with bucket-level and object immutability) and a SIEM via s","explanation":"## Why This Is Asked\n\nThis question probes practical design for tamper-resistant security data collection across heterogeneous Kubernetes estates, addressing post-breach forensics, automation, and resilience.\n\n## Key Concepts\n\n- Tamper-evident logs, audit policy, etcd encryption\n- Immutable storage, HMAC signing, key rotation\n- Secure log forwarding (webhooks, TLS)\n- Runtime security (Falco) and IR playbooks\n- Non-repudiation, data retention, failure modes\n\n## Code Example\n\n```javascript\nfunction signEvent(event, key) {\n  const crypto = require('crypto');\n  const h = crypto.createHmac('sha256', key);\n  h.update(event);\n  return h.digest('hex');\n}\n```\n\n## Follow-up Questions\n\n- How would you test integrity guarantees across region failures?\n- What are failure modes if the webhook sink is compromised?","diagram":"flowchart TD\n  A(API Server Audit) --> B(Immutable Store)\n  A --> C(SIEM Webhook)\n  D(Etcd Audit) --> B\n  E(Kubelet Audit) --> C\n  F(Falco) --> G(IR Playbook)\n  H(Integrity Verifier) --> B","difficulty":"advanced","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","PayPal","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T04:11:32.481Z","createdAt":"2026-01-15T04:11:32.482Z"},{"id":"q-920","question":"In a real-time chat service like Discord, deployed on Kubernetes with NVIDIA GPUs for video processing, you introduce a third-party plugin system that runs as WebAssembly modules to apply custom video filters. How would you design a secure plugin sandbox and runtime attestation to prevent leakage of streams or keys, ensure isolation from other plugins, and enable rapid rollback if a plugin behaves unexpectedly in production? Provide concrete approaches and trade-offs?","answer":"Adopt a per-plugin WebAssembly sandbox with strict memory caps (e.g., 32–64MB), syscall filtering (seccomp), and an isolated FS; require attestation using a signed manifest and a hardware-backed key f","explanation":"## Why This Is Asked\n\nThis question probes pragmatic security engineering for dynamic plugin ecosystems in real-time services with GPU workloads. It tests risk modeling, tooling choices, and trade-offs between performance and isolation.\n\n## Key Concepts\n\n- WebAssembly sandboxing\n- syscall filtering and memory limits\n- hardware-backed attestation and KMS\n- per-plugin encryption and data isolation\n- runtime observability and quick rollback\n\n## Code Example\n\n```javascript\n// Verify plugin signature before load\nconst sig = loadSignature(plugin);\nconst ok = verifySignature(sig, plugin.code, TRUSTED_PUBLIC_KEY);\nif (!ok) throw new Error(\"Invalid plugin\");\n```\n\n## Follow-up Questions\n\n- How would you test the sandbox boundaries and detect escapes?\n- What would you monitor to detect a compromised plugin, and how would you roll back safely?","diagram":"flowchart TD\n  A[Plugin Load] --> B{Attestation Passed?}\n  B -->|Yes| C[Isolate in WASM sandbox]\n  B -->|No| D[Reject]\n  C --> E[Run in separate process]\n  E --> F[Audit Logs to tamper-evident store]","difficulty":"intermediate","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","NVIDIA","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T15:31:52.573Z","createdAt":"2026-01-12T15:31:52.573Z"},{"id":"q-959","question":"Scenario: A service executes user-provided Python plugins inside a container. Design a concrete runtime hardening plan using Linux namespaces, a minimal seccomp profile, and capability bounding, ensuring plugins cannot access host files or network directly while preserving IPC with a controlled channel. Outline exact steps and validation tests?","answer":"Drop all capabilities, run in a dedicated user namespace, mount root as read-only, and bind‑mount only the plugin assets at /plugins. Use a strict seccomp profile that whitelists essential syscalls (r","explanation":"## Why This Is Asked\n\nTests practical security hardening with knowledge of namespaces, seccomp, and capabilities—core skills for roles that handle untrusted code.\n\n## Key Concepts\n\n- Linux namespaces (user/net)\n- Capability bounding and dropping all caps\n- Seccomp profiles with syscall whitelists\n- Read-only root filesystem and selective bind mounts\n- Inter-process communication (IPC) channel integrity\n\n## Code Example\n\n```javascript\n// Example: docker run sandbox flags (conceptual)\nconst cmd = [\n  'docker', 'run', '--rm',\n  '--cap-drop', 'ALL',\n  '--security-opt', 'seccomp=seccomp-profile.json',\n  '--read-only',\n  '--network', 'none',\n  '--mount', 'type=bind,source=/path/to/plugins,target=/plugins',\n  'python-plugin-runner'\n];\n```\n\n## Follow-up Questions\n\n- How would you test for privilege escalation attempts from within the plugin?\n- What trade-offs exist between security and plugin functionality, and how would you mitigate them?","diagram":"flowchart TD\n  A[User Plugin] -->|Runs in sandbox| B[Isolated Container]\n  B --> C{Seccomp Filters}\n  C --> D[Allowed syscalls]\n  C --> E[Blocked syscalls]\n  B --> F[IPC Channel to Host]\n  F --> G[Controlled IPC Endpoint]","difficulty":"beginner","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["PayPal","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T16:44:33.574Z","createdAt":"2026-01-12T16:44:33.574Z"},{"id":"q-967","question":"Scenario: You manage a microservice app deployed to Kubernetes with CI/CD; you need to prevent tampered container images. **Describe a practical, beginner-friendly plan** to implement image signing and verification using **cosign**, integrate it into a GitHub Actions workflow, and enforce verification at deployment (registry or admission webhook). Include concrete commands?","answer":"Plan: sign container images with cosign in CI, store keys in a cloud KMS, rotate monthly, and require signature verification before deployment. In CI: sign on push, verify before publish. Commands to ","explanation":"## Why This Is Asked\nTest practical understanding of software supply chain security with beginner-friendly tooling.\n\n## Key Concepts\n- Image signing with cosign\n- Key management and rotation\n- CI/CD integration (GitHub Actions)\n- Deployment-time verification (registry or admission webhook)\n- Basic audit logging\n\n## Code Example\n```bash\ncosign generate-key-pair\ncosign sign docker.io/org/app:tag --key cosign.key\ncosign verify docker.io/org/app:tag --key cosign.pub\n```\n\n## Follow-up Questions\n- How would you rotate signing keys with minimal downtime?\n- How would you handle private registries and secret key storage in CI?\n","diagram":null,"difficulty":"beginner","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","NVIDIA","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T17:28:30.376Z","createdAt":"2026-01-12T17:28:30.376Z"},{"id":"q-994","question":"Scenario: A Kubernetes-based ML platform serves multiple teams; outbound data exfiltration is a breach risk. Propose a concrete, end-to-end control plane approach to prevent unauthorized data egress using policy-as-code, Kubernetes NetworkPolicy, and a centralized egress gateway. Include a sample Rego policy for Gatekeeper that enforces a namespace label data-export=allowed and an annotation egress-proxy=https://proxy.internal, and outline testing and GitOps integration?","answer":"An example: I implement an OPA Gatekeeper deny rule requiring pods to carry label data-export=allowed and annotation egress-proxy=https://proxy.internal; Gatekeeper blocks otherwise. A Calico NetworkP","explanation":"## Why This Is Asked\n\nThis tests practical application of policy-as-code (OPA Gatekeeper), Kubernetes security controls (NetworkPolicy), and GitOps-driven deployment, plus testing and trade-offs.\n\n## Key Concepts\n\n- OPA Gatekeeper\n- Rego policies\n- Kubernetes NetworkPolicy / Calico egress\n- GitOps (ArgoCD)\n- End-to-end testing\n\n## Code Example\n\n```rego\npackage gatekeeper.sample\n\ndeny[{\"msg\":\"Outbound egress blocked\",\"ns\":ns}]\n{ input.review.kind == \"AdmissionReview\"; obj := input.review.object; ns := obj.metadata.namespace; not obj.metadata.labels[\"data-export\"] == \"allowed\"; not obj.metadata.annotations[\"egress-proxy\"] }\n```\n\n## Follow-up Questions\n\n- How would you test this in a CI pipeline?\n- What are potential bypass vectors and mitigations?","diagram":null,"difficulty":"intermediate","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["OpenAI","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T18:39:01.707Z","createdAt":"2026-01-12T18:39:01.707Z"}],"subChannels":["general"],"companies":["Adobe","Airbnb","Amazon","Anthropic","Apple","Bloomberg","Citadel","Cloudflare","Databricks","Discord","DoorDash","Google","Hugging Face","IBM","Instacart","LinkedIn","Meta","Microsoft","NVIDIA","Netflix","OpenAI","PayPal","Robinhood","Salesforce","Slack","Snap","Stripe","Tesla","Twitter","Two Sigma","Uber","Zoom"],"stats":{"total":22,"beginner":5,"intermediate":12,"advanced":5,"newThisWeek":22}}