{"questions":[{"id":"q-1121","question":"Scenario: You operate a shared Kubernetes cluster serving multiple product teams. You must prevent cross-namespace data leakage and enforce least-privilege access while remaining auditable and scalable. Describe a concrete strategy using either OPA Gatekeeper or Kyverno for admission control (with at least two constraints), implement namespace RBAC boundaries, apply Calico NetworkPolicy for namespace isolation, and outline a monitoring/audit plan with tests and runbooks. Include example policies and a minimal test commands snippet?","answer":"Implement policy-as-code with admission controls using either OPA Gatekeeper or Kyverno, enforcing at least two constraints: (1) workloads must run in approved namespaces, (2) pods must not run as pri","explanation":"## Why This Is Asked\n\nEvaluates practical multi-tenant security controls, policy-as-code, and operational testing in Kubernetes.\n\n## Key Concepts\n\n- Admission control with Gatekeeper or Kyverno\n- Namespace RBAC scoping\n- Calico NetworkPolicy isolation\n- Auditing and runbooks\n- CI validation and drift checks\n\n## Code Example\n\n```javascript\napiVersion: constraints.gatekeeper.sh/v1beta1\nkind: K8sNSApproved\nmetadata:\n  name: ns-approved\nspec:\n  match:\n    namespaces: [\"approved-*\"]\n```\n\n```javascript\napiVersion: kyverno.io/v1\nkind: ClusterPolicy\nmetadata:\n  name: restrict-privileges\nspec:\n  rules:\n  - name: disallow-privileged\n    match:\n      resources:\n        kinds: [\"Pod\"]\n    validate:\n      message: \"Privileged containers are not allowed\"\n      pattern:\n        spec:\n          containers:\n          - securityContext:\n              allowPrivilegeEscalation: false\n              readOnlyRootFilesystem: true\n```\n\n```javascript\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: deny-all\nspec:\n  podSelector: {}\n  policyTypes: [\"Ingress\",\"Egress\"]\n  ingress: []\n  egress: []\n```\n\n## Follow-up Questions\n\n- How would you test these policies in CI?\n- How would you handle policy drift and remediation?\n","diagram":"flowchart TD\n  A[Namespaces] --> B[RBAC]\n  B --> C[AdmissionPolicy]\n  C --> D[NetworkPolicy]\n  D --> E[Auditing]","difficulty":"intermediate","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","IBM","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T23:29:20.472Z","createdAt":"2026-01-12T23:29:20.472Z"},{"id":"q-1130","question":"You're running a Kubernetes cluster for a web app. A Pod mounting hostPath and running as root was detected in dev. Outline a practical plan to enforce least privilege across namespaces (Baseline/Restricted) using a policy engine (Kyverno or OPA Gatekeeper) and show how you would validate enforcement without disrupting workloads. What steps and files would you use?","answer":"Implement a baseline Pod Security Standard across namespaces by enabling Kyverno policy or OPA Gatekeeper constraint. Enforce runAsNonRoot: true, readOnlyRootFilesystem: true, disallow hostPath, and d","explanation":"## Why This Is Asked\nA practical beginner-level check for implementing and validating security controls in Kubernetes using policy engines, focusing on least privilege and safe defaults.\n\n## Key Concepts\n- Pod Security Standards Baseline/Restricted\n- Policy engines: Kyverno, OPA Gatekeeper\n- Privilege escalation controls: runAsNonRoot, readOnlyRootFilesystem, capabilities\n- Testing methods: compliant vs non-compliant pods, namespace scoping\n- Change management and rollback\n\n## Code Example\n```javascript\n// Conceptual Kyverno policy (JS object for illustration)\nconst policy = {\n  apiVersion: 'kyverno/v1',\n  kind: 'ClusterPolicy',\n  metadata: { name: 'require-baseline-security' },\n  spec: {\n    rules: [\n      {\n        name: 'require-baseline',\n        match: { resources: { kinds: ['Pod'] }},\n        validate: {\n          message: 'Pods must set runAsNonRoot, readOnlyRootFilesystem and avoid hostPath',\n          pattern: {\n            spec: {\n              containers: { any: { securityContext: { runAsNonRoot: true, readOnlyRootFilesystem: true } }},\n              hostPath: { absent: {} }\n            }\n          }\n        }\n      }\n    ]\n  }\n}\n```\n\n## Follow-up Questions\n- How would you handle exceptions for legitimate hostPath usage?\n- How would you monitor for policy violations in production?","diagram":"flowchart TD\n  A[Define Baseline] --> B[Apply to Namespace]\n  B --> C[Test Compliant Pod]\n  C --> D[Test Violating Pod]\n  D --> E[Monitor & Iterate]","difficulty":"beginner","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T01:22:06.201Z","createdAt":"2026-01-13T01:22:06.201Z"},{"id":"q-1167","question":"Scenario: You operate a multi-cluster Kubernetes data platform (cloud+on‑prem) where a Spark job can access customer data. Design an end-to-end approach to detect, prevent, and respond to data exfiltration attempts from pods across clusters. Include policy design, telemetry signals, enforcement, and incident runbooks; discuss trade-offs?","answer":"Adopt a data-classification driven policy wired through OPA Gatekeeper and Kyverno, enforce egress with default-deny for pods without explicit allow, and centralize audit logs across clusters. Use lab","explanation":"## Why This Is Asked\nTests ability to design end-to-end controls across multi-cluster environments, not just single-cluster examples.\n\n## Key Concepts\n- Data classification and tagging in Kubernetes\n- OPA Gatekeeper and Kyverno policy frameworks\n- Egress controls with Calico/Cilium\n- Kubernetes audit logging and tamper-evident storage\n- Spark / data platform security\n\n## Code Example\n```javascript\n// Example policy sketch (pseudo)\nfunction policy(pod, action, dest){ /* evaluate against labels, namespaces, and allowlists */ }\n```\n\n## Follow-up Questions\n- How would you test policy drift at scale?\n- What telemetry would you collect to distinguish exfiltration from legitimate data movement?","diagram":"flowchart TD\n  A[Data classification] --> B[Policy evaluation]\n  B --> C{Allow or Deny}\n  C --> D[Enforcement point]\n  A --> E[Audit & telemetry]\n  D --> E","difficulty":"advanced","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Instacart"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T03:32:11.099Z","createdAt":"2026-01-13T03:32:11.099Z"},{"id":"q-1278","question":"Scenario: A fintech data platform runs a multi-tenant data lake on Kubernetes. Each data job uses per-job ServiceAccounts to access restricted cloud storage. A rogue pod tries to exfiltrate data via the bucket. Propose a security approach that binds each pod to a dedicated cloud IAM role (workload identity), enforces namespace-scoped permissions, and provides tamper-evident audit trails. Include detection and response for abnormal egress and a safe rotation plan. What trade-offs?","answer":"Bind each pod to a dedicated cloud IAM role via workload identity federation, with namespace-scoped permissions and least privilege. Disable instance metadata access, use private endpoints, and short-","explanation":"## Why This Is Asked\n\nThis question assesses practical implementation of workload identity, RBAC scoping, and robust audit/response in a fintech-like Kubernetes deployment.\n\n## Key Concepts\n\n- Workload Identity Federation\n- Least privilege RBAC and namespace isolation\n- IMDS access control and private endpoints\n- Immutable audit logs and egress monitoring\n- Credential rotation and incident response\n\n## Code Example\n\n```yaml\n# GKE Workload Identity binding example\napiVersion: v1\nkind: Pod\nmetadata:\n  name: data-job\n  annotations:\n    iam.gke.io/gcp-service-account: data-job-sa@PROJECT.iam.gserviceaccount.com\nspec:\n  serviceAccountName: data-job-sa\n```\n\n## Follow-up Questions\n\n- How would you test these controls end-to-end?\n- How would you rotate keys without restarting jobs?\n- How would you handle missed bindings or fallback scenarios?","diagram":null,"difficulty":"advanced","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T07:41:55.251Z","createdAt":"2026-01-13T07:41:55.251Z"},{"id":"q-1301","question":"You're debugging a Kubernetes deployment in a multi-tenant environment where one namespace's pods delay startup by several minutes. Provide a practical, beginner-friendly diagnostic flow focusing on pod events, init containers, image pulls, and config maps. List concrete kubectl commands you would run and how you’d determine the root cause?","answer":"Begin with: kubectl describe pod <pod> -n <ns> to surface events and init-container status. Inspect init containers with kubectl get pod <pod> -n <ns> -o jsonpath '{.status.initContainerStatuses[*].st","explanation":"## Why This Is Asked\n\nTests practical Kubernetes debugging skills, focusing on actionable steps, not theory. It checks familiarity with pod lifecycle, init containers, and how to correlate events with configuration objects during startup delays.\n\n## Key Concepts\n\n- Pod lifecycle and init containers\n- kubectl debugging commands (describe, get, logs, jsonpath)\n- Image pull behavior and identifiers\n- ConfigMaps and Secrets mounting in pods\n- Deployment rollout and readiness checks\n\n## Code Example\n\n```javascript\n// Example diagnostic sequence (pseudo-commands, replace <pod> and <ns> accordingly)\nkubectl describe pod <pod> -n <ns>\nkubectl get pod <pod> -n <ns> -o jsonpath '{.status.initContainerStatuses[*].state}'\nkubectl logs <pod> -c <init-container> -n <ns>\nkubectl describe deployment <deploy> -n <ns>\nkubectl get cm -n <ns>\nkubectl get secret -n <ns>\n```\n\n## Follow-up Questions\n\n- What would you check if the pod remains Pending after image pull completes?\n- How would you distinguish between a misconfigured readinessProbe vs a slow startup?","diagram":"flowchart TD\n  A[Start] --> B[Describe Pod] \n  B --> C[InitContainers Status] \n  C --> D[Check InitContainer Logs] \n  D --> E[Inspect Image Pull Policy/IDs] \n  E --> F[Review ConfigMaps/Secrets] \n  F --> G[Check Deployment Rollout]\n  G --> H[Root Cause Identified]","difficulty":"beginner","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Netflix","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T08:47:12.930Z","createdAt":"2026-01-13T08:47:12.930Z"},{"id":"q-1323","question":"In a Kubernetes cluster deploying an ML inference service, models and weights live in a private registry. Outline a practical plan to sign models with cosign, publish attestations, and enforce runtime verification so only attested models can be deployed via GitOps (Argo CD) and an enforcement policy (OPA Gatekeeper or Kyverno). Include concrete commands and sample configuration?","answer":"Sign each model blob with cosign sign-blob, generate and publish an attestation, and require attested provenance in deployment. Integrate into Argo CD via a policy check and enforce at admission with ","explanation":"## Why This Is Asked\nTests end-to-end security for ML artifacts, combining signing attestations, GitOps, and runtime policy in Kubernetes.\n\n## Key Concepts\n- Sigstore Cosign sign-blob attest and verify-blob\n- Model provenance and SBOM\n- GitOps with Argo CD\n- Admission policies: OPA Gatekeeper or Kyverno\n\n## Code Example\n```bash\n# Sign a model blob and create an attestation\ncosign sign-blob -key cosign.key models/model-v1.bin\ncosign attest -key cosign.key --predicate predicate.json models/model-v1.bin\ncosign verify-blob -key cosign.pub models/model-v1.bin\n```\n\n```yaml\n# Kyverno policy requiring attestation before deployment\napiVersion: kyverno.io/v1\nkind: ClusterPolicy\nmetadata:\n  name: require-model-attestation\nspec:\n  validationFailureAction: enforce\n  rules:\n  - name: attested-model-artifact\n    match:\n      resources:\n        kinds: [Deployment]\n    validate:\n      message: Attestation required for model artifact\n      pattern:\n        metadata.annotations.cosign_attested: \"true\"\n```\n\n```rego\npackage kubernetes.admission\n\ndeny[{\"msg\": msg}] {\n  input.request.kind.kind == \"Deployment\"\n  not input.request.object.metadata.annotations[\"cosign_attested\"] == \"true\"\n  msg = \"Model artifact must be attested with cosign before deployment\"\n}\n```\n\n## Follow-up Questions\n- How would you handle key rotation and revocation?\n- How would you test end-to-end in CI/CD and cluster?","diagram":null,"difficulty":"intermediate","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","PayPal","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T11:34:44.913Z","createdAt":"2026-01-13T11:34:44.913Z"},{"id":"q-1356","question":"You manage a Kubernetes cluster hosting regulated data across tenants. Design a practical end-to-end plan to enable at-rest encryption with envelope encryption using a cloud KMS, protect both API server data and etcd data, rotate keys safely, and prove compliance via CI checks. Include concrete commands and sample manifests?","answer":"Configure Kubernetes EncryptionConfig to use a cloud KMS as the envelope-encryption provider, encrypting secrets and etcd data. Rotate keys by adding a new KMS key, applying it via the config, and rol","explanation":"## Why This Is Asked\nTests practical mastery of data protection in Kubernetes, including key management, rotation strategies, and CI validation for compliance.\n\n## Key Concepts\n- Envelope encryption with Kubernetes EncryptionConfig\n- Cloud KMS integration for kms provider\n- etcd and secrets encryption\n- Key rotation and rolling updates\n- CI gates and audit logging\n\n## Code Example\n```yaml\napiVersion: v1\nkind: EncryptionConfig\nresources:\n- resources:\n  - secrets\n  providers:\n  - kms:\n      name: \"cloud-kms\"\n      endpoint: \"unix:///var/run/kms/kms.sock\"\n      cachesize: 1000\n  - aesgcm:\n      keys:\n        - name: \"default-key\"\n          secret: \"BASE64ENCODEDKEY==\"\n```\n\n## Follow-up Questions\n- How would you validate re-encryption during rotation without downtime?\n- What logging/auditing would you implement to detect key mis-rotation or misuse?","diagram":null,"difficulty":"intermediate","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T13:11:29.765Z","createdAt":"2026-01-13T13:11:29.765Z"},{"id":"q-1374","question":"Scenario: A multi-cloud platform runs Kubernetes on EKS and serverless runtimes; ensure only cosign-signed images with verifiable attestations can be deployed across all targets. Describe an end-to-end plan to enforce cross-registry provenance, automate Rekor attestations, and integrate with a GitOps workflow (Argo CD), including concrete commands and sample policy rules?","answer":"Sign images in CI with a KMS-backed cosign key, push to all registries, and attach a Rekor attestation. Enforce via a Gatekeeper/OPA policy that rejects unsigned images and a GitOps gate in Argo CD. C","explanation":"## Why This Is Asked\nTests understanding of cross-environment image provenance and automated attestation in a multi-cloud setup.\n\n## Key Concepts\n- Cross-registry signing and verification\n- Rekor attestations and intoto format\n- Policy-driven deployment gates (OPA/Gatekeeper, Argo CD gate)\n\n## Code Example\n```bash\n# Sign and verify (example for AWS KMS key)\ncosign sign --key 'kms://aws/kms/cosign' ghcr.io/org/service:tag\ncosign verify --key 'kms://aws/kms/cosign' ghcr.io/org/service:tag\ncosign attest ghcr.io/org/service:tag --predicate attest.json --type intoto\n```\n\n## Follow-up Questions\n- How would you rotate signing keys without breaking deployments?\n- How do you audit failed attestations across clusters?","diagram":"flowchart TD\n  A[Code Commit] --> B[CI Build]\n  B --> C[cosign Sign]\n  C --> D[Publish to Registries]\n  D --> E[Attach Attestations]\n  E --> F[Policy Enforcement]\n  F --> G[Deployment Across Environments]","difficulty":"intermediate","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Meta","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T14:37:12.766Z","createdAt":"2026-01-13T14:37:12.766Z"},{"id":"q-1398","question":"In a multi-tenant Kubernetes cluster with images stored in a private OCI registry, design an end-to-end workflow to sign images with cosign, publish attestations, and enforce that only attested images are deployed. Include concrete commands, CI hints, and an admission control policy snippet?","answer":"Sign each image with cosign and publish attestations to the private OCI registry; automate in CI by running 'cosign sign' and 'cosign attest' on tag pushes. Enforce with a Kyverno deny policy (check f","explanation":"## Why This Is Asked\nReal-world supply chain integrity for multi-tenant clusters requires verifiable attestation and automated enforcement.\n\n## Key Concepts\n- Sigstore cosign attestations\n- Private OCI registries\n- CI/CD integration (GitHub Actions)\n- Admission control (Kyverno) and OPA/rego\n\n## Code Example\n```yaml\n# Kyverno policy example (high level)\napiVersion: kyverno.io/v1\nkind: ClusterPolicy\nmetadata:\n  name: require-signed-images\nspec:\n  rules:\n  - name: deny-unsigned\n    match:\n      resources:\n        kinds: [\"Deployment\"]\n    validate:\n      message: \"Image must be cosign-attested\"\n      pattern:\n        spec:\n          template:\n            spec:\n              containers:\n              - image: \"*\"\n```\n\n```rego\npackage imageauth\ndefault allow = false\n\nallow {\n  input.request.kind.kind == \"Pod\"\n  # imagine a field that indicates attestation validity\n  input.review.request.object.spec.containers[_].imageAttested == true\n}\n```\n\n## Follow-up Questions\n- How would you handle base-image rotation and attestation key rotation?\n- How do you test the policy across multiple clusters and during CI/CD failures?\n- What performance trade-offs arise with frequent attestations and how to mitigate them?","diagram":"flowchart TD\n  A[CI Build] --> B[cosign sign]\n  B --> C[cosign attest]\n  C --> D[Push to registry]\n  D --> E[GitOps deploy]\n  E --> F[Admission checks]","difficulty":"intermediate","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Discord","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T15:40:02.857Z","createdAt":"2026-01-13T15:40:02.857Z"},{"id":"q-1444","question":"Scenario: A poly-cloud serverless stack runs AWS Lambda, GCP Cloud Functions, and Azure Functions. CI/CD signs each function package and its dependencies with cosign and publishes SLSA attestations to a central registry. Deployments must be allowed only if attested across all clouds. Describe an end-to-end plan, including concrete commands and sample configs for signing, attesting, and cross-cloud enforcement?","answer":"Sign each function image digest and its SBOM with cosign sign-blob or cosign sign, then publish an attestation via cosign attest to Rekor. Enforce across clouds with OPA/Kyverno policies that block de","explanation":"## Why This Is Asked\nTests cross-cloud supply chain security for serverless deployments, requiring practical steps to sign artifacts, publish attestations, and enforce those attestations across multiple cloud platforms.\n\n## Key Concepts\n- Sigstore cosign and SLSA attestations\n- Multi-cloud deployment and policy enforcement (OPA, Kyverno)\n- CI/CD automation with GitHub Actions\n\n## Code Example\n```yaml\n# GitHub Actions: sign and attest serverless image\nname: Sign and Attest Serverless Images\non:\n  push:\n    branches: [ main ]\njobs:\n  sign_attest:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - name: Setup cosign\n        uses: sigstore/cosign-installer@v1\n      - name: Build image\n        run: |\n          IMAGE=registry.example.com/function-${{ github.sha }}:latest\n          docker build -t $IMAGE .\n      - name: Sign image\n        run: cosign sign --key cosign.key $IMAGE\n      - name: Attest image\n        run: cosign attest --key cosign.key --predicate attest.json $IMAGE\n```\n\n```rego\n# Kyverno policy enforcing attestations (example)\npackage sigstore\n\ndefault allow = false\n\nallow {\n  input.attestations[_].predicate.type == \"type.googleapis.com/slsa.Provenance\"\n  input.image.docker_content_digest != \"\"\n}\n```\n\n## Follow-up Questions\n- How would you handle key rotation and revocation across clouds?\n- What metrics and dashboards would you add to monitor attestation health?","diagram":"flowchart TD\n  A[Digest] --> B[Sign Image]\n  B --> C[Publish Attestation]\n  C --> D[Policy Check]\n  D --> E[Deploy Across Clouds]","difficulty":"intermediate","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","LinkedIn","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T17:37:52.667Z","createdAt":"2026-01-13T17:37:52.667Z"},{"id":"q-1522","question":"Scenario: In a multi-tenant SaaS, a central migration service applies Flyway SQL scripts to dozens of PostgreSQL instances. How would you implement a concrete plan to sign each migration with cosign in CI, publish attestations to a registry, and enforce at runtime that only attested migrations are executed? Include concrete signing commands, attestation storage approach, and a sample enforcement policy (Kyverno/OPA) plus integration steps with Flyway?","answer":"Sign each migration in CI with cosign using a dedicated key, publish the attestation to a Sigstore-compatible registry, and enforce runtime verification before Flyway applies the script. Fail the job ","explanation":"## Why This Is Asked\nTests practical signing and runtime enforcement in migration workflows.\n\n## Key Concepts\n- Artifact signing with cosign\n- Attestations and trust roots\n- Runtime verification before DB changes\n- Policy tooling (Kyverno/OPA)\n\n## Code Example\n```yaml\n# Kyverno or OPA policy samples would be here in a real setup\n```\n\n## Follow-up Questions\n- How would you scale attestation storage across tenants?\n- How do you handle key rotation and revocation?","diagram":"flowchart TD\n  CI[CI/CD] --> Sign[Sign Migration]\n  Sign --> Attest[Publish Attestation]\n  Attest --> Runner[Migration Runner]\n  Runner --> DB[PostgreSQL DBs]","difficulty":"intermediate","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Slack","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T20:40:10.489Z","createdAt":"2026-01-13T20:40:10.489Z"},{"id":"q-1551","question":"In a Kubernetes-based data platform hosting a multi-tenant ML feature store exposed via a high-volume API, design a privacy-preserving, end-to-end audit trail to support forensics without exposing PII. Specify architecture, RBAC/ABAC controls, OpenTelemetry instrumentation, log pipeline (Fluentd/Loki), encryption, data redaction, retention, and how you’d run production-scale incident drills. What would you monitor first and why?","answer":"Instrument all feature-store accesses with OpenTelemetry; funnel logs/traces to per-tenant Loki sinks; redact PII before persistence; encrypt at rest with KMS; enforce ABAC via OPA and per-tenant RBAC.","explanation":"## Why This Is Asked\nThis question tests end-to-end design for privacy-preserving forensics on a multi-tenant data platform, focusing on practical tooling choices and production safety.\n\n## Key Concepts\n- OpenTelemetry instrumentation\n- Per-tenant isolation (Namespaces, ServiceAccounts)\n- Loki/NDS for immutable logs\n- Data redaction, KMS, mTLS\n- ABAC with OPA, RBAC policies\n- Incident drills and validation\n\n## Code Example\n```javascript\n// No executable code required in interview; design rationale only\n```\n\n## Follow-up Questions\n- How would you validate retention and deletion across tenants?","diagram":null,"difficulty":"advanced","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","LinkedIn","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T06:29:58.858Z","createdAt":"2026-01-13T21:39:15.067Z"},{"id":"q-1691","question":"Scenario: A Terraform-driven multi-tenant cloud platform provisions resources across clouds. You must sign every Terraform plan in CI with cosign, publish an attestation to a central registry, and enforce at runtime that only attested plans are applied by the GitOps flow. Describe an end-to-end approach with concrete signing commands, attestation storage layout, an sample OPA policy, and integration steps with the deployment pipeline?","answer":"CI: terraform plan -out=tfplan; terraform show -json tfplan > tfplan.json; cosign sign-blob tfplan.json ghcr.io/org/terraform-plans/tenant-A:plan-${CI_COMMIT_SHORT_SHA}; cosign attest ghcr.io/org/terr","explanation":"## Why This Is Asked\nTests the ability to extend supply chain security to IaC, operationalize attestation, and integrate with GitOps across Terraform and multiple clouds. It covers plan hashing, artifact signing, attestation publication, and policy enforcement, including drift handling and key rotation.\n\n## Key Concepts\n- IaC plan provenance (Terraform plan JSON)\n- cosign sign-blob and attest\n- Rekor/Sigstore attestation registry\n- Open Policy Agent (OPA) for gate policies\n- GitOps gating (Argo CD / Tekton)\n\n## Code Example\n```javascript\nconst fs = require('fs');\nconst crypto = require('crypto');\nconst plan = fs.readFileSync('tfplan.json');\nconsole.log('SHA256', crypto.createHash('sha256').update(plan).digest('hex'));\n```\n\n## Follow-up Questions\n- How would you handle re-signing if a plan changes post-attestation?\n- How to scale attestations across dozens of tenants with separate keys and registries?","diagram":null,"difficulty":"intermediate","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Netflix","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T07:01:14.464Z","createdAt":"2026-01-14T07:01:14.464Z"},{"id":"q-1734","question":"In a multi-cloud Terraform deployment across AWS, GCP, and Azure, a central registry hosts official modules. Propose a concrete plan to: - sign every module and its dependencies with cosign during CI, - publish attestations to a provenance registry, - enforce at plan/apply time that only attested modules are used via an OPA policy or equivalent gate, including concrete signing commands, storage layout for attestations, and a sample enforcement policy with integration steps?","answer":"CI signs and attests every Terraform module tarball using cosign, stores attestations in registry.example.com/terraform/attestations, and gates plans with an OPA policy requiring an attestation for al","explanation":"## Why This Is Asked\nEvaluates end-to-end IaC provenance, cross‑cloud module governance, and CI/CD gating.\n\n## Key Concepts\n- Terraform module provenance, cosign attestations, SBOMs\n- GitHub Actions or CI gates, OPA policy enforcement\n- Multi-cloud module registry integration and access control\n\n## Code Example\n```javascript\npackage terraform\n\ndefault allow = false\n\nallow {\n  input.module == m\n  data.attestations[m].signed == true\n}\n```\n\n## Follow-up Questions\n- How would you handle rotated signing keys across clouds?\n- What changes when using a module registry instead of a local path?","diagram":null,"difficulty":"intermediate","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Apple","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T08:56:28.436Z","createdAt":"2026-01-14T08:56:28.436Z"},{"id":"q-1846","question":"In a Tesla-scale Databricks lakehouse on AWS, with Unity Catalog and a Kubernetes data plane, contractors routinely export aggregated datasets to external S3 buckets. Design a detection and response mechanism that distinguishes legitimate exports from exfiltration attempts. Include telemetry (Unity Catalog audit logs, IAM activity, Delta table operations), thresholds, alerting, and an incident playbook?","answer":"Implement a multi-layer detection using Unity Catalog audit logs, IAM/STS activity, and Delta Lake metadata to flag outbound exports to external buckets beyond a whitelist. Enrich with VPC flow logs a","explanation":"## Why This Is Asked\n\nTests ability to design multi-source telemetry-driven detection for data exfiltration in complex cloud-native data platforms, balancing legitimate data workflows with security controls. Evaluates incident response readiness and depth of tooling integration.\n\n## Key Concepts\n\n- Unity Catalog audit logs and Delta Lake metadata\n- IAM/STS activity and rule-based access governance\n- External data egress controls and approval workflows\n- VPC flow logs, Kubernetes data plane telemetry, and export APIs\n- Baseline-based anomaly detection and auto-containment\n\n## Code Example\n\n```python\ndef is_suspicious(event, whitelist, THRESHOLD_GB):\n    if event.type != \"EXPORT\":\n        return False\n    if event.destination_bucket not in whitelist:\n        return True\n    if event.size_gb > THRESHOLD_GB:\n        return True\n    return False\n```\n\n## Follow-up Questions\n- How would you minimize false positives while maintaining network security?\n- What metrics would you monitor to tune thresholds over time?","diagram":"flowchart TD\n  A[Export Event] --> B{IsDestinationWhitelisted?}\n  B -- Yes --> C[CheckSizeThreshold]\n  B -- No --> D[Flag as Suspicious]\n  C -- IfAboveThreshold --> D\n  C -- IfBelowThreshold --> E[Allow with Monitoring]","difficulty":"advanced","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T14:33:35.974Z","createdAt":"2026-01-14T14:33:35.976Z"},{"id":"q-1964","question":"In a multi-tenant notebook service on Kubernetes, each notebook runs in a transient pod and pulls dependencies from a private registry. Design a concrete plan to sign the notebook artifact (notebook.ipynb) and its dependencies with cosign, publish SLSA attestations to a central registry, and enforce at runtime that only attested notebooks and dependencies are allowed to run. Include concrete signing commands, attestations storage, and a sample Kyverno/OPA policy plus integration steps with the notebook runner?","answer":"Sign notebook.ipynb and its dependencies with cosign, e.g. cosign sign notebook.ipynb --key cosign.key and cosign sign libs/*.whl --key cosign.key; publish SLSA attestations to a central registry; enf","explanation":"## Why This Is Asked\nTests practical attestation workflows in a notebook-centric, multi-tenant Kubernetes setup, including artifact signing, attestation storage, and runtime enforcement. \n\n## Key Concepts\n- Cosign signing of notebooks and dependencies\n- SLSA attestations and registry storage\n- Runtime policy enforcement (Kyverno/OPA)\n- Notebook runner integration and rollback considerations\n\n## Code Example\n```javascript\n// Example: building a simple policy check string to feed into OPA or Kyverno\nconst policy = `\npackage notebook.authz\n\ndefault allow = false\n\nallow {\n  input.attested == true\n}\n`;\n```\n\n## Follow-up Questions\n- How would you handle key rotation and revocation for cosign keys in this flow?\n- How would you test this end-to-end in CI/CD and in disaster recovery scenarios?","diagram":null,"difficulty":"intermediate","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Citadel","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T18:58:00.356Z","createdAt":"2026-01-14T18:58:00.356Z"},{"id":"q-2092","question":"In a Kubernetes-based data-analytics platform with two namespaces, dev and prod, design a practical RBAC and Pod Security setup for a new microservice 'data-processor' to ensure it can only read its own ConfigMaps and Secrets, runs as a non-root user, and cannot escalate privileges. Provide concrete manifest fragments (RBAC, ServiceAccount, RoleBinding, PodSecurityContext) and describe how to validate at admission time that all pods comply?","answer":"Create namespace-scoped RBAC with a dedicated ServiceAccount for data-processor in each namespace, a Role granting read access to ConfigMaps and Secrets, and a RoleBinding associating the ServiceAccount with the Deployment. Include PodSecurityContext to enforce non-root execution and restrict capabilities.","explanation":"## Why This Is Asked\n\nThis question evaluates practical understanding of Kubernetes security primitives including RBAC, ServiceAccount isolation, PodSecurityContext, and admission controls—essential skills for securing containerized workloads.\n\n## Key Concepts\n\n- **Namespace-scoped RBAC**: Role and RoleBinding for resource access control within specific namespaces\n- **ServiceAccount isolation**: Dedicated service accounts to separate pod identities and permissions\n- **PodSecurityContext**: Security constraints including runAsNonRoot, runAsUser, readOnlyRootFilesystem, and capability dropping\n- **Admission controls**: PodSecurity standards and OPA Gatekeeper for policy enforcement at admission time\n\n## Code Example\n\n```yaml\n# ServiceAccount (dev)\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: data-processor-sa\n  namespace: dev\n---\n# Role for read-only access to ConfigMaps and Secrets\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: data-processor-role\n  namespace: dev\nrules:\n- apiGroups: [\"\"]\n  resources: [\"configmaps\", \"secrets\"]\n  verbs: [\"get\", \"list\"]\n---\n# RoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: data-processor-binding\n  namespace: dev\nsubjects:\n- kind: ServiceAccount\n  name: data-processor-sa\n  namespace: dev\nroleRef:\n  kind: Role\n  name: data-processor-role\n  apiGroup: rbac.authorization.k8s.io\n```\n\n## Follow-up Questions\n\n- How would you test the RBAC permissions locally?\n- What changes are needed for the prod namespace?\n- How would you implement network policies alongside these security controls?","diagram":null,"difficulty":"beginner","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Databricks","DoorDash"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T04:14:12.430Z","createdAt":"2026-01-14T23:42:07.566Z"},{"id":"q-2128","question":"In a high-sensitivity environment spanning on-prem and cloud, design a tamper-evident Kubernetes audit logging and incident-response pipeline that preserves evidence from the API server, etcd, and kubelets while enabling automated triage and containment during a breach. Specify data formats, forwarding targets, non-repudiation measures, and failure modes?","answer":"Implement a multi-layered, tamper‑evident pipeline: enable API server and etcd audit logs, forward securely to an immutable object store (S3 with bucket-level and object immutability) and a SIEM via s","explanation":"## Why This Is Asked\n\nThis question probes practical design for tamper-resistant security data collection across heterogeneous Kubernetes estates, addressing post-breach forensics, automation, and resilience.\n\n## Key Concepts\n\n- Tamper-evident logs, audit policy, etcd encryption\n- Immutable storage, HMAC signing, key rotation\n- Secure log forwarding (webhooks, TLS)\n- Runtime security (Falco) and IR playbooks\n- Non-repudiation, data retention, failure modes\n\n## Code Example\n\n```javascript\nfunction signEvent(event, key) {\n  const crypto = require('crypto');\n  const h = crypto.createHmac('sha256', key);\n  h.update(event);\n  return h.digest('hex');\n}\n```\n\n## Follow-up Questions\n\n- How would you test integrity guarantees across region failures?\n- What are failure modes if the webhook sink is compromised?","diagram":"flowchart TD\n  A(API Server Audit) --> B(Immutable Store)\n  A --> C(SIEM Webhook)\n  D(Etcd Audit) --> B\n  E(Kubelet Audit) --> C\n  F(Falco) --> G(IR Playbook)\n  H(Integrity Verifier) --> B","difficulty":"advanced","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","PayPal","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T04:11:32.481Z","createdAt":"2026-01-15T04:11:32.482Z"},{"id":"q-2188","question":"In a monorepo-managed Kubernetes fleet with per-service Helm charts and shared base images, design an attestation model where base image attestations are linked to derived service images via SBOM, and enforcement ensures every deployed chart references an attested base. Describe signing commands, SBOM workflow, attestation storage, and a gating policy (OPA/Kyverno) with CI/CD integration?","answer":"Sign both base and service images with cosign, generate SBOMs with Syft, and publish attestations to Rekor; link derived attestation to base by embedding the base digest in the predicate. Commands: co","explanation":"## Why This Is Asked\nTests ability to design provenance enforcement across multiple artifacts and enforce it in a GitOps workflow.\n\n## Key Concepts\n- Attestation chaining and SBOM provenance\n- Rekor as a verifiable registry of attestations\n- Policy enforcement with Kyverno/OPA in deployment pipelines\n\n## Code Example\n```bash\ncosign sign --key cosign.key base.img:tag\n```\n\n```bash\nsyft base.img -o json:base.json\n```\n\n```bash\ncosign attest --artifact service.img:tag --predicate base.json\n```\n\n## Follow-up Questions\n- How would you scale attestations across 100+ services?\n- How to handle base image updates without breaking attestations?","diagram":"flowchart TD\n  A[Base image] --> B[cosign sign base]\n  B --> C[Attest base to Rekor]\n  D[Service image] --> E[cosign sign service]\n  E --> F[Attest service to Rekor]\n  C & F --> G[Enforcement gate (Kyverno/OPA)]\n  G --> H[Argo CD deploy]","difficulty":"intermediate","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T06:56:20.700Z","createdAt":"2026-01-15T06:56:20.700Z"},{"id":"q-2418","question":"A data ingestion pipeline uses a private artifact registry hosting Spark JARs and Python wheels. Each artifact is cosign-signed with a SBOM attestation; runtime must guarantee that only attested artifacts execute in Spark jobs managed by Airflow on Kubernetes. Describe an end-to-end plan to sign, publish attestations, and enforce runtime checks across the data plane, including concrete cosign commands, attestation storage approach, and a sample Kyverno/OPA policy plus integration steps with Airflow and the registry?","answer":"Sign artifacts with cosign and attach SBOM attestations; publish to a private registry; enforce at runtime via a Kyverno/OPA policy that blocks SparkSubmit when the artifact digest lacks a valid attes","explanation":"## Why This Is Asked\nTests hands-on signing, attestation generation, and runtime enforcement across data-plane components (Airflow, Spark, Kubernetes). It probes operational trade-offs like key rotation and attestation storage.\n\n## Key Concepts\n- cosign attestations and SBOMs\n- private artifact registries\n- runtime policy (Kyverno/OPA)\n- data-plane integration (Airflow, SparkSubmit)\n\n## Code Example\n\n```javascript\n// Example sequence:\ncosign generate-key-pair\ncosign sign --key cosign.key registry.example.org/pipeline/transforms.jar:sha256:<digest>\ncosign attest --predicate sbom.json --key cosign.key registry.example.org/pipeline/transforms.jar:sha256:<digest>\n```\n\n## Follow-up Questions\n- How would you rotate keys without breaking existing attestations?\n- How would you test the end-to-end policy in CI/CD?\n","diagram":"flowchart TD\n  A[Artifact] --> B[cosign Sign]\n  B --> C[cosign Attest]\n  C --> D[Publish Attestation to Registry]\n  D --> E[Enforcement (Kyverno/OPA)]","difficulty":"intermediate","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T17:41:09.088Z","createdAt":"2026-01-15T17:41:09.090Z"},{"id":"q-2508","question":"In a Knative-based FaaS layer on Kubernetes, functions are built as OCI images stored in a private registry and loaded by the function runtime at cold start. Design an end-to-end plan to sign and attest function images, publish attestations to Rekor, and enforce at runtime that only attested functions execute in the Knative namespace. Include concrete cosign commands, attestation storage approach, and a sample Kyverno or OPA policy plus integration steps with the registry and the function loader?","answer":"Plan: generate cosign keys, sign the image, create an SBOM, attest, and enforce at admission. Example commands: cosign generate-key-pair cosign.key cosign.pub; cosign sign --key cosign.key registry.ex","explanation":"## Why This Is Asked\nTests end-to-end attestation workflows: signing, SBOM generation, log storage, and runtime enforcement.\n\n## Key Concepts\n- OCI image signing and attestation with cosign\n- SBOM generation with syft\n- Rekor as transparency log\n- Policy-based enforcement (OPA/Kyverno)\n\n## Code Example\n```rego\npackage kubernetes.admission\ndeny[msg] {\n  input.request.kind.kind == \"Deployment\"\n  digest := input.request.object.spec.template.spec.containers[0].image\n  not data.attestations[digest]\n  msg := sprintf(\"Image %s not attested\", [digest])\n}\n```\n\n## Follow-up Questions\n- How would you scale attestation checks across hundreds of images?\n- What are failure modes if Rekor is unavailable?","diagram":"flowchart TD\nA[Build image] --> B[Push to registry]\nB --> C[Sign with cosign]\nC --> D[Create SBOM with syft]\nD --> E[Attest for SBOM]\nE --> F[ Rekor logs ]\nF --> G[Enforce at runtime via policy]","difficulty":"beginner","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","LinkedIn","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T20:51:45.426Z","createdAt":"2026-01-15T20:51:45.426Z"},{"id":"q-2537","question":"In a policy-driven edge compute fleet, WASM modules are built per customer and pushed to a private OCI registry. You must ensure only attested modules run on edge devices. Design a concrete plan to sign modules with cosign, publish SLSA attestations, and enforce at runtime that only attested modules are loaded via an admission controller (Kyverno/OPA). Include concrete commands and sample policies?","answer":"Sign each WASM module with cosign, generate an SBOM with Syft, publish an attestation to the registry, and enforce at edge runtimes that only attested modules load via Kyverno/OPA. Commands: cosign sign --key cosign.key ghcr.io/org/module@tag && cosign attest --predicate attest.json --signature sig.cosign ghcr.io/org/module@tag","explanation":"## Why This Is Asked\nTests practical enforcement of attestation for edge modules, including SBOM integration and automation.\n\n## Key Concepts\n- Cosign signing and attestations\n- SBOMs for wasm modules\n- Runtime admission controls (Kyverno/OPA)\n- GitHub Actions automation\n\n## Code Example\n\n```bash\ncosign sign --key cosign.key ghcr.io/org/module@tag\ncosign attest --predicate attest.json --signature sig.cosign ghcr.io/org/module@tag\n```\n\n```yaml\napiVersion: kyverno.io/v1\nkind: ClusterPolicy\nmetadata:\n  name: require-attested-wasm\nspec:\n  rules:\n  - name: attestation-check\n    match:\n      resources:\n        kinds:\n        - Pod\n        selector:\n          matchLabels:\n            app: wasm-edge\n    validate:\n      pattern:\n        spec:\n          containers:\n          - image: \"ghcr.io/org/*\"\n            securityContext:\n              capabilities:\n                add: [\"SYS_ADMIN\"]\n  - name: verify-attestation\n    match:\n      resources:\n        kinds:\n        - Pod\n    validate:\n      anyPattern:\n      - pattern:\n          metadata:\n            annotations:\n              cosign.sigstore.dev/attestations: \"?*\"\n```","diagram":"flowchart TD\n  A[WASM Module] --> B{Signed?}\n  B -->|Yes| C[Attested in Registry]\n  B -->|No| D[Rejected]\n  C --> E[Edge Runtime]\n  E --> F[Policy Enforced]","difficulty":"intermediate","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Google","Instacart"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T04:25:33.253Z","createdAt":"2026-01-15T21:44:20.454Z"},{"id":"q-2639","question":"In an AWS EKS cluster with Istio, an attacker attempts DNS tunneling to exfiltrate data from the dev namespace. Design a practical detection pipeline using: (a) eBPF-based DNS egress monitoring, (b) CoreDNS query analytics, (c) Istio telemetry + network policies, and (d) alerting aligned to MITRE techniques. Explain data flow, signals to watch, and how you would validate alerts?","answer":"Flag DNS tunneling by watching: (i) high-entropy, long random subdomains; (ii) spikes in external DNS queries from dev namespace; (iii) frequent TXT/NULL queries. Collect via eBPF, enrich with pod/nam","explanation":"## Why This Is Asked\nTests ability to design a practical detection pipeline for cloud-native DNS tunneling, combining kernel telemetry, DNS-layer analytics, and service-mesh visibility; emphasizes real-world validation and false-positive control.\n\n## Key Concepts\n- eBPF DNS egress monitoring\n- CoreDNS analytics and entropy-based signals\n- Istio telemetry integration and namespace scoping\n- Multi-signal alerting and validation\n- Synthetic traffic to verify detections\n\n## Code Example\n```javascript\nfunction entropy(s){\n  const counts = {};\n  for(const ch of s) counts[ch]=(counts[ch]||0)+1;\n  const total = s.length;\n  let e=0; for(const c of Object.values(counts)){ const p=c/total; e -= p*Math.log2(p); }\n  return e;\n}\nfunction isSuspicious(domain){\n  const parts = domain.split('.');\n  const labels = parts.slice(0,-1);\n  const ent = labels.map(l=>entropy(l)).reduce((a,b)=>a+b,0);\n  const len = domain.length;\n  return ent>3.5 || len>30;\n}\n```\n\n## Follow-up Questions\n- How would you tune thresholds to reduce false positives across tenants?\n- What data retention and RBAC would you implement for CoreDNS and eBPF telemetry?","diagram":null,"difficulty":"advanced","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","Netflix","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T04:20:05.084Z","createdAt":"2026-01-16T04:20:05.084Z"},{"id":"q-2687","question":"Scenario: A Kafka Connect cluster loads dozens of custom connectors from a shared registry. Describe a concrete plan to sign each connector JAR and its JSON configs with cosign, publish attestations to Rekor, and enforce at deployment and runtime that only attested connectors are loaded. Include concrete signing commands, attestation storage layout, and a sample Kyverno/OPA policy and integration steps with Kafka Connect?","answer":"Generate a cosign key pair, sign each connector JAR and its JSON configs, and publish an attestation to Rekor. Store attestations in a central registry and reference them from the Connector manifest. ","explanation":"## Why This Is Asked\n\nTests practical end-to-end security for data integration pipelines, including artifact signing, attestation storage, and runtime enforcement with policy engines.\n\n## Key Concepts\n\n- Artifact signing with cosign and key management\n- Attestations stored in Rekor registry\n- Policy-driven runtime enforcement (Kyverno/OPA)\n- Integration points with Kafka Connect\n\n## Code Example\n\n```javascript\n// Minimal illustration: pseudo-check attestation reference in manifest\nasync function isAttested(manifest) {\n  const ref = manifest.attestationRef;\n  // fetch and verify against Rekor/public cosign key\n  return !!ref;\n}\n```\n\n## Follow-up Questions\n\n- How would you handle rotation of signing keys and revocation?\n- What metrics and alerts would you add to detect failed attestations at startup?","diagram":"flowchart TD\n  A[Connector Registry] --> B[Signing Service]\n  B --> C[ Rekor Registry ]\n  C --> D[Kafka Connect Deployment]\n  D --> E[Policy Engine (Kyverno/OPA)]","difficulty":"intermediate","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","Instacart","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T06:58:58.892Z","createdAt":"2026-01-16T06:58:58.893Z"},{"id":"q-2747","question":"Scenario: An edge compute platform runs user-provided WASM modules on 100 edge nodes. A compromised module could exfiltrate data or break isolation. Describe a concrete plan to sign each WASM module and its metadata with cosign in CI, publish attestations to Rekor, and enforce at deployment and runtime that only attested WASMs are loaded, including exact signing commands, attestation layout, and a sample Kyverno/OPA policy plus integration steps with the edge runtime?","answer":"Describe a concrete pipeline to secure edge WASM modules: sign modules with cosign in CI, publish Rekor attestations, and run a per-node verifier that only loads attested artifacts. Include exact sign","explanation":"## Why This Is Asked\nTests practical, implementable security controls for edge WASM deployment: signing, attestations, and runtime enforcement, plus policy-as-code integration.\n\n## Key Concepts\n- WASM module attestation and integrity\n- Cosign signing in CI and Rekor attestations\n- Edge runtimes and module-verification\n- Policy enforcement with Kyverno/OPA\n- Attestation lifecycle: rotation and revocation\n\n## Code Example\n```javascript\n// Signing commands (illustrative)\ncosign generate-key-pair\ncosign sign --key cosign.key wasm-module.wasm\ncosign attest --key cosign.key --artifact wasm-module.wasm --predicate '{\"build\":\"ci\"}'\n```\n\n## Follow-up Questions\n- How would you test end-to-end attestation validation across 100 edge nodes?\n- How would you handle key rotation and revocation without affecting in-flight modules?","diagram":"flowchart TD\n  A[WASM Module Published] --> B[Cosign Sign] \n  B --> C[Rekor Attestation] \n  C --> D[Edge Verifier Checks] \n  D --> E[Module Loaded on Edge Node]","difficulty":"intermediate","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Slack","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T10:36:21.058Z","createdAt":"2026-01-16T10:36:21.059Z"},{"id":"q-2837","question":"In a multi-tenant Kubernetes cluster used by several large services, you must build a scalable, privacy-conscious audit-logging and anomaly-detection pipeline that ingests API server audit events from multiple masters, normalizes per-tenant identity, and emits near-real-time alerts for suspicious access patterns (e.g., sudden spikes in secret fetches, impersonation). Outline the design, components, and trade-offs?","answer":"Design uses API server audit policy with tiered sampling, redact PII, route events to a centralized ledger via a mutating webhook and Kafka, normalize by tenant using the user/impersonation fields, st","explanation":"## Why This Is Asked\nAssesses ability to design scalable, privacy-aware audit pipelines for multi-tenant clusters, balancing compliance with operational overhead.\n\n## Key Concepts\n- Kubernetes Audit Logs\n- Webhook backends & mutating webhooks\n- Per-tenant identity normalization\n- Privacy redaction & data retention\n- Real-time anomaly detection & alerting\n\n## Code Example\n```javascript\n// Pseudo-processor: route audit events to tenant-scoped store\nfunction routeEvent(evt){\n  const tenant = evt.user.tenant;\n  store.append(tenant, evt);\n  if (detectAnomaly(evt, tenant)) alert(tenant, evt);\n}\n```\n\n## Follow-up Questions\n- How would you test for false positives at scale?\n- Which storage/processing stack would you choose for 1k+ tenants and why?","diagram":"flowchart TD\n  API[API Server] --> Policy[Audit Policy]\n  Policy --> Webhook[Webhook Backend]\n  Webhook --> Store[Audit Store]\n  Store --> Detector[Anomaly Detector]\n  Detector --> Alert[Alerts]","difficulty":"advanced","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Google","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T14:12:38.997Z","createdAt":"2026-01-16T14:12:38.997Z"},{"id":"q-2843","question":"Scenario: A data platform ingests dozens of datasets into a lakehouse from multiple teams. Design a concrete plan to sign each dataset file with cosign, publish attestations to Rekor, and enforce at read-time that only attested datasets are processed by downstream Airflow pipelines and Looker dashboards. Include concrete signing commands, attestation storage layout, and a sample Kyverno/OPA policy integration with Airflow?","answer":"Sign each dataset.parquet with cosign, publish attestations to Rekor, and enforce at read-time that only attested datasets flow to Airflow and Looker. Steps: (1) generate per-org cosign keys; (2) sign","explanation":"## Why This Is Asked\nTests data provenance, policy enforcement, and integration with real tools.\n\n## Key Concepts\n- Data provenance and attestations\n- Cosign and Rekor for file-level artifacts\n- OPA/Kyverno policies for data reads\n- Airflow integration points and looker read-time checks\n\n## Code Example\n```rego\npackage data.provenance\n\ndefault allow = false\n\nallow {\n  input.kind == \"dataset_read\"\n  attest := data.attestations[input.path]\n  attest.digest == input.digest\n  attest.exists\n}\n```\n\n## Follow-up Questions\n- How would you scale attestation storage for thousands of datasets?\n- How would you handle revoked attestations in Rekor?\n","diagram":"flowchart TD\n  A[Data Ingest] --> B[Compute Digest]\n  B --> C[Sign with cosign & attestation]\n  C --> D[Publish to Rekor]\n  D --> E[Catalog reference]\n  E --> F[OPA/Kyverno policy]\n  F --> G[Airflow DAGs / Looker reads]","difficulty":"intermediate","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Citadel","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T14:36:56.036Z","createdAt":"2026-01-16T14:36:56.036Z"},{"id":"q-2911","question":"In a private Python package index used by Apple and MongoDB, design a beginner CI workflow to generate CycloneDX SBOMs for wheels, cosign-sign artifacts, publish attestations to Rekor, and enforce that only attested wheels can be uploaded. Include concrete cosign commands, attestation storage layout, and a sample Kyverno/OPA policy?","answer":"Design a CI gate: for each wheel built, generate an SBOM (CycloneDX), sign the wheel with cosign, attach the SBOM as an attestation, and publish to Rekor. Gate uploads by running cosign verify and Rek","explanation":"## Why This Is Asked\nTests practical skills in securing third‑party artifacts in a private Python index, using SBOMs, cosign attestations, and policy gates.\n\n## Key Concepts\n- CycloneDX SBOM generation\n- Cosign signing and attestations\n- Rekor attestation store\n- CI gates and policy enforcement (Kyverno/OPA)\n\n## Code Example\n```bash\n# Build wheel\npython -m build --wheel\n\n# SBOM\ncyclonedx-bom -o dist/name-0.1-py3-none-any.bom.json\n\n# Sign\ncosign sign --key cosign.key dist/name-0.1-py3-none-any.whl\n\n# Attest SBOM\ncosign attest --key cosign.key --attestation dist/name-0.1-py3-none-any.bom.json dist/name-0.1-py3-none-any.whl\n\n# Verification gate\ncosign verify --key cosign.pub dist/name-0.1-py3-none-any.whl --rekor-url https://rekor.example.com\n```\n\n## Follow-up Questions\n- How would you handle key rotation and revocation for cosign keys?\n- How would you simulate a failed attestation in the CI pipeline and prevent uploads?","diagram":null,"difficulty":"beginner","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T17:31:09.683Z","createdAt":"2026-01-16T17:31:09.685Z"},{"id":"q-3034","question":"In a Kubernetes-based edge compute platform, workers pull WebAssembly plugins from a registry to extend behavior. Describe a concrete plan to sign WASM plugins with cosign, publish attestations to Rekor, and enforce at runtime that only attested WASM plugins are loaded by the plugin loader. Include concrete signing commands, attestation storage layout, and a sample Kyverno/OPA policy plus integration steps with the plugin loader?","answer":"Sign each WASM plugin with cosign using a private key, push the signed artifact to the registry, and publish a Rekor attestation. Commands: cosign sign --key cosign.key wasm-plugin.wasm; cosign attach sbom --sbom sbom.json wasm-plugin.wasm; cosign attest --predicate https://rekor.example/api/v1/log/entries --key cosign.key wasm-plugin.wasm","explanation":"## Why This Is Asked\n\nTests ability to apply attestation to WASM plugins in edge/k8s environments, a realistic gap beyond container images.\n\n## Key Concepts\n\n- WASM module signing with cosign\n- Rekor attestations for wasm artifacts\n- Runtime verification in a restricted loader\n- Policy enforcement with Kyverno/OPA\n- Edge CI/CD integration with registry labels\n\n## Code Example\n\n```javascript\n// Pseudo loader verification\nasync function verifyAndLoad(wasmName) {\n  const att = await fetch(`https://rekor.example/attestations/${wasmName}`);\n  if (!att?.valid) throw new Error('Attestation missing or invalid');\n  // Load WASM module\n  return await WebAssembly.instantiateStreaming(fetch(wasmName));\n}\n```","diagram":null,"difficulty":"intermediate","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","Microsoft","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T05:41:54.553Z","createdAt":"2026-01-16T21:49:09.596Z"},{"id":"q-3076","question":"In a Kubernetes cluster with Kyverno installed, draft a beginner policy to ensure all pods in namespace secure cannot run as root, require runAsNonRoot true and runAsUser 1000, enforce readOnlyRootFilesystem, disallow privileged containers and added capabilities, require imagePullPolicy Always, and images from registry example.com/secure/*. Provide the Kyverno policy YAML, an example compliant Pod spec and a noncompliant one, and the validation steps?","answer":"I would implement a Kyverno policy named secure-pods in namespace secure that enforces: runAsNonRoot true, runAsUser 1000, readOnlyRootFilesystem true, no privileged, and drop ALL capabilities. Also r","explanation":"## Why This Is Asked\n\nTests understanding of policy as code, basic Kubernetes security posture, and simple automation.\n\n## Key Concepts\n\n- Kyverno policy definitions\n- Pod securityContext fields\n- Image provenance enforcement\n\n## Code Example\n\n```yaml\napiVersion: kyverno.io/v1\nkind: ClusterPolicy\nmetadata:\n  name: secure-pods\nspec:\n  validationFailureAction: enforce\n  rules:\n  - name: ensure-secure-context\n    match:\n      resources:\n        kinds: [Pod]\n        namespaces: [secure]\n    validate:\n      message: SecurityContext must be non-root, readOnlyRootFilesystem, no privileged, drop ALL\n      pattern:\n        spec:\n          containers:\n          - name: \"*\"\n            securityContext:\n              runAsNonRoot: true\n              runAsUser: 1000\n              allowPrivilegeEscalation: false\n              readOnlyRootFilesystem: true\n              capabilities:\n                drop:\n                - ALL\n  - name: image-policy\n    match:\n      resources:\n        kinds: [Pod]\n        namespaces: [secure]\n    validate:\n      message: Image must come from allowed registry\n      pattern:\n        spec:\n          containers:\n          - image: example.com/secure/*\n  - name: image-pull-policy\n    match:\n      resources:\n        kinds: [Pod]\n        namespaces: [secure]\n    validate:\n      message: imagePullPolicy must be Always\n      pattern:\n        spec:\n          containers:\n          - imagePullPolicy: Always\n```\n\n## Follow-up Questions\n\n- How would you test policy enforcement with a compliant Pod? a noncompliant Pod?\n- How would you extend to cover initContainers or multiple namespaces?","diagram":null,"difficulty":"beginner","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","LinkedIn"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T23:45:30.368Z","createdAt":"2026-01-16T23:45:30.369Z"},{"id":"q-3103","question":"You operate a multi-tenant data science platform on Kubernetes used by multiple teams across Uber and Robinhood; design a concrete, automated control plane to enforce per-tenant data access, runtime isolation, and secrets management. Include: (1) a policy framework using Open Policy Agent (OPA) or Kyverno, (2) secret rotation and access controls with Kubernetes Secrets and CSI Secrets Store, (3) namespace network egress controls and audit logging to Rekor, (4) a test plan with compliant vs noncompliant examples, and (5) an integration with CI to attest policy conformance. How would you implement this end-to-end?","answer":"Design a comprehensive, automated control plane for multi-tenant Kubernetes environments: implement policy-as-code using OPA for complex authorization decisions or Kyverno for simpler Kubernetes-native policies, integrate Secrets Store CSI Driver with external secret providers (AWS Secrets Manager, Azure Key Vault, or HashiCorp Vault) for automatic credential rotation, enforce namespace-level NetworkPolicies with egress controls to restrict tenant traffic, implement comprehensive audit logging to Rekor for policy compliance tracking, and establish CI/CD integration with policy conformance testing using tools like Conftest or Kyverno CLI to ensure all deployments meet security requirements before production.","explanation":"## Why This Is Asked\n\nTests ability to design scalable, automated security controls in multi-tenant Kubernetes environments, focusing on policy-language choice, secret management, network isolation, and attestation.\n\n## Key Concepts\n\n- Policy-as-code (OPA vs Kyverno) for tenant isolation\n- Secrets management with CSI Secrets Store and automatic rotation\n- Namespace-level NetworkPolicy and egress controls\n- Audit/attestation with Rekor/Sigstore\n- CI integration for policy conformance and artifact attestation\n\n## Code Example\n\n```yaml\n# Example Kyverno policy snippet (placeholder)\napiVersion: kyverno.io/v1\nkind: ClusterPolicy\nmetadata:\n  name: tenant-isolation\nspec:\n  validationFailureAction: Enforce\n  rules:\n  - name: require-namespace-labels\n    match:\n      any:\n      - resources:\n          kinds: [Pod]\n    validate:\n      message: \"Pods must have tenant labels for isolation\"\n      pattern:\n        metadata:\n          labels:\n            tenant: \"?*\"\n```\n\n## Implementation Approach\n\n1. **Policy Framework**: Choose OPA Gatekeeper for complex cross-namespace policies or Kyverno for simpler, Kubernetes-native validation\n2. **Secrets Management**: Deploy CSI Secrets Store with provider-specific drivers and configure automatic rotation policies\n3. **Network Controls**: Implement namespace-scoped NetworkPolicies with egress deny-by-default and specific allow rules\n4. **Audit Logging**: Configure policy decision logging to Rekor with signed transparency logs\n5. **CI Integration**: Add policy testing stages using Conftest/Kyverno CLI and integrate with Sigstore for artifact signing","diagram":"flowchart TD\n  A[User tenant] --> B[Namespace per tenant]\n  B --> C[Policy gatekeeper (OPA/Kyverno)]\n  B --> D[Secrets Store CSI integration]\n  C --> E[Audit to Rekor]\n  D --> F[Network egress controls]\n  E --> G[Attestation workflow in CI]","difficulty":"advanced","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Robinhood","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T03:45:54.686Z","createdAt":"2026-01-17T02:21:59.015Z"},{"id":"q-3222","question":"Scenario: A Kubernetes-based plugin host loads third‑party data processors at runtime. Design a concrete plan to cosign sign each plugin binary and its manifest, publish attestations to Rekor, and enforce at load time that only attested plugins are admitted by a loader gatekeeper with OPA/Kyverno. Include concrete signing/verification commands, attestation layout, and sample policy?","answer":"Sign each plugin binary and its manifest with cosign using a dedicated key, publish attestations to Rekor, and enforce at load time via a loader gatekeeper using an OPA/Kyverno policy. Commands: cosig","explanation":"## Why This Is Asked\nThis probes practical risk mitigation for runtime extensibility, requiring hands-on knowledge of container image/artifact attestation, and runtime policy enforcement.\n\n## Key Concepts\n- Artifact attestation with cosign and Rekor\n- Runtime policy enforcement (OPA/Kyverno)\n- Attestation layout and versioned mapping to plugins\n\n## Code Example\n```javascript\ncosign sign --key cosign.key plugins/processor.so\ncosign sign --key cosign.key manifests/processor.yaml\n```\n\n## Follow-up Questions\n- How would you handle key rotation and revocation?\n- How would you test and audit the policy across diverse environments?","diagram":"flowchart TD\n  A[Plugin Release] --> B[Sign Binary]\n  A --> C[Sign Manifest]\n  B --> D[ Rekor Entry ]\n  C --> D\n  E[Loader Gatekeeper] --> F[Enforce Attestation]\n  D --> F","difficulty":"intermediate","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["NVIDIA","Salesforce","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T07:35:43.547Z","createdAt":"2026-01-17T07:35:43.548Z"},{"id":"q-3298","question":"In a security-focused edge compute platform with 50k devices, function bundles are delivered as WebAssembly modules from a private registry. Propose a concrete plan to sign WASM bundles and their JSON manifests with cosign, publish Rekor attestations, and enforce at load time that only attested bundles are executed. Include exact signing commands, an attestation storage layout, and a sample Kyverno/OPA policy plus integration steps with the edge runtime?","answer":"Sign and attest WASM bundles and their JSON manifests; concrete commands: generate keys; cosign sign --key cosign.key wasm/bundle.wasm; cosign sign --key cosign.key manifests/bundle.json; cosign attes","explanation":"## Why This Is Asked\n\nEdge compute poses unique trust boundaries; standard registry signing isn't enough. This question probes plan for attested code at scale, offline verification, and edge-specific policy enforcement, including integration with Rekor/ Kyverno/ OPA.\n\n## Key Concepts\n\n- WebAssembly module attestation\n- Edge runtime verification\n- Rekor attestations\n- Kyverno/OPA policy integration\n\n## Code Example\n\n```javascript\n// Example pseudo-code for edge loader verification\nfunction verifyEdgeBundle(digest, attestationURL) { /* ... */ }\n```\n\n## Follow-up Questions\n\n- How would you handle revocation of attested modules at scale?\n- What are failure modes if Rekor is temporarily unavailable?","diagram":"flowchart TD\n  A[Edge Registry] --> B[WASM Bundle digest]\n  B --> C[Cosign Sign]\n  C --> D[Rekor Attestations]\n  D --> E[Edge Loader]\n  E --> F[Execute Attested Bundle]","difficulty":"intermediate","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Coinbase","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T10:37:03.147Z","createdAt":"2026-01-17T10:37:03.147Z"},{"id":"q-3423","question":"Scenario: A multi-tenant data platform deploys user-defined data processing plugins as containerized extensions loaded by a central registry. Design a concrete plan to sign each plugin image and its JSON config with cosign, publish attestations to Rekor, and enforce at runtime that only attested plugins are loaded by the plugin host. Include concrete signing commands, attestation storage layout, and a sample Kyverno/OPA policy and integration steps with the plugin loader?","answer":"Adopt a plugin attestation flow: cosign-sign each container image and its config, publish attestations to Rekor, and enforce at runtime that only attested plugins load in the host. Commands: cosign si","explanation":"## Why This Is Asked\n\nThis probes end-to-end supply‑chain security for pluggable components, including signing scope, attestation layout, and runtime enforcement in multi-tenant environments.\n\n## Key Concepts\n\n- Cosign signing of images and artifact configs\n- Rekor transparency log attestations\n- Runtime verification inside a plugin loader\n- Kyverno/OPA policies for attestation enforcement\n- Attestation storage and rotation considerations\n\n## Code Example\n\n```javascript\nasync function verifyAttestation(imageDigest) {\n  // query Rekor for attestation of imageDigest and verify signature\n  // return true/false\n}\n```\n\n## Follow-up Questions\n\n- How would you handle key rotation and revocation for cosign keys in this setup?\n- How would you scale attestation verification in a high-churn plugin ecosystem?","diagram":"flowchart TD\n  A[Registry publishes plugin] --> B[Signer cosign image and config]\n  B --> C[Attestations stored in Rekor]\n  C --> D[Plugin loader enforces policy at load]\n  D --> E[Plugin runs in sandbox]","difficulty":"intermediate","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","IBM","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T15:38:09.824Z","createdAt":"2026-01-17T15:38:09.824Z"},{"id":"q-3474","question":"In a data lake, design a practical plan to sign each ingested dataset with cosign, publish Rekor attestations, and enforce at read time that only attested data is processed by Spark/Presto, including integration with a data catalog and an OPA policy. Include concrete commands and an artifact layout?","answer":"Ingest pipeline signs every dataset blob (Parquet/CSV) with cosign, publishes the attestation to Rekor, and records digest + attestation_id in the data catalog. Read-time enforcement via OPA rules tie","explanation":"## Why This Is Asked\nThis tests end-to-end data provenance, integrity, and policy enforcement in a real data platform.\n\n## Key Concepts\n- Cosign attestations\n- Rekor storage\n- Data catalog integration\n- OPA policy for read-time enforcement\n\n## Code Example\n```bash\n# sign the dataset\ncosign sign -key cosign.key datasets/ingest-202601.parquet\n# verify\ncosign verify datasets/ingest-202601.parquet\n```\n\n## Follow-up Questions\n- How to model attestation metadata in the data catalog and support revocation?\n- What latency and scale considerations arise when enforcing attestations in large Spark workloads?","diagram":null,"difficulty":"intermediate","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T17:41:08.077Z","createdAt":"2026-01-17T17:41:08.077Z"},{"id":"q-3561","question":"Design a lightweight runtime security check in Kubernetes using Falco to alert on containers running as root or spawning a shell; provide two concrete Falco rules (compliant vs noncompliant), a minimal test Pod manifest to trigger the alert, and a verification plan to confirm alerts route to the chosen sink?","answer":"Install Falco on the cluster and write two rules: (1) detect containers where user_id == 0 or runAsUser == 0; (2) detect shell spawning (exe contains /bin/sh or /bin/bash). Provide a tiny compliant Pod manifest that avoids these conditions, then a noncompliant manifest that triggers them, and verify alerts reach your chosen sink (e.g., Falco logs, webhook, or SIEM).","explanation":"## Why This Is Asked\nRuntime security is essential for Kubernetes. This beginner-friendly task tests practical rule-writing, basic detection, and end-to-end validation.\n\n## Key Concepts\n- Falco rule syntax and output formatting\n- Kubernetes security contexts (runAsUser, user)\n- Pod testing and alert sinks (log/monitoring)\n- False positives handling\n\n## Code Example\n```yaml\n# Example Falco rule\n- rule: Detect Root User\n  desc: Container runs as root\n  condition: evt.user.id == 0\n  output: 'root user in container (container=%container.name image=%container.image.repository)'\n  priority: WARNING\n```","diagram":"flowchart TD\n  A[Define threat] --> B[Write Falco rule] \n  B --> C[Deploy Falco] \n  C --> D[Test Pod] \n  D --> E[Validate alerts]","difficulty":"beginner","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","MongoDB","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T05:45:30.961Z","createdAt":"2026-01-17T21:31:11.376Z"},{"id":"q-3674","question":"Scenario: An edge platform runs WebAssembly (WASM) modules pushed from a central registry to thousands of devices. Devs want to ensure only attested modules load. Describe a concrete plan to cosign-sign each WASM module and its metadata, publish attestations to Rekor, and enforce at the loader level that only attested modules execute. Include concrete signing commands, attestation storage layout, and a sample policy (Kyverno/OPA) plus edge-loader integration steps?","answer":"Sign each WASM module and its metadata with cosign (e.g., cosign sign --key cosign.key wasm/module.wasm; cosign sign --key cosign.key wasm/module.json). Generate an attestation with cosign attest and ","explanation":"## Why This Is Asked\nEdge workloads require strong provenance for code that executes locally. This question probes real‑world practice for signing WASM modules, linking attestations to artifacts in Rekor, and enforcing runtime loading only of attested modules.\n\n## Key Concepts\n- WASM/module signing with cosign\n- Rekor attestations and artifact provenance\n- Edge loader enforcement and verification\n- Policy as code (Kyverno/OPA) for runtime guarantees\n- Attestation storage and mapping to artifacts\n\n## Code Example\n```javascript\n// signing and attestation commands\ncosign sign --key cosign.key wasm/module.wasm\ncosign attest --key cosign.key --artifact wasm/module.wasm --predicate '{\"op\":\"load\",\"name\":\"module.wasm\"}' --type cosign/attestation wasm/module.wasm\n```\n\n## Follow-up Questions\n- How would you rotate signing keys and invalidate old attestations without downtime?\n- How would you handle attestation lookups on devices with intermittent connectivity?","diagram":"flowchart TD\n  A[Edge Device] --> B[Loader]\n  B --> C{Attested?}\n  C -- Yes --> D[Load Module]\n  C -- No --> E[Abort]\n","difficulty":"intermediate","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","DoorDash","Hashicorp"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T04:22:38.493Z","createdAt":"2026-01-18T04:22:38.494Z"},{"id":"q-3768","question":"Advanced interview: In a production ML platform used by Tesla, Databricks, and PayPal, ensure every ML model artifact (weights, config) and its data dependencies are signed, attested, and verifiable before deployment to inference endpoints. Design a policy-driven end-to-end pipeline: 1) signing workflow with Cosign, 2) attestation storage with Rekor, 3) pre-deployment checks enforcing Rekor attestations and an SBOM, and 4) a test plan that detects tampering and blocks deployment. Include concrete Cosign commands and a sample Rekor layout?","answer":"Sign all artifacts with Cosign, publish Rekor attestations, and enforce a deployment gate that requires a Rekor attestation plus an SBOM before inference endpoints are updated. Provide concrete Cosign","explanation":"## Why This Is Asked\nTests ability to design end-to-end software supply chain security for ML deployments, including signing, attestation, and gatekeeping at deployment time across multi-cloud teams.\n\n## Key Concepts\n- ML artifact signing with Cosign\n- Rekor attestations and verification\n- SBOMs for ML assets and pipelines\n- Deployment gates via policy engines (OPA-like) and CI integration\n- Tamper testing and revocation handling\n\n## Code Example\n```bash\n# Generate signing keys (one-time)\ncosign generate-key-pair\n\n# Sign artifacts\ncosign sign -key cosign.key models/weights.pt\ncosign sign -key cosign.key data/config.json\n\n# Create attestations\ncosign attest -key cosign.key -predicate attestation.json models/weights.pt\n\n# Verify artifacts\ncosign verify weights.pt\ncosign verify data/config.json\n```\n\n## Follow-up Questions\n- How would you revoke a compromised artifact in Rekor?\n- How would you scale this across multiple regions and compute environments?","diagram":"flowchart TD\n  Artifacts[Artifacts] --> Sign[Sign artifacts with Cosign]\n  Sign --> Rekor[Store Rekor attestations]\n  Rekor --> Gate[Pre-deploy gate checks]\n  Gate --> Deploy[Deploy to inference endpoints]","difficulty":"advanced","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","PayPal","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T08:47:54.422Z","createdAt":"2026-01-18T08:47:54.422Z"},{"id":"q-3949","question":"In a multi-tenant SaaS, Terraform modules are published to a private registry. Propose a practical plan to cosign-sign each module package and its accompanying values.json, publish attestations to Rekor, and enforce at plan/apply that only attested modules are used. Include concrete signing commands, attestation storage layout, and a sample Kyverno/OPA policy plus Terraform integration steps?","answer":"CI should tarball the module and values.json, cosign sign both with a dedicated key, and cosign attest the module for Rekor. Example: tar czf mod.tar.gz module values.json; cosign sign mod.tar.gz --ke","explanation":"## Why This Is Asked\nTests ability to implement end-to-end attestation for IaC artifacts, covering signing, attestation storage, and runtime policy checks in a multi-tenant setting.\n\n## Key Concepts\n- Cosign attestations and Rekor integration\n- Artifact scope: module package + values.json\n- Attestation storage layout in the registry\n- Policy enforcement with Kyverno/OPA\n- CI/CD integration and pre-Plan validation\n\n## Code Example\n```bash\n# signing steps\ntar czf mod.tar.gz module values.json\ncosign sign mod.tar.gz --key cosign.key\ncosign sign values.json --key cosign.key\ncosign attest mod.tar.gz --key cosign.key\n```\n\n## Follow-up Questions\n- How would you rotate keys and manage revocation?\n- How to test policies across environments with drift detection?","diagram":"flowchart TD\n  A[Terraform Module] --> B[CI: Package & Sign]\n  B --> C[Attest & Push to Rekor]\n  C --> D[Policy Engine (Kyverno/OPA) Validate]\n  D --> E[Terraform Plan/Apply]","difficulty":"intermediate","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T16:45:44.882Z","createdAt":"2026-01-18T16:45:44.882Z"},{"id":"q-3961","question":"Scenario: An extensible web server loads dynamic plugins (shared objects) from a private registry at runtime. Design a concrete plan to sign each plugin artifact and its manifest with cosign, publish attestations to Rekor, and enforce at runtime that only attested plugins are loaded. Include concrete signing commands, attestation storage layout, and a sample Kyverno/OPA policy and integration steps with the plugin loader?","answer":"Build a signed attestation workflow: sign the plugin binary with cosign, create a predicate.json describing name/version, run cosign attest to publish to Rekor, and store attestations under /attestati","explanation":"## Why This Is Asked\nDemonstrates end-to-end understanding of signing, attestation, and runtime enforcement for dynamic plugins.\n\n## Key Concepts\n- Artifact signing and attestations\n- Rekor/Cosign integration\n- Runtime policy enforcement (Kyverno/OPA)\n- Loader integration and attestation verification\n\n## Code Example\n```javascript\n// Pseudo plugin loader check\nif (!attestedDigest(digest)) throw new Error('Unattested plugin');\nloadPlugin(digest);\n```\n\n## Follow-up Questions\n- How to scale attestation lookups?\n- How to handle key rotation and revocation?","diagram":"flowchart TD\n  A[Plugin Loader] --> B[Resolve Digest]\n  B --> C[Query Rekor for Attestation]\n  C --> D{Attested?}\n  D -->|Yes| E[Load Plugin]\n  D -->|No| F[Reject Load]","difficulty":"intermediate","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Robinhood","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T17:31:51.009Z","createdAt":"2026-01-18T17:31:51.009Z"},{"id":"q-4062","question":"Scenario: A data platform spanning Snowflake and IBM workloads runs Spark jobs in Kubernetes and must enforce that only pods with SPIRE-backed identities can call the Kubernetes API. Design a runtime access-control solution using OPA Gatekeeper with RBAC, SPIRE, and a mutating webhook to inject auditing sidecars. Provide a concrete constraint template, an example compliant Pod, a noncompliant Pod, and a validation plan?","answer":"Use SPIRE to assign pod identities and bind them to a restricted RBAC role, enforced by an OPA Gatekeeper constraint template that denies API access unless the identity matches an allowlist. Add a mut","explanation":"## Why This Is Asked\nTests understanding of identity-based runtime access control in Kubernetes, integrating SPIRE, OPA Gatekeeper, and admission/mutating webhooks for auditing. It also evaluates plan for cross-cloud (Snowflake/IBM) data workloads and secure Spark job execution.\n\n## Key Concepts\n- SPIRE workload identity and attestation\n- OPA Gatekeeper constraint templates and validation hooks\n- Kubernetes RBAC and API access control\n- Mutating webhook for audit sidecars\n- Validation strategy with compliant vs noncompliant pods\n\n## Code Example\n```yaml\n# ConstraintTemplate (example): OPA-based API access gate\napiVersion: templates.gatekeeper.sh/v1beta1\nkind: ConstraintTemplate\nmetadata:\n  name: k8sapiaiaccess\nspec:\n  crd:\n    spec:\n      names:\n        kind: K8sAPIAccess\n  targets:\n  - target: admission.k8s.gatekeeper.sh\n    rego: |-\n      package k8sapiaiaccess\n      violate[\"API access denied: identity not whitelisted\"] {\n        input.review.kind.kind == \"Pod\"\n        identity := input.review.object.metadata.annotations[\"spire.identity\"]\n        not identity_in_allowlist(identity)\n      }\n      \n      # allowlisted identities (simplified)\n      identity_in_allowlist(id) {\n        id == \"spiffe://cluster.local/ns/default/sa/spark-sa\"\n      }\n```\n\n```yaml\n# Compliant Pod (SPIRE-identity annotated)\napiVersion: v1\nkind: Pod\nmetadata:\n  name: compliant-spark\n  annotations:\n    spire.identity: \"spiffe://cluster.local/ns/default/sa/spark-sa\"\nspec:\n  serviceAccountName: spark-sa\n  containers:\n  - name: spark\n    image: myrepo/spark:latest\n```\n\n```yaml\n# Noncompliant Pod (no SPIRE identity)\napiVersion: v1\nkind: Pod\nmetadata:\n  name: noncompliant-spark\nspec:\n  containers:\n  - name: spark\n    image: myrepo/spark:latest\n```\n\n## Validation Plan\n- Deploy constraint template and constraint; attempt compliant pod deployment; verify admission allows and audit sidecar is injected. \n- Deploy noncompliant pod; confirm admission denies with clear event reason. \n- Run a Spark job and inspect API calls logged by the auditing sidecar for traceability.","diagram":null,"difficulty":"advanced","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T22:27:30.677Z","createdAt":"2026-01-18T22:27:30.679Z"},{"id":"q-4109","question":"In a Kubernetes cluster, implement a Gatekeeper policy that enforces container image provenance: images must come from an allowlisted registry and include a digest; provide YAML for ConstraintTemplate and Constraint, a compliant Pod manifest, a noncompliant Pod manifest, and a validation plan using kubectl?","answer":"Implement a Gatekeeper ConstraintTemplate named K8sImageDigestConstraint to enforce: image must reference an allowlisted registry and include a digest (@sha256). Include the YAML for ConstraintTemplat","explanation":"## Why This Is Asked\nUnderstanding admission control and policy as code.\n\n## Key Concepts\n- Gatekeeper ConstraintTemplate, Constraint\n- Rego-based policy, allowlist, image digest\n- Pod spec image references and admission outcomes\n\n## Code Example\n```rego\npackage k8simagedigest\nviolation[{\"msg\": msg}] {\n  container := input.review.object.spec.containers[_]\n  image := container.image\n  digest := index_of(image, \"@sha256:\")\n  not digest\n  msg := \"image must include a sha256 digest\"\n}\nviolation[{\"msg\": msg}] {\n  image := input.review.object.spec.containers[_].image\n  reg := strings.Split(image, \"/\")[0]\n  allowed := input.parameters.allowed_registries\n  reg notin allowed\n  msg := \"registry not allowlisted\"\n}\n```\n\n```yaml\napiVersion: templates.gatekeeper.sh/v1beta1\nkind: ConstraintTemplate\nmetadata:\n  name: k8simagedigest\nspec:\n  crd:\n    spec:\n      names:\n        kind: K8sImageDigestConstraint\n  targets:\n  - target: OPA\n    rego: |\n      package k8simagedigest\n      violation[{\"msg\": msg}] {\n        container := input.review.object.spec.containers[_]\n        image := container.image\n        digest := index_of(image, \"@sha256:\")\n        not digest\n        msg := \"image must include a sha256 digest\"\n      }\n      violation[{\"msg\": msg}] {\n        container := input.review.object.spec.containers[_]\n        image := container.image\n        reg := strings.Split(image, \"/\")[0]\n        allowed := input.parameters.allowed_registries\n        reg notin allowed\n        msg := \"registry not allowlisted\"\n      }\n```\n\n```yaml\napiVersion: constraints.gatekeeper.sh/v1beta1\nkind: K8sImageDigestConstraint\nmetadata:\n  name: require-image-digest-allowlist\nspec:\n  allowed_registries: [\"registry.company.local\", \"registry.dev.local\"]\n```\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: compliant-pod\nspec:\n  containers:\n  - name: app\n    image: registry.company.local/app@sha256:abcdef...\n```\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: noncompliant-pod\nspec:\n  containers:\n  - name: app\n    image: registry.unauthorized.local/app:latest\n```\n\n## Follow-up Questions\n- How would you test additional image attributes (size, architecture) in Gatekeeper? \n- How would you roll back a failing policy without downtime?","diagram":"flowchart TD\n  A[Pod Created] --> B[Admission Check: Gatekeeper]\n  B --> C{Compliance?}\n  C -->|Yes| D[Pod Starts]\n  C -->|No| E[Admission Denied]","difficulty":"beginner","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T02:42:40.852Z","createdAt":"2026-01-19T02:42:40.852Z"},{"id":"q-4165","question":"In a Kubernetes-based fintech platform, dozens of webhook-handler microservices are deployed from a private registry. Propose a concrete plan to sign both container images and their Kubernetes manifests with cosign, publish attestations to Rekor, and enforce at admission and runtime that only attested artifacts are deployed and loaded. Include concrete signing commands, attestation storage layout, and a sample Kyverno/OPA policy plus CI steps for the registry and admission controls?","answer":"Sign images with cosign in CI using a dedicated KMS key and publish Rekor attestations. Sign manifests with cosign and attach attestations to the YAML; push artifacts to a private registry. Enforce at","explanation":"## Why This Is Asked\n\nAssesses ability to design end-to-end attestation for both images and manifests in a Kubernetes-based, multi-tenant fintech environment, including how to store attestations and enforce policies at admission.\n\n## Key Concepts\n\n- cosign signing of images and manifests\n- Rekor as a transparency log\n- Kubernetes admission control with Kyverno/OPA\n- CI/CD integration for private registries\n\n## Code Example\n\n```javascript\n// Example command sketch (not executable in isolation)\nconst cmds = [\n  \"cosign sign --key kms://mykms/image-key registry.example.com/webhook:latest\",\n  \"cosign sign --key kms://mykms/manifest-key deployments/*.yaml\"\n];\n```\n\n## Follow-up Questions\n\n- How would you handle key rotation and revocation across teams?\n- How would you verify attestations during incident response?","diagram":null,"difficulty":"intermediate","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Plaid","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T05:53:09.079Z","createdAt":"2026-01-19T05:53:09.079Z"},{"id":"q-4281","question":"Scenario: A multi-tenant SaaS platform hosts user-contributed ML model plugins that load at runtime in isolation. Design a concrete plan to sign each model artifact (e.g., .onnx, .pt) and its metadata with cosign, publish attestations to Rekor, and enforce at load time that only attested models are accepted by the model loader. Include concrete signing commands, attestation storage layout, and a sample OPA policy and loader integration steps?","answer":"Sign each model artifact (.onnx, .pt) and its metadata with cosign using a KMS-backed key; publish attestations to Rekor; enforce at load time that only attested models load through the loader by quer","explanation":"## Why This Is Asked\n\nTests a practical, end-to-end attestation workflow for runtime ML plugins, not just static artifacts.\n\n## Key Concepts\n\n- Cosign attestations and Rekor transparency\n- Runtime model loader enforcement\n- OPA policy integration and loader hooks\n\n## Code Example\n\n```javascript\n// signing a model artifact\ncosign sign -key cosign.key models/model.onnx -a type=model\n```\n\n```javascript\n// create an attestation for the model\ncosign attest --artifact models/model.onnx --name model-baseline --key cosign.key\n```\n\n## Follow-up Questions\n\n- How would you revoke a compromised model and propagate revocation?\n- How to scale attestations for thousands of models across tenants?\n- How do you test loader failures without disrupting tenants?","diagram":"flowchart TD\n  A[Submit model] --> B[Sign artifact with cosign]\n  B --> C[Publish Rekor attestation]\n  C --> D[Loader verifies attestation]\n  D --> E[Model loaded in sandbox]","difficulty":"intermediate","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Lyft","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T11:31:25.139Z","createdAt":"2026-01-19T11:31:25.140Z"},{"id":"q-4368","question":"In a multi-tenant Kubernetes cluster, design a defence-in-depth strategy to prevent API abuse and runtime policy violations that bypass admission controls. Outline how you would combine: 1) OPA constraints enforcing image provenance via Rekor cosign attestations; 2) Kyverno mutation/validation to enforce non-root, readOnlyRootFilesystem, and to block unsigned images; 3) eBPF/Falco runtime checks for anomalous API usage; 4) a verification plan with a minimal test manifest. Provide concrete policy fragments and test steps?","answer":"Implement an admission chain: OPA constraints require Rekor-attested cosign signatures for images; Kyverno mutation/validation enforces runAsNonRoot and readOnlyRootFilesystem and rejects unsigned ima","explanation":"## Why This Is Asked\n\nTests integration of policy-as-code, attestation, and runtime security across a multi-tenant cluster. Candidates must show how to bind OPA, Kyverno, Rekor/cosign, and Falco to a coherent workflow, plus a concrete verification plan.\n\n## Key Concepts\n\n- Image provenance and attestations (cosign, Rekor)\n- Policy-as-code (OPA, Kyverno)\n- Runtime security (Falco/eBPF)\n- Multi-tenant governance and testability\n\n## Code Example\n\n```javascript\n// OPA Rego (conceptual)\npackage kubernetes.admission\nviolation[reason] {\n  input.review.object.kind == \"Pod\"\n  image := input.review.object.spec.containers[_].image\n  not imageIsAttested(image)\n  reason = \"image provenance missing\"\n}\n```\n\n```javascript\n// Kyverno policy (conceptual)\napiVersion: kyverno.io/v1\nkind: ClusterPolicy\nmetadata:\n  name: require-provenance-and-security\nspec:\n  validationFailureAction: enforce\n  rules:\n  - name: require-nonroot\n    match:\n      resources:\n        kinds: [\"Pod\"]\n    validate:\n      message: \"must set runAsNonRoot and readOnlyRootFilesystem\"\n      pattern:\n        spec:\n          containers:\n          - securityContext:\n              runAsNonRoot: true\n  - name: require-attested-image\n    match:\n      resources:\n        kinds: [\"Pod\"]\n    validate:\n      message: \"image must have Rekor cosign attestation\"\n      pattern:\n        spec:\n          containers:\n          - image: \"*attested*\"\n```\n\n```javascript\n// Falco rule (conceptual)\n- rule: Unsigned Image Usage\n  condition: container.image.signature == \"\"\n  output: \"Unsigned image detected: %container.image.repository\"\n  priority: WARN\n```\n\n## Follow-up Questions\n\n- How would you handle attestation key rotation?\n- How would you measure false positives and performance impact?","diagram":"flowchart TD\n  A[CI/Developer] --> B[Attestation (cosign/Rekor)]\n  B --> C[OPA constraints]\n  C --> D[Kyverno validation/mutation]\n  D --> E[Runtime checks (Falco/eBPF)]\n  E --> F[Alerts & Remediation]","difficulty":"advanced","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Hashicorp","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T15:58:48.404Z","createdAt":"2026-01-19T15:58:48.404Z"},{"id":"q-4397","question":"Scenario: A shared Kubernetes cluster has test and prod namespaces. Draft a minimal, beginner-friendly NetworkPolicy in the test namespace that isolates it from prod by default, allowing egress only to a logging endpoint at logs.internal:443 and to DNS, while denying all other egress. Provide the YAML, example compliant/noncompliant Pod specs, and verification steps?","answer":"Define a test-ns NetworkPolicy that denies egress by default, then allow egress to 10.1.2.3:443 (logging) and to DNS (port 53 UDP/TCP). Validate by deploying a pod in test, attempting to reach a prod ","explanation":"## Why This Is Asked\nTests understanding of namespace isolation and basic egress filtering using NetworkPolicy. It checks hands-on ability to craft minimal, enforceable rules and verify them.\n\n## Key Concepts\n- Kubernetes NetworkPolicy basics\n- Namespace-scoped isolation\n- Egress rules and default deny\n- Basic verification via kubectl and logs\n\n## Code Example\n```yaml\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: test-egress-isolate\n  namespace: test\nspec:\n  podSelector: {}\n  policyTypes:\n  - Egress\n  egress:\n  - to:\n    - ipBlock:\n        cidr: 10.1.2.3/32\n    ports:\n    - protocol: TCP\n      port: 443\n  - to:\n    - namespaceSelector: {}\n      podSelector:\n        matchLabels:\n          app: dns\n    ports:\n    - protocol: UDP\n      port: 53\n  - to:\n    - ipBlock:\n        cidr: 0.0.0.0/0\n        except:\n        - 255.255.255.255/32\n    ports:\n    - protocol: TCP\n      port: 53\n```\n\n```yaml\n# Compliant Pod in test namespace\napiVersion: v1\nkind: Pod\nmetadata:\n  name: compliant-pod\n  namespace: test\nspec:\n  containers:\n  - name: app\n    image: busybox\n    command: [\"sh\", \"-c\", \"sleep 300\"]\n```\n\n```yaml\n# Noncompliant Pod in test namespace (tries to access prod)\napiVersion: v1\nkind: Pod\nmetadata:\n  name: noncompliant-pod\n  namespace: test\nspec:\n  containers:\n  - name: app\n    image: busybox\n    command: [\"sh\", \"-c\", \"wget http://prod.svc.cluster.local || true\"]\n```\n\n```yaml\n# DNS test Pod (for compliance with DNS allow rule)\napiVersion: v1\nkind: Pod\nmetadata:\n  name: dns-test\n  namespace: test\nspec:\n  containers:\n  - name: app\n    image: busybox\n    command: [\"sh\", \"-c\", \"nslookup kubernetes.default.svc.cluster.local\"]\n```\n\n## Follow-up Steps\n- kubectl apply -f policy.yaml\n- kubectl apply -f compliant-pod.yaml; kubectl exec compliant-pod -- curl logs.internal:443\n- kubectl apply -f noncompliant-pod.yaml; observe failure to reach prod\n- Check events: kubectl describe networkpolicy/test-egress-isolate -n test\n","diagram":null,"difficulty":"beginner","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Oracle","Robinhood","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T17:30:57.730Z","createdAt":"2026-01-19T17:30:57.732Z"},{"id":"q-4449","question":"In a multi-tenant Kubernetes cluster with sensitive data, design an end-to-end defense that enables rapid containment and IR. Include (a) Cilium network policies for namespace isolation with restricted egress to trusted endpoints, (b) admission controls via OPA Gatekeeper and Kyverno for image provenance and security contexts, (c) Falco runtime rules for API abuse and container escapes, and (d) an incident response workflow with SIEM and Rekor/Cosign attestations. What would you implement first and why?","answer":"Start with admission controls to enforce image provenance and security contexts (Cosign attestations, digest pinning). Next, implement Cilium namespace isolation with strict egress to trusted endpoint","explanation":"## Why This Is Asked\n\nProbes end-to-end security design across admission, runtime, and IR in a multi-tenant cluster.\n\n## Key Concepts\n\n- Admission control with OPA Gatekeeper and Kyverno\n- Network security with Cilium and eBPF\n- Runtime detection with Falco\n- IR workflow with SIEM and Rekor/Cosign attestations\n\n## Code Example\n\n```yaml\n- rule: K8s API abuse\n  condition: evt.type = \"exec\" and user.name != \"system:serviceaccount:default:default\"\n  output: \"K8s API exec by non-service account %user.name\"\n  priority: ERROR\n  tags: [k8s, api, abuse]\n```\n\n## Follow-up Questions\n\n- How would you test RC and rollback?\n- How would you handle tenant onboarding exceptions?","diagram":null,"difficulty":"advanced","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Coinbase","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T19:31:28.402Z","createdAt":"2026-01-19T19:31:28.402Z"},{"id":"q-4591","question":"In a multi-tenant Kubernetes cluster used by IBM, Tesla, and Cloudflare, implement an admission-control policy to prevent pod service accounts from invoking the Kubernetes API outside a restricted set of verbs in all namespaces, except for a dedicated monitoring namespace and a set of approved SPIFFE IDs. Provide the OPA Gatekeeper ConstraintTemplate, a corresponding Constraint, a compliant API call example, a noncompliant one, and a validation plan?","answer":"Use an OPA Gatekeeper ConstraintTemplate to deny API verbs beyond get/list/watch for pods in all namespaces except monitoring, conditioned on SPIFFE ID in an allowedSVIDs set. Example: compliant: GET /api/v1/namespaces/default/pods, noncompliant: POST /api/v1/namespaces/default/pods.","explanation":"## Why This Is Asked\nMulti-tenant clusters need strict API access control; SPIFFE identity ensures only trusted services can perform sensitive calls. Gatekeeper + OPA enforces runtime policy with centralized visibility.\n\n## Key Concepts\n- OPA Gatekeeper\n- ConstraintTemplate and Constraint\n- SPIFFE IDs and SVIDs\n- Kubernetes API verbs and RBAC implications\n- Validation via audit logs and Gatekeeper status\n\n## Code Example\n```yaml\n# ConstraintTemplate\napiVersion: templates.gatekeeper.sh/v1beta1\nkind: ConstraintTemplate\nmetadata:\n  name: k8sapicnst\nspec:\n  crd:\n    spec:\n      names:\n        kind: K8sApiConstraint\n  targets:\n    - target: admission.k8s.gatekeeper.sh/v1beta1\n      rego: |\n        package k8sapi\n        violation[{\"msg\": msg}] {\n          input.review.kind.kind == \"Pod\"\n          input.review.object.metadata.namespace != \"monitoring\"\n          not allowed_spiffe(input.review.object)\n          msg := sprintf(\"Pod %s in namespace %s cannot use API verb %s\", [input.review.object.metadata.name, input.review.object.metadata.namespace, input.review.operation])\n        }\n        allowed_spiffe(pod) {\n          pod.metadata.annotations[\"spiffe.io/spiffe-id\"]\n          pod.metadata.annotations[\"spiffe.io/spiffe-id\"] in data.constraint.spec.allowedSVIDs\n        }\n```\n\n```yaml\n# Constraint\napiVersion: constraints.gatekeeper.sh/v1beta1\nkind: K8sApiConstraint\nmetadata:\n  name: restrict-api-verbs\nspec:\n  enforcementAction: deny\n  allowedSVIDs:\n    - \"spiffe://ibm.com/workload/web\"\n    - \"spiffe://tesla.com/workload/monitor\"\n    - \"spiffe://cloudflare.com/workload/edge\"\n```\n\n## Validation Plan\n1. Deploy ConstraintTemplate and Constraint\n2. Test compliant GET request from approved SPIFFE ID\n3. Test noncompliant POST request from unapproved SPIFFE ID\n4. Verify denial in Gatekeeper audit logs\n5. Confirm monitoring namespace bypass works correctly","diagram":null,"difficulty":"advanced","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","IBM","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T04:36:52.317Z","createdAt":"2026-01-20T02:48:42.292Z"},{"id":"q-4680","question":"In a multi-tenant data science platform used by Apple and Goldman Sachs, each user runs notebooks against a shared data lake. Design a concrete plan to prevent cross-tenant data access by implementing per-tenant data tag ACLs at the storage layer, notebook kernel isolation, and policy‑driven data lineage validation with OPA/Kyverno. Include a sample policy, a validation workflow, and a minimal end-to-end example?","answer":"Propose a concrete isolation strategy: 1) tag all data lake objects with tenant_id and enforce per-tenant ACLs at the storage layer; 2) run notebooks in tenant-scoped Kubernetes namespaces with kernel","explanation":"## Why This Is Asked\n\nTests real-world isolation for multi-tenant data science platforms, focusing on data tagging, kernel isolation, and policy-as-code enforcement across storage and compute layers.\n\n## Key Concepts\n\n- Tenant isolation\n- Data tagging and ACLs\n- Namespace-scoped compute\n- OPA/Kyverno policy-as-code\n- Data lineage validation\n\n## Code Example\n\n```rego\npackage data.access\ndefault allow = false\n\nallow {\n  input.tenant_id == input.user.tenant_id\n  input.object.metadata.labels[\\\"tenant_id\\\"] == input.tenant_id\n}\n```\n\n## Follow-up Questions\n\n- How would you test isolation without affecting prod?\n- How would you handle cross-tenant analytics requests?","diagram":null,"difficulty":"intermediate","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Goldman Sachs"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T07:49:09.011Z","createdAt":"2026-01-20T07:49:09.011Z"},{"id":"q-4759","question":"In a centralized data processing platform used by IBM, Databricks, and Google, design a policy that guarantees Spark batch jobs can only read datasets tagged with the tenant’s label and can only write to a per-tenant sandbox path, regardless of cluster or namespace. Provide a concrete OPA Gatekeeper ConstraintTemplate and Constraint to enforce dataset-level RBAC for Spark jobs, a compliant API submission example, a noncompliant example, and a verification plan with test scenarios?","answer":"Require each SparkJob CR to include tenant_id. The policy checks that the target dataset has a label tenant_id equal to the job’s tenant_id and that the output path starts with /sandbox/tenant_id/. It","explanation":"## Why This Is Asked\nTests policy design for tenant isolation in data processing.\n\n## Key Concepts\n- OPA Gatekeeper ConstraintTemplate and Constraint\n- SparkJob CR and dataset tagging\n- Tenant isolation via labels and path prefixes\n- Testing: unit/integration using sample SparkJob CR and mock datasets\n\n## Code Example\n```yaml\n# ConstraintTemplate skeleton (policy logic omitted)\napiVersion: templates.gatekeeper.sh/v1beta1\nkind: ConstraintTemplate\nmetadata:\n  name: tenantdatapolicy\nspec:\n  crd:\n    spec:\n      names:\n        kind: TenantDataPolicy\n        plural: tenantdatapolicies\n  targets:\n  - target: admissionreview\n    rego: |\n      # policy logic here\n```\n\n```yaml\n# Constraint example\napiVersion: constraints.gatekeeper.sh/v1beta1\nkind: TenantDataPolicy\nmetadata:\n  name: sparkjob-tenant-policy\nspec: {}\n```\n\n```yaml\n# Compliant SparkJob CR\napiVersion: dataops.example.org/v1\nkind: SparkJob\nmetadata:\n  name: sales-aggregation\nspec:\n  tenant_id: tenantA\n  datasetRef: datasets/tenantA/sales\n  outputPath: /sandbox/tenantA/jobs/sales-aggregation/\n  mainClass: com.company.SalesJob\n```\n\n```yaml\n# Noncompliant SparkJob CR (tenant mismatch)\napiVersion: dataops.example.org/v1\nkind: SparkJob\nmetadata:\n  name: inventory-agg\nspec:\n  tenant_id: tenantB\n  datasetRef: datasets/tenantA/inventory\n  outputPath: /sandbox/tenantB/jobs/inventory-agg/\n```\n\n## Verification Plan\n- Deploy ConstraintTemplate and Constraint; submit compliant and noncompliant SparkJob CRs; validate denial/report for noncompliant.\n- Run end-to-end test: SparkJob submission with mismatched tenant should not reach executor.","diagram":"flowchart TD\nA[Submit SparkJob CR] --> B{Tenant_id present}\nB -- tenant_id ok --> C{Dataset tag matches tenant}\nC -- yes --> D{Output path under /sandbox/tenant_id}\nD -- yes --> E[Allow Submission]\nD -- no --> F[Deny Submission]\nC -- no --> F\nB -- no --> F","difficulty":"intermediate","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Google","IBM"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T11:03:49.478Z","createdAt":"2026-01-20T11:03:49.479Z"},{"id":"q-4823","question":"In a shared ML model registry used by multiple teams at Meta and Nvidia, design a complete plan to enforce that every model artifact (weights, config, tokenizer) is attested with provenance including dataset version, training code hash, and environment, using Sigstore cosign, Rekor, and a Kyverno/OPA policy to gate pushes and deployments to inference clusters. Provide signing commands, attestation storage layout, a policy snippet, and an end-to-end validation plan with a compliant and a noncompliant push?","answer":"All artifacts must be cosign-signed with provenance annotations such as dataset version, training code hash, and environment image. Sign artifacts, publish Rekor attestations, and gate both registry p","explanation":"## Why This Is Asked\nThis question tests end-to-end think-through for ML artifact provenance in a high-stakes, multi-team setting.\n\n## Key Concepts\n- Model provenance and reproducibility\n- Sigstore cosign and Rekor attestations\n- Policy enforcement with Kyverno/OPA\n- CI/CD integration and registry gating\n\n## Code Example\n```\nbash\ncosign sign -a dataset=imagenet -a codehash=abc123 -a env=python-3.8 artifact.bin\n```\n```\n```\n\n## Follow-up Questions\n- How would you test policy under concurrent pushes?\n- How would you handle revocation of compromised attestations?\n","diagram":"flowchart TD\n  Registry[Model Registry] --> Sign[Cosign Sign]\n  Sign --> Rekor[Attestation in Rekor]\n  Rekor --> Gate[Policy Gate (Kyverno/OPA)]\n  Gate --> Deploy[Inference Deployment]","difficulty":"intermediate","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T15:03:57.585Z","createdAt":"2026-01-20T15:03:57.585Z"},{"id":"q-4974","question":"In a real-time chat service deployed on a multi-tenant Kubernetes cluster, implement a policy to block deployment of any container image whose SBOM reports high-severity vulnerabilities or missing attestation, ensuring only signed images with Rekor attestations are allowed in production namespaces, while a separate monitoring namespace remains unrestricted. Provide the OPA Gatekeeper ConstraintTemplate, a corresponding Constraint, a compliant API call example, a noncompliant one, and a validation plan?","answer":"I would implement a comprehensive security gate using CI/CD integration with SBOM generation, cryptographic signing, and OPA Gatekeeper admission control. The workflow includes: generating CycloneDX SBOMs for each container image, signing images with cosign, publishing Rekor attestations, and enforcing policies through a Gatekeeper ConstraintTemplate that validates both attestation presence and vulnerability severity levels before allowing deployment to production namespaces.","explanation":"## Why This Is Asked\nTests the ability to combine SBOM provenance, cryptographic attestation, and admission control in multi-tenant Kubernetes environments with concrete artifacts and workflow.\n\n## Key Concepts\n- SBOM provenance (CycloneDX)\n- Cosign signatures and Rekor attestations\n- Gatekeeper ConstraintTemplate and Constraint\n- Rego policy validation for image attestation and CVE severity\n\n## Code Example\n```yaml\napiVersion: templates.gatekeeper.sh/v1beta1\nkind: ConstraintTemplate\nmetadata:\n  name: ImageSBOMAttestation\nspec:\n  crd:\n    spec:\n      names:\n        kind: ImageSBOMAttestation\n  targets:\n  - target: admission.k8s.gatekeeper.sh\n    rego: |\n      package imagesbomattestation\n      \n      violation[{\"msg\": msg}] {\n        input.review.object.metadata.namespace != \"monitoring\"\n        container := input.review.object.spec.containers[_]\n        not valid_image(container.image)\n        msg := sprintf(\"Container image %v fails SBOM/attestation validation\", [container.image])\n      }\n      \n      valid_image(image) {\n        has_attestation(image)\n        no_high_severity_vulns(image)\n      }\n```","diagram":null,"difficulty":"intermediate","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Twitter","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-21T03:52:24.353Z","createdAt":"2026-01-20T21:58:19.655Z"},{"id":"q-5109","question":"Scenario: Two namespaces in a Kubernetes cluster host separate tenants. Create a simple NetworkPolicy that blocks all cross-namespace traffic while allowing intra-namespace DNS and HTTP; provide the manifest, a compliant Pod spec that only talks within tenant-a, a noncompliant Pod spec attempting cross-namespace access, and a verification plan using kubectl and curl?","answer":"Design a namespace-scoped NetworkPolicy in tenant-a that allows ingress/egress only from tenant-a and denies cross-namespace traffic by default. Include policy.yaml, a compliant Pod spec that curls te","explanation":"## Why This Is Asked\nTests practical understanding of Kubernetes NetworkPolicy default-deny behavior and namespace isolation.\n\n## Key Concepts\n- NetworkPolicy basics and default-deny\n- Namespace isolation and namespace selectors\n- Ingress vs Egress rules\n\n## Code Example\n```yaml\n# policy.yaml\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: deny-cross-namespace\n  namespace: tenant-a\nspec:\n  podSelector: {}\n  policyTypes:\n  - Ingress\n  - Egress\n  ingress:\n  - from:\n    - namespaceSelector:\n        matchLabels:\n          name: tenant-a\n  egress:\n  - to:\n    - namespaceSelector:\n        matchLabels:\n          name: tenant-a\n```\n\n```yaml\n# compliant-pod.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: p-a-compliant\n  namespace: tenant-a\nspec:\n  containers:\n  - name: curl\n    image: curlimages/curl:8.88.0\n    command: [\"sh\",\"-c\",\"curl -s http://tenant-a-service.tenant-a.svc.cluster.local:80/health || true\"]\n```\n\n```yaml\n# noncompliant-pod.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: p-a-cross\n  namespace: tenant-a\nspec:\n  containers:\n  - name: curl\n    image: curlimages/curl:8.88.0\n    command: [\"sh\",\"-c\",\"curl -s http://tenant-b-service.tenant-b.svc.cluster.local:80/health || true\"]\n```\n\n```bash\n# verification plan\nkubectl apply -f policy.yaml\nkubectl apply -f compliant-pod.yaml\nkubectl apply -f noncompliant-pod.yaml\n# test within tenant-a\nkubectl logs p-a-compliant -n tenant-a\n# cross-namespace should fail\nkubectl logs p-a-cross -n tenant-a\n```\n\n## Follow-up Questions\n- How would you adapt this for namespace-label-based multi-tenant isolation?\n- What changes if using Calico instead of default k8s NetworkPolicy support?","diagram":null,"difficulty":"beginner","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Anthropic","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T06:54:42.497Z","createdAt":"2026-01-21T06:54:42.497Z"},{"id":"q-5137","question":"In a Kubernetes multi-tenant cluster, design an admission-time policy to enforce environment-specific default NetworkPolicy: test namespaces must deny all egress except to registry.internal:443 and telemetry.internal:443; prod namespaces deny all egress except to internal-services:443; provide an OPA Gatekeeper ConstraintTemplate and Constraint, plus a compliant and a noncompliant Namespace manifest, and a concrete plan to validate across the cluster?","answer":"I would implement an OPA Gatekeeper ConstraintTemplate DefaultNetworkPolicyByEnv that requires each Namespace with env=prod or env=test to have a matching default-egress NetworkPolicy. For test envs, ","explanation":"## Why This Is Asked\n\nAssesses ability to design admission-time safeguards tied to runtime networking, scalable across tenants.\n\n## Key Concepts\n\n- Gatekeeper ConstraintTemplate/Constraint\n- Environment-scoped NetworkPolicy enforcement\n- Namespace labeling (env=prod|test)\n- End-to-end validation with egress tests\n\n## Code Example\n\n```rego\npackage gates.default_network_policy\n\nviolation[{\"msg\":\"NetworkPolicy missing or env misconfigured\"}] {\n  not input.review.object.metadata.name\n}\n```\n\n```yaml\napiVersion: templates.gatekeeper.sh/v1beta1\nkind: ConstraintTemplate\nmetadata:\n  name: DefaultNetworkPolicyByEnv\nspec: # simplified\n  crd: {}\n  targets:\n  - target: admission.k8s.gatekeeper.sh\n    rego: |\n      package gates.default_network_policy\n      # Real policy would check Namespace env label and NP presence\n```\n\n## Follow-up Questions\n\n- How would you handle exceptions for critical namespaces?\n- How would you test for policy drift and alert on violations?\n","diagram":"flowchart TD\n  A[Namespace created with env label] --> B[Gatekeeper evaluates policy]\n  B --> C{Policy satisfied}\n  C --> D[Namespace admitted]\n  C --> E[Admission denied - noncompliant]","difficulty":"intermediate","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","Microsoft","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T07:50:21.454Z","createdAt":"2026-01-21T07:50:21.454Z"},{"id":"q-5162","question":"Scenario: A GPU-accelerated ML inference cluster uses Nvidia GPUs and runtime-loaded kernel modules from a private registry. Propose a concrete plan to sign each kernel module and its manifest with cosign, publish Rekor attestations, and enforce at module-load time that only attested modules are loaded. Include concrete signing commands, attestation storage layout, and a sample OPA/Kyverno policy plus integration steps with the loader?","answer":"Sign each kernel module (.ko) and its manifest with cosign using a dedicated key, publish the Rekor attestation, and store artifacts in a private registry. The kernel loader queries Rekor/attestation ","explanation":"## Why This Is Asked\nGPU ML clusters rely on kernel modules from a private registry; enforcing integrity at load time is critical to prevent tampering.\n\n## Key Concepts\n- cosign signing and Rekor attestations\n- load-time attestation verification\n- policy enforcement via OPA/Kyverno\n\n## Code Example\n```bash\ncosign sign -key cosign.key path/to/module.ko\ncosign sign -key cosign.key path/to/module.json\ncosign verify path/to/module.ko\n```\n\n```bash\n# OPA snippet\npackage kernel.authz\n\nviolation[{\"msg\": msg}] {\n  input.attestation.subject == \"module.ko\"\n  not input.attestation.issuer in [\"approved-issuer-A\",\"approved-issuer-B\"]\n  msg := \"unapproved module attestation\"\n}\n```\n\n```yaml\n# Kyverno policy (simplified)\napiVersion: kyverno.io/v1\nkind: ClusterPolicy\nmetadata: { name: attested-modules }\nspec:\n  rules:\n  - id: require-attested-modules\n    match: { any: [{ context: { name: \"kernel-loader\" } }] }\n    validate:\n      message: \"Module must be attested\"\n      pattern:\n        any: true\n```\n\n## Follow-up Questions\n- How would you handle key rotation and revocation?\n- How would you test this policy at scale without affecting running workloads?","diagram":null,"difficulty":"intermediate","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","NVIDIA","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T08:54:08.224Z","createdAt":"2026-01-21T08:54:08.224Z"},{"id":"q-5336","question":"In a multi-tenant Kubernetes control plane spanning on-prem and cloud, design a concrete strategy to secure the API surface against compromised clients. Use SPIFFE/SPIRE identities, mutual TLS, and identity-aware authorization. Include a concrete sample OPA (rego) policy snippet, a minimal test manifest for an authorized and an unauthorized client, and a verification plan that demonstrates end-to-end policy enforcement across clusters?","answer":"Adopt SPIFFE IDs for all API clients, enforce mutual TLS via SPIRE, and enforce identity-aware authorization with OPA. Provide a Rego policy that allows access only when the subject matches a SPIFFE I","explanation":"## Why This Is Asked\n\nTests practical ability to implement cross-cluster identity-based API security, not just resource-level controls.\n\n## Key Concepts\n\n- SPIFFE/SPIRE identities\n- mTLS for API surface\n- OPA/rego identity-based access\n- Cross-cluster policy enforcement\n- Test manifests and verification plan\n\n## Code Example\n\n```javascript\n// Pseudo-rego policy example for demonstration\nconst policy = `package kubesecr.identity\ndefault allow = false\nallow {\n  input.subject == 'spiffe://cluster/example/ns/production/sa/api-server'\n  input.path == '/api/v1'\n  input.method == 'GET'\n}`\nexport default policy;\n```\n\n## Follow-up Questions\n\n- How would you test rotation of SPIs and revocation across clusters?\n- How would you audit and store identity-based access decisions for incident response?","diagram":null,"difficulty":"advanced","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Lyft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T17:31:39.212Z","createdAt":"2026-01-21T17:31:39.212Z"},{"id":"q-5343","question":"In a hosted ML inference platform used by Tesla and Hugging Face, a multi-tenant service loads user-submitted TorchScript models from a private registry at runtime. Design a concrete plan to sign each model artifact and its dependencies with cosign, publish attestations to Rekor, and enforce that only attested models are loaded by the model server. Include concrete signing commands, attestation storage layout, and a sample Kyverno/OPA policy plus an end-to-end test plan?","answer":"Use a digest-based flow: cosign sign model.pt; cosign sign requirements.txt; publish Rekor attestations; enforce at load time that only artifacts with a valid attestation digest are loaded. Store atte","explanation":"## Why This Is Asked\nAssesses practical experience with model provenance, attestation, and policy enforcement in production ML platforms.\n\n## Key Concepts\n- Attestation with cosign and Rekor\n- Model digest provenance and runtime enforcement\n- Kyverno vs OPA gate for admission control\n- End-to-end test strategies for signed vs unsigned models\n\n## Code Example\n```bash\n# Sign the model artifact and dependencies\ncosign sign -key cosign.key model.pt\ncosign sign -key cosign.key requirements.txt\n\n# Publish attestations to Rekor (example flow; adjust for registry)\ncosign attest -key cosign.key model.pt\ncosign attest -key cosign.key requirements.txt\n\n# Enforcement policy (example) would live in Kyverno/OPA\n```\n\n## Follow-up Questions\n- How would you handle rotating cosign keys and revoking attestations?\n- How would you audit failed loads and differentiate between missing vs invalid attestations?","diagram":"flowchart TD\n  A[Client submits model] --> B[Model registry]\n  B --> C[Load policy checks attestations]\n  C --> D{Attested?}\n  D -->|Yes| E[Load model]\n  D -->|No| F[Reject]","difficulty":"intermediate","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T18:58:39.092Z","createdAt":"2026-01-21T18:58:39.092Z"},{"id":"q-5455","question":"Using Open Policy Agent as an admission controller in Kubernetes, craft a minimal Rego policy that denies Pods with hostPath volumes, containers running as privileged, or with NET_ADMIN capability. Include the policy, a compliant Pod manifest, a noncompliant Pod manifest, and a short validation plan with commands to test the policy locally?","answer":"A minimal Rego policy denies any Pod where any volume is hostPath, or any container's securityContext.privileged is true, or any container.securityContext.capabilities.add contains NET_ADMIN. A compliant Pod manifest avoids these configurations, while a noncompliant Pod manifest includes at least one of them. The validation plan uses OPA's test framework and kubectl commands to verify policy enforcement locally.","explanation":"## Why This Is Asked\nAssess practical use of OPA in Kubernetes, translating security expectations into verifiable policies; tests demonstrate understanding of Pod spec structure.\n\n## Key Concepts\n- Rego fundamentals\n- PodSpec fields: volumes.hostPath, securityContext.privileged, capabilities\n- OPA test framework and unit tests\n- Admission testing approaches\n\n## Code Example\n```javascript\npackage kubernetes.admission\n\ndeny[msg] {\n  input.request.kind.kind == \"Pod\"\n  some vol in input.request.object.spec.volumes\n  vol.hostPath\n  msg := \"hostPath volumes are forbidden\"\n}\ndeny[msg] {\n  some c in input.request.object.spec.containers\n  c.securityContext.privileged == true\n  msg := \"privileged containers are forbidden\"\n}\ndeny[msg] {\n  some c in input.request.object.spec.containers\n  some cap in c.securityContext.capabilities.add\n  cap == \"NET_ADMIN\"\n  msg := \"NET_ADMIN capability is forbidden\"\n}\n```\n\n## Compliant Pod Manifest\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: compliant-pod\nspec:\n  containers:\n  - name: app\n    image: nginx\n    securityContext:\n      privileged: false\n      capabilities:\n        add: []\n  volumes:\n  - name: config\n    emptyDir: {}\n```\n\n## Noncompliant Pod Manifest\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: noncompliant-pod\nspec:\n  containers:\n  - name: app\n    image: nginx\n    securityContext:\n      privileged: true\n      capabilities:\n        add: [\"NET_ADMIN\"]\n  volumes:\n  - name: host-root\n    hostPath:\n      path: /\n```\n\n## Validation Plan\n1. Test policy with OPA: `opa test -v policy.rego`\n2. Validate compliant pod: `kubectl apply -f compliant.yaml`\n3. Verify noncompliant rejection: `kubectl apply -f noncompliant.yaml --dry-run=server`","diagram":null,"difficulty":"beginner","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Cloudflare"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T05:56:41.337Z","createdAt":"2026-01-21T22:55:55.474Z"},{"id":"q-5601","question":"In a multi-tenant edge compute platform that runs WebAssembly modules uploaded by tenants including Coinbase, Cloudflare, and MongoDB, design an end-to-end mechanism to enforce tenant-scoped outbound API access: (1) require module signing by the tenant and Rekor attestations; (2) an admission policy (OPA/Kyverno) that rejects non-compliant module calls; (3) provide concrete policy snippets, a sample module manifest, and a test plan to verify signing, attestation, and allowlist enforcement?","answer":"Each WASM module must be signed with the tenant's private key, verified against Rekor, and passed through an OPA-based admission check that enforces a per-tenant allowlist of outbound endpoints. Provi","explanation":"## Why This Is Asked\nThis tests end-to-end security for edge runtimes where tenants deploy code.\n\n## Key Concepts\n- WebAssembly at edge, per-tenant policies\n- Rekor attestations and verification\n- OPA/Rego policies for outbound access\n- Tenant onboarding and key rotation\n\n## Code Example\n```javascript\n// sample Rego-like policy snippet (for illustration)\npackage edge.authz\ndefault allow = false\nallow {\n  input.tenant in data.tenants\n  input.endpoint in data.tenants[input.tenant].allowlist\n  input.signature.attested == true\n}\n```\n\n## Follow-up Questions\n- How would you handle revocation of a tenant's signing key?\n- How would you test policy performance at scale?","diagram":null,"difficulty":"intermediate","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Coinbase","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T07:54:51.319Z","createdAt":"2026-01-22T07:54:51.319Z"},{"id":"q-5879","question":"You run a shared Kubernetes cluster for multiple tenants. Implement a concrete network segmentation policy to ensure web namespace pods can reach api namespace pods only over TLS (port 443), and api namespace pods can access db namespace pods only as needed. All other cross-namespace traffic must be denied. Provide concrete Calico/Policy YAMLs, rationale, and a test plan with validation and rollback steps?","answer":"Design namespace-scoped Calico NetworkPolicy to enforce least privilege: web->api on TLS/443, api->db only as needed, default deny all else. Use labels on namespaces, mutual-TLS via Istio for encrypti","explanation":"## Why This Is Asked\nDemonstrates practical multi-tenant isolation, policy layering, and testability in production. It also probes trade-offs between policy expressivity and performance in large clusters.\n\n## Key Concepts\n- Kubernetes NetworkPolicy, Calico/Cilium\n- Namespace scoping and default-deny posture\n- Service mesh mTLS for encryption in transit\n- Refusal of unintended cross-tenant traffic, rollback strategies\n- Validation via test pods and logs\n\n## Code Example\n```yaml\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-web-to-api\n  namespace: web\nspec:\n  podSelector: {}\n  policyTypes:\n  - Egress\n  egress:\n  - to:\n    - namespaceSelector:\n        matchLabels: {name: api}\n    ports:\n    - protocol: TCP\n      port: 443\n```\n\n## Follow-up Questions\n- How would you adapt policy when using a service mesh? explain sidecar proxy interactions.\n- How would you test rollback and what metrics indicate policy drift?","diagram":null,"difficulty":"advanced","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Coinbase","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T20:43:35.130Z","createdAt":"2026-01-22T20:43:35.130Z"},{"id":"q-5976","question":"In a large Kubernetes environment with hundreds of microservices across 60 namespaces and strict multi-tenant isolation, design an end-to-end security model using only Kubernetes-native features plus one optional control plane (Istio). Focus on pod identity, mTLS, least-privilege RBAC, network segmentation, and runtime enforcement. Provide concrete policy examples, deployment manifests, and a validation plan?","answer":"Adopt pod identity via Kubernetes service accounts tied to Istio SPIFFE IDs, enable mTLS across service mesh, and enforce least-privilege RBAC per namespace. Use namespace NetworkPolicies for east-wes","explanation":"## Why This Is Asked\nThis question probes the ability to architect a scalable, multi-tenant security model in Kubernetes, balancing native features with a single control plane for runtime enforcement. It assesses identity, policy, and observable security outcomes under real-world constraints.\n\n## Key Concepts\n- Pod identity via service accounts and SPIFFE IDs\n- Mutual TLS with Istio for east-west and north-south traffic\n- Namespace-scoped RBAC for least privilege\n- NetworkPolicy-based segmentation\n- Pod Security Standards and image signing (cosign) in CI/CD\n- Telemetry, audits, and validation plans\n\n## Code Example\n```yaml\n# Istio: mTLS\napiVersion: security.istio.io/v1beta1\nkind: PeerAuthentication\nmetadata:\n  name: default\n  namespace: default\nspec:\n  mtls:\n    mode: STRICT\n```\n```yaml\n# Istio: Authorization\napiVersion: security.istio.io/v1beta1\nkind: AuthorizationPolicy\nmetadata:\n  name: namespace-restrict\n  namespace: your-namespace\nspec:\n  rules:\n  - from:\n    - source:\n        principals: [\"cluster.local/ns/your-namespace/sa/sa-name\"]\n```\n\n## Follow-up Questions\n- How would you validate identity binding when workloads move between namespaces?\n- What metrics and logs would you correlate to prove isolation and policy effectiveness?","diagram":null,"difficulty":"advanced","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Meta","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T02:29:49.854Z","createdAt":"2026-01-23T02:29:49.854Z"},{"id":"q-6025","question":"Design a per-tenant data access policy for a multi-tenant data lake (Iceberg + Trino) using OPA; map each user to a tenant and enforce read access at query time. What policy, a Rego snippet, a compliant and a noncompliant example, and a test plan would you implement?","answer":"Map each user to a tenant and enforce read access at query time with OPA integrated into Trino. The policy constrains Iceberg schemas, tables, partitions, and columns per tenant. Provide a Rego snippe","explanation":"Why this is asked\n\nAssess hands-on ability to implement policy-driven data isolation in a real data lake stack. Requires practical knowledge of OPA, Rego, Iceberg, and Trino integration, plus a concrete test strategy.\n\nKey concepts\n\n- Fine-grained access control\n- OPA Rego policies\n- Iceberg partition and column ACLs\n- Trino integration and query-time enforcement\n- Testing: unit, integration, and end-to-end observability\n\nCode Example\n\n```javascript\n# Rego-like policy sketch (POC)\npackage data_access\n\ndefault allow = false\n\nallow {\n  input.tenant == data.tenants[_]\n  input.table in data.allowed_tables[_]\n  input.column in data.allowed_columns[_]\n  input.partition in data.allowed_partitions[_]\n}\n```\n\nFollow-up questions\n\n- How would tenant onboarding and revocation be handled without downtime?\n- How would you audit denied queries and demonstrate policy compliance?","diagram":null,"difficulty":"intermediate","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Anthropic","DoorDash"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T05:41:43.706Z","createdAt":"2026-01-23T05:41:43.706Z"},{"id":"q-6108","question":"In a multi-tenant production Kubernetes cluster, design a zero-trust image supply chain using Cosign attestations stored in Rekor and enforce them with Kyverno and OPA so only attested images from approved registries can deploy to prod. Provide the Kyverno policy YAML, a compliant Pod, a noncompliant Pod, and a practical testing plan including attestation generation, revocation, and observability hooks?","answer":"Sign images with cosign and publish attestations to Rekor; add Kyverno policy to require a Rekor-attested image for prod and a registry allowlist; integrate with OPA for an extra check on the attestat","explanation":"## Why This Is Asked\nTests ability to design end-to-end supply chain security with real tools and runtimes. It explores attestation workflows, policy composition, and operational tests.\n\n## Key Concepts\n- Image attestations (cosign/Rekor)\n- Kyverno/OPA policy integration\n- Registry allowlists and prod confinement\n- Attestation revocation and observability\n\n## Code Example\n```yaml\n# Kyverno policy example placeholder\n```\n\n## Follow-up Questions\n- How would you handle rotation of allowed registries?\n- What metrics/logs would you ship for security incidents?","diagram":"flowchart TD\n  S[Source Repo] --> B[Build]\n  B --> C[Cosign Sign]\n  C --> R[Rekor Attestation]\n  R --> E[Policy Check: Kyverno/OPA]\n  E --> P[Production Namespace]","difficulty":"advanced","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","LinkedIn","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T09:02:08.035Z","createdAt":"2026-01-23T09:02:08.035Z"},{"id":"q-6114","question":"In a multi-tenant edge platform running WebAssembly modules on edge nodes, design a concrete plan to ensure only signed attested WASM modules execute. Include per-tenant attestation flow, cosign signing commands, Rekor attestations, a runtime loader policy (OPA/Kyverno), module revocation, and a scalable storage layout with example commands?","answer":"Propose: each WASM module and its manifest are cosign-signed; attestations stored in Rekor and mapped to tenant IDs; edge loader validates provider, issuer, and freshness, not allowing unsigned or rev","explanation":"## Why This Is Asked\nThis question probes practical edge security controls: attestation-driven loading of WASM modules at scale.\n\n## Key Concepts\n- Sigstore cosign for artifacts and manifests\n- Rekor for attestations\n- Per-tenant scope and revocation workflows\n- Runtime loader integration with OPA/Kyverno\n\n## Code Example\n```javascript\n// Pseudo: verify Rekor-attested signature before loading WASM\nfunction verifyModule(moduleBytes, attestation) {\n  // verify signature + certificate chain against Rekor entry\n  // verify tenantId, expiry, revocation status\n  return isValid;\n}\n```\n\n## Follow-up Questions\n- How would you handle key rotation and tenant onboarding?\n- How would you test attestation at scale and during revocation events?","diagram":"flowchart TD\n  A[WASM Module] --> B{Attested?}\n  B -- Yes --> C[Loader Executes]\n  B -- No --> D[Reject]\n  C --> E{Revoked?}\n  E -- No --> F[Audit & Run]\n  E -- Yes --> D","difficulty":"intermediate","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","NVIDIA","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T09:38:48.376Z","createdAt":"2026-01-23T09:38:48.378Z"},{"id":"q-6175","question":"In a Kubernetes cluster hosting both Snap and Hugging Face services, design a zero-trust inter-service authorization policy for gRPC calls using a service mesh (Istio or Linkerd) with SPIFFE IDs and namespace-scoped permissions. Provide: (1) a mesh policy (Istio or Linkerd) allowing only whitelisted SPIFFE IDs per namespace; (2) an OPA Gatekeeper ConstraintTemplate and Constraint to enforce the same; (3) a compliant vs noncompliant traffic example; (4) a test plan and validation steps?","answer":"Enforce mTLS with SPIFFE IDs across all services, then restrict inter-service calls by namespace via Istio AuthorizationPolicy and a Gatekeeper Constraint. Map each service to a SPIFFE ID (e.g., spiff","explanation":"## Why This Is Asked\nTests service-mesh security, SPIFFE identity, and policy composition across multiple tooling stacks with practical artifacts.\n\n## Key Concepts\n- Kubernetes service mesh (Istio/Linkerd) mTLS and SPIFFE IDs\n- Namespace-scoped authorization policies\n- OPA Gatekeeper ConstraintTemplate and Constraint\n- Auditing, monitoring, and validation of policy enforcement\n\n## Code Example\n```yaml\n# Istio: allowlist per-namespace SPIFFE IDs for gRPC\napiVersion: security.istio.io/v1beta1\nkind: AuthorizationPolicy\nmetadata:\n  name: allow-spiffe-per-namespace\n  namespace: hf\nspec:\n  rules:\n  - from:\n    - source:\n        principals: [\"spiffe://cluster.local/ns:snap/sa:svc-a\"]\n    to:\n    - operation:\n        ports: [\"50051\"]\n        methods: [\"POST\"]\n```\n\n```yaml\n# Gatekeeper ConstraintTemplate (pseudo-structure)\napiVersion: constraints.gatekeeper.sh/v1beta1\nkind: ConstraintTemplate\nmetadata:\n  name: k8s_spiiffe_whitelist\nspec:\n  crd:\n    spec:\n      names:\n        kind: SPIFFEWhitelist\n        listKind: []\n        plural: SPIFFEWhitelists\n        singular: SPIFFEWhitelist\n  targets:\n  - target: admission.k8s.gatekeeper.sh\n    rego: |\n      package spiiffe_whitelist\n      violation[\"msg\"] { input.review.object.kind == \"Pod\"; ... }\n```\n\n## Follow-up Questions\n- How would you test policy changes in a CI/CD pipeline without affecting prod traffic?\n- How do you handle SPIFFE ID rotation and revocation in a long-running service mesh?","diagram":"flowchart TD\n  A[Clients/Services] --> B[Service Mesh (mTLS, SPIFFE)]\n  B --> C[Authorization Policy per Namespace]\n  C --> D[Gatekeeper Constraint (per-namespace)]\n  D --> E[Audit & Telemetry]","difficulty":"intermediate","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T11:47:29.802Z","createdAt":"2026-01-23T11:47:29.802Z"},{"id":"q-6251","question":"In a shared Kubernetes cluster hosting Nvidia GPU workloads for multiple tenants, design an admission policy that enforces GPU type alignment between pod requests and node labels. Only allow allocations to nodes labeled gpuType matching the requested type; enforce per-namespace allowances via Gatekeeper. Provide: (1) ConstraintTemplate, (2) Constraint, (3) compliant and noncompliant Pod specs, (4) a validation plan?","answer":"Use Gatekeeper with a ConstraintTemplate that enforces: pods requesting nvidia.com/gpu >0 must match a nodeAffinity rule on gpuType, constrained by per-namespace allowlists. Provide: (1) the Constrain","explanation":"## Why This Is Asked\nTests ability to enforce cross-tenant GPU isolation using Gatekeeper/OPA, Node labeling, and named GPU types in a realistic Nvidia/HashiCorp context.\n\n## Key Concepts\n- Kubernetes admission control\n- OPA Gatekeeper ConstraintTemplate\n- Node labeling and NodeAffinity\n- Per-namespace allowlists and error messaging\n- Testing with conftest and end-to-end scheduling\n\n## Code Example\n```yaml\n# Gatekeeper ConstraintTemplate sketch (simplified)\napiVersion: templates.gatekeeper.sh/v1beta1\nkind: ConstraintTemplate\nmetadata:\n  name: gputypematch\nspec:\n  crd:\n    spec:\n      names:\n        kind: GPUsTypeConstraint\n  targets:\n  - target: admission.k8s.gatekeeper.sh\n    rego: |\n      package gputypematch\n      violation[\"msg\"] = msg :-\n        input.review.object.kind == \"Pod\"\n        and some container in input.review.object.spec.containers\n        and container.resources.requests[\"nvidia.com/gpu\"] > 0\n        and not input.review.object.spec.nodeSelector[\"gpuType\"] in allowed_types\n        and not input.review.object.metadata.namespace in namespace_allowlist\n        msg := \"GPU type not allowed in namespace\"\n```\n\n## Follow-up Questions\n- How would you handle dynamic allowlists per namespace at scale?\n- What are trade-offs of using nodeAffinity vs taints/tolerations for GPU types?","diagram":null,"difficulty":"intermediate","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T16:18:46.836Z","createdAt":"2026-01-23T16:18:46.836Z"},{"id":"q-6253","question":"In a Kubernetes cluster, implement basic governance by auditing pod lifecycle events. Provide a minimal AuditPolicy YAML that records CREATE and UPDATE events for pods across all namespaces and forwards them to a webhook endpoint; include a compliant Pod manifest and a noncompliant Pod manifest (e.g., with a prohibited annotation), and a concrete verification plan to validate logs reach the webhook?","answer":"Define a policy that captures CREATE/UPDATE on pods (Metadata level) and route to a webhook sink. Show a compliant Pod manifest (no prohibited annotations) and a noncompliant one (e.g., with a forbidd","explanation":"## Why This Is Asked\nTests ability to translate governance requirements into Kubernetes Audit policies and webhook integration, plus end-to-end verification.\n\n## Key Concepts\n- Kubernetes Audit policy\n- External webhook sinks\n- Pod lifecycle events\n- Webhook verification\n\n## Code Example\n```yaml\napiVersion: audit.k8s.io/v1\nkind: Policy\nrules:\n- level: Metadata\n  resources:\n  - group: \"\"\n    resources: [\"pods\"]\n  verbs: [\"CREATE\", \"UPDATE\"]\n```\n\n```yaml\n# Compliant Pod manifest\napiVersion: v1\nkind: Pod\nmetadata:\n  name: compliant-pod\n  namespace: default\nspec:\n  containers:\n  - name: app\n    image: nginx:latest\n```\n\n```yaml\n# Noncompliant Pod manifest\napiVersion: v1\nkind: Pod\nmetadata:\n  name: noncompliant-pod\n  namespace: default\n  annotations:\n    audit.enforce: \"false\"\nspec:\n  containers:\n  - name: app\n    image: nginx:latest\n```\n\n## Diagram\n```flow\nflowchart TD\nA[Pod Create/Update] --> B[Audit Policy] \nB --> C[Webhook Receiver]\nC --> D[Monitoring/Alerts]\n```\n\n## Follow-up Questions\n- How would you test reliability if webhook is flaky?\n- How do you handle sensitive fields in Audit payloads?","diagram":null,"difficulty":"beginner","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","Microsoft","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T16:42:14.832Z","createdAt":"2026-01-23T16:42:14.833Z"},{"id":"q-6375","question":"In a private Kubernetes-based app repo, enforce that manifests pushed via CI do not contain hard-coded secrets and that only approved image tags are deployed. Describe a beginner-friendly workflow using a lightweight secret scanner (e.g., detect-secrets or truffleHog3) integrated into CI, plus a minimal configuration file and a test manifest that would fail. Include how you would verify it works locally and in CI?","answer":"Implement a CI gate that runs a lightweight secret scanner (detect-secrets or truffleHog3) against all Kubernetes manifests and Helm values files in pull requests. Configure the scanner to fail builds when high-entropy strings or known secret patterns are detected, and enforce image tag validation by maintaining an approved tags list that blocks deployments using unapproved versions.","explanation":"## Why This Is Asked\nThis question evaluates practical DevOps security skills by testing your ability to implement automated security gates that prevent secret leakage and ensure deployment consistency, directly connecting code hygiene to CI/CD workflows.\n\n## Key Concepts\n- Secret scanning integration in CI pipelines\n- Baseline management for handling false positives\n- Comprehensive coverage of manifests and Helm values\n- Local development vs CI environment verification\n- Image tag approval and enforcement workflows\n- Automated security gates in pull request processes\n\n## Code Example\n```yaml\n# GitHub Actions secret scan implementation\nname: Secret Scan & Image Validation\non: [pull_request]\njobs:\n  security-scan:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - name: Install detect-secrets\n        run: pip install detect-secrets\n      - name: Scan for secrets\n        run: |\n          detect-secrets scan --baseline .secrets.baseline\n          detect-secrets audit .secrets.baseline\n      - name: Validate image tags\n        run: |\n          # Script to check against approved tags list\n          ./scripts/validate-image-tags.sh\n```","diagram":"flowchart TD\n  A[Commit/PR] --> B[Secret Scan CI]\n  B --> C{Pass?}\n  C -- Yes --> D[Deploy/Merge]\n  C -- No --> E[Fail Build / Notify]","difficulty":"beginner","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T05:35:20.412Z","createdAt":"2026-01-23T21:38:51.807Z"},{"id":"q-6480","question":"In a Kubernetes cluster with Calico installed, implement a beginner NetworkPolicy that denies all egress by default in namespace tenant-a, but allows DNS lookups (UDP/TCP port 53), access to internal API 10.0.0.25:443, and intra-namespace pod-to-pod traffic. Provide the policy YAML, a compliant Pod manifest labeled app:web in tenant-a, a noncompliant Pod manifest, and a verification plan?","answer":"Implement a single NetworkPolicy for tenant-a selecting app:web pods with egress rules to (a) intra-namespace pods, (b) internal API 10.0.0.25:443, and (c) DNS on port 53 (UDP/TCP). Test with a compli","explanation":"## Why This Is Asked\n- Tests understanding of namespace-scoped network policies and the allow-by-default model.\n- Checks ability to craft precise egress rules and test scenarios.\n- Evaluates how to verify traffic patterns in a live cluster.\n\n## Key Concepts\n- Kubernetes NetworkPolicy basics, podSelector, egress rules\n- Protocols: TCP/UDP, DNS traffic behavior\n- Verification: controlled test pods and curl/dig tests\n\n## Code Example\n```yaml\n# Deny-all egress by default is achieved by allowing only specific egress in a policy\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: tenant-a-web-egress\n  namespace: tenant-a\nspec:\n  podSelector:\n    matchLabels:\n      app: web\n  policyTypes:\n  - Egress\n  egress:\n  - to:\n    - podSelector: {}\n  - to:\n    - ipBlock:\n        cidr: 10.0.0.25/32\n    ports:\n    - protocol: TCP\n      port: 443\n  - to:\n    - ipBlock:\n        cidr: 10.0.0.0/16\n    ports:\n    - protocol: UDP\n      port: 53\n    - protocol: TCP\n      port: 53\n```\n\n## Follow-up Questions\n- How would you adapt this policy for egress to a specific set of namespaces or external CIDRs?\n- How would you validate that the policy is enforced in a multi-tenant environment?","diagram":"flowchart TD\nA[Pod: app=web in tenant-a] --> B[Ingress/Egress policy: allow intra-namespace traffic]\nA --> C[Allow to internal API 10.0.0.25:443]\nA --> D[Allow DNS 10.0.0.0/16:53]\nA --> E[Block all other external egress]","difficulty":"beginner","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Salesforce","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T04:17:21.366Z","createdAt":"2026-01-24T04:17:21.367Z"},{"id":"q-6574","question":"Scenario: a Git repo contains Kubernetes manifests for multiple environments. Build a beginner-friendly CI gate using kube-score to catch at least 3 high-risk issues (e.g., containers run as root, readOnlyRootFilesystem missing, privileged containers). Provide a minimal GitHub Actions workflow, a failing manifest, a passing manifest, and how to interpret results?","answer":"Set up a GitHub Actions workflow that runs kube-score against manifests/ on push/PR and fails when high-risk issues are reported. Include two sample pods: failing pod with runAsNonRoot: false, privile","explanation":"## Why This Is Asked\n\nTests basic security gating in a beginner-friendly way that ties manifest hardening to CI.\n\n## Key Concepts\n\n- Kubernetes securityContext fields\n- kube-score risk levels (low/medium/high/critical)\n- CI gate integration and failure semantics\n\n## Code Example\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: insecure-pod\nspec:\n  containers:\n  - name: app\n    image: alpine:3.18\n    securityContext:\n      runAsNonRoot: false\n      privileged: true\n      readOnlyRootFilesystem: false\n```\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: secure-pod\nspec:\n  containers:\n  - name: app\n    image: alpine:3.18\n    securityContext:\n      runAsNonRoot: true\n      runAsUser: 1000\n      privileged: false\n      readOnlyRootFilesystem: true\n      allowPrivilegeEscalation: false\n```\n\n```yaml\nname: kube-score gate\non:\n  push:\n  pull_request:\n\njobs:\n  kube-score:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - name: Install kube-score\n        run: |\n          curl -L -o kube-score https://github.com/zegl/kube-score/releases/download/v1.9.0/kube-score-1.9.0-linux-amd64\n          chmod +x kube-score\n          sudo mv kube-score /usr/local/bin/kube-score\n      - name: Run kube-score\n        run: kube-score score manifests/ --output json > kube-score.json\n      - name: Fail on high risk\n        run: |\n          if grep -q high kube-score.json || grep -q critical kube-score.json; then\n            echo High-risk issues found\n            exit 1\n          fi\n```\n\n## Follow-up Questions\n\n- How would you scale this to multiple repos?\n- How would you handle false positives?\n","diagram":null,"difficulty":"beginner","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T08:45:00.492Z","createdAt":"2026-01-24T08:45:00.492Z"},{"id":"q-6690","question":"In a single-namespace Kubernetes app, set up a beginner-friendly CI gate using OPA Gatekeeper to enforce: (1) containers run as non-root (runAsNonRoot: true, runAsUser: 1000+), (2) readOnlyRootFilesystem: true, (3) image registry allowlist (e.g., registry.example.com/*). Provide a minimal ConstraintTemplate and Constraint, a compliant Pod manifest and a noncompliant one, and explain how you'd validate locally and in CI?","answer":"Use Gatekeeper with a ConstraintTemplate and Constraint to enforce: runAsNonRoot: true, runAsUser >= 1000, readOnlyRootFilesystem: true, and imageAllowList including registry.example.com/*. Provide a ","explanation":"## Why This Is Asked\nTests understanding of Gatekeeper policy design, basic Rego, and CI validation.\n\n## Key Concepts\n- OPA Gatekeeper, ConstraintTemplate, Constraint, Rego basics\n- Pod security: runAsNonRoot, readOnlyRootFilesystem, image allowlists\n- CI integration and local validation strategies\n\n## Code Example\n```yaml\n# ConstraintTemplate (conceptual)\napiVersion: templates.gatekeeper.sh/v1beta1\nkind: ConstraintTemplate\nmetadata:\n  name: k8spodsecurity\nspec:\n  crd:\n    spec:\n      names:\n        kind: K8SPodSecurity\n  targets:\n  - target: admission.k8s.gatekeeper.sh\n    rego: |\n      package k8spodsecurity\n      # policy consolidated in real template\n```\n```yaml\n# Constraint (conceptual)\napiVersion: constraints.gatekeeper.sh/v1beta1\nkind: K8SPodSecurity\nspec:\n  match:\n    kinds:\n      - apiGroups: [\"apps\"]\n        kinds: [\"Deployment\"]\n  parameters:\n    allowList:\n      - registry.example.com/*\n```\n\n## Follow-up Questions\n- How would you handle per-environment allowlists?\n- How would you audit Gatekeeper decisions historically?\n","diagram":"flowchart TD\n  A[Commit] --> B[CI gate: Gatekeeper]\n  B --> C{Pass?}\n  C -->|Yes| D[Proceed to deploy]\n  C -->|No| E[Fail pipeline]","difficulty":"beginner","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T13:39:53.753Z","createdAt":"2026-01-24T13:39:53.753Z"},{"id":"q-6803","question":"In a multi-tenant data lakehouse running on Kubernetes across AWS and GCP regions, design a concrete policy to enforce data residency: namespaces tied to tenants may only access storage endpoints located in approved regions for that tenant. Implement this with an OPA Gatekeeper ConstraintTemplate and Constraint, plus a Kyverno policy for runtime enforcement. Provide: (1) the ConstraintTemplate, (2) the Constraint, (3) a compliant vs noncompliant access example, (4) a test plan and validation steps?","answer":"Tag each tenant namespace with a region label and maintain an allowlist of storage endpoints per region. Gatekeeper ConstraintTemplate uses REGO to enforce that any pod in a namespace may only access ","explanation":"## Why This Is Asked\nTests ability to design policy wiring across Gatekeeper and Kyverno, and to think about runtime enforcement and testing.\n\n## Key Concepts\n- Policy as code\n- Data residency constraints\n- REGO evaluation path\n- Runtime enforcement\n- Testing strategy\n\n## Code Example\n```rego\npackage data_residency\ndefault allow = false\nallow {\n  input.request.kind.kind == \"Pod\"\n  region := input.metadata.namespace_region\n  ep := input.storage.endpoint\n  ep.region == region\n}\n```\n```\nyaml\n# Kyverno policy sketch\napiVersion: kyverno.io/v1\nkind: ClusterPolicy\nmetadata:\n  name: data-residency\nspec:\n  rules:\n  - name: deny-cross-region-storage\n    match:\n      resources:\n        kinds:\n        - Pod\n    validate:\n      message: \"Cross-region storage access not allowed\"\n      pattern:\n        spec:\n          containers:\n          - name: \"*\"\n```\n\n## Follow-up Questions\n- How would you simulate cross-region violations in CI?\n- How would you monitor policy drift in production?","diagram":"flowchart TD\n  Namespace[Namespace labeled region] --> Gatekeeper[Gatekeeper Constraint]\n  Gatekeeper --> Access[Access attempt to storage]\n  Access --> Outcome{Allowed?}\n  Outcome -->|Yes| Allow[Allow]\n  Outcome -->|No| Deny[Block]","difficulty":"intermediate","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Snowflake","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T18:03:57.453Z","createdAt":"2026-01-24T18:03:57.453Z"},{"id":"q-6864","question":"In a monorepo of microservices used by Salesforce/Airbnb, each service has a Dockerfile and Kubernetes manifests. Build a beginner-friendly CI gate that builds the image, generates an SBOM with Syft, and scans with Trivy before pushing. Provide a minimal GitHub Actions workflow, a failing Dockerfile (vulnerable) and a passing one, and guidance on interpreting the results?","answer":"Design a CI gate that builds the image, generates an SBOM with Syft, and scans the image with Trivy, failing on HIGH or CRITICAL vulnerabilities before pushing. Include a minimal GitHub Actions workfl","explanation":"## Why This Is Asked\n\nTests ability to implement a basic software supply chain gate: SBOM generation and vulnerability scanning integrated into CI, with clear pass/fail criteria.\n\n## Key Concepts\n\n- SBOM generation with Syft\n- Vulnerability scanning with Trivy\n- GitHub Actions workflow basics\n- Exit-code driven gates and report interpretation\n\n## Code Example\n\n```yaml\nname: SBOM and vulnerability gate\non:\n  push:\n    branches: [ main ]\njobs:\n  scan:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - name: Build image\n        run: docker build -t service:latest .\n      - name: Generate SBOM\n        uses: anchore/syft-action@v0\n        with:\n          image: service:latest\n      - name: Trivy scan\n        uses: aquasecurity/trivy-action@v0\n        with:\n          image: service:latest\n          severity: HIGH,CRITICAL\n          format: table\n          exit-code: 1\n```\n\n```dockerfile\n# Failing Dockerfile (vulnerable)\nFROM debian:stretch-slim\nRUN apt-get update && apt-get install -y curl\n```\n\n```dockerfile\n# Passing Dockerfile (secure, updated base)\nFROM debian:bullseye-slim\nRUN apt-get update && apt-get install -y curl\n```\n\n## Follow-up Questions\n\n- How would you adjust thresholds for different environments?\n- How would you add caching and parallel scans to speed up CI?","diagram":"flowchart TD\n  A[Repo] --> B[Build Image]\n  B --> C[Generate SBOM]\n  C --> D[Trivy Scan]\n  D --> E[Gate/Push]","difficulty":"beginner","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T20:44:42.514Z","createdAt":"2026-01-24T20:44:42.514Z"},{"id":"q-6883","question":"In a multi-tenant Kubernetes cluster hosting batch Spark jobs for a fintech data lake, implement a policy that ensures a SparkJob may read a data lake table only if the table is labeled data-class and the namespace specifies an allowed set of data-classes. Provide Gatekeeper ConstraintTemplate, a Constraint, compliant and noncompliant SparkJob specs, and a validation plan?","answer":"Implement a Gatekeeper policy using a ConstraintTemplate named SparkDataAccess with a Rego rule that validates SparkJob data access. The rule requires SparkJob.spec.read.table to include metadata.labels['data-class'] and verifies this data-class exists in the namespace's allowed_classes annotation.","explanation":"## Why This Is Asked\nThis tests the ability to translate data access policies into Gatekeeper constraints and implement namespace-scoped whitelisting for fintech data lake pipelines.\n\n## Key Concepts\n- Gatekeeper, ConstraintTemplate, and Constraint resources\n- Rego policy language for resource field validation and namespace mapping\n- Data-class labeling and per-namespace allowlists\n\n## Code Example\n```rego\npackage dataaccess\nviolation[{\"msg\": msg}] {\n  input.review.kind.kind == \"SparkJob\"\n  table := input.review.object.spec.read.table\n  not table\n  msg := \"SparkJob must specify data table\"\n}\n```","diagram":null,"difficulty":"intermediate","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Goldman Sachs","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T06:17:21.370Z","createdAt":"2026-01-24T21:34:52.555Z"},{"id":"q-7079","question":"You maintain a small microservices repo with a Dockerfile for a Node.js service and a Helm chart for deployment. Create a beginner-friendly CI gate that uses Trivy to scan both OS packages and application dependencies, failing if any high-severity vulnerability is found and passing otherwise. Provide a minimal GitHub Actions workflow, a failing Dockerfile/package.json demonstrating a high-severity vulnerability, and a passing manifest; explain how to interpret the results?","answer":"Use a GitHub Actions workflow that builds the image and then runs Trivy against the image and the Helm chart, failing on HIGH severity. Example: build image, then run: trivy image myservice:latest --s","explanation":"## Why This Is Asked\nTests practical understanding of container security in CI for beginner level, with concrete commands and outcomes.\n\n## Key Concepts\n- Trivy usage; severity filters\n- CI gate semantics; exit codes\n- Image vs filesystem scan; chart scanning\n\n## Code Example\n```yaml\nname: Security gate\non: [push]\njobs:\n  scan:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - name: Build image\n        uses: docker/build-push-action@v4\n        with:\n          context: .\n          push: false\n          tags: myservice:latest\n      - name: Trivy image\n        uses: aquasecurity/trivy-action@v2\n        with:\n          image: myservice:latest\n          severity: HIGH\n          exit-code: 1\n      - name: Trivy charts\n        uses: aquasecurity/trivy-action@v2\n        with:\n          directory: \"charts\"\n          severity: HIGH\n          exit-code: 1\n```\n\n## Follow-up Questions\n- How would you adapt for private registries?\n- How to handle false positives or outdated base images?","diagram":null,"difficulty":"beginner","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Meta","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T08:41:40.110Z","createdAt":"2026-01-25T08:41:40.110Z"},{"id":"q-7211","question":"In a Kubernetes cluster used by PayPal, Tesla, and Uber, implement a policy to ensure that no Pod can mount a Secret volume unless it references a CSI Secrets Store driver and the secret is in an allowlist per namespace. Provide: (1) a Gatekeeper ConstraintTemplate named SecretVolumePolicy, (2) a Constraint with per-namespace/secret-name allowlist, (3) a compliant Pod using a CSI secret mount, (4) a noncompliant Pod mounting a native Secret, (5) a validation plan with tests?","answer":"Leverage Gatekeeper: create ConstraintTemplate SecretVolumePolicy with a Rego rule that rejects any Pod spec containing a secret volume unless a CSI Secrets Store volume is used and namespace/name app","explanation":"## Why This Is Asked\nThis question probes policy design for secrets management in multi-tenant clusters, focusing on restricting secret access to CSI-based solutions and per-namespace allowlists.\n\n## Key Concepts\n- Gatekeeper ConstraintTemplate/Constraint\n- Rego enforcement for Pod specs\n- Secrets Store CSI driver integration and allowlists\n- Validation with dry-run and kubectl diff\n\n## Code Example\n```javascript\n// placeholder explanation\n```\n\n## Follow-up Questions\n- How to extend to audit secret usage across namespaces?\n- How to simulate failure modes in CI?\n","diagram":null,"difficulty":"intermediate","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["PayPal","Tesla","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T13:51:10.769Z","createdAt":"2026-01-25T13:51:10.770Z"},{"id":"q-7358","question":"In a mono-repo with Kubernetes manifests for several services, design a beginner-friendly GitHub Actions CI gate that ensures every Deployment/StatefulSet/DaemonSet includes readinessProbe and livenessProbe, each container runs as non-root (securityContext.runAsNonRoot: true, runAsUser: 1000), and imagePullPolicy: Always. Provide a minimal workflow, a failing manifest, a passing manifest, and how to interpret results?","answer":"I’d implement a GitHub Actions gate using yq to validate each manifest. It fails if a Deployment/StatefulSet/DaemonSet lacks both probes, or any container lacks runAsNonRoot: true with runAsUser: 1000","explanation":"## Why This Is Asked\nTests basic manifest linting and security checks in CI.\n\n## Key Concepts\n- Kubernetes manifests\n- YAML parsing with yq\n- GitHub Actions gating\n- SecurityContext and imagePullPolicy\n\n## Code Example\n```yaml\n# .github/workflows/ci-manifest-gate.yml\nname: Manifest Gate\non: [pull_request]\njobs:\n  gate:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - name: Validate manifests\n        run: |\n          set -e\n          violations=0\n          for f in $(git diff --name-only origin/main | grep -E '\\\\.ya?ml$' ); do\n            if ! yq e '.spec.template.spec.containers[].readinessProbe' \"$f\" >/dev/null 2>&1; then violations=$((violations+1)); fi\n            if ! yq e '.spec.template.spec.containers[].livenessProbe' \"$f\" >/dev/null 2>&1; then violations=$((violations+1)); fi\n            if ! yq e '.spec.template.spec.containers[].securityContext.runAsNonRoot' \"$f\" | grep -q true; then violations=$((violations+1)); fi\n            if ! yq e '.spec.template.spec.containers[].securityContext.runAsUser' \"$f\" | grep -q '1000'; then violations=$((violations+1)); fi\n            if ! yq e '.spec.template.spec.containers[].imagePullPolicy' \"$f\" | grep -q 'Always'; then violations=$((violations+1)); fi\n          done\n          if [ \"$violations\" -gt 0 ]; then exit 1; fi\n```\n\n```yaml\n# failing.yaml (no readiness/liveness probes or runAsNonRoot)\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: bad-app\nspec:\n  replicas: 1\n  template:\n    spec:\n      containers:\n        - name: app\n          image: example/app:latest\n          imagePullPolicy: IfNotPresent\n```\n\n```yaml\n# passing.yaml (probes present, non-root, Always)\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: good-app\nspec:\n  replicas: 1\n  template:\n    spec:\n      containers:\n        - name: app\n          image: example/app:latest\n          imagePullPolicy: Always\n          securityContext:\n            runAsNonRoot: true\n            runAsUser: 1000\n          readinessProbe:\n            httpGet:\n              path: /health\n              port: 80\n          livenessProbe:\n            httpGet:\n              path: /health\n              port: 80\n```\n\n## Follow-up Questions\n- How would you extend this gate to cover initContainers?\n- How would you surface violations to developers (e.g., PR checks, annotated reviews)?","diagram":"flowchart TD\n  A[PR] --> B[Gate]\n  B --> C{Pass?}\n  C -->|Yes| D[Merge]\n  C -->|No| E[Fail]","difficulty":"beginner","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","Instacart","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T19:46:12.571Z","createdAt":"2026-01-25T19:46:12.572Z"},{"id":"q-7370","question":"In a repo with multiple Dockerfiles and Kubernetes manifests, design a beginner-friendly CI gate: run Hadolint on all Dockerfiles and verify that every manifest pins image tags (no 'latest'). Provide a minimal GitHub Actions workflow, a failing Dockerfile (FROM alpine:latest), a passing manifest with a pinned tag, and guidance on interpreting results?","answer":"Implement a GitHub Actions gate that runs Hadolint on every Dockerfile and verifies Kubernetes manifests pin all image tags (no 'latest'). The workflow should fail with a clear log if any Dockerfile l","explanation":"## Why This Is Asked\nThis task tests practical CI gating for container hygiene: linting with Hadolint and enforcing image tagging discipline across manifests.\n\n## Key Concepts\n- Hadolint integration in GitHub Actions\n- YAML parsing to inspect image tags\n- Regular expressions for pinning (no latest, no -dev) and clear failure messages\n- Practical debugging of lint vs. policy failures\n\n## Code Example\n```yaml\n# .github/workflows/ci-docker-k8s.yml\nname: CI Gate: Dockerfile lint + tag pin\non:\n  push:\n    branches: [ main, master ]\n  pull_request:\n    branches: [ '**' ]\njobs:\n  gate:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - uses: hadolint/hadolint-action@v3\n        with:\n          dockerfile: '**/Dockerfile'\n      - name: Check pinned image tags\n        run: |\n          set -e\n          if grep -RIn 'image:.*:.*(latest|dev.*)' manifests || true; then\n            echo 'Unpinned image tag detected' ; exit 1;\n          fi\n```\n\n## Follow-up Questions\n- How would you scale this gate to many repos?\n- How would you extend checks for other tag policies (sha256 digests)?","diagram":null,"difficulty":"beginner","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Instacart","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T20:37:21.732Z","createdAt":"2026-01-25T20:37:21.733Z"},{"id":"q-7447","question":"In a data access service fronting Snowflake and Azure Data Lake, design a policy to enforce least privilege at query time. The gateway must authorize each request using JWT claims (roles, pii flag) and the target schema. Specify: (1) an OPA Rego policy, (2) a compliant API call example, (3) a noncompliant one, (4) a test plan and CI steps to validate policies and publish attestations to Rekor?","answer":"Implement a gateway-level OPA policy that validates JWT claims (roles, PII flag) and target schema authorization at query time. The Rego policy should: (a) allow SELECT operations only for users with the 'data_viewer' role on schemas in the approved allowlist; (b) require the 'data_pii_approved' claim for accessing PII-enabled schemas; (c) deny all other roles, unauthorized schemas, and non-compliant operations. This policy enforces least privilege by requiring both proper user identity and appropriate data context before executing any query.","explanation":"## Why This Is Asked\nTests practical policy-as-code implementation for data access governance with enforceable runtime controls across multi-platform data environments.\n\n## Key Concepts\n- OPA Rego for fine-grained access control\n- JWT claim-to-RBAC mapping\n- Least privilege enforcement in data warehouses and lakes\n- Rekor attestations for policy compliance\n- Gateway-based policy enforcement patterns\n\n## Code Example\n```rego\npackage data.access\n\ndefault allow = false\n\nallow {\n  input.method = \"SELECT\"\n  input.user.role = \"data_viewer\"\n  input.schema in data.allowlist\n  not input.pii\n}\n\nallow {\n  input.method = \"SELECT\"\n  input.user.role = \"data_viewer\"\n  input.schema in data.allowlist\n  input.pii\n  input.user.claims.data_pii_approved = true\n}\n```","diagram":null,"difficulty":"intermediate","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Microsoft","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T05:51:41.557Z","createdAt":"2026-01-25T23:49:56.290Z"},{"id":"q-7512","question":"In a repo with a Helm chart for a microservice, design a beginner-friendly CI gate that enforces 1) image tags not equal to latest and referenced from registry.example.com/secure, 2) containers run as non-root (runAsNonRoot true and runAsUser 1000), 3) resource requests/limits defined in templates, and 4) required Kubernetes labels in metadata for auditability. Provide a minimal GitHub Actions workflow, a failing chart snippet, a passing chart snippet, and an explanation of how to interpret results?","answer":"Beginner-friendly CI gate using helm template + yq to validate a Helm chart. Check: images must come from registry.example.com/secure, tags not latest; containers runAsNonRoot true with runAsUser 1000","explanation":"## Why This Is Asked\n\nTests understanding of Helm templating and basic policy checks in CI for secure defaults, plus ability to craft a simple gate that a junior engineer can implement.\n\n## Key Concepts\n\n- Helm templating and values.yaml\n- YAML validation with yq\n- Kubernetes securityContext basics\n- Image tag policy and registry scoping\n- CI gate design and test manifests\n\n## Code Example\n\n```yaml\n# .github/workflows/helm-gate.yml\nname: Helm gate\non: push\njobs:\n  gate:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - uses: azure/setup-helm@v1\n      - name: Render charts\n        run: helm template mychart charts/mychart > /tmp/rendered.yaml\n      - name: Validate images\n        run: |\n          if ! yq e '.items[].spec.template.spec.containers[].image' /tmp/rendered.yaml | grep -q registry.example.com/secure/; then echo image origin invalid; exit 1; fi\n```\n\n```yaml\n# failing Deployment manifest\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myservice\nspec:\n  replicas: 1\n  template:\n    spec:\n      containers:\n        - name: app\n          image: registry.example.com/secure/myservice:latest\n          # missing securityContext and resources\n```\n\n```yaml\n# passing Deployment manifest\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myservice\nspec:\n  replicas: 1\n  template:\n    spec:\n      containers:\n        - name: app\n          image: registry.example.com/secure/myservice:v1.2.3\n          securityContext:\n            runAsNonRoot: true\n            runAsUser: 1000\n          resources:\n            requests:\n              cpu: 100m\n              memory: 128Mi\n            limits:\n              cpu: 200m\n              memory: 256Mi\n```\n\n## Follow-up Questions\n\n- How would you extend to multi-container pods?\n- How would you adapt for multiple charts in a single repo?","diagram":"flowchart TD\n  A[CI gate] --> B[Render Helm templates]\n  B --> C[Image policy]\n  B --> D[SecurityContext policy]\n  B --> E[Resources policy]\n  C --> F[Pass/Fail]\n  D --> F\n  E --> F","difficulty":"beginner","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T05:40:00.615Z","createdAt":"2026-01-26T05:40:00.616Z"},{"id":"q-7599","question":"In a multi-tenant Kubernetes cluster hosting MongoDB and Vault, implement a policy ensuring Pod-to-MongoDB access is allowed only when the Pod carries a tenant-scoped SPIFFE ID and resides in the same tenant namespace; enforce via Istio MTLS + AuthorizationPolicy and an OPA Gatekeeper ConstraintTemplate/Constraint. Provide: (1) the Istio policy, (2) Gatekeeper template + constraint, (3) a compliant vs noncompliant traffic example, (4) a validation plan?","answer":"Per-tenant SPIFFE access: allow Pod in tenantA NS with SPIFFE ID spiffe://cluster/tenantA/pod-1 to MongoDB; deny cross-namespace or missing/invalid IDs. Enforce via Istio MTLS + AuthorizationPolicy re","explanation":"## Why This Is Asked\n\nThis probes practical enforcement of identity-bound service access in a multi-tenant cluster, combining service mesh, policy-as-code, and runtime validation.\n\n## Key Concepts\n\n- SPIFFE IDs and mTLS\n- Istio AuthorizationPolicy\n- Gatekeeper ConstraintTemplate/Constraint\n- Tenant isolation and RBAC\n\n## Code Example\n\n```yaml\n# Istio AuthorizationPolicy (compliant example)\napiVersion: security.istio.io/v1beta1\nkind: AuthorizationPolicy\nmetadata:\n  name: mongodb-per-tenant-access\n  namespace: tenantA\nspec:\n  action: ALLOW\n  rules:\n  - from:\n    - source:\n        principals: [\"spiffe://cluster/tenantA/pod-1\"]\n    to:\n    - operation:\n        ports: [\"27017\"]\n```\n\n```yaml\n# Gatekeeper ConstraintTemplate (pseudo)\napiVersion: templates.gatekeeper.sh/v1beta1\nkind: ConstraintTemplate\nmetadata:\n  name: k8sspiffe\nspec:\n  crd:\n    spec:\n      names:\n        kind: SPIFFEIsTenantScoped\n  targets:\n  - target: admission.k8s.gatekeeper.sh\n    rego: |\n      package k8sspiffe\n      violation[{\"msg\": msg}] {\n        input.review.kind.kind == \"Pod\"\n        not input.review.object.metadata.annotations[\"spiffe-id\"]\n        msg := \"Pod must have a tenant-scoped SPIFFE ID annotation\"\n      }\n```\n\n## Follow-up Questions\n\n- How would you test cross-tenant data access failures?\n- How would you monitor and alert on violations and drift?","diagram":"flowchart TD\n  A[Tenant Namespace] --> B[Istio MTLS]\n  B --> C[AuthorizationPolicy]\n  C --> D[MongoDB]\n  E[Gatekeeper Policy] --> C","difficulty":"intermediate","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Hashicorp","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T09:07:39.385Z","createdAt":"2026-01-26T09:07:39.385Z"},{"id":"q-7769","question":"Design an admission-control policy using OPA Gatekeeper in a multi-tenant Kubernetes cluster to enforce image provenance, runtime security, and namespace network isolation; provide ConstraintTemplate, sample Constraint, compliant/noncompliant manifests, and a test plan?","answer":"Use an OPA Gatekeeper ConstraintTemplate with rules for image provenance (registry.example.com/secure/*), imagePullPolicy Always, readOnlyRootFilesystem, runAsNonRoot with runAsUser: 1000, no Privileg","explanation":"## Why This Is Asked\nAssesses practical policy design for multi-tenant clusters and how to validate policy with real manifests.\n\n## Key Concepts\n- OPA Gatekeeper, ConstraintTemplate, Constraint\n- image provenance, runtime security, NetworkPolicy, namespace scoping\n- validation/testing workflows\n\n## Code Example\n```javascript\n# ConstraintTemplate.sample\napiVersion: templates.gatekeeper.sh/v1beta1\nkind: ConstraintTemplate\nmetadata:\n  name: k8ssecurity\nspec:\n  crd:\n    spec:\n      names:\n        kind: K8sSecurityConstraint\n      validation: # omit for brevity\n  targets:\n  - target: AdmissionReview\n    rego: |\n      package k8ssecurity\n      ...\n```\n```javascript\n# Constraint.sample\napiVersion: constraints.gatekeeper.sh/v1beta1\nkind: K8sSecurityConstraint\nmetadata:\n  name: require-secure-images\nspec:\n  match:\n    kinds: [{apiGroups: [\"apps\"], kinds: [\"Deployment\"]}]\n  parameters: {registry: \"registry.example.com/secure/*\"}\n```\n```javascript\n# Compliant Pod/Deployment manifest (snippet)\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: compliant-app\nspec:\n  template:\n    spec:\n      containers:\n      - name: app\n        image: registry.example.com/secure/app:1.0.0\n        imagePullPolicy: Always\n        securityContext:\n          runAsNonRoot: true\n          runAsUser: 1000\n          allowPrivilegeEscalation: false\n        readinessProbe:\n          httpGet: {path: /health, port: 8080}\n        livenessProbe:\n          httpGet: {path: /health, port: 8080}\n        volumeMounts: []\n        resources: {requests: {cpu: \"100m\", memory: \"128Mi\"}, limits: {cpu: \"500m\", memory: \"256Mi\"}}\n        readOnlyRootFilesystem: true\n```\n\n## Follow-up Questions\n- How would you test this at scale across 100+ namespaces?\n- How would you handle legitimate exceptions or registry migrations?","diagram":"flowchart TD\n  A[Push change] --> B[Gatekeeper Template/Constraint]\n  B --> C{Policy Violation?}\n  C -- Yes --> D[Admission Denied]\n  C -- No --> E[Admission Granted]","difficulty":"advanced","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Robinhood","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T17:08:50.192Z","createdAt":"2026-01-26T17:08:50.192Z"},{"id":"q-7836","question":"In a multi-tenant data science platform shared by Goldman Sachs and Two Sigma, implement a policy that prevents notebook containers from mounting datasets outside a whitelisted data catalog and restricts kernels to approved runtimes. Use OPA Gatekeeper and Kyverno with SPIFFE IDs. Provide: templates, sample manifests, and a test plan?","answer":"Implement a Gatekeeper ConstraintTemplate NotAllowedNotebookData with a Constraint that denies pods mounting non-whitelisted datasets, and a Kyverno policy requiring a valid SPIFFE ID and an approved ","explanation":"## Why This Is Asked\nEvaluate hands-on ability to translate policy into Gatekeeper and Kyverno, using SPIFFE for workload identity and enforcing data-scoping in a real multi-tenant setup.\n\n## Key Concepts\n- Gatekeeper ConstraintTemplate and Constraint\n- Kyverno policy enforcement\n- SPIFFE IDs for notebook workloads\n- Data catalog whitelisting and kernel/runtime controls\n\n## Code Example\n```javascript\n// Pseudo-check illustrating the policy guard\nfunction isCompliant(pod) {\n  const spiffe = pod?.metadata?.annotations?.['spiffe.io/id'];\n  const mounts = (pod?.spec?.containers?.[0]?.volumeMounts || [])\n    .map(m => m.mountPath);\n  const hasWhitelistedData = mounts.some(p => p.startsWith('/data/whitelist'));\n  const hasApprovedKernel = !!spiffe && spiffe.startsWith('spiffe://')\n    && (pod?.spec?.containers?.[0]?.image || '').includes('approved-runtime');\n  return hasWhitelistedData && hasApprovedKernel;\n}\n```\n\n## Follow-up Questions\n- How would you validate policy effectiveness in CI/CD and in prod?\n- How would you handle exceptions for legitimate data-science projects without widening risk?","diagram":null,"difficulty":"intermediate","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T19:44:40.986Z","createdAt":"2026-01-26T19:44:40.986Z"},{"id":"q-7962","question":"In a Knative-based serverless platform used by Airbnb, IBM, and Goldman Sachs, implement a policy that restricts each function's outbound access to only whitelisted Pub/Sub topics and cloud storage buckets. Tie permissions to per-function SPIFFE IDs and namespace scope. Provide: (1) an OPA Gatekeeper ConstraintTemplate and Constraint, (2) a Kyverno policy alternative, (3) sample compliant vs noncompliant manifests, (4) an automated test plan and expected outcomes?","answer":"To implement outbound access restrictions for Knative functions, annotate function deployments with `allowedTopics` and `allowedBuckets` annotations, and bind SPIFFE identities within the namespace scope. The solution requires: (1) an OPA Gatekeeper ConstraintTemplate with Rego validation rules that check input.review.object.metadata.annotations against whitelisted resources, (2) a Kyverno ClusterPolicy alternative using validate patterns, (3) sample manifests demonstrating compliant deployments with proper annotations versus noncompliant ones missing required allowlists, and (4) an automated test plan verifying policy enforcement through deployment attempts and expected rejection outcomes.","explanation":"## Why This Is Asked\n\nThis question tests practical policy enforcement for serverless workloads in enterprise environments, focusing on outbound access control, SPIFFE identity management, and namespace-scoped allowlists within Knative platforms.\n\n## Key Concepts\n\n- SPIFFE ID-based access control for function isolation\n- Namespace-scoped allowlists for Pub/Sub topics and storage buckets\n- Gatekeeper ConstraintTemplate with Rego policy validation\n- Kyverno ClusterPolicy as alternative enforcement mechanism\n- End-to-end testing with compliant and noncompliant deployment scenarios\n\n## Code Examples\n\nThe implementation demonstrates enterprise-grade security patterns for serverless architectures, showing how major organizations can enforce zero-trust networking principles while maintaining operational flexibility through declarative policy enforcement.","diagram":null,"difficulty":"intermediate","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Goldman Sachs","IBM"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T05:41:30.329Z","createdAt":"2026-01-27T02:44:52.932Z"},{"id":"q-8217","question":"How would you enforce per-namespace image provenance and runtime security in a Kubernetes AI platform with multiple tenants, blocking image pulls from non-allowlisted registries and requiring Cosign attestations before admission, while ensuring pods run as non-root with restricted capabilities? Provide: (1) Gatekeeper ConstraintTemplate and Constraint; (2) Kyverno policy alternative; (3) sample compliant vs noncompliant manifests; (4) an automated test plan and validation steps?","answer":"Block non-allowlisted registries and require Cosign attestations for all images, map attestations to per-namespace SPIFFE IDs, enforce non-root user and restricted capabilities, and provide both Gatek","explanation":"## Why This Is Asked\nEvaluates practical policy-as-code skills for image provenance, per-namespace governance, and runtime hardening. Tests integration of Cosign attestations with admission controls and insistence on secure PodSecurity settings.\n\n## Key Concepts\n- Gatekeeper ConstraintTemplate and Constraint\n- Kyverno policy alternative\n- Cosign attestations and Rekor\n- Per-namespace allowlists and SPIFFE ID mapping\n- Pod security basics: runAsNonRoot, dropped capabilities\n\n## Code Example\n```javascript\n// Gatekeeper ConstraintTemplate (conceptual)\napiVersion: templates.gatekeeper.sh/v1beta1\nkind: ConstraintTemplate\nmetadata:\n  name: k8simageprovenance\nspec:\n  crd:\n    spec:\n      names:\n        kind: K8sImageProvenance\n  targets:\n  - target: admission.k8s.gatekeeper.sh\n    rego: |\n      package k8simageprovenance\n      # ...pseudo-logic tying image registry allowlist to namespace SPIFFE IDs\n```\n```javascript\n// Kyverno policy (conceptual)\napiVersion: kyverno.io/v1\nkind: Policy\nmetadata:\n  name: image-provenance\nspec:\n  rules:\n  - name: check-image-attestation\n    match:\n      resources:\n        kinds:\n        - Pod\n    validate:\n      message: \"Image must be Cosign-attested and from an allowed registry\"\n      pattern:\n        spec:\n          containers:\n          - image: ?\n```\n\n## Follow-up Questions\n- How would you simulate attestation failures and ensure no silent fallbacks?\n- How would you manage registry allowlists when tenants are frequently added/removed?","diagram":null,"difficulty":"intermediate","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Instacart","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T15:04:54.512Z","createdAt":"2026-01-27T15:04:54.513Z"},{"id":"q-8241","question":"In a Netflix-scale data platform powered by Databricks with Unity Catalog and Delta Lake, implement a per-workspace data isolation policy: Spark jobs can only access their own catalogs/tables, with auditable logs and automated violation alerts. Provide: (1) a Unity Catalog SQL policy snippet (GRANT/REVOKE), (2) a representative compliant vs noncompliant job manifest, (3) a practical validation plan?","answer":"Use separate catalogs per workspace, grant a restricted role to each workspace, and revoke cross-workspace access. Example: GRANT USAGE ON CATALOG workspace_a TO ROLE wa_job; GRANT USAGE ON DATABASE w","explanation":"## Why This Is Asked\nTests ability to enforce data isolation with Unity Catalog, while ensuring auditable access and scalable policy management.\n\n## Key Concepts\n- Unity Catalog ACLs\n- Per-workspace isolation\n- Auditability, alerting\n\n## Code Example\n```sql\n-- policy example for workspace A\nGRANT USAGE ON CATALOG workspace_a TO ROLE wa_job;\nGRANT USAGE ON DATABASE workspace_a_db TO ROLE wa_job;\nGRANT SELECT ON ALL TABLES IN DATABASE workspace_a_db TO ROLE wa_job;\n```\n\n## Follow-up Questions\n- How would you test cross-tenant leakage?\n- How would you integrate with CI/CD for policy drift?","diagram":null,"difficulty":"intermediate","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T16:40:10.737Z","createdAt":"2026-01-27T16:40:10.739Z"},{"id":"q-8294","question":"In a multi-tenant Kubernetes cluster where namespaces are created on demand by internal teams, design a scalable admission-control solution using OPA Gatekeeper to enforce: per-Namespace ResourceQuota and LimitRange; PodSecurity with runAsNonRoot true and a fixed UID, readOnlyRootFilesystem, and restricted capabilities; image allowlist with cosign signing; and default NetworkPolicy isolating each Namespace? Provide templates, compliant/noncompliant manifests, and validation steps?","answer":"Implement an OPA Gatekeeper bundle: (1) ConstraintTemplate/ConstraintDefinition enforcing per-Namespace ResourceQuota and LimitRange; (2) Pod security: runAsNonRoot true with a fixed UID, readOnlyRoot","explanation":"## Why This Is Asked\n\nTests mastery of policy as code, multi-tenant security, and scalable validation in production Kubernetes clusters.\n\n## Key Concepts\n\n- OPA Gatekeeper: ConstraintTemplates and ConstraintDefinitions\n- Namespace defaults: ResourceQuota, LimitRange\n- Pod security: runAsNonRoot, runAsUser, readOnlyRootFilesystem, capabilities\n- Image security: allowlist + cosign signing\n- Network isolation: default NetworkPolicy per Namespace\n- Validation: Gatekeeper audits and kubectl describe\n\n## Code Example\n\n```yaml\n# ConstraintTemplate (simplified)\napiVersion: templates.gatekeeper.sh/v1beta1\nkind: ConstraintTemplate\nmetadata:\n  name: k8srequirement\nspec:\n  crd:\n    spec:\n      names:\n        kind: K8sRequirement\n  targets:\n  - target: AdmissionReview\n    rego: |\n      package k8srequirement\n      # minimal rule placeholder\n```\n\n```yaml\n# ConstraintDefinition (simplified)\napiVersion: constraints.gatekeeper.sh/v1beta1\nkind: K8sRequirement\nmetadata:\n  name: k8srequirement\nspec:\n  match:\n   Kinds:\n      - apiGroups: [\"\"]\n        kinds: [\"Pod\"]\n```\n\n```yaml\n# Compliant Pod manifest (simplified)\napiVersion: v1\nkind: Pod\nmetadata:\n  name: compliant-pod\nspec:\n  containers:\n  - name: app\n    image: registry.example.com/app:1.0.0\n    securityContext:\n      runAsNonRoot: true\n      runAsUser: 1000\n      allowPrivilegeEscalation: false\n    volumeMounts: []\n  securityContext:\n    runAsNonRoot: true\n    runAsUser: 1000\n```\n\n```yaml\n# Noncompliant Pod manifest (simplified)\napiVersion: v1\nkind: Pod\nmetadata:\n  name: noncompliant-pod\nspec:\n  containers:\n  - name: app\n    image: registry.example.com/app:latest\n    securityContext:\n      runAsNonRoot: false\n      runAsUser: 0\n      allowPrivilegeEscalation: true\n```\n\n## Follow-up Questions\n\n- How would you test Gatekeeper audits and metrics?\n- How would you handle exceptions and rollback?","diagram":"flowchart TD\n  Namespace[NAMESPACE] --> Gatekeeper[GATEKEEPER]\n  Gatekeeper --> ConstraintTemplates[ConstraintTemplates]\n  ConstraintTemplates --> ConstraintDefinitions[ConstraintDefinitions]\n  Namespace --> Quotas[ResourceQuota + LimitRange]\n  Namespace --> Net[NetworkPolicy]\n  ImageSigning[Image signing] --> Audit[Gatekeeper Audit]","difficulty":"advanced","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Goldman Sachs"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T19:02:30.670Z","createdAt":"2026-01-27T19:02:30.670Z"},{"id":"q-8331","question":"Design a beginner-friendly CI gate for a mono-repo with Node and Python services that runs tests only for changed services, enforces a minimum test coverage (e.g., 75%), and fails the PR if coverage drops; provide a minimal GitHub Actions workflow and example failing/passing test results?","answer":"Detect touched services via git diff; for each touched service run language-specific tests: Node: npm ci && npm test -- --coverage; Python: pip install -r requirements.txt && pytest --maxfail=1 --disa","explanation":"## Why This Is Asked\nGauges practical multi-language CI in a monorepo by targeting only changed services, highlighting test coverage enforcement and per-service reporting.\n\n### Key Concepts\n- Detect changed services with git diff filtered by language folders\n- Language-specific test commands and coverage collection\n- Aggregate results and fail-fast if any service falls below threshold\n\n### Code Example\n```yaml\nname: PR Gate: Changed Services Tests\non:\n  pull_request:\n    branches: [ main, master ]\njobs:\n  gate:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - uses: actions/setup-node@v3\n        with:\n          node-version: '18'\n      - uses: actions/setup-python@v4\n        with:\n          python-version: '3.11'\n      - id: changed\n        run: |\n          CHANGED=$(git diff --name-only ${{ github.event.before }} ${{ github.sha }} | sed -E 's|/.*$||' | sort -u)\n          echo \"changed=$CHANGED\" > changed.txt\n          echo \"services=$CHANGED\" >> changed.txt\n      - name: Run tests for changed services\n        if: always()\n        run: |\n          for s in ${GITHUB_WORKSPACE}/services/*; do\n            dir=$(basename \"$s\")\n            if [ -f \"services/$dir/package.json\" ]; then\n              echo \"Testing Node service: $dir\"\n              (cd services/$dir && npm ci && npm test -- --coverage)\n            fi\n            if [ -f \"services/$dir/requirements.txt\" ]; then\n              echo \"Testing Python service: $dir\"\n              (cd services/$dir && python -m venv .venv && source .venv/bin/activate && pip install -r requirements.txt && pytest --maxfail=1 --disable-warnings --cov=. --cov-report=xml && deactivate)\n            fi\n          done\n      - name: Enforce coverage\n        run: |\n          echo \"Coverage checks would be parsed here; fail if any < 75% per service\"\n```\n\n### Follow-up\n- Extend to handle missing tests per service gracefully\n- Add per-service summary artifacts for reviews","diagram":null,"difficulty":"beginner","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","LinkedIn","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T20:00:50.268Z","createdAt":"2026-01-27T20:00:50.268Z"},{"id":"q-8354","question":"In a production Kubernetes cluster using OPA Gatekeeper, design a production-grade policy to enforce: every Pod runs as a non-root user; container capabilities are restricted (no SYS_ADMIN); readOnlyRootFilesystem is true; imagePullPolicy Always; and hostPath volumes are disallowed except in namespaces 'infra' and 'ci'. Provide the Gatekeeper ConstraintTemplate, a compliant Pod manifest, a noncompliant manifest, and steps to verify via the constraint status?","answer":"Gatekeeper policy: ConstraintTemplate K8sSecurityPolicy with Rego enforcing runAsNonRoot and runAsUser, restricted capabilities (empty), readOnlyRootFilesystem, imagePullPolicy Always, and hostPath di","explanation":"## Why This Is Asked\n\nTests familiarity with policy-as-code, RBAC-lite container hardening, and real-world gatekeeping. \n\n## Key Concepts\n\n- OPA Gatekeeper, ConstraintTemplate, Constraint\n- Rego policy design for PodSecurity\n- Compliant vs noncompliant manifests, violations reporting\n\n## Code Example\n\n```javascript\n// Pseudo-implementation: verify Pod meets security constraints\nfunction isCompliant(pod) {\n  return pod?.spec?.containers?.every(c => c.securityContext?.runAsNonRoot === true) &&\n         pod?.spec?.securityContext?.runAsNonRoot !== false;\n}\n```\n\n## Follow-up Questions\n\n- How would you test across multiple namespaces and environments?\n- What are trade-offs of strict policies vs legitimate workload exceptions?","diagram":"flowchart TD\n  A[ConstraintTemplate] --> B[Constraint]\n  B --> C[Pods/Deployments]\n  C --> D{Violations?}\n  D -->|Yes| E[Gatekeeper blocks]\n  D -->|No| F[Policy Compliant]","difficulty":"advanced","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Instacart"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T20:58:37.821Z","createdAt":"2026-01-27T20:58:37.821Z"},{"id":"q-8466","question":"In a mono-repo with Kubernetes manifests for multiple services, design a beginner-friendly CI gate using Conftest that enforces a policy set: hostPath volumes are forbidden; imagePullPolicy is Always; all containers run as non-root with runAsUser: 1000; and resources.requests/limits are defined for every container. Provide minimal GitHub Actions workflow, a failing manifest, a passing manifest, and interpretation rules?","answer":"Use a Conftest gate with a small Rego policy that blocks hostPath volumes; requires imagePullPolicy: Always; enforces containers run as non-root (securityContext.runAsNonRoot: true, runAsUser: 1000); ","explanation":"## Why This Is Asked\nIntroduce policy-as-code for Kubernetes manifests using Conftest; beginner-friendly yet practical.\n\n## Key Concepts\n- Conftest, Rego policies\n- Kubernetes manifest validation\n- CI gate design and exit codes\n- Policy-driven security checks\n\n## Code Example\n```rego\npackage main\n\ndeny[msg] {\n  input.kind == \"Deployment\"\n  some v in input.spec.template.spec.volumes\n  v.hostPath != null\n  msg = \"hostPath volumes are forbidden\"\n}\ndeny[msg] {\n  input.kind == \"Deployment\"\n  cont := input.spec.template.spec.containers[_]\n  cont.imagePullPolicy != \"Always\"\n  msg = \"imagePullPolicy must be Always\"\n}\ndeny[msg] {\n  input.spec.template.spec.containers[_].securityContext.runAsNonRoot != true\n  msg = \"containers must run as non-root\"\n}\ndeny[msg] {\n  input.spec.template.spec.containers[_].resources == null\n  msg = \"resources must be defined\"\n}\n```\n\n```bash\n# CI usage\nconftest test manifests/\n```\n\n## Follow-up Questions\n- How would you add a policy for ephemeral containers?\n- How to extend for Helm-templated manifests?\n","diagram":"flowchart TD\nA[Code changes] --> B[CI gate runs conftest]\nB --> C{Policy Pass?}\nC -->|Yes| D[PR ok]\nC -->|No| E[PR blocked]","difficulty":"beginner","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-28T04:16:12.589Z","createdAt":"2026-01-28T04:16:12.589Z"},{"id":"q-920","question":"In a real-time chat service like Discord, deployed on Kubernetes with NVIDIA GPUs for video processing, you introduce a third-party plugin system that runs as WebAssembly modules to apply custom video filters. How would you design a secure plugin sandbox and runtime attestation to prevent leakage of streams or keys, ensure isolation from other plugins, and enable rapid rollback if a plugin behaves unexpectedly in production? Provide concrete approaches and trade-offs?","answer":"Adopt a per-plugin WebAssembly sandbox with strict memory caps (e.g., 32–64MB), syscall filtering (seccomp), and an isolated FS; require attestation using a signed manifest and a hardware-backed key f","explanation":"## Why This Is Asked\n\nThis question probes pragmatic security engineering for dynamic plugin ecosystems in real-time services with GPU workloads. It tests risk modeling, tooling choices, and trade-offs between performance and isolation.\n\n## Key Concepts\n\n- WebAssembly sandboxing\n- syscall filtering and memory limits\n- hardware-backed attestation and KMS\n- per-plugin encryption and data isolation\n- runtime observability and quick rollback\n\n## Code Example\n\n```javascript\n// Verify plugin signature before load\nconst sig = loadSignature(plugin);\nconst ok = verifySignature(sig, plugin.code, TRUSTED_PUBLIC_KEY);\nif (!ok) throw new Error(\"Invalid plugin\");\n```\n\n## Follow-up Questions\n\n- How would you test the sandbox boundaries and detect escapes?\n- What would you monitor to detect a compromised plugin, and how would you roll back safely?","diagram":"flowchart TD\n  A[Plugin Load] --> B{Attestation Passed?}\n  B -->|Yes| C[Isolate in WASM sandbox]\n  B -->|No| D[Reject]\n  C --> E[Run in separate process]\n  E --> F[Audit Logs to tamper-evident store]","difficulty":"intermediate","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","NVIDIA","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T15:31:52.573Z","createdAt":"2026-01-12T15:31:52.573Z"},{"id":"q-959","question":"Scenario: A service executes user-provided Python plugins inside a container. Design a concrete runtime hardening plan using Linux namespaces, a minimal seccomp profile, and capability bounding, ensuring plugins cannot access host files or network directly while preserving IPC with a controlled channel. Outline exact steps and validation tests?","answer":"Drop all capabilities, run in a dedicated user namespace, mount root as read-only, and bind‑mount only the plugin assets at /plugins. Use a strict seccomp profile that whitelists essential syscalls (r","explanation":"## Why This Is Asked\n\nTests practical security hardening with knowledge of namespaces, seccomp, and capabilities—core skills for roles that handle untrusted code.\n\n## Key Concepts\n\n- Linux namespaces (user/net)\n- Capability bounding and dropping all caps\n- Seccomp profiles with syscall whitelists\n- Read-only root filesystem and selective bind mounts\n- Inter-process communication (IPC) channel integrity\n\n## Code Example\n\n```javascript\n// Example: docker run sandbox flags (conceptual)\nconst cmd = [\n  'docker', 'run', '--rm',\n  '--cap-drop', 'ALL',\n  '--security-opt', 'seccomp=seccomp-profile.json',\n  '--read-only',\n  '--network', 'none',\n  '--mount', 'type=bind,source=/path/to/plugins,target=/plugins',\n  'python-plugin-runner'\n];\n```\n\n## Follow-up Questions\n\n- How would you test for privilege escalation attempts from within the plugin?\n- What trade-offs exist between security and plugin functionality, and how would you mitigate them?","diagram":"flowchart TD\n  A[User Plugin] -->|Runs in sandbox| B[Isolated Container]\n  B --> C{Seccomp Filters}\n  C --> D[Allowed syscalls]\n  C --> E[Blocked syscalls]\n  B --> F[IPC Channel to Host]\n  F --> G[Controlled IPC Endpoint]","difficulty":"beginner","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["PayPal","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T16:44:33.574Z","createdAt":"2026-01-12T16:44:33.574Z"},{"id":"q-967","question":"Scenario: You manage a microservice app deployed to Kubernetes with CI/CD; you need to prevent tampered container images. **Describe a practical, beginner-friendly plan** to implement image signing and verification using **cosign**, integrate it into a GitHub Actions workflow, and enforce verification at deployment (registry or admission webhook). Include concrete commands?","answer":"Plan: sign container images with cosign in CI, store keys in a cloud KMS, rotate monthly, and require signature verification before deployment. In CI: sign on push, verify before publish. Commands to ","explanation":"## Why This Is Asked\nTest practical understanding of software supply chain security with beginner-friendly tooling.\n\n## Key Concepts\n- Image signing with cosign\n- Key management and rotation\n- CI/CD integration (GitHub Actions)\n- Deployment-time verification (registry or admission webhook)\n- Basic audit logging\n\n## Code Example\n```bash\ncosign generate-key-pair\ncosign sign docker.io/org/app:tag --key cosign.key\ncosign verify docker.io/org/app:tag --key cosign.pub\n```\n\n## Follow-up Questions\n- How would you rotate signing keys with minimal downtime?\n- How would you handle private registries and secret key storage in CI?\n","diagram":null,"difficulty":"beginner","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","NVIDIA","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T17:28:30.376Z","createdAt":"2026-01-12T17:28:30.376Z"},{"id":"q-994","question":"Scenario: A Kubernetes-based ML platform serves multiple teams; outbound data exfiltration is a breach risk. Propose a concrete, end-to-end control plane approach to prevent unauthorized data egress using policy-as-code, Kubernetes NetworkPolicy, and a centralized egress gateway. Include a sample Rego policy for Gatekeeper that enforces a namespace label data-export=allowed and an annotation egress-proxy=https://proxy.internal, and outline testing and GitOps integration?","answer":"An example: I implement an OPA Gatekeeper deny rule requiring pods to carry label data-export=allowed and annotation egress-proxy=https://proxy.internal; Gatekeeper blocks otherwise. A Calico NetworkP","explanation":"## Why This Is Asked\n\nThis tests practical application of policy-as-code (OPA Gatekeeper), Kubernetes security controls (NetworkPolicy), and GitOps-driven deployment, plus testing and trade-offs.\n\n## Key Concepts\n\n- OPA Gatekeeper\n- Rego policies\n- Kubernetes NetworkPolicy / Calico egress\n- GitOps (ArgoCD)\n- End-to-end testing\n\n## Code Example\n\n```rego\npackage gatekeeper.sample\n\ndeny[{\"msg\":\"Outbound egress blocked\",\"ns\":ns}]\n{ input.review.kind == \"AdmissionReview\"; obj := input.review.object; ns := obj.metadata.namespace; not obj.metadata.labels[\"data-export\"] == \"allowed\"; not obj.metadata.annotations[\"egress-proxy\"] }\n```\n\n## Follow-up Questions\n\n- How would you test this in a CI pipeline?\n- What are potential bypass vectors and mitigations?","diagram":null,"difficulty":"intermediate","tags":["cks"],"channel":"cks","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["OpenAI","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T18:39:01.707Z","createdAt":"2026-01-12T18:39:01.707Z"}],"subChannels":["general"],"companies":["Adobe","Airbnb","Amazon","Anthropic","Apple","Bloomberg","Citadel","Cloudflare","Coinbase","Databricks","Discord","DoorDash","Goldman Sachs","Google","Hashicorp","Hugging Face","IBM","Instacart","LinkedIn","Lyft","Meta","Microsoft","MongoDB","NVIDIA","Netflix","OpenAI","Oracle","PayPal","Plaid","Robinhood","Salesforce","Scale Ai","Slack","Snap","Snowflake","Square","Stripe","Tesla","Twitter","Two Sigma","Uber","Zoom"],"stats":{"total":94,"beginner":25,"intermediate":49,"advanced":20,"newThisWeek":38}}