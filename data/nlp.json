{"questions":[{"id":"q-1035","question":"In an advanced NLP interview, design an end-to-end multilingual QA system over English, Spanish, and Mandarin medical documents. The user asks in English. Outline architecture, data flow, privacy controls, latency targets, domain adaptation, and an evaluation plan. Include concrete components, trade-offs, and a short example of validation for a high-risk medical claim?","answer":"Propose a retrieval-augmented pipeline: BM25 plus LaBSE dense retrieval with FAISS, a cross-encoder reranker, and a calibrated generator that returns citations. Enforce privacy via on-device or strict","explanation":"## Why This Is Asked\nTests system design, multilingual IR, safety and evaluation in a realistic medical QA scenario across major vendors.\n\n## Key Concepts\n- Retrieval-augmented generation (RAG)\n- Multilingual embeddings (LaBSE)\n- Dense + sparse hybrid retrieval\n- Privacy/compliance (HIPAA-like)\n- Evaluation metrics (P@5, NDCG, human evaluation)\n\n## Code Example\n```javascript\n// Pseudo-config for RAG stack\nconst retriever = new DenseRetriever({ model: 'LaBSE', index: 'faiss' });\nconst ranker = new CrossEncoder({ model: 'cross-encoder/ms-marco-MiniLM-L-6-v2' });\nconst generator = new LLM({ model: 'gpt-4' });\n```\n\n## Follow-up Questions\n- How would you handle unseen diseases with few-shot prompts?\n- How would you monitor and improve citation reliability over time?","diagram":"flowchart TD\n  A[Query] --> B[Retriever]\n  B --> C[Reranker]\n  C --> D[Generator]\n  D --> E[Answer + Citations]","difficulty":"advanced","tags":["nlp"],"channel":"nlp","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","IBM","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T20:23:51.509Z","createdAt":"2026-01-12T20:23:51.509Z"},{"id":"q-1101","question":"You're building a real-time brand-monitoring NLP service that ingests up to 100k tweets per minute in multiple languages. Design a scalable pipeline to classify sentiment and issue categories (e.g., billing, outages) with <300 ms latency per tweet, handle code-switching and slang, detect and adapt to drift, and provide a rollout plan including testing, monitoring, and rollback?","answer":"Design a streaming NLP pipeline ingesting 100k tweets/min in multiple languages. Use Kafka+Flink; a compact multilingual model (DistilBERT or distilled XLM-R) with a fast slang-handling fallback. Prod","explanation":"## Why This Is Asked\n\nAssesses ability to architect a low-latency, multilingual NLP pipeline at scale, with drift handling and practical rollout considerations.\n\n## Key Concepts\n\n- Streaming architectures (Kafka, Flink/Beam)\n- Multilingual, compact models with fast inference\n- Latency budgeting and fallback paths for slang\n- Drift detection, evaluation, and safe rollout strategies\n\n## Code Example\n\n```python\n# Pseudo latency-budgeted inference\nimport time\n\ndef classify(tweet, model, classifier):\n    t0 = time.time()\n    emb = model.encode(tweet.text)\n    pred = classifier.predict(emb)\n    latency = time.time() - t0\n    return pred, latency\n```\n\n## Follow-up Questions\n\n- How would you detect and respond to model drift in production? Which metrics and thresholds?\n- What rollback strategy would you use if latency spikes or drift exceed a limit?\n","diagram":null,"difficulty":"intermediate","tags":["nlp"],"channel":"nlp","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Stripe","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T22:32:19.759Z","createdAt":"2026-01-12T22:32:19.759Z"},{"id":"q-1215","question":"Design a beginner-friendly pipeline for a Slack-based support bot that lives in a single workspace. It should: (1) classify Slack messages into intents: 'password_reset', 'access_request', 'billing_issue', 'incident'. (2) retrieve and present the most relevant FAQ article from a 100-article KB in English or Spanish. (3) operate with minimal latency on a shared CPU, and include a simple drift-detection plan and a rollout strategy with a safe fallback. Provide concrete components, data flow, and a short code snippet showing the classifier and retriever?","answer":"Use language detection, TF-IDF features per language, train a logistic regression classifier for intents, then retrieve the top article by cosine similarity on a bilingual KB. Use per-language vectors","explanation":"## Why This Is Asked\nAssess ability to design a practical NLP pipeline for enterprise chat, focusing on bilingual classification and fast retrieval within a Slack-like setting, plus drift detection and rollback strategies.\n\n## Key Concepts\n- Language detection\n- TF-IDF features\n- Logistic regression for intents\n- Cosine similarity retrieval\n- Bilingual KB management\n- Drift monitoring and rollback\n\n## Code Example\n```javascript\n// Pseudo-code: fit a per-language classifier and a bilingual retriever\nconst modelEN = trainLR(tfIdf(examplesEN), labelsEN);\nconst modelES = trainLR(tfIdf(examplesES), labelsES);\n\nfunction predict(msg){\n  const lang = detect(msg);\n  const vec = tfIdf(msg, lang);\n  const model = lang === 'en' ? modelEN : modelES;\n  return model.predict(vec);\n}\nfunction retrieve(query){\n  return kbArticles.findTopSimilar(query);\n}\n```\n\n## Follow-up Questions\n\n- How would you handle slang and emojis?\n- How would you test drift and plan rollbacks?","diagram":null,"difficulty":"beginner","tags":["nlp"],"channel":"nlp","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Slack","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T05:27:08.629Z","createdAt":"2026-01-13T05:27:08.629Z"},{"id":"q-470","question":"How would you implement a sentiment analysis pipeline for customer reviews that handles negation and domain-specific slang? What preprocessing steps would you prioritize?","answer":"I'd implement a transformer-based model like BERT fine-tuned on domain data, with key preprocessing steps including subword tokenization, negation scope detection using dependency parsing, and custom slang dictionary normalization.","explanation":"## Implementation Approach\n- **Model Selection**: BERT or RoBERTa fine-tuned on sentiment data\n- **Negation Handling**: Dependency parsing to identify negation scope\n- **Domain Adaptation**: Continued pretraining on company-specific reviews\n\n## Preprocessing Pipeline\n- Tokenization with subword vocabulary\n- Slang normalization using custom dictionary\n- Negation detection and scope marking\n- Text cleaning preserving sentiment-bearing words\n\n## Performance Considerations\n- Batch processing for efficiency\n- Model quantization for deployment\n- A/B testing with baseline models","diagram":"flowchart TD\n  A[Raw Reviews] --> B[Text Cleaning]\n  B --> C[Slang Normalization]\n  C --> D[Negation Detection]\n  D --> E[Tokenization]\n  E --> F[BERT Model]\n  F --> G[Sentiment Score]","difficulty":"intermediate","tags":["nlp"],"channel":"nlp","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","IBM","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":["bert","tokenization","negation scope","dependency parsing","fine-tuning","domain-specific"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2026-01-09T09:02:02.138Z","createdAt":"2025-12-26 12:51:05"},{"id":"q-500","question":"How would you implement basic text preprocessing for sentiment analysis, including tokenization, stop word removal, and stemming?","answer":"Use NLTK for preprocessing: tokenize with word_tokenize, remove stop words using stopwords corpus, apply PorterStemmer for stemming. Handle punctuation, convert to lowercase, and filter empty tokens. ","explanation":"## Text Preprocessing Pipeline\n\n- **Tokenization**: Split text into individual words using word_tokenize()\n- **Normalization**: Convert to lowercase and remove punctuation\n- **Stop word removal**: Filter common words using NLTK's stopwords corpus\n- **Stemming**: Apply PorterStemmer to reduce words to root forms\n- **Filtering**: Remove empty tokens and special characters\n\n```python\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\n\ndef preprocess_text(text):\n    tokens = nltk.word_tokenize(text.lower())\n    stop_words = set(stopwords.words('english'))\n    stemmer = PorterStemmer()\n    \n    filtered = [stemmer.stem(token) for token in tokens \n                if token.isalpha() and token not in stop_words]\n    return filtered\n```\n\nThis pipeline is essential for NLP tasks as it reduces noise and standardizes text representation.","diagram":"flowchart TD\n  A[Raw Text] --> B[Tokenization]\n  B --> C[Lowercase & Punctuation Removal]\n  C --> D[Stop Word Filtering]\n  D --> E[Stemming]\n  E --> F[Clean Tokens]","difficulty":"beginner","tags":["nlp"],"channel":"nlp","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Apple","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":["nltk","tokenization","stop words","stemming","porterstemmer","word_tokenize"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2025-12-27T04:54:50.778Z","createdAt":"2025-12-26 12:51:06"},{"id":"q-584","question":"How would you implement a transformer-based model for real-time text generation with attention mechanisms that handle variable-length sequences efficiently?","answer":"I would implement a transformer-based model using causal self-attention with rotary positional embeddings for effective position encoding. The architecture would leverage key-value caching during inference to eliminate redundant computations, employ batch processing for parallelization across sequences, and utilize flash attention algorithms to optimize memory efficiency for variable-length inputs.","explanation":"## Core Architecture\n- Multi-head attention with causal masking to maintain autoregressive properties\n- Rotary positional embeddings for enhanced sequence understanding and better long-range dependencies\n- Layer normalization and residual connections for stable training depth\n\n## Performance Optimizations\n- Key-value caching during inference to avoid recomputing previous tokens\n- Flash attention implementation for memory-efficient attention computation\n- Mixed precision training with bfloat16 for faster computation and reduced memory footprint\n\n## Production Considerations\n- Gradient clipping to ensure training stability\n- Learning rate scheduling with warmup phases for optimal convergence\n- Robust tokenizer handling for variable-length sequences and proper padding strategies","diagram":"flowchart TD\n  A[Input Tokens] --> B[Token Embeddings]\n  B --> C[Positional Encodings]\n  C --> D[Multi-Head Attention]\n  D --> E[Feed Forward Network]\n  E --> F[Layer Norm]\n  F --> G[Output Logits]\n  D --> H[Key-Value Cache]","difficulty":"advanced","tags":["nlp"],"channel":"nlp","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Lyft","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-08T11:51:18.547Z","createdAt":"2025-12-27T01:14:06.459Z"},{"id":"q-229","question":"What is the difference between tokenization and stemming in NLP text preprocessing, and when would you choose lemmatization over stemming?","answer":"Tokenization is the process of splitting text into individual tokens (words, punctuation marks, or subwords), while stemming reduces words to their root forms by removing suffixes using algorithms such as Porter or Snowball. Stemming is computationally faster but can produce non-dictionary words (e.g., 'studies' becomes 'studi'). Lemmatization uses dictionary-based morphological analysis to produce valid root words ('studies' becomes 'study') but requires more processing time. Choose stemming for search and retrieval applications where speed is prioritized, and lemmatization for analytical tasks that require accurate word forms and semantic meaning.","explanation":"## Key Differences\n**Tokenization**: The foundational step of text segmentation that breaks down text into discrete tokens, including words, subwords, punctuation, and other linguistic units. This preprocessing step is essential for any NLP pipeline.\n\n**Stemming**: A rule-based approach that removes word suffixes using algorithms like Porter (lightweight and language-agnostic) or Snowball (language-specific implementations). This method is fast but linguistically crude.\n\n**Lemmatization**: A dictionary-driven morphological analysis that returns valid root words by considering part-of-speech and context. This approach is slower but yields more accurate results.\n\n## Performance Trade-offs\n- **Speed**: Stemming is approximately 10x faster than lemmatization\n- **Accuracy**: Lemmatization produces linguistically valid roots and preserves semantic meaning\n- **Resource Usage**: Stemming requires minimal computational resources\n\n## When to Use Each\n**Stemming**: Search engines, information retrieval systems, document clustering, and applications where processing speed outweighs precision.\n\n**Lemmatization**: Text analysis, sentiment analysis, question-answering systems, chatbots, and applications requiring accurate word representation and semantic understanding.","diagram":"graph TD\n    A[Raw Text] --> B[Tokenization]\n    B --> C[Tokens: 'running', 'dogs']\n    C --> D[Stemming]\n    D --> E[Stemmed: 'run', 'dog']\n    B --> F[Feature Extraction]\n    D --> G[Search Indexing]\n    F --> H[ML Model Input]\n    G --> I[Text Retrieval]","difficulty":"beginner","tags":["tokenization","stemming","ner"],"channel":"nlp","subChannel":"text-processing","sourceUrl":null,"videos":null,"companies":["Amazon","Apple","Google","Meta","Microsoft","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-29T08:39:34.286Z","createdAt":"2025-12-26 12:51:07"},{"id":"q-294","question":"How does the attention mechanism in transformers allow the model to handle variable-length sequences without recurrent connections?","answer":"Attention computes weighted relationships between all token pairs using scaled dot-product attention, while positional encoding injects sequence order information through sinusoidal embeddings, allowing parallel processing of variable-length sequences without recurrence.","explanation":"## Why Asked\nTests deep understanding of transformer architecture's core innovation - replacing sequential recurrence with parallel attention while preserving order information through positional encoding.\n\n## Key Concepts\nScaled dot-product attention uses query-key-value matrices where each token computes similarity scores with all others: `scores = QK^T / sqrt(d_k)`. The softmax weights determine how much each token attends to others. Positional encoding adds sequence information through sinusoidal functions: `PE(pos,2i) = sin(pos/10000^(2i/d_model))` and `PE(pos,2i+1) = cos(pos/10000^(2i/d_model))`. This creates unique position embeddings that the model can learn relative position relationships from.\n\n## Code Example\n```python\ndef attention_with_positional_encoding(Q, K, V, pos_encoding):\n    # Add positional information to queries and keys\n    Q_pos = Q + pos_encoding\n    K_pos = K + pos_encoding\n    \n    # Scaled dot-product attention\n    scores = Q_pos @ K_pos.T / math.sqrt(d_k)\n    weights = F.softmax(scores, dim=-1)\n    return weights @ V\n\n# Positional encoding generation\ndef get_positional_encoding(seq_len, d_model):\n    position = torch.arange(seq_len).unsqueeze(1)\n    div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n    pe = torch.zeros(seq_len, d_model)\n    pe[:, 0::2] = torch.sin(position * div_term)\n    pe[:, 1::2] = torch.cos(position * div_term)\n    return pe\n```\n\n## Follow-up Questions\nHow does relative positional encoding differ from absolute? Why are sinusoidal functions used instead of learned embeddings? How does attention complexity scale with sequence length?","diagram":"flowchart TD\n  A[Input Tokens] --> B[QKV Projection]\n  B --> C[Attention Scores]\n  C --> D[Softmax Weights]\n  D --> E[Weighted Sum]\n  E --> F[Output Context]","difficulty":"intermediate","tags":["cnn","rnn","transformer","attention"],"channel":"nlp","subChannel":"transformers","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-29T06:58:45.429Z","createdAt":"2025-12-26 12:51:07"}],"subChannels":["general","text-processing","transformers"],"companies":["Adobe","Airbnb","Amazon","Apple","Databricks","DoorDash","Google","IBM","Lyft","Meta","Microsoft","MongoDB","Netflix","Slack","Snap","Square","Stripe","Tesla","Twitter"],"stats":{"total":8,"beginner":3,"intermediate":3,"advanced":2,"newThisWeek":3}}