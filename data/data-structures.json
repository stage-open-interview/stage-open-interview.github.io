{"questions":[{"id":"q-1038","question":"Design a stack that supports push(x), pop(), top(), and getMin() in O(1) time per operation. Duplicates allowed. Explain the approach, discuss invariants and space usage, and provide a compact code sketch (Python or Java)?","answer":"Use two stacks: a main stack for values and a minStack to track the current minimum. On push(x): push x to main and push min(x, minStack.top) to minStack (or x if minStack empty). On pop: pop both. to","explanation":"## Why This Is Asked\nTests understanding of maintaining auxiliary structures to achieve O(1) operations and how to handle duplicates and edge cases.\n\n## Key Concepts\n- Two-stack technique\n- Invariants: minStack.top always <= all elements on main below\n- Duplicate handling\n- Space vs. time trade-offs\n\n## Code Example\n```javascript\nclass MinStack {\n  constructor() {\n    this.stack = [];\n    this.minStack = [];\n  }\n  push(x) {\n    this.stack.push(x);\n    const m = this.minStack.length ? Math.min(x, this.minStack[this.minStack.length - 1]) : x;\n    this.minStack.push(m);\n  }\n  pop() {\n    if (!this.stack.length) return;\n    this.stack.pop();\n    this.minStack.pop();\n  }\n  top() {\n    return this.stack[this.stack.length - 1];\n  }\n  getMin() {\n    return this.minStack[this.minStack.length - 1];\n  }\n}\n```\n\n## Follow-up Questions\n- How would you adapt this to support getMin() on a persistent stack?\n- How would you test boundary cases (empty pop, getMin on empty)?","diagram":"flowchart TD\n  S[Stack] --> P[Push]\n  S --> O[Pop]\n  S --> T[Top]\n  S --> M[GetMin]","difficulty":"beginner","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Databricks","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T20:26:41.071Z","createdAt":"2026-01-12T20:26:41.071Z"},{"id":"q-1213","question":"Design a dynamic autocomplete data structure for a code search tool that stores terms with frequencies; implement insertWord(word), eraseWord(word), and querySuggestions(prefix, k) returning up to k completions starting with prefix, ordered by descending frequency then lexicographically. Discuss a Trie-based design with per-node top-k structures and update costs?","answer":"Use a Trie where every node holds a balanced BST of words sharing that prefix, ordered by (−freq, word). On insertWord(word), bump freq and update all nodes along the path; on eraseWord, decrement and","explanation":"## Why This Is Asked\nAutocompletion is common in code search tools at scale; dynamic updates require keeping accurate frequencies while still returning top suggestions quickly.\n\n## Key Concepts\n- Trie with per-node sorted structures to support prefix queries\n- Dynamic frequency maintenance on insert/erase\n- Trade-offs: memory vs. update/query time; handling duplicates\n\n## Code Example\n```javascript\nclass TrieNode {\n  constructor() {\n    this.children = new Map();\n    // word -> freq for this prefix\n    this.words = new Map();\n  }\n}\nclass Autocomplete {\n  constructor(){ this.root = new TrieNode(); }\n  insertWord(word){ /* update freq and all prefix nodes */ }\n  eraseWord(word){ /* decrement and prune zeros */ }\n  query(prefix, k){ /* traverse to node and return top-k */ }\n}\n```\n\n## Follow-up Questions\n- How would you optimize memory for large vocabularies?\n- How would you adapt for multi-language or unicode support?\n- How would you test correctness and performance under bursts of updates?","diagram":null,"difficulty":"beginner","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","Hugging Face"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T05:26:07.065Z","createdAt":"2026-01-13T05:26:07.065Z"},{"id":"q-2010","question":"Design a dynamic session tracker: each session has an id, a score, and a lastActive timestamp. Support insertSession(id, score, ts), updateScore(id, delta), expireSessionsBefore(ts) removing expired sessions, topK(window, k) returning the k highest scores among active sessions (active = ts >= now - window) breaking ties by id, and medianScore(window) returning the median score among active sessions. Target near O(log n) per update and O(k log n) for topK. Explain data structures and tradeoffs?","answer":"Use a balanced BST keyed by (score, id) for fast top-k and median, plus a hash map id→node for O(1) updates, and a queue to purge expired sessions by lastActive ts. On insert/update: create/move node ","explanation":"## Why This Is Asked\nThe problem tests ability to design a time-windowed data structure that supports dynamic updates and queries that depend on recent activity, a common pattern in analytics systems.\n\n## Key Concepts\n- Sliding window constraints over timestamps\n- Order-statistics on a dynamic set\n- Lazy deletion/tombstones for expirations\n- Balanced BST or similar for O(log n) updates\n\n## Code Example\n```javascript\nclass SessionDS {\n  constructor() {\n    this.map = new Map(); // id -> node\n  }\n  insertSession(id, score, ts) { /* ... */ }\n  updateScore(id, delta) { /* ... */ }\n  expireSessionsBefore(ts) { /* ... */ }\n  topK(window, k) { /* ... */ }\n  medianScore(window) { /* ... */ }\n}\n```\n\n## Follow-up Questions\n- How would you handle very large k (near n)?\n- How would you scale to multiple concurrent windows (per-region analytics)?","diagram":null,"difficulty":"intermediate","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T20:48:50.928Z","createdAt":"2026-01-14T20:48:50.928Z"},{"id":"q-2087","question":"Design a dynamic leaderboard data structure to support a game with many players: addOrUpdatePlayer(playerId, score), removePlayer(playerId), getTopK(k), and getPlayerRank(playerId). Achieve O(log n) updates and O(log n + k) to fetch the top-k. Explain how you handle duplicates (equal scores) and score changes?","answer":"Maintain two complementary structures: a hash map `playerToScore` mapping `playerId -> score`, and a tree map `scoreToPlayers` mapping `score -> ordered set of players`. For `addOrUpdatePlayer`: remove the player from their old score set (if exists), update both maps, and insert into the new score set. For `removePlayer`: delete from both maps. For `getTopK`: iterate descending through tree map keys, collecting players until reaching k. For `getPlayerRank`: count total players in higher score sets plus the player's offset within their current score set.","explanation":"## Why This Is Asked\nTests practical data structure design for a common gaming feature, balancing efficient updates with query performance.\n\n## Key Concepts\n- Hash maps provide O(1) player score lookups\n- Ordered maps enable range and top-k access\n- Ordered sets handle duplicate scores within groups\n- Rank computation accounts for tied scores\n\n## Code Example\n```javascript\n// Pseudocode illustrating approach\nclass Leaderboard {\n  constructor(){\n    this.playerToScore = new Map();\n    this.scoreToPlayers = new Map(); // descending keys; each maps to a Set of players\n  }\n  addOrUpdate(playerId, score) {\n    // Remove from old score group if exists\n    const oldScore = this.playerToScore.get(playerId);\n    if (oldScore !== undefined) {\n      const oldSet = this.scoreToPlayers.get(oldScore);\n      oldSet.delete(playerId);\n      if (oldSet.size === 0) this.scoreToPlayers.delete(oldScore);\n    }\n    // Update maps and add to new score group\n    this.playerToScore.set(playerId, score);\n    if (!this.scoreToPlayers.has(score)) {\n      this.scoreToPlayers.set(score, new Set());\n    }\n    this.scoreToPlayers.get(score).add(playerId);\n  }\n}\n```","diagram":null,"difficulty":"beginner","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Lyft","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T05:04:02.750Z","createdAt":"2026-01-14T23:37:38.493Z"},{"id":"q-2178","question":"Design a dynamic prefix-aware dictionary that stores strings with counts (duplicates allowed). Implement: addWord(s), removeWord(s) removing one occurrence, queryPrefixCount(p) giving how many distinct words start with p, and kthWordWithPrefix(p, k) returning the k-th word in lexicographic order among words beginning with p. Handle up to 2e5 words, optimize memory and updates; propose a data layout (e.g., compressed Trie) and discuss trade-offs and edge cases?","answer":"Approach: a compressed Trie (radix) where each node stores: sorted children, endCount, subCount. addWord(s): walk along s, create nodes as needed, increment endCount at terminal, and update subCount a","explanation":"## Why This Is Asked\nTests designing a mutable, prefix-aware dictionary with lexicographic kth queries and duplicates.\n\n## Key Concepts\n- Trie augmentation with subtree counts to support prefix queries and kth ordering\n- Duplicate handling via endCount and pruning when zero\n- Path-based updates for add/remove with amortized linear in word length\n- Memory vs. speed trade-offs in a compressed trie\n\n## Code Example\n```javascript\n// sketch of operations (high-level, not full implementation)\nclass Node { constructor() { this.children = new Map(); this.end = 0; this.sub = 0; } }\nclass PrefixDict { /* addWord, removeWord, queryPrefixCount, kthWordWithPrefix */ }\n```\n\n## Follow-up Questions\n- How would you extend to support wildcard prefixes or case-insensitive matching?\n- How would you adapt for large alphabets or Unicode inputs?","diagram":"flowchart TD\n  Trie[Radix Trie] --> Add[addWord]\n  Trie --> Remove[removeWord]\n  Trie --> Prefix[queryPrefixCount]\n  Trie --> Kth[kthWordWithPrefix]","difficulty":"advanced","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Google","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T06:46:54.915Z","createdAt":"2026-01-15T06:46:54.915Z"},{"id":"q-2324","question":"Design a dynamic bitset over a universe [1, 10^9] that supports set(l, r), reset(l, r), flip(l, r), sum(l, r), and kthOne(l, r, k). Use a run-length encoded interval map to achieve near O(log M) updates, where M is number of stored intervals. Explain data structure, edge cases, and complexity; provide a robust approach for sparse updates in large N?","answer":"Use a run-length encoded set of disjoint intervals with bits. Maintain an ordered map of [L, R, bit]. For set(l,r), split at l and r+1, erase overlapping blocks, insert [l,r,1], and merge neighbors wi","explanation":"## Why This Is Asked\nTests ability to design a compact, scalable bitset for massive domains with dynamic range updates.\n\n## Key Concepts\n- Run-length encoding of intervals; - Split/merge semantics; - Range queries; - kthOne via prefix accumulation.\n\n## Code Example\n```python\nclass RunBitset:\n    def __init__(self):\n        self.blocks = []  # list of [L, R, val]\n    def _split_at(self, x):\n        pass  # split blocks so a boundary starts at x\n```\n\n## Follow-up Questions\n- How to implement on a language with immutable maps efficiently?\n- How to adapt for 64-bit ranges and memory guarantees?","diagram":"flowchart TD\n  A[Start] --> B[Split at L and R+1]\n  B --> C[Replace overlapped blocks]\n  C --> D[Merge adjacent blocks]\n  D --> E[Compute sum/kthOne]\n","difficulty":"advanced","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Google","Hugging Face"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T13:01:30.477Z","createdAt":"2026-01-15T13:01:30.479Z"},{"id":"q-2356","question":"Design a dynamic string editor data structure that supports insert at position, delete substring, and substring hash queries with O(log n) edits and O(log n) hash lookups. Propose a rope-based solution using an implicit treap, where each node stores size and a double rolling hash (mod1, mod2). Explain split/merge, hash maintenance, collision mitigation, and memory trade-offs with a realistic editor workload?","answer":"Adopt an implicit treap (rope) where each node stores its subtree size and a double rolling hash (mod1, mod2). For insert/delete at position, split at pos, merge with the new string; for hash of [l,r]","explanation":"## Why This Is Asked\n\nDynamic string editing with fast substring queries is critical in modern editors and versioned docs. A rope with an implicit treap provides randomized balance and localized updates, enabling logarithmic edits while maintaining per-substring hashes for quick comparisons.\n\n## Key Concepts\n\n- Implicit treap (rope) structure for sequence storage\n- Subtree size and rolling hash maintenance\n- Polynomial rolling hash with two moduli to reduce collisions\n- Split and merge operations for positional edits\n- Substring hash isolation without copying strings\n\n## Code Example\n\n```javascript\nclass Node {\n  constructor(ch) {\n    this.c = ch;\n    this.pr = Math.random();\n    this.left = null; this.right = null;\n    this.sz = 1;\n    this.h1 = ch.charCodeAt(0);\n    this.h2 = ch.charCodeAt(0);\n  }\n}\n// Update size and hashes from children\nfunction upd(n){ if(!n) return; n.sz = 1; n.h1 = n.c.charCodeAt(0); n.h2 = n.c.charCodeAt(0);\n  if(n.left){ n.sz += n.left.sz; n.h1 = n.left.h1 + somePow1; n.h2 = n.left.h2 + somePow2; }\n  if(n.right){ n.sz += n.right.sz; n.h1 = n.h1 + rightHash1; n.h2 = n.h2 + rightHash2; }\n}\nfunction split(t, k){ // split first k chars to a\n  if(!t) return [null, null];\n  if(size(t.left) >= k){ const [l, r] = split(t.left, k); t.left = r; upd(t); return [l, t]; }\n  const [l, r] = split(t.right, k - size(t.left) - 1); t.right = l; upd(t); return [t, r];\n}\nfunction merge(a, b){ if(!a||!b) return a||b; if(a.pr < b.pr){ a.right = merge(a.right, b); upd(a); return a; } else { b.left = merge(a, b.left); upd(b); return b; } }\n```\n\n## Follow-up Questions\n\n- How would you extend for Unicode normalization? \n- How to handle concurrent edits in a collaborative editor? \n- Can you quantify memory overhead and compare with a pure block-based rope?","diagram":"flowchart TD\n  A[Client edits text] --> B[Split rope at pos]\n  B --> C[Insert/delete substring]\n  C --> D[Isolate substring hash]\n  D --> E[Merge rope back]\n  E --> F[Return hash / updated state]","difficulty":"advanced","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Citadel","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T14:42:37.775Z","createdAt":"2026-01-15T14:42:37.775Z"},{"id":"q-2503","question":"Design a lightweight 1D histogram data structure for values in [0, 1_000_000] that supports: insertValue(v), eraseValue(v) (one occurrence), rangeCount(l, r) counting how many values lie in [l, r], and kthValueInRange(l, r, k) returning the k-th smallest value within [l, r]. Aim for O(log M) per operation. Compare Fenwick vs segment-tree-of-frequencies and discuss handling of duplicates and memory?","answer":"Approach: implement a segment tree over [0, 1,000,000], each node stores the count of values in its interval. insertValue(v) and eraseValue(v) update along the path in O(log M). rangeCount(l, r) sums ","explanation":"## Why This Is Asked\nA practical, beginner-friendly take on range queries with duplicates, focusing on a simple, real-world histogram use-case.\n\n## Key Concepts\n- Segment tree for counts over a fixed value domain\n- Range sum and order-statistics queries within a range\n- Handling duplicates and memory through implicit nodes\n\n## Code Example\n```javascript\nclass Hist1D {\n  constructor(M = 1000000) {\n    this.M = M;\n    this.root = null; // implicit segtree node\n  }\n  insertValue(v) { /* update path */ }\n  eraseValue(v) { /* update path */ }\n  rangeCount(l, r) { /* range sum */ }\n  kthValueInRange(l, r, k) { /* descend to kth */ }\n}\n```\n\n## Follow-up Questions\n- How would you adapt to a much larger value range (e.g., 1e9)?\n- How would you support dynamic resizing without rebuilding?","diagram":null,"difficulty":"beginner","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["OpenAI","Robinhood","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T20:45:14.324Z","createdAt":"2026-01-15T20:45:14.324Z"},{"id":"q-2666","question":"Design a dynamic string-frequency dictionary that stores strings with counts. Implement: addString(s), removeString(s) (one occurrence), and queryTopK(prefix p, k) that returns the k most frequent strings starting with p, ordered by frequency descending and then lexicographically. Up to 2e5 total insertions. Provide a practical approach and discuss time/memory trade-offs; keep it beginner-friendly?","answer":"Approach: keep a HashMap<String,Integer> for counts. To support top-k by prefix, use a Trie where each node maintains a small top-k heap keyed by (count, string). On addString/removeString, update the","explanation":"## Why This Is Asked\nTests ability to combine basic maps with a prefix-based lookup and top-k retrieval, a common real-world need in autocomplete/deduplication.\n\n## Key Concepts\n- Hash maps for counts\n- Prefix search with Trie\n- Local top-k maintenance via per-node heaps\n- Handling duplicates and updates\n\n## Code Example\n```javascript\nclass TrieNode{constructor(){this.next=new Map();this.top=[];this.endCount=0}}\nclass Trie{constructor(){this.root=new TrieNode()}\n add(s){let node=this.root; for(let ch of s){if(!node.next.has(ch)) node.next.set(ch,new TrieNode()); node=node.next.get(ch); // update end? }\n // simplified: later rebuild top lists\n }\n}\n```\n\n## Follow-up Questions\n- How would you handle memory pressure on the per-node heaps?\n- Could you implement a lazy rebuild strategy for the top-k lists? ","diagram":"flowchart TD\nA[Start] --> B[Insert/Remove string]\nB --> C[Update counts map]\nC --> D[Propagate to Trie nodes]\nD --> E[Maintain top-k in node heaps]\nE --> F[QueryTopK(prefix, k)]\nF --> G[Return results]","difficulty":"beginner","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Netflix","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T05:49:14.184Z","createdAt":"2026-01-16T05:49:14.184Z"},{"id":"q-677","question":"Design a data structure to maintain dynamic counters for items, supporting add(key, delta), get(key), and getTopK(k) returning the k keys with highest counts, ties broken by key. In a real-time analytics scenario, such as tracking top active users by event count, describe the structure, updates, and complexity trade-offs?","answer":"Use a hash map key→count and recompute top-k on demand by sorting entries in descending order (tie-break by key). On add, adjust the counter and delete if zero. getTopK(k) runs in O(n log n). This kee","explanation":"## Why This Is Asked\nEvaluates knowledge of practical data-structure trade-offs for dynamic ranking. Candidates should justify when to sort on demand vs. maintain auxiliary structures.\n\n## Key Concepts\n- Hash map for O(1) key access\n- On-demand sorting for top-k retrieval\n- Trade-off: simpler updates vs. slower top-k retrieval\n\n## Code Example\n```javascript\nclass CounterTopK {\n  constructor() {\n    this.counts = new Map(); // key -> count\n  }\n  add(key, delta) {\n    const cur = this.counts.get(key) || 0;\n    const next = cur + delta;\n    if (next <= 0) this.counts.delete(key); else this.counts.set(key, next);\n  }\n  get(key) { return this.counts.get(key) || 0; }\n  topK(k) {\n    const arr = Array.from(this.counts.entries());\n    arr.sort((a,b)=> b[1]-a[1] || a[0].localeCompare(b[0]));\n    return arr.slice(0,k).map(([key]) => key);\n  }\n}\n```","diagram":null,"difficulty":"intermediate","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Discord","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-11T15:56:10.144Z","createdAt":"2026-01-11T15:56:10.144Z"},{"id":"q-686","question":"Design a data structure that maintains a multiset of integers with insert(x), erase(x) for one occurrence, findMedian(), and findKthSmallest(k). Achieve O(log n) time per operation on average. Explain data layout, invariants, and handling duplicates and lazy deletions. For example: insert 1,2,3,4,5; erase 3; what is median and findKthSmallest(2)?","answer":"Propose a two-heap design (lower half in a max-heap, upper half in a min-heap) with a hashmap for lazy deletions to support erase(x) (one occurrence). Balance so the sizes differ by at most 1; prune l","explanation":"## Why This Is Asked\n\nTests understanding of advanced DS combining heaps and lazy deletion to support dynamic order statistics.\n\n## Key Concepts\n\n- Two-heap maintenance for median\n- Lazy deletion with a hashmap for duplicates\n- Order-statistics and findKthSmallest via balanced heaps\n\n## Code Example\n\n```javascript\nclass DualHeapMedianSet {\n  constructor(){ /* ... skeleton ... */ }\n  insert(x){ /* ... */ }\n  erase(x){ /* ... */ }\n  findMedian(){ /* ... */ }\n  findKthSmallest(k){ /* ... */ }\n}\n```\n\n## Follow-up Questions\n\n- How do you handle many duplicates efficiently?\n- How would you implement findKthSmallest(k) with guaranteed worst-case O(log n) time?","diagram":null,"difficulty":"advanced","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Oracle","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-11T16:22:02.207Z","createdAt":"2026-01-11T16:22:02.207Z"},{"id":"q-697","question":"Design a time-weighted event multiset data structure. Supports insertEvent(ts, w), eraseEvent(ts, w) removing one occurrence, rangeSum(a, b) returning the sum of weights for events with timestamps in [a, b], and findKthWeightInRange(a, b, k) returning the k-th smallest weight among events with timestamps in [a, b]. Target O(log^2 n) per operation; handle duplicate timestamps and weights; discuss memory and trade-offs?","answer":"Proposed solution: a segment tree over timestamps; each node maintains a Fenwick tree over compressed weights. insertEvent/eraseEvent update all nodes on the timestamp path and adjust the weight Fenwi","explanation":"## Why This Is Asked\nTests ability to combine range data structures for multi-criteria queries and a k-th statistic within a range. It exposes memory vs. time trade-offs and handling of duplicates.\n\n## Key Concepts\n- Segment tree over time domain\n- Fenwick trees at nodes for weight counts\n- Coordinate compression of timestamps and weights\n- Range sum and k-th order statistics within a range\n- Handling duplicates via counts\n\n## Code Example\n```javascript\nclass EventDS {\n  insertEvent(ts, w) {}\n  eraseEvent(ts, w) {}\n  rangeSum(a, b) { return 0; }\n  findKthWeightInRange(a, b, k) { return -1; }\n}\n```\n\n## Follow-up Questions\n- How would you support unbounded timestamps?\n- What are memory/time trade-offs vs. a wavelet tree alternative?\n- How would you parallelize queries across cores for large workloads?","diagram":"flowchart TD\n  A[Insert/Event] --> B[Timestamp Segment Tree]\n  B --> C[Leaf: Fenwick over weights]\n  A --> D[Range queries use path sums]\n  A --> E[findKthWeightInRange uses weight-domain binary search]","difficulty":"advanced","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Plaid","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-11T17:17:41.092Z","createdAt":"2026-01-11T17:17:41.093Z"},{"id":"q-701","question":"Design a dynamic multiset of 2D points (duplicates allowed) with insertPoint(x,y), erasePoint(x,y) removing one occurrence, rangeCount(x1,y1,x2,y2), and rangeKthX(x1,y1,x2,y2,k) returning the k-th smallest x in the rectangle (tie by y). Target O(log^2 n) per op. Propose a segment tree over x; each node stores an ordered multiset of (y, unique_id). Explain duplicates handling, updates, rangeCount, and rangeKthX via binary search over x?","answer":"Use a segment tree over x with each node maintaining an ordered multiset of (y, id). Insert/erase updates O(log^2 n) nodes; counts via order_of_key. rangeCount aggregates counts from nodes spanning [x","explanation":"## Why This Is Asked\nAssesses mastery of 2D dynamic data structures and order statistics under updates.\n\n## Key Concepts\n- Segment tree over x with per-node BST of (y, id)\n- Duplicate handling via unique_id\n- RangeCount via node contributions\n- RangeKthX via binary search on x using rangeCount\n- Time: O(log^2 n) per op; memory: O(n log n)\n\n## Code Example\n```javascript\nclass PointDS {\n  insertPoint(x, y) { /* ... */ }\n  erasePoint(x, y) { /* ... */ }\n  rangeCount(x1, y1, x2, y2) { /* ... */ }\n  rangeKthX(x1, y1, x2, y2, k) { /* ... */ }\n}\n```\n\n## Follow-up Questions\n- How would you adapt for large coordinate ranges with compression?\n- How would you optimize for frequent rangeCount queries vs. updates?","diagram":null,"difficulty":"intermediate","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Discord","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-11T18:25:49.256Z","createdAt":"2026-01-11T18:25:49.256Z"},{"id":"q-710","question":"Design a data structure that maintains a dynamic multiset of strings with operations insert(word), erase(word) removing a single occurrence, countWithPrefix(prefix) returning how many words start with the prefix, and mostFrequentWithPrefix(prefix) returning the most frequent word among those starting with the prefix (tie-break lexicographically). What data structure would you implement, and what are the time complexities and invariants?","answer":"Trie-based approach: each node stores children, a subtreeCount, and bestWord with its frequency in that subtree, plus a global map word->count for duplicates. On insert(word): increment word count, tr","explanation":"## Why This Is Asked\nRealistic text-processing requirement: dynamic, prefix-based retrieval with duplicates. This tests trie augmentation, per-prefix aggregates, and update propagation.\n\n## Key Concepts\n- Trie with per-node aggregates\n- Handling duplicates via global word counts\n- Prefix-range queries through subtree counters\n\n## Code Example\n```javascript\n// Outline only; for reference\nclass Node { constructor(){ this.next=new Map(); this.subCount=0; this.best={word:null,count:0}; } }\n```\n\n## Follow-up Questions\n- How would you handle tie-breaking when two words share max frequency? \n- How would you support bulk insertions or deletions? ","diagram":null,"difficulty":"intermediate","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Scale Ai","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-11T19:15:36.077Z","createdAt":"2026-01-11T19:15:36.078Z"},{"id":"q-722","question":"Design a data structure for an array of integers that supports point updates update(i, val) and range maximum subarray sum queries maxSubarray(l, r) in O(log n). Explain the segment tree node data (sum, bestPref, bestSuff, bestSub) and the merge logic, including tie-breaking for leftmost subarray and handling entirely negative ranges. Example: start with [1,-2,3,4,-1], update index 2 to 5, query maxSubarray(0,4)?","answer":"Use a segment tree where each node stores four values: sum, bestPref, bestSuff, and bestSub. Merge: sum = L.sum+R.sum; pref = max(L.pref, L.sum+R.pref); suff = max(R.suff, R.sum+L.suff); best = max(L.","explanation":"## Why This Is Asked\nThe problem tests knowledge of segment trees for Kadane-like queries and details of node merging.\n\n## Key Concepts\n- Node stores sum, bestPref, bestSuff, bestSub\n- Merge uses L.sum/R.sum and L.suff+R.pref\n- Tie-break leftmost subarray\n- Negative arrays handling\n\n## Code Example\n```javascript\ntype Node = {sum:number, pref:number, suff:number, best:number};\nfunction merge(L:Node, R:Node):Node {\n  const sum = L.sum + R.sum;\n  const pref = Math.max(L.pref, L.sum + R.pref);\n  const suff = Math.max(R.suff, R.sum + L.suff);\n  const best = Math.max(L.best, R.best, L.suff + R.pref);\n  return {sum, pref, suff, best};\n}\n```\n\n## Follow-up Questions\n- How would you extend for range updates?\n- How does tie-breaking affect correctness?","diagram":null,"difficulty":"intermediate","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Bloomberg"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-11T20:23:33.860Z","createdAt":"2026-01-11T20:23:33.860Z"},{"id":"q-727","question":"Design a dynamic data structure for weighted 3D points (x,y,t) with weight w. Support insertPoint(x,y,t,w) and erasePoint(x,y,t,w) (duplicates allowed), rangeSum3D(x1,x2,y1,y2,t1,t2) for total weight in the 3D box, and findKthLargestInRange(x1,x2,y1,y2,t1,t2,k) for the k-th largest weight inside the box. Target O(log^3 n) per op; O(log W * log^3 n) for kth; discuss coordinate compression, memory trade-offs, and handling duplicates?","answer":"Use coordinate compression on x, y, and t, and implement a 3D segment-tree (nested trees) to support dynamic inserts/deletes with duplicates. Each node stores aggregated weight sums and counts. rangeS","explanation":"## Why This Is Asked\nTests knowledge of multi-dimensional data structures, dynamic updates with duplicates, and how to support order-statistics within spatial ranges. It also probes trade-offs between dense 3D structures vs sparse representations and the practicality of coordinate compression.\n\n## Key Concepts\n- 3D range queries with dynamic updates\n- Coordinate compression and nested segment trees\n- Order statistics within geometric ranges and handling duplicates\n- Trade-offs: memory vs time, online vs offline approaches\n\n## Code Example\n```javascript\n// Skeleton prototype for 3D segment tree with insert/erase and range query\nclass Node3D {\n  constructor() { /* ... */ }\n  update(x, y, t, w, delta) { /* delta = +w or -w */ }\n  query(x1, x2, y1, y2, t1, t2) { /* returns sum of weights */ }\n  kthInRange(x1,x2,y1,y2,t1,t2,k){ /* via weight-binary-search on tree */ }\n}\n```\n\n## Follow-up Questions\n- How would you optimize memory for sparse data?\n- How would you extend to support dynamic weight updates (change weight w for an existing point)?","diagram":null,"difficulty":"advanced","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Google","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-11T21:17:28.215Z","createdAt":"2026-01-11T21:17:28.215Z"},{"id":"q-739","question":"Design a dynamic 2D weighted point data structure that supports insertPoint(x,y,w), erasePoint(x,y,w) (duplicates allowed), rangeSum2D(x1,x2,y1,y2) for the total weight in the rectangle, and kthLargestInRectangle(x1,x2,y1,y2,k) for the k-th largest weight inside the rectangle. Aim for average O(log^2 n) per operation; discuss coordinate compression, memory trade-offs, and duplicates handling?","answer":"Use a 2D range tree: segment tree on compressed X; each node stores a Fenwick over Y where each entry is a weight-count map for duplicates. insertPoint/erasePoint update O(log^2 n). rangeSum2D O(log^2","explanation":"## Why This Is Asked\nTests building a practical 2D index with range queries and top-k under dynamic updates, blending geometry and order-statistics.\n\n## Key Concepts\n- 2D range trees and coordinate compression\n- per-node multisets for duplicates\n- binary search on weight domain for kth within rect\n\n## Code Example\n```javascript\n// Sketch: insertPoint updates along X-tree paths, updating inner Y-structures\n```\n\n## Follow-up Questions\n- How would you handle high churning data?\n- Compare 2D range trees vs kd-trees for this workload.","diagram":"flowchart TD\n  A[Insert Point] --> B[Update X-tree]\n  B --> C[Update Y-structures]\n  A --> D[Query RangeSum/Kth]\n  D --> E[Return Result]","difficulty":"advanced","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Hugging Face","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-11T22:21:13.997Z","createdAt":"2026-01-11T22:21:13.997Z"},{"id":"q-746","question":"Design a dynamic multiset of integers that supports insert(x), erase(x) (one occurrence), countInRange(l, r) for the number of elements in [l, r], and kthSmallestInRange(l, r, k) returning the k-th smallest value among elements in [l, r]. Aim for expected O(log n) updates and O(log n * log U) queries. Explain data structure, balance, and duplicates handling?","answer":"A dynamic 1D order-statistics multiset built on a balanced BST (e.g., a treap) stores duplicates with a per-node count and subtree size. insert/erase adjust counts and sizes in O(log n). rank(x) retur","explanation":"## Why This Is Asked\nTests mastery of advanced order-statistics with dynamic updates and range constraints, a common interview challenge for scalable data services.\n\n## Key Concepts\n- Self-balancing BST with duplicate handling\n- Augmented subtree sizes for rank/select\n- Range counting via rank differences\n- Binary search over value domain to locate kth within a range\n\n## Code Example\n```javascript\nclass Node{ constructor(key){ this.key=key; this.pr=Math.random(); this.cnt=1; this.sz=1; this.l=null; this.r=null; } }\n```\n\n## Follow-up Questions\n- How would you adapt to support range deletions efficiently?\n- Compare treap vs. red-black tree in this context.\n","diagram":null,"difficulty":"advanced","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Google","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-11T23:20:13.860Z","createdAt":"2026-01-11T23:20:13.860Z"},{"id":"q-749","question":"Design a dynamic weighted string multiset with insertWord(word, w), eraseWord(word, w) (duplicates allowed), sumByPrefix(prefix) returning total weight of words starting with prefix, and kthLargestWeightInPrefix(prefix, k) returning the k-th largest weight among those words. Outline the data structure, invariants, and expected complexities; discuss handling of long words and memory trade-offs?","answer":"Use a prefix trie where each node holds an order-statistics tree (multiset of weights) for all words in its subtree. insertWord/eraseWord traverse the word's path, updating every node's weight multise","explanation":"## Why This Is Asked\nTests the ability to combine a prefix data structure with an order-statistics structure to support prefix-scope queries and duplicate handling, plus memory/time trade-offs.\n\n## Key Concepts\n- Prefix trees with per-node multiset of weights\n- Order-statistics trees for kth queries\n- Duplicate handling and lazy deletions when needed\n\n## Code Example\n```javascript\nclass TrieNode {\n  constructor() {\n    this.children = new Map();\n    this.subWeightOST = new OST(); // hypothetical OST supporting insert, erase, kth\n    this.totalWeight = 0;\n  }\n}\n```\n\n## Follow-up Questions\n- How would you bound memory with very large word vocabularies?\n- How would you adapt for dynamic weight ranges or external weight normalization?","diagram":null,"difficulty":"intermediate","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","IBM"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T01:28:11.710Z","createdAt":"2026-01-12T01:28:11.710Z"},{"id":"q-761","question":"Design a dynamic word-frequency histogram for a text stream. Implement addWord(word) to increment frequency, eraseWord(word) to decrement (removing word when count hits zero), getFrequency(word), and topKWords(n) returning the n most frequent distinct words (ties broken lexicographically). Target average O(log m) per update and O(k log m) for topK, where m is the number of distinct words. Explain approach and data structures you would use?","answer":"Use two structures: a hashmap counts: word -> freq, and a map freq -> an ordered set of words. addWord(word) increments counts and moves word to the new freq bucket; eraseWord(word) decrements and re-","explanation":"## Why This Is Asked\nTests ability to design a dynamic, frequency-based data structure for text streams, common in search, analytics, and logs.\n\n## Key Concepts\n- Dual-map design: word→freq and freq→ordered words\n- Efficient updates by relocating words between buckets\n- Tie-breaking by lexicographic order\n- Corner cases: zero counts, topK when fewer distinct words\n\n## Code Example\n```javascript\nclass WordFreqDS {\n  constructor() {\n    this.counts = new Map();\n    this.buckets = new Map(); // freq -> Set<string>\n  }\n  _ensureBucket(freq) {\n    if (!this.buckets.has(freq)) this.buckets.set(freq, new Set());\n  }\n  addWord(word) {\n    const prev = this.counts.get(word) || 0;\n    if (prev > 0) this.buckets.get(prev).delete(word);\n    const next = prev + 1;\n    this.counts.set(word, next);\n    this._ensureBucket(next);\n    this.buckets.get(next).add(word);\n  }\n  eraseWord(word) {\n    const prev = this.counts.get(word);\n    if (!prev) return;\n    this.buckets.get(prev).delete(word);\n    const next = prev - 1;\n    if (next === 0) {\n      this.counts.delete(word);\n      return;\n    }\n    this.counts.set(word, next);\n    this._ensureBucket(next);\n    this.buckets.get(next).add(word);\n  }\n  getFrequency(word) {\n    return this.counts.get(word) || 0;\n  }\n  topKWords(n) {\n    const freqs = Array.from(this.buckets.keys()).sort((a,b)=>b-a);\n    const res = [];\n    for (const freq of freqs) {\n      const words = Array.from(this.buckets.get(freq)).sort();\n      for (const w of words) {\n        res.push(w);\n        if (res.length === n) return res;\n      }\n    }\n    return res;\n  }\n}\n```\n\n## Follow-up Questions\n- How would you scale for hundreds of millions of words with memory limits?\n- How would you adapt to streaming constraints where topK must be maintained continuously?","diagram":null,"difficulty":"beginner","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T03:50:19.466Z","createdAt":"2026-01-12T03:50:19.466Z"},{"id":"q-774","question":"Design a data structure to manage a dynamic multiset of weighted intervals on the real line. Each interval is [l, r] with weight w; duplicates allowed. Implement insertInterval(l, r, w), eraseInterval(l, r, w) (one occurrence), rangeSumAt(x) returning the total weight of all intervals covering point x, and kthLargestWeightAt(x, k) returning the k-th largest weight among intervals covering x. Target average O(log n) update and O(log n) query; discuss coordinate compression, memory trade-offs, and handling duplicates?","answer":"Propose a segment tree over compressed coordinates; each node stores a multiset of weights for intervals that fully cover that node’s segment. Insert pushes w into multisets along the path; erase remo","explanation":"## Why This Is Asked\nAssess ability to design interval-based data structures with dynamic updates and per-point queries, a non-trivial extension of range trees.\n\n## Key Concepts\n- Interval multiset maintenance\n- Segment tree with per-node multisets\n- Point-query across path for sum\n- Order statistics across unions\n- Coordinate compression and memory trade-offs\n\n## Code Example\n```javascript\nclass IntervalDS {\n  insert(l, r, w) {}\n  erase(l, r, w) {}\n  rangeSumAt(x) {}\n  kthLargestAt(x, k) {}\n}\n```\n\n## Follow-up Questions\n- How would you support range queries for multiple points efficiently?\n- Analyze worst-case vs average guarantees and propose optimizations.","diagram":"flowchart TD\n  A[Input intervals] --> B[Coordinate compression]\n  B --> C[Segment tree with multisets]\n  C --> D[Update: insert/erase]\n  D --> E[Query: rangeSumAt, kthLargestWeightAt]","difficulty":"advanced","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T05:28:21.928Z","createdAt":"2026-01-12T05:28:21.928Z"},{"id":"q-782","question":"Design a fully dynamic data structure to maintain a set of linear cost functions y = m_i x + b_i. Support: addLine(id, m, b), removeLine(id) (one occurrence per id), queryMin(x) returning the minimum cost at x across active lines, and queryKthMin(x, k) returning the k-th smallest cost at x. Domain x in [Xmin, Xmax]. Aim for near O(log X) per operation; discuss how deletions are handled, precision, and memory trade-offs?","answer":"Context: a pricing engine with many suppliers; each supplier defines a line y = m x + c. Implement a fully dynamic data structure supporting: addLine(id, m, c); removeLine(id); queryMin(x); queryKthMi","explanation":"## Why This Is Asked\n\nFully dynamic sets of lines appear in pricing engines and risk models. Deletions complicate classic Li Chao trees; this question tests a candidate's ability to adapt the structure to support removals and order-statistics.\n\n## Key Concepts\n\n- Li Chao segment tree for lines on a fixed x-domain\n- per-node multisets to support deletions\n- extending to kth smallest via an order-statistics structure or value-based binary search\n- handling 64-bit arithmetic and domain compression\n\n## Code Example\n\n```cpp\n// skeleton illustrating structure; actual implementation omitted for brevity\nstruct Line { long long m,b; long long value(long long x) const { return m*x + b; } };\nclass DynLineSet {\n  // addLine, removeLine, queryMin, queryKthMin\n};\n```\n\n## Follow-up Questions\n\n- How would you optimize memory for huge x-domains?\n- How would you ensure robustness against floating-point precision in boundary comparisons?","diagram":"flowchart TD\n  A[AddLine] --> B[Update per-node multisets]\n  B --> C[QueryMin(x)]\n  D[RemoveLine] --> B\n  C --> E[Return min value]\n","difficulty":"advanced","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","DoorDash","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T06:34:09.760Z","createdAt":"2026-01-12T06:34:09.760Z"},{"id":"q-790","question":"Design a persistent 2D dynamic weighted point data structure that supports insertPoint(x,y,w) and erasePoint(x,y,w) (duplicates allowed). Extend with rangeSum2D(x1,x2,y1,y2,version) and kthLargestInRectangle(x1,x2,y1,y2,k,version). Each insertion creates a new version; queries run in O(log^2 n) time. Explain coordinate compression, memory management, and how duplicates are handled across versions?","answer":"Propose a persistent 2D segment-tree: outer tree over compressed x; each node holds a persistent inner tree over compressed y storing total weight. On insert/erase, copy-on-write along O(log n) x-node","explanation":"## Why This Is Asked\nPersistence across versions for 2D range queries with duplicates is practical for time-travel analytics and audit.\n\n## Key Concepts\n- Persistent segment trees with path copying\n- 2D range queries via nested trees\n- Coordinate compression and versioning trade-offs\n- Handling duplicates in leaf weights\n\n## Code Example\n```javascript\nclass PSTNode {\n  constructor(sum=0, left=null, right=null) { this.sum=sum; this.left=left; this.right=right; }\n}\n// skeleton: updateX/Y and query would follow standard persistent path-copying\n```\n\n## Follow-up Questions\n- How would you bound memory growth in practice?\n- How would you support range updates (adding to a rectangle) efficiently?","diagram":null,"difficulty":"advanced","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Databricks","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T07:26:03.242Z","createdAt":"2026-01-12T07:26:03.242Z"},{"id":"q-801","question":"Design a fixed-window streaming data structure: push(x) appends an integer to the window; if the window exceeds size W, remove the oldest value. Implement getKthSmallest(k) to return the k-th smallest value in the current window in O(log W). Propose a concrete structure (e.g., an order-statistics tree with duplicates) and outline push/evict/getKthSmallest, including how duplicates and memory are handled?","answer":"Use a fixed-size deque for the window and an order-statistics tree that stores pairs (value, unique_id) to handle duplicates. Each node tracks subtree size; push(x) inserts (x, id++); if size>W, remov","explanation":"## Why This Is Asked\n\nTests combining streaming data with an order-statistics structure and correct duplicate handling in a practical window scenario. Evaluates dynamic memory management and per-operation costs in real-time workloads (e.g., metrics, logs).\n\n## Key Concepts\n\n- Sliding window mechanics with eviction\n- Order-statistics trees (duplicable keys via (value, id))\n- Rank queries and kth selection\n\n## Code Example\n\n```javascript\n// Skeleton: an OST with insert, erase by (value,id), and kth\nclass Node{ constructor(val,id){ this.val=val; this.id=id; this.left=this.right=null; this.cnt=1; this.sz=1; } }\n```\n\n## Follow-up Questions\n- How would you support changing window size W dynamically?\n- How would you extend to support getKthLargest or range queries?","diagram":null,"difficulty":"intermediate","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Plaid","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T08:33:06.592Z","createdAt":"2026-01-12T08:33:06.592Z"},{"id":"q-805","question":"Design a dynamic forest data structure supporting: addNode(id, value), link(childId, parentId) to attach a child under a parent, cut(childId) to detach a subtree, update(id, delta) to adjust a node's value, pathQuery(u, v) for the sum of values along the path from u to v, and subtreeQuery(u) for the sum of values in the subtree rooted at u. Target amortized O(log n) per operation. Which approach would you pick (Link-Cut Tree vs Euler Tour Tree), and how would you handle edge cases like root changes and duplicate node values?","answer":"Propose a link-cut tree (splay-based). Each node stores val, sum, and a reverse flag. Implement makerRoot, access, link, cut, and update. pathQuery(u,v) exposes the path and returns the path sum; subt","explanation":"## Why This Is Asked\nTests knowledge of dynamic trees and path/subtree aggregates under frequent structural changes, a practical requirement in scalable graph-backed workloads.\n\n## Key Concepts\n- Link-Cut Tree basics: makerRoot, access, splay for path queries.\n- Path vs subtree aggregates; how to maintain sums under reversals.\n- Trade-offs with Euler Tour Trees; when to prefer each.\n- Handling duplicates via unique node IDs and careful memory management.\n- Edge cases: re-rooting, linking a node that already has a parent, cutting roots.\n\n## Code Example\n```java\n// skeleton: Link-Cut Tree node and core ops\nclass Node { long val, sum; boolean rev; Node left, right, parent; int id; }\n```\n\n## Follow-up Questions\n- How would you extend to support min/max path queries?\n- How would you verify amortized guarantees in a real system?","diagram":"flowchart TD\n  A[makerRoot(u)] --> B[access(u)]\n  B --> C[expose path to v]\n  C --> D[pathQuery]\n  E[link(child,parent)] --> F[cut(child)]","difficulty":"advanced","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","Meta","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T09:31:16.011Z","createdAt":"2026-01-12T09:31:16.011Z"},{"id":"q-815","question":"Design a dynamic data structure for a bipartite graph with left indices [1..N] and right indices [1..M]. Each edge (u,v) has weight w; duplicates allowed. Support: - addEdge(u,v,w), eraseEdge(u,v,w) (one occurrence) - rangeSum(uL,uR,vL,vR) total weight of edges with u in [uL,uR] and v in [vL,vR] - kthLargestEdgeWeight(uL,uR,vL,vR,k) the k-th largest edge weight in that submatrix. Aim for ~O(log N log M) per operation; discuss compression, duplicates, and memory trade-offs?","answer":"Use a 2D segment tree over left and right indices. Each node stores: a weight-sum and an order-statistics structure (weighted multiset) over edge weights to answer kthLargest via binary search on a co","explanation":"## Why This Is Asked\nAssesses dynamic 2D range queries with multiplicities, a core DS topic at scale. \n\n## Key Concepts\n- 2D segment tree with per-node weight aggregation\n- Order-statistics on multisets for kthLargest\n- Coordinate compression for weights and indices\n- Handling duplicates via multiplicity counts\n\n## Code Example\n```javascript\n// skeleton for 2D segtree node\nclass Node2D {\n  constructor() {\n    this.sum = 0;\n    this.weightCounts = new Map(); // weight -> count\n  }\n}\n```\n\n## Follow-up Questions\n- How would you adapt to dynamic N and M (growing graphs)?\n- What are the trade-offs vs. a Fenwick-of-Fenwick approach for very large sparse graphs?","diagram":"flowchart TD\n  A[Left Range] --> B[2D Segment Tree]\n  B --> C[RangeSum Query]\n  B --> D[KthLargestQuery]\n  C --> E[Sum of weights]\n  D --> F[Select by weight rank]","difficulty":"intermediate","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","NVIDIA","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T10:25:42.656Z","createdAt":"2026-01-12T10:25:42.656Z"},{"id":"q-820","question":"Design a dynamic sequence structure 'RopeArray' storing an integer array supporting insertAt(i, val), eraseAt(i), rangeSum(l, r), rangeMax(l, r), and getAt(i). How would you implement it to achieve average O(log n) per operation using a balanced tree with implicit keys and augmented fields (size, sum, max), and how would you test with an example sequence?","answer":"Use an implicit-key treap (rope) where each node stores val, priority, size, sum, and max. insertAt(i, v) splits at i, inserts a new node, and merges. eraseAt(i) isolates i, discards it, and merges. r","explanation":"## Why This Is Asked\nTests ability to implement a mutable sequence with range queries using a balanced BST with implicit indices.\n\n## Key Concepts\n- Implicit-key treap\n- Split/merge by position\n- Subtree aggregates: size, sum, max\n- Handling duplicates as separate nodes\n- Correct re-linking during splits/merges\n\n## Code Example\n```javascript\n// Implementation skeleton\nclass Node { constructor(val){ this.val = val; this.pr = Math.random(); this.l = null; this.r = null; this.sz = 1; this.sum = val; this.mx = val; } }\nfunction sz(n){ return n? n.sz:0; }\nfunction upd(n){ if(!n) return; n.sz = 1 + sz(n.l) + sz(n.r); n.sum = n.val + (n.l? n.l.sum:0) + (n.r? n.r.sum:0); n.mx = Math.max(n.val, n.l? n.l.mx:-Infinity, n.r? n.r.mx:-Infinity); }\nfunction split(root, leftSize){ /* ... */ }\nfunction merge(a,b){ /* ... */ }\n// ... core operations omitted for brevity\n```\n\n## Follow-up Questions\n- Extend with rangeAdd(l,r,delta) using lazy propagation.\n- How would you implement persistence for versioned queries?","diagram":"flowchart TD\n  A[Operate] --> B{Split by index}\n  B --> C[Isolate segment or insert]\n  C --> D[Merges back]\n  D --> E[Read aggregates]\n","difficulty":"intermediate","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T11:19:02.973Z","createdAt":"2026-01-12T11:19:02.973Z"},{"id":"q-832","question":"Design a data structure to maintain a dynamic multiset of words with three operations: insertWord(word), eraseWord(word) (one occurrence), and getAnagrams(word) that returns all current words that are anagrams of the given word (including duplicates). Explain how you would store the words, how to compute the canonical signature, and the expected time complexity for updates and queries. For example, after insertWord('listen') and insertWord('silent'), getAnagrams('tinsel') should return ['listen','silent']?","answer":"Store words by signature (the sorted letters of the word). Maintain a signature -> map<word, count> bucket. insertWord(word) computes signature S, increments count for word in bucket S; eraseWord(word","explanation":"## Why This Is Asked\nTests structuring a dynamic, duplicate-friendly grouping based on a stable signature.\n\n## Key Concepts\n- Canonical form via sorted letters for anagram grouping\n- Nested maps to track per-word counts within a signature bucket\n- Amortized update and linear-time retrieval relative to output size\n\n## Code Example\n```javascript\nclass AnagramMultiset {\n  constructor(){ this.sigMap = new Map(); }\n  _sig(w){ return w.split('').sort().join(''); }\n  insert(word){ const s = this._sig(word); const bucket = this.sigMap.get(s) || new Map(); bucket.set(word, (bucket.get(word) || 0) + 1); this.sigMap.set(s, bucket); }\n  erase(word){ const s = this._sig(word); const bucket = this.sigMap.get(s); if(!bucket) return; const c = (bucket.get(word) || 0) - 1; if(c<=0){ bucket.delete(word); if(bucket.size===0) this.sigMap.delete(s);} else { bucket.set(word, c); } }\n  getAnagrams(word){ const s = this._sig(word); const bucket = this.sigMap.get(s); if(!bucket) return []; const res = []; for(const [w,c] of bucket) for(let i=0;i<c;i++) res.push(w); return res; }\n}\n```\n\n## Follow-up Questions\n- How would you handle Unicode normalization and case-insensitive comparisons?\n- What changes if you also need to support a deleteAll(word) operation?","diagram":"flowchart TD\n  A[Start] --> B[Compute signature]\n  B --> C{Update bucket}\n  C -->|insert| D[Increment word count]\n  C -->|erase| E[Decrement word count]\n  D & E --> F[Get anagrams from signature bucket]\n","difficulty":"beginner","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["NVIDIA","Snowflake","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T12:43:44.827Z","createdAt":"2026-01-12T12:43:44.827Z"}],"subChannels":["general"],"companies":["Adobe","Airbnb","Amazon","Anthropic","Apple","Bloomberg","Citadel","Cloudflare","Databricks","Discord","DoorDash","Google","Hashicorp","Hugging Face","IBM","Lyft","Meta","Microsoft","NVIDIA","Netflix","OpenAI","Oracle","PayPal","Plaid","Robinhood","Scale Ai","Slack","Snap","Snowflake","Square","Tesla","Twitter","Two Sigma"],"stats":{"total":28,"beginner":7,"intermediate":9,"advanced":12,"newThisWeek":28}}