{"questions":[{"id":"q-1038","question":"Design a stack that supports push(x), pop(), top(), and getMin() in O(1) time per operation. Duplicates allowed. Explain the approach, discuss invariants and space usage, and provide a compact code sketch (Python or Java)?","answer":"Use two stacks: a main stack for values and a minStack to track the current minimum. On push(x): push x to main and push min(x, minStack.top) to minStack (or x if minStack empty). On pop: pop both. to","explanation":"## Why This Is Asked\nTests understanding of maintaining auxiliary structures to achieve O(1) operations and how to handle duplicates and edge cases.\n\n## Key Concepts\n- Two-stack technique\n- Invariants: minStack.top always <= all elements on main below\n- Duplicate handling\n- Space vs. time trade-offs\n\n## Code Example\n```javascript\nclass MinStack {\n  constructor() {\n    this.stack = [];\n    this.minStack = [];\n  }\n  push(x) {\n    this.stack.push(x);\n    const m = this.minStack.length ? Math.min(x, this.minStack[this.minStack.length - 1]) : x;\n    this.minStack.push(m);\n  }\n  pop() {\n    if (!this.stack.length) return;\n    this.stack.pop();\n    this.minStack.pop();\n  }\n  top() {\n    return this.stack[this.stack.length - 1];\n  }\n  getMin() {\n    return this.minStack[this.minStack.length - 1];\n  }\n}\n```\n\n## Follow-up Questions\n- How would you adapt this to support getMin() on a persistent stack?\n- How would you test boundary cases (empty pop, getMin on empty)?","diagram":"flowchart TD\n  S[Stack] --> P[Push]\n  S --> O[Pop]\n  S --> T[Top]\n  S --> M[GetMin]","difficulty":"beginner","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Databricks","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T20:26:41.071Z","createdAt":"2026-01-12T20:26:41.071Z"},{"id":"q-1213","question":"Design a dynamic autocomplete data structure for a code search tool that stores terms with frequencies; implement insertWord(word), eraseWord(word), and querySuggestions(prefix, k) returning up to k completions starting with prefix, ordered by descending frequency then lexicographically. Discuss a Trie-based design with per-node top-k structures and update costs?","answer":"Use a Trie where every node holds a balanced BST of words sharing that prefix, ordered by (−freq, word). On insertWord(word), bump freq and update all nodes along the path; on eraseWord, decrement and","explanation":"## Why This Is Asked\nAutocompletion is common in code search tools at scale; dynamic updates require keeping accurate frequencies while still returning top suggestions quickly.\n\n## Key Concepts\n- Trie with per-node sorted structures to support prefix queries\n- Dynamic frequency maintenance on insert/erase\n- Trade-offs: memory vs. update/query time; handling duplicates\n\n## Code Example\n```javascript\nclass TrieNode {\n  constructor() {\n    this.children = new Map();\n    // word -> freq for this prefix\n    this.words = new Map();\n  }\n}\nclass Autocomplete {\n  constructor(){ this.root = new TrieNode(); }\n  insertWord(word){ /* update freq and all prefix nodes */ }\n  eraseWord(word){ /* decrement and prune zeros */ }\n  query(prefix, k){ /* traverse to node and return top-k */ }\n}\n```\n\n## Follow-up Questions\n- How would you optimize memory for large vocabularies?\n- How would you adapt for multi-language or unicode support?\n- How would you test correctness and performance under bursts of updates?","diagram":null,"difficulty":"beginner","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","Hugging Face"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T05:26:07.065Z","createdAt":"2026-01-13T05:26:07.065Z"},{"id":"q-2010","question":"Design a dynamic session tracker: each session has an id, a score, and a lastActive timestamp. Support insertSession(id, score, ts), updateScore(id, delta), expireSessionsBefore(ts) removing expired sessions, topK(window, k) returning the k highest scores among active sessions (active = ts >= now - window) breaking ties by id, and medianScore(window) returning the median score among active sessions. Target near O(log n) per update and O(k log n) for topK. Explain data structures and tradeoffs?","answer":"Use a balanced BST keyed by (score, id) for fast top-k and median, plus a hash map id→node for O(1) updates, and a queue to purge expired sessions by lastActive ts. On insert/update: create/move node ","explanation":"## Why This Is Asked\nThe problem tests ability to design a time-windowed data structure that supports dynamic updates and queries that depend on recent activity, a common pattern in analytics systems.\n\n## Key Concepts\n- Sliding window constraints over timestamps\n- Order-statistics on a dynamic set\n- Lazy deletion/tombstones for expirations\n- Balanced BST or similar for O(log n) updates\n\n## Code Example\n```javascript\nclass SessionDS {\n  constructor() {\n    this.map = new Map(); // id -> node\n  }\n  insertSession(id, score, ts) { /* ... */ }\n  updateScore(id, delta) { /* ... */ }\n  expireSessionsBefore(ts) { /* ... */ }\n  topK(window, k) { /* ... */ }\n  medianScore(window) { /* ... */ }\n}\n```\n\n## Follow-up Questions\n- How would you handle very large k (near n)?\n- How would you scale to multiple concurrent windows (per-region analytics)?","diagram":null,"difficulty":"intermediate","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T20:48:50.928Z","createdAt":"2026-01-14T20:48:50.928Z"},{"id":"q-2087","question":"Design a dynamic leaderboard data structure to support a game with many players: addOrUpdatePlayer(playerId, score), removePlayer(playerId), getTopK(k), and getPlayerRank(playerId). Achieve O(log n) updates and O(log n + k) to fetch the top-k. Explain how you handle duplicates (equal scores) and score changes?","answer":"Maintain two complementary structures: a hash map `playerToScore` mapping `playerId -> score`, and a tree map `scoreToPlayers` mapping `score -> ordered set of players`. For `addOrUpdatePlayer`: remove the player from their old score set (if exists), update both maps, and insert into the new score set. For `removePlayer`: delete from both maps. For `getTopK`: iterate descending through tree map keys, collecting players until reaching k. For `getPlayerRank`: count total players in higher score sets plus the player's offset within their current score set.","explanation":"## Why This Is Asked\nTests practical data structure design for a common gaming feature, balancing efficient updates with query performance.\n\n## Key Concepts\n- Hash maps provide O(1) player score lookups\n- Ordered maps enable range and top-k access\n- Ordered sets handle duplicate scores within groups\n- Rank computation accounts for tied scores\n\n## Code Example\n```javascript\n// Pseudocode illustrating approach\nclass Leaderboard {\n  constructor(){\n    this.playerToScore = new Map();\n    this.scoreToPlayers = new Map(); // descending keys; each maps to a Set of players\n  }\n  addOrUpdate(playerId, score) {\n    // Remove from old score group if exists\n    const oldScore = this.playerToScore.get(playerId);\n    if (oldScore !== undefined) {\n      const oldSet = this.scoreToPlayers.get(oldScore);\n      oldSet.delete(playerId);\n      if (oldSet.size === 0) this.scoreToPlayers.delete(oldScore);\n    }\n    // Update maps and add to new score group\n    this.playerToScore.set(playerId, score);\n    if (!this.scoreToPlayers.has(score)) {\n      this.scoreToPlayers.set(score, new Set());\n    }\n    this.scoreToPlayers.get(score).add(playerId);\n  }\n}\n```","diagram":null,"difficulty":"beginner","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Lyft","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T05:04:02.750Z","createdAt":"2026-01-14T23:37:38.493Z"},{"id":"q-2178","question":"Design a dynamic prefix-aware dictionary that stores strings with counts (duplicates allowed). Implement: addWord(s), removeWord(s) removing one occurrence, queryPrefixCount(p) giving how many distinct words start with p, and kthWordWithPrefix(p, k) returning the k-th word in lexicographic order among words beginning with p. Handle up to 2e5 words, optimize memory and updates; propose a data layout (e.g., compressed Trie) and discuss trade-offs and edge cases?","answer":"Approach: a compressed Trie (radix) where each node stores: sorted children, endCount, subCount. addWord(s): walk along s, create nodes as needed, increment endCount at terminal, and update subCount a","explanation":"## Why This Is Asked\nTests designing a mutable, prefix-aware dictionary with lexicographic kth queries and duplicates.\n\n## Key Concepts\n- Trie augmentation with subtree counts to support prefix queries and kth ordering\n- Duplicate handling via endCount and pruning when zero\n- Path-based updates for add/remove with amortized linear in word length\n- Memory vs. speed trade-offs in a compressed trie\n\n## Code Example\n```javascript\n// sketch of operations (high-level, not full implementation)\nclass Node { constructor() { this.children = new Map(); this.end = 0; this.sub = 0; } }\nclass PrefixDict { /* addWord, removeWord, queryPrefixCount, kthWordWithPrefix */ }\n```\n\n## Follow-up Questions\n- How would you extend to support wildcard prefixes or case-insensitive matching?\n- How would you adapt for large alphabets or Unicode inputs?","diagram":"flowchart TD\n  Trie[Radix Trie] --> Add[addWord]\n  Trie --> Remove[removeWord]\n  Trie --> Prefix[queryPrefixCount]\n  Trie --> Kth[kthWordWithPrefix]","difficulty":"advanced","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Google","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T06:46:54.915Z","createdAt":"2026-01-15T06:46:54.915Z"},{"id":"q-2324","question":"Design a dynamic bitset over a universe [1, 10^9] that supports set(l, r), reset(l, r), flip(l, r), sum(l, r), and kthOne(l, r, k). Use a run-length encoded interval map to achieve near O(log M) updates, where M is number of stored intervals. Explain data structure, edge cases, and complexity; provide a robust approach for sparse updates in large N?","answer":"Use a run-length encoded set of disjoint intervals with bits. Maintain an ordered map of [L, R, bit]. For set(l,r), split at l and r+1, erase overlapping blocks, insert [l,r,1], and merge neighbors wi","explanation":"## Why This Is Asked\nTests ability to design a compact, scalable bitset for massive domains with dynamic range updates.\n\n## Key Concepts\n- Run-length encoding of intervals; - Split/merge semantics; - Range queries; - kthOne via prefix accumulation.\n\n## Code Example\n```python\nclass RunBitset:\n    def __init__(self):\n        self.blocks = []  # list of [L, R, val]\n    def _split_at(self, x):\n        pass  # split blocks so a boundary starts at x\n```\n\n## Follow-up Questions\n- How to implement on a language with immutable maps efficiently?\n- How to adapt for 64-bit ranges and memory guarantees?","diagram":"flowchart TD\n  A[Start] --> B[Split at L and R+1]\n  B --> C[Replace overlapped blocks]\n  C --> D[Merge adjacent blocks]\n  D --> E[Compute sum/kthOne]\n","difficulty":"advanced","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Google","Hugging Face"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T13:01:30.477Z","createdAt":"2026-01-15T13:01:30.479Z"},{"id":"q-2356","question":"Design a dynamic string editor data structure that supports insert at position, delete substring, and substring hash queries with O(log n) edits and O(log n) hash lookups. Propose a rope-based solution using an implicit treap, where each node stores size and a double rolling hash (mod1, mod2). Explain split/merge, hash maintenance, collision mitigation, and memory trade-offs with a realistic editor workload?","answer":"Adopt an implicit treap (rope) where each node stores its subtree size and a double rolling hash (mod1, mod2). For insert/delete at position, split at pos, merge with the new string; for hash of [l,r]","explanation":"## Why This Is Asked\n\nDynamic string editing with fast substring queries is critical in modern editors and versioned docs. A rope with an implicit treap provides randomized balance and localized updates, enabling logarithmic edits while maintaining per-substring hashes for quick comparisons.\n\n## Key Concepts\n\n- Implicit treap (rope) structure for sequence storage\n- Subtree size and rolling hash maintenance\n- Polynomial rolling hash with two moduli to reduce collisions\n- Split and merge operations for positional edits\n- Substring hash isolation without copying strings\n\n## Code Example\n\n```javascript\nclass Node {\n  constructor(ch) {\n    this.c = ch;\n    this.pr = Math.random();\n    this.left = null; this.right = null;\n    this.sz = 1;\n    this.h1 = ch.charCodeAt(0);\n    this.h2 = ch.charCodeAt(0);\n  }\n}\n// Update size and hashes from children\nfunction upd(n){ if(!n) return; n.sz = 1; n.h1 = n.c.charCodeAt(0); n.h2 = n.c.charCodeAt(0);\n  if(n.left){ n.sz += n.left.sz; n.h1 = n.left.h1 + somePow1; n.h2 = n.left.h2 + somePow2; }\n  if(n.right){ n.sz += n.right.sz; n.h1 = n.h1 + rightHash1; n.h2 = n.h2 + rightHash2; }\n}\nfunction split(t, k){ // split first k chars to a\n  if(!t) return [null, null];\n  if(size(t.left) >= k){ const [l, r] = split(t.left, k); t.left = r; upd(t); return [l, t]; }\n  const [l, r] = split(t.right, k - size(t.left) - 1); t.right = l; upd(t); return [t, r];\n}\nfunction merge(a, b){ if(!a||!b) return a||b; if(a.pr < b.pr){ a.right = merge(a.right, b); upd(a); return a; } else { b.left = merge(a, b.left); upd(b); return b; } }\n```\n\n## Follow-up Questions\n\n- How would you extend for Unicode normalization? \n- How to handle concurrent edits in a collaborative editor? \n- Can you quantify memory overhead and compare with a pure block-based rope?","diagram":"flowchart TD\n  A[Client edits text] --> B[Split rope at pos]\n  B --> C[Insert/delete substring]\n  C --> D[Isolate substring hash]\n  D --> E[Merge rope back]\n  E --> F[Return hash / updated state]","difficulty":"advanced","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Citadel","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T14:42:37.775Z","createdAt":"2026-01-15T14:42:37.775Z"},{"id":"q-2503","question":"Design a lightweight 1D histogram data structure for values in [0, 1_000_000] that supports: insertValue(v), eraseValue(v) (one occurrence), rangeCount(l, r) counting how many values lie in [l, r], and kthValueInRange(l, r, k) returning the k-th smallest value within [l, r]. Aim for O(log M) per operation. Compare Fenwick vs segment-tree-of-frequencies and discuss handling of duplicates and memory?","answer":"Approach: implement a segment tree over [0, 1,000,000], each node stores the count of values in its interval. insertValue(v) and eraseValue(v) update along the path in O(log M). rangeCount(l, r) sums ","explanation":"## Why This Is Asked\nA practical, beginner-friendly take on range queries with duplicates, focusing on a simple, real-world histogram use-case.\n\n## Key Concepts\n- Segment tree for counts over a fixed value domain\n- Range sum and order-statistics queries within a range\n- Handling duplicates and memory through implicit nodes\n\n## Code Example\n```javascript\nclass Hist1D {\n  constructor(M = 1000000) {\n    this.M = M;\n    this.root = null; // implicit segtree node\n  }\n  insertValue(v) { /* update path */ }\n  eraseValue(v) { /* update path */ }\n  rangeCount(l, r) { /* range sum */ }\n  kthValueInRange(l, r, k) { /* descend to kth */ }\n}\n```\n\n## Follow-up Questions\n- How would you adapt to a much larger value range (e.g., 1e9)?\n- How would you support dynamic resizing without rebuilding?","diagram":null,"difficulty":"beginner","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["OpenAI","Robinhood","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T20:45:14.324Z","createdAt":"2026-01-15T20:45:14.324Z"},{"id":"q-2666","question":"Design a dynamic string-frequency dictionary that stores strings with counts. Implement: addString(s), removeString(s) (one occurrence), and queryTopK(prefix p, k) that returns the k most frequent strings starting with p, ordered by frequency descending and then lexicographically. Up to 2e5 total insertions. Provide a practical approach and discuss time/memory trade-offs; keep it beginner-friendly?","answer":"Approach: keep a HashMap<String,Integer> for counts. To support top-k by prefix, use a Trie where each node maintains a small top-k heap keyed by (count, string). On addString/removeString, update the","explanation":"## Why This Is Asked\nTests ability to combine basic maps with a prefix-based lookup and top-k retrieval, a common real-world need in autocomplete/deduplication.\n\n## Key Concepts\n- Hash maps for counts\n- Prefix search with Trie\n- Local top-k maintenance via per-node heaps\n- Handling duplicates and updates\n\n## Code Example\n```javascript\nclass TrieNode{constructor(){this.next=new Map();this.top=[];this.endCount=0}}\nclass Trie{constructor(){this.root=new TrieNode()}\n add(s){let node=this.root; for(let ch of s){if(!node.next.has(ch)) node.next.set(ch,new TrieNode()); node=node.next.get(ch); // update end? }\n // simplified: later rebuild top lists\n }\n}\n```\n\n## Follow-up Questions\n- How would you handle memory pressure on the per-node heaps?\n- Could you implement a lazy rebuild strategy for the top-k lists? ","diagram":"flowchart TD\nA[Start] --> B[Insert/Remove string]\nB --> C[Update counts map]\nC --> D[Propagate to Trie nodes]\nD --> E[Maintain top-k in node heaps]\nE --> F[QueryTopK(prefix, k)]\nF --> G[Return results]","difficulty":"beginner","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Netflix","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T05:49:14.184Z","createdAt":"2026-01-16T05:49:14.184Z"},{"id":"q-2725","question":"Design a stack that supports push(x), pop(), top(), and getMin() in O(1) time. Duplicates allowed. Describe the two-stack approach that maintains the current minimum on each push, how pops update both stacks, and how to handle empty state. Include a short example sequence and expected min values?","answer":"Two-stack approach: maintain a main stack for values and a min stack for the current minimum at each depth. On push(x): push x to main and push min(x, mins.top()) to mins (or x if mins empty). On pop(","explanation":"## Why This Is Asked\n\nTests ability to design a simple data structure with guaranteed O(1) operations and handle edge cases like duplicates and emptiness.\n\n## Key Concepts\n\n- Two-stack technique to keep current minimum per depth\n- Constant-time top and getMin\n- Correct empty-state handling and error signaling\n\n## Code Example\n\n```javascript\nclass MinStack {\n  constructor() {\n    this.stack = [];\n    this.mins = [];\n  }\n  push(x) {\n    this.stack.push(x);\n    if (this.mins.length === 0) {\n      this.mins.push(x);\n    } else {\n      this.mins.push(Math.min(x, this.mins[this.mins.length - 1]));\n    }\n  }\n  pop() {\n    if (this.stack.length === 0) throw new Error('empty');\n    this.stack.pop();\n    this.mins.pop();\n  }\n  top() {\n    if (this.stack.length === 0) throw new Error('empty');\n    return this.stack[this.stack.length - 1];\n  }\n  getMin() {\n    if (this.mins.length === 0) throw new Error('empty');\n    return this.mins[this.mins.length - 1];\n  }\n}\n```\n\n## Follow-up Questions\n\n- How would you extend to also support getMax() efficiently? \n- How would you optimize memory if input values are highly repetitive? \n","diagram":"flowchart TD\n  A[Push(x)] --> B[MainStack.push(x)]\n  A --> C[MinStack.push(min(x, MinStack.top()))]\n  D[Pop()] --> E[MainStack.pop(), MinStack.pop()]\n  F[GetMin()] --> G[MinStack.top()]","difficulty":"beginner","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Lyft","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T09:38:12.669Z","createdAt":"2026-01-16T09:38:12.669Z"},{"id":"q-2919","question":"Design a dynamic priority queue for a real-time task scheduler: supports insertJob(id, priority, createdAt), updatePriority(id, newPriority), removeJob(id), popNext(), and pruneOld(now, maxAge). Tie-breaking: oldest createdAt. Aim for near O(log n) per operation. Explain data structures (augmented heap + hashmap), lazy deletion, edge cases, and memory trade-offs. Scenario: high-scale service processing user requests with varying priorities?","answer":"Use a max-heap augmented with id->node map. Each node stores priority, createdAt, index, and a valid flag. insertJob adds; updatePriority adjusts by sift up/down; removeJob marks invalid and deletes f","explanation":"## Why This Is Asked\nDemonstrates building a dynamic priority queue that supports updates and pruning of stale items, a common real-time scheduler primitive. Emphasizes achieving near O(log n) updates with a heap plus hashmap, and how lazy deletion affects complexity and memory.\n\n## Key Concepts\n- Max-heap with O(log n) inserts and pops\n- id -> node map for O(1) lookups\n- Lazy deletion to handle removals and updates\n- Tie-breaking by createdAt for deterministic ordering\n\n## Code Example\n```javascript\nclass DynPQ {\n  constructor(){ this.heap=[]; this.pos=new Map(); }\n  insert(id, p, t){ /* ... */ }\n  updatePriority(id, newP){ /* ... */ }\n  remove(id){ /* ... */ }\n  popNext(){ /* ... */ }\n  pruneOld(now, maxAge){ /* ... */ }\n}\n```\n\n## Follow-up Questions\n- How would you handle concurrent updates in a multi-threaded environment?\n- How would you adapt for multiple queues or workers with priority inversion safeguards?","diagram":"flowchart TD\n  A[Max-Heap] --> B[Id->Node Map]\n  B --> C{update/remove}\n  C --> D[Sift Up/Down]\n  C --> E[PopNext cleans invalids]\n  F[PruneOld] --> D","difficulty":"intermediate","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T17:41:33.018Z","createdAt":"2026-01-16T17:41:33.018Z"},{"id":"q-2956","question":"Design a dynamic 3D weighted point set that supports insertPoint(x,y,z,w), erasePoint(x,y,z,w) (duplicates allowed), movePoint(oldX,oldY,oldZ,newX,newY,newZ) to relocate one occurrence, rangeSum3D(x1,x2,y1,y2,z1,z2) for the total weight in the axis-aligned 3D box, and kthLargestIn3DBox(x1,x2,y1,y2,z1,z2,k) for the k-th largest weight inside that box. Target average O(log^3 N) per operation; discuss coordinate compression, memory trade-offs, and handling duplicates and moves?","answer":"Implement a dynamic 3D weighted point set with insertPoint, erasePoint, movePoint, rangeSum3D, and kthLargestIn3DBox. Compress coordinates in x,y,z; use a 3D Fenwick tree where each node stores a weig","explanation":"## Why This Is Asked\nTests mastery of high-dimension dynamic data structures and mix of update/query workloads common in advanced systems.\n\n## Key Concepts\n- 3D Fenwick/segment tree for sums\n- coordinate compression across x, y, z\n- per-node order statistics with duplicates\n- update by erase+insert for moves\n\n## Code Example\n```cpp\n// placeholder skeleton illustrating conceptual BIT3D with per-node OST\n```\n\n## Follow-up Questions\n- How would you reduce memory for sparse 3D spaces?\n- How would you add persistence and concurrent updates safely?","diagram":null,"difficulty":"advanced","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Google","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T18:58:47.646Z","createdAt":"2026-01-16T18:58:47.646Z"},{"id":"q-2969","question":"Design a dynamic histogram for integers in a large domain (e.g., 0..1,000,000). Implement addValue(x, delta), removeValue(x, delta), and queryQuantile(q) returning the smallest x with cumulative frequency ≥ q. Also support mergeHistogram(other). Explain coordinate compression, O(log n) updates, and memory trade-offs between sparse and dense representations. How would you implement this, including data structures and edge cases?","answer":"Use a Fenwick tree on compressed coordinates. addValue/removeValue update in O(log n). queryQuantile(q) uses binary lifting on the Fenwick to find the smallest index with prefix sum ≥ q in O(log n). F","explanation":"## Why This Is Asked\nTests understanding of order-statistics structures, coordinate compression, and practical trade-offs for large domains in a beginner-friendly way.\n\n## Key Concepts\n- Fenwick tree (BIT) for prefix sums\n- Coordinate compression to handle sparse domains\n- kth-order statistics via prefix sums\n- Merge strategy: small-to-large to reduce work\n\n## Code Example\n```javascript\nclass Fenwick {\n  constructor(n) {\n    this.n = n;\n    this.f = new Array(n + 1).fill(0);\n  }\n  add(i, delta) {\n    for (; i <= this.n; i += i & -i) this.f[i] += delta;\n  }\n  sum(i) {\n    let s = 0;\n    for (; i > 0; i -= i & -i) s += this.f[i];\n    return s;\n  }\n  kth(k) {\n    let idx = 0;\n    let bit = 1;\n    while ((bit << 1) <= this.n) bit <<= 1;\n    for (let d = bit; d > 0; d >>= 1) {\n      const next = idx + d;\n      if (next <= this.n && this.f[next] < k) {\n        idx = next;\n        k -= this.f[next];\n      }\n    }\n    return idx + 1;\n  }\n}\n```\n\n## Follow-up Questions\n- How would you handle domain expansion?\n- How would you implement mergeHistogram efficiently?\n","diagram":"flowchart TD\n  A[addValue(x, delta)] --> B[update Fenwick]\n  B --> C[queryQuantile(q)]\n  A --> D[removeValue(x, delta)]\n  D --> B","difficulty":"beginner","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Lyft","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T19:31:52.966Z","createdAt":"2026-01-16T19:31:52.966Z"},{"id":"q-3061","question":"Design a dynamic data structure for a set of 2D colored points that supports: insertPoint(x,y,color), erasePoint(x,y,color) (duplicates allowed), and countDistinctColorsInRect(x1,y1,x2,y2) returning how many unique colors appear in the rectangle. Optimize for average O(log^2 n) per op; discuss coordinate compression, per-node color maps, and duplicate handling?","answer":"Use a two-level dynamic range structure: a segment tree over x-coordinates, where each node stores a balanced BST keyed by y-coordinates with a multiset of colors. Insert/erase operations update O(log n) x-nodes and O(log n) y-positions, achieving O(log² n) average time complexity. For coordinate compression, maintain dynamic mapping of actual coordinates to compressed indices, rebuilding when the coordinate space grows significantly. Each segment tree node tracks distinct colors using a hash map with color counts, enabling O(1) distinct color queries per node.","explanation":"## Why This Is Asked\nThis question evaluates a candidate's ability to design hybrid data structures that support dynamic 2D range queries with an additional color dimension, a common requirement in spatial analytics and geographic information systems.\n\n## Key Concepts\n- Dynamic 2D range structures (segment trees, range trees)\n- Per-node multisets for color tracking and distinct counting\n- Coordinate compression for memory efficiency\n- Duplicate handling via color frequency counters\n- Trade-offs between memory usage and query performance\n- Lazy rebuilding strategies for coordinate space optimization\n\n## Code Example\n```javascript\n// Sketch: insertPoint updates across O(log n) x-nodes,\n// adjusting per-node y-color maps and distinct color counters\nfunction insertPoint(x, y, color) {\n  const compressedX = compressCoordinate(x);\n  updateSegmentTree(compressedX, y, color, 1);\n}\n```\n\n## Follow-up Considerations\n- How would you handle range queries for specific colors?\n- What optimizations could reduce the O(log² n) complexity?\n- How would you implement batch operations efficiently?","diagram":"flowchart TD\n  A[Segment Tree over x] --> B[Node: y-tree with color multisets]\n  B --> C[Insert/Erase: update nodes]\n  A --> D[Query: visit O(log n) x-nodes and merge color sets]\n  D --> E[Return distinct color count]","difficulty":"advanced","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Google","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T04:08:40.972Z","createdAt":"2026-01-16T23:31:15.717Z"},{"id":"q-3195","question":"Design a persistent 2D weighted point set that supports insertPoint(x,y,w) and erasePoint(x,y,w) (duplicates allowed) creating new versions, rangeSum4D(x1,x2,y1,y2,t) for version t, and kthLargestInRectInVersion(x1,x2,y1,y2,t,k) for that version; aim for O(log^2 N) per op; discuss coordinate compression, persistence strategy, and memory trade-offs?","answer":"Leverage coordinate compression for x and y; build a persistent 2D segment tree where each point update copies O(log N) outer nodes and O(log N) inner nodes. Each node stores a summed weight. rangeSum","explanation":"## Why This Is Asked\nTests persistence, 2D range queries, and order statistics under dynamic updates.\n\n## Key Concepts\n- Persistence via path copying\n- 2D segment trees with coordinate compression\n- Order statistics on a multiset within rectangles\n- Memory-time trade-offs under updates and queries\n\n## Code Example\n```javascript\n// Skeleton: updateX(rootX, xL,xR, x, y, w, delta, version)\n```\n\n## Follow-up Questions\n- How would you adapt to dynamic coordinate ranges without full compression?\n- Could you replace inner trees with a Fenwick-of-segments for smaller memory?","diagram":null,"difficulty":"advanced","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Apple","Lyft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T06:45:51.799Z","createdAt":"2026-01-17T06:45:51.800Z"},{"id":"q-3282","question":"Design a dynamic 2D dominance index for weighted points. Each point is (x,y,w,id) where id differentiates duplicates. Supports: insertPoint(x,y,w,id), erasePoint(x,y,w,id) removing one occurrence, queryDominatedCount(x0,y0) returning the number of points with x<=x0 and y<=y0, and queryKthLargestDominated(x0,y0,k) returning the k-th largest weight among dominated points. Aim for O(log^2 N) updates and O(log^2 N log W) kth queries. Discuss compression and per-node multisets?","answer":"Use a compressed 2D Fenwick where each x-node holds a BST over y with multisets of weights. Updates touch O(log X) x-nodes; each per-node update is O(log Y) for the y-structure. Count uses standard 2D","explanation":"## Why This Is Asked\nReal systems need dynamic, scalable range queries on 2D data with duplicates. This tests 2D indexing, order-statistics beyond simple sums, and careful handling of duplicates.\n\n## Key Concepts\n- Coordinate compression for x and y\n- 2D Fenwick tree structure (x-dimension with per-node y-structures)\n- Per-node multisets to support kth queries with duplicates\n- Binary search on weight to obtain kth largest in a prefix\n\n## Code Example\n```javascript\n// Pseudo: update and kth query skeleton for 2D Fenwick with multisets\nclass Node {\n  constructor(){ this.yMap = new Map(); }\n}\n```\n\n## Follow-up Questions\n- How would you adapt to deletions that remove all duplicates for a given (x,y,w,id)?\n- Analyze memory vs. time trade-offs when N grows to 1e6.","diagram":"flowchart TD\n  A[2D Fenwick Index] --> B{Update}\n  B --> C[Update O(log X) nodes]\n  A --> D{Query}\n  D --> E[Prefix sum for count]\n  E --> F[Compute kth via weight threshold]\n  F --> G[Return result]","difficulty":"intermediate","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Two Sigma","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T09:44:40.657Z","createdAt":"2026-01-17T09:44:40.657Z"},{"id":"q-3391","question":"Design a dynamic 2D data structure for a multiset of colored points (x, y, color). Implement insertPoint(x,y,color) and erasePoint(x,y,color) (duplicates allowed). Provide rangeCountInRect(x1,x2,y1,y2,color) and rangeModeInRect(x1,x2,y1,y2) that returns the color with the highest frequency in the rectangle (ties broken by smaller color id). Aim for average O(log^2 n) per op; discuss compression, memory trade-offs, and handling duplicates?","answer":"Use a 2D range tree: primary BST on x; each node stores a secondary BST on y plus a color-count map. On insertPoint/erasePoint, update O(log n) x-nodes and O(log n) y-structures, adjusting per-node co","explanation":"## Why This Is Asked\nAssesses ability to design a dynamic 2D data structure with non-trivial aggregates, not just sums.\n\n## Key Concepts\n- 2D range trees with implicit keys\n- per-node color-frequency maps for mode queries\n- coordinate compression to bound memory\n- correct handling of duplicates via counts\n\n## Code Example\n```javascript\n// Skeleton: insert/erase and count/mode queries using per-node color maps\nclass Node {\n  constructor() {\n    this.x = 0; this.left = null; this.right = null;\n    this.yTree = new BST(); // stores (y, color, delta)\n    this.colorCounts = new Map(); // color -> count\n  }\n}\n```\n\n## Follow-up Questions\n- How would you bound memory in degenerate inputs?\n- How would you adapt to online vs offline coordinate distributions?","diagram":"flowchart TD\n  A[Insert/Erase] --> B[Update O(log n) x-nodes]\n  A --> C[Update y-trees & color maps]\n  D[Query] --> E[Aggregate O(log n) nodes]\n  E --> F[Compute rangeCount and rangeMode]","difficulty":"advanced","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Instacart"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T14:31:38.128Z","createdAt":"2026-01-17T14:31:38.128Z"},{"id":"q-3495","question":"Design a dynamic forest data structure for rooted trees with labeled edges. Operations: link(u,v,w) attach the root of one tree as a child of another with edge label w; cut(u) detach a subtree rooted at u; pathLabels(u,v) return the multiset of edge labels along the path from u to v; and kthSmallestOnPath(u,v,k) return the k-th smallest label on that path. Achieve near O(log n) per operation by using a Link-Cut Tree augmented with order-statistics; discuss how you handle duplicates and path aggregation?","answer":"Use a Link-Cut Tree (LCT) augmented with per-node multisets of incident edge labels and a path-aggregated order-statistics structure. Expose path(u,v) to merge labels along the path; kthSmallestOnPath","explanation":"## Why This Is Asked\nTests dynamic tree manipulation (link/cut) and advanced path queries using order statistics on a changing path landscape. Combines practical DS design with real-world constraints (duplicates, updates).\n\n## Key Concepts\n- Link-Cut Tree (dynamic trees)\n- Path aggregation and exposure\n- Multisets with multiplicities for duplicates\n- Order-statistics on a path (binary search over weight domain)\n\n## Code Example\n```javascript\nclass Node {\n  constructor(val) {\n    this.val = val;\n    this.left = null;\n    this.right = null;\n    this.parent = null;\n    this.edgeLabels = new Map(); // weight -> count\n  }\n}\n// Skeleton: LCT operations would go here (splay, access, link, cut, path expose)\n```\n\n## Follow-up Questions\n- How would you optimize memory for very large dynamic forests?\n- How would you adapt to dynamic edge label updates without cutting/re-linking?\n- What are the performance trade-offs vs. alternative structures (Euler tour + segment trees)?","diagram":"flowchart TD\n  A[link(u,v,w)] --> B[expose path(u,v)]\n  B --> C[kthSmallestOnPath]\n  D[cut(u)] --> B\n  E[pathLabels(u,v)] --> B","difficulty":"advanced","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Google","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T18:51:23.902Z","createdAt":"2026-01-17T18:51:23.902Z"},{"id":"q-3644","question":"Design a data structure to process a stream of integers arriving one by one. Implement insert(x) and getMedian() that returns the current median after each insertion. Use two heaps: a max-heap for the lower half and a min-heap for the upper half. Describe balancing rules to keep sizes within one and how to compute the median for odd/even counts. Consider duplicates and streaming constraints?","answer":"Maintain two heaps: a max-heap for the lower half and a min-heap for the upper half. On insert(x), push into the appropriate heap, then rebalance so the sizes differ by at most 1. The median is the top of the larger heap when sizes differ, or the average of both heap tops when sizes are equal. This approach handles duplicates naturally and processes each insertion in O(log n) time with O(1) median retrieval.","explanation":"## Why This Is Asked\n\nEvaluates ability to implement streaming statistics with efficient data structures and real-time median calculation.\n\n## Key Concepts\n\n- Dual heap structure (max-heap + min-heap)\n- Balance invariant maintenance\n- Median computation in O(log n) per insert\n- Streaming data processing\n\n## Code Example\n\n```javascript\nclass MedianStream {\n  constructor() {\n    this.low = new MaxHeap();  // Lower half\n    this.high = new MinHeap(); // Upper half\n  }\n  \n  insert(x) {\n    // Insert into appropriate heap and rebalance\n    if (this.low.isEmpty() || x <= this.low.peek()) {\n      this.low.push(x);\n    } else {\n      this.high.push(x);\n    }\n    \n    // Rebalance to maintain size difference ≤ 1\n    if (this.low.size() > this.high.size() + 1) {\n      this.high.push(this.low.pop());\n    } else if (this.high.size() > this.low.size() + 1) {\n      this.low.push(this.high.pop());\n    }\n  }\n  \n  getMedian() {\n    if (this.low.size() > this.high.size()) {\n      return this.low.peek();\n    } else if (this.high.size() > this.low.size()) {\n      return this.high.peek();\n    } else {\n      return (this.low.peek() + this.high.peek()) / 2;\n    }\n  }\n}\n```\n\n## Implementation Notes\n\n- Duplicates are handled naturally by heap properties\n- Memory usage is O(n) for storing all elements\n- Suitable for real-time streaming applications\n- Can be extended to support removal operations with additional bookkeeping","diagram":null,"difficulty":"beginner","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","IBM","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T05:01:55.630Z","createdAt":"2026-01-18T02:45:45.547Z"},{"id":"q-3773","question":"Design a data structure that supports: addValue(x) to insert an integer x into a multiset, removeValue(x) to remove one occurrence, and getMode() to return the value with the highest frequency (ties broken by smaller value). Aim for average O(log n) updates. Example: after addValue(5), addValue(3), addValue(5), addValue(2), addValue(5), getMode() should return 5?","answer":"Maintain a hashmap counts and a max-heap of pairs [count, -value]. addValue(x): increment counts[x], push [count[x], -x]. removeValue(x): decrement or delete counts[x], push updated pair if still pres","explanation":"## Why This Is Asked\nTests ability to combine a hashmap with a priority structure to track a dynamic mode under duplicates, while keeping updates O(log n) on average. It also checks handling of stale entries and edge cases (removals of non-existent values).\n\n## Key Concepts\n- Hash map for value-to-count to enable O(1) updates\n- Max-heap keyed by (count, -value) to prioritize higher counts and break ties toward smaller values\n- Lazy deletion of stale heap entries to keep updates efficient\n- Edge cases: removing non-existent values, empty multiset\n\n## Code Example\n```javascript\nclass ModeMultiset {\n  constructor() {\n    this.cnt = new Map();\n    this.heap = []; // max-heap of [count, -value]\n  }\n  _heapPush(item) { /* standard binary-heap insert with comparator by count desc, value asc */ }\n  _heapPop() { /* standard binary-heap extract-max */ }\n  addValue(x) {\n    const c = (this.cnt.get(x) || 0) + 1;\n    this.cnt.set(x, c);\n    this._heapPush([c, -x]);\n  }\n  removeValue(x) {\n    const c = this.cnt.get(x);\n    if (!c) return;\n    const nc = c - 1;\n    if (nc > 0) this.cnt.set(x, nc);\n    else this.cnt.delete(x);\n    if (nc > 0) this._heapPush([nc, -x]);\n  }\n  getMode() {\n    while (this.heap.length) {\n      const [c, neg] = this.heap[0];\n      const v = -neg;\n      if ((this.cnt.get(v) || 0) === c) return v;\n      this._heapPop();\n    }\n    return null;\n  }\n}\n```\n\n## Follow-up Questions\n- How would you modify to support getModeTieBreaker() that returns all modes when multiple values share the max frequency?\n- What are the trade-offs of using lazy deletion vs a dedicated support structure (e.g., a TreeMap) for this problem?","diagram":"flowchart TD\n  A[addValue(x)] --> B[update counts]\n  B --> C[push to heap]\n  C --> D[getMode()]\n  E[removeValue(x)] --> F[decrement count]\n  F --> G[optional heap push]\n  D --> H[return mode value]\n","difficulty":"beginner","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T08:52:50.379Z","createdAt":"2026-01-18T08:52:50.379Z"},{"id":"q-3913","question":"Design a dynamic 2D weighted point set supporting insertPoint(x,y,w) and erasePoint(x,y,w) (duplicates allowed). Add time-aware queries: (a) sumWeightsInRectangle(x1,x2,y1,y2,t) for total weight inside the rectangle as of time t, and (b) kthLargestWeightInRectangle(x1,x2,y1,y2,t,k) for the k-th largest weight among active points in the rectangle at time t. Target avg O(log^2 n) updates and queries; discuss persistent structures and versioning strategies?","answer":"Propose a persistent 2D spatial index (e.g., persistent quadtree with per-node multiset of weights) that versioned inserts/deletes create new root paths. For sum and kthLargest, maintain prefix-sum an","explanation":"## Why This Is Asked\nTests ability to extend 2D data structures with time-versioning, combining spatial indexing with temporal queries; examines memory usage and version management under high write throughput.\n\n## Key Concepts\n- Persistent data structures\n- 2D range queries\n- Time-travel queries\n\n## Code Example\n```javascript\n// Pseudo sketch of a persistent 2D tree node structure\n```\n\n## Follow-up Questions\n- How would you handle deletions of the same weight multiple times?\n- What are the trade-offs between full persistence vs. partial persistence?\n","diagram":null,"difficulty":"advanced","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T14:51:25.733Z","createdAt":"2026-01-18T14:51:25.733Z"},{"id":"q-3962","question":"Design a queue that supports enqueue(x), dequeue(), and getMin() in O(1) amortized time. The queue must handle duplicates and scale to 1e5 operations. Explain the data structure and show how enqueue, dequeue, and getMin would work in detail?","answer":"Use two stacks, each cell stores (value, currentMin). Enqueue(x): push (x, min(x, inTopMin)) onto InStack. Dequeue(): if OutStack empty, transfer all items from InStack to OutStack, recomputing min on","explanation":"## Why This Is Asked\n\nEvaluates practical data-structure composition: a queue with constant-time min tracking, a common interview pattern for streams and real-time dashboards.\n\n## Key Concepts\n\n- Two-stack queue pattern\n- Per-stack min tracking\n- Amortized analysis and edge-case handling for duplicates\n\n## Code Example\n\n```javascript\nclass MinQueue {\n  constructor(){ this.inStack = []; this.outStack = []; }\n  enqueue(x){ const minIn = this.inStack.length ? Math.min(x, this.inStack[this.inStack.length-1][1]) : x; this.inStack.push([x, minIn]); }\n  dequeue(){ if (!this.outStack.length){ while (this.inStack.length){ const [val] = this.inStack.pop(); const minOut = this.outStack.length ? Math.min(val, this.outStack[this.outStack.length-1][1]) : val; this.outStack.push([val, minOut]); } }\n    const popped = this.outStack.pop(); return popped ? popped[0] : undefined; }\n  getMin(){ const minIn = this.inStack.length ? this.inStack[this.inStack.length-1][1] : Infinity; const minOut = this.outStack.length ? this.outStack[this.outStack.length-1][1] : Infinity; return Math.min(minIn, minOut); }\n}\n```\n\n## Follow-up Questions\n\n- How would you adapt for a circular buffer with fixed capacity? \n- Compare this with a min-heap alternative and discuss trade-offs.","diagram":"flowchart TD\n  A[Enqueue] --> B[Push to InStack]\n  B --> C{OutStack empty?}\n  C -- Yes --> D[Move InStack to OutStack]\n  D --> E[Dequeue from OutStack]\n  F[GetMin] --> G[Compare InStack min and OutStack min]","difficulty":"beginner","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","DoorDash","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T17:32:22.064Z","createdAt":"2026-01-18T17:32:22.064Z"},{"id":"q-4015","question":"Design a queue data structure that supports: push(x) to back, pop() from front, max() returning the current maximum element, and sum() returning the sum of all elements in the queue. All four operations must be amortized O(1). Provide a concrete implementation plan (two stacks with per-element max, and a running sum) and a short code sketch in a language of your choice?","answer":"Two-stack queue. Maintain two stacks of objects {val, max}. push(x) adds to the in-stack with max = max(x, in.top?.max). pop() moves items to the out-stack when needed and pops from it. max() returns ","explanation":"## Why This Is Asked\nTests practical DS reasoning for streams where both max and total are needed efficiently, using a familiar two-stack queue pattern.\n\n## Key Concepts\n- Two-stack queue with per-element max\n- Running sum maintenance for O(1) sum queries\n- Amortized analysis of transfer between stacks\n\n## Code Example\n```javascript\nclass MaxQueue {\n  constructor() {\n    this.in = [];\n    this.out = [];\n    this.total = 0;\n  }\n  push(x) {\n    const newMax = this.in.length === 0 ? x : Math.max(x, this.in[this.in.length - 1].max);\n    this.in.push({ val: x, max: newMax });\n    this.total += x;\n  }\n  pop() {\n    if (this.out.length === 0) {\n      while (this.in.length > 0) {\n        const v = this.in.pop().val;\n        const newMax = this.out.length === 0 ? v : Math.max(v, this.out[this.out.length - 1].max);\n        this.out.push({ val: v, max: newMax });\n      }\n    }\n    if (this.out.length === 0) return undefined;\n    const popped = this.out.pop();\n    this.total -= popped.val;\n    return popped.val;\n  }\n  max() {\n    const maxIn = this.in.length === 0 ? -Infinity : this.in[this.in.length - 1].max;\n    const maxOut = this.out.length === 0 ? -Infinity : this.out[this.out.length - 1].max;\n    if (maxIn === -Infinity && maxOut === -Infinity) return undefined;\n    return Math.max(maxIn, maxOut);\n  }\n  sum() {\n    return this.total;\n  }\n}\n``` \n\n## Follow-up Questions\n- How would you adapt this to support a sliding window of last N elements for max and sum?\n- How would you extend to also provide a min() with the same complexity guarantees?","diagram":"flowchart TD\n  A[Push] --> B[In-stack]\n  B --> C[Pop triggers transfer to Out-stack]\n  C --> D[Max from both stacks' tops]\n  D --> E[Sum maintained separately]\n  E --> F[Return values for max and sum]","difficulty":"beginner","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T19:33:31.367Z","createdAt":"2026-01-18T19:33:31.367Z"},{"id":"q-4040","question":"Design a data structure to process a dynamic undirected graph with parallel edges that can appear and disappear over time. Each edge has an active interval [l, r). Support addEdge(u, v, l, r, edgeId) and closeEdge(u, v, edgeId) to end early, plus isConnected(u, v, t) for any time t. Propose an offline segment-tree-over-time solution using a DSU with rollback; analyze time complexity and edge counts, including handling of parallel edges?","answer":"Use an offline segment-tree-over-time approach with a DSU that supports rollback. Each edge interval [l, r) is added to the segment tree; DFS traverses nodes, pushing edges onto the DSU, answering isConnected queries at leaf nodes, and rolling back after each recursive call.","explanation":"## Why This Is Asked\nDynamic connectivity with time-bounded edges is a core data structure challenge; offline segment trees with DSU rollback provide an elegant, scalable solution.\n\n## Key Concepts\n- Dynamic connectivity with interval edges\n- Segment tree over time\n- DSU with rollback for persistent-like queries\n- Handling parallel edges via multiplicity counts\n\n## Code Example\n```javascript\n// Pseudocode: segment-tree over time with DSU rollback\n```\n\n## Follow-up Questions\n- How would you adapt to directed graphs or support edge weights for max-min queries?\n- What are the memory implications?","diagram":null,"difficulty":"advanced","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Netflix","OpenAI","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-19T06:10:21.517Z","createdAt":"2026-01-18T21:25:39.758Z"},{"id":"q-4142","question":"Design a data structure to manage a dynamic multiset of circular intervals on a fixed circular timeline with period P. Each interval is represented as [start, end) on the circle; intervals may wrap around and duplicates are allowed. Implement insertInterval(start, end, w), eraseInterval(start, end, w) (one occurrence), rangeSumInArc(a, b) returning the total weight of all intervals intersecting arc [a, b] clockwise, and kthLargestInArc(a, b, k) returning the k-th largest weight among intervals intersecting that arc. Target average O(log n) per operation. Explain handling wrap-around, coordinate compression, memory trade-offs, and duplicates?","answer":"Normalize circle to [0, P). Split wrap intervals into at most two linear arcs. Use a dynamic segment tree or Fenwick on the compressed coordinate axis; on insertInterval and eraseInterval, update cove","explanation":"## Why This Is Asked\nIntersects circular intervals with dynamic updates and order-statistic queries on a ring, a practical pattern in scheduling and time-window analytics at scale.\n\n## Key Concepts\n- Circular intervals and wrap-around handling\n- Coordinate compression for large P\n- Dynamic range aggregations with multiplicities\n- Order-statistics over weighted intervals\n\n## Code Example\n```javascript\n// Implementation sketch for circular intervals (conceptual)\n```\n\n## Follow-up Questions\n- How would you adapt to support deletions of arbitrary w without traces when many duplicates exist?\n- What are the storage implications if P is extremely large and endpoints are sparse?","diagram":null,"difficulty":"advanced","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Oracle","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-19T04:47:54.846Z","createdAt":"2026-01-19T04:47:54.846Z"},{"id":"q-4254","question":"Design a dynamic double-ended queue (deque) that supports pushFront(x), pushBack(x), popFront(), popBack(), and returns getMin() and getMax() in O(1) time after each operation. Duplicates allowed. Explain how to maintain min/max with constant-time updates and analyze memory usage?","answer":"Maintain two stacks: front and back. Each element stores (value, minSoFar, maxSoFar). PushFront updates front; PushBack updates back. PopFront/PopBack pops from the respective stack. GetMin = min(fron","explanation":"## Why This Is Asked\nTests ability to design a mutable deque with constant-time min/max queries, a common interview pattern for surface-level data-structure mastery.\n\n## Key Concepts\n- Deque operations and invariants\n- Two-stack technique for amortized efficiency\n- Maintaining per-element min/max for O(1) queries\n- Edge-case handling (empty, duplicates)\n\n## Code Example\n```javascript\nclass MinMaxDeque {\n  constructor() {\n    this.front = [];\n    this.back = [];\n  }\n  pushFront(x) {\n    const last = this.front[this.front.length - 1];\n    const mn = last ? Math.min(x, last.min) : x;\n    const mx = last ? Math.max(x, last.max) : x;\n    this.front.push({ v: x, min: mn, max: mx });\n  }\n  pushBack(x) {\n    const last = this.back[this.back.length - 1];\n    const mn = last ? Math.min(x, last.min) : x;\n    const mx = last ? Math.max(x, last.max) : x;\n    this.back.push({ v: x, min: mn, max: mx });\n  }\n  popFront() {\n    if (this.front.length === 0) {\n      while (this.back.length) this.pushFront(this.back.pop().v);\n    }\n    if (this.front.length === 0) throw new Error('empty');\n    return this.front.pop().v;\n  }\n  popBack() {\n    if (this.back.length === 0) {\n      while (this.front.length) this.pushBack(this.front.pop().v);\n    }\n    if (this.back.length === 0) throw new Error('empty');\n    return this.back.pop().v;\n  }\n  getMin() {\n    if (this.front.length === 0 && this.back.length === 0) throw new Error('empty');\n    let mn = null;\n    if (this.front.length) mn = this.front[this.front.length - 1].min;\n    if (this.back.length) {\n      const bmin = this.back[this.back.length - 1].min;\n      mn = mn === null ? bmin : Math.min(mn, bmin);\n    }\n    return mn;\n  }\n  getMax() {\n    if (this.front.length === 0 && this.back.length === 0) throw new Error('empty');\n    let mx = null;\n    if (this.front.length) mx = this.front[this.front.length - 1].max;\n    if (this.back.length) {\n      const bmax = this.back[this.back.length - 1].max;\n      mx = mx === null ? bmax : Math.max(mx, bmax);\n    }\n    return mx;\n  }\n}\n```\n\n## Follow-up Questions\n- How would you adapt this to support getMinAtMost(k) for the k smallest elements in the deque?\n- Analyze worst-case memory when all pushes happen on one end, and suggest optimizations.","diagram":null,"difficulty":"beginner","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Oracle","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-19T10:39:58.453Z","createdAt":"2026-01-19T10:39:58.454Z"},{"id":"q-4413","question":"Design a dynamic 2D point set supporting insertPoint(x, y) and erasePoint(x, y) (duplicates allowed). Implement two queries: nearestNeighbor(x0, y0) returning the closest point to (x0, y0), and kNearestNeighbors(x0, y0, k) returning the k closest points. Optimize for practical performance with insertions and deletions; discuss using a dynamic KD-tree with lazy deletions and rebuild strategy, how to handle duplicates with unique IDs, and expected amortized costs in practice?","answer":"A practical approach is a dynamic KD-tree with lazy deletions. Each point stores a unique id; insertPoint assigns a new id; erasePoint marks the (x,y,id) deleted. Queries traverse nodes with bounding ","explanation":"## Why This Is Asked\nTests ability to design practical dynamic geometric data structures with real-world concerns like duplicates, deletions, and rebuilds.\n\n## Key Concepts\n- Dynamic KD-tree with lazy deletions\n- Unique IDs for duplicates\n- Nearest neighbor and k-NN search via bounding-box pruning and best-first traversal\n- Rebuild thresholds to bound amortized costs\n\n## Code Example\n```javascript\n// Skeleton: points with id, insert, erase, and query stubs\nclass Point { constructor(x, y, id) { this.x=x; this.y=y; this.id=id; } }\nclass KDNode { constructor(pt, left=null, right=null, min=null, max=null) { this.pt=pt; this.left=left; this.right=right; this.min=min; this.max=max; this.deleted=false; } }\n```\n\n## Follow-up Questions\n- How would you choose and adapt the rebuild threshold in response to data skew? \n- How would you extend to support approximate NN for strict performance budgets?","diagram":"flowchart TD\n  A[Client Requests] --> B[insertPoint/erasePoint]\n  B --> C[Mark or Insert in KD-tree]\n  C --> D[Periodic Rebuild?]\n  A --> E[nearestNeighbor / kNearestNeighbors]\n  E --> F[Best-first / Pruned Search]\n  F --> G[Return Results]","difficulty":"intermediate","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Square","Stripe","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-19T17:53:49.666Z","createdAt":"2026-01-19T17:53:49.667Z"},{"id":"q-4426","question":"Design a dynamic 2D weighted point set supporting insertPoint(x,y,w) and erasePoint(x,y,w) with duplicates allowed, plus a region toggle operation toggleRegion(x1,x2,y1,y2) that flips the active flag of all points inside the rectangle. Implement rangeSumActive2D(x1,x2,y1,y2) for total weight of active points and kthLargestActiveInRect(x1,x2,y1,y2,k) for the k-th largest weight among active points in the rectangle. Aim for average O(log^2 n) per operation; discuss data structure choices, persistence implications, and how to handle duplicates?","answer":"Use a discretized 2D segment tree; each node stores two order-statistics structures for weights: active and inactive, plus a lazy flip. Insertion/erasure place the point into the appropriate bucket. t","explanation":"Why This Is Asked\nThis probes 2D range structures, lazy region updates, and order-statistics under duplicates. Candidates justify discretization, per-node weight maps, and how to implement top-k from active points.\n\nKey Concepts\n- 2D segment tree with lazy propagation\n- per-node weight-indexed counts for kth order\n- duplicates handled via unique ids and stable ordering\n\nCode Example\n```javascript\n// High-level sketch omitted\n```\n\nFollow-up Questions\n- How would you adapt for real-time streaming where x,y arrive unordered? \n- What are memory and worst-case guarantees with many duplicates?","diagram":null,"difficulty":"advanced","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","OpenAI","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-19T18:44:58.760Z","createdAt":"2026-01-19T18:44:58.761Z"},{"id":"q-4468","question":"Design a dynamic graph connectivity data structure that processes a time-ordered sequence of operations on an undirected graph with N nodes. Supported ops: addEdge(u,v), removeEdge(u,v) (edges may be added again later), and queryComponents() to return the current number of connected components. All operations are known in advance. Describe and implement an offline solution using a segment tree over time and a rollback DSU to answer queries in O((N+Q) log Q) time. Explain edge lifespan handling and complexity trade-offs?","answer":"Use a rollback-capable DSU and a segment tree over the timeline. Map each edge’s active intervals by pairing add and subsequent remove; for edges never removed, use [tAdd, Q]. Build a segment tree whe","explanation":"## Why This Is Asked\n\nTests knowledge of offline dynamic connectivity and rollback data structures, a common interview topic for medium/high realism. It also probes careful handling of edge lifespans and queries interleaved with updates.\n\n## Key Concepts\n\n- DSU with rollback\n- Segment tree over time\n- Edge lifespan mapping (add/remove intervals)\n- Offline processing with DFS traversal for queries\n\n## Code Example\n\n```javascript\n// Implementation sketch for DSU with rollback and segment-tree processing\nclass DSURollback { constructor(n){ this.parent=Array.from({length:n},(_,i)=>i); this.rank=Array(n).fill(0); this.changes=[]; this.components=n; }\nfind(x){ while(this.parent[x]!=x){ x=this.parent[x]; } return x; }\nunion(a,b){ a=this.find(a); b=this.find(b); if(a==b){ this.changes.push(null); return; } if(this.rank[a]<this.rank[b]){ [a,b]=[b,a]; }\n this.changes.push([b,this.parent[b],this.rank[a]]); this.parent[b]=a; if(this.rank[a]==this.rank[b]) this.rank[a]++; this.components--; }\nrollback(){ const c=this.changes.pop(); if(!c) return; if(c===null) return; const [b, oldParent, oldRank] = c; this.parent[b]=oldParent; // restore rank by computing root if needed (simplified) this.components++; }\n}\n\n// Segment-tree over time with edge intervals and queryTimes\n```\n\n## Follow-up Questions\n\n- How would you adapt to online queries without offline knowledge?\n- What are memory implications for very large Q and dense edge activity?","diagram":"flowchart TD\n  A[Time] --> B[Active Edge Intervals]\n  B --> C[Segment Tree Nodes]\n  C --> D[DFS Processing with DSU Rollback]\n  D --> E[Answer Queries in O(1) at leaves]\n","difficulty":"intermediate","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Meta","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-19T19:54:12.018Z","createdAt":"2026-01-19T19:54:12.018Z"},{"id":"q-4577","question":"Design a dynamic multiset of integers supporting insertWeight(v) and eraseWeight(v) (duplicates allowed) and rangeSum(l, r) returning the total of all values in [l, r]. Use coordinate compression and two Fenwick trees (one for counts, one for sums) to support O(log M) updates and queries. Address duplicates, erase of non-existent values, and memory usage. Assume values are integers within a known domain?","answer":"Use coordinate compression to map values to indices 1..M. Maintain two Fenwick trees: one for sums and one for counts. On insertWeight(v): idx = compress(v); sums.update(idx, v); counts.update(idx, +1). On eraseWeight(v): if counts.query(idx) - counts.query(idx-1) > 0, then sums.update(idx, -v); counts.update(idx, -1). For rangeSum(l, r): return sums.query(compress(r)) - sums.query(compress(l)-1).","explanation":"## Why This Is Asked\nTests ability to combine a dynamic multiset with value-range queries, a common interview pattern, and checks handling of duplicates and memory-time trade-offs.\n\n## Key Concepts\n- Coordinate compression\n- Fenwick (BIT) trees for sums and counts\n- Edge cases: erase on absent value\n- Complexity: O(log M) per operation, O(M) space\n\n## Code Example\n\n```javascript\nclass Fenwick {\n  constructor(n){ this.n=n; this.bit=new Array(n+1).fill(0); }\n  update(i, delta){ for(; i<=this.n; i+= i&-i) this.bit[i]+=delta; }\n  query(i){ let s=0; for(; i>0; i-= i&-i) s+=this.bit[i]; return s; }\n```","diagram":null,"difficulty":"beginner","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Lyft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T05:59:45.508Z","createdAt":"2026-01-20T02:28:57.147Z"},{"id":"q-4817","question":"You're given a graph where edges can be added and removed over time. Design an offline dynamic connectivity solution that answers, at arbitrary times, whether two nodes are connected and the size of their connected component. Use a segment-tree-over-time plus a DSU with rollback; outline edge-lifetime mapping, query handling, and complexity for up to 2e5 operations?","answer":"Use a DSU with rollback and a segment tree over time. Each edge's active window [l, r) is inserted into the nodes covering that interval. DFS the segment tree, applying unions; at leaves answer connec","explanation":"## Why This Is Asked\nDynamic connectivity with insertions and deletions is a classic hard problem. This offline approach tests understanding of edge lifetimes and versioning, and how to combine segment trees with DSU rollback for log-scale updates.\n\n## Key Concepts\n- DSU with rollback (no path compression) to enable undo.\n- Segment tree over time to map edge active intervals to time ranges.\n- Offline processing: traverse tree, apply unions, answer leaf queries, then rollback.\n\n## Code Example\n```javascript\nclass RollbackDSU {\n  constructor(n){\n    this.parent = Array.from({length:n}, (_,i)=>i);\n    this.sz = Array(n).fill(1);\n    this.ch = [];\n  }\n  find(x){ while (this.parent[x]!==x) x = this.parent[x]; return x; }\n  union(a,b){\n    a = this.find(a); b = this.find(b);\n    if (a===b){ this.ch.push(null); return; }\n    if (this.sz[a] < this.sz[b]) [a,b] = [b,a];\n    this.ch.push([b, this.parent[b], a, this.sz[a]]);\n    this.parent[b] = a; this.sz[a] += this.sz[b];\n  }\n  snapshot(){ return this.ch.length; }\n  rollback(snap){\n    while (this.ch.length > snap){\n      const ch = this.ch.pop();\n      if (!ch) continue;\n      const [b, oldP, a, oldS] = ch;\n      this.parent[b] = oldP; this.sz[a] = oldS;\n    }\n  }\n}\n```","diagram":null,"difficulty":"intermediate","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google","IBM"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T14:48:54.736Z","createdAt":"2026-01-20T14:48:54.736Z"},{"id":"q-4864","question":"Design a dynamic word-frequency index for autocomplete. Support addWord(word, delta) with delta ± counts, and queryTopK(prefix, k) to return up to k words starting with prefix, ordered by descending frequency (ties broken lexicographically). Use a simple Trie plus a hashmap; discuss update, lookup, and memory trade-offs for a modest vocabulary size?","answer":"Use a Trie plus a hashmap of word->count. addWord(word, delta) updates the word’s count in the map and increments counts along the word’s path in the Trie. queryTopK(prefix, k) locates the prefix node","explanation":"## Why This Is Asked\nTests the ability to build a practical autocomplete index with dynamic updates, using common data structures and real-world trade-offs.\n\n## Key Concepts\n- Trie with per-word counts\n- Dynamic frequency updates via a hashmap\n- Top-k selection with a bounded DFS and a small heap\n- Trade-offs: update cost vs lookup speed; memory use for prefixes\n\n## Code Example\n```javascript\nclass Autocomplete {\n  constructor() {\n    this.root = {};\n    this.words = new Map();\n  }\n  addWord(word, delta) {\n    this.words.set(word, (this.words.get(word) || 0) + delta);\n    let node = this.root;\n    for (const ch of word) {\n      node = node[ch] = node[ch] || {};\n    }\n    node.__end = word;\n  }\n  queryTopK(prefix, k) {\n    // locate prefix node and DFS to collect up to k results\n  }\n}\n```\n\n## Follow-up Questions\n- How would you handle memory reclamation for words with zero counts?\n- How would you scale to large vocabularies, or add persistence across restarts?","diagram":null,"difficulty":"beginner","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Lyft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T16:53:43.987Z","createdAt":"2026-01-20T16:53:43.987Z"},{"id":"q-4889","question":"Design a data structure to maintain a dynamic sequence of integers with operations: insertAt(index, value), eraseAt(index), and rangeSum(l, r). Duplicates allowed. Achieve amortized O(log n) per operation. Describe the data structure, how inserts/erases adjust augmented data, and outline the algorithm for rangeSum?","answer":"I would implement an implicit treap where each node stores value, priority, size, and sum. To insertAt(i,x), split by i, merge left + new node + right. eraseAt(i) splits into left, mid, right, drops m","explanation":"## Why This Is Asked\nTests understanding of maintaining an ordered dynamic sequence with logarithmic updates while augmenting nodes with aggregate data for fast queries.\n\n## Key Concepts\n- Implicit treap for sequence indexing\n- Split by position and merge operations\n- Augmented data: size and sum to support rangeSum\n- Handling duplicates via node values and randomized priorities\n\n## Code Example\n```javascript\nclass Node { constructor(val){ this.val=val; this.pri=Math.random(); this.left=null; this.right=null; this.sz=1; this.sum=val; } }\nfunction sz(n){ return n? n.sz:0; }\nfunction sum(n){ return n? n.sum:0; }\nfunction upd(n){ if(n){ n.sz = 1 + sz(n.left) + sz(n.right); n.sum = n.val + sum(n.left) + sum(n.right); } }\nfunction split(root, key){ // first key elements go to left\n  if(!root) return [null,null];\n  if(sz(root.left) >= key){ const [l,r] = split(root.left, key); root.left = r; upd(root); return [l, root]; }\n  else { const [l,r] = split(root.right, key - sz(root.left) - 1); root.right = l; upd(root); return [root, r]; }\n}\nfunction merge(a,b){ if(!a||!b) return a||b; if(a.pri < b.pri){ a.right = merge(a.right, b); upd(a); return a; } else { b.left = merge(a, b.left); upd(b); return b; } }\nfunction insertAt(root, index, value){ const node = new Node(value); const [L,R] = split(root, index); return merge(merge(L, node), R); }\nfunction eraseAt(root, index){ const [L, R] = split(root, index); const [M, RR] = split(R, 1); return merge(L, RR); }\nfunction rangeSumQuery(root, l, r){ const [A, BR] = split(root, l); const [B, C] = split(BR, r - l + 1); const res = sum(B); const newRoot = merge(A, merge(B, C)); return {root: newRoot, sum: res}; }\n```","diagram":null,"difficulty":"beginner","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Discord","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T17:46:45.390Z","createdAt":"2026-01-20T17:46:45.390Z"},{"id":"q-4944","question":"Design a data structure to store a multiset of strings with operations: insertWord(word), deleteWord(word) (delete one occurrence if present), and queryPrefixLength(prefix) that returns the total number of characters of all words starting with prefix. Duplicates counted. Propose a Trie-based implementation where each node tracks the total length of words in its subtree and the number of words ending at that node; explain updates for insert/delete and how to answer the prefix query?","answer":"Implement a Trie where each node stores subLen (total characters of all words in its subtree) and endCount (words ending here). insertWord(w): walk letters, add w.length to subLen at each node, and in","explanation":"## Why This Is Asked\nTests understanding of adaptive prefix data structures and space-time trade-offs for dynamic multisets with non-trivial sum queries.\n\n## Key Concepts\n- Trie with per-node aggregates; handling duplicates; update of subtree sums on insert/delete; edge cases when counts drop to zero.\n\n## Code Example\n```javascript\nclass TrieNode {\n  constructor() {\n    this.children = new Map();\n    this.endCount = 0;\n    this.subLen = 0;\n  }\n}\nclass WordTrie {\n  constructor(){ this.root = new TrieNode(); }\n  insert(word){ let node=this.root; for(const ch of word){ node.subLen += word.length; if(!node.children.has(ch)) node.children.set(ch,new TrieNode()); node = node.children.get(ch); } node.subLen += word.length; node.endCount += 1; }\n  delete(word){ let node=this.root; for(const ch of word){ if(!node.children.has(ch)) return; node.subLen -= word.length; node = node.children.get(ch); } node.subLen -= word.length; if(node.endCount>0) node.endCount -= 1; }\n  query(prefix){ let node=this.root; for(const ch of prefix){ if(!node.children.has(ch)) return 0; node = node.children.get(ch); } return node.subLen; }\n}\n```\n\n## Follow-up Questions\n- How to handle memory reclamation for sparse nodes? \n- How would you extend to support length-weighted queries for arbitrary weights per insertion? ","diagram":"flowchart TD\n  A[insertWord] --> B[traverse letters]\n  B --> C[update subLen]\n  C --> D[end]\n  E[queryPrefixLength] --> F[traverse prefix]\n  F --> G[return subLen]","difficulty":"beginner","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Instacart","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T20:34:19.645Z","createdAt":"2026-01-20T20:34:19.645Z"},{"id":"q-5023","question":"Design a dynamic log store for multi-service API calls: each log is (userId, serviceId, timestamp, latency). Implement insertLog(userId, serviceId, timestamp, latency) and eraseLogsInTimeWindow(start, end). Provide two queries: (a) topKUsersByTotalLatencyInWindow(start, end, k) and (b) kthLargestLatencyForServiceInWindow(serviceId, start, end, k). Target average O(log n) updates and O(log^2 n) queries. Explain data structures, update rules, handling of duplicates, and edge cases?","answer":"Use a global time-ordered balanced tree storing logs by timestamp, with per-user and per-service aggregates updated on insert and lazily deleted during eraseLogsInTimeWindow operations. Maintain a per-user totalLatency balanced tree for top-k queries and per-service latency balanced trees for kth-largest queries. InsertLog performs O(log n) updates to all structures. EraseLogsInTimeWindow marks logs as deleted and lazily cleans aggregates when queries detect inconsistencies.","explanation":"## Why This Is Asked\nTests time-window dynamic data with multi-key aggregates and top-k queries, challenging both data layout and lazy deletions.\n\n## Key Concepts\n- Time-window maintenance with insertions and deletions\n- Per-user aggregate tracking for top-k queries\n- Per-service latency distribution for kth-largest queries\n\n## Code Example\n```javascript\nclass LogStore {\n  insertLog(userId, serviceId, timestamp, latency) {\n    // Add to global treap by timestamp; update per-user and per-service structures\n  }\n  eraseLogsInTimeWindow(start, end) {\n    // Mark logs in [start, end] as deleted; adjust aggregates lazily\n  }\n}\n```\n\n## Data Structures\n- **Global treap**: Time-ordered logs for window queries\n- **Per-user trees**: Track total latency by user for top-k\n- **Per-service trees**: Maintain latency distribution for kth-largest\n- **Lazy deletion**: Mark invalid entries, clean during queries\n\n## Update Rules\n- Insert: Update all structures in O(log n)\n- Delete: Mark as deleted, defer cleanup\n- Query: Verify consistency, trigger cleanup if needed\n\n## Edge Cases\n- Duplicate timestamps: Use secondary key (userId, serviceId)\n- Empty windows: Return empty results\n- Large deletions: Batch cleanup to maintain performance","diagram":"flowchart TD\n  A[Input Log] --> B[Insert Log]\n  B --> C[Windowed Stores\n  ]\n  C --> D[Query Top-K Users]\n  C --> E[Query kth Latency by Service]","difficulty":"intermediate","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Netflix","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T05:19:49.754Z","createdAt":"2026-01-21T00:02:21.057Z"},{"id":"q-5142","question":"Design a dynamic sequence data structure that supports insertAt(i, x), eraseAt(i), updateAt(i, x), and rangeKthSmallest(l, r, k) returning the k-th smallest value in A[l..r]. Duplicates allowed. Target O(log n) expected per operation. Explain the data structure (e.g., implicit treap with a dynamic order-statistics structure for subtrees), implementation details for splits/merges and range queries, and trade-offs?","answer":"Implicit treap on the sequence supports insertAt/eraseAt/updateAt in O(log n). Each node also maintains a compact frequency structure for its subtree (e.g., a dynamic wavelet tree or a Fenwick over co","explanation":"## Why This Is Asked\nTests mastery of dynamic sequences and range-order statistics with realistic constraints (duplicates, frequent updates). It also probes choice of data structures and practical trade-offs.\n\n## Key Concepts\n- Implicit treap/rope for dynamic sequence\n- Range order statistics with per-subtree frequency structure\n- Split/merge semantics to isolate and query subranges\n- Duplicate handling and memory considerations\n\n## Code Example\n```javascript\n// Sketch of rangeKthSmallest using splits\nfunction rangeKth(root, l, r, k) {\n  let [a, t] = split(root, l-1);\n  let [b, c] = split(t, r-l+1);\n  let ans = kthInSubtree(b, k);\n  root = merge(a, merge(b, c));\n  return {root, ans};\n}\n```\n\n## Follow-up Questions\n- How would persistence be added for historical queries?\n- How to handle large value domains with limited memory?","diagram":null,"difficulty":"intermediate","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T07:57:33.950Z","createdAt":"2026-01-21T07:57:33.950Z"},{"id":"q-5271","question":"Design a data structure for streaming user events with a fixed time window W seconds. It must support: addEvent(userId: int, ts: int); topK(now: int, k: int) -> list of up to k (userId, count) by event counts within [now-W+1, now]; memory proportional to distinct users; amortized O(log n) per addEvent and O(k log n) per topK. Explain eviction, duplicates and concurrency, and provide a concise code sketch in a familiar language?","answer":"Use a sliding window with: a hashmap userId -> count, a TreeMap<Integer, TreeSet<Integer>> buckets (count -> users), and a queue of (ts, userId) events. On addEvent(now, ts, user): prune events with t","explanation":"## Why This Is Asked\nTests ability to design a compact, eviction-aware data structure for time-windowed counting, a common pattern in real-time analytics.\n\n## Key Concepts\n- Sliding window eviction via timestamped events\n- Bucketed counts: map from count to set of users for fast top-k\n- Invariants: update counts atomically during insertion and eviction\n- Complexity: O(log n) updates, O(k log n) top-k retrieval\n\n## Code Example\n```javascript\nclass WindowCounter {\n  constructor(W) {\n    this.W = W;\n    this.count = new Map(); // user -> cnt\n    this.buckets = new Map(); // cnt -> Set<user>\n    this.events = new Deque(); // (ts, user)\n  }\n  _addToBucket(cnt, user) {\n    if (!this.buckets.has(cnt)) this.buckets.set(cnt, new Set());\n    this.buckets.get(cnt).add(user);\n  }\n  _removeFromBucket(cnt, user) {\n    if (!this.buckets.has(cnt)) return;\n    this.buckets.get(cnt).delete(user);\n    if (this.buckets.get(cnt).size === 0) this.buckets.delete(cnt);\n  }\n  prune(now) {\n    while (this.events.length && this.events.peek().ts <= now - this.W) {\n      const {ts, user} = this.events.shift();\n      const c = this.count.get(user) || 0;\n      if (c > 0) {\n        this._removeFromBucket(c, user);\n        const nc = c - 1;\n        if (nc > 0) {\n          this._addToBucket(nc, user);\n          this.count.set(user, nc);\n        } else {\n          this.count.delete(user);\n        }\n      }\n    }\n  }\n  addEvent(user, ts) {\n    this.events.push({ts, user});\n    this.prune(ts);\n    const c = this.count.get(user) || 0;\n    if (c > 0) this._removeFromBucket(c, user);\n    const nc = c + 1;\n    this._addToBucket(nc, user);\n    this.count.set(user, nc);\n  }\n  topK(now, k) {\n    const res = [];\n    // iterate counts from high to low\n    const counts = Array.from(this.buckets.keys()).sort((a,b)=>b-a);\n    for (const cnt of counts) {\n      const set = this.buckets.get(cnt);\n      const users = Array.from(set).sort((a,b)=>a-b); // tie-breaker by userId\n      for (const u of users) {\n        res.push([u, cnt]);\n        if (res.length === k) return res;\n      }\n    }\n    return res;\n  }\n}\n```\n\n## Follow-up Questions\n- How would you adapt this to support multiple simultaneous windows (e.g., W1 and W2) efficiently?\n- How would you handle very large userId spaces with memory constraints, or concurrent updates in a multi-threaded environment?","diagram":"flowchart TD\n  A[Event Arrives] --> B[Prune Old Events]\n  B --> C[Update User Counts]\n  C --> D[Maintain Buckets]\n  D --> E[Top-K Query]","difficulty":"intermediate","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","LinkedIn","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T14:41:59.654Z","createdAt":"2026-01-21T14:41:59.654Z"},{"id":"q-677","question":"Design a data structure to maintain dynamic counters for items, supporting add(key, delta), get(key), and getTopK(k) returning the k keys with highest counts, ties broken by key. In a real-time analytics scenario, such as tracking top active users by event count, describe the structure, updates, and complexity trade-offs?","answer":"Use a hash map key→count and recompute top-k on demand by sorting entries in descending order (tie-break by key). On add, adjust the counter and delete if zero. getTopK(k) runs in O(n log n). This kee","explanation":"## Why This Is Asked\nEvaluates knowledge of practical data-structure trade-offs for dynamic ranking. Candidates should justify when to sort on demand vs. maintain auxiliary structures.\n\n## Key Concepts\n- Hash map for O(1) key access\n- On-demand sorting for top-k retrieval\n- Trade-off: simpler updates vs. slower top-k retrieval\n\n## Code Example\n```javascript\nclass CounterTopK {\n  constructor() {\n    this.counts = new Map(); // key -> count\n  }\n  add(key, delta) {\n    const cur = this.counts.get(key) || 0;\n    const next = cur + delta;\n    if (next <= 0) this.counts.delete(key); else this.counts.set(key, next);\n  }\n  get(key) { return this.counts.get(key) || 0; }\n  topK(k) {\n    const arr = Array.from(this.counts.entries());\n    arr.sort((a,b)=> b[1]-a[1] || a[0].localeCompare(b[0]));\n    return arr.slice(0,k).map(([key]) => key);\n  }\n}\n```","diagram":null,"difficulty":"intermediate","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Discord","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T15:56:10.144Z","createdAt":"2026-01-11T15:56:10.144Z"},{"id":"q-686","question":"Design a data structure that maintains a multiset of integers with insert(x), erase(x) for one occurrence, findMedian(), and findKthSmallest(k). Achieve O(log n) time per operation on average. Explain data layout, invariants, and handling duplicates and lazy deletions. For example: insert 1,2,3,4,5; erase 3; what is median and findKthSmallest(2)?","answer":"Propose a two-heap design (lower half in a max-heap, upper half in a min-heap) with a hashmap for lazy deletions to support erase(x) (one occurrence). Balance so the sizes differ by at most 1; prune l","explanation":"## Why This Is Asked\n\nTests understanding of advanced DS combining heaps and lazy deletion to support dynamic order statistics.\n\n## Key Concepts\n\n- Two-heap maintenance for median\n- Lazy deletion with a hashmap for duplicates\n- Order-statistics and findKthSmallest via balanced heaps\n\n## Code Example\n\n```javascript\nclass DualHeapMedianSet {\n  constructor(){ /* ... skeleton ... */ }\n  insert(x){ /* ... */ }\n  erase(x){ /* ... */ }\n  findMedian(){ /* ... */ }\n  findKthSmallest(k){ /* ... */ }\n}\n```\n\n## Follow-up Questions\n\n- How do you handle many duplicates efficiently?\n- How would you implement findKthSmallest(k) with guaranteed worst-case O(log n) time?","diagram":null,"difficulty":"advanced","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Oracle","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T16:22:02.207Z","createdAt":"2026-01-11T16:22:02.207Z"},{"id":"q-697","question":"Design a time-weighted event multiset data structure. Supports insertEvent(ts, w), eraseEvent(ts, w) removing one occurrence, rangeSum(a, b) returning the sum of weights for events with timestamps in [a, b], and findKthWeightInRange(a, b, k) returning the k-th smallest weight among events with timestamps in [a, b]. Target O(log^2 n) per operation; handle duplicate timestamps and weights; discuss memory and trade-offs?","answer":"Proposed solution: a segment tree over timestamps; each node maintains a Fenwick tree over compressed weights. insertEvent/eraseEvent update all nodes on the timestamp path and adjust the weight Fenwi","explanation":"## Why This Is Asked\nTests ability to combine range data structures for multi-criteria queries and a k-th statistic within a range. It exposes memory vs. time trade-offs and handling of duplicates.\n\n## Key Concepts\n- Segment tree over time domain\n- Fenwick trees at nodes for weight counts\n- Coordinate compression of timestamps and weights\n- Range sum and k-th order statistics within a range\n- Handling duplicates via counts\n\n## Code Example\n```javascript\nclass EventDS {\n  insertEvent(ts, w) {}\n  eraseEvent(ts, w) {}\n  rangeSum(a, b) { return 0; }\n  findKthWeightInRange(a, b, k) { return -1; }\n}\n```\n\n## Follow-up Questions\n- How would you support unbounded timestamps?\n- What are memory/time trade-offs vs. a wavelet tree alternative?\n- How would you parallelize queries across cores for large workloads?","diagram":"flowchart TD\n  A[Insert/Event] --> B[Timestamp Segment Tree]\n  B --> C[Leaf: Fenwick over weights]\n  A --> D[Range queries use path sums]\n  A --> E[findKthWeightInRange uses weight-domain binary search]","difficulty":"advanced","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Plaid","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T17:17:41.092Z","createdAt":"2026-01-11T17:17:41.093Z"},{"id":"q-701","question":"Design a dynamic multiset of 2D points (duplicates allowed) with insertPoint(x,y), erasePoint(x,y) removing one occurrence, rangeCount(x1,y1,x2,y2), and rangeKthX(x1,y1,x2,y2,k) returning the k-th smallest x in the rectangle (tie by y). Target O(log^2 n) per op. Propose a segment tree over x; each node stores an ordered multiset of (y, unique_id). Explain duplicates handling, updates, rangeCount, and rangeKthX via binary search over x?","answer":"Use a segment tree over x with each node maintaining an ordered multiset of (y, id). Insert/erase updates O(log^2 n) nodes; counts via order_of_key. rangeCount aggregates counts from nodes spanning [x","explanation":"## Why This Is Asked\nAssesses mastery of 2D dynamic data structures and order statistics under updates.\n\n## Key Concepts\n- Segment tree over x with per-node BST of (y, id)\n- Duplicate handling via unique_id\n- RangeCount via node contributions\n- RangeKthX via binary search on x using rangeCount\n- Time: O(log^2 n) per op; memory: O(n log n)\n\n## Code Example\n```javascript\nclass PointDS {\n  insertPoint(x, y) { /* ... */ }\n  erasePoint(x, y) { /* ... */ }\n  rangeCount(x1, y1, x2, y2) { /* ... */ }\n  rangeKthX(x1, y1, x2, y2, k) { /* ... */ }\n}\n```\n\n## Follow-up Questions\n- How would you adapt for large coordinate ranges with compression?\n- How would you optimize for frequent rangeCount queries vs. updates?","diagram":null,"difficulty":"intermediate","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Discord","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T18:25:49.256Z","createdAt":"2026-01-11T18:25:49.256Z"},{"id":"q-710","question":"Design a data structure that maintains a dynamic multiset of strings with operations insert(word), erase(word) removing a single occurrence, countWithPrefix(prefix) returning how many words start with the prefix, and mostFrequentWithPrefix(prefix) returning the most frequent word among those starting with the prefix (tie-break lexicographically). What data structure would you implement, and what are the time complexities and invariants?","answer":"Trie-based approach: each node stores children, a subtreeCount, and bestWord with its frequency in that subtree, plus a global map word->count for duplicates. On insert(word): increment word count, tr","explanation":"## Why This Is Asked\nRealistic text-processing requirement: dynamic, prefix-based retrieval with duplicates. This tests trie augmentation, per-prefix aggregates, and update propagation.\n\n## Key Concepts\n- Trie with per-node aggregates\n- Handling duplicates via global word counts\n- Prefix-range queries through subtree counters\n\n## Code Example\n```javascript\n// Outline only; for reference\nclass Node { constructor(){ this.next=new Map(); this.subCount=0; this.best={word:null,count:0}; } }\n```\n\n## Follow-up Questions\n- How would you handle tie-breaking when two words share max frequency? \n- How would you support bulk insertions or deletions? ","diagram":null,"difficulty":"intermediate","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Scale Ai","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T19:15:36.077Z","createdAt":"2026-01-11T19:15:36.078Z"},{"id":"q-722","question":"Design a data structure for an array of integers that supports point updates update(i, val) and range maximum subarray sum queries maxSubarray(l, r) in O(log n). Explain the segment tree node data (sum, bestPref, bestSuff, bestSub) and the merge logic, including tie-breaking for leftmost subarray and handling entirely negative ranges. Example: start with [1,-2,3,4,-1], update index 2 to 5, query maxSubarray(0,4)?","answer":"Use a segment tree where each node stores four values: sum, bestPref, bestSuff, and bestSub. Merge: sum = L.sum+R.sum; pref = max(L.pref, L.sum+R.pref); suff = max(R.suff, R.sum+L.suff); best = max(L.","explanation":"## Why This Is Asked\nThe problem tests knowledge of segment trees for Kadane-like queries and details of node merging.\n\n## Key Concepts\n- Node stores sum, bestPref, bestSuff, bestSub\n- Merge uses L.sum/R.sum and L.suff+R.pref\n- Tie-break leftmost subarray\n- Negative arrays handling\n\n## Code Example\n```javascript\ntype Node = {sum:number, pref:number, suff:number, best:number};\nfunction merge(L:Node, R:Node):Node {\n  const sum = L.sum + R.sum;\n  const pref = Math.max(L.pref, L.sum + R.pref);\n  const suff = Math.max(R.suff, R.sum + L.suff);\n  const best = Math.max(L.best, R.best, L.suff + R.pref);\n  return {sum, pref, suff, best};\n}\n```\n\n## Follow-up Questions\n- How would you extend for range updates?\n- How does tie-breaking affect correctness?","diagram":null,"difficulty":"intermediate","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Bloomberg"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T20:23:33.860Z","createdAt":"2026-01-11T20:23:33.860Z"},{"id":"q-727","question":"Design a dynamic data structure for weighted 3D points (x,y,t) with weight w. Support insertPoint(x,y,t,w) and erasePoint(x,y,t,w) (duplicates allowed), rangeSum3D(x1,x2,y1,y2,t1,t2) for total weight in the 3D box, and findKthLargestInRange(x1,x2,y1,y2,t1,t2,k) for the k-th largest weight inside the box. Target O(log^3 n) per op; O(log W * log^3 n) for kth; discuss coordinate compression, memory trade-offs, and handling duplicates?","answer":"Use coordinate compression on x, y, and t, and implement a 3D segment-tree (nested trees) to support dynamic inserts/deletes with duplicates. Each node stores aggregated weight sums and counts. rangeS","explanation":"## Why This Is Asked\nTests knowledge of multi-dimensional data structures, dynamic updates with duplicates, and how to support order-statistics within spatial ranges. It also probes trade-offs between dense 3D structures vs sparse representations and the practicality of coordinate compression.\n\n## Key Concepts\n- 3D range queries with dynamic updates\n- Coordinate compression and nested segment trees\n- Order statistics within geometric ranges and handling duplicates\n- Trade-offs: memory vs time, online vs offline approaches\n\n## Code Example\n```javascript\n// Skeleton prototype for 3D segment tree with insert/erase and range query\nclass Node3D {\n  constructor() { /* ... */ }\n  update(x, y, t, w, delta) { /* delta = +w or -w */ }\n  query(x1, x2, y1, y2, t1, t2) { /* returns sum of weights */ }\n  kthInRange(x1,x2,y1,y2,t1,t2,k){ /* via weight-binary-search on tree */ }\n}\n```\n\n## Follow-up Questions\n- How would you optimize memory for sparse data?\n- How would you extend to support dynamic weight updates (change weight w for an existing point)?","diagram":null,"difficulty":"advanced","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Google","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T21:17:28.215Z","createdAt":"2026-01-11T21:17:28.215Z"},{"id":"q-739","question":"Design a dynamic 2D weighted point data structure that supports insertPoint(x,y,w), erasePoint(x,y,w) (duplicates allowed), rangeSum2D(x1,x2,y1,y2) for the total weight in the rectangle, and kthLargestInRectangle(x1,x2,y1,y2,k) for the k-th largest weight inside the rectangle. Aim for average O(log^2 n) per operation; discuss coordinate compression, memory trade-offs, and duplicates handling?","answer":"Use a 2D range tree: segment tree on compressed X; each node stores a Fenwick over Y where each entry is a weight-count map for duplicates. insertPoint/erasePoint update O(log^2 n). rangeSum2D O(log^2","explanation":"## Why This Is Asked\nTests building a practical 2D index with range queries and top-k under dynamic updates, blending geometry and order-statistics.\n\n## Key Concepts\n- 2D range trees and coordinate compression\n- per-node multisets for duplicates\n- binary search on weight domain for kth within rect\n\n## Code Example\n```javascript\n// Sketch: insertPoint updates along X-tree paths, updating inner Y-structures\n```\n\n## Follow-up Questions\n- How would you handle high churning data?\n- Compare 2D range trees vs kd-trees for this workload.","diagram":"flowchart TD\n  A[Insert Point] --> B[Update X-tree]\n  B --> C[Update Y-structures]\n  A --> D[Query RangeSum/Kth]\n  D --> E[Return Result]","difficulty":"advanced","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Hugging Face","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T22:21:13.997Z","createdAt":"2026-01-11T22:21:13.997Z"},{"id":"q-746","question":"Design a dynamic multiset of integers that supports insert(x), erase(x) (one occurrence), countInRange(l, r) for the number of elements in [l, r], and kthSmallestInRange(l, r, k) returning the k-th smallest value among elements in [l, r]. Aim for expected O(log n) updates and O(log n * log U) queries. Explain data structure, balance, and duplicates handling?","answer":"A dynamic 1D order-statistics multiset built on a balanced BST (e.g., a treap) stores duplicates with a per-node count and subtree size. insert/erase adjust counts and sizes in O(log n). rank(x) retur","explanation":"## Why This Is Asked\nTests mastery of advanced order-statistics with dynamic updates and range constraints, a common interview challenge for scalable data services.\n\n## Key Concepts\n- Self-balancing BST with duplicate handling\n- Augmented subtree sizes for rank/select\n- Range counting via rank differences\n- Binary search over value domain to locate kth within a range\n\n## Code Example\n```javascript\nclass Node{ constructor(key){ this.key=key; this.pr=Math.random(); this.cnt=1; this.sz=1; this.l=null; this.r=null; } }\n```\n\n## Follow-up Questions\n- How would you adapt to support range deletions efficiently?\n- Compare treap vs. red-black tree in this context.\n","diagram":null,"difficulty":"advanced","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Google","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T23:20:13.860Z","createdAt":"2026-01-11T23:20:13.860Z"},{"id":"q-749","question":"Design a dynamic weighted string multiset with insertWord(word, w), eraseWord(word, w) (duplicates allowed), sumByPrefix(prefix) returning total weight of words starting with prefix, and kthLargestWeightInPrefix(prefix, k) returning the k-th largest weight among those words. Outline the data structure, invariants, and expected complexities; discuss handling of long words and memory trade-offs?","answer":"Use a prefix trie where each node holds an order-statistics tree (multiset of weights) for all words in its subtree. insertWord/eraseWord traverse the word's path, updating every node's weight multise","explanation":"## Why This Is Asked\nTests the ability to combine a prefix data structure with an order-statistics structure to support prefix-scope queries and duplicate handling, plus memory/time trade-offs.\n\n## Key Concepts\n- Prefix trees with per-node multiset of weights\n- Order-statistics trees for kth queries\n- Duplicate handling and lazy deletions when needed\n\n## Code Example\n```javascript\nclass TrieNode {\n  constructor() {\n    this.children = new Map();\n    this.subWeightOST = new OST(); // hypothetical OST supporting insert, erase, kth\n    this.totalWeight = 0;\n  }\n}\n```\n\n## Follow-up Questions\n- How would you bound memory with very large word vocabularies?\n- How would you adapt for dynamic weight ranges or external weight normalization?","diagram":null,"difficulty":"intermediate","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","IBM"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T01:28:11.710Z","createdAt":"2026-01-12T01:28:11.710Z"},{"id":"q-761","question":"Design a dynamic word-frequency histogram for a text stream. Implement addWord(word) to increment frequency, eraseWord(word) to decrement (removing word when count hits zero), getFrequency(word), and topKWords(n) returning the n most frequent distinct words (ties broken lexicographically). Target average O(log m) per update and O(k log m) for topK, where m is the number of distinct words. Explain approach and data structures you would use?","answer":"Use two structures: a hashmap counts: word -> freq, and a map freq -> an ordered set of words. addWord(word) increments counts and moves word to the new freq bucket; eraseWord(word) decrements and re-","explanation":"## Why This Is Asked\nTests ability to design a dynamic, frequency-based data structure for text streams, common in search, analytics, and logs.\n\n## Key Concepts\n- Dual-map design: word→freq and freq→ordered words\n- Efficient updates by relocating words between buckets\n- Tie-breaking by lexicographic order\n- Corner cases: zero counts, topK when fewer distinct words\n\n## Code Example\n```javascript\nclass WordFreqDS {\n  constructor() {\n    this.counts = new Map();\n    this.buckets = new Map(); // freq -> Set<string>\n  }\n  _ensureBucket(freq) {\n    if (!this.buckets.has(freq)) this.buckets.set(freq, new Set());\n  }\n  addWord(word) {\n    const prev = this.counts.get(word) || 0;\n    if (prev > 0) this.buckets.get(prev).delete(word);\n    const next = prev + 1;\n    this.counts.set(word, next);\n    this._ensureBucket(next);\n    this.buckets.get(next).add(word);\n  }\n  eraseWord(word) {\n    const prev = this.counts.get(word);\n    if (!prev) return;\n    this.buckets.get(prev).delete(word);\n    const next = prev - 1;\n    if (next === 0) {\n      this.counts.delete(word);\n      return;\n    }\n    this.counts.set(word, next);\n    this._ensureBucket(next);\n    this.buckets.get(next).add(word);\n  }\n  getFrequency(word) {\n    return this.counts.get(word) || 0;\n  }\n  topKWords(n) {\n    const freqs = Array.from(this.buckets.keys()).sort((a,b)=>b-a);\n    const res = [];\n    for (const freq of freqs) {\n      const words = Array.from(this.buckets.get(freq)).sort();\n      for (const w of words) {\n        res.push(w);\n        if (res.length === n) return res;\n      }\n    }\n    return res;\n  }\n}\n```\n\n## Follow-up Questions\n- How would you scale for hundreds of millions of words with memory limits?\n- How would you adapt to streaming constraints where topK must be maintained continuously?","diagram":null,"difficulty":"beginner","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T03:50:19.466Z","createdAt":"2026-01-12T03:50:19.466Z"},{"id":"q-774","question":"Design a data structure to manage a dynamic multiset of weighted intervals on the real line. Each interval is [l, r] with weight w; duplicates allowed. Implement insertInterval(l, r, w), eraseInterval(l, r, w) (one occurrence), rangeSumAt(x) returning the total weight of all intervals covering point x, and kthLargestWeightAt(x, k) returning the k-th largest weight among intervals covering x. Target average O(log n) update and O(log n) query; discuss coordinate compression, memory trade-offs, and handling duplicates?","answer":"Propose a segment tree over compressed coordinates; each node stores a multiset of weights for intervals that fully cover that node’s segment. Insert pushes w into multisets along the path; erase remo","explanation":"## Why This Is Asked\nAssess ability to design interval-based data structures with dynamic updates and per-point queries, a non-trivial extension of range trees.\n\n## Key Concepts\n- Interval multiset maintenance\n- Segment tree with per-node multisets\n- Point-query across path for sum\n- Order statistics across unions\n- Coordinate compression and memory trade-offs\n\n## Code Example\n```javascript\nclass IntervalDS {\n  insert(l, r, w) {}\n  erase(l, r, w) {}\n  rangeSumAt(x) {}\n  kthLargestAt(x, k) {}\n}\n```\n\n## Follow-up Questions\n- How would you support range queries for multiple points efficiently?\n- Analyze worst-case vs average guarantees and propose optimizations.","diagram":"flowchart TD\n  A[Input intervals] --> B[Coordinate compression]\n  B --> C[Segment tree with multisets]\n  C --> D[Update: insert/erase]\n  D --> E[Query: rangeSumAt, kthLargestWeightAt]","difficulty":"advanced","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T05:28:21.928Z","createdAt":"2026-01-12T05:28:21.928Z"},{"id":"q-782","question":"Design a fully dynamic data structure to maintain a set of linear cost functions y = m_i x + b_i. Support: addLine(id, m, b), removeLine(id) (one occurrence per id), queryMin(x) returning the minimum cost at x across active lines, and queryKthMin(x, k) returning the k-th smallest cost at x. Domain x in [Xmin, Xmax]. Aim for near O(log X) per operation; discuss how deletions are handled, precision, and memory trade-offs?","answer":"Context: a pricing engine with many suppliers; each supplier defines a line y = m x + c. Implement a fully dynamic data structure supporting: addLine(id, m, c); removeLine(id); queryMin(x); queryKthMi","explanation":"## Why This Is Asked\n\nFully dynamic sets of lines appear in pricing engines and risk models. Deletions complicate classic Li Chao trees; this question tests a candidate's ability to adapt the structure to support removals and order-statistics.\n\n## Key Concepts\n\n- Li Chao segment tree for lines on a fixed x-domain\n- per-node multisets to support deletions\n- extending to kth smallest via an order-statistics structure or value-based binary search\n- handling 64-bit arithmetic and domain compression\n\n## Code Example\n\n```cpp\n// skeleton illustrating structure; actual implementation omitted for brevity\nstruct Line { long long m,b; long long value(long long x) const { return m*x + b; } };\nclass DynLineSet {\n  // addLine, removeLine, queryMin, queryKthMin\n};\n```\n\n## Follow-up Questions\n\n- How would you optimize memory for huge x-domains?\n- How would you ensure robustness against floating-point precision in boundary comparisons?","diagram":"flowchart TD\n  A[AddLine] --> B[Update per-node multisets]\n  B --> C[QueryMin(x)]\n  D[RemoveLine] --> B\n  C --> E[Return min value]\n","difficulty":"advanced","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","DoorDash","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T06:34:09.760Z","createdAt":"2026-01-12T06:34:09.760Z"},{"id":"q-790","question":"Design a persistent 2D dynamic weighted point data structure that supports insertPoint(x,y,w) and erasePoint(x,y,w) (duplicates allowed). Extend with rangeSum2D(x1,x2,y1,y2,version) and kthLargestInRectangle(x1,x2,y1,y2,k,version). Each insertion creates a new version; queries run in O(log^2 n) time. Explain coordinate compression, memory management, and how duplicates are handled across versions?","answer":"Propose a persistent 2D segment-tree: outer tree over compressed x; each node holds a persistent inner tree over compressed y storing total weight. On insert/erase, copy-on-write along O(log n) x-node","explanation":"## Why This Is Asked\nPersistence across versions for 2D range queries with duplicates is practical for time-travel analytics and audit.\n\n## Key Concepts\n- Persistent segment trees with path copying\n- 2D range queries via nested trees\n- Coordinate compression and versioning trade-offs\n- Handling duplicates in leaf weights\n\n## Code Example\n```javascript\nclass PSTNode {\n  constructor(sum=0, left=null, right=null) { this.sum=sum; this.left=left; this.right=right; }\n}\n// skeleton: updateX/Y and query would follow standard persistent path-copying\n```\n\n## Follow-up Questions\n- How would you bound memory growth in practice?\n- How would you support range updates (adding to a rectangle) efficiently?","diagram":null,"difficulty":"advanced","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Databricks","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T07:26:03.242Z","createdAt":"2026-01-12T07:26:03.242Z"},{"id":"q-801","question":"Design a fixed-window streaming data structure: push(x) appends an integer to the window; if the window exceeds size W, remove the oldest value. Implement getKthSmallest(k) to return the k-th smallest value in the current window in O(log W). Propose a concrete structure (e.g., an order-statistics tree with duplicates) and outline push/evict/getKthSmallest, including how duplicates and memory are handled?","answer":"Use a fixed-size deque for the window and an order-statistics tree that stores pairs (value, unique_id) to handle duplicates. Each node tracks subtree size; push(x) inserts (x, id++); if size>W, remov","explanation":"## Why This Is Asked\n\nTests combining streaming data with an order-statistics structure and correct duplicate handling in a practical window scenario. Evaluates dynamic memory management and per-operation costs in real-time workloads (e.g., metrics, logs).\n\n## Key Concepts\n\n- Sliding window mechanics with eviction\n- Order-statistics trees (duplicable keys via (value, id))\n- Rank queries and kth selection\n\n## Code Example\n\n```javascript\n// Skeleton: an OST with insert, erase by (value,id), and kth\nclass Node{ constructor(val,id){ this.val=val; this.id=id; this.left=this.right=null; this.cnt=1; this.sz=1; } }\n```\n\n## Follow-up Questions\n- How would you support changing window size W dynamically?\n- How would you extend to support getKthLargest or range queries?","diagram":null,"difficulty":"intermediate","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Plaid","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T08:33:06.592Z","createdAt":"2026-01-12T08:33:06.592Z"},{"id":"q-805","question":"Design a dynamic forest data structure supporting: addNode(id, value), link(childId, parentId) to attach a child under a parent, cut(childId) to detach a subtree, update(id, delta) to adjust a node's value, pathQuery(u, v) for the sum of values along the path from u to v, and subtreeQuery(u) for the sum of values in the subtree rooted at u. Target amortized O(log n) per operation. Which approach would you pick (Link-Cut Tree vs Euler Tour Tree), and how would you handle edge cases like root changes and duplicate node values?","answer":"Propose a link-cut tree (splay-based). Each node stores val, sum, and a reverse flag. Implement makerRoot, access, link, cut, and update. pathQuery(u,v) exposes the path and returns the path sum; subt","explanation":"## Why This Is Asked\nTests knowledge of dynamic trees and path/subtree aggregates under frequent structural changes, a practical requirement in scalable graph-backed workloads.\n\n## Key Concepts\n- Link-Cut Tree basics: makerRoot, access, splay for path queries.\n- Path vs subtree aggregates; how to maintain sums under reversals.\n- Trade-offs with Euler Tour Trees; when to prefer each.\n- Handling duplicates via unique node IDs and careful memory management.\n- Edge cases: re-rooting, linking a node that already has a parent, cutting roots.\n\n## Code Example\n```java\n// skeleton: Link-Cut Tree node and core ops\nclass Node { long val, sum; boolean rev; Node left, right, parent; int id; }\n```\n\n## Follow-up Questions\n- How would you extend to support min/max path queries?\n- How would you verify amortized guarantees in a real system?","diagram":"flowchart TD\n  A[makerRoot(u)] --> B[access(u)]\n  B --> C[expose path to v]\n  C --> D[pathQuery]\n  E[link(child,parent)] --> F[cut(child)]","difficulty":"advanced","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","Meta","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T09:31:16.011Z","createdAt":"2026-01-12T09:31:16.011Z"},{"id":"q-815","question":"Design a dynamic data structure for a bipartite graph with left indices [1..N] and right indices [1..M]. Each edge (u,v) has weight w; duplicates allowed. Support: - addEdge(u,v,w), eraseEdge(u,v,w) (one occurrence) - rangeSum(uL,uR,vL,vR) total weight of edges with u in [uL,uR] and v in [vL,vR] - kthLargestEdgeWeight(uL,uR,vL,vR,k) the k-th largest edge weight in that submatrix. Aim for ~O(log N log M) per operation; discuss compression, duplicates, and memory trade-offs?","answer":"Use a 2D segment tree over left and right indices. Each node stores: a weight-sum and an order-statistics structure (weighted multiset) over edge weights to answer kthLargest via binary search on a co","explanation":"## Why This Is Asked\nAssesses dynamic 2D range queries with multiplicities, a core DS topic at scale. \n\n## Key Concepts\n- 2D segment tree with per-node weight aggregation\n- Order-statistics on multisets for kthLargest\n- Coordinate compression for weights and indices\n- Handling duplicates via multiplicity counts\n\n## Code Example\n```javascript\n// skeleton for 2D segtree node\nclass Node2D {\n  constructor() {\n    this.sum = 0;\n    this.weightCounts = new Map(); // weight -> count\n  }\n}\n```\n\n## Follow-up Questions\n- How would you adapt to dynamic N and M (growing graphs)?\n- What are the trade-offs vs. a Fenwick-of-Fenwick approach for very large sparse graphs?","diagram":"flowchart TD\n  A[Left Range] --> B[2D Segment Tree]\n  B --> C[RangeSum Query]\n  B --> D[KthLargestQuery]\n  C --> E[Sum of weights]\n  D --> F[Select by weight rank]","difficulty":"intermediate","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","NVIDIA","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T10:25:42.656Z","createdAt":"2026-01-12T10:25:42.656Z"},{"id":"q-820","question":"Design a dynamic sequence structure 'RopeArray' storing an integer array supporting insertAt(i, val), eraseAt(i), rangeSum(l, r), rangeMax(l, r), and getAt(i). How would you implement it to achieve average O(log n) per operation using a balanced tree with implicit keys and augmented fields (size, sum, max), and how would you test with an example sequence?","answer":"Use an implicit-key treap (rope) where each node stores val, priority, size, sum, and max. insertAt(i, v) splits at i, inserts a new node, and merges. eraseAt(i) isolates i, discards it, and merges. r","explanation":"## Why This Is Asked\nTests ability to implement a mutable sequence with range queries using a balanced BST with implicit indices.\n\n## Key Concepts\n- Implicit-key treap\n- Split/merge by position\n- Subtree aggregates: size, sum, max\n- Handling duplicates as separate nodes\n- Correct re-linking during splits/merges\n\n## Code Example\n```javascript\n// Implementation skeleton\nclass Node { constructor(val){ this.val = val; this.pr = Math.random(); this.l = null; this.r = null; this.sz = 1; this.sum = val; this.mx = val; } }\nfunction sz(n){ return n? n.sz:0; }\nfunction upd(n){ if(!n) return; n.sz = 1 + sz(n.l) + sz(n.r); n.sum = n.val + (n.l? n.l.sum:0) + (n.r? n.r.sum:0); n.mx = Math.max(n.val, n.l? n.l.mx:-Infinity, n.r? n.r.mx:-Infinity); }\nfunction split(root, leftSize){ /* ... */ }\nfunction merge(a,b){ /* ... */ }\n// ... core operations omitted for brevity\n```\n\n## Follow-up Questions\n- Extend with rangeAdd(l,r,delta) using lazy propagation.\n- How would you implement persistence for versioned queries?","diagram":"flowchart TD\n  A[Operate] --> B{Split by index}\n  B --> C[Isolate segment or insert]\n  C --> D[Merges back]\n  D --> E[Read aggregates]\n","difficulty":"intermediate","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T11:19:02.973Z","createdAt":"2026-01-12T11:19:02.973Z"},{"id":"q-832","question":"Design a data structure to maintain a dynamic multiset of words with three operations: insertWord(word), eraseWord(word) (one occurrence), and getAnagrams(word) that returns all current words that are anagrams of the given word (including duplicates). Explain how you would store the words, how to compute the canonical signature, and the expected time complexity for updates and queries. For example, after insertWord('listen') and insertWord('silent'), getAnagrams('tinsel') should return ['listen','silent']?","answer":"Store words by signature (the sorted letters of the word). Maintain a signature -> map<word, count> bucket. insertWord(word) computes signature S, increments count for word in bucket S; eraseWord(word","explanation":"## Why This Is Asked\nTests structuring a dynamic, duplicate-friendly grouping based on a stable signature.\n\n## Key Concepts\n- Canonical form via sorted letters for anagram grouping\n- Nested maps to track per-word counts within a signature bucket\n- Amortized update and linear-time retrieval relative to output size\n\n## Code Example\n```javascript\nclass AnagramMultiset {\n  constructor(){ this.sigMap = new Map(); }\n  _sig(w){ return w.split('').sort().join(''); }\n  insert(word){ const s = this._sig(word); const bucket = this.sigMap.get(s) || new Map(); bucket.set(word, (bucket.get(word) || 0) + 1); this.sigMap.set(s, bucket); }\n  erase(word){ const s = this._sig(word); const bucket = this.sigMap.get(s); if(!bucket) return; const c = (bucket.get(word) || 0) - 1; if(c<=0){ bucket.delete(word); if(bucket.size===0) this.sigMap.delete(s);} else { bucket.set(word, c); } }\n  getAnagrams(word){ const s = this._sig(word); const bucket = this.sigMap.get(s); if(!bucket) return []; const res = []; for(const [w,c] of bucket) for(let i=0;i<c;i++) res.push(w); return res; }\n}\n```\n\n## Follow-up Questions\n- How would you handle Unicode normalization and case-insensitive comparisons?\n- What changes if you also need to support a deleteAll(word) operation?","diagram":"flowchart TD\n  A[Start] --> B[Compute signature]\n  B --> C{Update bucket}\n  C -->|insert| D[Increment word count]\n  C -->|erase| E[Decrement word count]\n  D & E --> F[Get anagrams from signature bucket]\n","difficulty":"beginner","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["NVIDIA","Snowflake","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T12:43:44.827Z","createdAt":"2026-01-12T12:43:44.827Z"}],"subChannels":["general"],"companies":["Adobe","Airbnb","Amazon","Anthropic","Apple","Bloomberg","Citadel","Cloudflare","Coinbase","Databricks","Discord","DoorDash","Goldman Sachs","Google","Hashicorp","Hugging Face","IBM","Instacart","LinkedIn","Lyft","Meta","Microsoft","MongoDB","NVIDIA","Netflix","OpenAI","Oracle","PayPal","Plaid","Robinhood","Salesforce","Scale Ai","Slack","Snap","Snowflake","Square","Stripe","Tesla","Twitter","Two Sigma","Zoom"],"stats":{"total":56,"beginner":18,"intermediate":17,"advanced":21,"newThisWeek":35}}