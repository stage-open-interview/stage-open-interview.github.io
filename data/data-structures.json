{"questions":[{"id":"q-677","question":"Design a data structure to maintain dynamic counters for items, supporting add(key, delta), get(key), and getTopK(k) returning the k keys with highest counts, ties broken by key. In a real-time analytics scenario, such as tracking top active users by event count, describe the structure, updates, and complexity trade-offs?","answer":"Use a hash map key→count and recompute top-k on demand by sorting entries in descending order (tie-break by key). On add, adjust the counter and delete if zero. getTopK(k) runs in O(n log n). This kee","explanation":"## Why This Is Asked\nEvaluates knowledge of practical data-structure trade-offs for dynamic ranking. Candidates should justify when to sort on demand vs. maintain auxiliary structures.\n\n## Key Concepts\n- Hash map for O(1) key access\n- On-demand sorting for top-k retrieval\n- Trade-off: simpler updates vs. slower top-k retrieval\n\n## Code Example\n```javascript\nclass CounterTopK {\n  constructor() {\n    this.counts = new Map(); // key -> count\n  }\n  add(key, delta) {\n    const cur = this.counts.get(key) || 0;\n    const next = cur + delta;\n    if (next <= 0) this.counts.delete(key); else this.counts.set(key, next);\n  }\n  get(key) { return this.counts.get(key) || 0; }\n  topK(k) {\n    const arr = Array.from(this.counts.entries());\n    arr.sort((a,b)=> b[1]-a[1] || a[0].localeCompare(b[0]));\n    return arr.slice(0,k).map(([key]) => key);\n  }\n}\n```","diagram":null,"difficulty":"intermediate","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Discord","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T15:56:10.144Z","createdAt":"2026-01-11T15:56:10.144Z"},{"id":"q-686","question":"Design a data structure that maintains a multiset of integers with insert(x), erase(x) for one occurrence, findMedian(), and findKthSmallest(k). Achieve O(log n) time per operation on average. Explain data layout, invariants, and handling duplicates and lazy deletions. For example: insert 1,2,3,4,5; erase 3; what is median and findKthSmallest(2)?","answer":"Propose a two-heap design (lower half in a max-heap, upper half in a min-heap) with a hashmap for lazy deletions to support erase(x) (one occurrence). Balance so the sizes differ by at most 1; prune l","explanation":"## Why This Is Asked\n\nTests understanding of advanced DS combining heaps and lazy deletion to support dynamic order statistics.\n\n## Key Concepts\n\n- Two-heap maintenance for median\n- Lazy deletion with a hashmap for duplicates\n- Order-statistics and findKthSmallest via balanced heaps\n\n## Code Example\n\n```javascript\nclass DualHeapMedianSet {\n  constructor(){ /* ... skeleton ... */ }\n  insert(x){ /* ... */ }\n  erase(x){ /* ... */ }\n  findMedian(){ /* ... */ }\n  findKthSmallest(k){ /* ... */ }\n}\n```\n\n## Follow-up Questions\n\n- How do you handle many duplicates efficiently?\n- How would you implement findKthSmallest(k) with guaranteed worst-case O(log n) time?","diagram":null,"difficulty":"advanced","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Oracle","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T16:22:02.207Z","createdAt":"2026-01-11T16:22:02.207Z"},{"id":"q-697","question":"Design a time-weighted event multiset data structure. Supports insertEvent(ts, w), eraseEvent(ts, w) removing one occurrence, rangeSum(a, b) returning the sum of weights for events with timestamps in [a, b], and findKthWeightInRange(a, b, k) returning the k-th smallest weight among events with timestamps in [a, b]. Target O(log^2 n) per operation; handle duplicate timestamps and weights; discuss memory and trade-offs?","answer":"Proposed solution: a segment tree over timestamps; each node maintains a Fenwick tree over compressed weights. insertEvent/eraseEvent update all nodes on the timestamp path and adjust the weight Fenwi","explanation":"## Why This Is Asked\nTests ability to combine range data structures for multi-criteria queries and a k-th statistic within a range. It exposes memory vs. time trade-offs and handling of duplicates.\n\n## Key Concepts\n- Segment tree over time domain\n- Fenwick trees at nodes for weight counts\n- Coordinate compression of timestamps and weights\n- Range sum and k-th order statistics within a range\n- Handling duplicates via counts\n\n## Code Example\n```javascript\nclass EventDS {\n  insertEvent(ts, w) {}\n  eraseEvent(ts, w) {}\n  rangeSum(a, b) { return 0; }\n  findKthWeightInRange(a, b, k) { return -1; }\n}\n```\n\n## Follow-up Questions\n- How would you support unbounded timestamps?\n- What are memory/time trade-offs vs. a wavelet tree alternative?\n- How would you parallelize queries across cores for large workloads?","diagram":"flowchart TD\n  A[Insert/Event] --> B[Timestamp Segment Tree]\n  B --> C[Leaf: Fenwick over weights]\n  A --> D[Range queries use path sums]\n  A --> E[findKthWeightInRange uses weight-domain binary search]","difficulty":"advanced","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Plaid","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T17:17:41.092Z","createdAt":"2026-01-11T17:17:41.093Z"},{"id":"q-701","question":"Design a dynamic multiset of 2D points (duplicates allowed) with insertPoint(x,y), erasePoint(x,y) removing one occurrence, rangeCount(x1,y1,x2,y2), and rangeKthX(x1,y1,x2,y2,k) returning the k-th smallest x in the rectangle (tie by y). Target O(log^2 n) per op. Propose a segment tree over x; each node stores an ordered multiset of (y, unique_id). Explain duplicates handling, updates, rangeCount, and rangeKthX via binary search over x?","answer":"Use a segment tree over x with each node maintaining an ordered multiset of (y, id). Insert/erase updates O(log^2 n) nodes; counts via order_of_key. rangeCount aggregates counts from nodes spanning [x","explanation":"## Why This Is Asked\nAssesses mastery of 2D dynamic data structures and order statistics under updates.\n\n## Key Concepts\n- Segment tree over x with per-node BST of (y, id)\n- Duplicate handling via unique_id\n- RangeCount via node contributions\n- RangeKthX via binary search on x using rangeCount\n- Time: O(log^2 n) per op; memory: O(n log n)\n\n## Code Example\n```javascript\nclass PointDS {\n  insertPoint(x, y) { /* ... */ }\n  erasePoint(x, y) { /* ... */ }\n  rangeCount(x1, y1, x2, y2) { /* ... */ }\n  rangeKthX(x1, y1, x2, y2, k) { /* ... */ }\n}\n```\n\n## Follow-up Questions\n- How would you adapt for large coordinate ranges with compression?\n- How would you optimize for frequent rangeCount queries vs. updates?","diagram":null,"difficulty":"intermediate","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Discord","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T18:25:49.256Z","createdAt":"2026-01-11T18:25:49.256Z"},{"id":"q-710","question":"Design a data structure that maintains a dynamic multiset of strings with operations insert(word), erase(word) removing a single occurrence, countWithPrefix(prefix) returning how many words start with the prefix, and mostFrequentWithPrefix(prefix) returning the most frequent word among those starting with the prefix (tie-break lexicographically). What data structure would you implement, and what are the time complexities and invariants?","answer":"Trie-based approach: each node stores children, a subtreeCount, and bestWord with its frequency in that subtree, plus a global map word->count for duplicates. On insert(word): increment word count, tr","explanation":"## Why This Is Asked\nRealistic text-processing requirement: dynamic, prefix-based retrieval with duplicates. This tests trie augmentation, per-prefix aggregates, and update propagation.\n\n## Key Concepts\n- Trie with per-node aggregates\n- Handling duplicates via global word counts\n- Prefix-range queries through subtree counters\n\n## Code Example\n```javascript\n// Outline only; for reference\nclass Node { constructor(){ this.next=new Map(); this.subCount=0; this.best={word:null,count:0}; } }\n```\n\n## Follow-up Questions\n- How would you handle tie-breaking when two words share max frequency? \n- How would you support bulk insertions or deletions? ","diagram":null,"difficulty":"intermediate","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Scale Ai","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T19:15:36.077Z","createdAt":"2026-01-11T19:15:36.078Z"},{"id":"q-722","question":"Design a data structure for an array of integers that supports point updates update(i, val) and range maximum subarray sum queries maxSubarray(l, r) in O(log n). Explain the segment tree node data (sum, bestPref, bestSuff, bestSub) and the merge logic, including tie-breaking for leftmost subarray and handling entirely negative ranges. Example: start with [1,-2,3,4,-1], update index 2 to 5, query maxSubarray(0,4)?","answer":"Use a segment tree where each node stores four values: sum, bestPref, bestSuff, and bestSub. Merge: sum = L.sum+R.sum; pref = max(L.pref, L.sum+R.pref); suff = max(R.suff, R.sum+L.suff); best = max(L.","explanation":"## Why This Is Asked\nThe problem tests knowledge of segment trees for Kadane-like queries and details of node merging.\n\n## Key Concepts\n- Node stores sum, bestPref, bestSuff, bestSub\n- Merge uses L.sum/R.sum and L.suff+R.pref\n- Tie-break leftmost subarray\n- Negative arrays handling\n\n## Code Example\n```javascript\ntype Node = {sum:number, pref:number, suff:number, best:number};\nfunction merge(L:Node, R:Node):Node {\n  const sum = L.sum + R.sum;\n  const pref = Math.max(L.pref, L.sum + R.pref);\n  const suff = Math.max(R.suff, R.sum + L.suff);\n  const best = Math.max(L.best, R.best, L.suff + R.pref);\n  return {sum, pref, suff, best};\n}\n```\n\n## Follow-up Questions\n- How would you extend for range updates?\n- How does tie-breaking affect correctness?","diagram":null,"difficulty":"intermediate","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Bloomberg"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T20:23:33.860Z","createdAt":"2026-01-11T20:23:33.860Z"},{"id":"q-727","question":"Design a dynamic data structure for weighted 3D points (x,y,t) with weight w. Support insertPoint(x,y,t,w) and erasePoint(x,y,t,w) (duplicates allowed), rangeSum3D(x1,x2,y1,y2,t1,t2) for total weight in the 3D box, and findKthLargestInRange(x1,x2,y1,y2,t1,t2,k) for the k-th largest weight inside the box. Target O(log^3 n) per op; O(log W * log^3 n) for kth; discuss coordinate compression, memory trade-offs, and handling duplicates?","answer":"Use coordinate compression on x, y, and t, and implement a 3D segment-tree (nested trees) to support dynamic inserts/deletes with duplicates. Each node stores aggregated weight sums and counts. rangeS","explanation":"## Why This Is Asked\nTests knowledge of multi-dimensional data structures, dynamic updates with duplicates, and how to support order-statistics within spatial ranges. It also probes trade-offs between dense 3D structures vs sparse representations and the practicality of coordinate compression.\n\n## Key Concepts\n- 3D range queries with dynamic updates\n- Coordinate compression and nested segment trees\n- Order statistics within geometric ranges and handling duplicates\n- Trade-offs: memory vs time, online vs offline approaches\n\n## Code Example\n```javascript\n// Skeleton prototype for 3D segment tree with insert/erase and range query\nclass Node3D {\n  constructor() { /* ... */ }\n  update(x, y, t, w, delta) { /* delta = +w or -w */ }\n  query(x1, x2, y1, y2, t1, t2) { /* returns sum of weights */ }\n  kthInRange(x1,x2,y1,y2,t1,t2,k){ /* via weight-binary-search on tree */ }\n}\n```\n\n## Follow-up Questions\n- How would you optimize memory for sparse data?\n- How would you extend to support dynamic weight updates (change weight w for an existing point)?","diagram":null,"difficulty":"advanced","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Google","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T21:17:28.215Z","createdAt":"2026-01-11T21:17:28.215Z"},{"id":"q-739","question":"Design a dynamic 2D weighted point data structure that supports insertPoint(x,y,w), erasePoint(x,y,w) (duplicates allowed), rangeSum2D(x1,x2,y1,y2) for the total weight in the rectangle, and kthLargestInRectangle(x1,x2,y1,y2,k) for the k-th largest weight inside the rectangle. Aim for average O(log^2 n) per operation; discuss coordinate compression, memory trade-offs, and duplicates handling?","answer":"Use a 2D range tree: segment tree on compressed X; each node stores a Fenwick over Y where each entry is a weight-count map for duplicates. insertPoint/erasePoint update O(log^2 n). rangeSum2D O(log^2","explanation":"## Why This Is Asked\nTests building a practical 2D index with range queries and top-k under dynamic updates, blending geometry and order-statistics.\n\n## Key Concepts\n- 2D range trees and coordinate compression\n- per-node multisets for duplicates\n- binary search on weight domain for kth within rect\n\n## Code Example\n```javascript\n// Sketch: insertPoint updates along X-tree paths, updating inner Y-structures\n```\n\n## Follow-up Questions\n- How would you handle high churning data?\n- Compare 2D range trees vs kd-trees for this workload.","diagram":"flowchart TD\n  A[Insert Point] --> B[Update X-tree]\n  B --> C[Update Y-structures]\n  A --> D[Query RangeSum/Kth]\n  D --> E[Return Result]","difficulty":"advanced","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Hugging Face","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T22:21:13.997Z","createdAt":"2026-01-11T22:21:13.997Z"},{"id":"q-746","question":"Design a dynamic multiset of integers that supports insert(x), erase(x) (one occurrence), countInRange(l, r) for the number of elements in [l, r], and kthSmallestInRange(l, r, k) returning the k-th smallest value among elements in [l, r]. Aim for expected O(log n) updates and O(log n * log U) queries. Explain data structure, balance, and duplicates handling?","answer":"A dynamic 1D order-statistics multiset built on a balanced BST (e.g., a treap) stores duplicates with a per-node count and subtree size. insert/erase adjust counts and sizes in O(log n). rank(x) retur","explanation":"## Why This Is Asked\nTests mastery of advanced order-statistics with dynamic updates and range constraints, a common interview challenge for scalable data services.\n\n## Key Concepts\n- Self-balancing BST with duplicate handling\n- Augmented subtree sizes for rank/select\n- Range counting via rank differences\n- Binary search over value domain to locate kth within a range\n\n## Code Example\n```javascript\nclass Node{ constructor(key){ this.key=key; this.pr=Math.random(); this.cnt=1; this.sz=1; this.l=null; this.r=null; } }\n```\n\n## Follow-up Questions\n- How would you adapt to support range deletions efficiently?\n- Compare treap vs. red-black tree in this context.\n","diagram":null,"difficulty":"advanced","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Google","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T23:20:13.860Z","createdAt":"2026-01-11T23:20:13.860Z"},{"id":"q-749","question":"Design a dynamic weighted string multiset with insertWord(word, w), eraseWord(word, w) (duplicates allowed), sumByPrefix(prefix) returning total weight of words starting with prefix, and kthLargestWeightInPrefix(prefix, k) returning the k-th largest weight among those words. Outline the data structure, invariants, and expected complexities; discuss handling of long words and memory trade-offs?","answer":"Use a prefix trie where each node holds an order-statistics tree (multiset of weights) for all words in its subtree. insertWord/eraseWord traverse the word's path, updating every node's weight multise","explanation":"## Why This Is Asked\nTests the ability to combine a prefix data structure with an order-statistics structure to support prefix-scope queries and duplicate handling, plus memory/time trade-offs.\n\n## Key Concepts\n- Prefix trees with per-node multiset of weights\n- Order-statistics trees for kth queries\n- Duplicate handling and lazy deletions when needed\n\n## Code Example\n```javascript\nclass TrieNode {\n  constructor() {\n    this.children = new Map();\n    this.subWeightOST = new OST(); // hypothetical OST supporting insert, erase, kth\n    this.totalWeight = 0;\n  }\n}\n```\n\n## Follow-up Questions\n- How would you bound memory with very large word vocabularies?\n- How would you adapt for dynamic weight ranges or external weight normalization?","diagram":null,"difficulty":"intermediate","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","IBM"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T01:28:11.710Z","createdAt":"2026-01-12T01:28:11.710Z"},{"id":"q-761","question":"Design a dynamic word-frequency histogram for a text stream. Implement addWord(word) to increment frequency, eraseWord(word) to decrement (removing word when count hits zero), getFrequency(word), and topKWords(n) returning the n most frequent distinct words (ties broken lexicographically). Target average O(log m) per update and O(k log m) for topK, where m is the number of distinct words. Explain approach and data structures you would use?","answer":"Use two structures: a hashmap counts: word -> freq, and a map freq -> an ordered set of words. addWord(word) increments counts and moves word to the new freq bucket; eraseWord(word) decrements and re-","explanation":"## Why This Is Asked\nTests ability to design a dynamic, frequency-based data structure for text streams, common in search, analytics, and logs.\n\n## Key Concepts\n- Dual-map design: word→freq and freq→ordered words\n- Efficient updates by relocating words between buckets\n- Tie-breaking by lexicographic order\n- Corner cases: zero counts, topK when fewer distinct words\n\n## Code Example\n```javascript\nclass WordFreqDS {\n  constructor() {\n    this.counts = new Map();\n    this.buckets = new Map(); // freq -> Set<string>\n  }\n  _ensureBucket(freq) {\n    if (!this.buckets.has(freq)) this.buckets.set(freq, new Set());\n  }\n  addWord(word) {\n    const prev = this.counts.get(word) || 0;\n    if (prev > 0) this.buckets.get(prev).delete(word);\n    const next = prev + 1;\n    this.counts.set(word, next);\n    this._ensureBucket(next);\n    this.buckets.get(next).add(word);\n  }\n  eraseWord(word) {\n    const prev = this.counts.get(word);\n    if (!prev) return;\n    this.buckets.get(prev).delete(word);\n    const next = prev - 1;\n    if (next === 0) {\n      this.counts.delete(word);\n      return;\n    }\n    this.counts.set(word, next);\n    this._ensureBucket(next);\n    this.buckets.get(next).add(word);\n  }\n  getFrequency(word) {\n    return this.counts.get(word) || 0;\n  }\n  topKWords(n) {\n    const freqs = Array.from(this.buckets.keys()).sort((a,b)=>b-a);\n    const res = [];\n    for (const freq of freqs) {\n      const words = Array.from(this.buckets.get(freq)).sort();\n      for (const w of words) {\n        res.push(w);\n        if (res.length === n) return res;\n      }\n    }\n    return res;\n  }\n}\n```\n\n## Follow-up Questions\n- How would you scale for hundreds of millions of words with memory limits?\n- How would you adapt to streaming constraints where topK must be maintained continuously?","diagram":null,"difficulty":"beginner","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T03:50:19.466Z","createdAt":"2026-01-12T03:50:19.466Z"},{"id":"q-774","question":"Design a data structure to manage a dynamic multiset of weighted intervals on the real line. Each interval is [l, r] with weight w; duplicates allowed. Implement insertInterval(l, r, w), eraseInterval(l, r, w) (one occurrence), rangeSumAt(x) returning the total weight of all intervals covering point x, and kthLargestWeightAt(x, k) returning the k-th largest weight among intervals covering x. Target average O(log n) update and O(log n) query; discuss coordinate compression, memory trade-offs, and handling duplicates?","answer":"Propose a segment tree over compressed coordinates; each node stores a multiset of weights for intervals that fully cover that node’s segment. Insert pushes w into multisets along the path; erase remo","explanation":"## Why This Is Asked\nAssess ability to design interval-based data structures with dynamic updates and per-point queries, a non-trivial extension of range trees.\n\n## Key Concepts\n- Interval multiset maintenance\n- Segment tree with per-node multisets\n- Point-query across path for sum\n- Order statistics across unions\n- Coordinate compression and memory trade-offs\n\n## Code Example\n```javascript\nclass IntervalDS {\n  insert(l, r, w) {}\n  erase(l, r, w) {}\n  rangeSumAt(x) {}\n  kthLargestAt(x, k) {}\n}\n```\n\n## Follow-up Questions\n- How would you support range queries for multiple points efficiently?\n- Analyze worst-case vs average guarantees and propose optimizations.","diagram":"flowchart TD\n  A[Input intervals] --> B[Coordinate compression]\n  B --> C[Segment tree with multisets]\n  C --> D[Update: insert/erase]\n  D --> E[Query: rangeSumAt, kthLargestWeightAt]","difficulty":"advanced","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T05:28:21.928Z","createdAt":"2026-01-12T05:28:21.928Z"},{"id":"q-782","question":"Design a fully dynamic data structure to maintain a set of linear cost functions y = m_i x + b_i. Support: addLine(id, m, b), removeLine(id) (one occurrence per id), queryMin(x) returning the minimum cost at x across active lines, and queryKthMin(x, k) returning the k-th smallest cost at x. Domain x in [Xmin, Xmax]. Aim for near O(log X) per operation; discuss how deletions are handled, precision, and memory trade-offs?","answer":"Context: a pricing engine with many suppliers; each supplier defines a line y = m x + c. Implement a fully dynamic data structure supporting: addLine(id, m, c); removeLine(id); queryMin(x); queryKthMi","explanation":"## Why This Is Asked\n\nFully dynamic sets of lines appear in pricing engines and risk models. Deletions complicate classic Li Chao trees; this question tests a candidate's ability to adapt the structure to support removals and order-statistics.\n\n## Key Concepts\n\n- Li Chao segment tree for lines on a fixed x-domain\n- per-node multisets to support deletions\n- extending to kth smallest via an order-statistics structure or value-based binary search\n- handling 64-bit arithmetic and domain compression\n\n## Code Example\n\n```cpp\n// skeleton illustrating structure; actual implementation omitted for brevity\nstruct Line { long long m,b; long long value(long long x) const { return m*x + b; } };\nclass DynLineSet {\n  // addLine, removeLine, queryMin, queryKthMin\n};\n```\n\n## Follow-up Questions\n\n- How would you optimize memory for huge x-domains?\n- How would you ensure robustness against floating-point precision in boundary comparisons?","diagram":"flowchart TD\n  A[AddLine] --> B[Update per-node multisets]\n  B --> C[QueryMin(x)]\n  D[RemoveLine] --> B\n  C --> E[Return min value]\n","difficulty":"advanced","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","DoorDash","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T06:34:09.760Z","createdAt":"2026-01-12T06:34:09.760Z"},{"id":"q-790","question":"Design a persistent 2D dynamic weighted point data structure that supports insertPoint(x,y,w) and erasePoint(x,y,w) (duplicates allowed). Extend with rangeSum2D(x1,x2,y1,y2,version) and kthLargestInRectangle(x1,x2,y1,y2,k,version). Each insertion creates a new version; queries run in O(log^2 n) time. Explain coordinate compression, memory management, and how duplicates are handled across versions?","answer":"Propose a persistent 2D segment-tree: outer tree over compressed x; each node holds a persistent inner tree over compressed y storing total weight. On insert/erase, copy-on-write along O(log n) x-node","explanation":"## Why This Is Asked\nPersistence across versions for 2D range queries with duplicates is practical for time-travel analytics and audit.\n\n## Key Concepts\n- Persistent segment trees with path copying\n- 2D range queries via nested trees\n- Coordinate compression and versioning trade-offs\n- Handling duplicates in leaf weights\n\n## Code Example\n```javascript\nclass PSTNode {\n  constructor(sum=0, left=null, right=null) { this.sum=sum; this.left=left; this.right=right; }\n}\n// skeleton: updateX/Y and query would follow standard persistent path-copying\n```\n\n## Follow-up Questions\n- How would you bound memory growth in practice?\n- How would you support range updates (adding to a rectangle) efficiently?","diagram":null,"difficulty":"advanced","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Databricks","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T07:26:03.242Z","createdAt":"2026-01-12T07:26:03.242Z"},{"id":"q-801","question":"Design a fixed-window streaming data structure: push(x) appends an integer to the window; if the window exceeds size W, remove the oldest value. Implement getKthSmallest(k) to return the k-th smallest value in the current window in O(log W). Propose a concrete structure (e.g., an order-statistics tree with duplicates) and outline push/evict/getKthSmallest, including how duplicates and memory are handled?","answer":"Use a fixed-size deque for the window and an order-statistics tree that stores pairs (value, unique_id) to handle duplicates. Each node tracks subtree size; push(x) inserts (x, id++); if size>W, remov","explanation":"## Why This Is Asked\n\nTests combining streaming data with an order-statistics structure and correct duplicate handling in a practical window scenario. Evaluates dynamic memory management and per-operation costs in real-time workloads (e.g., metrics, logs).\n\n## Key Concepts\n\n- Sliding window mechanics with eviction\n- Order-statistics trees (duplicable keys via (value, id))\n- Rank queries and kth selection\n\n## Code Example\n\n```javascript\n// Skeleton: an OST with insert, erase by (value,id), and kth\nclass Node{ constructor(val,id){ this.val=val; this.id=id; this.left=this.right=null; this.cnt=1; this.sz=1; } }\n```\n\n## Follow-up Questions\n- How would you support changing window size W dynamically?\n- How would you extend to support getKthLargest or range queries?","diagram":null,"difficulty":"intermediate","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Plaid","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T08:33:06.592Z","createdAt":"2026-01-12T08:33:06.592Z"},{"id":"q-805","question":"Design a dynamic forest data structure supporting: addNode(id, value), link(childId, parentId) to attach a child under a parent, cut(childId) to detach a subtree, update(id, delta) to adjust a node's value, pathQuery(u, v) for the sum of values along the path from u to v, and subtreeQuery(u) for the sum of values in the subtree rooted at u. Target amortized O(log n) per operation. Which approach would you pick (Link-Cut Tree vs Euler Tour Tree), and how would you handle edge cases like root changes and duplicate node values?","answer":"Propose a link-cut tree (splay-based). Each node stores val, sum, and a reverse flag. Implement makerRoot, access, link, cut, and update. pathQuery(u,v) exposes the path and returns the path sum; subt","explanation":"## Why This Is Asked\nTests knowledge of dynamic trees and path/subtree aggregates under frequent structural changes, a practical requirement in scalable graph-backed workloads.\n\n## Key Concepts\n- Link-Cut Tree basics: makerRoot, access, splay for path queries.\n- Path vs subtree aggregates; how to maintain sums under reversals.\n- Trade-offs with Euler Tour Trees; when to prefer each.\n- Handling duplicates via unique node IDs and careful memory management.\n- Edge cases: re-rooting, linking a node that already has a parent, cutting roots.\n\n## Code Example\n```java\n// skeleton: Link-Cut Tree node and core ops\nclass Node { long val, sum; boolean rev; Node left, right, parent; int id; }\n```\n\n## Follow-up Questions\n- How would you extend to support min/max path queries?\n- How would you verify amortized guarantees in a real system?","diagram":"flowchart TD\n  A[makerRoot(u)] --> B[access(u)]\n  B --> C[expose path to v]\n  C --> D[pathQuery]\n  E[link(child,parent)] --> F[cut(child)]","difficulty":"advanced","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","Meta","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T09:31:16.011Z","createdAt":"2026-01-12T09:31:16.011Z"},{"id":"q-815","question":"Design a dynamic data structure for a bipartite graph with left indices [1..N] and right indices [1..M]. Each edge (u,v) has weight w; duplicates allowed. Support: - addEdge(u,v,w), eraseEdge(u,v,w) (one occurrence) - rangeSum(uL,uR,vL,vR) total weight of edges with u in [uL,uR] and v in [vL,vR] - kthLargestEdgeWeight(uL,uR,vL,vR,k) the k-th largest edge weight in that submatrix. Aim for ~O(log N log M) per operation; discuss compression, duplicates, and memory trade-offs?","answer":"Use a 2D segment tree over left and right indices. Each node stores: a weight-sum and an order-statistics structure (weighted multiset) over edge weights to answer kthLargest via binary search on a co","explanation":"## Why This Is Asked\nAssesses dynamic 2D range queries with multiplicities, a core DS topic at scale. \n\n## Key Concepts\n- 2D segment tree with per-node weight aggregation\n- Order-statistics on multisets for kthLargest\n- Coordinate compression for weights and indices\n- Handling duplicates via multiplicity counts\n\n## Code Example\n```javascript\n// skeleton for 2D segtree node\nclass Node2D {\n  constructor() {\n    this.sum = 0;\n    this.weightCounts = new Map(); // weight -> count\n  }\n}\n```\n\n## Follow-up Questions\n- How would you adapt to dynamic N and M (growing graphs)?\n- What are the trade-offs vs. a Fenwick-of-Fenwick approach for very large sparse graphs?","diagram":"flowchart TD\n  A[Left Range] --> B[2D Segment Tree]\n  B --> C[RangeSum Query]\n  B --> D[KthLargestQuery]\n  C --> E[Sum of weights]\n  D --> F[Select by weight rank]","difficulty":"intermediate","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","NVIDIA","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T10:25:42.656Z","createdAt":"2026-01-12T10:25:42.656Z"},{"id":"q-820","question":"Design a dynamic sequence structure 'RopeArray' storing an integer array supporting insertAt(i, val), eraseAt(i), rangeSum(l, r), rangeMax(l, r), and getAt(i). How would you implement it to achieve average O(log n) per operation using a balanced tree with implicit keys and augmented fields (size, sum, max), and how would you test with an example sequence?","answer":"Use an implicit-key treap (rope) where each node stores val, priority, size, sum, and max. insertAt(i, v) splits at i, inserts a new node, and merges. eraseAt(i) isolates i, discards it, and merges. r","explanation":"## Why This Is Asked\nTests ability to implement a mutable sequence with range queries using a balanced BST with implicit indices.\n\n## Key Concepts\n- Implicit-key treap\n- Split/merge by position\n- Subtree aggregates: size, sum, max\n- Handling duplicates as separate nodes\n- Correct re-linking during splits/merges\n\n## Code Example\n```javascript\n// Implementation skeleton\nclass Node { constructor(val){ this.val = val; this.pr = Math.random(); this.l = null; this.r = null; this.sz = 1; this.sum = val; this.mx = val; } }\nfunction sz(n){ return n? n.sz:0; }\nfunction upd(n){ if(!n) return; n.sz = 1 + sz(n.l) + sz(n.r); n.sum = n.val + (n.l? n.l.sum:0) + (n.r? n.r.sum:0); n.mx = Math.max(n.val, n.l? n.l.mx:-Infinity, n.r? n.r.mx:-Infinity); }\nfunction split(root, leftSize){ /* ... */ }\nfunction merge(a,b){ /* ... */ }\n// ... core operations omitted for brevity\n```\n\n## Follow-up Questions\n- Extend with rangeAdd(l,r,delta) using lazy propagation.\n- How would you implement persistence for versioned queries?","diagram":"flowchart TD\n  A[Operate] --> B{Split by index}\n  B --> C[Isolate segment or insert]\n  C --> D[Merges back]\n  D --> E[Read aggregates]\n","difficulty":"intermediate","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T11:19:02.973Z","createdAt":"2026-01-12T11:19:02.973Z"},{"id":"q-832","question":"Design a data structure to maintain a dynamic multiset of words with three operations: insertWord(word), eraseWord(word) (one occurrence), and getAnagrams(word) that returns all current words that are anagrams of the given word (including duplicates). Explain how you would store the words, how to compute the canonical signature, and the expected time complexity for updates and queries. For example, after insertWord('listen') and insertWord('silent'), getAnagrams('tinsel') should return ['listen','silent']?","answer":"Store words by signature (the sorted letters of the word). Maintain a signature -> map<word, count> bucket. insertWord(word) computes signature S, increments count for word in bucket S; eraseWord(word","explanation":"## Why This Is Asked\nTests structuring a dynamic, duplicate-friendly grouping based on a stable signature.\n\n## Key Concepts\n- Canonical form via sorted letters for anagram grouping\n- Nested maps to track per-word counts within a signature bucket\n- Amortized update and linear-time retrieval relative to output size\n\n## Code Example\n```javascript\nclass AnagramMultiset {\n  constructor(){ this.sigMap = new Map(); }\n  _sig(w){ return w.split('').sort().join(''); }\n  insert(word){ const s = this._sig(word); const bucket = this.sigMap.get(s) || new Map(); bucket.set(word, (bucket.get(word) || 0) + 1); this.sigMap.set(s, bucket); }\n  erase(word){ const s = this._sig(word); const bucket = this.sigMap.get(s); if(!bucket) return; const c = (bucket.get(word) || 0) - 1; if(c<=0){ bucket.delete(word); if(bucket.size===0) this.sigMap.delete(s);} else { bucket.set(word, c); } }\n  getAnagrams(word){ const s = this._sig(word); const bucket = this.sigMap.get(s); if(!bucket) return []; const res = []; for(const [w,c] of bucket) for(let i=0;i<c;i++) res.push(w); return res; }\n}\n```\n\n## Follow-up Questions\n- How would you handle Unicode normalization and case-insensitive comparisons?\n- What changes if you also need to support a deleteAll(word) operation?","diagram":"flowchart TD\n  A[Start] --> B[Compute signature]\n  B --> C{Update bucket}\n  C -->|insert| D[Increment word count]\n  C -->|erase| E[Decrement word count]\n  D & E --> F[Get anagrams from signature bucket]\n","difficulty":"beginner","tags":["data-structures"],"channel":"data-structures","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["NVIDIA","Snowflake","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T12:43:44.827Z","createdAt":"2026-01-12T12:43:44.827Z"}],"subChannels":["general"],"companies":["Adobe","Airbnb","Amazon","Apple","Bloomberg","Citadel","Cloudflare","Databricks","Discord","DoorDash","Google","Hashicorp","Hugging Face","IBM","Lyft","Meta","Microsoft","NVIDIA","Netflix","Oracle","PayPal","Plaid","Scale Ai","Slack","Snap","Snowflake","Tesla","Twitter"],"stats":{"total":19,"beginner":2,"intermediate":8,"advanced":9,"newThisWeek":19}}