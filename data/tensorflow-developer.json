{"questions":[{"id":"tensorflow-developer-building-models-1768173557677-0","question":"In a Kubernetes cluster running a TFJob with multiple pods, you need synchronous distributed training across machines with minimal code changes. Which TensorFlow strategy should you use?","answer":"[{\"id\":\"a\",\"text\":\"tf.distribute.MirroredStrategy\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"tf.distribute.SingleWorkerMirroredStrategy\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"tf.distribute.MultiWorkerMirroredStrategy\",\"isCorrect\":true},{\"id\":\"d\",\"text\":\"tf.distribute.TPUStrategy\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption C is correct: tf.distribute.MultiWorkerMirroredStrategy coordinates synchronous updates across multiple workers in a distributed cluster, which is suitable for multi-node training on Kubernetes.\n\n## Why Other Options Are Wrong\n- Option A is incorrect because MirroredStrategy runs on a single machine with multiple GPUs, not across machines.\n- Option B is incorrect because SingleWorkerMirroredStrategy is limited to a single worker, not multi-node setups.\n- Option D is incorrect because TPUStrategy targets TPUs and is not applicable to a generic multi-machine GPU Kubernetes cluster.\n\n## Key Concepts\n- tf.distribute.MultiWorkerMirroredStrategy enables synchronous distributed training across multiple workers.\n- Requires a cluster configuration (TF_CONFIG or Kubernetes TFJob) and scope-based model construction.\n- Works well with Keras model.fit inside strategy.scope().\n\n## Real-World Application\n- Deploy on Kubernetes using TFJob with multiple pods; align GPU resources; monitor convergence with synchronized gradients while minimizing code changes to your model/training loop.","diagram":null,"difficulty":"intermediate","tags":["TensorFlow","Distributed Training","Kubernetes","TFJob","AWS-EKS","TFServing","TF 2.x","certification-mcq","domain-weight-30"],"channel":"tensorflow-developer","subChannel":"building-models","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T23:19:17.678Z","createdAt":"2026-01-11 23:19:18"},{"id":"tensorflow-developer-building-models-1768173557677-1","question":"You are training on cloud GPUs and want to automatically preserve the best model weights based on validation accuracy, without accumulating multiple copies of the model. Which approach is most appropriate?","answer":"[{\"id\":\"a\",\"text\":\"Implement a custom callback that saves weights only when val_accuracy improves\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use ModelCheckpoint with save_best_only=True and monitor=val_accuracy\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Save the entire model after every epoch to the same file\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use tf.train.Checkpoint to save after every batch\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct: ModelCheckpoint with save_best_only=True and monitor='val_accuracy' saves only the best weights when the monitored metric improves, preventing a flood of files and ensuring deployment uses the best model.\n\n## Why Other Options Are Wrong\n- Option A is plausible but requires custom logic and wiring; it is less standardized and error-prone compared to the built-in callback.\n- Option C overwrites the same file each epoch, and if a save is interrupted, you may lose the best model.\n- Option D saves checkpoints frequently (often each batch), consuming storage and complicating retrieval of the best model.\n\n## Key Concepts\n- ModelCheckpoint callback\n- save_best_only and monitor parameters\n- Validation metrics as a basis for best-model selection\n\n## Real-World Application\n- In a cloud training job, automatic best-model retention simplifies deployment and reduces CI/CD complexity by always providing a reliable artifact for serving.","diagram":null,"difficulty":"intermediate","tags":["TensorFlow","ModelCheckpoint","Kubernetes","AWS-EKS","CI/CD","SavedModel","certification-mcq","domain-weight-30"],"channel":"tensorflow-developer","subChannel":"building-models","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T23:19:18.150Z","createdAt":"2026-01-11 23:19:18"},{"id":"tensorflow-developer-building-models-1768173557677-2","question":"You have a trained TensorFlow model and want to deploy it to production with TensorFlow Serving on Kubernetes. Which export approach ensures a SavedModel with serving signatures that TF Serving can consume?","answer":"[{\"id\":\"a\",\"text\":\"model.save('/export/path/model.h5', save_format='h5')\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"tf.saved_model.save(model, '/export/path', signatures=None)\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"tf.saved_model.save(model, '/export/path', signatures=signature_fn)\",\"isCorrect\":true},{\"id\":\"d\",\"text\":\"Zip the weights and attach a signature.json for TF Serving\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption C is correct: tf.saved_model.save(model, '/export/path', signatures=signature_fn) explicitly exports a SavedModel with defined serving signatures, ensuring TF Serving can correctly map inputs to outputs.\n\n## Why Other Options Are Wrong\n- Option A saves an HDF5 file, not a SavedModel, which TF Serving cannot load.\n- Option B exports a SavedModel but without explicit signatures; while it may work in some cases, it does not guarantee a defined serving signature necessary for robust serving.\n- Option D describes an unsupported packaging method for TF Serving.\n\n## Key Concepts\n- SavedModel format\n- Serving signatures (e.g., serving_default)\n- tf.saved_model.save and signature_fn\n\n## Real-World Application\n- Exporting with explicit signatures avoids ambiguity in input names when loading the model in TensorFlow Serving on Kubernetes, enabling reliable production inference pipelines.","diagram":null,"difficulty":"intermediate","tags":["TensorFlow","SavedModel","TFServing","Kubernetes","AWS-EKS","Terraform","certification-mcq","domain-weight-30"],"channel":"tensorflow-developer","subChannel":"building-models","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T23:19:18.608Z","createdAt":"2026-01-11 23:19:18"},{"id":"tensorflow-developer-tensorflow-basics-1768213383183-0","question":"To maximize throughput in a TensorFlow training loop with a large image dataset, which data pipeline configuration best overlaps data preparation with training?","answer":"[{\"id\":\"a\",\"text\":\"dataset.map(parse_fn, num_parallel_calls=tf.data.AUTOTUNE).shuffle(buffer_size=1000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"dataset.cache().shuffle(1000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"dataset.batch(BATCH_SIZE).shuffle(1000).prefetch(tf.data.AUTOTUNE)\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"dataset.map(parse_fn).batch(BATCH_SIZE).repeat().prefetch(-1)\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA. Overlaps data prep and training by using a complete data pipeline: map with parallel parsing, shuffling, batching, and prefetching to overlap IO and computation.\n\n## Why Other Options Are Wrong\n- Option B: Caching stores the entire dataset in memory which may exceed RAM for large datasets and doesn’t guarantee overlap with training.\n- Option C: Lacks the parsing step with parallelism, reducing data preparation efficiency and throughput.\n- Option D: Uses repeat with an invalid prefetch value (-1) and can cause unintended looping, not aligning with a finite training run.\n\n## Key Concepts\n- tf.data, map, shuffle, batch, prefetch, AUTOTUNE\n\n## Real-World Application\nThis pattern is used in production training pipelines to keep GPUs fed with data while CPU work preloads and preprocesses the next batch, improving sustained throughput.","diagram":null,"difficulty":"intermediate","tags":["TensorFlow","tf.data","Keras","ModelCheckpoint","AWS","Kubernetes","Terraform","certification-mcq","domain-weight-25"],"channel":"tensorflow-developer","subChannel":"tensorflow-basics","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T10:23:03.187Z","createdAt":"2026-01-12 10:23:03"},{"id":"tensorflow-developer-tensorflow-basics-1768213383183-1","question":"During transfer learning with a CNN on a small dataset, which approach ensures the base feature extractor's weights are frozen while training only the top classifier layers?","answer":"[{\"id\":\"a\",\"text\":\"Set all layers to trainable and train with a very small learning rate\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Freeze the base feature extractor by setting trainable = False for its layers, then recompile the model\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Train only a separate optimizer for the top layers while keeping others in the graph\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use early stopping to freeze weights automatically after a few epochs\",\"isCorrect\":false}]","explanation":"## Correct Answer\nB. Freezing the base feature extractor by setting trainable = False for its layers and then recompiling ensures only the top classifier layers are updated during initial training.\n\n## Why Other Options Are Wrong\n- Option A: Making all layers trainable defeats the purpose of freezing the base; the base may overfit on small data.\n- Option C: An optimizer alone cannot selectively exclude layers from gradient updates without proper trainable flags; it’s not a standard reliable approach.\n- Option D: Early stopping does not automatically freeze layers; it stops training based on metrics.\n\n## Key Concepts\n- transfer learning, partial fine-tuning, trainable flag, model recompilation\n\n## Real-World Application\nThis approach helps quickly adapt a pre-trained model to a niche dataset by learning only the new task-specific head first, reducing data requirements and overfitting.","diagram":null,"difficulty":"intermediate","tags":["TensorFlow","transfer learning","tf.keras","Kubernetes","AWS","Terraform","certification-mcq","domain-weight-25"],"channel":"tensorflow-developer","subChannel":"tensorflow-basics","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T10:23:03.696Z","createdAt":"2026-01-12 10:23:04"},{"id":"tensorflow-developer-tensorflow-basics-1768213383183-2","question":"During Keras model training, which ModelCheckpoint configuration saves only the best weights based on validation loss?","answer":"[{\"id\":\"a\",\"text\":\"ModelCheckpoint(filepath='best_weights.h5', monitor='val_loss', save_weights_only=False, save_freq='epoch')\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"ModelCheckpoint(filepath='best_weights.h5', monitor='val_loss', save_best_only=True, mode='min')\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"ModelCheckpoint(filepath='best.weights', monitor='loss', save_best_only=True, mode='max')\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"ModelCheckpoint(filepath='weights/', monitor='val_accuracy', save_best_only=True, mode='max')\",\"isCorrect\":false}]","explanation":"## Correct Answer\nB. Setting monitor to val_loss with save_best_only=True and mode='min' ensures that only the best weights, according to validation loss, are saved.\n\n## Why Other Options Are Wrong\n- Option A: save_freq and save_weights_only do not enforce saving only the best weights based on val_loss.\n- Option C: Monitors loss rather than val_loss and mode='max' is inappropriate for minimization of validation loss.\n- Option D: Monitors val_accuracy, not val_loss; not aligned with saving best weights by validation loss.\n\n## Key Concepts\n- ModelCheckpoint, monitor, save_best_only, mode\n\n## Real-World Application\nThis configuration guarantees deployment uses the model version that generalizes best to validation data, reducing overfitting risk.","diagram":null,"difficulty":"intermediate","tags":["TensorFlow","tf.keras","ModelCheckpoint","AWS","Kubernetes","Terraform","certification-mcq","domain-weight-25"],"channel":"tensorflow-developer","subChannel":"tensorflow-basics","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T10:23:04.215Z","createdAt":"2026-01-12 10:23:04"}],"subChannels":["building-models","tensorflow-basics"],"companies":[],"stats":{"total":6,"beginner":0,"intermediate":6,"advanced":0,"newThisWeek":6}}