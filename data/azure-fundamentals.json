{"questions":[{"id":"q-1007","question":"A web app hosted in Azure App Service must securely call a private API hosted in an Azure Function behind a VNet. How would you implement private connectivity so traffic never leaves the Azure backbone? Include which resources you’d use, how to create a Private Endpoint for the API, DNS configuration, and how to restrict public access?","answer":"Use a Private Endpoint for the API (Function App) and VNet integration for the App Service. Place the Private Endpoint in a dedicated subnet, disable public access, and link a private DNS zone so the ","explanation":"## Why This Is Asked\n\nAssess private connectivity and DNS skills in Azure fundamentals.\n\n## Key Concepts\n\n- Private Endpoint\n- Private DNS Zone\n- VNet integration\n- Network Security Groups and subneting\n- Managed identities for auth\n\n## Code Example\n\n```bash\n# Create Private Endpoint for API\naz network private-endpoint create --name api-pe \\\n  --resource-group MyRG \\\n  --vnet-name MyVNet \\\n  --subnet PrivateEndpoints \\\n  --private-connection-resource-id /subscriptions/<sub>/resourceGroups/MyRG/providers/Microsoft.Web/sites/MyApi \\\n  --group-ids sites \\\n  --connection-name apiConn\n```\n\n## Follow-up Questions\n\n- How would you verify private traffic vs public exposure in a live environment?\n- Which logs would you enable to confirm traffic is on the private path?","diagram":null,"difficulty":"beginner","tags":["azure-fundamentals"],"channel":"azure-fundamentals","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Lyft","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T18:51:54.879Z","createdAt":"2026-01-12T18:51:54.880Z"},{"id":"q-1083","question":"An enterprise web app must enforce data residency per region, minimize egress costs, and meet real-time latency targets. Design an Azure-native, region-aware architecture that enforces residency, uses regional storage and a global entry point, and supports auditing and DR. Outline services, data replication, access control, and failover strategy?","answer":"Proposed solution: Route users through Azure Front Door to region-specific app instances; store user data in regional storage accounts with ZRS; enable Cosmos DB multi-region writes in the user region","explanation":"## Why This Is Asked\nThis tests practical Azure architecture for data residency, egress control, and DR in a globally distributed app.\n\n## Key Concepts\n- Data residency and private networking\n- Global routing vs regional endpoints\n- Cross-region replication and consistency models\n- Auditing and compliance instrumentation\n\n## Code Example\n```json\n{ 'policyRule': { 'if': {'field': 'location', 'notIn': ['eastus','westeurope']}, 'then': {'effect': 'deny'} } }\n```\n\n## Follow-up Questions\n- How would you monitor egress and enforce budgets per region?\n- What are trade-offs of using LRS vs ZRS for regional data?\n","diagram":null,"difficulty":"advanced","tags":["azure-fundamentals"],"channel":"azure-fundamentals","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Oracle","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T22:18:30.826Z","createdAt":"2026-01-12T22:18:30.826Z"},{"id":"q-1110","question":"Scenario: A web API running in Azure App Service must persist user uploads to an Azure Storage account. Public access to the storage is disabled. How would you implement a Private Endpoint so the App Service talks to Storage over a private network, including enabling VNet integration, creating the Private Endpoint in the storage subnet, DNS configuration for privatelink, and any trade-offs?","answer":"Create a dedicated VNet; enable regional VNet Integration on App Service; in the Storage account, create a Private Endpoint in the same VNet/subnet; disable public network access and link a Private DN","explanation":"## Why This Is Asked\nThis tests practical networking and security skills: isolating storage with Private Endpoint and DNS.\n\n## Key Concepts\n- Private Endpoint for Azure Storage\n- VNet integration for App Service\n- Private DNS zones and DNS resolution\n- Public network access controls and trade-offs\n\n## Code Example\n```javascript\n# Azure CLI steps (shown as code for consistency)\n# Create resource group, VNet and subnet\naz group create -l eastus -n rg\naz network vnet create -g rg -n vnet-app -l eastus --subnet-name appsub\n\n# Enable VNet integration for App Service\naz webapp vnet-integration add -g rg -n myapp --vnet vnet-app --subnet appsub\n\n# Create Storage account and Private Endpoint\naz storage account create -n mystorage -g rg -l eastus --sku Standard_LRS\naz network private-endpoint create -g rg -n pe-storage -a mystorage --vnet-name vnet-app --subnet appsub --private-connection-resource-id $(az storage account show -n mystorage -g rg --query id -o tsv) --connection-name storagepe\n\n# DNS zone for private endpoint\naz network private-dns zone create -g rg -n privatelink.blob.core.windows.net\naz network private-dns link vnet -g rg -n link-vnet-app --zone-name privatelink.blob.core.windows.net --virtual-network vnet-app\n```\n\n## Follow-up Questions\n- How would you test the connectivity from App Service to Storage after setup?\n- What are potential limitations of Private Endpoints for multi-region scenarios?","diagram":null,"difficulty":"beginner","tags":["azure-fundamentals"],"channel":"azure-fundamentals","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Coinbase","Goldman Sachs"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T23:18:29.664Z","createdAt":"2026-01-12T23:18:29.664Z"},{"id":"q-1212","question":"Scenario: a new web app stores and serves user-uploaded images globally. To keep costs predictable and delivery fast, which Azure services would you pair to store, serve, and secure access to images, and what trade-offs would you consider for storage tiering, CDN option, and access control?","answer":"Use Azure Blob Storage with Hot tier for current images and lifecycle to Cool/Archive; place a CDN (Azure CDN or Front Door) to cache and serve globally; secure access with SAS tokens or managed ident","explanation":"## Why This Is Asked\n\nTests knowledge of storage options, global delivery, and cost optimization in Azure for a common media use-case.\n\n## Key Concepts\n\n- Blob Storage tiers (Hot/Cool/Archive)\n- CDN vs Front Door for global caching\n- Secure access: SAS tokens, managed identities, public access controls\n- Lifecycle management policies\n- Egress costs and HTTPS considerations\n\n## Code Example\n\n```javascript\n// CLI example to set a lifecycle policy (illustrative)\naz storage account management-policy create --account-name <acct> --policy @policy.json\n```\n\n## Follow-up Questions\n\n- How would you handle image updates vs. cache invalidation?\n- What changes if most users are in a single region vs globally?","diagram":null,"difficulty":"beginner","tags":["azure-fundamentals"],"channel":"azure-fundamentals","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Hugging Face"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T05:25:35.637Z","createdAt":"2026-01-13T05:25:35.638Z"},{"id":"q-1242","question":"A startup runs a web app that stores user-uploaded images in Azure Blob Storage and serves them globally via CDN. Describe a practical storage design to minimize costs and latency: container layout, default and lifecycle tiers, lifecycle rules to auto-tier/delete, access control (RBAC vs SAS), and how you’d wire in CDN caching. Include concrete steps and example commands?","answer":"Store images in a dedicated hot container in a single Storage Account. Default to Hot, move to Cool after 30 days, to Archive after 90 days, delete after 730 days via lifecycle rules. Use RBAC for app","explanation":"## Why This Is Asked\nTests Blob Storage lifecycle, access control, and CDN integration in a concrete scenario.\n\n## Key Concepts\n- Azure Blob Storage tiers and lifecycle management\n- Access control: RBAC and SAS\n- CDN caching and origin configuration\n- Soft delete and versioning for resilience\n\n## Code Example\n```javascript\n// Pseudo: lifecycle policy as a JSON-like object (for illustration)\nconst policy = {\n  policy: {\n    rules: [\n      {\n        name: \"UploadsLifecycle\",\n        enabled: true,\n        definition: {\n          filters: { blobTypes: [\"BlockBlob\"], prefixMatch: [\"uploads/\"] },\n          actions: {\n            baseBlob: {\n              tierToCool: { daysAfterModificationGreaterThan: 30 },\n              tierToArchive: { daysAfterModificationGreaterThan: 90 },\n              delete: { daysAfterModificationGreaterThan: 730 }\n            }\n          }\n        }\n      }\n    ]\n  }\n}\n```\n\n## Follow-up Questions\n- How would you monitor storage costs and access patterns?\n- How would you revoke a user’s access if needed?","diagram":"flowchart TD\n  A[User] --> B[Azure CDN]\n  B --> C[Blob Storage]\n  C --> D[Lifecycle Policy]\n  C --> E[RBAC/SAS]\n","difficulty":"beginner","tags":["azure-fundamentals"],"channel":"azure-fundamentals","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","IBM","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T06:37:30.588Z","createdAt":"2026-01-13T06:37:30.588Z"},{"id":"q-1276","question":"Design an end-to-end Azure architecture for a globally distributed analytics platform servicing EU customers with strict data residency. Your plan should cover: data at rest with customer-managed keys, cross-region disaster recovery, private networking (Private Link/Endpoints), least-privilege access, encryption key rotation, and continuous policy compliance checks. Explain key services and trade-offs?","answer":"Implement BYOK using Azure Key Vault CMK to encrypt all data at rest: ADLS Gen2, Synapse, and backups; enable CMK for storage and compute where supported. Use Private Endpoints to data stores and Key ","explanation":"## Why This Is Asked\nTests architecture design for regulated, global workloads with security, DR, and compliance.\n\n## Key Concepts\n- Customer-managed keys (BYOK) in Azure Key Vault\n- Private Link/Endpoints for network isolation\n- Cross-region DR with paired regions and Azure Site Recovery\n- Azure Policy and Privileged Identity Management (PIM)\n- Data residency and auditability via Monitor and Purview\n\n## Code Example\n```javascript\n// Example: rotate a CMK version in Key Vault (pseudo-automation)\nasync function rotateCMK(vaultName, keyName){\n  // rotate to a new version; in practice use az keyvault key rotate\n  console.log(`Rotating ${keyName} in ${vaultName}`)\n}\n```\n\n## Follow-up Questions\n- How would you validate that CMK rotation does not disrupt ongoing queries?\n- How would you enforce EU residency at scale across multi-cloud components?","diagram":"flowchart TD\n  EU_Regions[EU Regions] --> KV[Key Vault CMK BYOK]\n  KV --> ADLS[ADLS Gen2 / Synapse]\n  ADLS --> Private[Private Endpoints]\n  Private --> DR[Cross-region DR: Site Recovery]\n  DR --> EU_Regions\n  Policy[Azure Policy & PIM] --> KV, ADLS, DR","difficulty":"advanced","tags":["azure-fundamentals"],"channel":"azure-fundamentals","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T07:40:19.876Z","createdAt":"2026-01-13T07:40:19.876Z"},{"id":"q-1386","question":"You're building a real-time telemetry pipeline in Azure. Ingest devices via IoT Hub, route events to a Function App for normalization, and persist to an ADLS Gen2 data lake. Compare Event Grid vs Event Hubs for this pub-sub pattern, and justify your choice, including at-least-once delivery, backoff strategy, and observability requirements?","answer":"Event Hubs is the better fit for high-throughput telemetry. Use a dedicated Event Hubs instance with consumer groups for parallelism; enable capture to Data Lake if needed. Implement at-least-once del","explanation":"## Why This Is Asked\nTests service selection and practical handling of reliability, observability, and cost in Azure messaging.\n\n## Key Concepts\n- Event Hubs vs Event Grid for telemetry\n- at-least-once, idempotency, dead-lettering\n- backoff retries and observability\n\n## Code Example\n```javascript\nmodule.exports = async function(context, eventHubMessages) {\n  for (const m of eventHubMessages) {\n    // normalize and upsert to sink with idempotent key\n  }\n};\n```\n\n## Follow-up Questions\n- How would you validate idempotency at scale?\n- How would you structure retries and DLQ messaging across functions?","diagram":null,"difficulty":"intermediate","tags":["azure-fundamentals"],"channel":"azure-fundamentals","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Microsoft","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T14:48:24.182Z","createdAt":"2026-01-13T14:48:24.183Z"},{"id":"q-1491","question":"In a global fleet telemetry use-case, events from devices arrive across regions at high volume. Propose an end-to-end Azure pipeline that achieves sub-200 ms latency, ingests 200k events/sec per region, and writes to Delta Lake on ADLS Gen2 with exactly-once semantics and cross-region DR. Which components would you choose (Event Hubs, Functions/Durable Functions, Databricks or Synapse, Delta Lake, identity/security), and how would you implement idempotent sinks, retries, and cross-region failover?","answer":"Propose an end-to-end Azure pipeline for global fleet telemetry: ingest 200k events/sec per region with Event Hubs, process with per-region Durable Functions for exact-once semantics, and write to Del","explanation":"## Why This Is Asked\nThis question probes architecting a real-time, cross-region Delta Lake pipeline with strict correctness guarantees and robust DR.\n\n## Key Concepts\n- Event Hubs ingestion for high-throughput streams\n- Durable Functions or Spark streaming for exactly-once processing\n- Delta Lake on ADLS Gen2 for ACID lakehouse storage\n- Cross-region DR: geo-redundant storage, failover strategies\n- Idempotent sinks, retry policies, managed identities, private endpoints\n\n## Code Example\n```javascript\n// Pseudo: idempotent Delta Lake sink\nDeltaTable.forPath(spark, '/delta/events').merge(incomingDF.alias('src'), 't.id = src.id').whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n```\n\n## Follow-up Questions\n- How would you monitor latency and backpressure across regions?\n- How would you simulate failure scenarios and test failover?","diagram":"flowchart TD\nA[Devices] --> B[Event Hubs]\nB --> C[Durable Functions per region]\nC --> D[Delta Lake on ADLS Gen2]\nD --> E[Analytics in Synapse/Databricks]\nE --> F[Monitoring & Governance]","difficulty":"advanced","tags":["azure-fundamentals"],"channel":"azure-fundamentals","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Databricks","Discord"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T19:04:37.730Z","createdAt":"2026-01-13T19:04:37.730Z"},{"id":"q-1516","question":"A startup hosts a static frontend in Azure Blob Storage and a small API in Azure Functions. They want low costs, automatic scaling, and simple CI/CD using GitHub Actions. Design a beginner-level end-to-end setup: storage for static site, enable static website hosting, configure Functions with a Consumption plan, set up GitHub Actions for deployment, and outline basic security considerations?","answer":"Host the frontend in Azure Blob Storage with the $web container and front it with Azure CDN and a custom domain. Deploy the API as Azure Functions on a Consumption plan. Use GitHub Actions to upload s","explanation":"## Why This Is Asked\nTests practical wiring of Azure core services for cost-effective, scalable apps.\n\n## Key Concepts\n- Blob Storage static hosting\n- Azure CDN + custom domain\n- Functions Consumption plan\n- GitHub Actions for CI/CD\n- Basic security: CORS, Key Vault, access restrictions\n\n## Code Example\n```javascript\n# Upload static site\naz storage blob upload-batch -d '$web' -s dist --account-name <storage> --account-key <key>\n\n# Deploy function\nzip -r functionapp.zip .\naz functionapp deployment source config-zip -g <rg> -n <functionapp> --src functionapp.zip\n```\n\n## Follow-up Questions\n- How would you migrate to a staging slot workflow? \n- How to monitor costs for these resources?","diagram":null,"difficulty":"beginner","tags":["azure-fundamentals"],"channel":"azure-fundamentals","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","NVIDIA","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T19:53:28.669Z","createdAt":"2026-01-13T19:53:28.669Z"},{"id":"q-1580","question":"You have a REST API backend hosted in a private subnet (AKS/VM) that must be exposed to external partners. Requirements: IP allowlisting, DDoS protection, WAF, token-based authentication via Azure AD, and per-client rate limiting. Which Azure services would you assemble, and what specific configurations would you apply to meet these requirements while keeping backend private?","answer":"Deploy Azure API Management as a secure gateway in front of the private backend, with Azure Front Door providing global routing, DDoS Protection Standard, and WAF capabilities. Expose the private API through Private Link/Private Endpoint to maintain backend privacy while enabling controlled external access.","explanation":"## Why This Is Asked\nThis question evaluates your ability to design a secure, scalable architecture for exposing private backend services to external partners while maintaining strict security controls.\n\n## Key Concepts\n- API Management serves as a centralized gateway with policy-driven security controls\n- Private Link/Private Endpoint ensures backend remains inaccessible from public internet\n- DDoS Protection Standard and WAF provide comprehensive external threat protection\n- Azure AD OAuth 2.0 integration with JWT validation in APIM for token-based authentication\n- Per-client rate limiting, comprehensive logging, and monitoring for operational visibility\n\n## Code Example\n```json\n{\n  \"policy\": {\n    \"inbound\": {\n      \"jwtValidation\": {\n        \"issuer\": \"https://sts.windows.net/{tenantId}/\",\n        \"audience\": \"{audience}\"\n      },\n      \"rateLimitBy\": {\n        \"calls\": 100,\n        \"renewalPeriod\": 60,\n        \"counter-key\": \"@(context.Request.IpAddress)\"\n      }\n    }\n  }\n}\n```","diagram":null,"difficulty":"intermediate","tags":["azure-fundamentals"],"channel":"azure-fundamentals","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T04:34:43.590Z","createdAt":"2026-01-13T22:47:50.146Z"},{"id":"q-1710","question":"For a new Azure project with a static site on Blob Storage and a REST API on Azure Functions (Consumption), outline a beginner-friendly, least-privilege deployment setup: create an Azure AD service principal for GitHub Actions, assign minimal roles to deploy both resources, and use Azure Key Vault to store API keys. Include concrete roles and example CLI commands?","answer":"Create RG MyProjRG, a Storage account for static site, and a Function App (Consumption). Create a service principal for GitHub Actions and grant Storage Blob Data Contributor on the storage, plus Cont","explanation":"## Why This Is Asked\nThis tests practical knowledge of Azure AD service principals, RBAC least privilege, and integrating with GitHub Actions, plus secure secret management via Key Vault.\n\n## Key Concepts\n- Least privilege RBAC for automation\n- Service principals for CI/CD\n- Storage Blob Data Contributor and Contributor roles\n- Managed identities and Key Vault access\n- GitHub Secrets integration for credentials\n\n## Code Example\n```\naz group create -l eastus -n MyProjRG\naz storage account create -n mystorageacct123 -g MyProjRG -l eastus --sku Standard_LRS\naz storage container create -n static\n# Enable static website\naz storage blob service-properties update --account-name mystorageacct123 --static-website true --index-document index.html --error-document 404.html\naz functionapp create -g MyProjRG -n MyProjFuncApp --storage-account mystorageacct123 --consumption-plan-location eastus --runtime node --runtime-version 18\n# Create Service Principal for GitHub Actions\naz ad sp create-for-rbac --name GitHubActionsSP --role Contributor --scopes /subscriptions/<SUB_ID>/resourceGroups/MyProjRG\n# Assign Storage and Function roles\naz role assignment create --assignee <APP_ID> --role \"Storage Blob Data Contributor\" --scope /subscriptions/<SUB_ID>/resourceGroups/MyProjRG/providers/Microsoft.Storage/storageAccounts/mystorageacct123\naz role assignment create --assignee <APP_ID> --role \"Contributor\" --scope /subscriptions/<SUB_ID>/resourceGroups/MyProjRG/providers/Microsoft.Web/sites/MyProjFuncApp\n# Key Vault usage\naz keyvault create -n MyProjKV -g MyProjRG\naz keyvault secret set --vault-name MyProjKV --name ApiKey --value \"<secret>\"\n```\n\n## Follow-up Questions\n- How would you rotate credentials and enforce access reviews for the service principal?\n- How would you implement a simple monitoring alert when the Key Vault secret is updated?","diagram":"flowchart TD\n  A[Start] --> B[Create RG]\n  B --> C[Provision Storage]\n  C --> D[Create Function App]\n  D --> E[Create SP for GitHub Actions]\n  E --> F[RBAC: Storage Blob Data Contributor & Contributor]\n  F --> G[Create Key Vault]\n  G --> H[Grant Function Identity to Key Vault]\n  H --> I[Configure GitHub Actions Secrets]","difficulty":"beginner","tags":["azure-fundamentals"],"channel":"azure-fundamentals","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Meta","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T07:45:30.565Z","createdAt":"2026-01-14T07:45:30.565Z"},{"id":"q-1743","question":"Design a beginner-friendly, low-cost Azure webhook receiver that ingests JSON POSTs, runs in a Consumption-plan Function, validates an HMAC signature with a shared secret, stores each event in Azure Table Storage with PartitionKey as source and RowKey as GUID, and emits a basic success/failure metric in Application Insights. Outline steps and provide a minimal code snippet for signature verification in JavaScript?","answer":"Use an HTTP-triggered Function (Consumption plan). Retrieve the secret from Azure Key Vault via Managed Identity, verify the request with HMAC-SHA256 using the body and secret, persist events to Azure","explanation":"## Why This Is Asked\nTests secure, cost-aware webhook ingestion using serverless Azure services and basic security patterns.\n\n## Key Concepts\n- HTTP-triggered Function on Consumption plan\n- Managed Identity and Key Vault secret retrieval\n- HMAC-SHA256 signature validation\n- Azure Table Storage data model: PartitionKey and RowKey\n- Application Insights telemetry\n\n## Code Example\n```javascript\nconst crypto = require('crypto');\nfunction verifySignature(body, secret, signature){\n  const hash = crypto.createHmac('sha256', secret).update(body, 'utf8').digest('hex');\n  return crypto.timingSafeEqual(Buffer.from(hash,'hex'), Buffer.from(signature,'hex'));\n}\n```\n\n## Follow-up Questions\n- How would you rotate the shared secret without downtime?\n- What are trade-offs of using Table Storage vs Cosmos DB for this pattern?","diagram":"flowchart TD\n  A[HTTP POST] --> B[Validate HMAC]\n  B --> C{Valid?}\n  C -- Yes --> D[Write to Table Storage]\n  C -- No --> E[Return 401]\n  D --> F[Log to App Insights]\n  F --> G[Return 200]","difficulty":"beginner","tags":["azure-fundamentals"],"channel":"azure-fundamentals","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Microsoft","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T09:02:56.515Z","createdAt":"2026-01-14T09:02:56.515Z"},{"id":"q-859","question":"You have a REST API hosted on Azure App Service that processes user uploads and stores them in a separate Azure Blob Storage account. To avoid embedding secrets, how would you securely grant the API read/write access to the blob container using a managed identity? Outline the exact steps and roles you would apply, and mention any network considerations?","answer":"Enable system-assigned managed identity on the App Service. Grant Storage Blob Data Contributor to that identity at the storage account/container scope. In code, use DefaultAzureCredential to instanti","explanation":"## Why This Is Asked\nTests practical understanding of secure service-to-resource access without secrets, a core Azure fundamentals scenario.\n\n## Key Concepts\n- Managed Identity\n- RBAC and Storage Blob Data Contributor\n- DefaultAzureCredential\n- Private Endpoint / Block Public Access\n\n## Code Example\n```javascript\nconst { BlobContainerClient } = require('@azure/storage-blob');\nconst containerUrl = 'https://<storage>.blob.core.windows.net/<container>';\nconst client = new BlobContainerClient(containerUrl, new DefaultAzureCredential());\n```\n\n## Follow-up Questions\n- Compare system-assigned vs user-assigned identities.\n- How would you audit access with Azure Activity Logs?","diagram":null,"difficulty":"beginner","tags":["azure-fundamentals"],"channel":"azure-fundamentals","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","MongoDB","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T13:41:14.618Z","createdAt":"2026-01-12T13:41:14.618Z"},{"id":"q-865","question":"An IoT platform deployed in Azure collects about 5 million device events per minute from global regions. You must build an ingestion and processing pipeline that guarantees at-least-once delivery with idempotent writes to Cosmos DB, tolerates regional outages, and minimizes cost. Describe the end-to-end architecture, data flow, dedup strategy, and monitoring?","answer":"Ingest with regional Event Hubs per region; route to a central processing plane using Event Grid and Functions; apply idempotent write to Cosmos DB using eventId as the partition key; store raw events","explanation":"## Why This Is Asked\nTests ability to design a resilient, scalable Azure data-pipeline using event-driven components, while ensuring data integrity with idempotent writes and deduplication.\n\n## Key Concepts\n- Event-driven ingestion across regions: Event Hubs, Event Grid\n- Processing semantics: at-least-once, idempotent writes, dedup via eventId\n- Global storage strategy: Cosmos DB multi-region writes + upserts\n- Observability: Azure Monitor, Log Analytics, Application Insights\n\n## Code Example\n```javascript\n// Pseudo-code: idempotent write using eventId\nasync function upsertEvent(event) {\n  await cosmos.upsert('events', event.eventId, event.payload);\n}\n```\n\n## Follow-up Questions\n- How would you implement exactly-once semantics across region failover?\n- How would you handle out-of-order events and duplicate bursts?","diagram":"flowchart TD\n  A(Ingest: regional Event Hubs) --> B(Event Grid & Functions)\n  B --> C{Cosmos DB Writes}\n  C -->|Multi-region writes| D[Cosmos DB]\n  B --> E[ADLS Gen2 (raw events)]\n  B --> F[Monitoring: Azure Monitor / APPS Insights]\n  D --> G[Downstream Analytics]\n  E --> G","difficulty":"intermediate","tags":["azure-fundamentals"],"channel":"azure-fundamentals","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T13:46:23.167Z","createdAt":"2026-01-12T13:46:23.167Z"},{"id":"q-932","question":"Scenario: a multi-tenant SaaS app runs on Azure App Service and stores per-tenant files in separate containers in Azure Blob Storage. To avoid hard-coded keys, describe a secure pattern using managed identities and RBAC to grant the app the correct container access while ensuring tenant isolation and minimal permission surface. Outline steps, roles, and any network considerations (e.g., Private Endpoint)?","answer":"Use a user-assigned managed identity for the App Service, assign the Storage Blob Data Contributor role scoped to the tenant’s blob container, and store container identifiers in Key Vault. The app aut","explanation":"## Why This Is Asked\nTests understanding of secure identity-based access patterns for multi-tenant storage in Azure, including least privilege, tenant isolation, and network shielding.\n\n## Key Concepts\n- Managed identities\n- RBAC scoping and access control\n- Azure Key Vault integration\n- Private endpoints\n\n## Code Example\n```javascript\nconst { BlobContainerClient } = require(\"@azure/storage-blob\");\nconst { DefaultAzureCredential } = require(\"@azure/identity\");\n\nconst containerUrl = process.env.TENANT_CONTAINER_URL; // per-tenant\nconst credential = new DefaultAzureCredential();\nconst containerClient = new BlobContainerClient(containerUrl, credential);\n\n// Upload\nawait containerClient.getBlockBlobClient(\"file.txt\").uploadData(buffer);\n```\n\n## Follow-up Questions\n- How would you handle tenant onboarding/offboarding and container lifecycle?\n- How to audit access and detect unauthorized attempts?","diagram":null,"difficulty":"beginner","tags":["azure-fundamentals"],"channel":"azure-fundamentals","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T15:41:44.359Z","createdAt":"2026-01-12T15:41:44.359Z"},{"id":"q-984","question":"In a globally distributed API using Azure Functions and Cosmos DB, implement per-tenant data isolation with minimal cross-region data transfer. How would you design the architecture, enforce least-privilege access via managed identities and RBAC, and ensure data residency with Private Endpoints and cross-region replication policies?","answer":"Design a Cosmos DB account with per-tenant isolation (tenantId partition key) and multi-region writes to minimize cross-region traffic. Authenticate Azure Functions via a system-assigned managed ident","explanation":"## Why This Is Asked\nAssesses ability to balance data residency, isolation, and least-privilege access in a real Azure setup.\n\n## Key Concepts\n- Cosmos DB multi-region writes\n- Azure AD RBAC on Cosmos DB\n- Managed identities for Functions\n- Private Endpoint and public network disablement\n- Tenant-aware partitioning\n\n## Code Example\n```javascript\n// Node.js Cosmos SDK example: parameterized tenant query\nconst querySpec = {\n  query: \"SELECT * FROM c WHERE c.tenantId = @tenantId\",\n  parameters: [{ name: \"@tenantId\", value: tenantId }]\n};\n```\n\n## Follow-up Questions\n- How would you monitor for access violations and rotate tenant keys?\n- What changes if tenants require cross-tenant analytics with strict isolation?","diagram":null,"difficulty":"advanced","tags":["azure-fundamentals"],"channel":"azure-fundamentals","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","MongoDB","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T17:49:01.803Z","createdAt":"2026-01-12T17:49:01.803Z"}],"subChannels":["general"],"companies":["Adobe","Airbnb","Amazon","Bloomberg","Citadel","Cloudflare","Coinbase","Databricks","Discord","DoorDash","Goldman Sachs","Google","Hugging Face","IBM","Lyft","Meta","Microsoft","MongoDB","NVIDIA","Netflix","OpenAI","Oracle","PayPal","Salesforce","Scale Ai","Snap","Square","Two Sigma"],"stats":{"total":16,"beginner":9,"intermediate":3,"advanced":4,"newThisWeek":16}}