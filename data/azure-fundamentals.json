{"questions":[{"id":"q-1007","question":"A web app hosted in Azure App Service must securely call a private API hosted in an Azure Function behind a VNet. How would you implement private connectivity so traffic never leaves the Azure backbone? Include which resources you’d use, how to create a Private Endpoint for the API, DNS configuration, and how to restrict public access?","answer":"Use a Private Endpoint for the API (Function App) and VNet integration for the App Service. Place the Private Endpoint in a dedicated subnet, disable public access, and link a private DNS zone so the ","explanation":"## Why This Is Asked\n\nAssess private connectivity and DNS skills in Azure fundamentals.\n\n## Key Concepts\n\n- Private Endpoint\n- Private DNS Zone\n- VNet integration\n- Network Security Groups and subneting\n- Managed identities for auth\n\n## Code Example\n\n```bash\n# Create Private Endpoint for API\naz network private-endpoint create --name api-pe \\\n  --resource-group MyRG \\\n  --vnet-name MyVNet \\\n  --subnet PrivateEndpoints \\\n  --private-connection-resource-id /subscriptions/<sub>/resourceGroups/MyRG/providers/Microsoft.Web/sites/MyApi \\\n  --group-ids sites \\\n  --connection-name apiConn\n```\n\n## Follow-up Questions\n\n- How would you verify private traffic vs public exposure in a live environment?\n- Which logs would you enable to confirm traffic is on the private path?","diagram":null,"difficulty":"beginner","tags":["azure-fundamentals"],"channel":"azure-fundamentals","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Lyft","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T18:51:54.879Z","createdAt":"2026-01-12T18:51:54.880Z"},{"id":"q-1083","question":"An enterprise web app must enforce data residency per region, minimize egress costs, and meet real-time latency targets. Design an Azure-native, region-aware architecture that enforces residency, uses regional storage and a global entry point, and supports auditing and DR. Outline services, data replication, access control, and failover strategy?","answer":"Proposed solution: Route users through Azure Front Door to region-specific app instances; store user data in regional storage accounts with ZRS; enable Cosmos DB multi-region writes in the user region","explanation":"## Why This Is Asked\nThis tests practical Azure architecture for data residency, egress control, and DR in a globally distributed app.\n\n## Key Concepts\n- Data residency and private networking\n- Global routing vs regional endpoints\n- Cross-region replication and consistency models\n- Auditing and compliance instrumentation\n\n## Code Example\n```json\n{ 'policyRule': { 'if': {'field': 'location', 'notIn': ['eastus','westeurope']}, 'then': {'effect': 'deny'} } }\n```\n\n## Follow-up Questions\n- How would you monitor egress and enforce budgets per region?\n- What are trade-offs of using LRS vs ZRS for regional data?\n","diagram":null,"difficulty":"advanced","tags":["azure-fundamentals"],"channel":"azure-fundamentals","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Oracle","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T22:18:30.826Z","createdAt":"2026-01-12T22:18:30.826Z"},{"id":"q-1110","question":"Scenario: A web API running in Azure App Service must persist user uploads to an Azure Storage account. Public access to the storage is disabled. How would you implement a Private Endpoint so the App Service talks to Storage over a private network, including enabling VNet integration, creating the Private Endpoint in the storage subnet, DNS configuration for privatelink, and any trade-offs?","answer":"Create a dedicated VNet; enable regional VNet Integration on App Service; in the Storage account, create a Private Endpoint in the same VNet/subnet; disable public network access and link a Private DN","explanation":"## Why This Is Asked\nThis tests practical networking and security skills: isolating storage with Private Endpoint and DNS.\n\n## Key Concepts\n- Private Endpoint for Azure Storage\n- VNet integration for App Service\n- Private DNS zones and DNS resolution\n- Public network access controls and trade-offs\n\n## Code Example\n```javascript\n# Azure CLI steps (shown as code for consistency)\n# Create resource group, VNet and subnet\naz group create -l eastus -n rg\naz network vnet create -g rg -n vnet-app -l eastus --subnet-name appsub\n\n# Enable VNet integration for App Service\naz webapp vnet-integration add -g rg -n myapp --vnet vnet-app --subnet appsub\n\n# Create Storage account and Private Endpoint\naz storage account create -n mystorage -g rg -l eastus --sku Standard_LRS\naz network private-endpoint create -g rg -n pe-storage -a mystorage --vnet-name vnet-app --subnet appsub --private-connection-resource-id $(az storage account show -n mystorage -g rg --query id -o tsv) --connection-name storagepe\n\n# DNS zone for private endpoint\naz network private-dns zone create -g rg -n privatelink.blob.core.windows.net\naz network private-dns link vnet -g rg -n link-vnet-app --zone-name privatelink.blob.core.windows.net --virtual-network vnet-app\n```\n\n## Follow-up Questions\n- How would you test the connectivity from App Service to Storage after setup?\n- What are potential limitations of Private Endpoints for multi-region scenarios?","diagram":null,"difficulty":"beginner","tags":["azure-fundamentals"],"channel":"azure-fundamentals","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Coinbase","Goldman Sachs"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T23:18:29.664Z","createdAt":"2026-01-12T23:18:29.664Z"},{"id":"q-1212","question":"Scenario: a new web app stores and serves user-uploaded images globally. To keep costs predictable and delivery fast, which Azure services would you pair to store, serve, and secure access to images, and what trade-offs would you consider for storage tiering, CDN option, and access control?","answer":"Use Azure Blob Storage with Hot tier for current images and lifecycle to Cool/Archive; place a CDN (Azure CDN or Front Door) to cache and serve globally; secure access with SAS tokens or managed ident","explanation":"## Why This Is Asked\n\nTests knowledge of storage options, global delivery, and cost optimization in Azure for a common media use-case.\n\n## Key Concepts\n\n- Blob Storage tiers (Hot/Cool/Archive)\n- CDN vs Front Door for global caching\n- Secure access: SAS tokens, managed identities, public access controls\n- Lifecycle management policies\n- Egress costs and HTTPS considerations\n\n## Code Example\n\n```javascript\n// CLI example to set a lifecycle policy (illustrative)\naz storage account management-policy create --account-name <acct> --policy @policy.json\n```\n\n## Follow-up Questions\n\n- How would you handle image updates vs. cache invalidation?\n- What changes if most users are in a single region vs globally?","diagram":null,"difficulty":"beginner","tags":["azure-fundamentals"],"channel":"azure-fundamentals","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Hugging Face"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T05:25:35.637Z","createdAt":"2026-01-13T05:25:35.638Z"},{"id":"q-1242","question":"A startup runs a web app that stores user-uploaded images in Azure Blob Storage and serves them globally via CDN. Describe a practical storage design to minimize costs and latency: container layout, default and lifecycle tiers, lifecycle rules to auto-tier/delete, access control (RBAC vs SAS), and how you’d wire in CDN caching. Include concrete steps and example commands?","answer":"Store images in a dedicated hot container in a single Storage Account. Default to Hot, move to Cool after 30 days, to Archive after 90 days, delete after 730 days via lifecycle rules. Use RBAC for app","explanation":"## Why This Is Asked\nTests Blob Storage lifecycle, access control, and CDN integration in a concrete scenario.\n\n## Key Concepts\n- Azure Blob Storage tiers and lifecycle management\n- Access control: RBAC and SAS\n- CDN caching and origin configuration\n- Soft delete and versioning for resilience\n\n## Code Example\n```javascript\n// Pseudo: lifecycle policy as a JSON-like object (for illustration)\nconst policy = {\n  policy: {\n    rules: [\n      {\n        name: \"UploadsLifecycle\",\n        enabled: true,\n        definition: {\n          filters: { blobTypes: [\"BlockBlob\"], prefixMatch: [\"uploads/\"] },\n          actions: {\n            baseBlob: {\n              tierToCool: { daysAfterModificationGreaterThan: 30 },\n              tierToArchive: { daysAfterModificationGreaterThan: 90 },\n              delete: { daysAfterModificationGreaterThan: 730 }\n            }\n          }\n        }\n      }\n    ]\n  }\n}\n```\n\n## Follow-up Questions\n- How would you monitor storage costs and access patterns?\n- How would you revoke a user’s access if needed?","diagram":"flowchart TD\n  A[User] --> B[Azure CDN]\n  B --> C[Blob Storage]\n  C --> D[Lifecycle Policy]\n  C --> E[RBAC/SAS]\n","difficulty":"beginner","tags":["azure-fundamentals"],"channel":"azure-fundamentals","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","IBM","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T06:37:30.588Z","createdAt":"2026-01-13T06:37:30.588Z"},{"id":"q-1276","question":"Design an end-to-end Azure architecture for a globally distributed analytics platform servicing EU customers with strict data residency. Your plan should cover: data at rest with customer-managed keys, cross-region disaster recovery, private networking (Private Link/Endpoints), least-privilege access, encryption key rotation, and continuous policy compliance checks. Explain key services and trade-offs?","answer":"Implement BYOK using Azure Key Vault CMK to encrypt all data at rest: ADLS Gen2, Synapse, and backups; enable CMK for storage and compute where supported. Use Private Endpoints to data stores and Key ","explanation":"## Why This Is Asked\nTests architecture design for regulated, global workloads with security, DR, and compliance.\n\n## Key Concepts\n- Customer-managed keys (BYOK) in Azure Key Vault\n- Private Link/Endpoints for network isolation\n- Cross-region DR with paired regions and Azure Site Recovery\n- Azure Policy and Privileged Identity Management (PIM)\n- Data residency and auditability via Monitor and Purview\n\n## Code Example\n```javascript\n// Example: rotate a CMK version in Key Vault (pseudo-automation)\nasync function rotateCMK(vaultName, keyName){\n  // rotate to a new version; in practice use az keyvault key rotate\n  console.log(`Rotating ${keyName} in ${vaultName}`)\n}\n```\n\n## Follow-up Questions\n- How would you validate that CMK rotation does not disrupt ongoing queries?\n- How would you enforce EU residency at scale across multi-cloud components?","diagram":"flowchart TD\n  EU_Regions[EU Regions] --> KV[Key Vault CMK BYOK]\n  KV --> ADLS[ADLS Gen2 / Synapse]\n  ADLS --> Private[Private Endpoints]\n  Private --> DR[Cross-region DR: Site Recovery]\n  DR --> EU_Regions\n  Policy[Azure Policy & PIM] --> KV, ADLS, DR","difficulty":"advanced","tags":["azure-fundamentals"],"channel":"azure-fundamentals","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T07:40:19.876Z","createdAt":"2026-01-13T07:40:19.876Z"},{"id":"q-1386","question":"You're building a real-time telemetry pipeline in Azure. Ingest devices via IoT Hub, route events to a Function App for normalization, and persist to an ADLS Gen2 data lake. Compare Event Grid vs Event Hubs for this pub-sub pattern, and justify your choice, including at-least-once delivery, backoff strategy, and observability requirements?","answer":"Event Hubs is the better fit for high-throughput telemetry. Use a dedicated Event Hubs instance with consumer groups for parallelism; enable capture to Data Lake if needed. Implement at-least-once del","explanation":"## Why This Is Asked\nTests service selection and practical handling of reliability, observability, and cost in Azure messaging.\n\n## Key Concepts\n- Event Hubs vs Event Grid for telemetry\n- at-least-once, idempotency, dead-lettering\n- backoff retries and observability\n\n## Code Example\n```javascript\nmodule.exports = async function(context, eventHubMessages) {\n  for (const m of eventHubMessages) {\n    // normalize and upsert to sink with idempotent key\n  }\n};\n```\n\n## Follow-up Questions\n- How would you validate idempotency at scale?\n- How would you structure retries and DLQ messaging across functions?","diagram":null,"difficulty":"intermediate","tags":["azure-fundamentals"],"channel":"azure-fundamentals","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Microsoft","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T14:48:24.182Z","createdAt":"2026-01-13T14:48:24.183Z"},{"id":"q-1491","question":"In a global fleet telemetry use-case, events from devices arrive across regions at high volume. Propose an end-to-end Azure pipeline that achieves sub-200 ms latency, ingests 200k events/sec per region, and writes to Delta Lake on ADLS Gen2 with exactly-once semantics and cross-region DR. Which components would you choose (Event Hubs, Functions/Durable Functions, Databricks or Synapse, Delta Lake, identity/security), and how would you implement idempotent sinks, retries, and cross-region failover?","answer":"Propose an end-to-end Azure pipeline for global fleet telemetry: ingest 200k events/sec per region with Event Hubs, process with per-region Durable Functions for exact-once semantics, and write to Del","explanation":"## Why This Is Asked\nThis question probes architecting a real-time, cross-region Delta Lake pipeline with strict correctness guarantees and robust DR.\n\n## Key Concepts\n- Event Hubs ingestion for high-throughput streams\n- Durable Functions or Spark streaming for exactly-once processing\n- Delta Lake on ADLS Gen2 for ACID lakehouse storage\n- Cross-region DR: geo-redundant storage, failover strategies\n- Idempotent sinks, retry policies, managed identities, private endpoints\n\n## Code Example\n```javascript\n// Pseudo: idempotent Delta Lake sink\nDeltaTable.forPath(spark, '/delta/events').merge(incomingDF.alias('src'), 't.id = src.id').whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n```\n\n## Follow-up Questions\n- How would you monitor latency and backpressure across regions?\n- How would you simulate failure scenarios and test failover?","diagram":"flowchart TD\nA[Devices] --> B[Event Hubs]\nB --> C[Durable Functions per region]\nC --> D[Delta Lake on ADLS Gen2]\nD --> E[Analytics in Synapse/Databricks]\nE --> F[Monitoring & Governance]","difficulty":"advanced","tags":["azure-fundamentals"],"channel":"azure-fundamentals","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Databricks","Discord"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T19:04:37.730Z","createdAt":"2026-01-13T19:04:37.730Z"},{"id":"q-1516","question":"A startup hosts a static frontend in Azure Blob Storage and a small API in Azure Functions. They want low costs, automatic scaling, and simple CI/CD using GitHub Actions. Design a beginner-level end-to-end setup: storage for static site, enable static website hosting, configure Functions with a Consumption plan, set up GitHub Actions for deployment, and outline basic security considerations?","answer":"Host the frontend in Azure Blob Storage with the $web container and front it with Azure CDN and a custom domain. Deploy the API as Azure Functions on a Consumption plan. Use GitHub Actions to upload s","explanation":"## Why This Is Asked\nTests practical wiring of Azure core services for cost-effective, scalable apps.\n\n## Key Concepts\n- Blob Storage static hosting\n- Azure CDN + custom domain\n- Functions Consumption plan\n- GitHub Actions for CI/CD\n- Basic security: CORS, Key Vault, access restrictions\n\n## Code Example\n```javascript\n# Upload static site\naz storage blob upload-batch -d '$web' -s dist --account-name <storage> --account-key <key>\n\n# Deploy function\nzip -r functionapp.zip .\naz functionapp deployment source config-zip -g <rg> -n <functionapp> --src functionapp.zip\n```\n\n## Follow-up Questions\n- How would you migrate to a staging slot workflow? \n- How to monitor costs for these resources?","diagram":null,"difficulty":"beginner","tags":["azure-fundamentals"],"channel":"azure-fundamentals","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","NVIDIA","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T19:53:28.669Z","createdAt":"2026-01-13T19:53:28.669Z"},{"id":"q-1580","question":"You have a REST API backend hosted in a private subnet (AKS/VM) that must be exposed to external partners. Requirements: IP allowlisting, DDoS protection, WAF, token-based authentication via Azure AD, and per-client rate limiting. Which Azure services would you assemble, and what specific configurations would you apply to meet these requirements while keeping backend private?","answer":"Deploy Azure API Management as a secure gateway in front of the private backend, with Azure Front Door providing global routing, DDoS Protection Standard, and WAF capabilities. Expose the private API through Private Link/Private Endpoint to maintain backend privacy while enabling controlled external access.","explanation":"## Why This Is Asked\nThis question evaluates your ability to design a secure, scalable architecture for exposing private backend services to external partners while maintaining strict security controls.\n\n## Key Concepts\n- API Management serves as a centralized gateway with policy-driven security controls\n- Private Link/Private Endpoint ensures backend remains inaccessible from public internet\n- DDoS Protection Standard and WAF provide comprehensive external threat protection\n- Azure AD OAuth 2.0 integration with JWT validation in APIM for token-based authentication\n- Per-client rate limiting, comprehensive logging, and monitoring for operational visibility\n\n## Code Example\n```json\n{\n  \"policy\": {\n    \"inbound\": {\n      \"jwtValidation\": {\n        \"issuer\": \"https://sts.windows.net/{tenantId}/\",\n        \"audience\": \"{audience}\"\n      },\n      \"rateLimitBy\": {\n        \"calls\": 100,\n        \"renewalPeriod\": 60,\n        \"counter-key\": \"@(context.Request.IpAddress)\"\n      }\n    }\n  }\n}\n```","diagram":null,"difficulty":"intermediate","tags":["azure-fundamentals"],"channel":"azure-fundamentals","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T04:34:43.590Z","createdAt":"2026-01-13T22:47:50.146Z"},{"id":"q-1710","question":"For a new Azure project with a static site on Blob Storage and a REST API on Azure Functions (Consumption), outline a beginner-friendly, least-privilege deployment setup: create an Azure AD service principal for GitHub Actions, assign minimal roles to deploy both resources, and use Azure Key Vault to store API keys. Include concrete roles and example CLI commands?","answer":"Create RG MyProjRG, a Storage account for static site, and a Function App (Consumption). Create a service principal for GitHub Actions and grant Storage Blob Data Contributor on the storage, plus Cont","explanation":"## Why This Is Asked\nThis tests practical knowledge of Azure AD service principals, RBAC least privilege, and integrating with GitHub Actions, plus secure secret management via Key Vault.\n\n## Key Concepts\n- Least privilege RBAC for automation\n- Service principals for CI/CD\n- Storage Blob Data Contributor and Contributor roles\n- Managed identities and Key Vault access\n- GitHub Secrets integration for credentials\n\n## Code Example\n```\naz group create -l eastus -n MyProjRG\naz storage account create -n mystorageacct123 -g MyProjRG -l eastus --sku Standard_LRS\naz storage container create -n static\n# Enable static website\naz storage blob service-properties update --account-name mystorageacct123 --static-website true --index-document index.html --error-document 404.html\naz functionapp create -g MyProjRG -n MyProjFuncApp --storage-account mystorageacct123 --consumption-plan-location eastus --runtime node --runtime-version 18\n# Create Service Principal for GitHub Actions\naz ad sp create-for-rbac --name GitHubActionsSP --role Contributor --scopes /subscriptions/<SUB_ID>/resourceGroups/MyProjRG\n# Assign Storage and Function roles\naz role assignment create --assignee <APP_ID> --role \"Storage Blob Data Contributor\" --scope /subscriptions/<SUB_ID>/resourceGroups/MyProjRG/providers/Microsoft.Storage/storageAccounts/mystorageacct123\naz role assignment create --assignee <APP_ID> --role \"Contributor\" --scope /subscriptions/<SUB_ID>/resourceGroups/MyProjRG/providers/Microsoft.Web/sites/MyProjFuncApp\n# Key Vault usage\naz keyvault create -n MyProjKV -g MyProjRG\naz keyvault secret set --vault-name MyProjKV --name ApiKey --value \"<secret>\"\n```\n\n## Follow-up Questions\n- How would you rotate credentials and enforce access reviews for the service principal?\n- How would you implement a simple monitoring alert when the Key Vault secret is updated?","diagram":"flowchart TD\n  A[Start] --> B[Create RG]\n  B --> C[Provision Storage]\n  C --> D[Create Function App]\n  D --> E[Create SP for GitHub Actions]\n  E --> F[RBAC: Storage Blob Data Contributor & Contributor]\n  F --> G[Create Key Vault]\n  G --> H[Grant Function Identity to Key Vault]\n  H --> I[Configure GitHub Actions Secrets]","difficulty":"beginner","tags":["azure-fundamentals"],"channel":"azure-fundamentals","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Meta","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T07:45:30.565Z","createdAt":"2026-01-14T07:45:30.565Z"},{"id":"q-1743","question":"Design a beginner-friendly, low-cost Azure webhook receiver that ingests JSON POSTs, runs in a Consumption-plan Function, validates an HMAC signature with a shared secret, stores each event in Azure Table Storage with PartitionKey as source and RowKey as GUID, and emits a basic success/failure metric in Application Insights. Outline steps and provide a minimal code snippet for signature verification in JavaScript?","answer":"Use an HTTP-triggered Function (Consumption plan). Retrieve the secret from Azure Key Vault via Managed Identity, verify the request with HMAC-SHA256 using the body and secret, persist events to Azure","explanation":"## Why This Is Asked\nTests secure, cost-aware webhook ingestion using serverless Azure services and basic security patterns.\n\n## Key Concepts\n- HTTP-triggered Function on Consumption plan\n- Managed Identity and Key Vault secret retrieval\n- HMAC-SHA256 signature validation\n- Azure Table Storage data model: PartitionKey and RowKey\n- Application Insights telemetry\n\n## Code Example\n```javascript\nconst crypto = require('crypto');\nfunction verifySignature(body, secret, signature){\n  const hash = crypto.createHmac('sha256', secret).update(body, 'utf8').digest('hex');\n  return crypto.timingSafeEqual(Buffer.from(hash,'hex'), Buffer.from(signature,'hex'));\n}\n```\n\n## Follow-up Questions\n- How would you rotate the shared secret without downtime?\n- What are trade-offs of using Table Storage vs Cosmos DB for this pattern?","diagram":"flowchart TD\n  A[HTTP POST] --> B[Validate HMAC]\n  B --> C{Valid?}\n  C -- Yes --> D[Write to Table Storage]\n  C -- No --> E[Return 401]\n  D --> F[Log to App Insights]\n  F --> G[Return 200]","difficulty":"beginner","tags":["azure-fundamentals"],"channel":"azure-fundamentals","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Microsoft","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T09:02:56.515Z","createdAt":"2026-01-14T09:02:56.515Z"},{"id":"q-1818","question":"A multi-tenant SaaS app must enforce strict per-tenant data isolation while serving a global user base with sub-100ms latencies. Propose an Azure-based storage and caching design (e.g., Cosmos DB with per-tenant partitioning vs relational sharding, caching strategy) and justify data residency, consistency, and failover choices. Include how you'd scale and monitor?","answer":"Design Cosmos DB with per-tenant partitioning and multi-region writes (East US, West Europe, Asia) to meet isolation and latency. Use Session consistency for writes, add a Redis cache for hot reads, a","explanation":"## Why This Is Asked\nTests ability to pick storage strategy for isolation, latency, and compliance in a global SaaS.\n\n## Key Concepts\n- Per-tenant isolation and partitioning\n- Global distribution and multi-region writes\n- Consistency models and latency trade-offs\n- Data residency and backups\n- Caching and cost control\n\n## Code Example\n```bash\naz cosmosdb create --name SaaSCosmos --resource-group SaaSRG --locations regionName=EastUS failoverPriority=0 regionName=WestEurope failoverPriority=1 --default-consistency-level Session\n```\n\n## Follow-up Questions\n- How would you test tenant isolation boundaries and failover?\n- How would you handle tenant onboarding/offboarding and backups across regions?","diagram":"flowchart TD\n  A[Tenant Isolation Need] --> B[Cosmos DB per-tenant partitions]\n  B --> C[Configure multi-region writes]\n  A --> D[Add Redis cache for hot reads]\n  C --> E[Monitor latency and RU usage]","difficulty":"intermediate","tags":["azure-fundamentals"],"channel":"azure-fundamentals","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Google","Hugging Face"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T11:52:05.711Z","createdAt":"2026-01-14T11:52:05.711Z"},{"id":"q-1829","question":"Advanced Azure Fundamentals: Design a cross-region real-time telemetry pipeline for a global IoT fleet (300k events/sec, 5 regions). Ingest with Event Hubs, process with Databricks Structured Streaming, and write to Delta Lake on ADLS Gen2 with exactly-once semantics. Implement geo-replication to a DR region, enforce data residency via Azure Policy and Purview, and ensure idempotent sinks with a tested failover process. What components and trade-offs would you choose, and how would you validate DR?","answer":"Design a cross-region real-time telemetry pipeline: ingest 300k events/sec from 5 regions via Event Hubs; process with Databricks Structured Streaming; write to Delta Lake on ADLS Gen2 with exactly-on","explanation":"## Why This Is Asked\n\nAssesses ability to architect cross-region streaming with data governance and DR.\n\n## Key Concepts\n\n- Event Hubs ingestion across regions\n- Databricks Structured Streaming\n- Delta Lake ACID semantics on ADLS Gen2\n- Geo-replication and DR strategies\n- Azure Policy and Purview governance\n\n## Code Example\n\n```scala\n// Spark Structured Streaming to Delta Lake (pseudo)\nval df = spark.readStream.format(\"eventhubs\").options(...).load()\nval parsed = df.selectExpr(\"CAST(body AS STRING) as json\").select(from_json(col(\"json\"), schema).as(\"d\")).select(\"d.*\")\nval q = parsed.writeStream.format(\"delta\")\n  .option(\"checkpointLocation\", \"/mnt/delta/.checkpoint\")\n  .start(\"abfss://container@account.dfs.core.windows.net/delta/table\")\n```\n\n## Follow-up Questions\n\n- How would you guarantee exactly-once when late events arrive?\n- How would you simulate and validate DR failover and data consistency?","diagram":"flowchart TD\n  A[Event Hubs Ingest] --> B[Databricks Structured Streaming]\n  B --> C[Delta Lake on ADLS Gen2]\n  C --> D[Geo-Replicate to DR Region]\n  C --> E[Governance: Purview + Policy]","difficulty":"advanced","tags":["azure-fundamentals"],"channel":"azure-fundamentals","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","MongoDB","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T13:11:32.054Z","createdAt":"2026-01-14T13:11:32.054Z"},{"id":"q-1921","question":"In a multinational SaaS app where data residency is user-controlled, design an end-to-end Azure architecture that auto-scales with demand, minimizes latency across regions, and enforces per-tenant data isolation. Include data stores, messaging, compute, routing, security, DR, and a plan for idempotent processing with exact-once semantics?","answer":"Architect a cross-region, data-residency SaaS: regional AKS behind Front Door; ingest via local Event Hubs; process with Durable Functions; store per-tenant data in Cosmos DB Global Database with tena","explanation":"## Why This Is Asked\nTests ability to design a multi-region, data-residency architecture with isolation, scalability, and governance; evaluates data flow, consistency, DR readiness, and cost trade-offs.\n\n## Key Concepts\n- Data residency and geo-replication with Cosmos DB Global Database\n- Global routing via Front Door and regional compute\n- Event-driven ingestion with Event Hubs and Durable Functions\n- Per-tenant isolation and RBAC/Policy governance\n- Idempotent processing with transactional outbox and deduplication\n\n## Code Example\n```javascript\n// Azure CLI: create Cosmos DB with multi-region failover\naz cosmosdb create --name mydb --locations regionName=eastus failoverPriority=0 --locations regionName=westeurope failoverPriority=1\n```\n\n## Follow-up Questions\n- How would you verify DR readiness and test cross-region failover with minimal outage?\n- What metrics and alerts would you surface to ensure per-tenant SLAs are met? ","diagram":"flowchart TD\n  A[Front Door] --> B[Regional AKS]\n  B --> C[Event Hubs]\n  C --> D[Durable Functions]\n  D --> E[Cosmos DB (tenant data)]\n  E --> F[ADLS Gen2]\n  F --> G[Analytics]","difficulty":"advanced","tags":["azure-fundamentals"],"channel":"azure-fundamentals","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Netflix","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T17:34:49.944Z","createdAt":"2026-01-14T17:34:49.946Z"},{"id":"q-1983","question":"Design a beginner-level, end-to-end global web API deployment using two Azure regions. The API serves mobile clients worldwide, requires low latency, automatic regional failover, and basic security. Which Azure services would you use (Front Door, Traffic Manager, App Service, WAF), and outline the minimal wiring: two regional API endpoints, Front Door with a backend pool and health probes, WAF policy, and DNS configuration?","answer":"Use Azure Front Door Standard for global routing and a WAF policy, with two regional App Services as backends. Create a Front Door frontend host, a backend pool containing region-1 and region-2 endpoi","explanation":"## Why This Is Asked\n\nTests understanding of global traffic management, security, and simple multi-region deployment using Front Door and WAF.\n\n## Key Concepts\n\n- Global routing with Front Door\n- Health probes and backend pools\n- WAF policy basics\n- DNS integration with Front Door\n\n## Code Example\n\n```bash\n# Example Azure CLI snippet (illustrative)\naz network front-door create --name FDExample --resource-group RG --routing-rule-name apiRule --accepted-protocols Http Https --sku Standard\n```\n\n## Follow-up Questions\n\n- How would you justify choosing Front Door over Traffic Manager in this scenario?\n- How would you test failover in a staging environment?","diagram":"flowchart TD\nA[Client] --> B[Front Door]\nB --> C[Region-1 API]\nB --> D[Region-2 API]\nB --> E[WAF Policy]\nF[DNS: CNAME to Front Door] --> B","difficulty":"beginner","tags":["azure-fundamentals"],"channel":"azure-fundamentals","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","MongoDB","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T19:37:34.343Z","createdAt":"2026-01-14T19:37:34.343Z"},{"id":"q-2192","question":"Design a scalable, multi-tenant analytics pipeline on Azure for a SaaS app with globally distributed customers. Each tenant's data must be isolated, while the system ingests hundreds of thousands of events per second in real time. Propose an end-to-end architecture using Event Hubs (region-per-tenant or per-tenant partitions), Functions/Durable Functions, and a lakehouse on Delta Lake in ADLS Gen2 or Synapse. Explain per-tenant isolation, exactly-once semantics, CMEK with Key Vault, RBAC, cross-region DR, and cost governance; include a concrete rationale and trade-offs?","answer":"Architect a global, multi-tenant analytics pipeline using region-scoped Event Hubs, a per-tenant partitioned lakehouse on Delta Lake in ADLS Gen2, and Durable Functions for stateful ingest. Enforce te","explanation":"## Why This Is Asked\nThis question probes practical multi-tenant data isolation, real-time ingestion, and cost-aware design across Azure services.\n\n## Key Concepts\n- Event Hubs (regional, partitions) for ingestion\n- Lakehouse pattern with Delta Lake on ADLS Gen2 or Synapse\n- Tenant isolation via storage paths and RBAC; CMEK in Key Vault\n- Exactly-once semantics and idempotent sinks\n- Durable Functions for orchestration and state\n- Cross-region DR and cost governance\n\n## Code Example\n```javascript\n// tenant-scoped write example (pseudo)\nconst tenantPath = `lake/tenant=${tenantId}/events/${Date.now()}_${uuid}.json`;\nconst data = JSON.stringify(event);\nawait blobClient.getBlobClient(tenantPath).upload(data, Buffer.byteLength(data));\n```\n\n## Follow-up Questions\n- How would you test tenant isolation and data residency?\n- How would you rotate CMEK and audit access across regions?","diagram":"flowchart TD\n  A(Event Ingest) --> B(Region Event Hub)\n  B --> C(Function Pipeline)\n  C --> D(Datastore: Delta Lake in ADLS Gen2)\n  D --> E(Dashboard/BI)","difficulty":"advanced","tags":["azure-fundamentals"],"channel":"azure-fundamentals","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Citadel","Discord"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T07:01:20.691Z","createdAt":"2026-01-15T07:01:20.691Z"},{"id":"q-2207","question":"In a globally distributed, multi-tenant BI platform, tenant data sovereignty and masked PII are required. Outline an Azure-native end-to-end architecture using Azure Purview for data catalog and classification, region-scoped ADLS Gen2 for raw data, Synapse Analytics with dynamic data masking and Row-Level Security, and Azure Key Vault for per-tenant keys. Explain how you enforce per-tenant data residency, masking, auditing, and cross-region governance?","answer":"Use Purview to classify PII, tag per-tenant datasets, store raw data regionally in ADLS Gen2, apply dynamic data masking on PII columns in Synapse with ALTER COLUMN ... ADD MASKED, implement ROW LEVEL","explanation":"## Why This Is Asked\n\nTests governance, masking, residency, and auditability in a multi-tenant analytics scenario using Azure services.\n\n## Key Concepts\n\n- Data classification with Purview and policy-driven tagging.\n- Dynamic data masking and Row-Level Security in Synapse for per-tenant isolation.\n- Region-scoped data storage (ADLS Gen2) with selective cross-region replication.\n- Customer-managed keys in Key Vault for tenant-bound encryption.\n- End-to-end auditing via Monitor logs and Purview lineage.\n\n## Code Example\n\n```sql\n-- Example: enable dynamic masking on a PII column (conceptual)\nALTER TABLE dbo.Users\nALTER COLUMN email ADD MASKED WITH (FUNCTION = 'email()');\n```\n\n## Follow-up Questions\n\n- How would you enforce data residency in a regulatory-compliant onboarding flow?\n- What are the potential performance impacts of masking and RLS at scale, and how would you mitigate them?\n","diagram":null,"difficulty":"advanced","tags":["azure-fundamentals"],"channel":"azure-fundamentals","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T07:37:14.090Z","createdAt":"2026-01-15T07:37:14.090Z"},{"id":"q-2232","question":"A beginner-level interview question: You have a web app storing user images in Azure Blob Storage. How would you implement a cost-conscious lifecycle strategy to automatically move infrequently accessed items to a cheaper tier and delete after retention, while preserving recoverability and basic governance?","answer":"Configure a Storage Account with blob tiering and lifecycle rules: two rules for the images container. Move to cool after 30 days since last modification; delete after 90 days. Enable blob soft delete","explanation":"## Why This Is Asked\n\nTests practical cost optimization, data retention, and basic governance using Azure Storage lifecycle rules.\n\n## Key Concepts\n\n- Lifecycle Management rules\n- Blob tiers and retention\n- Soft delete and versioning\n- Cost Management budgets and alerts\n\n## Code Example\n\n```json\n{\n  \"rules\": [\n    {\n      \"name\": \"MoveToCool\",\n      \"enabled\": true,\n      \"type\": \"Lifecycle\",\n      \"definition\": {\n        \"filters\": {\n          \"blobTypes\": [\"blockBlob\"],\n          \"prefixMatch\": [\"images/\"]\n        },\n        \"actions\": {\n          \"baseBlob\": {\n            \"tierToCool\": {\"daysAfterModificationGreaterThan\": 30},\n            \"delete\": {\"daysAfterModificationGreaterThan\": 90}\n          }\n        }\n      }\n    }\n  ]\n}\n```\n\n## Follow-up Questions\n\n- How would you test the lifecycle policy in a non-destructive way?\n- What are trade-offs of Cool vs Archive tiers for image data?","diagram":"flowchart TD\nA[User uploads images] --> B[Blob Storage]\nB --> C[Lifecycle: Move to Cool after 30d]\nB --> D[Lifecycle: Delete after 90d]\nB --> E[Enable Soft Delete & Versioning]\nF[Cost Budget: 100] --> G[Alerts @ 80%]","difficulty":"beginner","tags":["azure-fundamentals"],"channel":"azure-fundamentals","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T08:49:36.295Z","createdAt":"2026-01-15T08:49:36.295Z"},{"id":"q-2274","question":"You're building a multi-tenant SaaS on Azure with tenants worldwide and strict data residency. You need low-latency reads globally, per-tenant data isolation, and predictable costs. Design a practical data layer using Cosmos DB with multi-region writes, throttling, and governance. How would you handle partitioning, consistency, backups, and DR while meeting residency constraints?","answer":"Cosmos DB with multi-region writes enabled in key regions, a per-tenant partition key (tenantId), and autoscale RU/s. Use session consistency for low-latency reads while maintaining per-tenant isolati","explanation":"## Why This Is Asked\nTests practical, scalable design for global tenants with data residency requirements, balancing latency, isolation, and cost.\n\n## Key Concepts\n- Cosmos DB multi-region writes and per-tenant partitioning\n- Autoscale RU/s and consistency choices for latency\n- Data residency controls (region tags, CMEK) and governance (Purview)\n- DR: backups, geo-replication, failover drills\n- Networking: Private Link, Front Door integration\n\n## Code Example\n```javascript\nconst { CosmosClient } = require(\"@azure/cosmos\");\nconst client = new CosmosClient({\n  endpoint: \"<cosmos-endpoint>\",\n  key: \"<primary-key>\",\n  connectionPolicy: { preferredLocations: [\"West Europe\", \"East US\"] }\n});\n```\n\n## Follow-up Questions\n- How would you choose the consistency level and why?\n- How would you scale shards as tenants grow?\n- What metrics indicate DR readiness and cost health?","diagram":"flowchart TD\n  A[Tenant Request] --> B[API Gateway]\n  B --> C[Cosmos DB (Multi-Region)]\n  C --> D[Backups & DR]\n  C --> E[Purview & CMEK]","difficulty":"intermediate","tags":["azure-fundamentals"],"channel":"azure-fundamentals","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Google","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T09:56:15.929Z","createdAt":"2026-01-15T09:56:15.929Z"},{"id":"q-2278","question":"Design a compliant, scalable governance baseline for a multi-tenant analytics data lake on Azure (ADLS Gen2, Databricks, Synapse). Customers require BYOK with rotation, per-tenant isolation, private endpoints, and auditable RBAC. Outline the architecture, enforcement using Azure Policy, key management with Key Vault CMK, data cataloging with Purview, and a reproducible deployment blueprint?","answer":"Propose a hierarchical governance with Management Groups and Azure Policy to enforce encryption at rest using CMK in Key Vault for Storage and Databricks; Private Endpoints for network isolation; Purv","explanation":"## Why This Is Asked\n\nAssesses ability to design scalable, compliant data lake governance using Azure native tools, covering data security, access control, and cataloging in a multi-tenant setup.\n\n## Key Concepts\n\n- Azure Policy, Management Groups, Blueprints\n- Customer-managed keys (BYOK) in Key Vault for CMK\n- Private Endpoints and network isolation\n- Purview for data cataloging and lineage\n- RBAC by tenant via Azure AD groups\n- Defender for Cloud for auditing\n\n## Code Example\n\n```javascript\n// Example: outline policy assignment (pseudo-def)\nconst policyAssignment = {\n  name: 'enforce-encryption-cmk',\n  scope: '/subscriptions/xxx',\n  policyDefinitionId: '/providers/Microsoft.Authorization/policyDefinitions/encrypt-at-rest-with-cmk',\n  parameters: {\n    keyVaultKeyId: '/subscriptions/xxx/resourceGroups/rg/providers/Microsoft.KeyVault/vaults/mykv/keys/cmkv'\n  }\n};\n```\n\n## Follow-up Questions\n\n- How would you implement CMK rotation without downtime?\n- What monitoring/alerting would you add for policy violations and key rotation failures?","diagram":"flowchart TD\n  MG[Management Groups]\n  Policy[Azure Policy]\n  CMK[Key Vault CMK]\n  PE[Private Endpoints]\n  Purview[Purview]\n  RBAC[Tenant RBAC via AAD Groups]\n  Storage[ADLS Gen2]\n  Databricks[Databricks]\n  Synapse[Synapse]\n  MG --> Policy\n  Policy --> CMK\n  CMK --> Storage\n  CMK --> Databricks\n  CMK --> Synapse\n  PE --> Storage\n  Purview --> Storage\n  Purview --> Synapse\n  RBAC --> PE\n  RBAC --> Purview","difficulty":"advanced","tags":["azure-fundamentals"],"channel":"azure-fundamentals","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","LinkedIn","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T10:39:18.458Z","createdAt":"2026-01-15T10:39:18.458Z"},{"id":"q-2399","question":"In a multi-tenant Azure SaaS platform, describe how you would implement ephemeral administrator access to a production resource group using Privileged Identity Management (PIM). Include configuring eligible roles, approval workflows, maximum activation duration, MFA requirements, access reviews, and end-to-end auditing. How would you validate that least privilege is enforced and that there are no standing admin privileges outside the workflow?","answer":"Use PIM to convert Admin roles to eligible with time-bound, MFA-protected access, requiring one approval and justification. Enable activation windows (max 4h), auto-expiry, and full audit logs to Azur","explanation":"## Why This Is Asked\nTests practical use of PIM, Just-In-Time elevation, approvals, and auditing to enforce least privilege in a multi-tenant Azure environment.\n\n## Key Concepts\n- Privileged Identity Management (PIM)\n- Just-In-Time elevation\n- MFA enforcement\n- Approval workflows\n- Access reviews and auditing\n\n## Code Example\n```json\n{\n  \"policy\": \"PIM Privileged Role Activation\",\n  \"scope\": \"/subscriptions/<sub-id>/resourceGroups/RG-prod\",\n  \"roles\": [\"Owner\"],\n  \"activation\": {\n    \"durationHours\": 4,\n    \"approvers\": [\"<oncall@domain>\"],\n    \"requireMfa\": true,\n    \"justificationRequired\": true\n  },\n  \"auditing\": {\n    \"enable\": true,\n    \"targets\": [\"LogAnalytics\", \"StorageAccount\"]\n  }\n}\n```\n\n## Follow-up Questions\n- How would you test the on-call drill and ensure revocation is timely?\n- How would you audit and report access reviews to compliance?","diagram":"flowchart TD\n  A[On-call request] --> B[PIM activation with approval]\n  B --> C[Temporary privilege granted]\n  C --> D[Activation expiry]\n  D --> E[Audit log entry in Monitor]","difficulty":"intermediate","tags":["azure-fundamentals"],"channel":"azure-fundamentals","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","LinkedIn"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T16:49:46.189Z","createdAt":"2026-01-15T16:49:46.189Z"},{"id":"q-2449","question":"Global e-commerce platform with two primary regions and a DR region. You must ensure low latency, strict data residency, and automated failover with per-tenant data isolation. Design the data layer and DR plan using Azure services. Compare Cosmos DB with multi-region writes vs SQL DB for per-tenant isolation, routing with Front Door, and storage with ADLS Gen2. Include governance controls (Policy/RBAC) and a practical DR test plan with RPO/RTO targets?","answer":"Use Cosmos DB with multi-region writes in East US and Europe for per-tenant isolation, backed by region-locked storage. Front Door provides global routing with configurable failover, while ADLS Gen2 s","explanation":"## Why This Is Asked\n\nThis question probes design of a DR-ready, region-aware SaaS data layer with strict data residency and per-tenant isolation, plus global routing.\n\n## Key Concepts\n\n- Cosmos DB multi-region writes\n- Private Endpoints / Private Link\n- Azure Front Door for global routing\n- ADLS Gen2 with Private Endpoints\n- Azure Policy and RBAC for residency and isolation\n- DR testing with defined RPO/RTO\n\n## Code Example\n\n```javascript\n// Example: fetch tenant data with partition key to ensure isolation\nconst { CosmosClient } = require('@azure/cosmos');\nconst client = new CosmosClient({ endpoint: COSMOS_ENDPOINT, key: COSMOS_KEY });\nasync function getTenantItems(tenantId) {\n  const { resources } = await client.database('TenantDB').container('Events').items.query({\n    query: 'SELECT * FROM c WHERE c.tenantId = @tid',\n    parameters: [{ name: '@tid', value: tenantId }]\n  }).fetchAll();\n  return resources;\n}\n```\n\n## Follow-up Questions\n\n- How would you validate DR plan coverage during an actual outage without impacting customers?\n- What monitoring would you add to catch cross-region replication lag or schema drift?","diagram":null,"difficulty":"intermediate","tags":["azure-fundamentals"],"channel":"azure-fundamentals","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","NVIDIA","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T18:53:46.915Z","createdAt":"2026-01-15T18:53:46.916Z"},{"id":"q-2551","question":"Design an Azure-native, cost-conscious, multi-region, multi-tenant log-aggregation pipeline for a regulated finance app. Ingest public API logs into Event Hubs, route to per-tenant landing zones in Delta Lake on ADLS Gen2, apply per-tenant schema enforcement, and implement exactly-once semantics with retries, plus cross-region DR. Choose components (Event Hubs, Functions/ Durable Functions, Synapse or Databricks, Delta Lake, Key Vault, Private Link) and outline governance, security, and cost-control measures (RBAC, PIM, CMK, budgets)?","answer":"Implement tenant-isolated ingestion using per-tenant Event Hubs with namespace-level isolation, route through Private Link to regional landing zones, create per-tenant Delta Lake tables in ADLS Gen2 with schema enforcement, and ensure exactly-once semantics using idempotent sinks with deduplication keys backed by durable storage.","explanation":"## Why This Is Asked\nTests mastery of end-to-end multi-region data pipelines with strict isolation and cost discipline, using core Azure capabilities.\n\n## Key Concepts\n- Event Hubs with tenant isolation and Private Link\n- Delta Lake on ADLS Gen2 with per-tenant schema enforcement\n- Exactly-once processing using idempotent sinks\n- Cross-region disaster recovery with geo-redundant storage\n- RBAC, Privileged Identity Management (PIM), and customer-managed keys (CMK)\n- Cost governance via Budgets and autoscaling resource pools\n\n## Code Example\n```javascript\n// Pseudo idempotent sink\nfunction sinkE","diagram":"flowchart TD\n  Ingest[Ingest Logs to Event Hubs] --> Process[Process & Route to Tenant Zones]\n  Tenant[Per-Tenant Delta Lake Tables] --> DR[Cross-Region DR & Replication]\n  Security[RBAC, PIM, CMK] --> Ingest\n  Cost[Budgets & Autoscale] --> Ingest\n  Ingest --> Storage[Delta Lake on ADLS Gen2]\n  DR --> Access[Secure Access via Private Link]","difficulty":"advanced","tags":["azure-fundamentals"],"channel":"azure-fundamentals","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Databricks","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T05:27:05.816Z","createdAt":"2026-01-15T22:40:41.047Z"},{"id":"q-2678","question":"Design a compliant data lake ingestion pipeline for a multi-tenant fintech using Azure Data Lake Gen2. Each tenant must have strict data isolation and immutability, fixed retention with legal holds, and auditable access trails. Ingest from diverse sources with Data Factory, catalog with Purview, process with Synapse or Databricks, expose a curated layer for BI, and implement cross-region DR. Include per-tenant RBAC and row-level security, plus cost considerations?","answer":"Outline a hybrid Azure data-lake design for a multi-tenant fintech: per-tenant isolation, immutable storage, fixed retention with legal holds, and auditable trails. Use Data Factory for ingestion, Pur","explanation":"## Why This Is Asked\nThis question tests governance, hybrid architecture, and Azure data services for fintech-scale data with strict retention, immutability, and auditability.\n\n## Key Concepts\n- Data Lake Gen2 immutability and retention policies\n- Data Factory ingestion orchestration\n- Purview data catalog and governance\n- Per-tenant RBAC and Row-Level Security (RLS)\n- Cross-region DR and cost considerations\n- Synapse/Databricks for scalable processing\n- Auditing and compliance controls\n\n## Code Example\n```javascript\n// Skeleton: enforce tenant filter at query time (pseudo)\nconst tenant = getContextTenant();\ndb.execute(\"SELECT * FROM events WHERE tenant_id = ?\", [tenant]);\n```\n\n## Follow-up Questions\n- How would you test immutability and retention controls end-to-end?\n- How would you reconcile tenant isolation with global BI dashboards?","diagram":null,"difficulty":"advanced","tags":["azure-fundamentals"],"channel":"azure-fundamentals","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","IBM","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T06:51:46.636Z","createdAt":"2026-01-16T06:51:46.636Z"},{"id":"q-2706","question":"Scenario: You manage a three-subscription Azure environment (prod, staging, dev) for a global SaaS. You must enforce that no resource in prod or staging has a public IP, and automatically remediate exposures within 15 minutes. Describe the governance approach, policy initiative, remediation workflow, exemptions, and testing plan. Include integration with Private Endpoints and VNet peering to preserve connectivity while enforcing privacy?","answer":"Define an Azure Policy Initiative with two policies: Deny any PublicIP in prod or staging, and DeployIfNotExists remediation that detaches public IPs and binds a private IP on NICs and LB frontends. S","explanation":"## Why This Is Asked\nAssess practical governance tooling beyond basics.\n\n## Key Concepts\n- Azure Policy and Initiatives\n- Remediation tasks\n- Drift detection\n- Exemptions and per-subscription scoping\n- Private Endpoints and VNet peering\n\n## Code Example\n```javascript\n{\n  \"policyRule\": {\n    \"if\": {\n      \"allOf\": [\n        {\"field\": \"type\", \"equals\": \"Microsoft.Network/publicIPAddresses\"},\n        {\"field\": \"tags.environment\", \"in\": [\"prod\",\"staging\"]}\n      ]\n    },\n    \"then\": {\"effect\": \"deny\"}\n  },\n  \"name\": \"Deny_PublicIP_in_Prod_Staging\",\n  \"mode\": \"All\"\n}\n```\n\n## Follow-up Questions\n- How would you manage exemptions and approvals for dev?\n- How would you test remediation timing and drift alerts?","diagram":"flowchart TD\nA[Define Initiative] --> B[Policy: Deny PublicIP in Prod/Staging]\nA --> C[Policy: DeployIfNotExists remediation]\nB --> D[Scope: prod+staging; exemptions in dev]\nC --> E[Remediation: detach PublicIP, assign private IP]\nE --> F[Validate: Policy Insights, drift alerts, remediation cadence]","difficulty":"intermediate","tags":["azure-fundamentals"],"channel":"azure-fundamentals","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Bloomberg","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T07:40:16.822Z","createdAt":"2026-01-16T07:40:16.822Z"},{"id":"q-2792","question":"Design a per-tenant isolation pattern for a SaaS platform hosted in Azure that serves 100+ customers with strict EU data residency. Allocate each tenant a separate AKS namespace with NetworkPolicies, per-tenant data stores in Azure SQL (one DB or per-tenant schema) encrypted with CMK in Key Vault, and private endpoints for data access. Propose deployment using ARM/Bicep templates, governance via Azure Policy, and DR strategy. Compare separate DB vs per-tenant schema in terms of cost, latency, and isolation. Include validation tests?","answer":"Propose per-tenant isolation using AKS namespaces with network policies, private endpoints to per-tenant Azure SQL databases encrypted with CMK from Key Vault, and either one DB per tenant or per-tena","explanation":"## Why This Is Asked\nReal-world need to balance strict tenant isolation with cost and agility in a SaaS cloud-native stack.\n\n## Key Concepts\n- AKS namespace isolation and network policies\n- Private Link / Private Endpoints for data plane\n- Azure SQL: per-tenant DB vs per-tenant schema, encryption with CMK via Key Vault\n- Bicep/ARM templating for reproducible deployment\n- Azure Policy governance and EU data residency DR strategy\n\n## Code Example\n```javascript\n// Placeholder: pseudo-code for validating isolation boundaries in tests\nfunction assertTenantIsolation(requestTenant, dataTenant) {\n  return requestTenant !== dataTenant;\n}\n```\n\n## Follow-up Questions\n- How would you migrate tenants between databases with minimal downtime?\n- What telemetry would confirm effective isolation without impacting latency?","diagram":null,"difficulty":"intermediate","tags":["azure-fundamentals"],"channel":"azure-fundamentals","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T11:47:50.338Z","createdAt":"2026-01-16T11:47:50.338Z"},{"id":"q-2894","question":"You're building a photo-sharing app that stores user uploads in Azure Blob Storage and serves them via a CDN. Outline a beginner-end setup to minimize storage costs while keeping recent images fast: choose storage account type and tiers, enable a lifecycle policy to move old blobs to Cool and Archive, and describe how you would monitor costs?","answer":"Setup: a general-purpose v2 Storage Account with a blob container for user uploads, Hot tier for new blobs, CDN in front for fast access. Add a Lifecycle Policy: move to Cool after 30 days, Archive af","explanation":"## Why This Is Asked\nTests understanding of Azure Storage lifecycle, cost optimization, and delivery performance with CDN.\n\n## Key Concepts\n- Blob storage tiers Hot/Cool/Archive\n- Lifecycle management rules\n- CDN integration and egress costs\n- Cost Management budgets and alerts\n\n## Code Example\n```json\n{\n  \"rules\": [\n    {\"name\": \"MoveToCool\", \"definition\": {\"actions\": {\"baseBlob\": {\"tierToCool\": {\"daysSinceModifiedRead\": 30}}}, \"filters\": {\"blobTypes\": [\"blockBlob\"]}}},\n    {\"name\": \"MoveToArchive\", \"definition\": {\"actions\": {\"baseBlob\": {\"tierToArchive\": {\"daysSinceModifiedRead\": 180}}}, \"filters\": {\"blobTypes\": [\"blockBlob\"]}}\n  ]\n}\n```\n\n## Follow-up Questions\n- How would you tailor the policy for hot-access images?\n- What monitoring would you set up to detect cost spikes?","diagram":null,"difficulty":"beginner","tags":["azure-fundamentals"],"channel":"azure-fundamentals","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T16:46:10.866Z","createdAt":"2026-01-16T16:46:10.866Z"},{"id":"q-2968","question":"Design a global, multi-tenant log analytics pipeline on Azure where each tenant's logs are isolated, encrypted with customer-managed keys, and data never leaves its region. Specify ingestion, storage, governance, access control, and cost controls using per-tenant Event Hubs, per-tenant ADLS Gen2 with Delta Lake, Key Vault CMK, Lighthouse RBAC, Purview, Private Endpoints, and region-limited replication. Include rollback plan for schema changes and guardrails for quotas?","answer":"Use per-tenant Event Hubs feeding per-tenant ADLS Gen2 with Delta Lake; enforce CMK via Key Vault; delegate RBAC with Azure Lighthouse; catalog/lineage with Purview; isolate by region with Private End","explanation":"## Why This Is Asked\nTests hands-on ability to design isolated, governed, cross-region data pipelines on Azure, combining governance, security, and cost controls.\n\n## Key Concepts\n- Multi-tenant isolation with per-tenant Event Hubs and ADLS Gen2\n- Customer-managed keys in Key Vault and CMK rotation\n- Azure Lighthouse for cross-tenant RBAC delegation\n- Purview for data catalog and lineage; Delta Lake on ADLS Gen2\n- Private Endpoints, VNet service endpoints, region-limited replication\n- Quota-based cost controls and automated rollback\n\n## Code Example\n```javascript\n// Minimal infra sketch for a tenant\n{ \n  'tenantId': '<tenant-id>',\n  'eventHub': 'eh-<tenant-id>',\n  'storageAccount': 'st<tenant-id>',\n  'deltaLakePath': 'delta/<tenant-id>',\n  'keyVault': 'kv-<tenant-id>',\n  'policies': ['CMK', 'LighthouseRBAC', 'Purview']\n}\n```\n\n## Follow-up Questions\n- How would you rotate CMKs without downtime?\n- How do you validate tenant isolation in CI/CD?\n- How would you detect and remediate cross-tenant data egress violations?","diagram":"flowchart TD\n  A[Tenant] --> B[Event Hubs]\n  B --> C[ADLS Gen2 per tenant]\n  C --> D[Delta Lake]\n  D --> E[Purview catalog/lineage]\n  E --> F[Lighthouse RBAC]\n  F --> G[Private Endpoints/VNet]\n  G --> H[Region-bound replication]","difficulty":"advanced","tags":["azure-fundamentals"],"channel":"azure-fundamentals","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Hugging Face","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T19:31:17.404Z","createdAt":"2026-01-16T19:31:17.404Z"},{"id":"q-3009","question":"Scenario: You have an Azure Function (Consumption plan) that ingests JSON payloads and writes to Cosmos DB. How would you securely store and access the Cosmos DB connection string using Azure Key Vault and a Managed Identity? Include enabling the identity, granting access, retrieving the secret in code, and handling rotation?","answer":"Enable system-assigned managed identity on the Function, grant Secret.Get for the Key Vault secret containing the Cosmos DB connection string, and store the vault URI in app settings. In code, fetch t","explanation":"## Why This Is Asked\n\nTests understanding of cloud secret management basics: Managed Identities, Key Vault, and secure runtime access for a production-grade app.\n\n## Key Concepts\n\n- System-assigned managed identity\n- Azure Key Vault secret permissions\n- Azure SDKs for secret retrieval\n- Secret rotation considerations and caching\n\n## Code Example\n\n```javascript\nconst { DefaultAzureCredential } = require(\"@azure/identity\");\nconst { SecretClient } = require(\"@azure/keyvault-secrets\");\n\nconst credential = new DefaultAzureCredential();\nconst keyVaultUrl = process.env.KEYVAULT_URI; // https://<vault>.vault.azure.net\nconst client = new SecretClient(keyVaultUrl, credential);\nconst secret = await client.getSecret(\"CosmosDbConnectionString\");\nconst cosmosConn = secret.value;\n// use cosmosConn to initialize CosmosClient\n```\n\n## Follow-up Questions\n\n- How would you handle secret rotation and cache invalidation in a long-running function?\n- What changes if the Function is integrated with a VNet or private endpoint?","diagram":null,"difficulty":"beginner","tags":["azure-fundamentals"],"channel":"azure-fundamentals","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["NVIDIA","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T20:46:20.092Z","createdAt":"2026-01-16T20:46:20.092Z"},{"id":"q-3123","question":"Scenario: A SaaS API platform must serve multiple tenants with strict data isolation. External clients hit a public API endpoint, but tenant data must be isolated at the data layer. Propose an Azure architecture using API Management, Front Door, Private Link, Key Vault, and Managed Identities to enforce per-tenant quotas, auditability, and compliance. Include authentication/authorization strategy, deployment blueprint, and trade-offs between per-tenant VNet isolation vs shared networking. How would you test security and DR?","answer":"Use a public API surface via Azure API Management fronted by Front Door. Authenticate with Azure AD and extract tenantId from the JWT; enforce per-tenant quotas with APIM policy. Route to tenant-scope","explanation":"## Why This Is Asked\nTests multi-tenant API design using fundamental Azure primitives.\n\n## Key Concepts\n- API Management, Front Door, Private Link, Key Vault, Managed Identities\n- JWT tenant scoping, RBAC, quotas, auditing\n- Data isolation with tenantId partitioning\n\n## Code Example\n```xml\n<policies>\n  <inbound>\n    <validate-jwt header=\"Authorization\" failed validation-httpcode=\"401\" failed validation-error-message=\"Unauthorized\">\n      <openid-config url=\"https://login.microsoftonline.com/{tenantId}/v2.0/.well-known/openid-configuration\"/>\n      <required-claims>\n        <claim name=\"roles\"/>\n        <claim name=\"tid\"/>\n      </required-claims>\n    </validate-jwt>\n    <set-variable name=\"tenantId\" value=\"context.Request.headers.GetValueOrEmpty('X-Tenant-Id')\"/>\n    <quota-control-by-key key=\"@(context.Variables.GetValueOrDefault<string>('tenantId'))\" calls=\"100\" renewal-period=\"PT1H\"/>\n  </inbound>\n</policies>\n```\n\n## Follow-up Questions\n- How handle on/offboarding of tenants without downtime?\n- How to scale APIM across regions and ensure data residency?","diagram":"flowchart TD\n  APIM(API Management) --> FrontDoor[Azure Front Door]\n  APIM --> PrivateLinkToData[Private Link to Data Stores]\n  APIM --> AKV[Key Vault]\n  APIM --> MI[Managed Identities]\n  TenantData[(Tenant Data store, partitionKey=tenantId)] --> APIM","difficulty":"intermediate","tags":["azure-fundamentals"],"channel":"azure-fundamentals","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Instacart","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T04:04:31.787Z","createdAt":"2026-01-17T04:04:31.787Z"},{"id":"q-3218","question":"Design a global, latency‑sensitive bidding API on Azure to meet sub‑50ms p95 in multiple regions while preserving data residency. Propose regional API layers (AKS or App Service) behind Front Door, private endpoints to Cosmos DB and Storage, regional data replication, and a global cache/CDN. Outline telemetry, DR testing, and cost trade‑offs?","answer":"Design a global, latency‑sensitive bidding API on Azure to meet sub‑50ms p95 in multiple regions while preserving data residency. Propose regional API layers (AKS or App Service) behind Front Door, pr","explanation":"## Why This Is Asked\nTests practical Azure multi‑region latency design, data residency, and resilience using core services (Front Door, Private Endpoints, regional stores) plus DR planning.\n\n## Key Concepts\n- Global routing with Front Door\n- Private endpoints and VNets\n- Regional data stores with replication\n- Caching via CDN\n- Telemetry and observability\n- DR testing and cost considerations\n\n## Code Example\n```hcl\n# Terraform snippet (illustrative)\nresource \"azurerm_frontdoor\" \"bidding_fd\" {\n  name = \"bidding-frontdoor\"\n  location = \"global\"\n  # simplified configuration\n}\n```\n\n## Follow-up Questions\n- How would you validate p95 latency across regions?\n- How would you handle a region outage and DR failover?\n- What are the cost implications of AKS vs App Service for regional API layers?","diagram":"flowchart TD\n  A[Global Regions] --> B[Front Door]\n  B --> C[Regional API Layer (AKS/App Service)]\n  C --> D[Private Endpoints to Cosmos DB/Storage]\n  D --> E[Global Cache/CDN]\n  E --> F[Telemetry & Monitoring]","difficulty":"intermediate","tags":["azure-fundamentals"],"channel":"azure-fundamentals","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","MongoDB","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T07:33:07.554Z","createdAt":"2026-01-17T07:33:07.554Z"},{"id":"q-3262","question":"Global telemetry ingestion with data residency constraints: ingest regional events into a central analytics lake while raw payloads stay in-region; dashboards must access cross-region data with minimal egress and strict cost controls. Design a concrete Azure-based pipeline that provides regional ingestion with exactly-once semantics, cross-region analytics, data cataloging/auditing, and security/compliance controls. Name services, data formats, idempotent sinks, and disaster recovery plan?","answer":"In-region Event Hubs with capture to regional ADLS Gen2. Spark Structured Streaming (Synapse/Databricks) reads regional data and upserts into a centralized Delta Lake, ensuring exactly-once with org-w","explanation":"## Why This Is Asked\nRationale about data residency, cross-region analytics, and cost controls; tests ability to design Azure pipelines with exactly-once semantics and governance.\n\n## Key Concepts\n- Event Hubs capture, regional ADLS Gen2\n- Delta Lake upserts and exactly-once semantics\n- Azure Purview data catalog and RBAC\n- Cost governance and cross-region egress\n\n## Code Example\n```javascript\n// Pseudo Spark merge for idempotent sink\ndf.writeStream.format(\"delta\")\n  .option(\"checkpointLocation\",\"/chkpt/global\")\n  .foreachBatch((batchDF,batchId)=> {\n    DeltaTable.forPath(spark, \"/global/delta\")\n      .alias(\"g\").merge(batchDF.alias(\"s\"), \"g.key = s.key\")\n      .whenMatchedUpdateAll()\n      .whenNotMatchedInsertAll()\n      .execute()\n  })\n  .start()\n```\n\n## Follow-up Questions\n- How would you handle schema evolution across regions?\n- How to test idempotent behavior in CI/CD?","diagram":"flowchart TD\n  A[Regional Ingest] --> B[Event Hubs region]\n  B --> C[ADLS Gen2 regional capture]\n  C --> D[Global Spark/Orchestrator]\n  D --> E[Global Delta Lake]\n  E --> F[Dashboards/BI]","difficulty":"advanced","tags":["azure-fundamentals"],"channel":"azure-fundamentals","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Snowflake","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T09:29:22.802Z","createdAt":"2026-01-17T09:29:22.802Z"},{"id":"q-3398","question":"Scenario: A new feature stores user uploads in Azure Blob Storage. To minimize costs and meet retention policy, how would you implement a storage lifecycle policy to move blobs older than 60 days to Archive and delete blobs older than 365 days? Include steps to enable lifecycle management and verify with logs?","answer":"Create a blob lifecycle policy on the storage account with two rules: (1) move blobs older than 60 days to Archive, (2) delete blobs older than 365 days. Scope to the target container, enable lifecycl","explanation":"## Why This Is Asked\nThis tests practical cost control and data lifecycle in Azure Storage. It requires using lifecycle rules, storage tiers, and operational checks.\n\n## Key Concepts\n- Blob lifecycle management\n- Hot/Cool/Archive tiers\n- Delete and legal hold considerations\n- Soft delete and versioning\n\n## Code Example\n\n```javascript\n// Conceptual policy object (Azure uses JSON, this illustrates structure)\nconst policy = {\n  rules: [\n    {\n      name: 'Move60toArchive',\n      enabled: true,\n      definition: {\n        filters: { blobTypes: ['blockBlob'], prefixMatch: ['uploads/'] },\n        actions: { baseBlob: { tierToArchive: { daysSinceModificationGreaterThan: 60 } } }\n      }\n    },\n    {\n      name: 'DeleteOld365',\n      enabled: true,\n      definition: {\n        filters: { blobTypes: ['blockBlob'], prefixMatch: ['uploads/'] },\n        actions: { baseBlob: { delete: { daysSinceModificationGreaterThan: 365 } } }\n      }\n    }\n  ]\n}\n```\n\n## Follow-up Questions\n- How would you test the policy in a non-production storage account?\n- What are potential pitfalls with Archive tier access latency and restores?","diagram":null,"difficulty":"beginner","tags":["azure-fundamentals"],"channel":"azure-fundamentals","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Instacart"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T14:37:49.947Z","createdAt":"2026-01-17T14:37:49.947Z"},{"id":"q-3493","question":"Design a cross-tenant management pattern for a SaaS platform on Azure using Azure Lighthouse. How would you onboard a new customer, grant least-privilege access to your service across their subscription, enforce per-tenant RBAC, and maintain auditability and compliance across all tenants? Include onboarding steps, boundary definitions, and monitoring approaches?","answer":"Leverage Azure Lighthouse to grant a single managed service identity per tenant with least privilege via delegated resource management. Map roles to per-tenant scopes (e.g., Reader on app resources) a","explanation":"## Why This Is Asked\nAzure Lighthouse is central for cross-tenant SaaS management, testing familiarity with least privilege, per-tenant RBAC, auditability, and automated reviews in multi-tenant Azure.\n\n## Key Concepts\n- Azure Lighthouse\n- Delegated resource management\n- Per-tenant RBAC\n- Azure AD access reviews\n- Auditing and monitoring\n\n## Code Example\n```pseudo\n# Pseudocode for onboarding using Lighthouse\nOnboardTenant(tenantId, 'offerId', 'managementGroup')\nAssignRole(tenantId, '/tenantScope/app', 'Reader')\nEnableAuditLogs(tenantId)\n```\n\n## Follow-up Questions\n- How would you scale onboarding across thousands of tenants while keeping governance tight?\n- How would you handle revocation and tenant offboarding without service disruption?","diagram":null,"difficulty":"intermediate","tags":["azure-fundamentals"],"channel":"azure-fundamentals","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Meta","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T18:49:08.094Z","createdAt":"2026-01-17T18:49:08.094Z"},{"id":"q-3622","question":"Scenario: A multi-tenant telemetry pipeline on Azure ingests events per tenant via Event Grid and processes them in isolated per-tenant Function Apps. You must enforce per-tenant concurrency quotas, prevent budget overruns, and provide observable SLAs. Propose a practical end-to-end architecture and deployment plan using Event Grid, Service Bus (or Storage Queues), per-tenant Function Apps, and Application Insights. Include quota enforcement, scaling, failure modes, and onboarding?","answer":"Adopt a fan-out architecture: publish events to a centralized Event Grid topic; route to per-tenant Service Bus queues and deploy isolated per-tenant Function Apps; enforce concurrency quotas through Service Bus session locks and Function App host-level concurrency controls; implement budget governance via Azure Cost Management alerts and per-tenant resource limits; provide comprehensive observability with Application Insights custom metrics and Azure Monitor dashboards.","explanation":"## Why This Is Asked\nTests ability to design isolation-heavy, cost-aware, event-driven workloads on Azure with practical controls and observability. It also probes how to balance scaling, reliability, and governance in a real multi-tenant setting.\n\n## Key Concepts\n- Event Grid fan-out and per-tenant routing\n- Tenant isolation via per-tenant queues and Function Apps\n- Concurrency quotas and cross-tenant budget controls\n- Observability with Application Insights and Azure Monitor\n- Automation via Infrastructure as Code (ARM/Bicep)\n\n## Code Example\n```json\n{\n  \"perTenant\": {\n    \"tenantId\": \"tenant-001\",\n    \"concurrencyLimit\": 10,\n    \"budgetThreshold\": 100.0\n  }\n}\n```","diagram":"flowchart TD\n  EG[Event Grid Topic] --> SB[Per-tenant Queue]\n  SB --> FUNC[Per-tenant Functions]\n  FUNC --> DL[Data Lake]\n  FUNC --> MON[App Insights]","difficulty":"intermediate","tags":["azure-fundamentals"],"channel":"azure-fundamentals","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Snap","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T05:14:07.613Z","createdAt":"2026-01-17T23:43:01.729Z"},{"id":"q-3787","question":"Scenario: a SaaS platform ingests tenant data into a central Azure Data Lake Gen2. Tenants demand strict isolation, customer-controlled keys, and full auditability. Design an end-to-end ingestion and governance pattern using Azure Data Factory (or Synapse pipelines), ADLS Gen2, Event Grid, Azure Key Vault CMK, and per-tenant RBAC. Include data ingress paths, retry semantics, idempotency, and monitoring?","answer":"Implement per-tenant ingestion pipelines in Data Factory with managed identities, store raw data in tenant-scoped ADLS Gen2 folders, apply per-tenant RBAC, enable CMK-based encryption with rotation in","explanation":"## Why This Is Asked\nTests ability to design secure, multi-tenant data ingestion and governance on Azure with isolation, compliance, and automation.\n\n## Key Concepts\n- Data Factory or Synapse pipelines for ingestion\n- ADLS Gen2 tenant isolation and folder RBAC\n- Per-tenant customer-managed keys (CMK) with Key Vault\n- Event Grid triggers for event-driven pipelines\n- Idempotent upserts and watermarking for reliability\n- Purview for data catalog and auditing\n\n## Code Example\n```json\n{\n  \"name\": \"TenantIngest\",\n  \"type\": \"Pipeline\",\n  \"properties\": {\n    \"activities\": [\n      {\"name\": \"CopyData\",\n       \"type\": \"Copy\",\n       \"inputs\": [ {\"name\": \"RawInput\"} ],\n       \"outputs\": [ {\"name\": \"TenantRaw\"} ]}\n    ],\n    \"policy\": {\"tenantId\": \"${tenantId}\"}\n  }\n}\n```\n\n## Follow-up Questions\n- How would you handle security incident response and access revocation across tenants?\n- How would you validate data residency and retention policies per tenant?","diagram":null,"difficulty":"intermediate","tags":["azure-fundamentals"],"channel":"azure-fundamentals","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T09:37:18.514Z","createdAt":"2026-01-18T09:37:18.515Z"},{"id":"q-3828","question":"You’re provisioning a beginner Azure Fundamentals scenario: a small web app hosted on Azure App Service (Linux) connected to Azure Database for MySQL. Provide a minimal end-to-end setup a new developer can implement: (1) region/resource group choices, (2) GitHub Actions workflow to deploy on push to main, (3) secure connection strings via App Settings or Key Vault, (4) HTTPS enforcement and a custom domain, (5) a daily cost budget alert to keep spend in check. Keep it Azure-native and practical?","answer":"Create a Resource Group in East US; App Service Linux (B1) for the web app; Azure Database for MySQL in the same RG; GitHub Actions workflow using azure/login and azure/webapps-deploy to publish on pu","explanation":"## Why This Is Asked\n\nThis checks practical Azure fundamentals: resource grouping, App Service deployment, database connectivity, secret management, HTTPS enforcement, custom domains, GitHub Actions, and cost controls.\n\n## Key Concepts\n\n- Resource Groups and Regions\n- App Service (Linux) deployment\n- Azure Database for MySQL connectivity\n- Secret management via App Settings and Key Vault\n- HTTPS enforcement and custom domains\n- GitHub Actions for CI/CD\n- Cost Management budgets and alerts\n\n## Code Example\n\n```javascript\n// Example: read DB connection string from environment (used by app to connect)\nconst conn = process.env.DB_CONNECTION_STRING;\nconsole.log(!!conn);\n```\n\n## Follow-up Questions\n\n- How would you rotate the database credentials securely?\n- What changes would you make to support staging deployments and separate budgets for each environment?","diagram":null,"difficulty":"beginner","tags":["azure-fundamentals"],"channel":"azure-fundamentals","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Instacart","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T11:25:24.382Z","createdAt":"2026-01-18T11:25:24.382Z"},{"id":"q-3859","question":"You’re building a small web app with a static frontend in Azure Blob Storage and a serverless API in Azure Functions (Consumption). You must track costs by environment (dev/test/prod) and enforce tag requirements via policy. Describe the end-to-end setup: RGs per env, tagging strategy, per-env budget alerts, and a GitHub Actions workflow to deploy with environment-specific settings, all Azure-native?","answer":"Create three Resource Groups (dev, test, prod) and tag every resource with Environment=<env>. Enforce via an Azure Policy that requires this tag. Set per-RG Cost Management budgets with alerts at 80% ","explanation":"## Why This Is Asked\n\nThis checks understanding of environment isolation, cost tracking, and Azure-native automation for beginners.\n\n## Key Concepts\n\n- Resource Groups per environment\n- Tagging and Azure Policy enforcement\n- Cost Management budgets and alerts\n- ARM/Bicep templates for repeatable deploys\n- GitHub Actions for per-env deployment\n\n## Code Example\n\n```json\n{\n  \"env\": \"dev\"\n}\n```\n\n## Follow-up Questions\n\n- How would you test the policy in a dev environment?\n- What are trade-offs of per-env RGs vs a single RG with tags?","diagram":null,"difficulty":"beginner","tags":["azure-fundamentals"],"channel":"azure-fundamentals","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","DoorDash"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T13:03:25.423Z","createdAt":"2026-01-18T13:03:25.423Z"},{"id":"q-3954","question":"Design a globally distributed SaaS API on Azure that serves many tenants with strict data isolation. Propose a cost-aware pattern using Azure Functions, Cosmos DB, and Azure AD. Explain tenant scoping, data partitioning, per-tenant RLS, onboarding, least-privilege access, and observability at scale, including failure modes and DR options?","answer":"Use Cosmos DB with a tenantId partition key; enforce access with Azure AD RBAC and per-tenant claims; scope queries and API routes to the tenant. Onboard via scoped ARM templates and Managed Identitie","explanation":"## Why This Is Asked\n\nTests a candidate's ability to design multi-tenant isolation, scale, and cost control in Azure for a global SaaS.\n\n## Key Concepts\n\n- Cosmos DB partitioning by tenantId\n- Per-tenant access with Azure AD RBAC and Managed Identities\n- Onboarding, provisioning, and audit trails\n- Secrets management with Key Vault\n- Observability: metrics, logs, alerts; cost governance; DR planning\n\n## Code Example\n\n```javascript\n// Node.js sample: fetch tenant-scoped data\nconst { CosmosClient } = require(\"@azure/cosmos\");\nconst client = new CosmosClient({ endpoint: process.env.COSMOS_ENDPOINT, key: process.env.COSMOS_KEY });\nasync function getTenantDocs(tenantId) {\n  const container = client.database(\"saas\").container(\"tenantData\");\n  const { resources } = await container.items\n    .query({ query: \"SELECT * FROM c WHERE c.tenantId = @t\", parameters: [{ name: \"@t\", value: tenantId }] })\n    .fetchAll();\n  return resources;\n}\n```\n\n## Follow-up Questions\n\n- How would you validate tenant isolation during CI/CD?\n- How would you handle tenant onboarding, rotation of secrets, and audits?","diagram":null,"difficulty":"advanced","tags":["azure-fundamentals"],"channel":"azure-fundamentals","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Snap","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T16:53:11.318Z","createdAt":"2026-01-18T16:53:11.319Z"},{"id":"q-3968","question":"Design a minimal end-to-end Azure Fundamentals setup for a small web app using App Service for Linux with an Azure SQL Database backend. Ensure connectivity is private (Private Endpoint in a VNet), specify region/resource group strategy, include a GitHub Actions workflow to deploy on push to main, secure the connection string via Key Vault/App Settings, enforce HTTPS with a custom domain, and add a daily cost alert?","answer":"Create a single RG in one region, a VNet with subnets for App Service and a Private Endpoint to Azure SQL; deploy App Service Linux and Azure SQL; enable Private Endpoint and Private DNS; enable syste","explanation":"## Why This Is Asked\nTests ability to design an isolated, Azure-native end-to-end for beginners, touching networking, security, CI/CD, and cost controls.\n\n## Key Concepts\n- Private Endpoint and Private DNS\n- Virtual Network segmentation\n- Managed identity and Key Vault integration\n- GitHub Actions with Azure login\n- HTTPS-only and custom domain\n- Cost management budgets\n\n## Code Example\n```bash\n# Example: create resource group and basic network (simplified)\naz group create -l eastus -n rg-demo\naz network vnet create -g rg-demo -n vnet-demo --address-prefix 10.0.0.0/16 --subnet-name appsub --subnet-prefix 10.0.1.0/24\n```\n\n## Follow-up Questions\n- What are trade-offs of private endpoints vs service endpoints?\n- How would you validate connectivity from App Service to Azure SQL via the Private Endpoint?","diagram":"flowchart TD\n  A[Resource Group] --> B[VNet]\n  B --> C[Private Endpoint for SQL]\n  A --> D[App Service]\n  D --> E[Key Vault Access via Managed Identity]\n  D --> F[GitHub Actions Deployment]\n  G[Custom Domain] --> H[HTTPS Enforcement]\n  I[Cost Budget] --> J[Alerts]","difficulty":"beginner","tags":["azure-fundamentals"],"channel":"azure-fundamentals","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T17:37:15.264Z","createdAt":"2026-01-18T17:37:15.265Z"},{"id":"q-4039","question":"Design a geographically-distributed telemetry ingestion pipeline that keeps data residency per region, auto-scales with high throughput, and minimizes cross-region egress. Specify data ingress, storage, processing, and governance across Azure regions, justify Azure services (Event Hubs, Data Lake Storage, Synapse/Databricks), and show region-anchored processing, region-scoped RBAC, and cost controls?","answer":"Implement per-region data ingestion using Azure Event Hubs deployed in each geographic region, store raw telemetry in region-specific Data Lake Storage Gen2 accounts, process data with in-region Spark clusters via Azure Synapse Analytics or Databricks to create curated datasets, and expose insights through region-anchored Power BI workspaces or Azure Data Explorer clusters, ensuring strict data residency compliance while minimizing cross-region egress costs.","explanation":"Why This Is Asked\n\nThis question evaluates expertise in designing global-scale Azure data pipelines that address data residency requirements, cost optimization, and security governance across distributed systems.\n\nKey Concepts\n\n- Regional autonomy and data residency compliance\n- Ingress/egress cost optimization strategies\n- Policy enforcement and RBAC across multiple regions\n- Auto-scaling for high-throughput scenarios\n\nCode Example\n\n```javascript\n// pseudo configuration sketch\nconst region = 'eastus';\nconst eventHub = `eh-${region}`;\nconst dataLake = `dl-${region}`;\n```\n\nFollow-up Questions","diagram":"flowchart TD\n  A(Ingress) --> B(EventHubs_Region)\n  B --> C(Storage_Region)\n  C --> D(Processing_Spark_Region)\n  D --> E(Curated_Region_View)\n  E --> F(Dashboard_Region)","difficulty":"advanced","tags":["azure-fundamentals"],"channel":"azure-fundamentals","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Microsoft","Snowflake","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-19T06:10:47.513Z","createdAt":"2026-01-18T20:54:36.311Z"},{"id":"q-4130","question":"Design a scalable, multi-tenant telemetry ingestion path on Azure for a SaaS product. How would you ensure per-tenant data isolation, dynamic throughput, and auditable access while keeping costs predictable? Propose concrete services (IoT Hub vs Event Hubs, Functions, per-tenant storage, CMK) and onboarding, RBAC, and monitoring?","answer":"Architect a scalable, multi-tenant telemetry ingestion path on Azure. Use tenant-scoped Event Hubs or IoT Hub, a Functions-based pipeline, per-tenant storage with CMK in Key Vault, and automated onboa","explanation":"## Why This Is Asked\n\nThis question probes ability to design end-to-end scalable ingestion with strong isolation and cost discipline, plus onboarding automation.\n\n## Key Concepts\n\n- Tenant isolation strategies (namespaces, resource groups)\n- Ingestion options (IoT Hub vs Event Hubs) and scaling\n- Data encryption with CMK and Key Vault\n- RBAC, Service Principals, least privilege\n- Onboarding automation (ARM/Bicep/Terraform), CI/CD\n- Monitoring, auditing, budgets\n\n## Code Example\n\n```javascript\n// Pseudo onboarding snippet (illustrative only)\nasync function onboardTenant(tenantId){\n  await createEventHubNamespace(`tenant-${tenantId}-eh`);\n  await createStorageAccount(`tenant${tenantId}sa`);\n  await assignRBAC(tenantId, 'Contributor');\n  await configureCMK(`tenant-${tenantId}-cmk`);\n}\n```\n\n## Follow-up Questions\n\n- How would you test isolation boundaries at scale?\n- What are the trade-offs of per-tenant storage vs a shared data lake with tagging?","diagram":null,"difficulty":"advanced","tags":["azure-fundamentals"],"channel":"azure-fundamentals","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Google","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-19T04:31:23.199Z","createdAt":"2026-01-19T04:31:23.199Z"},{"id":"q-4320","question":"Scenario: A small web app runs on Azure App Service (Linux) and calls a 3rd-party API. Design a beginner-end-to-end setup to (a) place App Service into a dedicated VNet and enable outbound access control to a single trusted path, (b) securely manage the API key using Azure Key Vault with a managed identity, (c) enable HTTPS with a custom domain, and (d) set a daily budget alert. Include concrete steps and sample commands?","answer":"Create RG and VNet with a subnet; deploy App Service (Linux) and enable regional VNet integration; enable a system-assigned managed identity on the App Service; create a Key Vault, store the API key, ","explanation":"## Why This Is Asked\n\nTests practical use of core Azure fundamentals: network isolation, identity, secrets management, TLS, and cost control.\n\n## Key Concepts\n\n- VNet integration and outbound access\n- Managed identity and Key Vault access policies\n- Key Vault references in App Settings\n- HTTPS and custom domains\n- Cost management budgets\n\n## Code Example\n\n```bash\n# sample CLI steps\naz group create --name MyRG --location westus\naz network vnet create --resource-group MyRG --name MyVNet --address-prefix 10.0.0.0/16 --subnet-name AppSubnet --subnet-prefix 10.0.1.0/24\n```\n\n## Follow-up Questions\n\n- How would you rotate the API key automatically?\n- How would you monitor the outbound egress for the App Service?","diagram":null,"difficulty":"beginner","tags":["azure-fundamentals"],"channel":"azure-fundamentals","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Hashicorp","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-19T13:28:48.787Z","createdAt":"2026-01-19T13:28:48.787Z"},{"id":"q-4389","question":"Design an Azure data pipeline for a multinational retailer requiring data sovereignty: ingest from Event Hub in Region1, process with Functions (Managed Identity) masking PII, write to ADLS Gen2 Region1, replicate to Region2 for analytics, enforce per-tenant isolation with RBAC and separate storage, catalog data with Purview, encrypt with CMK in Key Vault, enable Private Link, and implement cross-region DR and cost governance. Provide the architecture, security controls, and operations plan?","answer":"Region-bound data pipeline: Ingest via Event Hub (Region1), process in Functions (Managed Identity) with PII masking, write to ADLS Gen2 Region1, replicate to Region2 for analytics, enforce per-tenant","explanation":"## Why This Is Asked\n\nTests end-to-end Azure data pipeline design, emphasizing data sovereignty, security, and governance.\n\n## Key Concepts\n\n- Data residency across regions\n- Event-driven processing with serverless\n- Per-tenant isolation and RBAC\n- Data catalog and encryption with CMK\n- Private connectivity and DR\n\n## Code Example\n\n```bash\n# placeholder\necho ok\n```\n\n## Follow-up Questions\n\n- How would you handle schema evolution across Purview?\n- What monitoring alerts would you configure for data leakage or cost anomalies?","diagram":"flowchart TD\n  Ingest --> Process\n  Process --> Store\n  Store --> Replicate\n  Replicate --> Catalog\n  Catalog --> DR","difficulty":"advanced","tags":["azure-fundamentals"],"channel":"azure-fundamentals","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","LinkedIn","Lyft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-19T16:54:21.891Z","createdAt":"2026-01-19T16:54:21.891Z"},{"id":"q-4416","question":"Design a multi-tenant telemetry pipeline in Azure that isolates data, scales for thousands of devices, and stays cost-efficient across two regions. Propose an end-to-end architecture using a central ingest (IoT Hub or Event Hubs), per-tenant RBAC, cross-region DR, and monitoring; describe data flow, security controls, and a minimal CI/CD path for infra-as-code?","answer":"Architect a single IoT Hub for ingestion; embed tenantId in messages and route via built-in Routes to per-tenant ADLS Gen2 via Data Factory pipelines. Use RBAC scoped to each storage container for str","explanation":"## Why This Is Asked\nTests ability to design scalable, compliant telemetry pipelines with tenant isolation, DR, and cost controls. It blends data ingress, storage security, and governance.\n\n## Key Concepts\n- Multi-tenant data isolation, RBAC, Private Link\n- Ingestion pattern with IoT Hub / Event Hubs\n- Cross-region DR with low RPO\n- Cost governance with auto-scaling and budgets\n\n## Code Example\n```json\n{ \"pipeline\": \"example\" }\n```\n\n## Follow-up Questions\n- How would you enforce per-tenant data residency if tenants span continents?\n- What changes if device count spikes by 10x in a peak hour?","diagram":null,"difficulty":"advanced","tags":["azure-fundamentals"],"channel":"azure-fundamentals","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-19T17:57:20.620Z","createdAt":"2026-01-19T17:57:20.620Z"},{"id":"q-4543","question":"You need a real-time GPU-accelerated AI inference service in Azure for multiple tenants, with automatic scaling and strict per-tenant isolation. Propose a concrete architecture using AKS with GPU node pools, namespaces per tenant, RBAC via Azure AD groups, NetworkPolicy, and an API gateway (Azure API Management) with rate limiting and auth. Include region choice, model registry (Azure ML/ACR), CI/CD (GitHub Actions), data egress controls, and monitoring/cost alerts?","answer":"Propose an AKS-based GPU inference architecture with strict per-tenant isolation: separate namespaces for each tenant, RBAC integrated with Azure AD groups for access control, NetworkPolicy for network segmentation, and Azure API Management as the gateway with per-tenant rate limiting and JWT validation.","explanation":"## Why This Is Asked\nTests practical Azure architecture skills for AI workloads, focusing on multi-tenant isolation, automatic scaling, and cost control beyond basic service configurations.\n\n## Key Concepts\n- AKS GPU scheduling, node pools, and autoscaling\n- Kubernetes namespaces and RBAC with Azure AD integration\n- NetworkPolicy for tenant network isolation\n- API gateway with rate limiting and authentication\n- Model registry and CI/CD for ML models\n- Cost governance and data egress controls\n\n## Code Example\n```yaml\n# Minimal Kubernetes RBAC example for a tenant namespace\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: tenant-a\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  namespace: tenant-a\n  name: tenant-a-role\nrules:\n- apiGroups: [\"\"]\n  resources: [\"pods\", \"services\"]\n  verbs: [\"get\", \"list\", \"create\"]\n```","diagram":"flowchart TD\n  TenantOnboard[Tenant Onboard] --> NamespacePerTenant[Namespace per tenant]\n  NamespacePerTenant --> APIGateway[API Management Gateway]\n  APIGateway --> GPUInference[GPU Inference Pod]\n  GPUInference --> ModelRegistry[Model Registry]\n  GPUInference --> CostMonitoring[Cost & Monitoring]","difficulty":"intermediate","tags":["azure-fundamentals"],"channel":"azure-fundamentals","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T06:15:14.044Z","createdAt":"2026-01-19T22:53:46.567Z"},{"id":"q-4665","question":"Design a global IoT telemetry pipeline on Azure that minimizes egress, enforces per-tenant quotas, and meets data residency rules. Specify edge gateway, ingestion (IoT Hub/Event Hub), storage (ADLS Gen2), compute (Functions/Databricks), and governance (Purview). Describe tenant isolation, RBAC with Lighthouse, cost controls, and DR. What steps and trade-offs would you choose?","answer":"Design a hub-and-spoke IoT pipeline: edge gateway, ingestion via IoT Hub, regional ADLS Gen2 for raw data, compute via Functions and Spark, and governance via Purview. Use Managed Identities and Azure","explanation":"## Why This Is Asked\n\nTests ability to design a scalable, compliant IoT data platform with data residency and multi-tenant governance.\n\n## Key Concepts\n\n- IoT edge and ingestion choices (IoT Hub vs Event Hub)\n- Regionalized storage and data residency\n- Tenant isolation and RBAC with Azure Lighthouse\n- Cost governance: quotas, budgets, egress controls\n- DR across regions and failover testing\n\n## Code Example\n\n```yaml\n# Skeleton policy for tenant quotas (illustrative)\npolicyName: TenantQuotaPolicy\nmode: All\n```\n\n## Follow-up Questions\n\n- How would you validate per-tenant quotas in production?\n- What metrics would you monitor to detect egress overruns?","diagram":null,"difficulty":"advanced","tags":["azure-fundamentals"],"channel":"azure-fundamentals","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","NVIDIA","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T07:09:54.435Z","createdAt":"2026-01-20T07:09:54.435Z"},{"id":"q-4804","question":"You're building a multi-tenant analytics portal in Azure for three regional customers. Each tenant must have isolated data, identity-based access, and data residency. Propose a practical Azure-native architecture using per-tenant Resource Groups and databases, API Management, Azure Functions, and App Service; include onboarding, least-privilege RBAC, auto-scaling, and auditing/compliance. How would you implement?","answer":"Adopt per-tenant isolation using individual Resource Groups and databases (Cosmos DB or SQL) per tenant; front the API with API Management and expose functions via App Service with managed identities.","explanation":"Why This Is Asked\\n\\nTests practical Azure-native architecture for multi-tenancy, data residency, governance.\\n\\nKey Concepts\\n- Per-tenant RBAC and resource isolation; API Management integration; serverless and app hosting; cost governance; auditing.\\n\\nCode Example\\n```javascript\\n// Not required for this question; deployment outline\\n```\\n\\nFollow-up Questions\\n- How would you handle onboarding of a new tenant with zero-downtime?\\n- How would you enforce data egress controls and cross-region replication?","diagram":null,"difficulty":"intermediate","tags":["azure-fundamentals"],"channel":"azure-fundamentals","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Goldman Sachs","Hashicorp"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T13:31:02.817Z","createdAt":"2026-01-20T13:31:02.818Z"},{"id":"q-859","question":"You have a REST API hosted on Azure App Service that processes user uploads and stores them in a separate Azure Blob Storage account. To avoid embedding secrets, how would you securely grant the API read/write access to the blob container using a managed identity? Outline the exact steps and roles you would apply, and mention any network considerations?","answer":"Enable system-assigned managed identity on the App Service. Grant Storage Blob Data Contributor to that identity at the storage account/container scope. In code, use DefaultAzureCredential to instanti","explanation":"## Why This Is Asked\nTests practical understanding of secure service-to-resource access without secrets, a core Azure fundamentals scenario.\n\n## Key Concepts\n- Managed Identity\n- RBAC and Storage Blob Data Contributor\n- DefaultAzureCredential\n- Private Endpoint / Block Public Access\n\n## Code Example\n```javascript\nconst { BlobContainerClient } = require('@azure/storage-blob');\nconst containerUrl = 'https://<storage>.blob.core.windows.net/<container>';\nconst client = new BlobContainerClient(containerUrl, new DefaultAzureCredential());\n```\n\n## Follow-up Questions\n- Compare system-assigned vs user-assigned identities.\n- How would you audit access with Azure Activity Logs?","diagram":null,"difficulty":"beginner","tags":["azure-fundamentals"],"channel":"azure-fundamentals","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","MongoDB","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:41:14.618Z","createdAt":"2026-01-12T13:41:14.618Z"},{"id":"q-865","question":"An IoT platform deployed in Azure collects about 5 million device events per minute from global regions. You must build an ingestion and processing pipeline that guarantees at-least-once delivery with idempotent writes to Cosmos DB, tolerates regional outages, and minimizes cost. Describe the end-to-end architecture, data flow, dedup strategy, and monitoring?","answer":"Ingest with regional Event Hubs per region; route to a central processing plane using Event Grid and Functions; apply idempotent write to Cosmos DB using eventId as the partition key; store raw events","explanation":"## Why This Is Asked\nTests ability to design a resilient, scalable Azure data-pipeline using event-driven components, while ensuring data integrity with idempotent writes and deduplication.\n\n## Key Concepts\n- Event-driven ingestion across regions: Event Hubs, Event Grid\n- Processing semantics: at-least-once, idempotent writes, dedup via eventId\n- Global storage strategy: Cosmos DB multi-region writes + upserts\n- Observability: Azure Monitor, Log Analytics, Application Insights\n\n## Code Example\n```javascript\n// Pseudo-code: idempotent write using eventId\nasync function upsertEvent(event) {\n  await cosmos.upsert('events', event.eventId, event.payload);\n}\n```\n\n## Follow-up Questions\n- How would you implement exactly-once semantics across region failover?\n- How would you handle out-of-order events and duplicate bursts?","diagram":"flowchart TD\n  A(Ingest: regional Event Hubs) --> B(Event Grid & Functions)\n  B --> C{Cosmos DB Writes}\n  C -->|Multi-region writes| D[Cosmos DB]\n  B --> E[ADLS Gen2 (raw events)]\n  B --> F[Monitoring: Azure Monitor / APPS Insights]\n  D --> G[Downstream Analytics]\n  E --> G","difficulty":"intermediate","tags":["azure-fundamentals"],"channel":"azure-fundamentals","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:46:23.167Z","createdAt":"2026-01-12T13:46:23.167Z"},{"id":"q-932","question":"Scenario: a multi-tenant SaaS app runs on Azure App Service and stores per-tenant files in separate containers in Azure Blob Storage. To avoid hard-coded keys, describe a secure pattern using managed identities and RBAC to grant the app the correct container access while ensuring tenant isolation and minimal permission surface. Outline steps, roles, and any network considerations (e.g., Private Endpoint)?","answer":"Use a user-assigned managed identity for the App Service, assign the Storage Blob Data Contributor role scoped to the tenant’s blob container, and store container identifiers in Key Vault. The app aut","explanation":"## Why This Is Asked\nTests understanding of secure identity-based access patterns for multi-tenant storage in Azure, including least privilege, tenant isolation, and network shielding.\n\n## Key Concepts\n- Managed identities\n- RBAC scoping and access control\n- Azure Key Vault integration\n- Private endpoints\n\n## Code Example\n```javascript\nconst { BlobContainerClient } = require(\"@azure/storage-blob\");\nconst { DefaultAzureCredential } = require(\"@azure/identity\");\n\nconst containerUrl = process.env.TENANT_CONTAINER_URL; // per-tenant\nconst credential = new DefaultAzureCredential();\nconst containerClient = new BlobContainerClient(containerUrl, credential);\n\n// Upload\nawait containerClient.getBlockBlobClient(\"file.txt\").uploadData(buffer);\n```\n\n## Follow-up Questions\n- How would you handle tenant onboarding/offboarding and container lifecycle?\n- How to audit access and detect unauthorized attempts?","diagram":null,"difficulty":"beginner","tags":["azure-fundamentals"],"channel":"azure-fundamentals","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T15:41:44.359Z","createdAt":"2026-01-12T15:41:44.359Z"},{"id":"q-984","question":"In a globally distributed API using Azure Functions and Cosmos DB, implement per-tenant data isolation with minimal cross-region data transfer. How would you design the architecture, enforce least-privilege access via managed identities and RBAC, and ensure data residency with Private Endpoints and cross-region replication policies?","answer":"Design a Cosmos DB account with per-tenant isolation (tenantId partition key) and multi-region writes to minimize cross-region traffic. Authenticate Azure Functions via a system-assigned managed ident","explanation":"## Why This Is Asked\nAssesses ability to balance data residency, isolation, and least-privilege access in a real Azure setup.\n\n## Key Concepts\n- Cosmos DB multi-region writes\n- Azure AD RBAC on Cosmos DB\n- Managed identities for Functions\n- Private Endpoint and public network disablement\n- Tenant-aware partitioning\n\n## Code Example\n```javascript\n// Node.js Cosmos SDK example: parameterized tenant query\nconst querySpec = {\n  query: \"SELECT * FROM c WHERE c.tenantId = @tenantId\",\n  parameters: [{ name: \"@tenantId\", value: tenantId }]\n};\n```\n\n## Follow-up Questions\n- How would you monitor for access violations and rotate tenant keys?\n- What changes if tenants require cross-tenant analytics with strict isolation?","diagram":null,"difficulty":"advanced","tags":["azure-fundamentals"],"channel":"azure-fundamentals","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","MongoDB","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T17:49:01.803Z","createdAt":"2026-01-12T17:49:01.803Z"}],"subChannels":["general"],"companies":["Adobe","Airbnb","Amazon","Apple","Bloomberg","Citadel","Cloudflare","Coinbase","Databricks","Discord","DoorDash","Goldman Sachs","Google","Hashicorp","Hugging Face","IBM","Instacart","LinkedIn","Lyft","Meta","Microsoft","MongoDB","NVIDIA","Netflix","OpenAI","Oracle","PayPal","Robinhood","Salesforce","Scale Ai","Slack","Snap","Snowflake","Square","Stripe","Tesla","Twitter","Two Sigma","Uber","Zoom"],"stats":{"total":53,"beginner":18,"intermediate":16,"advanced":19,"newThisWeek":42}}