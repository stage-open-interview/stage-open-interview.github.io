{"questions":[{"id":"q-845","question":"In a MongoDB-backed service, read latency tails spike during peak hours. Provide a concrete, practical debugging plan to determine whether the bottleneck is network, driver, query plan, or index design. Include exact steps, metrics to collect, and concrete changes (indexes, readConcern, pooling) you would apply, plus how you would verify impact?","answer":"Collect end-to-end latency, queue depth, and replica lag; enable slow query logs and run explain on top slow queries; verify index coverage with compound indexes; apply index hints or add a composite ","explanation":"## Why This Is Asked\nThis question probes practical debugging of latency in a MongoDB-backed service with real-world constraints.\n\n## Key Concepts\n- Tail latency diagnosis: slow queries, network, driver pool\n- Explain plans and index design: compound indexes, prefix rules\n- Replica lag and read concerns\n- Driver tuning and observability\n\n## Code Example\n```js\ndb.collection(`orders`).find({ userId: id }).explain(`executionStats`);\n```\n\n## Follow-up Questions\n- How would you validate the impact of an index change in production?\n- What metrics would you alert on for sustained tail latency?","diagram":null,"difficulty":"intermediate","tags":["cnpa"],"channel":"cnpa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Lyft","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:28:43.195Z","createdAt":"2026-01-12T13:28:43.195Z"},{"id":"q-873","question":"A CNPA service receives events over HTTP, writes to PostgreSQL, and publishes to Kafka. During peak hours, read latency spikes. Describe a concrete debugging plan to determine whether the bottleneck is the HTTP handler, the DB query, or the Kafka producer, including exact steps, metrics to collect, and concrete changes (indexes, pooling, prepared statements, idempotent producer) and how you would verify impact?","answer":"Instrument with OpenTelemetry to trace HTTP handler, DB queries, and Kafka publish; collect p95/p99 latency, error rate, and queue backpressure. Reproduce under load (k6). If HTTP slow, tune keep-aliv","explanation":"## Why This Is Asked\nTests practical debugging across a real CNPA pipeline and encourages measurable fixes.\n\n## Key Concepts\n- End-to-end tracing with OpenTelemetry\n- Performance attribution across HTTP, DB, and messaging\n- Idempotent producers and pool tuning\n- Load testing and metrics-driven verification\n\n## Code Example\n```javascript\n// OpenTelemetry tracing skeleton for CNPA flow\nconst { trace } = require('@opentelemetry/api');\nconst tracer = trace.getTracer('cnpa-trace');\nasync function handleEvent(req, res) {\n  await tracer.startActiveSpan('handle_event', async (span) => {\n    await processHttp();\n    await writeDb();\n    await publishKafka();\n    span.end();\n  });\n}\n```\n\n## Follow-up Questions\n- How would you measure impact of a pool size increase?\n- What changes would you apply to avoid future tail latency spikes?","diagram":"flowchart TD\n  A[HTTP Request] --> B[HTTP Handler]\n  B --> C[PostgreSQL]\n  B --> D[Kafka Producer]\n  C --> E[Indexes/Pooling]\n  D --> F[Configs]\n  G[Metrics] --> H[Tail Latency]","difficulty":"beginner","tags":["cnpa"],"channel":"cnpa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Instacart","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:55:00.870Z","createdAt":"2026-01-12T13:55:00.870Z"},{"id":"q-903","question":"In a CNPA stack, an HTTP API writes to Postgres, publishes to Kafka, and a Redis-backed dashboard consumes the stream. During peak load, HTTP latency tails spike. Provide a concrete debugging plan to isolate whether the bottleneck is HTTP, DB, Kafka, producer, consumer, or Redis, with exact metrics, sampling, and concrete changes (pooling, prepared statements, acks, batch sizes, caching strategies) and how you would verify impact?","answer":"Use end-to-end tracing to map latency per hop: HTTP handler, DB query (EXPLAIN ANALYZE), Kafka publish, consumer processing, Redis read. Gather p95/p99 latencies, throughput, queue depths, and GC/thre","explanation":"## Why This Is Asked\n\nTests practical debugging across CNPA stack tail latency.\n\n## Key Concepts\n\n- Distributed tracing\n- Postgres tuning\n- Kafka producer/consumer\n- Redis caching\n\n## Code Example\n\n```bash\n# Example commands to reproduce latency\nEXPLAIN ANALYZE SELECT ...;\n```\n\n## Follow-up Questions\n\n- How would you quantify improvement after each change?\n- Which metric thresholds signal success or regression?","diagram":null,"difficulty":"intermediate","tags":["cnpa"],"channel":"cnpa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Meta","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T14:41:59.785Z","createdAt":"2026-01-12T14:41:59.785Z"},{"id":"q-928","question":"In a CNPA stack consisting of an HTTP API, PostgreSQL, Kafka, and Redis, latency tails spike under peak load. Provide a concrete, beginner-friendly plan to enable end-to-end tracing with OpenTelemetry to pinpoint the bottleneck. Include trace propagation, spans for HTTP handler, DB query, Kafka publish/consume, Redis access, and verification steps with a sample end-to-end trace?","answer":"Instrument OpenTelemetry across all components and propagate traceparent. Create spans for HTTP handler, each DB query, Kafka producer/consumer, and Redis access. Route to a collector (Jaeger/Tempo), ","explanation":"## Why This Is Asked\nUnderstand practical observability across CNPA stack with hands-on tracing.\n\n## Key Concepts\n- OpenTelemetry, trace propagation, end-to-end latency\n- Span creation in HTTP, DB, Kafka, Redis\n- Collector backends (Jaeger/Tempo) and dashboards\n\n## Code Example\n```javascript\n// sample: initialize tracer and extract traceparent from incoming HTTP header\nconst { diag, trace } = require('@opentelemetry/api');\n// setup omitted for brevity\n```\n\n## Follow-up Questions\n- How would sampling change under high throughput?\n- How would you adapt if a component uses a different broker or cache?\n","diagram":"flowchart TD\n  HTTP[HTTP API] --> DB[(PostgreSQL)]\n  HTTP --> Kafka[(Kafka Producer)]\n  Kafka --> Broker[(Kafka Broker)]\n  Broker --> Consumer[(Kafka Consumer)]\n  Consumer --> Redis[(Redis)]","difficulty":"beginner","tags":["cnpa"],"channel":"cnpa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Hashicorp","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T15:39:06.442Z","createdAt":"2026-01-12T15:39:06.442Z"}],"subChannels":["general"],"companies":["Anthropic","Bloomberg","Databricks","Discord","Hashicorp","Instacart","Lyft","Meta","MongoDB","NVIDIA","Netflix","Tesla"],"stats":{"total":4,"beginner":2,"intermediate":2,"advanced":0,"newThisWeek":4}}