{"questions":[{"id":"q-1051","question":"CNPA stack: HTTP API writes to PostgreSQL and emits Kafka events. A hot, large users table needs a non-blocking schema change (e.g., adding a new NOT NULL column with default). Propose a production-grade online migration plan that minimizes downtime, keeps Kafka in sync, handles backfill, and describes rollback and validation steps?","answer":"Use online schema migration with a shadow table and dual writes: create users_new with migrated schema, keep users_old active, route reads via a feature flag to users_all view, backfill users_new in c","explanation":"## Why This Is Asked\nTests ability to design zero-downtime migrations in a CNPA stack, ensuring data correctness across Postgres and Kafka while validating with incremental rollout.\n\n## Key Concepts\n- Online migrations\n- Shadow tables\n- Dual writes\n- Atomic view swap\n- Chunked backfill\n- Rollback and validation\n\n## Code Example\n```sql\n-- Shadow table creation\nCREATE TABLE users_new (... migrated schema ...);\n-- Backfill in chunks\nINSERT INTO users_new (...) SELECT ... FROM users_old WHERE ...;\n-- Route reads via view\nCREATE OR REPLACE VIEW users_all AS SELECT * FROM users_new;\n```\n\n## Follow-up Questions\n- How would you validate data parity during backfill?\n- How would you monitor migration latency and rollback readiness?","diagram":null,"difficulty":"advanced","tags":["cnpa"],"channel":"cnpa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Instacart","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T20:36:16.305Z","createdAt":"2026-01-12T20:36:16.305Z"},{"id":"q-1150","question":"In a CNPA stack: HTTP API ingests events, writes to PostgreSQL, and publishes to Kafka; during spikes, retries duplicate processed events. Propose a concrete, end-to-end plan to guarantee idempotent processing and prevent duplicates under retry storms. Include idempotency key strategy, dedup enforcement point, Kafka/DB coordination, and validation?","answer":"Use an event-id per submission and make event_id unique in Postgres; perform INSERT into the events table with ON CONFLICT DO NOTHING so retries don't duplicate writes. Use a transactional outbox to a","explanation":"## Why This Is Asked\n\nTests ability to reason about retries, deduplication, and cross-service guarantees in CNPA pipelines.\n\n## Key Concepts\n\n- Idempotent processing\n- Outbox pattern\n- Postgres unique constraints\n- Kafka transactional producer\n\n## Code Example\n\n```javascript\n// Implementation example (pseudo)\n```\n\n## Follow-up Questions\n\n- How would you test edge cases like partial failures?\n- What about backpressure during spikes?","diagram":null,"difficulty":"advanced","tags":["cnpa"],"channel":"cnpa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Netflix","Snap","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T01:34:44.738Z","createdAt":"2026-01-13T01:34:44.738Z"},{"id":"q-1157","question":"CNPA pipeline uses an HTTP API -> PostgreSQL -> Kafka with Avro schemas in a Schema Registry. A new optional field is added to the events, and some consumers crash when they see older versions. Provide a concrete, zero-downtime plan for schema evolution, including compatibility mode, rollout strategy, topic/consumer changes, backfill approach, rollback, and validation?","answer":"Upgrade Avro schema with an optional field default, set Schema Registry compatibility to BACKWARD, and roll out via a canary topic. Publish new schema alongside the old one, route 5–10% traffic to the","explanation":"## Why This Is Asked\n\nTests real-world schema evolution discipline in CNPA pipelines, stressing zero-downtime migrations, compatibility, and rollback.\n\n## Key Concepts\n\n- Avro schema evolution and Schema Registry compatibility BACKWARD/FORWARD/FULL\n- Canary deployments and topic-level migrations\n- Backfill strategies and validation across HTTP, DB, and Kafka\n- Rollback paths and deserialization error handling\n- Feature-flag-driven routing and observability\n\n## Code Example\n\n```javascript\nconst newSchema = {\n  type: \"record\",\n  name: \"Event\",\n  fields: [\n    {name: \"id\", type: \"string\"},\n    {name: \"payload\", type: \"string\"},\n    {name: \"newField\", type: [\"null\", \"string\"], default: null}\n  ]\n}\n```\n\n## Follow-up Questions\n\n- How would you test compatibility in CI/CD?\n- How do you handle misbehaving consumers during rollout?\n- What metrics indicate a successful migration and what are alert thresholds?\n- How would you decommission the old schema gracefully after validation?","diagram":"flowchart TD\nA[HTTP API] --> B[Schema Registry v2]\nB --> C[New Kafka topic canary]\nC --> D[Canary consumer validation]\nD --> E[Full rollout]\nE --> F[Deprecate old schema]","difficulty":"advanced","tags":["cnpa"],"channel":"cnpa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Hugging Face","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T03:28:56.547Z","createdAt":"2026-01-13T03:28:56.547Z"},{"id":"q-1188","question":"CNPA stack: an HTTP API writes to PostgreSQL and publishes to Kafka. During peak load, duplicate events may be produced due to retries and at-least-once semantics. Describe a concrete end-to-end plan to enforce idempotent processing across HTTP, DB, and Kafka, including dedupe strategy, upsert/constraints, transactional writes, offset tracking, and how you’d validate correctness under load?","answer":"Implement end-to-end idempotency for the CNPA stack: HTTP API -> Postgres -> Kafka. Use a dedupe key (event_id) stored in Redis with TTL to guard against retries; enforce a unique constraint on Postgr","explanation":"## Why This Is Asked\nTests practical mastery of idempotent processing across a CNPA stack under retry pressure, focusing on concrete mechanisms that preserve data integrity without sacrificing throughput.\n\n## Key Concepts\n- Idempotent processing across HTTP, DB, and Kafka\n- Redis-based deduplication strategies (TTL, set membership, or Bloom filters)\n- Postgres upsert with unique event_id constraint\n- Kafka producer idempotence and transactional writes\n- Outbox pattern and offset tracking for exactly-once semantics\n- Observability: duplicate rate, latency impact, and throughput\n\n## Code Example\n```javascript\n// Pseudo: check Redis for seen event_id, if not, write to Postgres upsert and publish to Kafka within a transaction\n```\n\n## Follow-up Questions\n- How would you choose TTLs for dedupe keys and handle hot keys?\n- What are trade-offs of Bloom filter vs Redis sets, and how would you monitor false positives?\n- How would you test end-to-end correctness under burst traffic and partial outages?","diagram":"flowchart TD\n  A[HTTP API receives event] --> B{Event dedup check}\n  B -- exists --> C[Return 200 OK]\n  B -- new --> D[Postgres upsert]\n  D --> E[Publish to Kafka with idempotent producer]\n  E --> F[Kafka consumers process and update read model]\n  F --> G[Outbox/offset store updated]","difficulty":"intermediate","tags":["cnpa"],"channel":"cnpa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","PayPal","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T04:40:32.196Z","createdAt":"2026-01-13T04:40:32.196Z"},{"id":"q-1283","question":"CNPA stack with HTTP API, PostgreSQL, Kafka, and Redis: a Redis-based rate limiter fronting the API causes legitimate bursts to be 429-throttled during a promo, despite normal traffic. Provide a concrete debugging plan to isolate whether latency or errors come from Redis Lua script, Redis network, the HTTP handler, or downstream services, with exact metrics and concrete fixes and how you’d validate impact?","answer":"End-to-end tracing with per-layer metrics: HTTP latency, Redis INFO/LATENCY, Lua profiler, Postgres and Kafka timings. Replay a controlled burst to compare Redis vs HTTP. If Lua script is slow, refact","explanation":"## Why This Is Asked\n\nTests practical debugging of a common CNPA choke point under burst traffic, focusing on observability, instrumenting Lua, and safe fallbacks.\n\n## Key Concepts\n\n- End-to-end tracing\n- Lua script optimization\n- Redis connections and pipelining\n- Backpressure and fallbacks\n\n## Code Example\n\n```javascript\n// pseudo-snippet: sample Lua limiter optimization\nreturn redis.call('EVAL', 'return 1', 0)\n```\n\n## Follow-up Questions\n\n- How would you measure the impact of a persistent 429 throttle on business metrics?\n- What are safer alternatives to Redis-based rate limiting under spikes?\n","diagram":null,"difficulty":"beginner","tags":["cnpa"],"channel":"cnpa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T08:30:49.319Z","createdAt":"2026-01-13T08:30:49.319Z"},{"id":"q-845","question":"In a MongoDB-backed service, read latency tails spike during peak hours. Provide a concrete, practical debugging plan to determine whether the bottleneck is network, driver, query plan, or index design. Include exact steps, metrics to collect, and concrete changes (indexes, readConcern, pooling) you would apply, plus how you would verify impact?","answer":"Collect end-to-end latency, queue depth, and replica lag; enable slow query logs and run explain on top slow queries; verify index coverage with compound indexes; apply index hints or add a composite ","explanation":"## Why This Is Asked\nThis question probes practical debugging of latency in a MongoDB-backed service with real-world constraints.\n\n## Key Concepts\n- Tail latency diagnosis: slow queries, network, driver pool\n- Explain plans and index design: compound indexes, prefix rules\n- Replica lag and read concerns\n- Driver tuning and observability\n\n## Code Example\n```js\ndb.collection(`orders`).find({ userId: id }).explain(`executionStats`);\n```\n\n## Follow-up Questions\n- How would you validate the impact of an index change in production?\n- What metrics would you alert on for sustained tail latency?","diagram":null,"difficulty":"intermediate","tags":["cnpa"],"channel":"cnpa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Lyft","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:28:43.195Z","createdAt":"2026-01-12T13:28:43.195Z"},{"id":"q-873","question":"A CNPA service receives events over HTTP, writes to PostgreSQL, and publishes to Kafka. During peak hours, read latency spikes. Describe a concrete debugging plan to determine whether the bottleneck is the HTTP handler, the DB query, or the Kafka producer, including exact steps, metrics to collect, and concrete changes (indexes, pooling, prepared statements, idempotent producer) and how you would verify impact?","answer":"Instrument with OpenTelemetry to trace HTTP handler, DB queries, and Kafka publish; collect p95/p99 latency, error rate, and queue backpressure. Reproduce under load (k6). If HTTP slow, tune keep-aliv","explanation":"## Why This Is Asked\nTests practical debugging across a real CNPA pipeline and encourages measurable fixes.\n\n## Key Concepts\n- End-to-end tracing with OpenTelemetry\n- Performance attribution across HTTP, DB, and messaging\n- Idempotent producers and pool tuning\n- Load testing and metrics-driven verification\n\n## Code Example\n```javascript\n// OpenTelemetry tracing skeleton for CNPA flow\nconst { trace } = require('@opentelemetry/api');\nconst tracer = trace.getTracer('cnpa-trace');\nasync function handleEvent(req, res) {\n  await tracer.startActiveSpan('handle_event', async (span) => {\n    await processHttp();\n    await writeDb();\n    await publishKafka();\n    span.end();\n  });\n}\n```\n\n## Follow-up Questions\n- How would you measure impact of a pool size increase?\n- What changes would you apply to avoid future tail latency spikes?","diagram":"flowchart TD\n  A[HTTP Request] --> B[HTTP Handler]\n  B --> C[PostgreSQL]\n  B --> D[Kafka Producer]\n  C --> E[Indexes/Pooling]\n  D --> F[Configs]\n  G[Metrics] --> H[Tail Latency]","difficulty":"beginner","tags":["cnpa"],"channel":"cnpa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Instacart","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:55:00.870Z","createdAt":"2026-01-12T13:55:00.870Z"},{"id":"q-903","question":"In a CNPA stack, an HTTP API writes to Postgres, publishes to Kafka, and a Redis-backed dashboard consumes the stream. During peak load, HTTP latency tails spike. Provide a concrete debugging plan to isolate whether the bottleneck is HTTP, DB, Kafka, producer, consumer, or Redis, with exact metrics, sampling, and concrete changes (pooling, prepared statements, acks, batch sizes, caching strategies) and how you would verify impact?","answer":"Use end-to-end tracing to map latency per hop: HTTP handler, DB query (EXPLAIN ANALYZE), Kafka publish, consumer processing, Redis read. Gather p95/p99 latencies, throughput, queue depths, and GC/thre","explanation":"## Why This Is Asked\n\nTests practical debugging across CNPA stack tail latency.\n\n## Key Concepts\n\n- Distributed tracing\n- Postgres tuning\n- Kafka producer/consumer\n- Redis caching\n\n## Code Example\n\n```bash\n# Example commands to reproduce latency\nEXPLAIN ANALYZE SELECT ...;\n```\n\n## Follow-up Questions\n\n- How would you quantify improvement after each change?\n- Which metric thresholds signal success or regression?","diagram":null,"difficulty":"intermediate","tags":["cnpa"],"channel":"cnpa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Meta","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T14:41:59.785Z","createdAt":"2026-01-12T14:41:59.785Z"},{"id":"q-928","question":"In a CNPA stack consisting of an HTTP API, PostgreSQL, Kafka, and Redis, latency tails spike under peak load. Provide a concrete, beginner-friendly plan to enable end-to-end tracing with OpenTelemetry to pinpoint the bottleneck. Include trace propagation, spans for HTTP handler, DB query, Kafka publish/consume, Redis access, and verification steps with a sample end-to-end trace?","answer":"Instrument OpenTelemetry across all components and propagate traceparent. Create spans for HTTP handler, each DB query, Kafka producer/consumer, and Redis access. Route to a collector (Jaeger/Tempo), ","explanation":"## Why This Is Asked\nUnderstand practical observability across CNPA stack with hands-on tracing.\n\n## Key Concepts\n- OpenTelemetry, trace propagation, end-to-end latency\n- Span creation in HTTP, DB, Kafka, Redis\n- Collector backends (Jaeger/Tempo) and dashboards\n\n## Code Example\n```javascript\n// sample: initialize tracer and extract traceparent from incoming HTTP header\nconst { diag, trace } = require('@opentelemetry/api');\n// setup omitted for brevity\n```\n\n## Follow-up Questions\n- How would sampling change under high throughput?\n- How would you adapt if a component uses a different broker or cache?\n","diagram":"flowchart TD\n  HTTP[HTTP API] --> DB[(PostgreSQL)]\n  HTTP --> Kafka[(Kafka Producer)]\n  Kafka --> Broker[(Kafka Broker)]\n  Broker --> Consumer[(Kafka Consumer)]\n  Consumer --> Redis[(Redis)]","difficulty":"beginner","tags":["cnpa"],"channel":"cnpa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Hashicorp","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T15:39:06.442Z","createdAt":"2026-01-12T15:39:06.442Z"},{"id":"q-957","question":"CNPA stack with HTTP API, PostgreSQL, Kafka, and Redis dashboards requires a schema evolution: add a new optional field region_id to event records without downtime or breaking producers/consumers. Describe a practical, step-by-step migration plan: DB changes, schema registry versioning, producer/consumer updates, data backfill, testing, and rollback strategies, ensuring end-to-end consistency?","answer":"Zero-downtime: add nullable region_id to Postgres; evolve Avro/schema to optional region_id; publish new schema version via Confluent Schema Registry; deploy producers/consumers with dual-write mode a","explanation":"## Why This Is Asked\nTests schema evolution in CNPA and cross-service coordination with Kafka/Redis.\n\n## Key Concepts\n- Backward/forward compatibility\n- Schema Registry / Avro\n- Online backfill\n- Zero-downtime migrations\n- Canary rollout\n\n## Code Example\n```sql\nALTER TABLE events ADD COLUMN region_id VARCHAR(32) NULL;\n```\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nspec:\ntemplate:\nspec:\n  containers:\n    - name: api\n      image: myrepo/api:region-migrate\n      env:\n        - name: REGION_MIGRATION\n          value: true\n```\n\n## Follow-up Questions\n- If region_id becomes not-null, how to migrate without downtime?\n- How to validate backfill performance on large datasets?","diagram":"flowchart TD\nA[HTTP API] --> B[Postgres]\nB --> C[Kafka Producer]\nC --> D[Topics]\nD --> E[Redis Dashboards]","difficulty":"intermediate","tags":["cnpa"],"channel":"cnpa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["PayPal","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T16:43:17.765Z","createdAt":"2026-01-12T16:43:17.765Z"}],"subChannels":["general"],"companies":["Amazon","Anthropic","Bloomberg","Databricks","Discord","DoorDash","Hashicorp","Hugging Face","Instacart","Lyft","Meta","MongoDB","NVIDIA","Netflix","PayPal","Plaid","Salesforce","Scale Ai","Snap","Tesla","Twitter"],"stats":{"total":10,"beginner":3,"intermediate":4,"advanced":3,"newThisWeek":10}}