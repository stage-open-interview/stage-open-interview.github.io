{"questions":[{"id":"q-1051","question":"CNPA stack: HTTP API writes to PostgreSQL and emits Kafka events. A hot, large users table needs a non-blocking schema change (e.g., adding a new NOT NULL column with default). Propose a production-grade online migration plan that minimizes downtime, keeps Kafka in sync, handles backfill, and describes rollback and validation steps?","answer":"Use online schema migration with a shadow table and dual writes: create users_new with migrated schema, keep users_old active, route reads via a feature flag to users_all view, backfill users_new in c","explanation":"## Why This Is Asked\nTests ability to design zero-downtime migrations in a CNPA stack, ensuring data correctness across Postgres and Kafka while validating with incremental rollout.\n\n## Key Concepts\n- Online migrations\n- Shadow tables\n- Dual writes\n- Atomic view swap\n- Chunked backfill\n- Rollback and validation\n\n## Code Example\n```sql\n-- Shadow table creation\nCREATE TABLE users_new (... migrated schema ...);\n-- Backfill in chunks\nINSERT INTO users_new (...) SELECT ... FROM users_old WHERE ...;\n-- Route reads via view\nCREATE OR REPLACE VIEW users_all AS SELECT * FROM users_new;\n```\n\n## Follow-up Questions\n- How would you validate data parity during backfill?\n- How would you monitor migration latency and rollback readiness?","diagram":null,"difficulty":"advanced","tags":["cnpa"],"channel":"cnpa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Instacart","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T20:36:16.305Z","createdAt":"2026-01-12T20:36:16.305Z"},{"id":"q-1150","question":"In a CNPA stack: HTTP API ingests events, writes to PostgreSQL, and publishes to Kafka; during spikes, retries duplicate processed events. Propose a concrete, end-to-end plan to guarantee idempotent processing and prevent duplicates under retry storms. Include idempotency key strategy, dedup enforcement point, Kafka/DB coordination, and validation?","answer":"Use an event-id per submission and make event_id unique in Postgres; perform INSERT into the events table with ON CONFLICT DO NOTHING so retries don't duplicate writes. Use a transactional outbox to a","explanation":"## Why This Is Asked\n\nTests ability to reason about retries, deduplication, and cross-service guarantees in CNPA pipelines.\n\n## Key Concepts\n\n- Idempotent processing\n- Outbox pattern\n- Postgres unique constraints\n- Kafka transactional producer\n\n## Code Example\n\n```javascript\n// Implementation example (pseudo)\n```\n\n## Follow-up Questions\n\n- How would you test edge cases like partial failures?\n- What about backpressure during spikes?","diagram":null,"difficulty":"advanced","tags":["cnpa"],"channel":"cnpa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Netflix","Snap","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T01:34:44.738Z","createdAt":"2026-01-13T01:34:44.738Z"},{"id":"q-1157","question":"CNPA pipeline uses an HTTP API -> PostgreSQL -> Kafka with Avro schemas in a Schema Registry. A new optional field is added to the events, and some consumers crash when they see older versions. Provide a concrete, zero-downtime plan for schema evolution, including compatibility mode, rollout strategy, topic/consumer changes, backfill approach, rollback, and validation?","answer":"Upgrade Avro schema with an optional field default, set Schema Registry compatibility to BACKWARD, and roll out via a canary topic. Publish new schema alongside the old one, route 5–10% traffic to the","explanation":"## Why This Is Asked\n\nTests real-world schema evolution discipline in CNPA pipelines, stressing zero-downtime migrations, compatibility, and rollback.\n\n## Key Concepts\n\n- Avro schema evolution and Schema Registry compatibility BACKWARD/FORWARD/FULL\n- Canary deployments and topic-level migrations\n- Backfill strategies and validation across HTTP, DB, and Kafka\n- Rollback paths and deserialization error handling\n- Feature-flag-driven routing and observability\n\n## Code Example\n\n```javascript\nconst newSchema = {\n  type: \"record\",\n  name: \"Event\",\n  fields: [\n    {name: \"id\", type: \"string\"},\n    {name: \"payload\", type: \"string\"},\n    {name: \"newField\", type: [\"null\", \"string\"], default: null}\n  ]\n}\n```\n\n## Follow-up Questions\n\n- How would you test compatibility in CI/CD?\n- How do you handle misbehaving consumers during rollout?\n- What metrics indicate a successful migration and what are alert thresholds?\n- How would you decommission the old schema gracefully after validation?","diagram":"flowchart TD\nA[HTTP API] --> B[Schema Registry v2]\nB --> C[New Kafka topic canary]\nC --> D[Canary consumer validation]\nD --> E[Full rollout]\nE --> F[Deprecate old schema]","difficulty":"advanced","tags":["cnpa"],"channel":"cnpa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Hugging Face","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T03:28:56.547Z","createdAt":"2026-01-13T03:28:56.547Z"},{"id":"q-1188","question":"CNPA stack: an HTTP API writes to PostgreSQL and publishes to Kafka. During peak load, duplicate events may be produced due to retries and at-least-once semantics. Describe a concrete end-to-end plan to enforce idempotent processing across HTTP, DB, and Kafka, including dedupe strategy, upsert/constraints, transactional writes, offset tracking, and how you’d validate correctness under load?","answer":"Implement end-to-end idempotency for the CNPA stack: HTTP API -> Postgres -> Kafka. Use a dedupe key (event_id) stored in Redis with TTL to guard against retries; enforce a unique constraint on Postgr","explanation":"## Why This Is Asked\nTests practical mastery of idempotent processing across a CNPA stack under retry pressure, focusing on concrete mechanisms that preserve data integrity without sacrificing throughput.\n\n## Key Concepts\n- Idempotent processing across HTTP, DB, and Kafka\n- Redis-based deduplication strategies (TTL, set membership, or Bloom filters)\n- Postgres upsert with unique event_id constraint\n- Kafka producer idempotence and transactional writes\n- Outbox pattern and offset tracking for exactly-once semantics\n- Observability: duplicate rate, latency impact, and throughput\n\n## Code Example\n```javascript\n// Pseudo: check Redis for seen event_id, if not, write to Postgres upsert and publish to Kafka within a transaction\n```\n\n## Follow-up Questions\n- How would you choose TTLs for dedupe keys and handle hot keys?\n- What are trade-offs of Bloom filter vs Redis sets, and how would you monitor false positives?\n- How would you test end-to-end correctness under burst traffic and partial outages?","diagram":"flowchart TD\n  A[HTTP API receives event] --> B{Event dedup check}\n  B -- exists --> C[Return 200 OK]\n  B -- new --> D[Postgres upsert]\n  D --> E[Publish to Kafka with idempotent producer]\n  E --> F[Kafka consumers process and update read model]\n  F --> G[Outbox/offset store updated]","difficulty":"intermediate","tags":["cnpa"],"channel":"cnpa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","PayPal","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T04:40:32.196Z","createdAt":"2026-01-13T04:40:32.196Z"},{"id":"q-1283","question":"CNPA stack with HTTP API, PostgreSQL, Kafka, and Redis: a Redis-based rate limiter fronting the API causes legitimate bursts to be 429-throttled during a promo, despite normal traffic. Provide a concrete debugging plan to isolate whether latency or errors come from Redis Lua script, Redis network, the HTTP handler, or downstream services, with exact metrics and concrete fixes and how you’d validate impact?","answer":"End-to-end tracing with per-layer metrics: HTTP latency, Redis INFO/LATENCY, Lua profiler, Postgres and Kafka timings. Replay a controlled burst to compare Redis vs HTTP. If Lua script is slow, refact","explanation":"## Why This Is Asked\n\nTests practical debugging of a common CNPA choke point under burst traffic, focusing on observability, instrumenting Lua, and safe fallbacks.\n\n## Key Concepts\n\n- End-to-end tracing\n- Lua script optimization\n- Redis connections and pipelining\n- Backpressure and fallbacks\n\n## Code Example\n\n```javascript\n// pseudo-snippet: sample Lua limiter optimization\nreturn redis.call('EVAL', 'return 1', 0)\n```\n\n## Follow-up Questions\n\n- How would you measure the impact of a persistent 429 throttle on business metrics?\n- What are safer alternatives to Redis-based rate limiting under spikes?\n","diagram":null,"difficulty":"beginner","tags":["cnpa"],"channel":"cnpa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T08:30:49.319Z","createdAt":"2026-01-13T08:30:49.319Z"},{"id":"q-1383","question":"CNPA stack with HTTP API writing to PostgreSQL, publishing to Kafka, and Elasticsearch dashboards. A nightly backfill misses events, causing dashboards to report incorrect counts. Describe a concrete debugging plan to isolate whether loss occurs in HTTP write/transaction, Postgres-to-Kafka CDC, or Kafka-to-Elasticsearch sink, with exact metrics, sampling, and concrete fixes (idempotent sinks, transactional writes, producer retries, dedup IDs) and how you would verify end-to-end?","answer":"Collect: HTTP commit rate, PG replication lag, Kafka consumer lag, sink error counts, and ES refresh lag. Steps: enable end-to-end hashes per event_id, implement idempotent sink (UPSERT in ES, dedup i","explanation":"## Why This Is Asked\n\nTests ability to diagnose cross-system data integrity issues in a CNPA ETL path and design robust fixes.\n\n## Key Concepts\n\n- End-to-end data integrity across HTTP, Postgres, Kafka, and Elasticsearch\n- Exactly-once vs at-least-once guarantees and idempotent sinks\n- CDC latency, replication lag, and sink backpressure\n\n## Code Example\n\n```javascript\n// Pseudo idempotent sink for Elasticsearch\nasync function upsertElasticsearch(event) {\n  await es.update({\n    index: 'events',\n    id: event.id,\n    doc: event,\n    doc_as_upsert: true\n  });\n}\n```\n\n## Follow-up Questions\n\n- How would you detect drift between DB and ES in production?\n- How would you adjust backfill replay to avoid double-counting?","diagram":null,"difficulty":"intermediate","tags":["cnpa"],"channel":"cnpa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T14:45:54.789Z","createdAt":"2026-01-13T14:45:54.789Z"},{"id":"q-1401","question":"CNPA stack: HTTP API writes events to PostgreSQL, publishes to Kafka with event_time metadata, and a downstream analytics service reads from Kafka to produce 5-minute windowed counts. After a release, a dashboard shows both late counts and misaligned windows. Provide a concrete debugging plan to determine if the issue is event-time timestamps, Kafka timestamps, consumer windowing, or clock skew across services, including exact metrics, sampling, and fixes (re-timestamp, watermarking, idempotent sinks) and how you would validate end-to-end correctness?","answer":"Pinpoint drift between event-time and processing-time windows. Check: 1) producer event_time vs stored event_time in Postgres; 2) Kafka record timestamps and any broker clock drift; 3) consumer window","explanation":"## Why This Is Asked\n\nTests ability to debug time-sensitive streaming pipelines, differentiating event-time vs processing-time issues, watermarking, and clock skew across services. It also probes discipline in instrumentation and validation.\n\n## Key Concepts\n\n- Event-time vs processing-time\n- Watermarks and windowing\n- Kafka timestamps and broker clocks\n- Idempotent sinks and reprocessing\n- Backfill validation\n\n## Code Example\n\n```javascript\n// Producer emits with event_time for downstream correctness\nproducer.send({ value: JSON.stringify({ event_time }), timestamp: event_time })\n```\n\n## Follow-up Questions\n\n- How would you validate a backfill across multiple partitions?\n- What metrics would you surface in dashboards to prevent regressions?","diagram":null,"difficulty":"intermediate","tags":["cnpa"],"channel":"cnpa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Apple"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T15:43:07.920Z","createdAt":"2026-01-13T15:43:07.920Z"},{"id":"q-1440","question":"In a CNPA stack with an HTTP API writing to PostgreSQL and publishing to Kafka, add an optional field customer_segment; design a zero-downtime schema evolution and payload versioning plan, including DB changes, Kafka message formats, consumer upgrades, backfill, and validation?","answer":"Plan: add a nullable column customer_segment to Postgres; emit versioned payloads starting with version 2 that include the field; implement a short dual-write period to upgrade producers/consumers; ba","explanation":"## Why This Is Asked\nTests practical zero-downtime schema evolution and payload versioning in CNPA.\n\n## Key Concepts\n- Zero-downtime schema change\n- Backward/forward compatibility\n- Dual-write window\n- Versioned payload\n- Backfill strategy\n\n## Code Example\n```sql\nALTER TABLE events ADD COLUMN customer_segment TEXT;\n```\n\n```json\n{ \"version\": 2, \"customer_segment\": \"enterprise\" }\n```\n\n## Follow-up Questions\n- What metric would you monitor during the rollout?\n- How would you handle a rollback if a consumer fails to parse v2 payload?","diagram":null,"difficulty":"beginner","tags":["cnpa"],"channel":"cnpa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","NVIDIA","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T17:01:21.874Z","createdAt":"2026-01-13T17:01:21.874Z"},{"id":"q-1586","question":"In a CNPA stack with an HTTP API writing to PostgreSQL, publishing to Kafka, and Elasticsearch/Redis downstream, a burst causes duplicate rows in Postgres and delayed dashboard freshness. Propose a concrete end-to-end exactly-once plan: transactional Kafka producers, Postgres outbox, idempotent Elasticsearch sinks, Redis invalidation, and verification steps, plus rollback if needed?","answer":"Implement end-to-end exactly-once processing through a coordinated approach: wrap database writes and Kafka publishing in atomic transactions using Kafka's transactional producers with exactly-once semantics. Deploy the Postgres outbox pattern to store events alongside business data, with a separate consumer process handling Kafka publishing. Configure Elasticsearch sinks with deterministic document IDs for idempotent writes, and implement Redis cache invalidation using the same event identifiers. Establish verification through end-to-end tracing and duplicate detection mechanisms. Prepare rollback capabilities by maintaining transaction logs and implementing compensating transactions.","explanation":"## Why This Is Asked\nTests ability to architect end-to-end data consistency in CNPA stacks handling high throughput across multiple downstream systems.\n\n## Key Concepts\n- Exactly-once processing with Kafka transactions\n- Outbox pattern for atomic database operations\n- Idempotent sink configurations\n- Coordinated cache invalidation strategies\n- End-to-end verification and rollback procedures\n\n## Code Example\n```javascript\n// Pseudo: transactional write+publish pattern\n```\n\n## Follow-up Questions\n- How would you monitor event ordering across the pipeline?\n- What regression tests would validate exactly-once semantics?","diagram":"flowchart TD\n  HTTP_API[HTTP API] --> OUTBOX[Postgres Outbox]\n  OUTBOX --> KAFKA_TOPIC[Kafka Topic: events]\n  KAFKA_TOPIC --> ES_SINK[Elasticsearch Sink]\n  ES_SINK --> Redis[Redis Cache Invalidation]","difficulty":"advanced","tags":["cnpa"],"channel":"cnpa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","IBM","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T05:36:20.802Z","createdAt":"2026-01-13T22:52:41.118Z"},{"id":"q-1630","question":"In a CNPA stack with an HTTP API, PostgreSQL, Kafka, and Redis, dashboards display stale data for a cohort after a deployment; propose a concrete debugging plan to determine whether drift originates from writes to Postgres, the Kafka sink, or Redis caching, including exact metrics to collect, sampling strategy, and concrete fixes (transactional outbox, idempotent sinks, Redis invalidation, cache-warming) and how you would verify impact?","answer":"Propose a targeted approach: attach per-tenant event timestamps; compare Postgres commit times, Kafka offsets, and Redis cache timestamps to detect drift; enable transactional outbox with idempotent s","explanation":"## Why This Is Asked\nTests ability to diagnose cross-system data drift in a CNPA stack, covering Postgres, Kafka, and Redis, plus practical fixes and verification.\n\n## Key Concepts\n- Data drift across CNPA components\n- Transactional outbox pattern\n- Idempotent sinks\n- Redis invalidation and cache-warming\n- Replay-based validation\n\n## Code Example\n```javascript\n// Pseudo replay worker skeleton\nasync function replayOutbox(outboxBatch) {\n  for (const evt of outboxBatch) {\n    await kafka.send({ topic: evt.topic, key: evt.key, value: evt.value, headers: { replay: true } })\n  }\n}\n```\n\n## Follow-up Questions\n- How would you determine drift thresholds and set alerts?\n- How would you scale this approach to multiple tenants during replay and testing?","diagram":null,"difficulty":"intermediate","tags":["cnpa"],"channel":"cnpa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["NVIDIA","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T04:17:40.883Z","createdAt":"2026-01-14T04:17:40.883Z"},{"id":"q-1716","question":"CNPA stack: an HTTP API writes to PostgreSQL and publishes to Kafka. A schema evolution adds a new optional field to the event payload stored in Postgres and emitted to Kafka. Propose a concrete migration plan that preserves compatibility, uses a versioned envelope, coordinates writes and reads, validates with end-to-end tests, and provides a safe rollback. Include concrete steps, metrics, and rollback strategy?","answer":"Implement a versioned envelope for events. Step 1: add a new optional field in a v2 payload and publish to a bridging topic while continuing v1 writes. Step 2: extend consumers to accept both versions","explanation":"## Why This Is Asked\nThis question tests coordinating schema changes across DB and streaming systems while preserving compatibility and uptime.\n\n## Key Concepts\n- Schema evolution, envelope versioning, bridging topics\n- Coordinated rollout, backward/forward compatibility, backfill\n- Observability: lag, errors, schema-compat metrics\n\n## Code Example\n```javascript\n// Envelope types\n\ntype EventV1 = { id: string; payload: any; version: 1 };\ntype EventV2 = { id: string; payload: any; version: 2; newField?: string };\n```\n\n## Follow-up Questions\n- How would you test compatibility in CI/CD?\n- How would you rollback if issues arise?","diagram":null,"difficulty":"advanced","tags":["cnpa"],"channel":"cnpa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T07:50:42.667Z","createdAt":"2026-01-14T07:50:42.667Z"},{"id":"q-1732","question":"CNPA stack: HTTP API writes to PostgreSQL, emits to Kafka, and a Redis-backed read cache. A schema change adds a new optional field to the event payload; rollout under peak load leads to some consumers crashing due to compatibility. Provide a concrete, practical rollout and debugging plan to ensure no data loss or outages, including steps, metrics, and concrete changes (schema registry, backward/forward compatibility tests, dual-write, feature flags, cache invalidation) and how you would verify success?","answer":"Use a registry-based schema evolution (backward/forward compatible) for the new optional field, enable dual-write and gate behind a feature flag, then rollout via canary. Monitor producer/consumer lag","explanation":"## Why This Is Asked\n\nTests ability to safely evolve schemas in CNPA stacks with Kafka and Redis caches, avoiding outages.\n\n## Key Concepts\n\n- Schema registry and compatibility strategies\n- Backward/forward compatibility testing\n- Dual-write and idempotent consumers\n- Feature flags and canary deployments\n- Data replay and cache invalidation\n\n## Code Example\n\n```javascript\n// Example: gate new field behind feature flag and default handling\nif (featureEnabled('newPayloadField')) {\n  event.newField = computeValue(...);\n}\nproducer.send({ value: event, key: id });\n```\n\n## Follow-up Questions\n\n- How would you backfill events missing the new field during rollout?\n- What metrics would signal a successful canary and full rollout?","diagram":null,"difficulty":"intermediate","tags":["cnpa"],"channel":"cnpa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Coinbase","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T08:50:29.115Z","createdAt":"2026-01-14T08:50:29.116Z"},{"id":"q-1751","question":"CNPA stack: HTTP API → PostgreSQL → Kafka → stream processor → Redis dashboards. A new audit requires end-to-end latency visibility for late events during windows; latency spikes 2–3 minutes. Provide a concrete debugging plan to pinpoint whether the delay lies in HTTP ingress, DB writes, Kafka publish, stream windowing, or Redis caching. Include exact metrics, sampling, and fixes (idempotent sinks, transactional outbox, watermark tuning, checkpointing) and how you’d verify impact?","answer":"Describe how you would instrument end-to-end tracing with a correlation_id, propagate it across HTTP, DB write, Kafka publish, and sink, using OpenTelemetry. Collect per-hop latency histograms and tai","explanation":"## Why This Is Asked\nThis question probes practical end-to-end debugging in a CNPA pipeline with observability, sinks, and windowing.\n\n## Key Concepts\n- End-to-end tracing across HTTP, PostgreSQL, Kafka, stream processor, Redis\n- Correlation IDs, OpenTelemetry, sampling\n- Idempotent sinks, transactional outbox, watermarking, checkpointing\n\n## Code Example\n```javascript\n// Propagate correlation ID in Express middleware\napp.use((req,res,next)=>{\n  const id = req.headers['x-correlation-id'] || uuid.v4();\n  req.corrId = id;\n  res.setHeader('x-correlation-id', id);\n  next();\n});\n```\n\n## Follow-up Questions\n- How would you validate late-arriving data handling in watermarking?\n- What are the tradeoffs between at-least-once and exactly-once sinks in this stack?","diagram":"flowchart TD\n  A[HTTP] --> B[PostgreSQL]\n  B --> C[Kafka]\n  C --> D[Stream]\n  D --> E[Redis]","difficulty":"intermediate","tags":["cnpa"],"channel":"cnpa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Instacart","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T09:39:38.785Z","createdAt":"2026-01-14T09:39:38.786Z"},{"id":"q-1794","question":"In a CNPA stack—HTTP API, PostgreSQL, Kafka, Redis—latency spikes appear on a new endpoint that touches all components. Outline a practical plan to implement end-to-end tracing with a correlation_id: where to instrument, which metrics to collect (end-to-end latency, per-service latency, queue time, DB time, cache misses), and concrete changes (propagate correlation_id in HTTP, persist it in DB, attach it to Kafka messages, emit trace spans). How would you validate in staging before production?","answer":"Implement a single correlation_id header across HTTP, DB writes, Kafka messages, and Redis ops; emit distributed traces for HTTP handler, DB query, Redis access, and Kafka producer/consumer. Track end","explanation":"## Why This Is Asked\nThis question probes practical end-to-end tracing skills at CNPA scale, focusing on observable instrumentation and real-world validation.\n\n## Key Concepts\n- End-to-end tracing and correlation IDs\n- OpenTelemetry instrumentation\n- Latency breakdown across HTTP, DB, Kafka, Redis\n\n## Code Example\n```javascript\n// Propagate correlation_id across calls\nconst corrId = req.headers['Correlation-Id'] || generateId();\nres.setHeader('Correlation-Id', corrId);\nawait db.query('INSERT INTO events (...) VALUES (...)', [/* params */, corrId]);\nproducer.send({ topic: 'events', messages: [{ value: payload, headers: { 'Correlation-Id': corrId } }] });\n```\n\n## Follow-up Questions\n- How would you handle high cardinality correlation IDs?\n- How would you sample traces to limit overhead?","diagram":"flowchart TD\n  HTTP[HTTP API] --> DB[(PostgreSQL)]\n  HTTP --> Kafka[Kafka]\n  HTTP --> Redis[(Redis)]\n  Kafka --> Consumer[(Consumer)]\n  Correlation[Correlation ID] --> HTTP\n  Correlation --> DB\n  Correlation --> Redis\n  Correlation --> Kafka\n  Correlation --> Consumer","difficulty":"beginner","tags":["cnpa"],"channel":"cnpa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Google","Hugging Face"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T10:57:30.675Z","createdAt":"2026-01-14T10:57:30.675Z"},{"id":"q-1851","question":"In a CNPA stack: HTTP API ingests events, writes to PostgreSQL, and publishes to Kafka. A new enrichment step guarded by a feature flag calls a 3rd-party service. Design a concrete, end-to-end rollout plan that minimizes risk: per-tenant flag rollout, fallback behavior when the service is down, tracing with correlation IDs, circuit breaker, and backpressure; metrics to monitor (p50/p95 latency, error rate, backlog), rollback criteria, and validation steps?","answer":"Roll out the enrichment step with a per-tenant feature flag. Make enrichment asynchronous with a bounded retry queue and a fallback that returns un-enriched events if the external service is down. Pro","explanation":"## Why This Is Asked\nTests understanding of safe feature rollouts and resilience when integrating external services.\n\n## Key Concepts\n- Feature flags and per-tenant rollout\n- Async enrichment and fallbacks\n- End-to-end tracing and correlation IDs\n- Circuit breaker and backpressure\n- Observability metrics and rollback criteria\n\n## Code Example\n```javascript\n// pseudo-code illustrating flag check and fallback\n```\n\n## Follow-up Questions\n- How would you handle partial failures across tenants during rollback?\n- What metrics and dashboards would you add to validate a successful rollout?","diagram":null,"difficulty":"beginner","tags":["cnpa"],"channel":"cnpa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T14:36:47.043Z","createdAt":"2026-01-14T14:36:47.043Z"},{"id":"q-1875","question":"CNPA stack: HTTP API writes to PostgreSQL and publishes to Kafka; downstream analytics reads from Kafka and loads into a data warehouse. During deployment, dashboards drift and latency tails widen. Create a concrete, end-to-end debugging plan to isolate whether the root cause is HTTP serialization, DB write latency, Kafka publish, consumer, or ETL/warehouse load, with exact metrics, sampling, and concrete fixes (outbox pattern, idempotent sinks, backpressure, staged deploy) and verification steps?","answer":"Instrument end-to-end latency with per-stage traces: HTTP handler, DB write, Kafka publish, consumer, and ETL/warehouse stages. Track p95/p99, backlog/lag, error rates. Enable transactional outbox for","explanation":"## Why This Is Asked\nThis angle probes multi-service tracing, backpressure, and data consistency across CNPA with a data warehouse. It requires concrete observability and deployment discipline.\n\n## Key Concepts\n- End-to-end tracing across HTTP, Postgres, Kafka, consumer, and ETL\n- Outbox and idempotent sinks to avoid duplicates\n- Backpressure and staged deploys to protect warehouse loads\n\n## Code Example\n```javascript\n// Pseudo-code for enabling transactional outbox and idempotent sink\n```\n\n## Follow-up Questions\n- How would you simulate a deployment-induced lag in a prod-like environment?\n- What metrics would you automate alerting on to catch drift early?","diagram":"flowchart TD\n  A[HTTP API] --> B[Postgres]\n  B --> C[Kafka]\n  C --> D[Consumer]\n  D --> E[ETL/Warehouse]\n","difficulty":"advanced","tags":["cnpa"],"channel":"cnpa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","Slack","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T15:35:57.515Z","createdAt":"2026-01-14T15:35:57.516Z"},{"id":"q-1956","question":"In a CNPA stack: HTTP API ingests events, writes to PostgreSQL, and publishes to Kafka. A new optional field customer_region must be added without downtime or data loss. Describe a concrete, end-to-end migration plan: schema changes, producer/consumer compatibility, backfill strategy, validation checks, and rollback. Include metrics and canary signals to verify success?","answer":"Use a zero-downtime migration: add a nullable customer_region column; deploy producer and consumer changes that tolerate null; perform a backfill to populate region from existing mappings; roll out in","explanation":"## Why This Is Asked\nThis tests practical migration planning across HTTP, DB, and streaming components with zero-downtime guarantees.\n\n## Key Concepts\n- Backward/forward compatibility\n- Schema migrations with zero downtime\n- Backfill and validation\n- Canary deployments and rollback\n\n## Code Example\n```javascript\n// conceptual SQL string in JS (for tooling usage)\nconst addColumnSQL = `ALTER TABLE events ADD COLUMN IF NOT EXISTS customer_region VARCHAR(50);`;\n```\n\n## Follow-up Questions\n- How would you validate data integrity post-migration?\n- How would you monitor drift between producer schema and consumer expectations?","diagram":null,"difficulty":"beginner","tags":["cnpa"],"channel":"cnpa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Oracle","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T18:52:52.990Z","createdAt":"2026-01-14T18:52:52.990Z"},{"id":"q-2041","question":"In a CNPA stack (HTTP API -> Postgres -> Kafka -> Redis) you notice tail latency spikes during peak hours. Outline a concrete, beginner-friendly plan to diagnose end-to-end latency using distributed tracing. Include what to instrument, where to insert spans (HTTP handler, DB query, Kafka producer/consumer, Redis ops), how to propagate trace context, minimal metrics, and a simple verification checklist to confirm fixes?","answer":"Implement OpenTelemetry across all services in the CNPA stack. Create a root span for each HTTP request and child spans for Postgres queries, Kafka producer/consumer operations, and Redis commands. Propagate trace context through HTTP headers and Kafka message headers to maintain end-to-end correlation. Configure basic latency metrics (p95/p99) and set up dashboards to visualize the complete request flow.","explanation":"## Why This Is Asked\nDemonstrates practical observability skills and a systematic approach to diagnosing end-to-end latency issues in distributed systems.\n\n## Key Concepts\n- **Distributed Tracing with OpenTelemetry**: Industry-standard for tracing across service boundaries\n- **Span Hierarchy**: HTTP handler → Postgres → Kafka → Redis operations\n- **Context Propagation**: Trace headers and Kafka message metadata maintain correlation\n- **Latency Metrics**: p95/p99 percentiles for tail latency analysis\n- **Visualization Dashboards**: End-to-end request flow monitoring\n\n## Implementation Plan\n1. **Instrumentation Points**: HTTP handlers, database queries, message producers/consumers, cache operations\n2. **Trace Context**: W3C Trace Context headers for HTTP, custom headers for Kafka messages\n3. **Span Creation**: Root spans for entry points, child spans for downstream operations\n4. **Metrics Collection**: Duration, error rates, and throughput per service component\n5. **Dashboard Setup**: Service mesh visualization and latency heatmaps\n\n## Verification Checklist\n- [ ] Trace spans appear for all service components\n- [ ] Context properly propagates across HTTP and Kafka boundaries\n- [ ] Latency metrics show p95/p99 trends during peak hours\n- [ ] Dashboards display complete end-to-end request flows\n- [ ] Alert thresholds configured for tail latency spikes","diagram":null,"difficulty":"beginner","tags":["cnpa"],"channel":"cnpa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","Lyft","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T06:11:49.052Z","createdAt":"2026-01-14T21:46:55.826Z"},{"id":"q-2351","question":"CNPA pipeline: HTTP API writes to PostgreSQL and publishes events to Kafka. A rollout requires strict per-user ordering across a high-throughput, multi-partition topic; out-of-order deliveries break dashboards. Describe a concrete plan to guarantee per-user ordering while preserving throughput: data-path changes (outbox with transactional writes, per-user partitioning), producer settings (acks=all, enable.idempotence, max.in.flight=1, batch/linger), idempotent sinks, testing (replay, verifications per user), and rollback. Include concrete metrics and verification steps?","answer":"Partition by user_id; use an outbox and publish via Kafka transactions with idempotence; max.in.flight=1; acks=all; ensure a single partition per user to preserve order; sinks must be idempotent; test","explanation":"## Why This Is Asked\nNew per-user ordering constraint across partitions requires understanding coupling points and strong guarantees.\n\n## Key Concepts\n- Per-user partitioning and outbox pattern\n- Kafka transactions, idempotence, max.in.flight\n- Idempotent sinks and strict ordering verification\n\n## Code Example\n```javascript\n// Pseudo: write to outbox and commit Kafka txn atomically\nawait db.transaction(async trx => {\n  await trx.into('outbox').insert({user_id, event, seq});\n  await kafkaProducer.beginTransaction();\n  await kafkaProducer.send({topic, messages:[{key: String(user_id), value: event}]});\n  await kafkaProducer.commitTransaction();\n});\n```\n\n## Follow-up Questions\n- How would you monitor and alert on per-user ordering violations?\n- How would you handle a hot user skew if a single user dominates partition leadership?","diagram":"flowchart TD\n A[HTTP API] --> B[Postgres Outbox]\n B --> C[Kafka Producer (Txn)]\n C --> D[Kafka Topic (partitioned by user_id)]\n D --> E[Consumer] --> F[Read Model]","difficulty":"advanced","tags":["cnpa"],"channel":"cnpa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Plaid","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T14:39:09.837Z","createdAt":"2026-01-15T14:39:09.837Z"},{"id":"q-2414","question":"In a CNPA stack where an HTTP API ingests JSON events, writes to PostgreSQL, and publishes to Kafka, a deployment adds a required field 'customerTier' to the payload. Older clients omit it. Outline a concrete, practical debugging plan to locate the failure point and implement a safe rollout using a backward-compatible schema, optional field, or a schema registry with compatibility settings, plus a minimal migration and a feature flag. Include exact steps, data checks, and how you would verify no data loss?","answer":"Add a short runtime payload-version log at HTTP intake, Postgres insert, and Kafka publish, then rollback to a backward-compatible change: make customerTier optional or use a schema registry with back","explanation":"## Why This Is Asked\nTests ability to handle schema evolution, CNPA integration points, and safe rollouts in a beginner-friendly context.\n\n## Key Concepts\n- Schema evolution\n- Backward compatibility\n- Schema registry\n- Feature flags\n- Observability\n\n## Code Example\n```javascript\n// Validate optional field with fallback\nconst payload = JSON.parse(line)\nconst customerTier = payload.customerTier ?? 'standard'\n```\n\n## Follow-up Questions\n- How would you test with mixed payloads at scale?\n- How would you revert a rollout if issues arise?","diagram":"flowchart TD\n  A[HTTP API] --> B[Postgres]\n  A --> C[Kafka]\n  B --> D[DB Write]\n  C --> E[Kafka Topic]\n  D --> F[DB Sink]\n  E --> F","difficulty":"beginner","tags":["cnpa"],"channel":"cnpa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","IBM"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T17:01:47.114Z","createdAt":"2026-01-15T17:01:47.114Z"},{"id":"q-2450","question":"CNPA stack: HTTP API writes to PostgreSQL and publishes to Kafka. A feature flag routes writes through a shadow sink and a shadow Kafka topic. When enabled, dashboards show delayed data and downstream consumers occasionally duplicate messages due to retries. Provide a concrete debugging plan to verify data correctness and latency impact, including idempotency, outbox, transactional boundaries, and rollback procedures; specify metrics, sampling, and verification steps?","answer":"Reproduce in staging with the feature flag on/off; enable full tracing across HTTP, DB, Kafka, and the shadow sink. Enforce correctness with a transactional outbox: commit writes to Postgres and the o","explanation":"## Why This Is Asked\nThis tests practical debugging under feature-flagged shadow paths, end-to-end data correctness, and rollback safety in CNPA at scale.\n\n## Key Concepts\n- End-to-end tracing across HTTP, Postgres, Kafka, and caches\n- Transactional outbox pattern and idempotent publishing\n- Shadow sinks, data drift detection, rollback procedures\n- Latency tail metrics and data reconciliation\n\n## Code Example\n```javascript\n// Example: basic transactional outbox write\nawait db.transaction(async (trx) => {\n  await trx.insert({table:'events', data})\n  await trx.insert({table:'outbox', payload: data, event_id})\n});\n// publish from outbox with idempotent key\n```\n\n## Follow-up Questions\n- How would you test exactly-once semantics under replay?\n- How would you monitor drift between Postgres and the shadow sink?","diagram":"flowchart TD\nA[HTTP API] --> B[Postgres Write]\nB --> C[Kafka Publish]\nC --> D[Shadow Sink]\nD --> E[Dashboard]","difficulty":"advanced","tags":["cnpa"],"channel":"cnpa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Netflix","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T18:54:21.219Z","createdAt":"2026-01-15T18:54:21.220Z"},{"id":"q-2489","question":"CNPA stack: HTTP API writes to PostgreSQL and publishes events to Kafka using Avro; a schema migration is requested while dashboards rely on Redis caches updated by a Kafka consumer. Outline a concrete, end-to-end rollout plan that guarantees backward/forward compatibility, zero downtime, and data correctness. Include schema registry strategy, compatibility modes, dual-write/outbox techniques, canary rollout, monitoring, and rollback criteria with concrete change sets?","answer":"Enable a schema registry (Avro) with backward and forward compatibility. Use an outbox/dual-write so Postgres and Kafka stay in sync during migration. Roll out in canaries, validate end-to-end traces ","explanation":"## Why This Is Asked\nThis question probes practical mastery of safe schema evolution, exactly-once semantics, and end-to-end observability in CNPA pipelines under live migrations.\n\n## Key Concepts\n- Schema evolution with registry and compatibility checks\n- Outbox/dual-write patterns to avoid data loss\n- Canary rollout and tenant isolation\n- End-to-end tracing across HTTP, DB, Kafka, Redis\n- Rollback procedures and idempotent consumer design\n\n## Code Example\n```yaml\n# example config for registry and compatibility\nschema_registry:\n  url: http://sr.local\ncompatibility: backward+forward\n```\n\n## Follow-up Questions\n- What metrics confirm a successful migration and how would you alert on regressions?\n- How would you handle multi-tenant isolation during canary rollout?","diagram":"flowchart TD\n  A[HTTP API] --> B[Postgres]\n  A --> C[Kafka]\n  C --> D[Redis Cache]\n  D --> E[Dashboard]\n  style A fill:#f9f,stroke:#333,stroke-width:1px\n  style E fill:#bbf,stroke:#333,stroke-width:1px","difficulty":"advanced","tags":["cnpa"],"channel":"cnpa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Anthropic","Databricks"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T19:49:58.427Z","createdAt":"2026-01-15T19:49:58.427Z"},{"id":"q-2539","question":"CNPA stack with an HTTP API, PostgreSQL, and Kafka: a new schema migration on a hot table runs during peak, causing write latency spikes and intermittent 5xx errors. Provide a concrete debugging plan to isolate whether the bottleneck is the HTTP handler, the migration's locks in PostgreSQL, or the Kafka sink, with exact SQLs, metrics, and concrete fixes (online migrations, lock avoidance, prepared statements, idempotent sinks) and a validation strategy?","answer":"Begin by capturing pg_locks and pg_stat_activity for the hot table and migration; perform wait analysis and lock mode verification; monitor HTTP path latency and Kafka sink backlog; implement online migration using a shadow table approach with dual-write strategy, utilize prepared statements for HTTP handlers, and deploy idempotent Kafka sinks with deduplication keys.","explanation":"## Why This Is Asked\nThis question evaluates practical, production-grade debugging of blocking migrations in CNPA stacks. It assesses observability skills, risk-aware fixes, and validation strategies.\n\n## Key Concepts\n- PostgreSQL lock monitoring (pg_locks, pg_stat_activity)\n- Online schema migrations and shadow-table strategies\n- Kafka sink backpressure and idempotent sinks\n- Observability: latency, error rate, backlog metrics\n\n## Code Example\n```sql\n-- Sample diagnostic SQLs (inline for interview):\nSELECT pid, relation, mode, granted FROM pg_locks l \nJOIN pg_stat_activity a ON l.pid=a.pid \nWHERE l.relation = 'hot_table'::regclass;\n```","diagram":"flowchart TD\n  A[HTTP API] --> B[PostgreSQL]\n  B --> C[Kafka Sink]\n  subgraph Migration\n    D[ALTER TABLE on hot_table]\n  end\n  D -->|holds lock| B","difficulty":"intermediate","tags":["cnpa"],"channel":"cnpa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Netflix","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T05:34:21.956Z","createdAt":"2026-01-15T21:48:24.148Z"},{"id":"q-845","question":"In a MongoDB-backed service, read latency tails spike during peak hours. Provide a concrete, practical debugging plan to determine whether the bottleneck is network, driver, query plan, or index design. Include exact steps, metrics to collect, and concrete changes (indexes, readConcern, pooling) you would apply, plus how you would verify impact?","answer":"Collect end-to-end latency, queue depth, and replica lag; enable slow query logs and run explain on top slow queries; verify index coverage with compound indexes; apply index hints or add a composite ","explanation":"## Why This Is Asked\nThis question probes practical debugging of latency in a MongoDB-backed service with real-world constraints.\n\n## Key Concepts\n- Tail latency diagnosis: slow queries, network, driver pool\n- Explain plans and index design: compound indexes, prefix rules\n- Replica lag and read concerns\n- Driver tuning and observability\n\n## Code Example\n```js\ndb.collection(`orders`).find({ userId: id }).explain(`executionStats`);\n```\n\n## Follow-up Questions\n- How would you validate the impact of an index change in production?\n- What metrics would you alert on for sustained tail latency?","diagram":null,"difficulty":"intermediate","tags":["cnpa"],"channel":"cnpa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Lyft","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T13:28:43.195Z","createdAt":"2026-01-12T13:28:43.195Z"},{"id":"q-873","question":"A CNPA service receives events over HTTP, writes to PostgreSQL, and publishes to Kafka. During peak hours, read latency spikes. Describe a concrete debugging plan to determine whether the bottleneck is the HTTP handler, the DB query, or the Kafka producer, including exact steps, metrics to collect, and concrete changes (indexes, pooling, prepared statements, idempotent producer) and how you would verify impact?","answer":"Instrument with OpenTelemetry to trace HTTP handler, DB queries, and Kafka publish; collect p95/p99 latency, error rate, and queue backpressure. Reproduce under load (k6). If HTTP slow, tune keep-aliv","explanation":"## Why This Is Asked\nTests practical debugging across a real CNPA pipeline and encourages measurable fixes.\n\n## Key Concepts\n- End-to-end tracing with OpenTelemetry\n- Performance attribution across HTTP, DB, and messaging\n- Idempotent producers and pool tuning\n- Load testing and metrics-driven verification\n\n## Code Example\n```javascript\n// OpenTelemetry tracing skeleton for CNPA flow\nconst { trace } = require('@opentelemetry/api');\nconst tracer = trace.getTracer('cnpa-trace');\nasync function handleEvent(req, res) {\n  await tracer.startActiveSpan('handle_event', async (span) => {\n    await processHttp();\n    await writeDb();\n    await publishKafka();\n    span.end();\n  });\n}\n```\n\n## Follow-up Questions\n- How would you measure impact of a pool size increase?\n- What changes would you apply to avoid future tail latency spikes?","diagram":"flowchart TD\n  A[HTTP Request] --> B[HTTP Handler]\n  B --> C[PostgreSQL]\n  B --> D[Kafka Producer]\n  C --> E[Indexes/Pooling]\n  D --> F[Configs]\n  G[Metrics] --> H[Tail Latency]","difficulty":"beginner","tags":["cnpa"],"channel":"cnpa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Instacart","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T13:55:00.870Z","createdAt":"2026-01-12T13:55:00.870Z"},{"id":"q-903","question":"In a CNPA stack, an HTTP API writes to Postgres, publishes to Kafka, and a Redis-backed dashboard consumes the stream. During peak load, HTTP latency tails spike. Provide a concrete debugging plan to isolate whether the bottleneck is HTTP, DB, Kafka, producer, consumer, or Redis, with exact metrics, sampling, and concrete changes (pooling, prepared statements, acks, batch sizes, caching strategies) and how you would verify impact?","answer":"Use end-to-end tracing to map latency per hop: HTTP handler, DB query (EXPLAIN ANALYZE), Kafka publish, consumer processing, Redis read. Gather p95/p99 latencies, throughput, queue depths, and GC/thre","explanation":"## Why This Is Asked\n\nTests practical debugging across CNPA stack tail latency.\n\n## Key Concepts\n\n- Distributed tracing\n- Postgres tuning\n- Kafka producer/consumer\n- Redis caching\n\n## Code Example\n\n```bash\n# Example commands to reproduce latency\nEXPLAIN ANALYZE SELECT ...;\n```\n\n## Follow-up Questions\n\n- How would you quantify improvement after each change?\n- Which metric thresholds signal success or regression?","diagram":null,"difficulty":"intermediate","tags":["cnpa"],"channel":"cnpa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Meta","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T14:41:59.785Z","createdAt":"2026-01-12T14:41:59.785Z"},{"id":"q-928","question":"In a CNPA stack consisting of an HTTP API, PostgreSQL, Kafka, and Redis, latency tails spike under peak load. Provide a concrete, beginner-friendly plan to enable end-to-end tracing with OpenTelemetry to pinpoint the bottleneck. Include trace propagation, spans for HTTP handler, DB query, Kafka publish/consume, Redis access, and verification steps with a sample end-to-end trace?","answer":"Instrument OpenTelemetry across all components and propagate traceparent. Create spans for HTTP handler, each DB query, Kafka producer/consumer, and Redis access. Route to a collector (Jaeger/Tempo), ","explanation":"## Why This Is Asked\nUnderstand practical observability across CNPA stack with hands-on tracing.\n\n## Key Concepts\n- OpenTelemetry, trace propagation, end-to-end latency\n- Span creation in HTTP, DB, Kafka, Redis\n- Collector backends (Jaeger/Tempo) and dashboards\n\n## Code Example\n```javascript\n// sample: initialize tracer and extract traceparent from incoming HTTP header\nconst { diag, trace } = require('@opentelemetry/api');\n// setup omitted for brevity\n```\n\n## Follow-up Questions\n- How would sampling change under high throughput?\n- How would you adapt if a component uses a different broker or cache?\n","diagram":"flowchart TD\n  HTTP[HTTP API] --> DB[(PostgreSQL)]\n  HTTP --> Kafka[(Kafka Producer)]\n  Kafka --> Broker[(Kafka Broker)]\n  Broker --> Consumer[(Kafka Consumer)]\n  Consumer --> Redis[(Redis)]","difficulty":"beginner","tags":["cnpa"],"channel":"cnpa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Hashicorp","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T15:39:06.442Z","createdAt":"2026-01-12T15:39:06.442Z"},{"id":"q-957","question":"CNPA stack with HTTP API, PostgreSQL, Kafka, and Redis dashboards requires a schema evolution: add a new optional field region_id to event records without downtime or breaking producers/consumers. Describe a practical, step-by-step migration plan: DB changes, schema registry versioning, producer/consumer updates, data backfill, testing, and rollback strategies, ensuring end-to-end consistency?","answer":"Zero-downtime: add nullable region_id to Postgres; evolve Avro/schema to optional region_id; publish new schema version via Confluent Schema Registry; deploy producers/consumers with dual-write mode a","explanation":"## Why This Is Asked\nTests schema evolution in CNPA and cross-service coordination with Kafka/Redis.\n\n## Key Concepts\n- Backward/forward compatibility\n- Schema Registry / Avro\n- Online backfill\n- Zero-downtime migrations\n- Canary rollout\n\n## Code Example\n```sql\nALTER TABLE events ADD COLUMN region_id VARCHAR(32) NULL;\n```\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nspec:\ntemplate:\nspec:\n  containers:\n    - name: api\n      image: myrepo/api:region-migrate\n      env:\n        - name: REGION_MIGRATION\n          value: true\n```\n\n## Follow-up Questions\n- If region_id becomes not-null, how to migrate without downtime?\n- How to validate backfill performance on large datasets?","diagram":"flowchart TD\nA[HTTP API] --> B[Postgres]\nB --> C[Kafka Producer]\nC --> D[Topics]\nD --> E[Redis Dashboards]","difficulty":"intermediate","tags":["cnpa"],"channel":"cnpa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["PayPal","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T16:43:17.765Z","createdAt":"2026-01-12T16:43:17.765Z"}],"subChannels":["general"],"companies":["Adobe","Airbnb","Amazon","Anthropic","Apple","Bloomberg","Coinbase","Databricks","Discord","DoorDash","Goldman Sachs","Google","Hashicorp","Hugging Face","IBM","Instacart","Lyft","Meta","MongoDB","NVIDIA","Netflix","Oracle","PayPal","Plaid","Robinhood","Salesforce","Scale Ai","Slack","Snap","Snowflake","Tesla","Twitter","Two Sigma","Zoom"],"stats":{"total":28,"beginner":9,"intermediate":10,"advanced":9,"newThisWeek":28}}