{"questions":[{"id":"gh-12","question":"What are the three main service models of cloud computing and how do they differ?","answer":"Cloud computing offers three primary service models: IaaS (Infrastructure as a Service) provides foundational computing resources like virtual machines and storage, PaaS (Platform as a Service) delivers development platforms and tools for application deployment, and SaaS (Software as a Service) supplies complete, ready-to-use software applications accessible via the internet.","explanation":"## Why Asked\nAssesses fundamental cloud knowledge and understanding of the service delivery hierarchy\n\n## Key Concepts\nIaaS (Infrastructure as a Service), PaaS (Platform as a Service), SaaS (Software as a Service), resource abstraction levels, managed responsibility spectrum\n\n## Code Example\n```\n# AWS Service Models Examples\nIaaS: EC2 instances, S3 storage, VPC networking\nPaaS: Elastic Beanstalk, Lambda, RDS\nSaaS: AWS WorkMail, Chime, Salesforce\n```\n\n## Follow-up Questions\nWhen would you choose IaaS vs PaaS for a specific project?\nWhat are the cost implications and scalability considerations of each model?","diagram":"flowchart TD\n    A[Cloud Computing] --> B[IaaS]\n    A --> C[PaaS]\n    A --> D[SaaS]\n    B --> E[Virtual Machines]\n    B --> F[Storage]\n    C --> G[Runtime Environment]\n    C --> H[Development Tools]\n    D --> I[Applications]\n    D --> J[User Interface]","difficulty":"beginner","tags":["cloud","aws","azure","gcp"],"channel":"aws","subChannel":"compute","sourceUrl":null,"videos":{"shortVideo":"https://www.youtube.com/watch?v=M--5UlkNAl0","longVideo":"https://www.youtube.com/watch?v=YpXpmc6lTEg"},"companies":["Amazon","Google","Meta"],"eli5":"Imagine you're at a toy store! IaaS is like buying just the empty shelves - you get the space but must bring your own toys and arrange them. PaaS is like getting a toy box with shelves already set up - you just need to put your toys in. SaaS is like buying a ready-to-play toy set that's already assembled and fun to use right away! Each way gives you less work to do but also less control over how your toys are set up.","relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-29T08:49:28.737Z","createdAt":"2025-12-26 12:51:05"},{"id":"gh-13","question":"What is AWS (Amazon Web Services)?","answer":"AWS is a comprehensive cloud platform offering over 200 fully featured services from data centers worldwide.","explanation":"AWS is a comprehensive cloud platform offering over 200 fully featured services from data centers worldwide. Key services include:\n\n1. **Compute:**\n- EC2 (Elastic Compute Cloud)\n- Lambda (Serverless Computing)\n- ECS (Elastic Container Service)\n\n2. **Storage:**\n- S3 (Simple Storage Service)\n- EBS (Elastic Block Store)\n- EFS (Elastic File System)\n\n3. **Database:**\n- RDS (Relational Database Service)\n- DynamoDB (NoSQL Database)\n- Redshift (Data Warehouse)","diagram":"\ngraph TD\n    AWS --> EC2[EC2 Compute]\n    AWS --> S3[(S3 Storage)]\n    AWS --> RDS[(RDS Database)]\n    AWS --> Lambda[Lambda]\n","difficulty":"beginner","tags":["cloud","aws","azure","gcp"],"channel":"aws","subChannel":"compute","sourceUrl":null,"videos":null,"companies":["Amazon","Goldman Sachs","Google","Microsoft","Netflix"],"eli5":"Imagine AWS is like a giant toy store in the sky! Instead of buying all your toys and keeping them at home, you can rent toys whenever you want. Need a toy car? Just grab one! Need building blocks? They're ready! The toy store has everything - dolls, puzzles, games, and even special tools to help you build amazing things. You don't have to worry about where to keep your toys or fixing them when they break. The toy store takes care of everything! You just play and have fun. AWS is like that toy store, but for computer stuff instead of toys.","relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-08T11:31:44.287Z","createdAt":"2025-12-26 12:51:05"},{"id":"gh-15","question":"Compare AWS IaaS, PaaS, and SaaS service models with specific examples and use cases?","answer":"IaaS provides fundamental infrastructure components like virtual machines, storage, and networking (e.g., AWS EC2, S3, VPC), giving you full control over the operating system and applications. PaaS offers a managed platform for developing and deploying applications without worrying about underlying infrastructure (e.g., AWS Elastic Beanstalk, Lambda, RDS), handling runtime, deployment, and scaling automatically. SaaS delivers complete, ready-to-use applications accessed via the internet (e.g., AWS WorkDocs, Chime, Managed Microsoft AD), requiring zero infrastructure management. Choose IaaS for maximum control and customization, PaaS for faster development with managed operations, or SaaS for immediate productivity with no maintenance overhead.","explanation":"## Interview Context\nTests understanding of cloud service models and architectural decision-making for optimal resource utilization.\n\n## Key Concepts\n- **IaaS**: Virtual machines, storage, networking - complete infrastructure control\n- **PaaS**: Platform services - managed runtime, deployment, and scaling\n- **SaaS**: Complete applications - zero infrastructure management\n\n## AWS Examples\n```bash\n# IaaS - EC2 instance management\naws ec2 run-instances --image-id ami-12345 --instance-type t3.medium\n\n# PaaS - Elastic Beanstalk deployment\neb create my-app --application-version v1.0\n\n# SaaS - WorkDocs usage (no infrastructure needed)\n```\n\n## Decision Framework\n- **IaaS**: Custom applications, specialized configurations, full compliance control\n- **PaaS**: Web/mobile apps, microservices, rapid development cycles\n- **SaaS**: Business productivity, collaboration tools, enterprise software\n\n## Trade-offs\n- Control vs Management Responsibility\n- Cost Structure (pay-as-you-go vs subscription)\n- Technical Expertise Required\n- Scalability and Performance Needs","diagram":"\ngraph TD\n    IaaS[IaaS - Infra] --> PaaS[PaaS - Platform]\n    PaaS --> SaaS[SaaS - Software]\n    FaaS[FaaS - Serverless]\n","difficulty":"intermediate","tags":["cloud","aws","azure","gcp"],"channel":"aws","subChannel":"compute","sourceUrl":null,"videos":{"longVideo":"https://www.youtube.com/watch?v=NhDYbskXRgc"},"companies":["Amazon","Google","Meta","Microsoft","Netflix"],"eli5":"Imagine you want to build a sandcastle at the beach! IaaS is like getting a big empty sandbox - you have all the sand and tools, but you build everything yourself. PaaS is like getting a pre-made sandcastle kit - the castle shape is ready, you just add decorations. SaaS is like renting a finished sandcastle - you just show up and play! Choose based on how much work you want to do versus how much fun you want to have.","relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-04T06:38:58.405Z","createdAt":"2025-12-26 12:51:05"},{"id":"gh-34","question":"How would you design an Auto Scaling configuration for a high-traffic e-commerce application that handles 10,000 RPS with 99.99% availability, including scaling policies, health checks, and cost optimization?","answer":"Auto Scaling dynamically adjusts EC2 instances based on demand using scaling policies. For e-commerce, I'd configure target tracking at 60% CPU utilization, predictive scaling for traffic spikes, scheduled scaling for known events, and use ALB health checks with graceful termination to ensure 99.99% availability while optimizing costs through instance mix and reserved capacity.","explanation":"## Core Components\n\n**Auto Scaling Groups (ASG)** manage EC2 instances across multiple AZs for high availability. Key configurations include min/max/desired capacity, health check grace periods, and instance termination policies.\n\n## Scaling Policies\n\n- **Target Tracking**: Maintain 60% CPU/70% memory utilization\n- **Predictive Scaling**: ML-based forecasting for traffic patterns\n- **Scheduled Scaling**: Pre-warm instances for known traffic spikes\n- **Step Scaling**: Custom thresholds for rapid response\n\n## Health Checks & Monitoring\n\nALB health checks every 30s with 2/3 success threshold. EC2 status checks every minute. CloudWatch alarms trigger scaling actions. Use custom metrics for application-specific monitoring.\n\n## Cost Optimization\n\n- Mix of On-Demand, Reserved, and Spot instances\n- Instance rightsizing based on historical data\n- Savings Plans for predictable workloads\n- Termination policies to optimize for cost vs. availability","diagram":"\ngraph LR\n    Metrics[Metrics] --> ASG[Auto Scaling]\n    ASG -->|Scale Out| Add[Add Instances]\n    ASG -->|Scale In| Remove[Remove Instances]\n","difficulty":"advanced","tags":["scale","ha"],"channel":"aws","subChannel":"compute","sourceUrl":null,"videos":null,"companies":null,"eli5":"Imagine you're having a birthday party! Sometimes only 5 friends come, so you need just 1 pizza. But then 20 friends show up, so you need 4 pizzas! Auto Scaling is like having a magic pizza maker that watches how many friends arrive and automatically makes more or fewer pizzas. When lots of kids want to play on the swings, more swings magically appear. When only a few kids are playing, some swings go away so you don't waste space. It's like having a helper that counts how many people need something and adjusts it perfectly - never too much, never too little!","relevanceScore":null,"voiceKeywords":["auto scaling","target tracking","predictive scaling","health checks","graceful termination","99.99% availability"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2025-12-30T01:44:56.480Z","createdAt":"2025-12-26 12:51:06"},{"id":"gh-57","question":"What is Cloud Cost Optimization and what are the key strategies to reduce cloud spending in production environments?","answer":"Cloud Cost Optimization is the practice of minimizing cloud infrastructure expenses while maintaining performance and reliability by identifying waste, right-sizing resources, leveraging reserved instances, implementing auto-scaling, and continuously monitoring usage patterns.","explanation":"## Why Asked\nInterviewers assess your understanding of cloud financial management and practical cost-saving techniques that directly impact business profitability.\n\n## Key Concepts\nResource right-sizing, reserved instances, spot instances, auto-scaling, cost monitoring, tagging strategies, and architectural optimization.\n\n## Code Example\n```\nresource \"aws_instance\" \"optimized\" {\n  instance_type = \"t3.medium\" # Right-sized for workload\n  spot_price    = \"0.02\" # Cost-effective spot pricing\n  tags = {\n    CostCenter = \"engineering\"\n    Environment = \"production\"\n  }\n}\n```\n\n## Follow-up Questions\nHow do you measure cost optimization success? What tools do you use for cost monitoring? How do you balance cost savings with performance requirements?","diagram":"flowchart TD\n  A[Cost Analysis] --> B[Right-sizing]\n  A --> C[Reserved Instances]\n  A --> D[Auto-scaling]\n  B --> E[Reduced Waste]\n  C --> E\n  D --> E\n  E --> F[Optimized Costs]","difficulty":"beginner","tags":["finops","cost"],"channel":"aws","subChannel":"compute","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google","Meta","Microsoft","Netflix"],"eli5":"Imagine you have a big box of LEGOs that you rent from a toy store. Cloud cost optimization is like being super smart with your LEGO money! You don't want to pay for LEGOs you're not playing with, right? So you only take out the exact pieces you need for your castle, not the whole box. If you're building a small house, you use small LEGOs, not giant ones. Sometimes you tell the toy store 'I'll play with these LEGOs every Tuesday' and they give you a special discount. And you keep checking your LEGO box to make sure you're not wasting pieces on things you don't play with anymore. It's all about using your toy money wisely!","relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-29T08:32:27.462Z","createdAt":"2025-12-26 12:51:05"},{"id":"gh-58","question":"What are AWS Reserved Instances and how do they compare to On-Demand pricing?","answer":"Reserved Instances provide up to 75% discount vs On-Demand pricing in exchange for 1-3 year commitment to specific instance configuration.","explanation":"Reserved Instances (RIs) provide significant cost savings compared to On-Demand pricing in exchange for a commitment to use a specific instance configuration for a one or three-year term.\n\n## Types of Reserved Instances:\n\n**Standard RIs:**\n- Highest discount (up to 75%)\n- Least flexibility - cannot change instance attributes\n- Best for steady-state workloads with predictable usage\n- Can be sold in RI Marketplace\n\n**Convertible RIs:**\n- Lower discount (up to 54%)\n- More flexibility - can exchange for different instance families, OS, tenancy\n- Good for workloads that may change over time\n- Cannot be sold in RI Marketplace\n\n**Scheduled RIs:**\n- For predictable recurring schedules (daily, weekly, monthly)\n- Match capacity reservation to specific usage patterns\n- Available in limited regions and instance types\n\n## Payment Options:\n- **All Upfront:** Highest discount, pay entire term upfront\n- **Partial Upfront:** Medium discount, pay portion upfront + monthly\n- **No Upfront:** Lowest discount, pay monthly only\n\n## Key Benefits:\n- Significant cost reduction for predictable workloads\n- Capacity reservation in specific AZ\n- Can be shared across accounts in organization","diagram":"graph TD\n    A[AWS EC2 Pricing] --> B[On-Demand]\n    A --> C[Reserved Instances]\n    A --> D[Spot Instances]\n    \n    C --> E[Standard RI<br/>Up to 75% discount]\n    C --> F[Convertible RI<br/>Up to 54% discount]\n    C --> G[Scheduled RI<br/>Recurring patterns]\n    \n    E --> H[All Upfront]\n    E --> I[Partial Upfront]\n    E --> J[No Upfront]\n    \n    F --> K[Can Exchange<br/>Instance Types]\n    G --> L[Time-based<br/>Reservations]","difficulty":"intermediate","tags":["finops","cost"],"channel":"aws","subChannel":"compute","sourceUrl":null,"videos":{"shortVideo":"https://www.youtube.com/watch?v=YQsK4MtsELU"},"companies":["Amazon","Goldman Sachs","Google","Microsoft","Uber"],"eli5":"Imagine you want to rent a toy car at the playground. You can either pay for one ride at a time (that's On-Demand), or you can promise to use the same toy car every day for a whole year and get a special discount (that's Reserved Instances). When you make a promise to use the same toy for a long time, the playground owner gives you a much cheaper price because they know you'll keep coming back. The reserved toy car costs way less per ride than paying each time you show up. But you have to stick with the same toy car you picked at the beginning!","relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-27T05:47:46.852Z","createdAt":"2025-12-26 12:51:06"},{"id":"gh-83","question":"How do you evaluate cloud services for business needs using TCO analysis, SLA metrics, and migration strategies?","answer":"Cloud evaluation combines TCO analysis (3-year total cost including data transfer, storage, compute), SLA assessment (uptime, RTO/RPO), service comparison (EC2 vs Lambda vs Fargate), and migration strategy (rehost, refactor, rearchitect). Key factors include performance requirements, security compliance, vendor lock-in risks, and multi-cloud considerations.","explanation":"## TCO Analysis Methods\nCalculate 3-year total cost including:\n- Compute instances (on-demand vs reserved vs spot)\n- Storage costs (EBS, S3 tiers, Glacier)\n- Data transfer fees (egress costs often dominate)\n- Management overhead and operational costs\n\n## SLA Evaluation Framework\n- **Uptime**: 99.9% (8.76h downtime/year) vs 99.99% (52min)\n- **RTO/RPO**: Recovery time and point objectives\n- **Performance**: Latency SLAs, throughput guarantees\n- **Support**: Response times, escalation paths\n\n## Service Comparison Matrix\n| Use Case | EC2 | Lambda | Fargate |\n|----------|-----|--------|---------|\n| Web servers | ✓ | Limited | ✓ |\n| Event processing | ✓ | ✓ | ✓ |\n| Batch jobs | ✓ | Limited | ✓ |\n\n## Migration Strategies\n- **Rehost** (Lift & Shift): Quick, minimal changes\n- **Replatform** (Lift & Reshape): Some optimization\n- **Refactor**: Full cloud-native redesign\n- **Replace**: SaaS substitution\n\n## Multi-cloud Considerations\n- **Portability**: Container-based deployments\n- **Vendor lock-in**: Managed services vs open source\n- **Cost optimization**: Spot instances across providers\n- **Resilience**: Geographic distribution\n\n## Code Example: TCO Calculator\n```python\ndef calculate_tco(instance_type, storage_gb, months=36):\n    # AWS pricing example (simplified)\n    hourly_cost = get_pricing(instance_type)\n    monthly_compute = hourly_cost * 24 * 30\n    monthly_storage = storage_gb * 0.1  # $0.10/GB-month\n    data_transfer = estimate_egress(storage_gb * 0.3)  # 30% monthly\n    \n    monthly_total = monthly_compute + monthly_storage + data_transfer\n    return monthly_total * months\n```\n\n## Key Evaluation Criteria\n- **Performance**: Latency requirements, throughput needs\n- **Scalability**: Auto-scaling capabilities, burst capacity\n- **Security**: Compliance certifications, data residency\n- **Cost**: Pay-as-you-go vs committed spend discounts\n- **Vendor lock-in**: Proprietary services vs open standards","diagram":"flowchart TD\n    A[Business Requirements] --> B[Define Assessment Criteria]\n    B --> C[Identify Cloud Services]\n    C --> D[Technical Evaluation]\n    D --> E[Cost Analysis]\n    E --> F[Security Review]\n    F --> G[Compliance Check]\n    G --> H[Scoring Matrix]\n    H --> I[Recommendation Report]\n    I --> J[Decision & Implementation]\n    \n    D --> D1[Performance Metrics]\n    D --> D2[Scalability Tests]\n    E --> E1[TCO Calculation]\n    E --> E2[ROI Analysis]\n    F --> F1[Security Controls]\n    F --> F2[Risk Assessment]","difficulty":"advanced","tags":["migration","cloud"],"channel":"aws","subChannel":"compute","sourceUrl":null,"videos":{"shortVideo":"https://www.youtube.com/watch?v=RsjnwFSk6LU","longVideo":"https://www.youtube.com/watch?v=2qautbhuJC8"},"companies":["Amazon","Google","IBM","Microsoft","Oracle","Salesforce"],"eli5":"Imagine you're picking toys for a playground! You look at each toy and ask: Is it fun? Does it cost too much? Will it break easily? Can all kids play with it safely? Cloud services are like digital toys for businesses. You check if they're fast enough, don't cost too much money, keep your secrets safe, and follow the rules. Just like you'd pick the best slide that's not too scary, not too expensive, and everyone can use - you pick cloud services that work perfectly for what your business needs!","relevanceScore":null,"voiceKeywords":["tco","sla","migration strategy","uptime","rto","rpo","vendor lock-in"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2025-12-27T05:47:24.296Z","createdAt":"2025-12-26 12:51:06"},{"id":"gh-85","question":"How do cloud migration tools automate application and data transfer between on-premise and cloud environments, and what are the key technical challenges in ensuring data consistency and minimal downtime?","answer":"Tools like AWS Migration Hub and Azure Migrate automate discovery, planning, replication, and cutover while maintaining data consistency through continuous synchronization and validation.","explanation":"## Why Asked\nTests understanding of enterprise cloud migration complexity, tool selection, and technical implementation challenges that architects face in real-world migrations.\n\n## Key Concepts\n- Migration strategies (6 R's: Rehost, Replatform, Refactor, Rearchitect, Repurchase, Retire)\n- Discovery and assessment automation (inventory mapping, dependency analysis)\n- Data replication mechanisms (block-level, file-level, database-level)\n- Cutover strategies (big bang, phased, blue-green)\n- Validation and rollback procedures\n\n## Code Example\n```\n# AWS Migration Hub example workflow\n1. Discovery: Application Discovery Service collects metrics\n2. Assessment: Migration Evaluator analyzes TCO\n3. Replication: AWS DMS continuous data sync\n4. Validation: Compare source/target checksums\n5. Cutover: DNS switch with rollback plan\n```\n\n## Follow-up Questions\n- How would you handle a 10TB database migration with <5min downtime?\n- What tools would you choose for a hybrid multi-cloud migration strategy?\n- How do you ensure data consistency during the cutover phase?","diagram":"graph TD\n    A[On-Premise Infrastructure] --> B[Discovery Engine]\n    B --> C[Assessment Tools]\n    C --> D[Migration Planning]\n    D --> E[Replication Engine]\n    E --> F[Cloud Staging Environment]\n    F --> G[Validation Testing]\n    G --> H{Validation Passed?}\n    H -->|Yes| I[Cutover Automation]\n    H -->|No| J[Remediation]\n    J --> G\n    I --> K[Cloud Production Environment]\n    \n    subgraph \"Migration Tools\"\n        B\n        C\n        E\n        I\n    end\n    \n    subgraph \"Cloud Provider\"\n        F\n        K\n    end","difficulty":"intermediate","tags":["migration","cloud"],"channel":"aws","subChannel":"compute","sourceUrl":null,"videos":{"longVideo":"https://www.youtube.com/watch?v=zin8sn7jjJg"},"companies":["Amazon","Citadel","Goldman Sachs","Google","Microsoft"],"eli5":"Imagine you're moving all your toys from your bedroom to a new playroom. Special robot helpers come and count every toy you have, then make a plan to move them safely. While you're still playing in your old room, the robots make exact copies of all your toys in the new playroom. They keep checking that every toy is in the right place and nothing got lost during the move. When everything's ready, you just walk into the new playroom and can start playing immediately - no waiting! The hardest part is making sure no toys get lost or broken while moving, and that you can keep playing the whole time without stopping.","relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-25T13:05:55.187Z","createdAt":"2025-12-26 12:51:06"},{"id":"gh-87","question":"How would you implement a multi-cloud cost allocation system using tagging strategies and automation APIs?","answer":"Implement centralized tagging policies with automated cost allocation using cloud provider APIs and custom chargeback logic.","explanation":"## Interview Context\nThis question assesses your ability to design and implement technical solutions for cloud cost management, focusing on automation and system integration rather than financial governance.\n\n## Technical Implementation\n### Tagging Strategy\n```yaml\n# Centralized tagging policy\ntags:\n  - cost-center: \"engineering\"\n  - project: \"microservices-platform\"\n  - environment: \"${env}\"\n  - owner: \"${team}\"\n  - auto-tag: \"true\"\n```\n\n### Cost Allocation API Integration\n```python\n# AWS Cost Explorer API integration\nimport boto3\n\nclass CostAllocator:\n    def __init__(self):\n        self.ce = boto3.client('ce')\n        \n    def get_costs_by_tag(self, tag_key, time_period):\n        response = self.ce.get_cost_and_usage(\n            TimePeriod=time_period,\n            Granularity='MONTHLY',\n            GroupBy=[\n                {'Type': 'TAG', 'Key': tag_key},\n                {'Type': 'DIMENSION', 'Key': 'SERVICE'}\n            ],\n            Metrics=['BlendedCost']\n        )\n        return response['ResultsByTime']\n```\n\n### Multi-Cloud Aggregation\n```javascript\n// Multi-cloud cost aggregation\nclass MultiCloudCostAggregator {\n  constructor() {\n    this.providers = {\n      aws: new AWSCostExplorer(),\n      azure: new AzureCostManagement(),\n      gcp: new GCPCostAnalyzer()\n    };\n  }\n  \n  async getUnifiedCosts(timeRange) {\n    const costs = await Promise.all(\n      Object.entries(this.providers).map(\n        ([provider, client]) => client.getCosts(timeRange)\n      )\n    );\n    return this.normalizeAndAggregate(costs);\n  }\n}\n```\n\n## Follow-up Questions\n1. How would you handle tag propagation across auto-scaling resources?\n2. What strategies would you use for cost allocation in serverless architectures?\n3. How do you ensure data consistency when aggregating costs across different cloud providers?","diagram":"graph TD\n    A[Cloud Resources] --> B[Cost Tagging]\n    B --> C[Cost Collection Engine]\n    C --> D[Cost Allocation Rules]\n    D --> E[Showback Reports]\n    D --> F[Chargeback Invoices]\n    E --> G[Team Visibility]\n    F --> H[Finance Integration]\n    G --> I[Cost Optimization]\n    H --> J[Budget Planning]\n    I --> K[Resource Efficiency]\n    J --> L[Financial Governance]\n    K --> M[Reduced Waste]\n    L --> N[Business Alignment]","difficulty":"advanced","tags":["advanced","cloud"],"channel":"aws","subChannel":"compute","sourceUrl":null,"videos":{"longVideo":"https://www.youtube.com/watch?v=n7h1XFkQouU"},"companies":["Amazon","Google","Microsoft","Stripe","Uber"],"eli5":"Imagine you have a big box of LEGOs and you want to know who used which pieces. You put special stickers on each LEGO - red stickers for your toys, blue stickers for your sister's toys, and green stickers for shared toys. Then you have a magic robot that counts all the stickers and tells you exactly how many pieces each person used. The cloud is like that big LEGO box, the stickers are tags that show who owns what, and the robot is the computer that automatically counts everything and sends a bill to the right person!","relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-24T12:59:09.350Z","createdAt":"2025-12-26 12:51:05"},{"id":"q-174","question":"You have an EC2 instance that suddenly becomes unresponsive. What step-by-step troubleshooting methodology would you follow, which specific AWS tools and commands would you use at each stage, and how would you handle different instance states and recovery scenarios?","answer":"Start with CloudWatch metrics (CPUUtilization, NetworkIn/Out, StatusCheckFailed). Use AWS Systems Manager Session Manager or EC2 Serial Console for direct access. Check instance state transitions via AWS CLI: `aws ec2 describe-instance-status`. Examine system logs via CloudWatch Logs or `/var/log`. Verify security groups and Network ACLs. If needed, reboot via `aws ec2 reboot-instances` or stop/start for full recovery. Use AWS Backup or EBS snapshots for disaster recovery.","explanation":"## Troubleshooting Methodology\n\n**Step 1: Initial Assessment**\n- Check CloudWatch metrics for CPU, memory, network anomalies\n- Verify instance status checks (system/instance)\n- Use AWS CLI: `aws ec2 describe-instance-status --instance-ids i-1234567890abcdef0`\n\n**Step 2: Direct Access**\n- AWS Systems Manager Session Manager for SSH-less access\n- EC2 Serial Console for kernel-level debugging\n- Commands: `ssm start-session --target i-1234567890abcdef0`\n\n**Step 3: Log Analysis**\n- CloudWatch Logs integration\n- System logs: `/var/log/syslog`, `/var/log/messages`\n- Application logs in `/var/log/app`\n\n**Step 4: Network Verification**\n- Security group rules: `aws ec2 describe-security-groups`\n- Network ACLs: `aws ec2 describe-network-acls`\n- VPC Flow Logs for traffic analysis\n\n**Step 5: Recovery Procedures**\n- Soft reboot: `aws ec2 reboot-instances`\n- Hard stop/start: `aws ec2 stop-instances` + `aws ec2 start-instances`\n- EBS volume recovery: detach/attach to new instance\n\n**Step 6: Prevention**\n- CloudWatch alarms for proactive monitoring\n- AWS Backup for automated snapshots\n- Enhanced monitoring with detailed metrics\n\n## Edge Cases & Gotchas\n\n- **Instance Store**: Data loss on stop/restart\n- **EBS Optimization**: Verify I/O performance\n- **Burst Performance**: Check T2/T3 credit balance\n- **IAM Permissions**: Ensure SSM access policies\n\n## Real-World Applications\n\n- Production web servers with 99.9% uptime SLA\n- Database instances requiring consistent performance\n- Batch processing jobs with strict completion deadlines\n\n## Performance Monitoring Tools\n\n- CloudWatch Custom Metrics\n- AWS X-Ray for distributed tracing\n- Third-party tools: Datadog, New Relic integration\n- Enhanced Monitoring for RDS instances","diagram":"graph TD\n    A[EC2 Instance Unresponsive] --> B[Check CloudWatch Metrics]\n    B --> C{Resource Issues?}\n    C -->|Yes| D[Scale Up Resources]\n    C -->|No| E[Use Serial Console]\n    E --> F[Examine System Logs]\n    F --> G{Network Issues?}\n    G -->|Yes| H[Check Security Groups/NACLs]\n    G -->|No| I[Verify Instance Status]\n    I --> J[Reboot Instance]\n    J --> K{Fixed?}\n    K -->|No| L[Stop/Restart Instance]\n    K -->|Yes| M[Issue Resolved]","difficulty":"intermediate","tags":["ec2","compute"],"channel":"aws","subChannel":"compute","sourceUrl":null,"videos":null,"companies":["Amazon","Google","Meta","Microsoft","Netflix"],"eli5":"Imagine your toy robot suddenly stops moving! First, you'd check if its batteries are working (that's like checking CloudWatch metrics). Then you'd try talking to it through a special phone (that's the EC2 Serial Console). Next, you'd look at its diary to see what it was doing before it got stuck (that's reading system logs). Finally, you'd make sure no one put a 'keep out' sign on your playground or blocked the door (that's checking security groups and network ACLs). Just like fixing a stuck toy, you check its power, try to talk to it, see what it was doing, and make sure nothing is blocking its way!","relevanceScore":null,"voiceKeywords":["cloudwatch metrics","systems manager","session manager","serial console","instance status","security groups","network acls","ebs snapshots"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2025-12-27T04:58:52.953Z","createdAt":"2025-12-26 12:51:07"},{"id":"q-321","question":"You have a containerized web application that needs to handle variable traffic loads. When would you choose ECS Fargate over EKS and what are the key trade-offs?","answer":"Choose ECS Fargate for simpler workloads with less operational overhead. EKS for complex microservices needing Kubernetes features. Trade-offs: control vs simplicity, cost vs flexibility.","explanation":"## Why Asked\nInterview context at Figma and Cloudflare tests understanding of container orchestration decisions and cost optimization in production environments.\n## Key Concepts\n- ECS Fargate: Serverless containers, managed service\n- EKS: Managed Kubernetes, more control\n- Cost implications, operational overhead\n- Scaling patterns and traffic handling\n## Code Example\n```\n# ECS Fargate Task Definition\n{\n  \"family\": \"web-app\",\n  \"networkMode\": \"awsvpc\",\n  \"requiresCompatibilities\": [\"FARGATE\"],\n  \"cpu\": \"256\",\n  \"memory\": \"512\"\n}\n```\n## Follow-up Questions\n- How would you handle auto-scaling for","diagram":"flowchart TD\n  A[Containerized App] --> B{Traffic Pattern?}\n  B -->|Variable/Simple| C[ECS Fargate]\n  B -->|Complex/Microservices| D[EKS]\n  C --> E[Lower Ops Overhead]\n  D --> F[More Control]","difficulty":"beginner","tags":["ec2","ecs","eks","fargate"],"channel":"aws","subChannel":"compute","sourceUrl":null,"videos":{"shortVideo":"https://www.youtube.com/watch?v=oO-mGql5JvQ","longVideo":"https://www.youtube.com/watch?v=esISkPlnxL0"},"companies":["Cloudflare","Figma","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":["ecs fargate","eks","container orchestration","operational overhead","kubernetes","trade-offs","microservices"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2026-01-03T06:38:42.969Z","createdAt":"2025-12-26 12:51:05"},{"id":"q-216","question":"How would you design an eventual consistency strategy for a multi-region DynamoDB application using Global Tables to handle write conflicts, ensure data convergence, and minimize latency?","answer":"Implement DynamoDB Global Tables with version vectors for conflict resolution, using conditional writes with乐观并发控制. Combine Last-Writer-Wins for simple conflicts and custom application-specific resolvers for business logic conflicts. Monitor replication lag and implement conflict detection metrics.","explanation":"## Conflict Resolution Patterns\n\n**Version Vectors**: Track causal dependencies across regions to determine conflicting writes:\n\n```javascript\n// Item with version tracking\n{\n  \"pk\": \"user#123\",\n  \"sk\": \"profile\",\n  \"data\": {\"name\": \"John\"},\n  \"version\": {\"us-east-1\": 3, \"eu-west-1\": 2},\n  \"timestamp\": 1703123456789\n}\n\n// Conditional write with version check\nawait dynamodb.put({\n  TableName: \"users\",\n  Item: updatedItem,\n  ConditionExpression: \"attribute_not_exists(pk) OR version < :newVersion\"\n}).promise();\n```\n\n## Conflict Resolution Strategies\n\n**Last Writer Wins (LWW)**: Simple but acceptable for non-critical data like user preferences\n\n**Custom Resolvers**: For business-critical conflicts requiring domain logic:\n- Merge operations (shopping cart consolidation)\n- Business rule resolution (inventory vs orders)\n- Manual escalation for high-value conflicts\n\n## Consistency Trade-offs\n\n- **Latency**: ~100-200ms replication between regions\n- **Conflict Rate**: Typically <0.1% with proper data partitioning\n- **Storage Overhead**: 20-30 bytes per item for version metadata\n\n## Monitoring & Observability\n\nTrack key metrics:\n- Replication lag per region pair\n- Conflict frequency and resolution success\n- Conditional write failure rates\n- Data convergence time\n\nImplement dead letter queues for unresolvable conflicts requiring manual intervention.","diagram":"flowchart LR\n    A[Client Write US-East] --> B[DynamoDB US-East]\n    C[Client Write EU-West] --> D[DynamoDB EU-West]\n    B --> E[Async Replication]\n    D --> E\n    E --> F[Conflict Resolution]\n    F --> G[Final State]","difficulty":"intermediate","tags":["mongodb","dynamodb","cassandra","redis"],"channel":"aws","subChannel":"database","sourceUrl":null,"videos":null,"companies":["Amazon","Apple","Google","Meta","Microsoft","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-26T16:33:32.682Z","createdAt":"2025-12-26 12:51:07"},{"id":"q-357","question":"You're designing a security monitoring system that needs to store 10M+ events per day with millisecond read latency. How would you choose between DynamoDB, Aurora, and ElastiCache, and what's your data partitioning strategy?","answer":"Use DynamoDB as the primary storage with a composite key strategy: partition key by month (security-events#YYYY-MM) combined with time-based sort keys for time-series efficiency, implement hot partitioning for recent data with TTL for automatic cleanup, leverage Aurora PostgreSQL for complex analytical queries and reporting, and utilize ElastiCache Redis to cache frequently accessed security rules and threat intelligence data.","explanation":"## Why This Is Asked\nThis question evaluates real-world database selection skills, deep understanding of AWS service trade-offs, and the ability to design scalable architectures—critical competencies for security engineering roles at companies like Palo Alto Networks and Crowdstrike.\n\n## Expected Answer\nStrong candidates should discuss: DynamoDB for high-throughput write workloads with automatic scaling and TTL capabilities, Aurora for complex analytical queries requiring joins and aggregations, ElastiCache Redis for sub-millisecond access to hot data like security rules and threat intelligence, careful partitioning strategy to prevent hot keys and ensure even distribution, comprehensive cost optimization across services, and robust backup/recovery mechanisms for compliance requirements.\n\n## Code Example\n```typescript\n// DynamoDB optimized partition key strategy\nconst partitionKey = `security-events#${date.slice(0, 7)}`; // YYYY-MM\nconst sortKey = `${event.timestamp}#${event.sourceIp}`; // Prevents hot keys\n\n// TTL configuration for automatic data cleanup\nconst ttl = Math.floor(Date.now() / 1000) + (90 * 24 * 60 * 60); // 90 days\n\n// ElastiCache hot data pattern\nconst hotKey = `security-rule:${ruleId}`;\nawait redis.setex(hotKey, 3600, ruleData); // 1-hour cache\n```","diagram":"flowchart TD\n  A[Security Event] --> B{Event Age?}\n  B -->|< 24h| C[Write to Hot Partition]\n  B -->|>= 24h| D[Write to Regular Partition]\n  C --> E[DynamoDB + ElastiCache]\n  D --> F[DynamoDB]\n  E --> G[Millisecond Reads]\n  F --> G\n  G --> H[Aurora Analytics]","difficulty":"intermediate","tags":["rds","aurora","dynamodb","elasticache"],"channel":"aws","subChannel":"database","sourceUrl":null,"videos":{"shortVideo":"https://www.youtube.com/watch?v=JIbIYCM48to","longVideo":null},"companies":["Amazon","Apple","Google","Meta","Microsoft","Netflix","Palo Alto Networks"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-30T01:51:41.619Z","createdAt":"2025-12-26 12:51:05"},{"id":"q-401","question":"You're designing a real-time analytics dashboard for Scale AI that needs to handle 10,000 events/second. Your team is debating between using DynamoDB with DAX vs. Aurora with ElastiCache. What are the key trade-offs you'd consider, and which would you choose for this use case?","answer":"I would choose DynamoDB with DAX for this real-time analytics dashboard. The combination provides predictable performance at scale, lower operational overhead, and cost-effectiveness for handling 10,000 events per second.","explanation":"## Why This Is Asked\nThis question tests practical decision-making skills, understanding of AWS database services, and ability to analyze trade-offs for real-world scenarios at scale.\n\n## Expected Answer\nThe candidate should discuss:\n- **DynamoDB**: NoSQL database with unlimited scaling, DAX for in-memory caching, pay-per-request pricing model, and eventual consistency\n- **Aurora**: Relational database with ACID compliance, support for complex queries, connection pooling capabilities, and higher operational costs\n- **For 10k events/sec**: DynamoDB's horizontal scaling architecture and DAX caching layer provide better performance characteristics\n- **Aurora limitations**: Transaction overhead and connection pooling constraints could create bottlenecks at this throughput level\n\n## Code Example\n```typescript\n// Example DynamoDB with DAX implementation\n```","diagram":"flowchart TD\n  A[Real-time Events 10k/sec] --> B{Database Choice}\n  B -->|DynamoDB + DAX| C[Horizontal Scaling]\n  B -->|Aurora + ElastiCache| D[Vertical Scaling]\n  C --> E[Millisecond Latency]\n  C --> F[Pay-per-request]\n  D --> G[Complex Queries]\n  D --> H[Higher Cost]\n  E --> I[Analytics Dashboard]\n  G --> I","difficulty":"intermediate","tags":["rds","aurora","dynamodb","elasticache"],"channel":"aws","subChannel":"database","sourceUrl":null,"videos":{"shortVideo":"https://www.youtube.com/watch?v=crHwekf0gTA","longVideo":"https://www.youtube.com/watch?v=5iZ1o4w7354"},"companies":["Cohere","Oscar Health","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":["dynamodb","dax","aurora","elasticache","scalability","performance","cost"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2026-01-08T11:31:11.641Z","createdAt":"2025-12-26 12:51:05"},{"id":"q-413","question":"You're designing a real-time analytics dashboard for an IoT application that receives 10,000 events per second. The dashboard needs to show current metrics and historical trends. How would you design the database architecture using AWS services, and what caching strategy would you implement?","answer":"Use DynamoDB for event ingestion with time-based partitioning, Aurora for analytical queries, and ElastiCache Redis for real-time dashboard caching.","explanation":"## Why This Is Asked\nApple tests your ability to make informed database decisions, understand trade-offs between consistency and performance, and design scalable architectures that handle high-throughput workloads.\n\n## Expected Answer\nStrong candidates discuss DynamoDB for write-heavy workloads with on-demand capacity, Aurora for complex analytical queries with its MySQL compatibility, and ElastiCache Redis for sub-millisecond dashboard response times. They should mention data partitioning strategies, read replicas, and cache invalidation patterns.\n\n## Code Example\n```typescript\n// DynamoDB event ingestion\nconst putEvent = async (deviceId: string, eventData: any) => {\n  const params = {\n    TableName: 'iot-events',\n    Item: {\n      deviceId,\n      timestamp: Date.now(),\n      ...eventData,\n      ttl: Math.floor(Date.now() / 1000) + (30 * 24 * 60 * 60) // 30 days\n    }\n  };\n  await dynamodb.put(params).promise();\n  \n  // Update cache\n  await redis.zadd(`device:${deviceId}:metrics`, Date.now(), JSON.stringify(eventData));\n};\n```\n\n## Follow-up Questions\n- How would you handle backpressure if event ingestion exceeds 10,000/sec?\n- What's your strategy for data retention and cost optimization?\n- How would you ensure data consistency between DynamoDB and Aurora?","diagram":"flowchart TD\n    A[IoT Events] --> B[DynamoDB Stream]\n    B --> C[AWS Lambda Processor]\n    C --> D[Aurora Analytics DB]\n    C --> E[ElastiCache Redis]\n    E --> F[Real-time Dashboard]\n    D --> G[Historical Reports]\n    A --> H[DynamoDB Events Table]\n    H --> I[TTL Cleanup]\n    C --> J[CloudWatch Metrics]","difficulty":"intermediate","tags":["rds","aurora","dynamodb","elasticache"],"channel":"aws","subChannel":"database","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Apple","Databricks","Google","Micron","Microsoft","Netflix","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":["dynamodb","aurora","elasticache","redis","time-based partitioning","real-time analytics","caching strategy"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2025-12-27T05:51:15.279Z","createdAt":"2025-12-26 12:51:05"},{"id":"q-1256","question":"You're operating a global real-time analytics pipeline on AWS: data streams from mobile apps ingest via Kinesis Data Streams, processed by Lambda, stored in DynamoDB and S3 Parquet. A new release causes timeouts and duplicate processing under peak load. Propose a concrete plan to fix cold starts and throttling, ensure exactly-once semantics, and safely deploy with minimal data loss. Include services, config values, and rollout steps?","answer":"Plan: enable Lambda provisioned concurrency to avoid cold starts; use Kinesis enhanced fan-out; implement idempotent processing with DynamoDB conditional upserts and a dedupe key; add a DLQ for failed","explanation":"## Why This Is Asked\n\nAssess ability to design scalable, resilient streaming pipelines on AWS, balancing compute warm-start strategies, data correctness (exactly-once), deployment safety, and observability under real-world load spikes.\n\n## Key Concepts\n\n- Lambda provisioned concurrency\n- Kinesis Enhanced Fan-Out\n- Idempotent processing and deduplication\n- Dead-letter queues (DLQ)\n- Canary/Blue-Green deployment with CodeDeploy\n- Observability (CloudWatch, X-Ray)\n- Data durability in S3 with versioning\n\n## Code Example\n\n```javascript\n// Pseudo-idempotent handler\nfunction handle(record) {\n  const id = record.eventSourceARN + ':' + record.sequenceNumber;\n  if (cache.has(id)) return;\n  // process\n  storeRecord(record);\n  cache.set(id, true);\n}\n```\n\n## Follow-up Questions\n\n- How would you implement exactly-once semantics across DynamoDB and S3 writes?\n- What metrics signal a regression after deployment, and how would you respond?","diagram":"flowchart TD\n  A[Ingest: Mobile] --> B[Kinesis]\n  B --> C[Lambda]\n  C --> D[DynamoDB + S3 Parquet]\n  D --> E[BI/Queries]","difficulty":"advanced","tags":["aws"],"channel":"aws","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Robinhood","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T06:48:17.360Z","createdAt":"2026-01-13T06:48:17.360Z"},{"id":"q-2571","question":"How would you implement a cross-region, multi-account data ingestion pipeline for real-time analytics on AWS, ensuring tenant isolation, least-privilege IAM roles, cross-account access, and automatic CMK rotation, using Kinesis Streams, S3, Lake Formation, and Glue?","answer":"Design a cross-region, multi-account data ingestion pipeline using Kinesis Data Streams deployed in each region to collect real-time data, which feeds into a centralized S3 data lake organized with tenant-scoped prefixes for isolation. Implement per-tenant IAM roles following least-privilege principles with cross-account AssumeRole access patterns for secure delegation. Enable automatic CMK rotation through AWS KMS for encryption at rest, enforce strict data isolation via Lake Formation grants and ACLs, and utilize AWS Glue for centralized catalog management with proper partitioning and schema evolution support.","explanation":"## Why This Is Asked\nThis question tests your ability to design secure, scalable AWS architectures that handle complex requirements around cross-account access, multi-region deployment, tenant isolation, and real-time data processing patterns.\n\n## Key Concepts\n- Cross-account IAM roles and STS AssumeRole patterns for secure delegation\n- Real-time data ingestion with Kinesis Data Streams across multiple regions\n- Centralized data lake architecture in S3 with tenant-specific prefixes\n- Encryption at rest with automatic CMK rotation via AWS KMS\n- Data isolation enforcement through Lake Formation grants and ACLs\n- Schema evolution and partition management with AWS Glue\n\n## Code Example\n```javascript\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\"Effect\": \"Allow\",\"Action\":[\"kinesis:PutR\"","diagram":null,"difficulty":"intermediate","tags":["aws"],"channel":"aws","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Google","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T05:19:04.964Z","createdAt":"2026-01-15T23:32:45.393Z"},{"id":"q-2694","question":"In a real-time recommendation platform on AWS, eu-west-1 outage impacts feature storage and SageMaker endpoints. Propose a tested disaster-recovery plan that preserves data availability and latency: include cross-region S3 replication, multi-region SageMaker endpoints with traffic routing, a cross-account feature store, and an automated failover workflow using Step Functions. Outline validation steps for RTO and RPO?","answer":"Design a DR plan: enable S3 cross-region replication for feature data to a warm secondary region, deploy dual SageMaker endpoints with traffic routing and canary shifts, use DynamoDB Global Tables for","explanation":"## Why This Is Asked\nTests ability to design cross-region DR for a latency-sensitive ML service, covering data replication, endpoint failover, feature store consistency, cross-account IAM, and orchestrating recovery with Step Functions.\n\n## Key Concepts\n- Cross-region replication and eventual consistency\n- SageMaker multi-region endpoints and routing strategies\n- DynamoDB Global Tables for multi-region state\n- IAM roles and least privilege cross-account access\n- Step Functions canary deployments and validation\n\n## Code Example\n```yaml\n# CloudFormation/App arms for DR\nResources:\n  FeatureTableGlobal:\n    Type: AWS::DynamoDB::GlobalTable\n    Properties:\n      TableName: FeatureStore\n      ReplicationGroup:\n        - Region: us-east-1\n        - Region: eu-west-1\n```\n\n## Follow-up Questions\n- How would you measure RTO/RPO and automate drills?\n- What are the trade-offs of eventual vs strongly consistent reads in DR?","diagram":null,"difficulty":"advanced","tags":["aws"],"channel":"aws","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T07:05:19.047Z","createdAt":"2026-01-16T07:05:19.048Z"},{"id":"q-2817","question":"You run a data analytics platform on AWS spanning two regions with a central S3 data lake and a production Redshift (or Lakehouse) cluster. Design a disaster-recovery strategy to meet RPO of 5 minutes and RTO of 15 minutes, covering cross-region data replication, IAM boundaries, Secrets Manager rotation, and automated failover. Explain components, data-integrity checks, and failure modes?","answer":"Implement active-active regions: enable S3 cross-region replication for the data lake, replicate metadata and catalog with cross-region DynamoDB or RDS, and keep a warm standby Redshift cluster with a","explanation":"## Why This Is Asked\n\nDrills DR planning and real-world AWS service interactions under strict SLAs, ensuring you can specify automation, observability, and trade-offs for cross-region resilience.\n\n## Key Concepts\n\n- DR strategy with defined RPO/RTO\n- Cross-region replication for data lakes (S3 CRR)\n- Metadata replication (DynamoDB/RDS cross-region)\n- Warm standby and automated failover for analytics clusters\n- IAM boundaries, least privilege, STS short-lived creds\n- Secrets management (Secrets Manager) and rotation\n- Failover automation (Lambda, Route 53 health checks)\n- Data integrity checks and reconciliation\n\n## Code Example\n\n```javascript\n// Conceptual Route53 failover switch (pseudo-implementation)\nconst AWS = require('aws-sdk');\nconst route53 = new AWS.Route53({region: 'us-east-1'});\n\nasync function switchToSecondary(hostedZoneId, recordName, secondaryIp) {\n  const params = {\n    ChangeBatch: {\n      Changes: [\n        {Action: 'UPSERT', ResourceRecordSet: {Name: recordName, Type: 'A', TTL: 60, ResourceRecords: [{Value: secondaryIp}]}}\n      ],\n      Comment: 'DR failover switch'\n    },\n    HostedZoneId: hostedZoneId\n  };\n  return route53.changeResourceRecordSets(params).promise();\n}\n```\n\n## Follow-up Questions\n\n- How would you test DR readiness without impacting production?\n- How would you handle data consistency during asynchronous replication?\n- What monitoring dashboards and alerts would you implement?","diagram":null,"difficulty":"advanced","tags":["aws"],"channel":"aws","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T13:50:54.659Z","createdAt":"2026-01-16T13:50:54.659Z"},{"id":"q-2927","question":"Design a cross-region, multi-account data ingestion and analytics platform for a regulated partner. The data must remain resident in the partner's region, be isolated per tenant, support automatic CMK rotation, use AWS Lake Formation for cataloging, KMS for encryption, cross-account roles with least privilege, and enforce SCPs and VPC endpoints to prevent egress. Describe the architecture, data flow, and failure modes?","answer":"Configure a data lake per region with per-tenant IAM roles and Lake Formation catalog. Use a CMK rotated automatically; enforce SCPs that block cross-account writes unless explicitly allowed; create c","explanation":"## Why This Is Asked\nTests design of secure, compliant cross-region pipelines across accounts, with tenant isolation and data residency.\n\n## Key Concepts\n- Cross-account IAM roles, least privilege\n- Service Control Policies (SCPs)\n- AWS Lake Formation data lake\n- KMS CMK rotation\n- DR with Route 53, VPC Endpoints\n\n## Code Example\n```javascript\n// CDK snippet sketch for a cross-account role\nconst role = new iam.Role(this, 'TenantIngestRole', { assumedBy: new iam.AccountPrincipal(tenantAcc) });\n```\n\n## Follow-up Questions\n- How would you test CMK rotation impact on queries?\n- How would you enforce data residency if a partner requests limited cross-region access?","diagram":"flowchart TD\n  A[Tenant Region] --> B[Ingestion (Kinesis/SQS)]\n  B --> C[Lake Formation Catalog]\n  C --> D[Analytics (Glue/Athena)]\n  D --> E[S3 Data Lake]\n  E --> F[Cross-Region Replication]\n  G[DR via Route 53 failover] --> A","difficulty":"advanced","tags":["aws"],"channel":"aws","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","DoorDash","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T17:50:37.064Z","createdAt":"2026-01-16T17:50:37.065Z"},{"id":"q-3122","question":"You run a distributed job orchestration platform across 3 AWS regions. Job state is stored in DynamoDB, and workers pull tasks via region-specific SQS queues. To achieve true exactly-once processing across regions, outline an architecture using DynamoDB, SQS, EventBridge or Kinesis, and Step Functions, with idempotency keys and tombstones. Describe data flow, conflict resolution, replay handling, failure modes, and cost implications?","answer":"Use a central DynamoDB table as the truth store with a composite key (region, jobId) and an idempotency key. Route tasks into regional SQS queues fed by a cross-region EventBridge bus; orchestrate wit","explanation":"## Why This Is Asked\n\nThis tests ability to design distributed, exactly-once processing with cross-region interactions, which is common in real-time analytics and event-driven platforms at scale.\n\n## Key Concepts\n\n- Exactly-once processing\n- DynamoDB transactions\n- Idempotency keys\n- Cross-region eventing (EventBridge)\n- Step Functions orchestration\n- Dead-letter queues and replay handling\n\n## Code Example\n\n```javascript\n// Pseudo-code for idempotent write\nconst params = {\n  TableName: 'Jobs',\n  Item: { pk: 'REGION#us-east-1', sk: 'JOB#123', state: 'DONE', version: 3 },\n  ConditionExpression: 'attribute_not_exists(sk) OR version = :v',\n  ExpressionAttributeValues: { ':v': 2 }\n};\ndynamoDB.put(params).promise();\n```\n\n## Follow-up Questions\n\n- How would you test exactly-once guarantees across regional failures?\n- What are the trade-offs of using SQS with at-least-once vs. using Kinesis with exactly-once semantics?\n","diagram":"flowchart TD\n  A[Client] --> B[EventBridge bus]\n  B --> C[DynamoDB truth store]\n  B --> D[SQS regional queues]\n  D --> E[Worker nodes]\n  E --> F[Step Functions orchestration]\n  F --> G[Tombstones / completions]\n  G --> C","difficulty":"advanced","tags":["aws"],"channel":"aws","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Oracle","Twitter","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T04:03:50.166Z","createdAt":"2026-01-17T04:03:50.166Z"},{"id":"q-3202","question":"You're running a real-time telemetry pipeline on AWS: producers send to Kinesis Data Streams, a Lambda consumer writes to DynamoDB. During event spikes, latency spikes and some records back up. Describe a concrete, end-to-end plan to diagnose and fix, including how you determine shard count, Lambda concurrency, data partitioning, and DynamoDB throughput; also what changes you would roll back if the plan fails?","answer":"Start with CloudWatch: throughput, age, and Lambda concurrency. If backlogs occur, scale Kinesis shards (estimate shards ≈ TPS × processing time / 1000), enable enhanced fan-out, and reduce Lambda bat","explanation":"## Why This Is Asked\n\nTests ability to diagnose real-time streaming bottlenecks across multiple AWS services and to craft concrete scaling, tuning, and rollback plans.\n\n## Key Concepts\n\n- Kinesis throughput and shard scaling\n- Lambda event source mapping concurrency and batch size\n- DynamoDB throughput modes and partitioning\n- Enhanced fan-out and back-pressure\n- Observability and safe rollback planning\n\n## Code Example\n\n```javascript\n// Pseudo: compute required shards from observed TPS and processing time\nconst requiredShards = Math.ceil((throughput * avgProcessingMs) / 1000);\n```\n\n## Follow-up Questions\n\n- How would you validate the rollback plan during a non-disruptive test window?\n- What metrics would trigger auto-scaling adjustments and how would you automate them?","diagram":null,"difficulty":"advanced","tags":["aws"],"channel":"aws","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Hashicorp","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T06:50:31.906Z","createdAt":"2026-01-17T06:50:31.906Z"},{"id":"q-3353","question":"You deploy a static site to S3 behind CloudFront. After a release, users in a new region report 404/403 for assets; outline a concrete, beginner-friendly diagnostic and fix using AWS tools (S3, CloudFront, CloudWatch, IAM). Include steps to verify object existence, bucket policies, OAI if used, and how to invalidate paths and confirm resolution?","answer":"Check CloudFront and S3 permissions and object presence. Validate the bucket policy allows CloudFront (Origin Access Identity) or public read if appropriate, and ensure public access blocks aren’t too","explanation":"## Why This Is Asked\nDiagnoses real-world pointer: misconfigured permissions or caching can cause region-specific 4xx errors on a static site.\n\n## Key Concepts\n- CloudFront origin access and caching\n- S3 bucket policies and public access blocks\n- Invalidation and verification workflows\n\n## Code Example\n```bash\naws s3api head-object --bucket BUCKET --key PATH\naws cloudfront create-invalidation --distribution-id DIST_ID --paths '/path/*'\n```\n\n## Follow-up Questions\n- How would you verify using CloudFront logs and access logs from S3?\n- What changes to the bucket policy and OAI would you apply in a secure setup?","diagram":null,"difficulty":"beginner","tags":["aws"],"channel":"aws","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","LinkedIn"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T13:09:52.968Z","createdAt":"2026-01-17T13:09:52.968Z"},{"id":"q-3361","question":"You operate a multi-region, multi-account SaaS platform with strict data residency requirements: some tenants require data to stay in a specific region, others allow global storage. Design an AWS-based solution to enforce per-tenant data locality using region-bound S3 buckets, cross-account IAM boundaries, and automated onboarding/offboarding. Include data migration, lifecycle management, auditing, and failure handling?","answer":"Assign each tenant to a dedicated region-bound S3 bucket with a per-tenant CMK and strict bucket policy to prevent cross-region access. Map tenants to separate AWS accounts via Organizations and apply","explanation":"## Why This Is Asked\n\nTests ability to design compliant tenancy boundaries, automation, data migration, and auditability in AWS.\n\n## Key Concepts\n\n- Data residency per tenant\n- Region-bound S3 buckets and CMKs\n- Cross-account IAM boundaries and SCPs\n- AWS Organizations mapping and account provisioning\n- Data migration paths (DataSync) and lifecycle\n- Auditing with Config and CloudTrail\n\n## Code Example\n\n```bash\n# Onboard tenant example (high level)\naws s3 mb s3://tenant-a-us-east-1 --region us-east-1\naws kms create-key --description \"Tenant A CMK\" --tags Key=Tenant,Value=A\n```\n\n## Follow-up Questions\n\n- How would you test onboarding in CI/CD and ensure data locality remains intact post-migration?\n- What failure modes would you simulate to validate rollback and audit trails?","diagram":null,"difficulty":"intermediate","tags":["aws"],"channel":"aws","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Netflix","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T13:36:40.560Z","createdAt":"2026-01-17T13:36:40.560Z"},{"id":"q-3473","question":"Design a cross-region DR plan for a 2-region, multi-account deployment with Aurora PostgreSQL and a large analytics pipeline. Target RPO < 5 min and RTO < 10 min. Propose architecture using Aurora Global Database, S3 cross-region replication, cross-account IAM, Route 53 failover, and IaC automation. Include backups, testing, and rollback?","answer":"Propose a two-region DR with primary in us-east-1 and disaster region in us-west-2. Use Aurora Global Database for near-zero-latency OLTP replication to the secondary (read-only). Backups in S3 with c","explanation":"## Why This Is Asked\n\nTests a candidate's ability to design robust DR across regions, balancing RPO/RTO with service dependencies, and to specify concrete AWS primitives and automation.\n\n## Key Concepts\n\n- Aurora Global Database limitations: writes occur in the primary region; cross-region replicas are read-only until failover.\n- DR targets: translating RPO < 5 min and RTO < 10 min into architecture and automation.\n- Backups/Replication: S3 cross-region replication, immutable backups, and lifecycle.\n- IAM boundaries and DNS: cross-account IAM roles, Route 53 health checks and failover, IaC-driven promotion/rollback.\n\n## Code Example\n\n```javascript\n// illustrative CDK-like snippet for Global Cluster (not production-ready)\nconst globalCluster = new aws_rds.GlobalCluster(this, 'ProdGlobal', {\n  globalClusterIdentifier: 'prod-global',\n  engine: 'aurora-postgresql'\n});\n```\n\n## Follow-up Questions\n\n- How would you implement quarterly non-disruptive failover tests?\n- What cost controls and monitoring would you add to prevent runaway DR expenses?","diagram":null,"difficulty":"advanced","tags":["aws"],"channel":"aws","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Hashicorp"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T17:39:53.769Z","createdAt":"2026-01-17T17:39:53.770Z"},{"id":"q-3661","question":"Advanced AWS design: You run a multi-tenant analytics SaaS where per-tenant data residency is mandatory (data must stay in the tenant's region), but global dashboards must aggregate across tenants. Describe an end-to-end architecture using AWS services such as S3 IAM KMS Lambda Glue Athena Redshift API Gateway Cognito QuickSight that enforces per-tenant locality, supports automated on boarding off boarding, cross region analytics replication, and a disaster recovery strategy with defined RTO and RPO. Include security cost and scalability considerations?","answer":"Per-tenant region buckets enforced by IAM policies; data ingested regionally via Kinesis Firehose into tenant buckets; Glue catalog; Athena/Redshift Spectrum on tenant data; dashboards across regions ","explanation":"## Why This Is Asked\n\nTests ability to design data residency controls, cross-region analytics, and automated tenant lifecycle in a scalable, secure way. Requires reasoning about IAM boundaries, per-tenant data segregation, and disaster recovery trade-offs.\n\n## Key Concepts\n\n- Data residency and tenant isolation\n- Region-bound storage and access controls\n- Serverless analytics stack (Lambda, Glue, Athena/Redshift)\n- Automated onboarding/offboarding (CDK/Terraform)\n- Cross-region dashboards and DR strategy (RTO/RPO)\n\n## Code Example\n\n```javascript\n// CDK sketch (simplified)\nconst bucket = new s3.Bucket(this, 'TenantBucket', {\n  encryption: s3.BucketEncryption.S3_MANAGED,\n  removalPolicy: RemovalPolicy.RETAIN\n});\nbucket.addToResourcePolicy(new PolicyStatement({\n  principals: [new ArnPrincipal('*')],\n  actions: ['s3:*'],\n  resources: [bucket.bucketArn + '/*'],\n  conditions: { StringEquals: { 'aws:PrincipalOrgID': 'o-abc' } }\n}));\n```\n\n## Follow-up Questions\n\n- How would you validate data residency across tenants in CI/CD?\n- What a/b tests would you run to compare cross-region analytics performance?","diagram":null,"difficulty":"advanced","tags":["aws"],"channel":"aws","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Discord","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T04:14:39.418Z","createdAt":"2026-01-18T04:14:39.418Z"},{"id":"q-3763","question":"You operate a global data lake for a rideshare platform. Ingest tens of thousands of events per second from multiple tenants. Design an AWS-native pipeline that enforces per-tenant isolation, supports on-demand tenant data export to customer-owned accounts, and handles schema evolution and retention. Describe ingress, storage, cataloging, access control, and auditing?","answer":"Ingest via Kinesis Data Streams or EventBridge into tenant-scoped prefixes in S3; enforce isolation with Lake Formation permissions and cross-account roles; encrypt at rest with per-tenant CMKs; catal","explanation":"## Why This Is Asked\nTests real-world multi-tenant data lake design with isolation, encryption, and auditing.\n\n## Key Concepts\n- Multi-tenant isolation across accounts and prefixes\n- AWS Lake Formation permissions and cross-account roles\n- KMS CMKs per tenant and secure data export\n- Glue Data Catalog, Schema Registry, and lifecycle management\n\n## Code Example\n```javascript\n// Example: cross-account role trust (illustrative)\nconst trustPolicy = {\n  Version: '2012-10-17',\n  Statement: [{ Effect: 'Allow', Principal: { AWS: 'arn:aws:iam::TENANT:root' }, Action: 'sts:AssumeRole' }]\n};\n```\n\n## Follow-up Questions\n- How would you monitor tenant data egress and enforce quotas?\n- How do you evolve schemas without breaking analytics consumers?\n- How would you validate isolation boundaries during deployments?","diagram":"flowchart TD\n  Ingest[Ingest Layer: Kinesis/EventBridge]\n  Route[Route to Tenant Prefix in S3]\n  Catalog[Glue Catalog + Lake Formation]\n  Analyze[Analytics/BI]\n  Export[On-demand Tenant Export]\n  Ingest --> Route --> Catalog --> Analyze\n  Catalog --> Export","difficulty":"intermediate","tags":["aws"],"channel":"aws","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Microsoft","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T08:44:27.586Z","createdAt":"2026-01-18T08:44:27.587Z"},{"id":"q-3789","question":"Design a tamper-evident, multi-account, multi-region AWS logging pipeline for a high-traffic service. The system must store raw logs in per-region S3 with object lock and versioning, replicate to a central analytics account for long-term retention, use cross-account IAM roles, and provide auditable provenance with CloudTrail data events. Outline components, data flow, failure modes, and cost considerations?","answer":"Per-region S3 with Versioning and Object Lock for immutability; use cross-region replication to a central analytics bucket in a separate account; ship logs via Kinesis Firehose or CloudWatch Logs with","explanation":"## Why This Is Asked\nTo assess practical AWS security, data protection, cross-account boundaries, and cost/ops tradeoffs in a real-world logging pipeline.\n\n## Key Concepts\n- Data immutability (Object Lock, versioning)\n- Cross-account IAM roles and trust\n- S3 CRR, CloudTrail data events\n- Centralized analytics account and auditing\n- Monitoring and cost management\n\n## Code Example\n```javascript\n// Example: sample IAM trust policy snippet\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\"Service\": \"s3.amazonaws.com\"},\n      \"Action\": \"sts:AssumeRole\",\n      \"Condition\": {\"StringEquals\": {\"sts:ExternalId\": \"LOGS-CENTER-123\"}}\n    }\n  ]\n}\n```\n\n## Follow-up Questions\n- How would you handle failed deliveries from regional buckets?\n- What cost-optimization strategies would you apply for long-term retention across regions?","diagram":"flowchart TD\n  A[Per-region S3 bucket (immutability)] --> B[Central analytics bucket]\n  A --> C[Replication to central account]\n  B --> D[Analytics queries (Athena/Glue)]\n  E[Audit: CloudTrail data events] --> F[Compliance store]","difficulty":"intermediate","tags":["aws"],"channel":"aws","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Meta","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T09:38:20.644Z","createdAt":"2026-01-18T09:38:20.644Z"},{"id":"q-3937","question":"You operate a multi-account AWS SaaS where each tenant's data must stay within its designated region. Propose a practical onboarding/offboarding flow that enforces per-tenant region isolation using per-tenant S3 buckets in the correct region, cross-account IAM boundaries, and automated remediation for cross-region writes. Include: policy guardrails (SCPs), event-driven detection (EventBridge + Config), data migration paths, and auditing?","answer":"Onboard: provision per-tenant S3 bucket in the tenant's region, attach cross-account IAM roles, and apply an SCP to deny cross-region writes. Enforcement: EventBridge rules trigger a Config custom rul","explanation":"## Why This Is Asked\nTests practical data residency enforcement across accounts. It probes guardrails, automation, and auditability.\n\n## Key Concepts\n- SCPs for region isolation\n- Per-tenant bucket provisioning in Region\n- EventBridge + Config for detection\n- Lambda remediation and data migration\n- Auditing with CloudTrail\n\n## Code Example\n```javascript\n// Placeholder snippet: IAM policy snippet enforcing region-limited access\n```\n\n## Follow-up Questions\n- How would you test the remediation path? \n- How would you handle tenants with dynamic region requirements?","diagram":"flowchart TD\nA[Onboard Tenant] --> B{Provision bucket in tenant region}\nB --> C[Attach cross-account roles]\nC --> D{Detect cross-region writes}\nD --> E[Remediate: copy data & quarantine]\nE --> F[Audit: CloudTrail/Config]\nF --> G[Offboard: revoke, snapshot, delete]\n","difficulty":"intermediate","tags":["aws"],"channel":"aws","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Apple","Discord","LinkedIn"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T16:34:20.636Z","createdAt":"2026-01-18T16:34:20.636Z"},{"id":"q-4216","question":"Design a scalable, multi-tenant ingestion pipeline where each tenant's data is isolated, region-bound, and access-controlled across accounts. Use EventBridge, Lambda, S3, and Glue with Lake Formation. Include onboarding/offboarding, per-tenant CMKs, usage metering, and robust failure handling; also describe testing tenant isolation and failure scenarios?","answer":"Design a scalable, multi-tenant ingestion pipeline with strict isolation: per-tenant region-bound S3 buckets, Lake Formation access control, and cross-account EventBridge/Lambda processing. Include au","explanation":"## Why This Is Asked\nThis question probes practical design for multi-tenant data governance, cross-account IAM, Lake Formation permissions, per-tenant KMS keys, and automated onboarding—core for compliant services at scale.\n\n## Key Concepts\n- Tenant isolation across accounts\n- Region-bound storage and CMK rotation\n- Lake Formation access controls and data cataloging\n- EventBridge/Lambda data flow and failure handling\n- Metering, onboarding/offboarding, auditing\n\n## Code Example\n```javascript\n// Pseudo-CDK snippet illustrating per-tenant resources\nconst bucket = new s3.Bucket(this, 'TenantBucket', {\n  encryption: s3.BucketEncryption.KMS,\n  encryptionKey: kmsKey,\n  removalPolicy: RemovalPolicy.DESTROY\n});\n```\n\n## Follow-up Questions\n- How would you validate zero-data leakage during onboarding/offboarding?\n- What monitoring would you implement to detect cross-tenant access anomalies?","diagram":"flowchart TD\n  A[Ingest] --> B[EventBridge]\n  B --> C[Lambda]\n  C --> D[S3 region-bound bucket]\n  D --> E[Glue jobs]","difficulty":"intermediate","tags":["aws"],"channel":"aws","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Amazon","Bloomberg","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T09:07:10.125Z","createdAt":"2026-01-19T09:07:10.125Z"},{"id":"q-4295","question":"You operate a multi-tenant AI inference platform on AWS with per-tenant data locality, strict auditability, and dynamic model routing. Design an end-to-end architecture for real-time inferences that ensures per-tenant isolation, region-bound data, tenant-based quotas, and disaster recovery across regions. Describe data paths, deployment strategy, failure modes, and monitoring?","answer":"Design per-tenant isolation by deploying region-scoped SageMaker endpoints for each tenant, backed by a regional S3 data lake with Lake Formation policies. Enforce region boundaries with SCPs, STS Ass","explanation":"## Why This Is Asked\n\nAssesses ability to architect multi-tenant, region-aware inference systems with strong isolation, data residency, and resilience. Demonstrates integration of inference, data governance, and DR planning.\n\n## Key Concepts\n\n- Multi-account/region tenancy and SCPs\n- SageMaker endpoints for per-tenant inference\n- Lake Formation and region-bound S3 data lakes\n- IAM roles, STS, and cross-account access\n- Canary deployments, quotas with DynamoDB, and autoscaling\n- DR across regions with S3 replication and NLB/Route 53 failover\n- Monitoring, logging, and auditing with CloudWatch, CloudTrail, AWS Config\n\n## Code Example\n\n```javascript\n// CDK pseudo-code: create per-tenant endpoint in tenant's region\nimport * as sagemaker from '@aws-cdk/aws-sagemaker';\nnew sagemaker.CfnEndpoint(this, 'TenantEndpoint', {\n  endpointConfigName: 'TenantA-EndpointConfig',\n  endpointName: 'TenantA-Endpoint'\n});\n```\n\n## Follow-up Questions\n\n- How would you enforce per-tenant quotas and secure model updates without downtime?\n- How would you test cross-region failover for a tenant without impacting others?","diagram":"flowchart TD\n  TenantIsolation[Tenant Isolation] --> Endpoint[Regional SageMaker Endpoint]\n  Endpoint --> DataLake[Regional S3 Data Lake]\n  DataLake --> Audit[Audit & Compliance]\n  Endpoint --> Monitoring[Monitoring & Alerts]","difficulty":"advanced","tags":["aws"],"channel":"aws","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Anthropic","Discord"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T11:48:36.805Z","createdAt":"2026-01-19T11:48:36.805Z"},{"id":"q-4363","question":"Design a beginner-friendly AWS setup: static frontend on S3+CloudFront, API via API Gateway to Lambda, and data in DynamoDB. Explain how to enforce least-privilege IAM roles, isolate environments with separate buckets, and implement a simple backup/restore plan. Include a minimal IAM policy for Lambda to read DynamoDB and write to S3?","answer":"Outline a minimal, cost-aware architecture: frontend on S3+CloudFront, API Gateway + Lambda, DynamoDB for data. Show least-privilege IAM roles, separate S3 buckets per environment, and a basic backup/","explanation":"## Why This Is Asked\n\nTests practical AWS knowledge at a beginner level, focusing on core services (S3, CloudFront, API Gateway, Lambda, DynamoDB), plus security (least privilege) and basic backups.\n\n## Key Concepts\n\n- Static hosting with S3 and CloudFront\n- API exposure via API Gateway\n- Serverless compute with Lambda\n- Data layer with DynamoDB\n- IAM least-privilege roles and environment isolation\n- Basic backup/restore strategy (point-in-time, versioning)\n\n## Code Example\n\n```json\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\"dynamodb:GetItem\", \"dynamodb.Query\", \"dynamodb.Scan\"],\n      \"Resource\": \"arn:aws:dynamodb:REGION:ACCOUNT_ID:table/YourTable\"\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\"s3:PutObject\", \"s3:GetObject\"],\n      \"Resource\": \"arn:aws:s3:::your-bucket-name/*\"\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\"s3:ListBucket\"],\n      \"Resource\": \"arn:aws:s3:::your-bucket-name\"\n    }\n  ]\n}\n```\n\n## Follow-up Questions\n\n- How would you test and validate the IAM permissions in a dev environment?\n- How would the setup adapt to growing traffic (DynamoDB capacity, Lambda concurrency, caching)?","diagram":"flowchart TD\n  A[User] --> B[CloudFront]\n  B --> C[S3:Static Assets]\n  D[API Gateway] --> E[Lambda]\n  E --> F[DynamoDB]","difficulty":"beginner","tags":["aws"],"channel":"aws","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Apple","Cloudflare","Databricks"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T15:52:15.261Z","createdAt":"2026-01-19T15:52:15.261Z"},{"id":"q-4499","question":"Design a GPU-accelerated inference platform for multi-tenant workloads on AWS with data residency guarantees for each tenant. Compare using SageMaker endpoints (per-tenant or shared) vs an EKS cluster with Nvidia GPUs. Include onboarding/offboarding, model/versioning, tenant quotas, cost controls, auto-scaling, failure handling, monitoring, and auditability?","answer":"Propose two paths: (1) SageMaker multi-tenant with per-tenant endpoints or model packages, plus tenant-scoped S3 in regional buckets and KMS; (2) an EKS cluster with Nvidia GPUs (p4d/g5) using Kubernetes device plugins and tenant namespaces with resource quotas. For onboarding, automate tenant creation via CloudFormation/CDK with IAM roles, S3 buckets, and KMS keys. Model versioning uses SageMaker Model Registry or GitOps with ArgoCD. Implement tenant quotas via SageMaker endpoint limits or Kubernetes ResourceQuotas. Cost controls use AWS Budgets and tagging. Auto-scaling leverages SageMaker endpoint auto-scaling or Kubernetes HPA with custom metrics. Failure handling includes multi-AZ deployment, health checks, and circuit breakers. Monitoring uses CloudWatch, Prometheus, and Grafana. Auditability achieved through CloudTrail, S3 access logs, and tenant-specific logging.","explanation":"## Why This Is Asked\nThis question tests practical design for compliant, cost-aware GPU inference at multi-tenant scale on AWS, balancing isolation, latency, and governance.\n\n## Key Concepts\n- Multi-tenant isolation, tenancy models (per-tenant endpoint vs shared)\n- GPU runtimes on AWS (SageMaker vs EKS with NVIDIA Operator)\n- Data residency, KMS keys, tenant buckets, RBAC\n- Onboarding/offboarding automation, versioning, quotas\n- Observability and auditing (CloudWatch, CloudTrail, S3 logs)\n\n## Code Example\n```json\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"s3:GetObject\",\n        \"s3:PutObject\"\n      ],\n      \"Resource\": \"arn:aws:s3:::tenant-${aws:username}/*\"\n    }\n  ]\n}\n```","diagram":"flowchart TD\n  A[Inbound Request] --> B[Gateway/Ingress]\n  B --> C{Path}\n  C --> D[SageMaker Endpoint (per-tenant)]\n  C --> E[EKS with Nvidia GPUs]\n  D --> F[S3 region bucket]\n  E --> F\n  F --> G[Inference Result]\n  G --> H[Auditing (CloudTrail/S3 logs)]","difficulty":"intermediate","tags":["aws"],"channel":"aws","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Amazon","NVIDIA","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T07:53:04.116Z","createdAt":"2026-01-19T21:37:38.417Z"},{"id":"q-454","question":"You need to host a static website with high availability and low latency globally. How would you configure AWS S3 and CloudFront to achieve this?","answer":"Configure an S3 bucket with static website hosting enabled, implement versioning for backup protection, and block public access. Create a CloudFront distribution using the S3 bucket as the origin, set up an Origin Access Identity (OAI) for secure access, enable appropriate caching rules, and configure geographic distribution for global performance.","explanation":"## S3 Configuration\n- Enable static website hosting for the bucket\n- Configure bucket policy to allow CloudFront OAI access\n- Enable versioning for backup and recovery capabilities\n- Block all public access for enhanced security\n\n## CloudFront Setup\n- Create distribution with S3 bucket as origin\n- Configure Origin Access Identity for secure S3 access\n- Set optimal cache TTLs for different asset types\n- Enable compression to reduce payload sizes\n- Configure geographic edge locations for global distribution\n\n## Best Practices\n- Implement HTTPS with AWS Certificate Manager\n- Configure custom domain names\n- Set up comprehensive logging and monitoring\n- Implement proper error handling and custom error pages","diagram":"flowchart TD\n  A[User Request] --> B[CloudFront Edge]\n  B --> C[Cache Hit?]\n  C -->|Yes| D[Return Cached]\n  C -->|No| E[S3 Origin]\n  E --> F[Return Content]\n  F --> B","difficulty":"beginner","tags":["aws"],"channel":"aws","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Google","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":["cloudfront","oai","static website hosting","caching","versioning"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2026-01-08T11:44:13.190Z","createdAt":"2025-12-26 12:51:05"},{"id":"q-4565","question":"Design a per-tenant data isolation model for a multi-tenant data lake on AWS with strict data residency. Use region-bound S3 buckets, Lake Formation, and IAM roles to enforce access; implement per-tenant CMKs, Glue catalog scoping, and automated onboarding/offboarding. Include cross-account sharing, CloudTrail/Lake Formation audit, and DR with regional replication?","answer":"Implement per-tenant S3 buckets in each region to enforce data residency, Lake Formation permissions scoped to databases and tables for each tenant, and IAM conditional principals to enforce access; use tenant-specific KMS CMKs for envelope encryption, implement automated onboarding/offboarding via CloudFormation/CDK, enable cross-account sharing through Lake Formation resource links, ensure comprehensive audit with CloudTrail and Lake Formation access logs, and establish disaster recovery with regional replication using S3 Cross-Region Replication.","explanation":"## Why This Is Asked\nAssesses capability to design tenant isolation, data residency, and governance at scale using AWS services.\n\n## Key Concepts\n- Region-bound storage for residency compliance\n- Fine-grained access with Lake Formation and IAM conditions\n- Envelope encryption with per-tenant CMKs\n- Automated onboarding/offboarding and disaster recovery\n\n## Code Example\n```json\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\"s3:GetObject\",\"s3:ListBucket\"],\n      \"Resource\": [\n        \"arn:aws:s3:::tenant-eu-tenantA\",\n        \"arn:aws:s3:::tenant-eu-tenantA/*\"\n      ],\n      \"Condition\": {\n        \"StringEquals\": {\n          \"aws:SourceVpce\": \"vpce-12345\"\n        }\n      }\n    }\n  ]\n}\n```","diagram":"flowchart TD\n  TenantRequest --> ProvisionBucket\n  ProvisionBucket --> GrantAccess\n  GrantAccess --> Audit\n  Audit --> DR","difficulty":"advanced","tags":["aws"],"channel":"aws","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Oracle","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T06:12:04.197Z","createdAt":"2026-01-20T00:01:57.628Z"},{"id":"q-4642","question":"Design a latency-sensitive multi-tenant real-time analytics pipeline on AWS for trading data: ingest from venues, deduplicate and enrich events, enforce per-tenant data locality, and provide tamper-evident audit trails with on-demand queries and cost controls. Which services, data models, and deployment patterns would you use, and how would you handle exactly-once processing, retries, outages, and tenant onboarding?","answer":"Design a latency-sensitive, multi-tenant real-time analytics pipeline on AWS for trading data: ingest from venues via Kinesis Data Streams, deduplicate and enrich in Lambda or Kinesis Data Analytics, ","explanation":"## Why This Is Asked\nTests ability to design real-time, multi-tenant analytics with data locality and governance in AWS at scale.\n\n## Key Concepts\n- Real-time ingestion with Kinesis\n- Deduplication and enrichment pipelines\n- Per-tenant data locality and isolation\n- Tamper-evident audits (CloudTrail, S3 Object Lock, immutable logs)\n- Exactly-once processing and idempotency\n- Cost controls and on-demand query patterns\n\n## Code Example\n```javascript\n// AWS CDK snippet creating a Kinesis stream and a Lambda consumer\nconst stream = new kinesis.Stream(this, 'IngestStream', { shardCount: 4 });\nconst fn = new lambda.Function(this, 'DedupEnrich', {\n  runtime: lambda.Runtime.NODEJS_18_X,\n  handler: 'index.handler',\n  code: lambda.Code.fromInline(`exports.handler = async (event) => { /* idempotent path */ };`)\n});\nstream.grantRead(fn);\n```\n\n## Follow-up Questions\n- How would you onboard a new tenant without impacting existing workloads?\n- What monitoring would you implement to detect skew and late data?\n","diagram":null,"difficulty":"advanced","tags":["aws"],"channel":"aws","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Google","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T06:07:20.890Z","createdAt":"2026-01-20T06:07:20.890Z"},{"id":"q-4706","question":"Design a multi-tenant data lake on AWS where each tenant's data must be isolated, with dynamic onboarding/offboarding and strict data residency. Propose an architecture using AWS Lake Formation, per-tenant S3 Access Points, and per-tenant KMS CMKs; include cross-account IAM/SCPs, and auditing. Describe the data access grant/revoke workflow, automation (CDK/CloudFormation), and monitoring to enforce compliance?","answer":"Leverage Lake Formation to register per-tenant databases and grant access via STS-assumed roles, with a dedicated S3 Access Point and a per-tenant KMS CMK to enforce data residency. Automate onboardin","explanation":"## Why This Is Asked\n\nTests design of scalable, isolated, auditable data lakes for multiple tenants with lifecycle automation and strict residency controls. Also probes cross-account governance and observability.\n\n## Key Concepts\n\n- AWS Lake Formation: fine-grained permissions, data cataloging\n- S3 Access Points: tenant-level isolation\n- KMS CMKs: per-tenant encryption and residency\n- IAM Roles + STS: cross-account access\n- SCPs and cross-account governance\n- CDK/CloudFormation: automated onboarding/offboarding\n- Observability: CloudTrail, Lake Formation logs, Config, Athena\n\n## Code Example\n\n```javascript\n// CDK snippet: create per-tenant access role and attach Lake Formation permissions\n```\n\n## Follow-up Questions\n\n- How would you test onboarding/offboarding for residual access?\n- How would you handle data residency violations across regions?","diagram":"flowchart TD\n  A[Tenant] --> B[Lake Formation Registry]\n  B --> C[S3 Access Point per Tenant]\n  C --> D[Data Lake Buckets (region-bound)]\n  D --> E[KMS CMK per Tenant]\n  F[Audit & Compliance] --> G[CloudTrail/Lake Formation Logs]","difficulty":"advanced","tags":["aws"],"channel":"aws","subChannel":"general","sourceUrl":null,"videos":null,"companies":["IBM","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T09:07:39.017Z","createdAt":"2026-01-20T09:07:39.017Z"},{"id":"q-4721","question":"Design a beginner-friendly AWS setup for a small app: static frontend on S3 + CloudFront and a REST API backed by Lambda that reads/writes to DynamoDB. Place the Lambda in private subnets behind API Gateway, use a DynamoDB VPC endpoint, and keep cost and security in mind. Describe the VPC structure, IAM least-privilege, and a simple test plan to verify connectivity and data operations?","answer":"Place Lambda in private subnets of a single VPC across two AZs; API Gateway invokes the Lambda; enable a DynamoDB gateway endpoint in the VPC; add a NAT Gateway in a public subnet for any outbound nee","explanation":"## Why This Is Asked\nTests understanding of VPC design basics, Lambda in VPC, private subnets, endpoints, and IAM in a practical, low-risk setup.\n\n## Key Concepts\n- VPC subnets across AZs\n- Lambda in VPC with ENIs\n- DynamoDB VPC endpoint and private connectivity\n- Security groups and least privilege IAM roles\n- Testing and observability (CloudWatch)\n\n## Code Example\n```json\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\"dynamodb:GetItem\",\"dynamodb:PutItem\"],\n      \"Resource\": \"arn:aws:dynamodb:REGION:ACCOUNT_ID:table/YourTable\"\n    }\n  ]\n}\n```\n\n## Follow-up Questions\n- How would you rotate the Lambda execution role and manage permissions across environments?\n- What failure modes exist if the DynamoDB endpoint becomes unavailable, and how would you mitigate them?","diagram":null,"difficulty":"beginner","tags":["aws"],"channel":"aws","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Lyft","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T09:55:02.039Z","createdAt":"2026-01-20T09:55:02.039Z"},{"id":"q-4801","question":"In a multi-tenant AWS SaaS, each tenant can opt into maintenance windows and canaries. Design an automated deployment strategy that (1) performs per-tenant blue/green releases across regions, (2) routes traffic with per-tenant weighted API Gateway stages and Route 53 latency-based routing, (3) drains a tenant's traffic during maintenance without affecting others, and (4) audits changes with CloudTrail and Lake Formation. Include required IAM roles, data isolation, and rollback plan?","answer":"Use per-tenant canaries with region-scoped API Gateway stages and Lambda aliases. Route 53 weighted routing shifts tenants between regions during maintenance. Keep a per-tenant flag in DynamoDB to dis","explanation":"## Why This Is Asked\n\nAssesses practical multi-region, multi-tenant deployment strategy with per-tenant control, traffic routing, and auditability.\n\n## Key Concepts\n\n- Per-tenant blue/green releases across regions\n- API Gateway stages and Lambda aliases for tenant isolation\n- Route 53 weighted routing and latency-based routing\n- Maintenance window orchestration and traffic drainage\n- IAM least privilege and data isolation; CloudTrail + Lake Formation auditing\n\n## Code Example\n\n```yaml\nResources:\n  TenantLambdaAliasLive:\n    Type: AWS::Lambda::Alias\n    Properties:\n      FunctionName: !Ref TenantLambda\n      FunctionVersion: !GetAtt TenantLambda.Version\n      Name: live\n\n  TenantApiStageLive:\n    Type: AWS::ApiGatewayV2::Stage\n    Properties:\n      StageName: live\n      AutoDeploy: true\n\n  TenantRecord:\n    Type: AWS::Route53::RecordSet\n    Properties:\n      Name: tenant.example.com\n      Type: A\n      AliasTarget:\n        HostedZoneId: Z2ABCDEFGXYZ\n        DNSName: d1234abcd.execute-api.us-east-1.amazonaws.com\n```\n\n## Follow-up Questions\n\n- How would you handle tenant onboarding/offboarding without data leakage?\n- How would you test the rollback in a production-like environment before rollout?","diagram":null,"difficulty":"intermediate","tags":["aws"],"channel":"aws","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Adobe","Airbnb","Citadel"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T13:28:18.854Z","createdAt":"2026-01-20T13:28:18.854Z"},{"id":"q-4819","question":"Design a compliant, scalable real-time analytics pipeline for a global fintech with per-tenant data residency. Each tenant's raw data stays in their region; aggregated insights may be centralized. Propose an AWS-based solution using Kinesis Data Streams, Lake Formation/S3, Glue, Athena, and IAM/KMS. Include data partitioning, tenancy isolation, onboarding/offboarding, access auditing, and cost controls?","answer":"Implement per-tenant streams in each region feeding to region-local S3 via Firehose, with Lake Formation catalog and KMS keys per tenant. Use tenant-scoped IAM roles, strict resource policies, and reg","explanation":"## Why This Is Asked\nAssess practical knowledge of multi-region data isolation, streaming ingestion, cataloging, per-tenant security, and cost controls using real AWS services.\n\n## Key Concepts\n- Per-tenant isolation and residency\n- Lake Formation catalog and KMS keys per tenant\n- In-region streaming ingestion via Kinesis/Firehose\n- Cross-region aggregates and Athena querying\n- Onboarding/offboarding, retention, auditing, and cost governance\n\n## Follow-up Questions\n- How would you test data residency controls end-to-end?\n- How would you handle schema evolution across tenants?","diagram":null,"difficulty":"advanced","tags":["aws"],"channel":"aws","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Coinbase","Goldman Sachs","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T14:57:41.283Z","createdAt":"2026-01-20T14:57:41.283Z"},{"id":"q-4838","question":"You're operating a data-intensive analytics pipeline on AWS that ingests millions of events per minute from multiple regions into a central data lake. Design an end-to-end solution using Kinesis (or MSK), S3, Lambda/Fargate, and DynamoDB. Include data locality, fault tolerance, replay semantics, idempotency, and observability. What are your trade-offs?","answer":"Use multi-region streams with per-tenant shards, publish raw records to regional S3 buckets and a central data lake, and process with Lambda/Fargate. Ensure idempotent sinks using tenantId+eventId in ","explanation":"## Why This Is Asked\nTests end-to-end AWS streaming design, data locality, replay semantics, and observability across regions.\n\n## Key Concepts\n- Data locality, replay, idempotency, cross-region replication, shard throttling, and cost trade-offs.\n- Durability vs latency in KDS/Kafka vs Firehose patterns.\n- Observability via CloudWatch, X-Ray, and structured metrics.\n\n## Code Example\n```javascript\n// Pseudo-idempotent sink to DynamoDB\nasync function sinkEvent(event){\n  const key = event.tenantId + '#' + event.id;\n  await dynamo.put({\n    TableName: 'TenantEvents',\n    Item: { PK: key, ...event },\n    ConditionExpression: 'attribute_not_exists(PK)'\n  }).catch(err => {\n    if (err.code === 'ConditionalCheckFailedException') return; // idempotent retry\n    throw err\n  })\n}\n```\n\n## Follow-up Questions\n- How would you handle shard throttling and bursty traffic?\n- How would you onboard new tenants without impacting latency or isolation?","diagram":"flowchart TD\n  Ingest[Regional Ingest] --> Process[Stream Processor]\n  Process --> RawS3[Raw Data in S3]\n  RawS3 --> Lake[Central Data Lake]\n  Process --> MetaDynamo[DynamoDB: Metadata & Idempotency Keys]\n  Lake --> Coord[Observability: CloudWatch/X-Ray]","difficulty":"advanced","tags":["aws"],"channel":"aws","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Cloudflare","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T15:52:45.508Z","createdAt":"2026-01-20T15:52:45.508Z"},{"id":"q-484","question":"You're designing a real-time ML inference pipeline on AWS that must process 10,000 requests/second with sub-100ms latency. How would you architect this using serverless components, and what trade-offs would you consider?","answer":"I would architect a serverless ML inference pipeline using API Gateway for request routing and authentication, Lambda functions with provisioned concurrency to eliminate cold starts and ensure sub-100ms latency, Elastic Container Service (ECS) Fargate with GPU instances for heavy ML inference workloads, CloudFront for edge caching of model predictions and static assets, and DynamoDB for low-latency metadata storage with auto-scaling capabilities.","explanation":"## Architecture Components\n- **API Gateway**: Manages request routing, throttling, authentication, and provides HTTP endpoints\n- **Lambda with Provisioned Concurrency**: Guarantees sub-100ms response times by pre-warming functions and eliminating cold starts\n- **ECS Fargate with GPU**: Handles compute-intensive ML inference tasks with GPU acceleration\n- **CloudFront**: Provides edge caching for frequently requested predictions and static model assets\n- **DynamoDB**: Stores request metadata with single-digit millisecond latency and automatic scaling\n\n## Key Trade-offs\n- **Cost vs Performance**: Provisioned concurrency significantly increases costs but ensures consistent sub-100ms latency\n- **Stateless Design**: Lambda functions must remain stateless to enable horizontal scaling and fault tolerance\n- **Memory Allocation**: Higher memory allocations improve performance but increase costs and may lead to underutilization\n- **Cold Start Management**: Provisioned concurrency eliminates cold starts but requires capacity planning\n- **Data Consistency**: DynamoDB offers eventual consistency by default, requiring careful design for strongly consistent requirements","diagram":"flowchart TD\n  A[Client Request] --> B[CloudFront Edge]\n  B --> C[API Gateway]\n  C --> D[Lambda Provisioned Concurrency]\n  D --> E{Model Size}\n  E -->|Small| F[Lambda ML Runtime]\n  E -->|Large| G[ECS GPU Container]\n  F --> H[DynamoDB Metadata]\n  G --> H\n  H --> I[Response Cache]\n  I --> J[Client Response]\n  C --> K[ SQS Queue]\n  K --> L[Async Lambda Processor]","difficulty":"advanced","tags":["aws"],"channel":"aws","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Hugging Face","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-08T11:58:10.126Z","createdAt":"2025-12-26 12:51:06"},{"id":"q-4949","question":"Design a scalable, multi-tenant data lake on AWS across accounts and regions that enforces per-tenant isolation using Lake Formation, LF-tags, and region-bound S3 buckets. Include automated onboarding/offboarding, cross-account IAM scopes, per-tenant KMS keys, and full auditing. How would you ensure data residency while preserving analytics performance (Athena/Glue)?","answer":"Leverage AWS Lake Formation for per-tenant catalogs and LF-tags, with cross-account IAM roles scoped to LF-tags. Use region-specific S3 buckets per tenant and tenant KMS CMKs for encryption. Automate ","explanation":"## Why This Is Asked\n\nThis question probes knowledge of Lake Formation, cross-account access, data residency, and automation in onboarding/offboarding for a data lake at scale, reflecting real-world multi-tenant SaaS needs.\n\n## Key Concepts\n\n- Lake Formation permissions, LF-tags, cross-account access\n- S3 bucket scoping per tenant, region residency\n- KMS customer-managed keys per tenant\n- Automated onboarding/offboarding (CFN/Terraform, SCPs)\n- Auditing (CloudTrail, Lake Formation logs, Glue/Athena histories)\n- Data catalog with resource links for cross-region analytics\n\n## Code Example\n\n```yaml\nAWSTemplateFormatVersion: '2010-09-09'\nResources:\n  TenantDataPermissions:\n    Type: AWS::LakeFormation::Permissions\n    Properties:\n      DataLakePrincipal:\n        DataLakePrincipalIdentifier: !Sub \"arn:aws:iam::${TenantAccount}:role/TenantReader\"\n      Resource:\n        TableResource:\n          DatabaseName: \"tenant_db\"\n          Name: \"tenant_table\"\n      Permissions:\n        - SELECT\n```\n\n## Follow-up Questions\n\n- How would you rotate per-tenant KMS keys and revoke access during offboarding?\n- How would you validate that LF-tags enforce correct data access across accounts?\n","diagram":"flowchart TD\n  A[Tenant Onboarding] --> B[Create per-tenant S3 bucket in region]\n  B --> C[Register with Lake Formation]\n  C --> D[Grant LF-tag-based access to user roles across accounts]\n  D --> E[Configure Athena/Glue data catalog]\n  E --> F[Audit with CloudTrail + Lake Formation logs]\n  F --> G[On-demand analytics]\n  H[Tenant Offboarding] --> I[Revoke access, archive data to Glacier]","difficulty":"advanced","tags":["aws"],"channel":"aws","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Apple","Plaid","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T20:45:42.663Z","createdAt":"2026-01-20T20:45:42.663Z"},{"id":"q-5045","question":"You operate a multi-tenant SaaS API on API Gateway + Lambda. During peak hours, latency spikes and 5xx errors affect some tenants while others remain healthy. Provide a practical, production-ready troubleshooting and remediation plan that identifies root causes and stabilizes performance. Include metrics, tracing, throttling, concurrency, and a canary rollout with rollback criteria?","answer":"Implement comprehensive CloudWatch monitoring for p95 latency, error rates, throttle counts, and concurrency utilization, combined with AWS X-Ray tracing for end-to-end visibility. Enforce per-tenant usage plans and throttling limits in API Gateway to prevent noisy neighbor scenarios. Configure Lambda reserved concurrency per tenant and provisioned concurrency for critical workloads to ensure consistent performance. Deploy changes using canary releases with automated rollback triggers based on error rate thresholds exceeding 1% or latency degradation greater than 20% from baseline.","explanation":"## Why This Is Asked\nThis question assesses real-world incident response capabilities for serverless multi-tenant architectures, requiring systematic diagnosis of performance issues while maintaining service isolation between tenants.\n\n## Key Concepts\n- **CloudWatch metrics**: Monitor p95 latency, error rates, throttle counts, and concurrency utilization\n- **AWS X-Ray tracing**: Enable distributed system visibility for end-to-end request tracking\n- **API Gateway management**: Implement usage plans and per-tenant throttling to prevent noisy neighbor effects\n- **Lambda concurrency strategies**: Apply reserved concurrency per tenant and provisioned concurrency for critical workloads\n- **Canary deployments**: Execute gradual rollouts with automated rollback criteria based on performance thresholds\n- **Dependency analysis**: Identify downstream service bottlenecks and implement appropriate caching strategies","diagram":"flowchart TD\n  A[Client Request] --> B[API Gateway]\n  B --> C[Lambda]\n  C --> D[Downstream Service/DB]\n  D --> E[Response]","difficulty":"intermediate","tags":["aws"],"channel":"aws","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Citadel","IBM","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T05:08:22.269Z","createdAt":"2026-01-21T02:59:00.993Z"},{"id":"q-514","question":"You're building a serverless application that needs to process user uploads. How would you design an architecture using S3, Lambda, and API Gateway to handle file uploads securely and efficiently?","answer":"Use API Gateway with a Lambda authorizer for authentication, generate presigned S3 URLs for direct uploads to avoid proxying through Lambda. Trigger Lambda functions on S3 events for post-upload processing, and use S3 event notifications to initiate workflows like thumbnail generation or metadata extraction.","explanation":"## Architecture Overview\n- API Gateway handles HTTP requests and authentication\n- Lambda generates presigned URLs and processes files\n- S3 stores files securely with proper permissions\n\n## Key Components\n- **Presigned URLs**: Enable direct client uploads to S3, reducing Lambda costs and improving performance\n- **Lambda Authorizer**: Validates JWT tokens or API keys before granting access\n- **S3 Event Notifications**: Automatically trigger processing Lambda functions\n- **DynamoDB**: Store file metadata and processing status for tracking\n\n## Security Considerations\n- Enable S3 bucket policies with least privilege access\n- Use VPC endpoints for private connectivity when needed\n- Implement file type validation and size limits\n- Apply server-side encryption for data at rest","diagram":"flowchart TD\n  A[Client] -->|Upload Request| B[API Gateway]\n  B -->|Authorize| C[Lambda Authorizer]\n  C -->|Generate| D[Presigned S3 URL]\n  D -->|Direct Upload| E[S3 Bucket]\n  E -->|Event Notification| F[Processing Lambda]\n  F -->|Store Metadata| G[DynamoDB]\n  G -->|Response| H[Client]","difficulty":"beginner","tags":["aws"],"channel":"aws","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["OpenAI","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":["lambda","api gateway","presigned urls","serverless","event notifications"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2026-01-09T03:45:00.243Z","createdAt":"2025-12-26 12:51:06"},{"id":"q-5178","question":"Design a multi-tenant media-processing pipeline: each tenant owns an S3 bucket in their account for uploads; a central processing account runs Lambda to transcode and write results back to the tenant bucket. Explain onboarding/offboarding, cross-account access with AssumeRole, per-tenant KMS keys, bucket policies, least-privilege roles, and failure handling (DR and residency). Include concrete steps and service choices?","answer":"Onboard tenant: create a per-tenant S3 bucket and KMS CMK in the tenant's account; in the processing account, create an IAM role that can assume into the tenant account and access only that bucket and","explanation":"Why This Is Asked\\n\\nThis question probes tenancy boundaries across AWS accounts and how to enforce data residency and least-privilege access in a scalable, automated way.\\n\\nKey Concepts\\n\\n- Cross-account AssumeRole and STS\\n- Per-tenant S3 + CMK in tenant accounts\\n- Least-privilege IAM roles and bucket policies\\n- Data residency, rotation, and auditability\\n- DR/backup and cross-region considerations\\n\\nCode Example\\n\\n```javascript\\n// Trust policy snippet (JS object)\\nconst trustPolicy = {\\n  Version: '2012-10-17',\\n  Statement: [\\n    {\\n      Effect: 'Allow',\\n      Principal: { AWS: 'arn:aws:iam::<PROCESS_ACC>:root' },\\n      Action: 'sts:AssumeRole'\\n    }\\n  ]\\n};\\n```\\n\\n```javascript\\n// Bucket access policy snippet (JS object)\\nconst bucketPolicy = {\\n  Version: '2012-10-17',\\n  Statement: [\\n    {\\n      Effect: 'Allow',\\n      Action: ['s3:GetObject','s3:PutObject'],\\n      Resource: 'arn:aws:s3:::tenant-bucket/*'\\n    }\\n  ]\\n};\\n```\\n\\nFollow-up Questions\\n\\n- How would you monitor and enforce tenancy boundaries at scale?\\n- How would you test onboarding/offboarding and policy drift?","diagram":null,"difficulty":"intermediate","tags":["aws"],"channel":"aws","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Amazon","Apple","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T09:53:17.088Z","createdAt":"2026-01-21T09:53:17.088Z"},{"id":"q-5246","question":"You’re building a multi-tenant analytics platform on AWS where each tenant’s data sits in a dedicated S3 prefix and is encrypted at rest with per-tenant KMS keys. Design a scalable, automated strategy for rotating tenant data keys without downtime, while ensuring cross-account analytics workloads retain access, minimizing blast radius, and preserving auditability for compliance. Describe IAM/KMS policy structures, handling of data keys, and failure/recovery paths?","answer":"Leverage per-tenant envelope encryption: each tenant uses a CMK; data keys (DDKs) are generated via KMS and used to encrypt objects in the tenant’s S3 prefix. Rotate DDKs on a schedule; re-wrap with a","explanation":"## Why This Is Asked\n\nTests knowledge of encryption, key management, cross-account access, and operational rotation at scale with compliance demands.\n\n## Key Concepts\n\n- Envelope encryption\n- KMS CMKs and data keys\n- Key rotation strategies\n- Background re-encryption of existing data\n- IAM/bucket policies per tenant\n- Auditability with CloudTrail\n\n## Code Example\n\n```javascript\n// Pseudo: rotate tenant data keys\nconst AWS = require('aws-sdk');\nconst kms = new AWS.KMS();\nasync function rotateTenantDataKey(cmkArn) {\n  const { CiphertextBlob, Plaintext } = await kms.generateDataKey({ KeyId: cmkArn, KeySpec: 'AES_256' }).promise();\n  // Store CiphertextBlob in tenant metadata; use Plaintext to encrypt new data in memory (do not persist plaintext)\n}\n```\n\n## Follow-up Questions\n\n- How would you test rotation in CI/CD without impacting production?\n- How do you handle key revocation and tenant offboarding while preserving audits?","diagram":null,"difficulty":"advanced","tags":["aws"],"channel":"aws","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Databricks","Salesforce","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T13:11:21.452Z","createdAt":"2026-01-21T13:11:21.452Z"},{"id":"q-5306","question":"You operate a per-tenant telemetry ingestion service for autonomous vehicles with strict data residency per region. Design an AWS architecture that ingests raw telemetry per region, processes in near real-time, stores a tenant-scoped data lake in S3 with per-tenant partitions, and supports cross-region migration with zero-downtime. Include IAM cross-account boundaries, SSE-KMS CMKs, data retention policies, audit via CloudTrail, and failover handling?","answer":"Propose region-bound ingestion via Kinesis Data Streams, Lambda/Glue ETL, and a per-tenant S3 data lake with dynamic partitions. Use SSE-KMS CMKs per region and cross-account IAM roles; enable CloudTr","explanation":"## Why This Is Asked\nTests multi-region data residency, streaming ETL, and cross-account access in a production-grade, scalable telemetry pipeline.\n\n## Key Concepts\n- Real-time data ingestion with Kinesis\n- Data lake partitioning by tenant in S3\n- SSE-KMS CMKs and IAM boundaries\n- Cross-region replication and DR strategies\n- Auditing with CloudTrail; automated onboarding with CDK\n\n## Code Example\n```javascript\n// Example: CDK snippet sketch for region-scoped S3 bucket with KMS\nimport * as cdk from 'aws-cdk-lib';\nimport { Bucket, BucketEncryption } from 'aws-cdk-lib/aws-s3';\nimport { Key } from 'aws-cdk-lib/aws-kms';\nconst app = new cdk.App();\nconst stack = new cdk.Stack(app, 'TenantLake');\nconst key = new Key(stack, 'TenantKey', { enableKeyRotation: true });\nnew Bucket(stack, 'TenantDataLake', { encryption: BucketEncryption.KMS, encryptionKey: key });\n```\n\n## Follow-up Questions\n- How would you handle schema evolution in the data lake?\n- How would you validate tenant data locality during migrations?","diagram":"flowchart TD\n  A[Telemetry Ingestion] --> B[Kinesis Streams (Region)]\n  B --> C[Lambda/Glue ETL] \n  C --> D[S3 Data Lake (Tenant partitions)]\n  D --> E[Athena/Quicksight]\n  F[DR: Cross-Region Replication] --> G[Regional Failover]\n","difficulty":"advanced","tags":["aws"],"channel":"aws","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Adobe","NVIDIA","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T15:56:07.938Z","createdAt":"2026-01-21T15:56:07.938Z"},{"id":"q-5423","question":"Scenario: You're building a small web API using API Gateway -> Lambda with DynamoDB. The team requires end-to-end tracing and secure logs. Describe a concrete, beginner-friendly setup to enable end-to-end tracing with AWS X-Ray, configure CloudWatch Logs retention, and ensure sensitive data is not logged. Include what policies or steps you would apply to the Lambda role and API Gateway to achieve this, plus how you would test?","answer":"Enable AWS X-Ray active tracing on both API Gateway and Lambda. Grant the Lambda execution role minimal X-Ray permissions: xray:PutTraceSegments, xray:PutTelemetryRecords, and AWSXRayDaemonWriteAccess. Configure CloudWatch Logs with appropriate retention (e.g., 30 days) and implement log filtering to prevent sensitive data exposure. For API Gateway, enable X-Ray tracing and CloudWatch Logs with proper log format. Test by making API calls and verifying traces appear in X-Ray console, checking that logs flow to CloudWatch without sensitive data, and confirming IAM policies work through CloudTrail.","explanation":"## Why This Is Asked\nTests practical understanding of observability basics for serverless applications on AWS.\n\n## Key Concepts\n- AWS X-Ray integration in API Gateway and Lambda\n- Least-privilege IAM policies for tracing\n- CloudWatch Logs retention and data sensitivity\n- Basic verification of traces and logs\n\n## Code Example\n```javascript\n// Lambda IAM policy for X-Ray\n{\n  \"Effect\": \"Allow\",\n  \"Action\": [\n    \"xray:PutTraceSegments\",\n    \"xray:PutTelemetryRecords\"\n  ],\n  \"Resource\": \"*\"\n}\n```\n\n## Follow-up Questions\n- How would you sample traces to reduce overhead?\n- How would you implement structured logging with correlation IDs?\n- What metrics would you monitor to ensure observability system health?","diagram":"flowchart TD\n A[Client Request] --> B(API Gateway)\n B --> C(Lambda)\n C --> D[DynamoDB]\n B -->|Tracing| E[X-Ray]","difficulty":"beginner","tags":["aws"],"channel":"aws","subChannel":"general","sourceUrl":null,"videos":null,"companies":["IBM","PayPal","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T06:05:08.951Z","createdAt":"2026-01-21T21:50:24.770Z"},{"id":"q-543","question":"You're deploying a microservices application on AWS ECS. One service is experiencing intermittent 503 errors during peak traffic. How would you diagnose and resolve this issue?","answer":"To diagnose intermittent 503 errors in your AWS ECS microservice during peak traffic, first check CloudWatch metrics for CPU/memory throttling and ALB health check failures, then implement auto-scaling policies to increase task count and configure proper health check thresholds to prevent target group deregistrations.","explanation":"## Diagnosis\n- Monitor CloudWatch Container Insights for resource utilization\n- Check ALB access logs for 503 patterns and target health\n- Review ECS task history for stopped reasons\n\n## Resolution\n- Implement auto-scaling based on CPU/memory metrics\n- Configure proper health check grace period (60-300s)\n- Increase minimum task count for baseline capacity\n- Enable connection draining for graceful shutdowns\n\n## Prevention\n- Set up CloudWatch alarms for high error rates\n- Use load testing to determine optimal scaling thresholds\n- Implement circuit breakers in application code","diagram":"flowchart TD\n  A[503 Errors] --> B[Check ECS Metrics]\n  B --> C{Resource Throttling?}\n  C -->|Yes| D[Scale Tasks]\n  C -->|No| E[Check ALB Health]\n  E --> F{Healthy Targets?}\n  F -->|No| G[Fix Health Checks]\n  F -->|Yes| H[Review Logs]\n  H --> I[Container Issues]\n  I --> J[Implement Fixes]","difficulty":"intermediate","tags":["aws"],"channel":"aws","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Bloomberg","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-08T03:43:15.101Z","createdAt":"2025-12-26 12:51:07"},{"id":"q-5572","question":"You are building a small stateless API on AWS with API Gateway, Lambda, and DynamoDB for multi-tenant use. Describe a beginner-friendly deployment workflow that guarantees least privilege, environment separation (dev/stage/prod), and simple per-tenant data isolation. Include IAM roles/policies, a basic CodePipeline flow, and a safe rollback if a deployment fails?","answer":"Use three environments dev, stage, prod with separate IAM roles and restricted policies. DynamoDB uses tenant_id as the partition key; Lambda assumes a tenant-scoped role granting GetItem/PutItem only","explanation":"## Why This Is Asked\n\nTests practical, beginner-friendly wiring of core AWS services with proper access controls, environment separation, and basic CI/CD rollback.\n\n## Key Concepts\n\n- Least privilege IAM roles for Lambda and API Gateway\n- Environment separation (dev/stage/prod) with Lambda aliases\n- Per-tenant data isolation using DynamoDB partition keys\n- CI/CD with CodePipeline (Source, Build, Deploy)\n- Safe rollback via Lambda versioning and DynamoDB PITR\n\n## Code Example\n\n```javascript\n// Example IAM policy snippet (simplified)\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\"dynamodb:GetItem\",\"dynamodb:PutItem\"],\n      \"Resource\": \"arn:aws:dynamodb:us-east-1:123456789012:table/MultiTenantTable\",\n      \"Condition\": {\"ForAllValues:StringEquals\": {\"dynamodb:LeadingKeys\": [\"tenant123\"]}}\n    }\n  ]\n}\n```\n\n## Follow-up Questions\n\n- How would you test per-tenant isolation in development?\n- How would you rotate credentials for Lambda roles without causing downtime?","diagram":null,"difficulty":"beginner","tags":["aws"],"channel":"aws","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Hugging Face","IBM","Instacart"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T07:01:58.493Z","createdAt":"2026-01-22T07:01:58.493Z"},{"id":"q-567","question":"How would you design a multi-region serverless architecture for a real-time chat application using AWS services, ensuring low latency and high availability?","answer":"I would design a multi-region serverless architecture for a real-time chat application using AWS services with the following approach: AWS AppSync for GraphQL subscriptions with DynamoDB Global Tables for data replication, Lambda functions deployed across regions with API Gateway, CloudFront for CDN delivery, and WebSocket implementation for real-time messaging.","explanation":"## Architecture Overview\n- **Frontend**: CloudFront CDN with regional edge caching for optimal performance\n- **API Layer**: API Gateway V2 (WebSockets) + AppSync GraphQL for real-time communication\n- **Business Logic**: Lambda functions distributed across multiple AWS regions\n- **Data Layer**: DynamoDB Global Tables + ElastiCache Redis for data persistence and caching\n- **Messaging**: SQS for asynchronous processing, SNS for push notifications\n\n## Key Components\n- **AppSync**: Real-time subscriptions with offline sync capabilities\n- **DynamoDB**: Global Tables providing sub-second data replication across regions\n- **Lambda**: Regional deployment with configurable concurrency limits\n- **CloudFront**: Edge caching for static content and API responses\n\n## Trade-offs\n- DynamoDB Global Tables increase costs but provide seamless multi-region data consistency\n- Lambda cold starts can impact latency, mitigated through provisioned concurrency\n- CloudFront caching complexity requires careful invalidation strategies\n- WebSocket connections increase operational overhead compared to HTTP polling","diagram":"flowchart TD\n  A[User] --> B[CloudFront CDN]\n  B --> C[Route53 Latency Routing]\n  C --> D[Region 1 API Gateway]\n  C --> E[Region 2 API Gateway]\n  D --> F[Lambda Functions]\n  E --> G[Lambda Functions]\n  F --> H[DynamoDB Global Table]\n  G --> H\n  H --> I[Replication Stream]\n  I --> H","difficulty":"advanced","tags":["aws"],"channel":"aws","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Slack","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-08T11:53:58.717Z","createdAt":"2025-12-27T01:11:45.489Z"},{"id":"q-5891","question":"You're operating a multi-region, multi-tenant streaming platform on AWS. When a tenant's streams degrade under load, design an incident response and a scalable architecture that automatically detects the degradation, reroutes traffic to healthy edge locations, and isolates the tenant's data with per-tenant S3 buckets and IAM boundaries, using cross-account boundaries and SCPs. Specify concrete services (e.g., CloudWatch, Route 53, Lambda, Step Functions, MediaLive/MediaPackage, S3 replication, DynamoDB), monitoring, auto-remediation, data governance, and cost trade-offs. What would you implement first and why?","answer":"Detect degradation with CloudWatch metrics and Synthetics; auto-remediate with Step Functions. Route traffic via Route 53 failover and Lambda@Edge to healthy edge locations; isolate tenants with per-t","explanation":"## Why This Is Asked\n\nTests real-time incident response, cross-account governance, and scalable edge routing for a high-traffic service.\n\n## Key Concepts\n\n- CloudWatch metrics and Synthetics\n- Route 53 failover and Lambda@Edge\n- S3 data locality and KMS\n- DynamoDB partitioning per tenant\n- IAM SCPs and cross-account boundaries\n- Step Functions remediation\n\n## Code Example\n\n```javascript\n// Skeleton Lambda for remediation trigger\nexports.handler = async (event) => { /* check tenant health, switch endpoints, alert */ };\n```\n\n## Follow-up Questions\n\n- How would you test this end-to-end?\n- What are the latency and cost implications of per-tenant isolation at scale?","diagram":null,"difficulty":"advanced","tags":["aws"],"channel":"aws","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Netflix","Robinhood","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T21:04:21.615Z","createdAt":"2026-01-22T21:04:21.615Z"},{"id":"q-6136","question":"You’re building a beginner AWS setup: static site on S3+CloudFront, a small API via API Gateway to Lambda, and a DynamoDB table for config. Explain a concrete workflow to enforce least privilege using IAM roles, Secrets Manager for API keys, and a minimal policy for Lambda to read DynamoDB and write to S3, plus how you’d test rotation and access paths?","answer":"Create a dedicated Lambda role with explicit, least-privilege permissions: DynamoDB:GetItem on the specific config table ARN, S3:PutObject on the target bucket ARN, SecretsManager:GetSecretValue on th","explanation":"## Why This Is Asked\nAssess practical understanding of IAM least privilege, cross-service permissions, and secret management in a common AWS pattern.\n\n## Key Concepts\n- Least privilege IAM roles\n- Resource-based policies with ARNs\n- Secrets Manager integration and rotation\n- Policy simulation and testing\n\n## Code Example\n```javascript\n// Example IAM policy excerpt for Lambda execution role\n{\n  \"Version\":\"2012-10-17\",\n  \"Statement\":[\n    {\"Effect\":\"Allow\",\"Action\":[\"dynamodb:GetItem\"],\"Resource\":\"arn:aws:dynamodb:us-east-1:123456789012:table/config\"}\n  ]\n}\n```\n\n## Follow-up Questions\n- How would you audit and enforce these policies across environments?\n- How would you rotate and revoke secrets if a secret is compromised?","diagram":null,"difficulty":"beginner","tags":["aws"],"channel":"aws","subChannel":"general","sourceUrl":null,"videos":null,"companies":["MongoDB","OpenAI","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T10:21:16.142Z","createdAt":"2026-01-23T10:21:16.142Z"},{"id":"q-6193","question":"Design a multi-tenant analytics service on AWS with streaming ingestion per-tenant data, per-tenant S3 buckets, and per-tenant SageMaker Studio notebooks. Explain onboarding/offboarding automation, IAM, Lake Formation, and SCP-based boundaries, and show how you’d audit access, control costs, and respond to incidents?","answer":"Leverage a centralized control plane that provisions per-tenant stacks: IAM roles with least privilege, isolated S3 buckets, per-tenant SageMaker Studio user profiles, and a per-tenant Lake Formation ","explanation":"## Why This Is Asked\nTests ability to design scalable, secure, auditable multi-tenant data planes on AWS.\n\n## Key Concepts\n- Least-privilege IAM and per-tenant roles\n- Lake Formation data catalog and fine-grained access\n- SCPs, cross-account boundaries, and KMS:\n- Automated onboarding/offboarding via IaC\n- Centralized auditing, alarmed incident response\n\n## Code Example\n```javascript\n// Placeholder: IaC templates would create per-tenant resources\n```\n\n## Follow-up Questions\n- How would you handle tenant misuse or data exfiltration? \n- How would you scale onboarding to thousands of tenants?","diagram":"flowchart TD\n  Onboard[Onboard Tenant] --> Create[Create IAM Roles]\n  Create --> Bind[Bind to S3/Lake Formation]\n  Bind --> Studio[Create Studio users]\n  Studio --> Monitor[Audit & Monitor]","difficulty":"intermediate","tags":["aws"],"channel":"aws","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Lyft","Meta","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T13:33:34.480Z","createdAt":"2026-01-23T13:33:34.480Z"},{"id":"q-6286","question":"You operate a real-time analytics platform for thousands of tenants; data residency mandates per-tenant region binding; events come from mobile clients worldwide at peak 150k/sec. Design an AWS-based pipeline with per-tenant isolation, exactly-once semantics, cross-account access, and disaster recovery. Compare Kinesis Data Streams vs MSK, data model, and governance?","answer":"Use Kinesis Data Streams (or MSK) in each region with a per-tenant partition key (tenant_id) to enforce data locality. Process with idempotent Lambda consumers and DynamoDB writes using conditional ex","explanation":"## Why This Is Asked\nTests ability to design a scalable, compliant multi-tenant real-time pipeline with data residency requirements, cross-account access, and disaster recovery.\n\n## Key Concepts\n- Multi-tenant isolation and per-tenant data locality\n- Exactly-once delivery semantics across streaming and storage\n- Cross-account IAM with STS and least privilege\n- DR planning across regions, cost-aware choices\n- Observability with CloudWatch, X-Ray\n\n## Code Example\n```javascript\n// Pseudo: idempotent event handler skeleton\nasync function handleEvent(evt){\n  const key = evt.tenant_id + ':' + evt.event_id;\n  const exists = await db.get(key);\n  if(exists) return; // idempotent\n  await db.put(key, true);\n  // process\n}\n```\n\n## Follow-up Questions\n- How would you validate RPO/RTO and perform canary DR tests?\n- What are trade-offs between KDS and MSK in cost, latency, and reliability?","diagram":"flowchart TD\n  A[Ingest] --> B[Stream (KDS/MSK)]\n  B --> C[Processor (Lambda)]\n  C --> D[DynamoDB (tenant_id)]\n  C --> E[S3 Raw (per-tenant)]\n  D --> F[Audit & Governance]\n  E --> G[Geo DR]","difficulty":"advanced","tags":["aws"],"channel":"aws","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Apple","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T17:50:56.212Z","createdAt":"2026-01-23T17:50:56.212Z"},{"id":"q-6548","question":"Design a beginner-friendly AWS onboarding framework for a new data-ingest project. The setup uses **team-specific S3 landing buckets**, a **Lambda processor**, and a shared **DynamoDB** table for metadata. Explain how to enable **IAM Identity Center-based SSO**, grant per-team least-privilege access, and implement an auditable policy with explicit denies and MFA. Include concrete IAM role trust and permission policy snippets?","answer":"Use IAM Identity Center for per-team SSO, create a Lambda execution role assumed via SSO, and bind it to a least-privilege policy. Grant access only to the team's S3 landing bucket (read/list), allow ","explanation":"## Why This Is Asked\nShows practical onboarding governance, least privilege, and auditable IAM design in a real-data ingest context.\n\n## Key Concepts\n- IAM Identity Center (SSO)\n- Per-team IAM roles with least privilege\n- Explicit deny and MFA enforcement\n- CloudTrail auditing and S3 encryption\n\n## Code Example\n```json\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\"s3:ListBucket\",\"s3:GetObject\"],\n      \"Resource\": [\n        \"arn:aws:s3:::team-a-land\",\n        \"arn:aws:s3:::team-a-land/*\"\n      ]\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\"dynamodb:PutItem\",\"dynamodb:GetItem\"],\n      \"Resource\": \"arn:aws:dynamodb:region:acct-id:table/ProjectMeta\"\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\"logs:CreateLogGroup\",\"logs:CreateLogStream\",\"logs:PutLogEvents\"],\n      \"Resource\": \"arn:aws:logs:*:*:*\"\n    }\n  ]\n}\n```\n\n## Follow-up Questions\n- How would you test the onboarding flow end-to-end?\n- What changes if a new team needs access to a separate bucket and metadata table?","diagram":null,"difficulty":"beginner","tags":["aws"],"channel":"aws","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Anthropic","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T07:49:18.449Z","createdAt":"2026-01-24T07:49:18.449Z"},{"id":"q-6601","question":"Scenario: A new microservice accepts user image uploads via a web form. Uploads go to S3 bucket 'app-uploads', an event-triggered Lambda creates a 200x200 thumbnail in 'app-thumbs', and CloudFront serves thumbnails publicly. Provide the minimal IAM roles and policies for the Lambda (least privilege to read from uploads and write to thumbs), the S3 event configuration, and a simple retry/backoff strategy for transient failures?","answer":"Use a Lambda function triggered by S3 ObjectCreated events on app-uploads; it creates a 200x200 thumbnail in app-thumbs. IAM: a dedicated role with least privilege—s3:GetObject on app-uploads and s3:P","explanation":"## Why This Is Asked\nTests understanding of basic serverless integration: S3 event triggers, Lambda permissions, and safe cross-service access. Also checks practical retry handling and failure paths with a DLQ.\n\n## Key Concepts\n- S3 event notifications to Lambda\n- IAM least privilege for cross-service access\n- Lambda execution role and CloudWatch logs\n- Retry strategies and dead-letter queue (DLQ)\n\n## Code Example\n```javascript\n// Minimal IAM policy sketch for the Lambda role\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\"Effect\": \"Allow\", \"Action\": [\"s3:GetObject\"],\n     \"Resource\": \"arn:aws:s3:::app-uploads/*\"},\n    {\"Effect\": \"Allow\", \"Action\": [\"s3:PutObject\"],\n     \"Resource\": \"arn:aws:s3:::app-thumbs/*\"},\n    {\"Effect\": \"Allow\", \"Action\": [\"logs:CreateLogGroup\", \"logs:CreateLogStream\", \"logs:PutLogEvents\"],\n     \"Resource\": \"arn:aws:logs:*:*:*\"}\n  ]\n}\n```\n\n## Follow-up Questions\n- How would you test this locally and ensure idempotency for duplicate uploads?\n- What changes would you make to handle large uploads or multiple concurrent thumbnails?","diagram":"flowchart TD\n  A[User uploads image] --> B[S3:app-uploads ObjectCreated]\n  B --> C[Lambda: generate thumbnail]\n  C --> D[S3:app-thumbs]\n  D --> E[CloudFront: serves thumbnails]","difficulty":"beginner","tags":["aws"],"channel":"aws","subChannel":"general","sourceUrl":null,"videos":null,"companies":["LinkedIn","Meta","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T09:50:26.607Z","createdAt":"2026-01-24T09:50:26.607Z"},{"id":"q-6684","question":"You're managing a multi-account, multi-region AWS setup for a fintech product. An irregular spike in API activity suggests potential credential compromise affecting a subset of accounts. Outline an incident response architecture: detection with GuardDuty/Security Hub, rapid containment using IAM SCPs, temporary access revocation, cross-account role revocation, and automated recovery via Step Functions. Include service choices and a concise remediation playbook?","answer":"Detect with GuardDuty/Config/Security Hub alerts; contain by applying a deny SCP to the affected OU, revoking cross-account roles, and rotating credentials or suspending IAM principals; validate with ","explanation":"## Why This Is Asked\nTests real-world IR capabilities in a multi-account AWS environment with regulatory constraints. Requires specifics about detection, containment, credential rotation, and automation.\n\n## Key Concepts\n- GuardDuty/Security Hub/Config integration\n- IAM SCPs, cross-account trust management\n- Credential rotation, principal suspension\n- Step Functions orchestration of containment, validation, and recovery\n\n## Code Example\n```javascript\n// Example: minimal Step Functions state machine (pseudo)\nconst sm = {\n  Comment: 'IR workflow',\n  StartAt: 'Containment',\n  States: {\n    Containment: { Type: 'Task', Next: 'Rotate' },\n    Rotate: { Type: 'Task', Next: 'Audit' },\n    Audit: { Type: 'Task', Next: 'Notify' },\n    Notify: { Type: 'Succeed' }\n  }\n}\n```\n\n## Follow-up Questions\n- How would you test the IR playbook? | Which metrics indicate successful containment? | What changes after recovery?","diagram":"flowchart TD\n  A[Detect] --> B[Containment]\n  B --> C[Credential Rotation]\n  C --> D[Audit & Validate]\n  D --> E[Notify & Recover]","difficulty":"advanced","tags":["aws"],"channel":"aws","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Cloudflare","Coinbase","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T13:33:01.972Z","createdAt":"2026-01-24T13:33:01.972Z"},{"id":"q-6707","question":"Design a cross-region DR pattern for a streaming analytics app on AWS. Producers push to a Kinesis Data Stream in us-east-1; a Lambda consumer writes results to DynamoDB and S3. Implement RPO 5 minutes, tenant isolation, and auditable IAM boundaries. Describe architecture, data replication path (Kinesis cross-region stream or Firehose), encryption (KMS), and a concrete failover/test plan with CloudWatch alarms. Include minimal IAM policy snippets?","answer":"Architect a primary us-east-1 stack: Kinesis Data Stream -> Lambda -> DynamoDB and S3. Add a cross-region replica in us-west-2 (Kinesis cross-region stream or Firehose) with per-tenant IAM boundaries ","explanation":"## Why This Is Asked\nTests knowledge of cross-region DR for streaming workloads, including data replication paths, security boundaries, and measurable recovery objectives.\n\n## Key Concepts\n- Cross-region replication and DR strategies for streaming data\n- Per-tenant IAM boundaries and auditable access\n- KMS key management and SSE for streams and storage\n- DR testing, RPO/RTO validation, and monitoring with CloudWatch\n\n## Code Example\n```javascript\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\"Effect\": \"Allow\", \"Action\": [\"kinesis:PutRecord\",\"kinesis:PutRecords\"], \"Resource\": \"arn:aws:kinesis:us-east-1:123456789012:stream/PrimaryStream\"},\n    {\"Effect\": \"Deny\", \"Action\": \"*\", \"Resource\": \"*\", \"Condition\": {\"ForAllValues:StringEquals\": {\"aws:RequestedRegion\": \"eu-central-1\"}}}\n  ]\n}\n```\n\n## Follow-up Questions\n- How would you validate RPO/RTO under partial outages?\n- What metrics would you surface to detect replication lag and failover readiness?","diagram":null,"difficulty":"advanced","tags":["aws"],"channel":"aws","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Meta","Netflix","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T14:12:33.524Z","createdAt":"2026-01-24T14:12:33.524Z"},{"id":"q-6849","question":"In a multi-tenant analytics pipeline on AWS, ingest high-velocity JSON events from dozens of microservices into a centralized store with strict tenant isolation and governance. Design a practical pipeline using Kinesis Data Streams/Firehose, S3, Glue, Lake Formation, and a query layer (Athena/Redshift). Include IAM controls, data partitioning, and DR testing?","answer":"Use Kinesis Data Streams per tenant, route to Firehose into S3 under tenant-prefixes; Glue catalog with date+tenant partitions; analytics via Athena or Redshift Spectrum. Enforce per-tenant IAM roles ","explanation":"## Why This Is Asked\n\nDemonstrates ability to design scalable, secure multi-tenant data pipelines with governance and DR in AWS. Requires service choices, partitioning strategy, Lake Formation usage, and cost controls.\n\n## Key Concepts\n\n- Multi-tenant isolation: tenant prefixes, Lake Formation grants\n- Data lake architecture: KDS/Firehose, S3, Glue catalog, partitions\n- Query layer: Athena/Redshift Spectrum\n- IAM and governance: least privilege, trusted roles, cross-account access\n- DR and cost management: cross-region replication, budgets\n\n## Code Example\n\n```json\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\"Effect\": \"Allow\", \"Action\": [\"s3:GetObject\"], \"Resource\": [\"arn:aws:s3:::tenant-*/data/*\"]},\n    {\"Effect\": \"Allow\", \"Action\": [\"glue:GetTable\",\"glue:GetDatabase\"], \"Resource\": \"*\"}\n  ]\n}\n```\n\n## Follow-up Questions\n\n- How would you test the DR process and recovery time objective?\n- Which metrics would you instrument to detect tenant-level cost anomalies?","diagram":"flowchart TD\n  A[Producers: microservices] --> B[Kinesis Data Streams]\n  B --> C[Firehose to S3]\n  C --> D[S3 Data Lake: tenant prefixes]\n  D --> E[Glue Data Catalog]\n  E --> F[Athena/Redshift Spectrum]","difficulty":"advanced","tags":["aws"],"channel":"aws","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Discord","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T19:51:16.377Z","createdAt":"2026-01-24T19:51:16.377Z"},{"id":"q-6855","question":"Scenario: A serverless API path uses API Gateway -> Lambda (Python) -> DynamoDB. You notice sporadic latency during business hours. Provide a practical plan to reduce cold starts and latency focusing on provisioning strategies, API caching, DynamoDB capacity, and observability. Include concrete numbers (e.g., provisioned concurrency, cache TTL, RCUs/WCUs) and a staged deployment strategy to validate improvements without downtime?","answer":"Enable Lambda Provisioned Concurrency for the hot path (start with 4–8 concurrent executions) and API Gateway caching (TTL 60s) to reduce latency. Use DynamoDB on‑demand or provisioned with auto‑scali","explanation":"## Why This Is Asked\nTests practical serverless latency optimization and observability basics for a beginner.\n\n## Key Concepts\n- Provisioned Concurrency\n- API Gateway Caching\n- DynamoDB capacity modes and auto-scaling\n- CloudWatch alarms and X-Ray\n- Staged deployment canary strategies\n\n## Code Example\n```javascript\n// AWS SDK snippet enabling provisioned concurrency (pseudo)\nconst lambda = new AWS.Lambda();\nlambda.putProvisionedConcurrencyConfig({FunctionName: 'myFunc', Qualifier: '$LATEST', ProvisionedConcurrentExecutions: 8}).promise();\n```\n\n## Follow-up Questions\n- How would you automate rollback if latency worsens after changes?\n- How would you monitor per-endpoint latency vs. overall API latency and set alarms accordingly?","diagram":null,"difficulty":"beginner","tags":["aws"],"channel":"aws","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Meta","Microsoft","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T20:34:31.683Z","createdAt":"2026-01-24T20:34:31.683Z"},{"id":"q-6915","question":"Design a scalable, secure multi-tenant data lake ingestion on AWS for a SaaS with tenants in separate accounts. Describe per-tenant S3 isolation, Glue Data Catalog + Lake Formation governance, cross-account IAM roles and resource policies, KMS-encrypted data, and CloudTrail/audit logs. Include partitioning strategy and concrete per-tenant policy examples?","answer":"Design a scalable, secure multi-tenant data lake ingestion on AWS for a SaaS with tenants in separate accounts. Describe per-tenant S3 isolation, Glue Data Catalog + Lake Formation governance, cross-account IAM roles and resource policies, KMS-encrypted data, and CloudTrail/audit logs. Include partitioning strategy and concrete per-tenant policy examples.\n\n## Architecture Overview\n\nFor a multi-tenant SaaS with tenants in separate AWS accounts, implement a hub-spoke model with centralized governance:\n\n**Core Components:**\n- Central management account with Lake Formation\n- Per-tenant accounts with isolated S3 buckets\n- Cross-account IAM roles for secure access\n- KMS customer-managed keys per tenant\n- Glue Data Catalog with LF-tags for governance\n\n## Per-Tenant S3 Isolation\n\n**Bucket Structure:**\n```\ntenant-123-data-lake/\n├── raw/\n│   ├── year=2024/month=01/day=15/\n│   └── year=2024/month=01/day=16/\n├── processed/\n│   ├── year=2024/month=01/\n│   └── year=2024/month=02/\n└── curated/\n    └── department=sales/\n```\n\n**S3 Policy Example:**\n```json\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"AWS\": \"arn:aws:iam::CENTRAL-ACCOUNT:role/Tenant123IngestionRole\"\n      },\n      \"Action\": [\"s3:PutObject\", \"s3:GetObject\"],\n      \"Resource\": \"arn:aws:s3:::tenant-123-data-lake/*\",\n      \"Condition\": {\n        \"StringEquals\": {\n          \"s3:x-amz-server-side-encryption\": \"aws:kms\",\n          \"s3:x-amz-server-side-encryption-aws-kms-key-id\": \"arn:aws:kms:us-east-1:TENANT-ACCOUNT:key/tenant-123-key\"\n        }\n      }\n    }\n  ]\n}\n```\n\n## Lake Formation Governance\n\n**LF-Tag Strategy:**\n```sql\n-- Create LF-tags for tenant isolation\nCREATE LF-TAG tenant_id;\nCREATE LF-TAG data_classification;\n\n-- Apply to databases and tables\nALTER DATABASE tenant_123_db ADD LF-TAG (tenant_id='123', data_classification='confidential');\n```\n\n**Cross-Account LF-Grant:**\n```sql\n-- Grant access to tenant account\nGRANT SELECT ON DATABASE tenant_123_db \nTO ACCOUNT \"111122223333\" \nWITH LF-TAGS (tenant_id='123');\n```\n\n## Cross-Account IAM Architecture\n\n**Central Account Role:**\n```json\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"glue:GetDatabase\",\n        \"glue:GetTable\",\n        \"lakeformation:GetDataAccess\"\n      ],\n      \"Resource\": \"*\",\n      \"Condition\": {\n        \"StringEquals\": {\n          \"lakeformation:LfTags\": [\"tenant_id='123'\"]\n        }\n      }\n    }\n  ]\n}\n```\n\n**Tenant Account Role:**\n```json\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"s3:PutObject\",\n        \"s3:GetObject\",\n        \"kms:Encrypt\",\n        \"kms:Decrypt\"\n      ],\n      \"Resource\": [\n        \"arn:aws:s3:::tenant-123-data-lake/*\",\n        \"arn:aws:kms:us-east-1:TENANT-ACCOUNT:key/tenant-123-key\"\n      ]\n    }\n  ]\n}\n```\n\n## KMS Encryption Strategy\n\n**Per-Tenant CMK:**\n```json\n{\n  \"Description\": \"Tenant 123 Data Lake Encryption Key\",\n  \"KeyPolicy\": {\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n      {\n        \"Effect\": \"Allow\",\n        \"Principal\": {\n          \"AWS\": [\n            \"arn:aws:iam::CENTRAL-ACCOUNT:role/LakeFormationServiceRole\",\n            \"arn:aws:iam::TENANT-ACCOUNT:role/Tenant123DataRole\"\n          ]\n        },\n        \"Action\": [\"kms:Encrypt\", \"kms:Decrypt\", \"kms:GenerateDataKey\"],\n        \"Resource\": \"*\"\n      }\n    ]\n  }\n}\n```\n\n## Partitioning Strategy\n\n**Time-Based Partitioning:**\n- Raw data: `year/month/day/hour`\n- Processed data: `year/month`\n- Curated data: `business_unit/region`\n\n**Optimization:**\n- Use Glue crawlers with partition projection\n- Implement S3 Inventory for analytics\n- Apply lifecycle policies for data tiering\n\n## Audit and Monitoring\n\n**CloudTrail Configuration:**\n```json\n{\n  \"Trail\": {\n    \"Name\": \"MultiTenantDataLakeAudit\",\n    \"S3BucketName\": \"central-audit-logs\",\n    \"IncludeGlobalServiceEvents\": true,\n    \"IsMultiRegionTrail\": true,\n    \"EnableLogFileValidation\": true\n  }\n}\n```\n\n**Lake Formation Logging:**\n- Enable LF access logging\n- Monitor data access patterns\n- Set up CloudWatch alerts for unauthorized access\n\n## Scalability Considerations\n\n- Use S3 Requester Pays for cost allocation\n- Implement Glue Spark jobs for ETL scaling\n- Leverage Athena federated queries for cross-account analysis\n- Use Step Functions for orchestration\n\nThis architecture ensures complete tenant isolation while maintaining centralized governance and audit capabilities across the multi-tenant environment.","explanation":"## Why This Is Asked\n\nTests ability to architect scalable, secure data ingestion across accounts with governance at scale, matching enterprise needs from Databricks and Salesforce.\n\n## Key Concepts\n\n- Cross-account IAM roles and resource policies\n- Lake Formation governance and Glue Data Catalog\n- Per-tenant S3 isolation and partitioning strategy\n- KMS key management and envelope encryption\n- Audit/logging with CloudTrail, Config, and Lake Formation logs\n\n## Code Example\n\n```json\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\"s3:GetObject\", \"s3:PutObject\"],\n      \"Resource\": \"arn:aws:s3:::tenant-*-data-lake/*\",\n      \"Condition\": {\n        \"StringEquals\": {\n          \"aws:PrincipalTag/tenant_id\": \"${aws:userid}\"\n        }\n      }\n    }\n  ]\n}\n```\n\n## Interview Strategy\n\nFocus on demonstrating enterprise-scale architecture patterns, emphasizing security isolation, governance automation, and cost optimization across multi-tenant environments.","diagram":null,"difficulty":"advanced","tags":["aws"],"channel":"aws","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Databricks","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T05:54:01.829Z","createdAt":"2026-01-24T22:45:01.504Z"},{"id":"q-7166","question":"You’re building a beginner-friendly AWS backend for a small app: API Gateway public endpoint triggers a Lambda function that reads from a private DynamoDB table and writes artifacts to an S3 bucket in us-east-1. Propose a minimal VPC design to keep DynamoDB access private, including a DynamoDB VPC endpoint, necessary IAM roles/policies (least privilege), and basic network config (subnets, security groups). Describe how to verify end-to-end access and outline a simple disaster-recovery approach?","answer":"Use a VPC with private subnets where Lambda runs, attach a DynamoDB VPC Endpoint and an S3 VPC endpoint; restrict Lambda's IAM role to DynamoDB GetItem/Query on the table and PutObject on the bucket; ","explanation":"## Why This Is Asked\nAssesses understanding of VPCs, endpoints, and IAM least privilege in a simple backend.\n\n## Key Concepts\n- VPC Endpoints\n- Lambda in VPC\n- IAM least privilege\n- API Gateway to Lambda integration\n- CloudWatch and VPC Flow Logs\n- DynamoDB Global Tables for DR\n\n## Code Example\n```javascript\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\"Effect\":\"Allow\",\"Action\":[\"dynamodb:GetItem\",\"dynamodb:Query\"],\n     \"Resource\":\"arn:aws:dynamodb:us-east-1:123456789012:table/YourTable\"},\n    {\"Effect\":\"Allow\",\"Action\":[\"s3:PutObject\"],\n     \"Resource\":\"arn:aws:s3:::your-bucket/*\"}\n  ]\n}\n```\n\n## Follow-up Questions\n- How would you rotate the Lambda execution role credentials in this design?\n- How would you validate end-to-end security and monitor for misconfigurations?","diagram":"flowchart TD\n  A[API Gateway] --> B[Lambda]\n  B --> C[DynamoDB (private endpoint)]\n  B --> D[S3 Bucket]\n  subgraph VPC\n    E[Lambda ENI in private subnets]\n  end","difficulty":"beginner","tags":["aws"],"channel":"aws","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Bloomberg","Hugging Face","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T11:48:31.055Z","createdAt":"2026-01-25T11:48:31.055Z"},{"id":"q-7227","question":"You need to ingest real-time log data from a multi-region SaaS app into a central data lake on AWS. Design an end-to-end pipeline that uses Kinesis Data Streams or Firehose, S3, IAM roles and SCPs, cross-account access, and EventBridge; ensure strict least-privilege, encryption, failover to a secondary region, automatic replay of missed data, and cost controls. Explain how you monitor, test failover, and handle schema drift?","answer":"Design a multi-region data ingestion pipeline: SaaS app streams to Kinesis Data Streams in Region A; Firehose delivers to a centralized S3 Data Lake; grant cross-account access with least-privilege IA","explanation":"## Why This Is Asked\nExplores cross‑account, multi‑region data pipelines with strong security, DR, and data governance—crucial for large cloud providers.\n\n## Key Concepts\n- Cross‑account IAM roles and SCPs\n- Kinesis/Data Firehose to S3 data lake\n- SSE-KMS, bucket replication, lifecycle costs\n- EventBridge for drift alerts and replay mechanism\n\n## Code Example\n```javascript\n// Placeholder: your infra-as-code would define KMS keys, IAM roles, SCPs, and Firehose delivery streams\n```\n\n## Follow-up Questions\n- How would you test failover and data replay with minimal data loss?\n- How would you detect and remediate schema drift in the data lake?","diagram":"flowchart TD\n  SaaS[SaaS App] -->|Stream| KDS[Kinesis Data Streams in Region A]\n  KDS --> Firehose[Firehose to Central S3]\n  Firehose --> DataLake[S3 Central Data Lake]\n  DataLake --> DR[Replication to Region B]\n  EventBridge[EventBridge] --> Schema[Schema Drift & Replay]\n  Replay[Replay Bucket] --> Consumer","difficulty":"advanced","tags":["aws"],"channel":"aws","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Cloudflare","IBM"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T14:35:29.581Z","createdAt":"2026-01-25T14:35:29.581Z"},{"id":"q-7265","question":"Scenario: Build a scalable, isolated multi-tenant log ingestion on AWS where each tenant uses a dedicated Kinesis Data Stream feeding Lambda and persisting to S3 with tenant prefixes. A spike in one tenant throttles the system and affects others. Design a per-tenant shard strategy with auto-scaling, tenant quotas, and auditing/rollback plan. Include concrete services and a migration path?","answer":"Design per-tenant Kinesis Data Streams, each shard-count autoscaled by Application Auto Scaling, with a producer library mapping TenantID to a dedicated stream. Enforce per-tenant quotas, Lambda reser","explanation":"## Why This Is Asked\nIsolates tenants for predictable latency under load; tests practical use of Kinesis shard autoscaling, per-tenant quotas, and auditing.\n\n## Key Concepts\n- Per-tenant streams for isolation\n- Application Auto Scaling for Kinesis shard counts\n- Lambda per-tenant concurrency control\n- IAM boundaries and least privilege\n- CloudTrail logging and centralized auditing\n- Migration strategy with rollback\n\n## Code Example\n```bash\n# Create a per-tenant stream (example for tenant-123)\naws kinesis create-stream --stream-name tenant-123-logstream --shard-count 4\n# Register scalable target and scaling policy\naws application-autoscaling register-scalable-target --service-namespace kinesis --resource-id stream/tenant-123-logstream --scalable-dimension kinesis:stream:ShardCount --min-capacity 4 --max-capacity 20\naws application-autoscaling put-scaling-policy --policy-name tenant-123-scale-out --policy-type TargetTrackingScaling --resource-id stream/tenant-123-logstream --scalable-dimension kinesis:stream:ShardCount --service-namespace kinesis --target-tracking-policy-configuration '{\"TargetValue\":70.0,\"ScaleOutCooldown\":60,\"ScaleInCooldown\":60}'\n```\n\n```python\n# Pseudo-code: map TenantID to stream; ensure isolation in producer\ndef get_stream_for_tenant(tenant_id):\n  return f\"tenant-{tenant_id}-logstream\"\n```\n\n## Follow-up Questions\n- How would you monitor and alert on shard throttling per tenant? \n- How would you automate onboarding/offboarding across streams without data loss?","diagram":"flowchart TD\n  A[Tenant Ingest] --> B[Kinesis Stream per Tenant]\n  B --> C[Lambda Processor]\n  C --> D[S3 Tenant Prefix]\n  D --> E[Audit in CloudTrail]","difficulty":"intermediate","tags":["aws"],"channel":"aws","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Bloomberg","Cloudflare","Discord"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T15:51:09.408Z","createdAt":"2026-01-25T15:51:09.408Z"},{"id":"q-7310","question":"You’re building a multi-tenant file upload service on AWS. Tenants upload to S3 prefixes tenant-<id>/; a Lambda processes files and stores metadata in DynamoDB. For per-tenant isolation and auditable encryption at rest, which SSE approach would you choose and why? Outline a minimal Lambda IAM policy to access the tenant prefix and CMK, plus key rotation and audit plan?","answer":"Choose SSE-KMS with a per-tenant CMK and bucket default encryption. It provides per-tenant isolation, audit logging of encrypt/decrypt, and rotation. Grant the Lambda role kms:Encrypt/Decrypt on cmk/t","explanation":"## Why This Is Asked\nTests understanding of data-at-rest encryption, tenant isolation, and basic IAM/KMS primitives in a realistic multi-tenant setup.\n\n## Key Concepts\n- SSE-KMS with per-tenant CMK\n- IAM policy scoping to tenant resources\n- Key rotation and auditing via KMS + CloudTrail\n- Tenant prefix isolation in S3\n\n## Code Example\n```javascript\n// Pseudo code showing access check\nfunction authorize(tenantId, action, resource) {\n  // ensure tenant path access\n  return hasAccess(tenantId, action, resource);\n}\n```\n\n## Follow-up Questions\n- How would you automate per-tenant CMK creation and rotation in this flow?\n- What are potential pitfalls with per-tenant CMKs and how would you mitigate them?","diagram":null,"difficulty":"beginner","tags":["aws"],"channel":"aws","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Bloomberg","IBM","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T17:43:53.913Z","createdAt":"2026-01-25T17:43:53.913Z"},{"id":"q-7384","question":"In a two-account AWS setup (Prod and Audit), data lands in S3 in Prod; a new regulation requires PII in logs be anonymized before cross-account sharing. Design a repeatable, low-latency pipeline using S3, Glue ETL, Lake Formation, IAM, and CloudTrail Lake to classify, mask, and propagate PII to the Audit account, with concrete IAM roles, policies, and testing steps?","answer":"Design a two-account, event-driven pipeline: classify PII with a Glue classifier, mask via Spark UDF in the ETL job, and write masked data to the Audit account S3 using a dedicated KMS CMK. Use Lake F","explanation":"## Why This Is Asked\n\nTests cross-account data governance, real-time masking, and auditable trails.\n\n## Key Concepts\n- Cross-account access and least privilege\n- Data classification and masking\n- KMS encryption\n- Lake Formation permissions and data catalog\n- CloudTrail Lake auditing\n\n## Code Example\n```json\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\"Effect\": \"Allow\", \"Action\": [\"s3:PutObject\", \"s3:GetObject\"], \"Resource\": \"arn:aws:s3:::audit-bucket/*\"},\n    {\"Effect\": \"Allow\", \"Action\": [\"kms:Encrypt\", \"kms:Decrypt\"], \"Resource\": \"arn:aws:kms:us-east-1:123456789012:key/abcdef-1234\"}\n  ]\n}\n```\n\n```json\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\"Effect\": \"Allow\", \"Action\": [\"glue:GetTable\",\"glue:GetTableVersion\",\"glue:GetPartitions\"], \"Resource\": \"*\"}\n  ]\n}\n```\n\n## Follow-up Questions\n- How would you test for data leakage during role assumption?\n- How would you handle schema changes in the masked data?\n","diagram":"flowchart TD\n  A[Prod S3 bucket] --> B[Glue ETL Job (masking)]\n  B --> C[Audit S3 (masked data)]\n  C --> D[Athena/Analytics]\n  E[CloudTrail Lake] --> F[Audit Trail]","difficulty":"advanced","tags":["aws"],"channel":"aws","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Instacart","Lyft","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T20:58:32.155Z","createdAt":"2026-01-25T20:58:32.155Z"},{"id":"q-7410","question":"Describe a beginner-friendly deployment for a private static frontend hosted on S3 with CloudFront using an Origin Access Identity, plus a lightweight API via API Gateway and Lambda. Explain how you would implement per-environment isolation (dev/stage/prod) with separate buckets and endpoints, enforce least-privilege IAM roles, and a simple backup/restore plan for assets. Include a minimal Lambda policy to read S3 and publish logs to CloudWatch?","answer":"Create per-environment private S3 buckets with versioning enabled, then configure CloudFront distributions with Origin Access Identities to securely access each bucket. Deploy separate API Gateway endpoints and Lambda functions per environment, attaching least-privilege IAM roles that only allow necessary S3 read access and CloudWatch logging permissions. Implement a backup strategy using S3 versioning and lifecycle policies to archive older versions to Glacier.","explanation":"## Why This Is Asked\nThis question evaluates practical AWS deployment skills for a secure static frontend with API backend, focusing on environment isolation, IAM security principles, and data protection strategies that candidates can realistically implement.\n\n## Key Concepts\n- Origin Access Identity (OAI) for private S3 access via CloudFront\n- Per-environment isolation (dev/stage/prod) with separate resources\n- Least-privilege IAM roles for Lambda and API Gateway\n- S3 versioning and lifecycle policies for backup\n- CloudWatch Logs integration and CloudFront access logging\n\n## Code Example","diagram":null,"difficulty":"beginner","tags":["aws"],"channel":"aws","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Adobe","Discord","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T06:42:42.634Z","createdAt":"2026-01-25T21:50:22.038Z"},{"id":"q-7478","question":"You're building a SaaS data lake in AWS shared by multiple tenants. Each tenant must access only their own data in S3 and DynamoDB via temporary credentials issued by an API Gateway/Lambda flow. Outline a beginner-friendly pattern using IAM roles and STS for least-privilege access, tenant onboarding/offboarding, and example trust/policy snippets for cross-tenant isolation?","answer":"Implement tenant isolation by creating per-tenant IAM roles that your Lambda assumes to issue temporary credentials. Each tenant receives a dedicated IAM role with a trust relationship allowing API Gateway/Lambda to assume it, while the inline policy restricts access to that tenant's S3 prefix and DynamoDB partition keys. During onboarding, create a TenantRegistry entry and corresponding IAM role; offboarding involves removing both. The Lambda calls STS AssumeRole with the tenant-specific role ARN and returns temporary credentials scoped to that tenant's data.","explanation":"## Why This Is Asked\nTests understanding of multi-tenant isolation using AWS STS, least privilege principles, and automated tenant lifecycle management in SaaS architectures.\n\n## Key Concepts\n- AWS STS AssumeRole for tenant-scoped temporary credentials\n- S3 bucket isolation using tenant-specific prefixes\n- DynamoDB access control via LeadingKeys condition on partition keys\n- API Gateway/Lambda integration for secure credential vending\n- Automated IAM role creation/deletion for tenant lifecycle\n\n## Implementation Pattern\n1. **Tenant Registry**: Central store managing tenant metadata and role mappings\n2. **Per-Tenant IAM Roles**: Each tenant gets dedicated role with scoped permissions\n3. **Trust Policy**: Allows API Gateway service to assume tenant roles\n4. **Access Policies**: Restrict S3 to tenant prefixes and DynamoDB to partition keys\n5. **Token Vending**: Lambda assumes tenant role via STS and returns temporary credentials\n\n## Code Example\n```json\n{\n  \"TrustPolicy\": {\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [{\n      \"Effect\": \"Allow\",\n      \"Principal\": {\"Service\": \"apigateway.amazonaws.com\"},\n      \"Action\": \"sts:AssumeRole\"\n    }]\n  },\n  \"AccessPolicy\": {\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n      {\n        \"Effect\": \"Allow\",\n        \"Action\": [\"s3:ListBucket\", \"s3:GetObject\", \"s3:PutObject\"],\n        \"Resource\": [\n          \"arn:aws:s3:::shared-lake-bucket\",\n          \"arn:aws:s3:::shared-lake-bucket/tenant-${tenant-id}/*\"\n        ]\n      },\n      {\n        \"Effect\": \"Allow\",\n        \"Action\": [\"dynamodb:GetItem\", \"dynamodb:PutItem\", \"dynamodb:Query\"],\n        \"Resource\": \"arn:aws:dynamodb:region:account:table/tenant-data\",\n        \"Condition\": {\n          \"ForAllValues:StringLike\": {\n            \"dynamodb:LeadingKeys\": [\"${tenant-id}-*\"]\n          }\n        }\n      }\n    ]\n  }\n}\n```","diagram":null,"difficulty":"beginner","tags":["aws"],"channel":"aws","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Goldman Sachs","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T04:21:58.697Z","createdAt":"2026-01-26T02:57:38.761Z"},{"id":"q-7563","question":"You need a cost-aware, multi-region data ingestion and analytics pipeline for a real-time advertising dataset on AWS. Propose an architecture using Kinesis Data Streams, Firehose, S3, Glue, and Lake Formation. Explain exactly-once processing, idempotent writes, cross-account IAM roles, disaster recovery, and observability. What would be your data offset management strategy and failure modes?","answer":"Use an event-driven pipeline with Kinesis Data Streams in each region, Firehose to S3 for long-term storage, and a Glue catalog with Lake Formation read/write policies. Implement exactly-once processi","explanation":"## Why This Is Asked\nAssesses architecture, data integrity, and cross-region security for real-time AWS pipelines.\n\n## Key Concepts\n- Multi-region streaming, Kinesis Firehose, S3, Glue, Lake Formation\n- Exactly-once processing, deduplication state, idempotent sinks\n- Cross-account IAM, SCPs, least privilege\n- Disaster recovery, DR tests, observability\n\n## Code Example\n```javascript\n// IAM policy snippet (illustrative)\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\"Effect\": \"Allow\", \"Action\": [\"s3:PutObject\"], \"Resource\": \"arn:aws:s3:::bucket/*\"},\n    {\"Effect\": \"Allow\", \"Action\": [\"dynamodb:PutItem\"], \"Resource\": \"arn:aws:dynamodb:region:acct:table/Offsets\"}\n  ]\n}\n```\n\n## Follow-up Questions\n- How would you validate idempotency guarantees under backpressure?\n- Which metrics would you ship and what are alert thresholds for DR readiness?","diagram":"flowchart TD\n  A[Kinesis Ingest] --> B[Firehose->S3]\n  B --> C[Glue Catalog]\n  C --> D[Lake Formation Access]\n  D --> E[Analytics & DR]","difficulty":"advanced","tags":["aws"],"channel":"aws","subChannel":"general","sourceUrl":null,"videos":null,"companies":["PayPal","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T07:51:01.927Z","createdAt":"2026-01-26T07:51:01.927Z"},{"id":"q-7611","question":"You manage a multi-region, multi-tenant data ingestion pipeline on AWS for a streaming analytics product. Data lands in per-tenant S3 buckets, streamed via Kinesis Data Streams to a Lambda processor, then stored in a shared Glue Catalog and Lake Formation-protected data lake. Explain the exact steps to implement cross-account IAM roles, per-tenant isolation with least privilege, MFA, DR testing, and observability; include failure modes and rollback strategy?","answer":"Create per-tenant IAM roles in the data-lake account trusting each tenant account, enforce MFA on AssumeRole, and grant least-privilege Lake Formation permissions plus per-tenant S3/KMS access. Map Gl","explanation":"## Why This Is Asked\n\nTests ability to design scalable, secure, multi-account data pipelines with strict isolation, cross-region DR, and robust observability, using Lake Formation, KMS, MFA, and STS.\n\n## Key Concepts\n\n- Multi-account trust and least privilege\n- Lake Formation data access governance\n- MFA enforcement in IAM policies\n- Cross-region DR and data replication\n- Observability and failure-mode testing\n\n## Code Example\n\n```javascript\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\"AWS\": [\"arn:aws:iam::111111111111:root\"]},\n      \"Action\": \"sts:AssumeRole\",\n      \"Condition\": {\"Bool\": {\"aws:MultiFactorAuthPresent\": \"true\"}}\n    }\n  ]\n}\n```\n\n## Follow-up Questions\n\n- How would you verify tenant isolation in production without impacting others?\n- What metrics signal misconfig or data leakage and how would you alert on them?","diagram":"flowchart TD\n  A[Tenant Account] --> B[AssumeRole in Data Lake]\n  B --> C[Kinesis Data Streams]\n  C --> D[Lambda Processor]\n  D --> E[Glue Catalog / Lake Formation]\n  E --> F[S3 Data Lake / Analytics]","difficulty":"advanced","tags":["aws"],"channel":"aws","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Netflix","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T09:55:31.099Z","createdAt":"2026-01-26T09:55:31.099Z"},{"id":"q-7680","question":"Intermittent throttling in a multi-tenant AWS SaaS app: tenants experience spikes during bursts; API calls slow or fail. Outline an end-to-end debugging plan to distinguish API Gateway quotas, Lambda concurrency, and IAM throttling, and specify concrete mitigations (per-tenant usage plans, reserved concurrency, backoff strategies, and canary testing) with necessary instrumentation?","answer":"Begin by isolating layer behavior: confirm whether API Gateway quotas are hit, Lambda concurrency is constrained, or IAM throttling blocks calls. Check CloudWatch for per-tenant usage, API Gateway sta","explanation":"## Why This Is Asked\n\nProbes practical debugging across throttling layers in AWS and forces thinking about traceability and mitigation choices.\n\n## Key Concepts\n\n- API Gateway quotas and usage plans\n- Lambda concurrency and reserved concurrency\n- IAM throttling and error codes\n- Canary testing and rollback strategies\n\n## Code Example\n\n```bash\n# Example CLI snippet (pseudo)\naws cloudwatch get-metric-statistics --namespace AWS/ApiGateway --metric-name Throttle --dimensions Tenant=TenantA\n```\n\n## Follow-up Questions\n\n- How would you validate fixes in a staging environment with synthetic bursts?\n- What metrics would you alert on to catch regressions early?","diagram":null,"difficulty":"intermediate","tags":["aws"],"channel":"aws","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Bloomberg","LinkedIn","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T13:14:21.198Z","createdAt":"2026-01-26T13:14:21.198Z"},{"id":"q-7753","question":"You run a multi-tenant analytics platform on AWS spanning multiple accounts and regions. You must enforce tenant isolation, support per-tenant onboarding/offboarding, and enable compliant data purge while keeping dashboards responsive. Propose a concrete design using Lake Formation, S3, Glue, and IAM roles; describe data model, access controls, lifecycle, auditing, and failure handling?","answer":"Leverage a central Lake Formation catalog in a shared account with per-tenant databases and S3 prefixes per region. Map each tenant to a cross‑account IAM role LF trusts, granting SELECT/INSERT at the","explanation":"## Why This Is Asked\nThis question probes a candidate's ability to design a scalable, auditable, multi-tenant data lake using Lake Formation across accounts and regions, with robust onboarding/offboarding and purge workflows.\n\n## Key Concepts\n- Lake Formation cross-account grants and LF permissions\n- Tenant isolation via per-tenant databases/tables and S3 prefixes\n- Cross-account IAM role assumption\n- Data lifecycle and purge policies\n- Auditing with CloudTrail and LF logs\n\n## Code Example\n```bash\naws lakeformation grant-permissions \\\\\n  --principal 'arn:aws:iam::111111111111:role/TenantADataConsumer' \\\\\n  --resource '{\n      \"Table\": {\"DatabaseName\": \"tenant_db\", \"Name\": \"tbl_events\"}\n    }' \\\\\n  --permissions 'SELECT'\n```\n\n## Follow-up Questions\n- How would you test tenant onboarding and revocation of access?\n- How would you enforce per-tenant retention and purge policies across regions?","diagram":null,"difficulty":"intermediate","tags":["aws"],"channel":"aws","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Databricks","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T16:51:23.099Z","createdAt":"2026-01-26T16:51:23.099Z"},{"id":"q-7792","question":"Design a compliant multi-tenant real-time analytics platform on AWS where each tenant's data is strictly isolated, with per-tenant retention, cost controls, and governance. Describe the end-to-end architecture using Kinesis Data Streams or Firehose, S3 or Redshift Spectrum, Glue/Lake Formation, IAM boundaries, and per-tenant keys. Include onboarding/offboarding, data cataloging, cross-account access, failure handling, and auditability?","answer":"Implement per-tenant isolation with separate S3 prefixes and per-tenant KMS keys; use Lake Formation to grant table access only to the tenant, and a per-tenant Glue catalog entry. Ingest via a single ","explanation":"## Why This Is Asked\nEvaluates practical ability to implement strict tenant isolation and governance at scale, across accounts and services, with real-time data and auditability.\n\n## Key Concepts\n- Tenant isolation via per-tenant S3 prefixes and per-tenant KMS keys\n- Lake Formation permissions and Glue catalog entries per tenant\n- Scalable ingestion using Kinesis Data Streams (or Firehose) with shard-level isolation\n- Automated onboarding/offboarding with IaC; cost governance\n- Cross-account access, data residency, and comprehensive auditing (CloudTrail, Lake Formation logs)\n\n## Code Example\n```json\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\"Effect\":\"Allow\",\"Action\":[\"s3:PutObject\",\"s3:GetObject\"],\n     \"Resource\":\"arn:aws:s3:::tenant-bucket-*/tenant-data/*\",\n     \"Condition\":{\"StringEquals\":{\"aws:PrincipalAccount\":\"<TENANT_ACCOUNT_ID>\"}}\n    }\n  ]\n}\n```\n\n## Follow-up Questions\n- How would you scale per-tenant KMS keys across accounts/regions?\n- How would you validate onboarding/offboarding and retention policies in CI pipelines?","diagram":null,"difficulty":"intermediate","tags":["aws"],"channel":"aws","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Google","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T17:57:03.608Z","createdAt":"2026-01-26T17:57:03.608Z"},{"id":"q-7819","question":"You operate a multi-tenant data platform on AWS where each tenant’s data must be isolated in its own S3 bucket in a single region, accessed via short-lived tokens, with automated onboarding/offboarding. Design an end-to-end solution using KMS, per-tenant IAM roles, Lake Formation for governance, Glue catalog, and CloudTrail audit logs. Include data residency, lifecycle, DR, and incident response?","answer":"Leverage per-tenant IAM roles with STS tokens, each tenant data in its own S3 bucket tagged by tenant-id; enforce Lake Formation permissions bound to roles; use a centralized KMS key with per-tenant g","explanation":"## Why This Is Asked\nTests a practical multi-tenant data governance approach, emphasizing isolation, access control, and automation using AWS services like Lake Formation, Glue, CloudTrail, and KMS. It also covers data residency, lifecycle, DR, and incident response, which are common enterprise requirements.\n\n## Key Concepts\n- Per-tenant isolation: separate S3 buckets and tagging\n- Fine-grained access: Lake Formation and IAM roles bound to tenants\n- Centralized encryption: KMS with per-tenant grants\n- Data catalog & governance: Glue Data Catalog\n- Auditing & compliance: CloudTrail across accounts\n- Automation: onboarding/offboarding with Step Functions/Lambda\n- DR: cross-region replication and versioning\n\n## Code Example\n```javascript\n// Minimal Glue/Lake Formation-ish policy snippet (conceptual)\nconst policy = {\n  Version: '2012-10-17',\n  Statement: [\n    { Effect: 'Allow', Action: ['s3:GetObject'], Resource: ['arn:aws:s3:::tenant-data-*/{}'] },\n    { Effect: 'Allow', Action: ['glue:CreateTable','glue:GetTable'], Resource: ['arn:aws:glue:*:*:catalog'] }\n  ]\n}\n```\n\n## Follow-up Questions\n- How would you test onboarding/offboarding to ensure immediate revocation of access?\n- How would you handle tenant migrations between regions without downtime?","diagram":null,"difficulty":"intermediate","tags":["aws"],"channel":"aws","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Amazon","Discord","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T19:14:28.950Z","createdAt":"2026-01-26T19:14:28.950Z"},{"id":"q-7971","question":"Two-region DR design for a SaaS app: us-east-1 and us-west-2 hosting Aurora PostgreSQL, Redis (ElastiCache) and S3. Outline an automated DR workflow achieving RPO <= 5s and RTO <= 10m. Include replication (Aurora Global DB, Redis Global Datastore, S3 cross-region), orchestration (Route 53 failover, CloudFormation/StackSets), IAM controls, cost guards, and a tested runbook?","answer":"Use Aurora Global Database across regions for sub-5s RPO, Redis Global Datastore for fast cross-region failover, and S3 replication with versioning. Route 53 failover plus a DR CloudFormation stack to","explanation":"## Why This Is Asked\nTests practical DR knowledge in a multi-region AWS setup, emphasizing automated failover, data consistency, and runbook discipline.\n\n## Key Concepts\n- Aurora Global Database for near-zero RPO\n- Redis Global Datastore for rapid cache failover\n- S3 cross-region replication + Versioning\n- Route 53 failover routing + health checks\n- CloudFormation/StackSets for reproducible DR stacks\n- Least-privilege IAM and audit/cost controls\n\n## Code Example\n```javascript\n// Pseudo-DR failover trigger (illustrative)\nasync function triggerDR() {\n  await promoteGlobalWriter('us-west-2');\n  await updateDnsEndpoint('app-prod.example.com', 'us-west-2-endpoint');\n  await notifyOnCall('DR failover completed');\n}\n```\n\n## Follow-up Questions\n- How would you test DR without impacting production data?\n- What metrics and alarms would you monitor to detect DR readiness and success?","diagram":"flowchart TD\n  DRTrigger([DR Trigger])-->CheckRPO\n  CheckRPO-->FailoverWriter\n  FailoverWriter-->UpdateEndpoints\n  UpdateEndpoints-->Notify","difficulty":"intermediate","tags":["aws"],"channel":"aws","subChannel":"general","sourceUrl":null,"videos":null,"companies":["OpenAI","Stripe","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T02:58:30.625Z","createdAt":"2026-01-27T02:58:30.625Z"},{"id":"q-220","question":"How would you design a multi-AZ VPC architecture with Route53 latency-based routing to CloudFront, ALB, and private EC2 instances while ensuring failover within 30 seconds?","answer":"Use Route53 latency records with health checks, CloudFront with origin failover, ALB across AZs, and cross-AZ private subnets with NAT gateways.","explanation":"## Concept Overview\nDesigning a resilient AWS networking architecture requires understanding how different services interact for high availability and low latency.\n\n## Implementation Details\n\n### VPC Architecture\n- Create VPC with /16 CIDR block\n- 3 public subnets (one per AZ) for ALB and NAT\n- 3 private subnets for EC2 instances\n- Configure Internet Gateway and NAT gateways\n\n### Route53 Configuration\n```json\n{\n  \"RecordType\": \"A\",\n  \"SetIdentifier\": \"primary\",\n  \"HealthCheckId\": \"health-check-id\",\n  \"TTL\": 30\n}\n```\n\n### Load Balancer Setup\n- Application Load Balancer in public subnets\n- Cross-AZ deployment enabled\n- Health checks on /health endpoint\n- Target groups for EC2 instances\n\n### CloudFront Origin\n- Primary origin: ALB DNS name\n- Failover origin: S3 static backup\n- Origin Access Identity for security\n\n## Common Pitfalls\n- Health check intervals too long (>30s)\n- Missing cross-AZ ALB configuration\n- NAT gateway single point of failure\n- Inconsistent security group rules\n- Route53 TTL too high for quick failover","diagram":"graph TD\n    A[User] --> B[Route53]\n    B --> C[CloudFront]\n    C --> D[ALB Primary]\n    C --> E[ALB Secondary]\n    D --> F[EC2 AZ1]\n    D --> G[EC2 AZ2]\n    D --> H[EC2 AZ3]\n    E --> I[EC2 AZ1 Backup]\n    E --> J[EC2 AZ2 Backup]\n    E --> K[EC2 AZ3 Backup]\n    F --> L[Private Subnet]\n    G --> L\n    H --> L\n    I --> L\n    J --> L\n    K --> L\n    L --> M[NAT Gateway]\n    M --> N[Internet Gateway]","difficulty":"intermediate","tags":["vpc","route53","cloudfront","alb"],"channel":"aws","subChannel":"networking","sourceUrl":null,"videos":null,"companies":["Amazon","Databricks","Goldman Sachs","Microsoft","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":["multi-az vpc","route53","latency-based routing","cloudfront","alb","health checks","failover","private subnets","nat gateways"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2025-12-27T05:45:25.817Z","createdAt":"2025-12-26 12:51:07"},{"id":"q-384","question":"You're designing a multi-region SaaS application with users in North America and Europe. How would you configure Route53, CloudFront, ALB, and VPC to ensure low latency and high availability? What are the key trade-offs?","answer":"Use Route53 latency-based routing pointing to CloudFront edge locations, which cache content at regional ALBs in separate VPCs with cross-region replication.","explanation":"## Why This Is Asked\nTests understanding of AWS networking architecture, global content delivery, and trade-offs between latency, cost, and complexity.\n\n## Expected Answer\nCandidate should explain: Route53 latency-based routing to nearest CloudFront edge, CloudFront caching static content and routing dynamic requests to regional ALBs, ALBs distributing traffic across EC2 instances in separate VPCs per region, VPC peering or Transit Gateway for cross-region communication, and trade-offs like cost vs performance, data consistency vs availability.\n\n## Code Example\n```typescript\n// Route53 latency-based routing configuration\nconst hostedZone = new route53.HostedZone(this, 'HostedZone', {\n  zoneName: 'example.com'\n});\n\nnew route53.RecordSet(this, 'LatencyRecord', {\n  hostedZoneId: hostedZone.zoneId,\n  recordName: 'api.example.com',\n  type: 'A',\n  setIdentifier: 'us-east-1',\n  region: 'us-east-1',\n  latency: 50,\n  target: albUsEast.loadBalancer.dnsName\n});\n```\n\n## Follow-up Questions\n- How would you handle database replication between regions?\n- What happens if CloudFront cache misses and how do you optimize?\n- How do you implement failover between regions?","diagram":"flowchart TD\n    A[User Request] --> B[Route53 Latency Routing]\n    B --> C{User Region}\n    C -->|North America| D[CloudFront US-East Edge]\n    C -->|Europe| E[CloudFront EU-West Edge]\n    D --> F[ALB US-East]\n    E --> G[ALB EU-West]\n    F --> H[VPC US-East Subnets]\n    G --> I[VPC EU-West Subnets]\n    H --> J[EC2 Instances US]\n    I --> K[EC2 Instances EU]\n    J --> L[RDS Cross-Region Replication]\n    K --> L","difficulty":"intermediate","tags":["vpc","route53","cloudfront","alb"],"channel":"aws","subChannel":"networking","sourceUrl":null,"videos":{"shortVideo":"https://www.youtube.com/watch?v=27r4Bzuj5NQ","longVideo":null},"companies":["Airtable","Cisco","Epic Games"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-23T13:16:21.927Z","createdAt":"2025-12-26 12:51:05"},{"id":"gh-66","question":"How does serverless computing abstract infrastructure management and what are its key execution characteristics?","answer":"Serverless computing abstracts infrastructure through event-driven functions that auto-scale, with pay-per-use billing and zero server maintenance.","explanation":"## Concept Overview\nServerless computing is a cloud execution model where providers manage infrastructure, scaling, and resource allocation automatically. Developers focus solely on business logic through functions triggered by events.\n\n## Implementation\nServerless platforms use container-based execution environments that spin up on demand:\n\n```javascript\n// AWS Lambda example\nexports.handler = async (event) => {\n  const { action, data } = JSON.parse(event.body);\n  \n  switch(action) {\n    case 'process':\n      return await processData(data);\n    case 'validate':\n      return await validateInput(data);\n    default:\n      throw new Error('Unsupported action');\n  }\n};\n```\n\nKey execution characteristics:\n- **Cold Starts**: Initial invocation latency (100-3000ms)\n- **Stateless**: Each execution is independent\n- **Resource Limits**: Memory (128-3008MB), duration (max 15 minutes)\n- **Auto-scaling**: Concurrent instances based on request volume\n\n## Trade-offs\n\n**Pros:**\n- Zero operational overhead\n- Cost-effective for sporadic workloads\n- Built-in high availability and fault tolerance\n- Automatic scaling from 0 to thousands\n\n**Cons:**\n- Cold start latency\n- Vendor lock-in\n- Limited execution time and resources\n- Debugging complexity in distributed environments\n\n**When to use:**\n- API endpoints with unpredictable traffic\n- Data processing pipelines\n- Scheduled tasks and cron jobs\n- Real-time file processing\n\n## Common Pitfalls\n\n1. **Ignoring Cold Starts**: Not implementing provisioned concurrency for latency-sensitive applications\n\n2. **Stateful Anti-patterns**: Storing local data between invocations\n```javascript\n// BAD - stateful approach\nlet counter = 0;\nexports.handler = async (event) => {\n  counter++; // Lost between invocations\n  return { count: counter };\n};\n\n// GOOD - stateless with external storage\nexports.handler = async (event) => {\n  const currentCount = await getCounterFromDB();\n  await updateCounterInDB(currentCount + 1);\n  return { count: currentCount + 1 };\n};\n```\n\n3. **Timeout Misconfiguration**: Not setting appropriate timeouts for external service calls\n\n4. **Resource Over-provisioning**: Allocating excessive memory, increasing costs unnecessarily\n\n5. **Missing Error Handling**: Not implementing retry logic for transient failures","diagram":"flowchart TD\n    A[Client Request] --> B[API Gateway]\n    B --> C{Event Trigger}\n    C -->|HTTP| D[HTTP Function]\n    C -->|File Upload| E[Storage Function]\n    C -->|Database Change| F[DB Trigger Function]\n    \n    D --> G[Function Container]\n    E --> G\n    F --> G\n    \n    G --> H[Business Logic]\n    H --> I[External Services]\n    H --> J[Database]\n    \n    I --> K[Response]\n    J --> K\n    K --> L[Client]\n    \n    M[Auto Scaling] --> G\n    N[Monitoring] --> O[Logs & Metrics]\n    G --> N\n    \n    style G fill:#e1f5fe\n    style M fill:#f3e5f5\n    style N fill:#fff3e0","difficulty":"beginner","tags":["serverless","lambda"],"channel":"aws","subChannel":"serverless","sourceUrl":null,"videos":{"shortVideo":"https://www.youtube.com/watch?v=W_VV2Fx32_Y","longVideo":"https://www.youtube.com/watch?v=Fx3ZGy-mbV4"},"companies":["Airbnb","Amazon","Google","Microsoft","Uber"],"eli5":"Imagine you have a magic toy box that makes any toy you want, exactly when you want it! When you ask for a car, poof - a car appears. When you're done playing, it disappears. You don't have to clean up, store toys, or even know how the magic works. Serverless computing is like that magic toy box for computer programs. When someone needs your program to do something, it magically appears, does the job, then vanishes. You only pay for the few seconds it was working, like paying for just one ride at the playground instead of buying the whole playground. The best part? You never have to worry about fixing broken toys or organizing the toy box - the magic takes care of everything!","relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-24T12:54:30.274Z","createdAt":"2025-12-26 12:51:05"},{"id":"q-246","question":"How would you design a serverless order processing workflow using AWS Step Functions with Lambda functions, implementing specific retry patterns, error handling, and state management?","answer":"Design a state machine with Task states for order validation, payment processing, and inventory updates, implementing exponential backoff retries, custom error handling, and DynamoDB state persistence with distributed tracing.","explanation":"## Interview Context\nThis question evaluates serverless workflow design, AWS Step Functions expertise, and production-ready error handling patterns in distributed systems.\n\n## Key Components\n- **State Machine Design**: Task states for order validation, payment processing, inventory updates, and notification delivery with parallel branches for concurrent operations\n- **Retry Patterns**: Exponential backoff with jitter (max 3 retries), custom retry policies for transient failures, and circuit breaker patterns for downstream services\n- **Error Handling**: Catch blocks for specific error types (PaymentFailed, InsufficientInventory), dead letter queue integration, and custom error metrics with CloudWatch\n- **State Management**: DynamoDB for order state persistence, distributed tracing with X-Ray, and idempotency keys for safe retries\n- **Implementation**: Lambda functions with proper IAM roles, payload validation schemas, and timeout configurations aligned with Step Function limits\n\n## Code Example\n```json\n{\n  \"Comment\": \"Serverless order processing workflow\",\n  \"StartAt\": \"ValidateOrder\",\n  \"States\": {\n    \"ValidateOrder\": {\n      \"Type\": \"Task\",\n      \"Resource\": \"arn:aws:lambda:us-east-1:123456789012:function:validate-order\",\n      \"Retry\": [{\n        \"ErrorEquals\": [\"Lambda.Timeout\", \"Lambda.Unknown\"],\n        \"IntervalSeconds\": 2,\n        \"MaxAttempts\": 3,\n        \"BackoffRate\": 2.0\n      }],\n      \"Catch\": [{\n        \"ErrorEquals\": [\"InvalidOrder\"],\n        \"Next\": \"HandleValidationError\",\n        \"ResultPath\": \"$.error\"\n      }],\n      \"Next\": \"ProcessPayment\"\n    },\n    \"ProcessPayment\": {\n      \"Type\": \"Task\",\n      \"Resource\": \"arn:aws:lambda:us-east-1:123456789012:function:process-payment\",\n      \"Retry\": [{\n        \"ErrorEquals\": [\"PaymentGatewayTimeout\"],\n        \"IntervalSeconds\": 5,\n        \"MaxAttempts\": 2,\n        \"BackoffRate\": 1.5\n      }],\n      \"Catch\": [{\n        \"ErrorEquals\": [\"PaymentFailed\"],\n        \"Next\": \"HandlePaymentError\",\n        \"ResultPath\": \"$.paymentError\"\n      }],\n      \"Next\": \"UpdateInventory\"\n    },\n    \"UpdateInventory\": {\n      \"Type\": \"Task\",\n      \"Resource\": \"arn:aws:lambda:us-east-1:123456789012:function:update-inventory\",\n      \"TimeoutSeconds\": 30,\n      \"HeartbeatSeconds\": 10,\n      \"Retry\": [{\n        \"ErrorEquals\": [\"DynamoDB.ProvisionedThroughputExceeded\"],\n        \"IntervalSeconds\": 10,\n        \"MaxAttempts\": 3\n      }],\n      \"Next\": \"SendNotification\"\n    },\n    \"SendNotification\": {\n      \"Type\": \"Parallel\",\n      \"Branches\": [\n        {\n          \"StartAt\": \"EmailCustomer\",\n          \"States\": {\n            \"EmailCustomer\": {\n              \"Type\": \"Task\",\n              \"Resource\": \"arn:aws:lambda:us-east-1:123456789012:function:send-email\",\n              \"End\": true\n            }\n          }\n        },\n        {\n          \"StartAt\": \"UpdateAnalytics\",\n          \"States\": {\n            \"UpdateAnalytics\": {\n              \"Type\": \"Task\",\n              \"Resource\": \"arn:aws:lambda:us-east-1:123456789012:function:update-analytics\",\n              \"End\": true\n            }\n          }\n        }\n      ],\n      \"Next\": \"OrderComplete\"\n    },\n    \"HandleValidationError\": {\n      \"Type\": \"Task\",\n      \"Resource\": \"arn:aws:lambda:us-east-1:123456789012:function:log-validation-error\",\n      \"Next\": \"OrderFailed\"\n    },\n    \"HandlePaymentError\": {\n      \"Type\": \"Task\",\n      \"Resource\": \"arn:aws:lambda:us-east-1:123456789012:function:handle-payment-error\",\n      \"Next\": \"OrderFailed\"\n    },\n    \"OrderComplete\": {\n      \"Type\": \"Succeed\"\n    },\n    \"OrderFailed\": {\n      \"Type\": \"Fail\",\n      \"Cause\": \"Order processing failed\",\n      \"Error\": \"OrderProcessingError\"\n    }\n  }\n}\n```\n\n## Production Considerations\n- **Monitoring**: CloudWatch metrics for state transitions, error rates, and execution times\n- **Security**: IAM role least privilege, VPC endpoints for private resources, and encryption at rest\n- **Scalability**: Provisioned concurrency for critical Lambdas, DynamoDB auto-scaling, and Step Function throttling limits\n- **Testing**: Integration tests with mock services, chaos engineering for failure scenarios, and blue-green deployments","diagram":"graph TD\n    A[Order Received] --> B[Validate Order Lambda]\n    B -->|Success| C[Process Payment Lambda]\n    B -->|Invalid Order| D[Send Failure Notification]\n    C -->|Payment Success| E[Update Inventory Lambda]\n    C -->|Payment Failed| F[Retry Payment]\n    F -->|3 Attempts Failed| G[Cancel Order]\n    E --> H[Send Confirmation Lambda]\n    H --> I[Order Complete]\n    D --> J[End]\n    G --> J\n    I --> K[End]","difficulty":"intermediate","tags":["lambda","api-gateway","step-functions"],"channel":"aws","subChannel":"serverless","sourceUrl":null,"videos":null,"companies":["Amazon","Google","Microsoft","Netflix","Salesforce","Stripe"],"eli5":"Imagine you're building a LEGO castle with friends! Each friend has a special job - one checks if you have enough bricks, another builds the walls, and one adds the roof. If a friend drops their bricks, they can try again 3 times before asking for help. If they still can't do it, you call the teacher over! You keep a checklist to remember which part is finished and which still needs work. Sometimes you need to make choices - if you're building a tower, you go up, but if it's a bridge, you go across. Everything happens step by step, like following a recipe for making cookies!","relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-28T01:59:07.372Z","createdAt":"2025-12-26 12:51:07"},{"id":"q-292","question":"How would you design a data lifecycle strategy for a media company storing petabytes of video content requiring immediate access, archiving, and cost optimization across AWS storage services?","answer":"Implement tiered S3 storage with lifecycle policies: S3 Standard for hot content (first 30 days), S3 Intelligent-Tiering for variable access, S3 Glacier Instant Retrieval for warm archives (30-90 days), and S3 Glacier Deep Archive for compliance (365+ days), with cost monitoring via AWS Cost Explorer and data transfer optimization through CloudFront.","explanation":"## Why Asked\nTests comprehensive understanding of AWS storage economics, lifecycle management, and performance optimization at scale.\n\n## Key Concepts\nAWS storage hierarchy, lifecycle policies, cost optimization strategies, data transfer patterns, monitoring tools, compliance requirements.\n\n## Architecture Design\n**Hot Tier (0-30 days)**: S3 Standard ($0.023/GB) for frequently accessed content, serving immediate user requests.\n**Warm Tier (30-90 days)**: S3 Glacier Instant Retrieval ($0.004/GB) for content with occasional access needs.\n**Cold Tier (90-365 days)**: S3 Glacier Flexible Retrieval ($0.004/GB) for archival with 3-12 hour retrieval.\n**Deep Archive (365+ days)**: S3 Glacier Deep Archive ($0.00099/GB) for compliance and long-term retention.\n\n## Cost Optimization\nCalculate monthly storage costs: 100TB at $0.023 = $2,300/month (Standard) vs $99/month (Deep Archive) - 96% savings. Use S3 Intelligent-Tiering for unpredictable access patterns to avoid manual monitoring.\n\n## Data Transfer Strategy\nImplement CloudFront for global content delivery, reducing direct S3 GET costs from $0.09/GB to $0.085/GB while improving latency. Use S3 Transfer Acceleration for bulk uploads ($0.04/GB + $0.004/10,000 requests).\n\n## Monitoring & Compliance\nSet up AWS Cost Explorer alerts for storage budgets, CloudWatch metrics for retrieval patterns, and S3 Inventory reports for compliance auditing. Implement S3 Object Lock for WORM storage requirements and cross-region replication for disaster recovery.\n\n## Implementation\n```json\n{\n  \"Rules\": [{\n    \"ID\": \"MediaLifecycle\",\n    \"Status\": \"Enabled\",\n    \"Filter\": {\"Prefix\": \"videos/\"},\n    \"Transitions\": [\n      {\"Days\": 30, \"StorageClass\": \"INTELLIGENT_TIERING\"},\n      {\"Days\": 90, \"StorageClass\": \"GLACIER_IR\"},\n      {\"Days\": 365, \"StorageClass\": \"DEEP_ARCHIVE\"}\n    ],\n    \"NoncurrentVersionTransitions\": [\n      {\"NoncurrentDays\": 7, \"StorageClass\": \"GLACIER\"}\n    ]\n  }]\n}\n```\n\n## Real-world Considerations\nFor 1PB media library: Expect $23,000/month initially, dropping to $5,000/month after lifecycle transitions. Plan for retrieval costs: $0.01/GB for expedited restores vs $0.0025/GB for standard. Implement data validation using S3 Batch Operations and ensure compliance with GDPR/CCPA through appropriate retention policies.","diagram":"flowchart TD\n  A[New Video Upload] --> B[S3 Standard]\n  B --> C{30 days?}\n  C -->|Yes| D[S3 Standard IA]\n  C -->|No| B\n  D --> E{90 days?}\n  E -->|Yes| F[S3 Glacier]\n  E -->|No| D\n  F --> G{365 days?}\n  G -->|Yes| H[S3 Deep Archive]\n  G -->|No| F","difficulty":"advanced","tags":["s3","ebs","efs","glacier"],"channel":"aws","subChannel":"storage","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Netflix","Youtube"],"eli5":null,"relevanceScore":null,"voiceKeywords":["s3 standard","s3 ia","glacier","lifecycle policies","automated transitions","cost optimization","storage tiers"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2025-12-28T02:04:44.678Z","createdAt":"2025-12-26 12:51:07"},{"id":"q-307","question":"What are the key differences between S3, EBS, and EFS in terms of performance, scalability, and use cases?","answer":"S3 offers virtually unlimited object storage with 99.999999999% durability and 99.99% availability, costing $0.023/GB/month for standard storage; EBS provides high-performance block storage up to 64,000 IOPS with sub-millisecond latency, priced at $0.08/GB/month plus IOPS; EFS delivers shared file storage across multiple AZs with burst throughput up to 3GB/s, costing $0.30/GB/month.","explanation":"## Why Asked\nTests deep understanding of AWS storage services, including performance metrics, SLAs, and cost optimization strategies for production workloads.\n\n## Key Concepts\nObject vs Block vs File storage, durability SLAs, performance benchmarks, pricing models, throughput optimization, and multi-AZ considerations.\n\n## Performance Metrics & Cost Comparison\n\n**S3 (Simple Storage Service)**\n- Performance: 3,500-5,500 PUT/POST requests per second per prefix\n- Durability: 99.999999999% (11 9's)\n- Availability: 99.99% for Standard storage\n- Cost: $0.023/GB/month (Standard), $0.0004/1,000 PUT requests\n- Use Case: Static assets, backups, data lakes, content distribution\n\n**EBS (Elastic Block Store)**\n- Performance: gp3 volumes up to 16,000 IOPS baseline, 64,000 IOPS burst\n- Latency: Sub-millisecond for most operations\n- Cost: $0.08/GB/month + $0.005/provisioned IOPS-month + $0.06/MB/s throughput-month\n- Use Case: Database storage, boot volumes, high-performance applications\n\n**EFS (Elastic File System)**\n- Performance: Burst throughput up to 3GB/s, scales with storage size\n- Availability: 99.9% (Standard) or 99.99% (Max I/O)\n- Cost: $0.30/GB/month + $0.06/MB/s provisioned throughput\n- Use Case: Content management, shared file systems, web serving\n\n## Code Example\n```python\n# S3 - Cost-effective for large objects with infrequent access\nimport boto3\ns3 = boto3.client('s3')\ns3.put_object(\n    Bucket='my-bucket',\n    Key='data/large-dataset.json',\n    Body=data,\n    StorageClass='STANDARD_IA'  # 25% cheaper than Standard\n)\n\n# EBS - Optimized for database workloads\nimport ec2\ndatabase_volume = ec2.create_volume(\n    Size=500,\n    VolumeType='gp3',\n    Iops=8000,  # Database workload optimization\n    Throughput=250,\n    TagSpecifications=[{\n        'ResourceType': 'volume',\n        'Tags': [{'Key': 'Environment', 'Value': 'production'}]\n    }]\n)\n\n# EFS - Shared file system with cost monitoring\nimport subprocess\n# Monitor EFS burst credits\nsubprocess.run(['aws', 'efs', 'describe-file-systems', '--file-system-id', 'fs-12345'])\n```\n\n## Cost Optimization Tips\n- Use S3 Intelligent-Tiering for unknown access patterns\n- Right-size EBS volumes and use gp3 for better cost/performance ratio\n- Enable EFS lifecycle policies for EFS Infrequent Access storage class","diagram":"flowchart TD\n  A[Storage Need] --> B{Data Type?}\n  B -->|Objects| C[S3]\n  B -->|Block| D[EBS]\n  B -->|Files| E[EFS]\n  C --> F[Static Assets, Archives]\n  D --> G[Databases, Boot Volumes]\n  E --> H[Shared File Systems]","difficulty":"intermediate","tags":["s3","ebs","efs","glacier"],"channel":"aws","subChannel":"storage","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":"https://www.youtube.com/watch?v=_CN7KqC3y3s"},"companies":["Amazon","Google","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-29T06:57:49.960Z","createdAt":"2025-12-26 12:51:05"},{"id":"q-370","question":"You're designing a file storage system for Canva's design assets. Users upload large PSD files (up to 10GB) that need versioning and quick access. How would you architect this using AWS storage services, considering cost, performance, and durability?","answer":"For Canva's PSD file storage system, I'd use S3 Standard for active files with immediate access, S3 Standard-IA for versioning older PSDs, and Glacier Deep Archive for long-term archival, with lifecycle policies to automatically transition 10GB files between tiers based on access patterns while CloudFront CDN ensures quick user access.","explanation":"## Why This Is Asked\nTests practical AWS storage knowledge, cost optimization, and understanding of trade-offs between performance and expense - crucial for Canva's asset-heavy platform.\n\n## Expected Answer\nStrong candidates discuss S3 storage classes, lifecycle policies, CloudFront for CDN, versioning, cross-region replication, and cost calculations. They should mention EBS vs EFS vs S3 trade-offs.\n\n## Code Example\n```typescript\n// S3 lifecycle policy for cost optimization\nconst lifecyclePolicy = {\n  Rules: [{\n    ID: 'DesignAssetLifecycle',\n    Status: 'Enabled',\n    Transitions: [\n      { Days: 30, StorageClass: 'STANDARD_IA' },\n      { Days: 90, StorageClass: 'GLACIER' },\n      { Days: 365, StorageClass: 'DEEP_ARCHIVE' }\n    ]\n  }]\n};\n```\n\n## Follow-up Questions\n- How would you handle concurrent uploads of the same file?\n- What's your strategy for disaster recovery?\n- How would you optimize for global user access?","diagram":"flowchart TD\n    A[User Uploads 10GB PSD] --> B[S3 Standard - Active Files]\n    B --> C[CloudFront CDN Edge]\n    C --> D[Global User Access]\n    B --> E{30 Days Old?}\n    E -->|Yes| F[S3 Standard-IA]\n    E -->|No| B\n    F --> G{90 Days Old?}\n    G -->|Yes| H[S3 Glacier]\n    G -->|No| F\n    H --> I{365 Days Old?}\n    I -->|Yes| J[Glacier Deep Archive]\n    I -->|No| H","difficulty":"intermediate","tags":["s3","ebs","efs","glacier"],"channel":"aws","subChannel":"storage","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Affirm","Booking.com","Canva"],"eli5":null,"relevanceScore":null,"voiceKeywords":["s3 standard","s3 standard-ia","glacier deep archive","lifecycle policies","cloudfront cdn","versioning","cross-region replication","storage classes","cost optimization","disaster recovery","concurrent uploads","global user access"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2026-01-05T06:49:31.057Z","createdAt":"2025-12-26 12:51:04"}],"subChannels":["compute","database","general","networking","serverless","storage"],"companies":["Adobe","Affirm","Airbnb","Airtable","Amazon","Anthropic","Apple","Bloomberg","Booking.com","Canva","Cisco","Citadel","Cloudflare","Cohere","Coinbase","Databricks","Discord","DoorDash","Epic Games","Figma","Goldman Sachs","Google","Hashicorp","Hugging Face","IBM","Instacart","LinkedIn","Lyft","Meta","Micron","Microsoft","MongoDB","NVIDIA","Netflix","OpenAI","Oracle","Oscar Health","Palo Alto Networks","PayPal","Plaid","Robinhood","Salesforce","Scale Ai","Slack","Snap","Snowflake","Square","Stripe","Tesla","Twitter","Two Sigma","Uber","Youtube","Zoom"],"stats":{"total":84,"beginner":20,"intermediate":31,"advanced":33,"newThisWeek":38}}