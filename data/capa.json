{"questions":[{"id":"q-1091","question":"Scenario: An OTA firmware update causes GPS altitude drift in a subset of IoT devices across regions. Design a CAPA program to detect, collect evidence, and prevent recurrence. Include: 1) a CAPA data model, 2) a lifecycle state machine, 3) a minimal REST API for CAPAs with device logs, 4) region/device-type metrics (recurrence rate, MTTR, false positives), 5) an RCA template and a canary rollback plan for OTA updates?","answer":"Design a CAPA program with a data model including fields for CAPA id, incident id, device_id, region, firmware_version, evidence, actions, status, and timestamps; a lifecycle from DETECT to CLOSED; a ","explanation":"## Why This Is Asked\nTests end-to-end CAPA design for hardware deployments, emphasizing evidence capture, state management, and canary safety.\n\n## Key Concepts\n- CAPA data model tailored to IoT/OTA scenarios\n- Lifecycle state machine for CAPA progression\n- Minimal API design for linking CAPAs to device logs\n- Region/device-type metrics to prove effectiveness\n- RCA templates and safe rollback strategies\n\n## Code Example\n```javascript\n// Minimal CAPA creation schema (pseudo)\nconst createCAPA = (payload) => ({ id: generateId(), ...payload, status: 'OPEN' })\n```\n\n## Follow-up Questions\n- How would you ensure evidence provenance and tamper-evidence across devices?\n- How would you test the canary rollback plan with real devices at scale?","diagram":"flowchart TD\n  A[Detect Drift] --> B[Collect Evidence]\n  B --> C[Create CAPA]\n  C --> D[Canary OTA]\n  D --> E[Evaluate Metrics]","difficulty":"intermediate","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Plaid","Snowflake","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T22:23:48.606Z","createdAt":"2026-01-12T22:23:48.606Z"},{"id":"q-1111","question":"Scenario: Production ML feature store drift after a data refresh degrades latency and CTR across regions due to stale features and late streaming data. Propose a CAPA program: 1) a CAPA data model capturing evidence and artifacts, 2) a lifecycle state machine for CAPAs, 3) a minimal REST API to create/update CAPAs with linked logs/traces, 4) region-aware metrics to prove containment and recurrence, 5) an RCA template and a canary rollout plan to prevent recurrence during future refreshes?","answer":"CAPA data model should store evidence (logs, traces, feature clocks) and artifacts (RCA, mitigations, rollback scripts). Lifecycle: detected → triaged → containment → root cause → corrective action → ","explanation":"## Why This Is Asked\nTests ability to design scalable CAPA for data drift in production ML pipelines, including evidence management, lifecycle, API design, and robust canary strategies.\n\n## Key Concepts\n- CAPA data model with evidence links\n- Lifecycle state machine for CAPA progression\n- Minimal REST API design for CAPAs\n- Region-aware metrics (MTTR, recurrence, containment)\n- RCA templates and canary rollout planning\n\n## Code Example\n```javascript\n// CAPA data model sketch\n{\n  \"id\": \"CAPA-20260112-001\",\n  \"issue\": \"Feature store drift causing latency and CTR degradation\",\n  \"regions\": [\"us-east-1\",\"eu-west-1\"],\n  \"evidenceLinks\": [\"https://log/123\",\"https://trace/456\"],\n  \"lifecycle\": \"detected\",\n  \"actions\": [\n    {\"step\":\"containment\",\"owner\":\"SRE\",\"status\":\"pending\"}\n  ],\n  \"canaryPlan\": {\"enabled\":true,\"regions\":[\"us-east-1\"]}\n}\n```\n\n## Follow-up Questions\n- How would you scale the CAPA API for high throughput and cross-region consistency?\n- What RCA sections ensure actionable preventive measures and auditability?","diagram":null,"difficulty":"advanced","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T23:20:43.169Z","createdAt":"2026-01-12T23:20:43.169Z"},{"id":"q-1132","question":"Scenario: batch ingestion misses PII masking in two tenants across regions, raising privacy risk. Design a CAPA program: 1) a CAPA data model capturing evidence and artifacts, 2) a lifecycle state machine for CAPA progression, 3) a minimal REST API to create/update CAPAs with linked logs/traces, 4) tenant/region-aware metrics to prove containment and recurrence (MTTD, MTTR, false positives), 5) an RCA template and a canary rollout plan to validate policy enforcement before global deployment?","answer":"Approach: define CAPA data model with evidence, logs, and artifacts; lifecycle states: detected, triaged, in_progress, contained, closed; REST API endpoints POST /caps, PUT /caps/{id} with linked trac","explanation":"## Why This Is Asked\n\nTests ability to design end-to-end CAPA workflow for data privacy policy failures, including evidence capture and cross-tenant rollout.\n\n## Key Concepts\n\n- CAPA data modeling for evidence and artifacts\n- Lifecycle state machine and ownership\n- Minimal REST API design with linked logs/traces\n- Tenant/region metrics for containment and recurrence\n- RCA templates and canary rollout planning\n\n## Code Example\n\n```javascript\n// CAPA core types (overview)\ninterface CAPA {\n  id: string;\n  title: string;\n  evidence: string[];\n  artifacts: string[];\n  state: 'detected'|'triaged'|'in_progress'|'contained'|'closed';\n  createdAt: string;\n  updatedAt?: string;\n  links?: { type: string; url: string }[];\n}\n```\n\n## Follow-up Questions\n\n- How would you scale CAPA data stores across regions?\n- How to ensure regulatory auditability and evidence integrity?","diagram":"flowchart TD\n  A[Incident Detected] --> B[Evidence Collected]\n  B --> C[CAPA Created]\n  C --> D[Root Cause Analysis]\n  D --> E[Containment]\n  E --> F[Preventive Action]\n  F --> G[Canary Validation]\n  G --> H[Global Rollout]","difficulty":"advanced","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","MongoDB","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T01:23:32.007Z","createdAt":"2026-01-13T01:23:32.007Z"},{"id":"q-1156","question":"Scenario: A distributed streaming analytics pipeline processes click events for an online marketplace. A recently deployed shard rebalancing causes out-of-order events in two regions, leading to incorrect revenue attribution and fraud alerts. Design a CAPA program that covers: 1) a CAPA data model that captures evidence (events, traces, timestamps, orderings) and artifacts (config, manifest, canary results), 2) a lifecycle state machine for CAPA progression, 3) a minimal REST API to create/update CAPAs with linked logs/traces and event metadata, 4) region- and shard-aware metrics to prove containment (recurrence rate, MTTR, misattribution rate), 5) an RCA template and a canary-rollback plan for shard rebalancing?","answer":"Propose a CAPA program for out-of-order events from shard rebalancing in a streaming analytics pipeline. Data model: CAPA(id, region, shard, evidence: [eventId, traceId, timestamp], artifacts: [config","explanation":"Why This Is Asked\nTests end-to-end CAPA thinking for distributed streaming with event-ordering issues and rollback safety.\n\nKey Concepts\n- CAPA data model for evidence and artifacts\n- Lifecycle state machine with containment and prevention\n- Minimal REST API for linking logs/traces to CAPAs\n- Region/shard-aware metrics and attribution correctness\n- RCA template and canary rollback strategy\n\nCode Example\n```javascript\n// Minimal REST API sketch (Express)\nconst express = require('express');\nconst app = express();\napp.use(express.json());\napp.post('/caps', (req, res) => {\n  // validate and persist CAPA with evidence refs\n  res.status(201).send({ id: 'capa-123' });\n});\n```\n\nFollow-up Questions\n- How would you validate and test the canary rollback under varying traffic patterns?\n- What data retention and privacy controls are needed for CAPA artifacts?","diagram":"flowchart TD\nA[Incident Observed] --> B[Triage & Evidence]\nB --> C[Containment via Canary]\nC --> D[Root Cause Analysis]\nD --> E[Corrective Action]\nE --> F[Preventive Action]\nF --> G[Close CAPA]","difficulty":"advanced","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Apple"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T03:28:35.156Z","createdAt":"2026-01-13T03:28:35.156Z"},{"id":"q-1200","question":"Scenario: A global real-time telemetry platform for autonomous vehicles experiences intermittent PII exposure due to a log-redaction misconfiguration after a software update in two regions. Design a CAPA program to detect, document, and prevent recurrence. Include: 1) a CAPA data model capturing evidence and artifacts, 2) a lifecycle state machine, 3) a minimal REST API to create/update CAPAs with linked logs/traces, 4) tenant- and region-aware metrics (recurrence rate, MTTR, false positives), 5) an RCA template and a canary-based rollback/patch plan, 6) a policy gate preventing deployment until redact coverage is above threshold?","answer":"CAPA design: use a normalized CAPA data model with fields like id, tenants, regions, evidence_refs (logs/traces), artifacts, state, severity, and timestamps. Lifecycle: New → Open → Investigating → Co","explanation":"## Why This Is Asked\n\nTests ability to design end-to-end CAPA workflow for privacy incidents, including across tenants and regions, with concrete artifacts and controls.\n\n## Key Concepts\n\n- CAPA data model, state machine, API design, region-tenant metrics, RCA, canary rollback, policy gates.\n\n## Code Example\n\n```javascript\n// Example CAPA payload shape\ninterface CAPA { id: string; tenants: string[]; regions: string[]; evidence_refs: string[]; artifacts: string[]; state: string; severity: string; timestamps: Record<string,string>; }\n```\n\n## Follow-up Questions\n\n- How would you verify redact coverage? What tests would you add?\n- How do you design region-aware dashboards to avoid alert fatigue?\n","diagram":"flowchart TD\nA[New CAPA] --> B[Open]\nB --> C[Investigating]\nC --> D[Containment]\nD --> E[Corrective Action]\nE --> F[Verified]\nF --> G[Closed]","difficulty":"advanced","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T04:48:36.347Z","createdAt":"2026-01-13T04:48:36.347Z"},{"id":"q-1426","question":"Scenario: A multi-tenant SaaS billing system release unintentionally changes pricing rules for a subset of tenants, triggering incorrect invoices and revenue churn. Design a beginner CAPA program to address this. Your task: 1) propose a CAPA data model capturing evidence (invoices, logs, traces) and artifacts (config, feature flags, pricing rules); 2) define a lifecycle state machine for CAPA progression; 3) outline a minimal REST API to create/update CAPAs with linked billing events; 4) specify tenant- and plan-level metrics to prove containment (recurrence rate, MTTR, false positives); 5) provide an RCA template and a canary-based patch plan before full rollout; 6) draft a simple policy gate that prevents deployment until pricing rules pass a preflight check?","answer":"CAPA data model: capa_id, tenant_id, invoice_id, evidence_refs, artifacts, pricing_rule_version; lifecycle: detected -> validated -> in_progress -> contained -> resolved -> closed; API: POST/PUT /caps","explanation":"## Why This Is Asked\nTests practical CAPA thinking: data modeling, lifecycle, minimal API, and measurable outcomes for a realistic beginner scenario.\n\n## Key Concepts\n- CAPA data model with evidence and artifacts\n- Lifecycle state machine for progress\n- Minimal REST API for CAPAs\n- Tenant/plan metrics to prove containment\n- RCA template and canary remediation plan\n\n## Code Example\n```javascript\n// example schema outline\nconst CAPASchema = {\n  capa_id: String,\n  tenant_id: String,\n  invoice_id: String,\n  evidence_refs: [String],\n  artifacts: [String],\n  pricing_rule_version: String\n}\n```\n\n## Follow-up Questions\n- How would you version artifacts and signatures for integrity?\n- What alerting would you add to detect recurring pricing issues?","diagram":null,"difficulty":"beginner","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T16:47:28.090Z","createdAt":"2026-01-13T16:47:28.090Z"},{"id":"q-1450","question":"Scenario: A global multi-tenant data platform implements a new consent-logging feature. After rollout, a bug in the consent service causes PII redaction failures in two regions, risking data exposure and compliance violations. Design a CAPA program to detect, collect evidence, and prevent recurrence. Include: 1) a CAPA data model, 2) a lifecycle state machine, 3) a minimal REST API to create/update CAPAs with linked logs/traces, 4) region- and tenant-aware metrics (recurrence rate, MTTR, false positives), 5) an RCA template and a canary rollout plan to validate fixes before global deployment?","answer":"Design a CAPA: data model with CAPA, evidence, artifacts, linked logs/traces; lifecycle: Identified → Investigating → Contained → Eradicated → Verified → Closed; REST API: POST /caps to create, PATCH ","explanation":"## Why This Is Asked\nTests the ability to architect an end-to-end CAPA workflow for a data-platform reliability incident that spans regions and tenants, including how to model evidence and link traces for audit trails.\n\n## Key Concepts\n- CAPA data model with evidence and linked artifacts\n- Lifecycle state machine for CAPA progression\n- Minimal REST API design for creating/updating CAPAs\n- Region- and tenant-aware metrics (recurrence, MTTR, false positives)\n- RCA templates and canary rollout planning\n\n## Code Example\n```javascript\n// Example CAPA schema (illustrative)\n{\n  id: string,\n  title: string,\n  region: string,\n  tenant_id: string,\n  status: string,\n  evidence: string[],\n  artifacts: string[],\n  remediation: object\n}\n```\n\n## Follow-up Questions\n- How would you automate evidence collection across services?\n- How would you handle new tenants with no historical CAPA data?","diagram":"flowchart TD\n  A[Identified] --> B[Investigating]\n  B --> C[Contained]\n  C --> D[Eradicated]\n  D --> E[Verified]\n  E --> F[Closed]","difficulty":"intermediate","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","LinkedIn","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T17:46:10.476Z","createdAt":"2026-01-13T17:46:10.477Z"},{"id":"q-1485","question":"**Scenario**: A cross-tenant data export feature in a multi-tenant SaaS app accidentally exposes data from unrelated tenants during a canary rollout in two regions. Design a beginner **CAPA** program to detect, document, and prevent recurrence. Your task: 1) propose a CAPA data model framing evidence and artifacts, 2) define a lifecycle state machine, 3) outline a minimal REST API to create/update CAPAs with linked logs/traces, 4) specify tenant-scoped metrics for containment, 5) provide an RCA template and a canary-based fix plan to validate before broader rollout?","answer":"CAPA data model: CAPA{id, tenantId, region, evidenceRefs, severity, rootCause, correctiveAction, preventiveAction, status, timestamps}. Lifecycle: new→triage→investigate→remediate→verify→closed. API: POST /capas (create), GET /capas/{id}, PUT /capas/{id} (update), POST /capas/{id}/evidence (link logs/traces). Tenant-scoped metrics: dataExposureEvents{tenantId, region, count}, containmentTime{tenantId, region, ms}, rcaCompletionRate{tenantId, percentage}, canaryValidationSuccess{tenantId, boolean}. RCA template: incidentSummary, timeline, rootCauseAnalysis, impactAssessment, correctiveActions, preventiveActions, verificationCriteria. Canary fix plan: 1) implement fix in single tenant, 2) validate no data leakage, 3) monitor containment metrics, 4) gradual regional rollout, 5) full deployment after 24h clean monitoring.","explanation":"## Why This Is Asked\nTests ability to translate incident learnings into a repeatable CAPA workflow for multi-tenant data exposure, focusing on a basic data model, lifecycle, measurable containment, and practical API design.\n\n## Key Concepts\n- CAPA data modeling with tenant isolation\n- Lifecycle state machine for incident response\n- REST API design for evidence management\n- Tenant-scoped containment metrics\n- RCA templates and canary validation\n\n## Code Example\n```javascript\n// CAPA record with evidence linking\nconst CAPA = {\n  id, tenantId, region,\n  evidenceRefs: [\"log:trace-123\", \"metric:exposure-456\"],\n  severity, rootCause,\n  correctiveAction, preventiveAction,\n  status, createdAt, updatedAt\n};\n```\n\n## Follow-up Questions\n- How would privacy-by-design constraints influence CAPA data fields?\n- Which minimal tests verify the canary fix works without regressions?","diagram":"flowchart TD\n  A[CAPA Initiated] --> B[Triage]\n  B --> C[Investigation]\n  C --> D[Remediate]\n  D --> E[Verify]\n  E --> F[Closed]","difficulty":"beginner","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Salesforce","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":["capa data model","lifecycle state machine","rest api design","tenant-scoped metrics","rca template","canary fix plan","multi-tenant saas","data exposure","evidence management","containment metrics","root cause analysis","gradual rollout"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2026-01-14T05:11:47.713Z","createdAt":"2026-01-13T18:58:29.114Z"},{"id":"q-1502","question":"Scenario: A new analytics feature ingests raw customer IDs into a 3rd-party BI dataset due to a masking policy misconfiguration in the data pipeline. Design a CAPA program to detect, document, and prevent recurrence. Your tasks: 1) propose a CAPA data model capturing evidence and artifacts, 2) a lifecycle state machine for CAPA progression, 3) a minimal REST API to create/update CAPAs with linked logs/traces, 4) tenant-scoped metrics for containment and recurrence, 5) an RCA template and a canary-based remediation plan to validate before rollout?","answer":"CAPA data model: id, incident_id, tenant_id, data_asset, evidence_uris, lineage_digest, root_cause, containment_actions, corrective_actions, verification, status, created_at, updated_at. State machine","explanation":"## Why This Is Asked\nTests end-to-end CAPA design for data leakage via a 3rd-party BI integration, emphasizing evidence capture, containment, and validation before rollout.\n\n## Key Concepts\n- CAPA data model, lifecycle, tenant-scoped metrics, RCA templates, canary remediation.\n- Data lineage, masking policy correctness, logs/traces, DLP controls.\n\n## Code Example\n```javascript\nconst capa = {\n  id: 'CAPA-XXXX',\n  incident_id: 'INC-1234',\n  tenant_id: 'T1',\n  data_asset: 'BI_dataset_X',\n  evidence_uris: ['s3://logs/inc-1234/trace1','s3://evidence/policy.yaml'],\n  lineage_digest: 'abcd1234',\n  root_cause: 'masking_policy_misconfig',\n  containment_actions: ['disable_bi_feed'],\n  corrective_actions: ['update_masking', 'retrain_policy'],\n  verification: 'passed',\n  status: 'CLOSED',\n  created_at: '2026-01-01T00:00:00Z',\n  updated_at: '2026-01-02T12:00:00Z'\n}\n```\n\n## Follow-up Questions\n- How would you automate evidence collection across services?\n- What metrics would you visualize to verify containment and recurrence reduction?","diagram":null,"difficulty":"intermediate","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Oracle","PayPal","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T19:39:15.407Z","createdAt":"2026-01-13T19:39:15.407Z"},{"id":"q-1528","question":"Scenario: A vendor data feed for pricing and product metadata experiences occasional delays and duplicates during a quarterly refresh, causing price mismatches in two regions. Design a beginner CAPA program to address this. Your tasks: 1) propose a CAPA data model capturing evidence and artifacts, 2) define a lifecycle state machine, 3) outline a minimal REST API to create/update CAPAs with linked feed records, 4) specify region- and tenant-aware metrics to prove containment, 5) provide an RCA template and a canary-based preventive action plan to validate before broader rollout?","answer":"Propose a CAPA data model capturing evidence (feed_id, timestamp, region, tenant, price_record) and artifacts (config, rules, vendor_version); define a lifecycle state machine with states (open → investigate → containment → corrective → preventive → closed); outline a minimal REST API with endpoints POST /capas and PUT /capas/:id supporting linked feed records; specify region- and tenant-aware metrics including containment rate, duplicate frequency, and price mismatch latency; provide an RCA template covering timeline, root causes, and impact analysis; and implement a canary-based preventive action plan with staged validation before broader rollout.","explanation":"## Why This Is Asked\nAssesses practical CAPA skill under a vendor-feed incident with regional impact.\n\n## Key Concepts\n- CAPA data model, evidence artifacts\n- Lifecycle management, state transitions\n- API design with linked feed records\n- Region/tenant metrics for containment\n- RCA template and canary-based preventive actions\n\n## Code Example\n```javascript\n// Minimal CAPA schema sketch\n{ \"id\": \"CAPA-001\", \"incident\": \"vendor-feed\", \"state\": \"open\" }\n```\n\n## Follow-up Questions\n- How would you validate the canary plan across regions?\n- How would you prevent future duplicates in the feed?","diagram":null,"difficulty":"beginner","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Coinbase","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T06:49:29.965Z","createdAt":"2026-01-13T20:45:01.344Z"},{"id":"q-1556","question":"Two regions saw bids inflated after a cache invalidation caused an expired pricing model to be applied in an ad-bidding pipeline. Design a CAPA program to detect, document, and prevent recurrence. Your task: 1) CAPA data model for evidence and artifacts, 2) a lifecycle state machine, 3) a minimal REST API to create/update CAPAs with logs/traces, 4) region-asset metrics for containment (recurrence rate, MTTR, false positives), 5) RCA template and a canary cache-invalidation plan before rollout?","answer":"CAPA data model should include: id, region, asset, incident_id, evidence_links, artifacts, state, severity, timestamps, corrective_action, preventive_action, canary_results. State machine: New → In Progress → Review → Approved → Implemented → Verified → Closed. REST API endpoints: POST /capas, GET /capas/{id}, PUT /capas/{id}, GET /capas/{id}/logs, GET /capas/{id}/traces. Region-asset metrics: recurrence_rate, mttr, false_positive_rate, containment_time. RCA template sections: incident_summary, timeline, root_causes, impact_assessment, corrective_actions, preventive_actions, lessons_learned. Canary cache-invalidation plan: validate pricing model versioning, implement cache warming, add rollback triggers, monitor bid accuracy during rollout.","explanation":"## Why This Is Asked\nAssess ability to design end-to-end CAPA tooling for real-world pipeline failures, including data modeling, lifecycle governance, API ergonomics, measurable containment metrics, and a concrete RCA/canary plan.\n\n## Key Concepts\n- CAPA data modeling\n- State machine design\n- REST API design for incident artifacts\n- Regional metrics and RCA templates\n- Canary rollback for cache invalidation\n\n## Code Example\n```javascript\n// Skeleton CAPA entity\nclass CAPA { constructor(...) { ... } }\n```\n\n## Follow-up Questions\n- How would you version CAPA artifacts and enforce security constraints?","diagram":"flowchart TD\n  A[New CAPA] --> B[In Progress]\n  B --> C[Containment Verified]\n  C --> D[Root Cause]\n  D --> E[Corrective Action]\n  E --> F[Preventive Action]\n  F --> G[Closed]","difficulty":"advanced","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","DoorDash","LinkedIn"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T06:25:02.549Z","createdAt":"2026-01-13T21:42:31.859Z"},{"id":"q-1635","question":"A distributed cache layer across two regions intermittently serves stale reads after a deployment; design a beginner CAPA program to detect, document, and prevent recurrence?","answer":"CAPA data model: id, issue, evidence, containment, rootCause, actions, status, timeline. State machine: DETECTED → EVALUATED → CONTAINED → CORRECTED → VERIFIED → CLOSED. API: POST /caps, PATCH /caps/{","explanation":"## Why This Is Asked\n\nTests practical CAPA planning for a production cache issue, ensuring a clear, beginner-friendly workflow.\n\n## Key Concepts\n\n- CAPA data model design\n- Lifecycle state machine\n- Minimal REST API design\n- Metrics for containment\n- RCA structure\n- Canary rollout and rollback strategies\n- Policy gates\n\n## Code Example\n\n```json\n{\n  \"id\": \"CAPA-0002\",\n  \"issue\": \"stale-cache-read\",\n  \"evidence\": [\"log:cache-miss\", \"trace:deploy\"],\n  \"containment\": \"paused deploy to region A\",\n  \"rootCause\": \"cache TTL misconfiguration\",\n  \"actions\": [\"revert deploy\", \"adjust TTLs\", \"validate with canary\"],\n  \"status\": \"open\",\n  \"timeline\": [\"2026-01-14T12:00Z\"]\n}\n```\n\n## Follow-up Questions\n\n- How would you ensure idempotency of CAPA creation?\n- How would you extend metrics for multi-region detection latency?","diagram":"flowchart TD\n  Detect[Detect CAPA] --> Eval[Evaluate Evidence]\n  Eval --> Contain[Containment Actions]\n  Contain --> Root[Root Cause Analysis]\n  Root --> Plan[Plan Preventive Actions]\n  Plan --> Close[Close CAPA]","difficulty":"beginner","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Microsoft","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T04:21:10.430Z","createdAt":"2026-01-14T04:21:10.431Z"},{"id":"q-1745","question":"Scenario: In a multi-tenant payments platform serving PayPal, Lyft, and Coinbase, a policy change to transaction masking and logging fails to propagate to streaming and analytics pipelines in two regions, raising privacy risk and audit gaps. Design a CAPA program to address this: 1) a CAPA data model capturing evidence and artifacts, 2) a lifecycle state machine for CAPA progression, 3) a minimal REST API to create/update CAPAs with linked logs/traces and policy versions, 4) region-aware metrics to prove containment (recurrence rate, MTTR, false positives), 5) an RCA template and a canary rollback plan for policy changes?","answer":"Model CAPAs with: id, policy_version, evidence_refs, artifacts, region, status, root_cause, corrective_action, verification. Lifecycle: pending → evidence_collected → impact_assessed → containment → r","explanation":"## Why This Is Asked\nTests ability to design a scalable CAPA program for policy drift affecting data masking and logging, with cross-region impact.\n\n## Key Concepts\n- CAPA data model with evidence and artifacts\n- Lifecycle state machine for CAPA progression\n- Minimal REST API design for CAPAs with linkage to logs/traces and policy versions\n- Region-aware metrics (recurrence, MTTR, false positives)\n- RCA template and canary rollback strategy\n\n## Code Example\n```javascript\nconst CAPA = {\n  id: 'string',\n  policy_version: 'string',\n  region: 'string',\n  evidence_refs: ['string'],\n  artifacts: {config: 'string', manifest: 'string'},\n  status: 'pending',\n  root_cause: 'string',\n  corrective_action: 'string',\n  verification: 'string'\n}\n```\n\n## Follow-up Questions\n- How would you scale the CAPA data store for high-cardinality evidence refs?\n- What are safe-guards to prevent false rollbacks during canary rollout?","diagram":null,"difficulty":"advanced","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Lyft","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T09:04:06.160Z","createdAt":"2026-01-14T09:04:06.160Z"},{"id":"q-1848","question":"Design a CAPA for drift in an ML fraud detector across regions: 1) CAPA data model with driftIndex, affectedFeatures, modelVersion, evidence and artifact links; 2) lifecycle: detected → investigating → containment → RCA → closed; 3) minimal REST API to create/update CAPAs with logs and model snapshots; 4) region metrics: drift magnitude, MTTR, FPR/TPR changes; 5) RCA and canary rollback plan?","answer":"Design a CAPA for drift in an ML fraud detector across regions: 1) CAPA data model with driftIndex, affectedFeatures, modelVersion, evidence and artifact links; 2) lifecycle: detected → investigating ","explanation":"## Why This Is Asked\nML drift is a real, high-stakes problem across Netflix/Cloudflare/Uber. This question tests ability to model CAPA data, design lifecycle, and tie it to actionable metrics and rollback.\n\n## Key Concepts\n- Drift detection and data provenance\n- Model versioning and artifacts\n- Canary-based rollback vs retraining\n- Cross-region governance and metrics\n\n## Code Example\n```javascript\n// CAPA data model sketch\nconst CAPA = {\n  id: 'capa-123',\n  driftIndex: 0.42,\n  affectedFeatures: ['featureA','featureB'],\n  modelVersion: 'v1.2.3',\n  evidenceLinks: ['log/link1'],\n  artifacts: { modelSnapshot: 's3://bucket/model.v1.2.3.zip' }\n}\n```\n\n## Follow-up Questions\n- How would you test the canary rollback in production?\n- How do you prevent recurrence long-term?","diagram":null,"difficulty":"advanced","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Netflix","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T14:34:57.024Z","createdAt":"2026-01-14T14:34:57.024Z"},{"id":"q-1899","question":"Scenario: A real-time analytics service relies on a 3rd-party enrichment API. Under peak load, enrichment latency spikes cause backpressure and data loss in two regions. Design a beginner CAPA program to address this. Include: 1) a CAPA data model, 2) a lifecycle state machine, 3) a minimal REST API to create/update CAPAs with evidence, 4) region-aware metrics to prove containment, 5) an RCA template and a canary-based fallback plan?","answer":"Approach: CAPA data model includes id, region, service, dependency, evidenceLinks, artifacts, severity, status. Lifecycle: detected → investigating → mitigating → RCA → closed. API: POST/PUT CAPA with","explanation":"## Why This Is Asked\nTests ability to model CAPA for external dependencies and design minimal controls in a beginner context.\n\n## Key Concepts\n- CAPA data model and lifecycle\n- Evidence capture and artifact linking\n- Region-aware metrics and canary planning\n- RCA templates and rollback strategies\n\n## Code Example\n```javascript\n// Minimal CAPA object\nconst capa = {\n  id: \"capa-123\",\n  region: \"us-east-1\",\n  service: \"real-time-analytics\",\n  dependency: \"enrichment-api\",\n  evidenceLinks: [\"https://trace.example/1\"],\n  artifacts: [\"trace.json\", \"snapshot.png\"],\n  severity: \"medium\",\n  status: \"detected\"\n}\n```\n\n## Follow-up Questions\n- How would you validate and measure the success of the canary fallback before full rollout?\n- What privacy or data-handling considerations arise when collecting evidence across regions?","diagram":"flowchart TD\n  Detected[Detected] --> Investigating[Investigating]\n  Investigating --> Mitigating[Mitigating]\n  Mitigating --> RCA[RCA]\n  RCA --> Closed[Closed]","difficulty":"beginner","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Netflix","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T16:43:13.575Z","createdAt":"2026-01-14T16:43:13.575Z"},{"id":"q-1952","question":"Scenario: A multi-tenant data platform powering market data feeds experiences cross-tenant data leakage under peak load. Design a CAPA program to detect, contain, and prevent recurrence. Include: 1) a CAPA data model capturing tenantId, policyId, incidentId, evidence and artifact links; 2) a lifecycle state machine; 3) a minimal REST API to create/update CAPAs with tenant-scoped logs and traces; 4) tenant-aware metrics (recurrence, MTTR, spillover rate); 5) RCA template and a canary policy rollout plan?","answer":"Design focuses on strong isolation and traceability: a CAPA data model with tenantId, policyId, incidentId,EvidenceLinks,ArtifactSnapshots,affectedTables,dataClassification; states: detected→investiga","explanation":"## Why This Is Asked\nThis question tests ability to design a production-grade CAPA for a multi-tenant data platform, emphasizing isolation, auditability, and scalable rollout.\n\n## Key Concepts\n- Tenant-scoped CAPA data model with evidence and artifacts\n- Immutable links to logs/traces and resource metadata\n- Lifecycle state machine and canary-based enforcement\n- Per-tenant metrics (recurrence, MTTR, spillover rate)\n\n## Code Example\n```javascript\n// TypeScript interface for CAPA\ninterface CAPA {\n  id: string;\n  tenantId: string;\n  policyId: string;\n  incidentId: string;\n  evidenceLinks: string[];\n  artifacts: string[];\n  affectedResources: string[];\n  dataClassification: string;\n}\n```\n\n## Follow-up Questions\n- How would you handle CAPA versioning and backward compatibility?\n- How to simulate cross-tenant leakage in a canary without impacting production?","diagram":"flowchart TD\nA[Detected CAPA] --> B[Investigating]\nB --> C[Containment]\nC --> D[RCA]\nD --> E[Closed]","difficulty":"advanced","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","MongoDB","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T18:49:17.889Z","createdAt":"2026-01-14T18:49:17.890Z"},{"id":"q-2132","question":"Scenario: A multi-region real-time bidding system processes ad events via Kafka across us-east-1 and eu-west-1. A recently deployed bid normalization microservice causes timeouts and mispricing, leading to revenue variance and increased false positives in two regions. Design a CAPA program that covers: 1) a CAPA data model capturing evidence, artifacts, and cross-region traces; 2) a lifecycle state machine for CAPA progression; 3) a minimal REST API to create/update CAPAs with linked logs/traces; 4) region-aware metrics to prove containment (recurrence rate, MTTR, revenue delta, false-positive rate); 5) an RCA template and a canary rollout plan for the normalization service?","answer":"Craft a CAPA with: 1) data model capturing incidentId, region, service, timestamps, logs, traces, and artifact links; 2) a lifecycle: detected → triaged → containment → RCA → remediation → closed; 3) ","explanation":"## Why This Is Asked\nAssesses ability to design production-grade CAPA across multiple regions for real-time pipelines, with verifiable metrics and controlled rollout.\n\n## Key Concepts\n- CAPA data modeling with cross-region traces and artifacts\n- Lifecycle state machine for remediation workflows\n- Minimal REST API design for linking evidence\n- Region-aware metrics and alerting strategies\n- RCA template and canary rollout plans\n\n## Code Example\n```javascript\ntype CAPARecord = {\n  incidentId: string;\n  region: string;\n  service: string;\n  detectedAt: string;\n  status: 'detected'|'triaged'|'containment'|'RCA'|'remediation'|'closed';\n  evidenceLinks: string[];\n  artifactLinks: string[];\n  linkedLogs: string[];\n}\n```\n\n## Follow-up Questions\n- How would you scale the CAPA store for high cardinality regions and tenants?\n- What thresholds would trigger automatic containment vs. human triage?","diagram":"flowchart TD\n  A[Detect] --> B[Investigate]\n  B --> C[Containment]\n  C --> D[RCA]\n  D --> E[Remediation]\n  E --> F[Closed]","difficulty":"advanced","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Salesforce","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T04:15:47.411Z","createdAt":"2026-01-15T04:15:47.411Z"},{"id":"q-2174","question":"Scenario: A data-access policy violation occurs when a misconfigured feature-flag rollout temporarily allows cross-tenant data access in two regions. Design a CAPA program to detect, document, and prevent recurrence. Include: 1) a CAPA data model capturing lineage, access changes, and remediation artifacts; 2) a lifecycle state machine; 3) a minimal REST API to create/update CAPAs with linked logs/traces and policy changes; 4) region- and tenant-scoped metrics; 5) an RCA template and a canary-based remediation plan?","answer":"CAPA plan for a data-access policy violation caused by a misconfigured feature flag that briefly allowed cross-tenant access in two regions. 1) CAPA data model capturing lineage, access changes, and r","explanation":"## Why This Is Asked\nTests ability to plan CAPA around privacy and cross-tenant isolation, including evidence capture, lifecycle control, and safe remediation.\n\n## Key Concepts\n- CAPA data model with lineage and access-change artifacts\n- Data privacy/regulatory considerations\n- REST API design for CAPA creation/update with traces\n- Canary-based remediation and regional rollout controls\n- Metrics: containment MTTR, recurrence rate, false positives\n\n## Code Example\n```javascript\nconst capa = { id: \"CAPA-001\", incidentId: \"INC-12345\", evidence: [\"log:auth-01\"], state: \"detected\" };\n```\n\n## Follow-up Questions\n- How would you test the RCA template under high-traffic load?\n- What mitigations would you add to the canary remediation plan?","diagram":null,"difficulty":"intermediate","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Microsoft","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T05:49:29.109Z","createdAt":"2026-01-15T05:49:29.109Z"},{"id":"q-2228","question":"Scenario: A webhook-driven notification service intermittently loses messages in two regions after deploying a new retry-backoff policy. Design a beginner CAPA program to address this. Your task: 1) propose a CAPA data model, 2) define a lifecycle state machine, 3) outline a minimal REST API to create/update CAPAs with evidence, 4) specify practical, region-aware metrics to prove containment, 5) provide an RCA template and a canary-based rollback/test plan before full rollout?","answer":"Develop a CAPA for intermittent webhook message loss after switching to a stricter retry-backoff. Data model: id, detectedAt, service, region, issueDescription, evidenceLinks, artifacts, status, rootC","explanation":"## Why This Is Asked\n\nThis question tests a candidate's ability to translate a real incident into a repeatable CAPA workflow with concrete data modeling, lifecycle, measurable metrics, and an actionable rollback plan.\n\n## Key Concepts\n\n- CAPA data model design\n- Lifecycle state machine for CAPA\n- Region-aware metrics and evidence linking\n- RCA templating and canary-based validation\n\n## Code Example\n\n```javascript\n// Minimal CAPA store skeleton\nclass CAPA { constructor(id, region, issue){ this.id=id; this.region=region; this.issue=issue; this.status='detected'; } }\n```\n\n## Follow-up Questions\n\n- How would you extend the model for multi-tenant isolation and audit trails?\n- What tests would you add to validate the canary rollback triggers?","diagram":"flowchart TD\n  A[Detected] --> B[Investigating]\n  B --> C[Containment]\n  C --> D[RCA]\n  D --> E[Preventive/Canary]\n  E --> F[Closed]","difficulty":"beginner","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Instacart","LinkedIn"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T08:43:41.547Z","createdAt":"2026-01-15T08:43:41.547Z"},{"id":"q-2363","question":"In a multi-region streaming analytics platform, a surge triggers backpressure and late events in two regions. Design a CAPA program that focuses on data-plane backlogs affecting downstream attribution. Include: 1) a CAPA data model capturing evidence and artifacts, 2) a region/shard-aware lifecycle, 3) a minimal REST API to create/update CAPAs with logs/traces, 4) metrics proving containment (latency drift, backlog depth, MTTR), 5) RCA template and a canary rollout for a dynamic circuit-breaker in the ingestion path?","answer":"CAPA data model with id, incidentId, regions, shards, startTime, evidenceLinks, logsTraces, metricsSnapshots, containmentPlan, rootCause, RCA, artifacts; lifecycle states: detected→assessing→backlogCo","explanation":"## Why This Is Asked\nTests ability to craft a CAPA with data-plane focus, multi-region correlation, and concrete artifacts to track containment and recurrence.\n\n## Key Concepts\n- CAPA data model with evidence and artifact links\n- Region/shard-aware lifecycle and metrics\n- Minimal REST API to link logs/traces to CAPAs\n- RCA templates and safe canary rollout for ingestion controls\n\n## Code Example\n```javascript\n// Example REST handler sketch (for CAPA creation)\nfunction createCAPA(req, res) {\n  const capa = req.body;\n  // validate required fields: id, region, shard, startTime\n  res.status(201).send({ ok: true, id: capa.id });\n}\n```\n\n## Follow-up Questions\n- How would you validate canary success without customer impact?\n- What strategies mitigate false positives when region clocks drift? ","diagram":null,"difficulty":"advanced","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T14:52:47.749Z","createdAt":"2026-01-15T14:52:47.749Z"},{"id":"q-2393","question":"Scenario: A cost spike in cloud spend occurs due to a runaway batch job in two regions; design a beginner CAPA that addresses this. Include: 1) a CAPA data model, 2) a lifecycle state machine, 3) a minimal REST API to create/update CAPAs with evidence, 4) region-aware metrics to prove containment, 5) an RCA template and a canary-based remediation plan?","answer":"Cost spike CAPA for a runaway batch causing cloud spend in two regions. 1) Data model: id, risk, service, regions, evidenceLinks, artifacts, costDelta, window. 2) Lifecycle: detected → investigating →","explanation":"## Why This Is Asked\nTests ability to translate a real cost-incident into a structured CAPA program.\n\n## Key Concepts\n- CAPA data model, lifecycle, minimal API, region-aware metrics, RCA, canary remediation.\n- Trade-offs: sampling evidence vs. privacy, canary granularity, cost thresholds.\n\n## Code Example\n```javascript\n// Pseudo-structure for CAPA object\nconst capa = { id: '', risk: '', service: '', regions: [], evidenceLinks: [], artifacts: [], costDelta: 0, window: '' };\n```\n\n## Follow-up Questions\n- How would you measure preventiveness over time? \n- What thresholds would trigger automatic containment actions?","diagram":"flowchart TD\n  D[Detection] --> C[Containment]\n  C --> R[RCA]\n  R --> M[Remediation]\n  M --> V[Verification]","difficulty":"beginner","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","Snowflake","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T16:44:03.206Z","createdAt":"2026-01-15T16:44:03.208Z"},{"id":"q-2430","question":"Scenario: A multi-region analytics platform begins exporting customer PII to a BI partner after a schema change, risking regulatory non-compliance. Design a CAPA program to address this: 1) CAPA data model for evidence and artifacts, 2) lifecycle state machine, 3) minimal REST API to create/update CAPAs with linked logs and data-access policy checks, 4) region-aware metrics (policy-violation rate, MTTR, false positives), 5) RCA template and a canary rollout plan with automated data redaction?","answer":"Propose a CAPA for cross-region analytics where a schema change inadvertently exports PII to a BI partner. Deliver: 1) a CAPA data model capturing evidence and artifacts, 2) a lifecycle state machine,","explanation":"## Why This Is Asked\nTests ability to design CAPA from data governance, not just incident response.\n\n## Key Concepts\n- CAPA data model for evidence, artifacts, and policy checks\n- Lifecycle state machine with DETECT → INVESTIGATE → CONTAIN → REMEDIATE → VERIFY → CLOSED\n- Minimal REST API design for CAPA creation/update with linked traces\n- Region-aware compliance metrics (violation rate, MTTR, FP rate)\n- RCA template and canary rollout with automated data redaction\n\n## Code Example\n```javascript\n// CAPA data model sketch (TypeScript)\ntype CAPARecord = {\n  id: string;\n  title: string;\n  evidenceUrls: string[];\n  policyChecks: string[];\n  region: string;\n  state: 'DETECTED'|'INVESTIGATING'|'CONTAINED'|'REMEDIATED'|'VERIFIED'|'CLOSED';\n  createdAt: string;\n  updatedAt: string;\n};\n```\n\n## Follow-up Questions\n- How would you surface policy checks in runtime dashboards?\n- How would you automate data redaction in canary tests before broader rollout?","diagram":"flowchart TD\n  A[Detect Violation] --> B[Collect Evidence]\n  B --> C[Contain Data Export]\n  C --> D[Remediate Schema/Policy]\n  D --> E[Verify Compliance]\n  E --> F[Close CAPA]","difficulty":"advanced","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Tesla","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T17:49:16.501Z","createdAt":"2026-01-15T17:49:16.501Z"},{"id":"q-2611","question":"Scenario: A global data-analytics pipeline intermittently drops events in two regions during peak load due to a misconfigured ETL window. Design a beginner CAPA program: 1) CAPA data model, 2) lifecycle state machine, 3) a minimal REST API to create/update CAPAs with evidence, 4) region-aware metrics to prove containment and prevent recurrence, 5) RCA template and a canary-based remediation plan. Keep it implementable with simple tooling?","answer":"CAPA Data Model: {id, title, affectedRegions, evidence, rootCause, correctiveAction, preventiveAction, status, createdBy, createdAt, updatedAt}. Lifecycle State Machine: detected → validated → contained → investigating → remediated → verified → closed. REST API: POST /capas to create with evidence, PUT /capas/:id to update, GET /capas/:id to retrieve, PATCH /capas/:id/status for state transitions. Region-Aware Metrics: eventDropRate(region), pipelineLatency(region), containmentScore(region), remediationEffectiveness(region). RCA Template: {incidentSummary, timeline, rootCauseAnalysis, impactAssessment, containmentActions, lessonsLearned}. Canary-Based Remediation: Deploy fix to single region, monitor metrics for 30 minutes, verify containment, then execute staged global rollout with rollback capability.","explanation":"## Why This Is Asked\n\nAssesses ability to design a practical CAPA program for ensuring data integrity in global distributed systems, emphasizing traceability, containment, and systematic problem resolution.\n\n## Key Concepts\n\n- CAPA data modeling and lifecycle management\n- State machine design for systematic issue resolution\n- RESTful API design with evidence handling\n- Region-specific monitoring and metrics\n- Root Cause Analysis methodologies\n- Canary deployment strategies for safe remediation\n\n## Code Example\n\n```javascript\n// Minimal REST API for CAPA management\nconst express = require('express');\nconst app = express();\n\napp.use(express.json());\n\n// CAPA data storage (in production, use proper database)\nlet capas = {};\n\n// POST /capas - Create new CAPA with evidence\napp.post('/capas', (req, res) => {\n  const capa = {\n    id: Date.now().toString(),\n    ...req.body,\n    status: 'detected',\n    createdAt: new Date().toISOString()\n  };\n  capas[capa.id] = capa;\n  res.status(201).json(capa);\n});\n\n// PUT /capas/:id - Update existing CAPA\napp.put('/capas/:id', (req, res) => {\n  const id = req.params.id;\n  if (!capas[id]) return res.status(404).json({error: 'CAPA not found'});\n  \n  capas[id] = {\n    ...capas[id],\n    ...req.body,\n    updatedAt: new Date().toISOString()\n  };\n  res.json(capas[id]);\n});\n\n// PATCH /capas/:id/status - Update CAPA status\napp.patch('/capas/:id/status', (req, res) => {\n  const id = req.params.id;\n  if (!capas[id]) return res.status(404).json({error: 'CAPA not found'});\n  \n  capas[id].status = req.body.status;\n  capas[id].updatedAt = new Date().toISOString();\n  res.json(capas[id]);\n});\n\napp.listen(3000, () => console.log('CAPA API running on port 3000'));\n```","diagram":"flowchart TD\n  A[Detect CAPA] --> B[Validate Evidence]\n  B --> C[State:InProgress]\n  C --> D[Containment]\n  D --> E[Investigation]\n  E --> F[Remediation Canary]\n  F --> G[Verification]\n  G --> H[Closed]","difficulty":"beginner","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","MongoDB","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T05:01:45.867Z","createdAt":"2026-01-16T02:42:35.935Z"},{"id":"q-2731","question":"Scenario: A real-time telemetry pipeline logs PII after a schema update in two regions, triggering a privacy incident. Design a CAPA program to prevent recurrence: 1) a CAPA data model capturing evidence and artifacts, 2) a lifecycle state machine for CAPA progression, 3) a minimal REST API to create/update CAPAs with redacted evidence (logs/traces), 4) region-aware metrics to prove containment (PII_exposure_rate, MTTR, false_positives), 5) an RCA template and a canary rollout plan for patching the logging/path?","answer":"Define a CAPA data model: id, region, evidence, artifacts, root_cause, corrective_actions, preventive_actions, status, timestamps. Lifecycle: Detected → Triaged → Contained → Eradicated → Verified → C","explanation":"Why This Is Asked\nTests privacy-aware CAPA handling in multi-region pipelines with practical API design and measurable containment.\n\nKey Concepts\n- Data redaction and auditability\n- Region-aware metrics\n- Canary rollouts and rollback\n\nCode Example\n```javascript\nfunction redactLog(log){ return log.replace(/(email|phone|SSN|credit)/i, '[REDACTED]'); }\n```\n\nFollow-up Questions\n- How would you test the redaction function to ensure no PII leaks remains?\n- How would you structure RCA sections for a privacy CAPA and ensure regulatory traceability?","diagram":"flowchart TD\n  A[CAPA Detected] --> B[Evidence Collected]\n  B --> C{Containment Needed?}\n  C -->|Yes| D[Containment Implemented]\n  C -->|No| E[Escalate]\n  D --> F[Eradicated/Verified]\n  F --> G[Closed]\n  E --> H[Root Cause Analysis]\n  H --> I[Preventive Action]\n  I --> J[Canary Rollout]\n  J --> F","difficulty":"intermediate","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","NVIDIA","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T09:42:40.719Z","createdAt":"2026-01-16T09:42:40.720Z"},{"id":"q-2753","question":"In a global platform handling encrypted customer data, a key-rotation facility misdeploy causes decrypt failures in two regions, blocking access to data. Design a CAPA program to detect, contain, and prevent recurrence. Your task: 1) propose a CAPA data model capturing evidence and artifacts, 2) define a region-aware lifecycle state machine, 3) outline a minimal REST API to create/update CAPAs with linked logs/traces, 4) specify region-aware metrics to prove containment (downtime, MTTR, data-access success rate, false positives), 5) provide an RCA template and a canary-based remediation plan for key-rotation changes?","answer":"Propose a CAPA for a cross-region encryption-key rotation failure. Data model: CAPA {id, incidentId, regions, keyId, artifacts, logsLinks, evidence}; lifecycle: detected→triaged→contained→rootCause→re","explanation":"## Why This Is Asked\nAssesses ability to design CAPA for security-critical cross-region encryption failures, including evidence capture and regulatory considerations.\n\n## Key Concepts\n- CAPA data model capturing evidence and artifacts\n- Region-aware lifecycle with multi-region coordination\n- Minimal REST API for CAPAs with logs and traces\n- Metrics for containment, MTTR, recurrence, false positives\n- RCA templates and canary rollout strategies for secure rotation\n\n## Code Example\n```javascript\ntype CAPA = {\n  id: string;\n  incidentId: string;\n  regions: string[];\n  keyId: string;\n  artifacts: string[];\n  logs: string[];\n  evidenceLinks: string[];\n  status: string;\n  createdAt: string;\n  updatedAt: string;\n};\n```\n\n## Follow-up Questions\n- How would you integrate regulatory reporting and data-retention rules?\n- What monitoring would you add to prevent regression on rotation schedules?","diagram":"flowchart TD\n  DetectIssue[Detect Issue] --> Triage[Triage CAPA]\n  Triage --> Contain[Containment Initiated]\n  Contain --> RCA[Root Cause Analysis]\n  RCA --> Remed[Remediation Deployed]\n  Remed --> Closed[CAPA Closed]","difficulty":"intermediate","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","NVIDIA","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T10:40:27.897Z","createdAt":"2026-01-16T10:40:27.897Z"},{"id":"q-2785","question":"In a multi-tenant analytics platform, a schema evolution causes cross-tenant data leakage and inconsistent joins during peak load. Design a CAPA program that mitigates these risks: 1) a CAPA data model capturing evidence and lineage, 2) a cross-tenant lifecycle state machine, 3) a minimal REST API to create/update CAPAs with linked logs/traces, 4) tenant-aware metrics (recurrence rate, MTTR, false positives), 5) an RCA template and a canary rollout plan with rollback?","answer":"Propose a CAPA blueprint for multi-tenant schema evolution leakage. Data model: capa_id, tenant_id, evidence_uri, artifact_sha, affected_tables, root_cause, containment, remediation, status, timestamp","explanation":"## Why This Is Asked\nWhy this is asked: tests modeling of data-heavy CAPA workflows with tenant isolation and cross-tenant risk containment.\n\n## Key Concepts\n- CAPA data model with evidence lineage\n- Cross-tenant lifecycle and access controls\n- Canary rollout and rollback strategies for schema changes\n\n## Code Example\n```javascript\n// Example CAPA payload (pseudo)\n{\n  \"capa_id\":\"capa-123\",\n  \"tenant_id\":\"tenant-A\",\n  \"evidence_uri\":\"s3://bkt/evidence/123\",\n  \"artifact_sha\":\"abcdef\",\n  \"affected_tables\":[\"events\"],\n  \"root_cause\":\"schema drift during upgrade\",\n  \"containment\":[\"block writes to affected_tables\"],\n  \"remediation\":\"apply patch, reindex\",\n  \"status\":\"PREPARED\",\n  \"timestamps\":{ \"raised\":\"2025-09-01T12:00:00Z\" }\n}\n```\n\n## Follow-up Questions\n- How would you enforce tenant isolation in the API and data access during CAPA progression?\n- How would you validate canary success before global rollout?","diagram":null,"difficulty":"advanced","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","Snowflake","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T11:42:22.728Z","createdAt":"2026-01-16T11:42:22.728Z"},{"id":"q-2888","question":"A multi-tenant policy engine used for feature-flag entitlements in a globally distributed SaaS shows inconsistent user permissions after a policy rollout in two regions, causing sporadic authorization errors (429/403) and revenue leakage. Design a CAPA program focused on data-feed integrity and policy-vs-data latency. Include: 1) a CAPA data model, 2) a region-aware lifecycle, 3) a minimal REST API for CAPAs with linked logs, 4) metrics to prove containment, 5) RCA template and a canary rollout plan for policy updates?","answer":"Propose a CAPA with fields: id, incidentId, region, tenantId, policyVersion, evidence (log/trace IDs), state, createdAt, updatedAt, remediation. API: POST/PUT to /capas with evidence links; logs linke","explanation":"## Why This Is Asked\nTests end-to-end CAPA design focusing on data-feed integrity and cross-region policy latency.\n\n## Key Concepts\n- CAPA data model with provenance for logs/traces\n- Region-aware lifecycle and state transitions\n- REST API linking CAPAs to evidence logs/traces\n- Region-aware metrics: MTTR, recurrence, false positives, latency\n- RCA template and canary rollout plan for policy updates\n\n## Code Example\n```javascript\ninterface CAPA {\n  id: string;\n  incidentId: string;\n  region: string;\n  tenantId: string;\n  policyVersion: string;\n  evidence: string[]; // logs/traces\n  state: 'identified'|'investigating'|'contained'|'mitigated'|'closed';\n  createdAt: string;\n  updatedAt: string;\n  remediation?: string;\n}\n```\n\n## Follow-up Questions\n- How would you validate data-feed integrity across regions during rollback?\n- What trade-offs exist between rapid containment and thorough RCA in high-throughput systems?","diagram":null,"difficulty":"intermediate","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","NVIDIA","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T16:41:06.450Z","createdAt":"2026-01-16T16:41:06.450Z"},{"id":"q-2912","question":"Scenario: An ML labeling pipeline for ad quality causes bias in two regions after a data source change. Design a CAPA program to detect, contain, and prevent recurrence. Include: 1) a CAPA data model with incident_id, timestamps, regions, data_source, bias_metric, and evidence artifacts; 2) region-aware lifecycle: detected, containment, investigation, remediation, verification, closed; 3) REST API to create/update CAPAs with logs/traces; 4) metrics: bias_delta, MTTR, FP_rate by region; 5) RCA template and a canary remediation plan?","answer":"Implement a CAPA for an ML labeling pipeline bias incident. Data model includes incident_id, timestamps, regions, data_source, bias_metric, and evidence artifacts; lifecycle: detected, containment, in","explanation":"## Why This Is Asked\nAssesses CAPA design for ML bias, data provenance, and cross-region containment in a real product workflow.\n\n## Key Concepts\n- CAPA data model for bias incidents and lineage\n- Region-aware lifecycle and metrics\n- Minimal REST API for CAPA creation/update with evidence\n- RCA templates and canary remediation strategies\n\n## Code Example\n```javascript\n{\n  incident_id: 'INC123',\n  regions: ['us-east-1','eu-west-1'],\n  data_source: 'datasource-A',\n  bias_metric: { type: 'precision_bias', value: 0.08 },\n  evidence: ['logs.zip','trace.json']\n}\n```\n\n## Follow-up Questions\n- How would you enforce immutable data provenance?\n- How would you set thresholds for cross-region drift detection?","diagram":"flowchart TD\n  A[Bias detected] --> B[Evidence capture]\n  B --> C[Containment]\n  C --> D[Root-cause investigation]\n  D --> E[Remediation and verification]\n  E --> F[Closure]","difficulty":"intermediate","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","LinkedIn","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T17:31:51.028Z","createdAt":"2026-01-16T17:31:51.028Z"},{"id":"q-2962","question":"Scenario: A two-region rollout of a new ML-powered recommender model causes drift in user-item interactions and CTR drop within the first hour. Design a CAPA program to prevent recurrence. Include: 1) a CAPA data model capturing evidence and artifacts (drift stats, feature distributions, logs, and model/version with region), 2) a lifecycle state machine for CAPA progression, 3) a minimal REST API to create/update CAPAs with linked logs/traces, 4) region-aware metrics (drift decay, MTTR, false positives), 5) an RCA template and a canary-based rollback plan?","answer":"Design a CAPA for ML drift after a two-region rollout of a recommender model. Capture evidence: drift stats, feature distributions, logs, and model/version with region tags. Lifecycle: detect → triage","explanation":"## Why This Is Asked\nTests ability to handle production ML incidents and governance beyond basic infra issues, emphasizing data-driven CAPA tracing and regional controls.\n\n## Key Concepts\n- CAPA data models for ML drift artifacts\n- Lifecycle gating with detection, triage, containment, remediation, verification, and closure\n- REST API design linking CAPA records to logs/traces and model artifacts\n- Region-aware metrics for containment and false positives\n\n## Code Example\n```javascript\n// Example: CAPA creation payload (pseudo)\n{\n  id: 'capa-123',\n  region: 'us-east-1',\n  modelVersion: 'v1.3.2',\n  evidence: {\n    driftStats: {},\n    featureDistributions: {},\n    logsRef: 'log-id-xyz'\n  },\n  status: 'detect'\n}\n```\n\n## Follow-up Questions\n- How would you automate thresholding for drift signals?\n- How would you validate the effectiveness of the canary rollback?","diagram":"flowchart TD\n  Detect[Drift Detected] --> Triage[Triage CAPA]\n  Triage --> Contain[Contain Issue]\n  Contain --> Remediate[Remediate Model]\n  Remediate --> Verify[Verify Metrics]\n  Verify --> Close[Close CAPA]","difficulty":"intermediate","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Apple","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T19:25:59.333Z","createdAt":"2026-01-16T19:25:59.334Z"},{"id":"q-3157","question":"Scenario: A cross-tenant data-sharing microservice was extended to support dynamic data scoping, and a release leaked data across two regions. Design a CAPA program to detect, triage, and prevent recurrence. Include: 1) a CAPA data model, 2) a region-aware lifecycle state machine, 3) a minimal REST API to create/update CAPAs with linked logs/traces, 4) region-level metrics proving containment and recurrence, 5) an RCA template and a canary rollout plan for the data-scoping change?","answer":"Design a CAPA program for cross-tenant data leakage in a shared microservice across two regions. Data model: CAPA { id, region, service, evidenceLinks, artifacts, severity, status, lifecycle, rootCaus","explanation":"## Why This Is Asked\nTests ability to model CAPA data, design scalable regional state machines, and tie incidents to concrete metrics and RCA.\n\n## Key Concepts\n- CAPA data model, region-aware lifecycle, API design, region metrics, RCA templates, canary strategies.\n- Trade-offs: data privacy, auditability, latency vs safety.\n\n## Code Example\n```javascript\n// Example CAPA interface (TypeScript)\ninterface CAPA {\n  id: string\n  region: string\n  service: string\n  evidenceLinks: string[]\n  artifacts: string[]\n  severity: 'low'|'medium'|'high'\n  status: string\n  lifecycle: string[]\n  rootCause?: string\n  remediation?: string\n}\n```\n\n## Follow-up Questions\n- How would you enforce policy as code for data-scoping changes?\n- How would you automate RCA template population from logs and traces?","diagram":null,"difficulty":"advanced","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","DoorDash","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T04:52:57.380Z","createdAt":"2026-01-17T04:52:57.380Z"},{"id":"q-3177","question":"Scenario: An ML inference service deployed globally leaks user features after a feature-store refresh and model rollback. Design a CAPA program to detect, capture evidence, contain the leak, and prevent recurrence. Include: 1) a CAPA data model capturing evidence, artifacts, and model/feature lineage, 2) a lifecycle state machine for CAPA progression, 3) a minimal REST API to create/update CAPAs with linked logs/traces, 4) region- and data-class metrics (containment time, false negatives, drift delta), 5) an RCA template and a canary-based remediation plan (retrain on a subset, gated rollout, rollback if KPI fail)?","answer":"Design a CAPA for an ML inference service with regional leakage after a feature-store refresh. Include: 1) CAPA data model: capa_id, model_id, version, feature_store_snapshot, evidence_links, region, ","explanation":"## Why This Is Asked\nTests ability to design a CAPA system around ML model leakage, emphasizing traceability, cross-region governance, and safe remediation.\n\n## Key Concepts\n- CAPA data modeling for ML artifacts and lineage\n- Region-aware lifecycle and event-driven triggers\n- REST API design for evidence-linked CAPAs\n- Metrics for containment, drift, and leakage\n- RCA templates and canary-based risk mitigation\n\n## Code Example\n```javascript\n// Example CAPA payload (POST /caps)\n{\n  id: \"CAPA-2026-ML-01\",\n  model_id: \"m-1234\",\n  version: \"v2.1.3\",\n  feature_store_snapshot: \"fs-20260101\",\n  evidence_links: [\"log:http://..\", \"trace:trx-789\"],\n  region: \"us-west-2\",\n  timestamps: { created: \"2026-01-01T12:00:00Z\" },\n  root_causes: [\"feature leakage\", \"rollback bug\"]\n}\n```\n\n## Follow-up Questions\n- How would you test the canary remediation before full rollout?\n- What privacy constraints influence CAPA data retention and auditing?","diagram":null,"difficulty":"advanced","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T05:38:42.145Z","createdAt":"2026-01-17T05:38:42.146Z"},{"id":"q-3287","question":"In a real-time feature-store pipeline, a schema evolution causes feature drift and model drift across regions, impacting revenue. Design a CAPA program with: 1) a CAPA data model capturing evidence and artifacts, 2) a region-aware lifecycle, 3) a minimal REST API to create/update CAPAs with logs/traces, 4) region metrics (driftScore, featureAge, MTTR, FP rate), 5) an RCA template and a canary rollout plan for schema changes?","answer":"Create a CAPA that tracks drift and schema diffs tied to a central registry and feature-store versions. Core: (a) data model for evidence, diffs, tests; (b) region-aware lifecycle; (c) REST API to att","explanation":"## Why This Is Asked\n\nThis question probes end-to-end CAPA design for drift in a global, latency-sensitive feature-store, including data modeling, lifecycle, API surfaces, metrics, and rollback strategy.\n\n## Key Concepts\n\n- Schema evolution and drift detection\n- Region-aware containment and MTTR\n- Evidence collection and artifact stores\n- Canary-driven rollout for changes\n\n## Code Example\n\n```javascript\n// Sketch: CAPA data model (pseudo)\nconst CAPA = {\n  id: String,\n  region: String,\n  evidence: [{type:String, payload:Buffer, timestamp:Date}],\n  artifacts: [{type:String, location:String}],\n  state: 'detected'|'triaged'|'mitigated'|'verified',\n  driftScores: {[feature:String]: Number}\n}\n```\n\n## Follow-up Questions\n\n- How would you tune drift thresholds across regions?\n- What tests verify canary rollouts before global deployment?","diagram":"flowchart TD\n  Detect[Drift Detected] --> Triage[Triaged]\n  Triage --> Mitigate[Mitigate & Canary]\n  Mitigate --> Verify[Containment Verified]\n  Verify --> Complete[CAPA Closed]","difficulty":"advanced","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Netflix","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T10:29:37.606Z","createdAt":"2026-01-17T10:29:37.606Z"},{"id":"q-3333","question":"Scenario: A real-time fraud-detection service shows a surge in false positives after a deployment across three regions. Design a CAPA program focusing on model and data drift. Include: 1) a CAPA data model capturing evidence, signals, and data lineage; 2) a lifecycle state machine for CAPA progression; 3) a minimal REST API to create/update CAPAs with logs/traces; 4) region-aware metrics (precision, recall, F1, drift_score, MTTR); 5) RCA template and a canary retraining/feature rollout plan?","answer":"The CAPA should define a data model with id, region, trigger, evidence, logs, drift_score, metrics snapshot, and lineage. Use a state machine: open -> triaged -> investigating -> contained -> remediat","explanation":"## Why This Is Asked\nTests end-to-end CAPA design for ML-driven, multi-region systems, emphasizing drift handling, evidence capture, state management, and safe remediation.\n\n## Key Concepts\n- CAPA data model with evidence, lineage, and drift metrics\n- Drift detection signals and region-aware metrics\n- Lifecycle/state machines and minimal API design\n- RCA templates and canary rollout strategies\n\n## Code Example\n```javascript\n// Example CAPA data model\nconst CAPA = {\n  id: \"CAPA-123\",\n  region: \"us-central1\",\n  trigger: \"model_drift\",\n  evidence: [\"log1\", \"log2\"],\n  metrics: { precision: 0.82, recall: 0.65, drift_score: 0.72, mttr: 3600 },\n  state: \"open\",\n  lineage: { dataset: \"transactions_v2\", model: \"fraud_v3\" }\n}\n```\n\n## Follow-up Questions\n- How would you scale the CAPA API under high throughput?\n- Which drift detection techniques would you choose and why?","diagram":null,"difficulty":"intermediate","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Goldman Sachs","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T11:39:54.708Z","createdAt":"2026-01-17T11:39:54.709Z"},{"id":"q-3541","question":"In a multi-tenant data platform used by Zoom and Databricks, a sudden ingestion spike from one tenant causes backpressure that delays processing for others across regions. Design a CAPA program to detect, capture evidence, contain, and prevent recurrence. Include: 1) a CAPA data model, 2) a lifecycle state machine, 3) a minimal REST API for CAPAs with linked logs and traces, 4) tenant-/region-aware metrics (backlog depth, MTTR, recurrence rate), 5) RCA template and a canary rollback plan for throttle-based remediation?","answer":"Adopt a CAPA data model with tables CAPA, Evidence, Artifact keyed by tenant_id/region. Implement a state machine: DETECTED → ANALYZING → CONTAINMENT → INVESTIGATION → REMEDIATION → VERIFIED → CLOSED.","explanation":"## Why This Is Asked\n\nAssesses capability to design scalable, tenant-aware CAPA for a shared data platform, balancing fairness and speed while ensuring traceability.\n\n## Key Concepts\n\n- Multi-tenant isolation and backpressure control\n- Evidence collection and artifact management\n- Lifecycle state machine design\n- REST API design for CAPA with logs/traces\n- Tenant/region metrics and RCA structuring\n- Canary rollout and rollback strategies\n\n## Code Example\n\n```javascript\ninterface CAPA {\n  id: string;\n  tenantId: string;\n  region: string;\n  onset: string;\n  status: string;\n  logs: string[];\n}\n```\n\n## Follow-up Questions\n\n- How would you store and query cross-tenant artifacts at scale?\n- What tests would validate canary safety under burst traffic?","diagram":"flowchart TD\nA[Detected] --> B[Analyzing]\nB --> C[Containment]\nC --> D[Investigation]\nD --> E[Remediation]\nE --> F[Verified]\nF --> G[Closed]","difficulty":"advanced","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T20:39:51.992Z","createdAt":"2026-01-17T20:39:51.993Z"},{"id":"q-3568","question":"Scenario: A data ingestion pipeline across two regions experiences data skew during the daily batch window, causing delayed events and stale dashboards. Design a beginner CAPA to address this. Include: 1) a CAPA data model, 2) a lifecycle state machine, 3) a minimal REST API to create/update CAPAs with evidence, 4) region-aware metrics to prove containment and time-to-containment, 5) an RCA template and a canary-based remediation plan?","answer":"Proposed CAPA data model: id, title, region, evidence, rootCause, correctiveActions, preventiveActions, status, timestamps. Lifecycle: detected → triaged → rootCause → remediation → verified → closed.","explanation":"## Why This Is Asked\nTests the ability to craft a cross-region CAPA for ingestion skew; introduces a new angle not covered by existing questions.\n\n## Key Concepts\n- CAPA data model fields\n- State machine lifecycle\n- Lightweight REST API\n- Region-aware metrics\n- RCA template and canary remediation\n\n## Code Example\n```javascript\n// Minimal CAPA schema (illustrative)\nconst CAPASchema = {\n  id: 'string',\n  title: 'string',\n  region: 'string',\n  evidence: ['string'],\n  rootCause: 'string',\n  correctiveActions: ['string'],\n  preventiveActions: ['string'],\n  status: 'string',\n  timestamps: 'object',\n};\n```","diagram":null,"difficulty":"beginner","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Netflix","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T05:43:54.642Z","createdAt":"2026-01-17T21:35:49.260Z"},{"id":"q-3616","question":"Scenario: After a feature-store schema evolution, a real-time fraud-detection model begins misclassifying events, causing elevated false positives in two regions. Design a CAPA program to address this: 1) a CAPA data model capturing evidence and artifacts, 2) a lifecycle state machine for CAPA progression, 3) a minimal REST API to create/update CAPAs with linked logs/traces, 4) region-aware metrics to prove effectiveness (precision, recall drift, MTTR, false-positive rate), 5) an RCA template and a canary rollout plan for feature window validation and model rollback?","answer":"I propose a comprehensive CAPA program featuring a robust data model that captures incidentId, featureStoreVersion, modelVersion, affectedRegions, evidenceLinks, and traces to logs. The lifecycle follows: detected → triaged → containment → investigation → remediation → validation → closed. This includes a minimal REST API for CAPA creation and updates with linked logs and traces, region-aware metrics (precision, recall drift, MTTR, false-positive rate), an RCA template, and a canary rollout plan for feature window validation and model rollback.","explanation":"## Why This Is Asked\n\nThis tests robust containment and ML-driven CAPAs in data pipelines, focusing on schema-change impact, cross-region metrics, and auditable artifacts.\n\n## Key Concepts\n\n- CAPA data modeling for ML incidents\n- Region-aware metrics and drift detection\n- Canary-style rollout and rollback for feature windows\n- Traceability with logs/traces and model registry\n\n## Code Example\n\n```javascript\n// Example payload structure for CAPA API\nconst payload = { incidentId, featureStoreVersion, modelVersion, regions, evidenceLinks, traces };\n```\n\n## Follow-up Questions\n\n- How would you auto-generate CAPAs from drift alerts?\n- What triggers rollback vs. progressive canary expansion?\n- How do you ensure evidence immutability across regions?","diagram":null,"difficulty":"advanced","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Databricks","Instacart"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T05:14:41.669Z","createdAt":"2026-01-17T23:39:11.351Z"},{"id":"q-3687","question":"In a multi-region Databricks lakehouse, a misconfigured data-classification rule briefly exposes PII in two regions. Design a beginner CAPA to prevent recurrence. Include: 1) a CAPA data model to capture evidence, root cause, corrective and preventive actions, 2) a simple lifecycle state machine, 3) a minimal REST API to create/update CAPAs with linked logs/evidence, 4) region-aware containment metrics, 5) an RCA template and a canary-based remediation plan before full rollout?","answer":"Propose a CAPA: a data model with id, region, evidenceLinks, rootCause, correctiveActions, preventiveActions, metrics, state, timestamps; lifecycle Detected->Investigating->Containment->Remediation->V","explanation":"## Why This Is Asked\nTests practical CAPA design for data governance in multi-region data stacks.\n\n## Key Concepts\n- Data governance CAPA model\n- State machine lifecycle\n- Lightweight API design\n- Region-aware metrics and RCA\n- Canary risk mitigation\n\n## Code Example\n```javascript\n// Skeleton REST handler Mock\n```\n\n## Follow-up Questions\n- How would you test the canary reliably?\n- How to version CAPA templates for audits?","diagram":"flowchart TD\nA[Detected exposure] --> B[Containment]\nB --> C[RCA]\nC --> D[Corrective Action]\nD --> E[Verified]\nE --> F[Canary Rollout]\nF --> G[Closed]","difficulty":"beginner","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T05:37:32.986Z","createdAt":"2026-01-18T05:37:32.986Z"},{"id":"q-3780","question":"Scenario: An identity service misrouting leads to token leakage under load, exposing user data in two regions. Design a CAPA program that addresses security risk at scale. Deliverables: 1) a CAPA data model capturing evidence, tenants, and config changes, 2) a region-aware lifecycle state machine, 3) a minimal REST API to create/update CAPAs with linked logs/traces, 4) metrics for containment (leakage_rate, MTTR, recurrence, false_positives), 5) an RCA template and a canary rollout plan to validate remediation before global rollout, including key-rotation strategy?","answer":"Propose: 1) CAPA data model: incident_id, region, tenants, evidence (logs/traces), RCA, remediation, status; 2) lifecycle: detected → triaged → containment → remediation → verify → closed; 3) REST API","explanation":"## Why This Is Asked\nTests ability to design CAPA for security incidents in globally distributed services, emphasizing evidence management, lifecycle rigor, and measurable containment.\n\n## Key Concepts\n- Region-aware CAPA data model with audit-ready evidence\n- Immutable, linked logs/traces for traceability\n- Canary-driven remediation and safe key/token rotation\n- Production-scale metrics: leakage_rate, MTTR, recurrence, false_positives\n- RCA templating for repeatable learning\n\n## Code Example\n```javascript\n// Example CAPA data structure (simplified)\n{\n  incident_id: 'INC-202601',\n  region: 'us-east-1',\n  tenants: ['t1','t2'],\n  evidence: ['logs/abc', 'traces/xyz'],\n  rca: '',\n  remediation: '',\n  status: 'detected'\n}\n```\n\n## Follow-up Questions\n- How would you ensure PII remains protected in CAPA evidence?\n- How would you scale the REST API for millions of CAPAs and high write throughput?","diagram":null,"difficulty":"advanced","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Microsoft","OpenAI","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T09:33:03.503Z","createdAt":"2026-01-18T09:33:03.504Z"},{"id":"q-3822","question":"Scenario: A multi-tenant AI model hosting platform uncovers cross-tenant data leakage after a deployment that altered data partitioning. Design a CAPA program to detect, capture evidence, and prevent recurrence. Include: 1) a CAPA data model for evidence, lineage, and remediation; 2) a tenant-aware lifecycle; 3) a minimal REST API to create/update CAPAs with linked logs; 4) tenant-scoped metrics (leak rate, time-to-detection, MTTR); 5) RCA template and canary rollback plan targeting partitioning code?","answer":"I would implement a CAPA program for cross-tenant data leakage in a multi-tenant AI hosting platform. CAPA data model includes CAPA, Evidence, DataLineage, Tenant, Incident, Remediation; lifecycle: de","explanation":"## Why This Is Asked\nThis question probes end-to-end CAPA design under data-privacy risk in multi-tenant systems, including lineage, access control, and rollbacks.\n\n## Key Concepts\n- Data lineage and evidence aggregation across tenants\n- Tenant-aware lifecycle and RBAC implications\n- Canary-based remediation and RCA framing\n\n## Code Example\n```javascript\n// Example CAPA schema (simplified)\ntype CAPA = {\n  id: string\n  tenantId: string\n  incidentId: string\n  evidenceRefs: string[]\n  dataLineage: string\n  status: 'detected'|'triaged'|'contained'|'remediated'|'verified'|'closed'\n  createdAt: string\n  updatedAt: string\n}\n```\n\n## Follow-up Questions\n- How would you simulate data leakage to validate the CAPA workflow without exposing real data?\n- What additional metrics would you add for regulatory compliance reporting?","diagram":"flowchart TD\n  A[Leak detected] --> B[Create CAPA]\n  B --> C[Containment]\n  C --> D[Remediation]\n  D --> E[Verification]\n  E --> F[Closed]","difficulty":"advanced","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T10:45:31.504Z","createdAt":"2026-01-18T10:45:31.504Z"},{"id":"q-3869","question":"Scenario: In a multi-tenant data platform used by enterprise customers, an ingestion job drift triggers potential exposure of PII across two regions due to a schema evolution. Design a CAPA program focused on privacy and regulatory compliance: 1) a CAPA data model capturing privacy artifacts (PII fields, masking rules, data lineage, access controls), 2) a region-aware lifecycle state machine (discovery, containment, remediation, verification, closure), 3) a minimal REST API to create/update CAPAs with linked logs and lineage, 4) privacy metrics (time-to-containment for sensitive data drift, PIIs discovered, false positives), 5) an RCA template and a canary plan to validate masking and access controls before global rollout?","answer":"CAPA data model: CAPA, Evidence, PrivacyArtifact (PII fields, maskingRule, dataLineage), AccessControl, DataSubjectMetric. Lifecycle: discovery -> containment -> remediation -> verification -> closure","explanation":"## Why This Is Asked\nTests ability to design privacy-aware CAPA in a data platform, measure regulatory impact, and ship safe canaries. It exercises data lineage, masking, access control, and cross-region governance.\n\n## Key Concepts\n- Privacy-by-design CAPA, data lineage, masking rules, and access controls\n- Regulatory compliance considerations (GDPR/CCPA)\n- Cross-region data governance and canary testing for masking\n\n## Code Example\n```javascript\n// Example CAPA schema fragment\nconst CAPA_SCHEMA = {\n  id: 'string',\n  privacyArtifacts: {\n    piiFields: ['customer_email', 'ssn'],\n    maskingRule: 'partial_mask',\n    dataLineage: 'path/to/lineage'\n  },\n  lifecycle: ['discovery','containment','remediation','verification','closure'],\n  metrics: { timeToContainmentMs: 1234 }\n}\n```\n\n## Follow-up Questions\n- How would you automate drift detection of PII fields in logs across regions?\n- What trade-offs exist between masking depth and query performance when auditing large datasets?","diagram":"flowchart TD\n  A[Detection] --> B{Containment}\n  B --> C[Evidence Collection]\n  C --> D[Investigation]\n  D --> E[Remediation]\n  E --> F[Validation]\n  F --> G[Closure]","difficulty":"intermediate","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Salesforce","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T13:08:50.072Z","createdAt":"2026-01-18T13:08:50.072Z"},{"id":"q-3952","question":"Scenario: A globally deployed fraud-detection ML model starts showing elevated false positives in two regions after a retrain. Design a CAPA program to detect model drift, contain impact, and prevent recurrence. Include: 1) a CAPA data model capturing evidence (drift metrics, feature distributions, FP rate by region, logs, dataset snapshots) and artifacts (model version, data lineage), 2) a lifecycle state machine for CAPA progression, 3) a minimal REST API to create/update CAPAs with linked traces, 4) region-aware metrics proving containment (drift score, FP rate, MTTR), 5) RCA template and a canary-based remediation plan (retrain with backfill, rollback, feature-store checks)?","answer":"Outline a CAPA for ML drift: 1) CAPA data model storing evidence (drift metrics, feature distributions, FP rate by region, logs, dataset snapshots) and artifacts (model version, data lineage), 2) life","explanation":"## Why This Is Asked\nTests ability to design CAPA for ML drift across regions, integrating data lineage, artifacts, API design, and remediation strategies.\n\n## Key Concepts\n- ML drift detection and regional observability\n- CAPA data model and lifecycle\n- Canary-based remediation and rollback\n\n## Code Example\n```javascript\n// Payload shape (illustrative)\nconst payload = {\n  capaId: 'CAPA-123',\n  evidence: { drift: { score: 0.42 }, fpRateByRegion: { US: 0.03, EU: 0.05 } },\n  modelVersion: 'v1.2.3',\n  state: 'open'\n}\n```\n\n## Follow-up Questions\n- How would you set drift thresholds and alerting? \n- How would you validate a rollback without regressing customers?","diagram":"flowchart TD\n  A[Ingest] --> B[Drift detected]\n  B --> C[CAPA Open]\n  C --> D[Containment]\n  D --> E[Remediate]\n  E --> F[Verification]\n  F --> G[Closed]","difficulty":"intermediate","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Hashicorp","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T16:47:15.688Z","createdAt":"2026-01-18T16:47:15.688Z"},{"id":"q-4036","question":"Scenario: A cross-region ML content-safety classifier drift after a rollout causes increased unsafe-content passes in Region A while Region B remains fine. Design a CAPA program to detect, collect evidence, and prevent recurrence. Include: 1) a CAPA data model; 2) a region-aware lifecycle; 3) a minimal REST API to create/update CAPAs with evidence; 4) region-based metrics to prove containment; 5) RCA template and a canary/rollback plan?","answer":"Design a comprehensive CAPA program for cross-region ML content-safety classifier drift. Data model: CAPA(id, incident_id, region, model_name, version, evidence:[logs,traces,metrics], state, created_at, updated_at, owner). Region-aware lifecycle: detection → evidence collection → containment → RCA → remediation → verification → closure, with region-specific state tracking and cross-region coordination. Minimal REST API: POST /capas (create), PUT /capas/{id} (update), GET /capas/{id}/evidence, POST /capas/{id}/evidence, with region-based filtering and evidence attachment support. Region-based metrics: containment_rate, false_positive_rate, model_drift_score, latency_p99 per region, with trend analysis and alerting thresholds. RCA template: incident timeline, affected regions, drift analysis, root causes, containment actions, preventive measures, lessons learned. Canary/rollback plan: gradual traffic shifting (5% → 25% → 50% → 100%), automated monitoring with rollback triggers, region-specific canary groups, and blue-green deployment fallback.","explanation":"## Why This Is Asked\nTests ability to design a scalable, region-aware CAPA program for ML incidents with comprehensive evidence capture and controlled deployment strategies.\n\n## Key Concepts\n- CAPA data modeling for ML incidents with cross-region coordination\n- Region-aware lifecycle management with state tracking\n- Evidence collection from logs, traces, and metrics\n- Canary rollout strategy with automated rollback triggers\n- RCA documentation with preventive action planning\n- Region-specific metrics and containment verification\n\n## Code Example\n```javascript\n// CAPA schema with region awareness\nconst CAPASchema = {\n  id: 'uuid',\n  incident_id: 'string',\n  region: 'string',\n  model_name: 'string',\n  version: 'string',\n  evidence: ['logs', 'traces', 'metrics'],\n  state: 'enum',\n  created_at: 'timestamp',\n  updated_at: 'timestamp',\n  owner: 'string'\n};\n```","diagram":null,"difficulty":"advanced","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Hugging Face","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T06:22:17.910Z","createdAt":"2026-01-18T20:47:03.888Z"},{"id":"q-4140","question":"Scenario: A multi-region EV telemetry pipeline relies on a third‑party enrichment service. An outage causes inconsistent vehicle trip durations and location matching in Regions North and East. Design a CAPA program that covers: 1) a CAPA data model capturing evidence and artifacts, 2) a lifecycle state machine for cross‑vendor CAPAs, 3) a minimal REST API to create/update CAPAs with logs/traces and vendor links, 4) region-aware metrics to prove containment (recurrence rate, MTTR, false positive rate), 5) an RCA template and a canary rollback plan for vendor changes?","answer":"CAPA data model includes id, incidentId, region, vendor, dataSource, evidence, traces, rca, remediation, status, timestamps. Lifecycle: detected → triaged → containment → remediation → validated → clo","explanation":"## Why This Is Asked\nTests cross‑vendor CAPA governance, multi‑region data quality, and evidence‑driven remediation beyond basic incident handling.\n\n## Key Concepts\n- Cross‑vendor data quality\n- Region‑aware containment\n- Evidence‑driven CAPA data model\n- Canary rollback with supplier changes\n\n## Code Example\n```javascript\n// Minimal CAPA data model sketch\nconst CAPA = {\n  id: '',\n  incidentId: '',\n  region: '',\n  vendor: '',\n  dataSource: '',\n  evidence: [],\n  traces: [],\n  rca: '',\n  remediation: '',\n  status: 'detected',\n  createdAt: '',\n  updatedAt: ''\n};\n```\n\n## Follow-up Questions\n- How would you test the API and the metrics? \n- What privacy considerations apply to evidence logs in a multi‑vendor setup?","diagram":"flowchart TD\n  A[Incident] --> B[CAPA Created]\n  B --> C[Containment Initiated]\n  C --> D[Remediation Applied]\n  D --> E[Validation]\n  E --> F[Closed]","difficulty":"intermediate","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T04:41:50.036Z","createdAt":"2026-01-19T04:41:50.036Z"},{"id":"q-4187","question":"Scenario: A dynamic ride-pricing model experiences unexplained underpricing after a feature-store schema evolution, leading to regional revenue drop. Design a CAPA program to detect drift, collect evidence (model version, feature lineage, data distributions, logs), and prevent recurrence. Include: 1) a CAPA data model, 2) a region-aware lifecycle, 3) a minimal REST API to create/update CAPAs with linked logs/traces, 4) drift-aware metrics to prove containment (drift rate, MTTR, FP rate), 5) an RCA template and a canary remediation plan (rollback to prior feature/version, retraining)?","answer":"Propose a CAPA: data model with model_id, region, schema_version, feature_version, evidence_refs, containment_status. Lifecycle: detected → investigated → contained → eradicated → verified. API: POST/","explanation":"## Why This Is Asked\n\nEvaluates end-to-end CAPA design for ML pricing with data lineage across regions, including containment and rollback viability.\n\n## Key Concepts\n\n- ML drift detection and measurement\n- Data provenance and feature lineage\n- Region-aware canary remediation and metrics\n- Root-cause analysis templates\n\n## Code Example\n\n```json\n{\n  \"model_id\": \"pricing_v3\",\n  \"region\": \"us-east-1\",\n  \"schema_version\": \"2026-01-19\",\n  \"feature_version\": \"fv4\",\n  \"evidence_refs\": [\"log-789\",\"trace-101112\"],\n  \"containment_status\": \"detected\"\n}\n```\n\n## Follow-up Questions\n\n- How would you automate data-lineage capture across the feature store?\n- What drift thresholds would you use and how would you validate canaries?","diagram":"flowchart TD\n  Detect[Detect drift] --> Investigate[Investigate evidence]\n  Investigate --> Contain[Containment]\n  Contain --> RCA[Root cause + RCA]\n  RCA --> Canary[Canary remediation]","difficulty":"advanced","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Lyft","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T07:03:16.575Z","createdAt":"2026-01-19T07:03:16.576Z"},{"id":"q-4302","question":"In a real-time model serving pipeline for a high-frequency trading platform, a schema evolution in the feature store causes online scores to be computed with mismatched features across three regions, leading to incorrect risk estimates. Design a CAPA program to address this: 1) a CAPA data model capturing evidence and artifacts, 2) a region-aware lifecycle state machine for CAPA progression, 3) a minimal REST API to create/update CAPAs with logs/traces, 4) region/feature-type metrics to prove containment and avoid recurrence, 5) an RCA template and a canary-based rollback plan for feature changes?","answer":"Propose a versioned CAPA data model capturing CAPA metadata, evidence references, and a region-to-feature-map; implement a region-aware lifecycle with states: identified, contained, investigated, reme","explanation":"## Why This Is Asked\n\nThis question probes end-to-end CAPA design under real-time drift in a multi-region ML workflow, focusing on data models, lifecycle, APIs, metrics, RCA, and canary rollout.\n\n## Key Concepts\n\n- CAPA data modeling\n- region-aware lifecycle\n- evidence/logs integration\n- region-feature metrics\n- RCA templates and canary plans\n\n## Code Example\n\n```javascript\n// Minimal CAPA data model snippet\ntype CAPA = {\n  id: string;\n  region: string;\n  status: string;\n  evidenceRefs: string[];\n  artifacts: string[];\n  createdAt: string;\n  updatedAt: string;\n}\n```\n\n## Follow-up Questions\n\n- How would you version CAPA data models and migrations?\n- How do you ensure canary rollouts remain drift-free across regions?\n","diagram":null,"difficulty":"advanced","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Hugging Face","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T13:07:33.885Z","createdAt":"2026-01-19T13:07:33.885Z"},{"id":"q-4434","question":"Scenario: In a multi-tenant data lake used by a regulated enterprise, one tenant's ingestion triggers data skew that contaminates downstream analytics while costing spikes. Design a CAPA program focusing on tenant isolation, data provenance, and cost control. Include: 1) a CAPA data model capturing evidence and artifacts, 2) a tenant-scoped lifecycle state machine for CAPA progression, 3) a minimal REST API to create/update CAPAs with linked logs/traces, 4) tenant-aware metrics to prove containment and recurrence, 5) an RCA template and a safe rollback plan for data ingestion changes. What would you implement and why?","answer":"I would implement a CAPA with 1) a data model: capA_id, tenant_id, incident_id, evidence_links, logs_traces, affected_tables, feature_versions, severity, status; 2) lifecycle: new -> investigating -> ","explanation":"## Why This Is Asked\nTests ability to handle CAPA at multi-tenant scale with data provenance, cost control, and isolation. Requires concrete data model, lifecycle, API surface, and measurable metrics.\n\n## Key Concepts\n- Data provenance and lineage\n- Tenant isolation vs shared infra\n- Cost-aware remediation\n- Lifecycle state machine\n- REST API design and observability\n\n## Code Example\n```javascript\ntype CAPA = {\n  capa_id:string;\n  tenant_id:string;\n  incident_id:string;\n  evidence_links:string[];\n  logs_traces:string[];\n  affected_tables:string[];\n  feature_versions:string[];\n  severity:string;\n  status:'new'|'investigating'|'containment'|'RCA'|'remediation'|'closed';\n}\n```\n\n## Follow-up Questions\n- How would you validate the canary rollback in prod?\n- What trade-offs exist between strict tenant isolation and operational complexity?","diagram":"flowchart TD\n  A[New CAPA] --> B[Investigating]\n  B --> C[Containment]\n  C --> D[RCA]\n  D --> E[Remediation]\n  E --> F[Closed]","difficulty":"advanced","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","LinkedIn","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T18:56:03.798Z","createdAt":"2026-01-19T18:56:03.798Z"},{"id":"q-4526","question":"In a cross-region data-aggregation pipeline for fraud detection, a misconfigured regional join flag breaks data lineage and risks PII exposure in one region. Design a CAPA program with: 1) a CAPA data model for evidence and artifacts, 2) a cross-region lifecycle state machine, 3) a minimal REST API to create/update CAPAs with logs/traces, 4) region-specific privacy metrics (lineage completeness, PII access, MTTR), 5) an RCA template and a canary rollout plan for flag changes?","answer":"Design a comprehensive CAPA program with: 1) A structured data model capturing evidence, artifacts, and metadata with region-specific context; 2) A cross-region lifecycle state machine managing phases from IDENTIFIED → INVESTIGATING → CONTAINMENT → RESOLVED → CLOSED with escalation triggers; 3) REST API endpoints (POST /capas, PUT /capas/{id}) integrating logging, tracing, and audit trails; 4) Region-specific privacy metrics including lineage completeness scores, PII access patterns, and mean-time-to-resolution; 5) RCA template framework and canary rollout strategy with rollback criteria for flag configuration changes.","explanation":"## Why This Is Asked\nThe scenario evaluates end-to-end CAPA thinking spanning privacy engineering, data lineage management, cross-region coordination, and controlled remediation practices.\n\n## Key Concepts\n- CAPA data model with evidence/artifacts correlation\n- Cross-region lifecycle with state transitions\n- Privacy metrics and compliance tracking\n- Canary deployment and rollback mechanisms\n\n## Code Example\n```javascript\nconst CAPA = {\n  id: '',\n  region: '',\n  symptom: '',\n  evidence: [],\n  artifacts: [],\n  state: 'IDENTIFIED',\n  correlationId: ''\n}\n```\n\n## Follow-up Questions\n- What specific rollback triggers would you implement for flag changes?\n- How would you measure and report privacy metrics across regions?\n- What monitoring capabilities would ensure early detection of similar issues?","diagram":null,"difficulty":"intermediate","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T06:54:20.040Z","createdAt":"2026-01-19T22:38:03.753Z"},{"id":"q-4601","question":"Scenario: In a fleet of autonomous vehicles operating across multiple regions, a recently rolled OTA firmware update causes sporadic misdetections in sensor fusion, leading to unsafe behavior in Region A and Region B. Design a CAPA program to detect, contain, and prevent recurrence. Include: 1) a CAPA data model capturing evidence, telemetry, firmware/OTA versions, and logs; 2) a region-aware lifecycle state machine for CAPA progression; 3) a minimal REST API to create/update CAPAs with linked logs/traces; 4) region- and sensor-type metrics to prove containment (recurrence rate, MTTR, false positives, drift); 5) an RCA template and a staged canary rollout plan for firmware/feature changes?","answer":"Propose a CAPA plan that includes: 1) a CAPA data model mapping evidence to telemetry, logs, and OTA versions; 2) a regional lifecycle state machine (Triaged → Contained → Investigating → Remediated →","explanation":"## Why This Is Asked\nTests ability to design end-to-end CAPA, including data models, state management, and safety-conscious rollout in a high-stakes, multi-region domain.\n\n## Key Concepts\n- CAPA data modeling and evidence linking\n- Region-aware lifecycle and escalation\n- Lightweight API for CAPA CRUD with traces\n- Metrics for containment and recurrence\n- RCA templates and canary rollout strategies\n\n## Code Example\n```javascript\n// Minimal Express endpoint sketch\nconst express = require('express');\nconst app = express();\napp.use(express.json());\napp.post('/capa', (req, res) => {\n  // validate and persist CAPA with linked traces\n  res.status(201).send({ id: 'capa-123' });\n});\n```\n\n## Follow-up Questions\n- How would you model data retention for CAPA artifacts across regions?\n- How would you audit changes to CAPAs and ensure traceability?","diagram":"flowchart TD\n  CAPA[CAPA] --> DM[Data Model]\n  CAPA --> LS[Lifecycle State Machine]\n  CAPA --> API[REST API]\n  CAPA --> M[Metrics & RCA]\n  API --> TR[Linked Logs/Traces]\n  M --> Canary[Canary Rollout Plan]","difficulty":"intermediate","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["NVIDIA","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T04:13:43.645Z","createdAt":"2026-01-20T04:13:43.646Z"},{"id":"q-4722","question":"Scenario: After a deployment, a misconfigured ETL sink writes PII to a non-prod data lake in Region-2, triggering a data-governance alert. Design a CAPA program to detect, contain, and prevent recurrence. Include: 1) a CAPA data model with evidence, lineage, and owners; 2) a region-aware lifecycle with gates; 3) a minimal REST API to create/update CAPAs with logs/traces and lineage links; 4) region metrics for containment and leakage; 5) an RCA template and a canary rollback plan for sink changes?","answer":"CAPA data model with capa_id, incident_time, data_class, evidence_refs, lineage, owner, remediation, status, RCA_id. Lifecycle: IDENTIFY → CONTAIN → INVESTIGATE → REMEDIATE → VERIFY → CLOSED with gate","explanation":"## Why This Is Asked\nTests ability to design a CAPA program around data governance, cross-region containment, and traceability after a data-leak fault in a deployment.\n\n## Key Concepts\n- Data lineage and evidence tagging\n- Region-aware CAPA lifecycle gates\n- REST API design for CAPA management with linked logs/traces\n- Regional leakage metrics and false positive handling\n- RCA templates and canary rollback for data sinks\n\n## Code Example\n```javascript\n// Minimal CAPA data model in TypeScript\ntype CAPA = {\n  capa_id: string;\n  incident_time: string;\n  data_class: string;\n  evidence_refs: string[];\n  lineage: string;\n  owner: string;\n  remediation: string;\n  status: 'IDENTIFY'|'CONTAIN'|'INVESTIGATE'|'REMEDIATE'|'VERIFY'|'CLOSED';\n  RCA_id?: string;\n}\n```\n\n## Follow-up Questions\n- How would you integrate with an existing audit log system?\n- How would you enforce access controls for CAPA modification?","diagram":"flowchart TD\n  A[Detect PII Leak] --> B[Contain & Quarantine Sink]\n  B --> C[Trace Lineage & Gather Evidence]\n  C --> D[Root Cause Analysis]\n  D --> E[Remediate Sink & Canaries]\n  E --> F[Verify & Close]","difficulty":"intermediate","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Oracle","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T09:56:19.580Z","createdAt":"2026-01-20T09:56:19.580Z"},{"id":"q-4750","question":"Scenario: A multi-tenant streaming platform experiences sudden, per-tenant drift in sessionization in Region-2, causing incorrect churn metrics for several tenants. Design a CAPA program that addresses drift without impacting others. Include: 1) a CAPA data model capturing tenant scope, evidence, and lineage; 2) a region-tenant gated lifecycle; 3) a minimal REST API to create/update CAPAs with linked logs/traces; 4) tenant-scoped metrics to prove containment and recurrence; 5) an RCA template and a canary rollout plan for the sessionization logic updates?","answer":"Propose a CAPA with a data model: fields for id, tenant_id, region, evidence_links, candidate_root_cause, containment_actions, owners, timing. Lifecycle: DETECT -> CONTAIN -> ERADICATE -> VALIDATE -> ","explanation":"## Why This Is Asked\nTests handling of drift in a multi-tenant streaming system with region isolation, ensuring containment without collateral impact. It also probes data lineage, governance, and rollout discipline.\n\n## Key Concepts\n- CAPA data model with tenant scope, region, evidence, ownership, and lineage\n- Region-tenant gated lifecycle to prevent cross-tenant bleed\n- Minimal REST API for CAPA creation/update with linked logs/traces\n- Tenant-scoped metrics: containment MTTR, drift score, recurrence rate, false positives\n- RCA template and canary rollback plan for sessionization logic changes\n\n## Code Example\n```javascript\n// Skeleton: CAPA data model (conceptual)\nclass CAPA {\n  constructor(id, tenantId, region) {\n    this.id = id;\n    this.tenantId = tenantId;\n    this.region = region;\n    this.evidence = [];\n    this.rootCause = null;\n    this.actions = [];\n    this.owner = null;\n    this.state = 'DETECTED';\n    this.timestamps = {};\n  }\n}\n```\n\n## Follow-up Questions\n- How would you test isolation between tenants during a canary rollout?\n- How would you handle cross-tenant drift if tenants share resources?\n","diagram":"flowchart TD\n  A[Detect drift per tenant] --> B[Create CAPA with tenant scope]\n  B --> C{Gate: region-tenant}\n  C --> D[Containment actions]\n  D --> E[RCA template]\n  E --> F[Canary rollout on subset]\n  F --> G[Monitor acceptance and roll back if needed]","difficulty":"intermediate","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Netflix","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T10:50:46.363Z","createdAt":"2026-01-20T10:50:46.363Z"},{"id":"q-4950","question":"Scenario: A labeling quality issue in two regions after a policy update affects multilingual captions used to fine-tune a model. Design a beginner CAPA program to address this. Include: 1) a CAPA data model capturing evidence (samples, annotator IDs, tool version, policyVersion, region), 2) a regionalized lifecycle state machine, 3) a minimal REST API to create/update CAPAs with evidence, 4) region-aware metrics to prove containment (per-region labelAccuracy, interAnnotatorAgreement, MTTR, relapseRate), 5) an RCA template and a staged canary remediation plan (revert policy in one region, update annotations, re-run validation)?","answer":"Implement CAPA for a labeling quality issue in two regions after a policy change. Data model: CAPA {id, title, evidenceSamples[], annotatorIDs[], toolVersion, policyVersion, region}; lifecycle: {Disco","explanation":"Why This Is Asked\nEvaluates ability to design a practical CAPA for data labeling quality issues in ML pipelines, focusing on evidence provenance, regional containment, and staged remediation.\n\nKey Concepts\n- CAPA data model with evidence, versions, and regional scope\n- Lifecycle state machine for progression and governance\n- Minimal REST API for creating/updating CAPAs with evidence links\n- Region-aware metrics to prove containment and prevention\n- RCA template and canary-based remediation strategy\n\nCode Example\n```json\n{\n  \"CAPA\": {\n    \"id\": \"CAPA-001\",\n    \"title\": \"Labeling quality issue in multilingual captions\",\n    \"evidenceSamples\": [\"sample1\", \"sample2\"],\n    \"annotatorIDs\": [\"a1\", \"a2\"],\n    \"toolVersion\": \"v3.2\",\n    \"policyVersion\": \"2026-01\",\n    \"region\": \"eu-west-1\"\n  }\n}\n```\n\nFollow-up Questions\n- How would you validate the canary remediation plan before full rollout?\n- What logs and metrics would you collect to demonstrate containment to stakeholders?","diagram":null,"difficulty":"beginner","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T20:46:10.256Z","createdAt":"2026-01-20T20:46:10.256Z"},{"id":"q-4957","question":"Scenario: In a multi-tenant data platform, a newly deployed user-defined function (UDF) inadvertently leaks PII across tenants during aggregation. Design a CAPA program to detect, contain, and prevent recurrence. Include: 1) CAPA data model capturing evidence, access events, and UDF versions; 2) tenant-aware lifecycle state machine; 3) minimal REST API to create/update CAPAs with linked logs/traces; 4) per-tenant metrics proving containment; 5) RCA template and a canary rollout plan for UDF changes?","answer":"Design a comprehensive CAPA program for a multi-tenant data platform where a newly introduced UDF leaks PII across tenants during aggregation. Data model: id, tenant_id, event_time, evidence, logs, udf_version, affected_tables, remediation_status, containment_metrics. Tenant-aware lifecycle state machine: detected → analyzing → containing → remediating → verifying → closed, with per-tenant SLA tracking. Minimal REST API: POST /capas, GET /capas/{id}, PUT /capas/{id}, supporting evidence linkage and log/trace correlation. Per-tenant metrics: containment_time, data_leakage_volume, affected_users, remediation_success_rate. RCA template: timeline, root_cause, impact_assessment, containment_actions, prevention_measures. Canary rollout plan: tenant isolation, gradual rollout with automated monitoring, rollback triggers, and success criteria validation.","explanation":"## Why This Is Asked\nTests practical CAPA design for cross-tenant data leakage in a shared data platform, emphasizing isolation, traceability, and controlled deployments.\n\n## Key Concepts\n- Tenant-aware CAPA data model\n- Evidence linkage across logs/traces\n- Lifecycle governance and SLAs\n- Canary rollout with per-tenant gating\n\n## Code Example\n```javascript\n{\n  \"id\": \"CAPA-20260120-01\",\n  \"tenant_id\": \"tenant-a\",\n  \"event_time\": \"2026-01-20T12:34:56Z\",\n  \"evidence\": [\"s3://bucket/logs/123\",\"trace/789\"],\n  \"udf_version\": \"v2.4.0\",\n  \"affected_tables\": [\"orders\"],\n  \"remediation\": \"disable udf immediately\"\n}\n```","diagram":null,"difficulty":"advanced","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Databricks"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-21T06:14:16.361Z","createdAt":"2026-01-20T21:31:38.494Z"},{"id":"q-5120","question":"In a multi-region data platform with cross-border backups, an automated backup job writes a sensitive dataset to region with weaker residency controls, triggering a compliance alert. Design a CAPA program that: 1) captures evidence, lineage, owners in a CAPA data model; 2) defines a region-aware lifecycle with gates (detect, contain, remediate, verify); 3) provides a minimal REST API to create/update CAPAs with logs/traces and policy tags; 4) metrics for containment and cross-region recurrence; 5) an RCA template and a canary rollout plan for policy/guardrail changes?","answer":"Implement a CAPA using a data model that records evidence, data lineage, owners, and policy tags; a region-aware lifecycle with gates (detect → contain → remediate → verify); a minimal REST API to ups","explanation":"## Why This Is Asked\n\nTests ability to design CAPA for cross-border data governance with explicit owners, evidence capture, and trackable gates. It requires multi-region controls, policy tagging, and canary-based remediation to minimize risk.\n\n## Key Concepts\n\n- CAPA data model with evidence, lineage, owners\n- Region-aware lifecycle with gates\n- Minimal REST API for CAPA CRUD with logs/traces and policy tags\n- Cross-region residency metrics (MTTR, recurrence, false positives)\n- RCA templates and canary rollout plans\n\n## Code Example\n\n```javascript\n// Example CAPA data model (simplified)\nconst CAPA = {\n  id: 'capa-123',\n  incidentId: 'inc-987',\n  evidence: ['log1','data-map-123'],\n  owners: ['sec-team','data-owners'],\n  region: 'us-west-1',\n  status: 'detect',\n  lifecycle: ['detect','contain','remediate','verify','close']\n};\n```\n\n## Follow-up Questions\n\n- How would you test the region gates and canary rollouts?\n- What metrics would you use to decide closure versus escalation?","diagram":null,"difficulty":"intermediate","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Oracle","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T07:08:14.130Z","createdAt":"2026-01-21T07:08:14.130Z"},{"id":"q-5192","question":"Scenario: A multi-region data bus occasionally routes tenant data to the wrong region due to a routing bug, risking residency violations and regulatory alerts. Design a CAPA program: 1) a data model capturing evidence, lineage, and owners; 2) a region-aware lifecycle state machine with gates (detect, contain, remediate, verify); 3) a minimal REST API to create/update CAPAs with logs and lineage; 4) region metrics for leakage and dwell time; 5) RCA template and a canary rollout plan for routing changes?","answer":"CAPA data model: id, tenant_id, region, incident_time, evidence, lineage_links, owners, severity; store with versioning. Lifecycle: detect -> contain -> remediate -> verify -> close with canary gates.","explanation":"## Why This Is Asked\nTests practical CAPA engineering: data lineage, region-aware workflows, and automated containment. It also probes how candidates tie metrics to policy changes and canary strategies.\n\n## Key Concepts\n- CAPA data model with evidence and lineage\n- Region-aware lifecycle and gates\n- Minimal API design for create/update with logs\n- Metrics tying containment and recurrence\n- RCA template and canary rollout\n\n## Code Example\n```javascript\n// Example TypeScript-like CAPA data model\ninterface CAPA {\n  id: string\n  tenantId: string\n  region: string\n  incidentTime: string\n  evidenceLinks: string[]\n  lineageLinks: string[]\n  owners: string[]\n  severity: 'low'|'medium'|'high'\n}\n```\n\n## Follow-up Questions\n- How would you test the canary rollout across regions?\n- How would you ensure tamper-evident evidence logs?","diagram":"flowchart TD\n  Detect[Detect] --> Contain[Contain]\n  Contain --> Remediate[Remediate]\n  Remediate --> Verify[Verify]\n  Verify --> Close[Close]","difficulty":"intermediate","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","DoorDash","Goldman Sachs"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T10:17:02.214Z","createdAt":"2026-01-21T10:17:02.214Z"},{"id":"q-5221","question":"Scenario: A multi-tenant data platform used by advertisers experiences a region-specific schema evolution in the event ingestion path that, due to GDPR consent flags, only affects EU tenants. Design a CAPA program that addresses privacy-by-design while containment and recurrence prevention. Include: 1) a CAPA data model capturing evidence, consent flags, data locality, and audit traces; 2) a tenant-scoped lifecycle state machine for CAPA progression; 3) a minimal REST API to create/update CAPAs with linked logs/traces; 4) privacy-aware metrics (containment time, recurrence by tenant, consent-compliance rate) by region; 5) an RCA template and a canary rollout plan that respects data locality and consent constraints?","answer":"Model a CAPA with tenant_id, region, consent_flags, evidence_refs, data_location, logs, and SLA state. Lifecycle: DETECTED -> INVESTIGATING -> CONTAINED -> CORRECTED -> VERIFIED -> CLOSED, with region","explanation":"## Why This Is Asked\nTests ability to design CAPA with privacy-by-design in a multi-tenant, region-aware system.\n\n## Key Concepts\n- CAPA data model with consent, locality, and audit trails\n- Tenant-scoped lifecycle and regulatory constraints\n- Logs/traces linkage and privacy controls\n- Region-aware metrics and privacy validation\n- RCA template and guarded canary rollout\n\n## Code Example\n```javascript\nconst CAPA_STATE_MACHINE = [\"DETECTED\",\"INVESTIGATING\",\"CONTAINED\",\"CORRECTED\",\"VERIFIED\",\"CLOSED\"];\n```\n\n## Follow-up Questions\n- How would you test the canary plan under GDPR constraints?\n- Which metrics would you monitor to detect data leakage early?","diagram":"flowchart TD\n  A[DETECTED] --> B[INVESTIGATING]\n  B --> C[CONTAINED]\n  C --> D[CORRECTED]\n  D --> E[VERIFIED]\n  E --> F[CLOSED]","difficulty":"advanced","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T11:32:14.799Z","createdAt":"2026-01-21T11:32:14.801Z"},{"id":"q-5257","question":"Scenario: A multi-region, multi-tenant data platform experiences PII leakage after a schema evolution. Design a CAPA program to prevent recurrence and ensure containment across regions. What would 1) the CAPA data model, 2) the region-aware lifecycle, 3) a minimal REST API for CAPAs with linked logs, 4) cross-tenant metrics, and 5) the RCA template and canary remediation plan look like?","answer":"Design a CAPA program to prevent PII leakage after a schema evolution in a multi-region data platform. Include: 1) a CAPA data model capturing evidence, lineage, and policy/classification metadata; 2)","explanation":"## Why This Is Asked\nTests ability to design end-to-end CAPA for data governance with cross-region and cross-tenant constraints, focusing on containment, remediation, and measurable outcomes.\n\n## Key Concepts\n- PII leakage containment and policy tagging\n- CAPA data model with evidence, lineage, and classifications\n- Region-aware lifecycle with access controls\n- Minimal REST API for CAPAs and artifact linkage\n- Cross-tenant metrics: recurrence rate, MTTR, leakage rate\n- RCA template and canary remediation plan\n\n## Code Example\n```\n// Example CAPA object skeleton (illustrative)\nconst capa = {\n  id: \"...\",\n  evidence: [\"logId1\", \"traceId2\"],\n  lineage: { source: \"raw_events\", dest: \"processed_events\" },\n  policy: { tags: [\"PII\", \"RETENTION\"], classification: \"PII\" },\n  regionStatus: { us_east: \"OPEN\" },\n  lifecycle: [\"IDENTIFY\", \"INVESTIGATE\", \"CONTAIN\", \"ERADICATE\", \"VERIFY\"]\n};\n```\n\n## Follow-up Questions\n- How would you store and query cross-tenant policy metadata efficiently?\n- What metrics ensure no regression after remediation?","diagram":null,"difficulty":"advanced","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","Netflix","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T13:23:50.717Z","createdAt":"2026-01-21T13:23:50.717Z"},{"id":"q-5413","question":"In a multi-tenant streaming ingestion platform, a newly added data source intermittently emits late events during peak traffic, causing transient backlogs and incorrect aggregations. Design a CAPA program to address this: 1) a CAPA data model with evidence and lineage, 2) a region- and tenant-aware lifecycle with gates (detect, contain, remediate, verify), 3) a region-agnostic minimal REST API to create/update CAPAs with logs/traces and lineage links, 4) metrics for containment and recurrence by region/tenant, 5) an RCA template and a canary-based remediation plan for ingestion configuration and windowing logic?","answer":"Create a CAPA store with: id, owner, region, tenant, evidence, lineage, status. Use a state machine: DETECT → CONTAIN → REMEDIATE → VERIFY. REST API: POST/PUT /capa with evidence and lineage. Track re","explanation":"## Why This Is Asked\nEvaluates end-to-end CAPA design: data model, governance lifecycle, observability, and a safe remediation path in a multi-tenant streaming context.\n\n## Key Concepts\n- CAPA data model with evidence and lineage\n- Region/tenant-aware lifecycle with gates\n- Minimal REST API for CAPA creation/updates\n- Cross-tenant metrics: MTTR, recurrence, backlog\n- RCA template and canary-based remediation plan\n\n## Code Example\n```javascript\n// CAPA data model sketch\nclass CAPA { constructor(id, owner, region, tenant, evidence, lineage, status) { /*...*/ } }\n```\n\n## Follow-up Questions\n- How would you ensure idempotent CAPA creation in the face of retries?\n- What would your data-retention policy for CAPA artifacts look like?","diagram":"flowchart TD\n  Detect[Detect CAPA Trigger] --> Contain[Contain Issue]\n  Contain --> Remediate[Remediate Root Cause]\n  Remediate --> Verify[Verify Results]\n  Verify --> Close[Close CAPA]\n  REST[REST API: /capa] --> CAPARecord[CAPA Document]\n  CAPARecord --> Detect","difficulty":"intermediate","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Oracle","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T21:12:52.117Z","createdAt":"2026-01-21T21:12:52.117Z"},{"id":"q-5619","question":"Scenario: A multi-tenant SaaS collects PII and must enforce regional data-retention policies. A misconfigured retention job accidentally preserves data in Region EU beyond policy, risking compliance. Design a beginner CAPA to address this. Include: 1) a CAPA data model, 2) a lifecycle state machine, 3) a minimal REST API to create/update CAPAs with evidence, 4) region-aware metrics to prove containment, 5) an RCA template and a canary-based remediation plan?","answer":"CAPA data model: id, region, policy_id, incident_at, evidence_refs, root_causes, corrective_actions, status, audit_id; lifecycle: New -> Investigating -> Contained -> Corrected -> Verified; REST API: ","explanation":"## Why This Is Asked\nTests the ability to turn a privacy/policy failure into a concrete CAPA workflow, focusing on region containment and governance. It checks data modeling, a clear state machine, a minimal API interface, and how RCA and canary controls are defined for a beginner task.\n\n## Key Concepts\n- CAPA data model fields\n- Lifecycle state machine\n- Lightweight REST API design\n- Region-aware metrics for governance\n- RCA template and canary remediation\n\n## Code Example\n\n```javascript\n// Example skeleton for a CAPA object\nconst capa = {\n  id: 'CAPA-001',\n  region: 'EU',\n  policy_id: 'policy-retain-EU',\n  incident_at: '2026-01-22T12:00:00Z',\n  evidence_refs: ['e1','e2'],\n  root_causes: [],\n  corrective_actions: [],\n  status: 'New',\n  audit_id: 'audit-123'\n}\n```\n\n## Follow-up Questions\n- How would you test the canary rollout?\n- How would you monitor privacy compliance in real time?","diagram":"flowchart TD\n  A[New CAPA] --> B[Investigating]\n  B --> C[Contained]\n  C --> D[Corrected]\n  D --> E[Verified]","difficulty":"beginner","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T08:50:53.184Z","createdAt":"2026-01-22T08:50:53.184Z"},{"id":"q-5723","question":"Scenario: A real-time ad bidding platform across four regions experiences a revenue drop after a nightly model retraining; bid rejections spike in EU and APAC. Design a CAPA program to detect, contain, and prevent recurrence. Include: 1) a CAPA data model capturing evidence, logs, feature-store versions, and data residency flags; 2) a region-aware lifecycle state machine; 3) a minimal REST API to create/update CAPAs with traces; 4) region- and privacy-aware metrics (recurrence, MTTR, false positives, data egress); 5) an RCA template and a canary rollout plan with rollback options?","answer":"Proposed CAPA elements: a data model with incident_id, region, service, evidence_links, logs, traces, feature_versions, data_residency, containment_actions, root_cause, corrective_action, preventive_a","explanation":"## Why This Is Asked\nTests end-to-end CAPA thinking for a latency-sensitive, multi-region ad-bidding system with data governance.\n\n## Key Concepts\n- CAPA data models linking evidence, traces, and configs across regions\n- Region-aware lifecycle with data residency flags\n- Minimal REST API for CAPA creation/updating and linked logs\n- Privacy-aware metrics and rollback strategies\n\n## Code Example\n```javascript\n// Minimal CAPA payload sketch\nconst capa = {\n  incident_id: 'INC-2026-001',\n  region: 'EU',\n  service: 'bid-engine',\n  evidence_links: ['s3://.../log1','s3://.../trace1'],\n  feature_versions: ['v42'],\n  data_residency: 'EU',\n  root_cause: null,\n  corrective_action: null,\n  preventive_action: null\n};\n```\n\n## Follow-up Questions\n- How would you orchestrate a cross-region canary rollout with feature flags?\n- How would you integrate this CAPA with incident response tooling?","diagram":null,"difficulty":"advanced","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T13:18:10.112Z","createdAt":"2026-01-22T13:18:10.112Z"},{"id":"q-5751","question":"Scenario: In a multi-region telemetry pipeline for electric vehicles, a vendor schema drift changes field semantics, causing data corruption and sporadic duplicates in North America and Europe. Design a beginner CAPA to address this. Include: 1) a CAPA data model, 2) a lifecycle state machine, 3) a minimal REST API to create/update CAPAs with evidence, 4) region-aware metrics to prove containment, 5) an RCA template and a canary-based remediation plan before global rollout?","answer":"CAPA for schema-drift in multi-region telemetry: Data model CAPA{ id, region, source, timestamp, evidence, rootCause, correctiveAction, preventiveAction, state }. Lifecycle: detected → triaged → conta","explanation":"## Why This Is Asked\nAssesses ability to design a practical, beginner CAPA plan for data integrity across regions.\n\n## Key Concepts\n- CAPA data model fields and traceability\n- Lifecycle state machine for containment and verification\n- Evidence handling and regional metrics\n\n## Code Example\n```javascript\n// Example CAPA payload\n{\n  \"id\":\"CAPA-1003\",\n  \"region\":\"NA\",\n  \"source\":\"telemetry-ingest\",\n  \"timestamp\":\"2026-01-22T12:05:00Z\",\n  \"evidence\":[\"s3://logs/...\",\"trace://...\"],\n  \"rootCause\":\"schema-drift\",\n  \"correctiveAction\":\"update parser\",\n  \"preventiveAction\":\"schema validation on ingest\",\n  \"state\":\"detected\"\n}\n```\n\n## Follow-up Questions\n- How would you validate the canary remediation before wider rollout?\n- What logs and traces would you collect to support RCA and postmortems?","diagram":"flowchart TD\n  A[Event Arrives] --> B[Ingest Layer]\n  B --> C{Schema OK?}\n  C -- No --> D[CAPA Route]\n  C -- Yes --> E[Normal Processing]\n  D --> F[Containment & RCA]\n  F --> G[Verification]\n  G --> H[Closed]","difficulty":"beginner","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["NVIDIA","Tesla","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T15:06:14.073Z","createdAt":"2026-01-22T15:06:14.073Z"},{"id":"q-5866","question":"Scenario: In a multi-tenant data processing platform, a schema evolution and CDC misconfiguration cause cross-tenant data leakage and incorrect dashboards across regions. Design a CAPA program to detect, contain, and prevent recurrence. Include: 1) a CAPA data model capturing evidence, tenant_id, source_system, schema_version, logs; 2) a region-aware lifecycle state machine; 3) a minimal REST API to create/update CAPAs with linked traces; 4) tenant-aware metrics (recurrence, MTTR, false positives); 5) an RCA template and a canary rollout plan for remediated schemas and governance controls?","answer":"Identify tenants affected, capture evidence (logs, traces) with tenant_id, source, and schema_version; implement a staged lifecycle: Detected, Triaged, Contained, Eradicated, Verified, Closed; provide","explanation":"## Why This Is Asked\n\nTests ability to design end‑to‑end CAPA in multi-tenant, data‑driven systems with schema drift and CDC issues. Requires data model, state machine, API, metrics, RCA, rollback; ties governance to operational reality.\n\n## Key Concepts\n\n- CAPA data modeling for tenants and schema lineage\n- Region-aware lifecycle orchestration\n- Observability with traces, logs, data lineage\n- Canary governance and rollback in schema changes\n\n## Code Example\n\n```javascript\n// example pseudo-API handler shape\nPOST /caps { tenant_id, source_system, schema_version, evidence_refs }\n```\n\n## Follow-up Questions\n\n- How would you test MTTR and drift detection statistically?\n- How do you prevent future leakage via authorization controls?","diagram":"flowchart TD\n  A[Detect] --> B[Triage]\n  B --> C[Contain]\n  C --> D[Eradicate]\n  D --> E[Verify]\n  E --> F[Close]","difficulty":"advanced","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","DoorDash","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T19:46:24.935Z","createdAt":"2026-01-22T19:46:24.935Z"},{"id":"q-6335","question":"Scenario: A distributed ML inference service across three regions experiences sporadic drift in input feature distributions after a schema change in the feature store, degrading model accuracy and triggering alert floods. Design a CAPA program to detect, contain, and prevent recurrence. Include: 1) CAPA data model capturing evidence, feature store version, model version, drift metrics, logs; 2) a region-aware lifecycle state machine; 3) a minimal REST API to create/update CAPAs with linked logs/traces; 4) region/drift metrics to prove containment (recurrence rate, MTTR, false positives); 5) RCA template and canary rollback plan for feature-store/schema changes?","answer":"CAPA for ML drift after schema change in feature store across regions: a data model capturing evidence, drift per region, feature_store_version, model_version, and linked logs; a region-aware lifecycl","explanation":"## Why This Is Asked\nTests ability to design CAPA for ML/data drift with schema changes, across regions, not just generic incident handling.\n\n## Key Concepts\n- Data-modeling for evidence, versions, drift metrics\n- Region-aware lifecycle\n- Canary rollback and RCA templates\n- Traceable APIs linking logs/pipelines\n\n## Code Example\n```javascript\n// Minimal CAPA entity example\nclass CAPA { constructor(id){ this.id=id} }\n```\n\n## Follow-up Questions\n- How would you automate detection of drift without labeled ground truth?\n- How would you scale the API and store for large organizations?","diagram":"flowchart TD\n  A[Start] --> B[Detect drift]\n  B --> C[Containment]\n  C --> D[Investigate]\n  D --> E[Remediate]\n  E --> F[Validate]\n  F --> G[Closed]","difficulty":"advanced","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T19:46:32.243Z","createdAt":"2026-01-23T19:46:32.243Z"},{"id":"q-6441","question":"Scenario: A real-time fraud pipeline ingests multiple third‑party feeds. A contract/schema drift in one vendor’s feed triggers feature drift across Regions us-east-1 and eu-west-1, inflating false positives. Design a CAPA program to address this: 1) a CAPA data model capturing evidence, lineage, owners; 2) a region-aware lifecycle with gates (detect, contain, triage, diagnose, remediate, verify, rollback); 3) a minimal REST API to create/update CAPAs with logs/traces and lineage; 4) region/vendor metrics to prove containment and prevent recurrence; 5) an RCA template and a canary remediation plan (schema validation and provider failover)?","answer":"Design a CAPA for multi-party feed drift in a real-time fraud pipeline. CAPA model: id, incidentTime, vendor, feed, evidenceLinks, lineage, owners, actions, status, region. Lifecycle: detect, contain,","explanation":"## Why This Is Asked\n\nTests ability to model CAPA data, implement region-aware governance, and handle third-party feed drift with a real-world workflow, including evidence lineage, owners, and canary rollouts.\n\n## Key Concepts\n\n- CAPA data model: incidentTime, vendor, feed, evidenceLinks, lineage, owners, actions, status, region\n- Lifecycle: detect -> contain -> triage -> diagnose -> remediate -> verify -> rollback with region gates\n- APIs: minimal CRUD (POST /caps, PUT /caps/{id}) including logs/traces and lineage\n- Metrics: containment time by region/vendor, recurrence rate, false positives\n- RCA & canary: structured root-cause sections; schema validation; provider failover\n\n## Code Example\n\n```typescript\ntype CAPA = {\n  id: string;\n  incidentTime: string;\n  vendor: string;\n  feed: string;\n  evidenceLinks: string[];\n  lineage: string;\n  owners: string[];\n  actions: string[];\n  status: 'detect'|'contain'|'triage'|'diagnose'|'remediate'|'verify'|'rollback'|'closed';\n  region: string;\n}\n```\n\n```json\nPOST /caps\n{\n  \"vendor\": \"AcmeVendor\",\n  \"feed\": \"fraud-feed-v2\",\n  \"incidentTime\": \"2026-01-23T12:34:56Z\",\n  \"evidenceLinks\": [\"s3://bucket/logs/abc\", \"logs/trace-123\"],\n  \"lineage\": \"ingest -> enrichment -> features\",\n  \"owners\": [\"data-eng\", \"security\"],\n  \"actions\": [\"containment\", \"notify-vendors\"],\n  \"status\": \"detect\",\n  \"region\": \"us-east-1\"\n}\n```\n\n## Follow-up Questions\n\n- How would you validate the effectiveness of the canary remediation across regions?\n- What governance controls would you enforce for third‑party feeds to minimize drift?","diagram":null,"difficulty":"intermediate","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Meta","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T23:59:31.787Z","createdAt":"2026-01-23T23:59:31.787Z"},{"id":"q-6479","question":"In a multi-region streaming platform, a new edge-routing rule causes intermittent traffic blackouts in Regions A and B during peak load, triggering regulatory alerts due to data residency policy violations. Design a CAPA program to address this: 1) a CAPA data model capturing evidence, lineage, owners and policy tags; 2) region-aware lifecycle with gates (detect, contain, remediate, verify) and a rollback option; 3) a minimal REST API to create/update CAPAs with logs/traces and lineage; 4) region residency metrics and recurrence indicators; 5) an RCA template and a canary-based remediation plan for routing changes?","answer":"Design a CAPA program to address a faulty edge-routing rule causing regional outages during peak. Include: 1) a CAPA data model with evidence, lineage, owners and policy tags; 2) region-aware lifecycl","explanation":"## Why This Is Asked\nTests practical CAPA design for edge routing, multi-region governance, and real-time remediation, combining data lineage, policy tagging, and rollback safety.\n\n## Key Concepts\n- CAPA data model with evidence, lineage, owners, policy tags\n- region-aware lifecycle with detect, contain, remediate, verify gates\n- lightweight REST API for CAPAs with logs/traces and lineage links\n- residency-focused metrics and recurrence indicators\n- RCA template and canary rollback planning\n\n## Code Example\n```javascript\n// Minimal REST endpoint sketch (Express)\napp.post('/capa', (req, res) => {\n  // validate payload, persist CAPA with evidence and lineage\n  res.status(201).send({ id: 'capa-123' })\n})\n```\n\n## Follow-up Questions\n- How would policy tags and access controls be enforced across regions?\n- What metrics would you surface to demonstrate recurrence reduction over time?","diagram":"flowchart TD\n  Detect[Detect issue] --> Contain[Contain in region/quarantine]\n  Contain --> Remediate[Remediate with canary]\n  Remediate --> Verify[Verify via metrics/tests]\n  Verify --> Close[Close CAPA]","difficulty":"intermediate","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Netflix","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T04:16:16.810Z","createdAt":"2026-01-24T04:16:16.810Z"},{"id":"q-6530","question":"Scenario: A real-time fleet telemetry pipeline occasionally drops heartbeat messages from a subset of devices, causing the health dashboard to report stale or missing data for two regions. Design a beginner CAPA to address this. Your task: 1) a CAPA data model, 2) a simple lifecycle state machine, 3) a minimal REST API to create/update CAPAs with evidence, 4) region-aware metrics to prove containment, 5) an RCA template and a canary-based remediation plan to validate before full rollout?","answer":"CAPA data model: id, region, deviceId, eventType='heartbeat', timestamp, evidenceLinks, status, RCA, remediationPlan, owner. Lifecycle: Draft -> Investigating -> Contained -> Corrected -> Verified -> ","explanation":"## Why This Is Asked\nTests ability to model CAPA for telemetry data quality issues and demonstrate end-to-end thinking from data model to remediation.\n\n## Key Concepts\n- CAPA data model for telemetry\n- Region-aware metrics and containment\n- Lifecycle state machine\n- RCA templating and canary rollout\n- Minimal REST API design\n\n## Code Example\n```javascript\nconst express = require('express')\nconst app = express()\napp.use(express.json())\nlet capas = []\napp.post('/capa', (req,res)=>{ const c={ id: Date.now().toString(), ...req.body }; capas.push(c); res.status(201).json(c); })\n```\n\n## Follow-up Questions\n- How would you simulate a heartbeat drop scenario for testing?\n- How would you extend the model for additional event types?","diagram":"flowchart TD\n  A[CAPA] --> B[Data Model]\n  B --> C[Lifecycle]\n  C --> D[API]\n  D --> E[Metrics]\n  E --> F[RCA/Remediation]","difficulty":"beginner","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Lyft","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T07:08:29.964Z","createdAt":"2026-01-24T07:08:29.964Z"},{"id":"q-6659","question":"Scenario: A multi-region, multi-cloud telemetry ingestion pipeline experiences drift in time-based aggregations after a leap-second insertion in Region X, causing skewed anomaly detections and missed SLA targets. Design a CAPA program to detect, contain, and prevent recurrence. Include: 1) a CAPA data model capturing evidence, time sync state, ntp_sources, and artifacts; 2) a region-aware lifecycle state machine for CAPA progression; 3) a minimal REST API to create/update CAPAs with linked logs traces; 4) time-series metrics to prove containment (MTTR, recurrence rate, false positives); 5) an RCA template and a canary rollout plan for time-sync/config updates?","answer":"Detect and contain a leap-second time skew in multi-region telemetry ingestion. Build a CAPA: data model with evidence, time_drift, ntp_sources, region, device_ids, and artifacts; region-aware lifecyc","explanation":"## Why This Is Asked\nTests ability to design CAPA for time synchronization failures in a multi-region pipeline, including data models, lifecycle, API design, metrics, and RCA.\n\n## Key Concepts\n- Time synchronization, leap seconds, NTP reliability\n- Data lineage, artifacts, cross-region causality\n- Canary/rolling updates for config changes\n- Metrics: MTTR, recurrence rate, false positives\n\n## Code Example\n```javascript\nconst cap = {\n  capId: 'CAP-...',\n  evidence: [{ logId: 'L1', region: 'us-east-1', timeStamp: '2026-01-24T12:34:56Z' }],\n  timeDriftSeconds: 12,\n  ntpSources: ['ntp1.company.com','ntp2.company.com'],\n  linkedArtifacts: ['trace:abc']\n}\n```\n\n## Follow-up Questions\n- How would you measure success and ensure long-term resilience without impacting privacy? \n- What would trigger an automatic rollback vs. manual intervention?","diagram":"flowchart TD\n  A[Detect] --> B[Contain]\n  B --> C[Investigate]\n  C --> D[Fix Deployed]\n  D --> E[Verify]\n  E --> F[Close]","difficulty":"advanced","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["OpenAI","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T11:53:55.044Z","createdAt":"2026-01-24T11:53:55.045Z"},{"id":"q-6678","question":"Scenario: A multilingual text search service experiences a regional incident where a newly deployed indexing pipeline yields inconsistent search results and higher latency in Region EU and Region APAC. Design a beginner CAPA to address this. Your task: 1) define a CAPA data model capturing evidence, versioned ETLs, and privacy notes; 2) design a region-aware CAPA lifecycle state machine; 3) outline a minimal REST API to create/update CAPAs with linked evidence; 4) specify region-specific metrics to prove containment and drift (latency p95, errorRate, driftScore); 5) provide an RCA template and a staged remediation plan (canary by region, reversible rollback, and versioned reindex) plus a data-retention policy for compliance?","answer":"Define a CAPA with fields: id, region, issue, evidenceLinks, artifacts, etlVersion, indexingVersion, privacyNotes, timestamps. Lifecycle: Detected -> Contained -> Investigated -> Remediated -> Verifie","explanation":"## Why This Is Asked\nTests ability to model CAPA data, lifecycle, and evidence handling in a concrete, multi-region tooling context. Emphasizes actionable design, privacy considerations, and basic orchestration patterns suitable for entry-level roles at AI/ML-centric organizations.\n\n## Key Concepts\n- CAPA data model with provenance and privacy notes\n- Region-aware lifecycle and metrics\n- Minimal REST API for CAPA orchestration\n- Canary-based remediation and rollback planning\n\n## Code Example\n```javascript\n// pseudo REST handler sketch for creating CAPA\nfunction createCAPA(req, res) {\n  const {region, issue, evidence} = req.body;\n  // validate, persist CAPA with versioned artifacts, return id\n}\n```\n\n## Follow-up Questions\n- How would you extend the API for cross-region audit logging?\n- What would you monitor during canary to decide progression to remediation?","diagram":"flowchart TD\n  Detected --> Contained\n  Contained --> Investigated\n  Investigated --> Remediated\n  Remediated --> Verified\n  Verified --> Closed","difficulty":"beginner","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","IBM"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T13:18:07.742Z","createdAt":"2026-01-24T13:18:07.742Z"},{"id":"q-6724","question":"Scenario: A streaming data ingestion service experiences intermittent late-arriving events around DST transitions, causing out-of-order processing in two regions. Design a beginner CAPA to address this. Include: 1) a CAPA data model capturing evidence, timestamps, and data sources; 2) a region-aware lifecycle state machine for CAPA progression; 3) a minimal REST API to create/update CAPAs with linked logs/traces; 4) region-specific metrics to prove containment (late-arrival rate, max out-of-order ms, MTTR, backlog); 5) an RCA template and a canary-based remediation plan to adjust watermarking and backpressure handling?","answer":"CAPA data model with cap_id, region, timestamp, watermark_ms, window_size, evidence_refs, source_system, status; lifecycle: draft, detected, containment, remediation, verified, closed; REST API: POST ","explanation":"## Why This Is Asked\nTests ability to translate a real-world time-sensitive ingestion issue into a structured CAPA approach.\n\n## Key Concepts\n- Data model for CAPA with timestamps and evidence linkage\n- Region-aware lifecycle state machine\n- Lightweight REST API design for CAPA objects\n- Region metrics to prove containment and prevention\n- RCA template and canary remediation workflow\n\n## Code Example\n```javascript\n// Minimal CAPA type\ntype CAPA = {\n  id: string\n  region: string\n  timestamp: string\n  watermark_ms: number\n  window_size: number\n  evidence_refs: string[]\n  source_system: string\n  status: 'draft'|'detected'|'containment'|'remediation'|'verified'|'closed'\n}\n```\n\n## Follow-up Questions\n- How would you store evidences and logs efficiently for large volumes?\n- How would you validate the canary before broader rollout?","diagram":null,"difficulty":"beginner","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Lyft","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T14:42:26.367Z","createdAt":"2026-01-24T14:42:26.367Z"},{"id":"q-6814","question":"Explain how you would implement a CAPA program for a misclassified auto-remediation burst in a multi-cloud policy engine. Include: 1) a CAPA data model with evidence, lineage, and owners; 2) a region-aware lifecycle with gates (detect, contain, remediate, verify); 3) a minimal REST API to create/update CAPAs with logs/traces and policy links; 4) region/SLA metrics to prove containment and reduce false positives; 5) an RCA template and a canary rollout plan for policy changes?","answer":"Describe a CAPA approach for a misclassified auto-remediation burst in a multi-cloud policy engine: 1) CAPA data model with evidence, lineage, owners; 2) region-aware lifecycle with gates (detect, con","explanation":"## Why This Is Asked\nTests practical CAPA design for cross-region automation failures, focusing on concrete data models, governance, and safe rollouts rather than generic concepts.\n\n## Key Concepts\n- CAPA data model with evidence, lineage, owners\n- Region-aware lifecycle with gated stages\n- Lightweight REST API for CAPA CRUD with traces\n- Region/SLA metrics and recurrence tracking\n- RCA templates and canary rollout strategies\n\n```javascript\n// Example CAPA data structure\n{\n  \"id\": \"capa-20260124-1\",\n  \"title\": \"Burst misclassification\",\n  \"evidence\": { \"eventId\": \"evt-123\", \"region\": \"us-east-1\" },\n  \"owners\": [\"data-eng\", \"sre\"],\n  \"lifecycle\": [\"detect\", \"contain\", \"remediate\", \"verify\"]\n}\n```\n\n## Follow-up Questions\n- How would you test containment with cross-region synthetic bursts?\n- What dashboards would you expose to monitor false positives and containment across clouds?\n","diagram":"flowchart TD\n  A[Detection] --> B[Containment]\n  B --> C[Remediation]\n  C --> D[Verification]\n  D --> E[Closure]","difficulty":"intermediate","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Hashicorp","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T18:47:32.476Z","createdAt":"2026-01-24T18:47:32.476Z"},{"id":"q-6839","question":"Scenario: In a multi-region streaming feature store, a schema evolution on a topic causes validation failures in two regions, corrupting features. Design a CAPA program to detect, contain, and prevent recurrence. Include: 1) a CAPA data model with evidence, lineage, owners; 2) a region-aware lifecycle (detect, contain, remediate, verify) with SLAs; 3) a minimal REST API to create/update CAPAs with logs/traces and policy links; 4) region metrics for containment and recurrence; 5) an RCA template and a canary rollout plan for schema changes?","answer":"Propose a CAPA for a real-time streaming feature store where topic schema evolution causes cross-region validation failures. Build a CAPA data model with evidence, lineage, owners; implement a region-","explanation":"## Why This Is Asked\n\nTests ability to design CAPA for streaming data across regions with schema changes; evaluates data modeling, lifecycle design, observability, and rollout discipline.\n\n## Key Concepts\n\n- CAPA data model with evidence, lineage, owners\n- Region-aware lifecycle (detect, contain, remediate, verify)\n- Observability and metrics by region\n- RCA templates and canary rollout\n- Policy links and logs integration\n\n## Code Example\n\n```javascript\n// Minimal CAPA payload illustration\n{\\n  \"id\": \"capa-20260124-001\",\\n  \"title\": \"Schema evolution in streaming feature store\",\\n  \"evidence\": [\"logs\", \"schema-registry diff\"],\\n  \"lineage\": {\"source\": \"topic-A\", \"destination\": \"feature-store-B\"},\\n  \"owners\": [\"data-ingest\",\"ml-platform\"],\\n  \"region\": \"us-east-1\",\\n  \"lifecycle\": \"detect -> contain -> remediate -> verify\",\\n  \"links\": [\"policy:schema-evolution\"]\\n}\n```\n\n## Follow-up Questions\n\n- How would you instrument region-level metrics and alerting for this CAPA?\n- Which data retention or privacy controls affect the CAPA lifecycle?","diagram":null,"difficulty":"intermediate","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Netflix","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T19:33:03.842Z","createdAt":"2026-01-24T19:33:03.842Z"},{"id":"q-6903","question":"Scenario: In a global payment processing platform, a cross-region key rotation misconfiguration in Region-2 breaks decryption of audit logs, triggering alarm floods and partial outages across tenants. Design a CAPA program to detect, contain, and prevent recurrence. Include: 1) a CAPA data model with evidence, lineage, owners; 2) a region-aware lifecycle detect contain remediate verify with SLAs; 3) a minimal REST API to create/update CAPAs with logs traces and policy links; 4) region metrics for containment and recurrence; 5) an RCA template and a canary rollout plan for key rotation changes?","answer":"Design a comprehensive CAPA program for cross-region key rotation failures. Implement a structured CAPA data model with evidence collection (decryption failure logs, tenant impact metrics), service lineage mapping, and clear ownership hierarchy. Establish a region-aware lifecycle with detect-contain-remediate-verify phases, each with defined SLAs and automated escalation triggers. Develop a minimal REST API for CAPA management with integrated logging, distributed tracing, and policy enforcement. Create regional containment and recurrence metrics dashboards with real-time monitoring. Standardize RCA templates and implement canary rollout procedures for key rotation changes with automated rollback capabilities.","explanation":"## Why This Is Asked\nTests practical CAPA design under cross-region cryptography failures, emphasizing evidence collection, ownership chains, and measurable containment strategies.\n\n## Key Concepts\n- CAPA data model with evidence lineage and ownership\n- Region-aware lifecycle with gates and SLAs\n- Integrated logging, tracing, and policy enforcement in API\n- Regional containment metrics and RCA standardization\n- Canary deployment procedures for cryptographic operations\n\n## Code Example\n```javascript\n// API sketch for CAPA management (illustrative)\n```\n\n## Follow-up Questions\n- How would you validate the effectiveness of the containment metrics?\n- What automated testing would you implement for key rotation procedures?\n- How would you ensure cross-region consistency in RCA standards?","diagram":"flowchart TD\n  A[Detect Failures] --> B[Contain Region-2]\n  B --> C[Remediate Key Rotation]\n  C --> D[Verify Decryption]\n  D --> E[Prevent Recurrence]","difficulty":"intermediate","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","MongoDB","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T05:57:56.146Z","createdAt":"2026-01-24T22:28:43.844Z"},{"id":"q-7060","question":"Scenario: A multi-tenant data-ingest service experiences schema-drift in downstream processing, causing corrupted events for a subset of tenants. Design a beginner CAPA to address data integrity and containment. Include: 1) a CAPA data model capturing evidence, schemas, lineage, and tenant IDs; 2) a lifecycle state machine for CAPA progression; 3) a minimal REST API to create/update CAPAs with evidence; 4) tenant-aware metrics to prove containment and prevent recurrence; 5) an RCA template and a staged rollback/canary plan?","answer":"Design a beginner CAPA for multi-tenant ingest drift: 1) CAPA data model: id, tenant_id, evidence_refs, schema_version, affected_events, containment_status, remediation_id. 2) lifecycle: new -> triage","explanation":"## Why This Is Asked\n\nTests ability to design a controlled CAPA for data quality in a multi-tenant ingest, focusing on concrete data models and observable metrics rather than generic concepts.\n\n## Key Concepts\n- CAPA data modeling: evidence, schema_version, tenant lineage\n- Lifecycle state machines: transitions, guards, idempotency\n- Minimal API design: idempotent endpoints, evidence links\n- Tenant-aware metrics: MTTC by tenant, containment rate\n- RCA templates and canary rollback strategies\n\n## Code Example\n\n```javascript\n// Example schema for CAPA resource\nconst CAPA = { id, tenant_id, evidence_refs, schema_version, affected_events, containment_status, remediation_id }\n```\n\n## Follow-up Questions\n- How would you automate containment in a multi-tenant system with schema drift?\n- How would you prevent recurrence with schema evolution governance?","diagram":null,"difficulty":"beginner","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T07:42:43.516Z","createdAt":"2026-01-25T07:42:43.516Z"},{"id":"q-7074","question":"Scenario: A regional data pipeline exhibits data-skew after a schema change; the skew persists for hours before detection. Design a beginner CAPA to address this. Include: 1) a CAPA data model capturing evidence (events, schemas, git SHAs), telemetry, and data lineage; 2) a lifecycle state machine for CAPA progression; 3) a minimal REST API to create/update CAPAs with evidence links; 4) region-aware metrics to prove containment (skew rate, MTTR); 5) an RCA template and a staged remediation plan with canary data re-ingestion, schema checks, and rollback?","answer":"Model a CAPA for regional data-skew after a schema change. Key fields: id, region, evidenceLinks, schemaSHA, telemetry, lineage, rootCause, correctiveActions, status, createdAt. State machine: New → I","explanation":"## Why This Is Asked\nAssesses ability to design a beginner CAPA around data quality, evidence capture, and remediation, with practical artifacts.\n\n## Key Concepts\n- CAPA data model with evidence, schema version, telemetry, and data lineage\n- Lifecycle state machine for CAPA progression\n- Minimal REST API design for creating/updating CAPAs with links to evidence\n- Region-aware metrics and RCA templates\n\n## Code Example\n```javascript\n// Skeleton CAPA model and API handler\n```\n\n## Follow-up Questions\n- How would you validate the effectiveness of the canary remediation before full rollout?\n- What tests would you write to ensure data lineage remains intact after rollback?","diagram":"flowchart TD\nA(New CAPA) --> B(Investigating)\nB --> C(Containing)\nC --> D(Corrected)\nD --> E(Verifying)\nE --> F(Closed)","difficulty":"beginner","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Bloomberg"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T08:38:18.542Z","createdAt":"2026-01-25T08:38:18.543Z"},{"id":"q-7314","question":"Scenario: A critical third-party data feed used for live risk scoring experiences corrupted price data during a market stress across two regions. Design a CAPA program to detect, contain, and prevent recurrence. Include: 1) a CAPA data model capturing evidence, data lineage, feed version, and incident metadata; 2) a region-aware lifecycle state machine for CAPA progression; 3) a minimal REST API to create/update CAPAs with linked logs/traces; 4) region- and feed-level metrics to prove containment (recurrence rate, MTTR, false positives, drift); 5) an RCA template and a staged canary rollout plan for feed version updates?","answer":"Propose a CAPA data model with fields: id, region, feed_id, version, evidence_uris, log_trace_ids, t_incident, t_detected, containment_status. Implement a finite-state machine: Identified -> Contained","explanation":"## Why This Is Asked\nNew angle: third-party feed integrity across regions; captures data lineage and external fault containment.\n\n## Key Concepts\n- CAPA data model with lineage; - region-aware lifecycle; - logs/traces integration; - metrics for containment; - RCA and canary plan.\n\n## Code Example\n```javascript\n// Example CAPA data model outline\ntype CAPA = {\n  id: string; region: string; feedId: string; version: string;\n  evidenceUris: string[]; logTraceIds: string[]; t_incident: string; t_detected: string;\n  containmentStatus: 'Identified'|'Contained'|'Investigated'|'Remediated'|'Verified'|'Closed';\n  recurrenceRate?: number; mttr?: number;\n};\n```\n\n## Follow-up Questions\n- How would you simulate data-feed corruption to validate the canary rollout?\n- What tamper-evident mechanisms would you add to the evidence store?","diagram":"flowchart TD\n  A[Detect Issue] --> B[Containment]\n  B --> C[Evidence Capture]\n  C --> D[Root Cause]\n  D --> E[Remediation]\n  E --> F[Validation]\n  F --> G[Closure]","difficulty":"advanced","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Google","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T17:47:27.258Z","createdAt":"2026-01-25T17:47:27.258Z"},{"id":"q-7382","question":"Scenario: A multi-tenant SaaS platform experiences a spike in incidents in Region EU when a new feature flag is toggled. Design a beginner CAPA to address this. Include: 1) a CAPA data model, 2) a lifecycle state machine, 3) a minimal REST API to create/update CAPAs with evidence, 4) region-specific containment metrics, 5) an RCA template and a canary-based remediation plan?","answer":"CAPA data model: {id, region, tenant_id, flag_id, incident_id, evidence_url, severity, rootCause, correctiveAction, preventiveAction, status, createdAt, updatedAt}. Lifecycle: Open → Investigating → C","explanation":"## Why This Is Asked\nThis question tests practical CAPA thinking for a real multi-tenant system facing regional impact from a feature flag misconfiguration, including data modeling, lifecycle, and measurable containment.\n\n## Key Concepts\n- CAPA data model with evidence\n- Lifecycle state machine for CAPA progression\n- Region-aware metrics\n- Canary-based remediation strategy\n\n## Code Example\n\n```javascript\n// Minimal CAPA schema example\nconst CAPA_SCHEMA = {\n  id: 'string',\n  region: 'string',\n  tenant_id: 'string',\n  flag_id: 'string',\n  incident_id: 'string',\n  evidence_url: 'string',\n  severity: 'string',\n  rootCause: 'string',\n  correctiveAction: 'string',\n  preventiveAction: 'string',\n  status: 'enum(Open,Investigating,Contained,Remediated,Verified,Closed)',\n  createdAt: 'Date',\n  updatedAt: 'Date'\n}\n```\n\n## Follow-up Questions\n- How would you test the canary remediation before regional rollout?\n- What telemetry would you collect to validate containment post-deployment?","diagram":"flowchart TD\n  Open --> Investigating\n  Investigating --> Contained\n  Contained --> Remediated\n  Remediated --> Verified\n  Verified --> Closed","difficulty":"beginner","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Google","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T20:50:48.653Z","createdAt":"2026-01-25T20:50:48.653Z"},{"id":"q-7419","question":"In a real-time data platform used by Robinhood/Tesla/Instacart, a regional data leak triggers a CAPA; design a CAPA program with: 1) data model (evidence refs, lineage, owners, regions, severity); 2) region-aware lifecycle (detect, contain, remediate, verify) with SLAs; 3) a minimal REST API to create/update CAPAs with logs/traces; 4) region metrics for containment and recurrence; 5) RCA template and canary rollout plan?","answer":"Implement a comprehensive CAPA program with: 1) Data model featuring unique identifier, evidence references, data lineage mapping, stakeholder ownership, affected regions, and severity classification; 2) Region-aware lifecycle management spanning detection, containment, remediation, and verification phases with SLA targets of 4-hour maximum containment and MTTC metrics; 3) Minimal REST API for CAPA creation and updates with comprehensive logging and distributed tracing; 4) Region-specific performance metrics tracking containment duration and incident recurrence patterns; 5) Root cause analysis template with structured canary rollout strategy for controlled deployment.","explanation":"## Why This Is Asked\n\nEvaluates end-to-end corrective action program design in multi-region data platforms, testing ability to balance rapid incident response with systematic evidence capture and measurable outcomes.\n\n## Key Concepts\n\n- CAPA data model with evidence references and lineage tracking\n- Region-aware lifecycle: detect, contain, remediate, verify\n- REST API with audit trails and distributed tracing\n- Regional metrics: containment time, recurrence rates, data leakage\n- RCA methodology with canary deployment strategies\n\n## Code Example\n\n```javascript\ninterface CAPA { \n  id: string; \n","diagram":null,"difficulty":"intermediate","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","Robinhood","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T06:36:18.117Z","createdAt":"2026-01-25T22:32:22.978Z"},{"id":"q-7601","question":"Scenario: In a multi-tenant data export service used by financial clients, a bug in the audit-log redaction pipeline intermittently fails to redact PII for a subset of tenants, risking compliance regional audits. Design a CAPA program to detect, contain, and prevent recurrence. Include: 1) a CAPA data model capturing evidence, telemetry, redaction versions, and logs; 2) a region- and tenant-aware lifecycle state machine; 3) a minimal REST API to create/update CAPAs with linked logs/traces; 4) region- and tenant-aware metrics to prove containment (recurrence rate, MTTR, false negatives); 5) an RCA template and a staged canary rollout plan for redaction policy updates?","answer":"CAPA plan to address redaction bug in cross-tenant data exports: implement a CAPA data model storing CAPA id, tenantId, region, evidence (logs, redactionVersion, exportId), state, timestamps, and link","explanation":"## Why This Is Asked\nTests ability to design end‑to‑end CAPA for privacy/compliance risk in a multi‑tenant data export workflow, with cross‑region containment.\n\n## Key Concepts\n- CAPA data model with evidence, policy versions, and artifacts\n- Region/tenant-aware lifecycle and metrics\n- REST API for linked evidence; RCA templates\n- Canary rollout for policy updates\n\n## Code Example\n```typescript\ntype CAPA = {\n  id: string\n  tenantId: string\n  region: string\n  state: 'open'|'investigating'|'contained'|'rootCause'|'remedied'|'verified'|'closed'\n  createdAt: string\n  updatedAt: string\n  evidence: { exportId: string; logs: string; redactionVersion: string }[]\n  artifacts: string[]\n}\n```\n\n## Follow-up Questions\n- How would you scale the evidence store for high cardinality tenants?\n- How would you safeguard against metadata leakage in logs during RCA?","diagram":"flowchart TD\n  A[CAPA Open] --> B[Investigation]\n  B --> C[Containment]\n  C --> D[Root Cause]\n  D --> E[Remediation]\n  E --> F[Verification]\n  F --> G[Closed]","difficulty":"advanced","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Goldman Sachs","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T09:08:48.172Z","createdAt":"2026-01-26T09:08:48.172Z"},{"id":"q-7634","question":"Scenario: A multi-tenant logging service leaks tenant data due to misrouted logs across regional stores. Design a CAPA program to detect, contain, and prevent recurrence. Include: 1) a CAPA data model with evidence, tenant IDs, data types, and trace links; 2) a region-tenant-aware lifecycle state machine; 3) a minimal REST API to create/update CAPAs with linked logs; 4) metrics to prove containment (recurrence rate, MTTR, false positives) and data lineage checks; 5) an RCA template and a canary rollout plan for the remediation?","answer":"Implement a region-tenant CAPA with an evidence model including tenantId, dataType, traceId, logs; a lifecycle DETECT→CONTAIN→INVESTIGATE→FIX→VERIFY; a REST API for CAPA create/update with linked evid","explanation":"## Why This Is Asked\nNew angle on CAPA: privacy/regulatory risk and cross-tenant leakage in regional logging. Tests ability to model evidence, coordinate containment across tenants, and implement canary with tenant scoping.\n\n## Key Concepts\n- Region-tenant scoping\n- Data lineage and masking\n- Canary and rollback\n- Observability and metrics\n\n## Code Example\n```javascript\n// Pseudo REST handler sketch\nPOST /caps\n{\n  \"tenantId\": \"t123\",\n  \"evidence\": [...],\n  \"region\": \"us-west-1\"\n}\n```\n\n## Follow-up Questions\n- How would you scale CAPA data collection to 10k tenants?\n- How would you test data-leak containment offline?","diagram":null,"difficulty":"advanced","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Cloudflare","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T10:43:48.096Z","createdAt":"2026-01-26T10:43:48.096Z"},{"id":"q-7719","question":"Scenario: A cross-region data privacy breach occurs when user-profiles contain PII due to a pipeline change in event ingestion, leaking to downstream ad targeting regions. Design a CAPA program to detect, contain, and prevent recurrence. Include: 1) a CAPA data model capturing evidence, data lineage, and privacy impact; 2) a region-aware lifecycle state machine for CAPA progression; 3) a minimal REST API to create/update CAPAs with linked logs/traces; 4) region- and data-source metrics to prove containment (recurrence rate, MTTR, privacy violations); 5) an RCA template and a staged canary rollout plan for data-pipeline changes?","answer":"Implement CAPA for cross-region PII leakage after an ingestion-change. Data model captures case_id, evidence_links, data_source, privacy_impact, affected_regions, artifacts, owner, deadlines. Lifecycl","explanation":"## Why This Is Asked\nTests the ability to design CAPA for data privacy incidents at scale, including data lineage, governance, and cross-region containment.\n\n## Key Concepts\n- CAPA data model with evidence and lineage\n- Region-aware CAPA lifecycle state machine\n- Logs/traces integration (OpenTelemetry/Jaeger)\n- Metrics for containment, recurrence, and false positives\n- RCA templates and staged canary rollouts for pipeline changes\n\n## Code Example\n```javascript\n// Minimal CAPA data model sketch\nconst CAPA = {\n  id: \"string\",\n  evidenceLinks: [\"string\"],\n  dataSource: \"string\",\n  privacyImpact: \"string\",\n  regions: [\"string\"],\n  artifacts: [\"string\"],\n  owner: \"string\",\n  timelines: { detected: \"ISO8601\", deadline: \"ISO8601\" }\n}\n```\n\n## Follow-up Questions\n- How would you automate evidence collection and reduce manual effort?\n- How would you validate the RCA and ensure no recurrence across regions?","diagram":"flowchart TD\n  A[CAPA] --> B[Identified]\n  B --> C[Contained]\n  C --> D[Investigated]\n  D --> E[Remediated]\n  E --> F[Verified]\n  F --> G[Closed]","difficulty":"advanced","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","NVIDIA","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T15:02:00.801Z","createdAt":"2026-01-26T15:02:00.801Z"},{"id":"q-7775","question":"Scenario: A new client-side data-collector accidentally stamps events with customer identifiers into a shared analytics warehouse after flipping a feature flag, exposing PII in two regions. Design a CAPA program to detect, contain, and prevent recurrence. Include: 1) a CAPA data model with evidence, lineage, and owners; 2) a region-aware lifecycle (detect, contain, remediate, verify) with SLAs; 3) a minimal REST API to create/update CAPAs with logs/traces and policy links; 4) region metrics for containment and leakage; 5) an RCA template and a canary rollout plan for the collector change?","answer":"Define CAPA with: (1) data model capturing evidence hashes, lineage (collector→warehouse), owners, policy links; (2) region-aware lifecycle: detect via leak alerts, contain by pausing ingestion, remed","explanation":"## Why This Is Asked\nTests end-to-end CAPA thinking focusing on privacy leakage, multi-region containment, API design, and RCA planning, ensuring practical translation from policy to architecture.\n\n## Key Concepts\n- CAPA data model: evidence, lineage, owners, policy links\n- Region-aware lifecycle with SLAs\n- Logs/traces integration and policy-linking\n- Data leakage metrics: leakage_rate, MTTC, recurrence\n- Canary rollout and RCA templates\n\n## Code Example\n```javascript\n// Minimal CAPA object\nfunction createCapa(id, evidence, lineage, owners, policies){\n  return {id, evidence, lineage, owners, policies, createdAt: Date.now()};\n}\n```\n\n## Follow-up Questions\n- How would you measure leakage rate per region and alert on drift?\n- What privacy controls would you bake into the CAPA data model to satisfy regs like GDPR/CCPA?","diagram":null,"difficulty":"intermediate","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","MongoDB","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T17:14:45.490Z","createdAt":"2026-01-26T17:14:45.490Z"},{"id":"q-7839","question":"Scenario: A shared feature store in a multi-tenant platform begins leaking data between tenants during peak load, corrupting dashboards for Discord, Plaid and Anthropic tenants. Design a CAPA program to detect, contain, and prevent recurrence. Include: 1) a CAPA data model capturing incident_id, tenant_id, data_lineage, masking_config, evidence_logs, root_cause, remediation_actions, and status; 2) a tenant-aware lifecycle state machine; 3) a minimal REST API to create/update CAPAs with linked logs/traces; 4) tenant-scoped metrics (recurrence_rate_by_tenant, MTTR, leakage_false_positives); 5) an RCA template and a canary rollout plan for masking/config changes with rollback and audit trail?","answer":"Design a CAPA program for data leakage across tenants in a shared feature store. 1) CAPA data model: incident_id, tenant_id, data_lineage, masking_config, evidence_logs, root_cause, remediation_action","explanation":"## Why This Is Asked\nCross-tenant data leakage in shared infra is a realistic, high-risk CAPA scenario. It tests data modeling, cross-tenant governance, and safe remediation at scale.\n\n## Key Concepts\n- Multi-tenant data governance and isolation\n- Data lineage and dynamic masking\n- CAPA lifecycle with governance gates\n- Canary rollouts and rollback strategies\n- Tenant-scoped observability and RCA templates\n\n## Code Example\n```json\n{\n  \"incident_id\": \"INC-2026-001\",\n  \"tenant_id\": \"tenant-a\",\n  \"root_cause\": \"misconfigured masking policy\",\n  \"remediation_actions\": [\"update masking policy\", \"reprocess affected data\", \"audit logs\"]\n}\n```\n\n## Follow-up Questions\n- How would you test cross-tenant isolation under peak load?\n- What metrics would you alert on for rapid containment?","diagram":"flowchart TD\n  Detect[Detect leak] --> Contain[Containment action]\n  Contain --> Investigate[Investigate root cause]\n  Investigate --> Remediate[Remediate masking/configs]\n  Remediate --> Validate[Validate fix]\n  Validate --> Close[Close CAPA]","difficulty":"advanced","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Discord","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T19:46:45.281Z","createdAt":"2026-01-26T19:46:45.281Z"},{"id":"q-7863","question":"In a multi-tenant real-time recommender, a feature-store schema update shifts distributions for several tenants, degrading CTR in production. Design a CAPA program to detect, contain, and prevent recurrence. Include: 1) a CAPA data model capturing evidence, tenant_id, feature_set, model_version, drift_metrics, logs; 2) a lifecycle state machine for progression; 3) a minimal REST API to create/update CAPAs with linked traces; 4) tenant-aware metrics (recurrence rate, MTTR, drift amplification, false positives); 5) an RCA template and a canary rollback plan for feature-store changes?","answer":"Develop a CAPA for data drift in a multi-tenant feature store. Data model fields: id, tenant_id, feature_set, model_version, drift_score (KL, KS), evidence_logs, traces, status, timestamps. State mach","explanation":"## Why This Is Asked\n\nTests ability to design a CAPA system for data-drift scenarios in a multi-tenant ML/PL workflow, bridging data, observability, and risk controls.\n\n## Key Concepts\n\n- CAPA data model with evidence linkage and drift metrics for traceability\n- Lifecycle state machine with clear transitions and SLAs\n- REST API design for CAPA creation/update with linked logs/traces\n- Tenant-aware metrics to prove containment and avoid recurrence\n- RCA template and a staged canary rollback plan for feature-store changes\n\n## Code Example\n\n```javascript\n// TypeScript interfaces for CAPA data model\ninterface CAPA {\n  id: string;\n  tenant_id: string;\n  feature_set: string;\n  model_version: string;\n  drift_score: { KL: number; KS: number; Wasserstein?: number };\n  evidence_logs: string[];\n  traces: string[];\n  status: 'Detected'|'Investigating'|'Contained'|'Verified'|'Closed';\n  created_at: string;\n  updated_at: string;\n}\n```\n\n## Follow-up Questions\n\n- How would you scale drift detection for thousands of tenants without increasing false positives?\n- How would CAPA decisions be audited and rollback actions validated in production?","diagram":"flowchart TD\n  A[Detected] --> B[Investigating]\n  B --> C[Contained]\n  C --> D[Verified]\n  D --> E[Closed]","difficulty":"advanced","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Citadel","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T20:50:57.474Z","createdAt":"2026-01-26T20:50:57.474Z"},{"id":"q-8108","question":"Scenario: A security incident in a multi-tenant cloud service leads to exposure of customer tokens due to a misconfigured access policy during a live rollback. Design a CAPA program that addresses: 1) a CAPA data model capturing evidence (Audit logs, IAM changes, token anonymization), 2) a lifecycle state machine for containment, investigation, remediation, verification, 3) a minimal REST API to create/update CAPAs with cross-linking of logs/traces, 4) tenant-scoped metrics (recurrence rate, MTTR, false positives) and a canary-based remediation rollout plan, 5) an RCA template?","answer":"Propose a CAPA with a tenant-scoped data model: id, tenantId, incidentTime, evidenceRefs (logs, traces, IAM diffs), containmentActions, remediationSteps, RCA, and status. Lifecycle: Detected → Contain","explanation":"## Why This Is Asked\nTests ability to design CAPA for security incidents in multi-tenant cloud services, focusing on data-modeling, lifecycle, cross-linking evidence, tenant-scoped metrics, and structured RCA with controlled remediation rollouts.\n\n## Key Concepts\n- Tenant-scoped CAPA data model\n- Lifecycle state machine for containment and remediation\n- Linked evidence and traceability\n- Tenant-centric metrics and canary rollout strategy\n- RCA templating for post-incident learning\n\n## Code Example\n```javascript\n// Minimal CAPA schema (illustrative)\nconst CAPA = {\n  id: 'string',\n  tenantId: 'string',\n  incidentTime: 'ISODate',\n  evidenceRefs: ['logId','traceId'],\n  containmentActions: ['disable policy','revoke tokens'],\n  remediationSteps: ['patch IAM','redeploy service'],\n  rca: 'string',\n  status: 'Detected|Contained|Investigated|Remediated|Validated|Closed'\n};\n```\n\n## Follow-up Questions\n- How would you enforce tamper-evidence for evidence references?\n- How would you validate remediation effectiveness across tenants at scale?","diagram":"flowchart TD\n  A[Detected] --> B[Contained]\n  B --> C[Investigated]\n  C --> D[Remediated]\n  D --> E[Validated]\n  E --> F[Closed]","difficulty":"advanced","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Microsoft","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T10:00:13.778Z","createdAt":"2026-01-27T10:00:13.778Z"},{"id":"q-8129","question":"In a multi-tenant data platform used by Plaid, DoorDash, and Airbnb, a cross-tenant data leakage occurs after a shared data-access layer change. Design a CAPA program to detect, contain, and prevent recurrence. Include: 1) CAPA data model with tenant_id, data_class, pii_type, incident_time, evidence, artifacts, and logs; 2) a tenant-aware lifecycle state machine; 3) a minimal REST API to create/update CAPAs with linked logs/traces; 4) tenant-level metrics (recurrence per tenant, MTTR, false positives); 5) RCA template and a canary rollout plan per tenant?","answer":"CAPA data model with tenant_id, data_class, pii_type, incident_id, evidence_uris, artifacts, and remediation; lifecycle: detected -> contained -> investigated -> mitigated -> verified -> closed with p","explanation":"## Why This Is Asked\nThis prompt tests designing CAPA for cross-tenant data leakage, emphasizing isolation, evidence capture per tenant, and rollout controls.\n\n## Key Concepts\n- Multi-tenant data isolation, RBAC, data_classifications; - Evidence lifecycle; - Per-tenant metrics; - Canary-based mitigations.\n\n## Code Example\n```javascript\n// Example CAPA data shape (conceptual)\n{\n  incident_id: string,\n  tenant_id: string,\n  data_class: string,\n  pii_type: string,\n  evidence_uris: string[],\n  logs: string[],\n  remediation: string\n}\n```\n\n## Follow-up Questions\n- How would you enforce tenant isolation in the API layer?\n- How to validate canary rollout per tenant without impacting others?","diagram":"flowchart TD\n  A[Detection] --> B[Containment]\n  B --> C[Root Cause]\n  C --> D[Remediation]\n  D --> E[Closed]","difficulty":"advanced","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","DoorDash","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T10:57:35.202Z","createdAt":"2026-01-27T10:57:35.203Z"},{"id":"q-8161","question":"Scenario: A partner data feed in two regions occasionally drifts its message schema, breaking downstream transforms and triggering missed SLAs. Design a beginner CAPA program to address this. Include: 1) a CAPA data model capturing evidence from logs and schema checks, 2) a region-aware lifecycle state machine for CAPA progression, 3) a minimal REST API to create/update CAPAs with linked evidence, 4) region- and partner-aware metrics to prove containment and prevent recurrence, 5) an RCA template and a simple canary-based remediation plan, focusing on enforcing and validating schema contracts?","answer":"CAPA data model: id, incident_id, partner, region, evidence_refs, schema_version_before/after, containment, root_cause, remediation, status. Lifecycle: NEW -> DIAGNOSE -> CONTAIN -> CORRECT -> PREVENT","explanation":"## Why This Is Asked\n\nTests a beginner CAPA setup around data contract drift in a multi-region, partner-fed pipeline. Focuses on practical deliverables: a clear data model, a minimal lifecycle, a REST API shape, and measurable containment metrics.\n\n## Key Concepts\n\n- CAPA data model and evidence linkage\n- Region and partner awareness in metrics\n- Simple lifecycle for CAPA progression\n- RCA structure and canary-based remediation\n\n## Code Example\n\n```json\n{\n  \"id\": \"CAPA-0001\",\n  \"incident_id\": \"INC-1234\",\n  \"partner\": \"PartnerX\",\n  \"region\": \"us-east-1\",\n  \"evidence_refs\": [\"logs/ingest-123.json\"],\n  \"schema_version_before\": \"2.1\",\n  \"schema_version_after\": \"2.2\",\n  \"containment\": \"NOT_STARTED\",\n  \"root_cause\": \"\",\n  \"remediation\": \"\",\n  \"status\": \"NEW\"\n}\n```\n\n## Follow-up Questions\n\n- What tests would you add to verify the schema drift detector?\n- How would you choose a canary scope and rollback criteria?\n","diagram":"flowchart TD\n  NEW[New CAPA] --> DIAGNOSE[Diagnose]\n  DIAGNOSE --> CONTAIN[Containment]\n  CONTAIN --> CORRECT[Corrective Action]\n  CORRECT --> PREVENT[Prevent Recurrence]\n  PREVENT --> CLOSED[Closed]","difficulty":"beginner","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Scale Ai","Snap","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T11:51:03.868Z","createdAt":"2026-01-27T11:51:03.868Z"},{"id":"q-8198","question":"Scenario: A real-time streaming analytics platform ingests hundreds of millions of events per minute. Transient data loss occurs in peak load in Region US-EAST and EU-CENTRAL due to backpressure in the first-hop Kafka tier. Design a CAPA program to detect, contain, and prevent recurrence. Include: 1) a CAPA data model capturing evidence, event timestamps, sequence gaps, topology snapshot, 2) a region-aware lifecycle state machine for CAPA progression, 3) a minimal REST API to create/update CAPAs with linked logs/traces, 4) region- and data-lateness metrics to prove containment (recurrence rate, MTTR, data_loss_rate), 5) an RCA template and a canary rollout plan for stream topology changes?","answer":"CAPA data model: cap_id, regions, incident_type='data_loss', evidence_urls, topology_snapshot, event_timestamps, sequence_gaps, mitigations, logs_link. Lifecycle: Identified -> Contained -> Investigat","explanation":"## Why This Is Asked\nAddresses multi-region, streaming-backpressure CAPA, emphasizing data integrity and rapid containment in production.\n\n## Key Concepts\n- CAPA data model with evidence, topology, and versioned data\n- Region-aware lifecycle state machine\n- Minimal REST API with linked logs/traces\n- Measurable containment metrics and RCA templates\n- Canary rollout for topology changes\n\n## Code Example\n```javascript\n{\n  cap_id: 'cap-123',\n  regions: ['US-EAST','EU-CENTRAL'],\n  incident_type: 'data_loss',\n  evidence_urls: ['http://logs/...'],\n  topology_snapshot: '...',\n  event_timestamps: ['2026-01-01T12:00:00Z'],\n  sequence_gaps: true\n}\n```\n\n## Follow-up Questions\n- How would you ensure idempotent CAPA updates across concurrent tools?\n- What thresholds determine escalation to a full rollout vs canary termination?","diagram":"flowchart TD\n  Identified --> Contained\n  Contained --> Investigating\n  Investigating --> Diagnosed\n  Diagnosed --> Validated\n  Validated --> Closed","difficulty":"advanced","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T14:41:31.936Z","createdAt":"2026-01-27T14:41:31.936Z"},{"id":"q-8310","question":"In a multi-region event-streaming pipeline processing user activity in Regions North and South, a topology change caused sporadic out-of-order events, leading to brief misalignments in analytics dashboards. Design a beginner CAPA to detect, contain, and prevent recurrence. Tasks: 1) propose a CAPA data model capturing evidence, event timestamps, sequence numbers, and partition info; 2) define a simple lifecycle state machine for CAPA progression; 3) outline a minimal REST API to create/update CAPAs with linked logs; 4) specify region-aware metrics to prove containment; 5) provide an RCA template and a canary-based remediation plan?","answer":"Define CAPA with fields: id, region, event_id, timestamp, sequence_number, backlog_ms, evidence_url, RCA, remediation, status, created_at. Lifecycle: detected -> triaged -> containment -> investigatio","explanation":"## Why This Is Asked\nTests CAPA fundamentals under real-world data issues: region awareness, basic data modeling, and a simple remediation flow.\n\n## Key Concepts\n- CAPA data model, lifecycle, REST API, regional metrics, RCA, canary.\n- Trade-offs: latency of detection vs noise, data retention for evidence.\n\n## Code Example\n```javascript\n// Minimal Express REST endpoints\nconst express = require('express');\nconst app = express();\napp.use(express.json());\nlet store = [];\napp.post('/caps', (req, res)=>{ const cap={id:Date.now().toString(), ...req.body, status:'detected', created_at:new Date().toISOString()}; store.push(cap); res.status(201).json(cap);});\napp.put('/caps/:id', (req,res)=>{ const idx=store.findIndex(c=>c.id===req.params.id); if(idx>=0){store[idx]={...store[idx], ...req.body}; res.json(store[idx]);} else res.status(404).end();});\napp.listen(3000);\n```\n\n## Follow-up Questions\n- How would you validate and test the canary plan before broader rollout?\n- What telemetry would you capture to ensure no regression after remediation?","diagram":"flowchart TD\n A[Detect Issue] --> B[Containment]\n A --> C[Investigation]\n B --> D[Remediation]\n C --> E[Root Cause]\n D --> F[Validate]\n E --> F\n F --> G[Closed]","difficulty":"beginner","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Google","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T19:34:40.172Z","createdAt":"2026-01-27T19:34:40.175Z"},{"id":"q-841","question":"Design a CAPA workflow for a high-volume platform (Airbnb/LinkedIn scale). The system must log incidents, perform RCA, implement corrective and preventive actions, and verify outcomes before closing. Provide: 1) a CAPA data model, 2) a lifecycle state machine, 3) an API surface to create/update CAPAs, 4) metrics to prove effectiveness (recurrence rate, time-to-close)?","answer":"Propose: Incident → RCA → corrective action → preventive action → verification → closed. CAPA data: id, incident_id, root_cause, actions[], owner, status, due, evidence, metrics. State machine: Open →","explanation":"## Why This Is Asked\n\nThis question probes the candidate's ability to design a scalable CAPA workflow, integrating incident management with RCA, corrective/preventive actions, and verification. It also tests data modeling, API design, and measurable outcomes for compliance and quality.\n\n## Key Concepts\n\n- CAPA lifecycle\n- RCA techniques\n- Data modeling\n- Observability metrics\n- Compliance/audit trails\n\n## Code Example\n\n```javascript\nclass CAPA {\n  constructor(id, incidentId) {\n    this.id = id;\n    this.incidentId = incidentId;\n  }\n}\n```\n\n## Follow-up Questions\n\n- How would you test the CAPA workflow end-to-end?\n- How would you handle concurrency and race conditions in CAPA creation?\n- What privacy and retention considerations apply?","diagram":"flowchart TD\n  Incident --> RCA\n  RCA --> Action\n  Action --> Verification\n  Verification --> Closed","difficulty":"advanced","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","LinkedIn"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:25:13.492Z","createdAt":"2026-01-12T13:25:13.492Z"},{"id":"q-8434","question":"Scenario: In a multi-region CI/CD supply chain, a corrupted artifact is introduced into Region-1 and Region-3 private registries after a malicious PR, causing builds to pull the bad artifact and deploy a faulty feature with leaked credentials in logs. Design a CAPA program to detect, contain, and prevent recurrence. Include: 1) a CAPA data model with evidence, lineage, and owners; 2) a region-aware lifecycle (detect, contain, remediate, verify) with SLAs; 3) a minimal REST API to create/update CAPAs with logs/traces and policy links; 4) region metrics for containment and recurrence; 5) an RCA template and a canary rollout plan for dependency updates?","answer":"Design a comprehensive CAPA (Corrective and Preventive Action) program for multi-region supply chain incidents. The data model incorporates structured evidence collection including build logs, SBOMs, vulnerability scan results, and provenance metadata. Lineage tracking establishes complete traceability from source artifacts through build pipelines to deployment targets across regions. Clear ownership assignment ensures accountability with designated security, SRE, and development stakeholders. The region-aware lifecycle operates within strict SLAs: automated detection triggers within 15 minutes through provenance validation and anomaly scanning; immediate containment achieves registry quarantine and rollback procedures within 30 minutes; systematic remediation including artifact rebuilding and dependency updates completes within 2 hours; comprehensive verification through canary deployments and monitoring validates fixes within 1 hour.","explanation":"## Why This Is Asked\nTests ability to design enterprise-grade incident response for distributed supply chain attacks, emphasizing rapid detection, coordinated containment, and systematic prevention across multiple regions.\n\n## Key Concepts\n- CAPA data model with evidence, lineage, and ownership structures\n- Region-aware incident lifecycle with strict SLA enforcement\n- SBOM analysis, provenance validation, and registry quarantine procedures\n- Automated rollback mechanisms and canary deployment validation\n- RCA frameworks and measurable containment metrics\n\n## Code Example\n```javascript\n// Example CAPA payload (POST /caps)\n{\n  \"id\": \"CAPA-2026-01\",\n  \"evidence\": [\"build-logs\", \"SBOM\", \"vulnerability-scans\"],\n  \"lineage\": [\"artifact-A -> build-pipeline -> deployment-region1\"],\n  \"owners\": [\"security-team@example.com\", \"sre-team@example.com\"],\n  \"sla\": {\n    \"detection\": \"15m\",\n    \"containment\": \"30m\",\n    \"remediation\": \"2h\",\n    \"verification\": \"1h\"\n  }\n}\n```","diagram":"flowchart TD\n  A[Artifact] --> B[Build]\n  B --> C[Deploy]\n  D[CAPA] --> E[Containment]\n  C --> F[Remediation]\n  F --> G[Verification]","difficulty":"intermediate","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Robinhood","Tesla","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-28T05:53:23.963Z","createdAt":"2026-01-28T02:28:15.444Z"},{"id":"q-937","question":"Scenario: A post-rollout incident caused latency spikes and higher error rates for a subset of regions when a new feature flag was enabled. Design a beginner-friendly CAPA to address this. Your task: 1) propose a CAPA data model, 2) define a lifecycle state machine, 3) outline a minimal REST API to create/update CAPAs with evidence, 4) specify practical metrics to prove effectiveness, 5) provide a simple RCA template and a canary-based preventive action you would test before full rollout?","answer":"Open a CAPA for the feature-flag incident. Data model: capa_id, incident_id, flag_name, region, start_ts, end_ts, root_cause, corrective_actions, verification, status. Lifecycle: Open → Diagnosing → R","explanation":"## Why This Is Asked\n\nTests the ability to tailor CAPA for deployment-driven incidents tied to feature flags, mirroring real-world release risks.\n\n## Key Concepts\n\n- Incident-scoped CAPA data model\n- Lightweight state machine suitable for beginners\n- Minimal API design for CAPA lifecycle\n- Practical metrics to prove effectiveness\n- Canary-based verification strategy\n\n## Code Example\n\n```javascript\n// Example payload for creating a CAPA (JSON)\n{\n  \"capa_id\": \"CAPA-123\",\n  \"incident_id\": \"INC-456\",\n  \"flag_name\": \"new_home_feed\",\n  \"region\": \"eu-west\",\n  \"start_ts\": 1700000000,\n  \"end_ts\": 1700003600,\n  \"root_cause\": \"flag evaluation path added latency\",\n  \"corrective_actions\": [\"roll back flag\", \"optimize evaluation\"],\n  \"verification\": {\"canary_pass\": true, \"throughput_stable\": true},\n  \"status\": \"Open\"\n}\n```\n\n## Follow-up Questions\n\n- What fields would you deem optional to keep the CAPA lightweight, and why?\n- How would you extend the data model to include evidence provenance (logs/traces) without bloating the schema?\n","diagram":"flowchart TD\n  Open[Open] --> Diagnosing[Diagnosing]\n  Diagnosing --> RCAReady[RCA Ready]\n  RCAReady --> Actioned[Actioned]\n  Actioned --> Verified[Verified]\n  Verified --> Closed[Closed]","difficulty":"beginner","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T16:26:25.479Z","createdAt":"2026-01-12T16:26:25.479Z"},{"id":"q-965","question":"Scenario: A multilingual moderation model update causes spikes in unsafe content in two locales. Design a beginner CAPA plan focusing on locale-scoped evidence, drift checks, and a safe rollback with feature flags. Include: 1) a CAPA data model, 2) a lifecycle machine, 3) a minimal REST API to capture CAPAs with evidence, 4) locale-specific success metrics, 5) an RCA template and a locale-specific canary plan for preview before global rollout?","answer":"Data model: CAPA with locale, incident_id, evidence_refs, corrective_actions, preventive_actions, status, timestamps; Incident stores description, locale, severity; Lifecycle: detected → triaged → inv","explanation":"## Why This Is Asked\nThis question probes minimal-CAPA design for localization-specific ML incidents and safe rollout controls.\n\n## Key Concepts\n- Locale-scoped CAPA data\n- Drift-detection integration\n- Canary-based rollback\n- Lightweight API design\n\n## Code Example\n```javascript\ntype CAPA = { id: string; locale: string; incident_id: string; evidence_refs: string[]; corrective_actions: string[]; preventive_actions: string[]; status: string; created_at: string; updated_at: string; };\n```\n\n## Follow-up Questions\n- How would you test drift checks locally before tagging a canary?\n- What minimal schema changes if a locale adds a new language?","diagram":"flowchart TD\n  Detect[Detected] --> Triage[Triage]\n  Triage --> Investigate[Investigate]\n  Investigate --> Action[Implement Action]\n  Action --> Verify[Verify]\n  Verify --> Close[Closed]","difficulty":"beginner","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Hugging Face"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T17:26:38.660Z","createdAt":"2026-01-12T17:26:38.660Z"},{"id":"q-986","question":"Scenario: After a schema evolution in the event ingestion pipeline, latency spikes and incorrect bids appear in two regions. Design a CAPA program with: 1) a CAPA data model capturing evidence and artifacts, 2) a lifecycle state machine for CAPA progression, 3) a minimal REST API to create/update CAPAs with linked logs/traces, 4) region-aware metrics to prove effectiveness (recurrence rate, mean time to containment, false-positive rate), 5) an RCA template and a canary rollout plan to validate before global deployment?","answer":"CAPA data model: id, title, incidentId, region, evidenceLinks[], rootCause, correctiveActions[], preventiveActions[], status, created, updated. Lifecycle: DETECTED → ANALYZING → APPROVED → EXECUTE → V","explanation":"## Why This Is Asked\nTests end-to-end CAPA design for a real ingestion pipeline with regional scope, evidence traceability, and auditable outcomes.\n\n## Key Concepts\n- CAPA data model and evidence linkage\n- Lifecycle state machine with SLAs\n- API design for CAPA lifecycle management\n- Region-aware metrics and roll-back safety\n\n## Code Example\n```javascript\n// Example CAPA object skeleton\nconst capa = {\n  id: 'capa-987',\n  title: 'Schema rollback for ingestion',\n  incidentId: 'inc-555',\n  region: 'us-west-2',\n  evidenceLinks: ['logs/trace-1', 'trace-2'],\n  rootCause: 'schema mismatch',\n  correctiveActions: ['revert schema', 'replay events'],\n  preventiveActions: ['add schema checks'],\n  status: 'DETECTED',\n  created: '2026-01-12T12:00:00Z',\n  updated: '2026-01-12T12:00:00Z'\n};\n```\n\n## Follow-up Questions\n- How would you test the CAPA lifecycle transitions in CI/CD?\n- Which telemetry would you instrument to ensure accurate region-specific metrics?","diagram":"flowchart TD\n  A[Detect] --> B[Analyze]\n  B --> C[Plan]\n  C --> D[Execute]\n  D --> E[Verify]\n  E --> F[Close]","difficulty":"intermediate","tags":["capa"],"channel":"capa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Oracle","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T18:31:51.916Z","createdAt":"2026-01-12T18:31:51.916Z"}],"subChannels":["general"],"companies":["Adobe","Airbnb","Amazon","Anthropic","Apple","Bloomberg","Citadel","Cloudflare","Coinbase","Databricks","Discord","DoorDash","Goldman Sachs","Google","Hashicorp","Hugging Face","IBM","Instacart","LinkedIn","Lyft","Meta","Microsoft","MongoDB","NVIDIA","Netflix","OpenAI","Oracle","PayPal","Plaid","Robinhood","Salesforce","Scale Ai","Slack","Snap","Snowflake","Square","Stripe","Tesla","Twitter","Two Sigma","Uber","Zoom"],"stats":{"total":92,"beginner":23,"intermediate":29,"advanced":40,"newThisWeek":36}}