{"questions":[{"id":"q-196","question":"How would you implement a rate-limited async HTTP client using aiohttp and asyncio.Semaphore to handle 1000 requests while respecting API limits?","answer":"Use asyncio.Semaphore(50) for concurrency control and aiohttp.ClientSession with time-based rate limiting between requests.","explanation":"## Why Asked\nTests async programming and API rate limiting skills in production scenarios.\n\n## Key Concepts\nAsyncIO, Semaphore, Rate Limiting, HTTP Client Design\n\n## Code Example\n```\nimport aiohttp\nimport asyncio\n\nasync def rate_limited_client(urls):\n    semaphore = asyncio.Semaphore(50)\n    async with aiohttp.ClientSession() as session:\n        tasks = [fetch_url(session, url, semaphore) for url in urls]\n        return await asyncio.gather(*tasks)\n\nasync def fetch_url(session, url, semaphore):\n    async with semaphore:\n        await asyncio.sleep(0.1)  # Rate limit\n        async with session.get(url) as response:\n            return await response.text()\n```","diagram":"flowchart TD\n  A[Start] --> B[Create Semaphore]\n  B --> C[Create ClientSession]\n  C --> D[Process URLs Concurrently]\n  D --> E[Apply Rate Limits]\n  E --> F[Return Results]\n  F --> G[End]","difficulty":"intermediate","tags":["asyncio","aiohttp","concurrency"],"channel":"python","subChannel":"async","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":"https://www.youtube.com/watch?v=Qb9s3UiMSTA"},"companies":["Amazon","Google","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":["asyncio","semaphore","aiohttp","concurrency","rate limiting","async"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2026-01-08T11:27:41.329Z","createdAt":"2025-12-26 12:51:06"},{"id":"q-224","question":"How would you implement a thread-safe singleton in Python with lazy initialization, proper type hints, and discuss the trade-offs between metaclass, decorator, and module-level approaches for a production system?","answer":"Implement using metaclass with double-checked locking and TypeVar for generics: metaclass provides compile-time enforcement, decorator offers simplicity, while module-level singletons are most Pythonic. Use threading.Lock for thread safety and generic typing support.","explanation":"## Context\n\nThis question assesses understanding of design patterns, concurrency, and Python-specific implementation trade-offs. Senior developers should compare multiple singleton approaches and justify their choice.\n\n## Code Examples\n\n### Metaclass Approach\n\n```python\nfrom typing import TypeVar, Type, Optional\nimport threading\n\nT = TypeVar('T')\n\nclass SingletonMeta(type):\n    _instances: dict[Type, object] = {}\n    _lock: threading.Lock = threading.Lock()\n    \n    def __call__(cls: Type[T], *args, **kwargs) -> T:\n        if cls not in cls._instances:\n            with cls._lock:\n                if cls not in cls._instances:\n                    instance = super().__call__(*args, **kwargs)\n                    cls._instances[cls] = instance\n        return cls._instances[cls]\n```\n\n### Decorator Approach\n\n```python\ndef singleton(cls):\n    instances = {}\n    lock = threading.Lock()\n    \n    def wrapper(*args, **kwargs):\n        if cls not in instances:\n            with lock:\n                if cls not in instances:\n                    instances[cls] = cls(*args, **kwargs)\n        return instances[cls]\n    \n    return wrapper\n```\n\n### Module-Level Approach\n\n```python\n# singleton_module.py\nclass _DatabaseConnection:\n    def __init__(self):\n        self.connection = None\n    \n    def connect(self):\n        if not self.connection:\n            self.connection = create_db_connection()\n        return self.connection\n\n# Single instance at module import time\ndatabase = _DatabaseConnection()\n```\n\n## Trade-offs Analysis\n\n**Metaclass Approach:**\n- Pros: Compile-time enforcement, clean class definition, full type hint support\n- Cons: More complex, metaclass magic can be confusing for new developers\n- Best for: Production systems requiring strict singleton enforcement\n\n**Decorator Approach:**\n- Pros: Simple to apply, non-intrusive, easy to understand\n- Cons: Runtime enforcement only, can be bypassed with direct class instantiation\n- Best for: Quick implementations and prototyping\n\n**Module-Level Approach:**\n- Pros: Most Pythonic, guaranteed thread safety by import system, zero overhead\n- Cons: Less flexible, eager initialization, harder to test\n- Best for: Simple cases where lazy initialization isn't required\n\n## Production Recommendation\n\nFor production systems, use the metaclass approach with double-checked locking. It provides the best balance of thread safety, type hint support, and enforcement while maintaining clean API design.","diagram":"graph TD\n    A[Thread 1 Request] --> B{Instance Exists?}\n    C[Thread 2 Request] --> B\n    B -->|No| D[Acquire Lock]\n    B -->|Yes| K[Return Instance]\n    D --> E{Double Check}\n    E -->|No| F[Create Instance]\n    E -->|Yes| G[Release Lock]\n    F --> H[Initialize]\n    H --> I[Store in Class Dict]\n    I --> G\n    G --> K\n    J[Thread N Request] --> B","difficulty":"advanced","tags":["pep8","typing","testing"],"channel":"python","subChannel":"best-practices","sourceUrl":null,"videos":null,"companies":["Amazon","Google","Meta","Microsoft","Netflix"],"eli5":"Imagine you have a special toy that only one kid can play with at a time. When you want to play with it, you check if anyone else is using it first. If it's free, you grab it and tell everyone else 'I'm using this toy now!' So no two kids can accidentally both think they're the special toy owner. This is like making sure only one copy of something exists in your computer program. You can have different rules about how to share toys: one way is having a playground boss who decides (metaclass), another is putting a special sticker on the toy box (decorator), or just having one toy box for the whole school (module-level). Each way works, but some are easier to understand than others!","relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-30T06:40:35.857Z","createdAt":"2025-12-26 12:51:07"},{"id":"q-178","question":"In Python, when do `is` and `==` return different results, and why does this happen with object identity vs equality?","answer":"`is` checks memory identity (same object), `==` checks value equality. Different for distinct objects with same values: `[1,2] is [1,2]` is False but `[1,2] == [1,2]` is True.","explanation":"## Concept Overview\nPython distinguishes between object identity and value equality. `is` compares memory addresses using `id()`, while `==` compares values using `__eq__` method.\n\n## Implementation\n```python\n# Identity vs Equality\na = [1, 2, 3]\nb = [1, 2, 3]\nc = a\n\nprint(a == b)  # True - same values\nprint(a is b)  # False - different objects\nprint(a is c)  # True - same object\n\n# Integer optimization (small integers)\nx = 256\ny = 256\nprint(x is y)  # True - interned\n\nx = 257\ny = 257\nprint(x is y)  # False - different objects\n```\n\n## Trade-offs\n- `is`: Faster, checks if exactly same object\n- `==`: Slower, checks if values are equivalent\n- Use `is` for singletons (None, True, False)\n- Use `==` for value comparison\n\n## Common Pitfalls\n- Assuming `is` works for value comparison\n- Not understanding integer/string interning\n- Using `is` with mutable objects incorrectly\n- Forgetting that `==` can be overridden by custom classes","diagram":"graph TD\n    A[Object A] -->|id: 0x1234| C[Memory Location 0x1234]\n    B[Object B] -->|id: 0x5678| D[Memory Location 0x5678]\n    E[Object C] -->|id: 0x1234| C\n    \n    F[Value: [1,2,3]] --> G[Content Comparison]\n    H[Value: [1,2,3]] --> G\n    I[Value: [1,2,3]] --> G\n    \n    J[is operator] --> K[Compare memory addresses]\n    L[== operator] --> M[Compare values via __eq__]\n    \n    style C fill:#e1f5fe\n    style D fill:#e1f5fe\n    style G fill:#f3e5f5","difficulty":"intermediate","tags":["python","basics"],"channel":"python","subChannel":"fundamentals","sourceUrl":null,"videos":{"shortVideo":"https://www.youtube.com/watch?v=mO_dS3rXDIs","longVideo":"https://www.youtube.com/watch?v=CZ8bZPqtwU0"},"companies":["Amazon","Google","Meta","Microsoft","Uber"],"eli5":"Imagine you have two identical toy cars. They look exactly the same and can do the same tricks - that's like using == to check if they're equal. But are they the SAME exact toy car? No! They're two different cars that just happen to look alike. That's like using 'is' to check if they're the same object. In Python, when you make two lists with the same numbers, they're like those two toy cars - they look identical (== says True) but they're actually two separate objects in the computer's memory (is says False). Only when you point to the very same object will both 'is' and '==' say True!","relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-25T14:57:56.251Z","createdAt":"2025-12-26 12:51:06"},{"id":"q-1085","question":"You’re building a real-time log processor in Python for a messaging app. You receive a stream of JSON lines like {\"user_id\": 123, \"event\": \"message_sent\", \"ts\": 1610000000}. Emit only the first event per (user_id, event) within a 60-second sliding window. Design a memory-bounded, generator-based pipeline that reads from an iterable of strings and yields deduplicated dicts in order. Include edge cases and memory considerations?","answer":"Implement a generator-based pipeline that reads lines, parses JSON, and yields the first dict for each (user_id, event) in a 60-second sliding window. Maintain a bounded TTL cache (OrderedDict) key->t","explanation":"## Why This Is Asked\n\nTests ability to design streaming pipelines with memory constraints and dedup logic without external stores.\n\n## Key Concepts\n\n- Generators and streaming data\n- Sliding window deduplication\n- TTL-based cache with eviction\n- Robust JSON parsing and field validation\n\n## Code Example\n\n```python\ndef dedup_stream(lines, window_seconds=60):\n    import json\n    from collections import deque, OrderedDict\n    cache = OrderedDict()\n    for line in lines:\n        obj = json.loads(line)\n        user = obj.get(\"user_id\")\n        evt = obj.get(\"event\")\n        ts = obj.get(\"ts\")\n        if user is None or evt is None or ts is None:\n            continue\n        key = (user, evt)\n        # evict stale\n        for k in list(cache):\n            if ts - cache[k] > window_seconds:\n                del cache[k]\n        if key in cache:\n            continue\n        cache[key] = ts\n        yield obj\n```\n\n## Follow-up Questions\n\n- How would you adapt for out-of-order arrivals? \n- How to scale with multiple processes?","diagram":"flowchart TD\n  A[Input Stream] --> B[Parse JSON]\n  B --> C[Key by (user_id, event)]\n  C --> D[TTL Cache Lookup]\n  D --> E{Duplicate in window?}\n  E -->|No| F[Emit] --> G[Update Cache]\n  E -->|Yes| H[Drop]","difficulty":"intermediate","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T22:19:30.480Z","createdAt":"2026-01-12T22:19:30.480Z"},{"id":"q-1149","question":"In a streaming data etl, you're given a JSONL stream where each line is a JSON object with 'host' and 'value'. Implement a memory-efficient Python generator top_n_hosts(n, lines) that yields the top N hosts by total value after consuming the stream, without keeping the entire per-host totals in memory. Use a min-heap to maintain current top-N and skip invalid lines?","answer":"Process the JSONL stream line by line; validate host and numeric value; accumulate per-host totals in a dict; maintain a fixed-size min-heap of (total, host) for the current top-N; when a host total i","explanation":"## Why This Is Asked\n\nTests ability to craft streaming pipelines and manage memory with data structure choices.\n\n## Key Concepts\n\n- Generators and lazy evaluation\n- JSONL parsing with error handling\n- Per-key aggregation with a fixed-size min-heap (top-N)\n- Trade-offs: O(H) memory vs O(N log N) heap maintenance\n\n## Code Example\n\n```python\nimport json, heapq\n\ndef top_n_hosts(n, lines):\n    totals = {}\n    heap = []  # min-heap of (total, host)\n    seen = set()\n    for line in lines:\n        try:\n            obj = json.loads(line)\n            host = obj.get('host')\n            val = float(obj.get('value', 0))\n            if host is None:\n                continue\n        except Exception:\n            continue\n        totals[host] = totals.get(host, 0.0) + val\n        heapq.heappush(heap, (totals[host], host))\n        if len(heap) > n:\n            heapq.heappop(heap)\n        seen.add(host)\n    for total, host in sorted(heap, reverse=True):\n        yield host, total\n```\n\n## Follow-up Questions\n\n- How would you modify to handle streaming resets or windowed top-N?","diagram":null,"difficulty":"intermediate","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T01:34:23.375Z","createdAt":"2026-01-13T01:34:23.375Z"},{"id":"q-1250","question":"Design an asynchronous NDJSON ingestion pipeline in Python that reads an async iterable of lines, validates each with a Pydantic model, and routes records to per-tenant partitions based on the 'tenant_id'. Implement bounded queues per partition with a total memory cap, guarantee per-tenant in-order processing, and a slower downstream sink that creates backpressure. Provide a test plan with skewed partitions and slow sinks?","answer":"To implement, create an async function ingest(source, writer_factory) that builds a dict of per-tenant queues and a small worker pool per tenant. Each line is parsed as JSON, validated with a Pydantic","explanation":"## Why This Is Asked\nTests ability to design partitioned, back-pressured pipelines in Python with asyncio, ensuring per-tenant ordering and bounded memory, a realistic scalability concern at large-scale companies like Snowflake/Uber.\n\n## Key Concepts\n- Async data ingestion\n- Bounded queues and backpressure\n- Per-tenant partitioning and in-order semantics\n\n## Code Example\n```python\n# skeleton illustrating structure\nfrom typing import AsyncIterable, Dict\nfrom asyncio import Queue, TaskGroup\nfrom pydantic import BaseModel\nimport json\nimport asyncio\n\nclass Record(BaseModel):\n    tenant_id: str\n    payload: dict\n\nasync def ingest(source: AsyncIterable[str], writer_factory):\n    queues: Dict[str, Queue] = {}\n    async def worker(tid: str, q: Queue):\n        sink = writer_factory(tid)\n        while True:\n            item = await q.get()\n            if item is None:\n                break\n            await sink.write(item)\n    async with TaskGroup() as g:\n        async for line in source:\n            rec = Record(**json.loads(line))\n            q = queues.setdefault(rec.tenant_id, Queue(maxsize=128))\n            await q.put(rec)\n        for q in queues.values():\n            await q.put(None)\n        # workers auto-join on exit\n```\n\n## Follow-up Questions\n- How would you handle dynamic partition keys that can exceed memory?\n- How would you validate end-to-end ordering when a partition has backpressure and failure?","diagram":"flowchart TD\n  S[NDJSON Source] --> M[Parse & Validate]\n  M --> P{{Partition by tenant_id}}\n  P --> Q[Per-tenant Queues]\n  Q --> W[Per-tenant Workers]\n  W --> D[Downstream Sink]","difficulty":"advanced","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Snowflake","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T06:43:51.710Z","createdAt":"2026-01-13T06:43:51.710Z"},{"id":"q-1275","question":"Design an advanced async Python pipeline: NDJSON lines arrive via a TCP socket, each is validated with a Pydantic model, then a deduplication stage uses a sliding-window Bloom filter to emit each event at most once in a 24-hour window. The pipeline must stay memory-bounded, support backpressure to the producer, and allow the downstream sink to pace itself. Provide a concrete test plan with clock skew and bursty traffic?","answer":"Approach: implement an async NDJSON ingest with a bounded asyncio.Queue, Pydantic validation, and a dedupe stage using a sliding window Bloom filter implemented with buckets. The Bloom filter stores f","explanation":"## Why This Is Asked\nTests ability to design streaming pipelines with memory constraints, asynchronous backpressure, and practical deduplication.\n\n## Key Concepts\n- Async ingestion with bounded queues\n- Pydantic validation of streaming data\n- Sliding-window Bloom filter for deduplication\n- Backpressure and pacing of producers/consumers\n\n## Code Example\n```python\n#_placeholder\n```\n\n## Follow-up Questions\n- How would you calibrate the Bloom filter's false positive rate for varying traffic? \n- How would you extend to multi-node deduplication without shared state?","diagram":null,"difficulty":"advanced","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Hugging Face","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T07:39:46.313Z","createdAt":"2026-01-13T07:39:46.313Z"},{"id":"q-1285","question":"Given a list of dictionaries representing users, implement normalize_users(users) that returns a list of User dataclass instances with fields id:int, name:str, signup_ts:datetime, is_active:bool. Coerce id from str to int, parse signup_ts ISO8601 strings, interpret is_active from common truthy values, and fill defaults for missing fields. If any record is invalid, raise ValueError with the indices of invalid records?","answer":"Implement a User dataclass and normalize_users that maps dicts to User, coercing id to int, parsing signup_ts via datetime.fromisoformat, and is_active from common truthy values; default missing field","explanation":"## Why This Is Asked\nThis question tests data normalization, type coercion, and error reporting using Python basics.\n\n## Key Concepts\n- Dataclasses for lightweight models\n- Type hints and basic validation\n- ISO 8601 parsing with datetime\n- Error aggregation for batch records\n\n## Code Example\n```python\nfrom dataclasses import dataclass\nfrom datetime import datetime\nfrom typing import List, Dict, Any\n\n@dataclass\nclass User:\n    id: int\n    name: str\n    signup_ts: datetime\n    is_active: bool\n\ndef _to_int(v: Any) -> int:\n    if isinstance(v, int):\n        return v\n    return int(v)\n\ndef _parse_ts(ts: Any) -> datetime:\n    if isinstance(ts, datetime):\n        return ts\n    return datetime.fromisoformat(ts)\n\ndef _to_bool(v: Any) -> bool:\n    if isinstance(v, bool):\n        return v\n    if isinstance(v, str):\n        return v.strip().lower() in {\"true\", \"1\", \"yes\", \"y\"}\n    return bool(v)\n\n\ndef normalize_users(users: List[Dict[str, Any]]) -> List[User]:\n    out: List[User] = []\n    bad: List[int] = []\n    for i, rec in enumerate(users):\n        try:\n            u = User(\n                id=_to_int(rec.get(\"id\")),\n                name=rec.get(\"name\", \"\"),\n                signup_ts=_parse_ts(rec.get(\"signup_ts\")),\n                is_active=_to_bool(rec.get(\"is_active\", False)),\n            )\n            out.append(u)\n        except Exception:\n            bad.append(i)\n    if bad:\n        raise ValueError(f\"Invalid records at indices: {bad}\")\n    return out\n```\n\n## Follow-up Questions\n- How would you extend to nested user data?\n- How would you validate and report multiple fields per record?","diagram":"flowchart TD\n  A[Start] --> B[Iterate records]\n  B --> C[Coerce fields]\n  C --> D{All valid?}\n  D -->|Yes| E[Emit User]\n  D -->|No| F[Collect index]\n  F --> G[Raise error on end]","difficulty":"beginner","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","MongoDB","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T08:31:56.123Z","createdAt":"2026-01-13T08:31:56.123Z"},{"id":"q-1476","question":"Design an asynchronous Python engine to join two JSON event streams by id within a 5-second window. Streams A and B are async iterables yielding {'id': str, 'ts': int, 'payload': Any}. Emit matched pairs to a sink when both sides have an event with same id within the 5s window. Route late events past the lateness bound to a 'late' sink. Enforce a global memory bound for buffered events and propose a test plan with out-of-order arrivals?","answer":"Buffer events per id from each stream. On arrival of an A event, check B-buffer for the same id; if exists and abs(tsA - tsB) <= 5000, emit the joined pair and drop both. Otherwise enqueue in A-buffer","explanation":"## Why This Is Asked\nTests async coordination, windowed joins, and backpressure with late data handling in Python. It targets real-world streaming problems and metrics-conscious memory management.\n\n## Key Concepts\n- Async streaming and per-id buffering\n- Time-based windowing (5s tumbling window)\n- Late data routing and backpressure\n- Global memory bounds and eviction strategy\n\n## Code Example\n```python\n# Implementation sketch (high-level)\nfrom typing import AsyncIterable, Dict, Deque\nimport asyncio\n\nclass WindowJoin:\n    def __init__(self, window_ms: int, mem_cap: int):\n        self.window = window_ms\n        self.mem_cap = mem_cap\n        self.buf_a: Dict[str, Deque[dict]] = {}\n        self.buf_b: Dict[str, Deque[dict]] = {}\n        self.watermark = 0\n\n    async def ingest_a(self, item: dict, sink):\n        id_ = item['id']\n        ts = item['ts']\n        self._expire(ts)\n        self.buf_a.setdefault(id_, deque()).append(item)\n        self._try_emit(id_, sink)\n\n    async def ingest_b(self, item: dict, sink, late_sink):\n        id_ = item['id']\n        ts = item['ts']\n        self._expire(ts)\n        self.buf_b.setdefault(id_, deque()).append(item)\n        if id_ in self.buf_a:\n            self._emit_matches(id_, sink, late_sink)\n\n    def _expire(self, ts: int):\n        self.watermark = max(self.watermark, ts)\n        # eviction logic based on window and mem_cap would go here\n\n    def _try_emit(self, id_: str, sink):\n        # attempt to pair any ready events for id_\n        a_list = self.buf_a.get(id_, [])\n        b_list = self.buf_b.get(id_, [])\n        while a_list and b_list:\n            a = a_list[0]\n            b = b_list[0]\n            if abs(a['ts'] - b['ts']) <= self.window:\n                sink.emit({'id': id_, 'a': a, 'b': b})\n                a_list.popleft(); b_list.popleft()\n            else:\n                break\n\n    def _emit_matches(self, id_: str, sink, late_sink):\n        self._try_emit(id_, sink)\n        # any remaining items considered late by window semantics could go to late_sink\n        # (pseudo-logic for brevity)\n```\n","diagram":"flowchart TD\n  A[Stream A] --> B[Buffer A]\n  C[Stream B] --> D[Buffer B]\n  E[Join Engine] --> F[Sink]\n  E --> G[Late Sink]","difficulty":"advanced","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Lyft","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T18:50:10.589Z","createdAt":"2026-01-13T18:50:10.589Z"},{"id":"q-1639","question":"Write a Python function process_events(file_path) that reads a newline-delimited JSON (JSONL) log file of Stripe-like events. Each line is a JSON object with 'type' (str) and 'data' (dict with 'id' key). The function should return a dict mapping event 'type' to count, skipping lines with missing keys or invalid JSON, and writing errors to a separate errors.log. Make it memory-efficient using a streaming approach?","answer":"Propose a streaming, memory-safe approach: open the JSONL file, iterate lines, parse with json.loads, verify 'type' is str and 'data' is dict containing 'id'; increment a Counter keyed by type; skip i","explanation":"## Why This Is Asked\nReading streaming logs is common in production pipelines (e.g., Stripe webhook events). This task tests robust, memory-friendly parsing, simple validation, and error handling under real-world I/O.\n\n## Key Concepts\n- Memory-efficient line-by-line processing\n- JSONL parsing and validation\n- Defensive checks and error logging\n- Simple aggregation with collections.Counter\n\n## Code Example\n```javascript\ndef process_events(path):\n    import json, logging\n    from collections import Counter\n    counts = Counter()\n    with open(path, 'r', encoding='utf-8') as f:\n        for line in f:\n            try:\n                obj = json.loads(line)\n            except json.JSONDecodeError:\n                logging.warning(\"invalid line\")\n                continue\n            t = obj.get('type')\n            data = obj.get('data')\n            if not isinstance(t, str) or not isinstance(data, dict) or 'id' not in data:\n                logging.warning(\"missing fields or wrong types\")\n                continue\n            counts[t] += 1\n    return dict(counts)\n```\n\n## Follow-up Questions\n- How would you extend to handle out-of-order events or deduplicate by id?\n- How would you add unit tests for valid and invalid lines?\n","diagram":"flowchart TD\n  Start([Start]) --> Read[Read JSONL line-by-line]\n  Read --> Validate{Valid?}\n  Validate -- Yes --> Update[Update counts]\n  Validate -- No --> ErrorLog[Log error]\n  Update --> End([Return counts])\n  ErrorLog --> End","difficulty":"beginner","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T04:24:46.136Z","createdAt":"2026-01-14T04:24:46.136Z"},{"id":"q-1675","question":"Design a memory-bounded streaming processor in Python for a JSONL event stream with fields: timestamp, service, event_type, payload. Group by (service, event_type), maintain an in-order per-key queue with bounded capacity, and emit a 60-second rolling histogram of payload sizes per group to a downstream sink. Ensure backpressure via per-key queues and a global memory cap, and provide a test plan with skewed keys and slow sinks?","answer":"Use asyncio with per-key bounded queues and a global memory budget. Each key = (service, event_type) has a fixed-size asyncio.Queue and a 60-second circular histogram of payload sizes. On arrival, pus","explanation":"## Why This Is Asked\nEfficient streaming with per-key backpressure and bounded memory.\n\n## Key Concepts\n- asyncio, per-key bounded queues\n- sliding window histograms\n- memory budgeting and backpressure\n- test plan with skewed keys\n\n## Code Example\n```python\n# sketch: per-key histogram and bounded queues\nfrom collections import deque\nimport asyncio\n\nclass WindowHistogram:\n    def __init__(self, window=60):\n        self.bins = deque([0]*window, maxlen=window)\n        self._last_tick = None\n\n    def add(self, size, timestamp):\n        # simplistic implementation; real version rotates bins per second\n        self.bins[-1] += size\n```","diagram":"flowchart TD\n  S[Source] --> P[Parse JSONL]\n  P --> G{Group by (service, event_type)}\n  G --> Q[Per-key queues]\n  Q --> H[60s histogram]\n  H --> E[Emit summaries]\n  E --> D[Downstream sink]","difficulty":"advanced","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Amazon","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T06:47:26.037Z","createdAt":"2026-01-14T06:47:26.037Z"},{"id":"q-1783","question":"Implement a memory-efficient Python function top_n_words(filepath, n) that streams a text file line by line to find the n most frequent words. Normalize case, strip punctuation, ignore empty tokens, and return a list of the top n words sorted by frequency. Ensure it never loads the whole file into memory?","answer":"Use a streaming approach: read each line, lowercase it, remove punctuation, split into words, accumulate counts in a dict, then compute the top n by frequency with a heap (nlargest). This keeps memory","explanation":"## Why This Is Asked\nStreaming text processing is common in data pipelines; this tests memory awareness and basic text normalization.\n\n## Key Concepts\n- Streaming I/O to avoid loading large files\n- Simple tokenization and normalization\n- Extracting top-N with a heap\n\n## Code Example\n```javascript\nimport string, heapq\n\ndef top_n_words(filepath, n):\n    counts = {}\n    trans = str.maketrans('', '', string.punctuation)\n    with open(filepath, 'r', encoding='utf-8') as f:\n        for line in f:\n            line = line.lower().translate(trans)\n            for w in line.split():\n                counts[w] = counts.get(w, 0) + 1\n    return [w for w, _ in heapq.nlargest(n, counts.items(), key=lambda kv: kv[1])]\n``n\n## Follow-up Questions\n- How would you adapt this to handle memory spikes with extremely large vocabularies?\n- How would you test the function for correctness and performance?","diagram":null,"difficulty":"beginner","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T10:45:42.409Z","createdAt":"2026-01-14T10:45:42.410Z"},{"id":"q-1867","question":"Design a Python async NDJSON ingestion pipeline that reads lines from an async source, validates each line against a versioned Pydantic model, supports hot-reloadable schema versions from a shared config, and guarantees exactly-once delivery with an idempotent sink and per-record deduplication, all while enforcing bounded memory and backpressure. How would you implement it?","answer":"I would implement an asyncio pipeline with a bounded queue, an async producer reading NDJSON lines, a versioned validator registry loaded from a shared JSON config, and an idempotent sink using a smal","explanation":"## Why This Is Asked\n\nThis question probes experience with dynamic schemas, backpressure handling, and exactly-once semantics in a streaming Python pipeline.\n\n## Key Concepts\n\n- Async NDJSON ingestion with bounded memory\n- Versioned validators and runtime config reload\n- Idempotent sinks and per-record deduplication\n- Safe hot-reload and atomic registry swap\n\n## Code Example\n\n```python\nfrom typing import Any, Dict\nimport asyncio\nimport json\nfrom pydantic import BaseModel\n\nclass V1(BaseModel):\n    id: str\n    value: int\n    version: str = 'v1'\n\nclass V2(BaseModel):\n    id: str\n    value: int\n    meta: str = ''\n\nSchemaRegistry = {'v1': V1, 'v2': V2}\n\ndef load_schema(ver: str):\n    return SchemaRegistry.get(ver, V1)\n\nasync def validate_line(line: str) -> Dict[str, Any]:\n    data = json.loads(line)\n    ver = data.get('version', 'v1')\n    model = load_schema(ver)\n    obj = model(**data)\n    return obj.dict()\n```\n\n## Follow-up Questions\n- How would you test hot-reload safety and dedup correctness?\n- How would you scale to multiple workers and a shared dedup store?","diagram":null,"difficulty":"advanced","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T14:55:43.072Z","createdAt":"2026-01-14T14:55:43.073Z"},{"id":"q-1884","question":"Implement an asynchronous Python data transformer for a JSONL event stream where each line includes a 'version' field. Build transform_stream(input: AsyncIterable[str], schemas: Dict[int, Type[BaseModel]]) that validates each line against its versioned Pydantic model, applies a version-aware field mapping, and outputs transformed JSONL lines to a downstream sink while guaranteeing per-version in-order processing, memory-bounded streaming, and backpressure. Include a small test scaffold showing a v1→v2 migration?","answer":"Use an AsyncStream with per-version queues. Validate by loading schemas[version] and applying mapping; emit transformed JSON via an async sink. Maintain in-order by preserving sequence_id per version ","explanation":"## Why This Is Asked\nThis question probes the ability to design a robust streaming Python solution that handles schema evolution, backpressure, and memory constraints in real-time data processing.\n\n## Key Concepts\n- AsyncIO pipelines with bounded queues\n- Versioned Pydantic models loaded from a registry\n- In-order processing across dynamic schemas\n- Safe error handling and backpressure-driven flow control\n\n## Code Example\n```python\nfrom typing import AsyncIterable, Dict, Type\nfrom pydantic import BaseModel\nimport asyncio\nimport json\n\nasync def transform_stream(\n    input_lines: AsyncIterable[str],\n    schemas: Dict[int, Type[BaseModel]],\n    mapper: Dict[int, callable],\n    sink: asyncio.Queue\n) -> None:\n    # simplified skeleton\n    queues = {v: asyncio.Queue(maxsize=1024) for v in schemas}\n    async def worker(version: int, q: asyncio.Queue):\n        while True:\n            line = await q.get()\n            if line is None:\n                break\n            data = json.loads(line)\n            model = schemas[version](**data)  # validate\n            out = mapper[version](model)\n            await sink.put(json.dumps(out.dict()))\n            q.task_done()\n\n    procs = [asyncio.create_task(worker(v, queues[v])) for v in schemas]\n    async for line in input_lines:\n        data = json.loads(line)\n        ver = data.get(\"version\")\n        if ver not in queues:\n            continue\n        await queues[ver].put(line)\n    for q in queues.values():\n        await q.put(None)\n    await asyncio.gather(*procs)\n```\n\n## Follow-up Questions\n- How would you handle missing or unknown versions gracefully?\n- How would you test migration paths between versions?","diagram":"flowchart TD\n  A[Input JSONL] --> B[Versioned Validation]\n  B --> C[Per-Version Router]\n  C --> D[Output Sink]\n  D --> E[Backpressure]","difficulty":"intermediate","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","NVIDIA","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T15:45:20.047Z","createdAt":"2026-01-14T15:45:20.047Z"},{"id":"q-2090","question":"Design a memory-bounded streaming top-K aggregator in Python. Data arrives as an async iterable of numeric events with timestamps. Implement a class that maintains an approximate top-10 using a Count-Min Sketch plus a min-heap, supports a sliding time window, and exposes add(value, ts) and get_top_k() reflecting the current window. Describe API, memory guarantees, and a test plan for bursty traffic?","answer":"Use a Count-Min Sketch with configurable dimensions (width, depth) to estimate item frequencies, combined with a min-heap keyed by estimated counts to maintain the top-K elements. Implement a sliding window using a deque that stores (timestamp, value) pairs; when adding new items, update the sketch, maintain the heap, and remove expired entries by decrementing their counts in the sketch.","explanation":"## Why This Is Asked\nThis question tests the ability to design memory-bounded streaming analytics systems using approximate data structures while maintaining timely query results.\n\n## Key Concepts\n- Streaming algorithms with bounded memory\n- Count-Min Sketch for frequency estimation\n- Min-heap for top-K maintenance\n- Sliding time window management\n- Memory-accuracy trade-offs\n\n## Code Example\n```python\nclass TopKStreaming:\n    def __init__(self, k=10, width=1000, depth=5, window=60):\n        pass\n    def add(self, value, ts):\n        pass\n    def get_top_k(self):\n        pass\n```\n\n## Follow-up","diagram":"flowchart TD\n  A[Async data stream] --> B[Count-Min Sketch update]\n  B --> C[Top-K heap maintenance]\n  A --> D[Sliding window eviction]\n  C --> E[Current top-K snapshot]","difficulty":"intermediate","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Databricks","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T05:03:18.175Z","createdAt":"2026-01-14T23:40:32.566Z"},{"id":"q-2402","question":"You're building an asyncio Python client for a rate-limited REST API. How would you implement a function fetch_with_backoff(url, method='GET', max_retries=5, session, rate_limiter, metrics) that performs retries with exponential backoff and jitter, enforces a per-endpoint token-bucket rate limit, and records latency and outcomes into metrics? Include a minimal code sketch and a test plan?","answer":"Use an asyncio fetch_with_backoff that acquires a permit from rate_limiter per endpoint, runs the request with session, and on transient failures retries with jittered exponential backoff (base ~0.2s,","explanation":"## Why This Is Asked\n\nAssesses practical mastery of asyncio, rate limiting, robust retries, and observability in real-world networked Python code, matching needs at scale (e.g., Twitter, Nvidia, Adobe).\n\n## Key Concepts\n\n- Async I/O with aiohttp or httpx\n- Per-endpoint rate limiting (token bucket)\n- Exponential backoff with jitter\n- Latency and outcome metrics collection\n- Test strategy: simulate 429, 5xx, network partitions, concurrent callers\n\n## Code Example\n\n```javascript\n// Example sketch (not executable in Python)\nasync def fetch_with_backoff(url, method='GET', max_retries=5, session, rate_limiter, metrics):\n    endpoint = extract_endpoint(url)\n    await rate_limiter.acquire(endpoint)\n    base = 0.2\n    cap = 10\n    for attempt in range(max_retries + 1):\n        t0 = time.monotonic()\n        try:\n            async with session.request(method, url) as resp:\n                latency = time.monotonic() - t0\n                metrics.record(endpoint, resp.status, latency, attempt)\n                if resp.status in {429, 500, 502, 503, 504}:\n                    raise transient_error()\n                return await resp.read()\n        except Exception:\n            latency = time.monotonic() - t0\n            metrics.record(endpoint, None, latency, attempt)\n            if attempt == max_retries:\n                raise\n            delay = min(cap, base * (2 ** attempt)) + random.uniform(0, 0.1)\n            await asyncio.sleep(delay)\n```\n\n## Follow-up Questions\n\n- How would you test the rate limiter under bursty traffic?\n- How would you extend to idempotent vs non-idempotent methods?","diagram":null,"difficulty":"intermediate","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","NVIDIA","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T16:52:41.364Z","createdAt":"2026-01-15T16:52:41.364Z"},{"id":"q-2704","question":"Design an async Python NDJSON processor that reads lines from an AsyncIterable[str], each line containing a top-level version and payload. It must validate using a dynamic, versioned Pydantic model registry, support on-the-fly hot-swapping of versions without restart, ensure per-version in-order processing, memory-bounded streaming, and backpressure to a downstream sink. Include a test scaffold showing v1→v2 migration and a runtime version swap?","answer":"Implementation: maintain a thread-safe registry mapping version to Pydantic models; parse each line to dict, look up model, validate payload, then emit to a bounded asyncio.Queue consumed by downstrea","explanation":"## Why This Is Asked\nTests runtime schema evolution, memory-bounded streaming, and backpressure integration in a single async Python pipeline.\n\n## Key Concepts\n- Async NDJSON processing\n- Versioned Pydantic models\n- Atomic registry swap\n- In-order per-version processing\n- Bounded queues and reordering buffers\n\n## Code Example\n```python\nfrom typing import AsyncIterable, Dict, Type\nfrom pydantic import BaseModel\nimport asyncio\n\nclass V1(BaseModel):\n    x: int\nclass V2(BaseModel):\n    x: int\n    y: int\n\nasync def process(input_stream: AsyncIterable[str], registry: Dict[int, Type[BaseModel]], sink: asyncio.Queue):\n    # skeleton for versioned validation and backpressure\n    pass\n```\n\n## Follow-up Questions\n- How would you test memory usage and backpressure under slow sinks?\n- How would you retire old versions without disrupting in-flight lines?","diagram":"flowchart TD\n  A[NDJSON line] --> B[Registry lookup]\n  B --> C[Validate payload]\n  C --> D[Enqueue downstream]\n  D --> E[Downstream sink]","difficulty":"advanced","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Anthropic","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T07:39:09.489Z","createdAt":"2026-01-16T07:39:09.489Z"},{"id":"q-2835","question":"Design a Python async streaming solution: implement an async function process_metrics(feed: AsyncIterator[bytes]) -> AsyncIterator[Tuple[str, int, float]] that reads a live UDP multicast binary stream of metric events. Each message contains a 4-byte big-endian epoch timestamp (seconds), a 2-byte metric_id, and an 8-byte float value. Validate metric_id against a map, maintain a 60-second rolling window per metric_id, and emit (metric_id, window_start, sum) whenever the rolling sum changes by at least 5% or every 10 seconds, whichever comes first. It must be memory-bounded and backpressure friendly with a bounded internal queue. Include a concise test plan using bursty traffic and clock skew?","answer":"Implement an async reader of a UDP multicast binary stream and a memory-bounded rolling window aggregator. Decode frames with struct.unpack('>I H d'), map metric_id to key, and maintain a 60-second sl","explanation":"## Why This Is Asked\nTests real-world streaming, binary parsing, per-key rolling windows, and memory/backpressure management—critical for high-scale services such as Discord/Cloudflare.\n\n## Key Concepts\n- Async streaming and backpressure\n- Binary protocol parsing with struct\n- Sliding window per key\n- Memory bounding via deques and eviction\n- Threshold-based emission for efficiency\n\n## Code Example\n```python\nimport asyncio\nimport struct\nfrom collections import deque, defaultdict\nfrom typing import AsyncIterator, Tuple\n\nFrame = Tuple[str, int, float]\n\nasync def process_metrics(feed: AsyncIterator[bytes]) -> AsyncIterator[Frame]:\n    q = asyncio.Queue(maxsize=1024)  # backpressure\n    metric_map = {1: 'cpu', 2: 'mem'}  # example map\n\n    async def reader():\n        async for chunk in feed:\n            await q.put(chunk)\n\n    asyncio.create_task(reader())\n\n    windows = defaultdict(lambda: deque())  # (ts, val)\n    sums = defaultdict(float)\n    window = 60\n    last_emit = defaultdict(lambda: 0.0)\n\n    while True:\n        chunk = await q.get()\n        ts, mid, val = struct.unpack('>I H d', chunk)\n        key = metric_map.get(mid, f'mid_{mid}')\n        items = windows[key]\n        items.append((ts, val))\n        sums[key] += val\n        cutoff = ts - window\n        while items and items[0][0] <= cutoff:\n            old_ts, old_v = items.popleft()\n            sums[key] -= old_v\n        current = sums[key]\n        last = last_emit[key]\n        if last == 0 or abs(current - last) / max(abs(last), 1e-9) >= 0.05 or ts - (last_emit.get(key, 0)) >= 10:\n            last_emit[key] = current\n            yield (key, ts - window, current)\n```\n\n## Follow-up Questions\n- How would you handle out-of-order frames or missing frames?\n- How would you scale this across multiple multicast groups or nodes?","diagram":"flowchart TD\n  A[UDP Multicast Listener] --> B[Binary Decode with struct]\n  B --> C[Per-Key Rolling Window (60s)]\n  C --> D[Bounded Backpressure Queue]\n  D --> E[Emission of Updates]\n  E --> F[Consumer/Sink]","difficulty":"intermediate","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Discord"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T14:10:40.389Z","createdAt":"2026-01-16T14:10:40.390Z"},{"id":"q-2930","question":"How would you implement a memory-efficient Python function process_ndjson_by_type(input_path, out_dir) that streams an NDJSON file, parses each line as JSON, and writes every line to a per-type file named '{type}.ndjson' in out_dir? The function should create files on demand, reuse file handles to keep memory constant, track per-type line counts, and skip malformed lines while continuing?","answer":"Implement a streaming NDJSON type-partitioner in Python. Read input line-by-line, json.loads each line, require a 'type' field, and write the raw line to a per-type file named '{type}.ndjson' inside o","explanation":"## Why This Is Asked\nTests ability to design a memory-efficient streaming processor for line-delimited JSON, including dynamic partitioning, file management, and robust error handling. It also validates practical testing plans for I/O-heavy code.\n\n## Key Concepts\n- Streaming I/O with line-by-line processing\n- Dynamic per-type partitioning and lazy file creation\n- Maintaining a bounded set of open file handles\n- Robust error handling for malformed lines\n\n## Code Example\n```python\nimport json\nimport os\n\ndef process_ndjson_by_type(input_path, out_dir):\n    os.makedirs(out_dir, exist_ok=True)\n    handles = {}\n    counts = {}\n    with open(input_path, 'r', encoding='utf-8') as f:\n        for line in f:\n            line = line.rstrip('\\n')\n            if not line:\n                continue\n            try:\n                obj = json.loads(line)\n            except Exception:\n                continue\n            t = obj.get('type')\n            if not isinstance(t, str):\n                continue\n            fh = handles.get(t)\n            if fh is None:\n                fh = open(os.path.join(out_dir, f\"{t}.ndjson\"), 'a', encoding='utf-8')\n                handles[t] = fh\n            fh.write(line + '\\n')\n            counts[t] = counts.get(t, 0) + 1\n    for fh in handles.values():\n        fh.close()\n    return counts\n```\n\n## Follow-up Questions\n- How would you add validation to ensure schema compatibility across partitions?\n- How would you implement unit tests that simulate large NDJSON files with varied 'type' values?","diagram":"flowchart TD\n  A[NDJSON Input] --> B{Parse Line}\n  B -- valid --> C[Write to {type}.ndjson]\n  C --> D[Counts per type]\n  B -- invalid --> E[Skip line]\n  D --> F[Return counts]","difficulty":"beginner","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T17:52:21.915Z","createdAt":"2026-01-16T17:52:21.915Z"},{"id":"q-2955","question":"Design an async NDJSON processor in Python: read lines from an AsyncIterable[str], each line is JSON with a 'service' key. Implement partitioned_aggregate(stream, key_func, sink, max_keys=4096) that maintains per-key in-order processing, bounded buffering, and backpressure to the sink. It must allow hot-swapping key_func at runtime without restart and include a small test scaffold showing two services migrating?","answer":"To meet scale and latency, implement a central dispatcher that routes each input line to a per-key queue guarded by an asyncio.Lock. Spawn a dedicated task per key that drains its queue in-order and e","explanation":"## Why This Is Asked\n\nTests ability to design a streaming, memory-bounded Python solution with per-key ordering, dynamic behavior, and backpressure in an async setting.\n\n## Key Concepts\n\n- Async NDJSON streaming\n- Per-key partitioning and in-order processing\n- Bounded buffers and downstream backpressure\n- Runtime hot-swapping of function references\n- Lightweight test scaffolds for migrations\n\n## Code Example\n\n```python\nasync def partitioned_aggregate(stream: AsyncIterable[str], key_func: Callable[[dict], str], sink: AsyncSink, max_keys: int = 4096):\n    pass  # skeleton for per-key queues, workers, and hot-swap mechanism\n```\n\n## Follow-up Questions\n\n- How do you test memory usage with many keys?\n- How handle key worker failures without cascading? \n- How to extend to multi-sink backpressure and retries?","diagram":null,"difficulty":"advanced","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Apple","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T18:58:11.997Z","createdAt":"2026-01-16T18:58:11.997Z"},{"id":"q-3183","question":"Implement a memory-friendly CSV reader in Python: create a function stream_csv_with_required(file_path: str, required_fields: List[str]) that streams rows using csv.DictReader and yields only rows where all required_fields are present and non-empty. It should count and expose the number of skipped rows, without loading the entire file into memory. Provide a brief usage example?","answer":"Define a generator that opens the file and uses csv.DictReader, yielding rows only when all required_fields exist and have non-empty values; track skipped rows with a counter. Use typing.Iterator[Dict","explanation":"## Why This Is Asked\n\nTests memory-friendly CSV streaming and simple validation.\n\n## Key Concepts\n\n- streaming I/O\n- csv module\n- generators\n- basic type hints\n\n## Code Example\n\n```python\nimport csv\nfrom typing import Dict, Iterator, List\n\ndef stream_csv_with_required(file_path: str, required_fields: List[str]) -> Iterator[Dict[str, str]]:\n    with open(file_path, newline='', encoding='utf-8') as f:\n        reader = csv.DictReader(f)\n        dropped = 0\n        for row in reader:\n            if all(row.get(k) for k in required_fields):\n                yield row\n            else:\n                dropped += 1\n```\n\n## Follow-up Questions\n\n- How would you adapt it to handle extremely large required_fields or unknown schemas?\n- What tests would you write to verify correctness and performance?","diagram":"flowchart TD\n  Start([Start]) --> OpenCSV[Open CSV]\n  OpenCSV --> ReadRow[Read Row]\n  ReadRow --> Check{AllRequired?}\n  Check -- Yes --> Emit[Emit Row]\n  Check -- No --> Drop[Skip Row]\n  Drop --> ReadRow","difficulty":"beginner","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Oracle","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T05:43:19.182Z","createdAt":"2026-01-17T05:43:19.182Z"},{"id":"q-3267","question":"Design and implement a memory-efficient Python function aggregate_csvs(file_paths, group_by, sum_cols, output_path, max_mem_mb=100). It should streaming-read CSVs, group by the given columns, and sum the specified numeric columns per group. To stay within the memory bound, spill partial aggregates to disk after processing a chunk, then merge spills into the final output. Provide a minimal working skeleton and a test plan?","answer":"Use a streaming approach: read CSVs in chunks with the csv module, build in-memory aggregates keyed by the group_by columns, and sum the sum_cols. Track approximate memory usage; when you exceed max_m","explanation":"## Why This Is Asked\n\nTests ability to implement external-memory aggregation in Python, a common production pattern when datasets exceed RAM. It probes chunked I/O, memory budgeting, and correctness when merging partial results.\n\n## Key Concepts\n\n- Streaming I/O with csv module\n- Memory-bounded aggregation\n- Spill-to-disk algorithm\n- Merging partial results\n\n## Code Example\n\n```python\n# Skeleton code illustrating the spill logic\nimport csv\nfrom collections import defaultdict\ndef aggregate(file_paths, group_by, sum_cols, output_path, max_mem_mb=100):\n    pass\n```\n\n## Follow-up Questions\n\n- How would you handle numeric types with missing values?\n- How would you parallelize spills across multiple processes?","diagram":"flowchart TD\n  A[Read CSV chunks] --> B[Update in-memory aggregates]\n  B --> C{mem limit reached?}\n  C -->|Yes| D[Flush to spill files]\n  C -->|No| E[Continue]\n  D --> F[Reset in-memory]\n  F --> E\n  E --> G[Merge spills] --> H[Write output]","difficulty":"intermediate","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T09:32:24.255Z","createdAt":"2026-01-17T09:32:24.255Z"},{"id":"q-3339","question":"Design and implement an asynchronous Python function process_jsonl_stream(source: AsyncIterable[str], model: Type[BaseModel], sink: Callable[[dict], Awaitable[None]], max_memory_mb: int) that reads JSONL lines, validates each with the provided Pydantic model, deduplicates by the line's 'id' field using a Bloom filter, and streams only unique events to sink while enforcing a hard memory cap and applying backpressure when the sink laggs?","answer":"Implement an async generator that reads JSONL lines, validates each with the provided Pydantic BaseModel, deduplicates by the line id using a BloomFilter, and forwards new events to sink with backpres","explanation":"## Why This Is Asked\n\nIn production, streaming validation with backpressure and memory constraints is common; this tests integration of async IO, validation, deduping, and memory management.\n\n## Key Concepts\n\n- Async streaming and backpressure\n- Bloom filter deduping and rotation\n- Pydantic validation and error handling\n- Memory tracking with tracemalloc\n\n## Code Example\n\n```python\n# skeleton implementation\nfrom typing import AsyncIterable, Callable, Awaitable, Type\nfrom pydantic import BaseModel\nimport asyncio, json\nfrom bloom_filter2 import BloomFilter\n\nasync def process_jsonl_stream(source: AsyncIterable[str], model: Type[BaseModel], sink: Callable[[dict], Awaitable[None]], max_memory_mb: int):\n    bf = BloomFilter(max_elements=100000, error_rate=0.01)\n    q = asyncio.Queue(maxsize=1024)\n    # simplified loop\n    async for line in source:\n        try:\n            data = json.loads(line)\n            obj = model.parse_obj(data)\n        except Exception:\n            continue\n        if obj.id in bf:\n            continue\n        bf.add(obj.id)\n        await q.put(obj.dict())\n        if q.qsize() == q.maxsize:\n            item = await q.get()\n            await sink(item)\n```\n\n## Follow-up Questions\n\n- How would you rotate BloomFilter and coordinate eviction across workers?\n- How would you test backpressure behavior with a slow sink and bursty input?","diagram":null,"difficulty":"intermediate","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Cloudflare","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T12:58:42.049Z","createdAt":"2026-01-17T12:58:42.049Z"},{"id":"q-3397","question":"Design a beginner-friendly Python function analyze_log(file_path) that streams a log file line by line and returns a dict counting occurrences of INFO, WARN, and ERROR. Ignore malformed lines; use a generator to read lines and a defaultdict for counts. Ensure it scales to multi-GB files without loading all content?","answer":"Open the file and iterate line by line to avoid memory usage. Use defaultdict(int) to tally counts for INFO, WARN, ERROR. For each line, strip, split(None, 2) to separate level, timestamp, message; sk","explanation":"## Why This Is Asked\nTests ability to stream data, keep memory footprint small, and handle noisy inputs. \n\n## Key Concepts\n- Streaming I/O\n- Counter patterns; defaultdict\n- Input validation and robustness\n\n## Code Example\n```python\nfrom collections import defaultdict\n\ndef analyze_log(path):\n    counts = defaultdict(int)\n    with open(path, 'r', encoding='utf-8') as f:\n        for line in f:\n            parts = line.strip().split(None, 2)\n            if len(parts) < 3:\n                continue\n            level, _, _ = parts\n            if level in ('INFO','WARN','ERROR'):\n                counts[level] += 1\n    return dict(counts)\n```\n\n## Follow-up Questions\n- How would you extend this to parse timestamps and compute per-hour rates?\n- How would you adapt to log rotation and concurrent writers?","diagram":"flowchart TD\n  A[Open file] --> B{Line read}\n  B --> C{Malformed?}\n  C -- Yes --> D[Skip]\n  C -- No --> E[Extract level]\n  E --> F{Level valid?}\n  F -- Yes --> G[Counts[level]++]\n  F -- No --> D\n  G --> H[End or next line]","difficulty":"beginner","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Snowflake","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T14:36:48.341Z","createdAt":"2026-01-17T14:36:48.342Z"},{"id":"q-3420","question":"Design a Python async event router that ingests NDJSON lines from an AsyncIterable[str], routes each line to a per-type Pydantic model registry based on a 'type' field, supports hot-swapping of routing rules at runtime without restart, guarantees per-type in-order processing, and uses bounded asyncio.Queues to provide backpressure to downstream sinks. Include a minimal test scaffold showing a type A mapping to V1 and a runtime swap to V2?","answer":"Design an AsyncRouter with a registry mapping type->Pydantic model, guarded by an asyncio.Lock, plus per-type bounded queues and a dedicated consumer per type to preserve in-order processing. Implemen","explanation":"## Why This Is Asked\n\nThis question tests dynamic routing, hot-swapping, per-type in-order processing, and backpressure in an async Python pipeline. It also probes correctness under concurrent updates and memory safety.\n\n## Key Concepts\n\n- Asyncio queues for per-type backpressure\n- Atomic hot-swap of routing rules with asyncio.Lock\n- Per-type in-order workers\n- Pydantic validation and error handling\n- End-to-end testing scaffold\n\n## Code Example\n\n```python\n# Skeleton illustrating the pattern; full impl expected in interview\nfrom typing import Dict, Type\nfrom pydantic import BaseModel\nimport asyncio\n\nclass Router:\n    def __init__(self, registry: Dict[str, Type[BaseModel]]):\n        self._registry = registry\n        self._lock = asyncio.Lock()\n        self._queues: Dict[str, asyncio.Queue] = {}\n        self._consumers: Dict[str, asyncio.Task] = {}\n\n    async def hot_swap(self, new_registry: Dict[str, Type[BaseModel]]):\n        async with self._lock:\n            self._registry = new_registry\n            # optionally swap queues/consumers if needed\n```\n\n## Follow-up Questions\n\n- How would you test race conditions during hot_swap?\n- How would you scale to many types while preserving fairness?","diagram":"flowchart TD\n  Ingest[NDJSON AsyncIterable[str]] --> Router[AsyncRouter]\n  Router --> Q_A[Queue A]\n  Router --> Q_B[Queue B]\n  Q_A --> SinkA[Sink A]\n  Q_B --> SinkB[Sink B]","difficulty":"advanced","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","IBM","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T15:36:30.233Z","createdAt":"2026-01-17T15:36:30.233Z"},{"id":"q-3449","question":"You have a log file that grows indefinitely containing one numeric value per line (e.g., latency in ms) along with occasional non-numeric lines. Implement a Python function running_stats(file_path) that streams lines, ignores invalid ones, and after each valid line yields a tuple (count, mean, min, max) computed so far. Keep memory usage O(1)?","answer":"Implement a streaming function that reads file_path line by line, parses each line as float, skips invalid lines, and after each valid line yields (count, mean, min, max). Maintain only four scalars (","explanation":"## Why This Is Asked\nTests ability to design a streaming statistic, avoids loading entire file, handles dirty data, and maintains simple O(1) state with a generator.\n\n## Key Concepts\n- Streaming I/O with generators\n- Online statistics (count, total, min, max)\n- Defensive parsing and error handling\n- Memory efficiency with large data\n\n## Code Example\n```python\ndef running_stats(file_path):\n    count = 0\n    total = 0.0\n    min_v = None\n    max_v = None\n    with open(file_path, 'r') as f:\n        for line in f:\n            line = line.strip()\n            try:\n                val = float(line)\n            except ValueError:\n                continue\n            count += 1\n            total += val\n            min_v = val if min_v is None else min(min_v, val)\n            max_v = val if max_v is None else max(max_v, val)\n            yield count, (total / count), min_v, max_v\n```\n\n## Follow-up Questions\n- How would you extend to handle NaN values or infinity?\n- How would you modify to support streaming from a network source with backpressure?\n","diagram":null,"difficulty":"beginner","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Plaid","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T16:46:49.618Z","createdAt":"2026-01-17T16:46:49.619Z"},{"id":"q-3513","question":"In Python, implement a real-time metrics ingester that reads an async NDJSON stream where each line has host, metric, value, and ts. Write windowed_stats(stream, window_secs) to maintain a sliding window per (host, metric) using a deque of (ts, value) plus monotonic min/max queues; emit updated stats only when they move by more than 1%. Handle slight out-of-order arrivals with a small per-key buffer. Explain design choices and edge cases?","answer":"Per-(host, metric) sliding window with a dict of keys to a small state object. Each state holds a deque for (ts, value), a running sum, and two monotonic deques for min and max. On each line: evict ol","explanation":"## Why This Is Asked\nTests ability to implement per-key sliding windows with streaming data and a memory guarantee, plus handling of out-of-order data and threshold-based emissions.\n\n## Key Concepts\n- Async streaming ingestion\n- Per-key state in O(number of active keys)\n- Sliding window with deque; monotonic min/max queues\n- Change-detection threshold to throttle output\n- Late-event buffering strategy\n\n## Code Example\n```python\nfrom collections import deque, defaultdict\n\nclass SlidingWindowPerKey:\n    def __init__(self, window_secs):\n        self.window = window_secs\n        self.keys = defaultdict(self._new_key)\n\n    def _new_key(self):\n        return {'values': deque(), 'sum': 0.0, 'minq': deque(), 'maxq': deque(), 'last': None}\n\n    def push(self, host, metric, ts, value):\n        k = (host, metric)\n        rec = self.keys[k]\n        cutoff = ts - self.window\n        while rec['values'] and rec['values'][0][0] < cutoff:\n            old_ts, old_v = rec['values'].popleft()\n            rec['sum'] -= old_v\n            if rec['minq'] and rec['minq'][0][0] == old_ts:\n                rec['minq'].popleft()\n            if rec['maxq'] and rec['maxq'][0][0] == old_ts:\n                rec['maxq'].popleft()\n        rec['values'].append((ts, value))\n        rec['sum'] += value\n        while rec['minq'] and rec['minq'][-1][1] > value:\n            rec['minq'].pop()\n        rec['minq'].append((ts, value))\n        while rec['maxq'] and rec['maxq'][-1][1] < value:\n            rec['maxq'].pop()\n        rec['maxq'].append((ts, value))\n        avg = rec['sum'] / len(rec['values'])\n        mn = rec['minq'][0][1]\n        mx = rec['maxq'][0][1]\n        count = len(rec['values'])\n        stat = (avg, mn, mx, count)\n        if rec['last'] is None or any(abs(a-b) > 0.01 * max(abs(b),1) for a,b in zip(stat, rec['last'])):\n            rec['last'] = stat\n            yield (host, metric, avg, mn, mx, count)\n```\n\n## Follow-up Questions\n- How would you test this with clock skew and bursty data?\n- How would you adapt to multiple streams with backpressure and fault tolerance?","diagram":null,"difficulty":"intermediate","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","Microsoft","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T19:29:39.599Z","createdAt":"2026-01-17T19:29:39.599Z"},{"id":"q-3532","question":"Implement a Python function parse_semver_line(line) that parses a line of the form 'name: vMAJOR.MINOR.PATCH-PRERELEASE+BUILD' where v and prerelease/build are optional. Return a dict with keys: name, major, minor, patch, prerelease (or None), build (or None). Ignore extra whitespace. If invalid, raise ValueError. The line may have additional spaces; handle edge cases like 'pkg:1.2.3' or 'pkg: v1.2.3-alpha+001'?","answer":"Use a simple, robust parser that splits on ':' and handles an optional leading 'v', optional prerelease and build segments, then coerces major/minor/patch to int. If the format is invalid, raise Value","explanation":"## Why This Is Asked\nSemantic version parsing is common in package tooling and needs robust string handling.\n\n## Key Concepts\n- Regex parsing with named groups\n- Optional segments for prerelease/build\n- Safe int conversion and error handling\n- Trimming whitespace and invalid format signaling\n\n## Code Example\n```python\ndef parse_semver_line(line: str) -> dict:\n    try:\n        name, ver = line.split(':', 1)\n        name = name.strip()\n        ver = ver.strip()\n        if ver.startswith('v'):\n            ver = ver[1:]\n        prerelease = None\n        build = None\n        if '+' in ver:\n            ver, build = ver.split('+', 1)\n        if '-' in ver:\n            ver, prerelease = ver.split('-', 1)\n        parts = ver.split('.')\n        if len(parts) != 3:\n            raise ValueError('Invalid version')\n        major, minor, patch = map(int, parts)\n        return {\n            'name': name,\n            'major': major,\n            'minor': minor,\n            'patch': patch,\n            'prerelease': prerelease,\n            'build': build\n        }\n    except Exception:\n        raise ValueError('Invalid semver line')\n```\n\n## Follow-up Questions\n- How would you extend to validate numeric ranges for majors/minors/patches?\n- How would you adapt to stream lines from a file and yield valid dicts lazily?","diagram":null,"difficulty":"beginner","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Databricks"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T20:34:14.216Z","createdAt":"2026-01-17T20:34:14.216Z"},{"id":"q-3698","question":"Design a Python async NDJSON ingest pipeline that reads lines from an AsyncIterable[str], validates each line against a per-schema Pydantic model chosen by a schema_id field, and dispatches to per-schema handlers via bounded asyncio.Queues. Implement runtime hot-swapping of schemas (without restart) and guarantee in-order processing per schema even as schemas are swapped. Include a minimal test scaffold showing schema_id=1 mapped to V1 and swapped to V2 at runtime; discuss memory, latency, and safety trade-offs?","answer":"Architect a reader that streams NDJSON via an async generator, dispatching each line to a per-schema queue keyed by schema_id. Register schema_id to Pydantic model (V1, V2). Hot-swap by atomically upd","explanation":"## Why This Is Asked\nTests ability to design a dynamic, high-throughput streaming pipeline with runtime configurability and strong ordering guarantees. It also probes memory management, backpressure, and safe hot-swapping of schemas without restart.\n\n## Key Concepts\n- Async streaming and NDJSON parsing\n- Per-key routing and bounded queues for backpressure\n- Runtime hot-swapping of schemas with atomic registry updates\n- Per-schema in-order processing and sequence tracking\n\n## Code Example\n```python\n# Pydantic models and dispatcher sketch\nfrom pydantic import BaseModel\nimport asyncio, json\n\nclass V1(BaseModel):\n    id: int; value: int\n\nclass V2(BaseModel):\n    id: int; value: int; meta: str\n\nschema_registry = {1: V1, 2: V1}\nregistry_lock = asyncio.Lock()\nqueues = {1: asyncio.Queue(maxsize=1024), 2: asyncio.Queue(maxsize=1024)}\n\nasync def reader(async_iter):\n    async for line in async_iter:\n        payload = json.loads(line)\n        sid = payload[\"schema_id\"]\n        async with registry_lock:\n            Model = schema_registry.get(sid, None)\n        if Model is None:\n            continue\n        await queues[sid].put((payload, Model))\n\n# workers would validate and emit results preserving order\n```\n\n## Follow-up Questions\n- How would you implement failure handling for invalid lines while preserving ordering?\n- How to measure throughput and detect backpressure saturation?","diagram":null,"difficulty":"advanced","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","NVIDIA","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T05:44:50.681Z","createdAt":"2026-01-18T05:44:50.681Z"},{"id":"q-3778","question":"Design a Python async streaming pipeline for a live NDJSON feed of transactions over a socket. Each line is a JSON object with 'user_id', 'amount', and 'ts'. Implement windowed aggregation: per-user total amount in 5-minute tumbling windows, emit results when a window closes, and handle out-of-order events up to 2 seconds. Use asyncio, memory-bounded structures, and a backpressure-friendly sink. Provide a minimal test plan?","answer":"Use an asyncio-based streaming pipeline that maintains per-user, 5-minute tumbling windows in memory. Parse each NDJSON line, compute window_start = floor(ts to 5 minutes), and accumulate totals in a ","explanation":"## Why This Is Asked\nTests proficiency with asynchronous streams, in-order constraints, and real-time aggregation under memory limits. It also probes how to handle clock skew and backpressure in a live ingestion path.\n\n## Key Concepts\n- Async I/O and streaming NDJSON parsing\n- Tumbling window aggregation and watermarking for out-of-order data\n- Memory-safe in-memory dictionaries with eviction/flush strategy\n- Backpressure via bounded queues and sink pacing\n\n## Code Example\n```python\nimport asyncio\nimport json\nfrom collections import defaultdict\nfrom datetime import datetime, timezone, timedelta\n\n# Placeholder outline for the core idea; concrete implementation would flesh this out\nasync def process_stream(reader, sink, window_min=300):\n    totals = defaultdict(int)  # key: (user_id, window_start)\n    # ... read lines, parse JSON, bucket into (user_id, window_start)\n    # ... periodically flush expired windows to sink (bounded)\n    pass\n```\n\n## Follow-up Questions\n- How would you adapt for multiple concurrent sinks with different backpressure characteristics?\n- How would you test with clock skew, bursty traffic, and late events beyond the watermark?","diagram":"flowchart TD\n  A[NDJSON line] --> B[Parse JSON]\n  B --> C[Compute window_start]\n  C --> D[Accumulate totals[(user_id, window_start)]]\n  D --> E[Backpressure sink (bounded queue)]\n  E --> F[Emit on window close]\n  G[Watermark for out-of-order handling] --> H[Flush expired windows]","difficulty":"intermediate","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T09:31:50.697Z","createdAt":"2026-01-18T09:31:50.697Z"},{"id":"q-4343","question":"You're processing a CSV log of signups with fields: user_id, email, signup_ts. The file is potentially several gigabytes. Implement a function yield_valid_signups(path) that streams lines using the csv module, validates email with a simple regex, and verifies signup_ts is a valid ISO 8601 timestamp. Yield dictionaries for valid lines only; ignore malformed lines and extra columns. Ensure minimal memory usage?","answer":"Open the file and iterate with csv.DictReader to avoid loading all rows. For each row, validate email with a basic regex like r'^[^@]+@[^@]+\\.[^@]+$' and parse signup_ts with datetime.fromisoformat, s","explanation":"## Why This Is Asked\nTests ability to design streaming data pipelines with Python's standard library, validating inputs on the fly and ignoring bad data, which is common in real data ingestion tasks at scale.\n\n## Key Concepts\n- Streaming I/O with csv.DictReader\n- Lightweight validation (regex + ISO parsing)\n- In-place yields and memory efficiency\n- Robust error handling with try/except\n\n## Code Example\n```python\nimport csv\nimport re\nfrom datetime import datetime\n\nEMAIL_RE = re.compile(r'^[^@]+@[^@]+\\.[^@]+$')\n\ndef yield_valid_signups(path):\n    with open(path, 'r', newline='') as f:\n        reader = csv.DictReader(f)\n        for row in reader:\n            email = row.get('email')\n            ts = row.get('signup_ts')\n            if not email or not ts:\n                continue\n            if not EMAIL_RE.match(email):\n                continue\n            try:\n                datetime.fromisoformat(ts)\n            except Exception:\n                continue\n            yield {'user_id': row.get('user_id'), 'email': email, 'signup_ts': ts}\n```\n\n## Follow-up Questions\n- How would you adapt this to report invalid line counts without slowing throughput?\n- How would you unit-test this function with a small sample file?","diagram":"flowchart TD\n  A[Start] --> B[Open CSV]\n  B --> C[DictReader]\n  C --> D{Line valid?}\n  D -->|Yes| E[Yield dict]\n  D -->|No| F[Skip line]\n  E --> G[Process next]\n  G --> C","difficulty":"beginner","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":null,"companies":["NVIDIA","Slack","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T14:52:11.650Z","createdAt":"2026-01-19T14:52:11.650Z"},{"id":"q-4466","question":"Build an asyncio-based NDJSON pipeline that ingests lines with a 'module' key, fetches runtime transformation code from a registry, and applies transform(payload) in a sandboxed environment. Requirements: preserve input order, cap per-item CPU/memory, support hot-reload of module code without restart, and apply backpressure to downstream via bounded queues. Provide a test scaffold?","answer":"Leverage an asyncio pipeline: input NDJSON lines → per-record module loader → sandboxed worker executes transform(payload) with memory/CPU caps (e.g., 20MB, 50ms) and restricted builtins → results mer","explanation":"## Why This Is Asked\nTests real-world capability to safely execute user-provided transforms at scale, ensuring safety, ordering, and throughput without downtime.\n\n## Key Concepts\n- Async pipelines with NDJSON\n- Dynamic module loading and hot-reload\n- Sandboxing and resource limits (memory, CPU/time)\n- Ordering guarantees and backpressure\n- Observability and testing strategies\n\n## Code Example\n```python\n# Skeleton illustrating components and flow\nimport asyncio\nimport concurrent.futures\n\n# Pseudo interfaces: registry_fetch, sandbox_run, merge_ordered\n\ndef registry_fetch(module_id, version): ...\n\ndef sandbox_run(code, payload): ...\n\nasync def pipeline(input_q, output_q):\n    loop = asyncio.get_event_loop()\n    while True:\n        item = await input_q.get()\n        # load module, run sandboxed transform, push to output_q preserving order\n        yield\n```\n\n## Follow-up Questions\n- How would you test hot-reload correctness without dropping items?\n- How would you monitor and limit memory growth per module in production?","diagram":"flowchart TD\n  A[NDJSON Input] --> B[Module Loader]\n  B --> C[Sandboxed Transform]\n  C --> D[Order Merger]\n  D --> E[Downstream Sink]","difficulty":"advanced","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Amazon","Meta","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T19:52:55.695Z","createdAt":"2026-01-19T19:52:55.695Z"},{"id":"q-4486","question":"Implement a memory-efficient Python generator function user_action_stream(file_path, batch_size=1000) that streams a nightly user activity log line by line. Each line is 'user_id action' (e.g., 'u123 login'). Ignore malformed lines. Maintain per-user action counts in memory and yield a snapshot dict every batch_size lines with shape {user_id: {action: count}}. Explain your approach and edge cases?","answer":"This uses a generator that reads the file line by line, parses with split(), and updates a dict of user_id -> Counter(actions). Malformed lines are skipped. After batch_size hits, yield a shallow copy","explanation":"## Why This Is Asked\nThis tests streaming, dictionary-based aggregation, and careful memory usage when handling very large files. It also probes handling of malformed data and how batch flushing affects memory and latency.\n\n## Key Concepts\n- Streaming I/O with generators\n- In-memory aggregation by key\n- Robust parsing and error handling\n- Memory-time trade-offs via batch flushing\n\n## Code Example\n```python\nfrom collections import defaultdict, Counter\n\ndef user_action_stream(file_path, batch_size=1000):\n    counts = defaultdict(Counter)\n    seen = 0\n    with open(file_path, 'r') as f:\n        for line in f:\n            parts = line.strip().split()\n            if len(parts) != 2:\n                continue\n            user, action = parts\n            counts[user][action] += 1\n            seen += 1\n            if seen % batch_size == 0:\n                yield {u: dict(c) for u, c in counts.items()}\n    if seen % batch_size != 0:\n        yield {u: dict(c) for u, c in counts.items()}\n```\n\n## Follow-up Questions\n- How would you reset state for a new log file or rotate files?\n- How would you test with malformed lines and varying batch sizes?","diagram":null,"difficulty":"beginner","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Bloomberg","Cloudflare","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T20:48:15.092Z","createdAt":"2026-01-19T20:48:15.092Z"},{"id":"q-4658","question":"Design an asynchronous NDJSON pipeline that consumes lines from an AsyncIterable[str]. Each line contains: partition (str), opcode (int), payload (dict), version (int). Build a dynamic registry mapping (opcode, version) to a handler that transforms payload into an output dict. The registry must support hot-swapping a handler for a given (opcode, version) at runtime without stopping the pipeline, using an atomic switch. Ensure per-partition in-order processing with bounded queues and backpressure to a downstream sink. Include a test scaffold swapping (opcode=5, version=1) to version=2 mid-stream and verify no data loss or reordering?","answer":"I’d implement per-partition workers with a bounded asyncio.Queue. A global, lock-protected registry maps (opcode, version) to a coroutine-friendly handler. Swaps create a new registry copy and atomica","explanation":"## Why This Is Asked\nReveals capability to perform zero-downtime upgrades in streaming data paths, with ordering guarantees, backpressure, and dynamic state like hot-swap of handlers.\n\n## Key Concepts\n- AsyncNDJSON processing with per-partition ordering\n- Dynamic, atomic hot-swap of (opcode, version) handlers\n- Backpressure via bounded queues and per-partition workers\n- Safe runtimes swaps without stopping dataplane\n\n## Code Example\n```python\nimport asyncio\nfrom typing import Callable, Dict, Tuple, Any\n\nHandler = Callable[[dict], dict]\n\nclass HotSwapRegistry:\n    def __init__(self, initial: Dict[Tuple[int,int], Handler]):\n        self._lock = asyncio.Lock()\n        self._registry = initial\n\n    async def get(self, opcode: int, version: int) -> Handler:\n        async with self._lock:\n            return self._registry.get((opcode, version))\n\n    async def swap(self, opcode: int, version: int, handler: Handler):\n        async with self._lock:\n            new = dict(self._registry)\n            new[(opcode, version)] = handler\n            self._registry = new\n```\n\n## Follow-up Questions\n- How would you ensure fairness if a handler blocks on I/O?\n- How do you handle handler exceptions without stopping the pipeline?\n- What metrics would you surface for swap latency and backpressure?","diagram":"flowchart TD\n  A[NDJSON Source] --> B[Partitioned Queues]\n  B --> C[Per-Partition Consumers]\n  C --> D[Sink with Backpressure]\n  E[Hot-Swap Registry] --> F[Atomic Swap]\n  D --> G[Feedback/Monitoring]","difficulty":"advanced","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Adobe","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T07:04:02.389Z","createdAt":"2026-01-20T07:04:02.389Z"},{"id":"q-474","question":"Write a Python function that takes a list of integers and returns the sum of all even numbers. How would you handle edge cases?","answer":"Use a generator expression with sum() for optimal performance. Implement input validation to handle edge cases robustly. The function gracefully handles empty lists by returning 0 and ignores non-integer elements to prevent TypeErrors. For large datasets, the generator approach provides memory efficiency by processing items one at a time.","explanation":"## Solution\n\n```python\ndef sum_even_numbers(numbers):\n    if not isinstance(numbers, list):\n        raise TypeError(\"Input must be a list\")\n    return sum(num for num in numbers if isinstance(num, int) and num % 2 == 0)\n```\n\n## Key Points\n\n- **Generator Expression**: Memory-efficient processing of large datasets\n- **Input Validation**: Robust error handling for invalid input types\n- **Edge Case Handling**: Returns 0 for empty lists, safely skips non-integers\n- **Performance**: Single pass algorithm with O(n) time complexity\n\n## Complexity\n\n- **Time**: O(n) - iterates through each element once\n- **Space**: O(1) - constant extra space using generator expression","diagram":"flowchart TD\n  A[Input List] --> B{Validate Type}\n  B -->|Valid| C[Filter Even Numbers]\n  B -->|Invalid| D[Raise TypeError]\n  C --> E[Sum Results]\n  E --> F[Return Sum]","difficulty":"beginner","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Microsoft","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-09T09:05:08.903Z","createdAt":"2025-12-26 12:51:05"},{"id":"q-4808","question":"You're building a real-time log processor in Python. NDJSON lines arrive via a socket. Each line has timestamp (ISO 8601), service, level (DEBUG/INFO/WARN/ERROR), and message. Implement an async function stream_counts(source, window_seconds) that consumes an async iterator of NDJSON lines, validates with a Pydantic model, and keeps a sliding window of the last window_seconds by timestamp. It should emit every second a dict mapping (service, level) to counts in the window; memory bounded by active groups. Drop lines older than the window; include newer lines even if late arrival?","answer":"Use an async generator to consume NDJSON lines, validate with a Pydantic model, and maintain a deque of (ts, key) entries for the sliding window. Keep a dict of counts per (service, level). Every seco","explanation":"## Why This Is Asked\nTests real-time streaming, validation, and time-windowed aggregation with memory constraints. It also evaluates handling of out-of-order events and backpressure in an async context.\n\n## Key Concepts\n- Async generators for streaming data\n- Pydantic for strict schema validation\n- Deque for time-based sliding window\n- Grouped counters by (service, level) with O(G) memory where G is active groups\n\n## Code Example\n```python\nfrom collections import deque, defaultdict\nfrom datetime import datetime, timezone\nfrom pydantic import BaseModel, ValidationError\nimport asyncio\nimport json\n\nclass Event(BaseModel):\n    timestamp: datetime\n    service: str\n    level: str\n\nasync def stream_counts(source, window_seconds: int):\n    window = deque()  # stores (ts, key) where key = (service, level)\n    counts = defaultdict(int)\n    interval = 1.0\n    last_emit = asyncio.get_event_loop().time()\n\n    async for line in source:\n        try:\n            obj = json.loads(line)\n            obj[\"timestamp\"] = datetime.fromisoformat(obj[\"timestamp\"]).astimezone(timezone.utc)\n            ev = Event(**obj)\n        except (ValueError, ValidationError):\n            continue\n        ts = ev.timestamp.timestamp()\n        key = (ev.service, ev.level)\n        window.append((ts, key))\n        counts[key] += 1\n\n        # evict old entries\n        cutoff = datetime.utcnow().timestamp() - window_seconds\n        while window and window[0][0] < cutoff:\n            old_ts, old_key = window.popleft()\n            counts[old_key] -= 1\n            if counts[old_key] == 0:\n                del counts[old_key]\n\n        now = asyncio.get_event_loop().time()\n        if now - last_emit >= interval:\n            last_emit = now\n            yield dict(counts)\n```\n\n## Follow-up Questions\n- How would you adapt this to multi-process workers if the event stream is extremely high volume?\n- How would you add backpressure signaling to the producer when the window grows too large?","diagram":null,"difficulty":"intermediate","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Google","Robinhood","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T13:34:12.135Z","createdAt":"2026-01-20T13:34:12.135Z"},{"id":"q-4845","question":"Design a streaming binary frame decoder in Python that reads from an asyncio stream with length-prefixed frames: each frame starts with a 4-byte big-endian length, then 1-byte type, 2-byte version, then payload; implement a registry of per-type parsers, support runtime hot-swapping of a parser, ensure frames are decoded in-order, and backpressure is applied via a bounded asyncio.Queue; include a minimal example of type 0x01 v1 and a runtime switch to a new v2 parser?","answer":"Design an async binary frame decoder: read 4-byte big-endian length, then 1-byte type + 2-byte version, followed by payload. Maintain a (type,version) parser registry; parse payload into dict and push","explanation":"## Why This Is Asked\n\nTests practical streaming, memory boundaries, and runtime hot-swap safety.\n\n## Key Concepts\n\n- Async framing and buffering\n- Per-type/version parser registry with hot-swap\n- Backpressure via bounded queues\n- Concurrency safety with locks\n\n## Code Example\n\n```python\n# simplified sketch\n```\n\n## Follow-up Questions\n\n- How would you handle partial frames and framing errors?\n- How to measure backpressure impact in production?\n","diagram":"flowchart TD\n  A[AsyncReader] --> B[Frame Framing]\n  B --> C[Registry lookup]\n  C --> D[Parse payload]\n  D --> E[Queue downstream]","difficulty":"advanced","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Discord","MongoDB","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T16:06:39.744Z","createdAt":"2026-01-20T16:06:39.744Z"},{"id":"q-4876","question":"You have an NDJSON stream of user events; each line is a JSON object that should contain user_id (str), email (str), action (str), ts (int). Write a function sanitize_events(stream) that reads an iterable of JSON strings, validates shape, skips invalid lines, masks the email (mask the local part to asterisks except first and last chars) and redacts any 'phone' field if present, then yields sanitized dicts. Must be memory-efficient (O(1) extra) and suitable for streaming gigabytes of data. Include a simple test plan with edge cases?","answer":"I would implement sanitize_events(stream) as a streaming generator: for line in stream, json.loads; validate user_id (str), email (str), action (str), ts (int); skip invalid. Mask email local part: ke","explanation":"## Why This Is Asked\nTests ability to design a streaming data pipeline with on-the-fly validation and data sanitization without loading all data in memory.\n\n## Key Concepts\n- Streaming generators\n- Input validation and error handling\n- PII masking strategies\n- In-place data transformation with O(1) extra memory\n\n## Code Example\n```javascript\ndef sanitize_events(stream):\n    import json\n    for line in stream:\n        try:\n            obj = json.loads(line)\n        except Exception:\n            continue\n        if not isinstance(obj, dict):\n            continue\n        if not all(k in obj for k in (\"user_id\",\"email\",\"action\",\"ts\")):\n            continue\n        if not isinstance(obj[\"user_id\"], str):\n            continue\n        if not isinstance(obj[\"email\"], str):\n            continue\n        if not isinstance(obj[\"action\"], str):\n            continue\n        if not isinstance(obj[\"ts\"], int):\n            continue\n        email = obj[\"email\"]\n        local, _, domain = email.partition(\"@\")\n        if len(local) <= 2:\n            masked_local = \"*\" * len(local)\n        else:\n            masked_local = local[0] + \"*\" * (len(local) - 2) + local[-1]\n        masked_email = masked_local + \"@\" + domain\n        out = {\"user_id\": obj[\"user_id\"], \"email\": masked_email, \"action\": obj[\"action\"], \"ts\": obj[\"ts\"]}\n        if \"phone\" in obj:\n            out[\"phone\"] = \"REDACTED\"\n        yield out\n```","diagram":"flowchart TD\nA[Start] --> B[Read line]\nB --> C[Parse JSON]\nC --> D{Valid?}\nD -->|No| E[Skip line]\nD -->|Yes| F[Mask email]\nF --> G[Redact phone]\nG --> H[Emit sanitized dict]\nH --> A","difficulty":"beginner","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Databricks","Discord","LinkedIn"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T17:05:27.142Z","createdAt":"2026-01-20T17:05:27.142Z"},{"id":"q-4942","question":"Implement an async Python generator ingest(url, window_seconds, max_in_flight) that streams a gzip-compressed NDJSON payload over HTTP, decompresses on the fly, validates each line with a Pydantic model Event(id: str, ts: int, payload: dict), and deduplicates by id within a sliding window. It must be memory-bounded (no large buffering), apply backpressure via max_in_flight, and yield only the first occurrence per id in the window. Include a concrete test plan?","answer":"Use aiohttp to stream a gzip NDJSON, decompressing as data arrives; validate each line with Pydantic Event; maintain a per-id timestamp map and prune entries older than window_seconds; emit only the f","explanation":"## Why This Is Asked\n\nAssesses streaming I/O, in-flight backpressure, and in-memory dedup logic under memory constraints.\n\n## Key Concepts\n\n- Async HTTP streaming with aiohttp\n- On-the-fly gzip decompression\n- Pydantic model validation\n- Sliding-window deduplication with pruning\n- Bounded backpressure via asyncio.Queue\n\n## Code Example\n\n```python\n# sketch of types and flow (not full implementation)\nfrom pydantic import BaseModel\n\nclass Event(BaseModel):\n    id: str\n    ts: int\n    payload: dict\n\nasync def ingest(url: str, window_seconds: int, max_in_flight: int):\n    pass  # implementation would stream, validate, deduplicate, yield\n```\n\n## Follow-up Questions\n\n- How would clock drift affect dedup windowing?\n- How would you test behavior under bursty traffic with backpressure?","diagram":null,"difficulty":"intermediate","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Airbnb","Apple","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T20:32:24.307Z","createdAt":"2026-01-20T20:32:24.307Z"},{"id":"q-5005","question":"Design an **asynchronous** Python **aggregator** that ingests **NDJSON** lines from multiple producers (each line includes a **source** field) and emits deduplicated, in-order results downstream. Implement **1-second** tumbling windows for per-source dedup, memory-bounded streaming, and bound backpressure via per-source **asyncio.Queues**; ensure fair scheduling between sources?","answer":"To implement this asynchronous aggregator, create a Dispatcher that assigns incoming NDJSON lines to per-source queues (maxsize N). Maintain a time-based deduplication map: for each source, keep a deque of seen content hashes with timestamps; drop a line if its hash exists within the current 1-second window. Use asyncio.gather() with round-robin scheduling to ensure fair processing across sources, and implement bounded queues to apply backpressure when downstream processing slows.","explanation":"## Why This Is Asked\nThis question tests the ability to design a streaming architecture that handles backpressure, maintains deterministic per-source ordering, and provides runtime memory guarantees under high contention scenarios.\n\n## Key Concepts\n- Asyncio for concurrent processing and per-source bounded queues\n- Round-robin scheduling for fair source processing\n- 1-second tumbling windows for time-based deduplication\n- Backpressure management via bounded queues and shared downstream sink\n- Memory-bounded streaming with time-based data structures\n\n## Code Example\n```python\nimport asyncio\nimport hashlib\nimport json\nfrom collections import deque\nfrom datetime import datetime, timedelta\n\nclass AsyncAggregator:\n    def __init__(self, max_queue_size=1000, window_seconds=1):\n        self.queues = {}\n        self.max_queue_size = max_queue_size\n        self.window_seconds = window_seconds\n        self.dedup_maps = {}\n        self.running = False\n    \n    async def add_source(self, source_id):\n        self.queues[source_id] = asyncio.Queue(maxsize=self.max_queue_size)\n        self.dedup_maps[source_id] = deque()\n    \n    def _get_hash(self, line_data):\n        content = json.dumps(line_data, sort_keys=True)\n        return hashlib.md5(content.encode()).hexdigest()\n    \n    def _is_duplicate(self, source_id, line_hash, timestamp):\n        dedup_map = self.dedup_maps[source_id]\n        \n        # Remove old entries outside window\n        while dedup_map and (timestamp - dedup_map[0][1]).total_seconds() > self.window_seconds:\n            dedup_map.popleft()\n        \n        # Check for duplicate\n        for existing_hash, _ in dedup_map:\n            if existing_hash == line_hash:\n                return True\n        \n        dedup_map.append((line_hash, timestamp))\n        return False\n    \n    async def ingest_line(self, line_data):\n        source_id = line_data.get('source')\n        if source_id not in self.queues:\n            await self.add_source(source_id)\n        \n        try:\n            await self.queues[source_id].put(line_data)\n        except asyncio.QueueFull:\n            # Apply backpressure - could log or handle differently\n            pass\n    \n    async def process_source(self, source_id):\n        queue = self.queues[source_id]\n        \n        while self.running or not queue.empty():\n            try:\n                line_data = await asyncio.wait_for(queue.get(), timeout=0.1)\n                \n                line_hash = self._get_hash(line_data)\n                timestamp = datetime.now()\n                \n                if not self._is_duplicate(source_id, line_hash, timestamp):\n                    await self.emit_downstream(line_data)\n                \n                queue.task_done()\n            except asyncio.TimeoutError:\n                continue\n    \n    async def emit_downstream(self, line_data):\n        # Implementation for downstream processing\n        pass\n    \n    async def start(self):\n        self.running = True\n        \n        # Start processing tasks for each source\n        tasks = []\n        for source_id in self.queues:\n            task = asyncio.create_task(self.process_source(source_id))\n            tasks.append(task)\n        \n        # Use round-robin scheduling via asyncio.gather\n        await asyncio.gather(*tasks)\n    \n    async def stop(self):\n        self.running = False\n```","diagram":null,"difficulty":"advanced","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Anthropic","Apple","Coinbase"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-21T05:37:24.448Z","createdAt":"2026-01-20T23:37:56.110Z"},{"id":"q-5027","question":"Write a Python generator function stream_top_k_on_score(file_path, k) that reads a JSONL file where each line is a JSON object with fields 'user_id' (string) and 'score' (float). Ignore lines that are not valid JSON or lack fields. Maintain a min-heap of size k to track the top k by score (tie-break by smaller user_id). After processing each valid line, yield the current top-k as a list of (user_id, score) sorted by descending score?","answer":"Maintain a min-heap of (score, user_id) with a maximum size of k. Read the file line by line, parsing each line with json.loads() within a try/except block to skip invalid JSON. For valid entries with both 'user_id' and 'score' fields, add them to the heap using these rules: if the heap size is less than k, push the new entry; otherwise, replace the root if the new score is higher than the minimum score, or if scores are equal but the new user_id is lexicographically smaller. After processing each valid line, yield the current top-k results sorted by descending score and then by user_id.","explanation":"## Why This Is Asked\nThis problem tests your ability to implement streaming algorithms with bounded memory while providing real-time updates. It's a common pattern in data processing pipelines where you need to maintain top-k results without loading entire datasets into memory.\n\n## Key Concepts\n- **Generator patterns**: Yielding results incrementally enables memory-efficient processing\n- **Heap operations**: Using heapq provides O(log k) insertions and maintains the top-k efficiently\n- **Error handling**: Robust JSON parsing ensures the pipeline continues with malformed data\n- **Deterministic tie-breaking**: Consistent sorting order is crucial for reproducible results\n\n## Code Example\n```python\nimport json\nimport heapq\n\ndef stream_top_k_on_score(file_path, k):\n    \"\"\"Stream top-k results by score from a JSONL file.\n    \n    Args:\n        file_path: Path to JSONL file with user_id and score fields\n        k: Number of top results to maintain\n        \n    Yields:\n        List of (user_id, score) tuples sorted by descending score\n    \"\"\"\n    heap = []  # min-heap storing (score, user_id)\n    \n    with open(file_path, 'r', encoding='utf-8') as f:\n        for line in f:\n            try:\n                data = json.loads(line)\n                user_id = str(data['user_id'])\n                score = float(data['score'])\n                \n                # Maintain heap of size k\n                if len(heap) < k:\n                    heapq.heappush(heap, (score, user_id))\n                elif score > heap[0][0] or (score == heap[0][0] and user_id < heap[0][1]):\n                    heapq.heapreplace(heap, (score, user_id))\n                    \n            except (json.JSONDecodeError, KeyError, ValueError, TypeError):\n                continue  # Skip malformed lines\n            \n            # Yield sorted results: descending score, then ascending user_id\n            yield [(uid, sc) for sc, uid in sorted(heap, key=lambda x: (-x[0], x[1]))]\n```","diagram":"flowchart TD\n  A[Start] --> B[Open file]\n  B --> C[Read line]\n  C --> D{Valid JSON?}\n  D -- Yes --> E[Parse fields]\n  E --> F[Update heap]\n  F --> G[Yield top-k]\n  G --> C\n  D -- No --> H[Skip line]\n  H --> C","difficulty":"beginner","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Amazon","Snap","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-21T05:21:23.237Z","createdAt":"2026-01-21T02:25:38.011Z"},{"id":"q-503","question":"How would you implement a distributed rate limiter using Redis with sliding window algorithm to handle 10,000 requests per second across multiple API servers?","answer":"Use Redis sorted sets with timestamps as scores and request IDs as members. For each request, ZADD current timestamp, ZREMRANGEBYSCORE to remove old entries, then ZCARD to count. Use Lua script for atomic operations to prevent race conditions.","explanation":"## Implementation\n\nUse Redis sorted sets with timestamps as scores:\n\n```python\n# Lua script for atomic rate limiting\nlocal key = KEYS[1]\nlocal window = tonumber(ARGV[1])\nlocal limit = tonumber(ARGV[2])\nlocal now = tonumber(ARGV[3])\n\nredis.call('zadd', key, now, now)\nredis.call('zremrangebyscore', key, 0, now - window)\nlocal count = redis.call('zcard', key)\nredis.call('expire', key, window)\n\nreturn count <= limit\n```\n\n## Key Considerations\n\n- **Atomicity**: Lua script prevents race conditions\n- **Memory**: Sorted sets automatically clean old entries\n- **Performance**: O(log N) operations, handles 10,000+ RPS\n- **Scalability**: Single Redis instance can serve multiple API servers\n- **Sliding Window**: Provides accurate rate limiting over time windows","diagram":"flowchart TD\n  A[API Request] --> B[Redis Lua Script]\n  B --> C{Within Limit?}\n  C -->|Yes| D[Process Request]\n  C -->|No| E[Return 429]\n  F[Cleanup Old Entries] --> B\n  G[Set TTL] --> B","difficulty":"advanced","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","LinkedIn","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":["redis","sliding window","sorted sets","lua script","rate limiting","distributed"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2026-01-09T03:44:22.264Z","createdAt":"2025-12-26 12:51:06"},{"id":"q-5082","question":"You're given a huge CSV of transactions with columns: user_id, amount, timestamp, status. Implement top_n_users_by_amount(file_path, n) that streams the file (first line is header) and returns a list of (user_id, total_amount) for the top N users by total amount. Do not load entire file; keep memory O(N). Handle invalid rows gracefully and use Decimal for precision. Explain tie handling?","answer":"I would stream the CSV with csv.reader, accumulate per-user totals in a dict, and maintain a max-heap of (total, user_id). After processing all lines, pop valid entries to form the top-N. Use Decimal ","explanation":"## Why This Is Asked\nShows streaming data handling, memory-conscious top-k tracking, and exact arithmetic with Decimal.\n\n## Key Concepts\n- Streaming I/O with csv\n- Memory-efficient top-k using a heap\n- Decimal for monetary values\n\n## Code Example\n```python\nfrom decimal import Decimal\nimport csv\nimport heapq\nfrom collections import defaultdict\n\ndef top_n_users_by_amount(file_path, n):\n    totals = defaultdict(Decimal)\n    heap = []  # max-heap via negation\n\n    with open(file_path, newline='') as f:\n        reader = csv.reader(f)\n        header = next(reader, None)\n        if not header:\n            return []\n        try:\n            uid_idx = header.index('user_id')\n            amt_idx = header.index('amount')\n        except ValueError:\n            raise ValueError('Required columns missing')\n\n        for row in reader:\n            if len(row) <= max(uid_idx, amt_idx):\n                continue\n            try:\n                uid = row[uid_idx]\n                amt = Decimal(row[amt_idx])\n            except Exception:\n                continue\n            totals[uid] += amt\n            heapq.heappush(heap, (totals[uid], uid))\n\n    # extract top-N using lazy validation\n    result = []\n    seen = set()\n    while heap and len(result) < n:\n        total, uid = heapq.heappop(heap)\n        if uid in seen:\n            continue\n        if totals[uid] == total:\n            result.append((uid, total))\n            seen.add(uid)\n    return sorted(result, key=lambda x: x[1], reverse=True)\n```\n\n## Follow-up Questions\n- How would you adapt this for a streaming API (no local file)?\n- How to handle very large n relative to distinct users?\n- What if amounts use different currencies?","diagram":"flowchart TD\n  A[Stream CSV] --> B[Parse header]\n  B --> C[Identify indices for user_id, amount]\n  C --> D[Accumulate totals per user]\n  D --> E[Push to max-heap on update]\n  E --> F[Pop to obtain top-N]\n  F --> G[Return sorted top-N]","difficulty":"beginner","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Anthropic","Goldman Sachs","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T05:43:19.118Z","createdAt":"2026-01-21T05:43:19.118Z"},{"id":"q-5160","question":"Design a Python function merge_sorted_csvs(file_paths, key) that merges multiple CSV files, each sorted by a given column key, into a single, generator-based stream ordered by that key. Do this without loading all data into memory. Use a min-heap to pull the next row from the active files and yield dict rows lazily. Assume headers are identical across files and handle exhausted files cleanly?","answer":"Use a generator that opens all files with csv.DictReader, seed a min-heap with (row[key], file_index, row, reader), then repeatedly pop the smallest, yield row, and push the next row from that file. S","explanation":"## Why This Is Asked\nTests ability to implement streaming merge with memory constraints, using heapq, generators, and csv handling.\n\n## Key Concepts\n- Generators and lazy I/O\n- heapq based k-way merge\n- Memory efficiency with O(k) buffers\n- CSV handling with DictReader\n\n## Code Example\n```python\nimport csv, heapq\n\ndef merge_sorted_csvs(file_paths, key):\n    files = [open(p, 'r', newline='') for p in file_paths]\n    try:\n        readers = [csv.DictReader(f) for f in files]\n        heap = []\n        for i, rdr in enumerate(readers):\n            try:\n                row = next(rdr)\n                val = row.get(key)\n                if val is None:\n                    continue\n                heapq.heappush(heap, (val, i, row, rdr))\n            except StopIteration:\n                pass\n        while heap:\n            v, i, row, rdr = heapq.heappop(heap)\n            yield row\n            try:\n                row = next(rdr)\n                val = row.get(key)\n                if val is None:\n                    continue\n                heapq.heappush(heap, (val, i, row, rdr))\n            except StopIteration:\n                pass\n    finally:\n        for f in files:\n            f.close()\n```\n\n## Follow-up Questions\n- How to handle numeric keys and ensure proper type sorting?\n- How to test at scale with many files and perf constraints?","diagram":"flowchart TD\n  A[Open files] --> B[Prime heap with first rows]\n  B --> C{Heap not empty}\n  C -->|Yes| D[Pop smallest row]\n  D --> E[Yield row]\n  E --> F[Advance that file]\n  F --> G{More rows?}\n  G -->|Yes| H[Push next row]\n  G -->|No| I[Close file]\n  H --> C\n  I --> J[Done]","difficulty":"beginner","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Databricks","Hashicorp","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T08:52:40.352Z","createdAt":"2026-01-21T08:52:40.352Z"},{"id":"q-5211","question":"Design an asynchronous Python NDJSON pipeline that ingests lines from an AsyncIterable[str], validates each line against a runtime-swappable Pydantic registry keyed by (type,version), applies a version-aware transform, and writes to a sink with exactly-once semantics using a Redis-like idempotency store. Preserve per-key in-order processing with per-key asyncio.Lock and a bounded queue; provide a test scaffold showing v1→v2 migration?","answer":"Design an asynchronous NDJSON pipeline with a dynamic Pydantic registry keyed by (type,version) swapped at runtime under a lock; validate each line, apply a version-aware transform, and write to a sin","explanation":"## Why This Is Asked\nTests ability to design dynamic schemas, runtime hot swaps, strict per-key ordering, and idempotent sinks in a streaming Python pipeline, mirroring real-world data platforms at scale.\n\n## Key Concepts\n- Async NDJSON streaming\n- Runtime schema registry with atomic swap\n- Version-aware transformations\n- Per-key ordering and backpressure\n- Exactly-once semantics with idempotency store\n\n## Code Example\n\n```python\n# skeleton illustrating core contracts\nfrom typing import AsyncIterable, Dict, Tuple\nfrom pydantic import BaseModel\nimport asyncio\n\nclass Registry:\n    def __init__(self):\n        self._lock = asyncio.Lock()\n        self._schemas: Dict[Tuple[str, int], type[BaseModel]] = {}\n\n    async def swap(self, key: Tuple[str, int], new_model: type[BaseModel]):\n        async with self._lock:\n            self._schemas[key] = new_model\n\n    def get(self, key: Tuple[str, int]):\n        return self._schemas.get(key)\n```\n\n## Follow-up Questions\n- How would you test correctness under high contention?\n- How would you adapt for out-of-order arrivals and retries?","diagram":null,"difficulty":"advanced","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Netflix","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T10:58:32.690Z","createdAt":"2026-01-21T10:58:32.690Z"},{"id":"q-5228","question":"You're building a microservice that fetches product catalog data from multiple partner APIs. Implement an asynchronous fetcher in Python that accepts a list of (domain, endpoint) pairs and returns a per-domain rate-limited, memory-bounded stream of results. Requirements: per-domain concurrency limit (e.g., 2 concurrent requests per domain), robust retry with exponential backoff and jitter for transient HTTP errors, a timeout cap for the global operation, and a memory-friendly aggregator that yields each product as soon as arrived (no full in-memory merge). Provide a minimal runnable example using httpx or aiohttp and a simple test plan with simulated domains and failure modes?","answer":"Use per-domain concurrency with asyncio.Semaphore to cap parallel calls (e.g., 2 per domain). Implement fetch with httpx in a retry loop with exponential backoff and jitter; cap retries at 5. Enforce ","explanation":"## Why This Is Asked\nTests ability to design robust async data pipelines with domain-aware throttling, error handling, and memory safety in Python.\n\n## Key Concepts\n- Async IO and concurrency control per domain\n- Backoff and jitter for retries\n- Timeouts and cancellation handling\n- Streaming results to avoid loading all data in memory\n\n## Code Example\n```python\nimport asyncio\nimport httpx\nfrom collections import defaultdict\n\nasync def fetch_with_retry(client, url, retries=5, base=0.2, cap=8.0):\n    for i in range(retries + 1):\n        try:\n            resp = await client.get(url, timeout=10)\n            resp.raise_for_status()\n            return resp.json()\n        except Exception:\n            if i == retries:\n                raise\n            delay = min(cap, base * (2 ** i))\n            delay += (delay * (0.5 * asyncio.get_event_loop().time()) / 1000)\n            await asyncio.sleep(delay)\n\nasync def fetcher(pairs, per_domain=2, queue_max=100):\n    semaphores = defaultdict(lambda: asyncio.Semaphore(per_domain))\n    async with httpx.AsyncClient() as client:\n        async def worker(domain, endpoint):\n            url = f\"{domain}{endpoint}\"\n            async with semaphores[domain]:\n                data = await fetch_with_retry(client, url)\n                await out_queue.put(data)\n\n        out_queue = asyncio.Queue(maxsize=queue_max)\n        tasks = [asyncio.create_task(worker(d, e)) for d, e in pairs]\n        while any(not t.done() for t in tasks) or not out_queue.empty():\n            if out_queue.qsize():\n                yield await out_queue.get()\n            else:\n                await asyncio.sleep(0.01)\n        for t in tasks:\n            if not t.done():\n                t.cancel()\n```\n\n## Follow-up Questions\n- How would you test backpressure under burst traffic? \n- How would you adapt per-domain limits if domains vary in latency?","diagram":"flowchart TD\n  A[Input: (domain, endpoint) pairs] --> B[Domain Scheduler]\n  B --> C[HTTP Fetch + Retry]\n  C --> D[Parse & Yield]\n  D --> E[Consumer Stream]","difficulty":"intermediate","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":null,"companies":["IBM","Instacart"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T11:38:13.683Z","createdAt":"2026-01-21T11:38:13.683Z"},{"id":"q-5317","question":"Implement a Python function streaming_percentile(stream, p, buckets=2048, min_latency=0, max_latency=5000) that processes a continuous stream where each line is 'timestamp latency_ms'. It must maintain a memory-bounded histogram to estimate the p-th percentile in real time, clip out-of-range latencies, support reset, and expose get_percentile() to fetch current estimate without advancing the stream. Provide a minimal runnable example and a test plan?","answer":"Maintain a fixed-size histogram over latency values to estimate a percentile in real time. For each input line, parse latency_ms, clamp to [min_latency, max_latency], map to a bucket, and increment. T","explanation":"## Why This Is Asked\nStreaming metrics need memory-bounded, fast percentiles without storing all samples. An approximate histogram balances accuracy and resource use.\n\n## Key Concepts\n- Streaming histograms for percentile estimation\n- Memory-bounded design with fixed buckets\n- Out-of-range clipping and reset support\n- Real-time read of current percentile without consuming data\n\n## Code Example\n```python\nclass StreamingPercentile:\n    def __init__(self, p, buckets=2048, min_latency=0, max_latency=5000):\n        self.p = p\n        self.buckets = buckets\n        self.min_latency = min_latency\n        self.max_latency = max_latency\n        self.counts = [0] * buckets\n        self.total = 0\n\n    def _bucket(self, value):\n        if value <= self.min_latency:\n            return 0\n        if value >= self.max_latency:\n            return self.buckets - 1\n        scale = (value - self.min_latency) / (self.max_latency - self.min_latency)\n        return int(scale * self.buckets)\n\n    def update(self, line):\n        parts = line.strip().split()\n        if len(parts) < 2:\n            return\n        try:\n            latency = float(parts[1])\n        except:\n            return\n        idx = self._bucket(latency)\n        self.counts[idx] += 1\n        self.total += 1\n\n    def get_percentile(self, p=None):\n        if p is None:\n            p = self.p\n        target = max(0.0, min(1.0, p))\n        if self.total == 0:\n            return None\n        threshold = self.total * target\n        acc = 0\n        for i, c in enumerate(self.counts):\n            acc += c\n            if acc >= threshold:\n                # rough back-mapping to a latency value\n                frac = (threshold - (acc - c)) / max(1, c)\n                return self.min_latency + (i + frac) * (self.max_latency - self.min_latency) / self.buckets\n        return self.max_latency\n\n    def reset(self):\n        self.counts = [0] * self.buckets\n        self.total = 0\n```\n\n## Follow-up Questions\n- How does bucket count affect accuracy and latency range changes?\n- How would you adapt this for dynamic min/max ranges?\n- How would you test stability under burst traffic or skewed distributions?","diagram":"flowchart TD\n  A[Start] --> B[Stream input lines]\n  B --> C[Parse latency and clamp to range]\n  C --> D[Increment histogram bucket]\n  D --> E[On demand: compute percentile]\n  E --> F[Reset or continue streaming]\n  F --> G[End]","difficulty":"intermediate","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Hashicorp","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T17:10:04.261Z","createdAt":"2026-01-21T17:10:04.261Z"},{"id":"q-5382","question":"Design a streaming normalizer that accepts records as either dicts with keys 'id' and 'value' or objects implementing a Protocol with properties id and value. Implement a memory-efficient transformer that yields canonical dicts {'id': id, 'value': value} in input order without materializing all records. Use typing.Protocol to define the interface, and provide a minimal test showing both input shapes producing identical CSV output?","answer":"Use a generator normalize_records(source) with a Protocol: class RecordLike(Protocol): id: object; value: object. For each item, if isinstance(item, dict): id = item['id']; value = item['value']; else","explanation":"## Why This Is Asked\nTests memory-aware data normalization and duck typing through Protocols, ensuring streaming without full materialization.\n\n## Key Concepts\n- typing.Protocol for structural typing\n- runtime type handling for dict vs object\n- generator-based streaming with O(1) extra memory\n- minimal, deterministic tests comparing inputs across shapes\n\n## Code Example\n```python\nfrom typing import Protocol, Iterable, Dict\n\nclass RecordLike(Protocol):\n    id: object\n    value: object\n\ndef normalize_records(source: Iterable[RecordLike]) -> Iterable[Dict[str, object]]:\n    for item in source:\n        if isinstance(item, dict):\n            yield {\"id\": item[\"id\"], \"value\": item[\"value\"]}\n        else:\n            yield {\"id\": getattr(item, \"id\"), \"value\": getattr(item, \"value\")}\n```\n\n## Follow-up Questions\n- How would you handle missing keys/attributes gracefully?\n- How does this scale with very large streams and multiple producers?","diagram":null,"difficulty":"intermediate","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Google","IBM"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T19:55:31.775Z","createdAt":"2026-01-21T19:55:31.775Z"},{"id":"q-5403","question":"Design an asynchronous Python windowed VWAP aggregator for an incoming JSONL stream where each line is {'symbol':'A','price':float,'qty':float,'ts':int(ms)}; compute per-symbol, per-minute VWAP and emit updated windows to a downstream sink. Handle out-of-order data with watermarks, bound memory by trimming old windows, and allow live config to disable symbols. Provide a minimal test scaffold?","answer":"Implement an async VWAP aggregator with per-symbol, per-minute windows. Parse each JSONL line to a TradeEvent(symbol, price, qty, ts). Maintain per-symbol aggregates: sum_pq, sum_q, window_start. Use ","explanation":"## Why This Is Asked\nTests ability to implement streaming aggregation under real-time constraints, including out-of-order handling, memory budgeting, and dynamic config.\n\n## Key Concepts\n- Async streaming and per-symbol windowing\n- Watermarks and late data handling\n- Memory budgeting and backpressure to sinks\n\n## Code Example\n```python\nfrom collections import defaultdict\nfrom dataclasses import dataclass\nfrom typing import AsyncIterable, Dict\n\n@dataclass\nclass TradeEvent:\n    symbol: str\n    price: float\n    qty: float\n    ts: int\n\nclass VWAPState:\n    def __init__(self):\n        self.windows: Dict[str, Dict[int, Dict[str, float]]] = defaultdict(lambda: defaultdict(lambda: {'sum_pq': 0.0, 'sum_q': 0.0}))\n\ndef compute_vwap(sum_pq: float, sum_q: float) -> float:\n    return sum_pq / sum_q if sum_q else 0.0\n\nasync def aggregate(stream: AsyncIterable[str], window_ms: int = 60_000, cap_bytes: int = 10**7):\n    state = VWAPState()\n    async for line in stream:\n        e = parse_json_trade(line)  # user implements robust parser\n        w = (e.ts // window_ms) * window_ms\n        s = state.windows[e.symbol][w]\n        s['sum_pq'] += e.price * e.qty\n        s['sum_q'] += e.qty\n        # memory cap and emit logic omitted for brevity\n        yield {e.symbol: compute_vwap(s['sum_pq'], s['sum_q'])}\n```\n\n## Follow-up Questions\n- How would you scale this across multiple processes?\n- How would you enforce exactly-once delivery to the sink?\n","diagram":null,"difficulty":"advanced","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Coinbase","Discord","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T20:59:35.814Z","createdAt":"2026-01-21T20:59:35.814Z"},{"id":"q-5457","question":"Implement a Python function process_events_gz(file_path, target_event='purchase', max_users=10000) that streams a gzipped log file without loading it all, counts per-user target_event occurrences in 1-minute tumbling windows, and yields (window_start_iso, user_id, count) only when a window completes. Ensure memory usage stays within max_users and design a test plan with skewed user distribution and simulated slow sinks. How would you approach this?","answer":"Stream with gzip.open and a line-by-line CSV parser. Maintain a dict: user_id -> {window_start: count}. For each line, map timestamp to window_start, increment counts for target_event. When a window ends, yield (window_start_iso, user_id, count) and clean up to stay within max_users.","explanation":"## Why This Is Asked\n\nThis question checks practical streaming with bounded memory, real-time bucketing, and backpressure handling using Python—skills relevant at Google/Oracle.\n\n## Key Concepts\n\n- Streaming I/O (gzip.open), line parsing\n- Per-user 1-minute windows\n- Bounded memory with eviction (max_users)\n- Yielding completed windows for downstream processing\n- Trade-offs: latency vs memory\n\n## Code Example\n\n```python\nimport gzip, csv\n\ndef process_events_gz(file_path, target_event='purchase', max_users=10000):\n    # sketch implementing the approach\n    pass\n```\n\n## Follow-up Questions\n\n- How","diagram":"flowchart TD\n  A[Open gzip stream] --> B[Read lines]\n  B --> C[Parse user_id, ts, event]\n  C --> D[Compute window_start(ts)]\n  D --> E[Increment per-user per-window count]\n  E --> F[If window ends] --> G[Yield (start, user, count)]\n  F --> H[Prune evicted users via LRU]\n  H --> I[Back to A]","difficulty":"beginner","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Google","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T05:54:43.103Z","createdAt":"2026-01-21T22:58:08.374Z"},{"id":"q-5520","question":"Design a Python async micro-batcher that ingests NDJSON lines from an async source, batches by the 'symbol' key into micro-batches of at most N lines or T seconds, validates each line with a Pydantic model, computes per-batch moving average price, and emits results to a downstream sink. Must ensure per-symbol in-order processing, bounded memory, and backpressure to the source?","answer":"Implement a central dispatcher that maintains a map of per-symbol worker coroutines. Each worker consumes from a bounded asyncio.Queue of lines for its symbol, batches up to N items or T seconds, vali","explanation":"## Why This Is Asked\n\nTests ability to design a scalable, asyncio-based streaming pipeline with per-key concurrency, in-order guarantees, and backpressure.\n\n## Key Concepts\n\n- Asyncio queues with bounded capacity\n- Per-key (symbol) in-order processing\n- Time- and size-based batching\n- Pydantic validation and surface of errors without breaking flow\n\n## Code Example\n\n```python\nfrom typing import AsyncIterable\nfrom pydantic import BaseModel\n\nclass Tick(BaseModel):\n    symbol: str\n    price: float\n    ts: float\n```\n\n```python\n# Skeleton: dispatcher creates per-symbol workers, batches, emits to sink\n```\n\n## Follow-up Questions\n\n- How would you handle schema evolution where fields can be added/removed across versions?\n- How would you test backpressure behavior end-to-end without real network IO?","diagram":null,"difficulty":"advanced","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":null,"companies":["DoorDash","Microsoft","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T04:22:18.324Z","createdAt":"2026-01-22T04:22:18.324Z"},{"id":"q-559","question":"You're building a data processing pipeline that needs to handle large CSV files efficiently. How would you implement a memory-efficient solution using Python generators to process files that don't fit in RAM?","answer":"Use Python's `csv` module with generator functions to read files line-by-line. Implement `yield` in a custom generator that processes chunks, avoiding loading entire files. Combine with `itertools.islice` for batch processing.","explanation":"## Memory-Efficient CSV Processing\n\n- **Generator Pattern**: Use `yield` to process rows incrementally\n- **csv.reader**: Built-in CSV parser handles edge cases\n- **Chunk Processing**: Batch rows with `itertools.islice`\n- **Memory Control**: Constant O(1) memory usage\n\n```python\nimport csv\nfrom itertools import islice\n\ndef csv_generator(filename, chunk_size=1000):\n    with open(filename) as f:\n        reader = csv.DictReader(f)\n        while True:\n            chunk = list(islice(reader, chunk_size))\n            if not chunk:\n                break\n            yield chunk\n```\n\n## Key Benefits\n\n- Processes files of any size without memory constraints\n- Maintains constant memory usage regardless of file size\n- Allows streaming data transformation and analysis\n- Integrates seamlessly with existing Python data tools","diagram":"flowchart TD\n  A[Large CSV File] --> B[csv.reader]\n  B --> C[Generator Function]\n  C --> D[Process Chunk]\n  D --> E[yield Results]\n  E --> F[Next Chunk]\n  F --> D","difficulty":"intermediate","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Snap","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-08T11:56:21.596Z","createdAt":"2025-12-26 12:51:07"},{"id":"q-5597","question":"Implement an asynchronous Python processor that ingests JSON lines representing user actions from a streaming source. Each line contains user_id, action, timestamp. Build per-user sessions defined as activities within 5 minutes of each other; emit a session when the user has been idle for 5 minutes or when action == 'logout'. Ensure memory-bounded operation, in-order per user, handle late events within a bounded lateness L seconds, and apply backpressure to the source. Provide tests and discuss trade-offs?","answer":"Use asyncio with per-user state: a dict user_id -> Session{start,last_seen,events}. Ingest events in an asyncio.Queue, process in event-time order using a per-user timer/watermark. A session ends when","explanation":"## Why This Is Asked\nAssesses ability to design a real-time, per-user stateful pipeline with backpressure and late data handling.\n\n## Key Concepts\n- Event-time processing with watermarks\n- Per-user state management and memory bounds\n- Asyncio pipelines and backpressure control\n- Testing with out-of-order and late events\n\n## Code Example\n```python\nfrom dataclasses import dataclass\nfrom datetime import datetime, timedelta\nfrom typing import List, Dict\n\n@dataclass\nclass Session:\n    start: float\n    last_seen: float\n    events: List[dict]\n\ndef close_session(sess: Session):\n    return sess\n\n# Minimal skeleton illustrating session state handling; real implementation would wire timers and queues\n```\n\n## Follow-up Questions\n- How would you scale this to millions of concurrent users?\n- What persistence strategy would you use for fault tolerance and replay?","diagram":null,"difficulty":"intermediate","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Discord","Google","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T07:50:29.637Z","createdAt":"2026-01-22T07:50:29.637Z"},{"id":"q-5613","question":"Design a memory bounded Python streaming join: two NDJSON streams, trades and quotes, delivered via asynchronous file-like iterators. Each line has: timestamp ISO, symbol, and amount; build an as-of join that matches each trade to the latest quote for the same symbol within a 30 second tolerance, emitting (trade_ts, symbol, trade_price, quote_price). Ensure per-symbol buffers are bounded, maintain order by trade timestamp, validate lines with a Pydantic model, and support backpressure to the sources. Provide a test plan with late quotes and symbol skew?","answer":"Approach: implement an async generator that consumes two NDJSON streams, keeps per-symbol quote buffers limited by max_memory, and, for each trade, finds the most recent quote within tolerance (30s) t","explanation":"## Why This Is Asked\nStreaming as-of joins across two NDJSON streams test memory bounds, backpressure, and real-time correctness under skew.\n\n## Key Concepts\n- As-of join, per-symbol state, bounded buffers\n- Async IO, NDJSON parsing, Pydantic validation\n- Backpressure and ordering guarantees\n\n## Code Example\n```python\nfrom datetime import datetime, timedelta\nfrom pydantic import BaseModel\n\nclass Trade(BaseModel):\n    timestamp: datetime\n    symbol: str\n    price: float\n\nclass Quote(BaseModel):\n    timestamp: datetime\n    symbol: str\n    price: float\n\nasync def join_streams(trades, quotes, tolerance_seconds: int = 30):\n    # simplified placeholder: real implementation would maintain per-symbol buffers\n    async for t in trades:\n        yield t\n```\n\n## Follow-up Questions\n- How would you handle late quotes that arrive after a trade has been emitted?\n- How would you adapt buffers when symbols explode in distribution?\n","diagram":null,"difficulty":"intermediate","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Microsoft","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T08:46:02.341Z","createdAt":"2026-01-22T08:46:02.341Z"},{"id":"q-5706","question":"Implement a Python generator that streams a newline-delimited JSON (NDJSON) file containing lines like {\"group\": \"A\", \"hit\": 1, \"ts\": 1610000000}. Implement stream_group_traffic(file_path) as a generator that updates per-group counters (total hits) and latest timestamp, and yields a tuple (group, total_hits, latest_ts) after processing every line. Ensure memory usage stays O(number_of_groups) and the processing is deterministic per group. Provide edge-case handling for malformed lines?","answer":"Iterate file line by line with json.loads; maintain state dict: groups[group] = {'count': 0, 'latest_ts': 0}. For each line, increment count, set latest_ts, yield (group, count, latest_ts). If line in","explanation":"## Why This Is Asked\n\nTests the ability to design streaming stateful processing with per-key aggregation and memory considerations. It requires deterministic per-group updates and basic error handling.\n\n## Key Concepts\n\n- Streaming I/O with generators\n- Per-key aggregation and in-order semantics\n- Memory complexity O(k) where k is number of groups\n- Robust JSON parsing and error handling\n\n## Code Example\n\n```python\nimport json\n\ndef stream_group_traffic(file_path):\n    counts = {}\n    latest = {}\n    with open(file_path, 'r') as f:\n        for line in f:\n            try:\n                rec = json.loads(line)\n                g = rec['group']; c = int(rec['hit']); ts = int(rec['ts'])\n            except Exception:\n                continue\n            if g not in counts:\n                counts[g] = 0\n                latest[g] = 0\n            counts[g] += c\n            latest[g] = max(latest[g], ts)\n            yield (g, counts[g], latest[g])\n```\n\n## Follow-up Questions\n\n- How would you adapt for a very large number of groups to bound memory?\n- How would you test handling of malformed lines and out-of-order timestamps?\n- How would you turn this into a consumer-friendly sink with backpressure?","diagram":"flowchart TD\n  A[NDJSON line] --> B[parse json]\n  B --> C{Group}\n  C --> D[update per-group state]\n  D --> E[emit (group, count, latest_ts)]","difficulty":"beginner","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":null,"companies":["NVIDIA","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T11:56:02.953Z","createdAt":"2026-01-22T11:56:02.953Z"},{"id":"q-588","question":"How would you implement a rate limiter using Python's asyncio to prevent API abuse while maintaining high throughput?","answer":"Use asyncio.Semaphore with token bucket algorithm. Track request timestamps in a deque, calculate wait time based on rate limit, and use asyncio.sleep() for throttling. Implement per-client limits usi","explanation":"## Rate Limiting Implementation\n\n### Token Bucket Approach\n- Use asyncio.Semaphore for concurrent request control\n- Implement token bucket with asyncio.gather() for batch processing\n- Track timestamps in collections.deque for O(1) operations\n\n### Async Pattern\n```python\nimport asyncio\nfrom collections import deque\n\nclass RateLimiter:\n    def __init__(self, rate_limit, time_window):\n        self.rate_limit = rate_limit\n        self.time_window = time_window\n        self.requests = deque()\n    \n    async def acquire(self):\n        now = asyncio.get_event_loop().time()\n        # Remove old requests\n        while self.requests and self.requests[0] <= now - self.time_window:\n            self.requests.popleft()\n        \n        if len(self.requests) >= self.rate_limit:\n            sleep_time = self.time_window - (now - self.requests[0])\n            await asyncio.sleep(sleep_time)\n            return await self.acquire()\n        \n        self.requests.append(now)\n```\n\n### Production Considerations\n- Use Redis for distributed rate limiting across multiple instances\n- Implement sliding window for more accurate rate limiting\n- Add circuit breaker pattern for API protection\n- Monitor and log rate limit violations for debugging","diagram":"flowchart TD\n  A[Request] --> B{Check Rate Limit}\n  B -->|Within Limit| C[Process Request]\n  B -->|Exceeded| D[Calculate Wait Time]\n  D --> E[Async Sleep]\n  E --> B\n  C --> F[Update Request Log]\n  F --> G[Return Response]","difficulty":"intermediate","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","DoorDash","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":["asyncio","rate limiter","semaphore","token bucket","api abuse","throughput","deque"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2025-12-27T04:58:08.145Z","createdAt":"2025-12-27T01:14:34.507Z"},{"id":"q-5969","question":"Implement an AsyncRateLimiter class in Python to enforce per-user quotas of N requests per minute for async callers. Use a sliding window with 1-second buckets to bound memory, guarded by per-user asyncio.Lock. Expose acquire(user_id) coroutine that returns immediately or awaits until allowed. Include a minimal usage example and a test plan for bursty traffic and slow downstreams?","answer":"Design an AsyncRateLimiter class in Python to enforce per-user quotas of N requests per minute for async callers. Implement a sliding window with 1-second buckets to bound memory, guarded by per-user asyncio.Lock. Expose an acquire(user_id) coroutine that returns immediately or awaits until allowed. Include a minimal usage example and a test plan for bursty traffic and slow downstreams.","explanation":"This question tests knowledge of async synchronization, memory-bounded sliding windows, and per-key isolation. Look for a per-user bucket array sized to 60 seconds, efficient pruning, minimal locking, and correct handling of concurrency. Evaluate test plans for bursty input and backpressure scenarios, plus clock skew handling and bucket drift.","diagram":null,"difficulty":"intermediate","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Adobe","Instacart","Lyft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T05:02:37.931Z","createdAt":"2026-01-23T02:21:58.306Z"},{"id":"q-6003","question":"Design an async Python pipeline that reads length-delimited YAML documents over a TLS socket, validates each document against a dynamic JSON Schema retrieved from a remote registry, applies a per-version transformation, and emits JSON Lines to a sink. Constraints: bounded memory with a single bounded asyncio.Queue, per-version in-order processing, and hot-swap of schemas without restart. Include a v1→v2 migration example and runtime schema swap needs?","answer":"Implement an asyncio TLS reader framing 4-byte length prefixes, YAML.safe_load, validate with a JSON Schema fetched from registry (with ETag caching). Route by doc.version to per-version transformers,","explanation":"## Why This Is Asked\nTests streaming primitives, dynamic schemas, and hot-reload.\n\n## Key Concepts\n- Async IO, TLS streams, length framing\n- YAML parsing and JSON Schema validation\n- Dynamic schema loading and caching with ETag\n- Per-version routing and in-order guarantees\n- Bounded memory and backpressure\n- Hot schema swap without downtime\n\n## Code Example\n```python\n# skeleton: framed_reader -> validate -> route_by_version -> transform -> emit\n```\n\n## Follow-up Questions\n- How would you test memory boundedness under backpressure?\n- How to handle schema evolution that drops required fields?","diagram":null,"difficulty":"advanced","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":null,"companies":["IBM","LinkedIn","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T04:19:40.044Z","createdAt":"2026-01-23T04:19:40.044Z"},{"id":"q-6056","question":"Design an asynchronous NDJSON processor in Python that consumes NDJSON lines from an AsyncIterable[str]. Each line has fields: id (string), service (string), ts (int, epoch ms), payload (object). Guarantee exactly-once processing for each id by maintaining a bounded in-memory dedup store (LRU with TTL) and an optional spill-to-disk backend, and ensure per-id in-order emission to a downstream sink. The pipeline must respect backpressure using a bounded asyncio.Queue and support runtime swap of the sink implementation without restarting. Provide a minimal test scaffold showing a dedup store with TTL and a two-sink swap (V1 to V2)?","answer":"Approach: buffer NDJSON lines in a bounded asyncio.Queue, validate fields, and deduplicate by id with a TTL-aware LRU cache (memory-bounded, spill-to-disk optional). Emit to a downstream sink in id or","explanation":"## Why This Is Asked\n\nTests async streaming design with strong correctness guarantees (exactly-once semantics), backpressure, and runtime component swapping.\n\n## Key Concepts\n\n- Async NDJSON processing\n- TTL-bounded dedup caches with spill\n- In-order per-id emission\n- Backpressure and sink swapping\n\n## Code Example\n\n```javascript\n# Python-like pseudocode showing structure\n```\n\n## Follow-up Questions\n\n- How would you instrument and test TTL eviction impact on correctness?\n- How would you extend to multi-process workers while preserving in-order guarantees?","diagram":"flowchart TD\nA[Index] --> B[Queue]\nB --> C[Dedup & Validate]\nC --> D[Worker Emits to Sink]\nD --> E[Sink]\nE --> F[Sink Swap Point]","difficulty":"advanced","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":null,"companies":["NVIDIA","PayPal","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T07:03:44.430Z","createdAt":"2026-01-23T07:03:44.430Z"},{"id":"q-6174","question":"Implement a Python generator function stream_grouped_batches(path, key='user_id', max_batch=50, timeout=2.0) that reads an NDJSON file line by line, groups objects by the given key into per-key micro-batches, flushes a batch when it reaches max_batch or a per-key timeout, and yields (key_value, batch_list) in delivery order. Ensure per-key in-order processing and bounded memory?","answer":"Implement a Python generator function stream_grouped_batches(path, key='user_id', max_batch=50, timeout=2.0) that reads an NDJSON file line by line, groups objects by the given key into per-key micro-","explanation":"## Why This Is Asked\nStreaming per-key batches tests understanding of memory bounds, backpressure, and in-order delivery without loading whole files.\n\n## Key Concepts\n- NDJSON streaming and per-key batching\n- Generators and memory-bounded design\n- Time-based flush vs. size-based flush\n- Order guarantees and error handling\n\n## Code Example\n```python\nfrom collections import defaultdict, deque\nimport json, time\n\ndef stream_grouped_batches(path, key='user_id', max_batch=50, timeout=2.0):\n    queues = defaultdict(deque)\n    last_seen = {}\n    with open(path, 'r') as f:\n        for line in f:\n            obj = json.loads(line)\n            k = obj.get(key)\n            if k is None:\n                continue\n            queues[k].append(obj)\n            last_seen.setdefault(k, time.time())\n            if len(queues[k]) >= max_batch:\n                yield k, list(queues[k])\n                queues[k].clear()\n                last_seen[k] = time.time()\n            # Note: real implementation would also flush on timeout per key\n    for k, batch in queues.items():\n        if batch:\n            yield k, list(batch)\n```\n\n## Follow-up Questions\n- How would you simulate backpressure and verify memory usage?\n- How would you handle JSON decode errors and missing keys?","diagram":"flowchart TD\n  A[NDJSON stream] --> B[Group by key]\n  B --> C{Max batch or timeout}\n  C --> D[Emit batch]\n  D --> A","difficulty":"beginner","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Amazon","Lyft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T11:45:45.474Z","createdAt":"2026-01-23T11:45:45.474Z"},{"id":"q-6254","question":"Build an asyncio-driven Python pipeline that consumes two NDJSON streams (trades and quotes) from async iterators, performs a per-symbol time-bounded join within a 2-second tumbling window, emitting records (ts, symbol, trade_id, price, bid, ask) to a sink. Enforce memory bounds, maintain per-symbol order, handle late quotes with a 1-second grace, and provide a test harness with skewed symbols and backpressure?","answer":"Use two async iterators feeding bounded asyncio queues per stream; per-symbol state with deque for trades and a dict for latest quotes; schedule a 2-second window checker; on window end emit joined re","explanation":"## Why This Is Asked\n\nTests ability to design async dataflows with multi-stream joins, windowing, memory limits, and backpressure. It also probes correctness guarantees under skew and late data.\n\n## Key Concepts\n\n- Async streaming with Python asyncio\n- Per-symbol windowed joins and order preservation\n- Bounded memory budgeting and backpressure strategies\n- Test harness design for slow sinks and bursty input\n\n## Code Example\n\n```python\n# Skeleton illustrating per-symbol state and windowing\nimport asyncio\nfrom datetime import datetime\n\nclass Trade(BaseModel):\n    symbol: str\n    trade_id: str\n    price: float\n    ts: float\n\nclass Quote(BaseModel):\n    symbol: str\n    bid: float\n    ask: float\n    ts: float\n```\n\n## Follow-up Questions\n\n- How would you extend to more streams or dynamic window sizes?\n- How would you monitor backpressure and backfill stalled streams?","diagram":"flowchart TD\nA[Trades/Quotes streams] --> B[Per-symbol state]\nB --> C[2s window timer]\nC --> D[Emit joined records to sink]\nD --> E[Sink backpressure]","difficulty":"intermediate","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Google","IBM","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T16:43:42.275Z","createdAt":"2026-01-23T16:43:42.275Z"},{"id":"q-6358","question":"Implement sample_ndjson(input_path, output_path, k) that streams an NDJSON file and writes a random sample of k lines to the output while preserving the original order among sampled lines, using reservoir sampling with O(k) memory?","answer":"Use reservoir sampling: stream lines with an index i; maintain a reservoir of up to k items as (i, line). If i<k, push; else pick r = random.randrange(i+1); if r<k, replace reservoir[r] with (i, line)","explanation":"## Why This Is Asked\n\nThis checks ability to design a streaming, memory-bounded sampling utility over NDJSON, a common real-world task, with attention to preserving relative order of sampled lines and correct randomness.\n\n## Key Concepts\n\n- Reservoir sampling\n- Streaming I/O\n- NDJSON line semantics\n- Stable output order\n\n## Code Example\n\n```python\nimport random\n\ndef sample_ndjson(input_path, output_path, k):\n    reservoir = []\n    with open(input_path, 'r', encoding='utf-8') as fin:\n        for i, line in enumerate(fin):\n            if i < k:\n                reservoir.append((i, line))\n            else:\n                j = random.randrange(i + 1)\n                if j < k:\n                    reservoir[j] = (i, line)\n    reservoir.sort(key=lambda t: t[0])\n    with open(output_path, 'w', encoding='utf-8') as fout:\n        for _, line in reservoir:\n            fout.write(line)\n```\n\n## Follow-up Questions\n\n- How would you verify uniformity of the sample?\n- How to adapt to streaming to avoid a final sort if order is not required?\n- How would you handle compressed NDJSON inputs?","diagram":"flowchart TD\n  A[Start] --> B[Stream lines with index i]\n  B --> C{i<k}\n  C -- yes --> D[add to reservoir]\n  C -- no --> E[random j; replace if j<k]\n  E --> F[final sort by index]\n  F --> G[write to output]\n  G --> H[End]","difficulty":"beginner","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Adobe","Amazon","Citadel"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T20:47:32.680Z","createdAt":"2026-01-23T20:47:32.680Z"},{"id":"q-6385","question":"In a real-time analytics service that ingests JSON lines over an asyncio stream, implement a memory-bounded per-user sliding-window processor. Ingest, validate with Pydantic, bucket events into a 60s sliding window (e.g., 1s buckets). Keep a bounded LRU cache: user_id -> deque of buckets, pruning old users and buckets to cap memory. Emit per-user counts and avg latency to a sink when a window closes, and await sink to enforce backpressure. Test with bursty input and late arrivals up to 10s?","answer":"Develop an asyncio-based per-user sliding-window processor that ingests JSON lines, validates them with Pydantic, and buckets events into a 60-second sliding window using 1-second slots. Maintain a bounded LRU cache mapping user_id to deques of buckets, pruning old users and buckets to cap memory usage. Emit per-user counts and average latency to a sink when windows close, awaiting sink responses to enforce backpressure. Handle bursty input patterns and late arrivals up to 10 seconds.","explanation":"## Why This Is Asked\nThis question tests the ability to design memory-bounded streaming systems with per-key state management and backpressure control in Python asyncio environments.\n\n## Key Concepts\n- asyncio streams and backpressure enforcement\n- Per-user sliding window state management\n- Memory bounding with LRU cache pruning\n- Pydantic validation and fault tolerance\n- Late-event handling and window emission logic\n\n## Code Example\n```python\nfrom pydantic import BaseModel\n\nclass Event(BaseModel):\n    id: str\n    user_id: str\n    value: float\n    ts: float\n\nasync def process(source, sink, max_memory=1000):\n    # High-level implementation\n```","diagram":"flowchart TD\n  Ingest --> Validate\n  Validate --> Window\n  Window --> Emit\n  Emit --> Sink","difficulty":"intermediate","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Google","Scale Ai","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T05:33:16.650Z","createdAt":"2026-01-23T21:48:03.125Z"},{"id":"q-6461","question":"Design an asyncio-based Python data processor that ingests NDJSON lines from an AsyncIterable[str], where each line contains tenant_id, ts, and payload. Implement per-tenant in-order processing with bounded buffering: a dedicated, single consumer per tenant and a global memory cap. Include a dynamic rate limiter that adjusts per-tenant buffer limits based on overall load and propagates backpressure to the source. Provide a compact test scaffold with two tenants and mixed burstiness?","answer":"Use a central router that forwards lines to per-tenant bounded asyncio.Queues, with one dedicated worker per tenant to guarantee in-order processing. Maintain a global memory cap; a dynamic rate limiter adjusts per-tenant buffer limits based on queue depths and overall system load, propagating backpressure to the source when thresholds are exceeded.","explanation":"## Why This Is Asked\nThis question evaluates the candidate's ability to design robust, production-grade async streaming systems with multi-tenant support, enforcing per-tenant ordering guarantees, memory budgeting, and effective backpressure mechanisms. It also tests dynamic control logic under fluctuating load conditions.\n\n## Key Concepts\n- Asyncio patterns with per-tenant queues and serial workers to preserve ordering\n- Global memory caps and backpressure signaling to upstream sources\n- Dynamic rate limiting based on queue depth metrics and event-loop pressure\n- Test scaffolding that simulates realistic burst patterns across multiple tenants","diagram":null,"difficulty":"advanced","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Apple","Google","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T04:38:52.364Z","createdAt":"2026-01-24T02:37:18.848Z"},{"id":"q-6538","question":"Implement a Python generator function stream_dedup_ndjson(file_path: str, window_seconds: int = 60) -> Iterator[MyRecord], where each line in the NDJSON file is a JSON object with fields id: str, ts: str (ISO8601), symbol: str, price: float. Validate lines with a Pydantic model, skip invalid ones, and yield each unique record at most once within a sliding window. Memory must be bounded to the ids in the current window by discarding ids older than window_seconds. Assume timestamps are non-decreasing but may have duplicates?","answer":"Use a Pydantic model for validation, a deque to age-out entries, and a set for in-window ids. For each line: parse, validate, and convert ts to datetime. Pop aging entries beyond window_seconds, remov","explanation":"## Why This Is Asked\n\nThis exercises streaming validation, deduplication within a time window, and tight memory usage in Python.\n\n## Key Concepts\n- NDJSON parsing, Pydantic validation, sliding window, deques, sets, timestamp parsing\n\n## Code Example\n```python\nfrom collections import deque\nfrom datetime import datetime\nfrom pydantic import BaseModel\nimport json\nfrom typing import Iterator\n\nclass Record(BaseModel):\n    id: str\n    ts: datetime\n    symbol: str\n    price: float\n\n# simplified implementation sketch (full code would follow)\n```\n\n## Follow-up Questions\n- How would you handle out-of-order timestamps?\n- How would you scale to multiple concurrent streams?","diagram":"flowchart TD\n  A[Read NDJSON line] --> B{Validate with Pydantic}\n  B -->|Valid| C[Age window & dedupe]\n  C --> D{New id in window?}\n  D -->|Yes| E[Emit record]\n  D -->|No| E[Skip]","difficulty":"beginner","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Amazon","Citadel","Lyft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T07:26:01.798Z","createdAt":"2026-01-24T07:26:01.798Z"},{"id":"q-6781","question":"Build a streaming NDJSON processor that routes lines by tenant to per-tenant in-order workers. Each tenant has a bounded asyncio.Queue (memory cap in MB). When cap is reached, apply a runtime policy (drop_oldest or drop_newest) that can be swapped without restart. The dispatcher must support hot-swapping routing rules, guarantee per-tenant in-order processing, and provide backpressure to the source. Include a minimal test scaffold with tenants A and B and a policy swap?","answer":"Route NDJSON by tenant to per-tenant in-order workers using a central dispatcher and per-tenant bounded asyncio.Queues. Memory cap in items or bytes; on full apply a swapable policy (drop_oldest/drop_","explanation":"## Why This Is Asked\nTests advanced async routing, per-tenant ordering, bounded memory, and runtime reconfiguration.\n\n## Key Concepts\n- Per-tenant queues and backpressure\n- In-order processing with dynamic routing\n- Runtime policy swap and thread-safe registries\n- Memory accounting and backpressure signals\n\n## Code Example\n```python\nfrom dataclasses import dataclass\nfrom typing import Dict, Callable\n\n@dataclass\nclass Policy:\n    drop: str  # 'oldest' or 'newest'\n\ndef apply_policy(queue, item, policy: Policy):\n    # placeholder for policy implementation\n    pass\n```\n\n## Follow-up Questions\n- How would you measure latency per tenant?\n- How would you scale to thousands of tenants while preserving order?","diagram":null,"difficulty":"advanced","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":null,"companies":["IBM","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T17:08:52.392Z","createdAt":"2026-01-24T17:08:52.392Z"},{"id":"q-6848","question":"Design an asynchronous Python pipeline that ingests JSON lines from a WebSocket-like async source. Each line contains user_id, action, value, ts. Group by user_id, maintain a time-decayed score = sum(value * exp(-Δt/60)) for events in the last 60 seconds, emit (ts, user_id, score) when score crosses a threshold, preserving per-user in-order emission and bounded memory. Backpressure to source must be supported. Provide a test plan including bursty traffic and clock drift?","answer":"Use asyncio with per-user queues and a bounded sink. Maintain a 60s sliding window per user; on event at t, score = (prev_score * exp(-(t - last_t)/60)) + value, emit when score >= THRESHOLD. Process ","explanation":"## Why This Is Asked\n\nTests ability to design streaming pipelines, per-key state, time-based windows, and backpressure in Python async code.\n\n## Key Concepts\n\n- Async streaming with asyncio\n- Per-key state maintenance and in-order emission\n- Time-decayed windows and memory-bounded design\n- Backpressure via bounded queues and await\n\n## Code Example\n\n```javascript\n// placeholder example\n```\n\n## Follow-up Questions\n\n- How would you extend to multiple decay time constants per user?\n- How would you handle late arriving events (out-of-order ts)?","diagram":null,"difficulty":"intermediate","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Robinhood","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T19:42:10.011Z","createdAt":"2026-01-24T19:42:10.011Z"},{"id":"q-6954","question":"Design an async Python pipeline that consumes NDJSON lines from an async source (fields: user_id, action, page, ts) and performs per-user sessionization with a 30-second inactivity timeout. Emit per-session summaries: session_id, user_id, start, end, actions, unique_pages. Ensure per-user in-order processing, bounded memory, and backpressure to the source. Provide a test plan with bursty traffic and clock skew?","answer":"Implement an asyncio service using an AsyncQueue as the bridge from the source. Maintain per-user state including current_session_start, last_ts, actions, and pages. For each NDJSON line, extend the current session or start a new one when the 30-second inactivity timeout is exceeded. Use asyncio.gather() with bounded concurrency to enforce backpressure. Clean up completed sessions immediately to ensure bounded memory. Use monotonic time for timeout calculations to handle clock skew.","explanation":"## Why This Is Asked\nTests the ability to implement per-user state management, timers, and backpressure in asyncio without memory bloat.\n\n## Key Concepts\n- Async streaming and per-user state machines\n- Sessionization with inactivity timeout\n- Memory bounding and backpressure control\n- Time parsing and clock skew tolerance\n\n## Code Example\n```python\n# Placeholder for conceptual approach\n```","diagram":"flowchart TD\n  A[NDJSON Source] --> B[Parse Line]\n  B --> C[Per-user State]\n  C --> D{Session Timeout}\n  D --> E[Emit Session]\n  E --> F[Downstream Sink]","difficulty":"intermediate","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":null,"companies":["LinkedIn","Netflix","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T05:28:15.271Z","createdAt":"2026-01-25T02:35:24.957Z"},{"id":"q-7075","question":"You have multiple CSV log files, each sorted by timestamp, and you must output a single stream of rows in increasing timestamp without loading all files into memory. Implement a Python generator merge_sorted_csv(file_paths) that yields dictionaries for each row (using the header as keys) in merged order, streaming until all files are exhausted?","answer":"Use a generator that merges multiple sorted CSV logs by timestamp without loading full data. Maintain per-file iterators and a min-heap of (timestamp, file_index, row_dict). Seed with first row from e","explanation":"## Why This Is Asked\n- Evaluates streaming data merging across files without full materialization.\n- Exercises Python stdlib: heapq, csv, and generators under memory constraints.\n\n## Key Concepts\n- Generators, priority queue for multi-source merge, per-file iterators, lazy I/O, basic error handling.\n\n## Code Example\n```python\nimport csv, heapq\n\ndef merge_sorted_csv(file_paths):\n    files = [open(p, 'r', newline='') for p in file_paths]\n    readers = [csv.DictReader(f) for f in files]\n    heap = []\n    for i, rdr in enumerate(readers):\n        try:\n            row = next(rdr)\n            heap.append((row['timestamp'], i, row))\n        except StopIteration:\n            pass\n    heapq.heapify(heap)\n    while heap:\n        ts, idx, row = heapq.heappop(heap)\n        yield row\n        try:\n            nxt = next(readers[idx])\n            heapq.heappush(heap, (nxt['timestamp'], idx, nxt))\n        except StopIteration:\n            pass\n    for f in files:\n        f.close()\n```\n\n## Follow-up Questions\n- How handle missing/malformed timestamps?  \n- How adapt to different timestamp formats or large numbers of files?  \n- How to extend to streaming inputs that never end?","diagram":null,"difficulty":"beginner","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Airbnb","Discord"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T08:39:22.582Z","createdAt":"2026-01-25T08:39:22.582Z"},{"id":"q-7102","question":"Implement an asynchronous Python micro-batcher that consumes NDJSON lines from a streaming HTTP source. Each line contains fields: type (string), id (string), value (float). Batch by 'type' into per-type micro-batches with at most N lines or T seconds age. Validate lines with a Pydantic model. For each batch, compute per-type statistics (count, sum, min, max, mean) of 'value' and emit results to a downstream sink that can apply backpressure. Ensure per-type in-order processing and a bounded memory footprint?","answer":"Use per-type FIFO buffers (one asyncio.Task per type) reading NDJSON lines, validate with Pydantic, append to batch, flush when N lines or T seconds elapsed. Compute per-batch: count, sum, min, max, m","explanation":"## Why This Is Asked\nAssesses async IO, backpressure, memory management, and data validation under realistic streaming constraints.\n\n## Key Concepts\n- Async streaming with per-type batching\n- In-order delivery and bounded memory budgets\n- Pydantic validation of streaming records\n- Backpressure handling with downstream sinks\n- Bursty traffic and slow consumers testing\n\n## Code Example\n```python\n# high-level sketch of batching loop\n```\n\n## Follow-up Questions\n- How would you adapt the system to handle late-arriving records?\n- What monitoring would you add to detect memory leaks or backpressure deadlocks?\n","diagram":"flowchart TD\n  Source[NDJSON Stream] --> Batcher[Per-Type Batcher]\n  Batcher --> Validator[Pydantic Validation]\n  Validator --> aggregator[Compute stats]\n  aggregator --> Sink[Downstream Sink]\n  Sink --> Backpressure[Backpressure signaling]\n","difficulty":"intermediate","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Amazon","LinkedIn"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T09:33:34.267Z","createdAt":"2026-01-25T09:33:34.267Z"},{"id":"q-7153","question":"Design an asyncio-based streaming pipeline that consumes JSON lines of trades from a WebSocket. Route by symbol to per-symbol workers maintaining a bounded 5-minute sliding window. Each worker computes VWAP and a volume distribution, applying a hot-swappable per-symbol transformer registry. Offload heavy statistics to a ProcessPool, enforce per-symbol in-order delivery, bounded memory, and backpressure to the source. Include a test scaffold with two symbols and a runtime transformer swap A->A2?","answer":"Design an asyncio pipeline that ingests JSONL trades from a WebSocket, routes by symbol to per-symbol workers with a bounded 5-minute sliding window, computes VWAP and a volume profile, uses a hot-swa","explanation":"## Why This Is Asked\nTests mastery of asyncio streaming, per-key state, and memory discipline under backpressure. It also probes hot-swapping logic for production-transformers and CPU-bound offloads.\n\n## Key Concepts\n- Async streaming with WebSocket JSONL input\n- Per-symbol state and in-order guarantees\n- Bounded sliding window analytics (VWAP, volume)\n- Hot-swappable per-symbol transformers\n- ProcessPool for CPU-bound work and backpressure handling\n\n## Code Example\n```python\n# Skeleton illustrating registry and per-symbol worker\nfrom typing import Callable, Dict\nimport asyncio\n\nclass TransformerRegistry:\n    def __init__(self):\n        self._maps: Dict[str, Callable] = {}\n    def get(self, sym: str) -> Callable:\n        return self._maps.get(sym, lambda x: x)\n    def swap(self, sym: str, f: Callable):\n        self._maps[sym] = f\n\nasync def per_symbol_worker(sym: str, in_q: asyncio.Queue, out_q: asyncio.Queue, reg: TransformerRegistry):\n    transformer = reg.get(sym)\n    while True:\n        item = await in_q.get()\n        if item is None:\n            break\n        await out_q.put(transformer(item))\n```\n\n## Follow-up Questions\n- How would you verify runtime transformer swaps under burst load?\n- What strategies ensure no memory leaks when many symbols spin up and down?","diagram":null,"difficulty":"advanced","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Citadel","Tesla","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T11:31:01.541Z","createdAt":"2026-01-25T11:31:01.541Z"},{"id":"q-7196","question":"Design and implement a streaming inner-join operator in Python that consumes two asynchronous NDJSON streams and joins on user_id. Validate items with Pydantic; keep per-user buffers bounded; emit joined records as soon as a match exists while preserving per-user order and providing backpressure to sources. Include tests for skewed distributions and bursty arrivals?","answer":"Implement an async join: create two readers for streams A and B, parse lines to Pydantic models, store per-user buffers (deques) keyed by user_id, emit joined records as soon as both sides have a matc","explanation":"## Why This Is Asked\nThis tests streaming, memory budgeting, and correctness with out-of-order arrivals. It also exercises data validation and backpressure integration.\n\n## Key Concepts\n- Async pipelines with bounded buffers\n- Per-key ordering guarantees\n- Pydantic validation and schema evolution\n\n## Code Example\n```python\nfrom typing import AsyncIterable\nfrom pydantic import BaseModel\n\nclass A(BaseModel): user_id: str; ts: float\nclass B(BaseModel): user_id: str; ts: float\n\nasync def join_streams(a: AsyncIterable[str], b: AsyncIterable[str], max_buffer: int = 10000):\n  # outline\n  pass\n```\n\n## Follow-up Questions\n- How to extend to N streams?\n- How to handle late data and timeouts?\n","diagram":null,"difficulty":"intermediate","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Airbnb","Amazon","Hashicorp"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T13:39:26.823Z","createdAt":"2026-01-25T13:39:26.823Z"},{"id":"q-7488","question":"Design and implement an asynchronous Python data pipeline that ingests NDJSON events from an async source, validates each event via a Pydantic model, then enriches events by querying a rate-limited REST API. Route events by 'tenant' so per-tenant events are processed in order, with per-tenant buffers limited to a maximum size and total memory under a defined budget. The pipeline must apply backpressure to the producer, handle transient API errors with exponential backoff, and support graceful shutdown. Provide data models, components, and a minimal test plan?","answer":"Propose an async per-tenant queue system: assign each tenant its own worker, validate with Pydantic, enrich via an aiohttp call governed by a semaphore to enforce RPS, and emit to a sink with backpres","explanation":"## Why This Is Asked\nStreaming, backpressure, and per-tenant ordering reflect real systems.\n\n## Key Concepts\n- Async IO, per-key ordering, bounded buffers\n- Rate limiting, exponential backoff, retries\n- Pydantic validation, DLQ, graceful shutdown\n\n## Code Example\n```python\nfrom pydantic import BaseModel\n\nclass Event(BaseModel):\n    tenant: str\n    id: str\n    payload: float\n```\n\n## Follow-up Questions\n- How would you ensure exactly-once semantics?\n- How would you simulate and test backpressure and memory budgets?","diagram":null,"difficulty":"intermediate","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Amazon","Cloudflare","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T04:23:43.955Z","createdAt":"2026-01-26T04:23:43.955Z"},{"id":"q-7531","question":"Design an asynchronous Python module that consumes a WebSocket stream of JSON events, validates them with a Pydantic model, partitions by both region and device, batches per partition into micro-batches of at most N lines or T seconds, and computes per-batch events/sec. Emit results to a sink with backpressure. Enforce per-partition in-order processing, a bounded memory footprint, and late-event handling via a watermark (drop events older than watermark). Provide a test plan with simulated late events and slow sinks?","answer":"Propose an asyncio-based WebSocket consumer that uses a bounded per-partition queue keyed by region+device, validates with Pydantic, and batches into per-partition micro-batches by count or time. Main","explanation":"## Why This Is Asked\nThis asks for practical async streaming skills, multi-key partitioning, and robust backpressure handling in Python.\n\n## Key Concepts\n- Asyncio/WebSocket streaming and backpressure\n- Per-partition batching by multiple keys\n- Pydantic validation and error handling\n- Watermark-based late-event dropping and memory bounds\n- Correctness of in-order processing per partition\n\n## Code Example\n```python\nfrom pydantic import BaseModel\nfrom typing import Any\n\nclass Event(BaseModel):\n    region: str\n    device: str\n    ts: float\n    value: float\n```\n\n## Follow-up Questions\n- How would you test late-arriving events and clock skew?\n- What trade-offs exist between watermark delay and memory usage?","diagram":null,"difficulty":"intermediate","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Discord","Two Sigma","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T05:59:38.516Z","createdAt":"2026-01-26T05:59:38.516Z"},{"id":"q-7572","question":"Design an asynchronous Python pipeline that ingests NDJSON lines from an AsyncIterable[str], validates each line with a Pydantic model, deduplicates by a unique transaction_id using an in-memory TTL cache with a disk-backed checkpoint, partitions by tenant_id, computes per-tenant sliding-window aggregates (count and sum of value) over the last 60 seconds, enforces per-tenant in-order processing, and provides backpressure to the source while enabling crash recovery?","answer":"Implement per-tenant queues, an asyncio.Task per partition, an in-memory TTL cache to dedupe IDs, a disk-backed checkpoint (JSON) that writes after batches, use a sliding window with a deque per tenan","explanation":"## Why This Is Asked\nTests ability to design complex streaming state with dedup and crash recovery, not just single-pass processing.\n\n## Key Concepts\n- AsyncIterable NDJSON ingestion, Pydantic validation\n- Per-tenant partitioning, in-order processing\n- Deduplication with TTL cache and disk checkpoint\n- Sliding-window aggregates with bounded memory\n- Backpressure and crash recovery\n\n## Code Example\n```python\nfrom collections import deque\nfrom typing import Deque\n\n# simplified sketch\n```\n\n## Follow-up Questions\n- How would you scale to thousands of tenants?\n- How would you guarantee exactly-once semantics across restarts?\n","diagram":"flowchart TD\n  A[NDJSON AsyncSource] --> B[Validator/Deser]\n  B --> C{Dedup & Partition}\n  C --> D[Per-tenant Window & Aggregation]\n  D --> E[Sink]\n","difficulty":"advanced","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Amazon","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T07:57:32.862Z","createdAt":"2026-01-26T07:57:32.862Z"},{"id":"q-7588","question":"Design an asyncio-based Python pipeline that reads NDJSON lines from an HTTP/2 streaming endpoint, validates each line with a Pydantic model, partitions by both tenant and service, and uses per-partition micro-batching with flush on N items or T seconds. Enforce a global memory cap via a semaphore and apply backpressure to the source. Include a test plan with bursty traffic and slow sinks?","answer":"Implement per-partition asyncio tasks with bounded queues; read NDJSON from an HTTP/2 stream, validate with Pydantic, route by (tenant, service), batch by N or T, memory cap via a semaphore, and signa","explanation":"## Why This Is Asked\n\nTests a candidate’s ability to design an end-to-end async pipeline with real-world constraints: partitioned state, backpressure, memory budgeting, and data validation.\n\n## Key Concepts\n\n- Async IO orchestration with asyncio\n- Per-partition in-order processing\n- Adaptive batching (size or time)\n- Memory budgeting and backpressure signaling\n- Data validation with Pydantic\n- Bursty traffic and slow sinks in tests\n\n## Code Example\n\n```javascript\n# Python pseudo\nimport asyncio\nfrom pydantic import BaseModel\n\nclass Event(BaseModel):\n    tenant: str\n    service: str\n    value: float\n\nasync def batcher(events, N, T):\n    batch = []\n    timer = asyncio.get_event_loop().time()\n    async for e in events:\n        batch.append(e)\n        if len(batch) >= N or (time.time() - timer) >= T:\n            yield batch\n            batch = []\n            timer = time.time()\n```\n\n## Follow-up Questions\n\n- How would you handle late-arriving events?\n- How would you scale beyond a single host?","diagram":null,"difficulty":"intermediate","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Anthropic","Discord"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T08:55:44.258Z","createdAt":"2026-01-26T08:55:44.258Z"},{"id":"q-7616","question":"Design a beginner-friendly Python function stream_validate_agg(file_path, batch_size=100, timeout=2.0) for a JSONL file where each line is a JSON object with fields id:str, type:str, value=float. Read lazily, validate with a Pydantic model, and group values by 'type' into per-type batches. Emit a batch as {type: [value,...]} when a batch reaches batch_size or the batch's age exceeds timeout. Memory usage must stay O(batch_size). Ensure per-type in-order processing. Provide a small test snippet with synthetic JSONL data?","answer":"Read lazily using a generator; validate each line via a Pydantic model; accumulate per-type buffers in a dict of lists; when any buffer reaches batch_size or the batch age exceeds timeout, emit the pe","explanation":"## Why This Is Asked\nTests understanding of streaming I/O, incremental batching, per-key grouping, and memory-based flush semantics in a beginner-friendly setting.\n\n## Key Concepts\n- Generators and lazy file I/O\n- Pydantic validation\n- Per-type buffering and batch flushing\n- Time-based flush with time.monotonic\n- Memory-bounded streaming\n\n## Code Example\n```python\nfrom typing import Dict, List, Iterator\nimport time\nfrom pydantic import BaseModel, ValidationError\n\nclass Record(BaseModel):\n    id: str\n    type: str\n    value: float\n\n\ndef stream_validate_agg(file_path: str, batch_size: int = 100, timeout: float = 2.0) -> Iterator[Dict[str, List[float]]]:\n    buffers: Dict[str, List[float]] = {}\n    start = None\n    with open(file_path, 'r') as f:\n        for line in f:\n            line = line.strip()\n            if not line:\n                continue\n            try:\n                rec = Record.parse_raw(line)\n            except ValidationError:\n                continue\n            if start is None:\n                start = time.monotonic()\n            buffers.setdefault(rec.type, []).append(rec.value)\n            flush = False\n            if len(buffers[rec.type]) >= batch_size:\n                flush = True\n            elif time.monotonic() - start >= timeout:\n                flush = True\n            if flush:\n                yield {t: vals[:] for t, vals in buffers.items() if vals}\n                buffers.clear()\n                start = time.monotonic()\n```\n\n## Follow-up Questions\n- How would you adapt this to async file I/O or WebSocket streams?\n- What testing strategy would you use for timing-sensitive batches?","diagram":"flowchart TD\n  A[Start] --> B[Read line]\n  B --> C{Valid?}\n  C -->|Yes| D[Buffer by type]\n  D --> E{Flush?}\n  E -->|Yes| F[Emit batch]\n  F --> B\n  E -->|No| B","difficulty":"beginner","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":null,"companies":["LinkedIn","Two Sigma","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T09:58:57.958Z","createdAt":"2026-01-26T09:58:57.958Z"},{"id":"q-7752","question":"Design a Python function process_logs(stream) that reads a stream of JSON lines representing logs (keys: level, ts, msg). Validate each line against a lightweight schema using TypedDict, and emit a real-time aggregation of counts per log level every 50 lines. Lines that fail validation should increment an error counter and be skipped. Ensure memory usage stays bounded and lines are processed in order?","answer":"Propose a generator-based approach: stream lines, json.loads each line, validate with a minimal TypedDict{level:str, ts:int, msg:str}. Maintain a per-level counter and a total line counter; on every 5","explanation":"## Why This Is Asked\nTests practical streaming with Python basics: IO handling, lightweight typing, error resilience, and bounded memory.\n\n## Key Concepts\n- Generators and streaming IO\n- TypedDict for lightweight schema\n- Real-time aggregation with bounded memory\n- Error handling and metrics\n\n## Code Example\n```python\nfrom typing import TypedDict, Iterator, Dict\nimport json\n\nclass LogLine(TypedDict):\n    level: str\n    ts: int\n    msg: str\n\ndef process_logs(stream: Iterator[str], batch_size: int = 50):\n    counts: Dict[str, int] = {}\n    total = 0\n    errors = 0\n    for line in stream:\n        total += 1\n        try:\n            data = json.loads(line)\n            if not isinstance(data, dict) or not all(k in data for k in (\"level\", \"ts\", \"msg\")):\n                raise ValueError(\"Invalid keys\")\n            log: LogLine = {\"level\": str(data[\"level\"]), \"ts\": int(data[\"ts\"]), \"msg\": str(data[\"msg\"])}\n            counts[log[\"level\"]] = counts.get(log[\"level\"], 0) + 1\n        except Exception:\n            errors += 1\n        if total % batch_size == 0:\n            yield {\"counts\": counts.copy(), \"total\": total, \"errors\": errors}\n    if total % batch_size != 0:\n        yield {\"counts\": counts, \"total\": total, \"errors\": errors}\n```\n\n## Follow-up Questions\n- How would you extend this to handle out-of-order lines with timestamps? \n- How would you benchmark latency between input and snapshots?","diagram":"flowchart TD\n  A[Start] --> B[Read line]\n  B --> C{Valid JSON with keys}\n  C -- Yes --> D[Update counts]\n  C -- No --> E[Count error]\n  D --> F{Flush every 50 lines}\n  F -- Yes --> G[Emit snapshot]\n  F -- No --> H[Continue]\n  G --> I[End?]\n","difficulty":"beginner","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Coinbase","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T16:45:41.352Z","createdAt":"2026-01-26T16:45:41.352Z"},{"id":"q-7778","question":"Design an asynchronous Python event router for a real-time analytics pipeline: events arrive as NDJSON lines with fields including 'type' and 'payload'; build a registry of async handlers keyed by type, ensure per-type FIFO ordering, and route each event to its handler while applying backpressure via bounded queues. Validate events with Pydantic and provide a practical test plan?","answer":"Implement an async router with a bounded per-type queue, a central dispatcher, and per-type workers that strictly preserve order. Validate each NDJSON line via a Pydantic model, then enqueue into the ","explanation":"## Why This Is Asked\nReal-time analytics need dynamic routing by event type with backpressure and per-type ordering, testing throughput vs. memory.\n\n## Key Concepts\n- Async IO with bounded queues to implement backpressure\n- Per-type FIFO ordering via dedicated worker pools\n- Pydantic validation and robust error handling\n- Dynamic handler registry and testability\n\n## Code Example\n```python\nfrom pydantic import BaseModel\n\nclass Event(BaseModel):\n    type: str\n    payload: dict\n```\n\n## Follow-up Questions\n- How would you handle idempotent retries for failed events?\n- How would you measure per-type latency under bursty traffic?","diagram":"flowchart TD\n  A[NDJSON Source] --> B[Decoder + Validator]\n  B --> C[Central Dispatcher]\n  C --> D[Type Queue: per type]\n  D --> E[Per-Type Worker] --> F[Sink]","difficulty":"intermediate","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Airbnb","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T17:39:19.053Z","createdAt":"2026-01-26T17:39:19.053Z"},{"id":"q-7941","question":"Design a Python decorator retry that applies exponential backoff with jitter for a flaky API call, supporting both sync and async functions, capping max_retries, and preserving metadata; how would you implement and test wrapping a fetch(url) that may raise a transient TimeoutError?","answer":"Implement a decorator retry(max_retries=3, base_delay=0.1, jitter=0.05, retry_exceptions=(TimeoutError, ConnectionError)). Detect async using inspect.iscoroutinefunction and await for async, sleep for","explanation":"## Why This Is Asked\n\nResilience in API calls is critical in fintech integrations; this tests practical handling of transient failures without blocking throttling.\n\n## Key Concepts\n\n- Exponential backoff with jitter\n- Distinguishing async vs sync paths\n- Exception filtering and retries\n- Preserving function metadata with functools.wraps\n\n## Code Example\n\n```python\nimport asyncio\nimport functools\nimport random\nimport inspect\nimport time\n\n\ndef retry(max_retries=3, base_delay=0.1, jitter=0.05, retry_exceptions=(TimeoutError, ConnectionError)):\n    def decorator(fn):\n        @functools.wraps(fn)\n        async def async_wrapper(*args, **kwargs):\n            last_exc = None\n            for i in range(max_retries + 1):\n                try:\n                    return await fn(*args, **kwargs)\n                except retry_exceptions as e:\n                    last_exc = e\n                    if i == max_retries:\n                        raise\n                    delay = base_delay * (2 ** i) * (1 + random.uniform(-jitter, jitter))\n                    await asyncio.sleep(delay)\n        @functools.wraps(fn)\n        def sync_wrapper(*args, **kwargs):\n            last_exc = None\n            for i in range(max_retries + 1):\n                try:\n                    return fn(*args, **kwargs)\n                except retry_exceptions as e:\n                    last_exc = e\n                    if i == max_retries:\n                        raise\n                    delay = base_delay * (2 ** i) * (1 + random.uniform(-jitter, jitter))\n                    time.sleep(delay)\n        if inspect.iscoroutinefunction(fn):\n            return async_wrapper\n        else:\n            return sync_wrapper\n    return decorator\n```\n\n## Follow-up Questions\n\n- How would you extend this for per-domain rate limiting in a multi-tenant fintech client?\n- How would you test both sync and async paths for failure, success, and maximum backoffs?","diagram":"flowchart TD\n  Start([Start]) --> Call([Call function])\n  Call --> Catch{Exception raised?}\n  Catch --> Decide{Retries left?}\n  Decide -->|Yes| Sleep([Wait with backoff]) --> Call\n  Decide -->|No| End([End])","difficulty":"beginner","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Coinbase","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T23:55:48.997Z","createdAt":"2026-01-26T23:55:48.997Z"},{"id":"q-7992","question":"Design an asynchronous Python module that ingests NDJSON lines from an async HTTP stream, validates each line with a Pydantic model, and batches events by user_id into micro-batches of at most N lines or T seconds. For each user flush, compute the 95th percentile of amount within the batch and emit to a backpressured sink via asyncio.Queue. Enforce per-user in-order processing, a bounded memory footprint, and a watermark to drop events older than 5 minutes; provide a concise test plan with out-of-order events and slow sinks?","answer":"Consume NDJSON lines asynchronously, validate with a Pydantic model, and batch by user_id into micro-batches of at most N lines or T seconds. For each user flush, compute the 95th percentile of amount","explanation":"## Why This Is Asked\nTests ability to build an async, per-key batcher with backpressure, memory budgeting, and watermarking; also validates test planning and validation logic.\n\n## Key Concepts\n- Async streaming and per-key partitioning\n- Micro-batching by size or time window\n- Pydantic validation and error handling\n- Backpressure via asyncio.Queue\n- Watermarks and bounded memory management\n- Test strategy for out-of-order events and slow sinks\n\n## Code Example\n```python\nfrom pydantic import BaseModel\nfrom typing import List\nimport asyncio\nimport statistics\n\nclass Event(BaseModel):\n    user_id: str\n    amount: float\n    ts: float\n    action: str\n```\n\n## Follow-up Questions\n- How would you adapt this to handle late events arriving after a batch has flushed?\n- What metrics would you collect to monitor backpressure and memory usage in production?","diagram":"flowchart TD\n  A[NDJSON Source] --> B[Per-User Batches]\n  B --> C[Batch Flush (N or T)]\n  C --> D[Compute p95 per-user]\n  D --> E[Sink with Backpressure]\n  E --> F[Flow Control]","difficulty":"intermediate","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Discord","NVIDIA","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T04:29:42.588Z","createdAt":"2026-01-27T04:29:42.588Z"},{"id":"q-8054","question":"Implement a Python generator follow_file(path, poll_interval=0.5) that yields new lines appended to a log file in real time (like tail -f). It must gracefully handle log rotation (when the file is moved/replaced), and stop yielding if the file is deleted. Include a minimal test example showing rotation and deletion scenarios; keep memory usage O(1)?","answer":"Proposed answer outline: Implement follow_file(path, poll_interval=0.5) that opens path and seeks to end; in a loop, read lines as they are appended and yield them; detect rotation by watching inode c","explanation":"## Why This Is Asked\nThis tests streaming I/O, file handles, and edge-case resilience (rotation, deletion) using a simple generator, matching real-world log ingestion tasks.\n\n## Key Concepts\n- Generators for streaming data\n- File I/O: seek, read, rotation detection\n- Inode-based rotation check; deletion handling\n- O(1) memory aside from current line\n\n## Code Example\n```python\nimport os\nimport time\n\ndef follow_file(path, poll_interval=0.5):\n    with open(path, 'r') as f:\n        while True:\n            line = f.readline()\n            if line:\n                yield line.rstrip('\\n')\n                continue\n            # no new line yet; check rotation or deletion\n            try:\n                stat = os.stat(path)\n            except FileNotFoundError:\n                break\n            if f.fileno() != os.open(path, os.O_RDONLY).fileno():\n                # rotation detected; reopen\n                f.close()\n                f = open(path, 'r')\n            time.sleep(poll_interval)\n```\n\n## Follow-up Questions\n- How would you adapt this to multiple files concurrently?\n- How would you integrate with asyncio for non-blocking I/O?\n","diagram":"flowchart TD\n  A[Start] --> B[Open file], C[Seek to end]\n  B --> D{New lines?}\n  D -->|Yes| E[Yield line]\n  D -->|No| F[Check rotation/deletion]\n  F --> G{Rotation}\n  G -->|Yes| H[Reopen file]\n  G -->|No| I[Sleep]\n  H --> E\n  I --> D","difficulty":"beginner","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Instacart","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T07:40:27.088Z","createdAt":"2026-01-27T07:40:27.088Z"},{"id":"q-8112","question":"Design a Python async pipeline that ingests NDJSON lines from AsyncIterable[str], validates each line with a Pydantic model, partitions by tenant_id and event_type, batches per partition into micro-batches of at most N lines or T seconds, computes a deterministic feature vector per batch, and streams results to a sink that applies backpressure; ensure per-partition in-order processing, bounded memory, hot-swappable batch policy at runtime, and provide a test plan with synthetic late data and slow sinks?","answer":"Design a Python async pipeline that ingests NDJSON lines, validates each with a Pydantic model, partitions by tenant_id and event_type, batches per partition into micro-batches of at most N lines or T","explanation":"## Why This Is Asked\n\nAssesses ability to build a scalable, low-latency async pipeline with per-partition ordering, memory control, and runtime configurability. Also tests backpressure handling and robust testing strategies.\n\n## Key Concepts\n\n- Async batching per partition\n- Pydantic validate-and-parse\n- Per-tenant and per-event partitioning\n- Bounded memory with backpressure\n- Hot-swappable batch policy at runtime\n\n## Code Example\n\n```python\nfrom pydantic import BaseModel\nfrom typing import AsyncIterable\nimport asyncio\n\nclass Event(BaseModel):\n    tenant_id: str\n    event_type: str\n    value: float\n\nasync def process(stream: AsyncIterable[str]):\n    async for line in stream:\n        e = Event.parse_raw(line)\n        # route to partition and batch (implementation omitted)\n        pass\n```\n\n## Follow-up Questions\n\n- How would you implement runtime policy swapping without restarting the pipeline?\n- How do you ensure exactly-once semantics with retries and idempotent sinks?","diagram":"flowchart TD\n  A[NDJSON In] --> B[Validate with Pydantic]\n  B --> C[Partition by tenant/event]\n  C --> D[Per-partition batcher]\n  D --> E[Compute feature vector]\n  E --> F[Sink with backpressure]","difficulty":"advanced","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Discord","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T10:06:43.568Z","createdAt":"2026-01-27T10:06:43.568Z"},{"id":"q-8151","question":"Python beginner task: Implement a streaming CSV aggregator. Given a file-like source producing lines with fields timestamp, category, amount (float), create a function stream_aggregator(source, batch_size=1000) that groups by category, sums amounts, and yields a dict category->sum after every batch_size lines, ignoring malformed lines, and keeping memory usage O(number of categories)?","answer":"Read lines with csv.reader from the source, aggregate amounts per category in a defaultdict(float), and yield a dict after every batch_size lines; reset the sums after each yield. Ignore malformed row","explanation":"## Why This Is Asked\nTests basic streaming, parsing, and memory-conscious aggregation in Python.\n\n## Key Concepts\n- Generators/iterators, csv module, defaultdict\n- Memory proportional to number of categories\n- Graceful error handling\n\n## Code Example\n```python\nimport csv\nfrom collections import defaultdict\n\ndef stream_aggregator(source, batch_size=1000):\n    sums = defaultdict(float)\n    line_count = 0\n    reader = csv.reader(source)\n    for row in reader:\n        if len(row) < 3:\n            continue\n        try:\n            _, category, amount_s = row[0], row[1], row[2]\n            amount = float(amount_s)\n        except ValueError:\n            continue\n        sums[category] += amount\n        line_count += 1\n        if line_count >= batch_size:\n            yield dict(sums)\n            sums.clear()\n            line_count = 0\n    if line_count > 0:\n        yield dict(sums)\n```","diagram":"flowchart TD\n  A[Start] --> B[Read line from source]\n  B --> C{Parse OK?}\n  C -- Yes --> D[Accumulate sums]\n  D --> E{Batch full?}\n  E -- Yes --> F[Emit dict and reset]\n  E -- No --> B\n  C -- No --> B","difficulty":"beginner","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Databricks","Robinhood","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T11:38:42.541Z","createdAt":"2026-01-27T11:38:42.541Z"},{"id":"q-8220","question":"Implement a Python generator function merge_streams(stream1, stream2) that each yield dicts with keys 'ts' (int) and 'val' (float). Each stream is individually sorted by 'ts'. The function should yield merged dicts in non-decreasing 'ts' order, preserving stream1 order on ties, and without buffering beyond one item per stream. Validate each item has the required keys and numeric types; raise ValueError on invalid input. Provide a usage example?","answer":"Approach: two-lookahead iterators, compare a['ts'] and b['ts'], yield the smaller (or a first on ties), advance the corresponding iterator, and handle StopIteration to drain the other stream. Validate","explanation":"## Why This Is Asked\nThis tests understanding of streaming data, in-order merging, memory efficiency, and robust input validation using Python generators.\n\n## Key Concepts\n- Generators and iterators\n- Two-way merge with stable tie-breaking\n- Input validation and error handling\n- StopIteration and draining remaining streams\n\n## Code Example\n```python\ndef merge_streams(stream1, stream2):\n    it1 = iter(stream1)\n    it2 = iter(stream2)\n    try:\n        a = next(it1)\n        b = next(it2)\n    except StopIteration:\n        if 'a' in locals(): yield a  # if one empty\n        if 'b' in locals(): yield b\n        if 'a' in locals():\n            for x in it1: yield x\n        if 'b' in locals():\n            for x in it2: yield x\n        return\n    while True:\n        if not isinstance(a, dict) or not isinstance(a.get('ts'), int) or not isinstance(a.get('val'), (int, float)):\n            raise ValueError(\"Invalid item in stream1\")\n        if not isinstance(b, dict) or not isinstance(b.get('ts'), int) or not isinstance(b.get('val'), (int, float)):\n            raise ValueError(\"Invalid item in stream2\")\n        if a['ts'] <= b['ts']:\n            yield a\n            try:\n                a = next(it1)\n            except StopIteration:\n                yield b\n                for x in it2:\n                    yield x\n                return\n        else:\n            yield b\n            try:\n                b = next(it2)\n            except StopIteration:\n                yield a\n                for x in it1:\n                    yield x\n                return\n```\n\n## Follow-up Questions\n- How would you generalize to N streams?\n- How would you modify to handle out-of-order events with a bounded lookback window?\n- How would you test this with empty streams and invalid items?","diagram":null,"difficulty":"beginner","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Cloudflare","Google","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T15:41:41.346Z","createdAt":"2026-01-27T15:41:41.346Z"},{"id":"q-8416","question":"Design a Python generator-based function stream_dedup(source, window_seconds) that reads NDJSON-like dicts from an iterator, deduplicates by the 'id' field within a sliding window, and yields only the first occurrence per id. Memory must be bounded by the number of ids seen in the window. Do not buffer the whole source; use per-id timestamps and a deque for eviction. Show a small test demonstrating behavior?","answer":"I'll implement a generator function `stream_dedup(source, window_seconds)` that processes NDJSON-like dictionaries, deduplicates by the 'id' field within a sliding time window, and yields only the first occurrence per id. The implementation maintains bounded memory by tracking per-id timestamps and using a deque for time-based eviction.","explanation":"## Why This Is Asked\nStreaming deduplication within a time window is a common requirement in real-time data pipelines. This evaluates memory-bounded processing skills and proper eviction logic implementation.\n\n## Key Concepts\n- Sliding window with bounded memory\n- Per-id timestamp tracking\n- Generator-based streaming architecture\n- Time-based eviction using deque\n\n## Code Example\n```python\ndef stream_dedup(source, window_seconds: int):\n    from collections import deque\n    import time\n    seen = {}  # id -> timestamp\n    queue = deque()  # (id, ts)\n    for item in source:\n        now = time.ti","diagram":null,"difficulty":"beginner","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Meta","Netflix","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-28T05:59:52.358Z","createdAt":"2026-01-27T23:40:06.967Z"},{"id":"q-8437","question":"Design an advanced Python streaming processor that ingests NDJSON from an async source, partitions by 'user_id', and applies a pluggable transform pipeline loaded from a runtime registry that supports hot-swapping transforms without restart. Ensure per-user in-order processing, bounded memory, and backpressure to the source. Include a minimal test showing swapping to a new transform version mid-run and verifying outputs reflect the swap without duplicates?","answer":"Implement per-user partitioned asyncio workers; validate lines with Pydantic; apply a transform chain from a runtime registry that can hot-swap versions atomically; use bounded asyncio.Queues for back","explanation":"## Why This Is Asked\n\nThis question probes mastery of advanced Python async patterns, per-key ordering guarantees, bounded memory and backpressure, and a robust hot-swapping mechanism for transform pipelines without downtime. It also stresses testability with mid-run version swaps and dedup safeguards.\n\n## Key Concepts\n\n- Per-key partitioning and in-order processing\n- Bounded memory and backpressure in async pipelines\n- Runtime registry and atomic hot-swapping of transforms\n- Data validation with Pydantic and NDJSON handling\n- Idempotent sinks and deduplication strategies\n\n## Code Example\n\n```python\n# Sketch: hot-swappable transform registry\nimport asyncio\nfrom typing import Callable, Dict\n\nclass TransformRegistry:\n    def __init__(self):\n        self._version = 'v1'\n        self._transform: Callable = lambda x: x\n        self._lock = asyncio.Lock()\n\n    async def get(self) -> Callable:\n        async with self._lock:\n            return self._transform\n\n    async def swap(self, new_version: str, new_callable: Callable):\n        async with self._lock:\n            self._version = new_version\n            self._transform = new_callable\n```\n\n## Follow-up Questions\n\n- How would you ensure exactly-once semantics if the downstream sink fails?\n- How would you adapt this to handle bursts from a single user while others stay idle?","diagram":null,"difficulty":"advanced","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Goldman Sachs","NVIDIA","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-28T02:30:25.256Z","createdAt":"2026-01-28T02:30:25.256Z"},{"id":"q-8473","question":"Implement a Python function sensor_stream_mavg(source, k) that reads one JSON line per item from source, validates it with a Pydantic model (sensor_id: str, ts: int, value: float), and yields a dict {sensor_id: moving_avg} after each line, where moving_avg is the average of the last k values for that sensor. Memory should be bounded by O(S*k); skip malformed lines?","answer":"Use per-sensor sliding windows: a defaultdict of deque(maxlen=k) and a running sum per sensor. Validate each line with a Pydantic model (sensor_id, ts, value). On a new line, if the sensor window is f","explanation":"## Why This Is Asked\n\nAssess the ability to design a simple per-key streaming state with bounded memory and validation using Python primitives.\n\n## Key Concepts\n\n- per-key state management\n- sliding window technique\n- Pydantic validation\n- generator/iterator pattern\n- memory complexity analysis\n\n## Code Example\n\n```python\nfrom collections import defaultdict, deque\nfrom typing import Iterable, Iterator, Dict\nfrom pydantic import BaseModel, ValidationError\n\nclass Reading(BaseModel):\n    sensor_id: str\n    ts: int\n    value: float\n\ndef sensor_stream_mavg(source: Iterable[str], k: int) -> Iterator[Dict[str, float]]:\n    windows = defaultdict(lambda: deque(maxlen=k))\n    sums = defaultdict(float)\n    for line in source:\n        try:\n            rec = Reading.parse_raw(line)\n        except ValidationError:\n            continue\n        s = rec.sensor_id\n        dq = windows[s]\n        if len(dq) == k:\n            old = dq[0]\n            sums[s] -= old\n        dq.append(rec.value)\n        sums[s] += rec.value\n        yield {s: sums[s] / len(dq)}\n```\n\n## Follow-up Questions\n\n- How would you handle out-of-order timestamps?\n- How would you extend to dynamic k per sensor?","diagram":null,"difficulty":"beginner","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":null,"companies":["LinkedIn","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-28T04:25:41.474Z","createdAt":"2026-01-28T04:25:41.474Z"},{"id":"q-852","question":"Design a memory-efficient streaming dedup pipeline in Python that reads a multi-GB log file line-by-line and prints only the first occurrence of each unique line, skipping duplicates, while persisting dedup state across restarts. Use a probabilistic data structure with a configurable false-positive rate and provide a minimal runnable snippet?","answer":"Use a Bloom-filter-backed streaming pipeline: read lines via a generator, check a Bloom filter; if a line is not present, yield it and add it to the filter; if present, skip. Persist the filter's bit ","explanation":"## Why This Is Asked\nTests ability to design streaming data processing with memory constraints, probabilistic data structures, and persistence. It touches trade-offs between false positives, memory, and restart behavior; requires specifying parameters and a runnable snippet.\n\n## Key Concepts\n- Streaming IO and generators\n- Bloom filters and false-positive rate\n- State persistence across restarts\n- Parameter tuning m, k for given n, p\n\n## Code Example\n```javascript\n# Python-like BloomFilter (illustrative)\nclass BloomFilter:\n    def __init__(self, m, k):\n        self.bits = bytearray(m // 8 + 1)\n        self.k = k\n    def _hashes(self, item):\n        h1 = hash(item)\n        h2 = (hash(str(item)) * 1103515245) & 0x7fffffff\n        for i in range(self.k):\n            yield (h1 + i * h2) % len(self.bits)\n    def add(self, item):\n        for idx in self._hashes(item):\n            self.bits[idx] = 1\n    def __contains__(self, item):\n        return all(self.bits[idx] for idx in self._hashes(item))\n```\n\n## Follow-up Questions\n- How would you adapt this for distributed workers?\n- How would you handle log rotation and cache eviction?","diagram":"flowchart TD\n  A[Read line] --> B[Check Bloom filter]\n  B --> C{New}\n  C --> D[Emit line and add to Bloom]\n  B --> E{Duplicate}\n  E --> F[Skip]","difficulty":"intermediate","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","MongoDB","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:35:10.938Z","createdAt":"2026-01-12T13:35:10.938Z"},{"id":"q-975","question":"Design an asynchronous NDJSON pipeline in Python that ingests lines of JSON from an async source, validates each line against a Pydantic model, applies a lightweight transform, and yields results to a downstream consumer. Ensure memory stays bounded when the consumer slows by using a bounded asyncio.Queue?","answer":"Use an async producer–consumer with a bounded asyncio.Queue. The producer reads NDJSON lines from the async source, validates with a Pydantic model, and enqueues transformed events. The consumer drain","explanation":"## Why This Is Asked\n\nTests ability to build streaming, backpressure-aware pipelines; real-world data rates and validation.\n\n## Key Concepts\n\n- Async I/O and producers/consumers\n- NDJSON streaming\n- Pydantic validation\n- Bounded queues for backpressure\n- Error handling and fault tolerance\n\n## Code Example\n\n```javascript\n# Python-like skeleton showing producer/consumer with asyncio.Queue\nimport asyncio\nfrom pydantic import BaseModel\n\nclass Event(BaseModel):\n    ts: float\n    user: str\n    value: float\n\nasync def producer(src, q: asyncio.Queue):\n    async for line in src:\n        evt = Event.parse_raw(line)\n        await q.put(evt)\n\nasync def consumer(q: asyncio.Queue):\n    while True:\n        item = await q.get()\n        # process item\n        q.task_done()\n```\n\n## Follow-up Questions\n\n- How would you measure backpressure and adapt queue size?\n- How to handle malformed lines without dropping the stream?","diagram":null,"difficulty":"intermediate","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Coinbase","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T17:39:00.639Z","createdAt":"2026-01-12T17:39:00.639Z"}],"subChannels":["async","best-practices","fundamentals","general"],"companies":["Adobe","Airbnb","Amazon","Anthropic","Apple","Bloomberg","Citadel","Cloudflare","Coinbase","Databricks","Discord","DoorDash","Goldman Sachs","Google","Hashicorp","Hugging Face","IBM","Instacart","LinkedIn","Lyft","Meta","Microsoft","MongoDB","NVIDIA","Netflix","OpenAI","Oracle","PayPal","Plaid","Robinhood","Salesforce","Scale Ai","Slack","Snap","Snowflake","Square","Stripe","Tesla","Twitter","Two Sigma","Uber","Zoom"],"stats":{"total":90,"beginner":29,"intermediate":35,"advanced":26,"newThisWeek":43}}