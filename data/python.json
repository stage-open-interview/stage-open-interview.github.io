{"questions":[{"id":"q-196","question":"How would you implement a rate-limited async HTTP client using aiohttp and asyncio.Semaphore to handle 1000 requests while respecting API limits?","answer":"Use asyncio.Semaphore(50) for concurrency control and aiohttp.ClientSession with time-based rate limiting between requests.","explanation":"## Why Asked\nTests async programming and API rate limiting skills in production scenarios.\n\n## Key Concepts\nAsyncIO, Semaphore, Rate Limiting, HTTP Client Design\n\n## Code Example\n```\nimport aiohttp\nimport asyncio\n\nasync def rate_limited_client(urls):\n    semaphore = asyncio.Semaphore(50)\n    async with aiohttp.ClientSession() as session:\n        tasks = [fetch_url(session, url, semaphore) for url in urls]\n        return await asyncio.gather(*tasks)\n\nasync def fetch_url(session, url, semaphore):\n    async with semaphore:\n        await asyncio.sleep(0.1)  # Rate limit\n        async with session.get(url) as response:\n            return await response.text()\n```","diagram":"flowchart TD\n  A[Start] --> B[Create Semaphore]\n  B --> C[Create ClientSession]\n  C --> D[Process URLs Concurrently]\n  D --> E[Apply Rate Limits]\n  E --> F[Return Results]\n  F --> G[End]","difficulty":"intermediate","tags":["asyncio","aiohttp","concurrency"],"channel":"python","subChannel":"async","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":"https://www.youtube.com/watch?v=Qb9s3UiMSTA"},"companies":["Amazon","Google","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":["asyncio","semaphore","aiohttp","concurrency","rate limiting","async"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2026-01-08T11:27:41.329Z","createdAt":"2025-12-26 12:51:06"},{"id":"q-224","question":"How would you implement a thread-safe singleton in Python with lazy initialization, proper type hints, and discuss the trade-offs between metaclass, decorator, and module-level approaches for a production system?","answer":"Implement using metaclass with double-checked locking and TypeVar for generics: metaclass provides compile-time enforcement, decorator offers simplicity, while module-level singletons are most Pythonic. Use threading.Lock for thread safety and generic typing support.","explanation":"## Context\n\nThis question assesses understanding of design patterns, concurrency, and Python-specific implementation trade-offs. Senior developers should compare multiple singleton approaches and justify their choice.\n\n## Code Examples\n\n### Metaclass Approach\n\n```python\nfrom typing import TypeVar, Type, Optional\nimport threading\n\nT = TypeVar('T')\n\nclass SingletonMeta(type):\n    _instances: dict[Type, object] = {}\n    _lock: threading.Lock = threading.Lock()\n    \n    def __call__(cls: Type[T], *args, **kwargs) -> T:\n        if cls not in cls._instances:\n            with cls._lock:\n                if cls not in cls._instances:\n                    instance = super().__call__(*args, **kwargs)\n                    cls._instances[cls] = instance\n        return cls._instances[cls]\n```\n\n### Decorator Approach\n\n```python\ndef singleton(cls):\n    instances = {}\n    lock = threading.Lock()\n    \n    def wrapper(*args, **kwargs):\n        if cls not in instances:\n            with lock:\n                if cls not in instances:\n                    instances[cls] = cls(*args, **kwargs)\n        return instances[cls]\n    \n    return wrapper\n```\n\n### Module-Level Approach\n\n```python\n# singleton_module.py\nclass _DatabaseConnection:\n    def __init__(self):\n        self.connection = None\n    \n    def connect(self):\n        if not self.connection:\n            self.connection = create_db_connection()\n        return self.connection\n\n# Single instance at module import time\ndatabase = _DatabaseConnection()\n```\n\n## Trade-offs Analysis\n\n**Metaclass Approach:**\n- Pros: Compile-time enforcement, clean class definition, full type hint support\n- Cons: More complex, metaclass magic can be confusing for new developers\n- Best for: Production systems requiring strict singleton enforcement\n\n**Decorator Approach:**\n- Pros: Simple to apply, non-intrusive, easy to understand\n- Cons: Runtime enforcement only, can be bypassed with direct class instantiation\n- Best for: Quick implementations and prototyping\n\n**Module-Level Approach:**\n- Pros: Most Pythonic, guaranteed thread safety by import system, zero overhead\n- Cons: Less flexible, eager initialization, harder to test\n- Best for: Simple cases where lazy initialization isn't required\n\n## Production Recommendation\n\nFor production systems, use the metaclass approach with double-checked locking. It provides the best balance of thread safety, type hint support, and enforcement while maintaining clean API design.","diagram":"graph TD\n    A[Thread 1 Request] --> B{Instance Exists?}\n    C[Thread 2 Request] --> B\n    B -->|No| D[Acquire Lock]\n    B -->|Yes| K[Return Instance]\n    D --> E{Double Check}\n    E -->|No| F[Create Instance]\n    E -->|Yes| G[Release Lock]\n    F --> H[Initialize]\n    H --> I[Store in Class Dict]\n    I --> G\n    G --> K\n    J[Thread N Request] --> B","difficulty":"advanced","tags":["pep8","typing","testing"],"channel":"python","subChannel":"best-practices","sourceUrl":null,"videos":null,"companies":["Amazon","Google","Meta","Microsoft","Netflix"],"eli5":"Imagine you have a special toy that only one kid can play with at a time. When you want to play with it, you check if anyone else is using it first. If it's free, you grab it and tell everyone else 'I'm using this toy now!' So no two kids can accidentally both think they're the special toy owner. This is like making sure only one copy of something exists in your computer program. You can have different rules about how to share toys: one way is having a playground boss who decides (metaclass), another is putting a special sticker on the toy box (decorator), or just having one toy box for the whole school (module-level). Each way works, but some are easier to understand than others!","relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-30T06:40:35.857Z","createdAt":"2025-12-26 12:51:07"},{"id":"q-178","question":"In Python, when do `is` and `==` return different results, and why does this happen with object identity vs equality?","answer":"`is` checks memory identity (same object), `==` checks value equality. Different for distinct objects with same values: `[1,2] is [1,2]` is False but `[1,2] == [1,2]` is True.","explanation":"## Concept Overview\nPython distinguishes between object identity and value equality. `is` compares memory addresses using `id()`, while `==` compares values using `__eq__` method.\n\n## Implementation\n```python\n# Identity vs Equality\na = [1, 2, 3]\nb = [1, 2, 3]\nc = a\n\nprint(a == b)  # True - same values\nprint(a is b)  # False - different objects\nprint(a is c)  # True - same object\n\n# Integer optimization (small integers)\nx = 256\ny = 256\nprint(x is y)  # True - interned\n\nx = 257\ny = 257\nprint(x is y)  # False - different objects\n```\n\n## Trade-offs\n- `is`: Faster, checks if exactly same object\n- `==`: Slower, checks if values are equivalent\n- Use `is` for singletons (None, True, False)\n- Use `==` for value comparison\n\n## Common Pitfalls\n- Assuming `is` works for value comparison\n- Not understanding integer/string interning\n- Using `is` with mutable objects incorrectly\n- Forgetting that `==` can be overridden by custom classes","diagram":"graph TD\n    A[Object A] -->|id: 0x1234| C[Memory Location 0x1234]\n    B[Object B] -->|id: 0x5678| D[Memory Location 0x5678]\n    E[Object C] -->|id: 0x1234| C\n    \n    F[Value: [1,2,3]] --> G[Content Comparison]\n    H[Value: [1,2,3]] --> G\n    I[Value: [1,2,3]] --> G\n    \n    J[is operator] --> K[Compare memory addresses]\n    L[== operator] --> M[Compare values via __eq__]\n    \n    style C fill:#e1f5fe\n    style D fill:#e1f5fe\n    style G fill:#f3e5f5","difficulty":"intermediate","tags":["python","basics"],"channel":"python","subChannel":"fundamentals","sourceUrl":null,"videos":{"shortVideo":"https://www.youtube.com/watch?v=mO_dS3rXDIs","longVideo":"https://www.youtube.com/watch?v=CZ8bZPqtwU0"},"companies":["Amazon","Google","Meta","Microsoft","Uber"],"eli5":"Imagine you have two identical toy cars. They look exactly the same and can do the same tricks - that's like using == to check if they're equal. But are they the SAME exact toy car? No! They're two different cars that just happen to look alike. That's like using 'is' to check if they're the same object. In Python, when you make two lists with the same numbers, they're like those two toy cars - they look identical (== says True) but they're actually two separate objects in the computer's memory (is says False). Only when you point to the very same object will both 'is' and '==' say True!","relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-25T14:57:56.251Z","createdAt":"2025-12-26 12:51:06"},{"id":"q-1085","question":"You’re building a real-time log processor in Python for a messaging app. You receive a stream of JSON lines like {\"user_id\": 123, \"event\": \"message_sent\", \"ts\": 1610000000}. Emit only the first event per (user_id, event) within a 60-second sliding window. Design a memory-bounded, generator-based pipeline that reads from an iterable of strings and yields deduplicated dicts in order. Include edge cases and memory considerations?","answer":"Implement a generator-based pipeline that reads lines, parses JSON, and yields the first dict for each (user_id, event) in a 60-second sliding window. Maintain a bounded TTL cache (OrderedDict) key->t","explanation":"## Why This Is Asked\n\nTests ability to design streaming pipelines with memory constraints and dedup logic without external stores.\n\n## Key Concepts\n\n- Generators and streaming data\n- Sliding window deduplication\n- TTL-based cache with eviction\n- Robust JSON parsing and field validation\n\n## Code Example\n\n```python\ndef dedup_stream(lines, window_seconds=60):\n    import json\n    from collections import deque, OrderedDict\n    cache = OrderedDict()\n    for line in lines:\n        obj = json.loads(line)\n        user = obj.get(\"user_id\")\n        evt = obj.get(\"event\")\n        ts = obj.get(\"ts\")\n        if user is None or evt is None or ts is None:\n            continue\n        key = (user, evt)\n        # evict stale\n        for k in list(cache):\n            if ts - cache[k] > window_seconds:\n                del cache[k]\n        if key in cache:\n            continue\n        cache[key] = ts\n        yield obj\n```\n\n## Follow-up Questions\n\n- How would you adapt for out-of-order arrivals? \n- How to scale with multiple processes?","diagram":"flowchart TD\n  A[Input Stream] --> B[Parse JSON]\n  B --> C[Key by (user_id, event)]\n  C --> D[TTL Cache Lookup]\n  D --> E{Duplicate in window?}\n  E -->|No| F[Emit] --> G[Update Cache]\n  E -->|Yes| H[Drop]","difficulty":"intermediate","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T22:19:30.480Z","createdAt":"2026-01-12T22:19:30.480Z"},{"id":"q-1149","question":"In a streaming data etl, you're given a JSONL stream where each line is a JSON object with 'host' and 'value'. Implement a memory-efficient Python generator top_n_hosts(n, lines) that yields the top N hosts by total value after consuming the stream, without keeping the entire per-host totals in memory. Use a min-heap to maintain current top-N and skip invalid lines?","answer":"Process the JSONL stream line by line; validate host and numeric value; accumulate per-host totals in a dict; maintain a fixed-size min-heap of (total, host) for the current top-N; when a host total i","explanation":"## Why This Is Asked\n\nTests ability to craft streaming pipelines and manage memory with data structure choices.\n\n## Key Concepts\n\n- Generators and lazy evaluation\n- JSONL parsing with error handling\n- Per-key aggregation with a fixed-size min-heap (top-N)\n- Trade-offs: O(H) memory vs O(N log N) heap maintenance\n\n## Code Example\n\n```python\nimport json, heapq\n\ndef top_n_hosts(n, lines):\n    totals = {}\n    heap = []  # min-heap of (total, host)\n    seen = set()\n    for line in lines:\n        try:\n            obj = json.loads(line)\n            host = obj.get('host')\n            val = float(obj.get('value', 0))\n            if host is None:\n                continue\n        except Exception:\n            continue\n        totals[host] = totals.get(host, 0.0) + val\n        heapq.heappush(heap, (totals[host], host))\n        if len(heap) > n:\n            heapq.heappop(heap)\n        seen.add(host)\n    for total, host in sorted(heap, reverse=True):\n        yield host, total\n```\n\n## Follow-up Questions\n\n- How would you modify to handle streaming resets or windowed top-N?","diagram":null,"difficulty":"intermediate","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T01:34:23.375Z","createdAt":"2026-01-13T01:34:23.375Z"},{"id":"q-1250","question":"Design an asynchronous NDJSON ingestion pipeline in Python that reads an async iterable of lines, validates each with a Pydantic model, and routes records to per-tenant partitions based on the 'tenant_id'. Implement bounded queues per partition with a total memory cap, guarantee per-tenant in-order processing, and a slower downstream sink that creates backpressure. Provide a test plan with skewed partitions and slow sinks?","answer":"To implement, create an async function ingest(source, writer_factory) that builds a dict of per-tenant queues and a small worker pool per tenant. Each line is parsed as JSON, validated with a Pydantic","explanation":"## Why This Is Asked\nTests ability to design partitioned, back-pressured pipelines in Python with asyncio, ensuring per-tenant ordering and bounded memory, a realistic scalability concern at large-scale companies like Snowflake/Uber.\n\n## Key Concepts\n- Async data ingestion\n- Bounded queues and backpressure\n- Per-tenant partitioning and in-order semantics\n\n## Code Example\n```python\n# skeleton illustrating structure\nfrom typing import AsyncIterable, Dict\nfrom asyncio import Queue, TaskGroup\nfrom pydantic import BaseModel\nimport json\nimport asyncio\n\nclass Record(BaseModel):\n    tenant_id: str\n    payload: dict\n\nasync def ingest(source: AsyncIterable[str], writer_factory):\n    queues: Dict[str, Queue] = {}\n    async def worker(tid: str, q: Queue):\n        sink = writer_factory(tid)\n        while True:\n            item = await q.get()\n            if item is None:\n                break\n            await sink.write(item)\n    async with TaskGroup() as g:\n        async for line in source:\n            rec = Record(**json.loads(line))\n            q = queues.setdefault(rec.tenant_id, Queue(maxsize=128))\n            await q.put(rec)\n        for q in queues.values():\n            await q.put(None)\n        # workers auto-join on exit\n```\n\n## Follow-up Questions\n- How would you handle dynamic partition keys that can exceed memory?\n- How would you validate end-to-end ordering when a partition has backpressure and failure?","diagram":"flowchart TD\n  S[NDJSON Source] --> M[Parse & Validate]\n  M --> P{{Partition by tenant_id}}\n  P --> Q[Per-tenant Queues]\n  Q --> W[Per-tenant Workers]\n  W --> D[Downstream Sink]","difficulty":"advanced","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Snowflake","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T06:43:51.710Z","createdAt":"2026-01-13T06:43:51.710Z"},{"id":"q-1275","question":"Design an advanced async Python pipeline: NDJSON lines arrive via a TCP socket, each is validated with a Pydantic model, then a deduplication stage uses a sliding-window Bloom filter to emit each event at most once in a 24-hour window. The pipeline must stay memory-bounded, support backpressure to the producer, and allow the downstream sink to pace itself. Provide a concrete test plan with clock skew and bursty traffic?","answer":"Approach: implement an async NDJSON ingest with a bounded asyncio.Queue, Pydantic validation, and a dedupe stage using a sliding window Bloom filter implemented with buckets. The Bloom filter stores f","explanation":"## Why This Is Asked\nTests ability to design streaming pipelines with memory constraints, asynchronous backpressure, and practical deduplication.\n\n## Key Concepts\n- Async ingestion with bounded queues\n- Pydantic validation of streaming data\n- Sliding-window Bloom filter for deduplication\n- Backpressure and pacing of producers/consumers\n\n## Code Example\n```python\n#_placeholder\n```\n\n## Follow-up Questions\n- How would you calibrate the Bloom filter's false positive rate for varying traffic? \n- How would you extend to multi-node deduplication without shared state?","diagram":null,"difficulty":"advanced","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Hugging Face","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T07:39:46.313Z","createdAt":"2026-01-13T07:39:46.313Z"},{"id":"q-1285","question":"Given a list of dictionaries representing users, implement normalize_users(users) that returns a list of User dataclass instances with fields id:int, name:str, signup_ts:datetime, is_active:bool. Coerce id from str to int, parse signup_ts ISO8601 strings, interpret is_active from common truthy values, and fill defaults for missing fields. If any record is invalid, raise ValueError with the indices of invalid records?","answer":"Implement a User dataclass and normalize_users that maps dicts to User, coercing id to int, parsing signup_ts via datetime.fromisoformat, and is_active from common truthy values; default missing field","explanation":"## Why This Is Asked\nThis question tests data normalization, type coercion, and error reporting using Python basics.\n\n## Key Concepts\n- Dataclasses for lightweight models\n- Type hints and basic validation\n- ISO 8601 parsing with datetime\n- Error aggregation for batch records\n\n## Code Example\n```python\nfrom dataclasses import dataclass\nfrom datetime import datetime\nfrom typing import List, Dict, Any\n\n@dataclass\nclass User:\n    id: int\n    name: str\n    signup_ts: datetime\n    is_active: bool\n\ndef _to_int(v: Any) -> int:\n    if isinstance(v, int):\n        return v\n    return int(v)\n\ndef _parse_ts(ts: Any) -> datetime:\n    if isinstance(ts, datetime):\n        return ts\n    return datetime.fromisoformat(ts)\n\ndef _to_bool(v: Any) -> bool:\n    if isinstance(v, bool):\n        return v\n    if isinstance(v, str):\n        return v.strip().lower() in {\"true\", \"1\", \"yes\", \"y\"}\n    return bool(v)\n\n\ndef normalize_users(users: List[Dict[str, Any]]) -> List[User]:\n    out: List[User] = []\n    bad: List[int] = []\n    for i, rec in enumerate(users):\n        try:\n            u = User(\n                id=_to_int(rec.get(\"id\")),\n                name=rec.get(\"name\", \"\"),\n                signup_ts=_parse_ts(rec.get(\"signup_ts\")),\n                is_active=_to_bool(rec.get(\"is_active\", False)),\n            )\n            out.append(u)\n        except Exception:\n            bad.append(i)\n    if bad:\n        raise ValueError(f\"Invalid records at indices: {bad}\")\n    return out\n```\n\n## Follow-up Questions\n- How would you extend to nested user data?\n- How would you validate and report multiple fields per record?","diagram":"flowchart TD\n  A[Start] --> B[Iterate records]\n  B --> C[Coerce fields]\n  C --> D{All valid?}\n  D -->|Yes| E[Emit User]\n  D -->|No| F[Collect index]\n  F --> G[Raise error on end]","difficulty":"beginner","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","MongoDB","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T08:31:56.123Z","createdAt":"2026-01-13T08:31:56.123Z"},{"id":"q-1476","question":"Design an asynchronous Python engine to join two JSON event streams by id within a 5-second window. Streams A and B are async iterables yielding {'id': str, 'ts': int, 'payload': Any}. Emit matched pairs to a sink when both sides have an event with same id within the 5s window. Route late events past the lateness bound to a 'late' sink. Enforce a global memory bound for buffered events and propose a test plan with out-of-order arrivals?","answer":"Buffer events per id from each stream. On arrival of an A event, check B-buffer for the same id; if exists and abs(tsA - tsB) <= 5000, emit the joined pair and drop both. Otherwise enqueue in A-buffer","explanation":"## Why This Is Asked\nTests async coordination, windowed joins, and backpressure with late data handling in Python. It targets real-world streaming problems and metrics-conscious memory management.\n\n## Key Concepts\n- Async streaming and per-id buffering\n- Time-based windowing (5s tumbling window)\n- Late data routing and backpressure\n- Global memory bounds and eviction strategy\n\n## Code Example\n```python\n# Implementation sketch (high-level)\nfrom typing import AsyncIterable, Dict, Deque\nimport asyncio\n\nclass WindowJoin:\n    def __init__(self, window_ms: int, mem_cap: int):\n        self.window = window_ms\n        self.mem_cap = mem_cap\n        self.buf_a: Dict[str, Deque[dict]] = {}\n        self.buf_b: Dict[str, Deque[dict]] = {}\n        self.watermark = 0\n\n    async def ingest_a(self, item: dict, sink):\n        id_ = item['id']\n        ts = item['ts']\n        self._expire(ts)\n        self.buf_a.setdefault(id_, deque()).append(item)\n        self._try_emit(id_, sink)\n\n    async def ingest_b(self, item: dict, sink, late_sink):\n        id_ = item['id']\n        ts = item['ts']\n        self._expire(ts)\n        self.buf_b.setdefault(id_, deque()).append(item)\n        if id_ in self.buf_a:\n            self._emit_matches(id_, sink, late_sink)\n\n    def _expire(self, ts: int):\n        self.watermark = max(self.watermark, ts)\n        # eviction logic based on window and mem_cap would go here\n\n    def _try_emit(self, id_: str, sink):\n        # attempt to pair any ready events for id_\n        a_list = self.buf_a.get(id_, [])\n        b_list = self.buf_b.get(id_, [])\n        while a_list and b_list:\n            a = a_list[0]\n            b = b_list[0]\n            if abs(a['ts'] - b['ts']) <= self.window:\n                sink.emit({'id': id_, 'a': a, 'b': b})\n                a_list.popleft(); b_list.popleft()\n            else:\n                break\n\n    def _emit_matches(self, id_: str, sink, late_sink):\n        self._try_emit(id_, sink)\n        # any remaining items considered late by window semantics could go to late_sink\n        # (pseudo-logic for brevity)\n```\n","diagram":"flowchart TD\n  A[Stream A] --> B[Buffer A]\n  C[Stream B] --> D[Buffer B]\n  E[Join Engine] --> F[Sink]\n  E --> G[Late Sink]","difficulty":"advanced","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Lyft","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T18:50:10.589Z","createdAt":"2026-01-13T18:50:10.589Z"},{"id":"q-1639","question":"Write a Python function process_events(file_path) that reads a newline-delimited JSON (JSONL) log file of Stripe-like events. Each line is a JSON object with 'type' (str) and 'data' (dict with 'id' key). The function should return a dict mapping event 'type' to count, skipping lines with missing keys or invalid JSON, and writing errors to a separate errors.log. Make it memory-efficient using a streaming approach?","answer":"Propose a streaming, memory-safe approach: open the JSONL file, iterate lines, parse with json.loads, verify 'type' is str and 'data' is dict containing 'id'; increment a Counter keyed by type; skip i","explanation":"## Why This Is Asked\nReading streaming logs is common in production pipelines (e.g., Stripe webhook events). This task tests robust, memory-friendly parsing, simple validation, and error handling under real-world I/O.\n\n## Key Concepts\n- Memory-efficient line-by-line processing\n- JSONL parsing and validation\n- Defensive checks and error logging\n- Simple aggregation with collections.Counter\n\n## Code Example\n```javascript\ndef process_events(path):\n    import json, logging\n    from collections import Counter\n    counts = Counter()\n    with open(path, 'r', encoding='utf-8') as f:\n        for line in f:\n            try:\n                obj = json.loads(line)\n            except json.JSONDecodeError:\n                logging.warning(\"invalid line\")\n                continue\n            t = obj.get('type')\n            data = obj.get('data')\n            if not isinstance(t, str) or not isinstance(data, dict) or 'id' not in data:\n                logging.warning(\"missing fields or wrong types\")\n                continue\n            counts[t] += 1\n    return dict(counts)\n```\n\n## Follow-up Questions\n- How would you extend to handle out-of-order events or deduplicate by id?\n- How would you add unit tests for valid and invalid lines?\n","diagram":"flowchart TD\n  Start([Start]) --> Read[Read JSONL line-by-line]\n  Read --> Validate{Valid?}\n  Validate -- Yes --> Update[Update counts]\n  Validate -- No --> ErrorLog[Log error]\n  Update --> End([Return counts])\n  ErrorLog --> End","difficulty":"beginner","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T04:24:46.136Z","createdAt":"2026-01-14T04:24:46.136Z"},{"id":"q-1675","question":"Design a memory-bounded streaming processor in Python for a JSONL event stream with fields: timestamp, service, event_type, payload. Group by (service, event_type), maintain an in-order per-key queue with bounded capacity, and emit a 60-second rolling histogram of payload sizes per group to a downstream sink. Ensure backpressure via per-key queues and a global memory cap, and provide a test plan with skewed keys and slow sinks?","answer":"Use asyncio with per-key bounded queues and a global memory budget. Each key = (service, event_type) has a fixed-size asyncio.Queue and a 60-second circular histogram of payload sizes. On arrival, pus","explanation":"## Why This Is Asked\nEfficient streaming with per-key backpressure and bounded memory.\n\n## Key Concepts\n- asyncio, per-key bounded queues\n- sliding window histograms\n- memory budgeting and backpressure\n- test plan with skewed keys\n\n## Code Example\n```python\n# sketch: per-key histogram and bounded queues\nfrom collections import deque\nimport asyncio\n\nclass WindowHistogram:\n    def __init__(self, window=60):\n        self.bins = deque([0]*window, maxlen=window)\n        self._last_tick = None\n\n    def add(self, size, timestamp):\n        # simplistic implementation; real version rotates bins per second\n        self.bins[-1] += size\n```","diagram":"flowchart TD\n  S[Source] --> P[Parse JSONL]\n  P --> G{Group by (service, event_type)}\n  G --> Q[Per-key queues]\n  Q --> H[60s histogram]\n  H --> E[Emit summaries]\n  E --> D[Downstream sink]","difficulty":"advanced","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Amazon","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T06:47:26.037Z","createdAt":"2026-01-14T06:47:26.037Z"},{"id":"q-1783","question":"Implement a memory-efficient Python function top_n_words(filepath, n) that streams a text file line by line to find the n most frequent words. Normalize case, strip punctuation, ignore empty tokens, and return a list of the top n words sorted by frequency. Ensure it never loads the whole file into memory?","answer":"Use a streaming approach: read each line, lowercase it, remove punctuation, split into words, accumulate counts in a dict, then compute the top n by frequency with a heap (nlargest). This keeps memory","explanation":"## Why This Is Asked\nStreaming text processing is common in data pipelines; this tests memory awareness and basic text normalization.\n\n## Key Concepts\n- Streaming I/O to avoid loading large files\n- Simple tokenization and normalization\n- Extracting top-N with a heap\n\n## Code Example\n```javascript\nimport string, heapq\n\ndef top_n_words(filepath, n):\n    counts = {}\n    trans = str.maketrans('', '', string.punctuation)\n    with open(filepath, 'r', encoding='utf-8') as f:\n        for line in f:\n            line = line.lower().translate(trans)\n            for w in line.split():\n                counts[w] = counts.get(w, 0) + 1\n    return [w for w, _ in heapq.nlargest(n, counts.items(), key=lambda kv: kv[1])]\n``n\n## Follow-up Questions\n- How would you adapt this to handle memory spikes with extremely large vocabularies?\n- How would you test the function for correctness and performance?","diagram":null,"difficulty":"beginner","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T10:45:42.409Z","createdAt":"2026-01-14T10:45:42.410Z"},{"id":"q-1867","question":"Design a Python async NDJSON ingestion pipeline that reads lines from an async source, validates each line against a versioned Pydantic model, supports hot-reloadable schema versions from a shared config, and guarantees exactly-once delivery with an idempotent sink and per-record deduplication, all while enforcing bounded memory and backpressure. How would you implement it?","answer":"I would implement an asyncio pipeline with a bounded queue, an async producer reading NDJSON lines, a versioned validator registry loaded from a shared JSON config, and an idempotent sink using a smal","explanation":"## Why This Is Asked\n\nThis question probes experience with dynamic schemas, backpressure handling, and exactly-once semantics in a streaming Python pipeline.\n\n## Key Concepts\n\n- Async NDJSON ingestion with bounded memory\n- Versioned validators and runtime config reload\n- Idempotent sinks and per-record deduplication\n- Safe hot-reload and atomic registry swap\n\n## Code Example\n\n```python\nfrom typing import Any, Dict\nimport asyncio\nimport json\nfrom pydantic import BaseModel\n\nclass V1(BaseModel):\n    id: str\n    value: int\n    version: str = 'v1'\n\nclass V2(BaseModel):\n    id: str\n    value: int\n    meta: str = ''\n\nSchemaRegistry = {'v1': V1, 'v2': V2}\n\ndef load_schema(ver: str):\n    return SchemaRegistry.get(ver, V1)\n\nasync def validate_line(line: str) -> Dict[str, Any]:\n    data = json.loads(line)\n    ver = data.get('version', 'v1')\n    model = load_schema(ver)\n    obj = model(**data)\n    return obj.dict()\n```\n\n## Follow-up Questions\n- How would you test hot-reload safety and dedup correctness?\n- How would you scale to multiple workers and a shared dedup store?","diagram":null,"difficulty":"advanced","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T14:55:43.072Z","createdAt":"2026-01-14T14:55:43.073Z"},{"id":"q-1884","question":"Implement an asynchronous Python data transformer for a JSONL event stream where each line includes a 'version' field. Build transform_stream(input: AsyncIterable[str], schemas: Dict[int, Type[BaseModel]]) that validates each line against its versioned Pydantic model, applies a version-aware field mapping, and outputs transformed JSONL lines to a downstream sink while guaranteeing per-version in-order processing, memory-bounded streaming, and backpressure. Include a small test scaffold showing a v1→v2 migration?","answer":"Use an AsyncStream with per-version queues. Validate by loading schemas[version] and applying mapping; emit transformed JSON via an async sink. Maintain in-order by preserving sequence_id per version ","explanation":"## Why This Is Asked\nThis question probes the ability to design a robust streaming Python solution that handles schema evolution, backpressure, and memory constraints in real-time data processing.\n\n## Key Concepts\n- AsyncIO pipelines with bounded queues\n- Versioned Pydantic models loaded from a registry\n- In-order processing across dynamic schemas\n- Safe error handling and backpressure-driven flow control\n\n## Code Example\n```python\nfrom typing import AsyncIterable, Dict, Type\nfrom pydantic import BaseModel\nimport asyncio\nimport json\n\nasync def transform_stream(\n    input_lines: AsyncIterable[str],\n    schemas: Dict[int, Type[BaseModel]],\n    mapper: Dict[int, callable],\n    sink: asyncio.Queue\n) -> None:\n    # simplified skeleton\n    queues = {v: asyncio.Queue(maxsize=1024) for v in schemas}\n    async def worker(version: int, q: asyncio.Queue):\n        while True:\n            line = await q.get()\n            if line is None:\n                break\n            data = json.loads(line)\n            model = schemas[version](**data)  # validate\n            out = mapper[version](model)\n            await sink.put(json.dumps(out.dict()))\n            q.task_done()\n\n    procs = [asyncio.create_task(worker(v, queues[v])) for v in schemas]\n    async for line in input_lines:\n        data = json.loads(line)\n        ver = data.get(\"version\")\n        if ver not in queues:\n            continue\n        await queues[ver].put(line)\n    for q in queues.values():\n        await q.put(None)\n    await asyncio.gather(*procs)\n```\n\n## Follow-up Questions\n- How would you handle missing or unknown versions gracefully?\n- How would you test migration paths between versions?","diagram":"flowchart TD\n  A[Input JSONL] --> B[Versioned Validation]\n  B --> C[Per-Version Router]\n  C --> D[Output Sink]\n  D --> E[Backpressure]","difficulty":"intermediate","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","NVIDIA","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T15:45:20.047Z","createdAt":"2026-01-14T15:45:20.047Z"},{"id":"q-2090","question":"Design a memory-bounded streaming top-K aggregator in Python. Data arrives as an async iterable of numeric events with timestamps. Implement a class that maintains an approximate top-10 using a Count-Min Sketch plus a min-heap, supports a sliding time window, and exposes add(value, ts) and get_top_k() reflecting the current window. Describe API, memory guarantees, and a test plan for bursty traffic?","answer":"Use a Count-Min Sketch with configurable dimensions (width, depth) to estimate item frequencies, combined with a min-heap keyed by estimated counts to maintain the top-K elements. Implement a sliding window using a deque that stores (timestamp, value) pairs; when adding new items, update the sketch, maintain the heap, and remove expired entries by decrementing their counts in the sketch.","explanation":"## Why This Is Asked\nThis question tests the ability to design memory-bounded streaming analytics systems using approximate data structures while maintaining timely query results.\n\n## Key Concepts\n- Streaming algorithms with bounded memory\n- Count-Min Sketch for frequency estimation\n- Min-heap for top-K maintenance\n- Sliding time window management\n- Memory-accuracy trade-offs\n\n## Code Example\n```python\nclass TopKStreaming:\n    def __init__(self, k=10, width=1000, depth=5, window=60):\n        pass\n    def add(self, value, ts):\n        pass\n    def get_top_k(self):\n        pass\n```\n\n## Follow-up","diagram":"flowchart TD\n  A[Async data stream] --> B[Count-Min Sketch update]\n  B --> C[Top-K heap maintenance]\n  A --> D[Sliding window eviction]\n  C --> E[Current top-K snapshot]","difficulty":"intermediate","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Databricks","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T05:03:18.175Z","createdAt":"2026-01-14T23:40:32.566Z"},{"id":"q-2402","question":"You're building an asyncio Python client for a rate-limited REST API. How would you implement a function fetch_with_backoff(url, method='GET', max_retries=5, session, rate_limiter, metrics) that performs retries with exponential backoff and jitter, enforces a per-endpoint token-bucket rate limit, and records latency and outcomes into metrics? Include a minimal code sketch and a test plan?","answer":"Use an asyncio fetch_with_backoff that acquires a permit from rate_limiter per endpoint, runs the request with session, and on transient failures retries with jittered exponential backoff (base ~0.2s,","explanation":"## Why This Is Asked\n\nAssesses practical mastery of asyncio, rate limiting, robust retries, and observability in real-world networked Python code, matching needs at scale (e.g., Twitter, Nvidia, Adobe).\n\n## Key Concepts\n\n- Async I/O with aiohttp or httpx\n- Per-endpoint rate limiting (token bucket)\n- Exponential backoff with jitter\n- Latency and outcome metrics collection\n- Test strategy: simulate 429, 5xx, network partitions, concurrent callers\n\n## Code Example\n\n```javascript\n// Example sketch (not executable in Python)\nasync def fetch_with_backoff(url, method='GET', max_retries=5, session, rate_limiter, metrics):\n    endpoint = extract_endpoint(url)\n    await rate_limiter.acquire(endpoint)\n    base = 0.2\n    cap = 10\n    for attempt in range(max_retries + 1):\n        t0 = time.monotonic()\n        try:\n            async with session.request(method, url) as resp:\n                latency = time.monotonic() - t0\n                metrics.record(endpoint, resp.status, latency, attempt)\n                if resp.status in {429, 500, 502, 503, 504}:\n                    raise transient_error()\n                return await resp.read()\n        except Exception:\n            latency = time.monotonic() - t0\n            metrics.record(endpoint, None, latency, attempt)\n            if attempt == max_retries:\n                raise\n            delay = min(cap, base * (2 ** attempt)) + random.uniform(0, 0.1)\n            await asyncio.sleep(delay)\n```\n\n## Follow-up Questions\n\n- How would you test the rate limiter under bursty traffic?\n- How would you extend to idempotent vs non-idempotent methods?","diagram":null,"difficulty":"intermediate","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","NVIDIA","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T16:52:41.364Z","createdAt":"2026-01-15T16:52:41.364Z"},{"id":"q-2704","question":"Design an async Python NDJSON processor that reads lines from an AsyncIterable[str], each line containing a top-level version and payload. It must validate using a dynamic, versioned Pydantic model registry, support on-the-fly hot-swapping of versions without restart, ensure per-version in-order processing, memory-bounded streaming, and backpressure to a downstream sink. Include a test scaffold showing v1→v2 migration and a runtime version swap?","answer":"Implementation: maintain a thread-safe registry mapping version to Pydantic models; parse each line to dict, look up model, validate payload, then emit to a bounded asyncio.Queue consumed by downstrea","explanation":"## Why This Is Asked\nTests runtime schema evolution, memory-bounded streaming, and backpressure integration in a single async Python pipeline.\n\n## Key Concepts\n- Async NDJSON processing\n- Versioned Pydantic models\n- Atomic registry swap\n- In-order per-version processing\n- Bounded queues and reordering buffers\n\n## Code Example\n```python\nfrom typing import AsyncIterable, Dict, Type\nfrom pydantic import BaseModel\nimport asyncio\n\nclass V1(BaseModel):\n    x: int\nclass V2(BaseModel):\n    x: int\n    y: int\n\nasync def process(input_stream: AsyncIterable[str], registry: Dict[int, Type[BaseModel]], sink: asyncio.Queue):\n    # skeleton for versioned validation and backpressure\n    pass\n```\n\n## Follow-up Questions\n- How would you test memory usage and backpressure under slow sinks?\n- How would you retire old versions without disrupting in-flight lines?","diagram":"flowchart TD\n  A[NDJSON line] --> B[Registry lookup]\n  B --> C[Validate payload]\n  C --> D[Enqueue downstream]\n  D --> E[Downstream sink]","difficulty":"advanced","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Anthropic","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T07:39:09.489Z","createdAt":"2026-01-16T07:39:09.489Z"},{"id":"q-2835","question":"Design a Python async streaming solution: implement an async function process_metrics(feed: AsyncIterator[bytes]) -> AsyncIterator[Tuple[str, int, float]] that reads a live UDP multicast binary stream of metric events. Each message contains a 4-byte big-endian epoch timestamp (seconds), a 2-byte metric_id, and an 8-byte float value. Validate metric_id against a map, maintain a 60-second rolling window per metric_id, and emit (metric_id, window_start, sum) whenever the rolling sum changes by at least 5% or every 10 seconds, whichever comes first. It must be memory-bounded and backpressure friendly with a bounded internal queue. Include a concise test plan using bursty traffic and clock skew?","answer":"Implement an async reader of a UDP multicast binary stream and a memory-bounded rolling window aggregator. Decode frames with struct.unpack('>I H d'), map metric_id to key, and maintain a 60-second sl","explanation":"## Why This Is Asked\nTests real-world streaming, binary parsing, per-key rolling windows, and memory/backpressure management—critical for high-scale services such as Discord/Cloudflare.\n\n## Key Concepts\n- Async streaming and backpressure\n- Binary protocol parsing with struct\n- Sliding window per key\n- Memory bounding via deques and eviction\n- Threshold-based emission for efficiency\n\n## Code Example\n```python\nimport asyncio\nimport struct\nfrom collections import deque, defaultdict\nfrom typing import AsyncIterator, Tuple\n\nFrame = Tuple[str, int, float]\n\nasync def process_metrics(feed: AsyncIterator[bytes]) -> AsyncIterator[Frame]:\n    q = asyncio.Queue(maxsize=1024)  # backpressure\n    metric_map = {1: 'cpu', 2: 'mem'}  # example map\n\n    async def reader():\n        async for chunk in feed:\n            await q.put(chunk)\n\n    asyncio.create_task(reader())\n\n    windows = defaultdict(lambda: deque())  # (ts, val)\n    sums = defaultdict(float)\n    window = 60\n    last_emit = defaultdict(lambda: 0.0)\n\n    while True:\n        chunk = await q.get()\n        ts, mid, val = struct.unpack('>I H d', chunk)\n        key = metric_map.get(mid, f'mid_{mid}')\n        items = windows[key]\n        items.append((ts, val))\n        sums[key] += val\n        cutoff = ts - window\n        while items and items[0][0] <= cutoff:\n            old_ts, old_v = items.popleft()\n            sums[key] -= old_v\n        current = sums[key]\n        last = last_emit[key]\n        if last == 0 or abs(current - last) / max(abs(last), 1e-9) >= 0.05 or ts - (last_emit.get(key, 0)) >= 10:\n            last_emit[key] = current\n            yield (key, ts - window, current)\n```\n\n## Follow-up Questions\n- How would you handle out-of-order frames or missing frames?\n- How would you scale this across multiple multicast groups or nodes?","diagram":"flowchart TD\n  A[UDP Multicast Listener] --> B[Binary Decode with struct]\n  B --> C[Per-Key Rolling Window (60s)]\n  C --> D[Bounded Backpressure Queue]\n  D --> E[Emission of Updates]\n  E --> F[Consumer/Sink]","difficulty":"intermediate","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Discord"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T14:10:40.389Z","createdAt":"2026-01-16T14:10:40.390Z"},{"id":"q-2930","question":"How would you implement a memory-efficient Python function process_ndjson_by_type(input_path, out_dir) that streams an NDJSON file, parses each line as JSON, and writes every line to a per-type file named '{type}.ndjson' in out_dir? The function should create files on demand, reuse file handles to keep memory constant, track per-type line counts, and skip malformed lines while continuing?","answer":"Implement a streaming NDJSON type-partitioner in Python. Read input line-by-line, json.loads each line, require a 'type' field, and write the raw line to a per-type file named '{type}.ndjson' inside o","explanation":"## Why This Is Asked\nTests ability to design a memory-efficient streaming processor for line-delimited JSON, including dynamic partitioning, file management, and robust error handling. It also validates practical testing plans for I/O-heavy code.\n\n## Key Concepts\n- Streaming I/O with line-by-line processing\n- Dynamic per-type partitioning and lazy file creation\n- Maintaining a bounded set of open file handles\n- Robust error handling for malformed lines\n\n## Code Example\n```python\nimport json\nimport os\n\ndef process_ndjson_by_type(input_path, out_dir):\n    os.makedirs(out_dir, exist_ok=True)\n    handles = {}\n    counts = {}\n    with open(input_path, 'r', encoding='utf-8') as f:\n        for line in f:\n            line = line.rstrip('\\n')\n            if not line:\n                continue\n            try:\n                obj = json.loads(line)\n            except Exception:\n                continue\n            t = obj.get('type')\n            if not isinstance(t, str):\n                continue\n            fh = handles.get(t)\n            if fh is None:\n                fh = open(os.path.join(out_dir, f\"{t}.ndjson\"), 'a', encoding='utf-8')\n                handles[t] = fh\n            fh.write(line + '\\n')\n            counts[t] = counts.get(t, 0) + 1\n    for fh in handles.values():\n        fh.close()\n    return counts\n```\n\n## Follow-up Questions\n- How would you add validation to ensure schema compatibility across partitions?\n- How would you implement unit tests that simulate large NDJSON files with varied 'type' values?","diagram":"flowchart TD\n  A[NDJSON Input] --> B{Parse Line}\n  B -- valid --> C[Write to {type}.ndjson]\n  C --> D[Counts per type]\n  B -- invalid --> E[Skip line]\n  D --> F[Return counts]","difficulty":"beginner","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T17:52:21.915Z","createdAt":"2026-01-16T17:52:21.915Z"},{"id":"q-2955","question":"Design an async NDJSON processor in Python: read lines from an AsyncIterable[str], each line is JSON with a 'service' key. Implement partitioned_aggregate(stream, key_func, sink, max_keys=4096) that maintains per-key in-order processing, bounded buffering, and backpressure to the sink. It must allow hot-swapping key_func at runtime without restart and include a small test scaffold showing two services migrating?","answer":"To meet scale and latency, implement a central dispatcher that routes each input line to a per-key queue guarded by an asyncio.Lock. Spawn a dedicated task per key that drains its queue in-order and e","explanation":"## Why This Is Asked\n\nTests ability to design a streaming, memory-bounded Python solution with per-key ordering, dynamic behavior, and backpressure in an async setting.\n\n## Key Concepts\n\n- Async NDJSON streaming\n- Per-key partitioning and in-order processing\n- Bounded buffers and downstream backpressure\n- Runtime hot-swapping of function references\n- Lightweight test scaffolds for migrations\n\n## Code Example\n\n```python\nasync def partitioned_aggregate(stream: AsyncIterable[str], key_func: Callable[[dict], str], sink: AsyncSink, max_keys: int = 4096):\n    pass  # skeleton for per-key queues, workers, and hot-swap mechanism\n```\n\n## Follow-up Questions\n\n- How do you test memory usage with many keys?\n- How handle key worker failures without cascading? \n- How to extend to multi-sink backpressure and retries?","diagram":null,"difficulty":"advanced","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Apple","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T18:58:11.997Z","createdAt":"2026-01-16T18:58:11.997Z"},{"id":"q-3183","question":"Implement a memory-friendly CSV reader in Python: create a function stream_csv_with_required(file_path: str, required_fields: List[str]) that streams rows using csv.DictReader and yields only rows where all required_fields are present and non-empty. It should count and expose the number of skipped rows, without loading the entire file into memory. Provide a brief usage example?","answer":"Define a generator that opens the file and uses csv.DictReader, yielding rows only when all required_fields exist and have non-empty values; track skipped rows with a counter. Use typing.Iterator[Dict","explanation":"## Why This Is Asked\n\nTests memory-friendly CSV streaming and simple validation.\n\n## Key Concepts\n\n- streaming I/O\n- csv module\n- generators\n- basic type hints\n\n## Code Example\n\n```python\nimport csv\nfrom typing import Dict, Iterator, List\n\ndef stream_csv_with_required(file_path: str, required_fields: List[str]) -> Iterator[Dict[str, str]]:\n    with open(file_path, newline='', encoding='utf-8') as f:\n        reader = csv.DictReader(f)\n        dropped = 0\n        for row in reader:\n            if all(row.get(k) for k in required_fields):\n                yield row\n            else:\n                dropped += 1\n```\n\n## Follow-up Questions\n\n- How would you adapt it to handle extremely large required_fields or unknown schemas?\n- What tests would you write to verify correctness and performance?","diagram":"flowchart TD\n  Start([Start]) --> OpenCSV[Open CSV]\n  OpenCSV --> ReadRow[Read Row]\n  ReadRow --> Check{AllRequired?}\n  Check -- Yes --> Emit[Emit Row]\n  Check -- No --> Drop[Skip Row]\n  Drop --> ReadRow","difficulty":"beginner","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Oracle","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T05:43:19.182Z","createdAt":"2026-01-17T05:43:19.182Z"},{"id":"q-3267","question":"Design and implement a memory-efficient Python function aggregate_csvs(file_paths, group_by, sum_cols, output_path, max_mem_mb=100). It should streaming-read CSVs, group by the given columns, and sum the specified numeric columns per group. To stay within the memory bound, spill partial aggregates to disk after processing a chunk, then merge spills into the final output. Provide a minimal working skeleton and a test plan?","answer":"Use a streaming approach: read CSVs in chunks with the csv module, build in-memory aggregates keyed by the group_by columns, and sum the sum_cols. Track approximate memory usage; when you exceed max_m","explanation":"## Why This Is Asked\n\nTests ability to implement external-memory aggregation in Python, a common production pattern when datasets exceed RAM. It probes chunked I/O, memory budgeting, and correctness when merging partial results.\n\n## Key Concepts\n\n- Streaming I/O with csv module\n- Memory-bounded aggregation\n- Spill-to-disk algorithm\n- Merging partial results\n\n## Code Example\n\n```python\n# Skeleton code illustrating the spill logic\nimport csv\nfrom collections import defaultdict\ndef aggregate(file_paths, group_by, sum_cols, output_path, max_mem_mb=100):\n    pass\n```\n\n## Follow-up Questions\n\n- How would you handle numeric types with missing values?\n- How would you parallelize spills across multiple processes?","diagram":"flowchart TD\n  A[Read CSV chunks] --> B[Update in-memory aggregates]\n  B --> C{mem limit reached?}\n  C -->|Yes| D[Flush to spill files]\n  C -->|No| E[Continue]\n  D --> F[Reset in-memory]\n  F --> E\n  E --> G[Merge spills] --> H[Write output]","difficulty":"intermediate","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T09:32:24.255Z","createdAt":"2026-01-17T09:32:24.255Z"},{"id":"q-3339","question":"Design and implement an asynchronous Python function process_jsonl_stream(source: AsyncIterable[str], model: Type[BaseModel], sink: Callable[[dict], Awaitable[None]], max_memory_mb: int) that reads JSONL lines, validates each with the provided Pydantic model, deduplicates by the line's 'id' field using a Bloom filter, and streams only unique events to sink while enforcing a hard memory cap and applying backpressure when the sink laggs?","answer":"Implement an async generator that reads JSONL lines, validates each with the provided Pydantic BaseModel, deduplicates by the line id using a BloomFilter, and forwards new events to sink with backpres","explanation":"## Why This Is Asked\n\nIn production, streaming validation with backpressure and memory constraints is common; this tests integration of async IO, validation, deduping, and memory management.\n\n## Key Concepts\n\n- Async streaming and backpressure\n- Bloom filter deduping and rotation\n- Pydantic validation and error handling\n- Memory tracking with tracemalloc\n\n## Code Example\n\n```python\n# skeleton implementation\nfrom typing import AsyncIterable, Callable, Awaitable, Type\nfrom pydantic import BaseModel\nimport asyncio, json\nfrom bloom_filter2 import BloomFilter\n\nasync def process_jsonl_stream(source: AsyncIterable[str], model: Type[BaseModel], sink: Callable[[dict], Awaitable[None]], max_memory_mb: int):\n    bf = BloomFilter(max_elements=100000, error_rate=0.01)\n    q = asyncio.Queue(maxsize=1024)\n    # simplified loop\n    async for line in source:\n        try:\n            data = json.loads(line)\n            obj = model.parse_obj(data)\n        except Exception:\n            continue\n        if obj.id in bf:\n            continue\n        bf.add(obj.id)\n        await q.put(obj.dict())\n        if q.qsize() == q.maxsize:\n            item = await q.get()\n            await sink(item)\n```\n\n## Follow-up Questions\n\n- How would you rotate BloomFilter and coordinate eviction across workers?\n- How would you test backpressure behavior with a slow sink and bursty input?","diagram":null,"difficulty":"intermediate","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Cloudflare","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T12:58:42.049Z","createdAt":"2026-01-17T12:58:42.049Z"},{"id":"q-3397","question":"Design a beginner-friendly Python function analyze_log(file_path) that streams a log file line by line and returns a dict counting occurrences of INFO, WARN, and ERROR. Ignore malformed lines; use a generator to read lines and a defaultdict for counts. Ensure it scales to multi-GB files without loading all content?","answer":"Open the file and iterate line by line to avoid memory usage. Use defaultdict(int) to tally counts for INFO, WARN, ERROR. For each line, strip, split(None, 2) to separate level, timestamp, message; sk","explanation":"## Why This Is Asked\nTests ability to stream data, keep memory footprint small, and handle noisy inputs. \n\n## Key Concepts\n- Streaming I/O\n- Counter patterns; defaultdict\n- Input validation and robustness\n\n## Code Example\n```python\nfrom collections import defaultdict\n\ndef analyze_log(path):\n    counts = defaultdict(int)\n    with open(path, 'r', encoding='utf-8') as f:\n        for line in f:\n            parts = line.strip().split(None, 2)\n            if len(parts) < 3:\n                continue\n            level, _, _ = parts\n            if level in ('INFO','WARN','ERROR'):\n                counts[level] += 1\n    return dict(counts)\n```\n\n## Follow-up Questions\n- How would you extend this to parse timestamps and compute per-hour rates?\n- How would you adapt to log rotation and concurrent writers?","diagram":"flowchart TD\n  A[Open file] --> B{Line read}\n  B --> C{Malformed?}\n  C -- Yes --> D[Skip]\n  C -- No --> E[Extract level]\n  E --> F{Level valid?}\n  F -- Yes --> G[Counts[level]++]\n  F -- No --> D\n  G --> H[End or next line]","difficulty":"beginner","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Snowflake","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T14:36:48.341Z","createdAt":"2026-01-17T14:36:48.342Z"},{"id":"q-3420","question":"Design a Python async event router that ingests NDJSON lines from an AsyncIterable[str], routes each line to a per-type Pydantic model registry based on a 'type' field, supports hot-swapping of routing rules at runtime without restart, guarantees per-type in-order processing, and uses bounded asyncio.Queues to provide backpressure to downstream sinks. Include a minimal test scaffold showing a type A mapping to V1 and a runtime swap to V2?","answer":"Design an AsyncRouter with a registry mapping type->Pydantic model, guarded by an asyncio.Lock, plus per-type bounded queues and a dedicated consumer per type to preserve in-order processing. Implemen","explanation":"## Why This Is Asked\n\nThis question tests dynamic routing, hot-swapping, per-type in-order processing, and backpressure in an async Python pipeline. It also probes correctness under concurrent updates and memory safety.\n\n## Key Concepts\n\n- Asyncio queues for per-type backpressure\n- Atomic hot-swap of routing rules with asyncio.Lock\n- Per-type in-order workers\n- Pydantic validation and error handling\n- End-to-end testing scaffold\n\n## Code Example\n\n```python\n# Skeleton illustrating the pattern; full impl expected in interview\nfrom typing import Dict, Type\nfrom pydantic import BaseModel\nimport asyncio\n\nclass Router:\n    def __init__(self, registry: Dict[str, Type[BaseModel]]):\n        self._registry = registry\n        self._lock = asyncio.Lock()\n        self._queues: Dict[str, asyncio.Queue] = {}\n        self._consumers: Dict[str, asyncio.Task] = {}\n\n    async def hot_swap(self, new_registry: Dict[str, Type[BaseModel]]):\n        async with self._lock:\n            self._registry = new_registry\n            # optionally swap queues/consumers if needed\n```\n\n## Follow-up Questions\n\n- How would you test race conditions during hot_swap?\n- How would you scale to many types while preserving fairness?","diagram":"flowchart TD\n  Ingest[NDJSON AsyncIterable[str]] --> Router[AsyncRouter]\n  Router --> Q_A[Queue A]\n  Router --> Q_B[Queue B]\n  Q_A --> SinkA[Sink A]\n  Q_B --> SinkB[Sink B]","difficulty":"advanced","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","IBM","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T15:36:30.233Z","createdAt":"2026-01-17T15:36:30.233Z"},{"id":"q-3449","question":"You have a log file that grows indefinitely containing one numeric value per line (e.g., latency in ms) along with occasional non-numeric lines. Implement a Python function running_stats(file_path) that streams lines, ignores invalid ones, and after each valid line yields a tuple (count, mean, min, max) computed so far. Keep memory usage O(1)?","answer":"Implement a streaming function that reads file_path line by line, parses each line as float, skips invalid lines, and after each valid line yields (count, mean, min, max). Maintain only four scalars (","explanation":"## Why This Is Asked\nTests ability to design a streaming statistic, avoids loading entire file, handles dirty data, and maintains simple O(1) state with a generator.\n\n## Key Concepts\n- Streaming I/O with generators\n- Online statistics (count, total, min, max)\n- Defensive parsing and error handling\n- Memory efficiency with large data\n\n## Code Example\n```python\ndef running_stats(file_path):\n    count = 0\n    total = 0.0\n    min_v = None\n    max_v = None\n    with open(file_path, 'r') as f:\n        for line in f:\n            line = line.strip()\n            try:\n                val = float(line)\n            except ValueError:\n                continue\n            count += 1\n            total += val\n            min_v = val if min_v is None else min(min_v, val)\n            max_v = val if max_v is None else max(max_v, val)\n            yield count, (total / count), min_v, max_v\n```\n\n## Follow-up Questions\n- How would you extend to handle NaN values or infinity?\n- How would you modify to support streaming from a network source with backpressure?\n","diagram":null,"difficulty":"beginner","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Plaid","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T16:46:49.618Z","createdAt":"2026-01-17T16:46:49.619Z"},{"id":"q-3513","question":"In Python, implement a real-time metrics ingester that reads an async NDJSON stream where each line has host, metric, value, and ts. Write windowed_stats(stream, window_secs) to maintain a sliding window per (host, metric) using a deque of (ts, value) plus monotonic min/max queues; emit updated stats only when they move by more than 1%. Handle slight out-of-order arrivals with a small per-key buffer. Explain design choices and edge cases?","answer":"Per-(host, metric) sliding window with a dict of keys to a small state object. Each state holds a deque for (ts, value), a running sum, and two monotonic deques for min and max. On each line: evict ol","explanation":"## Why This Is Asked\nTests ability to implement per-key sliding windows with streaming data and a memory guarantee, plus handling of out-of-order data and threshold-based emissions.\n\n## Key Concepts\n- Async streaming ingestion\n- Per-key state in O(number of active keys)\n- Sliding window with deque; monotonic min/max queues\n- Change-detection threshold to throttle output\n- Late-event buffering strategy\n\n## Code Example\n```python\nfrom collections import deque, defaultdict\n\nclass SlidingWindowPerKey:\n    def __init__(self, window_secs):\n        self.window = window_secs\n        self.keys = defaultdict(self._new_key)\n\n    def _new_key(self):\n        return {'values': deque(), 'sum': 0.0, 'minq': deque(), 'maxq': deque(), 'last': None}\n\n    def push(self, host, metric, ts, value):\n        k = (host, metric)\n        rec = self.keys[k]\n        cutoff = ts - self.window\n        while rec['values'] and rec['values'][0][0] < cutoff:\n            old_ts, old_v = rec['values'].popleft()\n            rec['sum'] -= old_v\n            if rec['minq'] and rec['minq'][0][0] == old_ts:\n                rec['minq'].popleft()\n            if rec['maxq'] and rec['maxq'][0][0] == old_ts:\n                rec['maxq'].popleft()\n        rec['values'].append((ts, value))\n        rec['sum'] += value\n        while rec['minq'] and rec['minq'][-1][1] > value:\n            rec['minq'].pop()\n        rec['minq'].append((ts, value))\n        while rec['maxq'] and rec['maxq'][-1][1] < value:\n            rec['maxq'].pop()\n        rec['maxq'].append((ts, value))\n        avg = rec['sum'] / len(rec['values'])\n        mn = rec['minq'][0][1]\n        mx = rec['maxq'][0][1]\n        count = len(rec['values'])\n        stat = (avg, mn, mx, count)\n        if rec['last'] is None or any(abs(a-b) > 0.01 * max(abs(b),1) for a,b in zip(stat, rec['last'])):\n            rec['last'] = stat\n            yield (host, metric, avg, mn, mx, count)\n```\n\n## Follow-up Questions\n- How would you test this with clock skew and bursty data?\n- How would you adapt to multiple streams with backpressure and fault tolerance?","diagram":null,"difficulty":"intermediate","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","Microsoft","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T19:29:39.599Z","createdAt":"2026-01-17T19:29:39.599Z"},{"id":"q-3532","question":"Implement a Python function parse_semver_line(line) that parses a line of the form 'name: vMAJOR.MINOR.PATCH-PRERELEASE+BUILD' where v and prerelease/build are optional. Return a dict with keys: name, major, minor, patch, prerelease (or None), build (or None). Ignore extra whitespace. If invalid, raise ValueError. The line may have additional spaces; handle edge cases like 'pkg:1.2.3' or 'pkg: v1.2.3-alpha+001'?","answer":"Use a simple, robust parser that splits on ':' and handles an optional leading 'v', optional prerelease and build segments, then coerces major/minor/patch to int. If the format is invalid, raise Value","explanation":"## Why This Is Asked\nSemantic version parsing is common in package tooling and needs robust string handling.\n\n## Key Concepts\n- Regex parsing with named groups\n- Optional segments for prerelease/build\n- Safe int conversion and error handling\n- Trimming whitespace and invalid format signaling\n\n## Code Example\n```python\ndef parse_semver_line(line: str) -> dict:\n    try:\n        name, ver = line.split(':', 1)\n        name = name.strip()\n        ver = ver.strip()\n        if ver.startswith('v'):\n            ver = ver[1:]\n        prerelease = None\n        build = None\n        if '+' in ver:\n            ver, build = ver.split('+', 1)\n        if '-' in ver:\n            ver, prerelease = ver.split('-', 1)\n        parts = ver.split('.')\n        if len(parts) != 3:\n            raise ValueError('Invalid version')\n        major, minor, patch = map(int, parts)\n        return {\n            'name': name,\n            'major': major,\n            'minor': minor,\n            'patch': patch,\n            'prerelease': prerelease,\n            'build': build\n        }\n    except Exception:\n        raise ValueError('Invalid semver line')\n```\n\n## Follow-up Questions\n- How would you extend to validate numeric ranges for majors/minors/patches?\n- How would you adapt to stream lines from a file and yield valid dicts lazily?","diagram":null,"difficulty":"beginner","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Databricks"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T20:34:14.216Z","createdAt":"2026-01-17T20:34:14.216Z"},{"id":"q-3698","question":"Design a Python async NDJSON ingest pipeline that reads lines from an AsyncIterable[str], validates each line against a per-schema Pydantic model chosen by a schema_id field, and dispatches to per-schema handlers via bounded asyncio.Queues. Implement runtime hot-swapping of schemas (without restart) and guarantee in-order processing per schema even as schemas are swapped. Include a minimal test scaffold showing schema_id=1 mapped to V1 and swapped to V2 at runtime; discuss memory, latency, and safety trade-offs?","answer":"Architect a reader that streams NDJSON via an async generator, dispatching each line to a per-schema queue keyed by schema_id. Register schema_id to Pydantic model (V1, V2). Hot-swap by atomically upd","explanation":"## Why This Is Asked\nTests ability to design a dynamic, high-throughput streaming pipeline with runtime configurability and strong ordering guarantees. It also probes memory management, backpressure, and safe hot-swapping of schemas without restart.\n\n## Key Concepts\n- Async streaming and NDJSON parsing\n- Per-key routing and bounded queues for backpressure\n- Runtime hot-swapping of schemas with atomic registry updates\n- Per-schema in-order processing and sequence tracking\n\n## Code Example\n```python\n# Pydantic models and dispatcher sketch\nfrom pydantic import BaseModel\nimport asyncio, json\n\nclass V1(BaseModel):\n    id: int; value: int\n\nclass V2(BaseModel):\n    id: int; value: int; meta: str\n\nschema_registry = {1: V1, 2: V1}\nregistry_lock = asyncio.Lock()\nqueues = {1: asyncio.Queue(maxsize=1024), 2: asyncio.Queue(maxsize=1024)}\n\nasync def reader(async_iter):\n    async for line in async_iter:\n        payload = json.loads(line)\n        sid = payload[\"schema_id\"]\n        async with registry_lock:\n            Model = schema_registry.get(sid, None)\n        if Model is None:\n            continue\n        await queues[sid].put((payload, Model))\n\n# workers would validate and emit results preserving order\n```\n\n## Follow-up Questions\n- How would you implement failure handling for invalid lines while preserving ordering?\n- How to measure throughput and detect backpressure saturation?","diagram":null,"difficulty":"advanced","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","NVIDIA","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T05:44:50.681Z","createdAt":"2026-01-18T05:44:50.681Z"},{"id":"q-3778","question":"Design a Python async streaming pipeline for a live NDJSON feed of transactions over a socket. Each line is a JSON object with 'user_id', 'amount', and 'ts'. Implement windowed aggregation: per-user total amount in 5-minute tumbling windows, emit results when a window closes, and handle out-of-order events up to 2 seconds. Use asyncio, memory-bounded structures, and a backpressure-friendly sink. Provide a minimal test plan?","answer":"Use an asyncio-based streaming pipeline that maintains per-user, 5-minute tumbling windows in memory. Parse each NDJSON line, compute window_start = floor(ts to 5 minutes), and accumulate totals in a ","explanation":"## Why This Is Asked\nTests proficiency with asynchronous streams, in-order constraints, and real-time aggregation under memory limits. It also probes how to handle clock skew and backpressure in a live ingestion path.\n\n## Key Concepts\n- Async I/O and streaming NDJSON parsing\n- Tumbling window aggregation and watermarking for out-of-order data\n- Memory-safe in-memory dictionaries with eviction/flush strategy\n- Backpressure via bounded queues and sink pacing\n\n## Code Example\n```python\nimport asyncio\nimport json\nfrom collections import defaultdict\nfrom datetime import datetime, timezone, timedelta\n\n# Placeholder outline for the core idea; concrete implementation would flesh this out\nasync def process_stream(reader, sink, window_min=300):\n    totals = defaultdict(int)  # key: (user_id, window_start)\n    # ... read lines, parse JSON, bucket into (user_id, window_start)\n    # ... periodically flush expired windows to sink (bounded)\n    pass\n```\n\n## Follow-up Questions\n- How would you adapt for multiple concurrent sinks with different backpressure characteristics?\n- How would you test with clock skew, bursty traffic, and late events beyond the watermark?","diagram":"flowchart TD\n  A[NDJSON line] --> B[Parse JSON]\n  B --> C[Compute window_start]\n  C --> D[Accumulate totals[(user_id, window_start)]]\n  D --> E[Backpressure sink (bounded queue)]\n  E --> F[Emit on window close]\n  G[Watermark for out-of-order handling] --> H[Flush expired windows]","difficulty":"intermediate","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T09:31:50.697Z","createdAt":"2026-01-18T09:31:50.697Z"},{"id":"q-4343","question":"You're processing a CSV log of signups with fields: user_id, email, signup_ts. The file is potentially several gigabytes. Implement a function yield_valid_signups(path) that streams lines using the csv module, validates email with a simple regex, and verifies signup_ts is a valid ISO 8601 timestamp. Yield dictionaries for valid lines only; ignore malformed lines and extra columns. Ensure minimal memory usage?","answer":"Open the file and iterate with csv.DictReader to avoid loading all rows. For each row, validate email with a basic regex like r'^[^@]+@[^@]+\\.[^@]+$' and parse signup_ts with datetime.fromisoformat, s","explanation":"## Why This Is Asked\nTests ability to design streaming data pipelines with Python's standard library, validating inputs on the fly and ignoring bad data, which is common in real data ingestion tasks at scale.\n\n## Key Concepts\n- Streaming I/O with csv.DictReader\n- Lightweight validation (regex + ISO parsing)\n- In-place yields and memory efficiency\n- Robust error handling with try/except\n\n## Code Example\n```python\nimport csv\nimport re\nfrom datetime import datetime\n\nEMAIL_RE = re.compile(r'^[^@]+@[^@]+\\.[^@]+$')\n\ndef yield_valid_signups(path):\n    with open(path, 'r', newline='') as f:\n        reader = csv.DictReader(f)\n        for row in reader:\n            email = row.get('email')\n            ts = row.get('signup_ts')\n            if not email or not ts:\n                continue\n            if not EMAIL_RE.match(email):\n                continue\n            try:\n                datetime.fromisoformat(ts)\n            except Exception:\n                continue\n            yield {'user_id': row.get('user_id'), 'email': email, 'signup_ts': ts}\n```\n\n## Follow-up Questions\n- How would you adapt this to report invalid line counts without slowing throughput?\n- How would you unit-test this function with a small sample file?","diagram":"flowchart TD\n  A[Start] --> B[Open CSV]\n  B --> C[DictReader]\n  C --> D{Line valid?}\n  D -->|Yes| E[Yield dict]\n  D -->|No| F[Skip line]\n  E --> G[Process next]\n  G --> C","difficulty":"beginner","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":null,"companies":["NVIDIA","Slack","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-19T14:52:11.650Z","createdAt":"2026-01-19T14:52:11.650Z"},{"id":"q-4466","question":"Build an asyncio-based NDJSON pipeline that ingests lines with a 'module' key, fetches runtime transformation code from a registry, and applies transform(payload) in a sandboxed environment. Requirements: preserve input order, cap per-item CPU/memory, support hot-reload of module code without restart, and apply backpressure to downstream via bounded queues. Provide a test scaffold?","answer":"Leverage an asyncio pipeline: input NDJSON lines → per-record module loader → sandboxed worker executes transform(payload) with memory/CPU caps (e.g., 20MB, 50ms) and restricted builtins → results mer","explanation":"## Why This Is Asked\nTests real-world capability to safely execute user-provided transforms at scale, ensuring safety, ordering, and throughput without downtime.\n\n## Key Concepts\n- Async pipelines with NDJSON\n- Dynamic module loading and hot-reload\n- Sandboxing and resource limits (memory, CPU/time)\n- Ordering guarantees and backpressure\n- Observability and testing strategies\n\n## Code Example\n```python\n# Skeleton illustrating components and flow\nimport asyncio\nimport concurrent.futures\n\n# Pseudo interfaces: registry_fetch, sandbox_run, merge_ordered\n\ndef registry_fetch(module_id, version): ...\n\ndef sandbox_run(code, payload): ...\n\nasync def pipeline(input_q, output_q):\n    loop = asyncio.get_event_loop()\n    while True:\n        item = await input_q.get()\n        # load module, run sandboxed transform, push to output_q preserving order\n        yield\n```\n\n## Follow-up Questions\n- How would you test hot-reload correctness without dropping items?\n- How would you monitor and limit memory growth per module in production?","diagram":"flowchart TD\n  A[NDJSON Input] --> B[Module Loader]\n  B --> C[Sandboxed Transform]\n  C --> D[Order Merger]\n  D --> E[Downstream Sink]","difficulty":"advanced","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Amazon","Meta","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-19T19:52:55.695Z","createdAt":"2026-01-19T19:52:55.695Z"},{"id":"q-4486","question":"Implement a memory-efficient Python generator function user_action_stream(file_path, batch_size=1000) that streams a nightly user activity log line by line. Each line is 'user_id action' (e.g., 'u123 login'). Ignore malformed lines. Maintain per-user action counts in memory and yield a snapshot dict every batch_size lines with shape {user_id: {action: count}}. Explain your approach and edge cases?","answer":"This uses a generator that reads the file line by line, parses with split(), and updates a dict of user_id -> Counter(actions). Malformed lines are skipped. After batch_size hits, yield a shallow copy","explanation":"## Why This Is Asked\nThis tests streaming, dictionary-based aggregation, and careful memory usage when handling very large files. It also probes handling of malformed data and how batch flushing affects memory and latency.\n\n## Key Concepts\n- Streaming I/O with generators\n- In-memory aggregation by key\n- Robust parsing and error handling\n- Memory-time trade-offs via batch flushing\n\n## Code Example\n```python\nfrom collections import defaultdict, Counter\n\ndef user_action_stream(file_path, batch_size=1000):\n    counts = defaultdict(Counter)\n    seen = 0\n    with open(file_path, 'r') as f:\n        for line in f:\n            parts = line.strip().split()\n            if len(parts) != 2:\n                continue\n            user, action = parts\n            counts[user][action] += 1\n            seen += 1\n            if seen % batch_size == 0:\n                yield {u: dict(c) for u, c in counts.items()}\n    if seen % batch_size != 0:\n        yield {u: dict(c) for u, c in counts.items()}\n```\n\n## Follow-up Questions\n- How would you reset state for a new log file or rotate files?\n- How would you test with malformed lines and varying batch sizes?","diagram":null,"difficulty":"beginner","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Bloomberg","Cloudflare","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-19T20:48:15.092Z","createdAt":"2026-01-19T20:48:15.092Z"},{"id":"q-4658","question":"Design an asynchronous NDJSON pipeline that consumes lines from an AsyncIterable[str]. Each line contains: partition (str), opcode (int), payload (dict), version (int). Build a dynamic registry mapping (opcode, version) to a handler that transforms payload into an output dict. The registry must support hot-swapping a handler for a given (opcode, version) at runtime without stopping the pipeline, using an atomic switch. Ensure per-partition in-order processing with bounded queues and backpressure to a downstream sink. Include a test scaffold swapping (opcode=5, version=1) to version=2 mid-stream and verify no data loss or reordering?","answer":"I’d implement per-partition workers with a bounded asyncio.Queue. A global, lock-protected registry maps (opcode, version) to a coroutine-friendly handler. Swaps create a new registry copy and atomica","explanation":"## Why This Is Asked\nReveals capability to perform zero-downtime upgrades in streaming data paths, with ordering guarantees, backpressure, and dynamic state like hot-swap of handlers.\n\n## Key Concepts\n- AsyncNDJSON processing with per-partition ordering\n- Dynamic, atomic hot-swap of (opcode, version) handlers\n- Backpressure via bounded queues and per-partition workers\n- Safe runtimes swaps without stopping dataplane\n\n## Code Example\n```python\nimport asyncio\nfrom typing import Callable, Dict, Tuple, Any\n\nHandler = Callable[[dict], dict]\n\nclass HotSwapRegistry:\n    def __init__(self, initial: Dict[Tuple[int,int], Handler]):\n        self._lock = asyncio.Lock()\n        self._registry = initial\n\n    async def get(self, opcode: int, version: int) -> Handler:\n        async with self._lock:\n            return self._registry.get((opcode, version))\n\n    async def swap(self, opcode: int, version: int, handler: Handler):\n        async with self._lock:\n            new = dict(self._registry)\n            new[(opcode, version)] = handler\n            self._registry = new\n```\n\n## Follow-up Questions\n- How would you ensure fairness if a handler blocks on I/O?\n- How do you handle handler exceptions without stopping the pipeline?\n- What metrics would you surface for swap latency and backpressure?","diagram":"flowchart TD\n  A[NDJSON Source] --> B[Partitioned Queues]\n  B --> C[Per-Partition Consumers]\n  C --> D[Sink with Backpressure]\n  E[Hot-Swap Registry] --> F[Atomic Swap]\n  D --> G[Feedback/Monitoring]","difficulty":"advanced","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Adobe","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T07:04:02.389Z","createdAt":"2026-01-20T07:04:02.389Z"},{"id":"q-474","question":"Write a Python function that takes a list of integers and returns the sum of all even numbers. How would you handle edge cases?","answer":"Use a generator expression with sum() for optimal performance. Implement input validation to handle edge cases robustly. The function gracefully handles empty lists by returning 0 and ignores non-integer elements to prevent TypeErrors. For large datasets, the generator approach provides memory efficiency by processing items one at a time.","explanation":"## Solution\n\n```python\ndef sum_even_numbers(numbers):\n    if not isinstance(numbers, list):\n        raise TypeError(\"Input must be a list\")\n    return sum(num for num in numbers if isinstance(num, int) and num % 2 == 0)\n```\n\n## Key Points\n\n- **Generator Expression**: Memory-efficient processing of large datasets\n- **Input Validation**: Robust error handling for invalid input types\n- **Edge Case Handling**: Returns 0 for empty lists, safely skips non-integers\n- **Performance**: Single pass algorithm with O(n) time complexity\n\n## Complexity\n\n- **Time**: O(n) - iterates through each element once\n- **Space**: O(1) - constant extra space using generator expression","diagram":"flowchart TD\n  A[Input List] --> B{Validate Type}\n  B -->|Valid| C[Filter Even Numbers]\n  B -->|Invalid| D[Raise TypeError]\n  C --> E[Sum Results]\n  E --> F[Return Sum]","difficulty":"beginner","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Microsoft","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-09T09:05:08.903Z","createdAt":"2025-12-26 12:51:05"},{"id":"q-4808","question":"You're building a real-time log processor in Python. NDJSON lines arrive via a socket. Each line has timestamp (ISO 8601), service, level (DEBUG/INFO/WARN/ERROR), and message. Implement an async function stream_counts(source, window_seconds) that consumes an async iterator of NDJSON lines, validates with a Pydantic model, and keeps a sliding window of the last window_seconds by timestamp. It should emit every second a dict mapping (service, level) to counts in the window; memory bounded by active groups. Drop lines older than the window; include newer lines even if late arrival?","answer":"Use an async generator to consume NDJSON lines, validate with a Pydantic model, and maintain a deque of (ts, key) entries for the sliding window. Keep a dict of counts per (service, level). Every seco","explanation":"## Why This Is Asked\nTests real-time streaming, validation, and time-windowed aggregation with memory constraints. It also evaluates handling of out-of-order events and backpressure in an async context.\n\n## Key Concepts\n- Async generators for streaming data\n- Pydantic for strict schema validation\n- Deque for time-based sliding window\n- Grouped counters by (service, level) with O(G) memory where G is active groups\n\n## Code Example\n```python\nfrom collections import deque, defaultdict\nfrom datetime import datetime, timezone\nfrom pydantic import BaseModel, ValidationError\nimport asyncio\nimport json\n\nclass Event(BaseModel):\n    timestamp: datetime\n    service: str\n    level: str\n\nasync def stream_counts(source, window_seconds: int):\n    window = deque()  # stores (ts, key) where key = (service, level)\n    counts = defaultdict(int)\n    interval = 1.0\n    last_emit = asyncio.get_event_loop().time()\n\n    async for line in source:\n        try:\n            obj = json.loads(line)\n            obj[\"timestamp\"] = datetime.fromisoformat(obj[\"timestamp\"]).astimezone(timezone.utc)\n            ev = Event(**obj)\n        except (ValueError, ValidationError):\n            continue\n        ts = ev.timestamp.timestamp()\n        key = (ev.service, ev.level)\n        window.append((ts, key))\n        counts[key] += 1\n\n        # evict old entries\n        cutoff = datetime.utcnow().timestamp() - window_seconds\n        while window and window[0][0] < cutoff:\n            old_ts, old_key = window.popleft()\n            counts[old_key] -= 1\n            if counts[old_key] == 0:\n                del counts[old_key]\n\n        now = asyncio.get_event_loop().time()\n        if now - last_emit >= interval:\n            last_emit = now\n            yield dict(counts)\n```\n\n## Follow-up Questions\n- How would you adapt this to multi-process workers if the event stream is extremely high volume?\n- How would you add backpressure signaling to the producer when the window grows too large?","diagram":null,"difficulty":"intermediate","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Google","Robinhood","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T13:34:12.135Z","createdAt":"2026-01-20T13:34:12.135Z"},{"id":"q-4845","question":"Design a streaming binary frame decoder in Python that reads from an asyncio stream with length-prefixed frames: each frame starts with a 4-byte big-endian length, then 1-byte type, 2-byte version, then payload; implement a registry of per-type parsers, support runtime hot-swapping of a parser, ensure frames are decoded in-order, and backpressure is applied via a bounded asyncio.Queue; include a minimal example of type 0x01 v1 and a runtime switch to a new v2 parser?","answer":"Design an async binary frame decoder: read 4-byte big-endian length, then 1-byte type + 2-byte version, followed by payload. Maintain a (type,version) parser registry; parse payload into dict and push","explanation":"## Why This Is Asked\n\nTests practical streaming, memory boundaries, and runtime hot-swap safety.\n\n## Key Concepts\n\n- Async framing and buffering\n- Per-type/version parser registry with hot-swap\n- Backpressure via bounded queues\n- Concurrency safety with locks\n\n## Code Example\n\n```python\n# simplified sketch\n```\n\n## Follow-up Questions\n\n- How would you handle partial frames and framing errors?\n- How to measure backpressure impact in production?\n","diagram":"flowchart TD\n  A[AsyncReader] --> B[Frame Framing]\n  B --> C[Registry lookup]\n  C --> D[Parse payload]\n  D --> E[Queue downstream]","difficulty":"advanced","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Discord","MongoDB","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T16:06:39.744Z","createdAt":"2026-01-20T16:06:39.744Z"},{"id":"q-4876","question":"You have an NDJSON stream of user events; each line is a JSON object that should contain user_id (str), email (str), action (str), ts (int). Write a function sanitize_events(stream) that reads an iterable of JSON strings, validates shape, skips invalid lines, masks the email (mask the local part to asterisks except first and last chars) and redacts any 'phone' field if present, then yields sanitized dicts. Must be memory-efficient (O(1) extra) and suitable for streaming gigabytes of data. Include a simple test plan with edge cases?","answer":"I would implement sanitize_events(stream) as a streaming generator: for line in stream, json.loads; validate user_id (str), email (str), action (str), ts (int); skip invalid. Mask email local part: ke","explanation":"## Why This Is Asked\nTests ability to design a streaming data pipeline with on-the-fly validation and data sanitization without loading all data in memory.\n\n## Key Concepts\n- Streaming generators\n- Input validation and error handling\n- PII masking strategies\n- In-place data transformation with O(1) extra memory\n\n## Code Example\n```javascript\ndef sanitize_events(stream):\n    import json\n    for line in stream:\n        try:\n            obj = json.loads(line)\n        except Exception:\n            continue\n        if not isinstance(obj, dict):\n            continue\n        if not all(k in obj for k in (\"user_id\",\"email\",\"action\",\"ts\")):\n            continue\n        if not isinstance(obj[\"user_id\"], str):\n            continue\n        if not isinstance(obj[\"email\"], str):\n            continue\n        if not isinstance(obj[\"action\"], str):\n            continue\n        if not isinstance(obj[\"ts\"], int):\n            continue\n        email = obj[\"email\"]\n        local, _, domain = email.partition(\"@\")\n        if len(local) <= 2:\n            masked_local = \"*\" * len(local)\n        else:\n            masked_local = local[0] + \"*\" * (len(local) - 2) + local[-1]\n        masked_email = masked_local + \"@\" + domain\n        out = {\"user_id\": obj[\"user_id\"], \"email\": masked_email, \"action\": obj[\"action\"], \"ts\": obj[\"ts\"]}\n        if \"phone\" in obj:\n            out[\"phone\"] = \"REDACTED\"\n        yield out\n```","diagram":"flowchart TD\nA[Start] --> B[Read line]\nB --> C[Parse JSON]\nC --> D{Valid?}\nD -->|No| E[Skip line]\nD -->|Yes| F[Mask email]\nF --> G[Redact phone]\nG --> H[Emit sanitized dict]\nH --> A","difficulty":"beginner","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Databricks","Discord","LinkedIn"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T17:05:27.142Z","createdAt":"2026-01-20T17:05:27.142Z"},{"id":"q-4942","question":"Implement an async Python generator ingest(url, window_seconds, max_in_flight) that streams a gzip-compressed NDJSON payload over HTTP, decompresses on the fly, validates each line with a Pydantic model Event(id: str, ts: int, payload: dict), and deduplicates by id within a sliding window. It must be memory-bounded (no large buffering), apply backpressure via max_in_flight, and yield only the first occurrence per id in the window. Include a concrete test plan?","answer":"Use aiohttp to stream a gzip NDJSON, decompressing as data arrives; validate each line with Pydantic Event; maintain a per-id timestamp map and prune entries older than window_seconds; emit only the f","explanation":"## Why This Is Asked\n\nAssesses streaming I/O, in-flight backpressure, and in-memory dedup logic under memory constraints.\n\n## Key Concepts\n\n- Async HTTP streaming with aiohttp\n- On-the-fly gzip decompression\n- Pydantic model validation\n- Sliding-window deduplication with pruning\n- Bounded backpressure via asyncio.Queue\n\n## Code Example\n\n```python\n# sketch of types and flow (not full implementation)\nfrom pydantic import BaseModel\n\nclass Event(BaseModel):\n    id: str\n    ts: int\n    payload: dict\n\nasync def ingest(url: str, window_seconds: int, max_in_flight: int):\n    pass  # implementation would stream, validate, deduplicate, yield\n```\n\n## Follow-up Questions\n\n- How would clock drift affect dedup windowing?\n- How would you test behavior under bursty traffic with backpressure?","diagram":null,"difficulty":"intermediate","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Airbnb","Apple","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T20:32:24.307Z","createdAt":"2026-01-20T20:32:24.307Z"},{"id":"q-5005","question":"Design an **asynchronous** Python **aggregator** that ingests **NDJSON** lines from multiple producers (each line includes a **source** field) and emits deduplicated, in-order results downstream. Implement **1-second** tumbling windows for per-source dedup, memory-bounded streaming, and bound backpressure via per-source **asyncio.Queues**; ensure fair scheduling between sources?","answer":"To implement this asynchronous aggregator, create a Dispatcher that assigns incoming NDJSON lines to per-source queues (maxsize N). Maintain a time-based deduplication map: for each source, keep a deque of seen content hashes with timestamps; drop a line if its hash exists within the current 1-second window. Use asyncio.gather() with round-robin scheduling to ensure fair processing across sources, and implement bounded queues to apply backpressure when downstream processing slows.","explanation":"## Why This Is Asked\nThis question tests the ability to design a streaming architecture that handles backpressure, maintains deterministic per-source ordering, and provides runtime memory guarantees under high contention scenarios.\n\n## Key Concepts\n- Asyncio for concurrent processing and per-source bounded queues\n- Round-robin scheduling for fair source processing\n- 1-second tumbling windows for time-based deduplication\n- Backpressure management via bounded queues and shared downstream sink\n- Memory-bounded streaming with time-based data structures\n\n## Code Example\n```python\nimport asyncio\nimport hashlib\nimport json\nfrom collections import deque\nfrom datetime import datetime, timedelta\n\nclass AsyncAggregator:\n    def __init__(self, max_queue_size=1000, window_seconds=1):\n        self.queues = {}\n        self.max_queue_size = max_queue_size\n        self.window_seconds = window_seconds\n        self.dedup_maps = {}\n        self.running = False\n    \n    async def add_source(self, source_id):\n        self.queues[source_id] = asyncio.Queue(maxsize=self.max_queue_size)\n        self.dedup_maps[source_id] = deque()\n    \n    def _get_hash(self, line_data):\n        content = json.dumps(line_data, sort_keys=True)\n        return hashlib.md5(content.encode()).hexdigest()\n    \n    def _is_duplicate(self, source_id, line_hash, timestamp):\n        dedup_map = self.dedup_maps[source_id]\n        \n        # Remove old entries outside window\n        while dedup_map and (timestamp - dedup_map[0][1]).total_seconds() > self.window_seconds:\n            dedup_map.popleft()\n        \n        # Check for duplicate\n        for existing_hash, _ in dedup_map:\n            if existing_hash == line_hash:\n                return True\n        \n        dedup_map.append((line_hash, timestamp))\n        return False\n    \n    async def ingest_line(self, line_data):\n        source_id = line_data.get('source')\n        if source_id not in self.queues:\n            await self.add_source(source_id)\n        \n        try:\n            await self.queues[source_id].put(line_data)\n        except asyncio.QueueFull:\n            # Apply backpressure - could log or handle differently\n            pass\n    \n    async def process_source(self, source_id):\n        queue = self.queues[source_id]\n        \n        while self.running or not queue.empty():\n            try:\n                line_data = await asyncio.wait_for(queue.get(), timeout=0.1)\n                \n                line_hash = self._get_hash(line_data)\n                timestamp = datetime.now()\n                \n                if not self._is_duplicate(source_id, line_hash, timestamp):\n                    await self.emit_downstream(line_data)\n                \n                queue.task_done()\n            except asyncio.TimeoutError:\n                continue\n    \n    async def emit_downstream(self, line_data):\n        # Implementation for downstream processing\n        pass\n    \n    async def start(self):\n        self.running = True\n        \n        # Start processing tasks for each source\n        tasks = []\n        for source_id in self.queues:\n            task = asyncio.create_task(self.process_source(source_id))\n            tasks.append(task)\n        \n        # Use round-robin scheduling via asyncio.gather\n        await asyncio.gather(*tasks)\n    \n    async def stop(self):\n        self.running = False\n```","diagram":null,"difficulty":"advanced","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Anthropic","Apple","Coinbase"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T05:37:24.448Z","createdAt":"2026-01-20T23:37:56.110Z"},{"id":"q-5027","question":"Write a Python generator function stream_top_k_on_score(file_path, k) that reads a JSONL file where each line is a JSON object with fields 'user_id' (string) and 'score' (float). Ignore lines that are not valid JSON or lack fields. Maintain a min-heap of size k to track the top k by score (tie-break by smaller user_id). After processing each valid line, yield the current top-k as a list of (user_id, score) sorted by descending score?","answer":"Maintain a min-heap of (score, user_id) with a maximum size of k. Read the file line by line, parsing each line with json.loads() within a try/except block to skip invalid JSON. For valid entries with both 'user_id' and 'score' fields, add them to the heap using these rules: if the heap size is less than k, push the new entry; otherwise, replace the root if the new score is higher than the minimum score, or if scores are equal but the new user_id is lexicographically smaller. After processing each valid line, yield the current top-k results sorted by descending score and then by user_id.","explanation":"## Why This Is Asked\nThis problem tests your ability to implement streaming algorithms with bounded memory while providing real-time updates. It's a common pattern in data processing pipelines where you need to maintain top-k results without loading entire datasets into memory.\n\n## Key Concepts\n- **Generator patterns**: Yielding results incrementally enables memory-efficient processing\n- **Heap operations**: Using heapq provides O(log k) insertions and maintains the top-k efficiently\n- **Error handling**: Robust JSON parsing ensures the pipeline continues with malformed data\n- **Deterministic tie-breaking**: Consistent sorting order is crucial for reproducible results\n\n## Code Example\n```python\nimport json\nimport heapq\n\ndef stream_top_k_on_score(file_path, k):\n    \"\"\"Stream top-k results by score from a JSONL file.\n    \n    Args:\n        file_path: Path to JSONL file with user_id and score fields\n        k: Number of top results to maintain\n        \n    Yields:\n        List of (user_id, score) tuples sorted by descending score\n    \"\"\"\n    heap = []  # min-heap storing (score, user_id)\n    \n    with open(file_path, 'r', encoding='utf-8') as f:\n        for line in f:\n            try:\n                data = json.loads(line)\n                user_id = str(data['user_id'])\n                score = float(data['score'])\n                \n                # Maintain heap of size k\n                if len(heap) < k:\n                    heapq.heappush(heap, (score, user_id))\n                elif score > heap[0][0] or (score == heap[0][0] and user_id < heap[0][1]):\n                    heapq.heapreplace(heap, (score, user_id))\n                    \n            except (json.JSONDecodeError, KeyError, ValueError, TypeError):\n                continue  # Skip malformed lines\n            \n            # Yield sorted results: descending score, then ascending user_id\n            yield [(uid, sc) for sc, uid in sorted(heap, key=lambda x: (-x[0], x[1]))]\n```","diagram":"flowchart TD\n  A[Start] --> B[Open file]\n  B --> C[Read line]\n  C --> D{Valid JSON?}\n  D -- Yes --> E[Parse fields]\n  E --> F[Update heap]\n  F --> G[Yield top-k]\n  G --> C\n  D -- No --> H[Skip line]\n  H --> C","difficulty":"beginner","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Amazon","Snap","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T05:21:23.237Z","createdAt":"2026-01-21T02:25:38.011Z"},{"id":"q-503","question":"How would you implement a distributed rate limiter using Redis with sliding window algorithm to handle 10,000 requests per second across multiple API servers?","answer":"Use Redis sorted sets with timestamps as scores and request IDs as members. For each request, ZADD current timestamp, ZREMRANGEBYSCORE to remove old entries, then ZCARD to count. Use Lua script for atomic operations to prevent race conditions.","explanation":"## Implementation\n\nUse Redis sorted sets with timestamps as scores:\n\n```python\n# Lua script for atomic rate limiting\nlocal key = KEYS[1]\nlocal window = tonumber(ARGV[1])\nlocal limit = tonumber(ARGV[2])\nlocal now = tonumber(ARGV[3])\n\nredis.call('zadd', key, now, now)\nredis.call('zremrangebyscore', key, 0, now - window)\nlocal count = redis.call('zcard', key)\nredis.call('expire', key, window)\n\nreturn count <= limit\n```\n\n## Key Considerations\n\n- **Atomicity**: Lua script prevents race conditions\n- **Memory**: Sorted sets automatically clean old entries\n- **Performance**: O(log N) operations, handles 10,000+ RPS\n- **Scalability**: Single Redis instance can serve multiple API servers\n- **Sliding Window**: Provides accurate rate limiting over time windows","diagram":"flowchart TD\n  A[API Request] --> B[Redis Lua Script]\n  B --> C{Within Limit?}\n  C -->|Yes| D[Process Request]\n  C -->|No| E[Return 429]\n  F[Cleanup Old Entries] --> B\n  G[Set TTL] --> B","difficulty":"advanced","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","LinkedIn","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":["redis","sliding window","sorted sets","lua script","rate limiting","distributed"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2026-01-09T03:44:22.264Z","createdAt":"2025-12-26 12:51:06"},{"id":"q-5082","question":"You're given a huge CSV of transactions with columns: user_id, amount, timestamp, status. Implement top_n_users_by_amount(file_path, n) that streams the file (first line is header) and returns a list of (user_id, total_amount) for the top N users by total amount. Do not load entire file; keep memory O(N). Handle invalid rows gracefully and use Decimal for precision. Explain tie handling?","answer":"I would stream the CSV with csv.reader, accumulate per-user totals in a dict, and maintain a max-heap of (total, user_id). After processing all lines, pop valid entries to form the top-N. Use Decimal ","explanation":"## Why This Is Asked\nShows streaming data handling, memory-conscious top-k tracking, and exact arithmetic with Decimal.\n\n## Key Concepts\n- Streaming I/O with csv\n- Memory-efficient top-k using a heap\n- Decimal for monetary values\n\n## Code Example\n```python\nfrom decimal import Decimal\nimport csv\nimport heapq\nfrom collections import defaultdict\n\ndef top_n_users_by_amount(file_path, n):\n    totals = defaultdict(Decimal)\n    heap = []  # max-heap via negation\n\n    with open(file_path, newline='') as f:\n        reader = csv.reader(f)\n        header = next(reader, None)\n        if not header:\n            return []\n        try:\n            uid_idx = header.index('user_id')\n            amt_idx = header.index('amount')\n        except ValueError:\n            raise ValueError('Required columns missing')\n\n        for row in reader:\n            if len(row) <= max(uid_idx, amt_idx):\n                continue\n            try:\n                uid = row[uid_idx]\n                amt = Decimal(row[amt_idx])\n            except Exception:\n                continue\n            totals[uid] += amt\n            heapq.heappush(heap, (totals[uid], uid))\n\n    # extract top-N using lazy validation\n    result = []\n    seen = set()\n    while heap and len(result) < n:\n        total, uid = heapq.heappop(heap)\n        if uid in seen:\n            continue\n        if totals[uid] == total:\n            result.append((uid, total))\n            seen.add(uid)\n    return sorted(result, key=lambda x: x[1], reverse=True)\n```\n\n## Follow-up Questions\n- How would you adapt this for a streaming API (no local file)?\n- How to handle very large n relative to distinct users?\n- What if amounts use different currencies?","diagram":"flowchart TD\n  A[Stream CSV] --> B[Parse header]\n  B --> C[Identify indices for user_id, amount]\n  C --> D[Accumulate totals per user]\n  D --> E[Push to max-heap on update]\n  E --> F[Pop to obtain top-N]\n  F --> G[Return sorted top-N]","difficulty":"beginner","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Anthropic","Goldman Sachs","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T05:43:19.118Z","createdAt":"2026-01-21T05:43:19.118Z"},{"id":"q-559","question":"You're building a data processing pipeline that needs to handle large CSV files efficiently. How would you implement a memory-efficient solution using Python generators to process files that don't fit in RAM?","answer":"Use Python's `csv` module with generator functions to read files line-by-line. Implement `yield` in a custom generator that processes chunks, avoiding loading entire files. Combine with `itertools.islice` for batch processing.","explanation":"## Memory-Efficient CSV Processing\n\n- **Generator Pattern**: Use `yield` to process rows incrementally\n- **csv.reader**: Built-in CSV parser handles edge cases\n- **Chunk Processing**: Batch rows with `itertools.islice`\n- **Memory Control**: Constant O(1) memory usage\n\n```python\nimport csv\nfrom itertools import islice\n\ndef csv_generator(filename, chunk_size=1000):\n    with open(filename) as f:\n        reader = csv.DictReader(f)\n        while True:\n            chunk = list(islice(reader, chunk_size))\n            if not chunk:\n                break\n            yield chunk\n```\n\n## Key Benefits\n\n- Processes files of any size without memory constraints\n- Maintains constant memory usage regardless of file size\n- Allows streaming data transformation and analysis\n- Integrates seamlessly with existing Python data tools","diagram":"flowchart TD\n  A[Large CSV File] --> B[csv.reader]\n  B --> C[Generator Function]\n  C --> D[Process Chunk]\n  D --> E[yield Results]\n  E --> F[Next Chunk]\n  F --> D","difficulty":"intermediate","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Snap","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-08T11:56:21.596Z","createdAt":"2025-12-26 12:51:07"},{"id":"q-588","question":"How would you implement a rate limiter using Python's asyncio to prevent API abuse while maintaining high throughput?","answer":"Use asyncio.Semaphore with token bucket algorithm. Track request timestamps in a deque, calculate wait time based on rate limit, and use asyncio.sleep() for throttling. Implement per-client limits usi","explanation":"## Rate Limiting Implementation\n\n### Token Bucket Approach\n- Use asyncio.Semaphore for concurrent request control\n- Implement token bucket with asyncio.gather() for batch processing\n- Track timestamps in collections.deque for O(1) operations\n\n### Async Pattern\n```python\nimport asyncio\nfrom collections import deque\n\nclass RateLimiter:\n    def __init__(self, rate_limit, time_window):\n        self.rate_limit = rate_limit\n        self.time_window = time_window\n        self.requests = deque()\n    \n    async def acquire(self):\n        now = asyncio.get_event_loop().time()\n        # Remove old requests\n        while self.requests and self.requests[0] <= now - self.time_window:\n            self.requests.popleft()\n        \n        if len(self.requests) >= self.rate_limit:\n            sleep_time = self.time_window - (now - self.requests[0])\n            await asyncio.sleep(sleep_time)\n            return await self.acquire()\n        \n        self.requests.append(now)\n```\n\n### Production Considerations\n- Use Redis for distributed rate limiting across multiple instances\n- Implement sliding window for more accurate rate limiting\n- Add circuit breaker pattern for API protection\n- Monitor and log rate limit violations for debugging","diagram":"flowchart TD\n  A[Request] --> B{Check Rate Limit}\n  B -->|Within Limit| C[Process Request]\n  B -->|Exceeded| D[Calculate Wait Time]\n  D --> E[Async Sleep]\n  E --> B\n  C --> F[Update Request Log]\n  F --> G[Return Response]","difficulty":"intermediate","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","DoorDash","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":["asyncio","rate limiter","semaphore","token bucket","api abuse","throughput","deque"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2025-12-27T04:58:08.145Z","createdAt":"2025-12-27T01:14:34.507Z"},{"id":"q-852","question":"Design a memory-efficient streaming dedup pipeline in Python that reads a multi-GB log file line-by-line and prints only the first occurrence of each unique line, skipping duplicates, while persisting dedup state across restarts. Use a probabilistic data structure with a configurable false-positive rate and provide a minimal runnable snippet?","answer":"Use a Bloom-filter-backed streaming pipeline: read lines via a generator, check a Bloom filter; if a line is not present, yield it and add it to the filter; if present, skip. Persist the filter's bit ","explanation":"## Why This Is Asked\nTests ability to design streaming data processing with memory constraints, probabilistic data structures, and persistence. It touches trade-offs between false positives, memory, and restart behavior; requires specifying parameters and a runnable snippet.\n\n## Key Concepts\n- Streaming IO and generators\n- Bloom filters and false-positive rate\n- State persistence across restarts\n- Parameter tuning m, k for given n, p\n\n## Code Example\n```javascript\n# Python-like BloomFilter (illustrative)\nclass BloomFilter:\n    def __init__(self, m, k):\n        self.bits = bytearray(m // 8 + 1)\n        self.k = k\n    def _hashes(self, item):\n        h1 = hash(item)\n        h2 = (hash(str(item)) * 1103515245) & 0x7fffffff\n        for i in range(self.k):\n            yield (h1 + i * h2) % len(self.bits)\n    def add(self, item):\n        for idx in self._hashes(item):\n            self.bits[idx] = 1\n    def __contains__(self, item):\n        return all(self.bits[idx] for idx in self._hashes(item))\n```\n\n## Follow-up Questions\n- How would you adapt this for distributed workers?\n- How would you handle log rotation and cache eviction?","diagram":"flowchart TD\n  A[Read line] --> B[Check Bloom filter]\n  B --> C{New}\n  C --> D[Emit line and add to Bloom]\n  B --> E{Duplicate}\n  E --> F[Skip]","difficulty":"intermediate","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","MongoDB","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:35:10.938Z","createdAt":"2026-01-12T13:35:10.938Z"},{"id":"q-975","question":"Design an asynchronous NDJSON pipeline in Python that ingests lines of JSON from an async source, validates each line against a Pydantic model, applies a lightweight transform, and yields results to a downstream consumer. Ensure memory stays bounded when the consumer slows by using a bounded asyncio.Queue?","answer":"Use an async producer–consumer with a bounded asyncio.Queue. The producer reads NDJSON lines from the async source, validates with a Pydantic model, and enqueues transformed events. The consumer drain","explanation":"## Why This Is Asked\n\nTests ability to build streaming, backpressure-aware pipelines; real-world data rates and validation.\n\n## Key Concepts\n\n- Async I/O and producers/consumers\n- NDJSON streaming\n- Pydantic validation\n- Bounded queues for backpressure\n- Error handling and fault tolerance\n\n## Code Example\n\n```javascript\n# Python-like skeleton showing producer/consumer with asyncio.Queue\nimport asyncio\nfrom pydantic import BaseModel\n\nclass Event(BaseModel):\n    ts: float\n    user: str\n    value: float\n\nasync def producer(src, q: asyncio.Queue):\n    async for line in src:\n        evt = Event.parse_raw(line)\n        await q.put(evt)\n\nasync def consumer(q: asyncio.Queue):\n    while True:\n        item = await q.get()\n        # process item\n        q.task_done()\n```\n\n## Follow-up Questions\n\n- How would you measure backpressure and adapt queue size?\n- How to handle malformed lines without dropping the stream?","diagram":null,"difficulty":"intermediate","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Coinbase","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T17:39:00.639Z","createdAt":"2026-01-12T17:39:00.639Z"}],"subChannels":["async","best-practices","fundamentals","general"],"companies":["Adobe","Airbnb","Amazon","Anthropic","Apple","Bloomberg","Cloudflare","Coinbase","Databricks","Discord","DoorDash","Goldman Sachs","Google","Hugging Face","IBM","LinkedIn","Lyft","Meta","Microsoft","MongoDB","NVIDIA","Netflix","OpenAI","Oracle","PayPal","Plaid","Robinhood","Scale Ai","Slack","Snap","Snowflake","Square","Stripe","Tesla","Twitter","Two Sigma","Uber","Zoom"],"stats":{"total":47,"beginner":14,"intermediate":18,"advanced":15,"newThisWeek":30}}