{"questions":[{"id":"q-196","question":"How would you implement a rate-limited async HTTP client using aiohttp and asyncio.Semaphore to handle 1000 requests while respecting API limits?","answer":"Use asyncio.Semaphore(50) for concurrency control and aiohttp.ClientSession with time-based rate limiting between requests","explanation":"## Why Asked\nTests async programming and API rate limiting skills in production scenarios\n## Key Concepts\nAsyncIO, Semaphore, Rate Limiting, HTTP Client Design\n## Code Example\n```\nimport aiohttp\nimport asyncio\n\nasync def rate_limited_client(urls):\n    semaphore = asyncio.Semaphore(50)\n    async with aiohttp.ClientSession() as session:\n        tasks = [fetch_url(session, url, semaphore) for url in urls]\n        return await asyncio.gather(*tasks)\n\nasync def fetch_url(session, url, semaphore):\n    async with semaphore:\n        await asyncio.sleep(0.1)  # Rate limit\n        async with session.get(url) as response:\n            return await response.text()\n```\n## Follow-up Questions\nHow would you handle retries? What about exponential backoff?","diagram":"flowchart TD\n  A[Start] --> B[Create Semaphore]\n  B --> C[Create ClientSession]\n  C --> D[Process URLs Concurrently]\n  D --> E[Apply Rate Limits]\n  E --> F[Return Results]\n  F --> G[End]","difficulty":"intermediate","tags":["asyncio","aiohttp","concurrency"],"channel":"python","subChannel":"async","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":"https://www.youtube.com/watch?v=Qb9s3UiMSTA"},"companies":["Amazon","Google","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":["asyncio","semaphore","aiohttp","concurrency","rate limiting","async"],"voiceSuitable":true,"lastUpdated":"2025-12-27T04:58:33.927Z","createdAt":"2025-12-26 12:51:06"},{"id":"q-224","question":"How would you implement a thread-safe singleton in Python with lazy initialization, proper type hints, and discuss the trade-offs between metaclass, decorator, and module-level approaches for a production system?","answer":"Implement using metaclass with double-checked locking and TypeVar for generics: metaclass provides compile-time enforcement, decorator offers simplicity, while module-level singletes are most Pythonic. Use threading.Lock for thread safety and generic typing support.","explanation":"## Context\n\nThis question assesses understanding of design patterns, concurrency, and Python-specific implementation trade-offs. Senior developers should compare multiple singleton approaches and justify their choice.\n\n## Code Examples\n\n### Metaclass Approach\n\n```python\nfrom typing import TypeVar, Type, Optional\nimport threading\n\nT = TypeVar('T')\n\nclass SingletonMeta(type):\n    _instances: dict[Type, object] = {}\n    _lock: threading.Lock = threading.Lock()\n    \n    def __call__(cls: Type[T], *args, **kwargs) -> T:\n        if cls not in cls._instances:\n            with cls._lock:\n                if cls not in cls._instances:\n                    instance = super().__call__(*args, **kwargs)\n                    cls._instances[cls] = instance\n        return cls._instances[cls]\n\nclass DatabaseConnection(metaclass=SingletonMeta):\n    def __init__(self, connection_string: str):\n        self.connection_string = connection_string\n        self._connection = None\n```\n\n### Decorator Approach\n\n```python\ndef singleton(cls: Type[T]) -> Type[T]:\n    instances: dict[Type[T], T] = {}\n    lock = threading.Lock()\n    \n    @functools.wraps(cls)\n    def wrapper(*args, **kwargs) -> T:\n        if cls not in instances:\n            with lock:\n                if cls not in instances:\n                    instances[cls] = cls(*args, **kwargs)\n        return instances[cls]\n    \n    return wrapper\n\n@singleton\nclass Logger:\n    def log(self, message: str) -> None:\n        print(f\"Log: {message}\")\n```\n\n### Module-level Approach\n\n```python\n# config_manager.py\nclass _ConfigManager:\n    def __init__(self):\n        self.settings: dict[str, Any] = {}\n    \n    def get(self, key: str, default=None):\n        return self.settings.get(key, default)\n    \n    def set(self, key: str, value: Any) -> None:\n        self.settings[key] = value\n\nconfig_manager = _ConfigManager()  # Module-level singleton\n```\n\n## Trade-offs\n\n- **Metaclass**: Compile-time enforcement, supports inheritance, complex to understand\n- **Decorator**: Runtime flexibility, easy to apply, affects class signature\n- **Module-level**: Simplest, GIL-protected, no lazy loading, less flexible\n\n## Follow-up Questions\n\n- How would you test singleton behavior across multiple processes?\n- What happens with pickle serialization of singletons?\n- How would you implement a singleton that resets for testing?\n- When would you choose dependency injection over singleton?","diagram":"graph TD\n    A[Thread 1 Request] --> B{Instance Exists?}\n    C[Thread 2 Request] --> B\n    B -->|No| D[Acquire Lock]\n    B -->|Yes| K[Return Instance]\n    D --> E{Double Check}\n    E -->|No| F[Create Instance]\n    E -->|Yes| G[Release Lock]\n    F --> H[Initialize]\n    H --> I[Store in Class Dict]\n    I --> G\n    G --> K\n    J[Thread N Request] --> B","difficulty":"advanced","tags":["pep8","typing","testing"],"channel":"python","subChannel":"best-practices","sourceUrl":null,"videos":null,"companies":["Amazon","Google","Meta","Microsoft","Netflix"],"eli5":"Imagine you have a special toy that only one kid can play with at a time. When you want to play with it, you check if anyone else is using it first. If it's free, you grab it and tell everyone else 'I'm using this toy now!' So no two kids can accidentally both think they're the special toy owner. This is like making sure only one copy of something exists in your computer program. You can have different rules about how to share toys: one way is having a playground boss who decides (metaclass), another is putting a special sticker on the toy box (decorator), or just having one toy box for the whole school (module-level). Each way works, but some are easier to understand than others!","relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"lastUpdated":"2025-12-26T16:35:10.466Z","createdAt":"2025-12-26 12:51:07"},{"id":"q-178","question":"In Python, when do `is` and `==` return different results, and why does this happen with object identity vs equality?","answer":"`is` checks memory identity (same object), `==` checks value equality. Different for distinct objects with same values: `[1,2] is [1,2]` is False but `[1,2] == [1,2]` is True.","explanation":"## Concept Overview\nPython distinguishes between object identity and value equality. `is` compares memory addresses using `id()`, while `==` compares values using `__eq__` method.\n\n## Implementation\n```python\n# Identity vs Equality\na = [1, 2, 3]\nb = [1, 2, 3]\nc = a\n\nprint(a == b)  # True - same values\nprint(a is b)  # False - different objects\nprint(a is c)  # True - same object\n\n# Integer optimization (small integers)\nx = 256\ny = 256\nprint(x is y)  # True - interned\n\nx = 257\ny = 257\nprint(x is y)  # False - different objects\n```\n\n## Trade-offs\n- `is`: Faster, checks if exactly same object\n- `==`: Slower, checks if values are equivalent\n- Use `is` for singletons (None, True, False)\n- Use `==` for value comparison\n\n## Common Pitfalls\n- Assuming `is` works for value comparison\n- Not understanding integer/string interning\n- Using `is` with mutable objects incorrectly\n- Forgetting that `==` can be overridden by custom classes","diagram":"graph TD\n    A[Object A] -->|id: 0x1234| C[Memory Location 0x1234]\n    B[Object B] -->|id: 0x5678| D[Memory Location 0x5678]\n    E[Object C] -->|id: 0x1234| C\n    \n    F[Value: [1,2,3]] --> G[Content Comparison]\n    H[Value: [1,2,3]] --> G\n    I[Value: [1,2,3]] --> G\n    \n    J[is operator] --> K[Compare memory addresses]\n    L[== operator] --> M[Compare values via __eq__]\n    \n    style C fill:#e1f5fe\n    style D fill:#e1f5fe\n    style G fill:#f3e5f5","difficulty":"intermediate","tags":["python","basics"],"channel":"python","subChannel":"fundamentals","sourceUrl":null,"videos":{"shortVideo":"https://www.youtube.com/watch?v=mO_dS3rXDIs","longVideo":"https://www.youtube.com/watch?v=CZ8bZPqtwU0"},"companies":["Amazon","Google","Meta","Microsoft","Uber"],"eli5":"Imagine you have two identical toy cars. They look exactly the same and can do the same tricks - that's like using == to check if they're equal. But are they the SAME exact toy car? No! They're two different cars that just happen to look alike. That's like using 'is' to check if they're the same object. In Python, when you make two lists with the same numbers, they're like those two toy cars - they look identical (== says True) but they're actually two separate objects in the computer's memory (is says False). Only when you point to the very same object will both 'is' and '==' say True!","relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"lastUpdated":"2025-12-25T14:57:56.251Z","createdAt":"2025-12-26 12:51:06"},{"id":"q-474","question":"Write a Python function that takes a list of integers and returns the sum of all even numbers. How would you handle edge cases?","answer":"Use list comprehension with sum() for efficiency. Handle empty lists by returning 0. Validate input types to avoid TypeError. Consider using generator expression for memory efficiency with large lists","explanation":"## Solution\n\n```python\ndef sum_even_numbers(numbers):\n    if not isinstance(numbers, list):\n        raise TypeError(\"Input must be a list\")\n    return sum(num for num in numbers if isinstance(num, int) and num % 2 == 0)\n```\n\n## Key Points\n\n- **List comprehension**: Efficient filtering and transformation\n- **Type checking**: Prevents runtime errors with invalid inputs\n- **Edge cases**: Empty list returns 0, non-integers are ignored\n- **Memory efficiency**: Generator expression for large datasets\n\n## Complexity\n\n- Time: O(n) - single pass through list\n- Space: O(1) - constant extra space","diagram":"flowchart TD\n  A[Input List] --> B{Validate Type}\n  B -->|Valid| C[Filter Even Numbers]\n  B -->|Invalid| D[Raise TypeError]\n  C --> E[Sum Results]\n  E --> F[Return Sum]","difficulty":"beginner","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Microsoft","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"lastUpdated":"2025-12-27T05:46:17.451Z","createdAt":"2025-12-26 12:51:05"},{"id":"q-503","question":"How would you implement a distributed rate limiter using Redis with sliding window algorithm to handle 10,000 requests per second across multiple API servers?","answer":"Use Redis sorted sets with timestamps as scores and request IDs as members. For each request, ZADD current timestamp, ZREMRANGEBYSCORE to remove old entries, then ZCARD to count. Use Lua script for at","explanation":"## Implementation\n\nUse Redis sorted sets with timestamps as scores:\n\n```python\n# Lua script for atomic rate limiting\nlocal key = KEYS[1]\nlocal window = tonumber(ARGV[1])\nlocal limit = tonumber(ARGV[2])\nlocal now = tonumber(ARGV[3])\n\nredis.call('zadd', key, now, now)\nredis.call('zremrangebyscore', key, 0, now - window)\nlocal count = redis.call('zcard', key)\nredis.call('expire', key, window)\n\nreturn count <= limit\n```\n\n## Key Considerations\n\n- **Atomicity**: Lua script prevents race conditions\n- **Memory**: Sorted sets automatically clean old entries\n- **Performance**: O(log N) operations, handles 10k+ RPS\n- **Fallback**: Local rate limiter when Redis unavailable\n- **Monitoring**: Track Redis latency and hit rates","diagram":"flowchart TD\n  A[API Request] --> B[Redis Lua Script]\n  B --> C{Within Limit?}\n  C -->|Yes| D[Process Request]\n  C -->|No| E[Return 429]\n  F[Cleanup Old Entries] --> B\n  G[Set TTL] --> B","difficulty":"advanced","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","LinkedIn","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":["redis","sliding window","sorted sets","lua script","rate limiting","distributed"],"voiceSuitable":true,"lastUpdated":"2025-12-27T05:52:37.589Z","createdAt":"2025-12-26 12:51:06"},{"id":"q-559","question":"You're building a data processing pipeline that needs to handle large CSV files efficiently. How would you implement a memory-efficient solution using Python generators to process files that don't fit in RAM?","answer":"Use Python's `csv` module with generator functions to read files line-by-line. Implement `yield` in a custom generator that processes chunks, avoiding loading entire files. Combine with `itertools.isl","explanation":"## Memory-Efficient CSV Processing\n\n- **Generator Pattern**: Use `yield` to process rows incrementally\n- **csv.reader**: Built-in CSV parser handles edge cases\n- **Chunk Processing**: Batch rows with `itertools.islice`\n- **Memory Control**: Constant O(1) memory usage\n\n```python\nimport csv\nfrom itertools import islice\n\ndef csv_generator(filename, chunk_size=1000):\n    with open(filename) as f:\n        reader = csv.DictReader(f)\n        while True:\n            chunk = list(islice(reader, chunk_size))\n            if not chunk:\n                break\n            yield chunk\n```\n\n## Key Benefits\n\n- **Scalability**: Processes files of any size\n- **Performance**: Minimal memory overhead\n- **Flexibility**: Easy to add data transformation logic","diagram":"flowchart TD\n  A[Large CSV File] --> B[csv.reader]\n  B --> C[Generator Function]\n  C --> D[Process Chunk]\n  D --> E[yield Results]\n  E --> F[Next Chunk]\n  F --> D","difficulty":"intermediate","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Snap","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"lastUpdated":"2025-12-26T01:15:45.177Z","createdAt":"2025-12-26 12:51:07"},{"id":"q-588","question":"How would you implement a rate limiter using Python's asyncio to prevent API abuse while maintaining high throughput?","answer":"Use asyncio.Semaphore with token bucket algorithm. Track request timestamps in a deque, calculate wait time based on rate limit, and use asyncio.sleep() for throttling. Implement per-client limits usi","explanation":"## Rate Limiting Implementation\n\n### Token Bucket Approach\n- Use asyncio.Semaphore for concurrent request control\n- Implement token bucket with asyncio.gather() for batch processing\n- Track timestamps in collections.deque for O(1) operations\n\n### Async Pattern\n```python\nimport asyncio\nfrom collections import deque\n\nclass RateLimiter:\n    def __init__(self, rate_limit, time_window):\n        self.rate_limit = rate_limit\n        self.time_window = time_window\n        self.requests = deque()\n    \n    async def acquire(self):\n        now = asyncio.get_event_loop().time()\n        # Remove old requests\n        while self.requests and self.requests[0] <= now - self.time_window:\n            self.requests.popleft()\n        \n        if len(self.requests) >= self.rate_limit:\n            sleep_time = self.time_window - (now - self.requests[0])\n            await asyncio.sleep(sleep_time)\n            return await self.acquire()\n        \n        self.requests.append(now)\n```\n\n### Production Considerations\n- Use Redis for distributed rate limiting across multiple instances\n- Implement sliding window for more accurate rate limiting\n- Add circuit breaker pattern for API protection\n- Monitor and log rate limit violations for debugging","diagram":"flowchart TD\n  A[Request] --> B{Check Rate Limit}\n  B -->|Within Limit| C[Process Request]\n  B -->|Exceeded| D[Calculate Wait Time]\n  D --> E[Async Sleep]\n  E --> B\n  C --> F[Update Request Log]\n  F --> G[Return Response]","difficulty":"intermediate","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","DoorDash","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":["asyncio","rate limiter","semaphore","token bucket","api abuse","throughput","deque"],"voiceSuitable":true,"lastUpdated":"2025-12-27T04:58:08.145Z","createdAt":"2025-12-27T01:14:34.507Z"}],"subChannels":["async","best-practices","fundamentals","general"],"companies":["Amazon","Apple","Bloomberg","DoorDash","Google","LinkedIn","Meta","Microsoft","MongoDB","Netflix","Robinhood","Snap","Snowflake","Uber","Zoom"],"stats":{"total":7,"beginner":1,"intermediate":4,"advanced":2,"newThisWeek":7}}