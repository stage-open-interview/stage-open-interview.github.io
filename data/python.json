{"questions":[{"id":"q-196","question":"How would you implement a rate-limited async HTTP client using aiohttp and asyncio.Semaphore to handle 1000 requests while respecting API limits?","answer":"Use asyncio.Semaphore(50) for concurrency control and aiohttp.ClientSession with time-based rate limiting between requests.","explanation":"## Why Asked\nTests async programming and API rate limiting skills in production scenarios.\n\n## Key Concepts\nAsyncIO, Semaphore, Rate Limiting, HTTP Client Design\n\n## Code Example\n```\nimport aiohttp\nimport asyncio\n\nasync def rate_limited_client(urls):\n    semaphore = asyncio.Semaphore(50)\n    async with aiohttp.ClientSession() as session:\n        tasks = [fetch_url(session, url, semaphore) for url in urls]\n        return await asyncio.gather(*tasks)\n\nasync def fetch_url(session, url, semaphore):\n    async with semaphore:\n        await asyncio.sleep(0.1)  # Rate limit\n        async with session.get(url) as response:\n            return await response.text()\n```","diagram":"flowchart TD\n  A[Start] --> B[Create Semaphore]\n  B --> C[Create ClientSession]\n  C --> D[Process URLs Concurrently]\n  D --> E[Apply Rate Limits]\n  E --> F[Return Results]\n  F --> G[End]","difficulty":"intermediate","tags":["asyncio","aiohttp","concurrency"],"channel":"python","subChannel":"async","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":"https://www.youtube.com/watch?v=Qb9s3UiMSTA"},"companies":["Amazon","Google","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":["asyncio","semaphore","aiohttp","concurrency","rate limiting","async"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2026-01-08T11:27:41.329Z","createdAt":"2025-12-26 12:51:06"},{"id":"q-224","question":"How would you implement a thread-safe singleton in Python with lazy initialization, proper type hints, and discuss the trade-offs between metaclass, decorator, and module-level approaches for a production system?","answer":"Implement using metaclass with double-checked locking and TypeVar for generics: metaclass provides compile-time enforcement, decorator offers simplicity, while module-level singletons are most Pythonic. Use threading.Lock for thread safety and generic typing support.","explanation":"## Context\n\nThis question assesses understanding of design patterns, concurrency, and Python-specific implementation trade-offs. Senior developers should compare multiple singleton approaches and justify their choice.\n\n## Code Examples\n\n### Metaclass Approach\n\n```python\nfrom typing import TypeVar, Type, Optional\nimport threading\n\nT = TypeVar('T')\n\nclass SingletonMeta(type):\n    _instances: dict[Type, object] = {}\n    _lock: threading.Lock = threading.Lock()\n    \n    def __call__(cls: Type[T], *args, **kwargs) -> T:\n        if cls not in cls._instances:\n            with cls._lock:\n                if cls not in cls._instances:\n                    instance = super().__call__(*args, **kwargs)\n                    cls._instances[cls] = instance\n        return cls._instances[cls]\n```\n\n### Decorator Approach\n\n```python\ndef singleton(cls):\n    instances = {}\n    lock = threading.Lock()\n    \n    def wrapper(*args, **kwargs):\n        if cls not in instances:\n            with lock:\n                if cls not in instances:\n                    instances[cls] = cls(*args, **kwargs)\n        return instances[cls]\n    \n    return wrapper\n```\n\n### Module-Level Approach\n\n```python\n# singleton_module.py\nclass _DatabaseConnection:\n    def __init__(self):\n        self.connection = None\n    \n    def connect(self):\n        if not self.connection:\n            self.connection = create_db_connection()\n        return self.connection\n\n# Single instance at module import time\ndatabase = _DatabaseConnection()\n```\n\n## Trade-offs Analysis\n\n**Metaclass Approach:**\n- Pros: Compile-time enforcement, clean class definition, full type hint support\n- Cons: More complex, metaclass magic can be confusing for new developers\n- Best for: Production systems requiring strict singleton enforcement\n\n**Decorator Approach:**\n- Pros: Simple to apply, non-intrusive, easy to understand\n- Cons: Runtime enforcement only, can be bypassed with direct class instantiation\n- Best for: Quick implementations and prototyping\n\n**Module-Level Approach:**\n- Pros: Most Pythonic, guaranteed thread safety by import system, zero overhead\n- Cons: Less flexible, eager initialization, harder to test\n- Best for: Simple cases where lazy initialization isn't required\n\n## Production Recommendation\n\nFor production systems, use the metaclass approach with double-checked locking. It provides the best balance of thread safety, type hint support, and enforcement while maintaining clean API design.","diagram":"graph TD\n    A[Thread 1 Request] --> B{Instance Exists?}\n    C[Thread 2 Request] --> B\n    B -->|No| D[Acquire Lock]\n    B -->|Yes| K[Return Instance]\n    D --> E{Double Check}\n    E -->|No| F[Create Instance]\n    E -->|Yes| G[Release Lock]\n    F --> H[Initialize]\n    H --> I[Store in Class Dict]\n    I --> G\n    G --> K\n    J[Thread N Request] --> B","difficulty":"advanced","tags":["pep8","typing","testing"],"channel":"python","subChannel":"best-practices","sourceUrl":null,"videos":null,"companies":["Amazon","Google","Meta","Microsoft","Netflix"],"eli5":"Imagine you have a special toy that only one kid can play with at a time. When you want to play with it, you check if anyone else is using it first. If it's free, you grab it and tell everyone else 'I'm using this toy now!' So no two kids can accidentally both think they're the special toy owner. This is like making sure only one copy of something exists in your computer program. You can have different rules about how to share toys: one way is having a playground boss who decides (metaclass), another is putting a special sticker on the toy box (decorator), or just having one toy box for the whole school (module-level). Each way works, but some are easier to understand than others!","relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-30T06:40:35.857Z","createdAt":"2025-12-26 12:51:07"},{"id":"q-178","question":"In Python, when do `is` and `==` return different results, and why does this happen with object identity vs equality?","answer":"`is` checks memory identity (same object), `==` checks value equality. Different for distinct objects with same values: `[1,2] is [1,2]` is False but `[1,2] == [1,2]` is True.","explanation":"## Concept Overview\nPython distinguishes between object identity and value equality. `is` compares memory addresses using `id()`, while `==` compares values using `__eq__` method.\n\n## Implementation\n```python\n# Identity vs Equality\na = [1, 2, 3]\nb = [1, 2, 3]\nc = a\n\nprint(a == b)  # True - same values\nprint(a is b)  # False - different objects\nprint(a is c)  # True - same object\n\n# Integer optimization (small integers)\nx = 256\ny = 256\nprint(x is y)  # True - interned\n\nx = 257\ny = 257\nprint(x is y)  # False - different objects\n```\n\n## Trade-offs\n- `is`: Faster, checks if exactly same object\n- `==`: Slower, checks if values are equivalent\n- Use `is` for singletons (None, True, False)\n- Use `==` for value comparison\n\n## Common Pitfalls\n- Assuming `is` works for value comparison\n- Not understanding integer/string interning\n- Using `is` with mutable objects incorrectly\n- Forgetting that `==` can be overridden by custom classes","diagram":"graph TD\n    A[Object A] -->|id: 0x1234| C[Memory Location 0x1234]\n    B[Object B] -->|id: 0x5678| D[Memory Location 0x5678]\n    E[Object C] -->|id: 0x1234| C\n    \n    F[Value: [1,2,3]] --> G[Content Comparison]\n    H[Value: [1,2,3]] --> G\n    I[Value: [1,2,3]] --> G\n    \n    J[is operator] --> K[Compare memory addresses]\n    L[== operator] --> M[Compare values via __eq__]\n    \n    style C fill:#e1f5fe\n    style D fill:#e1f5fe\n    style G fill:#f3e5f5","difficulty":"intermediate","tags":["python","basics"],"channel":"python","subChannel":"fundamentals","sourceUrl":null,"videos":{"shortVideo":"https://www.youtube.com/watch?v=mO_dS3rXDIs","longVideo":"https://www.youtube.com/watch?v=CZ8bZPqtwU0"},"companies":["Amazon","Google","Meta","Microsoft","Uber"],"eli5":"Imagine you have two identical toy cars. They look exactly the same and can do the same tricks - that's like using == to check if they're equal. But are they the SAME exact toy car? No! They're two different cars that just happen to look alike. That's like using 'is' to check if they're the same object. In Python, when you make two lists with the same numbers, they're like those two toy cars - they look identical (== says True) but they're actually two separate objects in the computer's memory (is says False). Only when you point to the very same object will both 'is' and '==' say True!","relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-25T14:57:56.251Z","createdAt":"2025-12-26 12:51:06"},{"id":"q-1085","question":"You’re building a real-time log processor in Python for a messaging app. You receive a stream of JSON lines like {\"user_id\": 123, \"event\": \"message_sent\", \"ts\": 1610000000}. Emit only the first event per (user_id, event) within a 60-second sliding window. Design a memory-bounded, generator-based pipeline that reads from an iterable of strings and yields deduplicated dicts in order. Include edge cases and memory considerations?","answer":"Implement a generator-based pipeline that reads lines, parses JSON, and yields the first dict for each (user_id, event) in a 60-second sliding window. Maintain a bounded TTL cache (OrderedDict) key->t","explanation":"## Why This Is Asked\n\nTests ability to design streaming pipelines with memory constraints and dedup logic without external stores.\n\n## Key Concepts\n\n- Generators and streaming data\n- Sliding window deduplication\n- TTL-based cache with eviction\n- Robust JSON parsing and field validation\n\n## Code Example\n\n```python\ndef dedup_stream(lines, window_seconds=60):\n    import json\n    from collections import deque, OrderedDict\n    cache = OrderedDict()\n    for line in lines:\n        obj = json.loads(line)\n        user = obj.get(\"user_id\")\n        evt = obj.get(\"event\")\n        ts = obj.get(\"ts\")\n        if user is None or evt is None or ts is None:\n            continue\n        key = (user, evt)\n        # evict stale\n        for k in list(cache):\n            if ts - cache[k] > window_seconds:\n                del cache[k]\n        if key in cache:\n            continue\n        cache[key] = ts\n        yield obj\n```\n\n## Follow-up Questions\n\n- How would you adapt for out-of-order arrivals? \n- How to scale with multiple processes?","diagram":"flowchart TD\n  A[Input Stream] --> B[Parse JSON]\n  B --> C[Key by (user_id, event)]\n  C --> D[TTL Cache Lookup]\n  D --> E{Duplicate in window?}\n  E -->|No| F[Emit] --> G[Update Cache]\n  E -->|Yes| H[Drop]","difficulty":"intermediate","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T22:19:30.480Z","createdAt":"2026-01-12T22:19:30.480Z"},{"id":"q-1149","question":"In a streaming data etl, you're given a JSONL stream where each line is a JSON object with 'host' and 'value'. Implement a memory-efficient Python generator top_n_hosts(n, lines) that yields the top N hosts by total value after consuming the stream, without keeping the entire per-host totals in memory. Use a min-heap to maintain current top-N and skip invalid lines?","answer":"Process the JSONL stream line by line; validate host and numeric value; accumulate per-host totals in a dict; maintain a fixed-size min-heap of (total, host) for the current top-N; when a host total i","explanation":"## Why This Is Asked\n\nTests ability to craft streaming pipelines and manage memory with data structure choices.\n\n## Key Concepts\n\n- Generators and lazy evaluation\n- JSONL parsing with error handling\n- Per-key aggregation with a fixed-size min-heap (top-N)\n- Trade-offs: O(H) memory vs O(N log N) heap maintenance\n\n## Code Example\n\n```python\nimport json, heapq\n\ndef top_n_hosts(n, lines):\n    totals = {}\n    heap = []  # min-heap of (total, host)\n    seen = set()\n    for line in lines:\n        try:\n            obj = json.loads(line)\n            host = obj.get('host')\n            val = float(obj.get('value', 0))\n            if host is None:\n                continue\n        except Exception:\n            continue\n        totals[host] = totals.get(host, 0.0) + val\n        heapq.heappush(heap, (totals[host], host))\n        if len(heap) > n:\n            heapq.heappop(heap)\n        seen.add(host)\n    for total, host in sorted(heap, reverse=True):\n        yield host, total\n```\n\n## Follow-up Questions\n\n- How would you modify to handle streaming resets or windowed top-N?","diagram":null,"difficulty":"intermediate","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T01:34:23.375Z","createdAt":"2026-01-13T01:34:23.375Z"},{"id":"q-1250","question":"Design an asynchronous NDJSON ingestion pipeline in Python that reads an async iterable of lines, validates each with a Pydantic model, and routes records to per-tenant partitions based on the 'tenant_id'. Implement bounded queues per partition with a total memory cap, guarantee per-tenant in-order processing, and a slower downstream sink that creates backpressure. Provide a test plan with skewed partitions and slow sinks?","answer":"To implement, create an async function ingest(source, writer_factory) that builds a dict of per-tenant queues and a small worker pool per tenant. Each line is parsed as JSON, validated with a Pydantic","explanation":"## Why This Is Asked\nTests ability to design partitioned, back-pressured pipelines in Python with asyncio, ensuring per-tenant ordering and bounded memory, a realistic scalability concern at large-scale companies like Snowflake/Uber.\n\n## Key Concepts\n- Async data ingestion\n- Bounded queues and backpressure\n- Per-tenant partitioning and in-order semantics\n\n## Code Example\n```python\n# skeleton illustrating structure\nfrom typing import AsyncIterable, Dict\nfrom asyncio import Queue, TaskGroup\nfrom pydantic import BaseModel\nimport json\nimport asyncio\n\nclass Record(BaseModel):\n    tenant_id: str\n    payload: dict\n\nasync def ingest(source: AsyncIterable[str], writer_factory):\n    queues: Dict[str, Queue] = {}\n    async def worker(tid: str, q: Queue):\n        sink = writer_factory(tid)\n        while True:\n            item = await q.get()\n            if item is None:\n                break\n            await sink.write(item)\n    async with TaskGroup() as g:\n        async for line in source:\n            rec = Record(**json.loads(line))\n            q = queues.setdefault(rec.tenant_id, Queue(maxsize=128))\n            await q.put(rec)\n        for q in queues.values():\n            await q.put(None)\n        # workers auto-join on exit\n```\n\n## Follow-up Questions\n- How would you handle dynamic partition keys that can exceed memory?\n- How would you validate end-to-end ordering when a partition has backpressure and failure?","diagram":"flowchart TD\n  S[NDJSON Source] --> M[Parse & Validate]\n  M --> P{{Partition by tenant_id}}\n  P --> Q[Per-tenant Queues]\n  Q --> W[Per-tenant Workers]\n  W --> D[Downstream Sink]","difficulty":"advanced","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Snowflake","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T06:43:51.710Z","createdAt":"2026-01-13T06:43:51.710Z"},{"id":"q-1275","question":"Design an advanced async Python pipeline: NDJSON lines arrive via a TCP socket, each is validated with a Pydantic model, then a deduplication stage uses a sliding-window Bloom filter to emit each event at most once in a 24-hour window. The pipeline must stay memory-bounded, support backpressure to the producer, and allow the downstream sink to pace itself. Provide a concrete test plan with clock skew and bursty traffic?","answer":"Approach: implement an async NDJSON ingest with a bounded asyncio.Queue, Pydantic validation, and a dedupe stage using a sliding window Bloom filter implemented with buckets. The Bloom filter stores f","explanation":"## Why This Is Asked\nTests ability to design streaming pipelines with memory constraints, asynchronous backpressure, and practical deduplication.\n\n## Key Concepts\n- Async ingestion with bounded queues\n- Pydantic validation of streaming data\n- Sliding-window Bloom filter for deduplication\n- Backpressure and pacing of producers/consumers\n\n## Code Example\n```python\n#_placeholder\n```\n\n## Follow-up Questions\n- How would you calibrate the Bloom filter's false positive rate for varying traffic? \n- How would you extend to multi-node deduplication without shared state?","diagram":null,"difficulty":"advanced","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Hugging Face","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T07:39:46.313Z","createdAt":"2026-01-13T07:39:46.313Z"},{"id":"q-1285","question":"Given a list of dictionaries representing users, implement normalize_users(users) that returns a list of User dataclass instances with fields id:int, name:str, signup_ts:datetime, is_active:bool. Coerce id from str to int, parse signup_ts ISO8601 strings, interpret is_active from common truthy values, and fill defaults for missing fields. If any record is invalid, raise ValueError with the indices of invalid records?","answer":"Implement a User dataclass and normalize_users that maps dicts to User, coercing id to int, parsing signup_ts via datetime.fromisoformat, and is_active from common truthy values; default missing field","explanation":"## Why This Is Asked\nThis question tests data normalization, type coercion, and error reporting using Python basics.\n\n## Key Concepts\n- Dataclasses for lightweight models\n- Type hints and basic validation\n- ISO 8601 parsing with datetime\n- Error aggregation for batch records\n\n## Code Example\n```python\nfrom dataclasses import dataclass\nfrom datetime import datetime\nfrom typing import List, Dict, Any\n\n@dataclass\nclass User:\n    id: int\n    name: str\n    signup_ts: datetime\n    is_active: bool\n\ndef _to_int(v: Any) -> int:\n    if isinstance(v, int):\n        return v\n    return int(v)\n\ndef _parse_ts(ts: Any) -> datetime:\n    if isinstance(ts, datetime):\n        return ts\n    return datetime.fromisoformat(ts)\n\ndef _to_bool(v: Any) -> bool:\n    if isinstance(v, bool):\n        return v\n    if isinstance(v, str):\n        return v.strip().lower() in {\"true\", \"1\", \"yes\", \"y\"}\n    return bool(v)\n\n\ndef normalize_users(users: List[Dict[str, Any]]) -> List[User]:\n    out: List[User] = []\n    bad: List[int] = []\n    for i, rec in enumerate(users):\n        try:\n            u = User(\n                id=_to_int(rec.get(\"id\")),\n                name=rec.get(\"name\", \"\"),\n                signup_ts=_parse_ts(rec.get(\"signup_ts\")),\n                is_active=_to_bool(rec.get(\"is_active\", False)),\n            )\n            out.append(u)\n        except Exception:\n            bad.append(i)\n    if bad:\n        raise ValueError(f\"Invalid records at indices: {bad}\")\n    return out\n```\n\n## Follow-up Questions\n- How would you extend to nested user data?\n- How would you validate and report multiple fields per record?","diagram":"flowchart TD\n  A[Start] --> B[Iterate records]\n  B --> C[Coerce fields]\n  C --> D{All valid?}\n  D -->|Yes| E[Emit User]\n  D -->|No| F[Collect index]\n  F --> G[Raise error on end]","difficulty":"beginner","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","MongoDB","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T08:31:56.123Z","createdAt":"2026-01-13T08:31:56.123Z"},{"id":"q-1476","question":"Design an asynchronous Python engine to join two JSON event streams by id within a 5-second window. Streams A and B are async iterables yielding {'id': str, 'ts': int, 'payload': Any}. Emit matched pairs to a sink when both sides have an event with same id within the 5s window. Route late events past the lateness bound to a 'late' sink. Enforce a global memory bound for buffered events and propose a test plan with out-of-order arrivals?","answer":"Buffer events per id from each stream. On arrival of an A event, check B-buffer for the same id; if exists and abs(tsA - tsB) <= 5000, emit the joined pair and drop both. Otherwise enqueue in A-buffer","explanation":"## Why This Is Asked\nTests async coordination, windowed joins, and backpressure with late data handling in Python. It targets real-world streaming problems and metrics-conscious memory management.\n\n## Key Concepts\n- Async streaming and per-id buffering\n- Time-based windowing (5s tumbling window)\n- Late data routing and backpressure\n- Global memory bounds and eviction strategy\n\n## Code Example\n```python\n# Implementation sketch (high-level)\nfrom typing import AsyncIterable, Dict, Deque\nimport asyncio\n\nclass WindowJoin:\n    def __init__(self, window_ms: int, mem_cap: int):\n        self.window = window_ms\n        self.mem_cap = mem_cap\n        self.buf_a: Dict[str, Deque[dict]] = {}\n        self.buf_b: Dict[str, Deque[dict]] = {}\n        self.watermark = 0\n\n    async def ingest_a(self, item: dict, sink):\n        id_ = item['id']\n        ts = item['ts']\n        self._expire(ts)\n        self.buf_a.setdefault(id_, deque()).append(item)\n        self._try_emit(id_, sink)\n\n    async def ingest_b(self, item: dict, sink, late_sink):\n        id_ = item['id']\n        ts = item['ts']\n        self._expire(ts)\n        self.buf_b.setdefault(id_, deque()).append(item)\n        if id_ in self.buf_a:\n            self._emit_matches(id_, sink, late_sink)\n\n    def _expire(self, ts: int):\n        self.watermark = max(self.watermark, ts)\n        # eviction logic based on window and mem_cap would go here\n\n    def _try_emit(self, id_: str, sink):\n        # attempt to pair any ready events for id_\n        a_list = self.buf_a.get(id_, [])\n        b_list = self.buf_b.get(id_, [])\n        while a_list and b_list:\n            a = a_list[0]\n            b = b_list[0]\n            if abs(a['ts'] - b['ts']) <= self.window:\n                sink.emit({'id': id_, 'a': a, 'b': b})\n                a_list.popleft(); b_list.popleft()\n            else:\n                break\n\n    def _emit_matches(self, id_: str, sink, late_sink):\n        self._try_emit(id_, sink)\n        # any remaining items considered late by window semantics could go to late_sink\n        # (pseudo-logic for brevity)\n```\n","diagram":"flowchart TD\n  A[Stream A] --> B[Buffer A]\n  C[Stream B] --> D[Buffer B]\n  E[Join Engine] --> F[Sink]\n  E --> G[Late Sink]","difficulty":"advanced","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Lyft","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T18:50:10.589Z","createdAt":"2026-01-13T18:50:10.589Z"},{"id":"q-1639","question":"Write a Python function process_events(file_path) that reads a newline-delimited JSON (JSONL) log file of Stripe-like events. Each line is a JSON object with 'type' (str) and 'data' (dict with 'id' key). The function should return a dict mapping event 'type' to count, skipping lines with missing keys or invalid JSON, and writing errors to a separate errors.log. Make it memory-efficient using a streaming approach?","answer":"Propose a streaming, memory-safe approach: open the JSONL file, iterate lines, parse with json.loads, verify 'type' is str and 'data' is dict containing 'id'; increment a Counter keyed by type; skip i","explanation":"## Why This Is Asked\nReading streaming logs is common in production pipelines (e.g., Stripe webhook events). This task tests robust, memory-friendly parsing, simple validation, and error handling under real-world I/O.\n\n## Key Concepts\n- Memory-efficient line-by-line processing\n- JSONL parsing and validation\n- Defensive checks and error logging\n- Simple aggregation with collections.Counter\n\n## Code Example\n```javascript\ndef process_events(path):\n    import json, logging\n    from collections import Counter\n    counts = Counter()\n    with open(path, 'r', encoding='utf-8') as f:\n        for line in f:\n            try:\n                obj = json.loads(line)\n            except json.JSONDecodeError:\n                logging.warning(\"invalid line\")\n                continue\n            t = obj.get('type')\n            data = obj.get('data')\n            if not isinstance(t, str) or not isinstance(data, dict) or 'id' not in data:\n                logging.warning(\"missing fields or wrong types\")\n                continue\n            counts[t] += 1\n    return dict(counts)\n```\n\n## Follow-up Questions\n- How would you extend to handle out-of-order events or deduplicate by id?\n- How would you add unit tests for valid and invalid lines?\n","diagram":"flowchart TD\n  Start([Start]) --> Read[Read JSONL line-by-line]\n  Read --> Validate{Valid?}\n  Validate -- Yes --> Update[Update counts]\n  Validate -- No --> ErrorLog[Log error]\n  Update --> End([Return counts])\n  ErrorLog --> End","difficulty":"beginner","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T04:24:46.136Z","createdAt":"2026-01-14T04:24:46.136Z"},{"id":"q-1675","question":"Design a memory-bounded streaming processor in Python for a JSONL event stream with fields: timestamp, service, event_type, payload. Group by (service, event_type), maintain an in-order per-key queue with bounded capacity, and emit a 60-second rolling histogram of payload sizes per group to a downstream sink. Ensure backpressure via per-key queues and a global memory cap, and provide a test plan with skewed keys and slow sinks?","answer":"Use asyncio with per-key bounded queues and a global memory budget. Each key = (service, event_type) has a fixed-size asyncio.Queue and a 60-second circular histogram of payload sizes. On arrival, pus","explanation":"## Why This Is Asked\nEfficient streaming with per-key backpressure and bounded memory.\n\n## Key Concepts\n- asyncio, per-key bounded queues\n- sliding window histograms\n- memory budgeting and backpressure\n- test plan with skewed keys\n\n## Code Example\n```python\n# sketch: per-key histogram and bounded queues\nfrom collections import deque\nimport asyncio\n\nclass WindowHistogram:\n    def __init__(self, window=60):\n        self.bins = deque([0]*window, maxlen=window)\n        self._last_tick = None\n\n    def add(self, size, timestamp):\n        # simplistic implementation; real version rotates bins per second\n        self.bins[-1] += size\n```","diagram":"flowchart TD\n  S[Source] --> P[Parse JSONL]\n  P --> G{Group by (service, event_type)}\n  G --> Q[Per-key queues]\n  Q --> H[60s histogram]\n  H --> E[Emit summaries]\n  E --> D[Downstream sink]","difficulty":"advanced","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Amazon","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T06:47:26.037Z","createdAt":"2026-01-14T06:47:26.037Z"},{"id":"q-1783","question":"Implement a memory-efficient Python function top_n_words(filepath, n) that streams a text file line by line to find the n most frequent words. Normalize case, strip punctuation, ignore empty tokens, and return a list of the top n words sorted by frequency. Ensure it never loads the whole file into memory?","answer":"Use a streaming approach: read each line, lowercase it, remove punctuation, split into words, accumulate counts in a dict, then compute the top n by frequency with a heap (nlargest). This keeps memory","explanation":"## Why This Is Asked\nStreaming text processing is common in data pipelines; this tests memory awareness and basic text normalization.\n\n## Key Concepts\n- Streaming I/O to avoid loading large files\n- Simple tokenization and normalization\n- Extracting top-N with a heap\n\n## Code Example\n```javascript\nimport string, heapq\n\ndef top_n_words(filepath, n):\n    counts = {}\n    trans = str.maketrans('', '', string.punctuation)\n    with open(filepath, 'r', encoding='utf-8') as f:\n        for line in f:\n            line = line.lower().translate(trans)\n            for w in line.split():\n                counts[w] = counts.get(w, 0) + 1\n    return [w for w, _ in heapq.nlargest(n, counts.items(), key=lambda kv: kv[1])]\n``n\n## Follow-up Questions\n- How would you adapt this to handle memory spikes with extremely large vocabularies?\n- How would you test the function for correctness and performance?","diagram":null,"difficulty":"beginner","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T10:45:42.409Z","createdAt":"2026-01-14T10:45:42.410Z"},{"id":"q-1867","question":"Design a Python async NDJSON ingestion pipeline that reads lines from an async source, validates each line against a versioned Pydantic model, supports hot-reloadable schema versions from a shared config, and guarantees exactly-once delivery with an idempotent sink and per-record deduplication, all while enforcing bounded memory and backpressure. How would you implement it?","answer":"I would implement an asyncio pipeline with a bounded queue, an async producer reading NDJSON lines, a versioned validator registry loaded from a shared JSON config, and an idempotent sink using a smal","explanation":"## Why This Is Asked\n\nThis question probes experience with dynamic schemas, backpressure handling, and exactly-once semantics in a streaming Python pipeline.\n\n## Key Concepts\n\n- Async NDJSON ingestion with bounded memory\n- Versioned validators and runtime config reload\n- Idempotent sinks and per-record deduplication\n- Safe hot-reload and atomic registry swap\n\n## Code Example\n\n```python\nfrom typing import Any, Dict\nimport asyncio\nimport json\nfrom pydantic import BaseModel\n\nclass V1(BaseModel):\n    id: str\n    value: int\n    version: str = 'v1'\n\nclass V2(BaseModel):\n    id: str\n    value: int\n    meta: str = ''\n\nSchemaRegistry = {'v1': V1, 'v2': V2}\n\ndef load_schema(ver: str):\n    return SchemaRegistry.get(ver, V1)\n\nasync def validate_line(line: str) -> Dict[str, Any]:\n    data = json.loads(line)\n    ver = data.get('version', 'v1')\n    model = load_schema(ver)\n    obj = model(**data)\n    return obj.dict()\n```\n\n## Follow-up Questions\n- How would you test hot-reload safety and dedup correctness?\n- How would you scale to multiple workers and a shared dedup store?","diagram":null,"difficulty":"advanced","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T14:55:43.072Z","createdAt":"2026-01-14T14:55:43.073Z"},{"id":"q-1884","question":"Implement an asynchronous Python data transformer for a JSONL event stream where each line includes a 'version' field. Build transform_stream(input: AsyncIterable[str], schemas: Dict[int, Type[BaseModel]]) that validates each line against its versioned Pydantic model, applies a version-aware field mapping, and outputs transformed JSONL lines to a downstream sink while guaranteeing per-version in-order processing, memory-bounded streaming, and backpressure. Include a small test scaffold showing a v1→v2 migration?","answer":"Use an AsyncStream with per-version queues. Validate by loading schemas[version] and applying mapping; emit transformed JSON via an async sink. Maintain in-order by preserving sequence_id per version ","explanation":"## Why This Is Asked\nThis question probes the ability to design a robust streaming Python solution that handles schema evolution, backpressure, and memory constraints in real-time data processing.\n\n## Key Concepts\n- AsyncIO pipelines with bounded queues\n- Versioned Pydantic models loaded from a registry\n- In-order processing across dynamic schemas\n- Safe error handling and backpressure-driven flow control\n\n## Code Example\n```python\nfrom typing import AsyncIterable, Dict, Type\nfrom pydantic import BaseModel\nimport asyncio\nimport json\n\nasync def transform_stream(\n    input_lines: AsyncIterable[str],\n    schemas: Dict[int, Type[BaseModel]],\n    mapper: Dict[int, callable],\n    sink: asyncio.Queue\n) -> None:\n    # simplified skeleton\n    queues = {v: asyncio.Queue(maxsize=1024) for v in schemas}\n    async def worker(version: int, q: asyncio.Queue):\n        while True:\n            line = await q.get()\n            if line is None:\n                break\n            data = json.loads(line)\n            model = schemas[version](**data)  # validate\n            out = mapper[version](model)\n            await sink.put(json.dumps(out.dict()))\n            q.task_done()\n\n    procs = [asyncio.create_task(worker(v, queues[v])) for v in schemas]\n    async for line in input_lines:\n        data = json.loads(line)\n        ver = data.get(\"version\")\n        if ver not in queues:\n            continue\n        await queues[ver].put(line)\n    for q in queues.values():\n        await q.put(None)\n    await asyncio.gather(*procs)\n```\n\n## Follow-up Questions\n- How would you handle missing or unknown versions gracefully?\n- How would you test migration paths between versions?","diagram":"flowchart TD\n  A[Input JSONL] --> B[Versioned Validation]\n  B --> C[Per-Version Router]\n  C --> D[Output Sink]\n  D --> E[Backpressure]","difficulty":"intermediate","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","NVIDIA","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T15:45:20.047Z","createdAt":"2026-01-14T15:45:20.047Z"},{"id":"q-2090","question":"Design a memory-bounded streaming top-K aggregator in Python. Data arrives as an async iterable of numeric events with timestamps. Implement a class that maintains an approximate top-10 using a Count-Min Sketch plus a min-heap, supports a sliding time window, and exposes add(value, ts) and get_top_k() reflecting the current window. Describe API, memory guarantees, and a test plan for bursty traffic?","answer":"Use a Count-Min Sketch with configurable dimensions (width, depth) to estimate item frequencies, combined with a min-heap keyed by estimated counts to maintain the top-K elements. Implement a sliding window using a deque that stores (timestamp, value) pairs; when adding new items, update the sketch, maintain the heap, and remove expired entries by decrementing their counts in the sketch.","explanation":"## Why This Is Asked\nThis question tests the ability to design memory-bounded streaming analytics systems using approximate data structures while maintaining timely query results.\n\n## Key Concepts\n- Streaming algorithms with bounded memory\n- Count-Min Sketch for frequency estimation\n- Min-heap for top-K maintenance\n- Sliding time window management\n- Memory-accuracy trade-offs\n\n## Code Example\n```python\nclass TopKStreaming:\n    def __init__(self, k=10, width=1000, depth=5, window=60):\n        pass\n    def add(self, value, ts):\n        pass\n    def get_top_k(self):\n        pass\n```\n\n## Follow-up","diagram":"flowchart TD\n  A[Async data stream] --> B[Count-Min Sketch update]\n  B --> C[Top-K heap maintenance]\n  A --> D[Sliding window eviction]\n  C --> E[Current top-K snapshot]","difficulty":"intermediate","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Databricks","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T05:03:18.175Z","createdAt":"2026-01-14T23:40:32.566Z"},{"id":"q-2402","question":"You're building an asyncio Python client for a rate-limited REST API. How would you implement a function fetch_with_backoff(url, method='GET', max_retries=5, session, rate_limiter, metrics) that performs retries with exponential backoff and jitter, enforces a per-endpoint token-bucket rate limit, and records latency and outcomes into metrics? Include a minimal code sketch and a test plan?","answer":"Use an asyncio fetch_with_backoff that acquires a permit from rate_limiter per endpoint, runs the request with session, and on transient failures retries with jittered exponential backoff (base ~0.2s,","explanation":"## Why This Is Asked\n\nAssesses practical mastery of asyncio, rate limiting, robust retries, and observability in real-world networked Python code, matching needs at scale (e.g., Twitter, Nvidia, Adobe).\n\n## Key Concepts\n\n- Async I/O with aiohttp or httpx\n- Per-endpoint rate limiting (token bucket)\n- Exponential backoff with jitter\n- Latency and outcome metrics collection\n- Test strategy: simulate 429, 5xx, network partitions, concurrent callers\n\n## Code Example\n\n```javascript\n// Example sketch (not executable in Python)\nasync def fetch_with_backoff(url, method='GET', max_retries=5, session, rate_limiter, metrics):\n    endpoint = extract_endpoint(url)\n    await rate_limiter.acquire(endpoint)\n    base = 0.2\n    cap = 10\n    for attempt in range(max_retries + 1):\n        t0 = time.monotonic()\n        try:\n            async with session.request(method, url) as resp:\n                latency = time.monotonic() - t0\n                metrics.record(endpoint, resp.status, latency, attempt)\n                if resp.status in {429, 500, 502, 503, 504}:\n                    raise transient_error()\n                return await resp.read()\n        except Exception:\n            latency = time.monotonic() - t0\n            metrics.record(endpoint, None, latency, attempt)\n            if attempt == max_retries:\n                raise\n            delay = min(cap, base * (2 ** attempt)) + random.uniform(0, 0.1)\n            await asyncio.sleep(delay)\n```\n\n## Follow-up Questions\n\n- How would you test the rate limiter under bursty traffic?\n- How would you extend to idempotent vs non-idempotent methods?","diagram":null,"difficulty":"intermediate","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","NVIDIA","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T16:52:41.364Z","createdAt":"2026-01-15T16:52:41.364Z"},{"id":"q-2704","question":"Design an async Python NDJSON processor that reads lines from an AsyncIterable[str], each line containing a top-level version and payload. It must validate using a dynamic, versioned Pydantic model registry, support on-the-fly hot-swapping of versions without restart, ensure per-version in-order processing, memory-bounded streaming, and backpressure to a downstream sink. Include a test scaffold showing v1→v2 migration and a runtime version swap?","answer":"Implementation: maintain a thread-safe registry mapping version to Pydantic models; parse each line to dict, look up model, validate payload, then emit to a bounded asyncio.Queue consumed by downstrea","explanation":"## Why This Is Asked\nTests runtime schema evolution, memory-bounded streaming, and backpressure integration in a single async Python pipeline.\n\n## Key Concepts\n- Async NDJSON processing\n- Versioned Pydantic models\n- Atomic registry swap\n- In-order per-version processing\n- Bounded queues and reordering buffers\n\n## Code Example\n```python\nfrom typing import AsyncIterable, Dict, Type\nfrom pydantic import BaseModel\nimport asyncio\n\nclass V1(BaseModel):\n    x: int\nclass V2(BaseModel):\n    x: int\n    y: int\n\nasync def process(input_stream: AsyncIterable[str], registry: Dict[int, Type[BaseModel]], sink: asyncio.Queue):\n    # skeleton for versioned validation and backpressure\n    pass\n```\n\n## Follow-up Questions\n- How would you test memory usage and backpressure under slow sinks?\n- How would you retire old versions without disrupting in-flight lines?","diagram":"flowchart TD\n  A[NDJSON line] --> B[Registry lookup]\n  B --> C[Validate payload]\n  C --> D[Enqueue downstream]\n  D --> E[Downstream sink]","difficulty":"advanced","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Anthropic","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T07:39:09.489Z","createdAt":"2026-01-16T07:39:09.489Z"},{"id":"q-474","question":"Write a Python function that takes a list of integers and returns the sum of all even numbers. How would you handle edge cases?","answer":"Use a generator expression with sum() for optimal performance. Implement input validation to handle edge cases robustly. The function gracefully handles empty lists by returning 0 and ignores non-integer elements to prevent TypeErrors. For large datasets, the generator approach provides memory efficiency by processing items one at a time.","explanation":"## Solution\n\n```python\ndef sum_even_numbers(numbers):\n    if not isinstance(numbers, list):\n        raise TypeError(\"Input must be a list\")\n    return sum(num for num in numbers if isinstance(num, int) and num % 2 == 0)\n```\n\n## Key Points\n\n- **Generator Expression**: Memory-efficient processing of large datasets\n- **Input Validation**: Robust error handling for invalid input types\n- **Edge Case Handling**: Returns 0 for empty lists, safely skips non-integers\n- **Performance**: Single pass algorithm with O(n) time complexity\n\n## Complexity\n\n- **Time**: O(n) - iterates through each element once\n- **Space**: O(1) - constant extra space using generator expression","diagram":"flowchart TD\n  A[Input List] --> B{Validate Type}\n  B -->|Valid| C[Filter Even Numbers]\n  B -->|Invalid| D[Raise TypeError]\n  C --> E[Sum Results]\n  E --> F[Return Sum]","difficulty":"beginner","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Microsoft","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-09T09:05:08.903Z","createdAt":"2025-12-26 12:51:05"},{"id":"q-503","question":"How would you implement a distributed rate limiter using Redis with sliding window algorithm to handle 10,000 requests per second across multiple API servers?","answer":"Use Redis sorted sets with timestamps as scores and request IDs as members. For each request, ZADD current timestamp, ZREMRANGEBYSCORE to remove old entries, then ZCARD to count. Use Lua script for atomic operations to prevent race conditions.","explanation":"## Implementation\n\nUse Redis sorted sets with timestamps as scores:\n\n```python\n# Lua script for atomic rate limiting\nlocal key = KEYS[1]\nlocal window = tonumber(ARGV[1])\nlocal limit = tonumber(ARGV[2])\nlocal now = tonumber(ARGV[3])\n\nredis.call('zadd', key, now, now)\nredis.call('zremrangebyscore', key, 0, now - window)\nlocal count = redis.call('zcard', key)\nredis.call('expire', key, window)\n\nreturn count <= limit\n```\n\n## Key Considerations\n\n- **Atomicity**: Lua script prevents race conditions\n- **Memory**: Sorted sets automatically clean old entries\n- **Performance**: O(log N) operations, handles 10,000+ RPS\n- **Scalability**: Single Redis instance can serve multiple API servers\n- **Sliding Window**: Provides accurate rate limiting over time windows","diagram":"flowchart TD\n  A[API Request] --> B[Redis Lua Script]\n  B --> C{Within Limit?}\n  C -->|Yes| D[Process Request]\n  C -->|No| E[Return 429]\n  F[Cleanup Old Entries] --> B\n  G[Set TTL] --> B","difficulty":"advanced","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","LinkedIn","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":["redis","sliding window","sorted sets","lua script","rate limiting","distributed"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2026-01-09T03:44:22.264Z","createdAt":"2025-12-26 12:51:06"},{"id":"q-559","question":"You're building a data processing pipeline that needs to handle large CSV files efficiently. How would you implement a memory-efficient solution using Python generators to process files that don't fit in RAM?","answer":"Use Python's `csv` module with generator functions to read files line-by-line. Implement `yield` in a custom generator that processes chunks, avoiding loading entire files. Combine with `itertools.islice` for batch processing.","explanation":"## Memory-Efficient CSV Processing\n\n- **Generator Pattern**: Use `yield` to process rows incrementally\n- **csv.reader**: Built-in CSV parser handles edge cases\n- **Chunk Processing**: Batch rows with `itertools.islice`\n- **Memory Control**: Constant O(1) memory usage\n\n```python\nimport csv\nfrom itertools import islice\n\ndef csv_generator(filename, chunk_size=1000):\n    with open(filename) as f:\n        reader = csv.DictReader(f)\n        while True:\n            chunk = list(islice(reader, chunk_size))\n            if not chunk:\n                break\n            yield chunk\n```\n\n## Key Benefits\n\n- Processes files of any size without memory constraints\n- Maintains constant memory usage regardless of file size\n- Allows streaming data transformation and analysis\n- Integrates seamlessly with existing Python data tools","diagram":"flowchart TD\n  A[Large CSV File] --> B[csv.reader]\n  B --> C[Generator Function]\n  C --> D[Process Chunk]\n  D --> E[yield Results]\n  E --> F[Next Chunk]\n  F --> D","difficulty":"intermediate","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Snap","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-08T11:56:21.596Z","createdAt":"2025-12-26 12:51:07"},{"id":"q-588","question":"How would you implement a rate limiter using Python's asyncio to prevent API abuse while maintaining high throughput?","answer":"Use asyncio.Semaphore with token bucket algorithm. Track request timestamps in a deque, calculate wait time based on rate limit, and use asyncio.sleep() for throttling. Implement per-client limits usi","explanation":"## Rate Limiting Implementation\n\n### Token Bucket Approach\n- Use asyncio.Semaphore for concurrent request control\n- Implement token bucket with asyncio.gather() for batch processing\n- Track timestamps in collections.deque for O(1) operations\n\n### Async Pattern\n```python\nimport asyncio\nfrom collections import deque\n\nclass RateLimiter:\n    def __init__(self, rate_limit, time_window):\n        self.rate_limit = rate_limit\n        self.time_window = time_window\n        self.requests = deque()\n    \n    async def acquire(self):\n        now = asyncio.get_event_loop().time()\n        # Remove old requests\n        while self.requests and self.requests[0] <= now - self.time_window:\n            self.requests.popleft()\n        \n        if len(self.requests) >= self.rate_limit:\n            sleep_time = self.time_window - (now - self.requests[0])\n            await asyncio.sleep(sleep_time)\n            return await self.acquire()\n        \n        self.requests.append(now)\n```\n\n### Production Considerations\n- Use Redis for distributed rate limiting across multiple instances\n- Implement sliding window for more accurate rate limiting\n- Add circuit breaker pattern for API protection\n- Monitor and log rate limit violations for debugging","diagram":"flowchart TD\n  A[Request] --> B{Check Rate Limit}\n  B -->|Within Limit| C[Process Request]\n  B -->|Exceeded| D[Calculate Wait Time]\n  D --> E[Async Sleep]\n  E --> B\n  C --> F[Update Request Log]\n  F --> G[Return Response]","difficulty":"intermediate","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","DoorDash","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":["asyncio","rate limiter","semaphore","token bucket","api abuse","throughput","deque"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2025-12-27T04:58:08.145Z","createdAt":"2025-12-27T01:14:34.507Z"},{"id":"q-852","question":"Design a memory-efficient streaming dedup pipeline in Python that reads a multi-GB log file line-by-line and prints only the first occurrence of each unique line, skipping duplicates, while persisting dedup state across restarts. Use a probabilistic data structure with a configurable false-positive rate and provide a minimal runnable snippet?","answer":"Use a Bloom-filter-backed streaming pipeline: read lines via a generator, check a Bloom filter; if a line is not present, yield it and add it to the filter; if present, skip. Persist the filter's bit ","explanation":"## Why This Is Asked\nTests ability to design streaming data processing with memory constraints, probabilistic data structures, and persistence. It touches trade-offs between false positives, memory, and restart behavior; requires specifying parameters and a runnable snippet.\n\n## Key Concepts\n- Streaming IO and generators\n- Bloom filters and false-positive rate\n- State persistence across restarts\n- Parameter tuning m, k for given n, p\n\n## Code Example\n```javascript\n# Python-like BloomFilter (illustrative)\nclass BloomFilter:\n    def __init__(self, m, k):\n        self.bits = bytearray(m // 8 + 1)\n        self.k = k\n    def _hashes(self, item):\n        h1 = hash(item)\n        h2 = (hash(str(item)) * 1103515245) & 0x7fffffff\n        for i in range(self.k):\n            yield (h1 + i * h2) % len(self.bits)\n    def add(self, item):\n        for idx in self._hashes(item):\n            self.bits[idx] = 1\n    def __contains__(self, item):\n        return all(self.bits[idx] for idx in self._hashes(item))\n```\n\n## Follow-up Questions\n- How would you adapt this for distributed workers?\n- How would you handle log rotation and cache eviction?","diagram":"flowchart TD\n  A[Read line] --> B[Check Bloom filter]\n  B --> C{New}\n  C --> D[Emit line and add to Bloom]\n  B --> E{Duplicate}\n  E --> F[Skip]","difficulty":"intermediate","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","MongoDB","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T13:35:10.938Z","createdAt":"2026-01-12T13:35:10.938Z"},{"id":"q-975","question":"Design an asynchronous NDJSON pipeline in Python that ingests lines of JSON from an async source, validates each line against a Pydantic model, applies a lightweight transform, and yields results to a downstream consumer. Ensure memory stays bounded when the consumer slows by using a bounded asyncio.Queue?","answer":"Use an async producer–consumer with a bounded asyncio.Queue. The producer reads NDJSON lines from the async source, validates with a Pydantic model, and enqueues transformed events. The consumer drain","explanation":"## Why This Is Asked\n\nTests ability to build streaming, backpressure-aware pipelines; real-world data rates and validation.\n\n## Key Concepts\n\n- Async I/O and producers/consumers\n- NDJSON streaming\n- Pydantic validation\n- Bounded queues for backpressure\n- Error handling and fault tolerance\n\n## Code Example\n\n```javascript\n# Python-like skeleton showing producer/consumer with asyncio.Queue\nimport asyncio\nfrom pydantic import BaseModel\n\nclass Event(BaseModel):\n    ts: float\n    user: str\n    value: float\n\nasync def producer(src, q: asyncio.Queue):\n    async for line in src:\n        evt = Event.parse_raw(line)\n        await q.put(evt)\n\nasync def consumer(q: asyncio.Queue):\n    while True:\n        item = await q.get()\n        # process item\n        q.task_done()\n```\n\n## Follow-up Questions\n\n- How would you measure backpressure and adapt queue size?\n- How to handle malformed lines without dropping the stream?","diagram":null,"difficulty":"intermediate","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Coinbase","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T17:39:00.639Z","createdAt":"2026-01-12T17:39:00.639Z"}],"subChannels":["async","best-practices","fundamentals","general"],"companies":["Adobe","Airbnb","Amazon","Anthropic","Apple","Bloomberg","Coinbase","Databricks","DoorDash","Google","Hugging Face","IBM","LinkedIn","Lyft","Meta","Microsoft","MongoDB","NVIDIA","Netflix","PayPal","Robinhood","Scale Ai","Slack","Snap","Snowflake","Stripe","Tesla","Twitter","Two Sigma","Uber","Zoom"],"stats":{"total":23,"beginner":4,"intermediate":11,"advanced":8,"newThisWeek":16}}