{"questions":[{"id":"q-196","question":"How would you implement a rate-limited async HTTP client using aiohttp and asyncio.Semaphore to handle 1000 requests while respecting API limits?","answer":"Use asyncio.Semaphore(50) for concurrency control and aiohttp.ClientSession with time-based rate limiting between requests.","explanation":"## Why Asked\nTests async programming and API rate limiting skills in production scenarios.\n\n## Key Concepts\nAsyncIO, Semaphore, Rate Limiting, HTTP Client Design\n\n## Code Example\n```\nimport aiohttp\nimport asyncio\n\nasync def rate_limited_client(urls):\n    semaphore = asyncio.Semaphore(50)\n    async with aiohttp.ClientSession() as session:\n        tasks = [fetch_url(session, url, semaphore) for url in urls]\n        return await asyncio.gather(*tasks)\n\nasync def fetch_url(session, url, semaphore):\n    async with semaphore:\n        await asyncio.sleep(0.1)  # Rate limit\n        async with session.get(url) as response:\n            return await response.text()\n```","diagram":"flowchart TD\n  A[Start] --> B[Create Semaphore]\n  B --> C[Create ClientSession]\n  C --> D[Process URLs Concurrently]\n  D --> E[Apply Rate Limits]\n  E --> F[Return Results]\n  F --> G[End]","difficulty":"intermediate","tags":["asyncio","aiohttp","concurrency"],"channel":"python","subChannel":"async","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":"https://www.youtube.com/watch?v=Qb9s3UiMSTA"},"companies":["Amazon","Google","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":["asyncio","semaphore","aiohttp","concurrency","rate limiting","async"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2026-01-08T11:27:41.329Z","createdAt":"2025-12-26 12:51:06"},{"id":"q-224","question":"How would you implement a thread-safe singleton in Python with lazy initialization, proper type hints, and discuss the trade-offs between metaclass, decorator, and module-level approaches for a production system?","answer":"Implement using metaclass with double-checked locking and TypeVar for generics: metaclass provides compile-time enforcement, decorator offers simplicity, while module-level singletons are most Pythonic. Use threading.Lock for thread safety and generic typing support.","explanation":"## Context\n\nThis question assesses understanding of design patterns, concurrency, and Python-specific implementation trade-offs. Senior developers should compare multiple singleton approaches and justify their choice.\n\n## Code Examples\n\n### Metaclass Approach\n\n```python\nfrom typing import TypeVar, Type, Optional\nimport threading\n\nT = TypeVar('T')\n\nclass SingletonMeta(type):\n    _instances: dict[Type, object] = {}\n    _lock: threading.Lock = threading.Lock()\n    \n    def __call__(cls: Type[T], *args, **kwargs) -> T:\n        if cls not in cls._instances:\n            with cls._lock:\n                if cls not in cls._instances:\n                    instance = super().__call__(*args, **kwargs)\n                    cls._instances[cls] = instance\n        return cls._instances[cls]\n```\n\n### Decorator Approach\n\n```python\ndef singleton(cls):\n    instances = {}\n    lock = threading.Lock()\n    \n    def wrapper(*args, **kwargs):\n        if cls not in instances:\n            with lock:\n                if cls not in instances:\n                    instances[cls] = cls(*args, **kwargs)\n        return instances[cls]\n    \n    return wrapper\n```\n\n### Module-Level Approach\n\n```python\n# singleton_module.py\nclass _DatabaseConnection:\n    def __init__(self):\n        self.connection = None\n    \n    def connect(self):\n        if not self.connection:\n            self.connection = create_db_connection()\n        return self.connection\n\n# Single instance at module import time\ndatabase = _DatabaseConnection()\n```\n\n## Trade-offs Analysis\n\n**Metaclass Approach:**\n- Pros: Compile-time enforcement, clean class definition, full type hint support\n- Cons: More complex, metaclass magic can be confusing for new developers\n- Best for: Production systems requiring strict singleton enforcement\n\n**Decorator Approach:**\n- Pros: Simple to apply, non-intrusive, easy to understand\n- Cons: Runtime enforcement only, can be bypassed with direct class instantiation\n- Best for: Quick implementations and prototyping\n\n**Module-Level Approach:**\n- Pros: Most Pythonic, guaranteed thread safety by import system, zero overhead\n- Cons: Less flexible, eager initialization, harder to test\n- Best for: Simple cases where lazy initialization isn't required\n\n## Production Recommendation\n\nFor production systems, use the metaclass approach with double-checked locking. It provides the best balance of thread safety, type hint support, and enforcement while maintaining clean API design.","diagram":"graph TD\n    A[Thread 1 Request] --> B{Instance Exists?}\n    C[Thread 2 Request] --> B\n    B -->|No| D[Acquire Lock]\n    B -->|Yes| K[Return Instance]\n    D --> E{Double Check}\n    E -->|No| F[Create Instance]\n    E -->|Yes| G[Release Lock]\n    F --> H[Initialize]\n    H --> I[Store in Class Dict]\n    I --> G\n    G --> K\n    J[Thread N Request] --> B","difficulty":"advanced","tags":["pep8","typing","testing"],"channel":"python","subChannel":"best-practices","sourceUrl":null,"videos":null,"companies":["Amazon","Google","Meta","Microsoft","Netflix"],"eli5":"Imagine you have a special toy that only one kid can play with at a time. When you want to play with it, you check if anyone else is using it first. If it's free, you grab it and tell everyone else 'I'm using this toy now!' So no two kids can accidentally both think they're the special toy owner. This is like making sure only one copy of something exists in your computer program. You can have different rules about how to share toys: one way is having a playground boss who decides (metaclass), another is putting a special sticker on the toy box (decorator), or just having one toy box for the whole school (module-level). Each way works, but some are easier to understand than others!","relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-30T06:40:35.857Z","createdAt":"2025-12-26 12:51:07"},{"id":"q-178","question":"In Python, when do `is` and `==` return different results, and why does this happen with object identity vs equality?","answer":"`is` checks memory identity (same object), `==` checks value equality. Different for distinct objects with same values: `[1,2] is [1,2]` is False but `[1,2] == [1,2]` is True.","explanation":"## Concept Overview\nPython distinguishes between object identity and value equality. `is` compares memory addresses using `id()`, while `==` compares values using `__eq__` method.\n\n## Implementation\n```python\n# Identity vs Equality\na = [1, 2, 3]\nb = [1, 2, 3]\nc = a\n\nprint(a == b)  # True - same values\nprint(a is b)  # False - different objects\nprint(a is c)  # True - same object\n\n# Integer optimization (small integers)\nx = 256\ny = 256\nprint(x is y)  # True - interned\n\nx = 257\ny = 257\nprint(x is y)  # False - different objects\n```\n\n## Trade-offs\n- `is`: Faster, checks if exactly same object\n- `==`: Slower, checks if values are equivalent\n- Use `is` for singletons (None, True, False)\n- Use `==` for value comparison\n\n## Common Pitfalls\n- Assuming `is` works for value comparison\n- Not understanding integer/string interning\n- Using `is` with mutable objects incorrectly\n- Forgetting that `==` can be overridden by custom classes","diagram":"graph TD\n    A[Object A] -->|id: 0x1234| C[Memory Location 0x1234]\n    B[Object B] -->|id: 0x5678| D[Memory Location 0x5678]\n    E[Object C] -->|id: 0x1234| C\n    \n    F[Value: [1,2,3]] --> G[Content Comparison]\n    H[Value: [1,2,3]] --> G\n    I[Value: [1,2,3]] --> G\n    \n    J[is operator] --> K[Compare memory addresses]\n    L[== operator] --> M[Compare values via __eq__]\n    \n    style C fill:#e1f5fe\n    style D fill:#e1f5fe\n    style G fill:#f3e5f5","difficulty":"intermediate","tags":["python","basics"],"channel":"python","subChannel":"fundamentals","sourceUrl":null,"videos":{"shortVideo":"https://www.youtube.com/watch?v=mO_dS3rXDIs","longVideo":"https://www.youtube.com/watch?v=CZ8bZPqtwU0"},"companies":["Amazon","Google","Meta","Microsoft","Uber"],"eli5":"Imagine you have two identical toy cars. They look exactly the same and can do the same tricks - that's like using == to check if they're equal. But are they the SAME exact toy car? No! They're two different cars that just happen to look alike. That's like using 'is' to check if they're the same object. In Python, when you make two lists with the same numbers, they're like those two toy cars - they look identical (== says True) but they're actually two separate objects in the computer's memory (is says False). Only when you point to the very same object will both 'is' and '==' say True!","relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-25T14:57:56.251Z","createdAt":"2025-12-26 12:51:06"},{"id":"q-1085","question":"You’re building a real-time log processor in Python for a messaging app. You receive a stream of JSON lines like {\"user_id\": 123, \"event\": \"message_sent\", \"ts\": 1610000000}. Emit only the first event per (user_id, event) within a 60-second sliding window. Design a memory-bounded, generator-based pipeline that reads from an iterable of strings and yields deduplicated dicts in order. Include edge cases and memory considerations?","answer":"Implement a generator-based pipeline that reads lines, parses JSON, and yields the first dict for each (user_id, event) in a 60-second sliding window. Maintain a bounded TTL cache (OrderedDict) key->t","explanation":"## Why This Is Asked\n\nTests ability to design streaming pipelines with memory constraints and dedup logic without external stores.\n\n## Key Concepts\n\n- Generators and streaming data\n- Sliding window deduplication\n- TTL-based cache with eviction\n- Robust JSON parsing and field validation\n\n## Code Example\n\n```python\ndef dedup_stream(lines, window_seconds=60):\n    import json\n    from collections import deque, OrderedDict\n    cache = OrderedDict()\n    for line in lines:\n        obj = json.loads(line)\n        user = obj.get(\"user_id\")\n        evt = obj.get(\"event\")\n        ts = obj.get(\"ts\")\n        if user is None or evt is None or ts is None:\n            continue\n        key = (user, evt)\n        # evict stale\n        for k in list(cache):\n            if ts - cache[k] > window_seconds:\n                del cache[k]\n        if key in cache:\n            continue\n        cache[key] = ts\n        yield obj\n```\n\n## Follow-up Questions\n\n- How would you adapt for out-of-order arrivals? \n- How to scale with multiple processes?","diagram":"flowchart TD\n  A[Input Stream] --> B[Parse JSON]\n  B --> C[Key by (user_id, event)]\n  C --> D[TTL Cache Lookup]\n  D --> E{Duplicate in window?}\n  E -->|No| F[Emit] --> G[Update Cache]\n  E -->|Yes| H[Drop]","difficulty":"intermediate","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T22:19:30.480Z","createdAt":"2026-01-12T22:19:30.480Z"},{"id":"q-1149","question":"In a streaming data etl, you're given a JSONL stream where each line is a JSON object with 'host' and 'value'. Implement a memory-efficient Python generator top_n_hosts(n, lines) that yields the top N hosts by total value after consuming the stream, without keeping the entire per-host totals in memory. Use a min-heap to maintain current top-N and skip invalid lines?","answer":"Process the JSONL stream line by line; validate host and numeric value; accumulate per-host totals in a dict; maintain a fixed-size min-heap of (total, host) for the current top-N; when a host total i","explanation":"## Why This Is Asked\n\nTests ability to craft streaming pipelines and manage memory with data structure choices.\n\n## Key Concepts\n\n- Generators and lazy evaluation\n- JSONL parsing with error handling\n- Per-key aggregation with a fixed-size min-heap (top-N)\n- Trade-offs: O(H) memory vs O(N log N) heap maintenance\n\n## Code Example\n\n```python\nimport json, heapq\n\ndef top_n_hosts(n, lines):\n    totals = {}\n    heap = []  # min-heap of (total, host)\n    seen = set()\n    for line in lines:\n        try:\n            obj = json.loads(line)\n            host = obj.get('host')\n            val = float(obj.get('value', 0))\n            if host is None:\n                continue\n        except Exception:\n            continue\n        totals[host] = totals.get(host, 0.0) + val\n        heapq.heappush(heap, (totals[host], host))\n        if len(heap) > n:\n            heapq.heappop(heap)\n        seen.add(host)\n    for total, host in sorted(heap, reverse=True):\n        yield host, total\n```\n\n## Follow-up Questions\n\n- How would you modify to handle streaming resets or windowed top-N?","diagram":null,"difficulty":"intermediate","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T01:34:23.375Z","createdAt":"2026-01-13T01:34:23.375Z"},{"id":"q-1250","question":"Design an asynchronous NDJSON ingestion pipeline in Python that reads an async iterable of lines, validates each with a Pydantic model, and routes records to per-tenant partitions based on the 'tenant_id'. Implement bounded queues per partition with a total memory cap, guarantee per-tenant in-order processing, and a slower downstream sink that creates backpressure. Provide a test plan with skewed partitions and slow sinks?","answer":"To implement, create an async function ingest(source, writer_factory) that builds a dict of per-tenant queues and a small worker pool per tenant. Each line is parsed as JSON, validated with a Pydantic","explanation":"## Why This Is Asked\nTests ability to design partitioned, back-pressured pipelines in Python with asyncio, ensuring per-tenant ordering and bounded memory, a realistic scalability concern at large-scale companies like Snowflake/Uber.\n\n## Key Concepts\n- Async data ingestion\n- Bounded queues and backpressure\n- Per-tenant partitioning and in-order semantics\n\n## Code Example\n```python\n# skeleton illustrating structure\nfrom typing import AsyncIterable, Dict\nfrom asyncio import Queue, TaskGroup\nfrom pydantic import BaseModel\nimport json\nimport asyncio\n\nclass Record(BaseModel):\n    tenant_id: str\n    payload: dict\n\nasync def ingest(source: AsyncIterable[str], writer_factory):\n    queues: Dict[str, Queue] = {}\n    async def worker(tid: str, q: Queue):\n        sink = writer_factory(tid)\n        while True:\n            item = await q.get()\n            if item is None:\n                break\n            await sink.write(item)\n    async with TaskGroup() as g:\n        async for line in source:\n            rec = Record(**json.loads(line))\n            q = queues.setdefault(rec.tenant_id, Queue(maxsize=128))\n            await q.put(rec)\n        for q in queues.values():\n            await q.put(None)\n        # workers auto-join on exit\n```\n\n## Follow-up Questions\n- How would you handle dynamic partition keys that can exceed memory?\n- How would you validate end-to-end ordering when a partition has backpressure and failure?","diagram":"flowchart TD\n  S[NDJSON Source] --> M[Parse & Validate]\n  M --> P{{Partition by tenant_id}}\n  P --> Q[Per-tenant Queues]\n  Q --> W[Per-tenant Workers]\n  W --> D[Downstream Sink]","difficulty":"advanced","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Snowflake","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T06:43:51.710Z","createdAt":"2026-01-13T06:43:51.710Z"},{"id":"q-1275","question":"Design an advanced async Python pipeline: NDJSON lines arrive via a TCP socket, each is validated with a Pydantic model, then a deduplication stage uses a sliding-window Bloom filter to emit each event at most once in a 24-hour window. The pipeline must stay memory-bounded, support backpressure to the producer, and allow the downstream sink to pace itself. Provide a concrete test plan with clock skew and bursty traffic?","answer":"Approach: implement an async NDJSON ingest with a bounded asyncio.Queue, Pydantic validation, and a dedupe stage using a sliding window Bloom filter implemented with buckets. The Bloom filter stores f","explanation":"## Why This Is Asked\nTests ability to design streaming pipelines with memory constraints, asynchronous backpressure, and practical deduplication.\n\n## Key Concepts\n- Async ingestion with bounded queues\n- Pydantic validation of streaming data\n- Sliding-window Bloom filter for deduplication\n- Backpressure and pacing of producers/consumers\n\n## Code Example\n```python\n#_placeholder\n```\n\n## Follow-up Questions\n- How would you calibrate the Bloom filter's false positive rate for varying traffic? \n- How would you extend to multi-node deduplication without shared state?","diagram":null,"difficulty":"advanced","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Hugging Face","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T07:39:46.313Z","createdAt":"2026-01-13T07:39:46.313Z"},{"id":"q-1285","question":"Given a list of dictionaries representing users, implement normalize_users(users) that returns a list of User dataclass instances with fields id:int, name:str, signup_ts:datetime, is_active:bool. Coerce id from str to int, parse signup_ts ISO8601 strings, interpret is_active from common truthy values, and fill defaults for missing fields. If any record is invalid, raise ValueError with the indices of invalid records?","answer":"Implement a User dataclass and normalize_users that maps dicts to User, coercing id to int, parsing signup_ts via datetime.fromisoformat, and is_active from common truthy values; default missing field","explanation":"## Why This Is Asked\nThis question tests data normalization, type coercion, and error reporting using Python basics.\n\n## Key Concepts\n- Dataclasses for lightweight models\n- Type hints and basic validation\n- ISO 8601 parsing with datetime\n- Error aggregation for batch records\n\n## Code Example\n```python\nfrom dataclasses import dataclass\nfrom datetime import datetime\nfrom typing import List, Dict, Any\n\n@dataclass\nclass User:\n    id: int\n    name: str\n    signup_ts: datetime\n    is_active: bool\n\ndef _to_int(v: Any) -> int:\n    if isinstance(v, int):\n        return v\n    return int(v)\n\ndef _parse_ts(ts: Any) -> datetime:\n    if isinstance(ts, datetime):\n        return ts\n    return datetime.fromisoformat(ts)\n\ndef _to_bool(v: Any) -> bool:\n    if isinstance(v, bool):\n        return v\n    if isinstance(v, str):\n        return v.strip().lower() in {\"true\", \"1\", \"yes\", \"y\"}\n    return bool(v)\n\n\ndef normalize_users(users: List[Dict[str, Any]]) -> List[User]:\n    out: List[User] = []\n    bad: List[int] = []\n    for i, rec in enumerate(users):\n        try:\n            u = User(\n                id=_to_int(rec.get(\"id\")),\n                name=rec.get(\"name\", \"\"),\n                signup_ts=_parse_ts(rec.get(\"signup_ts\")),\n                is_active=_to_bool(rec.get(\"is_active\", False)),\n            )\n            out.append(u)\n        except Exception:\n            bad.append(i)\n    if bad:\n        raise ValueError(f\"Invalid records at indices: {bad}\")\n    return out\n```\n\n## Follow-up Questions\n- How would you extend to nested user data?\n- How would you validate and report multiple fields per record?","diagram":"flowchart TD\n  A[Start] --> B[Iterate records]\n  B --> C[Coerce fields]\n  C --> D{All valid?}\n  D -->|Yes| E[Emit User]\n  D -->|No| F[Collect index]\n  F --> G[Raise error on end]","difficulty":"beginner","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","MongoDB","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T08:31:56.123Z","createdAt":"2026-01-13T08:31:56.123Z"},{"id":"q-474","question":"Write a Python function that takes a list of integers and returns the sum of all even numbers. How would you handle edge cases?","answer":"Use a generator expression with sum() for optimal performance. Implement input validation to handle edge cases robustly. The function gracefully handles empty lists by returning 0 and ignores non-integer elements to prevent TypeErrors. For large datasets, the generator approach provides memory efficiency by processing items one at a time.","explanation":"## Solution\n\n```python\ndef sum_even_numbers(numbers):\n    if not isinstance(numbers, list):\n        raise TypeError(\"Input must be a list\")\n    return sum(num for num in numbers if isinstance(num, int) and num % 2 == 0)\n```\n\n## Key Points\n\n- **Generator Expression**: Memory-efficient processing of large datasets\n- **Input Validation**: Robust error handling for invalid input types\n- **Edge Case Handling**: Returns 0 for empty lists, safely skips non-integers\n- **Performance**: Single pass algorithm with O(n) time complexity\n\n## Complexity\n\n- **Time**: O(n) - iterates through each element once\n- **Space**: O(1) - constant extra space using generator expression","diagram":"flowchart TD\n  A[Input List] --> B{Validate Type}\n  B -->|Valid| C[Filter Even Numbers]\n  B -->|Invalid| D[Raise TypeError]\n  C --> E[Sum Results]\n  E --> F[Return Sum]","difficulty":"beginner","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Microsoft","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-09T09:05:08.903Z","createdAt":"2025-12-26 12:51:05"},{"id":"q-503","question":"How would you implement a distributed rate limiter using Redis with sliding window algorithm to handle 10,000 requests per second across multiple API servers?","answer":"Use Redis sorted sets with timestamps as scores and request IDs as members. For each request, ZADD current timestamp, ZREMRANGEBYSCORE to remove old entries, then ZCARD to count. Use Lua script for atomic operations to prevent race conditions.","explanation":"## Implementation\n\nUse Redis sorted sets with timestamps as scores:\n\n```python\n# Lua script for atomic rate limiting\nlocal key = KEYS[1]\nlocal window = tonumber(ARGV[1])\nlocal limit = tonumber(ARGV[2])\nlocal now = tonumber(ARGV[3])\n\nredis.call('zadd', key, now, now)\nredis.call('zremrangebyscore', key, 0, now - window)\nlocal count = redis.call('zcard', key)\nredis.call('expire', key, window)\n\nreturn count <= limit\n```\n\n## Key Considerations\n\n- **Atomicity**: Lua script prevents race conditions\n- **Memory**: Sorted sets automatically clean old entries\n- **Performance**: O(log N) operations, handles 10,000+ RPS\n- **Scalability**: Single Redis instance can serve multiple API servers\n- **Sliding Window**: Provides accurate rate limiting over time windows","diagram":"flowchart TD\n  A[API Request] --> B[Redis Lua Script]\n  B --> C{Within Limit?}\n  C -->|Yes| D[Process Request]\n  C -->|No| E[Return 429]\n  F[Cleanup Old Entries] --> B\n  G[Set TTL] --> B","difficulty":"advanced","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","LinkedIn","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":["redis","sliding window","sorted sets","lua script","rate limiting","distributed"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2026-01-09T03:44:22.264Z","createdAt":"2025-12-26 12:51:06"},{"id":"q-559","question":"You're building a data processing pipeline that needs to handle large CSV files efficiently. How would you implement a memory-efficient solution using Python generators to process files that don't fit in RAM?","answer":"Use Python's `csv` module with generator functions to read files line-by-line. Implement `yield` in a custom generator that processes chunks, avoiding loading entire files. Combine with `itertools.islice` for batch processing.","explanation":"## Memory-Efficient CSV Processing\n\n- **Generator Pattern**: Use `yield` to process rows incrementally\n- **csv.reader**: Built-in CSV parser handles edge cases\n- **Chunk Processing**: Batch rows with `itertools.islice`\n- **Memory Control**: Constant O(1) memory usage\n\n```python\nimport csv\nfrom itertools import islice\n\ndef csv_generator(filename, chunk_size=1000):\n    with open(filename) as f:\n        reader = csv.DictReader(f)\n        while True:\n            chunk = list(islice(reader, chunk_size))\n            if not chunk:\n                break\n            yield chunk\n```\n\n## Key Benefits\n\n- Processes files of any size without memory constraints\n- Maintains constant memory usage regardless of file size\n- Allows streaming data transformation and analysis\n- Integrates seamlessly with existing Python data tools","diagram":"flowchart TD\n  A[Large CSV File] --> B[csv.reader]\n  B --> C[Generator Function]\n  C --> D[Process Chunk]\n  D --> E[yield Results]\n  E --> F[Next Chunk]\n  F --> D","difficulty":"intermediate","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Snap","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-08T11:56:21.596Z","createdAt":"2025-12-26 12:51:07"},{"id":"q-588","question":"How would you implement a rate limiter using Python's asyncio to prevent API abuse while maintaining high throughput?","answer":"Use asyncio.Semaphore with token bucket algorithm. Track request timestamps in a deque, calculate wait time based on rate limit, and use asyncio.sleep() for throttling. Implement per-client limits usi","explanation":"## Rate Limiting Implementation\n\n### Token Bucket Approach\n- Use asyncio.Semaphore for concurrent request control\n- Implement token bucket with asyncio.gather() for batch processing\n- Track timestamps in collections.deque for O(1) operations\n\n### Async Pattern\n```python\nimport asyncio\nfrom collections import deque\n\nclass RateLimiter:\n    def __init__(self, rate_limit, time_window):\n        self.rate_limit = rate_limit\n        self.time_window = time_window\n        self.requests = deque()\n    \n    async def acquire(self):\n        now = asyncio.get_event_loop().time()\n        # Remove old requests\n        while self.requests and self.requests[0] <= now - self.time_window:\n            self.requests.popleft()\n        \n        if len(self.requests) >= self.rate_limit:\n            sleep_time = self.time_window - (now - self.requests[0])\n            await asyncio.sleep(sleep_time)\n            return await self.acquire()\n        \n        self.requests.append(now)\n```\n\n### Production Considerations\n- Use Redis for distributed rate limiting across multiple instances\n- Implement sliding window for more accurate rate limiting\n- Add circuit breaker pattern for API protection\n- Monitor and log rate limit violations for debugging","diagram":"flowchart TD\n  A[Request] --> B{Check Rate Limit}\n  B -->|Within Limit| C[Process Request]\n  B -->|Exceeded| D[Calculate Wait Time]\n  D --> E[Async Sleep]\n  E --> B\n  C --> F[Update Request Log]\n  F --> G[Return Response]","difficulty":"intermediate","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","DoorDash","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":["asyncio","rate limiter","semaphore","token bucket","api abuse","throughput","deque"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2025-12-27T04:58:08.145Z","createdAt":"2025-12-27T01:14:34.507Z"},{"id":"q-852","question":"Design a memory-efficient streaming dedup pipeline in Python that reads a multi-GB log file line-by-line and prints only the first occurrence of each unique line, skipping duplicates, while persisting dedup state across restarts. Use a probabilistic data structure with a configurable false-positive rate and provide a minimal runnable snippet?","answer":"Use a Bloom-filter-backed streaming pipeline: read lines via a generator, check a Bloom filter; if a line is not present, yield it and add it to the filter; if present, skip. Persist the filter's bit ","explanation":"## Why This Is Asked\nTests ability to design streaming data processing with memory constraints, probabilistic data structures, and persistence. It touches trade-offs between false positives, memory, and restart behavior; requires specifying parameters and a runnable snippet.\n\n## Key Concepts\n- Streaming IO and generators\n- Bloom filters and false-positive rate\n- State persistence across restarts\n- Parameter tuning m, k for given n, p\n\n## Code Example\n```javascript\n# Python-like BloomFilter (illustrative)\nclass BloomFilter:\n    def __init__(self, m, k):\n        self.bits = bytearray(m // 8 + 1)\n        self.k = k\n    def _hashes(self, item):\n        h1 = hash(item)\n        h2 = (hash(str(item)) * 1103515245) & 0x7fffffff\n        for i in range(self.k):\n            yield (h1 + i * h2) % len(self.bits)\n    def add(self, item):\n        for idx in self._hashes(item):\n            self.bits[idx] = 1\n    def __contains__(self, item):\n        return all(self.bits[idx] for idx in self._hashes(item))\n```\n\n## Follow-up Questions\n- How would you adapt this for distributed workers?\n- How would you handle log rotation and cache eviction?","diagram":"flowchart TD\n  A[Read line] --> B[Check Bloom filter]\n  B --> C{New}\n  C --> D[Emit line and add to Bloom]\n  B --> E{Duplicate}\n  E --> F[Skip]","difficulty":"intermediate","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","MongoDB","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T13:35:10.938Z","createdAt":"2026-01-12T13:35:10.938Z"},{"id":"q-975","question":"Design an asynchronous NDJSON pipeline in Python that ingests lines of JSON from an async source, validates each line against a Pydantic model, applies a lightweight transform, and yields results to a downstream consumer. Ensure memory stays bounded when the consumer slows by using a bounded asyncio.Queue?","answer":"Use an async producer–consumer with a bounded asyncio.Queue. The producer reads NDJSON lines from the async source, validates with a Pydantic model, and enqueues transformed events. The consumer drain","explanation":"## Why This Is Asked\n\nTests ability to build streaming, backpressure-aware pipelines; real-world data rates and validation.\n\n## Key Concepts\n\n- Async I/O and producers/consumers\n- NDJSON streaming\n- Pydantic validation\n- Bounded queues for backpressure\n- Error handling and fault tolerance\n\n## Code Example\n\n```javascript\n# Python-like skeleton showing producer/consumer with asyncio.Queue\nimport asyncio\nfrom pydantic import BaseModel\n\nclass Event(BaseModel):\n    ts: float\n    user: str\n    value: float\n\nasync def producer(src, q: asyncio.Queue):\n    async for line in src:\n        evt = Event.parse_raw(line)\n        await q.put(evt)\n\nasync def consumer(q: asyncio.Queue):\n    while True:\n        item = await q.get()\n        # process item\n        q.task_done()\n```\n\n## Follow-up Questions\n\n- How would you measure backpressure and adapt queue size?\n- How to handle malformed lines without dropping the stream?","diagram":null,"difficulty":"intermediate","tags":["python"],"channel":"python","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Coinbase","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T17:39:00.639Z","createdAt":"2026-01-12T17:39:00.639Z"}],"subChannels":["async","best-practices","fundamentals","general"],"companies":["Amazon","Apple","Bloomberg","Coinbase","DoorDash","Google","Hugging Face","IBM","LinkedIn","Meta","Microsoft","MongoDB","Netflix","Robinhood","Slack","Snap","Snowflake","Stripe","Two Sigma","Uber","Zoom"],"stats":{"total":14,"beginner":2,"intermediate":8,"advanced":4,"newThisWeek":7}}