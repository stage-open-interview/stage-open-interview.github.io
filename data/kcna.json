{"questions":[{"id":"q-1138","question":"You are building a real-time KCNA feed service used by Snap, Meta, and Discord-scale clients to publish and deliver announcements across regions with sub-100ms tail latency. Describe the end-to-end architecture, data model for Announcement, ingestion and delivery pipeline, guarantees (at-least-once vs exactly-once), ordering, deduplication, failover, tests, and observability. How would you scale to 10k updates/sec with 99.999% uptime?","answer":"Design a real-time KCNA feed with an event-sourced pipeline: publish to a durable log (Kafka/Kinesis), process via idempotent services, store state in a scalable DB, and stream updates to regional cac","explanation":"## Why This Is Asked\n\nThis question probes system design at scale, covering data modeling, ingestion pipelines, delivery guarantees, ordering, and observability in a globally distributed, low-latency context.\n\n## Key Concepts\n\n- Event sourcing and durable logs (Kafka/Kinesis)\n- Idempotent processing and deduplication\n- Region-local vs global ordering\n- Delivery guarantees (at-least-once vs exactly-once)\n- Observability, testing, and chaos engineering\n\n## Code Example\n\n```javascript\n// Example: idempotent publish wrapper\nfunction publishAnnouncement(store, event) {\n  const id = event.id;\n  if (store.has(id)) return; // dedup\n  store.set(id, event);\n  // emit to downstream\n}\n```\n\n## Follow-up Questions\n\n- How would you design feature flags for regional rollouts and rollback strategies?\n- What monitoring and tracing would you implement to detect tail latency regressions?","diagram":"flowchart TD\n  Ingest[Ingest] --> Proc[Process]\n  Proc --> Ent[Event Store]\n  Ent --> Del[Delivery]\n  Del --> Client[Client]","difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Meta","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T01:26:25.116Z","createdAt":"2026-01-13T01:26:25.116Z"},{"id":"q-1295","question":"**KCNA Consumer Backpressure & Gap Handling**: In a beginner-friendly KCNA consumer, design a pull-based ingestion path that preserves per-topic offsets, guarantees at-least-once processing, and recovers from transient network slowdowns without duplicating messages. Describe the API shape, offset persistence, retry/backoff strategy, and a minimal test plan including a canary scenario?","answer":"Proposed answer (concise example): A pull-based consumer tracks per-topic offsets in a durable local store, commits after successful processing, and uses idempotent handlers. Retries use exponential b","explanation":"## Why This Is Asked\nThis question probes practical dataflow design for reliable at-least-once delivery and simple backpressure handling in a beginner context.\n\n## Key Concepts\n- Pull-based consumption with per-topic offsets\n- Idempotent processing and offset commits\n- Retry/backoff with jitter and restart replay\n- Canary and integration testing\n\n## Code Example\n```javascript\n// Pseudo API sketch\nclass KCNAConsumer {\n  constructor(store, process) { this.store=store; this.process=process }\n  async poll() { /* fetch by offset, call process, commit */ }\n  commit(offset) { this.store.save(offset) }\n}\n```\n\n## Follow-up Questions\n- How would you test exactly-once vs at-least-once boundaries in this setup?\n- How would you extend this to handle multiple topics with independent offsets?","diagram":null,"difficulty":"beginner","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Plaid","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T08:41:04.859Z","createdAt":"2026-01-13T08:41:04.859Z"},{"id":"q-1431","question":"KCNA cross-region, multi-tenant QoS: Propose an architecture and API for declaring per-tenant Topic SLAs (retention, max throughput) and a cross-region offset store with region-local commit logs. How would you implement per-tenant backpressure, quota enforcement, and exactly-once vs at-least-once semantics under regional outages? Include a concrete canary and testing plan?","answer":"Implement a quota ledger per tenant, topic-level SLA, region-local offsets, and a global commit log. Enforce producer backpressure by dynamically throttling when quotas near limit; ensure exactly-once","explanation":"## Why This Is Asked\nTests ability to design multi-region, multi-tenant KCNA with fair sharing, fault tolerance, and strong guarantees.\n\n## Key Concepts\n- Per-tenant QoS and SLAs\n- Cross-region offsets and commit log\n- Backpressure and quota enforcement\n- Exactly-once vs at-least-once tradeoffs\n- Canary testing and regional outages\n\n## Code Example\n```javascript\n// TS types for TopicSpec and QuotaLedger\ntype TopicSpec = { name:string; retentionMs:number; maxThroughputQps:number; tenant:string };\ntype QuotaLedger = Map<string, number>; // tenant -> remainingQuota\n```\n\n## Follow-up Questions\n- How would you test under bursty traffic and a regional partition?\n- How do you handle tenant migration between regions?\n","diagram":"flowchart TD\n  A[Tenant] --> B[Topic]\n  B --> C[Partition]\n  A --> D[QuotaLedger]\n  C --> E[OffsetsStore]\n  D --> F[ThrottleEngine]\n  F --> G[RegionalGateway]\n  G --> H[GlobalCommitLog]","difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Oracle","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T16:53:08.237Z","createdAt":"2026-01-13T16:53:08.237Z"},{"id":"q-1471","question":"KCNA multi-tenant schema-evolution: design a zero-downtime migration for a KCNA-based event bus used by many tenants where the Event schema evolves from v1 to v2 (add region field, deprecate payload wrapper). How do you enforce backward/forward compatibility, isolate tenants during migration, and validate with canaries? Include tooling, rollback plans, and observability?","answer":"Adopt a central versioned KCNA Schema Registry with per-tenant namespaces and per-topic compatibility. For v1â†’v2, add an optional region field and keep existing fields. Run a migrator that rewrites in","explanation":"## Why This Is Asked\n\nTests ability to design scalable, safe schema migrations in a multi-tenant KCNA setup, including compatibility strategies, canary rollout, and rollback handling.\n\n## Key Concepts\n\n- Schema Registry with versioned, per-tenant schemas\n- Backward, forward, and full compatibility modes\n- Canary deployments and per-tenant offset preservation\n- In-flight data migration and rollback plans\n\n## Code Example\n\n```javascript\nfunction isBackwardCompatible(oldSchema, newSchema) {\n  // Added fields must be optional; no required removals\n  // No type-breaking changes\n  return true;\n}\n```\n\n## Follow-up Questions\n\n- How would you monitor for schema drift and consumer failures during migration?\n- What metrics and alerts would you add to ensure safe rollback timing?","diagram":"flowchart TD\n  A[Producer] --> B[KCNA Topic]\n  B --> C[Schema Registry]\n  C --> D[Versioned Schemas]\n  D --> E[Migration Orchestrator]\n  E --> F[Canary Tenants]\n  F --> G[Rollout to All Tenants]","difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Stripe","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T18:46:26.092Z","createdAt":"2026-01-13T18:46:26.092Z"},{"id":"q-1503","question":"KCNA cross-region tenancy isolation: design a replication topology where tenants' streams stay regional unless opted into global analytics; implement per-tenant topic partitioning, region-aware routing, and idempotent retries with de-dup. How do you enforce locality, protect privacy in cross-region analytics, and handle failover/lag? Include concrete configs, testing strategy, and rollback plan?","answer":"Use region-local KCNA clusters with tenant-scoped partitions and a policy gate for opt-in analytics. Route data by tenant region, avoid cross-region replicas unless flagged, and apply idempotent produ","explanation":"## Why This Is Asked\nTo examine practical cross-region data locality, tenancy boundaries, and privacy-preserving analytics with KCNA, plus testing/rollback discipline.\n\n## Key Concepts\n- Region-local clusters\n- Per-tenant partitions and offsets\n- Opt-in analytics gating\n- Idempotent producers/consumers\n- Observability and rollback\n\n## Code Example\n```javascript\n// Example policy snippet\nconst policy = {\n  tenants: {\n    A: { region: 'us-east-1', analytics: false },\n    B: { region: 'eu-west-1', analytics: true }\n  },\n  globalAnalyticsEnabled: true\n}\n```\n\n## Follow-up Questions\n- How do you monitor cross-region privacy boundaries and lag?\n- What tests would you run to validate failover without impacting tenants?","diagram":"flowchart TD\n  A[Tenant streams] --> B[Region-local KCNA]\n  B --> C[Policy gate]\n  C --> D[Global analytics (optional)]\n  D --> E[Sinks]","difficulty":"intermediate","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T19:40:00.393Z","createdAt":"2026-01-13T19:40:00.393Z"},{"id":"q-1520","question":"KCNA TTL Retention: design a per-topic TTL policy for KCNA events. How would you store TTL metadata, drop expired events without breaking consumer offsets, handle late-arriving events post-expiry, and observe/verify with canary tests? Provide a minimal API shape for setting TTL per topic, a compact storage layout, and a lightweight cleanup workflow?","answer":"Store per-topic TTL in a lightweight index: topic -> expiry epoch. Each message carries publishTime; a background cleaner deletes messages older than TTL while advancing a deletion frontier to preserv","explanation":"## Why This Is Asked\nThis tests understanding of retention, per-topic configuration, and safe cleanup without disturbing consumer progress.\n\n## Key Concepts\n- TTL metadata storage per topic\n- Deletion frontier and per-topic offsets\n- Late-arriving vs expired events\n- Observability and canary validation\n\n## Code Example\n```javascript\n// Pseudo TTL check\nfunction isExpired(msg, ttlSec, now=new Date()) { return (now.getTime() - msg.publishTime) > ttlSec*1000; }\n```\n\n## Follow-up Questions\n- How would you test TTL edge cases (exact expiry, late arrival)?\n- How do you prevent TTL cleanup from blocking high-priority topics? ","diagram":"flowchart TD\n  A[Topic TTL Policy] --> B[Cleanup Job]\n  B --> C[Expire Messages]\n  C --> D[Update Frontiers]\n  D --> E[Offets Consistent Delivery]","difficulty":"beginner","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Citadel","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T20:38:42.651Z","createdAt":"2026-01-13T20:38:42.651Z"},{"id":"q-1602","question":"KCNA key management & tenant isolation: In a **KCNA**-based multi-tenant event bus, each tenant uses a per-tenant envelope encryption key managed by a centralized **KMS**. Design a **zero-downtime** key rotation workflow that rotates tenant keys without breaking consumption, re-encrypts in-flight payloads, and prevents cross-tenant leakage. Include data model (**tenant_id**, key_version, wrapped_key), rollout strategy, rollback, and observability?","answer":"Implement envelope encryption with per-tenant keys managed by a centralized KMS. Each message includes key_version in metadata for decryption. During rotation, create a new key version, publish rotation tokens to consumers, and encrypt new messages under the new version while maintaining backward compatibility for existing in-flight messages.","explanation":"## Why This Is Asked\nTests practical handling of per-tenant security in KCNA, emphasizing zero-downtime rotation, data integrity, and cross-tenant isolation.\n\n## Key Concepts\n- Envelope encryption and per-tenant KMS key versions\n- In-flight data re-encryption and backward compatibility\n- Rollout strategies, canaries, and observability/audit trails\n\n## Code Example\n```javascript\n// Pseudo: resolve key for decryption\nfunction resolveKey(tenantId, payload) {\n  const v = fetchKeyVersion(tenantId, payload);\n  return kms.getKey(tenantId, v);\n}\n```\n\n## Follow-up Questions\n- How to revoke compromised tenant keys?\n- What monitoring metrics indicate successful rotation?\n- How to handle consumers that miss rotation tokens?","diagram":"flowchart TD\n  A[Rotation Initiation] --> B[Publish Metadata]\n  B --> C[Clients Fetch New Key Version]\n  C --> D[Re-encrypt New Messages]\n  D --> E[Grace Window for In-Flight Messages]\n  E --> F[Promote New Key Version]\n  F --> G[Observability & Audit]\n  G --> H[Validation Canary]","difficulty":"intermediate","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Microsoft","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T05:32:04.492Z","createdAt":"2026-01-14T02:31:03.848Z"},{"id":"q-1698","question":"Design a KCNA privacy-first feed where tenants specify a region (US/EU/APAC) and all data remains local. Use per-tenant envelope encryption with KMS, region-scoped brokers, field-level redaction before delivery, and tenant-aware access controls. Add audit trails and canary tests to prove zero cross-tenant leakage, correct redaction, and retention under burst load?","answer":"Design a KCNA privacy-first feed where tenants specify a region (US/EU/APAC) and all data remains local. Use per-tenant envelope encryption with KMS, region-scoped brokers, field-level redaction befor","explanation":"## Why This Is Asked\n\nThis question probes practical privacy-by-design in streaming: data residency, per-tenant cryptography, access control, auditing, and testability under burst traffic. It also checks operational thinking for cross-tenant isolation and retention policies.\n\n## Key Concepts\n\n- Per-tenant residency and tenancy isolation\n- Envelope encryption with KMS and tenant-scoped keys\n- Field-level redaction during transit and at rest\n- Region routing and data locality guarantees\n- Auditing, retention, and compliance controls\n- Canary-based validation under burst load and failure scenarios\n\n## Code Example\n\n```javascript\nfunction redact(record, schema) {\n  const redacted = {};\n  for (const [k, v] of Object.entries(record)) {\n    if (schema.redact?.includes(k)) redacted[k] = \"***\";\n    else redacted[k] = v;\n  }\n  return redacted;\n}\n```\n\n## Follow-up Questions\n\n- How would you test a migration that adds a new redaction rule without breaking existing tenants?\n- How would you verify cross-region data leakage is impossible during network partitions?\n","diagram":"flowchart TD\n  A[Tenant Config] --> B[Regional Router]\n  B --> C[KCNA Regional Broker]\n  C --> D[Envelope Encrypt with Tenant Key]\n  D --> E[Deliver to Region-Specific Client]","difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Tesla","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T07:34:51.806Z","createdAt":"2026-01-14T07:34:51.808Z"},{"id":"q-1723","question":"KCNA dynamic tenancy fairness under bursty workloads: design a per-tenant ingestion path with token-bucket quotas and fair queuing; detail API surface, quota persistence, backpressure signaling, and dynamic rebalancing from telemetry. How do you validate isolation under burst traffic, and what would your canary rollout look like?","answer":"Implement per-tenant KCNA ingestion using token-bucket quotas and a fair-queuing layer. API: POST /ingest with tenantId, topic, payload; on over-quota return 429 with Retry-After. Use telemetry-driven","explanation":"## Why This Is Asked\n\nAssess the candidate's ability to design per-tenant fairness in a high-throughput KCNA pipeline, balancing isolation, latency, and dynamic scaling under bursty traffic.\n\n## Key Concepts\n\n- Per-tenant token-bucket quotas and fair queuing\n- Backpressure signaling (429s, retry-after) and quota persistence\n- Telemetry-driven dynamic rebalancing and burst forgiveness\n- Canary-based validation and observability\n\n## Code Example\n\n```javascript\nclass TokenBucket {\n  constructor(rate, capacity) {\n    this.rate = rate;\n    this.capacity = capacity;\n    this.tokens = capacity;\n    this.last = Date.now();\n  }\n  tryConsume(n = 1) {\n    const now = Date.now();\n    const elapsed = (now - this.last) / 1000;\n    this.tokens = Math.min(this.capacity, this.tokens + elapsed * this.rate);\n    this.last = now;\n    if (this.tokens >= n) {\n      this.tokens -= n;\n      return true;\n    }\n    return false;\n  }\n}\n```\n\n## Follow-up Questions\n\n- How would you test dynamic quota rebalancing under tenant churn?\n- How do you prevent misbehaving tenants from starving others while preserving low latency?","diagram":null,"difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T08:41:19.803Z","createdAt":"2026-01-14T08:41:19.803Z"},{"id":"q-1806","question":"KCNA privacy-by-design: design a tenant-isolated KCNA ingestion and delivery path that enforces per-tenant encryption keys for in-flight and at-rest data, supports on-the-fly key rotation with zero-downtime, and provides auditable access controls for analytic consumers. Describe the KMS integration, key-wrapping strategy, performance impact, and a minimal canary test for rotation?","answer":"Per-tenant envelope encryption: each tenant has a data-key wrapped by KMS; producers encrypt payloads with tenant keys and rotate data-keys via versioned IDs with on-the-fly rewrapping of in-flight en","explanation":"## Why This Is Asked\\nPrivacy-first multi-tenant KCNA is a real challenge; rotation and audit are painful at scale.\\n\\n## Key Concepts\\n- Envelope encryption\\n- Per-tenant KMS keys\\n- On-the-fly rotation with no downtime\\n- Audit trails and access control\\n- Backward compatibility with legacy envelopes\\n- Canary tests for rotation\\n\\n## Code Example\\n```javascript\\n// Pseudo: envelope encrypt data per tenant\\nfunction encryptForTenant(tenantId, plaintext, version) {\\n  const keyId = kms.getKeyId(tenantId, version);\\n  const dataKey = kms.generateDataKey(tenantId, version);\\n  const ciphertext = crypto.aesGcmEncrypt(dataKey.plaintext, plaintext);\\n  const envelope = { keyId, cipher: ciphertext, iv: dataKey.iv };\\n  return envelope;\\n}\\n```\\n\\n## Follow-up Questions\\n- How to handle key compromise and revocation?\\n- How to measure rotation latency and impact on throughput?\\n- How to ensure consistent decryption across hot/cold storage?","diagram":null,"difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Robinhood","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T11:41:32.131Z","createdAt":"2026-01-14T11:41:32.131Z"},{"id":"q-1901","question":"KCNA Time-Travel Replay for Tenants: Suppose a tenant needs to audit a precise 2-hour window of events without halting live ingestion. Design a tenant-scoped time-travel replay feature in KCNA that allows replaying events from a given timestamp while live streams continue, guarantees exactly-once delivery to downstream analytics, and preserves per-tenant offsets. Describe API shapes, storage layout, consistency guarantees, security controls, and a minimal test plan (canaries)?","answer":"Implement a tenant-scoped replay plane that materializes a time-indexed replay log per tenant starting at given timestamp T. Downstream analytics subscribe to a replay stream with idempotent consumers","explanation":"## Why This Is Asked\nThe question probes how to add time-travel auditing without impacting live flow, focusing on idempotency, isolation, and operational safety in KCNA.\n\n## Key Concepts\n- Time-indexed replay per tenant\n- Tenant isolation and per-tenant offsets\n- Idempotent downstream delivery and replay-window semantics\n- Access control, auditing, observability\n- Canary-driven rollout and rollback\n\n## Code Example\n```javascript\n// Minimal API surface for replay\ninterface ReplayOptions { tenantId: string; startTs: number; endTs?: number; mode?: 'replay'|'live'; }\nfunction startTenantReplay(opts: ReplayOptions): Promise<ReplaySession>;\n```\n\n## Follow-up Questions\n- How would you ensure exactly-once semantics across distributed replay streams?\n- How would you monitor replay impact on backpressure and SLAs?\n","diagram":"flowchart TD\n  A(Tenant) --> B(Replay Plane)\n  B --> C(Replay Log)\n  A --> D(Live Ingestion)\n  C --> E(Analytics)\n  D --> E","difficulty":"intermediate","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Meta","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T16:45:56.511Z","createdAt":"2026-01-14T16:45:56.511Z"},{"id":"q-1926","question":"Design a beginner-friendly KCNA feature: tenant-scoped data TTL. Each tenant can configure a TTL (e.g., 24h) for their streams. Describe the API (setTTL on a topic with ttlMs), how per-message metadata is stored, how a background purge runs safely without breaking consumers, and a minimal test plan with canaries?","answer":"Implement per-tenant TTL with an API like POST /tenants/{id}/topics/{topic}/ttl { ttlMs }. Attach a ttlMs timestamp to each message and run a purge daemon that deletes messages older than TTL while em","explanation":"## Why This Is Asked\nThis question tests understanding of per-tenant data isolation, retention controls, and safe data purging in a live streaming system. It requires concrete API shape, data model decisions, and a practical testing strategy suitable for beginners while exposing real trade-offs.\n\n## Key Concepts\n- Tenant-scoped retention policy and TTL\n- Message metadata and tombstone semantics for replay\n- Purge safety relative to consumer offsets\n- Canary-based testing and end-to-end validation\n\n## Code Example\n```javascript\n// Pseudo: set TTL for a tenant-topic\nasync function setTTL(tenantId, topic, ttlMs) {\n  // persist TTL in config store per tenant-topic\n  await configStore.set(`/tenants/${tenantId}/topics/${topic}/ttl`, ttlMs);\n}\n\n// Pseudo: purge loop (simplified)\nfunction purgeOldMessages(tenantId, topic) {\n  const ttlMs = configStore.get(`/tenants/${tenantId}/topics/${topic}/ttl`);\n  const cutoff = Date.now() - ttlMs;\n  for (const msg of storage.scan(tenantId, topic)) {\n    if (msg.timestamp < cutoff) {\n      storage.delete(msg.id);\n      // emit tombstone to preserve replay semantics\n      storage.appendTombstone({ tenantId, topic, id: msg.id });\n    }\n  }\n}\n```\n\n## Follow-up Questions\n- How would you handle TTL changes mid-flight without losing data consistency?\n- How do you verify no live consumers are affected during purge windows?","diagram":null,"difficulty":"beginner","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T17:40:54.254Z","createdAt":"2026-01-14T17:40:54.254Z"},{"id":"q-2025","question":"KCNA ingest fairness at scale: design a per-tenant fair-queuing strategy for bursty producers in a multi-tenant KCNA channel. Implement a two-layer approach with a per-tenant token-bucket at the gateway and a global weighted-round-robin scheduler across tenants to prevent starvation. Define quotas, bounded bursts, and backpressure signaling; outline API contracts, config knobs, and a test plan with synthetic tenants and canaries?","answer":"Two-layer fairness: per-tenant token-bucket at gateway plus a global weighted RR scheduler across tenants. Enforce quotas and bounded bursts; unutilized tokens spill to a pending queue to avoid jitter","explanation":"## Why This Is Asked\nInterviews real-world scaling: fairness in multi-tenant KCNA ingestion under bursty traffic, preventing starvation and ensuring predictable latency for all tenants.\n\n## Key Concepts\n- Per-tenant quotas and bounded bursts\n- Gateways and global scheduling\n- Backpressure signaling and observability\n- Canary-style validation\n\n## Code Example\n```javascript\nclass TokenBucket {\n  constructor(rate, burst) { this.rate = rate; this.burst = burst; this.tokens = burst; this.last = Date.now(); }\n  allow(n=1){ this._drip(); if(this.tokens>=n){ this.tokens-=n; return true; } return false; }\n  _drip(){ const now=Date.now(); const elapsed=(now-this.last)/1000; this.tokens = Math.min(this.burst, this.tokens + elapsed*this.rate); this.last=now; }\n}\n```\n\n## Follow-up Questions\n- How would you monitor fairness and detect starvation?\n- How would quotas adapt during traffic spikes?","diagram":"flowchart TD\n  IngestRequest --> QuotaCheck\n  QuotaCheck -->|Allowed| GatewayQueue\n  GatewayQueue --> KCNA_Core\n  KCNA_Core --> Ack\n  KCNA_Core --> Backpressure\n","difficulty":"intermediate","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Hugging Face","IBM"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T21:33:52.678Z","createdAt":"2026-01-14T21:33:52.679Z"},{"id":"q-2101","question":"KCNA policy-driven tenant isolation: design a per-tenant access-control mechanism at the gateway and stream processors that enforces tenant-scoped authorization for ingest and delivery, supports dynamic policy updates with zero-downtime rollout, and provides an auditable per-event trail. Outline the policy model (tenants, roles, actions), integration with a policy engine (e.g., OPA/ABAC), JWT-based identity, and testing strategy including canaries and rollback?","answer":"Design a policy-driven gateway layer using ABAC with a central policy store (OPA). Each KCNA client presents a JWT containing tenant_id and roles; gateways enforce actions (ingest, read, admin) per resource. Stream processors inherit tenant context from the gateway, maintaining isolation throughout the pipeline. Policy updates are versioned and rolled out via canary deployments with automatic rollback on failure. All events are logged with tenant, action, and policy version for complete audit trails.","explanation":"## Why This Is Asked\nTo probe real-world policy, security, and reliability trade-offs in KCNA at scale.\n\n## Key Concepts\n- ABAC with tenant_id and roles\n- Central policy store (OPA)\n- JWT-based identity\n- Policy versioning and canary rollouts\n- Per-event auditing\n\n## Code Example\n```javascript\n// Pseudo gateway policy check\nconst ok = evalPolicy({tenant_id, action, resource});\nif (!ok) throw new Error('Access denied');\n```\n\n## Follow-up Questions\n- How would you test policy upgrades without impacting live tenants?\n- What guarantees exist for audit log integrity during rollback?","diagram":null,"difficulty":"intermediate","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","NVIDIA","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T05:00:32.575Z","createdAt":"2026-01-15T02:16:01.304Z"},{"id":"q-2135","question":"KCNA Dead-Letter & Poison Message Handling: For a multi-tenant KCNA deployment, design a per-tenant dead-letter queue strategy that routes malformed events out of the normal path, enforces per-tenant retry budgets, and preserves idempotent replays. Describe DLQ schema, routing rules, retention, operator visibility, and a safe rollout with canaries?","answer":"Route malformed events to a per-tenant DLQ and enforce per-tenant retry budgets. Use dedicated DLQ topics (dlq/tenant-{id}) with event_id and reason. Apply per-tenant retry cap (e.g., 3 attempts/hour)","explanation":"## Why This Is Asked\nThis checks robustness of error handling, tenant isolation, and rollout discipline in multi-tenant KCNA systems.\n\n## Key Concepts\n- Per-tenant DLQ routing and isolation\n- Retry budgeting and backoff strategy\n- Idempotent replays and deduplication\n- Retention, auditing, and operator visibility\n- Canary rollout and rollback plans\n\n## Code Example\n```javascript\n// Pseudo routing snippet\nif (!isValid(event)) {\n  const dlqTopic = `dlq/tenant-${event.tenant_id}`;\n  publish(dlqTopic, { event_id: event.id, tenant_id: event.tenant_id, reason: 'validation_error' });\n} else {\n  routeToNormalPath(event);\n}\n```\n\n## Follow-up Questions\n- How would you test DLQ behavior under bursty tenants and ensure no data leakage?\n- How would you monitor DLQ health and automate cleanup without affecting active tenants?","diagram":"flowchart TD\n  Gateway[Gateway] --> Validator[Validation Stage]\n  Validator -- malformed --> DLQ_Tenant[DLQ per tenant]\n  Validator -- good --> Ingest[Ingestion Path]\n  DLQ_Tenant --> Replay[Reingest / Replay Path]","difficulty":"intermediate","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Instacart","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T04:17:33.369Z","createdAt":"2026-01-15T04:17:33.369Z"},{"id":"q-2165","question":"KCNA observability & tenant-scoped tracing: design end-to-end observability for a multi-tenant KCNA event bus under burst traffic, preserving tenant data isolation while enabling operations debugging. Specify: how to propagate tenant_id and correlation_id across producer, gateway, and consumer; per-tenant metrics and alerting; privacy-preserving trace UI that exposes only metadata; retention, RBAC, and failure-mode testing with canaries?","answer":"Propose OpenTelemetry traces carrying tenant_id and correlation_id across producer, gateway, and consumer; add per-tenant metrics in Prometheus and alerting rules; redact payload data in traces and st","explanation":"Why This Is Asked\n\nTests ability to design scalable, tenant-aware observability for a high-throughput KCNA setup, balancing debugging needs with data isolation and privacy.\n\nKey Concepts\n\n- Distributed tracing with tenant context\n- Data minimization and redaction\n- Per-tenant metrics and alerting\n- RBAC and topic-level access\n- Canary rollout for instrumentation\n\nCode Example\n\n```javascript\n// Minimal tracing snippet showing span with tenant and correlation attributes\nconst { trace } = require('@opentelemetry/api');\nconst tracer = trace.getTracer('kcna');\nfunction emitEvent(tenantId, correlationId, evt) {\n  const span = tracer.startSpan('emitEvent', { attributes: { tenantId, correlationId, action: 'emit' }});\n  // ... emit logic\n  span.end();\n}\n```\n\nFollow-up Questions\n\n- How would you test privacy controls (redaction, audience limits) in traces and verify no cross-tenant leakage?\n- What monitoring and retention policies would you apply to balance cost and debugging fidelity?","diagram":"flowchart TD\n  A[Producer] --> B[Gateway]\n  B --> C[Consumer]\n  subgraph Tenant Isolation\n    D1[Trace with tenant_id] --> D2[Masked Payload]\n  end","difficulty":"intermediate","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Goldman Sachs","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T05:42:53.445Z","createdAt":"2026-01-15T05:42:53.445Z"},{"id":"q-941","question":"Scenario: A global chat platform with 2B MAUs must detect policy-violating content (spam, hate speech) in near real-time while preserving user privacy and multilingual support. Propose an end-to-end pipeline: ingestion, moderation models (rules + ML), latency SLOs (1-2s), privacy safeguards, backpressure handling, retries, and dead-letter queues. Compare on-device vs cloud inference and monitoring?","answer":"Propose a tiered moderation pipeline: client-side tokenization + on-device classifier for first-pass filtering (multilingual light-weight model), with encrypted message IDs, then server-side streaming","explanation":"## Why This Is Asked\nThis question gauges real-time, scalable moderation design, privacy-safe multi-language handling, and the trade-offs between edge and cloud inference.\n\n## Key Concepts\n- Real-time streaming pipelines, SLOs, backpressure\n- Edge (on-device) vs cloud inference, multilingual models\n- Privacy safeguards (encryption, minimal data)\n- DLQ, retries, circuit breakers, monitoring\n\n## Code Example\n```javascript\n// Latency guard example\nif (latencyMs > 2000) {\n  tagAsSlowPath();\n  redirectToFallbackQueue();\n}\n```\n\n## Follow-up Questions\n- How would you validate model drift and false positives in production?\n- What metrics would you surface in dashboards to detect abuse transparently?","diagram":"flowchart TD\n  A[Ingest] --> B[Queue]\n  B --> C[Moderation]\n  C --> D[Enforce/Notify]\n  D --> E[Audit]","difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Slack","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T16:31:29.566Z","createdAt":"2026-01-12T16:31:29.566Z"}],"subChannels":["general"],"companies":["Adobe","Amazon","Anthropic","Bloomberg","Citadel","Databricks","Discord","DoorDash","Goldman Sachs","Hugging Face","IBM","Instacart","LinkedIn","Meta","Microsoft","NVIDIA","Netflix","Oracle","Plaid","Robinhood","Slack","Snap","Snowflake","Stripe","Tesla","Twitter","Two Sigma","Zoom"],"stats":{"total":17,"beginner":3,"intermediate":7,"advanced":7,"newThisWeek":17}}