{"questions":[{"id":"kcna-cloud-native-arch-1768224106527-0","question":"In a 3-node etcd-backed Kubernetes control plane, which backup method yields a consistent, restorable snapshot without taking the API servers offline?","answer":"[{\"id\":\"a\",\"text\":\"Run etcdctl snapshot save on one member while the cluster continues serving requests\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Stop all Kubernetes API servers, then snapshot each member individually\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use etcdctl snapshot save on the leader only\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Back up the etcd data directory from the current leader and restore from that\",\"isCorrect\":false}]","explanation":"## Correct Answer\nRun etcdctl snapshot save on one member while the cluster continues serving requests, which yields a consistent snapshot of the whole cluster without downtime.\n\n## Why Other Options Are Wrong\n- Stop all API servers: downtime and not necessary; you can snapshot while running.\n- Leader-only snapshot is not guaranteed to capture a consistent state if the leader changes; etcd snapshots can be taken from any member.\n- Copying the data directory is unsafe for live clusters and does not guarantee a consistent snapshot.\n\n## Key Concepts\n- etcd backups\n- etcd snapshot semantics\n- Disaster recovery testing\n\n## Real-World Application\n- DR planning for Kubernetes control plane; ensures quick restore using etcd snapshots; must store to durable storage and test restore regularly.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","etcd","AWS","EKS","Terraform","Prometheus","certification-mcq","domain-weight-16"],"channel":"kcna","subChannel":"cloud-native-arch","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:21:46.531Z","createdAt":"2026-01-12 13:21:46"},{"id":"kcna-cloud-native-arch-1768224106527-1","question":"You deploy PostgreSQL as a StatefulSet with persistent volumes across 3 nodes. To minimize downtime during rolling updates, which combination ensures data consistency and high availability?","answer":"[{\"id\":\"a\",\"text\":\"Use a Deployment with a single PersistentVolume\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"StatefulSet with volumeClaimTemplates, a headless service, readiness probes, and a PodDisruptionBudget set to 1\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Use a DaemonSet to run a single pod per node\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use a Job to perform rolling updates\",\"isCorrect\":false}]","explanation":"## Correct Answer\nStatefulSet with volumeClaimTemplates, a headless service, readiness probes, and a PodDisruptionBudget set to 1 ensures stable network identities, durable storage, and controlled evictions, which minimizes downtime during rolling updates.\n\n## Why Other Options Are Wrong\n- Deployments are not suited for stateful databases and do not provide stable storage guarantees.\n- DaemonSets are per-node workers and do not provide the required database topology.\n- Jobs are one-off tasks, not suitable for ongoing rolling updates of a stateful service.\n\n## Key Concepts\n- StatefulSet, VolumeClaimTemplates, headless service, readiness probes\n- PodDisruptionBudget\n\n## Real-World Application\n- Rolling updates for a clustered database with minimal downtime in production environments.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","StatefulSet","PostgreSQL","AWS-EKS","Terraform","Prometheus","certification-mcq","domain-weight-16"],"channel":"kcna","subChannel":"cloud-native-arch","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:21:47.075Z","createdAt":"2026-01-12 13:21:47"},{"id":"kcna-cloud-native-arch-1768224106527-2","question":"In a shared Kubernetes cluster, you want to prevent noisy neighbors from exhausting cluster resources. Which combination best enforces per-namespace resource limits?","answer":"[{\"id\":\"a\",\"text\":\"ResourceQuota only\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"LimitRange only\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"ResourceQuota + LimitRange\",\"isCorrect\":true},{\"id\":\"d\",\"text\":\"PodSecurityPolicy + ResourceQuota\",\"isCorrect\":false}]","explanation":"## Correct Answer\nResourceQuota plus LimitRange enforces namespace-level limits and sets default/requested limits for pods, preventing resource starvation.\n\n## Why Other Options Are Wrong\n- ResourceQuota alone lacks per-pod defaults and maximums without explicit limits.\n- LimitRange alone cannot cap total per-namespace usage without an overall quota.\n- PodSecurityPolicy governs security, not resource usage quotas.\n\n## Key Concepts\n- ResourceQuota, LimitRange, per-namespace isolation\n\n## Real-World Application\n- Enforcing fair resource distribution in multi-tenant clusters, avoiding noisy neighbors.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","ResourceQuota","Terraform","EKS","Prometheus","certification-mcq","domain-weight-16"],"channel":"kcna","subChannel":"cloud-native-arch","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:21:47.613Z","createdAt":"2026-01-12 13:21:47"},{"id":"kcna-cloud-native-arch-1768224106527-3","question":"You need centralized log and metrics collection across the cluster to support incident response. Which pairing best provides durable logs and real-time metrics?","answer":"[{\"id\":\"a\",\"text\":\"kubectl logs for individual pods and cluster-wide Grafana\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Centralized logging stack (EFK/ELK) and metrics stack (Prometheus + Grafana)\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Only Kubernetes events\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Node-level journald without aggregation\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA centralized logging stack (EFK/ELK) plus a metrics stack (Prometheus + Grafana) provides durable, searchable logs and real-time metrics for effective incident response.\n\n## Why Other Options Are Wrong\n- kubectl logs per-pod is ad-hoc and hard to correlate across nodes.\n- Kubernetes events alone miss detailed logs and long-term retention.\n- Node journald without aggregation is not centralized and lacks long-term retention.\n\n## Key Concepts\n- Centralized logging, metrics collection, observability stacks\n\n## Real-World Application\n- Rapid incident detection, root-cause analysis, and post-incident reviews.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","EFK","Prometheus","Grafana","AWS-EKS","Terraform","certification-mcq","domain-weight-16"],"channel":"kcna","subChannel":"cloud-native-arch","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:21:47.796Z","createdAt":"2026-01-12 13:21:47"},{"id":"kcna-cloud-native-arch-1768224106527-4","question":"In a multi-cluster Kubernetes deployment, you want seamless service discovery across clusters without exposing services via NodePort or relying on VPNs. Which approach best achieves this?","answer":"[{\"id\":\"a\",\"text\":\"Use a cloud VPN to connect clusters and expose services via NodePort\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use a service mesh with a multi-cluster control plane and cross-cluster service discovery (Istio or Linkerd) with a shared CA\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Use an Ingress resource with a single global load balancer\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Run all services in a single cluster only\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA service mesh with a multi-cluster control plane and cross-cluster service discovery provides seamless interaction across clusters without NodePort exposure or VPNs, especially when using a shared CA for mTLS.\n\n## Why Other Options Are Wrong\n- VPN plus NodePort reintroduces exposure and management overhead.\n- Ingress with a global load balancer does not inherently provide native cross-cluster service discovery.\n- Running all services in one cluster defeats the multi-cluster requirement.\n\n## Key Concepts\n- Multi-cluster service discovery, Istio/Linkerd, cross-cluster control plane\n\n## Real-World Application\n- Consistent user experience and traffic routing across regional clusters in a cloud-native environment.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","Istio","Linkerd","AWS-EKS","Terraform","Prometheus","certification-mcq","domain-weight-16"],"channel":"kcna","subChannel":"cloud-native-arch","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:21:47.978Z","createdAt":"2026-01-12 13:21:48"},{"id":"kcna-container-orchestration-1768193476058-0","question":"In a three-node Kubernetes cluster, you want a deployment to evenly distribute its pods across nodes and adapt as nodes are added or removed. Which scheduling feature should you enable?","answer":"[{\"id\":\"a\",\"text\":\"nodeSelector\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"taints and tolerations\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"podAffinity\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"TopologySpreadConstraints\",\"isCorrect\":true}]","explanation":"## Correct Answer\n**TopologySpreadConstraints** evenly distributes Pods across topology domains such as nodes, helping the scheduler minimize skew. In this scenario, enabling topology spread ensures pods are not concentrated on a subset of nodes as nodes change.\n\n## Why Other Options Are Wrong\n- **nodeSelector** pins Pods to specific nodes and does not guarantee even distribution\n- **taints and tolerations** influence scheduling but do not guarantee even spread\n- **podAffinity** can encourage co-location of pods, which may worsen skew rather than improve it\n\n## Key Concepts\n- **TopologySpreadConstraints**\n- topologyKey (e.g., kubernetes.io/hostname)\n- workload distribution across nodes\n\n## Real-World Application\nApply topologySpreadConstraints to the PodSpec in your Deployment:\n\n```yaml\nspec:\n  template:\n    spec:\n      topologySpreadConstraints:\n      - maxSkew: 1\n        topologyKey: kubernetes.io/hostname\n        whenUnsatisfiable: DoNotSchedule\n```\nThis configuration ensures pods spread evenly across nodes and adapts when nodes are added or removed.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","kubectl","TopologySpreadConstraints","EKS","AWS","Terraform","Container Orchestration","certification-mcq","domain-weight-22"],"channel":"kcna","subChannel":"container-orchestration","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T04:51:16.059Z","createdAt":"2026-01-12 04:51:16"},{"id":"kcna-container-orchestration-1768193476058-1","question":"In an AWS-hosted Kubernetes cluster (e.g., EKS), which Service type automatically provisions a cloud load balancer to expose the service to the internet with a stable external endpoint?","answer":"[{\"id\":\"a\",\"text\":\"ClusterIP\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"NodePort\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"LoadBalancer\",\"isCorrect\":true},{\"id\":\"d\",\"text\":\"ExternalName\",\"isCorrect\":false}]","explanation":"## Correct Answer\n**LoadBalancer** service automatically provisions an AWS Elastic Load Balancer (ELB) for the cluster and provides a stable external endpoint. \n\n## Why Other Options Are Wrong\n- **ClusterIP** only exposes the service within the cluster\n- **NodePort** opens a port on each node but does not automatically create a cloud LB with a stable external address\n- **ExternalName** maps the service to an external DNS name; it does not create an internal cluster service with a load balancer\n\n## Key Concepts\n- **Service types**: ClusterIP, NodePort, LoadBalancer\n- Cloud provider integration on **AWS/EKS** to provision external resources\n\n## Real-World Application\nIn production on AWS/EKS, use a Service of type **LoadBalancer** to obtain a public endpoint that remains stable for the life of the load balancer. For advanced routing patterns, you can pair this with an Ingress controller (e.g., ALB Ingress Controller).\n","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","kubectl","LoadBalancer","EKS","AWS","Terraform","Container Orchestration","certification-mcq","domain-weight-22"],"channel":"kcna","subChannel":"container-orchestration","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T04:51:16.470Z","createdAt":"2026-01-12 04:51:16"},{"id":"kcna-container-orchestration-1768193476058-2","question":"You rolled out a Deployment and want to revert to the previous revision if the new revision has issues. Which command should you run?","answer":"[{\"id\":\"a\",\"text\":\"kubectl rollout undo deployment/myapp\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"kubectl rollout restart deployment/myapp\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"kubectl apply -f deployment.yaml\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"kubectl delete deployment myapp\",\"isCorrect\":false}]","explanation":"## Correct Answer\n**kubectl rollout undo deployment/myapp** reverts to the previous revision of the Deployment by restoring the prior ReplicaSet. \n\n## Why Other Options Are Wrong\n- **kubectl rollout restart deployment/myapp** triggers a rolling restart of the current revision rather than a rollback\n- **kubectl apply -f deployment.yaml** applies changes but does not automatically revert to a previous revision\n- **kubectl delete deployment myapp** removes the deployment entirely, not a rollback\n\n## Key Concepts\n- **kubectl rollout undo**\n- Deployment revision history and ReplicaSets\n- Rollout status monitoring\n\n## Real-World Application\nWhen a new rollout causes issues, run the undo command to quickly revert, then monitor with `kubectl rollout status deployment/myapp` and validate application health before deciding on subsequent steps.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","kubectl","rollout undo","EKS","AWS","Terraform","Container Orchestration","certification-mcq","domain-weight-22"],"channel":"kcna","subChannel":"container-orchestration","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T04:51:16.882Z","createdAt":"2026-01-12 04:51:16"},{"id":"kcna-k8s-fundamentals-1768151951289-0","question":"A Deployment creates Pods that start Running but show 0/1 READY, and the corresponding Service has 0 endpoints. Which change is most likely to fix the issue?","answer":"[{\"id\":\"a\",\"text\":\"Correct the readinessProbe to ensure it reports readiness (e.g., correct path/port) so the Pod becomes Ready.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Increase the number of replicas to 2.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Switch the Service type to NodePort to bypass readiness.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Remove the livenessProbe.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe Pod never reports Ready, so the endpoint set for the Service remains empty. Fixing the readinessProbe so the container signals readiness (correct path/port) makes the Pod Ready and populates endpoints.\n\n## Why Other Options Are Wrong\n- Increase replicas doesn't solve readiness of individual pods; endpoints remain 0 if pods aren't ready.\n- Switching to NodePort does not bypass readiness checks; traffic will still not be routed until the Pod reports Ready.\n- Removing the livenessProbe does not affect initial readiness state.\n\n## Key Concepts\n- Readiness probes control service endpoints exposure\n- Service endpoints reflect pods that report Ready\n- Kubernetes networking routes traffic to ready pods only\n\n## Real-World Application\n- When deploying new versions, ensure readiness probes accurately reflect application readiness to avoid routing errors and user-visible failures.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","Networking","PodReadiness","Probes","certification-mcq","domain-weight-46"],"channel":"kcna","subChannel":"k8s-fundamentals","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T17:19:11.290Z","createdAt":"2026-01-11 17:19:11"},{"id":"kcna-k8s-fundamentals-1768151951289-1","question":"A Pod in namespace-a needs to access a ConfigMap in namespace-b; RBAC denies cross-namespace access by default. Which RBAC setup correctly grants this access across namespaces?","answer":"[{\"id\":\"a\",\"text\":\"Create a Role in namespace-a granting get on ConfigMaps in namespace-a and bind it in namespace-a.\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Create a ClusterRole that permits get on configmaps and a ClusterRoleBinding binding the Pod's ServiceAccount from namespace-a across namespaces.\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Create a Role in namespace-b granting get on ConfigMaps in namespace-b and bind it in namespace-b.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Create a ServiceAccount in namespace-a and bind to a Role in namespace-a.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct because cluster-scoped roles (ClusterRole) with a ClusterRoleBinding can grant permissions across namespaces to a ServiceAccount. This allows the SA in namespace-a to perform the configured access on ConfigMaps (in namespace-b) depending on the policy.\n\n## Why Other Options Are Wrong\n- A incorrectly limits permissions to namespace-a, not enabling cross-namespace access.\n- C binds within namespace-b, which does not grant cross-namespace access from namespace-a.\n- D binds within namespace-a but does not grant cross-namespace access to ConfigMaps.\n\n## Key Concepts\n- RBAC: ClusterRole vs Role semantics\n- ClusterRoleBinding can grant permissions across namespaces\n- ServiceAccount scope and subject namespace matter\n\n## Real-World Application\n- Designing cross-namespace access in multi-tenant clusters requires ClusterRole-based bindings to avoid per-namespace role duplication.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","RBAC","ClusterRole","CrossNamespace","certification-mcq","domain-weight-46"],"channel":"kcna","subChannel":"k8s-fundamentals","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T17:19:11.705Z","createdAt":"2026-01-11 17:19:12"},{"id":"kcna-k8s-fundamentals-1768151951289-2","question":"During a rolling update of a Deployment with 5 replicas, you want to guarantee at least 4 pods are always available and not more than one can be unavailable during the update. Which configuration achieves this?","answer":"[{\"id\":\"a\",\"text\":\"Set RollingUpdate strategy with maxUnavailable: 1 and create a PodDisruptionBudget with minAvailable: 4.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Set RollingUpdate strategy with maxUnavailable: 0 and minAvailable: 5.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use RollingUpdate with maxSurge: 2 and PodDisruptionBudget minAvailable: 3.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use Recreate strategy with PodDisruptionBudget minAvailable: 5.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A is correct because maxUnavailable: 1 ensures at most one pod is down during the update, and PodDisruptionBudget with minAvailable: 4 guarantees at least four pods are always available.\n\n## Why Other Options Are Wrong\n- B is overly strict and would block updates since zero pods can be unavailable, which is not required by the scenario.\n- C uses minAvailable 3, which would allow up to two pods to be down during disruptions, not meeting the 4-pod availability requirement.\n- D uses the Recreate strategy which stops all pods before recreating them, not providing rolling updates.\n\n## Key Concepts\n- Deployment rollingUpdate controls maxUnavailable\n- PodDisruptionBudget constrains voluntary disruptions\n- Combined settings yield predictable upgrade behavior\n\n## Real-World Application\n- In production, aligning deployment strategy with PDB prevents service outage during upgrades and maintenance.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","PodDisruptionBudget","RollingUpdate","Deployment","certification-mcq","domain-weight-46"],"channel":"kcna","subChannel":"k8s-fundamentals","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T17:19:12.138Z","createdAt":"2026-01-11 17:19:12"}],"subChannels":["cloud-native-arch","container-orchestration","k8s-fundamentals"],"companies":[],"stats":{"total":11,"beginner":0,"intermediate":11,"advanced":0,"newThisWeek":11}}