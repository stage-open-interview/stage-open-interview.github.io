{"questions":[{"id":"q-1138","question":"You are building a real-time KCNA feed service used by Snap, Meta, and Discord-scale clients to publish and deliver announcements across regions with sub-100ms tail latency. Describe the end-to-end architecture, data model for Announcement, ingestion and delivery pipeline, guarantees (at-least-once vs exactly-once), ordering, deduplication, failover, tests, and observability. How would you scale to 10k updates/sec with 99.999% uptime?","answer":"Design a real-time KCNA feed with an event-sourced pipeline: publish to a durable log (Kafka/Kinesis), process via idempotent services, store state in a scalable DB, and stream updates to regional cac","explanation":"## Why This Is Asked\n\nThis question probes system design at scale, covering data modeling, ingestion pipelines, delivery guarantees, ordering, and observability in a globally distributed, low-latency context.\n\n## Key Concepts\n\n- Event sourcing and durable logs (Kafka/Kinesis)\n- Idempotent processing and deduplication\n- Region-local vs global ordering\n- Delivery guarantees (at-least-once vs exactly-once)\n- Observability, testing, and chaos engineering\n\n## Code Example\n\n```javascript\n// Example: idempotent publish wrapper\nfunction publishAnnouncement(store, event) {\n  const id = event.id;\n  if (store.has(id)) return; // dedup\n  store.set(id, event);\n  // emit to downstream\n}\n```\n\n## Follow-up Questions\n\n- How would you design feature flags for regional rollouts and rollback strategies?\n- What monitoring and tracing would you implement to detect tail latency regressions?","diagram":"flowchart TD\n  Ingest[Ingest] --> Proc[Process]\n  Proc --> Ent[Event Store]\n  Ent --> Del[Delivery]\n  Del --> Client[Client]","difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Meta","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T01:26:25.116Z","createdAt":"2026-01-13T01:26:25.116Z"},{"id":"q-1295","question":"**KCNA Consumer Backpressure & Gap Handling**: In a beginner-friendly KCNA consumer, design a pull-based ingestion path that preserves per-topic offsets, guarantees at-least-once processing, and recovers from transient network slowdowns without duplicating messages. Describe the API shape, offset persistence, retry/backoff strategy, and a minimal test plan including a canary scenario?","answer":"Proposed answer (concise example): A pull-based consumer tracks per-topic offsets in a durable local store, commits after successful processing, and uses idempotent handlers. Retries use exponential b","explanation":"## Why This Is Asked\nThis question probes practical dataflow design for reliable at-least-once delivery and simple backpressure handling in a beginner context.\n\n## Key Concepts\n- Pull-based consumption with per-topic offsets\n- Idempotent processing and offset commits\n- Retry/backoff with jitter and restart replay\n- Canary and integration testing\n\n## Code Example\n```javascript\n// Pseudo API sketch\nclass KCNAConsumer {\n  constructor(store, process) { this.store=store; this.process=process }\n  async poll() { /* fetch by offset, call process, commit */ }\n  commit(offset) { this.store.save(offset) }\n}\n```\n\n## Follow-up Questions\n- How would you test exactly-once vs at-least-once boundaries in this setup?\n- How would you extend this to handle multiple topics with independent offsets?","diagram":null,"difficulty":"beginner","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Plaid","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T08:41:04.859Z","createdAt":"2026-01-13T08:41:04.859Z"},{"id":"q-1431","question":"KCNA cross-region, multi-tenant QoS: Propose an architecture and API for declaring per-tenant Topic SLAs (retention, max throughput) and a cross-region offset store with region-local commit logs. How would you implement per-tenant backpressure, quota enforcement, and exactly-once vs at-least-once semantics under regional outages? Include a concrete canary and testing plan?","answer":"Implement a quota ledger per tenant, topic-level SLA, region-local offsets, and a global commit log. Enforce producer backpressure by dynamically throttling when quotas near limit; ensure exactly-once","explanation":"## Why This Is Asked\nTests ability to design multi-region, multi-tenant KCNA with fair sharing, fault tolerance, and strong guarantees.\n\n## Key Concepts\n- Per-tenant QoS and SLAs\n- Cross-region offsets and commit log\n- Backpressure and quota enforcement\n- Exactly-once vs at-least-once tradeoffs\n- Canary testing and regional outages\n\n## Code Example\n```javascript\n// TS types for TopicSpec and QuotaLedger\ntype TopicSpec = { name:string; retentionMs:number; maxThroughputQps:number; tenant:string };\ntype QuotaLedger = Map<string, number>; // tenant -> remainingQuota\n```\n\n## Follow-up Questions\n- How would you test under bursty traffic and a regional partition?\n- How do you handle tenant migration between regions?\n","diagram":"flowchart TD\n  A[Tenant] --> B[Topic]\n  B --> C[Partition]\n  A --> D[QuotaLedger]\n  C --> E[OffsetsStore]\n  D --> F[ThrottleEngine]\n  F --> G[RegionalGateway]\n  G --> H[GlobalCommitLog]","difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Oracle","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T16:53:08.237Z","createdAt":"2026-01-13T16:53:08.237Z"},{"id":"q-1471","question":"KCNA multi-tenant schema-evolution: design a zero-downtime migration for a KCNA-based event bus used by many tenants where the Event schema evolves from v1 to v2 (add region field, deprecate payload wrapper). How do you enforce backward/forward compatibility, isolate tenants during migration, and validate with canaries? Include tooling, rollback plans, and observability?","answer":"Adopt a central versioned KCNA Schema Registry with per-tenant namespaces and per-topic compatibility. For v1â†’v2, add an optional region field and keep existing fields. Run a migrator that rewrites in","explanation":"## Why This Is Asked\n\nTests ability to design scalable, safe schema migrations in a multi-tenant KCNA setup, including compatibility strategies, canary rollout, and rollback handling.\n\n## Key Concepts\n\n- Schema Registry with versioned, per-tenant schemas\n- Backward, forward, and full compatibility modes\n- Canary deployments and per-tenant offset preservation\n- In-flight data migration and rollback plans\n\n## Code Example\n\n```javascript\nfunction isBackwardCompatible(oldSchema, newSchema) {\n  // Added fields must be optional; no required removals\n  // No type-breaking changes\n  return true;\n}\n```\n\n## Follow-up Questions\n\n- How would you monitor for schema drift and consumer failures during migration?\n- What metrics and alerts would you add to ensure safe rollback timing?","diagram":"flowchart TD\n  A[Producer] --> B[KCNA Topic]\n  B --> C[Schema Registry]\n  C --> D[Versioned Schemas]\n  D --> E[Migration Orchestrator]\n  E --> F[Canary Tenants]\n  F --> G[Rollout to All Tenants]","difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Stripe","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T18:46:26.092Z","createdAt":"2026-01-13T18:46:26.092Z"},{"id":"q-1503","question":"KCNA cross-region tenancy isolation: design a replication topology where tenants' streams stay regional unless opted into global analytics; implement per-tenant topic partitioning, region-aware routing, and idempotent retries with de-dup. How do you enforce locality, protect privacy in cross-region analytics, and handle failover/lag? Include concrete configs, testing strategy, and rollback plan?","answer":"Use region-local KCNA clusters with tenant-scoped partitions and a policy gate for opt-in analytics. Route data by tenant region, avoid cross-region replicas unless flagged, and apply idempotent produ","explanation":"## Why This Is Asked\nTo examine practical cross-region data locality, tenancy boundaries, and privacy-preserving analytics with KCNA, plus testing/rollback discipline.\n\n## Key Concepts\n- Region-local clusters\n- Per-tenant partitions and offsets\n- Opt-in analytics gating\n- Idempotent producers/consumers\n- Observability and rollback\n\n## Code Example\n```javascript\n// Example policy snippet\nconst policy = {\n  tenants: {\n    A: { region: 'us-east-1', analytics: false },\n    B: { region: 'eu-west-1', analytics: true }\n  },\n  globalAnalyticsEnabled: true\n}\n```\n\n## Follow-up Questions\n- How do you monitor cross-region privacy boundaries and lag?\n- What tests would you run to validate failover without impacting tenants?","diagram":"flowchart TD\n  A[Tenant streams] --> B[Region-local KCNA]\n  B --> C[Policy gate]\n  C --> D[Global analytics (optional)]\n  D --> E[Sinks]","difficulty":"intermediate","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T19:40:00.393Z","createdAt":"2026-01-13T19:40:00.393Z"},{"id":"q-1520","question":"KCNA TTL Retention: design a per-topic TTL policy for KCNA events. How would you store TTL metadata, drop expired events without breaking consumer offsets, handle late-arriving events post-expiry, and observe/verify with canary tests? Provide a minimal API shape for setting TTL per topic, a compact storage layout, and a lightweight cleanup workflow?","answer":"Store per-topic TTL in a lightweight index: topic -> expiry epoch. Each message carries publishTime; a background cleaner deletes messages older than TTL while advancing a deletion frontier to preserv","explanation":"## Why This Is Asked\nThis tests understanding of retention, per-topic configuration, and safe cleanup without disturbing consumer progress.\n\n## Key Concepts\n- TTL metadata storage per topic\n- Deletion frontier and per-topic offsets\n- Late-arriving vs expired events\n- Observability and canary validation\n\n## Code Example\n```javascript\n// Pseudo TTL check\nfunction isExpired(msg, ttlSec, now=new Date()) { return (now.getTime() - msg.publishTime) > ttlSec*1000; }\n```\n\n## Follow-up Questions\n- How would you test TTL edge cases (exact expiry, late arrival)?\n- How do you prevent TTL cleanup from blocking high-priority topics? ","diagram":"flowchart TD\n  A[Topic TTL Policy] --> B[Cleanup Job]\n  B --> C[Expire Messages]\n  C --> D[Update Frontiers]\n  D --> E[Offets Consistent Delivery]","difficulty":"beginner","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Citadel","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T20:38:42.651Z","createdAt":"2026-01-13T20:38:42.651Z"},{"id":"q-1602","question":"KCNA key management & tenant isolation: In a **KCNA**-based multi-tenant event bus, each tenant uses a per-tenant envelope encryption key managed by a centralized **KMS**. Design a **zero-downtime** key rotation workflow that rotates tenant keys without breaking consumption, re-encrypts in-flight payloads, and prevents cross-tenant leakage. Include data model (**tenant_id**, key_version, wrapped_key), rollout strategy, rollback, and observability?","answer":"Implement envelope encryption with per-tenant keys managed by a centralized KMS. Each message includes key_version in metadata for decryption. During rotation, create a new key version, publish rotation tokens to consumers, and encrypt new messages under the new version while maintaining backward compatibility for existing in-flight messages.","explanation":"## Why This Is Asked\nTests practical handling of per-tenant security in KCNA, emphasizing zero-downtime rotation, data integrity, and cross-tenant isolation.\n\n## Key Concepts\n- Envelope encryption and per-tenant KMS key versions\n- In-flight data re-encryption and backward compatibility\n- Rollout strategies, canaries, and observability/audit trails\n\n## Code Example\n```javascript\n// Pseudo: resolve key for decryption\nfunction resolveKey(tenantId, payload) {\n  const v = fetchKeyVersion(tenantId, payload);\n  return kms.getKey(tenantId, v);\n}\n```\n\n## Follow-up Questions\n- How to revoke compromised tenant keys?\n- What monitoring metrics indicate successful rotation?\n- How to handle consumers that miss rotation tokens?","diagram":"flowchart TD\n  A[Rotation Initiation] --> B[Publish Metadata]\n  B --> C[Clients Fetch New Key Version]\n  C --> D[Re-encrypt New Messages]\n  D --> E[Grace Window for In-Flight Messages]\n  E --> F[Promote New Key Version]\n  F --> G[Observability & Audit]\n  G --> H[Validation Canary]","difficulty":"intermediate","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Microsoft","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T05:32:04.492Z","createdAt":"2026-01-14T02:31:03.848Z"},{"id":"q-1698","question":"Design a KCNA privacy-first feed where tenants specify a region (US/EU/APAC) and all data remains local. Use per-tenant envelope encryption with KMS, region-scoped brokers, field-level redaction before delivery, and tenant-aware access controls. Add audit trails and canary tests to prove zero cross-tenant leakage, correct redaction, and retention under burst load?","answer":"Design a KCNA privacy-first feed where tenants specify a region (US/EU/APAC) and all data remains local. Use per-tenant envelope encryption with KMS, region-scoped brokers, field-level redaction befor","explanation":"## Why This Is Asked\n\nThis question probes practical privacy-by-design in streaming: data residency, per-tenant cryptography, access control, auditing, and testability under burst traffic. It also checks operational thinking for cross-tenant isolation and retention policies.\n\n## Key Concepts\n\n- Per-tenant residency and tenancy isolation\n- Envelope encryption with KMS and tenant-scoped keys\n- Field-level redaction during transit and at rest\n- Region routing and data locality guarantees\n- Auditing, retention, and compliance controls\n- Canary-based validation under burst load and failure scenarios\n\n## Code Example\n\n```javascript\nfunction redact(record, schema) {\n  const redacted = {};\n  for (const [k, v] of Object.entries(record)) {\n    if (schema.redact?.includes(k)) redacted[k] = \"***\";\n    else redacted[k] = v;\n  }\n  return redacted;\n}\n```\n\n## Follow-up Questions\n\n- How would you test a migration that adds a new redaction rule without breaking existing tenants?\n- How would you verify cross-region data leakage is impossible during network partitions?\n","diagram":"flowchart TD\n  A[Tenant Config] --> B[Regional Router]\n  B --> C[KCNA Regional Broker]\n  C --> D[Envelope Encrypt with Tenant Key]\n  D --> E[Deliver to Region-Specific Client]","difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Tesla","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T07:34:51.806Z","createdAt":"2026-01-14T07:34:51.808Z"},{"id":"q-1723","question":"KCNA dynamic tenancy fairness under bursty workloads: design a per-tenant ingestion path with token-bucket quotas and fair queuing; detail API surface, quota persistence, backpressure signaling, and dynamic rebalancing from telemetry. How do you validate isolation under burst traffic, and what would your canary rollout look like?","answer":"Implement per-tenant KCNA ingestion using token-bucket quotas and a fair-queuing layer. API: POST /ingest with tenantId, topic, payload; on over-quota return 429 with Retry-After. Use telemetry-driven","explanation":"## Why This Is Asked\n\nAssess the candidate's ability to design per-tenant fairness in a high-throughput KCNA pipeline, balancing isolation, latency, and dynamic scaling under bursty traffic.\n\n## Key Concepts\n\n- Per-tenant token-bucket quotas and fair queuing\n- Backpressure signaling (429s, retry-after) and quota persistence\n- Telemetry-driven dynamic rebalancing and burst forgiveness\n- Canary-based validation and observability\n\n## Code Example\n\n```javascript\nclass TokenBucket {\n  constructor(rate, capacity) {\n    this.rate = rate;\n    this.capacity = capacity;\n    this.tokens = capacity;\n    this.last = Date.now();\n  }\n  tryConsume(n = 1) {\n    const now = Date.now();\n    const elapsed = (now - this.last) / 1000;\n    this.tokens = Math.min(this.capacity, this.tokens + elapsed * this.rate);\n    this.last = now;\n    if (this.tokens >= n) {\n      this.tokens -= n;\n      return true;\n    }\n    return false;\n  }\n}\n```\n\n## Follow-up Questions\n\n- How would you test dynamic quota rebalancing under tenant churn?\n- How do you prevent misbehaving tenants from starving others while preserving low latency?","diagram":null,"difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T08:41:19.803Z","createdAt":"2026-01-14T08:41:19.803Z"},{"id":"q-941","question":"Scenario: A global chat platform with 2B MAUs must detect policy-violating content (spam, hate speech) in near real-time while preserving user privacy and multilingual support. Propose an end-to-end pipeline: ingestion, moderation models (rules + ML), latency SLOs (1-2s), privacy safeguards, backpressure handling, retries, and dead-letter queues. Compare on-device vs cloud inference and monitoring?","answer":"Propose a tiered moderation pipeline: client-side tokenization + on-device classifier for first-pass filtering (multilingual light-weight model), with encrypted message IDs, then server-side streaming","explanation":"## Why This Is Asked\nThis question gauges real-time, scalable moderation design, privacy-safe multi-language handling, and the trade-offs between edge and cloud inference.\n\n## Key Concepts\n- Real-time streaming pipelines, SLOs, backpressure\n- Edge (on-device) vs cloud inference, multilingual models\n- Privacy safeguards (encryption, minimal data)\n- DLQ, retries, circuit breakers, monitoring\n\n## Code Example\n```javascript\n// Latency guard example\nif (latencyMs > 2000) {\n  tagAsSlowPath();\n  redirectToFallbackQueue();\n}\n```\n\n## Follow-up Questions\n- How would you validate model drift and false positives in production?\n- What metrics would you surface in dashboards to detect abuse transparently?","diagram":"flowchart TD\n  A[Ingest] --> B[Queue]\n  B --> C[Moderation]\n  C --> D[Enforce/Notify]\n  D --> E[Audit]","difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Slack","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T16:31:29.566Z","createdAt":"2026-01-12T16:31:29.566Z"}],"subChannels":["general"],"companies":["Amazon","Anthropic","Citadel","Databricks","Discord","DoorDash","Meta","Microsoft","NVIDIA","Oracle","Plaid","Slack","Snap","Snowflake","Stripe","Tesla","Twitter","Two Sigma","Zoom"],"stats":{"total":10,"beginner":2,"intermediate":2,"advanced":6,"newThisWeek":10}}