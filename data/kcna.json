{"questions":[{"id":"q-1138","question":"You are building a real-time KCNA feed service used by Snap, Meta, and Discord-scale clients to publish and deliver announcements across regions with sub-100ms tail latency. Describe the end-to-end architecture, data model for Announcement, ingestion and delivery pipeline, guarantees (at-least-once vs exactly-once), ordering, deduplication, failover, tests, and observability. How would you scale to 10k updates/sec with 99.999% uptime?","answer":"Design a real-time KCNA feed with an event-sourced pipeline: publish to a durable log (Kafka/Kinesis), process via idempotent services, store state in a scalable DB, and stream updates to regional cac","explanation":"## Why This Is Asked\n\nThis question probes system design at scale, covering data modeling, ingestion pipelines, delivery guarantees, ordering, and observability in a globally distributed, low-latency context.\n\n## Key Concepts\n\n- Event sourcing and durable logs (Kafka/Kinesis)\n- Idempotent processing and deduplication\n- Region-local vs global ordering\n- Delivery guarantees (at-least-once vs exactly-once)\n- Observability, testing, and chaos engineering\n\n## Code Example\n\n```javascript\n// Example: idempotent publish wrapper\nfunction publishAnnouncement(store, event) {\n  const id = event.id;\n  if (store.has(id)) return; // dedup\n  store.set(id, event);\n  // emit to downstream\n}\n```\n\n## Follow-up Questions\n\n- How would you design feature flags for regional rollouts and rollback strategies?\n- What monitoring and tracing would you implement to detect tail latency regressions?","diagram":"flowchart TD\n  Ingest[Ingest] --> Proc[Process]\n  Proc --> Ent[Event Store]\n  Ent --> Del[Delivery]\n  Del --> Client[Client]","difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Meta","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T01:26:25.116Z","createdAt":"2026-01-13T01:26:25.116Z"},{"id":"q-1295","question":"**KCNA Consumer Backpressure & Gap Handling**: In a beginner-friendly KCNA consumer, design a pull-based ingestion path that preserves per-topic offsets, guarantees at-least-once processing, and recovers from transient network slowdowns without duplicating messages. Describe the API shape, offset persistence, retry/backoff strategy, and a minimal test plan including a canary scenario?","answer":"Proposed answer (concise example): A pull-based consumer tracks per-topic offsets in a durable local store, commits after successful processing, and uses idempotent handlers. Retries use exponential b","explanation":"## Why This Is Asked\nThis question probes practical dataflow design for reliable at-least-once delivery and simple backpressure handling in a beginner context.\n\n## Key Concepts\n- Pull-based consumption with per-topic offsets\n- Idempotent processing and offset commits\n- Retry/backoff with jitter and restart replay\n- Canary and integration testing\n\n## Code Example\n```javascript\n// Pseudo API sketch\nclass KCNAConsumer {\n  constructor(store, process) { this.store=store; this.process=process }\n  async poll() { /* fetch by offset, call process, commit */ }\n  commit(offset) { this.store.save(offset) }\n}\n```\n\n## Follow-up Questions\n- How would you test exactly-once vs at-least-once boundaries in this setup?\n- How would you extend this to handle multiple topics with independent offsets?","diagram":null,"difficulty":"beginner","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Plaid","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T08:41:04.859Z","createdAt":"2026-01-13T08:41:04.859Z"},{"id":"q-1431","question":"KCNA cross-region, multi-tenant QoS: Propose an architecture and API for declaring per-tenant Topic SLAs (retention, max throughput) and a cross-region offset store with region-local commit logs. How would you implement per-tenant backpressure, quota enforcement, and exactly-once vs at-least-once semantics under regional outages? Include a concrete canary and testing plan?","answer":"Implement a quota ledger per tenant, topic-level SLA, region-local offsets, and a global commit log. Enforce producer backpressure by dynamically throttling when quotas near limit; ensure exactly-once","explanation":"## Why This Is Asked\nTests ability to design multi-region, multi-tenant KCNA with fair sharing, fault tolerance, and strong guarantees.\n\n## Key Concepts\n- Per-tenant QoS and SLAs\n- Cross-region offsets and commit log\n- Backpressure and quota enforcement\n- Exactly-once vs at-least-once tradeoffs\n- Canary testing and regional outages\n\n## Code Example\n```javascript\n// TS types for TopicSpec and QuotaLedger\ntype TopicSpec = { name:string; retentionMs:number; maxThroughputQps:number; tenant:string };\ntype QuotaLedger = Map<string, number>; // tenant -> remainingQuota\n```\n\n## Follow-up Questions\n- How would you test under bursty traffic and a regional partition?\n- How do you handle tenant migration between regions?\n","diagram":"flowchart TD\n  A[Tenant] --> B[Topic]\n  B --> C[Partition]\n  A --> D[QuotaLedger]\n  C --> E[OffsetsStore]\n  D --> F[ThrottleEngine]\n  F --> G[RegionalGateway]\n  G --> H[GlobalCommitLog]","difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Oracle","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T16:53:08.237Z","createdAt":"2026-01-13T16:53:08.237Z"},{"id":"q-1471","question":"KCNA multi-tenant schema-evolution: design a zero-downtime migration for a KCNA-based event bus used by many tenants where the Event schema evolves from v1 to v2 (add region field, deprecate payload wrapper). How do you enforce backward/forward compatibility, isolate tenants during migration, and validate with canaries? Include tooling, rollback plans, and observability?","answer":"Adopt a central versioned KCNA Schema Registry with per-tenant namespaces and per-topic compatibility. For v1→v2, add an optional region field and keep existing fields. Run a migrator that rewrites in","explanation":"## Why This Is Asked\n\nTests ability to design scalable, safe schema migrations in a multi-tenant KCNA setup, including compatibility strategies, canary rollout, and rollback handling.\n\n## Key Concepts\n\n- Schema Registry with versioned, per-tenant schemas\n- Backward, forward, and full compatibility modes\n- Canary deployments and per-tenant offset preservation\n- In-flight data migration and rollback plans\n\n## Code Example\n\n```javascript\nfunction isBackwardCompatible(oldSchema, newSchema) {\n  // Added fields must be optional; no required removals\n  // No type-breaking changes\n  return true;\n}\n```\n\n## Follow-up Questions\n\n- How would you monitor for schema drift and consumer failures during migration?\n- What metrics and alerts would you add to ensure safe rollback timing?","diagram":"flowchart TD\n  A[Producer] --> B[KCNA Topic]\n  B --> C[Schema Registry]\n  C --> D[Versioned Schemas]\n  D --> E[Migration Orchestrator]\n  E --> F[Canary Tenants]\n  F --> G[Rollout to All Tenants]","difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Stripe","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T18:46:26.092Z","createdAt":"2026-01-13T18:46:26.092Z"},{"id":"q-1503","question":"KCNA cross-region tenancy isolation: design a replication topology where tenants' streams stay regional unless opted into global analytics; implement per-tenant topic partitioning, region-aware routing, and idempotent retries with de-dup. How do you enforce locality, protect privacy in cross-region analytics, and handle failover/lag? Include concrete configs, testing strategy, and rollback plan?","answer":"Use region-local KCNA clusters with tenant-scoped partitions and a policy gate for opt-in analytics. Route data by tenant region, avoid cross-region replicas unless flagged, and apply idempotent produ","explanation":"## Why This Is Asked\nTo examine practical cross-region data locality, tenancy boundaries, and privacy-preserving analytics with KCNA, plus testing/rollback discipline.\n\n## Key Concepts\n- Region-local clusters\n- Per-tenant partitions and offsets\n- Opt-in analytics gating\n- Idempotent producers/consumers\n- Observability and rollback\n\n## Code Example\n```javascript\n// Example policy snippet\nconst policy = {\n  tenants: {\n    A: { region: 'us-east-1', analytics: false },\n    B: { region: 'eu-west-1', analytics: true }\n  },\n  globalAnalyticsEnabled: true\n}\n```\n\n## Follow-up Questions\n- How do you monitor cross-region privacy boundaries and lag?\n- What tests would you run to validate failover without impacting tenants?","diagram":"flowchart TD\n  A[Tenant streams] --> B[Region-local KCNA]\n  B --> C[Policy gate]\n  C --> D[Global analytics (optional)]\n  D --> E[Sinks]","difficulty":"intermediate","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T19:40:00.393Z","createdAt":"2026-01-13T19:40:00.393Z"},{"id":"q-1520","question":"KCNA TTL Retention: design a per-topic TTL policy for KCNA events. How would you store TTL metadata, drop expired events without breaking consumer offsets, handle late-arriving events post-expiry, and observe/verify with canary tests? Provide a minimal API shape for setting TTL per topic, a compact storage layout, and a lightweight cleanup workflow?","answer":"Store per-topic TTL in a lightweight index: topic -> expiry epoch. Each message carries publishTime; a background cleaner deletes messages older than TTL while advancing a deletion frontier to preserv","explanation":"## Why This Is Asked\nThis tests understanding of retention, per-topic configuration, and safe cleanup without disturbing consumer progress.\n\n## Key Concepts\n- TTL metadata storage per topic\n- Deletion frontier and per-topic offsets\n- Late-arriving vs expired events\n- Observability and canary validation\n\n## Code Example\n```javascript\n// Pseudo TTL check\nfunction isExpired(msg, ttlSec, now=new Date()) { return (now.getTime() - msg.publishTime) > ttlSec*1000; }\n```\n\n## Follow-up Questions\n- How would you test TTL edge cases (exact expiry, late arrival)?\n- How do you prevent TTL cleanup from blocking high-priority topics? ","diagram":"flowchart TD\n  A[Topic TTL Policy] --> B[Cleanup Job]\n  B --> C[Expire Messages]\n  C --> D[Update Frontiers]\n  D --> E[Offets Consistent Delivery]","difficulty":"beginner","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Citadel","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T20:38:42.651Z","createdAt":"2026-01-13T20:38:42.651Z"},{"id":"q-1602","question":"KCNA key management & tenant isolation: In a **KCNA**-based multi-tenant event bus, each tenant uses a per-tenant envelope encryption key managed by a centralized **KMS**. Design a **zero-downtime** key rotation workflow that rotates tenant keys without breaking consumption, re-encrypts in-flight payloads, and prevents cross-tenant leakage. Include data model (**tenant_id**, key_version, wrapped_key), rollout strategy, rollback, and observability?","answer":"Implement envelope encryption with per-tenant keys managed by a centralized KMS. Each message includes key_version in metadata for decryption. During rotation, create a new key version, publish rotation tokens to consumers, and encrypt new messages under the new version while maintaining backward compatibility for existing in-flight messages.","explanation":"## Why This Is Asked\nTests practical handling of per-tenant security in KCNA, emphasizing zero-downtime rotation, data integrity, and cross-tenant isolation.\n\n## Key Concepts\n- Envelope encryption and per-tenant KMS key versions\n- In-flight data re-encryption and backward compatibility\n- Rollout strategies, canaries, and observability/audit trails\n\n## Code Example\n```javascript\n// Pseudo: resolve key for decryption\nfunction resolveKey(tenantId, payload) {\n  const v = fetchKeyVersion(tenantId, payload);\n  return kms.getKey(tenantId, v);\n}\n```\n\n## Follow-up Questions\n- How to revoke compromised tenant keys?\n- What monitoring metrics indicate successful rotation?\n- How to handle consumers that miss rotation tokens?","diagram":"flowchart TD\n  A[Rotation Initiation] --> B[Publish Metadata]\n  B --> C[Clients Fetch New Key Version]\n  C --> D[Re-encrypt New Messages]\n  D --> E[Grace Window for In-Flight Messages]\n  E --> F[Promote New Key Version]\n  F --> G[Observability & Audit]\n  G --> H[Validation Canary]","difficulty":"intermediate","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Microsoft","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T05:32:04.492Z","createdAt":"2026-01-14T02:31:03.848Z"},{"id":"q-1698","question":"Design a KCNA privacy-first feed where tenants specify a region (US/EU/APAC) and all data remains local. Use per-tenant envelope encryption with KMS, region-scoped brokers, field-level redaction before delivery, and tenant-aware access controls. Add audit trails and canary tests to prove zero cross-tenant leakage, correct redaction, and retention under burst load?","answer":"Design a KCNA privacy-first feed where tenants specify a region (US/EU/APAC) and all data remains local. Use per-tenant envelope encryption with KMS, region-scoped brokers, field-level redaction befor","explanation":"## Why This Is Asked\n\nThis question probes practical privacy-by-design in streaming: data residency, per-tenant cryptography, access control, auditing, and testability under burst traffic. It also checks operational thinking for cross-tenant isolation and retention policies.\n\n## Key Concepts\n\n- Per-tenant residency and tenancy isolation\n- Envelope encryption with KMS and tenant-scoped keys\n- Field-level redaction during transit and at rest\n- Region routing and data locality guarantees\n- Auditing, retention, and compliance controls\n- Canary-based validation under burst load and failure scenarios\n\n## Code Example\n\n```javascript\nfunction redact(record, schema) {\n  const redacted = {};\n  for (const [k, v] of Object.entries(record)) {\n    if (schema.redact?.includes(k)) redacted[k] = \"***\";\n    else redacted[k] = v;\n  }\n  return redacted;\n}\n```\n\n## Follow-up Questions\n\n- How would you test a migration that adds a new redaction rule without breaking existing tenants?\n- How would you verify cross-region data leakage is impossible during network partitions?\n","diagram":"flowchart TD\n  A[Tenant Config] --> B[Regional Router]\n  B --> C[KCNA Regional Broker]\n  C --> D[Envelope Encrypt with Tenant Key]\n  D --> E[Deliver to Region-Specific Client]","difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Tesla","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T07:34:51.806Z","createdAt":"2026-01-14T07:34:51.808Z"},{"id":"q-1723","question":"KCNA dynamic tenancy fairness under bursty workloads: design a per-tenant ingestion path with token-bucket quotas and fair queuing; detail API surface, quota persistence, backpressure signaling, and dynamic rebalancing from telemetry. How do you validate isolation under burst traffic, and what would your canary rollout look like?","answer":"Implement per-tenant KCNA ingestion using token-bucket quotas and a fair-queuing layer. API: POST /ingest with tenantId, topic, payload; on over-quota return 429 with Retry-After. Use telemetry-driven","explanation":"## Why This Is Asked\n\nAssess the candidate's ability to design per-tenant fairness in a high-throughput KCNA pipeline, balancing isolation, latency, and dynamic scaling under bursty traffic.\n\n## Key Concepts\n\n- Per-tenant token-bucket quotas and fair queuing\n- Backpressure signaling (429s, retry-after) and quota persistence\n- Telemetry-driven dynamic rebalancing and burst forgiveness\n- Canary-based validation and observability\n\n## Code Example\n\n```javascript\nclass TokenBucket {\n  constructor(rate, capacity) {\n    this.rate = rate;\n    this.capacity = capacity;\n    this.tokens = capacity;\n    this.last = Date.now();\n  }\n  tryConsume(n = 1) {\n    const now = Date.now();\n    const elapsed = (now - this.last) / 1000;\n    this.tokens = Math.min(this.capacity, this.tokens + elapsed * this.rate);\n    this.last = now;\n    if (this.tokens >= n) {\n      this.tokens -= n;\n      return true;\n    }\n    return false;\n  }\n}\n```\n\n## Follow-up Questions\n\n- How would you test dynamic quota rebalancing under tenant churn?\n- How do you prevent misbehaving tenants from starving others while preserving low latency?","diagram":null,"difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T08:41:19.803Z","createdAt":"2026-01-14T08:41:19.803Z"},{"id":"q-1806","question":"KCNA privacy-by-design: design a tenant-isolated KCNA ingestion and delivery path that enforces per-tenant encryption keys for in-flight and at-rest data, supports on-the-fly key rotation with zero-downtime, and provides auditable access controls for analytic consumers. Describe the KMS integration, key-wrapping strategy, performance impact, and a minimal canary test for rotation?","answer":"Per-tenant envelope encryption: each tenant has a data-key wrapped by KMS; producers encrypt payloads with tenant keys and rotate data-keys via versioned IDs with on-the-fly rewrapping of in-flight en","explanation":"## Why This Is Asked\\nPrivacy-first multi-tenant KCNA is a real challenge; rotation and audit are painful at scale.\\n\\n## Key Concepts\\n- Envelope encryption\\n- Per-tenant KMS keys\\n- On-the-fly rotation with no downtime\\n- Audit trails and access control\\n- Backward compatibility with legacy envelopes\\n- Canary tests for rotation\\n\\n## Code Example\\n```javascript\\n// Pseudo: envelope encrypt data per tenant\\nfunction encryptForTenant(tenantId, plaintext, version) {\\n  const keyId = kms.getKeyId(tenantId, version);\\n  const dataKey = kms.generateDataKey(tenantId, version);\\n  const ciphertext = crypto.aesGcmEncrypt(dataKey.plaintext, plaintext);\\n  const envelope = { keyId, cipher: ciphertext, iv: dataKey.iv };\\n  return envelope;\\n}\\n```\\n\\n## Follow-up Questions\\n- How to handle key compromise and revocation?\\n- How to measure rotation latency and impact on throughput?\\n- How to ensure consistent decryption across hot/cold storage?","diagram":null,"difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Robinhood","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T11:41:32.131Z","createdAt":"2026-01-14T11:41:32.131Z"},{"id":"q-1901","question":"KCNA Time-Travel Replay for Tenants: Suppose a tenant needs to audit a precise 2-hour window of events without halting live ingestion. Design a tenant-scoped time-travel replay feature in KCNA that allows replaying events from a given timestamp while live streams continue, guarantees exactly-once delivery to downstream analytics, and preserves per-tenant offsets. Describe API shapes, storage layout, consistency guarantees, security controls, and a minimal test plan (canaries)?","answer":"Implement a tenant-scoped replay plane that materializes a time-indexed replay log per tenant starting at given timestamp T. Downstream analytics subscribe to a replay stream with idempotent consumers","explanation":"## Why This Is Asked\nThe question probes how to add time-travel auditing without impacting live flow, focusing on idempotency, isolation, and operational safety in KCNA.\n\n## Key Concepts\n- Time-indexed replay per tenant\n- Tenant isolation and per-tenant offsets\n- Idempotent downstream delivery and replay-window semantics\n- Access control, auditing, observability\n- Canary-driven rollout and rollback\n\n## Code Example\n```javascript\n// Minimal API surface for replay\ninterface ReplayOptions { tenantId: string; startTs: number; endTs?: number; mode?: 'replay'|'live'; }\nfunction startTenantReplay(opts: ReplayOptions): Promise<ReplaySession>;\n```\n\n## Follow-up Questions\n- How would you ensure exactly-once semantics across distributed replay streams?\n- How would you monitor replay impact on backpressure and SLAs?\n","diagram":"flowchart TD\n  A(Tenant) --> B(Replay Plane)\n  B --> C(Replay Log)\n  A --> D(Live Ingestion)\n  C --> E(Analytics)\n  D --> E","difficulty":"intermediate","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Meta","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T16:45:56.511Z","createdAt":"2026-01-14T16:45:56.511Z"},{"id":"q-1926","question":"Design a beginner-friendly KCNA feature: tenant-scoped data TTL. Each tenant can configure a TTL (e.g., 24h) for their streams. Describe the API (setTTL on a topic with ttlMs), how per-message metadata is stored, how a background purge runs safely without breaking consumers, and a minimal test plan with canaries?","answer":"Implement per-tenant TTL with an API like POST /tenants/{id}/topics/{topic}/ttl { ttlMs }. Attach a ttlMs timestamp to each message and run a purge daemon that deletes messages older than TTL while em","explanation":"## Why This Is Asked\nThis question tests understanding of per-tenant data isolation, retention controls, and safe data purging in a live streaming system. It requires concrete API shape, data model decisions, and a practical testing strategy suitable for beginners while exposing real trade-offs.\n\n## Key Concepts\n- Tenant-scoped retention policy and TTL\n- Message metadata and tombstone semantics for replay\n- Purge safety relative to consumer offsets\n- Canary-based testing and end-to-end validation\n\n## Code Example\n```javascript\n// Pseudo: set TTL for a tenant-topic\nasync function setTTL(tenantId, topic, ttlMs) {\n  // persist TTL in config store per tenant-topic\n  await configStore.set(`/tenants/${tenantId}/topics/${topic}/ttl`, ttlMs);\n}\n\n// Pseudo: purge loop (simplified)\nfunction purgeOldMessages(tenantId, topic) {\n  const ttlMs = configStore.get(`/tenants/${tenantId}/topics/${topic}/ttl`);\n  const cutoff = Date.now() - ttlMs;\n  for (const msg of storage.scan(tenantId, topic)) {\n    if (msg.timestamp < cutoff) {\n      storage.delete(msg.id);\n      // emit tombstone to preserve replay semantics\n      storage.appendTombstone({ tenantId, topic, id: msg.id });\n    }\n  }\n}\n```\n\n## Follow-up Questions\n- How would you handle TTL changes mid-flight without losing data consistency?\n- How do you verify no live consumers are affected during purge windows?","diagram":null,"difficulty":"beginner","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T17:40:54.254Z","createdAt":"2026-01-14T17:40:54.254Z"},{"id":"q-2025","question":"KCNA ingest fairness at scale: design a per-tenant fair-queuing strategy for bursty producers in a multi-tenant KCNA channel. Implement a two-layer approach with a per-tenant token-bucket at the gateway and a global weighted-round-robin scheduler across tenants to prevent starvation. Define quotas, bounded bursts, and backpressure signaling; outline API contracts, config knobs, and a test plan with synthetic tenants and canaries?","answer":"Two-layer fairness: per-tenant token-bucket at gateway plus a global weighted RR scheduler across tenants. Enforce quotas and bounded bursts; unutilized tokens spill to a pending queue to avoid jitter","explanation":"## Why This Is Asked\nInterviews real-world scaling: fairness in multi-tenant KCNA ingestion under bursty traffic, preventing starvation and ensuring predictable latency for all tenants.\n\n## Key Concepts\n- Per-tenant quotas and bounded bursts\n- Gateways and global scheduling\n- Backpressure signaling and observability\n- Canary-style validation\n\n## Code Example\n```javascript\nclass TokenBucket {\n  constructor(rate, burst) { this.rate = rate; this.burst = burst; this.tokens = burst; this.last = Date.now(); }\n  allow(n=1){ this._drip(); if(this.tokens>=n){ this.tokens-=n; return true; } return false; }\n  _drip(){ const now=Date.now(); const elapsed=(now-this.last)/1000; this.tokens = Math.min(this.burst, this.tokens + elapsed*this.rate); this.last=now; }\n}\n```\n\n## Follow-up Questions\n- How would you monitor fairness and detect starvation?\n- How would quotas adapt during traffic spikes?","diagram":"flowchart TD\n  IngestRequest --> QuotaCheck\n  QuotaCheck -->|Allowed| GatewayQueue\n  GatewayQueue --> KCNA_Core\n  KCNA_Core --> Ack\n  KCNA_Core --> Backpressure\n","difficulty":"intermediate","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Hugging Face","IBM"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T21:33:52.678Z","createdAt":"2026-01-14T21:33:52.679Z"},{"id":"q-2101","question":"KCNA policy-driven tenant isolation: design a per-tenant access-control mechanism at the gateway and stream processors that enforces tenant-scoped authorization for ingest and delivery, supports dynamic policy updates with zero-downtime rollout, and provides an auditable per-event trail. Outline the policy model (tenants, roles, actions), integration with a policy engine (e.g., OPA/ABAC), JWT-based identity, and testing strategy including canaries and rollback?","answer":"Design a policy-driven gateway layer using ABAC with a central policy store (OPA). Each KCNA client presents a JWT containing tenant_id and roles; gateways enforce actions (ingest, read, admin) per resource. Stream processors inherit tenant context from the gateway, maintaining isolation throughout the pipeline. Policy updates are versioned and rolled out via canary deployments with automatic rollback on failure. All events are logged with tenant, action, and policy version for complete audit trails.","explanation":"## Why This Is Asked\nTo probe real-world policy, security, and reliability trade-offs in KCNA at scale.\n\n## Key Concepts\n- ABAC with tenant_id and roles\n- Central policy store (OPA)\n- JWT-based identity\n- Policy versioning and canary rollouts\n- Per-event auditing\n\n## Code Example\n```javascript\n// Pseudo gateway policy check\nconst ok = evalPolicy({tenant_id, action, resource});\nif (!ok) throw new Error('Access denied');\n```\n\n## Follow-up Questions\n- How would you test policy upgrades without impacting live tenants?\n- What guarantees exist for audit log integrity during rollback?","diagram":null,"difficulty":"intermediate","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","NVIDIA","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T05:00:32.575Z","createdAt":"2026-01-15T02:16:01.304Z"},{"id":"q-2135","question":"KCNA Dead-Letter & Poison Message Handling: For a multi-tenant KCNA deployment, design a per-tenant dead-letter queue strategy that routes malformed events out of the normal path, enforces per-tenant retry budgets, and preserves idempotent replays. Describe DLQ schema, routing rules, retention, operator visibility, and a safe rollout with canaries?","answer":"Route malformed events to a per-tenant DLQ and enforce per-tenant retry budgets. Use dedicated DLQ topics (dlq/tenant-{id}) with event_id and reason. Apply per-tenant retry cap (e.g., 3 attempts/hour)","explanation":"## Why This Is Asked\nThis checks robustness of error handling, tenant isolation, and rollout discipline in multi-tenant KCNA systems.\n\n## Key Concepts\n- Per-tenant DLQ routing and isolation\n- Retry budgeting and backoff strategy\n- Idempotent replays and deduplication\n- Retention, auditing, and operator visibility\n- Canary rollout and rollback plans\n\n## Code Example\n```javascript\n// Pseudo routing snippet\nif (!isValid(event)) {\n  const dlqTopic = `dlq/tenant-${event.tenant_id}`;\n  publish(dlqTopic, { event_id: event.id, tenant_id: event.tenant_id, reason: 'validation_error' });\n} else {\n  routeToNormalPath(event);\n}\n```\n\n## Follow-up Questions\n- How would you test DLQ behavior under bursty tenants and ensure no data leakage?\n- How would you monitor DLQ health and automate cleanup without affecting active tenants?","diagram":"flowchart TD\n  Gateway[Gateway] --> Validator[Validation Stage]\n  Validator -- malformed --> DLQ_Tenant[DLQ per tenant]\n  Validator -- good --> Ingest[Ingestion Path]\n  DLQ_Tenant --> Replay[Reingest / Replay Path]","difficulty":"intermediate","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Instacart","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T04:17:33.369Z","createdAt":"2026-01-15T04:17:33.369Z"},{"id":"q-2165","question":"KCNA observability & tenant-scoped tracing: design end-to-end observability for a multi-tenant KCNA event bus under burst traffic, preserving tenant data isolation while enabling operations debugging. Specify: how to propagate tenant_id and correlation_id across producer, gateway, and consumer; per-tenant metrics and alerting; privacy-preserving trace UI that exposes only metadata; retention, RBAC, and failure-mode testing with canaries?","answer":"Propose OpenTelemetry traces carrying tenant_id and correlation_id across producer, gateway, and consumer; add per-tenant metrics in Prometheus and alerting rules; redact payload data in traces and st","explanation":"Why This Is Asked\n\nTests ability to design scalable, tenant-aware observability for a high-throughput KCNA setup, balancing debugging needs with data isolation and privacy.\n\nKey Concepts\n\n- Distributed tracing with tenant context\n- Data minimization and redaction\n- Per-tenant metrics and alerting\n- RBAC and topic-level access\n- Canary rollout for instrumentation\n\nCode Example\n\n```javascript\n// Minimal tracing snippet showing span with tenant and correlation attributes\nconst { trace } = require('@opentelemetry/api');\nconst tracer = trace.getTracer('kcna');\nfunction emitEvent(tenantId, correlationId, evt) {\n  const span = tracer.startSpan('emitEvent', { attributes: { tenantId, correlationId, action: 'emit' }});\n  // ... emit logic\n  span.end();\n}\n```\n\nFollow-up Questions\n\n- How would you test privacy controls (redaction, audience limits) in traces and verify no cross-tenant leakage?\n- What monitoring and retention policies would you apply to balance cost and debugging fidelity?","diagram":"flowchart TD\n  A[Producer] --> B[Gateway]\n  B --> C[Consumer]\n  subgraph Tenant Isolation\n    D1[Trace with tenant_id] --> D2[Masked Payload]\n  end","difficulty":"intermediate","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Goldman Sachs","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T05:42:53.445Z","createdAt":"2026-01-15T05:42:53.445Z"},{"id":"q-2265","question":"Design a KCNA-based privacy-preserving cross-tenant analytics pipeline where tenants publish raw events but analysts receive aggregated metrics only. Tenants can opt into regional sharing with differential privacy, per-tenant encryption keys, and auditable data lineage. Describe topology, data model, access control, latency targets, and a canary rollout plan?","answer":"Isolate tenants with per-tenant KCNA namespaces; a DP-enricher subscribes to raw streams, emits ε-DP metrics to a guarded analytics topic; use tenant-scoped keys for at-rest/in-transit encryption; enf","explanation":"## Why This Is Asked\nTests ability to design privacy-preserving cross-tenant analytics with real-world constraints and audits.\n\n## Key Concepts\n- Tenant isolation via namespaces\n- Differential privacy budgets per region\n- Tenant-key management and encryption\n- Auditing and data lineage\n- Canary rollout in multi-region setup\n\n## Code Example\n```javascript\nfunction applyDP(data, epsilon) {\n  // placeholder DP mechanism\n  return data.map(x => x + (Math.random() < epsilon ? 0 : 0));\n}\n```\n\n## Follow-up Questions\n- How would you monitor DP budget exhaustion across regions?\n- How would you validate no PII leakage during audits?\n- How would revoking a tenant key affect ongoing streams?","diagram":"flowchart TD\n  P[Tenant Publisher] --> R[KCNA Raw Topic]\n  R --> E[DP Enricher]\n  E --> A[Analytics Output]\n  A --> L[Audit Logs]","difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Netflix","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T09:48:36.908Z","createdAt":"2026-01-15T09:48:36.908Z"},{"id":"q-2311","question":"KCNA Dead-Letter Queue (DLQ): For a beginner-friendly KCNA, design a per-tenant DLQ strategy for messages that fail processing after N retries. Describe API shape to move from main topic to DLQ, retention, how to reprocess, and a simple test canary that demonstrates DLQ routing, backoff between retries, and alerting?","answer":"Per-tenant DLQ should route failed messages to a dedicated DLQ (per tenant or with tenant_id in payload), store original topic and offset, and apply maxRetries with exponential backoff. Include a repr","explanation":"## Why This Is Asked\n\nThis question probes practical handling of failed messages in a multi-tenant KCNA, a common production need that beginners can implement with simple routing, retry/backoff, and reprocessing.\n\n## Key Concepts\n\n- Dead-letter queues per tenant\n- Failure isolation and offset preservation\n- Backoff and max retries\n- Safe reprocessing and idempotence\n- Observability and alerts\n\n## Code Example\n\n```javascript\n// Minimal DLQ routing sketch\nfunction routeToDLQ(event, error){\n  return {tenantId: event.tenantId, originalTopic: event.topic, offset: event.offset, payload: event.payload, error};\n}\n```\n\n## Follow-up Questions\n\n- How to test DLQ under burst failures? \n- How to monitor DLQ latency and replay health?","diagram":null,"difficulty":"beginner","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T11:39:54.578Z","createdAt":"2026-01-15T11:39:54.578Z"},{"id":"q-2425","question":"KCNA per-tenant feature flags: design a control plane to selectively enable a new routing path and compression for KCNA streams, with zero-downtime rollout, tenant-scoped rollback, and audit logs; how would you model toggles, propagate config, implement canaries, and validate impact before full enablement?","answer":"Design a per-tenant feature flag system with a central, strongly consistent store mapping tenantId -> {flags}. Evaluate flags at publish and routing points, ensure idempotent updates, and use canaries","explanation":"## Why This Is Asked\nAssess the ability to design an operational control plane for multi-tenant streaming systems, including per-tenant configurability and safe deployment.\n\n## Key Concepts\n- Feature flags at scale for multi-tenant KCNA\n- Centralized per-tenant config store and propagation\n- Canary rollout gates and safety checks\n- Audit logging and rollback mechanisms\n\n## Code Example\n```javascript\n// Pseudo-code: evaluate if a flag is enabled for a tenant\nfunction isFlagEnabled(tenantId, flagName, defaultVal=false){\n  const cfg = fetchTenantConfig(tenantId); // central store (etcd/kv)\n  return cfg?.flags?.[flagName] ?? defaultVal;\n}\n```\n\n## Follow-up Questions\n- How would you test rollouts with synthetic tenants and simulate partial failures?\n- What observability metrics and dashboards would you add to detect misconfigurations quickly?","diagram":null,"difficulty":"intermediate","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T17:45:49.250Z","createdAt":"2026-01-15T17:45:49.250Z"},{"id":"q-2464","question":"KCNA retention governance: design a per-tenant data lifecycle in KCNA that enforces tenant-specific retention windows, supports legal holds, and runs background tombstone-based deletions with GC across partitions. Explain metadata storage, purge triggers without breaking at-least-once semantics, observability, and a rollback/hold-release workflow?","answer":"Per-tenant retention is stored in a policy store keyed by tenant, topic, and region. Purges enqueue tombstones; GC runs in background with per-tenant quotas to avoid starving in-flight consumers. Lega","explanation":"## Why This Is Asked\nTenants require governance over data lifetime, intersecting privacy and multi-tenant isolation. This tests policy stores, scalable purge, and safe deletion while preserving streaming guarantees.\n\n## Key Concepts\n- Per-tenant retention policy: duration, start, scope\n- Tombstones vs. physical deletion; GC pacing\n- Legal holds: hold flag, release, audit trail\n- Observability: metrics, dashboards, alerts\n- Rollback strategy: canary purge, replay hooks\n\n## Code Example\n```javascript\n// Pseudo-implementation sketch\nclass RetentionPolicy { constructor(tenant, topic, retentionMs, hold) { ... } }\nfunction enqueueTombstone(tenant, topic, offset) { ... }\nfunction runPurgeCycle() { ... } // respects quotas and in-flight reads\n```\n\n## Follow-up Questions\n- How would you test legal-hold behavior across tenants at scale?\n- What changes would you make to ensure cross-tenant isolation during purge and tombstone propagation?","diagram":null,"difficulty":"intermediate","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T19:05:08.222Z","createdAt":"2026-01-15T19:05:08.223Z"},{"id":"q-2471","question":"KCNA security and governance: design a per-tenant envelope encryption scheme for KCNA payloads using a central KMS. Each tenant has a dedicated DEK wrapped by a tenant-specific KEK. Encrypt payloads at produce time and decrypt only at authorized consumers with per-tenant IAM. Support per-tenant key rotation with zero-downtime re-encryption, and maintain per-tenant audit trails and deletion rules that respect retention. How would you implement lifecycle, performance trade-offs, and backward-compatibility?","answer":"Implement envelope encryption: assign each tenant a unique DEK, wrapped by a tenant KEK from a central KMS. Encrypt payloads at producer side; decrypt only with tenant IAM. Support per-tenant key rota","explanation":"## Why This Is Asked\nReal-world KCNA deployments must protect data across tenants; this question probes envelope encryption design, key lifecycle, and how to audit and delete data without breaking consistency.\n\n## Key Concepts\n- Envelope encryption\n- Per-tenant KEK/DEK lifecycle\n- Key rotation with zero-downtime re-encryption\n- Auditability and tenant-scoped deletion\n\n## Code Example\n```javascript\n// Pseudocode for decrypting payload using tenant DEK\nconst wrappedDek = metadata.getTenantWrappedDek(tenantId);\nconst dek = kms.unwrapDek(tenantId, wrappedDek);\nconst plaintext = crypto.decrypt(payload, dek);\n```\n\n## Follow-up Questions\n- How would you test key rotation without service disruption?\n- How do you enforce least-privilege access per tenant across decryptors?","diagram":null,"difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Goldman Sachs","LinkedIn"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T19:34:50.218Z","createdAt":"2026-01-15T19:34:50.219Z"},{"id":"q-2510","question":"KCNA tamper-evident audit trails: design per-tenant verifiable logs for event streams using append-only shards and Merkle proofs, with per-batch signing and external audit proofs. Outline data structures, key rotation, canary rollout, rollback, and a test plan that proves tamper-resistance without leaking tenant data?","answer":"Design a per-tenant verifiable audit trail for KCNA events using append-only shards, per-tenant hashes, and Merkle proofs. Each batch is hashed, signed, and stored in a tamper-evident ledger; auditors","explanation":"## Why This Is Asked\nIn multi-tenant KCNA deployments, tamper-evident audit trails enable compliance, forensics, and governance without sacrificing performance.\n\n## Key Concepts\n- Append-only per-tenant logs and digest chaining\n- Merkle proofs for cross-checks and tamper evidence\n- Cryptographic signing and secure key rotation\n- Canary-driven rollout and safe rollback\n\n## Code Example\n```javascript\nfunction verifyBatch(batch, root) {\n  const digest = hash(batch);\n  const proof = getMerkleProof(batch.id);\n  return Merkle.verify(digest, proof, root);\n}\n```\n\n## Follow-up Questions\n- How to scale verification and audit dashboards?\n- How to handle key compromise and revocation?","diagram":null,"difficulty":"intermediate","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Cloudflare","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T20:53:35.476Z","createdAt":"2026-01-15T20:53:35.476Z"},{"id":"q-2565","question":"KCNA per-tenant data isolation with confidential payloads: in a single KCNA cluster serving multiple tenants, design a mechanism to store tenant-scoped payloads with per-tenant encryption keys, support cross-tenant analytics only if opted-in, ensure query isolation, key rotation, and audit trails. Describe API contracts, key management, and performance implications?","answer":"Design a comprehensive per-tenant data isolation model for KCNA that enables secure multi-tenancy within a single cluster. Implement envelope encryption using tenant-specific keys managed through an external KMS, enforce field-level redaction for cross-tenant analytics when opted-in, maintain strict query isolation through namespace-based access controls, support automated key rotation with zero-downtime migration, and provide tamper-evident audit trails for all data access operations.","explanation":"## Why This Is Asked\n\nThis question evaluates the ability to design sophisticated data isolation architectures within KCNA for enterprise multi-tenant scenarios, specifically addressing encryption lifecycle management, query isolation, and auditability requirements.\n\n## Key Concepts\n\n- Envelope encryption with per-tenant key management\n- KMS integration and automated key rotation\n- Namespace-based access control and query isolation\n- Field-level redaction for cross-tenant analytics\n- Comprehensive audit trails and tamper-evidence mechanisms\n\n## Code Example\n\n```javascript\n// Pseudo example demonstrating tenant-specific key lookup and decryption\nfunction decryptForTenant(tenantId, ciphertext) {\n  const tenantKey = kms.getTenantKey(tenantId);\n  return crypto.decryptWithEnvelope(tenantKey, ciphertext);\n}\n```\n\n## Follow-up Questions\n\n- How would you handle tenant key rotation with zero downtime?\n- What strategies would you implement for cross-tenant analytics performance?\n- How do you ensure audit trail integrity in a distributed environment?","diagram":null,"difficulty":"intermediate","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Microsoft","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T05:22:01.017Z","createdAt":"2026-01-15T22:54:27.086Z"},{"id":"q-2582","question":"In KCNA, design a per-tenant event-time processing layer that tolerates late events within a tenant-defined latency budget, computes per-tenant 5-minute tumbling window aggregations, and guarantees at-least-once delivery. Describe per-tenant watermarks, state partitioning, late-data handling, fault tolerance, and a test plan to validate SLA compliance?","answer":"The implementation leverages per-tenant keyed streams with independent tenant-specific watermarks, backed by a 5-minute tumbling window aggregator utilizing per-tenant RocksDB state stores. Late events arriving within the tenant-defined latency budget are merged into the current active window, while events exceeding the budget are routed to a dead-letter queue for audit and manual review. State is partitioned by tenant ID with periodic checkpointing to durable storage for fault tolerance, and at-least-once delivery semantics are achieved through idempotent processing and transactional state updates.","explanation":"## Why This Is Asked\n\nThis question evaluates expertise in per-tenant event-time semantics, SLA-bound late data handling, and scalable state management in KCNA under multi-tenant workloads.\n\n## Key Concepts\n\n- Event-time processing with tenant-specific watermarks\n- Per-tenant state stores and partitioning strategies\n- Late data handling within SLA constraints\n- Fault tolerance through checkpointing and recovery\n- At-least-once delivery guarantees\n\n## Code Example\n\n```javascript\n// Pseudocode: per-tenant watermark advancement and late event routing\nclass TenantWindowManager {\n  constructor() {\n    this.watermark = -Infinity;\n    this.windows = new Map();\n  }\n  \n  onEvent(tenantId, event) {\n    const isLate = event.timestamp < this.watermark;\n    if (isLate) {\n      this.routeToDeadLetterQueue(tenantId, event);\n    } else {\n      this.mergeIntoActiveWindow(tenantId, event);\n    }\n  }\n}\n```\n\n## Follow-up Questions\n\n- How would you handle backpressure when tenants have varying event volumes?\n- What strategies would you implement to optimize state store compaction?\n- How do you ensure watermark fairness across tenants with different latency budgets?","diagram":"flowchart TD\n  A[KCNA Ingest] --> B[Per-Tenant Window Manager]\n  B --> C[State Store: per-tenant]\n  B --> D[Late Data Route]\n  A --> E[Watermark Propagation]","difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Goldman Sachs","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T05:13:48.636Z","createdAt":"2026-01-15T23:41:57.674Z"},{"id":"q-2621","question":"In KCNA, design a per-tenant field-level access control and masking layer that operates in real time on streaming payloads for analytics, ensuring authorized tenants can decrypt unmasked fields while unauthorized tenants only see masked data, without breaking at-least-once semantics or replay safety. Explain data structures, policy evaluation, KMS key management, and testing with canaries?","answer":"Per-tenant masking layer with policy-as-code, envelope encryption per-tenant KEK from a KMS, and visibility tokens. Ingested events carry tenant + ACLs; sensitive fields are encrypted and re-wrapped f","explanation":"## Why This Is Asked\n\nReal-time masking with per-tenant controls is a practical, security-critical feature at scale; tests edge cases like key rotation and replay.\n\n## Key Concepts\n\n- Field-level access control\n- Policy-as-code\n- Envelope encryption and KEKs\n- Replay safety and exactly-once\n- Key rotation and auditing\n\n## Code Example\n\n```javascript\n// Pseudo-code for policy evaluation and masking\n```\n\n## Follow-up Questions\n\n- How would you test cross-tenant isolation with canaries?\n- How would you handle key revocation and rotation without service disruption?","diagram":"flowchart TD\n  A[Ingest Event] --> B[Policy Eval]\n  B --> C[Masking Layer]\n  C --> D[Masked Output]\n  B --> E[Decrypt Permissions]\n  E --> F[Authorized View] --> D","difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Apple"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T04:06:41.724Z","createdAt":"2026-01-16T04:06:41.726Z"},{"id":"q-2710","question":"KCNA per-tenant envelope encryption: design a CMK-backed scheme where each tenant's messages are encrypted at ingest with a tenant KEK wrapped by a KMS CMK, keys rotate monthly, and revocation triggers re-encryption with minimal downtime while preserving canary readers. Describe data model, API surface, rotation and revocation flows, and test strategy?","answer":"Use per-tenant KEKs in KMS, envelope-encrypt payloads with per-tenant DEKs; store DEK metadata (tenant_id, key_ver, cipher) in KCNA; rotate KEKs monthly by re-wrapping DEKs in place; on revocation, to","explanation":"## Why This Is Asked\n\nInterview context explanation.\n\n## Key Concepts\n\n- Envelope encryption, CMKs, KEKs, KMS integration\n- Key rotation, revocation, re-encryption strategy\n- Data model for keys, metadata, and auditability\n\n## Code Example\n\n```javascript\n// Pseudo API surface\nclass KCNAEnvelope {\n  constructor(tenantId, kekName, version) {}\n  encrypt(payload) {}\n  decrypt(encrypted) {}\n}\n```\n\n## Follow-up Questions\n\n- How would you test rotation impact on latency?\n- How do you audit key usage and detect leakage?","diagram":null,"difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T07:44:01.064Z","createdAt":"2026-01-16T07:44:01.064Z"},{"id":"q-2771","question":"KCNA privacy-preserving analytics: design a per-tenant analytics pipeline where tenants opt into server-side aggregation over their event streams without exposing raw data. Include architecture for data isolation, privacy budgets via differential privacy, a streaming topology (topics, shards, processors), and how you validate tamper-resistance while preserving privacy. Provide testing, rollback, and performance targets?","answer":"Design a per-tenant analytics flow in KCNA that returns only privacy-preserving aggregates (no raw events). Use tenant-scoped topics, ACLs, and per-tenant privacy budgets; implement a streaming DAG: I","explanation":"## Why This Is Asked\n\nLeverages real-world needs for privacy-preserving analytics in a multi-tenant KCNA deployment, including DP budgeting, auditability, and safe exposure of aggregates.\n\n## Key Concepts\n\n- Per-tenant isolation via topic prefixes and ACLs\n- Differential privacy budgeting and noise mechanisms\n- Streaming topology with processors and backpressure\n- Tamper-resistance via cryptographic proofs (hash chains, Merkle) and audit logs\n- Canary rollout and rollback strategies\n\n## Code Example\n\n```javascript\nfunction addLaplaceNoise(value, epsilon, sensitivity){\n  const rnd = Math.random() - 0.5;\n  const scale = sensitivity / epsilon;\n  // simple Laplace sample using inverse CDF\n  const noise = -scale * Math.sign(rnd) * Math.log(1 - 2 * Math.abs(rnd));\n  return value + noise;\n}\n```\n\n## Follow-up Questions\n\n- How would you test DP accuracy and privacy budget accounting in prod?\n- How do you handle tenants with evolving privacy requirements or opt-out scenarios?","diagram":"flowchart TD\n  Ingest --> TenantFilter\n  TenantFilter --> PartialAggregate\n  PartialAggregate --> DPNoise\n  DPNoise --> Publish","difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","PayPal","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T11:29:27.253Z","createdAt":"2026-01-16T11:29:27.254Z"},{"id":"q-2989","question":"KCNA cost-aware metering at scale: design a tenant-scoped metering system that bills on ingress, egress, and partition-hours, with per-tenant soft quotas, burst credits, and an offline audit log. Describe data models, sampling windows, aggregation, reconciliation, and how to enforce limits without dropping messages. Include API contracts, observability, and rollback/dispute handling; provide a high-level migration plan?","answer":"Implement per-tenant meters in a central ledger with atomic increments at gateway; aggregate 5-minute windows for ingress, egress, and partition-hours; use a per-tenant burst credit (token-bucket) and","explanation":"## Why This Is Asked\nThis question probes multi-tenant metering, accuracy, and fault-tolerant billing at scale.\n\n## Key Concepts\n- Per-tenant metering\n- Windowed aggregation\n- Soft quotas and burst credits\n- Reconciliation and audit logs\n- Dispute handling and rollback\n\n## Code Example\n```javascript\n// Pseudo: updateMeter for an event\nfunction updateMeter(tenantId, ingressBytes, egressBytes, partitions) {\n  const now = floorToWindow(Date.now());\n  meters[tenantId].ingress += ingressBytes;\n  meters[tenantId].egress += egressBytes;\n  meters[tenantId].partitions += partitions;\n  if (meters[tenantId].burst < tokens.current) tokens.consume(...);\n  // emit to central ledger\n}\n```\n\n## Follow-up Questions\n- How would you handle clock skew across data centers?\n- What are the failure modes and how would you test them?","diagram":"flowchart TD\n  A[Ingest] --> B[Metering] \n  B --> C[Aggregate windows] \n  C --> D[Billing shard] \n  D --> E[Audit log]","difficulty":"intermediate","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Google","Lyft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T20:32:16.221Z","createdAt":"2026-01-16T20:32:16.222Z"},{"id":"q-3047","question":"KCNA Audit Trail: Build a per-tenant, append-only audit log for KCNA actions (topic creation, partition changes, offset commits) that is tamper-evident and queryable without impacting throughput. Describe storage format, API surface, retention, and a minimal canary test that demonstrates append, read, and integrity checks?","answer":"Design a per-tenant audit log with append-only writes, per-tenant streams, and a signed MAC for tamper detection. Expose simple APIs: AuditLog.write(tenant, event) and AuditLog.query(tenant, since, limit). Use a storage format with sequential entries containing timestamp, event type, payload, and cryptographic signature. Implement retention policies with automatic cleanup of old entries while preserving integrity. Create a canary test that demonstrates append operations, query functionality, and integrity verification through signature validation.","explanation":"Why This Is Asked\n- Validates per-tenant auditable trails without impacting throughput\n- Ensures tamper evidence via signatures and secure storage\n- Checks ability to query across tenants with predictable retention\n\nKey Concepts\n- Append-only logs, tenant isolation, data integrity, retention strategy\n- Tamper detection using MACs or signatures, and secure storage backends\n- API design for write/read with minimal surface area and clear semantics\n\nCode Example\n```javascript\nclass AuditLog {\n  constructor(storage, signer) {\n    this.storage = storage; // per-tenant append-only store\n    this.signer = signer;   // cryptographic signing service\n  }\n\n  async write(tenant, event) {\n    const entry = {\n      timestamp: Date.now(),\n      tenant,\n      event,\n      signature: await this.signer.sign(event)\n    };\n    return await this.storage.append(tenant, entry);\n  }\n\n  async query(tenant, since, limit) {\n    const entries = await this.storage.read(tenant, since, limit);\n    return entries.filter(entry => \n      this.signer.verify(entry.event, entry.signature)\n    );\n  }\n}\n```","diagram":"flowchart TD\n  A[KCNA event] --> B[AuditLog.write]\n  B --> C[Persistent Storage]\n  C --> D[Query API]\n  D --> E[Audits per tenant]","difficulty":"beginner","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Cloudflare","Databricks"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T04:15:39.153Z","createdAt":"2026-01-16T22:41:05.318Z"},{"id":"q-3131","question":"KCNA Security: Per-tenant encryption at rest using envelope encryption in KCNA. Each tenant has a unique data encryption key (DEK) wrapped by a centralized master key; rotate DEKs monthly and re-encrypt in-flight messages during rotation? Describe key storage, access controls, rotation workflow, and a small canary test to validate encryption at rest and post-rotation decryption?","answer":"Implement per-tenant envelope encryption: store a DEK per tenant in a secure vault, wrap each DEK with a central KMS master key; encrypt KCNA payloads with AES-256-GCM; rotate DEKs monthly by re-wrapp","explanation":"## Why This Is Asked\n\nThis question probes practical crypto for multi-tenant KCNA: encryption at rest, per-tenant keys, rotation, and in-flight re-encryption.\n\n## Key Concepts\n\n- Envelope encryption\n- Per-tenant DEKs\n- KMS master key\n- Rotation workflow\n- Least privilege and auditing\n\n## Code Example\n\n```javascript\n// Pseudo-encryption flow for a tenant payload\nfunction encryptPayload(tenantId, payload, dekStore, kms) {\n  const dek = dekStore.getDEK(tenantId);\n  const wrapped = kms.wrapKey(dek);\n  const iv = crypto.randomBytes(12);\n  const cipher = crypto.createCipheriv('aes-256-gcm', dek, iv);\n  // ... encryption steps\n  return { ciphertext, dekWrapped: wrapped };\n}\n```\n\n## Follow-up Questions\n\n- How would you scale DEK storage and rotation across thousands of tenants?\n- What if rotation fails during processing; how ensure atomicity and replay safety?","diagram":"flowchart TD\n  A[Tenant] --> B[DEK Store]\n  B --> C[Encrypt Payload]\n  C --> D[KCNA Topic]\n  D --> E[Rotation Trigger]\n  E --> F[Re-encrypt In-Flight Messages]","difficulty":"beginner","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T04:12:34.491Z","createdAt":"2026-01-17T04:12:34.491Z"},{"id":"q-3252","question":"KCNA per-tenant real-time anomaly detection: design a streaming pipeline that flags tenants with anomalous event-rate patterns at ingest time, ensuring isolation, data locality, and minimal false positives. Outline architecture, per-tenant state, windowing strategy, privacy considerations, rollout plan, and testing?","answer":"Design a per-tenant streaming anomaly detector that runs at ingest: maintain a per-tenant sliding window (5 minutes) with lightweight stats (mean, stdev) to flag outliers; shard models by tenant-id to","explanation":"## Why This Is Asked\nTests ability to design real-time, multi-tenant analytics with privacy-friendly isolation and scalable state.\n\n## Key Concepts\n- Streaming windows, per-tenant state, thresholding\n- Data locality, backpressure, privacy preservation\n- Testing with synthetic tenants and drift detection\n\n## Code Example\n```javascript\n// Minimal skeleton for per-tenant detector\nclass TenantDetector {\n  constructor(windowMs) {\n    this.windowMs = windowMs\n    this.states = new Map()\n  }\n  ingest(tenantId, value, ts) {\n    // maintain per-tenant window and compute simple stats\n  }\n}\n```\n\n## Follow-up Questions\n- How would you handle non-stationary workloads and adapt thresholds over time?\n- How would you verify no cross-tenant data leakage across shards during scaling?","diagram":"flowchart TD\n  Ingest[Ingest KCNA events] --> State[Per-tenant State (sliding window)]\n  State --> Compute[Compute stats & detect]\n  Compute --> Alert[Emit alert / log provenance]\n  Alert --> Roll[Rollout & rollback]","difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T08:45:30.175Z","createdAt":"2026-01-17T08:45:30.175Z"},{"id":"q-3300","question":"KCNA tenancy revocation and data deletion: design a scalable policy to revoke a tenant's access within 2 minutes, gracefully quiesce in-flight events, switch downstream analytics to de-identified data, and preserve historical auditability. Outline data flow, API contracts, and test strategy, including canaries?","answer":"Implement a per-tenant revoke token and gateway feature flag to block new writes within 2 minutes, and route inflight events to a shadow path for draining. Apply field-level masking before analytics, ","explanation":"## Why This Is Asked\nThis question probes tenancy revocation, data privacy, and live-system safety under real-world SLAs.\n\n## Key Concepts\n- Per-tenant policy enforcement with low-latency revocation\n- In-flight event draining and shadow routing\n- Field masking for analytics without breaking schemas\n- Auditability, rollback, and canary-driven rollout\n\n## Code Example\n```yaml\npolicies:\n  revokeAtSeconds: 120\n  maskPII: true\n  audit: true\n```\n\n## Follow-up Questions\n- How would you test canary rollout at scale without impacting tenants?\n- What metrics indicate a safe rollback should occur?","diagram":"flowchart TD\n  A[Event Arrives] --> B{TenantRevoked?}\n  B -->|Yes| C[Block Writes & Redirect to Shadow]\n  B -->|No| D[Forward to KCNA]\n  C --> E[Mask/Transform & Persist Shadow]\n  D --> E\n  E --> F[Audit Trail Available]\n  F --> G[Canary Rollout Checks]","difficulty":"intermediate","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Hashicorp","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T10:38:12.430Z","createdAt":"2026-01-17T10:38:12.430Z"},{"id":"q-3323","question":"Design a per-tenant deduplication system for KCNA that guarantees exactly-once-like semantics for idempotent event processing across tenants, without cross-tenant data leakage. Propose event_id schema, fast per-tenant dedupe checks, eviction policy, tombstones, and failure handling; discuss producer/consumer changes and a practical test plan that demonstrates correctness and scalability?","answer":"Use a stable event_id composed of (tenant_id, stream_id, event_seq). Broker checks a per-tenant in-memory cache or Redis with a TTL (e.g., 24 hours). On miss, record and forward; on hit, drop. Enforce","explanation":"## Why This Is Asked\n\nAssess practical understanding of cross-tenant deduplication at KCNA scale, including latency considerations, late duplicates, and shard rebalancing impacts.\n\n## Key Concepts\n\n- Exactly-once-like semantics vs dedupe\n- Tenant isolation and TTL caches\n- Event_id design for idempotent processing\n- Test strategies for duplicates, latency, and re-sharding\n\n## Code Example\n\n```javascript\n// Pseudo dedupe cache interface\nclass DedupeStore {\n  async hasSeen(tenant, eventId) { /* check */ }\n  async record(tenant, eventId) { /* store tombstone */ }\n}\n```\n\n## Follow-up Questions\n\n- How would you handle eviction and memory pressure in a multi-tenant cluster?\n- How do you test correctness during rolling upgrades and shard rebalancing?","diagram":null,"difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","PayPal","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T11:31:25.100Z","createdAt":"2026-01-17T11:31:25.100Z"},{"id":"q-3350","question":"KCNA privacy-preserving analytics: design a server-side cross-tenant aggregation layer that lets dashboards derive insights from multi-tenant event streams without exposing raw data. Specify a differential privacy or secure-aggregation protocol, per-tenant access controls, data formats, auditable results, and a minimal canary test plan to verify privacy and accuracy under load?","answer":"Use a privacy-preserving aggregator that either applies a DP mechanism (input clipping, noise calibrated to epsilon) or uses secure sums with per-tenant keys and a shared DP budget. Return only DP-saf","explanation":"## Why This Is Asked\n\nThis explores privacy-preserving cross-tenant analytics, auditing, and key management in KCNA at scale, aligning with enterprise needs around data sharing without leakage.\n\n## Key Concepts\n\n- Differential privacy and secure aggregation\n- Per-tenant access controls and key rotation\n- Tamper-evident auditing of query results\n- API design for aggregated analytics\n- Performance considerations under multi-tenant load\n\n## Code Example\n\n```javascript\n// Demonstrative DP aggregation stub\nfunction clipAndNoisify(values, epsilon, sensitivity) {\n  const clipped = values.map(v => Math.max(-sensitivity, Math.min(sensitivity, v)));\n  const b = sensitivity / epsilon;\n  const noise = (Math.random() < 0.5 ? -1 : 1) * b * Math.log(1 - Math.random());\n  return clipped.reduce((a,b)=>a+b,0) + noise;\n}\n```\n\n## Follow-up Questions\n\n- How would you measure privacy budget consumption over time in a live dashboard?\n- What changes would you make to support streaming DP with backpressure and multi-tenant isolation?","diagram":"flowchart TD\n  A[KCNA event streams] --> B[Server-side Aggregation]\n  B --> C[Noise Injection / DP]\n  B --> D[Tenant ACLs]\n  C --> E[Aggregated Results]\n  D --> E\n  E --> F[Dashboards]\n  F --> G[Tamper-evident Audit]","difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Hashicorp","Hugging Face"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T13:06:34.396Z","createdAt":"2026-01-17T13:06:34.396Z"},{"id":"q-3419","question":"KCNA per-tenant data purge: design a per-tenant purge operation that deletes messages older than a given timestamp while preserving per-tenant offset semantics and avoiding consumer surprises. Describe API shape, tombstone strategy, offset handling, GC durability, and a minimal canary test that demonstrates purge, offset stability, and alerting?","answer":"Design a per-tenant purge API for KCNA: POST /kcna/purge with body {tenantId, topic, cutoffTs}. Purge marks messages older than cutoffTs as tombstones; offsets remain valid but readers skip tombstones","explanation":"## Why This Is Asked\nAddresses data lifecycle, privacy, and regulatory needs for multi-tenant KCNA without breaking offset guarantees.\n\n## Key Concepts\n- Per-tenant retention and purge granularity\n- Tombstone semantics and consumer view\n- Offset progression stability during purge\n- Durable GC with audit logs and alerts\n\n## Code Example\n```javascript\n// TypeScript sketch\ntype PurgeRequest = { tenantId: string; topic: string; cutoffTs: number }\nasync function purgeTenantTopic(req: PurgeRequest): Promise<void> {\n  // validate inputs\n  // scan messages older than cutoff and emit tombstones\n  // ensure idempotence; commit tombstones in a transaction\n  // log purge event for auditing\n}\n```\n\n## Follow-up Questions\n- How would you test concurrent purges and in-flight offset commits?\n- How do tombstones interact with replication and cross-region copies?","diagram":"flowchart TD\n  A[Client sends purge request] --> B[Purge service validates]\n  B --> C{Oldest offset guard}\n  C -->|Yes| D[Tombstone records written]\n  C -->|No| E[Abort purge]\n  D --> F[GC marks tombstones consumable]\n  F --> G[Consumers see tombstones as deletes]","difficulty":"beginner","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T15:35:59.186Z","createdAt":"2026-01-17T15:35:59.187Z"},{"id":"q-3511","question":"In KCNA, tenants share a cluster. Propose a per-tenant envelope encryption scheme for streams where each tenant's messages are encrypted at rest with a tenant-specific key managed by a central KMS. Describe key rotation, revocation, legacy data handling, latency impact, and a concrete test plan. How would you implement and verify it?","answer":"Use per-tenant envelope encryption: each tenant has a CMK in a centralized KMS; KCNA encrypts payloads with a tenant-specific DEK that is wrapped by the CMK. Rotate DEKs periodically and version ciphe","explanation":"## Why This Is Asked\nThis probes secure multi-tenant data protection in KCNA, focusing on scalable key management, auditability, and performance impact.\n\n## Key Concepts\n- Envelope encryption, per-tenant CMK, DEK lifecycle\n- Key rotation, revocation, legacy data handling\n- Performance trade-offs: latency, CPU, GC impact\n\n## Code Example\n```javascript\n// Pseudo-code: envelope encryption per tenant\nconst dek = getOrCreateDEK(tenantId); // tenant DEK\nconst ciphertext = aesGcmEncrypt(plaintext, dek);\nconst wrappedDEK = kms.wrapKey(dek, cmkForTenant(tenantId));\nstore(ciphertext, {wrappedDEK, dekVersion: dek.version});\n```\n\n## Follow-up Questions\n- How would you test key rollover without downtime?\n- How do you ensure auditability of key usage across tenants?","diagram":null,"difficulty":"intermediate","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T19:27:45.843Z","createdAt":"2026-01-17T19:27:45.843Z"},{"id":"q-3540","question":"KCNA cross-region replication: design a protocol that preserves per-tenant data sovereignty across two regions, ensuring at-least-once delivery and per-tenant ordering during failover. Describe region-local shards, per-tenant keys for encryption, key rotation, and tombstone GC, plus a DR test plan with progressive canaries and rollback. How would you ensure correctness and observability?","answer":"Propose per-tenant streams written to region-local shards with tenant-scoped sequence numbers. Use async replication with idempotent writes and per-tenant isolation, encrypted at rest with per-tenant ","explanation":"## Why This Is Asked\n\nAssess cross-region replication, tenant isolation, security, and DR readiness; emphasis on verifiable correctness and observability across regions.\n\n## Key Concepts\n\n- Multi-region replication with per-tenant isolation\n- Ordering guarantees and at-least-once delivery\n- Encryption at rest with per-tenant keys and rotation\n- Tombstone GC and cross-region proofs\n- Canary-based DR testing and rollback plans\n\n## Code Example\n\n```javascript\n// Pseudo-code: per-tenant write path\nfunction publish(tenantId, event) {\n  const shard = getShardForTenant(tenantId);\n  shard.append({ tenantId, event, seq: shard.nextSeq(tenantId) });\n  replicateToRegionB(tenantId, event, shard.latestSeq(tenantId));\n}\n```\n\n## Follow-up Questions\n\n- How would you validate ordering guarantees during DR failover?\n- How would you monitor cross-region latency and data sovereignty compliance?\n","diagram":null,"difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Cloudflare","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T20:39:28.658Z","createdAt":"2026-01-17T20:39:28.659Z"},{"id":"q-3621","question":"KCNA Cross-Region Replication: In a globally deployed KCNA, design cross-region replication that guarantees per-tenant event order and cross-region consistency, while producing end-to-end verifiable batch proofs across regions. Include shard ownership, canaries, key rotation, rollback, and a DR test plan?","answer":"Cross-Region KCNA replication: assign per-tenant shards with local ordering, replicate batches to a DR region. Each batch carries a per-tenant Merkle root and cross-region signature; DR reconciliation validates proofs before advancing offsets.","explanation":"## Why This Is Asked\nThis question probes ability to design robust cross-region replication with verifiable proofs and DR considerations.\n\n## Key Concepts\n- Per-tenant shard ownership and ordering\n- End-to-end verifiable proofs (Merkle roots) and signatures\n- Cross-region reconciliation and offset advancement\n- Secure key management and rotation\n- Canary-based DR testing and rollback\n\n## Code Example\n```javascript\n// Pseudo: batch payload structure\n{ tenantId, batchId, merkleRoot, payload, signature }\n```\n\n## Follow-up Questions\n- How would proof validation scale across regions?\n- What are latency implications for cross-region consistency?\n- How would you handle network partitions between regions?","diagram":null,"difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Snap","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T05:13:51.607Z","createdAt":"2026-01-17T23:42:33.072Z"},{"id":"q-3670","question":"KCNA: Design a tenant-scoped encryption model for KCNA events where each tenant's data is encrypted at rest with a per-tenant KEK sourced from a KMS, and payloads are envelope-encrypted with per-tenant DEKs that are rotated without downtime. Explain provisioning, rotation, revocation, audit proofs, and isolation guarantees under load. Include performance trade-offs and a concrete failure scenario?","answer":"Outline a per-tenant envelope encryption model: provision per-tenant KEKs from a KMS, generate per-tenant DEKs to encrypt payloads, wrap DEKs with KEKs, and rotate KEKs with zero-downtime rewrap. Defi","explanation":"## Why This Is Asked\n\nTo assess practical integration of KMS-based encryption, tenant isolation, and auditability in KCNA at scale, including key rotation, revocation, and observability under load.\n\n## Key Concepts\n\n- Per-tenant KEK\n- Envelope encryption\n- Zero-downtime key rotation\n- Tenant isolation proofs\n- Auditability and proofs\n\n## Code Example\n\n```javascript\n// Pseudo: fetch KEK, generate data key, encrypt payload, store ciphertext and wrapped key\nasync function sealPayload(tenantId, payload, kms){\n  const KEK = await kms.getKEK(tenantId);\n  const DEK = crypto.getRandomValues(new Uint8Array(32));\n  const wrappedDEK = await wrapKeyWithKEK(DEK, KEK);\n  const ciphertext = await encrypt(payload, DEK);\n  return { ciphertext, wrappedDEK };\n}\n```\n\n## Follow-up Questions\n\n- How to rotate DEKs per-tenant without re-encrypting old data?\n- How to detect/mitigate KEK compromise and revoke access quickly?\n","diagram":"flowchart TD\n  A[Tenant] --> B[KCNA Ingest Path]\n  B --> C[DEK Wrap via KEK from KMS]\n  C --> D[Encrypt Payload]\n  D --> E[Store Ciphertext]\n  E --> F[Audit Proofs]","difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Netflix","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T04:20:33.724Z","createdAt":"2026-01-18T04:20:33.725Z"},{"id":"q-3691","question":"KCNA Security: Design a tenant-scoped envelope-encryption framework for KCNA where each event payload is encrypted with a per-tenant data key (DEK) wrapped by a centralized KMS/HSM, enabling hot key rotation, forward secrecy, and revocation. Outline the key hierarchy, data structures, per-tenant access checks, audit proofs, and a rollout and rollback strategy with performance considerations?","answer":"Use a two-layer envelope: each tenant has a DEK that encrypts payloads; a tenant KEK wraps the DEK in a KMS/HSM. Rotate DEKs with epoch tags and purge old envelopes; revoke access by re-wrapping with ","explanation":"## Why This Is Asked\nThis question probes KCNA security design, tenant isolation, and practical key management choices under constraints like rotation and revocation.\n\n## Key Concepts\n- Envelope encryption with per-tenant DEKs\n- Hierarchical KMS/HSM wrapping and rotation\n- Per-tenant access checks and audit proofs\n- Rollout/rollback with minimal downtime and observability\n\n## Code Example\n```javascript\n// Pseudocode for envelope encryption\nfunction encryptEvent(event, tenantId, kms, store) {\n  const dek = store.getDEK(tenantId);\n  const ciphertext = aesGcmEncrypt(event.payload, dek);\n  return { ciphertext, headers: { tenantId, keyId: dek.id, epoch: dek.epoch } };\n}\nfunction decryptEvent(record, store) {\n  const dek = store.getDEK(record.headers.tenantId, record.headers.keyId);\n  return aesGcmDecrypt(record.ciphertext, dek);\n}\n```\n\n## Follow-up Questions\n- How would you validate rotation can be performed with zero downtime?\n- How would you handle compromised KEKs and re-encryption scope?","diagram":"flowchart TD\n  A[Ingest Event] --> B[Fetch Tenant KEK]\n  B --> C[Unwrap DEK, Decrypt Payload]\n  C --> D[Re-encrypt for KCNA Store]\n  D --> E[Store with Headers (tenantId, keyId, epoch)]","difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Microsoft","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T05:40:07.313Z","createdAt":"2026-01-18T05:40:07.313Z"},{"id":"q-3758","question":"KCNA per-tenant encryption & key management: design a CMEK-backed model for KCNA where each tenant's event payload is encrypted at ingestion with per-tenant keys managed by an external KMS. How would you implement key rotation without downtime, revocation, and auditable proofs (Merkle-based) without exposing payloads? Include data structures mapping tenants to keys, API surfaces, and a practical test plan?","answer":"Use envelope encryption with per-tenant data keys (DEKs) wrapped by a CMEK in an external KMS. Ingest fetches the DEK, encrypts the payload, and stores ciphertext alongside the DEK ID. Rotate by rolli","explanation":"## Why This Is Asked\nDemonstrates deep understanding of per-tenant data isolation, external KMS integration, and verifiable auditing in a streaming system. Tests handling of key rotation, revocation, and tamper-evidence without leaking data.\n\n## Key Concepts\n- CMEK-based envelope encryption with per-tenant DEKs\n- External KMS integration and key-wrapping\n- Zero-downtime rotation and revocation workflows\n- Tamper-evident auditing using Merkle proofs\n- Tenant-to-key metadata, APIs, and test strategies\n\n## Code Example\n```javascript\n// Pseudo: obtain per-tenant DEK, encrypt, store ciphertext with key reference\nasync function ingestEvent(tenantId, plaintext) {\n  const dek = await kms.getTenantDEK(tenantId);\n  const iv = crypto.randomBytes(12);\n  const ciphertext = crypto.aesGCM(plaintext, dek.key, iv);\n  await storeCiphertext(tenantId, ciphertext, iv, dek.id);\n}\n```\n\n## Follow-up Questions\n- How would you coordinate DEK rotation with in-flight data?\n- How would you prove key usage for auditors without exposing payloads?","diagram":"flowchart TD\n  Ingest(Ingest KCNA Event) --> Encrypt(Encrypt with Tenant DEK)\n  Encrypt --> Store(Store Ciphertext + KeyID)\n  Store --> Audit(Audit proof with Merkle root)\n  Audit --> Rotate(Rotate keys without downtime)\n  Rotate --> Ingest","difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T08:40:55.410Z","createdAt":"2026-01-18T08:40:55.411Z"},{"id":"q-3790","question":"KCNA exactly-once semantics at scale: design a per-tenant streaming pipeline in KCNA that guarantees exactly-once delivery from producer to consumer despite retries and partition rebalances. Propose a per-tenant commit-log with transactional writes, a monotonic sequence number, and a bounded dedup cache. How do you handle failure modes, rebalances, and tombstone processing? Include API contracts and a testing plan with canaries?","answer":"In practice, I’d implement per-tenant sequence numbers and a per-tenant commit log. Producers attach an idempotence key and a monotonic offset; KCNA uses a two-phase commit to write to the per-tenant ","explanation":"## Why This Is Asked\nThis question probes end-to-end exactly-once semantics in a multi-tenant streaming system, focusing on per-tenant isolation, commit logging, and failure scenarios.\n\n## Key Concepts\n- Exactly-once delivery, idempotent producers, per-tenant commit logs\n- Two-phase commit, offsets, tombstones, dedupe caching\n- Failure modes: broker crash, partition rebalance, network retry\n\n## Code Example\n\n```javascript\n// Pseudo code: producer writes to per-tenant commit log with idempotence key\n```\n\n## Follow-up Questions\n- How would you validate OC tests with canaries across tenants? \n- How would you measure latency budgets and saturation during rebalance?\n","diagram":"flowchart TD\n  A[Producer] --> B[PerTenantCommitLog]\n  B --> C[OffsetIndex]\n  C --> D[Consumer]\n  D --> E[DedupeCache]","difficulty":"intermediate","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Hashicorp"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T09:38:48.222Z","createdAt":"2026-01-18T09:38:48.222Z"},{"id":"q-3861","question":"KCNA policy-driven field-level de-identification: design an ingestion-time pipeline that applies per-tenant de-identification rules (tokenize or redact PII fields) while preserving deterministic tokens for analytics, and supports on-demand re-identification with strict authorization. Explain policy model, evaluation order, key management, performance bounds, and a test plan for leak-avoidance and auditability?","answer":"Implement a per-tenant policy engine that runs at ingestion, applying field-level rules: tokenize PII deterministically for analytics, redact non-analytics fields, and preserve a reversible re-identif","explanation":"## Why This Is Asked\n\nTests a candidate's ability to design a privacy-preserving, per-tenant data path with strong auditability and revocation controls in KCNA. It requires integrating policy evaluation, key management, and immutable logs, plus performance considerations for real-time ingestion.\n\n## Key Concepts\n\n- Policy-driven de-identification\n- Deterministic tokenization with per-tenant KEK\n- Ingestion-time enforcement and data isolation\n- Auditability with Merkle proofs and immutable logs\n- Key rotation and re-identification safeguards\n\n## Code Example\n\n```javascript\n// Pseudo-policy evaluation skeleton\nfunction applyPolicy(record, policy) {\n  // tokenize or redact fields based on policy\n  // return transformed record\n}\n```\n\n## Follow-up Questions\n\n- How to audit policy changes across tenants without leaking data?\n- How to simulate leakage scenarios and measure protection","diagram":"flowchart TD\n  A[Policy Engine] --> B[Ingestion Pipeline]\n  B --> C[Event Store KCNA]\n  A --> D[Key Management]\n  B --> E[Audit Trail]\n  E --> F[External Audit]","difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Citadel","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T13:04:24.489Z","createdAt":"2026-01-18T13:04:24.489Z"},{"id":"q-3965","question":"You're building a beginner KCNA-based event bus for a multi-tenant chat/telemetry platform used by Slack-like teams. Each tenant has isolated topics. Design a per-tenant, rate-limited KCNA consumer that preserves per-tenant offsets, guarantees at-least-once delivery, and does not block healthy tenants when a downstream service lags. Describe API surface, offset persistence, fault handling, and a minimal test harness to simulate a slow downstream while other tenants continue?","answer":"Design a per-tenant KCNA consumer with topic isolation, per-tenant rate limiting (token-bucket), and per-tenant offsets persisted durably. Implement at-least-once processing with idempotent handlers a","explanation":"Why This Is Asked\n\nTests practical multi-tenant isolation, per-tenant offset management, and resilience in KCNA with simple backpressure and testing.\n\nKey Concepts\n\n- Per-tenant isolation and routing\n- Token-bucket rate limiting per tenant\n- Durable per-tenant offsets and recovery\n- Idempotent processing with robust retries\n- Lightweight test harness simulating downstream lag\n\nCode Example\n\n```javascript\n// Pseudo-code: per-tenant rate limiter and offset store\nclass TenantState {\n  constructor(tenant, rate) {\n    this.tenant = tenant;\n    this.offsets = new Map(); // topic -> offset\n    this.limiter = new TokenBucket(rate); // per-tenant rate\n  }\n  canConsume() { return this.limiter.tryConsume(this.tenant); }\n  commitOffset(topic, offset) { this.offsets.set(topic, offset); }\n}\nclass TokenBucket {\n  constructor(rate) {\n    this.rate = rate; this.tokens = rate; this.last = Date.now();\n  }\n  tryConsume() {\n    const now = Date.now();\n    const elapsed = (now - this.last) / 1000;\n    this.tokens = Math.min(this.rate, this.tokens + elapsed * this.rate);\n    this.last = now;\n    if (this.tokens >= 1) { this.tokens -= 1; return true; }\n    return false;\n  }\n}\n```\n\nFollow-up Questions\n\n- How would you persist offsets across restarts and recover after crash?\n- How would you scale the rate limiter across many tenants?\n- What metrics would you collect to detect backlog per tenant?","diagram":null,"difficulty":"beginner","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T17:35:38.349Z","createdAt":"2026-01-18T17:35:38.350Z"},{"id":"q-4080","question":"In KCNA, design a privacy-preserving analytics layer that lets tenants run cross-tenant aggregates (e.g., events per hour) without enabling reconstruction of any single tenant's data, using differential privacy and optional zero-knowledge proofs; specify the data-plane, privacy budgets, DP noise scaling, auditability, and how you'd validate resilience against collusion and timing attacks?","answer":"Implement per-tenant differential privacy: cap counts, apply Gaussian noise with a per-tenant epsilon, and publish only bucketed totals. Keep raw streams inert; use a separation of duties for noise generation and aggregation, with zero-knowledge proofs for auditability.","explanation":"## Why This Is Asked\nThis question probes privacy-preserving data analytics in KCNA at multi-tenant scale, combining differential privacy with optional zero-knowledge proofs, testing for collusion risks and system-level tradeoffs.\n\n## Key Concepts\n- Differential privacy per tenant with clamped counts\n- Gaussian noise scaling and budget accounting\n- Zero-knowledge proofs for auditability\n- Collusion resistance and timing-attack considerations\n- Observability and canary testing\n\n## Code Example\n```javascript\nfunction addGaussianNoise(value, sigma) {\n  const u1 = Math.random();\n  const u2 = Math.random();\n  const z0 = Math.sqrt(-2 * Math.log(u1)) * Math.cos(2 * Math.PI * u2);\n  return value + z0 * sigma;\n}\n```","diagram":"flowchart TD\n  A[Ingest KCNA event] --> B[Aggregate per hour by tenant]\n  B --> C[Apply DP noise per tenant]\n  C --> D[Publish per-tenant totals]\n  D --> E[ZK proof attest budget]\n  E --> F[Audit log]","difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Scale Ai","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T05:51:29.401Z","createdAt":"2026-01-18T22:51:07.305Z"},{"id":"q-4122","question":"KCNA: Verifiable cross-tenant analytics API: design a per-tenant aggregation endpoint (e.g., total events, average payload size) that returns results with a cryptographic proof showing only data from the requesting tenant was included and no double-counting occurred. Outline data structures, proof generation/verification, key rotation, and a regression test plan with security properties?","answer":"Propose a per-tenant analytics endpoint that computes aggregates (sums, counts, percentiles) over a tenant's events, returning a verifiable proof that only that tenant's data contributed. Use per-tenant Merkle proofs with tenant-specific signing keys, implement key rotation with versioned key IDs, and ensure batch-level isolation guarantees.","explanation":"## Why This Is Asked\n\nTests ability to design cryptographic verifiability in multi-tenant analytics, ensuring data isolation and auditable results at scale.\n\n## Key Concepts\n\n- Per-tenant data isolation in a shared KCNA\n- Verifiable analytics using Merkle proofs over batches\n- Tenant-key based signing and rotation\n- End-to-end latency and auditability constraints\n- Regression tests and security properties verification\n\n## Code Example\n\n```javascript\nfunction verifyBatchProof(batchAggregate, proof, root) {\n  // verify that batchAggregate is included in root using proof\n  // returns boolean\n}\n```","diagram":null,"difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","Robinhood","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T05:07:54.273Z","createdAt":"2026-01-19T02:55:20.613Z"},{"id":"q-4339","question":"KCNA Data Locality & Regional Pinning: In a multi-tenant KCNA deployment, design a policy to pin each tenant's event stream to a specific region to minimize latency, while allowing automatic re-pinning during regional outages. Detail data structures, decision criteria, failover workflow, and testing strategy for correctness under burst load?","answer":"Pin each tenant's stream to a designated region via a durable per-tenant region map in a fast config store. Route producers/consumers accordingly; on regional outage, re-pin to a backup region with gr","explanation":"## Why This Is Asked\nEvaluates ability to design low-latency, fault-tolerant regional routing with per-tenant isolation in KCNA.\n\n## Key Concepts\n- Data locality and region pinning\n- Per-tenant routing tables\n- Fast failover and graceful migration\n- Observability and canary testing\n\n## Code Example\n```javascript\n// Minimal config example\ntype TenantPin = { tenantId: string; region: string; lastUpdated: number };\nconst pins: Map<string, string> = new Map(); // tenantId -> region\n```\n\n## Follow-up Questions\n- How would you handle tenants that require multi-region active-active streaming?\n- How would you test drift between desired and actual region pins during outages?","diagram":"flowchart TD\n  A[Tenant] --> B[Region Pin]\n  B --> C[Producer/Consumer routing]\n  C --> D{Outage?}\n  D -->|Yes| E[Re-pin to backup region]\n  E --> F[Graceful drain & dedupe]\n","difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Discord","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T14:48:38.641Z","createdAt":"2026-01-19T14:48:38.642Z"},{"id":"q-4371","question":"KCNA Burst Guard: For a beginner KCNA producer scenario, design a per-topic token-bucket rate limiter that caps publish to 100 messages/sec with a burst capacity of 200, preserving per-topic order, and deferring excess messages. Describe the API surface, token accounting, backpressure strategy, and a minimal test plan simulating a 2x burst?","answer":"Use a per-topic token bucket: capacity 200 tokens, refill 100 tokens/sec. API: publish(topic, msg) returns a promise that resolves when accepted or queued; each topic maintains a small in-memory queue","explanation":"## Why This Is Asked\nThis question probes practical producer-side rate limiting, per-topic isolation, and basic backpressure, all at a beginner level but with real constraints.\n\n## Key Concepts\n- Per-topic state and ordering guarantees\n- Token bucket rate limiting and refill logic\n- Backpressure strategies: accept/queue vs reject with retry\n- Minimal test plan to validate burst handling and latency\n\n## Code Example\n```javascript\nclass TokenBucket {\n  constructor(capacity, rate) {\n    this.capacity = capacity;\n    this.rate = rate; // tokens per second\n    this.tokens = capacity;\n    this.last = Date.now();\n  }\n  tryRemove(n = 1) {\n    const now = Date.now();\n    const elapsed = (now - this.last) / 1000;\n    this.tokens = Math.min(this.capacity, this.tokens + elapsed * this.rate);\n    this.last = now;\n    if (this.tokens >= n) { this.tokens -= n; return true; }\n    return false;\n  }\n}\n```\n\n## Follow-up Questions\n- How would you extend this to fairly share tokens among many topics?\n- How would you observe and alert on burst spikes and depletion across topics?","diagram":null,"difficulty":"beginner","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","LinkedIn","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T16:03:26.558Z","createdAt":"2026-01-19T16:03:26.559Z"},{"id":"q-4422","question":"KCNA policy-driven access: design a per-tenant data-access policy system that enforces data locality, minimizes cross-tenant leakage during ingestion, storage, and query. Provide a concrete design for label-based access (RBAC/ABAC), policy evaluation points (ingest, index, query planner), and auditability with tamper-evident logs. Include API surfaces, data structures, and a testing plan with rollback and canary deployment?","answer":"In KCNA, build tenant-scoped policy store (YAML) with labels and actions. Enforce at ingestion by tagging records with tenant ID and policy-compliant classifiers; at storage via per-tenant namespaces ","explanation":"## Why This Is Asked\nEvaluates ability to design end-to-end policy enforcement in KCNA with strong isolation, privacy, and auditability under multi-tenant constraints, plus how to test and roll out safely.\n\n## Key Concepts\n- Tenant-scoped policy store and versioning\n- Ingest-time tagging and namespace isolation\n- Per-tenant audit trails and tamper resistance\n- Query planner enforcement and minimum latency trade-offs\n\n## Code Example\n```javascript\n// Pseudo policy example: allow read only if tenant matches and row-level ACL passes\nfunction policyCheck(tenantId, row) {\n  return row.tenantId === tenantId && row.acl.includes(tenantId);\n}\n```\n\n## Follow-up Questions\n- How would you measure policy evaluation latency at ingest and query time?\n- How would you handle policy revocation without delaying in-flight queries?","diagram":"flowchart TD\n  Ingest[Ingest] --> PolicyEval[Policy Eval]\n  PolicyEval --> Storage[Storage Layer]\n  PolicyEval --> QueryPlanner[Query Planner]\n  Storage --> Audit[Audit Trail]\n  QueryPlanner --> Audit","difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","DoorDash","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T18:42:55.660Z","createdAt":"2026-01-19T18:42:55.660Z"},{"id":"q-4550","question":"KCNA security and access control: design a per-tenant ACL model for KCNA where tenants define topic-level read/write permissions, with short-lived credentials issued by an authorization service. Explain how to enforce at gateway and broker, handle key rotation, token revocation, and audit logging, plus a test plan simulating misissued credentials and revocation delays?","answer":"Propose a per-tenant ACL scheme using topic-level grants, JWT-based tokens from an auth service, and short-lived refresh tokens. Enforce at gateway via per-tenant policy middleware and at broker with ","explanation":"## Why This Is Asked\n\nTests knowledge of per-tenant security, ACLs, and token lifecycles in KCNA. Assesses gateway/broker enforcement, rotation and revocation strategies, and robust auditing under multi-tenant pressure. Calls for practical test plans and rollback considerations.\n\n## Key Concepts\n\n- Per-tenant ACLs with topic-level granularity\n- JWT/OAuth tokens and short-lived credentials\n- Gateway and broker ACL enforcement and performance impact\n- Rotation, revocation, and audit logging\n- Observability and test strategies with edge cases\n\n## Code Example\n\n```javascript\n// Implementation snippet: token + ACL check\nfunction authorize(token, topic, operation) {\n  const claims = decodeJWT(token);\n  if (Date.now() >= claims.exp) return false;\n  if (!claims.tenants.includes(claims.tenant)) return false;\n  const acl = ACL_STORE[claims.tenant]?.[topic];\n  if (!acl) return false;\n  return acl.includes(operation);\n}\n```\n\n## Follow-up Questions\n\n- How would you model ACL changes without downtime?\n- How would you test revocation in a large-scale tenant set?\n","diagram":null,"difficulty":"intermediate","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T23:39:32.325Z","createdAt":"2026-01-19T23:39:32.325Z"},{"id":"q-4610","question":"KCNA: Design a per-tenant real-time aggregation layer that joins events across multiple KCNA namespaces into a tenant-scoped view, delivering low-latency dashboards while guaranteeing isolation and at-least-once delivery. Describe dataflow, partitioning, fault tolerance, security (ACLs, encryption), and a test plan with canaries?","answer":"Build a per-tenant materialized view engine that consumes from multiple KCNA namespaces, runs windowed joins, and writes to a tenant-scoped view store. Route data by tenant-id, enforce ACLs, and isola","explanation":"## Why This Is Asked\nWhy a real-world cross-namespace aggregation is hard and valuable.\n\n## Key Concepts\n- Per-tenant isolation via tenant-id routing and ACLs\n- Cross-namespace join semantics with windowing\n- Fault tolerance, backpressure, and state TTL\n- Idempotent upserts and deduplication for at-least-once\n- Observability and canary rollout\n\n## Code Example\n```javascript\n// Pseudo: dedupe by (tenantId, eventId)\nfunction dedupe(events) {\n  const seen = new Set();\n  const result = [];\n  for (const e of events) {\n    const key = `${e.tenantId}:${e.eventId}`;\n    if (!seen.has(key)) {\n      seen.add(key);\n      result.push(e);\n    }\n  }\n  return result;\n}\n```\n\n## Follow-up Questions\n- How would you test cross-tenant isolation violations in CI/CD?\n- How would you handle schema evolution for the view while preserving backwards compatibility?","diagram":"flowchart TD\n  A[KCNA Namespace A] --> B[Joiner]\n  C[KCNA Namespace B] --> B\n  B --> D[Tenant View Store]\n  D --> E[Dashboard Publisher]","difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Meta","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T04:21:15.454Z","createdAt":"2026-01-20T04:21:15.454Z"},{"id":"q-4717","question":"For a multi-tenant KCNA-based event bus used by teams at Citadel and Lyft, design a tenant-scoped observability layer that collects metrics, traces, and logs per tenant while preventing data leakage; include per-tenant sampling, redaction rules, and privacy controls. Explain data models, storage, dashboards, and canary-validation plan?","answer":"Per-tenant observability pipeline: attach tenant_id to every span/metric/log, route to a tenant-scoped sink, and redact sensitive fields in logs. Apply per-tenant sampling rates, bounded cardinality, ","explanation":"## Why This Is Asked\nDesigning isolated, privacy-preserving observability for multi-tenant KCNA ensures debugging without data leakage. It also highlights per-tenant SLAs and operational controls.\n\n## Key Concepts\n- Tenant tagging on all telemetry; privacy controls; per-tenant dashboards\n- Sampling and cardinality controls to prevent explosion\n- Canary-based validation for isolation and latency\n- Observability storage and RBAC permissions\n\n## Code Example\n```javascript\n// pseudocode: add tenant_id to spans and redact PII in logs\ntelemetry.span({name:'event', tenant_id, attributes:{...}, redactPII:true})\n```\n\n## Follow-up Questions\n- How would you enforce per-tenant quotas on telemetry ingestion?\n- How would you detect and remediate cross-tenant leakage in production?","diagram":null,"difficulty":"intermediate","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Lyft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T09:46:16.956Z","createdAt":"2026-01-20T09:46:16.956Z"},{"id":"q-4744","question":"KCNA access control and data isolation: design a per-tenant access policy for a shared KCNA cluster. Propose a token-based scheme with short-lived tokens containing tenant_id and per-topic permissions; enforce via an authorization service that validates signatures against a rotating KMS keyset; implement per-tenant revocation and audit logs; describe how you test isolation under bursty traffic and simulated token compromise. Include API signatures, config knobs, and a concrete test plan?","answer":"Implement per-tenant ACLs at the broker with a central RBAC model. Issue short-lived JWTs carrying tenant_id and per-topic permissions; validate against a rotating KMS-backed keyset. Support per-tenan","explanation":"Why This Is Asked\n- Rationale: Isolate tenants in shared KCNA while maintaining performance and security.\n\nKey Concepts\n- Token-based per-tenant authorization; rotating KMS keys; revocation lists; tamper-evident audit logs; cross-tenant breach handling.\n\nCode Example\n```javascript\n// Pseudo auth check\nfunction verifyToken(token){\n  const payload = jwt.verify(token, getCurrentPublicKey());\n  return { tenantId: payload.tenant_id, perms: payload.perms };\n}\n```\n\nFollow-up Questions\n- How would you handle key rotation without new tokens invalidating? \n- How would you simulate token theft in production tests? ","diagram":"flowchart TD\n  Client[Client App] --> Authz[Authorization Service]\n  Authz --> KCNA[KCNA Broker]\n  KCNA --> Audit[Audit Log]\n  Authz -- revocation --> RevStore[Revocation Store]","difficulty":"intermediate","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","Robinhood","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T10:42:12.115Z","createdAt":"2026-01-20T10:42:12.115Z"},{"id":"q-4814","question":"KCNA cross-tenant exactly-once delivery: design an end-to-end exactly-once mode per tenant across producers, topics, and rebalance events in a multi-tenant KCNA cluster. Propose a dedup scheme (tenant_id + message_id), transactional offsets to downstream stores, and per-tenant transaction boundaries. How would you test robustness under producer failures, rebalance, and tenant migrations?","answer":"Implement per-tenant dedup keys (tenant_id + message_id) stored in a fast in-memory cache with a durable backing store. Use idempotent downstream writers and transactional offsets so commits are atomi","explanation":"## Why This Is Asked\n\nTests ability to design strong delivery guarantees in multi-tenant KCNA, including dedup, offsets, and isolation during rebalances.\n\n## Key Concepts\n\n- Exactly-once semantics across tenants and rebalance events\n- Deduplication strategy using tenant_id and message_id\n- Atomic offsets and transactional writes to downstream systems\n- Tenant isolation boundaries during rebalances and migrations\n\n## Code Example\n\n```javascript\n// Pseudo dedup guard during processing\nif (lastSeen[tenant][topic] < messageId) {\n  writeDownstream();\n  lastSeen[tenant][topic] = messageId;\n  commitOffset();\n}\n```\n\n## Follow-up Questions\n\n- How would you scale the dedup store across regions?\n- What are failure modes to test and how would you detect them early?\n","diagram":"flowchart TD\n  A[Producer] --> B[KCNA Topic]\n  B --> C[Consumer]\n  C --> D[Downstream Store]\n  E[ rebalance ] --> F[ Offset Guard ]","difficulty":"intermediate","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Plaid","Robinhood","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T14:45:23.636Z","createdAt":"2026-01-20T14:45:23.637Z"},{"id":"q-4947","question":"KCNA regional data locality: design a per-tenant residency policy in KCNA so data for tenant stays within EU unless explicitly allowed to be moved, while still supporting cross-tenant analytics with privacy-preserving aggregates. Define policy representation, enforcement points, data-tagging, encryption keys, and a test plan?","answer":"Implement a per-tenant residency layer in KCNA: data is tagged with region constraints at ingest, shards are bound to allowed regions, and cross-region exports use privacy-preserving aggregates (e.g.,","explanation":"## Why This Is Asked\nThis checks ability to enforce data locality and privacy in a global SaaS pipeline while still enabling compliant analytics.\n\n## Key Concepts\n- Data residency policies and policy DSLs\n- Ingest tagging, shard binding, and routing\n- Cross-region analytics with privacy guarantees\n- Audit proofs and key management\n\n## Code Example\n```javascript\n// Pseudo policy snippet\nconst policy = {\n  tenantId: 'tenant-EU',\n  region: 'EU',\n  allowMove: false\n};\nfunction enforcePolicy(event) { /* tag, route, and log */ }\n```\n\n## Follow-up Questions\n- How would you test cross-region export attempts? \n- How do you handle legal holds that require regional data access across borders?","diagram":"flowchart TD\n  Ingest[Ingest KCNA Event] --> Tag[Tag with Region Policy]\n  Tag --> Route[Route to Bound Shard]\n  Route --> Store[Store in Region-bound Storage]\n  Route --> CrossRegion[Cross-Region Export Probe]\n  CrossRegion --> Analytics[Produce Privacy-preserving Aggregates]\n  Analytics --> Audit[Audit Proofs]","difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Snap","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T20:38:45.198Z","createdAt":"2026-01-20T20:38:45.198Z"},{"id":"q-5107","question":"KCNA tenant-scoped access revocation: Design a live policy engine and per-tenant access tokens that allow operators to revoke a tenant's publish/consume rights in KCNA without downtime. Explain token lifecycle, revocation propagation, cross-region consistency, audit proofs, and how you'd test immediate revocation under mixed workloads?","answer":"Use short-lived per-tenant tokens issued by a central authority, validated at edge brokers. Implement a revocation manifest with a TTL and a tamper-evident store, plus per-tenant ACLs. Propagate revoc","explanation":"## Why This Is Asked\nTests ability to design real-time access control in a multi-tenant KCNA deployment with minimal downtime and strong auditability.\n\n## Key Concepts\n- Live policy engine design\n- Token lifecycle and revocation mechanisms\n- Cross-region propagation and consistency\n- Tamper-evident audit proofs and observability\n\n## Code Example\n```javascript\n// Pseudo-token validation sketch\nfunction validateToken(token, tenantId, currentTime) {\n  const payload = decodeAndVerify(token);\n  if (!payload || payload.tenantId !== tenantId) return false;\n  if (payload.exp < currentTime) return false;\n  if (isRevoked(tenantId, payload.jti)) return false;\n  return true;\n}\n```\n\n## Follow-up Questions\n- How would clock skew and revocation storms be handled?\n- How would you simulate cross-region revocation latency with mixed workloads?","diagram":"flowchart TD\n  A[Operator issues revocation] --> B[Revocation manifest]\n  B --> C[Edge token cache]\n  C --> D[KCNA brokers]\n  D --> E[Publish/Consume blocked for tenant]\n  E --> F[Audit/log propagation]","difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","LinkedIn","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T06:52:46.673Z","createdAt":"2026-01-21T06:52:46.673Z"},{"id":"q-5240","question":"KCNA privacy-driven cross-cluster visibility: In a multi-region KCNA deployment, a privacy policy change requires tenants to have per-event visibility windows and revocation capabilities across replicated clusters. Design a system to (1) enforce per-tenant visibility windows during cross-cluster replication, (2) retract or redact events after policy change without breaking at-least-once semantics, (3) provide tamper-evident audit logs and per-tenant revocation, with rollback and observability. Outline data model, replication protocol, API contracts, and a test strategy?","answer":"Propose a cross-region KCNA privacy feature: per-tenant visibility windows with policy-driven revoke capabilities across replicated clusters. Describe a data model change (tenant_id, visibility_end_ts","explanation":"## Why This Is Asked\nPrivacy compliance across regions with replay semantics is tricky. This question probes data modeling, policy enforcement, and auditability in a live KCNA deployment.\n\n## Key Concepts\n- Per-tenant visibility windows and redaction markers\n- Cross-region replication with visibility checks at ingest and replay\n- Tamper-evident audit logs and policy revocation\n- Rollback/rollbackability and observability\n\n## Code Example\n```python\n# simplified redaction example\nfrom datetime import datetime\n\ndef redact(event, policy, now=None):\n    now = now or datetime.utcnow()\n    if now > policy.visible_until(event.tenant_id):\n        event.redacted = True\n    return event\n```\n\n## Follow-up Questions\n- How would you test rollout with canaries and policy-versioning?\n- How do you guard against clock drift affecting visibility windows?","diagram":"flowchart TD\n  P[Producer] --> I[Ingest]\n  I --> R[Replication]\n  R --> S[Policy Service]\n  S --> A[Audit Log]","difficulty":"intermediate","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","LinkedIn"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T11:54:03.801Z","createdAt":"2026-01-21T11:54:03.801Z"},{"id":"q-5262","question":"KCNA Access Control: You manage a KCNA-based event bus serving tenants A, B, C. Design a beginner-friendly ACL layer that attaches per-tenant credentials to publish/subscribe requests, enforces allowlists per topic, supports runtime rotation, and audits ACL changes without blocking traffic. Describe the API surface, data model for ACLs, and a minimal test that shows a denied publish and a permitted subscription with audit entry?","answer":"Publishers/consumers must present a tenant-specific token; the broker checks an ACL store (tenant -> {allowedTopics, allowedActions, tokenVersion}) before forwarding. ACLs are versioned and rotatable;","explanation":"## Why This Is Asked\n\nTests understanding of access control in KCNA, multi-tenant isolation, and auditable, low-friction token rotation.\n\n## Key Concepts\n\n- Tenant-scoped ACLs and per-topic allowlists\n- Token-based authentication with runtime rotation\n- Non-blocking authorization checks and audit logs\n- Backward-compatible ACL evolution\n\n## Code Example\n\n```javascript\n// ACL data model and quick middleware example\nconst acl = {\n  \"tenantA\": { topics: [\"t1\",\"t2\"], actions: [\"publish\",\"subscribe\"], version: 1 }\n}\nfunction authorize(tenant, action, topic, tokenVersion) {\n  const entry = acl[tenant];\n  if (!entry) return false;\n  return entry.version === tokenVersion && entry.topics.includes(topic) && entry.actions.includes(action);\n}\n```\n\n## Follow-up Questions\n\n- How would you store ACLs to support auditability and rotation without downtime?\n- How would you test for token revocation affecting in-flight requests?","diagram":null,"difficulty":"beginner","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Robinhood","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T13:28:30.046Z","createdAt":"2026-01-21T13:28:30.046Z"},{"id":"q-5275","question":"**KCNA Windowed Aggregation**: In a beginner KCNA consumer, implement a per-topic tumbling window of 60s that preserves per-topic offsets, supports late events up to 10s, and emits windowed sums. Describe the API surface (startWindow, onMessage, emitWindow, commitOffsets), watermark semantics, offset commits tied to window emission, and a minimal test harness that exercises on-time and late events?","answer":"Implement a per-topic tumbling window (60s) consumer in KCNA that preserves offsets and supports late events up to 10s. API: startWindow(topic, durationSec, latenessMs) -> WindowId; onMessage(topic, o","explanation":"## Why This Is Asked\n\nTests understanding of event-time processing basics in KCNA, including windowing, watermarking, and offset semantics at the beginner level.\n\n## Key Concepts\n\n- Event-time vs processing-time\n- Watermarks and lateness handling\n- Per-topic offset management\n- Exactly-once vs at-least-once trade-offs in window emission\n\n## Code Example\n\n```javascript\n// Pseudo interface for window state\ntype Window = { id: string, start: number, end: number, sum: number, lastOffset: number|null }\n```\n\n## Follow-up Questions\n\n- How would clock skew across workers impact watermark progress and late events?\n- How to extend to sliding windows without breaking per-topic offsets?","diagram":"flowchart TD\n  A[KCNA Consumer] --> B[Ingest per-topic messages]\n  B --> C[Update watermark]\n  C --> D[EmitWindow & CommitOffsets]\n  D --> E[Downstream processing]","difficulty":"beginner","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","LinkedIn"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T14:50:44.388Z","createdAt":"2026-01-21T14:50:44.388Z"},{"id":"q-5393","question":"KCNA cross-tenant observability: design an end-to-end tracing and auditing pipeline for cross-tenant event flows where multiple tenants publish to shared topics. Describe how to propagate per-tenant correlation IDs, collect per-tenant metrics without leaking data, enable safe live debugging with canary tenants, and ensure minimal performance impact. Include required components, data retention, and rollback considerations?","answer":"Use OpenTelemetry with tenant_id propagation in trace context; route spans to a per-tenant index in Tempo/Jaeger. Apply per-tenant sampling (e.g., 0.1–1%), redact payloads, and store traces with tenan","explanation":"## Why This Is Asked\nThis question probes ability to build cross-tenant observability without violating data isolation; they must consider tracing, sampling, privacy, performance.\n\n## Key Concepts\n- End-to-end tracing with per-tenant isolation\n- Data minimization and redaction\n- Canary and rollback strategy\n- Observability tooling: OpenTelemetry, Tempo/Jaeger, dashboards\n\n## Code Example\n```javascript\n// Pseudo-code: propagate tenant_id in trace context\nconst span = tracer.startSpan(\"kcna.publish\", { attributes: { tenant_id } });\n// inject to carrier\n```\n\n## Follow-up Questions\n- How would you enforce cross-tenant privacy when audits require cross-tenant queries?\n- How would you test performance impact of tracing at scale?\n","diagram":null,"difficulty":"intermediate","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Hugging Face","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T20:44:31.632Z","createdAt":"2026-01-21T20:44:31.632Z"},{"id":"q-5491","question":"KCNA policy-driven access control: design a pluggable authorization layer that enforces per-tenant access policies at topic and shard level, supports hot-reload of policy rules, and returns verifiable audit tokens for every API call, while preserving at-least-once delivery and per-tenant isolation. Outline API contracts (authorize, auditToken), data structures for policy, testing plan, and a small canary rollout strategy?","answer":"A policy engine implementing a per-tenant ABAC (Attribute-Based Access Control) model with policies stored in a distributed configuration store and propagated to shards. Each API call carries a cryptographically signed audit token containing tenantId, action, resource, timestamp, and nonce for replay protection.","explanation":"## Why This Is Asked\nThis question evaluates the design of a secure, scalable KCNA extension that enforces tenant isolation while maintaining high throughput and auditability.\n\n## Key Concepts\n- ABAC with per-tenant policies\n- Pluggable authorization layer integrated at KCNA's request path\n- Hot-reload capability for policy updates without service restart\n- Cryptographically signed audit tokens per request\n- Preservation of at-least-once delivery semantics during authorization checks\n\n## Code Example\n```javascript\n// Pseudocode for audit token generation (server-side)\nfunction signAuditToken(t\n```","diagram":"flowchart TD\n  A[Client Request] --> B[Policy Engine] --> C[KCNA Core]\n  B --> D{Policy Match?}\n  D -->|Yes| E[Emit Audit Token]\n  D -->|No| F[Access Denied]","difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Lyft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T05:29:28.677Z","createdAt":"2026-01-22T02:31:01.932Z"},{"id":"q-5624","question":"KCNA Auditability & Tamper-Evidence: In a multi-tenant KCNA cluster used by Robinhood/HashiCorp/Anthropic, design an audit trail system that records every publish/subscribe event with tenant_id, topic, timestamp, and a cryptographic hash chain to ensure tamper-evidence. Explain data retention, query capabilities, minimizing overhead, and how you would test integrity with canaries?","answer":"Design an immutable per-tenant audit log stream that records tenant_id, topic, event_id, timestamp, action, and a hash-chain linking entries. Use per-tenant KMS keys rotated quarterly, sign entries, a","explanation":"## Why This Is Asked\n\nIn a multi-tenant KCNA cluster, compliance requires auditable, tamper-evident logs for all publish/subscribe actions without leaking tenant data.\n\n## Key Concepts\n\n- Auditable, append-only logs\n- Tamper-evidence via hash chains and per-tenant keys\n- Tenant isolation in audit data\n- Canary-based integrity checks\n- Forensic query performance and retention controls\n\n## Code Example\n\n```javascript\nfunction hashEntry(prevHash, entry) {\n  const h = crypto.createHash('sha256');\n  h.update(prevHash);\n  h.update(JSON.stringify(entry));\n  return h.digest('hex');\n}\n```\n\n## Follow-up Questions\n\n- How to scale hash computation and storage in high-throughput workloads?\n- What if a tenant key is compromised?\n- How to implement efficient time-bounded queries over audits?\n","diagram":null,"difficulty":"intermediate","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Hashicorp","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T08:56:40.962Z","createdAt":"2026-01-22T08:56:40.962Z"},{"id":"q-5753","question":"KCNA Poison Pill Handling: In a beginner KCNA consumer, implement a per-topic poison message strategy: when a message fails to process after N retries, publish it to a per-topic dead-letter (poison) topic, commit offsets only after successful processing or dead-letter enqueue, and ensure pause/resume behavior doesn't lose messages. Describe API surface (maxRetries, deadLetterTopic, backoff), offset semantics, and a minimal test plan with a canary scenario?","answer":"Design a per-topic poison-message handler: configure maxRetries, deadLetterTopic naming, and a backoff. On message failure after N retries, publish to DLQ, and commit the offset. Ensure consumer pause","explanation":"## Why This Is Asked\nTests reliability and failure handling in a simple KCNA consumer, a common real-world need.\n\n## Key Concepts\n- Dead-letter queues per topic\n- Retry backoff and maxRetries\n- Offset commits semantics after DLQ\n- Idempotent publish to DLQ\n\n## Code Example\n```javascript\n// pseudo\nclass PoisonHandler {\n  constructor(maxRetries, dlqTopic) {}\n  handle(msg, attempt) { /* if attempt >= maxRetries -> publish to dlq and commit */ }\n}\n```\n\n## Follow-up Questions\n- How would you test DLQ saturation handling?\n- What metrics would you emit for operational visibility?","diagram":"flowchart TD\n  A[KCNA Message] --> B[Process]\n  B --> C{Success}\n  C -->|Yes| D[Commit Offset]\n  C -->|No, retries left| E[Backoff & Retry]\n  E --> B\n  C -->|No retries left| F[Publish to DLQ Topic]\n  F --> G[Commit Offset]\n","difficulty":"beginner","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Oracle","Plaid","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T15:07:16.926Z","createdAt":"2026-01-22T15:07:16.926Z"},{"id":"q-5759","question":"KCNA cross-region disaster recovery: design an active-active KCNA deployment across Region A and Region B for 10k tenants with per-tenant data residency, exactly-once publish/deliver semantics, and global ordering guarantees during failover?","answer":"KCNA (Kafka-Compatible Native Asynchrony) is a distributed messaging system that provides Kafka-like semantics with enhanced multi-tenancy and cross-region capabilities. For an active-active deployment across Region A and Region B serving 10k tenants with per-tenant data residency, I propose a dual-region topology with bidirectional log replication. Each region runs identical KCNA clusters with per-tenant log partitions isolated by tenant ID. Global ordering is maintained through a hybrid approach: per-tenant sequences within regions and a lightweight global sequencer service for cross-region coordination. Exactly-once semantics are achieved through idempotent producers with transactional writes, where each message carries a unique composite key (tenantId:region:messageId) for deduplication. During failover, the surviving region continues processing using replicated logs, with the global sequencer ensuring no gaps or duplicates in the event stream.","explanation":"## Why This Is Asked\n\nAssesses ability to design resilient cross-region messaging systems with strong consistency guarantees while maintaining multi-tenant isolation and data residency requirements.\n\n## Key Concepts\n\n- KCNA: Kafka-compatible distributed messaging with native multi-tenancy\n- Active-active cross-region replication with bidirectional log streaming\n- Per-tenant data residency through isolated partitioning\n- Hybrid ordering: local sequences + lightweight global sequencer\n- Exactly-once semantics through idempotent producers and transactional writes\n- Failover mechanisms with automatic leader election and log replay\n\n## Code Example\n\n```javascript\n// KCNA publisher with idempotency and cross-region coordination\nclass KCNAPublisher {\n  constructor(tenantId, region, globalSequencer) {\n    this.tenantId = tenantId;\n    this.region = region;\n    this.sequencer = globalSequencer;\n    this.dedupStore = new Map();\n  }\n\n  async publishEvent(event) {\n    const messageId = `${this.tenantId}:${this.region}:${event.id}`;\n    \n    // Idempotency check\n    if (this.dedupStore.has(messageId)) {\n      return { status: 'duplicate', seq: null };\n    }\n\n    // Get global sequence for ordering\n    const globalSeq = await this.sequencer.getNextSequence(\n      this.tenantId, this.region\n    );\n\n    const enrichedEvent = {\n      ...event,\n      tenantId: this.tenantId,\n      region: this.region,\n      globalSequence: globalSeq,\n      timestamp: Date.now()\n    };\n\n    // Write to local log with transaction\n    await this.writeToLog(enrichedEvent);\n    this.dedupStore.set(messageId, true);\n\n    // Trigger cross-region replication\n    await this.replicateToPeerRegion(enrichedEvent);\n\n    return { status: 'success', seq: globalSeq };\n  }\n\n  async writeToLog(event) {\n    // Transactional write to tenant-specific partition\n    const partition = this.getPartition(event.tenantId);\n    return partition.append(event, { transactional: true });\n  }\n\n  async replicateToPeerRegion(event) {\n    // Async replication to peer region\n    const peerRegion = this.region === 'A' ? 'B' : 'A';\n    return this.replicator.send(peerRegion, event);\n  }\n}\n\n// Global sequencer for cross-region ordering\nclass GlobalSequencer {\n  constructor() {\n    this.counters = new Map(); // tenantId -> sequence\n  }\n\n  async getNextSequence(tenantId, region) {\n    const key = `${tenantId}`;\n    const current = this.counters.get(key) || 0;\n    const next = current + 1;\n    this.counters.set(key, next);\n    return next;\n  }\n}\n```\n\n## Follow-up Questions\n\n- How would you handle network partitions between regions to prevent split-brain scenarios?\n- What strategies would you use for tombstone-based garbage collection of replicated logs?\n- How would you implement automated canary testing for failover scenarios?\n- What monitoring and observability patterns would you deploy to detect ordering violations?","diagram":"flowchart TD\n  A[Tenant] --> B[Region A KCNA]\n  C[Tenant] --> D[Region B KCNA]\n  B --> E[Replication]\n  D --> E\n  E --> F[Consistent ordering]","difficulty":"intermediate","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Meta","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T05:09:28.264Z","createdAt":"2026-01-22T15:41:45.448Z"},{"id":"q-5842","question":"KCNA Cross-Partition Ordering & Parallelism: In a beginner KCNA consumer, design a per-topic, per-partition ordered processing path that allows parallel processing across partitions, preserves per-partition order, commits offsets per partition after success, and handles rebalances without message loss. How would you implement this, including API hints and a minimal test plan?","answer":"Use one worker per partition to guarantee per-partition order, with a central coordinator mapping in-flight offsets. Each partition has its own queue; after processing a message, commit that partition","explanation":"## Why This Is Asked\n\nTests practical understanding of KCNA's partitioning model, ordering guarantees, and rebalance behavior, which are core to reliable streaming jobs.\n\n## Key Concepts\n\n- Per-partition ordering guarantees\n- Parallelism across partitions\n- Offset commits scoped by partition\n- Rebalance coordination and buffering\n\n## Code Example\n\n```javascript\n// sketch: per-partition worker pool with partitioned queues and offset tracking\n```\n\n## Follow-up Questions\n\n- How would you handle late-arriving messages that arrive after a partition offset was committed?\n- How would you extend the model to dynamic partition counts?\n","diagram":null,"difficulty":"beginner","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","Hugging Face","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T19:02:38.363Z","createdAt":"2026-01-22T19:02:38.363Z"},{"id":"q-5929","question":"KCNA Real-time Tenant-Aware Sink Router: In a multi-tenant KCNA deployment used by finance analytics, implement a tenant-aware router that reads per-tenant event streams, routes to per-tenant downstream sinks with independent backpressure, supports dynamic sink provisioning, per-tenant rate limiting, and circuit-breaking, while preserving per-tenant order within shards and ensuring no cross-tenant data leakage. Outline protocol for sink readiness, backpressure signaling, and a test plan with failure scenarios?","answer":"Design a tenant-aware sink router for KCNA that routes per-tenant event streams to dedicated downstream sinks with independent backpressure management, dynamic sink provisioning, per-tenant rate limiting, and circuit-breaking capabilities. The solution preserves per-tenant ordering guarantees within shards while ensuring strict data isolation between tenants. The router implements a sink readiness protocol, backpressure signaling mechanism, and supports dynamic scaling of sink instances based on tenant workload patterns.","explanation":"## Why This Is Asked\n\nAssesses the ability to architect tenant-scoped routing with backpressure, ordering, and fault tolerance in a streaming system. Evaluates data isolation under load and how routing decisions impact latency and throughput.\n\n## Key Concepts\n\n- Tenant isolation and boundary enforcement\n- Per-tenant backpressure and rate limiting\n- Ordering guarantees within shards\n- Dynamic sink provisioning and circuit breakers\n- Observability and failover semantics\n\n## Code Example\n\n```java\n// Pseudo-code: skeleton of a tenant-aware router\nclass TenantSinkRouter {\n  void route(Event e, String t","diagram":"flowchart TD\n  KCNA[KCNA] --> Router[Sink Router]\n  Router --> TenantSink[Tenant Sink]\n  TenantSink --> Downstream[Downstream Systems]\n  Downstream -->|Backpressure| Router\n  Router -->|Offsets| KCNA","difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Discord","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T05:44:18.029Z","createdAt":"2026-01-22T22:36:32.878Z"},{"id":"q-5981","question":"KCNA Schema Evolution: In a multi-tenant KCNA deployment, tenants introduce new event schemas while producers/consumers run continuously. Design a per-tenant schema registry and compatibility policy that (a) supports per-tenant versioning and isolation, (b) enforces backward compatibility for existing consumers, (c) enables in-flight upgrades with zero data loss, and (d) propagates changes to all consumers with minimal disruption. Describe data structures, API surface, migration strategy, and a test plan?","answer":"Implement a per-tenant Schema Registry with versioned schemas stored in KCNA metadata; each event includes a schema_id and subject key per tenant. Enforce compatibility through a matrix (backward for producers, forward for consumers) with automatic validation. Deploy canary upgrades using dual-write patterns and consumer cache invalidation to ensure zero data loss during schema evolution.","explanation":"## Why This Is Asked\nThis question evaluates schema evolution control in multi-tenant KCNA environments, balancing isolation, compatibility, and continuous throughput.\n\n## Key Concepts\n- Per-tenant versioned schemas\n- Compatibility matrix (backward/forward)\n- Canary upgrades and rollback mechanisms\n- In-flight data safety and consumer cache invalidation\n\n## Code Example\n```javascript\nclass SchemaRegistry {\n  constructor() { this.store = new Map(); }\n  register(tenant, subject, schema) { /* versioning logic */ }\n  validate(tenant, subject, version, payload) { /* type check */ }\n}\n```\n\n## Follow-up Questions\n- How would you handle schema conflicts between tenants?\n- What monitoring would you implement for schema evolution failures?\n- How do you ensure consumer cache consistency during upgrades?","diagram":null,"difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Slack","Tesla","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T04:55:38.276Z","createdAt":"2026-01-23T02:38:59.234Z"},{"id":"q-6116","question":"KCNA cross-region replication and data sovereignty: in a multi-tenant KCNA, tenants require data to reside in a primary region while reads are served from nearby mirrors. Design the per-tenant regional topology, isolation, and DR strategy, including conflict resolution, replay safety, legal holds, and observability. Include topology diagrams, API changes, and a canary-based failover test plan?","answer":"Propose per-tenant primary region with regional mirrors; enforce tenant-scoped writes and isolation. Use causal vector clocks plus per-tenant tombstones for replay safety. Implement legal-hold logs an","explanation":"## Why This Is Asked\nTests ability to design global KCNA topology with data sovereignty, cross-region replication, and DR that preserves tenant isolation.\n\n## Key Concepts\n- Cross-region replication and data locality\n- Tenant isolation and per-tenant topology\n- Conflict resolution and replay safety\n- Legal holds and auditability\n- Observability and testing\n\n## Code Example\n```python\n# Pseudo vector clock merge for conflict resolution\nfrom typing import Dict\n\ndef merge_vc(vc1: Dict[str,int], vc2: Dict[str,int]) -> Dict[str,int]:\n    keys = set(vc1) | set(vc2)\n    return {k: max(vc1.get(k,0), vc2.get(k,0)) for k in keys}\n```\n\n## Follow-up Questions\n- How would you model per-tenant holds across regions?\n- What is the impact on read latency and consistency guarantees?\n- How would you validate data sovereignty in CI/CD and audits?","diagram":"flowchart TD\n  A[Tenant Primary Region] --> B[Regional Mirrors]\n  B --> C[Conflict Resolution]\n  C --> D[Observability & DR]\n  D --> E[Legal Holds & Audit Trails]","difficulty":"intermediate","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Databricks","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T09:40:35.308Z","createdAt":"2026-01-23T09:40:35.308Z"},{"id":"q-6212","question":"KCNA multi-tenant: design a tenant-aware partition reallocation mechanism that allows a tenant to scale out by moving a subset of its partitions to new brokers without stopping producers/consumers. Describe data structures for per-tenant partition ownership, the protocol (graceful stop, fence commits) to transfer ownership, how to preserve per-tenant order and exactly-once delivery, and a test plan with canaries and rollback procedure?","answer":"Use a per-tenant partition ownership map and a two-phase rebalancing protocol. Phase 1: phase-1 freeze—producers stop offset advances for the tenant locally, emit a commit fence, and route to target b","explanation":"## Why This Is Asked\nAssesses practical knowledge of multi-tenant KCNA scaling, ordering guarantees, and zero-downtime rebalancing.\n\n## Key Concepts\n- Per-tenant partition ownership and fast-rebalancing\n- Two-phase commit style fencing and graceful stop\n- Ordering guarantees and exactly-once semantics across transfers\n- Observability, canaries, rollback\n\n## Code Example\n```javascript\n// Pseudo protocol for per-tenant partition reallocation\nasync function rebalanceTenantPartitions(tenantId, partitions, targetBrokers){\n  await freezeOffsets(tenantId, partitions);\n  await migrateData(partitions, targetBrokers);\n  await commitOwnershipChange(tenantId, partitions, targetBrokers);\n  await resumeProcessing(tenantId);\n}\n```\n\n## Follow-up Questions\n- How would you validate no data loss during mid-transfer?\n- How do you handle in-flight messages that cross the transfer boundary?","diagram":"flowchart TD\n  A[Tenant] --> B[Partition Ownership Map]\n  B --> C[Graceful Stop]\n  C --> D[Transfer to New Brokers]\n  D --> E[Resume Processing]","difficulty":"intermediate","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Microsoft","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T14:45:17.542Z","createdAt":"2026-01-23T14:45:17.542Z"},{"id":"q-6395","question":"KCNA per-tenant privacy-enabled analytics: In a single KCNA deployment for DoorDash and Citadel, design an ad-hoc analytics layer that lets a tenant run queries on their own event streams without data leakage to others. Include (a) per-tenant encryption and key management with rotation, (b) policy-driven access controls and safe cross-tenant joins using hashed IDs, and (c) an auditable query verification trail. Provide data models, rollback, and a test plan?","answer":"Implement per-tenant envelope encryption with keys managed in a KMS, storing tenant-scoped data keys and key encryption keys (KEKs) for metadata. Data at rest is encrypted using tenant-specific keys, while TLS protects data in transit. Enforce fine-grained RBAC/ABAC with policy-based access controls that restrict queries to tenant-specific data only. Enable safe cross-tenant joins using deterministic hashed IDs that preserve privacy while allowing correlation across datasets. Maintain an auditable query verification trail that logs all access attempts, query executions, and key rotations with tamper-evident cryptographic signatures.","explanation":"## Why This Is Asked\n\nTests ability to design privacy-preserving analytics in a multi-tenant KCNA deployment, covering encryption, access control, cross-tenant data handling, and auditable verification under real-time production loads.\n\n## Key Concepts\n\n- Tenant-scoped keys and envelope encryption\n- Hash-based cross-tenant joins\n- Fine-grained access control (RBAC/ABAC)\n- Verifiable audit trails and key rotation\n- Performance impact and caching strategies\n\n## Code Example\n\n```javascript\n// Pseudo: derive tenant key and encrypt data\nfunction encryptForTenant(tenantId, plaintext) {\n  const tenantKey = kms.getTenantKe","diagram":"flowchart TD\n  A[Tenant] --> B[KCNA Analytics Layer]\n  B --> C[Encrypted Data Store]\n  B --> D[Audit Trail]\n  C --> E[Query Results]","difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","DoorDash"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T05:31:45.342Z","createdAt":"2026-01-23T22:26:47.646Z"},{"id":"q-6421","question":"KCNA Performance Isolation: design a per-tenant QoS controller that guarantees bounded tail latency for high-priority tenants in a shared KCNA cluster. Describe (1) dynamic shard allocation with per-tenant quotas, (2) safe preemption/rate-limiting to avoid data loss, (3) preserving at-least-once semantics and idempotence during reallocation, and (4) observability and canary rollout with rollback?","answer":"Implement a per-tenant QoS controller that monitors tail latency and backlog per tenant; use a dynamic shard allocator to enforce quotas, with safe preemption that throttles lower-priority producers and preserves at-least-once semantics through idempotent reallocation.","explanation":"## Why This Is Asked\n\nTo assess ability to design multi-tenant QoS at scale, with focus on latency guarantees, data safety, and controlled rollouts under workload skew.\n\n## Key Concepts\n\n- Per-tenant QoS control\n- Dynamic shard allocation\n- Safe preemption and idempotence\n- Observability and canary rollouts\n\n## Code Example\n\n```javascript\n// Pseudo QoS controller sketch\nclass QoSController {\n  constructor(targetTail, quotas) { this.targetTail = targetTail; this.quotas = quotas; }\n  onBacklogUpdate(backlogs) { /* adjust quotas and shard map */ }\n}\n```\n\n## Follow-up Questions\n\n- How would you te","diagram":"flowchart TD\nA[KCNA QoS Controller] --> B{Decide action}\nB --> C[Quota adjust]\nB --> D[Shard migration]\nC --> E[KCNA Shards]\nD --> E\nE --> F[Telemetry]","difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","LinkedIn"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T05:04:57.194Z","createdAt":"2026-01-23T23:33:35.162Z"},{"id":"q-6560","question":"KCNA Cross-Region DR: In a multi-tenant KCNA deployment spanning two regions, design a disaster-recovery plan that preserves per-tenant data residency, supports active-active reads, and guarantees at-least-once processing during failover. Describe per-tenant region routing, tombstone handling, cross-region idempotence, and a verifiable point-in-time rollback plan with testing steps and rollback criteria?","answer":"Pin tenants to a regional primary, enable cross-region reads with local replicas, and guarantee at-least-once processing by idempotent handlers and monotonic sequence numbers. Use per-tenant tombstone","explanation":"## Why This Is Asked\n\nDesign DR with per-tenant residency, cross-region replication, canary testing.\n\n## Key Concepts\n\n- Cross-region replication\n- Per-tenant residency\n- Tombstone windows\n- Point-in-time rollback\n- Canary-based failover\n\n## Code Example\n\n```javascript\n// Pseudo-config for tenant DR routing\nconst tenantPolicy = {\n  tenantId: 't1',\n  primaryRegion: 'us-east-1',\n  allowCrossRegionReads: true\n}\n```\n\n## Follow-up Questions\n\n- How would you test DR with a rolling canary?\n- How would you ensure no cross-tenant data leakage during failover?\\n","diagram":"flowchart TD\n  Tenant[Tenant] --> Route[Per-tenant Region Routing]\n  Route --> Region1[Region1 Shards]\n  Route --> Region2[Region2 Shards]\n  Region1 --> Commit1[Commit & Tombstone Window]\n  Region2 --> Commit2[Commit & Tombstone Window]\n  Commit1 --> Rollback[Point-in-Time Rollback]\n  Commit2 --> Rollback","difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Snap","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T08:03:09.083Z","createdAt":"2026-01-24T08:03:09.083Z"},{"id":"q-6585","question":"KCNA Per-tenant envelope encryption and key rotation: In a multi-tenant KCNA deployment spanning two regions, design a per-tenant envelope encryption strategy for persisted event payloads. Describe how to manage per-tenant KMS keys, versioned envelope keys, zero-downtime rotation, cross-region key distribution, and audit trails. Include API surface, data structures, rollback, and test plan?","answer":"Per-tenant envelope encryption with KMS: assign each tenant a KEK in the KMS; data is encrypted with a per-tenant data key (DEK) that is wrapped by the KEK and stored alongside the payload metadata. R","explanation":"## Why This Is Asked\nTests encryption design, tenancy isolation, rotation semantics, cross-region distribution, and auditability in KCNA.\n\n## Key Concepts\n- Envelope encryption with per-tenant KEK and DEK\n- KMS integration, key-versioning, and cross-region propagation\n- Zero-downtime rotation and background re-wrapping\n- Audit trails and RBAC for keys\n\n## Code Example\n```javascript\n// Pseudo: envelope encrypt a payload\nfunction encryptPayload(plaintext, tenantId, kmsClient) {\n  const DEK = kmsClient.getDataKey(tenantId);\n  const wrappedDEK = kmsClient.wrapKey(DEK, tenantId);\n  const ciphertext = cryptoAesGCM(nonce, DEK, plaintext);\n  return { ciphertext, wrappedDEK, keyVersion: kmsClient.currentVersion(tenantId) };\n}\n\nfunction decryptPayload(payloadPkg, tenantId, kmsClient) {\n  const DEK = kmsClient.unwrapKey(payloadPkg.wrappedDEK, tenantId);\n  return cryptoAesGCMDecrypt(payloadPkg.ciphertext, DEK);\n}\n```\n\n## Follow-up Questions\n- How would you test the zero-downtime rotation in production, including canaries and rollback?\n- What metrics and alerts would you add to detect rotation-related failures and data-inaccessibility?","diagram":"flowchart TD\nA[Insert Payload] --> B[Encrypt with DEK_Tn]\nB --> C[Wrap with KEK_Tn from KMS]\nC --> D[Store payload with key_version]\nE[Rotate KEK_Tn] --> F[Rewrap DEKs for existing data in background]\nF --> G[Update key_version]\nG --> H[Consume decrypt with DEK_Tn]","difficulty":"intermediate","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","MongoDB","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T08:57:27.509Z","createdAt":"2026-01-24T08:57:27.510Z"},{"id":"q-6972","question":"KCNA runs multi-tenant, geo-distributed streams. Design an end-to-end encryption and data isolation scheme using per-tenant envelope keys wrapped by a central KMS/HSM, plus per-region rotation, revocation, and audit of key usage. Explain how you secure data-at-rest and in-flight, backups, cross-region replication with zero data leakage, and performance implications. Include API and data-model choices?","answer":"Design per-tenant envelope encryption: each tenant has a data key wrapped by a central KMS/HSM, with per-region KEKs rotated quarterly; enforce strict cross-region isolation for replicas; audit key usage with immutable logs; secure data-at-rest with AES-256-GCM, data-in-flight with TLS 1.3, and backups with encrypted snapshots; implement zero-knowledge cross-region replication using region-specific KEKs; optimize performance through key caching and batch cryptographic operations.","explanation":"## Why This Is Asked\n\nTests the ability to design secure, scalable data isolation in KCNA across geo regions, balancing cryptography, auditability, and performance.\n\n## Key Concepts\n\n- Per-tenant envelope encryption\n- Central KMS/HSM integration\n- Per-region KEK rotation and revocation\n- Cross-region replication safety\n- Auditability and immutable backups\n- Performance trade-offs with key caching\n\n## Code Example\n\n```javascript\n// Pseudo API for envelope encryption\nconst dk = getTenantDataKey(tenantId);\nconst wrapped = kms.wrapKey(dk);\nconst ct = encrypt(plaintext, dk);\n```\n\n## Follow-up Questions","diagram":"flowchart TD\n  A[Tenant Data] --> B[Envelope Key]\n  B --> C[KMS/HSM Wrap]\n  C --> D[Region KEK Rotation]\n  D --> E[Encrypted Data Store]","difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["NVIDIA","Salesforce","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T05:15:53.862Z","createdAt":"2026-01-25T02:50:55.464Z"},{"id":"q-7007","question":"In a beginner KCNA setup, design a cross-topic join between two streams: 'events' and 'analytics', where a joined record is emitted when eventId matches within a 30s window, while preserving per-topic offsets and guaranteeing at-least-once delivery. Describe API surface (registerJoin, onEventA, onEventB, emitJoined), offset commit rules, late-arrival handling up to 15s, and a minimal canary test plan?","answer":"Store unpaired messages per eventId and topic. OnEventA/OnEventB match by eventId; when both exist within a 30s window, emit the joined record and commit the corresponding offsets. Use a watermark at ","explanation":"## Why This Is Asked\n\nThis question tests practical skills in cross-stream joins, a common real-world KCNA pattern, focusing on correctness with per-topic offsets, windowing, and late data handling.\n\n## Key Concepts\n\n- Cross-topic joins and per-event-state\n- Windowing and watermarking (30s window, 15s lateness)\n- Offset semantics and commit discipline\n- At-least-once guarantees with restart recovery\n\n## Code Example\n\n```javascript\nclass KCNAJoiner {\n  constructor(windowMs, lateMs) {\n    // internal stores and timers\n  }\n  onEventA(event) {\n    // store aEvent by eventId\n  }\n  onEventB(event) {\n    // store bEvent by eventId; attempt join\n  }\n  emitJoined(eventId, a, b) {\n    // emit and advance offsets\n  }\n}\n```\n\n## Follow-up Questions\n\n- How would you handle duplicate messages for the same eventId from both topics?\n- How would you scale this across partitions and maintain ordering guarantees?\n- How would you extend to configurable window sizes and late-arrival thresholds?","diagram":"flowchart TD\n  A[EventA] -->|eventId| J[Join Store]\n  B[EventB] -->|eventId| J\n  J --> E[EmitJoined]\n  E --> C[OffsetsCommitted]","difficulty":"beginner","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Coinbase","IBM"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T05:38:36.474Z","createdAt":"2026-01-25T05:38:36.474Z"},{"id":"q-7030","question":"KCNA Cross-Topic Reconciliation: In a beginner KCNA setup with two topics, implement a simple per-topic-offset consumer to join events from 'orders' and 'payments' by orderId within a 30-second tumbling window, emitting a reconciled record to 'reconciliations'. Preserve offsets, ensure at-least-once delivery, and provide a minimal canary test scenario?","answer":"Use a 30s in-memory window per orderId. On an order event, stash it with timestamp; on a payment event, pair if exists, emit a reconciled record with reconciliationId, then commit both offsets; to han","explanation":"## Why This Is Asked\nThis checks ability to design cross-topic joins, windowing, and correct offset semantics for KCNA at the beginner level.\n\n## Key Concepts\n- Cross-topic correlation, per-topic offsets, 30s window\n- Idempotent emits via reconciliationId\n- Simple in-memory state with TTL and cleanup\n\n## Code Example\n```javascript\n// Pseudo-code for a simple join engine\nclass JoinEngine {\n  // In-memory maps and window cleanup logic\n}\n```\n\n## Follow-up Questions\n- How would you scale this to multiple partitions?\n- How do you handle skewed event arrival across topics?","diagram":"flowchart TD\n  Orders[Orders topic consumer] --> Join[Join window 30s]\n  Payments[Payments topic consumer] --> Join\n  Join --> Reconciled[Emit to reconciliations topic]\n  Reconciled --> Offsets[Commit offsets]","difficulty":"beginner","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T06:47:33.470Z","createdAt":"2026-01-25T06:47:33.470Z"},{"id":"q-7059","question":"KCNA Security Key Rotation: In a beginner KCNA setup, implement per-topic envelope encryption using a rotating AES-GCM key per topic. Messages include a header with keyId. Design a minimal KeyProvider interface, ensure decrypt on the fly, and rotate keys without stopping consumption. Describe API, data format, and a minimal test plan?","answer":"Approach: define a per-topic KeyRing with a 24h rotation. Each KCNA message carries keyId; decrypt with AES-256-GCM using topicName as AAD. Consumer caches current key, switches on rotation, replays f","explanation":"## Why This Is Asked\nThis question probes practical crypto handling in KCNA, focusing on rotation, on-the-fly decryption, and failure paths.\n\n## Key Concepts\n- Per-topic key rotation\n- AES-GCM with AAD\n- KeyProvider interface\n- Decrypt path and DLQ fallback\n\n## Code Example\n```javascript\n// Pseudo-code sketch\nclass KeyProvider { getKeyId(topic, timestamp): string; getKey(keyId): Buffer; }\nfunction decrypt(msg, provider) { const key = provider.getKey(msg.keyId); return aesGcmDecrypt(msg.payload, key, msg.topic, msg.iv); }\n```\n\n## Follow-up Questions\n- How to coordinate rotations across many topics without downtime?\n- How would you verify key provenance and rotation audits?","diagram":null,"difficulty":"beginner","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T07:40:34.912Z","createdAt":"2026-01-25T07:40:34.912Z"},{"id":"q-7105","question":"KCNA Geo-Locality: Your KCNA deployment serves tenants across NA, EU, and APAC. Design a tenant-aware routing and data residency policy that keeps tenant data within their region, supports isolation, and allows disaster recovery across regions. Describe data placement, cross-region replication, failover, and testing strategy with canaries?","answer":"Bind each tenant to a primary region; writes are region-local with in-region quorum, cross-region replication is asynchronous per-tenant; use region-scoped encryption keys and quotas; tombstones for d","explanation":"## Why This Is Asked\n\nTests understanding of geo-distributed systems, data residency, and disaster recovery across KCNA shards while preserving tenancy isolation and low latency.\n\n## Key Concepts\n\n- Region affinity and data locality\n- Asynchronous cross-region replication per tenant\n- Tenant-scoped encryption keys and quotas\n- Tombstones and data pruning\n- Canary testing and SLA monitoring\n\n## Code Example\n\n```javascript\n// Pseudo: route writes to tenant's primary region and mirror to others asynchronously\nfunction routeWrite(tenantId, data) {\n  const region = mapTenantToRegion(tenantId);\n  writeInRegion(tenantId, region, data);\n  replicateAcrossRegions(tenantId, region, data);\n}\n```\n\n## Follow-up Questions\n\n- How would you test failover to a regional DR site with zero data loss?\n- How do you handle tenants migrating regions without service disruption?\n","diagram":"flowchart TD\n  T[Tenant] --> R[Region Primary]\n  R --> CR[Cross-Region Replication (Tenant)]\n  CR --> R2[Region Secondary]\n  T --> Q[Reads/Writes Routed to Primary]\n","difficulty":"intermediate","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","Instacart","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T09:35:02.423Z","createdAt":"2026-01-25T09:35:02.423Z"},{"id":"q-7141","question":"KCNA Topic Migration Protocol: design a safe, zero-downtime topic migration between KCNA clusters for a single tenant. Requirements: preserve at-least-once delivery, maintain per-tenant access control, ensure message order within a topic during split/join, and guarantee no data loss during cross-cluster offset rewinds. Outline data structures, migration protocol steps, and rollback?","answer":"Use a dual-topic alias migration per tenant with a cutover window. Steps: 1) Create destination topic alias and publish to both source and target. 2) Maintain a transactional offset mapping to preserv","explanation":"## Why This Is Asked\nTests understanding of cross-cluster migrations, latency/throughput tradeoffs, and strong delivery guarantees.\n\n## Key Concepts\n- Zero-downtime migrations across KCNA clusters\n- Exactly-once vs at-least-once, transactional writes\n- Tenant isolation, access control, offset mapping\n\n## Code Example\n```javascript\n// Pseudo: startMirrorMigration(source, target, tenant){}\n```\n\n## Follow-up Questions\n- How would you validate correctness during canary rollout?\n- How do you handle failures during cutover with minimal user impact?","diagram":null,"difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Hugging Face","LinkedIn"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T10:44:18.145Z","createdAt":"2026-01-25T10:44:18.146Z"},{"id":"q-7206","question":"KCNA Tenant-Scoped Materialized Views: In a multi-tenant KCNA cluster used by data-heavy apps, design a tenant-scoped materialized view subsystem that builds per-tenant views from a shared event stream while guaranteeing bounded memory, per-tenant quotas, and exactly-once incremental updates. Describe update pipeline, fault tolerance, eviction strategy, and a concrete test plan?","answer":"Propose a per-tenant projection engine that subscribes to the common KCNA stream via per-tenant workers, writes incremental deltas to a per-tenant MV log, and applies idempotent upserts using an incre","explanation":"## Why This Is Asked\nReal-time analytics across tenants requires isolation, predictable latency, and robust data correctness; a per-tenant MV design must handle resource limits and fault tolerance.\n\n## Key Concepts\n- Tenant-scoped materialized views\n- Incremental, exactly-once updates with per-tenant sequence numbers\n- Per-tenant quotas and backpressure\n- Checkpointed replay and eviction strategies\n- Observability and testability across tenants\n\n## Code Example\n```javascript\nfunction applyDelta(view, delta){\n  if(delta.seq <= view.lastSeq) return; // idempotent\n  view.data[delta.key] = delta.value;\n  view.lastSeq = delta.seq;\n}\n```\n\n## Follow-up Questions\n- How would you test late-arriving events and out-of-order delivery?\n- How to monitor per-tenant memory and enforce quotas without affecting others?","diagram":null,"difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T13:46:30.595Z","createdAt":"2026-01-25T13:46:30.595Z"},{"id":"q-7316","question":"In KCNA, two tenants publish events about the same business entity but data must stay isolated. Design a privacy-preserving cross-tenant reconciliation: match events by business key (orderId) without exposing the key across tenants, support late arrivals up to 15 minutes, and emit reconciled records to a shared 'reconciliations' topic. Explain data formats, per-tenant masking approach, how you ensure at-least-once semantics, and how you test with canaries?","answer":"Use per-tenant masking keys via HMAC-SHA256 with a tenant secret, producing maskedOrderId. Join streams on maskedOrderId in a windowed state store. Upsert a reconciled record with a stable reconciliat","explanation":"## Why This Is Asked\n\nAssess privacy-preserving cross-tenant joins, security trade-offs, and end-to-end KCNA reconciliation semantics.\n\n## Key Concepts\n\n- Privacy-preserving cross-tenant joins using per-tenant masking\n- Windowed joins with late-arrival handling\n- Idempotent outputs to the reconciliations topic\n- Canary testing and observability for safety\n\n## Code Example\n\n```javascript\n// Masking function\nfunction maskKey(tenantSecret, orderId) {\n  const h = crypto.createHmac('sha256', tenantSecret);\n  h.update(orderId);\n  return h.digest('hex');\n}\n```\n\n## Follow-up Questions\n\n- How would you rotate tenant secrets without breaking in-flight joins?\n- How would you detect masking collisions or privacy leaks in production?","diagram":"flowchart TD\n  A[Tenant A Event] --> B[Masking (Tenant A Key)]\n  C[Tenant B Event] --> B\n  B --> D[Windowed Join Store (15m)]\n  D --> E[Reconciliations Topic]","difficulty":"intermediate","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T17:48:38.369Z","createdAt":"2026-01-25T17:48:38.369Z"},{"id":"q-7343","question":"KCNA Beginner: Build a per-topic, offset-aware consumer that joins 'cart_events' and 'checkout_events' by sessionId within a 20-second window, outputting 'session_summaries'. Buffer and deduplicate per session, emit once per match or on timeout, and commit offsets only after emission. Provide a minimal canary test simulating out-of-order and late events?","answer":"Buffer cart and checkout events per sessionId, keep per-topic offsets, and perform a windowed join with a 20-second tumbling window. Deduplicate by (topic, offset, sessionId). Emit a single session_su","explanation":"## Why This Is Asked\nShows practical KCNA offset handling and windowed join with dedupe and at-least-once semantics.\n\n## Key Concepts\n- per-topic offsets\n- windowed join semantics\n- idempotent sinks\n- timeout-driven emission\n- simple Canary testing\n\n## Code Example\n```javascript\n// Pseudo-join sketch\nconst cartBuffer = new Map();\nconst checkoutBuffer = new Map();\nfunction tryEmit(sessionId){\n  // if both sides exist, emit and clear\n}\n```\n\n## Follow-up Questions\n- How would you validate dedup correctness under replay?\n- How would you simulate network partitions and observe consumer offsets?\n","diagram":null,"difficulty":"beginner","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T19:30:12.262Z","createdAt":"2026-01-25T19:30:12.262Z"},{"id":"q-7614","question":"KCNA Auditable Events: In a multi-tenant KCNA cluster handling regulated data, implement a per-tenant, tamper-evident publish history. Requirements: (1) append-only ledger per tenant with cryptographic hash chaining, (2) ability to replay history for a tenant to reconstruct state, (3) policy-driven retention, legal holds, and (4) impact on producer/consumer latency; provide API outline, data format, and a minimal test plan?","answer":"Per-tenant audit ledgers as append-only KCNA topics. Each event includes payloadHash, prevHash, timestamp, tenantId, and producerSignature; verification uses a chained hash, enabling tamper-evidence. ","explanation":"## Why This Is Asked\nDemonstrates real-world need for tamper-evident auditing in shared data platforms and how per-tenant isolation supports compliance.\n\n## Key Concepts\n- Append-only ledgers per tenant; cryptographic hash chaining; signatures\n- Efficient replay by offsets; export proofs; retention and holds\n- Performance impact and observability\n\n## Code Example\n```javascript\n// Not provided here; outline only in interview\n```\n\n## Follow-up Questions\n- How would you handle key rotation for signatures without breaking the chain?\n- How would you test strong integrity guarantees under node failures?","diagram":null,"difficulty":"intermediate","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","Scale Ai","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T09:57:06.888Z","createdAt":"2026-01-26T09:57:06.888Z"},{"id":"q-7660","question":"KCNA Idempotent Replay Guard: In a beginner KCNA setup with two topics, implement a per-topic deduplication strategy that survives restarts. Use a per-topic in-memory cache of recent eventId values, backed by a compact 'dedup-store' topic for recovery. Ensure offsets are committed only after a successful (non-duplicate) processing decision, and provide a minimal canary test?","answer":"Design a per-topic dedup cache (bounded LRU or Bloom filter) keyed by eventId. On each message, check cache; if new, process and emit a marker to a 'dedup-store' and commit offsets; if seen, skip proc","explanation":"## Why This Is Asked\nTests understanding of idempotent KCNA processing, restart recovery, and compact dedup state management across topics.\n\n## Key Concepts\n- Per-topic deduplication state\n- Durable recovery via a side topic\n- Correct offset handling with bypassed duplicates\n- Bounded memory strategies (LRU vs Bloom filter)\n\n### Code Example\n```javascript\n// Pseudo: dedup check and commit logic\n```\n\n## Follow-up Questions\n- How would you choose between Bloom filter and LRU-based cache in memory-constrained environments?\n- How would you test failure during dedup-store write?","diagram":"flowchart TD\n  A[Message from Topic] --> B{IsDuplicate?}\n  B -- Yes --> C[Skip Processing]\n  C --> D[Commit Offsets]\n  B -- No --> E[Process Message] --> F[Write to dedup-store] --> G[Commit Offsets]","difficulty":"beginner","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T11:46:14.922Z","createdAt":"2026-01-26T11:46:14.922Z"},{"id":"q-7717","question":"KCNA Access Control: you operate a shared KCNA cluster for Google and Nvidia engineers. Design a per-tenant, per-topic access control scheme that supports (1) topic-level ACLs, (2) per-record visibility via headers, (3) short-lived, rotatable tokens, and (4) auditability with tamper-evident logs. Explain data structures, token lifecycle, replay protections, and a rollout plan with canaries and rollback. Include concrete API surfaces and failure modes?","answer":"Implement a per-tenant/topic ACL registry, issue short-lived JWTs scoped to tenant/topic, attach tenantId and topicId in record headers, enforce ACL at broker ingress, and log every access to an appen","explanation":"## Why This Is Asked\n\nTests ability to design robust RBAC in KCNA with per-tenant isolation, real-time token rotation, and auditable access.\n\n## Key Concepts\n\n- Per-tenant ACLs\n- Token lifecycle and revocation\n- Tamper-evident audit logs\n- Read/write isolation and failure modes\n\n## Code Example\n\n````javascript\n// Token validation sketch\nfunction validate(token, tenant, topic) {\n  const payload = decodeJWT(token);\n  if (payload.tenant !== tenant || payload.topic !== topic) return false;\n  return verifySignature(token);\n}\n````\n\n## Follow-up Questions\n\n- How would you test revocation without service downtime?\n- How to handle token leakage across tenants?\n","diagram":null,"difficulty":"intermediate","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T14:57:57.790Z","createdAt":"2026-01-26T14:57:57.790Z"},{"id":"q-7812","question":"KCNA Tenant Data Retention: In a KCNA cluster servicing tenants with stringent data-retention requirements, design a per-tenant retention policy (e.g., 30 days) with automatic per-tenant purges. Explain how tombstones, compaction, and replication semantics are handled to prevent data resurrection, and outline the API (setRetention, getRetention, purgeTenant) and a minimal test plan with canaries?","answer":"Implement a per-tenant retention policy (e.g., 30 days) with automatic purges. Ensure tombstones are removed and compaction does not resurrect data; coordinate with replication so purge is consistent ","explanation":"## Why This Is Asked\nThis question probes per-tenant data governance, cross-tenant isolation, and purge semantics in KCNA.\n\n## Key Concepts\n- Per-tenant retention windows\n- Tombstone lifecycle and compaction interaction\n- Cross-region replication safety during purge\n- Auditability and canary checks\n\n## Code Example\n```javascript\ninterface TenantRetention {\n  tenantId: string;\n  retentionMs: number;\n}\nclass KCNARetainer {\n  setRetention(tenant: string, ms: number): void {}\n  purgeTenant(tenant: string): void {}\n  getRetention(tenant: string): number {}\n}\n```\n\n## Follow-up Questions\n- How would you verify no resurrected data after purge?\n- How to handle policy changes mid-flight?","diagram":"flowchart TD\n  A[Client] --> B[KCNA API: setRetention]\n  B --> C[Replication Layer]\n  C --> D[Tombstone/Compact]\n  D --> E[Purge Complete]","difficulty":"intermediate","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","DoorDash"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T19:03:04.361Z","createdAt":"2026-01-26T19:03:04.361Z"},{"id":"q-7872","question":"In a beginner KCNA workflow with topics events, users, and audits, implement a TTL-based pruning stage in the consumer: when a key's latest event is older than 60 seconds, emit a prune signal for that key to a 'pruned' topic while preserving per-topic offsets and ensuring at-least-once delivery. Late events up to 10 seconds must still be processed. Describe the data model, TTL bookkeeping, watermark handling, and offset commit strategy, plus a minimal test harness including a canary scenario to verify pruning, reprocessing, and no message loss?","answer":"Implement a TTL prune in the consumer: track per-key latestTimestamp in a small state store (topic, partition, key). If currentTime - latestTimestamp exceeds TTL (60s), emit a prune record to the 'pru","explanation":"## Why This Is Asked\nThis checks practical TTL-based pruning with per-key state, offset semantics, and testability in a beginner KCNA setup.\n\n## Key Concepts\n- TTL-based pruning per key\n- Per-topic offsets and canary tests\n- Watermarks and late events handling\n- Idempotent prune writes and commit strategy\n\n## Code Example\n```javascript\n// Pseudo-code sketch showing state store, prune condition, and prune sink emit\n```\n\n## Follow-up Questions\n- How would you adjust TTL for skewed event-time vs processing-time?\n- How would you test recovery after consumer restart with preserved prune state?","diagram":null,"difficulty":"beginner","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Oracle","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T20:59:58.439Z","createdAt":"2026-01-26T20:59:58.439Z"},{"id":"q-7955","question":"KCNA Observability for Multi-Tenant Deployments: In a KCNA cluster serving Zoom and Discord tenants, design a per-tenant observability stack that (a) collects traces and metrics with tenant isolation, (b) implements per-tenant sampling and privacy filters, (c) supports cross-tenant correlation without data leaks, and (d) enables canary rollout/testing for new observability features. Include data model, sampling policy, storage plan, and a minimal test plan?","answer":"Use OpenTelemetry with tenantId as a resource attribute; implement per-tenant sampling (fixed-rate or adaptive) and redact sensitive fields; store traces/metrics in per-tenant indices with encryption ","explanation":"## Why This Is Asked\n\nObservability in a multi-tenant KCNA is essential for debugging, performance, and privacy compliance. This question probes practical patterns for tenant isolation, data minimization, and safe feature rollout.\n\n## Key Concepts\n\n- Per-tenant resource attributes and data partitions\n- Tenant-aware sampling and dynamic quotas\n- Privacy filters and data masking in traces/metrics\n- Cross-tenant correlation without data leakage\n- Canary rollouts with rollback criteria\n\n## Code Example\n\n```javascript\n// Pseudocode: tenant-aware sampling decision\nconst tenantRates = { zoom: 0.5, discord: 0.2 };\nfunction tenantSampler(tenantId) {\n  const rate = tenantRates[tenantId] ?? 0.1;\n  return new TraceIdRatioBased(rate);\n}\n```\n\n## Follow-up Questions\n\n- How would you test isolation under bursty traffic and ensure no cross-tenant data exposure?\n- What metrics would you surface to operators for canary feature health, and how would you roll back if privacy flags trigger issues?","diagram":"flowchart TD\n  A[Tenant: zoom] --> B[Instrument KCNA Producer/Consumer]\n  B --> C[Broker / Dispatcher]\n  C --> D[Per-tenant Storage (encrypted indices)]\n  D --> E[Observability Dashboard]\n  E --> F[Canary rollout control]","difficulty":"intermediate","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T02:39:52.669Z","createdAt":"2026-01-27T02:39:52.669Z"},{"id":"q-941","question":"Scenario: A global chat platform with 2B MAUs must detect policy-violating content (spam, hate speech) in near real-time while preserving user privacy and multilingual support. Propose an end-to-end pipeline: ingestion, moderation models (rules + ML), latency SLOs (1-2s), privacy safeguards, backpressure handling, retries, and dead-letter queues. Compare on-device vs cloud inference and monitoring?","answer":"Propose a tiered moderation pipeline: client-side tokenization + on-device classifier for first-pass filtering (multilingual light-weight model), with encrypted message IDs, then server-side streaming","explanation":"## Why This Is Asked\nThis question gauges real-time, scalable moderation design, privacy-safe multi-language handling, and the trade-offs between edge and cloud inference.\n\n## Key Concepts\n- Real-time streaming pipelines, SLOs, backpressure\n- Edge (on-device) vs cloud inference, multilingual models\n- Privacy safeguards (encryption, minimal data)\n- DLQ, retries, circuit breakers, monitoring\n\n## Code Example\n```javascript\n// Latency guard example\nif (latencyMs > 2000) {\n  tagAsSlowPath();\n  redirectToFallbackQueue();\n}\n```\n\n## Follow-up Questions\n- How would you validate model drift and false positives in production?\n- What metrics would you surface in dashboards to detect abuse transparently?","diagram":"flowchart TD\n  A[Ingest] --> B[Queue]\n  B --> C[Moderation]\n  C --> D[Enforce/Notify]\n  D --> E[Audit]","difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Slack","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T16:31:29.566Z","createdAt":"2026-01-12T16:31:29.566Z"}],"subChannels":["general"],"companies":["Adobe","Airbnb","Amazon","Anthropic","Apple","Bloomberg","Citadel","Cloudflare","Coinbase","Databricks","Discord","DoorDash","Goldman Sachs","Google","Hashicorp","Hugging Face","IBM","Instacart","LinkedIn","Lyft","Meta","Microsoft","MongoDB","NVIDIA","Netflix","Oracle","PayPal","Plaid","Robinhood","Salesforce","Scale Ai","Slack","Snap","Snowflake","Square","Stripe","Tesla","Twitter","Two Sigma","Uber","Zoom"],"stats":{"total":89,"beginner":19,"intermediate":32,"advanced":38,"newThisWeek":38}}