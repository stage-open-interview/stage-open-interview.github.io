{"questions":[{"id":"q-1138","question":"You are building a real-time KCNA feed service used by Snap, Meta, and Discord-scale clients to publish and deliver announcements across regions with sub-100ms tail latency. Describe the end-to-end architecture, data model for Announcement, ingestion and delivery pipeline, guarantees (at-least-once vs exactly-once), ordering, deduplication, failover, tests, and observability. How would you scale to 10k updates/sec with 99.999% uptime?","answer":"Design a real-time KCNA feed with an event-sourced pipeline: publish to a durable log (Kafka/Kinesis), process via idempotent services, store state in a scalable DB, and stream updates to regional cac","explanation":"## Why This Is Asked\n\nThis question probes system design at scale, covering data modeling, ingestion pipelines, delivery guarantees, ordering, and observability in a globally distributed, low-latency context.\n\n## Key Concepts\n\n- Event sourcing and durable logs (Kafka/Kinesis)\n- Idempotent processing and deduplication\n- Region-local vs global ordering\n- Delivery guarantees (at-least-once vs exactly-once)\n- Observability, testing, and chaos engineering\n\n## Code Example\n\n```javascript\n// Example: idempotent publish wrapper\nfunction publishAnnouncement(store, event) {\n  const id = event.id;\n  if (store.has(id)) return; // dedup\n  store.set(id, event);\n  // emit to downstream\n}\n```\n\n## Follow-up Questions\n\n- How would you design feature flags for regional rollouts and rollback strategies?\n- What monitoring and tracing would you implement to detect tail latency regressions?","diagram":"flowchart TD\n  Ingest[Ingest] --> Proc[Process]\n  Proc --> Ent[Event Store]\n  Ent --> Del[Delivery]\n  Del --> Client[Client]","difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Meta","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T01:26:25.116Z","createdAt":"2026-01-13T01:26:25.116Z"},{"id":"q-1295","question":"**KCNA Consumer Backpressure & Gap Handling**: In a beginner-friendly KCNA consumer, design a pull-based ingestion path that preserves per-topic offsets, guarantees at-least-once processing, and recovers from transient network slowdowns without duplicating messages. Describe the API shape, offset persistence, retry/backoff strategy, and a minimal test plan including a canary scenario?","answer":"Proposed answer (concise example): A pull-based consumer tracks per-topic offsets in a durable local store, commits after successful processing, and uses idempotent handlers. Retries use exponential b","explanation":"## Why This Is Asked\nThis question probes practical dataflow design for reliable at-least-once delivery and simple backpressure handling in a beginner context.\n\n## Key Concepts\n- Pull-based consumption with per-topic offsets\n- Idempotent processing and offset commits\n- Retry/backoff with jitter and restart replay\n- Canary and integration testing\n\n## Code Example\n```javascript\n// Pseudo API sketch\nclass KCNAConsumer {\n  constructor(store, process) { this.store=store; this.process=process }\n  async poll() { /* fetch by offset, call process, commit */ }\n  commit(offset) { this.store.save(offset) }\n}\n```\n\n## Follow-up Questions\n- How would you test exactly-once vs at-least-once boundaries in this setup?\n- How would you extend this to handle multiple topics with independent offsets?","diagram":null,"difficulty":"beginner","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Plaid","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T08:41:04.859Z","createdAt":"2026-01-13T08:41:04.859Z"},{"id":"q-1431","question":"KCNA cross-region, multi-tenant QoS: Propose an architecture and API for declaring per-tenant Topic SLAs (retention, max throughput) and a cross-region offset store with region-local commit logs. How would you implement per-tenant backpressure, quota enforcement, and exactly-once vs at-least-once semantics under regional outages? Include a concrete canary and testing plan?","answer":"Implement a quota ledger per tenant, topic-level SLA, region-local offsets, and a global commit log. Enforce producer backpressure by dynamically throttling when quotas near limit; ensure exactly-once","explanation":"## Why This Is Asked\nTests ability to design multi-region, multi-tenant KCNA with fair sharing, fault tolerance, and strong guarantees.\n\n## Key Concepts\n- Per-tenant QoS and SLAs\n- Cross-region offsets and commit log\n- Backpressure and quota enforcement\n- Exactly-once vs at-least-once tradeoffs\n- Canary testing and regional outages\n\n## Code Example\n```javascript\n// TS types for TopicSpec and QuotaLedger\ntype TopicSpec = { name:string; retentionMs:number; maxThroughputQps:number; tenant:string };\ntype QuotaLedger = Map<string, number>; // tenant -> remainingQuota\n```\n\n## Follow-up Questions\n- How would you test under bursty traffic and a regional partition?\n- How do you handle tenant migration between regions?\n","diagram":"flowchart TD\n  A[Tenant] --> B[Topic]\n  B --> C[Partition]\n  A --> D[QuotaLedger]\n  C --> E[OffsetsStore]\n  D --> F[ThrottleEngine]\n  F --> G[RegionalGateway]\n  G --> H[GlobalCommitLog]","difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Oracle","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T16:53:08.237Z","createdAt":"2026-01-13T16:53:08.237Z"},{"id":"q-1471","question":"KCNA multi-tenant schema-evolution: design a zero-downtime migration for a KCNA-based event bus used by many tenants where the Event schema evolves from v1 to v2 (add region field, deprecate payload wrapper). How do you enforce backward/forward compatibility, isolate tenants during migration, and validate with canaries? Include tooling, rollback plans, and observability?","answer":"Adopt a central versioned KCNA Schema Registry with per-tenant namespaces and per-topic compatibility. For v1→v2, add an optional region field and keep existing fields. Run a migrator that rewrites in","explanation":"## Why This Is Asked\n\nTests ability to design scalable, safe schema migrations in a multi-tenant KCNA setup, including compatibility strategies, canary rollout, and rollback handling.\n\n## Key Concepts\n\n- Schema Registry with versioned, per-tenant schemas\n- Backward, forward, and full compatibility modes\n- Canary deployments and per-tenant offset preservation\n- In-flight data migration and rollback plans\n\n## Code Example\n\n```javascript\nfunction isBackwardCompatible(oldSchema, newSchema) {\n  // Added fields must be optional; no required removals\n  // No type-breaking changes\n  return true;\n}\n```\n\n## Follow-up Questions\n\n- How would you monitor for schema drift and consumer failures during migration?\n- What metrics and alerts would you add to ensure safe rollback timing?","diagram":"flowchart TD\n  A[Producer] --> B[KCNA Topic]\n  B --> C[Schema Registry]\n  C --> D[Versioned Schemas]\n  D --> E[Migration Orchestrator]\n  E --> F[Canary Tenants]\n  F --> G[Rollout to All Tenants]","difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Stripe","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T18:46:26.092Z","createdAt":"2026-01-13T18:46:26.092Z"},{"id":"q-1503","question":"KCNA cross-region tenancy isolation: design a replication topology where tenants' streams stay regional unless opted into global analytics; implement per-tenant topic partitioning, region-aware routing, and idempotent retries with de-dup. How do you enforce locality, protect privacy in cross-region analytics, and handle failover/lag? Include concrete configs, testing strategy, and rollback plan?","answer":"Use region-local KCNA clusters with tenant-scoped partitions and a policy gate for opt-in analytics. Route data by tenant region, avoid cross-region replicas unless flagged, and apply idempotent produ","explanation":"## Why This Is Asked\nTo examine practical cross-region data locality, tenancy boundaries, and privacy-preserving analytics with KCNA, plus testing/rollback discipline.\n\n## Key Concepts\n- Region-local clusters\n- Per-tenant partitions and offsets\n- Opt-in analytics gating\n- Idempotent producers/consumers\n- Observability and rollback\n\n## Code Example\n```javascript\n// Example policy snippet\nconst policy = {\n  tenants: {\n    A: { region: 'us-east-1', analytics: false },\n    B: { region: 'eu-west-1', analytics: true }\n  },\n  globalAnalyticsEnabled: true\n}\n```\n\n## Follow-up Questions\n- How do you monitor cross-region privacy boundaries and lag?\n- What tests would you run to validate failover without impacting tenants?","diagram":"flowchart TD\n  A[Tenant streams] --> B[Region-local KCNA]\n  B --> C[Policy gate]\n  C --> D[Global analytics (optional)]\n  D --> E[Sinks]","difficulty":"intermediate","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T19:40:00.393Z","createdAt":"2026-01-13T19:40:00.393Z"},{"id":"q-1520","question":"KCNA TTL Retention: design a per-topic TTL policy for KCNA events. How would you store TTL metadata, drop expired events without breaking consumer offsets, handle late-arriving events post-expiry, and observe/verify with canary tests? Provide a minimal API shape for setting TTL per topic, a compact storage layout, and a lightweight cleanup workflow?","answer":"Store per-topic TTL in a lightweight index: topic -> expiry epoch. Each message carries publishTime; a background cleaner deletes messages older than TTL while advancing a deletion frontier to preserv","explanation":"## Why This Is Asked\nThis tests understanding of retention, per-topic configuration, and safe cleanup without disturbing consumer progress.\n\n## Key Concepts\n- TTL metadata storage per topic\n- Deletion frontier and per-topic offsets\n- Late-arriving vs expired events\n- Observability and canary validation\n\n## Code Example\n```javascript\n// Pseudo TTL check\nfunction isExpired(msg, ttlSec, now=new Date()) { return (now.getTime() - msg.publishTime) > ttlSec*1000; }\n```\n\n## Follow-up Questions\n- How would you test TTL edge cases (exact expiry, late arrival)?\n- How do you prevent TTL cleanup from blocking high-priority topics? ","diagram":"flowchart TD\n  A[Topic TTL Policy] --> B[Cleanup Job]\n  B --> C[Expire Messages]\n  C --> D[Update Frontiers]\n  D --> E[Offets Consistent Delivery]","difficulty":"beginner","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Citadel","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T20:38:42.651Z","createdAt":"2026-01-13T20:38:42.651Z"},{"id":"q-1602","question":"KCNA key management & tenant isolation: In a **KCNA**-based multi-tenant event bus, each tenant uses a per-tenant envelope encryption key managed by a centralized **KMS**. Design a **zero-downtime** key rotation workflow that rotates tenant keys without breaking consumption, re-encrypts in-flight payloads, and prevents cross-tenant leakage. Include data model (**tenant_id**, key_version, wrapped_key), rollout strategy, rollback, and observability?","answer":"Implement envelope encryption with per-tenant keys managed by a centralized KMS. Each message includes key_version in metadata for decryption. During rotation, create a new key version, publish rotation tokens to consumers, and encrypt new messages under the new version while maintaining backward compatibility for existing in-flight messages.","explanation":"## Why This Is Asked\nTests practical handling of per-tenant security in KCNA, emphasizing zero-downtime rotation, data integrity, and cross-tenant isolation.\n\n## Key Concepts\n- Envelope encryption and per-tenant KMS key versions\n- In-flight data re-encryption and backward compatibility\n- Rollout strategies, canaries, and observability/audit trails\n\n## Code Example\n```javascript\n// Pseudo: resolve key for decryption\nfunction resolveKey(tenantId, payload) {\n  const v = fetchKeyVersion(tenantId, payload);\n  return kms.getKey(tenantId, v);\n}\n```\n\n## Follow-up Questions\n- How to revoke compromised tenant keys?\n- What monitoring metrics indicate successful rotation?\n- How to handle consumers that miss rotation tokens?","diagram":"flowchart TD\n  A[Rotation Initiation] --> B[Publish Metadata]\n  B --> C[Clients Fetch New Key Version]\n  C --> D[Re-encrypt New Messages]\n  D --> E[Grace Window for In-Flight Messages]\n  E --> F[Promote New Key Version]\n  F --> G[Observability & Audit]\n  G --> H[Validation Canary]","difficulty":"intermediate","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Microsoft","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T05:32:04.492Z","createdAt":"2026-01-14T02:31:03.848Z"},{"id":"q-1698","question":"Design a KCNA privacy-first feed where tenants specify a region (US/EU/APAC) and all data remains local. Use per-tenant envelope encryption with KMS, region-scoped brokers, field-level redaction before delivery, and tenant-aware access controls. Add audit trails and canary tests to prove zero cross-tenant leakage, correct redaction, and retention under burst load?","answer":"Design a KCNA privacy-first feed where tenants specify a region (US/EU/APAC) and all data remains local. Use per-tenant envelope encryption with KMS, region-scoped brokers, field-level redaction befor","explanation":"## Why This Is Asked\n\nThis question probes practical privacy-by-design in streaming: data residency, per-tenant cryptography, access control, auditing, and testability under burst traffic. It also checks operational thinking for cross-tenant isolation and retention policies.\n\n## Key Concepts\n\n- Per-tenant residency and tenancy isolation\n- Envelope encryption with KMS and tenant-scoped keys\n- Field-level redaction during transit and at rest\n- Region routing and data locality guarantees\n- Auditing, retention, and compliance controls\n- Canary-based validation under burst load and failure scenarios\n\n## Code Example\n\n```javascript\nfunction redact(record, schema) {\n  const redacted = {};\n  for (const [k, v] of Object.entries(record)) {\n    if (schema.redact?.includes(k)) redacted[k] = \"***\";\n    else redacted[k] = v;\n  }\n  return redacted;\n}\n```\n\n## Follow-up Questions\n\n- How would you test a migration that adds a new redaction rule without breaking existing tenants?\n- How would you verify cross-region data leakage is impossible during network partitions?\n","diagram":"flowchart TD\n  A[Tenant Config] --> B[Regional Router]\n  B --> C[KCNA Regional Broker]\n  C --> D[Envelope Encrypt with Tenant Key]\n  D --> E[Deliver to Region-Specific Client]","difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Tesla","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T07:34:51.806Z","createdAt":"2026-01-14T07:34:51.808Z"},{"id":"q-1723","question":"KCNA dynamic tenancy fairness under bursty workloads: design a per-tenant ingestion path with token-bucket quotas and fair queuing; detail API surface, quota persistence, backpressure signaling, and dynamic rebalancing from telemetry. How do you validate isolation under burst traffic, and what would your canary rollout look like?","answer":"Implement per-tenant KCNA ingestion using token-bucket quotas and a fair-queuing layer. API: POST /ingest with tenantId, topic, payload; on over-quota return 429 with Retry-After. Use telemetry-driven","explanation":"## Why This Is Asked\n\nAssess the candidate's ability to design per-tenant fairness in a high-throughput KCNA pipeline, balancing isolation, latency, and dynamic scaling under bursty traffic.\n\n## Key Concepts\n\n- Per-tenant token-bucket quotas and fair queuing\n- Backpressure signaling (429s, retry-after) and quota persistence\n- Telemetry-driven dynamic rebalancing and burst forgiveness\n- Canary-based validation and observability\n\n## Code Example\n\n```javascript\nclass TokenBucket {\n  constructor(rate, capacity) {\n    this.rate = rate;\n    this.capacity = capacity;\n    this.tokens = capacity;\n    this.last = Date.now();\n  }\n  tryConsume(n = 1) {\n    const now = Date.now();\n    const elapsed = (now - this.last) / 1000;\n    this.tokens = Math.min(this.capacity, this.tokens + elapsed * this.rate);\n    this.last = now;\n    if (this.tokens >= n) {\n      this.tokens -= n;\n      return true;\n    }\n    return false;\n  }\n}\n```\n\n## Follow-up Questions\n\n- How would you test dynamic quota rebalancing under tenant churn?\n- How do you prevent misbehaving tenants from starving others while preserving low latency?","diagram":null,"difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T08:41:19.803Z","createdAt":"2026-01-14T08:41:19.803Z"},{"id":"q-1806","question":"KCNA privacy-by-design: design a tenant-isolated KCNA ingestion and delivery path that enforces per-tenant encryption keys for in-flight and at-rest data, supports on-the-fly key rotation with zero-downtime, and provides auditable access controls for analytic consumers. Describe the KMS integration, key-wrapping strategy, performance impact, and a minimal canary test for rotation?","answer":"Per-tenant envelope encryption: each tenant has a data-key wrapped by KMS; producers encrypt payloads with tenant keys and rotate data-keys via versioned IDs with on-the-fly rewrapping of in-flight en","explanation":"## Why This Is Asked\\nPrivacy-first multi-tenant KCNA is a real challenge; rotation and audit are painful at scale.\\n\\n## Key Concepts\\n- Envelope encryption\\n- Per-tenant KMS keys\\n- On-the-fly rotation with no downtime\\n- Audit trails and access control\\n- Backward compatibility with legacy envelopes\\n- Canary tests for rotation\\n\\n## Code Example\\n```javascript\\n// Pseudo: envelope encrypt data per tenant\\nfunction encryptForTenant(tenantId, plaintext, version) {\\n  const keyId = kms.getKeyId(tenantId, version);\\n  const dataKey = kms.generateDataKey(tenantId, version);\\n  const ciphertext = crypto.aesGcmEncrypt(dataKey.plaintext, plaintext);\\n  const envelope = { keyId, cipher: ciphertext, iv: dataKey.iv };\\n  return envelope;\\n}\\n```\\n\\n## Follow-up Questions\\n- How to handle key compromise and revocation?\\n- How to measure rotation latency and impact on throughput?\\n- How to ensure consistent decryption across hot/cold storage?","diagram":null,"difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Robinhood","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T11:41:32.131Z","createdAt":"2026-01-14T11:41:32.131Z"},{"id":"q-1901","question":"KCNA Time-Travel Replay for Tenants: Suppose a tenant needs to audit a precise 2-hour window of events without halting live ingestion. Design a tenant-scoped time-travel replay feature in KCNA that allows replaying events from a given timestamp while live streams continue, guarantees exactly-once delivery to downstream analytics, and preserves per-tenant offsets. Describe API shapes, storage layout, consistency guarantees, security controls, and a minimal test plan (canaries)?","answer":"Implement a tenant-scoped replay plane that materializes a time-indexed replay log per tenant starting at given timestamp T. Downstream analytics subscribe to a replay stream with idempotent consumers","explanation":"## Why This Is Asked\nThe question probes how to add time-travel auditing without impacting live flow, focusing on idempotency, isolation, and operational safety in KCNA.\n\n## Key Concepts\n- Time-indexed replay per tenant\n- Tenant isolation and per-tenant offsets\n- Idempotent downstream delivery and replay-window semantics\n- Access control, auditing, observability\n- Canary-driven rollout and rollback\n\n## Code Example\n```javascript\n// Minimal API surface for replay\ninterface ReplayOptions { tenantId: string; startTs: number; endTs?: number; mode?: 'replay'|'live'; }\nfunction startTenantReplay(opts: ReplayOptions): Promise<ReplaySession>;\n```\n\n## Follow-up Questions\n- How would you ensure exactly-once semantics across distributed replay streams?\n- How would you monitor replay impact on backpressure and SLAs?\n","diagram":"flowchart TD\n  A(Tenant) --> B(Replay Plane)\n  B --> C(Replay Log)\n  A --> D(Live Ingestion)\n  C --> E(Analytics)\n  D --> E","difficulty":"intermediate","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Meta","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T16:45:56.511Z","createdAt":"2026-01-14T16:45:56.511Z"},{"id":"q-1926","question":"Design a beginner-friendly KCNA feature: tenant-scoped data TTL. Each tenant can configure a TTL (e.g., 24h) for their streams. Describe the API (setTTL on a topic with ttlMs), how per-message metadata is stored, how a background purge runs safely without breaking consumers, and a minimal test plan with canaries?","answer":"Implement per-tenant TTL with an API like POST /tenants/{id}/topics/{topic}/ttl { ttlMs }. Attach a ttlMs timestamp to each message and run a purge daemon that deletes messages older than TTL while em","explanation":"## Why This Is Asked\nThis question tests understanding of per-tenant data isolation, retention controls, and safe data purging in a live streaming system. It requires concrete API shape, data model decisions, and a practical testing strategy suitable for beginners while exposing real trade-offs.\n\n## Key Concepts\n- Tenant-scoped retention policy and TTL\n- Message metadata and tombstone semantics for replay\n- Purge safety relative to consumer offsets\n- Canary-based testing and end-to-end validation\n\n## Code Example\n```javascript\n// Pseudo: set TTL for a tenant-topic\nasync function setTTL(tenantId, topic, ttlMs) {\n  // persist TTL in config store per tenant-topic\n  await configStore.set(`/tenants/${tenantId}/topics/${topic}/ttl`, ttlMs);\n}\n\n// Pseudo: purge loop (simplified)\nfunction purgeOldMessages(tenantId, topic) {\n  const ttlMs = configStore.get(`/tenants/${tenantId}/topics/${topic}/ttl`);\n  const cutoff = Date.now() - ttlMs;\n  for (const msg of storage.scan(tenantId, topic)) {\n    if (msg.timestamp < cutoff) {\n      storage.delete(msg.id);\n      // emit tombstone to preserve replay semantics\n      storage.appendTombstone({ tenantId, topic, id: msg.id });\n    }\n  }\n}\n```\n\n## Follow-up Questions\n- How would you handle TTL changes mid-flight without losing data consistency?\n- How do you verify no live consumers are affected during purge windows?","diagram":null,"difficulty":"beginner","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T17:40:54.254Z","createdAt":"2026-01-14T17:40:54.254Z"},{"id":"q-2025","question":"KCNA ingest fairness at scale: design a per-tenant fair-queuing strategy for bursty producers in a multi-tenant KCNA channel. Implement a two-layer approach with a per-tenant token-bucket at the gateway and a global weighted-round-robin scheduler across tenants to prevent starvation. Define quotas, bounded bursts, and backpressure signaling; outline API contracts, config knobs, and a test plan with synthetic tenants and canaries?","answer":"Two-layer fairness: per-tenant token-bucket at gateway plus a global weighted RR scheduler across tenants. Enforce quotas and bounded bursts; unutilized tokens spill to a pending queue to avoid jitter","explanation":"## Why This Is Asked\nInterviews real-world scaling: fairness in multi-tenant KCNA ingestion under bursty traffic, preventing starvation and ensuring predictable latency for all tenants.\n\n## Key Concepts\n- Per-tenant quotas and bounded bursts\n- Gateways and global scheduling\n- Backpressure signaling and observability\n- Canary-style validation\n\n## Code Example\n```javascript\nclass TokenBucket {\n  constructor(rate, burst) { this.rate = rate; this.burst = burst; this.tokens = burst; this.last = Date.now(); }\n  allow(n=1){ this._drip(); if(this.tokens>=n){ this.tokens-=n; return true; } return false; }\n  _drip(){ const now=Date.now(); const elapsed=(now-this.last)/1000; this.tokens = Math.min(this.burst, this.tokens + elapsed*this.rate); this.last=now; }\n}\n```\n\n## Follow-up Questions\n- How would you monitor fairness and detect starvation?\n- How would quotas adapt during traffic spikes?","diagram":"flowchart TD\n  IngestRequest --> QuotaCheck\n  QuotaCheck -->|Allowed| GatewayQueue\n  GatewayQueue --> KCNA_Core\n  KCNA_Core --> Ack\n  KCNA_Core --> Backpressure\n","difficulty":"intermediate","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Hugging Face","IBM"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T21:33:52.678Z","createdAt":"2026-01-14T21:33:52.679Z"},{"id":"q-2101","question":"KCNA policy-driven tenant isolation: design a per-tenant access-control mechanism at the gateway and stream processors that enforces tenant-scoped authorization for ingest and delivery, supports dynamic policy updates with zero-downtime rollout, and provides an auditable per-event trail. Outline the policy model (tenants, roles, actions), integration with a policy engine (e.g., OPA/ABAC), JWT-based identity, and testing strategy including canaries and rollback?","answer":"Design a policy-driven gateway layer using ABAC with a central policy store (OPA). Each KCNA client presents a JWT containing tenant_id and roles; gateways enforce actions (ingest, read, admin) per resource. Stream processors inherit tenant context from the gateway, maintaining isolation throughout the pipeline. Policy updates are versioned and rolled out via canary deployments with automatic rollback on failure. All events are logged with tenant, action, and policy version for complete audit trails.","explanation":"## Why This Is Asked\nTo probe real-world policy, security, and reliability trade-offs in KCNA at scale.\n\n## Key Concepts\n- ABAC with tenant_id and roles\n- Central policy store (OPA)\n- JWT-based identity\n- Policy versioning and canary rollouts\n- Per-event auditing\n\n## Code Example\n```javascript\n// Pseudo gateway policy check\nconst ok = evalPolicy({tenant_id, action, resource});\nif (!ok) throw new Error('Access denied');\n```\n\n## Follow-up Questions\n- How would you test policy upgrades without impacting live tenants?\n- What guarantees exist for audit log integrity during rollback?","diagram":null,"difficulty":"intermediate","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","NVIDIA","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T05:00:32.575Z","createdAt":"2026-01-15T02:16:01.304Z"},{"id":"q-2135","question":"KCNA Dead-Letter & Poison Message Handling: For a multi-tenant KCNA deployment, design a per-tenant dead-letter queue strategy that routes malformed events out of the normal path, enforces per-tenant retry budgets, and preserves idempotent replays. Describe DLQ schema, routing rules, retention, operator visibility, and a safe rollout with canaries?","answer":"Route malformed events to a per-tenant DLQ and enforce per-tenant retry budgets. Use dedicated DLQ topics (dlq/tenant-{id}) with event_id and reason. Apply per-tenant retry cap (e.g., 3 attempts/hour)","explanation":"## Why This Is Asked\nThis checks robustness of error handling, tenant isolation, and rollout discipline in multi-tenant KCNA systems.\n\n## Key Concepts\n- Per-tenant DLQ routing and isolation\n- Retry budgeting and backoff strategy\n- Idempotent replays and deduplication\n- Retention, auditing, and operator visibility\n- Canary rollout and rollback plans\n\n## Code Example\n```javascript\n// Pseudo routing snippet\nif (!isValid(event)) {\n  const dlqTopic = `dlq/tenant-${event.tenant_id}`;\n  publish(dlqTopic, { event_id: event.id, tenant_id: event.tenant_id, reason: 'validation_error' });\n} else {\n  routeToNormalPath(event);\n}\n```\n\n## Follow-up Questions\n- How would you test DLQ behavior under bursty tenants and ensure no data leakage?\n- How would you monitor DLQ health and automate cleanup without affecting active tenants?","diagram":"flowchart TD\n  Gateway[Gateway] --> Validator[Validation Stage]\n  Validator -- malformed --> DLQ_Tenant[DLQ per tenant]\n  Validator -- good --> Ingest[Ingestion Path]\n  DLQ_Tenant --> Replay[Reingest / Replay Path]","difficulty":"intermediate","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Instacart","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T04:17:33.369Z","createdAt":"2026-01-15T04:17:33.369Z"},{"id":"q-2165","question":"KCNA observability & tenant-scoped tracing: design end-to-end observability for a multi-tenant KCNA event bus under burst traffic, preserving tenant data isolation while enabling operations debugging. Specify: how to propagate tenant_id and correlation_id across producer, gateway, and consumer; per-tenant metrics and alerting; privacy-preserving trace UI that exposes only metadata; retention, RBAC, and failure-mode testing with canaries?","answer":"Propose OpenTelemetry traces carrying tenant_id and correlation_id across producer, gateway, and consumer; add per-tenant metrics in Prometheus and alerting rules; redact payload data in traces and st","explanation":"Why This Is Asked\n\nTests ability to design scalable, tenant-aware observability for a high-throughput KCNA setup, balancing debugging needs with data isolation and privacy.\n\nKey Concepts\n\n- Distributed tracing with tenant context\n- Data minimization and redaction\n- Per-tenant metrics and alerting\n- RBAC and topic-level access\n- Canary rollout for instrumentation\n\nCode Example\n\n```javascript\n// Minimal tracing snippet showing span with tenant and correlation attributes\nconst { trace } = require('@opentelemetry/api');\nconst tracer = trace.getTracer('kcna');\nfunction emitEvent(tenantId, correlationId, evt) {\n  const span = tracer.startSpan('emitEvent', { attributes: { tenantId, correlationId, action: 'emit' }});\n  // ... emit logic\n  span.end();\n}\n```\n\nFollow-up Questions\n\n- How would you test privacy controls (redaction, audience limits) in traces and verify no cross-tenant leakage?\n- What monitoring and retention policies would you apply to balance cost and debugging fidelity?","diagram":"flowchart TD\n  A[Producer] --> B[Gateway]\n  B --> C[Consumer]\n  subgraph Tenant Isolation\n    D1[Trace with tenant_id] --> D2[Masked Payload]\n  end","difficulty":"intermediate","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Goldman Sachs","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T05:42:53.445Z","createdAt":"2026-01-15T05:42:53.445Z"},{"id":"q-2265","question":"Design a KCNA-based privacy-preserving cross-tenant analytics pipeline where tenants publish raw events but analysts receive aggregated metrics only. Tenants can opt into regional sharing with differential privacy, per-tenant encryption keys, and auditable data lineage. Describe topology, data model, access control, latency targets, and a canary rollout plan?","answer":"Isolate tenants with per-tenant KCNA namespaces; a DP-enricher subscribes to raw streams, emits ε-DP metrics to a guarded analytics topic; use tenant-scoped keys for at-rest/in-transit encryption; enf","explanation":"## Why This Is Asked\nTests ability to design privacy-preserving cross-tenant analytics with real-world constraints and audits.\n\n## Key Concepts\n- Tenant isolation via namespaces\n- Differential privacy budgets per region\n- Tenant-key management and encryption\n- Auditing and data lineage\n- Canary rollout in multi-region setup\n\n## Code Example\n```javascript\nfunction applyDP(data, epsilon) {\n  // placeholder DP mechanism\n  return data.map(x => x + (Math.random() < epsilon ? 0 : 0));\n}\n```\n\n## Follow-up Questions\n- How would you monitor DP budget exhaustion across regions?\n- How would you validate no PII leakage during audits?\n- How would revoking a tenant key affect ongoing streams?","diagram":"flowchart TD\n  P[Tenant Publisher] --> R[KCNA Raw Topic]\n  R --> E[DP Enricher]\n  E --> A[Analytics Output]\n  A --> L[Audit Logs]","difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Netflix","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T09:48:36.908Z","createdAt":"2026-01-15T09:48:36.908Z"},{"id":"q-2311","question":"KCNA Dead-Letter Queue (DLQ): For a beginner-friendly KCNA, design a per-tenant DLQ strategy for messages that fail processing after N retries. Describe API shape to move from main topic to DLQ, retention, how to reprocess, and a simple test canary that demonstrates DLQ routing, backoff between retries, and alerting?","answer":"Per-tenant DLQ should route failed messages to a dedicated DLQ (per tenant or with tenant_id in payload), store original topic and offset, and apply maxRetries with exponential backoff. Include a repr","explanation":"## Why This Is Asked\n\nThis question probes practical handling of failed messages in a multi-tenant KCNA, a common production need that beginners can implement with simple routing, retry/backoff, and reprocessing.\n\n## Key Concepts\n\n- Dead-letter queues per tenant\n- Failure isolation and offset preservation\n- Backoff and max retries\n- Safe reprocessing and idempotence\n- Observability and alerts\n\n## Code Example\n\n```javascript\n// Minimal DLQ routing sketch\nfunction routeToDLQ(event, error){\n  return {tenantId: event.tenantId, originalTopic: event.topic, offset: event.offset, payload: event.payload, error};\n}\n```\n\n## Follow-up Questions\n\n- How to test DLQ under burst failures? \n- How to monitor DLQ latency and replay health?","diagram":null,"difficulty":"beginner","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T11:39:54.578Z","createdAt":"2026-01-15T11:39:54.578Z"},{"id":"q-2425","question":"KCNA per-tenant feature flags: design a control plane to selectively enable a new routing path and compression for KCNA streams, with zero-downtime rollout, tenant-scoped rollback, and audit logs; how would you model toggles, propagate config, implement canaries, and validate impact before full enablement?","answer":"Design a per-tenant feature flag system with a central, strongly consistent store mapping tenantId -> {flags}. Evaluate flags at publish and routing points, ensure idempotent updates, and use canaries","explanation":"## Why This Is Asked\nAssess the ability to design an operational control plane for multi-tenant streaming systems, including per-tenant configurability and safe deployment.\n\n## Key Concepts\n- Feature flags at scale for multi-tenant KCNA\n- Centralized per-tenant config store and propagation\n- Canary rollout gates and safety checks\n- Audit logging and rollback mechanisms\n\n## Code Example\n```javascript\n// Pseudo-code: evaluate if a flag is enabled for a tenant\nfunction isFlagEnabled(tenantId, flagName, defaultVal=false){\n  const cfg = fetchTenantConfig(tenantId); // central store (etcd/kv)\n  return cfg?.flags?.[flagName] ?? defaultVal;\n}\n```\n\n## Follow-up Questions\n- How would you test rollouts with synthetic tenants and simulate partial failures?\n- What observability metrics and dashboards would you add to detect misconfigurations quickly?","diagram":null,"difficulty":"intermediate","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T17:45:49.250Z","createdAt":"2026-01-15T17:45:49.250Z"},{"id":"q-2464","question":"KCNA retention governance: design a per-tenant data lifecycle in KCNA that enforces tenant-specific retention windows, supports legal holds, and runs background tombstone-based deletions with GC across partitions. Explain metadata storage, purge triggers without breaking at-least-once semantics, observability, and a rollback/hold-release workflow?","answer":"Per-tenant retention is stored in a policy store keyed by tenant, topic, and region. Purges enqueue tombstones; GC runs in background with per-tenant quotas to avoid starving in-flight consumers. Lega","explanation":"## Why This Is Asked\nTenants require governance over data lifetime, intersecting privacy and multi-tenant isolation. This tests policy stores, scalable purge, and safe deletion while preserving streaming guarantees.\n\n## Key Concepts\n- Per-tenant retention policy: duration, start, scope\n- Tombstones vs. physical deletion; GC pacing\n- Legal holds: hold flag, release, audit trail\n- Observability: metrics, dashboards, alerts\n- Rollback strategy: canary purge, replay hooks\n\n## Code Example\n```javascript\n// Pseudo-implementation sketch\nclass RetentionPolicy { constructor(tenant, topic, retentionMs, hold) { ... } }\nfunction enqueueTombstone(tenant, topic, offset) { ... }\nfunction runPurgeCycle() { ... } // respects quotas and in-flight reads\n```\n\n## Follow-up Questions\n- How would you test legal-hold behavior across tenants at scale?\n- What changes would you make to ensure cross-tenant isolation during purge and tombstone propagation?","diagram":null,"difficulty":"intermediate","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T19:05:08.222Z","createdAt":"2026-01-15T19:05:08.223Z"},{"id":"q-2471","question":"KCNA security and governance: design a per-tenant envelope encryption scheme for KCNA payloads using a central KMS. Each tenant has a dedicated DEK wrapped by a tenant-specific KEK. Encrypt payloads at produce time and decrypt only at authorized consumers with per-tenant IAM. Support per-tenant key rotation with zero-downtime re-encryption, and maintain per-tenant audit trails and deletion rules that respect retention. How would you implement lifecycle, performance trade-offs, and backward-compatibility?","answer":"Implement envelope encryption: assign each tenant a unique DEK, wrapped by a tenant KEK from a central KMS. Encrypt payloads at producer side; decrypt only with tenant IAM. Support per-tenant key rota","explanation":"## Why This Is Asked\nReal-world KCNA deployments must protect data across tenants; this question probes envelope encryption design, key lifecycle, and how to audit and delete data without breaking consistency.\n\n## Key Concepts\n- Envelope encryption\n- Per-tenant KEK/DEK lifecycle\n- Key rotation with zero-downtime re-encryption\n- Auditability and tenant-scoped deletion\n\n## Code Example\n```javascript\n// Pseudocode for decrypting payload using tenant DEK\nconst wrappedDek = metadata.getTenantWrappedDek(tenantId);\nconst dek = kms.unwrapDek(tenantId, wrappedDek);\nconst plaintext = crypto.decrypt(payload, dek);\n```\n\n## Follow-up Questions\n- How would you test key rotation without service disruption?\n- How do you enforce least-privilege access per tenant across decryptors?","diagram":null,"difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Goldman Sachs","LinkedIn"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T19:34:50.218Z","createdAt":"2026-01-15T19:34:50.219Z"},{"id":"q-2510","question":"KCNA tamper-evident audit trails: design per-tenant verifiable logs for event streams using append-only shards and Merkle proofs, with per-batch signing and external audit proofs. Outline data structures, key rotation, canary rollout, rollback, and a test plan that proves tamper-resistance without leaking tenant data?","answer":"Design a per-tenant verifiable audit trail for KCNA events using append-only shards, per-tenant hashes, and Merkle proofs. Each batch is hashed, signed, and stored in a tamper-evident ledger; auditors","explanation":"## Why This Is Asked\nIn multi-tenant KCNA deployments, tamper-evident audit trails enable compliance, forensics, and governance without sacrificing performance.\n\n## Key Concepts\n- Append-only per-tenant logs and digest chaining\n- Merkle proofs for cross-checks and tamper evidence\n- Cryptographic signing and secure key rotation\n- Canary-driven rollout and safe rollback\n\n## Code Example\n```javascript\nfunction verifyBatch(batch, root) {\n  const digest = hash(batch);\n  const proof = getMerkleProof(batch.id);\n  return Merkle.verify(digest, proof, root);\n}\n```\n\n## Follow-up Questions\n- How to scale verification and audit dashboards?\n- How to handle key compromise and revocation?","diagram":null,"difficulty":"intermediate","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Cloudflare","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T20:53:35.476Z","createdAt":"2026-01-15T20:53:35.476Z"},{"id":"q-2565","question":"KCNA per-tenant data isolation with confidential payloads: in a single KCNA cluster serving multiple tenants, design a mechanism to store tenant-scoped payloads with per-tenant encryption keys, support cross-tenant analytics only if opted-in, ensure query isolation, key rotation, and audit trails. Describe API contracts, key management, and performance implications?","answer":"Design a comprehensive per-tenant data isolation model for KCNA that enables secure multi-tenancy within a single cluster. Implement envelope encryption using tenant-specific keys managed through an external KMS, enforce field-level redaction for cross-tenant analytics when opted-in, maintain strict query isolation through namespace-based access controls, support automated key rotation with zero-downtime migration, and provide tamper-evident audit trails for all data access operations.","explanation":"## Why This Is Asked\n\nThis question evaluates the ability to design sophisticated data isolation architectures within KCNA for enterprise multi-tenant scenarios, specifically addressing encryption lifecycle management, query isolation, and auditability requirements.\n\n## Key Concepts\n\n- Envelope encryption with per-tenant key management\n- KMS integration and automated key rotation\n- Namespace-based access control and query isolation\n- Field-level redaction for cross-tenant analytics\n- Comprehensive audit trails and tamper-evidence mechanisms\n\n## Code Example\n\n```javascript\n// Pseudo example demonstrating tenant-specific key lookup and decryption\nfunction decryptForTenant(tenantId, ciphertext) {\n  const tenantKey = kms.getTenantKey(tenantId);\n  return crypto.decryptWithEnvelope(tenantKey, ciphertext);\n}\n```\n\n## Follow-up Questions\n\n- How would you handle tenant key rotation with zero downtime?\n- What strategies would you implement for cross-tenant analytics performance?\n- How do you ensure audit trail integrity in a distributed environment?","diagram":null,"difficulty":"intermediate","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Microsoft","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T05:22:01.017Z","createdAt":"2026-01-15T22:54:27.086Z"},{"id":"q-2582","question":"In KCNA, design a per-tenant event-time processing layer that tolerates late events within a tenant-defined latency budget, computes per-tenant 5-minute tumbling window aggregations, and guarantees at-least-once delivery. Describe per-tenant watermarks, state partitioning, late-data handling, fault tolerance, and a test plan to validate SLA compliance?","answer":"The implementation leverages per-tenant keyed streams with independent tenant-specific watermarks, backed by a 5-minute tumbling window aggregator utilizing per-tenant RocksDB state stores. Late events arriving within the tenant-defined latency budget are merged into the current active window, while events exceeding the budget are routed to a dead-letter queue for audit and manual review. State is partitioned by tenant ID with periodic checkpointing to durable storage for fault tolerance, and at-least-once delivery semantics are achieved through idempotent processing and transactional state updates.","explanation":"## Why This Is Asked\n\nThis question evaluates expertise in per-tenant event-time semantics, SLA-bound late data handling, and scalable state management in KCNA under multi-tenant workloads.\n\n## Key Concepts\n\n- Event-time processing with tenant-specific watermarks\n- Per-tenant state stores and partitioning strategies\n- Late data handling within SLA constraints\n- Fault tolerance through checkpointing and recovery\n- At-least-once delivery guarantees\n\n## Code Example\n\n```javascript\n// Pseudocode: per-tenant watermark advancement and late event routing\nclass TenantWindowManager {\n  constructor() {\n    this.watermark = -Infinity;\n    this.windows = new Map();\n  }\n  \n  onEvent(tenantId, event) {\n    const isLate = event.timestamp < this.watermark;\n    if (isLate) {\n      this.routeToDeadLetterQueue(tenantId, event);\n    } else {\n      this.mergeIntoActiveWindow(tenantId, event);\n    }\n  }\n}\n```\n\n## Follow-up Questions\n\n- How would you handle backpressure when tenants have varying event volumes?\n- What strategies would you implement to optimize state store compaction?\n- How do you ensure watermark fairness across tenants with different latency budgets?","diagram":"flowchart TD\n  A[KCNA Ingest] --> B[Per-Tenant Window Manager]\n  B --> C[State Store: per-tenant]\n  B --> D[Late Data Route]\n  A --> E[Watermark Propagation]","difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Goldman Sachs","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T05:13:48.636Z","createdAt":"2026-01-15T23:41:57.674Z"},{"id":"q-2621","question":"In KCNA, design a per-tenant field-level access control and masking layer that operates in real time on streaming payloads for analytics, ensuring authorized tenants can decrypt unmasked fields while unauthorized tenants only see masked data, without breaking at-least-once semantics or replay safety. Explain data structures, policy evaluation, KMS key management, and testing with canaries?","answer":"Per-tenant masking layer with policy-as-code, envelope encryption per-tenant KEK from a KMS, and visibility tokens. Ingested events carry tenant + ACLs; sensitive fields are encrypted and re-wrapped f","explanation":"## Why This Is Asked\n\nReal-time masking with per-tenant controls is a practical, security-critical feature at scale; tests edge cases like key rotation and replay.\n\n## Key Concepts\n\n- Field-level access control\n- Policy-as-code\n- Envelope encryption and KEKs\n- Replay safety and exactly-once\n- Key rotation and auditing\n\n## Code Example\n\n```javascript\n// Pseudo-code for policy evaluation and masking\n```\n\n## Follow-up Questions\n\n- How would you test cross-tenant isolation with canaries?\n- How would you handle key revocation and rotation without service disruption?","diagram":"flowchart TD\n  A[Ingest Event] --> B[Policy Eval]\n  B --> C[Masking Layer]\n  C --> D[Masked Output]\n  B --> E[Decrypt Permissions]\n  E --> F[Authorized View] --> D","difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Apple"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T04:06:41.724Z","createdAt":"2026-01-16T04:06:41.726Z"},{"id":"q-2710","question":"KCNA per-tenant envelope encryption: design a CMK-backed scheme where each tenant's messages are encrypted at ingest with a tenant KEK wrapped by a KMS CMK, keys rotate monthly, and revocation triggers re-encryption with minimal downtime while preserving canary readers. Describe data model, API surface, rotation and revocation flows, and test strategy?","answer":"Use per-tenant KEKs in KMS, envelope-encrypt payloads with per-tenant DEKs; store DEK metadata (tenant_id, key_ver, cipher) in KCNA; rotate KEKs monthly by re-wrapping DEKs in place; on revocation, to","explanation":"## Why This Is Asked\n\nInterview context explanation.\n\n## Key Concepts\n\n- Envelope encryption, CMKs, KEKs, KMS integration\n- Key rotation, revocation, re-encryption strategy\n- Data model for keys, metadata, and auditability\n\n## Code Example\n\n```javascript\n// Pseudo API surface\nclass KCNAEnvelope {\n  constructor(tenantId, kekName, version) {}\n  encrypt(payload) {}\n  decrypt(encrypted) {}\n}\n```\n\n## Follow-up Questions\n\n- How would you test rotation impact on latency?\n- How do you audit key usage and detect leakage?","diagram":null,"difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T07:44:01.064Z","createdAt":"2026-01-16T07:44:01.064Z"},{"id":"q-2771","question":"KCNA privacy-preserving analytics: design a per-tenant analytics pipeline where tenants opt into server-side aggregation over their event streams without exposing raw data. Include architecture for data isolation, privacy budgets via differential privacy, a streaming topology (topics, shards, processors), and how you validate tamper-resistance while preserving privacy. Provide testing, rollback, and performance targets?","answer":"Design a per-tenant analytics flow in KCNA that returns only privacy-preserving aggregates (no raw events). Use tenant-scoped topics, ACLs, and per-tenant privacy budgets; implement a streaming DAG: I","explanation":"## Why This Is Asked\n\nLeverages real-world needs for privacy-preserving analytics in a multi-tenant KCNA deployment, including DP budgeting, auditability, and safe exposure of aggregates.\n\n## Key Concepts\n\n- Per-tenant isolation via topic prefixes and ACLs\n- Differential privacy budgeting and noise mechanisms\n- Streaming topology with processors and backpressure\n- Tamper-resistance via cryptographic proofs (hash chains, Merkle) and audit logs\n- Canary rollout and rollback strategies\n\n## Code Example\n\n```javascript\nfunction addLaplaceNoise(value, epsilon, sensitivity){\n  const rnd = Math.random() - 0.5;\n  const scale = sensitivity / epsilon;\n  // simple Laplace sample using inverse CDF\n  const noise = -scale * Math.sign(rnd) * Math.log(1 - 2 * Math.abs(rnd));\n  return value + noise;\n}\n```\n\n## Follow-up Questions\n\n- How would you test DP accuracy and privacy budget accounting in prod?\n- How do you handle tenants with evolving privacy requirements or opt-out scenarios?","diagram":"flowchart TD\n  Ingest --> TenantFilter\n  TenantFilter --> PartialAggregate\n  PartialAggregate --> DPNoise\n  DPNoise --> Publish","difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","PayPal","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T11:29:27.253Z","createdAt":"2026-01-16T11:29:27.254Z"},{"id":"q-2989","question":"KCNA cost-aware metering at scale: design a tenant-scoped metering system that bills on ingress, egress, and partition-hours, with per-tenant soft quotas, burst credits, and an offline audit log. Describe data models, sampling windows, aggregation, reconciliation, and how to enforce limits without dropping messages. Include API contracts, observability, and rollback/dispute handling; provide a high-level migration plan?","answer":"Implement per-tenant meters in a central ledger with atomic increments at gateway; aggregate 5-minute windows for ingress, egress, and partition-hours; use a per-tenant burst credit (token-bucket) and","explanation":"## Why This Is Asked\nThis question probes multi-tenant metering, accuracy, and fault-tolerant billing at scale.\n\n## Key Concepts\n- Per-tenant metering\n- Windowed aggregation\n- Soft quotas and burst credits\n- Reconciliation and audit logs\n- Dispute handling and rollback\n\n## Code Example\n```javascript\n// Pseudo: updateMeter for an event\nfunction updateMeter(tenantId, ingressBytes, egressBytes, partitions) {\n  const now = floorToWindow(Date.now());\n  meters[tenantId].ingress += ingressBytes;\n  meters[tenantId].egress += egressBytes;\n  meters[tenantId].partitions += partitions;\n  if (meters[tenantId].burst < tokens.current) tokens.consume(...);\n  // emit to central ledger\n}\n```\n\n## Follow-up Questions\n- How would you handle clock skew across data centers?\n- What are the failure modes and how would you test them?","diagram":"flowchart TD\n  A[Ingest] --> B[Metering] \n  B --> C[Aggregate windows] \n  C --> D[Billing shard] \n  D --> E[Audit log]","difficulty":"intermediate","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Google","Lyft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T20:32:16.221Z","createdAt":"2026-01-16T20:32:16.222Z"},{"id":"q-3047","question":"KCNA Audit Trail: Build a per-tenant, append-only audit log for KCNA actions (topic creation, partition changes, offset commits) that is tamper-evident and queryable without impacting throughput. Describe storage format, API surface, retention, and a minimal canary test that demonstrates append, read, and integrity checks?","answer":"Design a per-tenant audit log with append-only writes, per-tenant streams, and a signed MAC for tamper detection. Expose simple APIs: AuditLog.write(tenant, event) and AuditLog.query(tenant, since, limit). Use a storage format with sequential entries containing timestamp, event type, payload, and cryptographic signature. Implement retention policies with automatic cleanup of old entries while preserving integrity. Create a canary test that demonstrates append operations, query functionality, and integrity verification through signature validation.","explanation":"Why This Is Asked\n- Validates per-tenant auditable trails without impacting throughput\n- Ensures tamper evidence via signatures and secure storage\n- Checks ability to query across tenants with predictable retention\n\nKey Concepts\n- Append-only logs, tenant isolation, data integrity, retention strategy\n- Tamper detection using MACs or signatures, and secure storage backends\n- API design for write/read with minimal surface area and clear semantics\n\nCode Example\n```javascript\nclass AuditLog {\n  constructor(storage, signer) {\n    this.storage = storage; // per-tenant append-only store\n    this.signer = signer;   // cryptographic signing service\n  }\n\n  async write(tenant, event) {\n    const entry = {\n      timestamp: Date.now(),\n      tenant,\n      event,\n      signature: await this.signer.sign(event)\n    };\n    return await this.storage.append(tenant, entry);\n  }\n\n  async query(tenant, since, limit) {\n    const entries = await this.storage.read(tenant, since, limit);\n    return entries.filter(entry => \n      this.signer.verify(entry.event, entry.signature)\n    );\n  }\n}\n```","diagram":"flowchart TD\n  A[KCNA event] --> B[AuditLog.write]\n  B --> C[Persistent Storage]\n  C --> D[Query API]\n  D --> E[Audits per tenant]","difficulty":"beginner","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Cloudflare","Databricks"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T04:15:39.153Z","createdAt":"2026-01-16T22:41:05.318Z"},{"id":"q-3131","question":"KCNA Security: Per-tenant encryption at rest using envelope encryption in KCNA. Each tenant has a unique data encryption key (DEK) wrapped by a centralized master key; rotate DEKs monthly and re-encrypt in-flight messages during rotation? Describe key storage, access controls, rotation workflow, and a small canary test to validate encryption at rest and post-rotation decryption?","answer":"Implement per-tenant envelope encryption: store a DEK per tenant in a secure vault, wrap each DEK with a central KMS master key; encrypt KCNA payloads with AES-256-GCM; rotate DEKs monthly by re-wrapp","explanation":"## Why This Is Asked\n\nThis question probes practical crypto for multi-tenant KCNA: encryption at rest, per-tenant keys, rotation, and in-flight re-encryption.\n\n## Key Concepts\n\n- Envelope encryption\n- Per-tenant DEKs\n- KMS master key\n- Rotation workflow\n- Least privilege and auditing\n\n## Code Example\n\n```javascript\n// Pseudo-encryption flow for a tenant payload\nfunction encryptPayload(tenantId, payload, dekStore, kms) {\n  const dek = dekStore.getDEK(tenantId);\n  const wrapped = kms.wrapKey(dek);\n  const iv = crypto.randomBytes(12);\n  const cipher = crypto.createCipheriv('aes-256-gcm', dek, iv);\n  // ... encryption steps\n  return { ciphertext, dekWrapped: wrapped };\n}\n```\n\n## Follow-up Questions\n\n- How would you scale DEK storage and rotation across thousands of tenants?\n- What if rotation fails during processing; how ensure atomicity and replay safety?","diagram":"flowchart TD\n  A[Tenant] --> B[DEK Store]\n  B --> C[Encrypt Payload]\n  C --> D[KCNA Topic]\n  D --> E[Rotation Trigger]\n  E --> F[Re-encrypt In-Flight Messages]","difficulty":"beginner","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T04:12:34.491Z","createdAt":"2026-01-17T04:12:34.491Z"},{"id":"q-3252","question":"KCNA per-tenant real-time anomaly detection: design a streaming pipeline that flags tenants with anomalous event-rate patterns at ingest time, ensuring isolation, data locality, and minimal false positives. Outline architecture, per-tenant state, windowing strategy, privacy considerations, rollout plan, and testing?","answer":"Design a per-tenant streaming anomaly detector that runs at ingest: maintain a per-tenant sliding window (5 minutes) with lightweight stats (mean, stdev) to flag outliers; shard models by tenant-id to","explanation":"## Why This Is Asked\nTests ability to design real-time, multi-tenant analytics with privacy-friendly isolation and scalable state.\n\n## Key Concepts\n- Streaming windows, per-tenant state, thresholding\n- Data locality, backpressure, privacy preservation\n- Testing with synthetic tenants and drift detection\n\n## Code Example\n```javascript\n// Minimal skeleton for per-tenant detector\nclass TenantDetector {\n  constructor(windowMs) {\n    this.windowMs = windowMs\n    this.states = new Map()\n  }\n  ingest(tenantId, value, ts) {\n    // maintain per-tenant window and compute simple stats\n  }\n}\n```\n\n## Follow-up Questions\n- How would you handle non-stationary workloads and adapt thresholds over time?\n- How would you verify no cross-tenant data leakage across shards during scaling?","diagram":"flowchart TD\n  Ingest[Ingest KCNA events] --> State[Per-tenant State (sliding window)]\n  State --> Compute[Compute stats & detect]\n  Compute --> Alert[Emit alert / log provenance]\n  Alert --> Roll[Rollout & rollback]","difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T08:45:30.175Z","createdAt":"2026-01-17T08:45:30.175Z"},{"id":"q-3300","question":"KCNA tenancy revocation and data deletion: design a scalable policy to revoke a tenant's access within 2 minutes, gracefully quiesce in-flight events, switch downstream analytics to de-identified data, and preserve historical auditability. Outline data flow, API contracts, and test strategy, including canaries?","answer":"Implement a per-tenant revoke token and gateway feature flag to block new writes within 2 minutes, and route inflight events to a shadow path for draining. Apply field-level masking before analytics, ","explanation":"## Why This Is Asked\nThis question probes tenancy revocation, data privacy, and live-system safety under real-world SLAs.\n\n## Key Concepts\n- Per-tenant policy enforcement with low-latency revocation\n- In-flight event draining and shadow routing\n- Field masking for analytics without breaking schemas\n- Auditability, rollback, and canary-driven rollout\n\n## Code Example\n```yaml\npolicies:\n  revokeAtSeconds: 120\n  maskPII: true\n  audit: true\n```\n\n## Follow-up Questions\n- How would you test canary rollout at scale without impacting tenants?\n- What metrics indicate a safe rollback should occur?","diagram":"flowchart TD\n  A[Event Arrives] --> B{TenantRevoked?}\n  B -->|Yes| C[Block Writes & Redirect to Shadow]\n  B -->|No| D[Forward to KCNA]\n  C --> E[Mask/Transform & Persist Shadow]\n  D --> E\n  E --> F[Audit Trail Available]\n  F --> G[Canary Rollout Checks]","difficulty":"intermediate","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Hashicorp","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T10:38:12.430Z","createdAt":"2026-01-17T10:38:12.430Z"},{"id":"q-3323","question":"Design a per-tenant deduplication system for KCNA that guarantees exactly-once-like semantics for idempotent event processing across tenants, without cross-tenant data leakage. Propose event_id schema, fast per-tenant dedupe checks, eviction policy, tombstones, and failure handling; discuss producer/consumer changes and a practical test plan that demonstrates correctness and scalability?","answer":"Use a stable event_id composed of (tenant_id, stream_id, event_seq). Broker checks a per-tenant in-memory cache or Redis with a TTL (e.g., 24 hours). On miss, record and forward; on hit, drop. Enforce","explanation":"## Why This Is Asked\n\nAssess practical understanding of cross-tenant deduplication at KCNA scale, including latency considerations, late duplicates, and shard rebalancing impacts.\n\n## Key Concepts\n\n- Exactly-once-like semantics vs dedupe\n- Tenant isolation and TTL caches\n- Event_id design for idempotent processing\n- Test strategies for duplicates, latency, and re-sharding\n\n## Code Example\n\n```javascript\n// Pseudo dedupe cache interface\nclass DedupeStore {\n  async hasSeen(tenant, eventId) { /* check */ }\n  async record(tenant, eventId) { /* store tombstone */ }\n}\n```\n\n## Follow-up Questions\n\n- How would you handle eviction and memory pressure in a multi-tenant cluster?\n- How do you test correctness during rolling upgrades and shard rebalancing?","diagram":null,"difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","PayPal","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T11:31:25.100Z","createdAt":"2026-01-17T11:31:25.100Z"},{"id":"q-3350","question":"KCNA privacy-preserving analytics: design a server-side cross-tenant aggregation layer that lets dashboards derive insights from multi-tenant event streams without exposing raw data. Specify a differential privacy or secure-aggregation protocol, per-tenant access controls, data formats, auditable results, and a minimal canary test plan to verify privacy and accuracy under load?","answer":"Use a privacy-preserving aggregator that either applies a DP mechanism (input clipping, noise calibrated to epsilon) or uses secure sums with per-tenant keys and a shared DP budget. Return only DP-saf","explanation":"## Why This Is Asked\n\nThis explores privacy-preserving cross-tenant analytics, auditing, and key management in KCNA at scale, aligning with enterprise needs around data sharing without leakage.\n\n## Key Concepts\n\n- Differential privacy and secure aggregation\n- Per-tenant access controls and key rotation\n- Tamper-evident auditing of query results\n- API design for aggregated analytics\n- Performance considerations under multi-tenant load\n\n## Code Example\n\n```javascript\n// Demonstrative DP aggregation stub\nfunction clipAndNoisify(values, epsilon, sensitivity) {\n  const clipped = values.map(v => Math.max(-sensitivity, Math.min(sensitivity, v)));\n  const b = sensitivity / epsilon;\n  const noise = (Math.random() < 0.5 ? -1 : 1) * b * Math.log(1 - Math.random());\n  return clipped.reduce((a,b)=>a+b,0) + noise;\n}\n```\n\n## Follow-up Questions\n\n- How would you measure privacy budget consumption over time in a live dashboard?\n- What changes would you make to support streaming DP with backpressure and multi-tenant isolation?","diagram":"flowchart TD\n  A[KCNA event streams] --> B[Server-side Aggregation]\n  B --> C[Noise Injection / DP]\n  B --> D[Tenant ACLs]\n  C --> E[Aggregated Results]\n  D --> E\n  E --> F[Dashboards]\n  F --> G[Tamper-evident Audit]","difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Hashicorp","Hugging Face"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T13:06:34.396Z","createdAt":"2026-01-17T13:06:34.396Z"},{"id":"q-3419","question":"KCNA per-tenant data purge: design a per-tenant purge operation that deletes messages older than a given timestamp while preserving per-tenant offset semantics and avoiding consumer surprises. Describe API shape, tombstone strategy, offset handling, GC durability, and a minimal canary test that demonstrates purge, offset stability, and alerting?","answer":"Design a per-tenant purge API for KCNA: POST /kcna/purge with body {tenantId, topic, cutoffTs}. Purge marks messages older than cutoffTs as tombstones; offsets remain valid but readers skip tombstones","explanation":"## Why This Is Asked\nAddresses data lifecycle, privacy, and regulatory needs for multi-tenant KCNA without breaking offset guarantees.\n\n## Key Concepts\n- Per-tenant retention and purge granularity\n- Tombstone semantics and consumer view\n- Offset progression stability during purge\n- Durable GC with audit logs and alerts\n\n## Code Example\n```javascript\n// TypeScript sketch\ntype PurgeRequest = { tenantId: string; topic: string; cutoffTs: number }\nasync function purgeTenantTopic(req: PurgeRequest): Promise<void> {\n  // validate inputs\n  // scan messages older than cutoff and emit tombstones\n  // ensure idempotence; commit tombstones in a transaction\n  // log purge event for auditing\n}\n```\n\n## Follow-up Questions\n- How would you test concurrent purges and in-flight offset commits?\n- How do tombstones interact with replication and cross-region copies?","diagram":"flowchart TD\n  A[Client sends purge request] --> B[Purge service validates]\n  B --> C{Oldest offset guard}\n  C -->|Yes| D[Tombstone records written]\n  C -->|No| E[Abort purge]\n  D --> F[GC marks tombstones consumable]\n  F --> G[Consumers see tombstones as deletes]","difficulty":"beginner","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T15:35:59.186Z","createdAt":"2026-01-17T15:35:59.187Z"},{"id":"q-3511","question":"In KCNA, tenants share a cluster. Propose a per-tenant envelope encryption scheme for streams where each tenant's messages are encrypted at rest with a tenant-specific key managed by a central KMS. Describe key rotation, revocation, legacy data handling, latency impact, and a concrete test plan. How would you implement and verify it?","answer":"Use per-tenant envelope encryption: each tenant has a CMK in a centralized KMS; KCNA encrypts payloads with a tenant-specific DEK that is wrapped by the CMK. Rotate DEKs periodically and version ciphe","explanation":"## Why This Is Asked\nThis probes secure multi-tenant data protection in KCNA, focusing on scalable key management, auditability, and performance impact.\n\n## Key Concepts\n- Envelope encryption, per-tenant CMK, DEK lifecycle\n- Key rotation, revocation, legacy data handling\n- Performance trade-offs: latency, CPU, GC impact\n\n## Code Example\n```javascript\n// Pseudo-code: envelope encryption per tenant\nconst dek = getOrCreateDEK(tenantId); // tenant DEK\nconst ciphertext = aesGcmEncrypt(plaintext, dek);\nconst wrappedDEK = kms.wrapKey(dek, cmkForTenant(tenantId));\nstore(ciphertext, {wrappedDEK, dekVersion: dek.version});\n```\n\n## Follow-up Questions\n- How would you test key rollover without downtime?\n- How do you ensure auditability of key usage across tenants?","diagram":null,"difficulty":"intermediate","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T19:27:45.843Z","createdAt":"2026-01-17T19:27:45.843Z"},{"id":"q-3540","question":"KCNA cross-region replication: design a protocol that preserves per-tenant data sovereignty across two regions, ensuring at-least-once delivery and per-tenant ordering during failover. Describe region-local shards, per-tenant keys for encryption, key rotation, and tombstone GC, plus a DR test plan with progressive canaries and rollback. How would you ensure correctness and observability?","answer":"Propose per-tenant streams written to region-local shards with tenant-scoped sequence numbers. Use async replication with idempotent writes and per-tenant isolation, encrypted at rest with per-tenant ","explanation":"## Why This Is Asked\n\nAssess cross-region replication, tenant isolation, security, and DR readiness; emphasis on verifiable correctness and observability across regions.\n\n## Key Concepts\n\n- Multi-region replication with per-tenant isolation\n- Ordering guarantees and at-least-once delivery\n- Encryption at rest with per-tenant keys and rotation\n- Tombstone GC and cross-region proofs\n- Canary-based DR testing and rollback plans\n\n## Code Example\n\n```javascript\n// Pseudo-code: per-tenant write path\nfunction publish(tenantId, event) {\n  const shard = getShardForTenant(tenantId);\n  shard.append({ tenantId, event, seq: shard.nextSeq(tenantId) });\n  replicateToRegionB(tenantId, event, shard.latestSeq(tenantId));\n}\n```\n\n## Follow-up Questions\n\n- How would you validate ordering guarantees during DR failover?\n- How would you monitor cross-region latency and data sovereignty compliance?\n","diagram":null,"difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Cloudflare","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T20:39:28.658Z","createdAt":"2026-01-17T20:39:28.659Z"},{"id":"q-3621","question":"KCNA Cross-Region Replication: In a globally deployed KCNA, design cross-region replication that guarantees per-tenant event order and cross-region consistency, while producing end-to-end verifiable batch proofs across regions. Include shard ownership, canaries, key rotation, rollback, and a DR test plan?","answer":"Cross-Region KCNA replication: assign per-tenant shards with local ordering, replicate batches to a DR region. Each batch carries a per-tenant Merkle root and cross-region signature; DR reconciliation validates proofs before advancing offsets.","explanation":"## Why This Is Asked\nThis question probes ability to design robust cross-region replication with verifiable proofs and DR considerations.\n\n## Key Concepts\n- Per-tenant shard ownership and ordering\n- End-to-end verifiable proofs (Merkle roots) and signatures\n- Cross-region reconciliation and offset advancement\n- Secure key management and rotation\n- Canary-based DR testing and rollback\n\n## Code Example\n```javascript\n// Pseudo: batch payload structure\n{ tenantId, batchId, merkleRoot, payload, signature }\n```\n\n## Follow-up Questions\n- How would proof validation scale across regions?\n- What are latency implications for cross-region consistency?\n- How would you handle network partitions between regions?","diagram":null,"difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Snap","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T05:13:51.607Z","createdAt":"2026-01-17T23:42:33.072Z"},{"id":"q-3670","question":"KCNA: Design a tenant-scoped encryption model for KCNA events where each tenant's data is encrypted at rest with a per-tenant KEK sourced from a KMS, and payloads are envelope-encrypted with per-tenant DEKs that are rotated without downtime. Explain provisioning, rotation, revocation, audit proofs, and isolation guarantees under load. Include performance trade-offs and a concrete failure scenario?","answer":"Outline a per-tenant envelope encryption model: provision per-tenant KEKs from a KMS, generate per-tenant DEKs to encrypt payloads, wrap DEKs with KEKs, and rotate KEKs with zero-downtime rewrap. Defi","explanation":"## Why This Is Asked\n\nTo assess practical integration of KMS-based encryption, tenant isolation, and auditability in KCNA at scale, including key rotation, revocation, and observability under load.\n\n## Key Concepts\n\n- Per-tenant KEK\n- Envelope encryption\n- Zero-downtime key rotation\n- Tenant isolation proofs\n- Auditability and proofs\n\n## Code Example\n\n```javascript\n// Pseudo: fetch KEK, generate data key, encrypt payload, store ciphertext and wrapped key\nasync function sealPayload(tenantId, payload, kms){\n  const KEK = await kms.getKEK(tenantId);\n  const DEK = crypto.getRandomValues(new Uint8Array(32));\n  const wrappedDEK = await wrapKeyWithKEK(DEK, KEK);\n  const ciphertext = await encrypt(payload, DEK);\n  return { ciphertext, wrappedDEK };\n}\n```\n\n## Follow-up Questions\n\n- How to rotate DEKs per-tenant without re-encrypting old data?\n- How to detect/mitigate KEK compromise and revoke access quickly?\n","diagram":"flowchart TD\n  A[Tenant] --> B[KCNA Ingest Path]\n  B --> C[DEK Wrap via KEK from KMS]\n  C --> D[Encrypt Payload]\n  D --> E[Store Ciphertext]\n  E --> F[Audit Proofs]","difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Netflix","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T04:20:33.724Z","createdAt":"2026-01-18T04:20:33.725Z"},{"id":"q-3691","question":"KCNA Security: Design a tenant-scoped envelope-encryption framework for KCNA where each event payload is encrypted with a per-tenant data key (DEK) wrapped by a centralized KMS/HSM, enabling hot key rotation, forward secrecy, and revocation. Outline the key hierarchy, data structures, per-tenant access checks, audit proofs, and a rollout and rollback strategy with performance considerations?","answer":"Use a two-layer envelope: each tenant has a DEK that encrypts payloads; a tenant KEK wraps the DEK in a KMS/HSM. Rotate DEKs with epoch tags and purge old envelopes; revoke access by re-wrapping with ","explanation":"## Why This Is Asked\nThis question probes KCNA security design, tenant isolation, and practical key management choices under constraints like rotation and revocation.\n\n## Key Concepts\n- Envelope encryption with per-tenant DEKs\n- Hierarchical KMS/HSM wrapping and rotation\n- Per-tenant access checks and audit proofs\n- Rollout/rollback with minimal downtime and observability\n\n## Code Example\n```javascript\n// Pseudocode for envelope encryption\nfunction encryptEvent(event, tenantId, kms, store) {\n  const dek = store.getDEK(tenantId);\n  const ciphertext = aesGcmEncrypt(event.payload, dek);\n  return { ciphertext, headers: { tenantId, keyId: dek.id, epoch: dek.epoch } };\n}\nfunction decryptEvent(record, store) {\n  const dek = store.getDEK(record.headers.tenantId, record.headers.keyId);\n  return aesGcmDecrypt(record.ciphertext, dek);\n}\n```\n\n## Follow-up Questions\n- How would you validate rotation can be performed with zero downtime?\n- How would you handle compromised KEKs and re-encryption scope?","diagram":"flowchart TD\n  A[Ingest Event] --> B[Fetch Tenant KEK]\n  B --> C[Unwrap DEK, Decrypt Payload]\n  C --> D[Re-encrypt for KCNA Store]\n  D --> E[Store with Headers (tenantId, keyId, epoch)]","difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Microsoft","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T05:40:07.313Z","createdAt":"2026-01-18T05:40:07.313Z"},{"id":"q-3758","question":"KCNA per-tenant encryption & key management: design a CMEK-backed model for KCNA where each tenant's event payload is encrypted at ingestion with per-tenant keys managed by an external KMS. How would you implement key rotation without downtime, revocation, and auditable proofs (Merkle-based) without exposing payloads? Include data structures mapping tenants to keys, API surfaces, and a practical test plan?","answer":"Use envelope encryption with per-tenant data keys (DEKs) wrapped by a CMEK in an external KMS. Ingest fetches the DEK, encrypts the payload, and stores ciphertext alongside the DEK ID. Rotate by rolli","explanation":"## Why This Is Asked\nDemonstrates deep understanding of per-tenant data isolation, external KMS integration, and verifiable auditing in a streaming system. Tests handling of key rotation, revocation, and tamper-evidence without leaking data.\n\n## Key Concepts\n- CMEK-based envelope encryption with per-tenant DEKs\n- External KMS integration and key-wrapping\n- Zero-downtime rotation and revocation workflows\n- Tamper-evident auditing using Merkle proofs\n- Tenant-to-key metadata, APIs, and test strategies\n\n## Code Example\n```javascript\n// Pseudo: obtain per-tenant DEK, encrypt, store ciphertext with key reference\nasync function ingestEvent(tenantId, plaintext) {\n  const dek = await kms.getTenantDEK(tenantId);\n  const iv = crypto.randomBytes(12);\n  const ciphertext = crypto.aesGCM(plaintext, dek.key, iv);\n  await storeCiphertext(tenantId, ciphertext, iv, dek.id);\n}\n```\n\n## Follow-up Questions\n- How would you coordinate DEK rotation with in-flight data?\n- How would you prove key usage for auditors without exposing payloads?","diagram":"flowchart TD\n  Ingest(Ingest KCNA Event) --> Encrypt(Encrypt with Tenant DEK)\n  Encrypt --> Store(Store Ciphertext + KeyID)\n  Store --> Audit(Audit proof with Merkle root)\n  Audit --> Rotate(Rotate keys without downtime)\n  Rotate --> Ingest","difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T08:40:55.410Z","createdAt":"2026-01-18T08:40:55.411Z"},{"id":"q-3790","question":"KCNA exactly-once semantics at scale: design a per-tenant streaming pipeline in KCNA that guarantees exactly-once delivery from producer to consumer despite retries and partition rebalances. Propose a per-tenant commit-log with transactional writes, a monotonic sequence number, and a bounded dedup cache. How do you handle failure modes, rebalances, and tombstone processing? Include API contracts and a testing plan with canaries?","answer":"In practice, I’d implement per-tenant sequence numbers and a per-tenant commit log. Producers attach an idempotence key and a monotonic offset; KCNA uses a two-phase commit to write to the per-tenant ","explanation":"## Why This Is Asked\nThis question probes end-to-end exactly-once semantics in a multi-tenant streaming system, focusing on per-tenant isolation, commit logging, and failure scenarios.\n\n## Key Concepts\n- Exactly-once delivery, idempotent producers, per-tenant commit logs\n- Two-phase commit, offsets, tombstones, dedupe caching\n- Failure modes: broker crash, partition rebalance, network retry\n\n## Code Example\n\n```javascript\n// Pseudo code: producer writes to per-tenant commit log with idempotence key\n```\n\n## Follow-up Questions\n- How would you validate OC tests with canaries across tenants? \n- How would you measure latency budgets and saturation during rebalance?\n","diagram":"flowchart TD\n  A[Producer] --> B[PerTenantCommitLog]\n  B --> C[OffsetIndex]\n  C --> D[Consumer]\n  D --> E[DedupeCache]","difficulty":"intermediate","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Hashicorp"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T09:38:48.222Z","createdAt":"2026-01-18T09:38:48.222Z"},{"id":"q-3861","question":"KCNA policy-driven field-level de-identification: design an ingestion-time pipeline that applies per-tenant de-identification rules (tokenize or redact PII fields) while preserving deterministic tokens for analytics, and supports on-demand re-identification with strict authorization. Explain policy model, evaluation order, key management, performance bounds, and a test plan for leak-avoidance and auditability?","answer":"Implement a per-tenant policy engine that runs at ingestion, applying field-level rules: tokenize PII deterministically for analytics, redact non-analytics fields, and preserve a reversible re-identif","explanation":"## Why This Is Asked\n\nTests a candidate's ability to design a privacy-preserving, per-tenant data path with strong auditability and revocation controls in KCNA. It requires integrating policy evaluation, key management, and immutable logs, plus performance considerations for real-time ingestion.\n\n## Key Concepts\n\n- Policy-driven de-identification\n- Deterministic tokenization with per-tenant KEK\n- Ingestion-time enforcement and data isolation\n- Auditability with Merkle proofs and immutable logs\n- Key rotation and re-identification safeguards\n\n## Code Example\n\n```javascript\n// Pseudo-policy evaluation skeleton\nfunction applyPolicy(record, policy) {\n  // tokenize or redact fields based on policy\n  // return transformed record\n}\n```\n\n## Follow-up Questions\n\n- How to audit policy changes across tenants without leaking data?\n- How to simulate leakage scenarios and measure protection","diagram":"flowchart TD\n  A[Policy Engine] --> B[Ingestion Pipeline]\n  B --> C[Event Store KCNA]\n  A --> D[Key Management]\n  B --> E[Audit Trail]\n  E --> F[External Audit]","difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Citadel","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T13:04:24.489Z","createdAt":"2026-01-18T13:04:24.489Z"},{"id":"q-941","question":"Scenario: A global chat platform with 2B MAUs must detect policy-violating content (spam, hate speech) in near real-time while preserving user privacy and multilingual support. Propose an end-to-end pipeline: ingestion, moderation models (rules + ML), latency SLOs (1-2s), privacy safeguards, backpressure handling, retries, and dead-letter queues. Compare on-device vs cloud inference and monitoring?","answer":"Propose a tiered moderation pipeline: client-side tokenization + on-device classifier for first-pass filtering (multilingual light-weight model), with encrypted message IDs, then server-side streaming","explanation":"## Why This Is Asked\nThis question gauges real-time, scalable moderation design, privacy-safe multi-language handling, and the trade-offs between edge and cloud inference.\n\n## Key Concepts\n- Real-time streaming pipelines, SLOs, backpressure\n- Edge (on-device) vs cloud inference, multilingual models\n- Privacy safeguards (encryption, minimal data)\n- DLQ, retries, circuit breakers, monitoring\n\n## Code Example\n```javascript\n// Latency guard example\nif (latencyMs > 2000) {\n  tagAsSlowPath();\n  redirectToFallbackQueue();\n}\n```\n\n## Follow-up Questions\n- How would you validate model drift and false positives in production?\n- What metrics would you surface in dashboards to detect abuse transparently?","diagram":"flowchart TD\n  A[Ingest] --> B[Queue]\n  B --> C[Moderation]\n  C --> D[Enforce/Notify]\n  D --> E[Audit]","difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Slack","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T16:31:29.566Z","createdAt":"2026-01-12T16:31:29.566Z"}],"subChannels":["general"],"companies":["Adobe","Amazon","Anthropic","Apple","Bloomberg","Citadel","Cloudflare","Coinbase","Databricks","Discord","DoorDash","Goldman Sachs","Google","Hashicorp","Hugging Face","IBM","Instacart","LinkedIn","Lyft","Meta","Microsoft","MongoDB","NVIDIA","Netflix","Oracle","PayPal","Plaid","Robinhood","Slack","Snap","Snowflake","Square","Stripe","Tesla","Twitter","Two Sigma","Uber","Zoom"],"stats":{"total":44,"beginner":7,"intermediate":15,"advanced":22,"newThisWeek":44}}