{"questions":[{"id":"kcna-cloud-native-arch-1768224106527-0","question":"In a 3-node etcd-backed Kubernetes control plane, which backup method yields a consistent, restorable snapshot without taking the API servers offline?","answer":"[{\"id\":\"a\",\"text\":\"Run etcdctl snapshot save on one member while the cluster continues serving requests\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Stop all Kubernetes API servers, then snapshot each member individually\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use etcdctl snapshot save on the leader only\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Back up the etcd data directory from the current leader and restore from that\",\"isCorrect\":false}]","explanation":"## Correct Answer\nRun etcdctl snapshot save on one member while the cluster continues serving requests, which yields a consistent snapshot of the whole cluster without downtime.\n\n## Why Other Options Are Wrong\n- Stop all API servers: downtime and not necessary; you can snapshot while running.\n- Leader-only snapshot is not guaranteed to capture a consistent state if the leader changes; etcd snapshots can be taken from any member.\n- Copying the data directory is unsafe for live clusters and does not guarantee a consistent snapshot.\n\n## Key Concepts\n- etcd backups\n- etcd snapshot semantics\n- Disaster recovery testing\n\n## Real-World Application\n- DR planning for Kubernetes control plane; ensures quick restore using etcd snapshots; must store to durable storage and test restore regularly.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","etcd","AWS","EKS","Terraform","Prometheus","certification-mcq","domain-weight-16"],"channel":"kcna","subChannel":"cloud-native-arch","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:21:46.531Z","createdAt":"2026-01-12 13:21:46"},{"id":"kcna-cloud-native-arch-1768224106527-1","question":"You deploy PostgreSQL as a StatefulSet with persistent volumes across 3 nodes. To minimize downtime during rolling updates, which combination ensures data consistency and high availability?","answer":"[{\"id\":\"a\",\"text\":\"Use a Deployment with a single PersistentVolume\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"StatefulSet with volumeClaimTemplates, a headless service, readiness probes, and a PodDisruptionBudget set to 1\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Use a DaemonSet to run a single pod per node\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use a Job to perform rolling updates\",\"isCorrect\":false}]","explanation":"## Correct Answer\nStatefulSet with volumeClaimTemplates, a headless service, readiness probes, and a PodDisruptionBudget set to 1 ensures stable network identities, durable storage, and controlled evictions, which minimizes downtime during rolling updates.\n\n## Why Other Options Are Wrong\n- Deployments are not suited for stateful databases and do not provide stable storage guarantees.\n- DaemonSets are per-node workers and do not provide the required database topology.\n- Jobs are one-off tasks, not suitable for ongoing rolling updates of a stateful service.\n\n## Key Concepts\n- StatefulSet, VolumeClaimTemplates, headless service, readiness probes\n- PodDisruptionBudget\n\n## Real-World Application\n- Rolling updates for a clustered database with minimal downtime in production environments.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","StatefulSet","PostgreSQL","AWS-EKS","Terraform","Prometheus","certification-mcq","domain-weight-16"],"channel":"kcna","subChannel":"cloud-native-arch","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:21:47.075Z","createdAt":"2026-01-12 13:21:47"},{"id":"kcna-cloud-native-arch-1768224106527-2","question":"In a shared Kubernetes cluster, you want to prevent noisy neighbors from exhausting cluster resources. Which combination best enforces per-namespace resource limits?","answer":"[{\"id\":\"a\",\"text\":\"ResourceQuota only\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"LimitRange only\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"ResourceQuota + LimitRange\",\"isCorrect\":true},{\"id\":\"d\",\"text\":\"PodSecurityPolicy + ResourceQuota\",\"isCorrect\":false}]","explanation":"## Correct Answer\nResourceQuota plus LimitRange enforces namespace-level limits and sets default/requested limits for pods, preventing resource starvation.\n\n## Why Other Options Are Wrong\n- ResourceQuota alone lacks per-pod defaults and maximums without explicit limits.\n- LimitRange alone cannot cap total per-namespace usage without an overall quota.\n- PodSecurityPolicy governs security, not resource usage quotas.\n\n## Key Concepts\n- ResourceQuota, LimitRange, per-namespace isolation\n\n## Real-World Application\n- Enforcing fair resource distribution in multi-tenant clusters, avoiding noisy neighbors.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","ResourceQuota","Terraform","EKS","Prometheus","certification-mcq","domain-weight-16"],"channel":"kcna","subChannel":"cloud-native-arch","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:21:47.613Z","createdAt":"2026-01-12 13:21:47"},{"id":"kcna-cloud-native-arch-1768224106527-3","question":"You need centralized log and metrics collection across the cluster to support incident response. Which pairing best provides durable logs and real-time metrics?","answer":"[{\"id\":\"a\",\"text\":\"kubectl logs for individual pods and cluster-wide Grafana\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Centralized logging stack (EFK/ELK) and metrics stack (Prometheus + Grafana)\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Only Kubernetes events\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Node-level journald without aggregation\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA centralized logging stack (EFK/ELK) plus a metrics stack (Prometheus + Grafana) provides durable, searchable logs and real-time metrics for effective incident response.\n\n## Why Other Options Are Wrong\n- kubectl logs per-pod is ad-hoc and hard to correlate across nodes.\n- Kubernetes events alone miss detailed logs and long-term retention.\n- Node journald without aggregation is not centralized and lacks long-term retention.\n\n## Key Concepts\n- Centralized logging, metrics collection, observability stacks\n\n## Real-World Application\n- Rapid incident detection, root-cause analysis, and post-incident reviews.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","EFK","Prometheus","Grafana","AWS-EKS","Terraform","certification-mcq","domain-weight-16"],"channel":"kcna","subChannel":"cloud-native-arch","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:21:47.796Z","createdAt":"2026-01-12 13:21:47"},{"id":"kcna-cloud-native-arch-1768224106527-4","question":"In a multi-cluster Kubernetes deployment, you want seamless service discovery across clusters without exposing services via NodePort or relying on VPNs. Which approach best achieves this?","answer":"[{\"id\":\"a\",\"text\":\"Use a cloud VPN to connect clusters and expose services via NodePort\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use a service mesh with a multi-cluster control plane and cross-cluster service discovery (Istio or Linkerd) with a shared CA\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Use an Ingress resource with a single global load balancer\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Run all services in a single cluster only\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA service mesh with a multi-cluster control plane and cross-cluster service discovery provides seamless interaction across clusters without NodePort exposure or VPNs, especially when using a shared CA for mTLS.\n\n## Why Other Options Are Wrong\n- VPN plus NodePort reintroduces exposure and management overhead.\n- Ingress with a global load balancer does not inherently provide native cross-cluster service discovery.\n- Running all services in one cluster defeats the multi-cluster requirement.\n\n## Key Concepts\n- Multi-cluster service discovery, Istio/Linkerd, cross-cluster control plane\n\n## Real-World Application\n- Consistent user experience and traffic routing across regional clusters in a cloud-native environment.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","Istio","Linkerd","AWS-EKS","Terraform","Prometheus","certification-mcq","domain-weight-16"],"channel":"kcna","subChannel":"cloud-native-arch","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:21:47.978Z","createdAt":"2026-01-12 13:21:48"},{"id":"kcna-cloud-native-delivery-1768249779486-0","question":"In Kubernetes, you are delivering a microservices app via GitOps and want safe progressive delivery with automatic rollback if the canary fails, which approach best achieves this?","answer":"[{\"id\":\"a\",\"text\":\"Use a standard Deployment with maxUnavailable and maxSurge for rolling updates\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use an Ingress to route and shift traffic between versions\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use Argo Rollouts with a canary strategy and traffic routing\",\"isCorrect\":true},{\"id\":\"d\",\"text\":\"Use a StatefulSet with manual rollouts\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nOption C is correct because Argo Rollouts supports progressive delivery with canary or blue/green strategies and integrated traffic routing, enabling safe rollout and quick rollback if the canary health checks fail.\n\n## Why Other Options Are Wrong\n\n- Option A: A traditional Deployment rolling update lacks built-in traffic shaping and automated canary validation; rollback can be manual and less safe for complex services.\n- Option B: Ingress traffic shaping alone does not provide progressive delivery or rollback controls for per-version traffic splitting.\n- Option D: StatefulSet with manual rollouts is inappropriate for stateless microservices delivery and lacks automated progressive delivery features.\n\n## Key Concepts\n\n- Progressive delivery\n- Canary deployments\n- Traffic routing and observability\n- Argo Rollouts\n\n## Real-World Application\n\n- In production, configure Argo Rollouts with a canary step sequence and metrics-based analysis; if the canary fails, the system automatically rolls back to the stable version and halts the promotion to production.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","AWS","Terraform","GitOps","CI/CD","certification-mcq","domain-weight-8"],"channel":"kcna","subChannel":"cloud-native-delivery","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T20:29:39.487Z","createdAt":"2026-01-12 20:29:39"},{"id":"kcna-cloud-native-delivery-1768249779486-1","question":"In a multi-cluster Kubernetes delivery pipeline on AWS, you want to automatically update image tags in Git and roll out to all clusters when a new image is built, which approach best supports this?","answer":"[{\"id\":\"a\",\"text\":\"Manually edit the Git repo and push changes\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use Kubernetes to automatically pull the latest images without Git updates\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use Flux CD ImageUpdateAutomation to automatically update image tags in Git and trigger deployment across clusters\",\"isCorrect\":true},{\"id\":\"d\",\"text\":\"Use Helm to bump chart versions and apply across clusters\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nOption C is correct because Flux CD ImageUpdateAutomation automatically updates image tags in the Git repository and triggers application syncs across clusters, enabling automated promotion in a GitOps workflow.\n\n## Why Other Options Are Wrong\n\n- Option A: Manual updates introduce delays and drift between environments.\n- Option B: Kubernetes does not automatically update container images in GitOps without a controller.\n- Option D: Helm alone does not automate image tag updates across clusters within a GitOps pipeline.\n\n## Key Concepts\n\n- GitOps\n- Flux CD, ImageUpdateAutomation\n- Multicluster delivery\n- Image tagging automation\n\n## Real-World Application\n\n- Integrate your CI pipeline to push new images and trigger ImageUpdateAutomation; Flux CD applies the updated manifests to all clusters, reducing manual steps and drift.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","AWS","Terraform","GitOps","CI/CD","certification-mcq","domain-weight-8"],"channel":"kcna","subChannel":"cloud-native-delivery","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T20:29:39.867Z","createdAt":"2026-01-12 20:29:40"},{"id":"kcna-cloud-native-delivery-1768249779486-2","question":"You want to prevent non-compliant Kubernetes manifests from being deployed via your GitOps flow, ensuring every Namespace has a ResourceQuota and LimitRange. Which tool best enforces this policy at admission time?","answer":"[{\"id\":\"a\",\"text\":\"A custom admission webhook\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"OPA Gatekeeper with ConstraintTemplates\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"PodSecurityPolicy\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"NetworkPolicy\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nOption B is correct because OPA Gatekeeper enforces policy as code, and ConstraintTemplates can require ResourceQuota and LimitRange on namespaces before admission, aligning with GitOps checks.\n\n## Why Other Options Are Wrong\n\n- Option A: A custom admission webhook could enforce similar checks but would require maintenance; Gatekeeper offers a standard policy-as-code approach.\n- Option C: PodSecurityPolicy is deprecated and not recommended for new clusters.\n- Option D: NetworkPolicy governs pod networking, not resource constraints or namespace quotas.\n\n## Key Concepts\n\n- OPA Gatekeeper\n- ConstraintTemplates and Constraints\n- Policy-as-code\n- Resource quotas and limits\n\n## Real-World Application\n\n- Define a ConstraintTemplate that requires a ResourceQuota and a LimitRange in each namespace; enable in the cluster; GitOps changes will be blocked if non-compliant manifests are applied.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","AWS","Terraform","Policy-as-Code","GitOps","certification-mcq","domain-weight-8"],"channel":"kcna","subChannel":"cloud-native-delivery","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T20:29:40.235Z","createdAt":"2026-01-12 20:29:40"},{"id":"kcna-cloud-native-delivery-1768249779486-3","question":"Two clusters in different AWS regions host services that must communicate reliably and securely. Which approach best enables cross-cluster service discovery and traffic routing?","answer":"[{\"id\":\"a\",\"text\":\"Expose services via NodePort across clusters\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use a single Ingress with global DNS\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use a service mesh (e.g., Istio) with an east-west gateway for cross-cluster traffic\",\"isCorrect\":true},{\"id\":\"d\",\"text\":\"Use a VPN to connect clusters and rely on standard DNS\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nOption C is correct because a service mesh with an east-west gateway (e.g., Istio) enables secure, discoverable, and observable cross-cluster traffic between services with mTLS and consistent routing.\n\n## Why Other Options Are Wrong\n\n- Option A: NodePort is brittle, non-discoverable across clusters, and insecure for production traffic.\n- Option B: A single Ingress does not automatically provide cross-cluster service discovery or consistent mTLS across clusters.\n- Option D: A VPN can connect networks but lacks native service discovery and dynamic traffic management between Kubernetes services.\n\n## Key Concepts\n\n- Service mesh\n- Istio/Linkerd east-west gateway\n- Cross-cluster traffic\n- mTLS and observability\n\n## Real-World Application\n\n- Deploy a multi-cluster Istio control plane or replicated gateways; create ServiceEntries for remote services; configure consistent mTLS and gateway routing to enable seamless cross-cluster calls.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","AWS","Terraform","ServiceMesh","CrossCluster","certification-mcq","domain-weight-8"],"channel":"kcna","subChannel":"cloud-native-delivery","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T20:29:40.367Z","createdAt":"2026-01-12 20:29:40"},{"id":"kcna-cloud-native-delivery-1768249779486-4","question":"You want a Kubernetes-native CI/CD pipeline that runs on every commit, supports gated approvals, and orchestrates tasks across resources. Which approach best fits?","answer":"[{\"id\":\"a\",\"text\":\"Jenkins running on a virtual machine\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"GitHub Actions with Kubernetes runners\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Tekton Pipelines with TriggerBindings and Tasks\",\"isCorrect\":true},{\"id\":\"d\",\"text\":\"Spinnaker deployed on a cloud provider\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nOption C is correct because Tekton Pipelines provide Kubernetes-native CI/CD with reusable Tasks, and TriggerBindings/Events enable automated pipelines that respond to git events and support gated approvals.\n\n## Why Other Options Are Wrong\n\n- Option A: Jenkins on VMs is not Kubernetes-native and adds external management overhead.\n- Option B: GitHub Actions is external to the cluster and, while common, is not a Kubernetes-native solution.\n- Option D: Spinnaker is powerful but heavier and more complex than a focused Kubernetes-native Tekton setup for this scenario.\n\n## Key Concepts\n\n- Tekton Pipelines\n- Tasks, Pipelines, Triggers\n- Kubernetes-native CI/CD\n- Approvals and gating\n\n## Real-World Application\n\n- Define a Tekton PipelineRun to build, test, and deploy; configure a TriggerBinding to react to Git events; add a manual approval Task to gate promotions; tie to Kubernetes cluster contexts.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","AWS","Terraform","Tekton","CI/CD","certification-mcq","domain-weight-8"],"channel":"kcna","subChannel":"cloud-native-delivery","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T20:29:40.498Z","createdAt":"2026-01-12 20:29:40"},{"id":"kcna-cloud-native-observability-1768235481833-0","question":"In a Kubernetes-based microservices deployment, you need end-to-end traceability across services and correlation of traces with logs and metrics. Which approach best enables this?","answer":"[{\"id\":\"a\",\"text\":\"Instrument only the front-end service and rely on separate log correlation IDs\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Implement distributed tracing with a common propagation format (e.g., W3C Trace Context) with OpenTelemetry, and enrich logs with trace IDs to correlate across traces, logs, and metrics\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Collect only Prometheus metrics and Grafana dashboards\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Run sidecar collectors that duplicate traffic for sampling\",\"isCorrect\":false}]","explanation":"## Correct Answer\nB. End-to-end tracing across services is achieved by distributed tracing with a consistent propagation format and by enriching logs with trace IDs to correlate traces, logs, and metrics.\n\n## Why Other Options Are Wrong\n- Option A: It only instruments a subset and lacks end-to-end trace propagation and log correlation.\n- Option C: Collecting only metrics misses trace data and cross-resource correlation.\n- Option D: Duplicating traffic for sampling does not provide observability or correlation.\n\n## Key Concepts\n- Distributed tracing and trace context propagation (e.g., W3C Trace Context)\n- Log correlation with trace IDs\n- Cross-service metrics-traces-logs integration\n\n## Real-World Application\n- Instrument all services with OpenTelemetry, propagate trace context, and include trace_id in logs; centralize traces, logs, and metrics in a single backend for interactive dashboards.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","OpenTelemetry","Jaeger","Prometheus","Grafana","Istio","Observability","certification-mcq","domain-weight-8"],"channel":"kcna","subChannel":"cloud-native-observability","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T16:31:21.834Z","createdAt":"2026-01-12 16:31:22"},{"id":"kcna-cloud-native-observability-1768235481833-1","question":"When instrumenting a high-throughput application with OpenTelemetry, which sampling strategy balances trace fidelity with overhead?","answer":"[{\"id\":\"a\",\"text\":\"Sample 100% of traces at all times\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use probabilistic sampling with a fixed rate and adjust dynamically based on traffic and service criticality\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Disable sampling for all services\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Sample only during business hours\",\"isCorrect\":false}]","explanation":"## Correct Answer\nB. Probabilistic sampling with a fixed rate and dynamic adjustments balances fidelity and overhead.\n\n## Why Other Options Are Wrong\n- A: 100% sampling creates excessive overhead and storage/\n- B: Correct approach balances visibility and cost.\n- C: Disables tracing visibility entirely.\n- D: Introduces time-based bias and misses incidents outside business hours.\n\n## Key Concepts\n- Sampling strategies (probabilistic vs deterministic)\n- OpenTelemetry sampler configuration\n- Throughput-aware observability\n\n## Real-World Application\n- Start with a modest rate (e.g., 5â€“20%) and increase for high-value endpoints or critical services, then observe latency and error rates to tune.","diagram":null,"difficulty":"intermediate","tags":["OpenTelemetry","Tracing","Prometheus","Kubernetes","certification-mcq","domain-weight-8"],"channel":"kcna","subChannel":"cloud-native-observability","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T16:31:22.371Z","createdAt":"2026-01-12 16:31:22"},{"id":"kcna-cloud-native-observability-1768235481833-2","question":"To understand latency distribution across endpoints in Prometheus-based observability, which metric type and aggregation should you rely on?","answer":"[{\"id\":\"a\",\"text\":\"Gauge metric for average latency\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Histogram with bucketed latency, using histogram_quantile to derive percentiles\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Counter for latency\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Summary metric for percentiles only\",\"isCorrect\":false}]","explanation":"## Correct Answer\nB. Histograms with buckets enable percentile calculations via histogram_quantile, giving latency distribution insights.\n\n## Why Other Options Are Wrong\n- A: Averages hide tail latency and distribution shape.\n- C: A latency metric as a counter is not meaningful.\n- D: Summaries can be used but lack stable bucket-based percentile calculations in Prometheus dashboards.\n\n## Key Concepts\n- Histograms enable percentile calculations\n- PromQL histogram_quantile for latency percentiles\n- Importance of latency distribution over simple averages\n\n## Real-World Application\n- Instrument latency with histogram buckets across critical endpoints and query with histogram_quantile to alert on p95/p99 latencies.","diagram":null,"difficulty":"intermediate","tags":["Prometheus","PromQL","Observability","Kubernetes","certification-mcq","domain-weight-8"],"channel":"kcna","subChannel":"cloud-native-observability","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T16:31:22.939Z","createdAt":"2026-01-12 16:31:23"},{"id":"kcna-cloud-native-observability-1768235481833-3","question":"In a service mesh environment (Istio), which combination best provides per-service request rate, latency, and error metrics along with distributed traces?","answer":"[{\"id\":\"a\",\"text\":\"Enable Istio telemetry with Prometheus metrics and Jaeger tracing\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Rely solely on application logs\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use only kube-state-metrics\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Rely on node-level metrics only\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA. Istio telemetry with Prometheus metrics and Jaeger tracing provides per-service metrics and distributed traces.\n\n## Why Other Options Are Wrong\n- B: Logs alone lack quantitative service-level metrics and tracing.\n- C: Kube-state-metrics does not capture per-service latency or traces.\n- D: Node-level metrics miss service-mmesh-level observability.\n\n## Key Concepts\n- Istio telemetry, Prometheus, Jaeger/Tempo\n- Service mesh observability patterns\n- End-to-end tracing across services\n\n## Real-World Application\n- Enable Istio telemetry to emit metrics and traces; visualize with Grafana and trace backends for cross-service debugging.","diagram":null,"difficulty":"intermediate","tags":["Istio","Prometheus","Jaeger","Kubernetes","Observability","certification-mcq","domain-weight-8"],"channel":"kcna","subChannel":"cloud-native-observability","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T16:31:23.118Z","createdAt":"2026-01-12 16:31:23"},{"id":"kcna-cloud-native-observability-1768235481833-4","question":"What practice ensures logs can be correlated with traces across pods for effective debugging in a distributed system?","answer":"[{\"id\":\"a\",\"text\":\"Use separate, uncorrelated logs per service with no trace identifiers\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Instrument logs with trace IDs and centralize in a log backend (e.g., Loki) to enable cross-service correlation with traces and metrics\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Disable distributed tracing and rely only on logs\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Write logs to local files without centralization\",\"isCorrect\":false}]","explanation":"## Correct Answer\nB. Instrument logs with trace IDs and centralize in a log backend (e.g., Loki) to enable cross-service correlation with traces and metrics.\n\n## Why Other Options Are Wrong\n- A: Without trace IDs, cross-service correlation is not possible.\n- C: Disabling tracing reduces end-to-end visibility.\n- D: Local, uncentralized logs hinder cross-pod correlation.\n\n## Key Concepts\n- Trace-context propagation in logs\n- Centralized log storage (e.g., Loki, Elasticsearch)\n- End-to-end debugging with correlated logs, traces, and metrics\n\n## Real-World Application\n- Enrich log lines with trace_id and span_id, ship to a central store, and use Grafana/Tempo/Jaeger for cross-service debugging.","diagram":null,"difficulty":"intermediate","tags":["Logging","OpenTelemetry","Loki","Tempo","Kubernetes","Observability","certification-mcq","domain-weight-8"],"channel":"kcna","subChannel":"cloud-native-observability","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T16:31:23.314Z","createdAt":"2026-01-12 16:31:23"},{"id":"kcna-container-orchestration-1768193476058-0","question":"In a three-node Kubernetes cluster, you want a deployment to evenly distribute its pods across nodes and adapt as nodes are added or removed. Which scheduling feature should you enable?","answer":"[{\"id\":\"a\",\"text\":\"nodeSelector\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"taints and tolerations\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"podAffinity\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"TopologySpreadConstraints\",\"isCorrect\":true}]","explanation":"## Correct Answer\n**TopologySpreadConstraints** evenly distributes Pods across topology domains such as nodes, helping the scheduler minimize skew. In this scenario, enabling topology spread ensures pods are not concentrated on a subset of nodes as nodes change.\n\n## Why Other Options Are Wrong\n- **nodeSelector** pins Pods to specific nodes and does not guarantee even distribution\n- **taints and tolerations** influence scheduling but do not guarantee even spread\n- **podAffinity** can encourage co-location of pods, which may worsen skew rather than improve it\n\n## Key Concepts\n- **TopologySpreadConstraints**\n- topologyKey (e.g., kubernetes.io/hostname)\n- workload distribution across nodes\n\n## Real-World Application\nApply topologySpreadConstraints to the PodSpec in your Deployment:\n\n```yaml\nspec:\n  template:\n    spec:\n      topologySpreadConstraints:\n      - maxSkew: 1\n        topologyKey: kubernetes.io/hostname\n        whenUnsatisfiable: DoNotSchedule\n```\nThis configuration ensures pods spread evenly across nodes and adapts when nodes are added or removed.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","kubectl","TopologySpreadConstraints","EKS","AWS","Terraform","Container Orchestration","certification-mcq","domain-weight-22"],"channel":"kcna","subChannel":"container-orchestration","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T04:51:16.059Z","createdAt":"2026-01-12 04:51:16"},{"id":"kcna-container-orchestration-1768193476058-1","question":"In an AWS-hosted Kubernetes cluster (e.g., EKS), which Service type automatically provisions a cloud load balancer to expose the service to the internet with a stable external endpoint?","answer":"[{\"id\":\"a\",\"text\":\"ClusterIP\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"NodePort\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"LoadBalancer\",\"isCorrect\":true},{\"id\":\"d\",\"text\":\"ExternalName\",\"isCorrect\":false}]","explanation":"## Correct Answer\n**LoadBalancer** service automatically provisions an AWS Elastic Load Balancer (ELB) for the cluster and provides a stable external endpoint. \n\n## Why Other Options Are Wrong\n- **ClusterIP** only exposes the service within the cluster\n- **NodePort** opens a port on each node but does not automatically create a cloud LB with a stable external address\n- **ExternalName** maps the service to an external DNS name; it does not create an internal cluster service with a load balancer\n\n## Key Concepts\n- **Service types**: ClusterIP, NodePort, LoadBalancer\n- Cloud provider integration on **AWS/EKS** to provision external resources\n\n## Real-World Application\nIn production on AWS/EKS, use a Service of type **LoadBalancer** to obtain a public endpoint that remains stable for the life of the load balancer. For advanced routing patterns, you can pair this with an Ingress controller (e.g., ALB Ingress Controller).\n","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","kubectl","LoadBalancer","EKS","AWS","Terraform","Container Orchestration","certification-mcq","domain-weight-22"],"channel":"kcna","subChannel":"container-orchestration","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T04:51:16.470Z","createdAt":"2026-01-12 04:51:16"},{"id":"kcna-container-orchestration-1768193476058-2","question":"You rolled out a Deployment and want to revert to the previous revision if the new revision has issues. Which command should you run?","answer":"[{\"id\":\"a\",\"text\":\"kubectl rollout undo deployment/myapp\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"kubectl rollout restart deployment/myapp\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"kubectl apply -f deployment.yaml\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"kubectl delete deployment myapp\",\"isCorrect\":false}]","explanation":"## Correct Answer\n**kubectl rollout undo deployment/myapp** reverts to the previous revision of the Deployment by restoring the prior ReplicaSet. \n\n## Why Other Options Are Wrong\n- **kubectl rollout restart deployment/myapp** triggers a rolling restart of the current revision rather than a rollback\n- **kubectl apply -f deployment.yaml** applies changes but does not automatically revert to a previous revision\n- **kubectl delete deployment myapp** removes the deployment entirely, not a rollback\n\n## Key Concepts\n- **kubectl rollout undo**\n- Deployment revision history and ReplicaSets\n- Rollout status monitoring\n\n## Real-World Application\nWhen a new rollout causes issues, run the undo command to quickly revert, then monitor with `kubectl rollout status deployment/myapp` and validate application health before deciding on subsequent steps.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","kubectl","rollout undo","EKS","AWS","Terraform","Container Orchestration","certification-mcq","domain-weight-22"],"channel":"kcna","subChannel":"container-orchestration","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T04:51:16.882Z","createdAt":"2026-01-12 04:51:16"},{"id":"kcna-container-orchestration-1768286173057-0","question":"In Kubernetes, a Deployment manages pods for a stateless application. To guarantee CPU resources and preserve QoS, which change ensures the pods receive Guaranteed QoS?","answer":"[{\"id\":\"a\",\"text\":\"Increase limits far beyond requests\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Set requests and limits to the same value\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Set only limits and leave requests unlimited\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Remove resource requests entirely\",\"isCorrect\":false}]","explanation":"## Correct Answer\n- The correct answer is B because Kubernetes assigns QoS classes based on how requests and limits are set. When every container in a pod has requests equal to limits, the pod is classified as Guaranteed, ensuring CPU resources are reserved as requested.\n\n## Why Other Options Are Wrong\n- A: Increasing limits without matching requests moves the pod to Burstable QoS, not Guaranteed.\n- C: Setting only limits with no requests means requests are zero, which does not grant Guaranteed QoS.\n- D: Removing requests eliminates the QoS guarantees entirely and can affect scheduling decisions.\n\n## Key Concepts\n- Resource requests and limits delineate scheduling and QoS classes\n- Guaranteed QoS requires equal requests and limits for all containers\n\n## Real-World Application\n- For critical services, configure equal CPU requests and limits, validate with kubectl describe pod to confirm the Guaranteed QoS class and monitor CPU usage under load.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","AWS EKS","Terraform","Container Orchestration","certification-mcq","domain-weight-22"],"channel":"kcna","subChannel":"container-orchestration","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T06:36:13.058Z","createdAt":"2026-01-13 06:36:13"},{"id":"kcna-container-orchestration-1768286173057-1","question":"Scenario: You have a Deployment with 4 replicas and want to guarantee at most one pod is unavailable during the rolling update. Which field should you configure to enforce this during updates?","answer":"[{\"id\":\"a\",\"text\":\"maxSurge\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"minReadySeconds\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"maxUnavailable\",\"isCorrect\":true},{\"id\":\"d\",\"text\":\"progressDeadlineSeconds\",\"isCorrect\":false}]","explanation":"## Correct Answer\n- The correct answer is C because maxUnavailable controls how many pods can be simultaneously unavailable during a RollingUpdate.\n\n## Why Other Options Are Wrong\n- A: maxSurge governs how many extra pods can be created during the update, not the number unavailable.\n- B: minReadySeconds delays starting new pods but does not cap unavailability.\n- D: progressDeadlineSeconds only sets a timeout for rollout progress; it does not limit concurrent unavailability.\n\n## Key Concepts\n- RollingUpdate strategy uses maxUnavailable and maxSurge\n- Availability during upgrades depends on these settings\n\n## Real-World Application\n- Tune maxUnavailable to meet HA requirements during deployments; validate with kubectl rollout status and observe pod readiness.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","AWS EKS","Terraform","Container Orchestration","certification-mcq","domain-weight-22"],"channel":"kcna","subChannel":"container-orchestration","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T06:36:13.426Z","createdAt":"2026-01-13 06:36:13"},{"id":"kcna-container-orchestration-1768286173057-2","question":"Scenario: You manage multiple namespaces and want to cap the total CPU usage per namespace to prevent resource exhaustion. Which object should you create?","answer":"[{\"id\":\"a\",\"text\":\"LimitRange\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"ResourceQuota\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"NetworkPolicy\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"PodDisruptionBudget\",\"isCorrect\":false}]","explanation":"## Correct Answer\n- The correct answer is B because ResourceQuota enforces aggregate resource limits per namespace, preventing over-consumption across all pods.\n\n## Why Other Options Are Wrong\n- A: LimitRange sets per-pod defaults but does not cap total namespace usage.\n- C: NetworkPolicy controls network traffic, not resource quotas.\n- D: PodDisruptionBudget ensures minimum available pods during evictions, not resource caps.\n\n## Key Concepts\n- ResourceQuota enforces per-namespace resource constraints\n- Limits can be measured for CPU, memory, and counts\n\n## Real-World Application\n- Apply namespace-scoped quotas to allocate fair share of cluster resources and prevent noisy neighbor issues.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","AWS EKS","Terraform","Container Orchestration","certification-mcq","domain-weight-22"],"channel":"kcna","subChannel":"container-orchestration","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T06:36:13.768Z","createdAt":"2026-01-13 06:36:13"},{"id":"kcna-container-orchestration-1768286173057-3","question":"Scenario: You want to restrict inbound traffic to pods in namespace 'backend' so that only pods from namespace 'frontend' can access port 8080. Which statement is true?","answer":"[{\"id\":\"a\",\"text\":\"You can achieve this without any CNI support\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"It is implemented by the CNI plugin and requires a policy selecting the relevant pods\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"It denies all ingress by default regardless of policies\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"It only applies to egress traffic\",\"isCorrect\":false}]","explanation":"## Correct Answer\n- The correct answer is B because NetworkPolicy enforcement is performed by the CNI (e.g., Calico, Cilium) and only affects pods selected by a policy; traffic from allowed namespaces/pods can reach the backend pods.\n\n## Why Other Options Are Wrong\n- A: Requires CNI support; without a compatible CNI, NetworkPolicy is not enforced.\n- C: Default deny behavior only occurs when a policy selects the pod; not automatic for all clusters.\n- D: NetworkPolicy applies to ingress and egress depending on policy definitions; this statement is incomplete.\n\n## Key Concepts\n- NetworkPolicy is implemented by CNI plugins\n- Policies only affect pods matched by the policy selectors\n\n## Real-World Application\n- Use Calico/Cilium to enforce namespace-scoped ingress rules and validate with traffic tests between namespaces.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","AWS EKS","Terraform","Container Orchestration","certification-mcq","domain-weight-22"],"channel":"kcna","subChannel":"container-orchestration","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T06:36:13.894Z","createdAt":"2026-01-13 06:36:13"},{"id":"kcna-container-orchestration-1768286173057-4","question":"Scenario: You plan to expose services externally using domain-based routing with TLS termination in a Kubernetes cluster. What must be true for the Ingress resource to work?","answer":"[{\"id\":\"a\",\"text\":\"Ingress Controller must be deployed in the cluster\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Ingress automatically creates a cloud load balancer without an Ingress controller\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Ingress only works with NodePort services\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Ingress TLS termination is not supported by any controller\",\"isCorrect\":false}]","explanation":"## Correct Answer\n- The correct answer is A because Ingress is an API object that requires an Ingress controller (such as NGINX, Traefik, or cloud-specific controllers) to implement the rules and TLS termination.\n\n## Why Other Options Are Wrong\n- B: Ingress does not automatically create load balancers; an Ingress controller is needed to provision a load balancer when using cloud providers.\n- C: Ingress works with various service types; NodePort is not a requirement.\n- D: TLS termination is supported by several controllers; it's not universally unsupported.\n\n## Key Concepts\n- Ingress defines routing rules; requires an Ingress controller\n- TLS termination is provided by controllers configured with certificates\n\n## Real-World Application\n- Deploy an Ingress controller (e.g., NGINX) and configure TLS certificates to expose services with domain-based routing.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","AWS EKS","Terraform","Container Orchestration","certification-mcq","domain-weight-22"],"channel":"kcna","subChannel":"container-orchestration","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T06:36:14.018Z","createdAt":"2026-01-13 06:36:14"},{"id":"q-1138","question":"You are building a real-time KCNA feed service used by Snap, Meta, and Discord-scale clients to publish and deliver announcements across regions with sub-100ms tail latency. Describe the end-to-end architecture, data model for Announcement, ingestion and delivery pipeline, guarantees (at-least-once vs exactly-once), ordering, deduplication, failover, tests, and observability. How would you scale to 10k updates/sec with 99.999% uptime?","answer":"Design a real-time KCNA feed with an event-sourced pipeline: publish to a durable log (Kafka/Kinesis), process via idempotent services, store state in a scalable DB, and stream updates to regional cac","explanation":"## Why This Is Asked\n\nThis question probes system design at scale, covering data modeling, ingestion pipelines, delivery guarantees, ordering, and observability in a globally distributed, low-latency context.\n\n## Key Concepts\n\n- Event sourcing and durable logs (Kafka/Kinesis)\n- Idempotent processing and deduplication\n- Region-local vs global ordering\n- Delivery guarantees (at-least-once vs exactly-once)\n- Observability, testing, and chaos engineering\n\n## Code Example\n\n```javascript\n// Example: idempotent publish wrapper\nfunction publishAnnouncement(store, event) {\n  const id = event.id;\n  if (store.has(id)) return; // dedup\n  store.set(id, event);\n  // emit to downstream\n}\n```\n\n## Follow-up Questions\n\n- How would you design feature flags for regional rollouts and rollback strategies?\n- What monitoring and tracing would you implement to detect tail latency regressions?","diagram":"flowchart TD\n  Ingest[Ingest] --> Proc[Process]\n  Proc --> Ent[Event Store]\n  Ent --> Del[Delivery]\n  Del --> Client[Client]","difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Meta","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T01:26:25.116Z","createdAt":"2026-01-13T01:26:25.116Z"},{"id":"q-1295","question":"**KCNA Consumer Backpressure & Gap Handling**: In a beginner-friendly KCNA consumer, design a pull-based ingestion path that preserves per-topic offsets, guarantees at-least-once processing, and recovers from transient network slowdowns without duplicating messages. Describe the API shape, offset persistence, retry/backoff strategy, and a minimal test plan including a canary scenario?","answer":"Proposed answer (concise example): A pull-based consumer tracks per-topic offsets in a durable local store, commits after successful processing, and uses idempotent handlers. Retries use exponential b","explanation":"## Why This Is Asked\nThis question probes practical dataflow design for reliable at-least-once delivery and simple backpressure handling in a beginner context.\n\n## Key Concepts\n- Pull-based consumption with per-topic offsets\n- Idempotent processing and offset commits\n- Retry/backoff with jitter and restart replay\n- Canary and integration testing\n\n## Code Example\n```javascript\n// Pseudo API sketch\nclass KCNAConsumer {\n  constructor(store, process) { this.store=store; this.process=process }\n  async poll() { /* fetch by offset, call process, commit */ }\n  commit(offset) { this.store.save(offset) }\n}\n```\n\n## Follow-up Questions\n- How would you test exactly-once vs at-least-once boundaries in this setup?\n- How would you extend this to handle multiple topics with independent offsets?","diagram":null,"difficulty":"beginner","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Plaid","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T08:41:04.859Z","createdAt":"2026-01-13T08:41:04.859Z"},{"id":"q-941","question":"Scenario: A global chat platform with 2B MAUs must detect policy-violating content (spam, hate speech) in near real-time while preserving user privacy and multilingual support. Propose an end-to-end pipeline: ingestion, moderation models (rules + ML), latency SLOs (1-2s), privacy safeguards, backpressure handling, retries, and dead-letter queues. Compare on-device vs cloud inference and monitoring?","answer":"Propose a tiered moderation pipeline: client-side tokenization + on-device classifier for first-pass filtering (multilingual light-weight model), with encrypted message IDs, then server-side streaming","explanation":"## Why This Is Asked\nThis question gauges real-time, scalable moderation design, privacy-safe multi-language handling, and the trade-offs between edge and cloud inference.\n\n## Key Concepts\n- Real-time streaming pipelines, SLOs, backpressure\n- Edge (on-device) vs cloud inference, multilingual models\n- Privacy safeguards (encryption, minimal data)\n- DLQ, retries, circuit breakers, monitoring\n\n## Code Example\n```javascript\n// Latency guard example\nif (latencyMs > 2000) {\n  tagAsSlowPath();\n  redirectToFallbackQueue();\n}\n```\n\n## Follow-up Questions\n- How would you validate model drift and false positives in production?\n- What metrics would you surface in dashboards to detect abuse transparently?","diagram":"flowchart TD\n  A[Ingest] --> B[Queue]\n  B --> C[Moderation]\n  C --> D[Enforce/Notify]\n  D --> E[Audit]","difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Slack","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T16:31:29.566Z","createdAt":"2026-01-12T16:31:29.566Z"},{"id":"kcna-k8s-fundamentals-1768151951289-0","question":"A Deployment creates Pods that start Running but show 0/1 READY, and the corresponding Service has 0 endpoints. Which change is most likely to fix the issue?","answer":"[{\"id\":\"a\",\"text\":\"Correct the readinessProbe to ensure it reports readiness (e.g., correct path/port) so the Pod becomes Ready.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Increase the number of replicas to 2.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Switch the Service type to NodePort to bypass readiness.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Remove the livenessProbe.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe Pod never reports Ready, so the endpoint set for the Service remains empty. Fixing the readinessProbe so the container signals readiness (correct path/port) makes the Pod Ready and populates endpoints.\n\n## Why Other Options Are Wrong\n- Increase replicas doesn't solve readiness of individual pods; endpoints remain 0 if pods aren't ready.\n- Switching to NodePort does not bypass readiness checks; traffic will still not be routed until the Pod reports Ready.\n- Removing the livenessProbe does not affect initial readiness state.\n\n## Key Concepts\n- Readiness probes control service endpoints exposure\n- Service endpoints reflect pods that report Ready\n- Kubernetes networking routes traffic to ready pods only\n\n## Real-World Application\n- When deploying new versions, ensure readiness probes accurately reflect application readiness to avoid routing errors and user-visible failures.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","Networking","PodReadiness","Probes","certification-mcq","domain-weight-46"],"channel":"kcna","subChannel":"k8s-fundamentals","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T17:19:11.290Z","createdAt":"2026-01-11 17:19:11"},{"id":"kcna-k8s-fundamentals-1768151951289-1","question":"A Pod in namespace-a needs to access a ConfigMap in namespace-b; RBAC denies cross-namespace access by default. Which RBAC setup correctly grants this access across namespaces?","answer":"[{\"id\":\"a\",\"text\":\"Create a Role in namespace-a granting get on ConfigMaps in namespace-a and bind it in namespace-a.\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Create a ClusterRole that permits get on configmaps and a ClusterRoleBinding binding the Pod's ServiceAccount from namespace-a across namespaces.\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Create a Role in namespace-b granting get on ConfigMaps in namespace-b and bind it in namespace-b.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Create a ServiceAccount in namespace-a and bind to a Role in namespace-a.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct because cluster-scoped roles (ClusterRole) with a ClusterRoleBinding can grant permissions across namespaces to a ServiceAccount. This allows the SA in namespace-a to perform the configured access on ConfigMaps (in namespace-b) depending on the policy.\n\n## Why Other Options Are Wrong\n- A incorrectly limits permissions to namespace-a, not enabling cross-namespace access.\n- C binds within namespace-b, which does not grant cross-namespace access from namespace-a.\n- D binds within namespace-a but does not grant cross-namespace access to ConfigMaps.\n\n## Key Concepts\n- RBAC: ClusterRole vs Role semantics\n- ClusterRoleBinding can grant permissions across namespaces\n- ServiceAccount scope and subject namespace matter\n\n## Real-World Application\n- Designing cross-namespace access in multi-tenant clusters requires ClusterRole-based bindings to avoid per-namespace role duplication.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","RBAC","ClusterRole","CrossNamespace","certification-mcq","domain-weight-46"],"channel":"kcna","subChannel":"k8s-fundamentals","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T17:19:11.705Z","createdAt":"2026-01-11 17:19:12"},{"id":"kcna-k8s-fundamentals-1768151951289-2","question":"During a rolling update of a Deployment with 5 replicas, you want to guarantee at least 4 pods are always available and not more than one can be unavailable during the update. Which configuration achieves this?","answer":"[{\"id\":\"a\",\"text\":\"Set RollingUpdate strategy with maxUnavailable: 1 and create a PodDisruptionBudget with minAvailable: 4.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Set RollingUpdate strategy with maxUnavailable: 0 and minAvailable: 5.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use RollingUpdate with maxSurge: 2 and PodDisruptionBudget minAvailable: 3.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use Recreate strategy with PodDisruptionBudget minAvailable: 5.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A is correct because maxUnavailable: 1 ensures at most one pod is down during the update, and PodDisruptionBudget with minAvailable: 4 guarantees at least four pods are always available.\n\n## Why Other Options Are Wrong\n- B is overly strict and would block updates since zero pods can be unavailable, which is not required by the scenario.\n- C uses minAvailable 3, which would allow up to two pods to be down during disruptions, not meeting the 4-pod availability requirement.\n- D uses the Recreate strategy which stops all pods before recreating them, not providing rolling updates.\n\n## Key Concepts\n- Deployment rollingUpdate controls maxUnavailable\n- PodDisruptionBudget constrains voluntary disruptions\n- Combined settings yield predictable upgrade behavior\n\n## Real-World Application\n- In production, aligning deployment strategy with PDB prevents service outage during upgrades and maintenance.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","PodDisruptionBudget","RollingUpdate","Deployment","certification-mcq","domain-weight-46"],"channel":"kcna","subChannel":"k8s-fundamentals","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T17:19:12.138Z","createdAt":"2026-01-11 17:19:12"},{"id":"kcna-k8s-fundamentals-1768267416129-0","question":"A node in your Kubernetes cluster is tainted with dedicated=web:NoSchedule. You want your Deployment pods to be scheduled only on this node. Which change is required?","answer":"[{\"id\":\"a\",\"text\":\"Add a toleration for key dedicated, value web, and effect NoSchedule in the PodSpec\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Add a nodeSelector for dedicated=web in the PodSpec\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Add a requiredDuringSchedulingIgnoredDuringExecution nodeAffinity for label dedicated=web\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Apply a PodSecurityPolicy to permit taints to be used\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA. Tolerations in PodSpec allow scheduling on tainted nodes.\n\n## Why Other Options Are Wrong\n- B) NodeSelector cannot bypass taints; taints prevent scheduling unless a toleration exists. \n- C) NodeAffinity relies on matching labels, not taints, and will not bypass taints by itself. \n- D) PodSecurityPolicy is unrelated to taint-based scheduling.\n\n## Key Concepts\n- Taints and tolerations control which nodes a pod can be scheduled on.\n- NodeSelector/NodeAffinity use labels, not taints.\n\n## Real-World Application\n- Use tainted nodes to isolate workloads (e.g., dedicated web nodes) and add corresponding tolerations to the pods that must run there.\n","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","EKS","kubectl","Helm","Terraform","certification-mcq","domain-weight-46"],"channel":"kcna","subChannel":"k8s-fundamentals","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T01:23:36.130Z","createdAt":"2026-01-13 01:23:36"},{"id":"kcna-k8s-fundamentals-1768267416129-1","question":"You need dynamic configuration updates from a ConfigMap to reflect in a running pod without restarting the pod. Which pattern is recommended for minimal downtime?","answer":"[{\"id\":\"a\",\"text\":\"Use environment variables sourced from the ConfigMap, as they update automatically\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Mount the ConfigMap as files in a volume and have the application watch the files for changes\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Trigger a rolling restart by editing the Deployment annotation each time the ConfigMap changes\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Rely on Kubernetes to push ConfigMap changes to the running pods automatically\",\"isCorrect\":false}]","explanation":"## Correct Answer\nB. Mounting the ConfigMap as files in a volume allows the application to monitor the mounted files for changes and reload configuration without a full pod restart.\n\n## Why Other Options Are Wrong\n- A) Env vars are set at Pod start and do not update automatically when the ConfigMap changes.\n- C) Rolling restarts cause downtime and are not dynamic updates.\n- D) Kubernetes does not push ConfigMap changes to running containers automatically.\n\n## Key Concepts\n- ConfigMaps can be exposed to pods via volumes (files) for dynamic reload if the application supports it.\n- Applications must be capable of reloading config on file change.\n\n## Real-World Application\n- Preferred pattern for apps needing live-config reloads (e.g., feature flags, external service endpoints) with minimal user-visible downtime.\n","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","EKS","kubectl","Helm","Terraform","certification-mcq","domain-weight-46"],"channel":"kcna","subChannel":"k8s-fundamentals","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T01:23:36.798Z","createdAt":"2026-01-13 01:23:37"},{"id":"kcna-k8s-fundamentals-1768267416129-2","question":"In a service that handles HTTP requests, you want traffic to be directed only to pods that are ready to serve requests. Which Kubernetes mechanism achieves this?","answer":"[{\"id\":\"a\",\"text\":\"Liveness probe\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Readiness probe\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Startup probe\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Affinity rules\",\"isCorrect\":false}]","explanation":"## Correct Answer\nB. Readiness probes indicate when a pod is ready to receive traffic; the endpoints controller only includes ready pods in the service endpoint list.\n\n## Why Other Options Are Wrong\n- A) Liveness probes detect if a pod is alive; they do not influence traffic routing.\n- C) Startup probes determine startup completion, not ongoing readiness for traffic.\n- D) Affinity rules influence placement, not traffic readiness.\n\n## Key Concepts\n- Readiness probes control service endpoints and traffic routing.\n- Liveness probes control health but not initial routing readiness.\n\n## Real-World Application\n- Implement readiness checks for microservices that may take time to initialize, ensuring users donâ€™t hit pods that arenâ€™t yet ready.\n","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","EKS","kubectl","Helm","Terraform","certification-mcq","domain-weight-46"],"channel":"kcna","subChannel":"k8s-fundamentals","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T01:23:37.262Z","createdAt":"2026-01-13 01:23:37"},{"id":"kcna-k8s-fundamentals-1768267416129-3","question":"To provide stable network identities for each replica in a scalable stateful service, which combination is recommended?","answer":"[{\"id\":\"a\",\"text\":\"Deployment with a ClusterIP service\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"StatefulSet with a Headless Service (clusterIP None)\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"DaemonSet with a NodePort service\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"ReplicaSet with an ExternalName service\",\"isCorrect\":false}]","explanation":"## Correct Answer\nB. StatefulSet with a Headless Service provides stable network identities (hostname like web-0, web-1) for each replica, enabling ordered startup and predictable DNS.\n\n## Why Other Options Are Wrong\n- A) Deployments with ClusterIP do not provide stable per-pod identities across reschedules.\n- C) DaemonSets are for single pod per node, not stable per-replica DNS identities.\n- D) ReplicaSets do not guarantee stable network identities; ExternalName is not suitable for stable in-cluster DNS.\n\n## Key Concepts\n- StatefulSets with Headless Services enable stable DNS and storage identity for each replica.\n\n## Real-World Application\n- Use for databases or clustered stateful services requiring persistent identities across upgrades and scaling.\n","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","EKS","kubectl","Helm","Terraform","certification-mcq","domain-weight-46"],"channel":"kcna","subChannel":"k8s-fundamentals","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T01:23:37.417Z","createdAt":"2026-01-13 01:23:37"},{"id":"kcna-k8s-fundamentals-1768267416129-4","question":"You need to grant a user read/write access to pods, services, and configmaps only within the namespace 'prod'. Which RBAC pattern best achieves namespace-scoped permissions?","answer":"[{\"id\":\"a\",\"text\":\"ClusterRole and ClusterRoleBinding\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Role and RoleBinding\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Role and ClusterRoleBinding\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"ClusterRole and RoleBinding\",\"isCorrect\":false}]","explanation":"## Correct Answer\nB. A Role defines namespace-scoped permissions, and RoleBinding binds that Role to the user within the same namespace, providing scoped access.\n\n## Why Other Options Are Wrong\n- A) ClusterRole/ClusterRoleBinding grants cluster-wide permissions, not namespace-scoped.\n- C) Role + ClusterRoleBinding would bind a namespace-scoped role to a user at cluster scope, which is inconsistent.\n- D) ClusterRole with RoleBinding would attempt to bind cluster-wide permissions to a namespace, which is not correct scope.\n\n## Key Concepts\n- Role defines permissions within a namespace; RoleBinding grants those permissions to a user/serviceaccount in that namespace.\n\n## Real-World Application\n- Use Role and RoleBinding to delegate namespace-scoped access control for separate environments (e.g., prod, dev) without widening access across the cluster.\n","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","EKS","kubectl","Helm","Terraform","certification-mcq","domain-weight-46"],"channel":"kcna","subChannel":"k8s-fundamentals","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T01:23:37.573Z","createdAt":"2026-01-13 01:23:37"}],"subChannels":["cloud-native-arch","cloud-native-delivery","cloud-native-observability","container-orchestration","general","k8s-fundamentals"],"companies":["Discord","Meta","Plaid","Slack","Snap","Twitter","Two Sigma"],"stats":{"total":34,"beginner":1,"intermediate":31,"advanced":2,"newThisWeek":34}}