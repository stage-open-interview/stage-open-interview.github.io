{"questions":[{"id":"q-1138","question":"You are building a real-time KCNA feed service used by Snap, Meta, and Discord-scale clients to publish and deliver announcements across regions with sub-100ms tail latency. Describe the end-to-end architecture, data model for Announcement, ingestion and delivery pipeline, guarantees (at-least-once vs exactly-once), ordering, deduplication, failover, tests, and observability. How would you scale to 10k updates/sec with 99.999% uptime?","answer":"Design a real-time KCNA feed with an event-sourced pipeline: publish to a durable log (Kafka/Kinesis), process via idempotent services, store state in a scalable DB, and stream updates to regional cac","explanation":"## Why This Is Asked\n\nThis question probes system design at scale, covering data modeling, ingestion pipelines, delivery guarantees, ordering, and observability in a globally distributed, low-latency context.\n\n## Key Concepts\n\n- Event sourcing and durable logs (Kafka/Kinesis)\n- Idempotent processing and deduplication\n- Region-local vs global ordering\n- Delivery guarantees (at-least-once vs exactly-once)\n- Observability, testing, and chaos engineering\n\n## Code Example\n\n```javascript\n// Example: idempotent publish wrapper\nfunction publishAnnouncement(store, event) {\n  const id = event.id;\n  if (store.has(id)) return; // dedup\n  store.set(id, event);\n  // emit to downstream\n}\n```\n\n## Follow-up Questions\n\n- How would you design feature flags for regional rollouts and rollback strategies?\n- What monitoring and tracing would you implement to detect tail latency regressions?","diagram":"flowchart TD\n  Ingest[Ingest] --> Proc[Process]\n  Proc --> Ent[Event Store]\n  Ent --> Del[Delivery]\n  Del --> Client[Client]","difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Meta","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T01:26:25.116Z","createdAt":"2026-01-13T01:26:25.116Z"},{"id":"q-1295","question":"**KCNA Consumer Backpressure & Gap Handling**: In a beginner-friendly KCNA consumer, design a pull-based ingestion path that preserves per-topic offsets, guarantees at-least-once processing, and recovers from transient network slowdowns without duplicating messages. Describe the API shape, offset persistence, retry/backoff strategy, and a minimal test plan including a canary scenario?","answer":"Proposed answer (concise example): A pull-based consumer tracks per-topic offsets in a durable local store, commits after successful processing, and uses idempotent handlers. Retries use exponential b","explanation":"## Why This Is Asked\nThis question probes practical dataflow design for reliable at-least-once delivery and simple backpressure handling in a beginner context.\n\n## Key Concepts\n- Pull-based consumption with per-topic offsets\n- Idempotent processing and offset commits\n- Retry/backoff with jitter and restart replay\n- Canary and integration testing\n\n## Code Example\n```javascript\n// Pseudo API sketch\nclass KCNAConsumer {\n  constructor(store, process) { this.store=store; this.process=process }\n  async poll() { /* fetch by offset, call process, commit */ }\n  commit(offset) { this.store.save(offset) }\n}\n```\n\n## Follow-up Questions\n- How would you test exactly-once vs at-least-once boundaries in this setup?\n- How would you extend this to handle multiple topics with independent offsets?","diagram":null,"difficulty":"beginner","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Plaid","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T08:41:04.859Z","createdAt":"2026-01-13T08:41:04.859Z"},{"id":"q-1431","question":"KCNA cross-region, multi-tenant QoS: Propose an architecture and API for declaring per-tenant Topic SLAs (retention, max throughput) and a cross-region offset store with region-local commit logs. How would you implement per-tenant backpressure, quota enforcement, and exactly-once vs at-least-once semantics under regional outages? Include a concrete canary and testing plan?","answer":"Implement a quota ledger per tenant, topic-level SLA, region-local offsets, and a global commit log. Enforce producer backpressure by dynamically throttling when quotas near limit; ensure exactly-once","explanation":"## Why This Is Asked\nTests ability to design multi-region, multi-tenant KCNA with fair sharing, fault tolerance, and strong guarantees.\n\n## Key Concepts\n- Per-tenant QoS and SLAs\n- Cross-region offsets and commit log\n- Backpressure and quota enforcement\n- Exactly-once vs at-least-once tradeoffs\n- Canary testing and regional outages\n\n## Code Example\n```javascript\n// TS types for TopicSpec and QuotaLedger\ntype TopicSpec = { name:string; retentionMs:number; maxThroughputQps:number; tenant:string };\ntype QuotaLedger = Map<string, number>; // tenant -> remainingQuota\n```\n\n## Follow-up Questions\n- How would you test under bursty traffic and a regional partition?\n- How do you handle tenant migration between regions?\n","diagram":"flowchart TD\n  A[Tenant] --> B[Topic]\n  B --> C[Partition]\n  A --> D[QuotaLedger]\n  C --> E[OffsetsStore]\n  D --> F[ThrottleEngine]\n  F --> G[RegionalGateway]\n  G --> H[GlobalCommitLog]","difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Oracle","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T16:53:08.237Z","createdAt":"2026-01-13T16:53:08.237Z"},{"id":"q-1471","question":"KCNA multi-tenant schema-evolution: design a zero-downtime migration for a KCNA-based event bus used by many tenants where the Event schema evolves from v1 to v2 (add region field, deprecate payload wrapper). How do you enforce backward/forward compatibility, isolate tenants during migration, and validate with canaries? Include tooling, rollback plans, and observability?","answer":"Adopt a central versioned KCNA Schema Registry with per-tenant namespaces and per-topic compatibility. For v1→v2, add an optional region field and keep existing fields. Run a migrator that rewrites in","explanation":"## Why This Is Asked\n\nTests ability to design scalable, safe schema migrations in a multi-tenant KCNA setup, including compatibility strategies, canary rollout, and rollback handling.\n\n## Key Concepts\n\n- Schema Registry with versioned, per-tenant schemas\n- Backward, forward, and full compatibility modes\n- Canary deployments and per-tenant offset preservation\n- In-flight data migration and rollback plans\n\n## Code Example\n\n```javascript\nfunction isBackwardCompatible(oldSchema, newSchema) {\n  // Added fields must be optional; no required removals\n  // No type-breaking changes\n  return true;\n}\n```\n\n## Follow-up Questions\n\n- How would you monitor for schema drift and consumer failures during migration?\n- What metrics and alerts would you add to ensure safe rollback timing?","diagram":"flowchart TD\n  A[Producer] --> B[KCNA Topic]\n  B --> C[Schema Registry]\n  C --> D[Versioned Schemas]\n  D --> E[Migration Orchestrator]\n  E --> F[Canary Tenants]\n  F --> G[Rollout to All Tenants]","difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Stripe","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T18:46:26.092Z","createdAt":"2026-01-13T18:46:26.092Z"},{"id":"q-1503","question":"KCNA cross-region tenancy isolation: design a replication topology where tenants' streams stay regional unless opted into global analytics; implement per-tenant topic partitioning, region-aware routing, and idempotent retries with de-dup. How do you enforce locality, protect privacy in cross-region analytics, and handle failover/lag? Include concrete configs, testing strategy, and rollback plan?","answer":"Use region-local KCNA clusters with tenant-scoped partitions and a policy gate for opt-in analytics. Route data by tenant region, avoid cross-region replicas unless flagged, and apply idempotent produ","explanation":"## Why This Is Asked\nTo examine practical cross-region data locality, tenancy boundaries, and privacy-preserving analytics with KCNA, plus testing/rollback discipline.\n\n## Key Concepts\n- Region-local clusters\n- Per-tenant partitions and offsets\n- Opt-in analytics gating\n- Idempotent producers/consumers\n- Observability and rollback\n\n## Code Example\n```javascript\n// Example policy snippet\nconst policy = {\n  tenants: {\n    A: { region: 'us-east-1', analytics: false },\n    B: { region: 'eu-west-1', analytics: true }\n  },\n  globalAnalyticsEnabled: true\n}\n```\n\n## Follow-up Questions\n- How do you monitor cross-region privacy boundaries and lag?\n- What tests would you run to validate failover without impacting tenants?","diagram":"flowchart TD\n  A[Tenant streams] --> B[Region-local KCNA]\n  B --> C[Policy gate]\n  C --> D[Global analytics (optional)]\n  D --> E[Sinks]","difficulty":"intermediate","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T19:40:00.393Z","createdAt":"2026-01-13T19:40:00.393Z"},{"id":"q-1520","question":"KCNA TTL Retention: design a per-topic TTL policy for KCNA events. How would you store TTL metadata, drop expired events without breaking consumer offsets, handle late-arriving events post-expiry, and observe/verify with canary tests? Provide a minimal API shape for setting TTL per topic, a compact storage layout, and a lightweight cleanup workflow?","answer":"Store per-topic TTL in a lightweight index: topic -> expiry epoch. Each message carries publishTime; a background cleaner deletes messages older than TTL while advancing a deletion frontier to preserv","explanation":"## Why This Is Asked\nThis tests understanding of retention, per-topic configuration, and safe cleanup without disturbing consumer progress.\n\n## Key Concepts\n- TTL metadata storage per topic\n- Deletion frontier and per-topic offsets\n- Late-arriving vs expired events\n- Observability and canary validation\n\n## Code Example\n```javascript\n// Pseudo TTL check\nfunction isExpired(msg, ttlSec, now=new Date()) { return (now.getTime() - msg.publishTime) > ttlSec*1000; }\n```\n\n## Follow-up Questions\n- How would you test TTL edge cases (exact expiry, late arrival)?\n- How do you prevent TTL cleanup from blocking high-priority topics? ","diagram":"flowchart TD\n  A[Topic TTL Policy] --> B[Cleanup Job]\n  B --> C[Expire Messages]\n  C --> D[Update Frontiers]\n  D --> E[Offets Consistent Delivery]","difficulty":"beginner","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Citadel","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T20:38:42.651Z","createdAt":"2026-01-13T20:38:42.651Z"},{"id":"q-1602","question":"KCNA key management & tenant isolation: In a **KCNA**-based multi-tenant event bus, each tenant uses a per-tenant envelope encryption key managed by a centralized **KMS**. Design a **zero-downtime** key rotation workflow that rotates tenant keys without breaking consumption, re-encrypts in-flight payloads, and prevents cross-tenant leakage. Include data model (**tenant_id**, key_version, wrapped_key), rollout strategy, rollback, and observability?","answer":"Implement envelope encryption with per-tenant keys managed by a centralized KMS. Each message includes key_version in metadata for decryption. During rotation, create a new key version, publish rotation tokens to consumers, and encrypt new messages under the new version while maintaining backward compatibility for existing in-flight messages.","explanation":"## Why This Is Asked\nTests practical handling of per-tenant security in KCNA, emphasizing zero-downtime rotation, data integrity, and cross-tenant isolation.\n\n## Key Concepts\n- Envelope encryption and per-tenant KMS key versions\n- In-flight data re-encryption and backward compatibility\n- Rollout strategies, canaries, and observability/audit trails\n\n## Code Example\n```javascript\n// Pseudo: resolve key for decryption\nfunction resolveKey(tenantId, payload) {\n  const v = fetchKeyVersion(tenantId, payload);\n  return kms.getKey(tenantId, v);\n}\n```\n\n## Follow-up Questions\n- How to revoke compromised tenant keys?\n- What monitoring metrics indicate successful rotation?\n- How to handle consumers that miss rotation tokens?","diagram":"flowchart TD\n  A[Rotation Initiation] --> B[Publish Metadata]\n  B --> C[Clients Fetch New Key Version]\n  C --> D[Re-encrypt New Messages]\n  D --> E[Grace Window for In-Flight Messages]\n  E --> F[Promote New Key Version]\n  F --> G[Observability & Audit]\n  G --> H[Validation Canary]","difficulty":"intermediate","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Microsoft","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T05:32:04.492Z","createdAt":"2026-01-14T02:31:03.848Z"},{"id":"q-1698","question":"Design a KCNA privacy-first feed where tenants specify a region (US/EU/APAC) and all data remains local. Use per-tenant envelope encryption with KMS, region-scoped brokers, field-level redaction before delivery, and tenant-aware access controls. Add audit trails and canary tests to prove zero cross-tenant leakage, correct redaction, and retention under burst load?","answer":"Design a KCNA privacy-first feed where tenants specify a region (US/EU/APAC) and all data remains local. Use per-tenant envelope encryption with KMS, region-scoped brokers, field-level redaction befor","explanation":"## Why This Is Asked\n\nThis question probes practical privacy-by-design in streaming: data residency, per-tenant cryptography, access control, auditing, and testability under burst traffic. It also checks operational thinking for cross-tenant isolation and retention policies.\n\n## Key Concepts\n\n- Per-tenant residency and tenancy isolation\n- Envelope encryption with KMS and tenant-scoped keys\n- Field-level redaction during transit and at rest\n- Region routing and data locality guarantees\n- Auditing, retention, and compliance controls\n- Canary-based validation under burst load and failure scenarios\n\n## Code Example\n\n```javascript\nfunction redact(record, schema) {\n  const redacted = {};\n  for (const [k, v] of Object.entries(record)) {\n    if (schema.redact?.includes(k)) redacted[k] = \"***\";\n    else redacted[k] = v;\n  }\n  return redacted;\n}\n```\n\n## Follow-up Questions\n\n- How would you test a migration that adds a new redaction rule without breaking existing tenants?\n- How would you verify cross-region data leakage is impossible during network partitions?\n","diagram":"flowchart TD\n  A[Tenant Config] --> B[Regional Router]\n  B --> C[KCNA Regional Broker]\n  C --> D[Envelope Encrypt with Tenant Key]\n  D --> E[Deliver to Region-Specific Client]","difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Tesla","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T07:34:51.806Z","createdAt":"2026-01-14T07:34:51.808Z"},{"id":"q-1723","question":"KCNA dynamic tenancy fairness under bursty workloads: design a per-tenant ingestion path with token-bucket quotas and fair queuing; detail API surface, quota persistence, backpressure signaling, and dynamic rebalancing from telemetry. How do you validate isolation under burst traffic, and what would your canary rollout look like?","answer":"Implement per-tenant KCNA ingestion using token-bucket quotas and a fair-queuing layer. API: POST /ingest with tenantId, topic, payload; on over-quota return 429 with Retry-After. Use telemetry-driven","explanation":"## Why This Is Asked\n\nAssess the candidate's ability to design per-tenant fairness in a high-throughput KCNA pipeline, balancing isolation, latency, and dynamic scaling under bursty traffic.\n\n## Key Concepts\n\n- Per-tenant token-bucket quotas and fair queuing\n- Backpressure signaling (429s, retry-after) and quota persistence\n- Telemetry-driven dynamic rebalancing and burst forgiveness\n- Canary-based validation and observability\n\n## Code Example\n\n```javascript\nclass TokenBucket {\n  constructor(rate, capacity) {\n    this.rate = rate;\n    this.capacity = capacity;\n    this.tokens = capacity;\n    this.last = Date.now();\n  }\n  tryConsume(n = 1) {\n    const now = Date.now();\n    const elapsed = (now - this.last) / 1000;\n    this.tokens = Math.min(this.capacity, this.tokens + elapsed * this.rate);\n    this.last = now;\n    if (this.tokens >= n) {\n      this.tokens -= n;\n      return true;\n    }\n    return false;\n  }\n}\n```\n\n## Follow-up Questions\n\n- How would you test dynamic quota rebalancing under tenant churn?\n- How do you prevent misbehaving tenants from starving others while preserving low latency?","diagram":null,"difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T08:41:19.803Z","createdAt":"2026-01-14T08:41:19.803Z"},{"id":"q-1806","question":"KCNA privacy-by-design: design a tenant-isolated KCNA ingestion and delivery path that enforces per-tenant encryption keys for in-flight and at-rest data, supports on-the-fly key rotation with zero-downtime, and provides auditable access controls for analytic consumers. Describe the KMS integration, key-wrapping strategy, performance impact, and a minimal canary test for rotation?","answer":"Per-tenant envelope encryption: each tenant has a data-key wrapped by KMS; producers encrypt payloads with tenant keys and rotate data-keys via versioned IDs with on-the-fly rewrapping of in-flight en","explanation":"## Why This Is Asked\\nPrivacy-first multi-tenant KCNA is a real challenge; rotation and audit are painful at scale.\\n\\n## Key Concepts\\n- Envelope encryption\\n- Per-tenant KMS keys\\n- On-the-fly rotation with no downtime\\n- Audit trails and access control\\n- Backward compatibility with legacy envelopes\\n- Canary tests for rotation\\n\\n## Code Example\\n```javascript\\n// Pseudo: envelope encrypt data per tenant\\nfunction encryptForTenant(tenantId, plaintext, version) {\\n  const keyId = kms.getKeyId(tenantId, version);\\n  const dataKey = kms.generateDataKey(tenantId, version);\\n  const ciphertext = crypto.aesGcmEncrypt(dataKey.plaintext, plaintext);\\n  const envelope = { keyId, cipher: ciphertext, iv: dataKey.iv };\\n  return envelope;\\n}\\n```\\n\\n## Follow-up Questions\\n- How to handle key compromise and revocation?\\n- How to measure rotation latency and impact on throughput?\\n- How to ensure consistent decryption across hot/cold storage?","diagram":null,"difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Robinhood","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T11:41:32.131Z","createdAt":"2026-01-14T11:41:32.131Z"},{"id":"q-1901","question":"KCNA Time-Travel Replay for Tenants: Suppose a tenant needs to audit a precise 2-hour window of events without halting live ingestion. Design a tenant-scoped time-travel replay feature in KCNA that allows replaying events from a given timestamp while live streams continue, guarantees exactly-once delivery to downstream analytics, and preserves per-tenant offsets. Describe API shapes, storage layout, consistency guarantees, security controls, and a minimal test plan (canaries)?","answer":"Implement a tenant-scoped replay plane that materializes a time-indexed replay log per tenant starting at given timestamp T. Downstream analytics subscribe to a replay stream with idempotent consumers","explanation":"## Why This Is Asked\nThe question probes how to add time-travel auditing without impacting live flow, focusing on idempotency, isolation, and operational safety in KCNA.\n\n## Key Concepts\n- Time-indexed replay per tenant\n- Tenant isolation and per-tenant offsets\n- Idempotent downstream delivery and replay-window semantics\n- Access control, auditing, observability\n- Canary-driven rollout and rollback\n\n## Code Example\n```javascript\n// Minimal API surface for replay\ninterface ReplayOptions { tenantId: string; startTs: number; endTs?: number; mode?: 'replay'|'live'; }\nfunction startTenantReplay(opts: ReplayOptions): Promise<ReplaySession>;\n```\n\n## Follow-up Questions\n- How would you ensure exactly-once semantics across distributed replay streams?\n- How would you monitor replay impact on backpressure and SLAs?\n","diagram":"flowchart TD\n  A(Tenant) --> B(Replay Plane)\n  B --> C(Replay Log)\n  A --> D(Live Ingestion)\n  C --> E(Analytics)\n  D --> E","difficulty":"intermediate","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Meta","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T16:45:56.511Z","createdAt":"2026-01-14T16:45:56.511Z"},{"id":"q-1926","question":"Design a beginner-friendly KCNA feature: tenant-scoped data TTL. Each tenant can configure a TTL (e.g., 24h) for their streams. Describe the API (setTTL on a topic with ttlMs), how per-message metadata is stored, how a background purge runs safely without breaking consumers, and a minimal test plan with canaries?","answer":"Implement per-tenant TTL with an API like POST /tenants/{id}/topics/{topic}/ttl { ttlMs }. Attach a ttlMs timestamp to each message and run a purge daemon that deletes messages older than TTL while em","explanation":"## Why This Is Asked\nThis question tests understanding of per-tenant data isolation, retention controls, and safe data purging in a live streaming system. It requires concrete API shape, data model decisions, and a practical testing strategy suitable for beginners while exposing real trade-offs.\n\n## Key Concepts\n- Tenant-scoped retention policy and TTL\n- Message metadata and tombstone semantics for replay\n- Purge safety relative to consumer offsets\n- Canary-based testing and end-to-end validation\n\n## Code Example\n```javascript\n// Pseudo: set TTL for a tenant-topic\nasync function setTTL(tenantId, topic, ttlMs) {\n  // persist TTL in config store per tenant-topic\n  await configStore.set(`/tenants/${tenantId}/topics/${topic}/ttl`, ttlMs);\n}\n\n// Pseudo: purge loop (simplified)\nfunction purgeOldMessages(tenantId, topic) {\n  const ttlMs = configStore.get(`/tenants/${tenantId}/topics/${topic}/ttl`);\n  const cutoff = Date.now() - ttlMs;\n  for (const msg of storage.scan(tenantId, topic)) {\n    if (msg.timestamp < cutoff) {\n      storage.delete(msg.id);\n      // emit tombstone to preserve replay semantics\n      storage.appendTombstone({ tenantId, topic, id: msg.id });\n    }\n  }\n}\n```\n\n## Follow-up Questions\n- How would you handle TTL changes mid-flight without losing data consistency?\n- How do you verify no live consumers are affected during purge windows?","diagram":null,"difficulty":"beginner","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T17:40:54.254Z","createdAt":"2026-01-14T17:40:54.254Z"},{"id":"q-2025","question":"KCNA ingest fairness at scale: design a per-tenant fair-queuing strategy for bursty producers in a multi-tenant KCNA channel. Implement a two-layer approach with a per-tenant token-bucket at the gateway and a global weighted-round-robin scheduler across tenants to prevent starvation. Define quotas, bounded bursts, and backpressure signaling; outline API contracts, config knobs, and a test plan with synthetic tenants and canaries?","answer":"Two-layer fairness: per-tenant token-bucket at gateway plus a global weighted RR scheduler across tenants. Enforce quotas and bounded bursts; unutilized tokens spill to a pending queue to avoid jitter","explanation":"## Why This Is Asked\nInterviews real-world scaling: fairness in multi-tenant KCNA ingestion under bursty traffic, preventing starvation and ensuring predictable latency for all tenants.\n\n## Key Concepts\n- Per-tenant quotas and bounded bursts\n- Gateways and global scheduling\n- Backpressure signaling and observability\n- Canary-style validation\n\n## Code Example\n```javascript\nclass TokenBucket {\n  constructor(rate, burst) { this.rate = rate; this.burst = burst; this.tokens = burst; this.last = Date.now(); }\n  allow(n=1){ this._drip(); if(this.tokens>=n){ this.tokens-=n; return true; } return false; }\n  _drip(){ const now=Date.now(); const elapsed=(now-this.last)/1000; this.tokens = Math.min(this.burst, this.tokens + elapsed*this.rate); this.last=now; }\n}\n```\n\n## Follow-up Questions\n- How would you monitor fairness and detect starvation?\n- How would quotas adapt during traffic spikes?","diagram":"flowchart TD\n  IngestRequest --> QuotaCheck\n  QuotaCheck -->|Allowed| GatewayQueue\n  GatewayQueue --> KCNA_Core\n  KCNA_Core --> Ack\n  KCNA_Core --> Backpressure\n","difficulty":"intermediate","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Hugging Face","IBM"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T21:33:52.678Z","createdAt":"2026-01-14T21:33:52.679Z"},{"id":"q-2101","question":"KCNA policy-driven tenant isolation: design a per-tenant access-control mechanism at the gateway and stream processors that enforces tenant-scoped authorization for ingest and delivery, supports dynamic policy updates with zero-downtime rollout, and provides an auditable per-event trail. Outline the policy model (tenants, roles, actions), integration with a policy engine (e.g., OPA/ABAC), JWT-based identity, and testing strategy including canaries and rollback?","answer":"Design a policy-driven gateway layer using ABAC with a central policy store (OPA). Each KCNA client presents a JWT containing tenant_id and roles; gateways enforce actions (ingest, read, admin) per resource. Stream processors inherit tenant context from the gateway, maintaining isolation throughout the pipeline. Policy updates are versioned and rolled out via canary deployments with automatic rollback on failure. All events are logged with tenant, action, and policy version for complete audit trails.","explanation":"## Why This Is Asked\nTo probe real-world policy, security, and reliability trade-offs in KCNA at scale.\n\n## Key Concepts\n- ABAC with tenant_id and roles\n- Central policy store (OPA)\n- JWT-based identity\n- Policy versioning and canary rollouts\n- Per-event auditing\n\n## Code Example\n```javascript\n// Pseudo gateway policy check\nconst ok = evalPolicy({tenant_id, action, resource});\nif (!ok) throw new Error('Access denied');\n```\n\n## Follow-up Questions\n- How would you test policy upgrades without impacting live tenants?\n- What guarantees exist for audit log integrity during rollback?","diagram":null,"difficulty":"intermediate","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","NVIDIA","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T05:00:32.575Z","createdAt":"2026-01-15T02:16:01.304Z"},{"id":"q-2135","question":"KCNA Dead-Letter & Poison Message Handling: For a multi-tenant KCNA deployment, design a per-tenant dead-letter queue strategy that routes malformed events out of the normal path, enforces per-tenant retry budgets, and preserves idempotent replays. Describe DLQ schema, routing rules, retention, operator visibility, and a safe rollout with canaries?","answer":"Route malformed events to a per-tenant DLQ and enforce per-tenant retry budgets. Use dedicated DLQ topics (dlq/tenant-{id}) with event_id and reason. Apply per-tenant retry cap (e.g., 3 attempts/hour)","explanation":"## Why This Is Asked\nThis checks robustness of error handling, tenant isolation, and rollout discipline in multi-tenant KCNA systems.\n\n## Key Concepts\n- Per-tenant DLQ routing and isolation\n- Retry budgeting and backoff strategy\n- Idempotent replays and deduplication\n- Retention, auditing, and operator visibility\n- Canary rollout and rollback plans\n\n## Code Example\n```javascript\n// Pseudo routing snippet\nif (!isValid(event)) {\n  const dlqTopic = `dlq/tenant-${event.tenant_id}`;\n  publish(dlqTopic, { event_id: event.id, tenant_id: event.tenant_id, reason: 'validation_error' });\n} else {\n  routeToNormalPath(event);\n}\n```\n\n## Follow-up Questions\n- How would you test DLQ behavior under bursty tenants and ensure no data leakage?\n- How would you monitor DLQ health and automate cleanup without affecting active tenants?","diagram":"flowchart TD\n  Gateway[Gateway] --> Validator[Validation Stage]\n  Validator -- malformed --> DLQ_Tenant[DLQ per tenant]\n  Validator -- good --> Ingest[Ingestion Path]\n  DLQ_Tenant --> Replay[Reingest / Replay Path]","difficulty":"intermediate","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Instacart","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T04:17:33.369Z","createdAt":"2026-01-15T04:17:33.369Z"},{"id":"q-2165","question":"KCNA observability & tenant-scoped tracing: design end-to-end observability for a multi-tenant KCNA event bus under burst traffic, preserving tenant data isolation while enabling operations debugging. Specify: how to propagate tenant_id and correlation_id across producer, gateway, and consumer; per-tenant metrics and alerting; privacy-preserving trace UI that exposes only metadata; retention, RBAC, and failure-mode testing with canaries?","answer":"Propose OpenTelemetry traces carrying tenant_id and correlation_id across producer, gateway, and consumer; add per-tenant metrics in Prometheus and alerting rules; redact payload data in traces and st","explanation":"Why This Is Asked\n\nTests ability to design scalable, tenant-aware observability for a high-throughput KCNA setup, balancing debugging needs with data isolation and privacy.\n\nKey Concepts\n\n- Distributed tracing with tenant context\n- Data minimization and redaction\n- Per-tenant metrics and alerting\n- RBAC and topic-level access\n- Canary rollout for instrumentation\n\nCode Example\n\n```javascript\n// Minimal tracing snippet showing span with tenant and correlation attributes\nconst { trace } = require('@opentelemetry/api');\nconst tracer = trace.getTracer('kcna');\nfunction emitEvent(tenantId, correlationId, evt) {\n  const span = tracer.startSpan('emitEvent', { attributes: { tenantId, correlationId, action: 'emit' }});\n  // ... emit logic\n  span.end();\n}\n```\n\nFollow-up Questions\n\n- How would you test privacy controls (redaction, audience limits) in traces and verify no cross-tenant leakage?\n- What monitoring and retention policies would you apply to balance cost and debugging fidelity?","diagram":"flowchart TD\n  A[Producer] --> B[Gateway]\n  B --> C[Consumer]\n  subgraph Tenant Isolation\n    D1[Trace with tenant_id] --> D2[Masked Payload]\n  end","difficulty":"intermediate","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Goldman Sachs","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T05:42:53.445Z","createdAt":"2026-01-15T05:42:53.445Z"},{"id":"q-2265","question":"Design a KCNA-based privacy-preserving cross-tenant analytics pipeline where tenants publish raw events but analysts receive aggregated metrics only. Tenants can opt into regional sharing with differential privacy, per-tenant encryption keys, and auditable data lineage. Describe topology, data model, access control, latency targets, and a canary rollout plan?","answer":"Isolate tenants with per-tenant KCNA namespaces; a DP-enricher subscribes to raw streams, emits ε-DP metrics to a guarded analytics topic; use tenant-scoped keys for at-rest/in-transit encryption; enf","explanation":"## Why This Is Asked\nTests ability to design privacy-preserving cross-tenant analytics with real-world constraints and audits.\n\n## Key Concepts\n- Tenant isolation via namespaces\n- Differential privacy budgets per region\n- Tenant-key management and encryption\n- Auditing and data lineage\n- Canary rollout in multi-region setup\n\n## Code Example\n```javascript\nfunction applyDP(data, epsilon) {\n  // placeholder DP mechanism\n  return data.map(x => x + (Math.random() < epsilon ? 0 : 0));\n}\n```\n\n## Follow-up Questions\n- How would you monitor DP budget exhaustion across regions?\n- How would you validate no PII leakage during audits?\n- How would revoking a tenant key affect ongoing streams?","diagram":"flowchart TD\n  P[Tenant Publisher] --> R[KCNA Raw Topic]\n  R --> E[DP Enricher]\n  E --> A[Analytics Output]\n  A --> L[Audit Logs]","difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Netflix","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T09:48:36.908Z","createdAt":"2026-01-15T09:48:36.908Z"},{"id":"q-2311","question":"KCNA Dead-Letter Queue (DLQ): For a beginner-friendly KCNA, design a per-tenant DLQ strategy for messages that fail processing after N retries. Describe API shape to move from main topic to DLQ, retention, how to reprocess, and a simple test canary that demonstrates DLQ routing, backoff between retries, and alerting?","answer":"Per-tenant DLQ should route failed messages to a dedicated DLQ (per tenant or with tenant_id in payload), store original topic and offset, and apply maxRetries with exponential backoff. Include a repr","explanation":"## Why This Is Asked\n\nThis question probes practical handling of failed messages in a multi-tenant KCNA, a common production need that beginners can implement with simple routing, retry/backoff, and reprocessing.\n\n## Key Concepts\n\n- Dead-letter queues per tenant\n- Failure isolation and offset preservation\n- Backoff and max retries\n- Safe reprocessing and idempotence\n- Observability and alerts\n\n## Code Example\n\n```javascript\n// Minimal DLQ routing sketch\nfunction routeToDLQ(event, error){\n  return {tenantId: event.tenantId, originalTopic: event.topic, offset: event.offset, payload: event.payload, error};\n}\n```\n\n## Follow-up Questions\n\n- How to test DLQ under burst failures? \n- How to monitor DLQ latency and replay health?","diagram":null,"difficulty":"beginner","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T11:39:54.578Z","createdAt":"2026-01-15T11:39:54.578Z"},{"id":"q-2425","question":"KCNA per-tenant feature flags: design a control plane to selectively enable a new routing path and compression for KCNA streams, with zero-downtime rollout, tenant-scoped rollback, and audit logs; how would you model toggles, propagate config, implement canaries, and validate impact before full enablement?","answer":"Design a per-tenant feature flag system with a central, strongly consistent store mapping tenantId -> {flags}. Evaluate flags at publish and routing points, ensure idempotent updates, and use canaries","explanation":"## Why This Is Asked\nAssess the ability to design an operational control plane for multi-tenant streaming systems, including per-tenant configurability and safe deployment.\n\n## Key Concepts\n- Feature flags at scale for multi-tenant KCNA\n- Centralized per-tenant config store and propagation\n- Canary rollout gates and safety checks\n- Audit logging and rollback mechanisms\n\n## Code Example\n```javascript\n// Pseudo-code: evaluate if a flag is enabled for a tenant\nfunction isFlagEnabled(tenantId, flagName, defaultVal=false){\n  const cfg = fetchTenantConfig(tenantId); // central store (etcd/kv)\n  return cfg?.flags?.[flagName] ?? defaultVal;\n}\n```\n\n## Follow-up Questions\n- How would you test rollouts with synthetic tenants and simulate partial failures?\n- What observability metrics and dashboards would you add to detect misconfigurations quickly?","diagram":null,"difficulty":"intermediate","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T17:45:49.250Z","createdAt":"2026-01-15T17:45:49.250Z"},{"id":"q-2464","question":"KCNA retention governance: design a per-tenant data lifecycle in KCNA that enforces tenant-specific retention windows, supports legal holds, and runs background tombstone-based deletions with GC across partitions. Explain metadata storage, purge triggers without breaking at-least-once semantics, observability, and a rollback/hold-release workflow?","answer":"Per-tenant retention is stored in a policy store keyed by tenant, topic, and region. Purges enqueue tombstones; GC runs in background with per-tenant quotas to avoid starving in-flight consumers. Lega","explanation":"## Why This Is Asked\nTenants require governance over data lifetime, intersecting privacy and multi-tenant isolation. This tests policy stores, scalable purge, and safe deletion while preserving streaming guarantees.\n\n## Key Concepts\n- Per-tenant retention policy: duration, start, scope\n- Tombstones vs. physical deletion; GC pacing\n- Legal holds: hold flag, release, audit trail\n- Observability: metrics, dashboards, alerts\n- Rollback strategy: canary purge, replay hooks\n\n## Code Example\n```javascript\n// Pseudo-implementation sketch\nclass RetentionPolicy { constructor(tenant, topic, retentionMs, hold) { ... } }\nfunction enqueueTombstone(tenant, topic, offset) { ... }\nfunction runPurgeCycle() { ... } // respects quotas and in-flight reads\n```\n\n## Follow-up Questions\n- How would you test legal-hold behavior across tenants at scale?\n- What changes would you make to ensure cross-tenant isolation during purge and tombstone propagation?","diagram":null,"difficulty":"intermediate","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T19:05:08.222Z","createdAt":"2026-01-15T19:05:08.223Z"},{"id":"q-2471","question":"KCNA security and governance: design a per-tenant envelope encryption scheme for KCNA payloads using a central KMS. Each tenant has a dedicated DEK wrapped by a tenant-specific KEK. Encrypt payloads at produce time and decrypt only at authorized consumers with per-tenant IAM. Support per-tenant key rotation with zero-downtime re-encryption, and maintain per-tenant audit trails and deletion rules that respect retention. How would you implement lifecycle, performance trade-offs, and backward-compatibility?","answer":"Implement envelope encryption: assign each tenant a unique DEK, wrapped by a tenant KEK from a central KMS. Encrypt payloads at producer side; decrypt only with tenant IAM. Support per-tenant key rota","explanation":"## Why This Is Asked\nReal-world KCNA deployments must protect data across tenants; this question probes envelope encryption design, key lifecycle, and how to audit and delete data without breaking consistency.\n\n## Key Concepts\n- Envelope encryption\n- Per-tenant KEK/DEK lifecycle\n- Key rotation with zero-downtime re-encryption\n- Auditability and tenant-scoped deletion\n\n## Code Example\n```javascript\n// Pseudocode for decrypting payload using tenant DEK\nconst wrappedDek = metadata.getTenantWrappedDek(tenantId);\nconst dek = kms.unwrapDek(tenantId, wrappedDek);\nconst plaintext = crypto.decrypt(payload, dek);\n```\n\n## Follow-up Questions\n- How would you test key rotation without service disruption?\n- How do you enforce least-privilege access per tenant across decryptors?","diagram":null,"difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Goldman Sachs","LinkedIn"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T19:34:50.218Z","createdAt":"2026-01-15T19:34:50.219Z"},{"id":"q-2510","question":"KCNA tamper-evident audit trails: design per-tenant verifiable logs for event streams using append-only shards and Merkle proofs, with per-batch signing and external audit proofs. Outline data structures, key rotation, canary rollout, rollback, and a test plan that proves tamper-resistance without leaking tenant data?","answer":"Design a per-tenant verifiable audit trail for KCNA events using append-only shards, per-tenant hashes, and Merkle proofs. Each batch is hashed, signed, and stored in a tamper-evident ledger; auditors","explanation":"## Why This Is Asked\nIn multi-tenant KCNA deployments, tamper-evident audit trails enable compliance, forensics, and governance without sacrificing performance.\n\n## Key Concepts\n- Append-only per-tenant logs and digest chaining\n- Merkle proofs for cross-checks and tamper evidence\n- Cryptographic signing and secure key rotation\n- Canary-driven rollout and safe rollback\n\n## Code Example\n```javascript\nfunction verifyBatch(batch, root) {\n  const digest = hash(batch);\n  const proof = getMerkleProof(batch.id);\n  return Merkle.verify(digest, proof, root);\n}\n```\n\n## Follow-up Questions\n- How to scale verification and audit dashboards?\n- How to handle key compromise and revocation?","diagram":null,"difficulty":"intermediate","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Cloudflare","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T20:53:35.476Z","createdAt":"2026-01-15T20:53:35.476Z"},{"id":"q-2565","question":"KCNA per-tenant data isolation with confidential payloads: in a single KCNA cluster serving multiple tenants, design a mechanism to store tenant-scoped payloads with per-tenant encryption keys, support cross-tenant analytics only if opted-in, ensure query isolation, key rotation, and audit trails. Describe API contracts, key management, and performance implications?","answer":"Design a comprehensive per-tenant data isolation model for KCNA that enables secure multi-tenancy within a single cluster. Implement envelope encryption using tenant-specific keys managed through an external KMS, enforce field-level redaction for cross-tenant analytics when opted-in, maintain strict query isolation through namespace-based access controls, support automated key rotation with zero-downtime migration, and provide tamper-evident audit trails for all data access operations.","explanation":"## Why This Is Asked\n\nThis question evaluates the ability to design sophisticated data isolation architectures within KCNA for enterprise multi-tenant scenarios, specifically addressing encryption lifecycle management, query isolation, and auditability requirements.\n\n## Key Concepts\n\n- Envelope encryption with per-tenant key management\n- KMS integration and automated key rotation\n- Namespace-based access control and query isolation\n- Field-level redaction for cross-tenant analytics\n- Comprehensive audit trails and tamper-evidence mechanisms\n\n## Code Example\n\n```javascript\n// Pseudo example demonstrating tenant-specific key lookup and decryption\nfunction decryptForTenant(tenantId, ciphertext) {\n  const tenantKey = kms.getTenantKey(tenantId);\n  return crypto.decryptWithEnvelope(tenantKey, ciphertext);\n}\n```\n\n## Follow-up Questions\n\n- How would you handle tenant key rotation with zero downtime?\n- What strategies would you implement for cross-tenant analytics performance?\n- How do you ensure audit trail integrity in a distributed environment?","diagram":null,"difficulty":"intermediate","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Microsoft","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T05:22:01.017Z","createdAt":"2026-01-15T22:54:27.086Z"},{"id":"q-2582","question":"In KCNA, design a per-tenant event-time processing layer that tolerates late events within a tenant-defined latency budget, computes per-tenant 5-minute tumbling window aggregations, and guarantees at-least-once delivery. Describe per-tenant watermarks, state partitioning, late-data handling, fault tolerance, and a test plan to validate SLA compliance?","answer":"The implementation leverages per-tenant keyed streams with independent tenant-specific watermarks, backed by a 5-minute tumbling window aggregator utilizing per-tenant RocksDB state stores. Late events arriving within the tenant-defined latency budget are merged into the current active window, while events exceeding the budget are routed to a dead-letter queue for audit and manual review. State is partitioned by tenant ID with periodic checkpointing to durable storage for fault tolerance, and at-least-once delivery semantics are achieved through idempotent processing and transactional state updates.","explanation":"## Why This Is Asked\n\nThis question evaluates expertise in per-tenant event-time semantics, SLA-bound late data handling, and scalable state management in KCNA under multi-tenant workloads.\n\n## Key Concepts\n\n- Event-time processing with tenant-specific watermarks\n- Per-tenant state stores and partitioning strategies\n- Late data handling within SLA constraints\n- Fault tolerance through checkpointing and recovery\n- At-least-once delivery guarantees\n\n## Code Example\n\n```javascript\n// Pseudocode: per-tenant watermark advancement and late event routing\nclass TenantWindowManager {\n  constructor() {\n    this.watermark = -Infinity;\n    this.windows = new Map();\n  }\n  \n  onEvent(tenantId, event) {\n    const isLate = event.timestamp < this.watermark;\n    if (isLate) {\n      this.routeToDeadLetterQueue(tenantId, event);\n    } else {\n      this.mergeIntoActiveWindow(tenantId, event);\n    }\n  }\n}\n```\n\n## Follow-up Questions\n\n- How would you handle backpressure when tenants have varying event volumes?\n- What strategies would you implement to optimize state store compaction?\n- How do you ensure watermark fairness across tenants with different latency budgets?","diagram":"flowchart TD\n  A[KCNA Ingest] --> B[Per-Tenant Window Manager]\n  B --> C[State Store: per-tenant]\n  B --> D[Late Data Route]\n  A --> E[Watermark Propagation]","difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Goldman Sachs","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T05:13:48.636Z","createdAt":"2026-01-15T23:41:57.674Z"},{"id":"q-2621","question":"In KCNA, design a per-tenant field-level access control and masking layer that operates in real time on streaming payloads for analytics, ensuring authorized tenants can decrypt unmasked fields while unauthorized tenants only see masked data, without breaking at-least-once semantics or replay safety. Explain data structures, policy evaluation, KMS key management, and testing with canaries?","answer":"Per-tenant masking layer with policy-as-code, envelope encryption per-tenant KEK from a KMS, and visibility tokens. Ingested events carry tenant + ACLs; sensitive fields are encrypted and re-wrapped f","explanation":"## Why This Is Asked\n\nReal-time masking with per-tenant controls is a practical, security-critical feature at scale; tests edge cases like key rotation and replay.\n\n## Key Concepts\n\n- Field-level access control\n- Policy-as-code\n- Envelope encryption and KEKs\n- Replay safety and exactly-once\n- Key rotation and auditing\n\n## Code Example\n\n```javascript\n// Pseudo-code for policy evaluation and masking\n```\n\n## Follow-up Questions\n\n- How would you test cross-tenant isolation with canaries?\n- How would you handle key revocation and rotation without service disruption?","diagram":"flowchart TD\n  A[Ingest Event] --> B[Policy Eval]\n  B --> C[Masking Layer]\n  C --> D[Masked Output]\n  B --> E[Decrypt Permissions]\n  E --> F[Authorized View] --> D","difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Apple"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T04:06:41.724Z","createdAt":"2026-01-16T04:06:41.726Z"},{"id":"q-2710","question":"KCNA per-tenant envelope encryption: design a CMK-backed scheme where each tenant's messages are encrypted at ingest with a tenant KEK wrapped by a KMS CMK, keys rotate monthly, and revocation triggers re-encryption with minimal downtime while preserving canary readers. Describe data model, API surface, rotation and revocation flows, and test strategy?","answer":"Use per-tenant KEKs in KMS, envelope-encrypt payloads with per-tenant DEKs; store DEK metadata (tenant_id, key_ver, cipher) in KCNA; rotate KEKs monthly by re-wrapping DEKs in place; on revocation, to","explanation":"## Why This Is Asked\n\nInterview context explanation.\n\n## Key Concepts\n\n- Envelope encryption, CMKs, KEKs, KMS integration\n- Key rotation, revocation, re-encryption strategy\n- Data model for keys, metadata, and auditability\n\n## Code Example\n\n```javascript\n// Pseudo API surface\nclass KCNAEnvelope {\n  constructor(tenantId, kekName, version) {}\n  encrypt(payload) {}\n  decrypt(encrypted) {}\n}\n```\n\n## Follow-up Questions\n\n- How would you test rotation impact on latency?\n- How do you audit key usage and detect leakage?","diagram":null,"difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T07:44:01.064Z","createdAt":"2026-01-16T07:44:01.064Z"},{"id":"q-941","question":"Scenario: A global chat platform with 2B MAUs must detect policy-violating content (spam, hate speech) in near real-time while preserving user privacy and multilingual support. Propose an end-to-end pipeline: ingestion, moderation models (rules + ML), latency SLOs (1-2s), privacy safeguards, backpressure handling, retries, and dead-letter queues. Compare on-device vs cloud inference and monitoring?","answer":"Propose a tiered moderation pipeline: client-side tokenization + on-device classifier for first-pass filtering (multilingual light-weight model), with encrypted message IDs, then server-side streaming","explanation":"## Why This Is Asked\nThis question gauges real-time, scalable moderation design, privacy-safe multi-language handling, and the trade-offs between edge and cloud inference.\n\n## Key Concepts\n- Real-time streaming pipelines, SLOs, backpressure\n- Edge (on-device) vs cloud inference, multilingual models\n- Privacy safeguards (encryption, minimal data)\n- DLQ, retries, circuit breakers, monitoring\n\n## Code Example\n```javascript\n// Latency guard example\nif (latencyMs > 2000) {\n  tagAsSlowPath();\n  redirectToFallbackQueue();\n}\n```\n\n## Follow-up Questions\n- How would you validate model drift and false positives in production?\n- What metrics would you surface in dashboards to detect abuse transparently?","diagram":"flowchart TD\n  A[Ingest] --> B[Queue]\n  B --> C[Moderation]\n  C --> D[Enforce/Notify]\n  D --> E[Audit]","difficulty":"advanced","tags":["kcna"],"channel":"kcna","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Slack","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T16:31:29.566Z","createdAt":"2026-01-12T16:31:29.566Z"}],"subChannels":["general"],"companies":["Adobe","Amazon","Anthropic","Apple","Bloomberg","Citadel","Cloudflare","Coinbase","Databricks","Discord","DoorDash","Goldman Sachs","Google","Hugging Face","IBM","Instacart","LinkedIn","Lyft","Meta","Microsoft","NVIDIA","Netflix","Oracle","Plaid","Robinhood","Slack","Snap","Snowflake","Stripe","Tesla","Twitter","Two Sigma","Uber","Zoom"],"stats":{"total":27,"beginner":4,"intermediate":11,"advanced":12,"newThisWeek":27}}