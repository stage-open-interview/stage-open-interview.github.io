{"questions":[{"id":"q-1044","question":"On a Linux host running multiple tenant services, peak I/O from a data ingestion daemon saturates the disk, causing latency spikes for others. Propose a production plan to isolate and throttle disk I/O across tenants using systemd slices and cgroup v2 io.max. Include concrete unit and slice snippets, per-device limits for a SATA HDD and NVMe, and a test plan using fio and iostat to verify tail latency improvements under concurrent workloads?","answer":"Implement per-tenant I/O isolation using systemd slices and cgroup v2 io.max. Create tenant-<name>.slice, assign services to it, and throttle devices (e.g., 8:0 rbps=50M wbps=50M). Include per-device ","explanation":"## Why This Is Asked\nTests ability to design resource isolation and practical configuration with systemd and cgroups in a multi-tenant environment, focusing on I/O pressure rather than CPU/memory alone.\n\n## Key Concepts\n- cgroup v2 io.max throttling\n- systemd slices and per-service isolation\n- per-device I/O limits and hierarchy\n- reproducible load testing with fio and iostat\n\n## Code Example\n```javascript\n# Example: set per-device I/O throttling for tenant-a.slice (pseudo)\nsudo mkdir -p /sys/fs/cgroup/tenant-a.slice\necho \"8:0 rbps=50M wbps=50M\" | sudo tee /sys/fs/cgroup/tenant-a.slice/io.max\n```\n\n## Follow-up Questions\n- How would you adapt if the workload alternates between read-heavy and write-heavy?\n- What are potential pitfalls with block layer throttling and caches?","diagram":null,"difficulty":"advanced","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Microsoft","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T20:30:42.832Z","createdAt":"2026-01-12T20:30:42.832Z"},{"id":"q-1056","question":"On a Linux host, a TLS proxy reads its certificate from /etc/ssl/certs/app.crt and currently reloads by restarting, causing brief downtime during renewal. Propose a zero-downtime rotation using a systemd.path trigger that fires on a new cert symlink, with a separate service unit for ExecReload and an atomic update scheme (swap in a new cert, then switch a symlink). Include concrete unit snippets and a test plan?","answer":"Propose a zero-downtime TLS cert rotation using systemd.path to trigger on an updated certificate symlink, plus a dedicated systemd service that ExecReloads the proxy. Use atomic cert updates (place n","explanation":"## Why This Is Asked\nTests practical mastery of systemd path activation, service reloads, and atomic file updates for TLS without downtime. It also validates understanding of race-free certificate rotation and testability.\n\n## Key Concepts\n- systemd.path and PathChanged/PathModified\n- ExecReload vs Restart semantics\n- Atomic file replacement via symlinks\n- Reproducible test plan for cert rotation\n\n## Code Example\n```ini\n# app-cert.path\n[Unit]\nDescription=Trigger TLS cert rotation on new cert\n\n[Path]\nPathChanged=/etc/ssl/certs/app.crt\nUnit=app-cert.service\n\n[Install]\nWantedBy=multi-user.target\n```\n```ini\n# app-cert.service\n[Unit]\nDescription=Reload TLS proxy with new certificate\n\n[Service]\nType=oneshot\nExecStart=/usr/local/bin/app-cert-rotate-reload.sh\n```\n```bash\n# app-cert-rotate-reload.sh\n#!/bin/sh\nset -e\n# Swap in the new cert and reload the proxy\nmv -f /etc/ssl/certs/app.crt.new /etc/ssl/certs/app.crt\nsystemctl reload app-proxy.service\n```\n```\n\n## Follow-up Questions\n- How to handle reload failure and ensure idempotence?\n- How to monitor for stale symlinks or missing new certs?\n- How would you extend this for multiple certificates or canary rollouts?","diagram":"flowchart TD\n  A[Cert renewal arrives] --> B[Write app.crt.new]\n  B --> C[symlink swap via script]\n  C --> D{Reload trigger}\n  D --> E[app-proxy reloads without downtime]","difficulty":"advanced","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T21:18:27.923Z","createdAt":"2026-01-12T21:18:27.923Z"},{"id":"q-1122","question":"On a Linux host running multiple ML model servers in separate systemd slices using cgroup v2, a spike in one tenant consumes memory and triggers host memory pressure despite per-tenant limits. Propose a production plan to enforce strict isolation and backpressure using memory.max and memory.high, enable group OOM behavior, and implement eviction/playback strategies. Include concrete unit and cgroup settings and a test plan with a reproducible spike and validation steps?","answer":"Implement per-tenant slices with MemoryMax and MemoryHigh in a cgroup v2 setup; attach each model server to its slice. Enable memory.oom_group for group-based OOM handling and configure a hard cap wit","explanation":"## Why This Is Asked\nTests the ability to design strict memory isolation in a multi-tenant Linux environment, covering cgroup v2 memory controls, systemd slices, and OOM behavior under load.\n\n## Key Concepts\n- cgroup v2 memory.max and memory.high for hard/soft limits\n- memory.oom_group for group-level OOM decisions\n- systemd slices and per-tenant unit configurations\n- observability: cgroup events, dmesg, and host health verification\n\n## Code Example\n```javascript\n# Example: tenant-A.slice\n[Slice]\nMemoryHigh=6G\nMemoryMax=8G\n\n# Example: tenant-A service\n[Service]\nSlice=tenant-A.slice\nExecStart=/usr/bin/modelA_server\nRestart=on-failure\n```\n\n## Follow-up Questions\n- How would you automate scale-out when new tenants are added?\n- What metrics and alerts would you surface to detect pressure early?","diagram":"flowchart TD\n  A[Tenant A] --> B[Cgroup v2 memory.max]\n  A --> C[Cgroup v2 memory.high]\n  B --> D[Host stability checks]\n  C --> D","difficulty":"advanced","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","MongoDB","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T23:29:53.881Z","createdAt":"2026-01-12T23:29:53.881Z"},{"id":"q-1230","question":"On a Linux host with a multi-tenant workload sharing a single 10GbE NIC configured with multiple receive queues, one tenant bursts UDP traffic and starves others, causing increased latency and packet loss. Propose a production plan to enforce tenant isolation and fairness using NIC multi-queue, RSS/XPS mapping, IRQ affinity, and per-tenant cgroups (v2) with io.max and tc shaping. Include concrete steps and a test plan with realistic traffic?","answer":"Plan to isolate tenants by binding NIC RX queues to per-tenant CPUs, use RSS/XPS to map flows, create per-tenant systemd slices with per-tenant CPU budgets, apply io.max quotas, and shape egress with ","explanation":"## Why This Is Asked\nAssesses practical network isolation skills in a multi-tenant Linux environment, focusing on NIC multi-queue tuning, IRQ affinity, and per-tenant resource containers to prevent bursts from one tenant affecting others.\n\n## Key Concepts\n- NIC multi-queue, RSS, XPS\n- IRQ affinity per-tenant isolation\n- cgroups v2 with io.max and per-tenant slices\n- tc shaping and fq_codel for fairness\n- Observability with iperf3/pktgen, iostat\n\n## Code Example\n```bash\n# Enable 4 RX queues\nethtool -L eth0 rx 4\n# Bind a few IRQs to CPUs (example)\necho 2-3,6-7 > /proc/irq/46/smp_affinity_list\n```\n\n## Follow-up Questions\n- How would you validate fairness under concurrent tenants?\n- What are NUMA pitfalls and how would you mitigate them?","diagram":"flowchart TD\n A[Tenants] --> B[RX queues]\n A --> C[CPU cores]\n B --> D[Mapping rules]\n C --> E[Isolation boundary]","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","MongoDB","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T05:40:43.739Z","createdAt":"2026-01-13T05:40:43.739Z"},{"id":"q-1327","question":"On a Linux host running multiple tenants via containers, a CPU-intensive workload from one tenant starves others. Propose a production plan to enforce CPU isolation using cgroup v2 slices, systemd integration, and per-tenant quotas; include concrete settings (cpu.max, cpu.weight), scheduling options (cpuset and isolcpus), and a test plan with realistic workloads and verification steps?","answer":"Create per-tenant cgroup v2 slices and assign each container to its slice. Use cpu.max and cpu.weight to cap and weight CPU usage (e.g., tenantA 250000 1000000, tenants B, C, D 150000 1000000; weights","explanation":"## Why This Is Asked\nAssesses practical mastery of modern Linux isolation (cgroup v2, systemd slices, cpuset) and real-world tradeoffs between throughput and fairness.\n\n## Key Concepts\n- cgroup v2 CPU controller: cpu.max, cpu.weight\n- systemd slices and container assignment\n- cpuset/isolation of CPUs; isolcpus boot option\n- realistic workloads: fio, sysbench; tail latency tracking\n\n## Code Example\n```bash\n# Illustrative: create and configure tenant slices (pseudo-commands)\nsystemd-run --unit=tenantA.slice --property=CPUQuota=250000 --property=CPUQuotaPeriodUS=1000000 --slice=tenantA.slice sleep 1h\nsystemd-run --unit=tenantB.slice --property=CPUQuota=150000 --property=CPUQuotaPeriodUS=1000000 --slice=tenantB.slice sleep 1h\n# Bind slices to CPUs via cpuset (illustrative)\nchmod 600 /sys/fs/cgroup/unified/tenantA.slice/cpuset.cpus\n# Attach containers to their slices as they spawn\n```\n\n## Follow-up Questions\n- How would you monitor CPU-steal and detect fairness violations in production?\n- What are risks of over-isolating CPUs, and how would you safely rebalance during load spikes?","diagram":null,"difficulty":"advanced","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Google","IBM"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T11:37:36.271Z","createdAt":"2026-01-13T11:37:36.271Z"},{"id":"q-1346","question":"On a production Linux host serving multiple ephemeral monitoring daemons, a heartbeat worker must emit a 5-second ping to a central collector. During peak load, the heartbeat misses several intervals, delaying alerts. Propose a concrete production plan to guarantee cadence and resilience using: (a) per-service CPU isolation and pinning, (b) high-priority timer/RT scheduling or systemd timers with precise tuning, (c) a fallback mechanism for missed heartbeats, and (d) a validation strategy with a controlled load. Include concrete config snippets for systemd, cpuset, and timer settings?","answer":"Plan pins heartbeat to a dedicated CPU set, uses a systemd timer with 5s cadence and AccuracySec=1s, and a high-priority heartbeat service with CPUQuota=50% and SCHED_FIFO-like behavior. Add a fallbac","explanation":"## Why This Is Asked\n\nTests knowledge of per-service isolation, timer accuracy, fault tolerance, and measurable validation.\n\n## Key Concepts\n\n- systemd slices timers CPU affinity\n- cpuset pinning and CPUQuota/AccuracySec\n- RT priority and fallback mechanisms\n- end-to-end verification under load\n\n## Code Example\n\n```\n# See /etc/systemd/system/heartbeat.* for concrete files\n```\n\n## Follow-up Questions\n\n- How would you detect missed heartbeats and trigger alerts?\n- What are the trade-offs of increasing CPU isolation vs. responsiveness?","diagram":"flowchart TD\n  A[Create heartbeat.slice] --> B[Pin heartbeat service]\n  B --> C[Configure 5s timer]\n  C --> D[Add fallback heartbeat]\n  D --> E[Test under load]","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","LinkedIn"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T13:05:27.792Z","createdAt":"2026-01-13T13:05:27.792Z"},{"id":"q-1385","question":"On a Linux host running ephemeral build jobs managed by systemd, a rogue job floods /tmp with tiny files, exhausting inodes and delaying builds. Propose a production plan to isolate and bound /tmp usage and auto-clean stale files. Include: (a) per-service PrivateTmp and tmpfs /tmp with a size cap; (b) systemd-tmpfiles cleanup rules; (c) inode usage monitoring; (d) a reproducible test that proves isolation and cleanup. How would you implement this?","answer":"Configure per-service isolation: set PrivateTmp=true and PrivateDevices=true for build services; mount /tmp as tmpfs with size=128M and mode=1777; add systemd-tmpfiles cleanup rules to prune stale fil","explanation":"## Why This Is Asked\nAssesses practical isolation of temporary storage for multi-tenant build workloads and how to enforce cleanups under bursty IO.\n\n## Key Concepts\n- systemd PrivateTmp and PrivateDevices\n- tmpfs sizing and mounting options\n- inode monitoring and alerting (df -i, thresholds)\n- systemd-tmpfiles cleanup rules and scheduling\n\n## Code Example\n```javascript\n// Example pseudo-configuration snippets for illustration\n// systemd unit: PrivateTmp=true\n// /etc/fstab: tmpfs /tmp tmpfs size=128M,mode=1777 0 0\n```\n\n## Follow-up Questions\n- How would you scale this across a cluster of CI workers?\n- What are potential edge cases with cleanup timing and race conditions?","diagram":null,"difficulty":"beginner","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Coinbase","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T14:47:50.803Z","createdAt":"2026-01-13T14:47:50.803Z"},{"id":"q-1490","question":"On a Linux server, a small web app runs as a systemd service and reads its port and log level from /etc/myapp/config.yaml. Changes to this file must apply without dropping connections. Design a production-safe hot-reload mechanism using systemd ExecReload, a SIGHUP-based reload, and a small validation script. Include unit file snippets and a test plan with a reproducible config change and verification steps?","answer":"Use systemd ExecReload to send SIGHUP to the daemon and a small validator script. Create /usr/local/bin/myapp_reload to validate /etc/myapp/config.yaml (e.g., ensure .port and .log_level exist) and th","explanation":"## Why This Is Asked\nTests hot-reload capability and safe config changes without dropping connections.\n\n## Key Concepts\n- systemd ExecReload\n- SIGHUP reload semantics\n- config validation (YAML)\n- graceful reload vs restart\n- test plan design\n\n## Code Example\n```javascript\n// Implementation sketch\n```\n\n## Follow-up Questions\n- How would you handle a daemon that does not support SIGHUP?\n- How would you verify that in-flight requests survive a reload?","diagram":"flowchart TD\n  A[config.yaml change] --> B[validate YAML]\n  B --> C{valid?}\n  C -->|yes| D[send SIGHUP]\n  D --> E[service reload]\n  E --> F[verify endpoints]\n  C -->|no| G[abort reload]","difficulty":"beginner","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T19:03:23.816Z","createdAt":"2026-01-13T19:03:23.816Z"},{"id":"q-1497","question":"On a Linux host running multiple tenants in systemd services sharing a single NVMe SSD and a 25Gbps NIC, a new requirement enforces strict I/O isolation to prevent a noisy tenant from stalling others. Propose a production plan to enforce per-tenant I/O isolation and backpressure using: (a) cgroup v2 io.max per service; (b) per-tenant filesystem isolation via PrivateMounts and PrivateDevices with a dedicated data path; (c) I/O scheduler tuning (none/deadline) and per-tenant block IO queuing discipline with tc; (d) a deterministic test plan with a reproducible burst scenario and rollback steps. Include concrete config snippets for systemd units and cgroups?","answer":"Plan: use per-tenant cgroup v2 io.max to cap disk writes, create tenant.slice with PrivateMounts for isolation, assign each service to a sub-cgroup, attach per-tenant tc qdisc to shape writes, and iso","explanation":"## Why This Is Asked\nTests hands-on mastery of Linux IO isolation primitives under multi-tenant workloads and production risk management.\n\n## Key Concepts\n- cgroup v2 io.max\n- systemd Slice and isolation\n- tc/bpf/qdisc for per-tenant IO shaping\n- PrivateMounts/PrivateDevices for filesystem isolation\n- Reproducible test methodology with fio\n\n## Code Example\n```javascript\n# systemd unit\n[Unit]\nDescription=Tenant A worker\nAfter=network-online.target\n\n[Service]\nSlice=tenant-A.slice\nPrivateMounts=yes\nPrivateDevices=yes\nExecStart=/usr/local/bin/tenant-a-worker\n```\n\n```javascript\n# cgroup v2 IO limit (pseudo)\nmkdir -p /sys/fs/cgroup/tenant-A\necho \"8:0 100M\" > /sys/fs/cgroup/tenant-A/io.max\n```\n\n## Follow-up Questions\n- How would you roll back if another tenant regressed?\n- What metrics and alerting would you install to detect drift?","diagram":null,"difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T19:32:24.815Z","createdAt":"2026-01-13T19:32:24.816Z"},{"id":"q-1588","question":"On a dual-socket Linux host running PostgreSQL and a data-processor ETL job, both services contend for memory and cache across NUMA nodes; design an advanced plan to enforce NUMA-aware isolation with cpuset/memcg, disable NUMA balancing, set per-node IRQ affinity, and validate via targeted benchmarks. Include concrete commands and a test plan?","answer":"Pin two services to separate NUMA nodes using cpuset and memory binding; PostgreSQL on node0, ETL on node1. Disable NUMA balancing, set per-node IRQ affinity, and ensure memory policies via memcg. Validate isolation through targeted benchmarks measuring memory locality, cache coherence, and latency under mixed workload conditions.","explanation":"## Why This Is Asked\nNUMA awareness is critical for predictable latency in multi-socket systems; this question tests practical implementation of resource isolation, memory binding, and performance verification techniques.\n\n## Key Concepts\n- NUMA topology and memory affinity\n- cpuset and memory controller (memcg) binding\n- NUMA balancing disable and IRQ affinity configuration\n- Performance validation under mixed workload scenarios\n\n## Code Example\n```bash\n# Detect NUMA topology and create cpusets\nlscpu | grep NUMA\nsudo mkdir -p /sys/fs/cgroup/cpuset/postgres\nsudo bash -c 'echo 0-15 > /sys/fs/cgroup/cpuset/postgres/cpuset.cpus'\nsudo bash -c 'echo 0 > /sys/fs/cgroup/cpuset/postgres/cpuset.mems'\n# Similar setup for ETL on node1\n```\n\n## Follow-up Considerations\nMonitor NUMA statistics via `numastat` and validate memory locality using `perf` and custom benchmarks measuring cross-NUMA traffic impact.","diagram":null,"difficulty":"advanced","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T04:28:01.530Z","createdAt":"2026-01-13T22:54:02.606Z"},{"id":"q-1665","question":"On a Linux server hosting a latency-sensitive database and a heavy batch analytics job that share a single 2-socket NUMA node and HDD I/O, propose a production plan to guarantee tail latency (P95/P99) for the database while allowing analytics to finish within a bounded window. Include NUMA memory binding, CPU pinning via cpuset/systemd slices, I/O throttling via cgroup v2 io.max, IRQ affinity, and a validation test plan with realistic workloads?","answer":"Pin the latency-sensitive database to CPUs 0-3 on NUMA node 0 with a dedicated systemd slice and cpuset; bind memory local to node 0 and set memory.low to favor the DB. Run analytics on CPUs 4-7 with ","explanation":"## Why This Is Asked\nTests ability to reason about cross-domain isolation (CPU, memory, I/O) and NUMA-awareness to guarantee tail latency under mixed workloads.\n\n## Key Concepts\n- NUMA locality and memory policies\n- cpuset and systemd Slice-based isolation\n- cgroup v2 io.max and memory.high for throttling\n- IRQ affinity and CPU affinity for stable paths\n- Validation with realistic, reproducible workloads\n\n## Code Example\n```javascript\n// systemd cpuset and io.max example (illustrative)\n[Unit]\nDescription=Latency DB\n\n[Service]\nSlice=db.slice\nCPUS=0-3\nMemoryHigh=0\n\n```\n\n## Follow-up Questions\n- How would you adjust if analytics occasionally spikes beyond expected bounds?\n- How would you monitor to detect regressive behavior after updates?","diagram":null,"difficulty":"advanced","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T05:49:28.663Z","createdAt":"2026-01-14T05:49:28.663Z"},{"id":"q-1700","question":"On a Linux server running several io_uring-based data ingestors, one worker's backlog bursts, triggering tail latency spikes for others. Propose a production plan to guarantee per-worker latency and fairness using: (a) per-worker io_uring ring sizing and SQ depth; (b) per-worker cgroups v2 with io.max and task limits; (c) backpressure and pacing (budget tokens, per-worker deadlines); (d) monitoring and a reproducible test plan to validate latency under burst?","answer":"Configure per-worker io_uring rings with fixed SQ depth (e.g., 256), pin workers to dedicated CPUs via cpuset, set per-cgroup io.max (e.g., 8MB/s), implement pacing in ingestion to cap outstanding I/O","explanation":"## Why This Is Asked\nTests ability to design per-worker I/O isolation for io_uring workloads, a realistic pattern as data ingestion scales, and how to enforce fairness without sacrificing throughput.\n\n## Key Concepts\n- io_uring ring isolation and SQ budgeting\n- cgroup v2 io.max and per-task constraints\n- cpuset or task-pinning for CPU affinity\n- backpressure/pacing mechanisms in async I/O\n- instrumentation: latency percentiles, eBPF-based tracing\n\n## Code Example\n```javascript\n# Example cgroup setup (bash-like)\nsudo mkdir -p /sys/fs/cgroup/myio/worker1\necho 10485760 > /sys/fs/cgroup/myio/worker1/io.max\n# add pid\necho <pid> > /sys/fs/cgroup/myio/worker1/cgroup.procs\n```\n\n## Follow-up Questions\n- How would you extend to dynamic reallocation of rings if a worker consistently underperforms?\n- What are the risks of strict per-worker quotas on bursty workloads and how would you mitigate them?","diagram":"flowchart TD\n  A[Worker] --> B[io_uring ring]\n  B --> C[Submit IOs]\n  C --> D[Latency Monitor]\n  D --> E{Budget OK?}\n  E -->|Yes| F[Continue]\n  E -->|No| G[Apply Backpressure]\n  G --> H[Ballast Queue]\n  H --> F","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T07:36:58.713Z","createdAt":"2026-01-14T07:36:58.713Z"},{"id":"q-1774","question":"On a Linux host, a file-watcher daemon uses inotify to monitor a large directory tree. As load grows, events start being missed and the daemon lags. Propose a production plan to keep event delivery reliable: tune inotify limits, raise file descriptor limits for the service, add a polling fallback or batching, and validate with a reproducible test that simulates churn?","answer":"Increase inotify capacity and harden limits; implement a polling fallback for missed events; add a watchdog health check to detect lag and restart gracefully. Steps: 1) set fs.inotify.max_user_watches","explanation":"## Why This Is Asked\nTests understanding of Linux kernel parameters, systemd service limits, and practical reliability strategies for event-driven workloads.\n\n## Key Concepts\n- Inotify limits (fs.inotify.*) and kernel tuning\n- Per-service file descriptor limits (LimitNOFILE, ulimit)\n- Fault-tolerant design (polling fallback, batching)\n- Observability and testing with churn scenarios\n\n## Code Example\n```ini\n# Watch daemon systemd unit override\n[Service]\nLimitNOFILE=512000\n```\n\n```bash\n# Increase inotify capacity (runtime)\nsysctl -w fs.inotify.max_user_watches=300000\n```\n\n```bash\n# Simple churn test (pseudo)\nfor i in {1..100000}; do touch /watch/dir/file_$i; done\n```\n\n## Follow-up Questions\n- How would you measure inotify queue length and event lag in production?\n- How would you ensure compatibility across kernel versions and containers?","diagram":"flowchart TD\n  A[Inotify Watch] --> B[Event]\n  B --> C[Worker]\n  C --> D[Lag Monitor]\n  D --> E{Lag > threshold}\n  E -->|Yes| F[Switch to Polling]\n  E -->|No| G[Normal]\n","difficulty":"beginner","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","Oracle","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T10:36:39.086Z","createdAt":"2026-01-14T10:36:39.086Z"},{"id":"q-881","question":"On a Linux host, a long-running daemon writes to `/var/log/myapp.log` and is managed by systemd, but log rotation occasionally causes logging to stop after rotation. Propose a practical fix to ensure logging continues after rotation without restarting the daemon. Include the exact approach and a sample logrotate config snippet and testing steps?","answer":"Configure logrotate to reopen the log file after rotation instead of copying or truncating. After rotating, signal the daemon to reopen logs (e.g., SIGHUP). The config should include a postrotate that","explanation":"## Why This Is Asked\nTests practical handling of log rotation, signals, and ensuring service continuity without downtime. It checks knowledge of logrotate postrotate scripts, choosing the right approach (signal vs copytruncate), and how to validate in a controlled test.\n\n## Key Concepts\n- logrotate configuration fields and postrotate scripts\n- signaling daemons (SIGHUP) to reopen logs\n- copytruncate vs signaling trade-offs\n- testing routine for rotation\n\n## Code Example\n```bash\n/var/log/myapp.log {\n  rotate 5\n  weekly\n  missingok\n  notifempty\n  postrotate\n    kill -HUP `cat /var/run/myapp.pid` 2>/dev/null || true\n  endscript\n}\n```\n\n## Follow-up Questions\n- What are the trade-offs of copytruncate vs signaling?\n- How would you monitor and alert if log rotation fails to reopen logs?","diagram":null,"difficulty":"beginner","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Meta","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T13:59:55.582Z","createdAt":"2026-01-12T13:59:55.582Z"},{"id":"q-888","question":"A production Linux host runs a data-backup agent that uses /var/lib/backup/backup.lock to enforce a single instance. Sometimes a stale lock remains after a crash, blocking new runs; the agent also leaves non-terminating children on stop, risking partial backups. Propose a systemdâ€‘based lifecycle fix: ensure one instance, auto-clean stale lock, and graceful stop with timeout and fallback to kill. Include concrete unit snippets and verification steps?","answer":"Use a systemd unit with Type=forking, PIDFile, and ExecStartPre that checks and clears a stale lock: if /var/lib/backup/backup.lock exists and its PID is not running, delete it. Stop uses KillMode=con","explanation":"## Why This Is Asked\nTests understanding of robust service lifecycle management with systemd, handling singleton constraints, and clean termination of complex processes.\n\n## Key Concepts\n- systemd lifecycle: ExecStartPre, ExecStop, KillMode, TimeoutStopSec\n- singleton enforcement via lock files\n- graceful termination vs. forceful kill for child processes\n\n## Code Example\n```ini\n; /etc/systemd/system/backup-agent.service\n[Unit]\nDescription=Backup Agent\nAfter=network.target\n\n[Service]\nType=forking\nPIDFile=/var/run/backup/backup.pid\nExecStartPre=/bin/sh -c 'LOCK=/var/lib/backup/backup.lock; if [ -e \"$LOCK\" ]; then pid=$(cat \"$LOCK\"); if [ -d /proc/$pid ]; then exit 1; else rm -f \"$LOCK\"; fi; fi'\nExecStart=/usr/local/bin/backup-agent\nExecStop=/bin/kill -TERM $MAINPID\nExecStopPost=/bin/rm -f /var/lib/backup/backup.lock\nTimeoutStopSec=120s\nKillMode=control-group\nRestart=on-failure\n```\n\n## Follow-up Questions\n- How would you test idempotency for consecutive startups?\n- How would you adapt if the agent uses a PID file instead of a lock file?","diagram":null,"difficulty":"advanced","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T14:27:44.056Z","createdAt":"2026-01-12T14:27:44.056Z"},{"id":"q-961","question":"On a Linux host, a memory-hungry log-harvester daemon managed by systemd sporadically triggers the kernel OOM killer during peak load, crashing the service and delaying alerts. Propose a production-safe plan to prevent OOM termination while preserving throughput. Include concrete systemd settings (MemoryLimit, MemorySwapMax, OOMScoreAdjust, Restart), kernel tuning (swap, swappiness), and a test plan with a reproducible high-memory scenario and verification steps?","answer":"Apply per-service memory controls and OOM protection. Set MemoryLimit=2G, MemorySwapMax=2G, OOMScoreAdjust=-100, and Restart=on-failure with a 5s backoff. Tune vm.swappiness=10 and ensure swap is enab","explanation":"## Why This Is Asked\nTests memory pressure handling, systemd tuning, and safe recovery without service disruption.\n\n## Key Concepts\n- Kernel OOM killer and oom_score_adj\n- systemd memory constraints (MemoryLimit, MemorySwapMax)\n- Restart strategies and timeouts\n- memory tuning and swap behavior\n\n## Code Example\n```ini\n[Unit]\nDescription=Log Harvester\n\n[Service]\nExecStart=/usr/local/bin/logharvester\nType=simple\nMemoryLimit=2G\nMemorySwapMax=2G\nOOMScoreAdjust=-100\nRestart=on-failure\nRestartSec=5s\n```\n\n## Follow-up Questions\n- How would you observe and alert on OOM events?\n- How would you adjust for multiple high-memory services competing for swap?","diagram":"flowchart TD\n  A[Memory Pressure] --> B{OOM killer?}\n  B -->|Yes| C[Adjust OOMScore/MemoryLimit]\n  B -->|No| D[Normal Operation]\n  C --> E[Daemon Survives, backlog Drains]","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T17:22:50.353Z","createdAt":"2026-01-12T17:22:50.353Z"},{"id":"linux-foundation-sysadmin-networking-1768260262823-4","question":"You suspect ARP storms on a host with interface eth1. To quickly verify ARP traffic and identify abnormal ARP activity on that interface, which command should you run?","answer":"To quickly verify ARP traffic and identify abnormal ARP activity on interface eth1, run 'tcpdump -i eth1 arp'. This command captures and displays ARP packets specifically on the eth1 interface, allowing you to monitor for ARP storms.","explanation":"## Correct Answer\nA. tcpdump with the arp filter on the specific interface captures ARP requests/replies, which is exactly what you need to observe ARP storms.\n\n## Why Other Options Are Wrong\n- Option B: ICMP captures are unrelated to ARP activity.\n- Option C: TCP traffic does not reveal ARP behavior.\n- Option D: arp -a shows ARP table entries but not live ARP traffic, making storms harder to observe in real time.\n\n## Key Concepts\n- ARP monitoring with packet captures\n- Interface-specific traffic analysis\n- Distinguishing ARP storms from normal ARP chatter\n\n## Real-World Application\n- Proactively diagnosing network broadcast storms and ARP-related outages in data-center or campus networks.","diagram":null,"difficulty":"intermediate","tags":["Networking","ARP","Linux","Kubernetes","certification-mcq","domain-weight-12"],"channel":"linux-foundation-sysadmin","subChannel":"networking","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T03:32:44.361Z","createdAt":"2026-01-12 23:24:24"}],"subChannels":["general","networking"],"companies":["Anthropic","Apple","Bloomberg","Citadel","Coinbase","Databricks","Discord","DoorDash","Goldman Sachs","Google","Hugging Face","IBM","Instacart","LinkedIn","Meta","Microsoft","MongoDB","Oracle","Plaid","Snap","Snowflake","Tesla","Twitter","Two Sigma"],"stats":{"total":17,"beginner":4,"intermediate":6,"advanced":7,"newThisWeek":17}}