{"questions":[{"id":"q-1044","question":"On a Linux host running multiple tenant services, peak I/O from a data ingestion daemon saturates the disk, causing latency spikes for others. Propose a production plan to isolate and throttle disk I/O across tenants using systemd slices and cgroup v2 io.max. Include concrete unit and slice snippets, per-device limits for a SATA HDD and NVMe, and a test plan using fio and iostat to verify tail latency improvements under concurrent workloads?","answer":"Implement per-tenant I/O isolation using systemd slices and cgroup v2 io.max. Create tenant-<name>.slice, assign services to it, and throttle devices (e.g., 8:0 rbps=50M wbps=50M). Include per-device ","explanation":"## Why This Is Asked\nTests ability to design resource isolation and practical configuration with systemd and cgroups in a multi-tenant environment, focusing on I/O pressure rather than CPU/memory alone.\n\n## Key Concepts\n- cgroup v2 io.max throttling\n- systemd slices and per-service isolation\n- per-device I/O limits and hierarchy\n- reproducible load testing with fio and iostat\n\n## Code Example\n```javascript\n# Example: set per-device I/O throttling for tenant-a.slice (pseudo)\nsudo mkdir -p /sys/fs/cgroup/tenant-a.slice\necho \"8:0 rbps=50M wbps=50M\" | sudo tee /sys/fs/cgroup/tenant-a.slice/io.max\n```\n\n## Follow-up Questions\n- How would you adapt if the workload alternates between read-heavy and write-heavy?\n- What are potential pitfalls with block layer throttling and caches?","diagram":null,"difficulty":"advanced","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Microsoft","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T20:30:42.832Z","createdAt":"2026-01-12T20:30:42.832Z"},{"id":"q-1056","question":"On a Linux host, a TLS proxy reads its certificate from /etc/ssl/certs/app.crt and currently reloads by restarting, causing brief downtime during renewal. Propose a zero-downtime rotation using a systemd.path trigger that fires on a new cert symlink, with a separate service unit for ExecReload and an atomic update scheme (swap in a new cert, then switch a symlink). Include concrete unit snippets and a test plan?","answer":"Propose a zero-downtime TLS cert rotation using systemd.path to trigger on an updated certificate symlink, plus a dedicated systemd service that ExecReloads the proxy. Use atomic cert updates (place n","explanation":"## Why This Is Asked\nTests practical mastery of systemd path activation, service reloads, and atomic file updates for TLS without downtime. It also validates understanding of race-free certificate rotation and testability.\n\n## Key Concepts\n- systemd.path and PathChanged/PathModified\n- ExecReload vs Restart semantics\n- Atomic file replacement via symlinks\n- Reproducible test plan for cert rotation\n\n## Code Example\n```ini\n# app-cert.path\n[Unit]\nDescription=Trigger TLS cert rotation on new cert\n\n[Path]\nPathChanged=/etc/ssl/certs/app.crt\nUnit=app-cert.service\n\n[Install]\nWantedBy=multi-user.target\n```\n```ini\n# app-cert.service\n[Unit]\nDescription=Reload TLS proxy with new certificate\n\n[Service]\nType=oneshot\nExecStart=/usr/local/bin/app-cert-rotate-reload.sh\n```\n```bash\n# app-cert-rotate-reload.sh\n#!/bin/sh\nset -e\n# Swap in the new cert and reload the proxy\nmv -f /etc/ssl/certs/app.crt.new /etc/ssl/certs/app.crt\nsystemctl reload app-proxy.service\n```\n```\n\n## Follow-up Questions\n- How to handle reload failure and ensure idempotence?\n- How to monitor for stale symlinks or missing new certs?\n- How would you extend this for multiple certificates or canary rollouts?","diagram":"flowchart TD\n  A[Cert renewal arrives] --> B[Write app.crt.new]\n  B --> C[symlink swap via script]\n  C --> D{Reload trigger}\n  D --> E[app-proxy reloads without downtime]","difficulty":"advanced","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T21:18:27.923Z","createdAt":"2026-01-12T21:18:27.923Z"},{"id":"q-1122","question":"On a Linux host running multiple ML model servers in separate systemd slices using cgroup v2, a spike in one tenant consumes memory and triggers host memory pressure despite per-tenant limits. Propose a production plan to enforce strict isolation and backpressure using memory.max and memory.high, enable group OOM behavior, and implement eviction/playback strategies. Include concrete unit and cgroup settings and a test plan with a reproducible spike and validation steps?","answer":"Implement per-tenant slices with MemoryMax and MemoryHigh in a cgroup v2 setup; attach each model server to its slice. Enable memory.oom_group for group-based OOM handling and configure a hard cap wit","explanation":"## Why This Is Asked\nTests the ability to design strict memory isolation in a multi-tenant Linux environment, covering cgroup v2 memory controls, systemd slices, and OOM behavior under load.\n\n## Key Concepts\n- cgroup v2 memory.max and memory.high for hard/soft limits\n- memory.oom_group for group-level OOM decisions\n- systemd slices and per-tenant unit configurations\n- observability: cgroup events, dmesg, and host health verification\n\n## Code Example\n```javascript\n# Example: tenant-A.slice\n[Slice]\nMemoryHigh=6G\nMemoryMax=8G\n\n# Example: tenant-A service\n[Service]\nSlice=tenant-A.slice\nExecStart=/usr/bin/modelA_server\nRestart=on-failure\n```\n\n## Follow-up Questions\n- How would you automate scale-out when new tenants are added?\n- What metrics and alerts would you surface to detect pressure early?","diagram":"flowchart TD\n  A[Tenant A] --> B[Cgroup v2 memory.max]\n  A --> C[Cgroup v2 memory.high]\n  B --> D[Host stability checks]\n  C --> D","difficulty":"advanced","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","MongoDB","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T23:29:53.881Z","createdAt":"2026-01-12T23:29:53.881Z"},{"id":"q-1230","question":"On a Linux host with a multi-tenant workload sharing a single 10GbE NIC configured with multiple receive queues, one tenant bursts UDP traffic and starves others, causing increased latency and packet loss. Propose a production plan to enforce tenant isolation and fairness using NIC multi-queue, RSS/XPS mapping, IRQ affinity, and per-tenant cgroups (v2) with io.max and tc shaping. Include concrete steps and a test plan with realistic traffic?","answer":"Plan to isolate tenants by binding NIC RX queues to per-tenant CPUs, use RSS/XPS to map flows, create per-tenant systemd slices with per-tenant CPU budgets, apply io.max quotas, and shape egress with ","explanation":"## Why This Is Asked\nAssesses practical network isolation skills in a multi-tenant Linux environment, focusing on NIC multi-queue tuning, IRQ affinity, and per-tenant resource containers to prevent bursts from one tenant affecting others.\n\n## Key Concepts\n- NIC multi-queue, RSS, XPS\n- IRQ affinity per-tenant isolation\n- cgroups v2 with io.max and per-tenant slices\n- tc shaping and fq_codel for fairness\n- Observability with iperf3/pktgen, iostat\n\n## Code Example\n```bash\n# Enable 4 RX queues\nethtool -L eth0 rx 4\n# Bind a few IRQs to CPUs (example)\necho 2-3,6-7 > /proc/irq/46/smp_affinity_list\n```\n\n## Follow-up Questions\n- How would you validate fairness under concurrent tenants?\n- What are NUMA pitfalls and how would you mitigate them?","diagram":"flowchart TD\n A[Tenants] --> B[RX queues]\n A --> C[CPU cores]\n B --> D[Mapping rules]\n C --> E[Isolation boundary]","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","MongoDB","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T05:40:43.739Z","createdAt":"2026-01-13T05:40:43.739Z"},{"id":"q-1327","question":"On a Linux host running multiple tenants via containers, a CPU-intensive workload from one tenant starves others. Propose a production plan to enforce CPU isolation using cgroup v2 slices, systemd integration, and per-tenant quotas; include concrete settings (cpu.max, cpu.weight), scheduling options (cpuset and isolcpus), and a test plan with realistic workloads and verification steps?","answer":"Create per-tenant cgroup v2 slices and assign each container to its slice. Use cpu.max and cpu.weight to cap and weight CPU usage (e.g., tenantA 250000 1000000, tenants B, C, D 150000 1000000; weights","explanation":"## Why This Is Asked\nAssesses practical mastery of modern Linux isolation (cgroup v2, systemd slices, cpuset) and real-world tradeoffs between throughput and fairness.\n\n## Key Concepts\n- cgroup v2 CPU controller: cpu.max, cpu.weight\n- systemd slices and container assignment\n- cpuset/isolation of CPUs; isolcpus boot option\n- realistic workloads: fio, sysbench; tail latency tracking\n\n## Code Example\n```bash\n# Illustrative: create and configure tenant slices (pseudo-commands)\nsystemd-run --unit=tenantA.slice --property=CPUQuota=250000 --property=CPUQuotaPeriodUS=1000000 --slice=tenantA.slice sleep 1h\nsystemd-run --unit=tenantB.slice --property=CPUQuota=150000 --property=CPUQuotaPeriodUS=1000000 --slice=tenantB.slice sleep 1h\n# Bind slices to CPUs via cpuset (illustrative)\nchmod 600 /sys/fs/cgroup/unified/tenantA.slice/cpuset.cpus\n# Attach containers to their slices as they spawn\n```\n\n## Follow-up Questions\n- How would you monitor CPU-steal and detect fairness violations in production?\n- What are risks of over-isolating CPUs, and how would you safely rebalance during load spikes?","diagram":null,"difficulty":"advanced","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Google","IBM"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T11:37:36.271Z","createdAt":"2026-01-13T11:37:36.271Z"},{"id":"q-1346","question":"On a production Linux host serving multiple ephemeral monitoring daemons, a heartbeat worker must emit a 5-second ping to a central collector. During peak load, the heartbeat misses several intervals, delaying alerts. Propose a concrete production plan to guarantee cadence and resilience using: (a) per-service CPU isolation and pinning, (b) high-priority timer/RT scheduling or systemd timers with precise tuning, (c) a fallback mechanism for missed heartbeats, and (d) a validation strategy with a controlled load. Include concrete config snippets for systemd, cpuset, and timer settings?","answer":"Plan pins heartbeat to a dedicated CPU set, uses a systemd timer with 5s cadence and AccuracySec=1s, and a high-priority heartbeat service with CPUQuota=50% and SCHED_FIFO-like behavior. Add a fallbac","explanation":"## Why This Is Asked\n\nTests knowledge of per-service isolation, timer accuracy, fault tolerance, and measurable validation.\n\n## Key Concepts\n\n- systemd slices timers CPU affinity\n- cpuset pinning and CPUQuota/AccuracySec\n- RT priority and fallback mechanisms\n- end-to-end verification under load\n\n## Code Example\n\n```\n# See /etc/systemd/system/heartbeat.* for concrete files\n```\n\n## Follow-up Questions\n\n- How would you detect missed heartbeats and trigger alerts?\n- What are the trade-offs of increasing CPU isolation vs. responsiveness?","diagram":"flowchart TD\n  A[Create heartbeat.slice] --> B[Pin heartbeat service]\n  B --> C[Configure 5s timer]\n  C --> D[Add fallback heartbeat]\n  D --> E[Test under load]","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","LinkedIn"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T13:05:27.792Z","createdAt":"2026-01-13T13:05:27.792Z"},{"id":"q-881","question":"On a Linux host, a long-running daemon writes to `/var/log/myapp.log` and is managed by systemd, but log rotation occasionally causes logging to stop after rotation. Propose a practical fix to ensure logging continues after rotation without restarting the daemon. Include the exact approach and a sample logrotate config snippet and testing steps?","answer":"Configure logrotate to reopen the log file after rotation instead of copying or truncating. After rotating, signal the daemon to reopen logs (e.g., SIGHUP). The config should include a postrotate that","explanation":"## Why This Is Asked\nTests practical handling of log rotation, signals, and ensuring service continuity without downtime. It checks knowledge of logrotate postrotate scripts, choosing the right approach (signal vs copytruncate), and how to validate in a controlled test.\n\n## Key Concepts\n- logrotate configuration fields and postrotate scripts\n- signaling daemons (SIGHUP) to reopen logs\n- copytruncate vs signaling trade-offs\n- testing routine for rotation\n\n## Code Example\n```bash\n/var/log/myapp.log {\n  rotate 5\n  weekly\n  missingok\n  notifempty\n  postrotate\n    kill -HUP `cat /var/run/myapp.pid` 2>/dev/null || true\n  endscript\n}\n```\n\n## Follow-up Questions\n- What are the trade-offs of copytruncate vs signaling?\n- How would you monitor and alert if log rotation fails to reopen logs?","diagram":null,"difficulty":"beginner","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Meta","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T13:59:55.582Z","createdAt":"2026-01-12T13:59:55.582Z"},{"id":"q-888","question":"A production Linux host runs a data-backup agent that uses /var/lib/backup/backup.lock to enforce a single instance. Sometimes a stale lock remains after a crash, blocking new runs; the agent also leaves non-terminating children on stop, risking partial backups. Propose a systemdâ€‘based lifecycle fix: ensure one instance, auto-clean stale lock, and graceful stop with timeout and fallback to kill. Include concrete unit snippets and verification steps?","answer":"Use a systemd unit with Type=forking, PIDFile, and ExecStartPre that checks and clears a stale lock: if /var/lib/backup/backup.lock exists and its PID is not running, delete it. Stop uses KillMode=con","explanation":"## Why This Is Asked\nTests understanding of robust service lifecycle management with systemd, handling singleton constraints, and clean termination of complex processes.\n\n## Key Concepts\n- systemd lifecycle: ExecStartPre, ExecStop, KillMode, TimeoutStopSec\n- singleton enforcement via lock files\n- graceful termination vs. forceful kill for child processes\n\n## Code Example\n```ini\n; /etc/systemd/system/backup-agent.service\n[Unit]\nDescription=Backup Agent\nAfter=network.target\n\n[Service]\nType=forking\nPIDFile=/var/run/backup/backup.pid\nExecStartPre=/bin/sh -c 'LOCK=/var/lib/backup/backup.lock; if [ -e \"$LOCK\" ]; then pid=$(cat \"$LOCK\"); if [ -d /proc/$pid ]; then exit 1; else rm -f \"$LOCK\"; fi; fi'\nExecStart=/usr/local/bin/backup-agent\nExecStop=/bin/kill -TERM $MAINPID\nExecStopPost=/bin/rm -f /var/lib/backup/backup.lock\nTimeoutStopSec=120s\nKillMode=control-group\nRestart=on-failure\n```\n\n## Follow-up Questions\n- How would you test idempotency for consecutive startups?\n- How would you adapt if the agent uses a PID file instead of a lock file?","diagram":null,"difficulty":"advanced","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T14:27:44.056Z","createdAt":"2026-01-12T14:27:44.056Z"},{"id":"q-961","question":"On a Linux host, a memory-hungry log-harvester daemon managed by systemd sporadically triggers the kernel OOM killer during peak load, crashing the service and delaying alerts. Propose a production-safe plan to prevent OOM termination while preserving throughput. Include concrete systemd settings (MemoryLimit, MemorySwapMax, OOMScoreAdjust, Restart), kernel tuning (swap, swappiness), and a test plan with a reproducible high-memory scenario and verification steps?","answer":"Apply per-service memory controls and OOM protection. Set MemoryLimit=2G, MemorySwapMax=2G, OOMScoreAdjust=-100, and Restart=on-failure with a 5s backoff. Tune vm.swappiness=10 and ensure swap is enab","explanation":"## Why This Is Asked\nTests memory pressure handling, systemd tuning, and safe recovery without service disruption.\n\n## Key Concepts\n- Kernel OOM killer and oom_score_adj\n- systemd memory constraints (MemoryLimit, MemorySwapMax)\n- Restart strategies and timeouts\n- memory tuning and swap behavior\n\n## Code Example\n```ini\n[Unit]\nDescription=Log Harvester\n\n[Service]\nExecStart=/usr/local/bin/logharvester\nType=simple\nMemoryLimit=2G\nMemorySwapMax=2G\nOOMScoreAdjust=-100\nRestart=on-failure\nRestartSec=5s\n```\n\n## Follow-up Questions\n- How would you observe and alert on OOM events?\n- How would you adjust for multiple high-memory services competing for swap?","diagram":"flowchart TD\n  A[Memory Pressure] --> B{OOM killer?}\n  B -->|Yes| C[Adjust OOMScore/MemoryLimit]\n  B -->|No| D[Normal Operation]\n  C --> E[Daemon Survives, backlog Drains]","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T17:22:50.353Z","createdAt":"2026-01-12T17:22:50.353Z"},{"id":"linux-foundation-sysadmin-networking-1768260262823-4","question":"You suspect ARP storms on a host with interface eth1. To quickly verify ARP traffic and identify abnormal ARP activity on that interface, which command should you run?","answer":"To quickly verify ARP traffic and identify abnormal ARP activity on interface eth1, run 'tcpdump -i eth1 arp'. This command captures and displays ARP packets specifically on the eth1 interface, allowing you to monitor for ARP storms.","explanation":"## Correct Answer\nA. tcpdump with the arp filter on the specific interface captures ARP requests/replies, which is exactly what you need to observe ARP storms.\n\n## Why Other Options Are Wrong\n- Option B: ICMP captures are unrelated to ARP activity.\n- Option C: TCP traffic does not reveal ARP behavior.\n- Option D: arp -a shows ARP table entries but not live ARP traffic, making storms harder to observe in real time.\n\n## Key Concepts\n- ARP monitoring with packet captures\n- Interface-specific traffic analysis\n- Distinguishing ARP storms from normal ARP chatter\n\n## Real-World Application\n- Proactively diagnosing network broadcast storms and ARP-related outages in data-center or campus networks.","diagram":null,"difficulty":"intermediate","tags":["Networking","ARP","Linux","Kubernetes","certification-mcq","domain-weight-12"],"channel":"linux-foundation-sysadmin","subChannel":"networking","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T03:32:44.361Z","createdAt":"2026-01-12 23:24:24"}],"subChannels":["general","networking"],"companies":["Apple","Bloomberg","Citadel","Databricks","DoorDash","Goldman Sachs","Google","Hugging Face","IBM","LinkedIn","Meta","Microsoft","MongoDB","Plaid","Snowflake","Tesla","Twitter","Two Sigma"],"stats":{"total":10,"beginner":1,"intermediate":4,"advanced":5,"newThisWeek":10}}