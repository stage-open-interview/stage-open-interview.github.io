{"questions":[{"id":"linux-foundation-sysadmin-essential-commands-1768169900531-0","question":"Which command creates a gzipped tarball of the contents of the directory /srv/app, excluding the file config.yaml?","answer":"[{\"id\":\"a\",\"text\":\"tar czf /backup/app.tar.gz -C /srv/app --exclude=config.yaml .\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"tar czf /backup/app.tar.gz -C /srv/app/config.yaml .\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"tar czf /backup/app.tar.gz -C /srv/app --exclude=/srv/app/config.yaml .\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"tar czf /backup/app.tar.gz -C /srv/app --exclude=config.yaml\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA. The command tar czf /backup/app.tar.gz -C /srv/app --exclude=config.yaml . creates a gzipped tarball of the directory /srv/app, excluding the file named config.yaml. The -C /srv/app switches to that directory, and the trailing . includes all contents. The --exclude flag patterns exclude matching paths from the archive.\n\n## Why Other Options Are Wrong\n- B attempts to archive a single file path instead of the directory contents and would not produce the full directory tree as intended.\n- C uses an absolute path in --exclude which tar may interpret relative to the -C path and could fail to exclude the intended file.\n- D uses a non-existent short option -exclude and would fail to parse.\n\n## Key Concepts\n- Tar with compression: czf\n- -C to change directory before archiving\n- --exclude to skip patterns\n- including the contents of a directory with .\n\n## Real-World Application\n- Backing up an application directory while omitting sensitive or large config files during automated backups.","diagram":null,"difficulty":"intermediate","tags":["Linux","Essential Commands","AWS","Kubernetes","Terraform","certification-mcq","domain-weight-25"],"channel":"linux-foundation-sysadmin","subChannel":"essential-commands","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T22:18:20.535Z","createdAt":"2026-01-11 22:18:20"},{"id":"linux-foundation-sysadmin-essential-commands-1768169900531-1","question":"You want to list the 20 most recently modified files under /var/log, including their full paths, and constrain results to regular files only. Which command is most robust?","answer":"[{\"id\":\"a\",\"text\":\"find /var/log -type f -printf '%T@ %p\\\\n' | sort -n | tail -n 20\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"ls -ltR /var/log | head -n 20\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"find /var/log -type d -mtime -1\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"stat -c '%y %n' /var/log/* | sort -n | tail -n 20\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA. The find command with -type f restricts to files, -printf '%T@ %p' prints a timestamp and path, and sorting by time then tail -n 20 returns the 20 most recently modified files with full paths.\n\n## Why Other Options Are Wrong\n- B lists entries recursively but may include directories and non-files, and head -n 20 may miss the actual most recently modified files and not show full paths consistently.\n- C searches only directories, not files.\n- D uses wildcard expansion that may fail with spaces or hidden files and does not filter by modification time as robustly.\n\n## Key Concepts\n- find with -type f and -printf\n- Sorting by timestamp to identify recent changes\n- Handling full paths for real-world log audits\n\n## Real-World Application\n- Quickly auditing the most recently updated log files for incident response or troubleshooting.","diagram":null,"difficulty":"intermediate","tags":["Linux","Essential Commands","AWS","Kubernetes","Terraform","certification-mcq","domain-weight-25"],"channel":"linux-foundation-sysadmin","subChannel":"essential-commands","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T22:18:21.057Z","createdAt":"2026-01-11 22:18:21"},{"id":"linux-foundation-sysadmin-essential-commands-1768169900531-2","question":"Which command prints the top memory consuming process's PID and RSS value, ignoring the header line, in a single line?","answer":"[{\"id\":\"a\",\"text\":\"ps -eo pid,rss,cmd --sort=-rss | awk 'NR==2{print $1, $2}'\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"ps -eo pid,rss,cmd --sort=-rss | sed -n '2p'\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"ps -eo pid,rss,cmd --sort=-rss | tail -n +2 | head -n 1\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"ps aux --sort=-rss | head -n 1\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA. The command prints only the PID and RSS fields for the second line of the ps output, which corresponds to the top memory-consuming process when sorted by RSS. The use of awk ensures only PID and RSS are shown.\n\n## Why Other Options Are Wrong\n- B prints the entire line which also contains the CMD, not just PID and RSS.\n- C skips the header but may return the top line differently depending on environment and still includes more fields if not filtered.\n- D shows the header line or the first line of ps output in some environments, which does not guarantee extracting PID and RSS of the top memory process.\n\n## Key Concepts\n- ps with --sort=-rss for memory ranking\n- awk to select specific columns\n- Excluding the header for clean data extraction\n\n## Real-World Application\n- Quickly triaging memory hogs on a live server during incident response.","diagram":null,"difficulty":"intermediate","tags":["Linux","Essential Commands","AWS","Kubernetes","Terraform","certification-mcq","domain-weight-25"],"channel":"linux-foundation-sysadmin","subChannel":"essential-commands","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T22:18:21.523Z","createdAt":"2026-01-11 22:18:21"},{"id":"q-1044","question":"On a Linux host running multiple tenant services, peak I/O from a data ingestion daemon saturates the disk, causing latency spikes for others. Propose a production plan to isolate and throttle disk I/O across tenants using systemd slices and cgroup v2 io.max. Include concrete unit and slice snippets, per-device limits for a SATA HDD and NVMe, and a test plan using fio and iostat to verify tail latency improvements under concurrent workloads?","answer":"Implement per-tenant I/O isolation using systemd slices and cgroup v2 io.max. Create tenant-<name>.slice, assign services to it, and throttle devices (e.g., 8:0 rbps=50M wbps=50M). Include per-device ","explanation":"## Why This Is Asked\nTests ability to design resource isolation and practical configuration with systemd and cgroups in a multi-tenant environment, focusing on I/O pressure rather than CPU/memory alone.\n\n## Key Concepts\n- cgroup v2 io.max throttling\n- systemd slices and per-service isolation\n- per-device I/O limits and hierarchy\n- reproducible load testing with fio and iostat\n\n## Code Example\n```javascript\n# Example: set per-device I/O throttling for tenant-a.slice (pseudo)\nsudo mkdir -p /sys/fs/cgroup/tenant-a.slice\necho \"8:0 rbps=50M wbps=50M\" | sudo tee /sys/fs/cgroup/tenant-a.slice/io.max\n```\n\n## Follow-up Questions\n- How would you adapt if the workload alternates between read-heavy and write-heavy?\n- What are potential pitfalls with block layer throttling and caches?","diagram":null,"difficulty":"advanced","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Microsoft","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T20:30:42.832Z","createdAt":"2026-01-12T20:30:42.832Z"},{"id":"q-1056","question":"On a Linux host, a TLS proxy reads its certificate from /etc/ssl/certs/app.crt and currently reloads by restarting, causing brief downtime during renewal. Propose a zero-downtime rotation using a systemd.path trigger that fires on a new cert symlink, with a separate service unit for ExecReload and an atomic update scheme (swap in a new cert, then switch a symlink). Include concrete unit snippets and a test plan?","answer":"Propose a zero-downtime TLS cert rotation using systemd.path to trigger on an updated certificate symlink, plus a dedicated systemd service that ExecReloads the proxy. Use atomic cert updates (place n","explanation":"## Why This Is Asked\nTests practical mastery of systemd path activation, service reloads, and atomic file updates for TLS without downtime. It also validates understanding of race-free certificate rotation and testability.\n\n## Key Concepts\n- systemd.path and PathChanged/PathModified\n- ExecReload vs Restart semantics\n- Atomic file replacement via symlinks\n- Reproducible test plan for cert rotation\n\n## Code Example\n```ini\n# app-cert.path\n[Unit]\nDescription=Trigger TLS cert rotation on new cert\n\n[Path]\nPathChanged=/etc/ssl/certs/app.crt\nUnit=app-cert.service\n\n[Install]\nWantedBy=multi-user.target\n```\n```ini\n# app-cert.service\n[Unit]\nDescription=Reload TLS proxy with new certificate\n\n[Service]\nType=oneshot\nExecStart=/usr/local/bin/app-cert-rotate-reload.sh\n```\n```bash\n# app-cert-rotate-reload.sh\n#!/bin/sh\nset -e\n# Swap in the new cert and reload the proxy\nmv -f /etc/ssl/certs/app.crt.new /etc/ssl/certs/app.crt\nsystemctl reload app-proxy.service\n```\n```\n\n## Follow-up Questions\n- How to handle reload failure and ensure idempotence?\n- How to monitor for stale symlinks or missing new certs?\n- How would you extend this for multiple certificates or canary rollouts?","diagram":"flowchart TD\n  A[Cert renewal arrives] --> B[Write app.crt.new]\n  B --> C[symlink swap via script]\n  C --> D{Reload trigger}\n  D --> E[app-proxy reloads without downtime]","difficulty":"advanced","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T21:18:27.923Z","createdAt":"2026-01-12T21:18:27.923Z"},{"id":"q-1122","question":"On a Linux host running multiple ML model servers in separate systemd slices using cgroup v2, a spike in one tenant consumes memory and triggers host memory pressure despite per-tenant limits. Propose a production plan to enforce strict isolation and backpressure using memory.max and memory.high, enable group OOM behavior, and implement eviction/playback strategies. Include concrete unit and cgroup settings and a test plan with a reproducible spike and validation steps?","answer":"Implement per-tenant slices with MemoryMax and MemoryHigh in a cgroup v2 setup; attach each model server to its slice. Enable memory.oom_group for group-based OOM handling and configure a hard cap wit","explanation":"## Why This Is Asked\nTests the ability to design strict memory isolation in a multi-tenant Linux environment, covering cgroup v2 memory controls, systemd slices, and OOM behavior under load.\n\n## Key Concepts\n- cgroup v2 memory.max and memory.high for hard/soft limits\n- memory.oom_group for group-level OOM decisions\n- systemd slices and per-tenant unit configurations\n- observability: cgroup events, dmesg, and host health verification\n\n## Code Example\n```javascript\n# Example: tenant-A.slice\n[Slice]\nMemoryHigh=6G\nMemoryMax=8G\n\n# Example: tenant-A service\n[Service]\nSlice=tenant-A.slice\nExecStart=/usr/bin/modelA_server\nRestart=on-failure\n```\n\n## Follow-up Questions\n- How would you automate scale-out when new tenants are added?\n- What metrics and alerts would you surface to detect pressure early?","diagram":"flowchart TD\n  A[Tenant A] --> B[Cgroup v2 memory.max]\n  A --> C[Cgroup v2 memory.high]\n  B --> D[Host stability checks]\n  C --> D","difficulty":"advanced","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","MongoDB","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T23:29:53.881Z","createdAt":"2026-01-12T23:29:53.881Z"},{"id":"q-1230","question":"On a Linux host with a multi-tenant workload sharing a single 10GbE NIC configured with multiple receive queues, one tenant bursts UDP traffic and starves others, causing increased latency and packet loss. Propose a production plan to enforce tenant isolation and fairness using NIC multi-queue, RSS/XPS mapping, IRQ affinity, and per-tenant cgroups (v2) with io.max and tc shaping. Include concrete steps and a test plan with realistic traffic?","answer":"Plan to isolate tenants by binding NIC RX queues to per-tenant CPUs, use RSS/XPS to map flows, create per-tenant systemd slices with per-tenant CPU budgets, apply io.max quotas, and shape egress with ","explanation":"## Why This Is Asked\nAssesses practical network isolation skills in a multi-tenant Linux environment, focusing on NIC multi-queue tuning, IRQ affinity, and per-tenant resource containers to prevent bursts from one tenant affecting others.\n\n## Key Concepts\n- NIC multi-queue, RSS, XPS\n- IRQ affinity per-tenant isolation\n- cgroups v2 with io.max and per-tenant slices\n- tc shaping and fq_codel for fairness\n- Observability with iperf3/pktgen, iostat\n\n## Code Example\n```bash\n# Enable 4 RX queues\nethtool -L eth0 rx 4\n# Bind a few IRQs to CPUs (example)\necho 2-3,6-7 > /proc/irq/46/smp_affinity_list\n```\n\n## Follow-up Questions\n- How would you validate fairness under concurrent tenants?\n- What are NUMA pitfalls and how would you mitigate them?","diagram":"flowchart TD\n A[Tenants] --> B[RX queues]\n A --> C[CPU cores]\n B --> D[Mapping rules]\n C --> E[Isolation boundary]","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","MongoDB","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T05:40:43.739Z","createdAt":"2026-01-13T05:40:43.739Z"},{"id":"q-881","question":"On a Linux host, a long-running daemon writes to `/var/log/myapp.log` and is managed by systemd, but log rotation occasionally causes logging to stop after rotation. Propose a practical fix to ensure logging continues after rotation without restarting the daemon. Include the exact approach and a sample logrotate config snippet and testing steps?","answer":"Configure logrotate to reopen the log file after rotation instead of copying or truncating. After rotating, signal the daemon to reopen logs (e.g., SIGHUP). The config should include a postrotate that","explanation":"## Why This Is Asked\nTests practical handling of log rotation, signals, and ensuring service continuity without downtime. It checks knowledge of logrotate postrotate scripts, choosing the right approach (signal vs copytruncate), and how to validate in a controlled test.\n\n## Key Concepts\n- logrotate configuration fields and postrotate scripts\n- signaling daemons (SIGHUP) to reopen logs\n- copytruncate vs signaling trade-offs\n- testing routine for rotation\n\n## Code Example\n```bash\n/var/log/myapp.log {\n  rotate 5\n  weekly\n  missingok\n  notifempty\n  postrotate\n    kill -HUP `cat /var/run/myapp.pid` 2>/dev/null || true\n  endscript\n}\n```\n\n## Follow-up Questions\n- What are the trade-offs of copytruncate vs signaling?\n- How would you monitor and alert if log rotation fails to reopen logs?","diagram":null,"difficulty":"beginner","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Meta","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:59:55.582Z","createdAt":"2026-01-12T13:59:55.582Z"},{"id":"q-888","question":"A production Linux host runs a data-backup agent that uses /var/lib/backup/backup.lock to enforce a single instance. Sometimes a stale lock remains after a crash, blocking new runs; the agent also leaves non-terminating children on stop, risking partial backups. Propose a systemd‑based lifecycle fix: ensure one instance, auto-clean stale lock, and graceful stop with timeout and fallback to kill. Include concrete unit snippets and verification steps?","answer":"Use a systemd unit with Type=forking, PIDFile, and ExecStartPre that checks and clears a stale lock: if /var/lib/backup/backup.lock exists and its PID is not running, delete it. Stop uses KillMode=con","explanation":"## Why This Is Asked\nTests understanding of robust service lifecycle management with systemd, handling singleton constraints, and clean termination of complex processes.\n\n## Key Concepts\n- systemd lifecycle: ExecStartPre, ExecStop, KillMode, TimeoutStopSec\n- singleton enforcement via lock files\n- graceful termination vs. forceful kill for child processes\n\n## Code Example\n```ini\n; /etc/systemd/system/backup-agent.service\n[Unit]\nDescription=Backup Agent\nAfter=network.target\n\n[Service]\nType=forking\nPIDFile=/var/run/backup/backup.pid\nExecStartPre=/bin/sh -c 'LOCK=/var/lib/backup/backup.lock; if [ -e \"$LOCK\" ]; then pid=$(cat \"$LOCK\"); if [ -d /proc/$pid ]; then exit 1; else rm -f \"$LOCK\"; fi; fi'\nExecStart=/usr/local/bin/backup-agent\nExecStop=/bin/kill -TERM $MAINPID\nExecStopPost=/bin/rm -f /var/lib/backup/backup.lock\nTimeoutStopSec=120s\nKillMode=control-group\nRestart=on-failure\n```\n\n## Follow-up Questions\n- How would you test idempotency for consecutive startups?\n- How would you adapt if the agent uses a PID file instead of a lock file?","diagram":null,"difficulty":"advanced","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T14:27:44.056Z","createdAt":"2026-01-12T14:27:44.056Z"},{"id":"q-961","question":"On a Linux host, a memory-hungry log-harvester daemon managed by systemd sporadically triggers the kernel OOM killer during peak load, crashing the service and delaying alerts. Propose a production-safe plan to prevent OOM termination while preserving throughput. Include concrete systemd settings (MemoryLimit, MemorySwapMax, OOMScoreAdjust, Restart), kernel tuning (swap, swappiness), and a test plan with a reproducible high-memory scenario and verification steps?","answer":"Apply per-service memory controls and OOM protection. Set MemoryLimit=2G, MemorySwapMax=2G, OOMScoreAdjust=-100, and Restart=on-failure with a 5s backoff. Tune vm.swappiness=10 and ensure swap is enab","explanation":"## Why This Is Asked\nTests memory pressure handling, systemd tuning, and safe recovery without service disruption.\n\n## Key Concepts\n- Kernel OOM killer and oom_score_adj\n- systemd memory constraints (MemoryLimit, MemorySwapMax)\n- Restart strategies and timeouts\n- memory tuning and swap behavior\n\n## Code Example\n```ini\n[Unit]\nDescription=Log Harvester\n\n[Service]\nExecStart=/usr/local/bin/logharvester\nType=simple\nMemoryLimit=2G\nMemorySwapMax=2G\nOOMScoreAdjust=-100\nRestart=on-failure\nRestartSec=5s\n```\n\n## Follow-up Questions\n- How would you observe and alert on OOM events?\n- How would you adjust for multiple high-memory services competing for swap?","diagram":"flowchart TD\n  A[Memory Pressure] --> B{OOM killer?}\n  B -->|Yes| C[Adjust OOMScore/MemoryLimit]\n  B -->|No| D[Normal Operation]\n  C --> E[Daemon Survives, backlog Drains]","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T17:22:50.353Z","createdAt":"2026-01-12T17:22:50.353Z"},{"id":"linux-foundation-sysadmin-networking-1768260262823-0","question":"A Linux server has two NICs: eth0 in 192.168.1.0/24 with gateway 192.168.1.1 and eth1 in 10.0.0.0/24 with gateway 10.0.0.1. You need all traffic destined for 203.0.113.0/24 to exit via eth1 using the 10.0.0.1 gateway, while all other traffic continues using the main routing table. Which configuration will achieve this using policy routing?","answer":"[{\"id\":\"a\",\"text\":\"ip rule add from 10.0.0.0/24 lookup 100\\nip route add default via 10.0.0.1 dev eth1 table 100\\nip route add 203.0.113.0/24 via 10.0.0.1 dev eth1 table 100\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"ip rule add from 203.0.113.0/24 lookup 100\\nip route add default via 203.0.113.1 dev eth0 table 100\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"ip route add 203.0.113.0/24 via 192.168.1.1 dev eth0\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"sysctl -w net.ipv4.conf.all.rp_filter=1\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA. The policy routing approach using ip rule and a separate routing table (100) is correct because it matches traffic based on the source network and directs it to the desired gateway via eth1. Other options either match the wrong criteria or rely on the main table.\n\n## Why Other Options Are Wrong\n- Option B: Tries to match based on destination and uses an unrelated gateway, not enforcing per-source routing.\n- Option C: Routes the 203.0.113.0/24 destination via eth0, bypassing the policy routing intention.\n- Option D: rp_filter settings do not influence routing decisions; they only affect reverse path filtering.\n\n## Key Concepts\n- Policy-based routing with ip rule and multiple routing tables\n- Source-based routing for multi-homed hosts\n- Importance of proper table lookup and source matching\n\n## Real-World Application\n- Use case: multi-homed servers (eg, cloud VMs with multiple NICs or VPN/physical uplinks) requiring destinations to exit via a specific interface for policy, regulatory, or latency considerations.","diagram":null,"difficulty":"intermediate","tags":["Networking","PolicyRouting","Linux","AWS EC2","Kubernetes","certification-mcq","domain-weight-12"],"channel":"linux-foundation-sysadmin","subChannel":"networking","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T23:24:22.824Z","createdAt":"2026-01-12 23:24:23"},{"id":"linux-foundation-sysadmin-networking-1768260262823-1","question":"You need to configure a firewall on a Linux host using iptables to allow SSH (port 22) and ICMP echo requests inbound on interface eth0, permit established and related connections, and drop all other inbound traffic. Which option provides a correct minimal rule set?","answer":"[{\"id\":\"a\",\"text\":\"iptables -A INPUT -i eth0 -m conntrack --ctstate ESTABLISHED,RELATED -j ACCEPT\\niptables -A INPUT -i eth0 -p tcp --dport 22 -j ACCEPT\\niptables -A INPUT -i eth0 -p icmp -j ACCEPT\\niptables -A INPUT -j DROP\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"iptables -A INPUT -i eth0 -p tcp --dport 22 -j ACCEPT\\niptables -A INPUT -i eth0 -p icmp -j ACCEPT\\niptables -A INPUT -j DROP\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"iptables -P INPUT ACCEPT\\niptables -A INPUT -i eth0 -p tcp --dport 22 -j ACCEPT\\niptables -A INPUT -j DROP\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"ufw allow 22/tcp\\nufw default deny Incoming\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA. This rule set explicitly allows established/related traffic, permits SSH on eth0, allows ICMP, and then drops other inbound traffic, providing a secure baseline.\n\n## Why Other Options Are Wrong\n- Option B: Omits the established/related rule, which may block legitimate responses to outbound connections.\n- Option C: Sets a permissive default policy and drops after, but the combination is inconsistent and relies on an already open baseline.\n- Option D: Uses ufw, which is not the requested iptables approach and may bypass the exact chain rules demonstrated in the scenario.\n\n## Key Concepts\n- Stateful firewall rules with conntrack\n- Least privilege for inbound traffic\n- Interface-scoped filtering\n\n## Real-World Application\n- Hardening a host temporarily exposed to the internet while allowing essential services.","diagram":null,"difficulty":"intermediate","tags":["Networking","Firewall","Linux","AWS EC2","Kubernetes","certification-mcq","domain-weight-12"],"channel":"linux-foundation-sysadmin","subChannel":"networking","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T23:24:23.276Z","createdAt":"2026-01-12 23:24:23"},{"id":"linux-foundation-sysadmin-networking-1768260262823-2","question":"A host connects to a network through a single physical interface that carries VLANs 10 and 20. You want to create VLAN subinterfaces eth0.10 and eth0.20 with IPs 192.168.10.1/24 and 192.168.20.1/24 respectively. Which command sequence correctly creates and activates these subinterfaces?","answer":"[{\"id\":\"a\",\"text\":\"ip link add link eth0 name eth0.10 type vlan id 10\\nip link add link eth0 name eth0.20 type vlan id 20\\nip addr add 192.168.10.1/24 dev eth0.10\\nip addr add 192.168.20.1/24 dev eth0.20\\nip link set up dev eth0.10\\nip link set up dev eth0.20\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"vconfig add eth0 10\\nvconfig add eth0 20\\nifconfig eth0.10 192.168.10.1 netmask 255.255.255.0 up\\nifconfig eth0.20 192.168.20.1 netmask 255.255.255.0 up\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"ip link add eth0.10 type vlan id 10\\nip link add eth0.20 type vlan id 20\\nip addr add 192.168.10.1/24 dev eth0.10\\nip addr add 192.168.20.1/24 dev eth0.20\\nip link set up dev eth0.10\\nip link set up dev eth0.20\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"ifconfig eth0.10 192.168.10.1 netmask 255.255.255.0 up\\nifconfig eth0.20 192.168.20.1 netmask 255.255.255.0 up\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA. The modern, correct method to create VLAN subinterfaces is using 802.1Q via ip link with vlan id, then assign IPs and bring interfaces up. The other options rely on deprecated tools (vconfig/ifconfig) or omit the proper vlan subinterface creation syntax.\n\n## Why Other Options Are Wrong\n- Option B: Uses deprecated vconfig/ifconfig approach and may not work on newer kernels.\n- Option C: Repeats creation steps but lacks the exact modern syntax for VLAN subinterfaces; also uses the wrong interface naming in some distros.\n- Option D: Uses only legacy tools and lacks VLAN subinterface creation through 802.1Q tagging.\n\n## Key Concepts\n- VLAN tagging with 802.1Q\n- Creating VLAN subinterfaces and assigning IPs\n- Modern Linux networking tools (ip) vs legacy tools\n\n## Real-World Application\n- Connecting a single NIC to a trunk port with multiple VLANs in data-center or cloud-lan scenarios.","diagram":null,"difficulty":"intermediate","tags":["Networking","VLAN","Linux","Kubernetes","certification-mcq","domain-weight-12"],"channel":"linux-foundation-sysadmin","subChannel":"networking","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T23:24:23.666Z","createdAt":"2026-01-12 23:24:23"},{"id":"linux-foundation-sysadmin-networking-1768260262823-3","question":"A Linux host is connected to a VPN tunnel that adds overhead, causing large TCP flows to repeatedly fragment. What is a practical Linux-level adjustment to reduce fragmentation without changing the end-to-end path?","answer":"[{\"id\":\"a\",\"text\":\"Set the VPN interface MTU to 1400 and apply MSS clamping on the VPN tunnel to 1360\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Increase the global MTU to 9000 on all interfaces\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Disable PMTUD on all routers in the path\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Enable IP fragmentation on the VPN interface\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA. Reducing the VPN interface MTU to account for encapsulation overhead and clamping the MSS for TCP prevents fragmentation of large packets across the tunnel.\n\n## Why Other Options Are Wrong\n- Option B: Increasing MTU globally often causes more fragmentation and requires end-to-end changes; not VPN-friendly.\n- Option C: Disabling PMTUD can hide fragmentation issues but does not fix performance; it can worsen reliability.\n- Option D: Enabling fragmentation is undesirable; it increases fragmentation risk and reduces performance.\n\n## Key Concepts\n- MTU/MSS tuning for VPNs\n- MSS clamping to prevent fragmentation\n- TCP performance over encapsulated tunnels\n\n## Real-World Application\n- Data-center or cloud VPNs where overheads require tuning to maintain throughput without fragmentation.","diagram":null,"difficulty":"intermediate","tags":["Networking","VPN","Linux","AWS VPC","Terraform","certification-mcq","domain-weight-12"],"channel":"linux-foundation-sysadmin","subChannel":"networking","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T23:24:23.814Z","createdAt":"2026-01-12 23:24:23"},{"id":"linux-foundation-sysadmin-networking-1768260262823-4","question":"You suspect ARP storms on a host with interface eth1. To quickly verify ARP traffic and identify abnormal ARP activity on that interface, which command should you run?","answer":"To quickly verify ARP traffic and identify abnormal ARP activity on interface eth1, run 'tcpdump -i eth1 arp'. This command captures and displays ARP packets specifically on the eth1 interface, allowing you to monitor for ARP storms.","explanation":"## Correct Answer\nA. tcpdump with the arp filter on the specific interface captures ARP requests/replies, which is exactly what you need to observe ARP storms.\n\n## Why Other Options Are Wrong\n- Option B: ICMP captures are unrelated to ARP activity.\n- Option C: TCP traffic does not reveal ARP behavior.\n- Option D: arp -a shows ARP table entries but not live ARP traffic, making storms harder to observe in real time.\n\n## Key Concepts\n- ARP monitoring with packet captures\n- Interface-specific traffic analysis\n- Distinguishing ARP storms from normal ARP chatter\n\n## Real-World Application\n- Proactively diagnosing network broadcast storms and ARP-related outages in data-center or campus networks.","diagram":null,"difficulty":"intermediate","tags":["Networking","ARP","Linux","Kubernetes","certification-mcq","domain-weight-12"],"channel":"linux-foundation-sysadmin","subChannel":"networking","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T03:32:44.361Z","createdAt":"2026-01-12 23:24:24"},{"id":"linux-foundation-sysadmin-operation-running-1768206659074-0","question":"On a server running systemd, the persistent journal is consuming disk space. You want to reduce the stored logs to a maximum 500M while preserving the most recent logs. Which command should you run?","answer":"[{\"id\":\"a\",\"text\":\"journalctl --rotate && journalctl --vacuum-size=500M\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"journalctl --vacuum-size=500M\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"systemctl restart systemd-journald\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"rm -rf /var/log/journal/*\",\"isCorrect\":false}]","explanation":"## Q1 Correct Answer\n- B\n\n## Why Other Options Are Wrong\n- A: Rotating logs without vacuuming may not reduce total space to the desired limit and can still leave old entries. Vacuum requires size limit and keeps recent data when combined with rotation; using only rotate without vacuum is insufficient.\n- C: Restarting the service does not reduce the amount of stored logs; it only restarts the daemon and may trigger a new log file but does not enforce retention.\n- D: Manual deletion of journal files can corrupt the journal or lose the ability to query it reliably; it bypasses proper retention policies and may break integrity.\n\n## Key Concepts\n- Systemd journal retention\n- journalctl vacuum options\n- Persistent vs volatile journald storage\n\n## Real-World Application\n- You implement automated log retention for hosts running systemd, ensuring disk space is reclaimed without losing access to the most recent events.","diagram":null,"difficulty":"intermediate","tags":["Linux","systemd","journald","AWS","Kubernetes","certification-mcq","domain-weight-20"],"channel":"linux-foundation-sysadmin","subChannel":"operation-running","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T08:30:59.075Z","createdAt":"2026-01-12 08:30:59"},{"id":"linux-foundation-sysadmin-operation-running-1768206659074-1","question":"You maintain a daemon on a systemd-based host. The daemon occasionally terminates due to transient errors. You want systemd to restart it automatically but with a backoff to avoid flapping. Which unit configuration is correct?","answer":"[{\"id\":\"a\",\"text\":\"Restart=on-failure, RestartSec=10s\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Restart=always, RestartSec=0s\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Restart=on-success, RestartSec=60s\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"TimeoutStartSec=2s, StartLimitInterval=60s\",\"isCorrect\":false}]","explanation":"## Q2 Correct Answer\n- A\n\n## Why Other Options Are Wrong\n- B: Restart=always restarts the service even on normal exits or successes, which can cause unnecessary restarts.\n- C: Restart=on-success would only restart on a clean exit, which is not suitable for transient failures.\n- D: TimeoutStartSec and StartLimitInterval are unrelated to implementing a backoff restart policy and do not guarantee backoff behavior.\n\n## Key Concepts\n- systemd Restart options\n- Backoff strategies\n- StartLimit to prevent crash loops\n\n## Real-World Application\n- Applying backoff policies prevents log floods and helps maintain service stability during transient issues.","diagram":null,"difficulty":"intermediate","tags":["Linux","systemd","monitoring","AWS","Kubernetes","certification-mcq","domain-weight-20"],"channel":"linux-foundation-sysadmin","subChannel":"operation-running","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T08:30:59.498Z","createdAt":"2026-01-12 08:30:59"},{"id":"linux-foundation-sysadmin-operation-running-1768206659074-2","question":"A backup script run by cron fails because required commands are not in cron's default PATH. What change will most reliably ensure the script runs under cron?","answer":"[{\"id\":\"a\",\"text\":\"Call absolute paths for all commands and set PATH explicitly at the top of the script\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Source the user's profile from cron before running the script\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Move binaries into a directory that is in cron's default PATH\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Run the script using at instead of cron\",\"isCorrect\":false}]","explanation":"## Q3 Correct Answer\n- A\n\n## Why Other Options Are Wrong\n- B: Cron typically does not source user profiles; relying on profiles is not dependable for cron jobs.\n- C: Moving binaries to fit cron's PATH is impractical and can introduce maintenance issues; absolute paths are more reliable.\n- D: Using at changes scheduling semantics and is not a drop-in replacement for cron for recurring backups.\n\n## Key Concepts\n- Cron PATH limitations\n- Absolute paths in scripts\n- Environment isolation for scheduled jobs\n\n## Real-World Application\n- Ensures scheduled backups run reliably in production environments by eliminating PATH-related failures.","diagram":null,"difficulty":"intermediate","tags":["Linux","cron","shell","AWS","Kubernetes","certification-mcq","domain-weight-20"],"channel":"linux-foundation-sysadmin","subChannel":"operation-running","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T08:30:59.912Z","createdAt":"2026-01-12 08:30:59"},{"id":"linux-foundation-sysadmin-service-config-1768225717070-0","question":"Which approach ensures a systemd service runs with environment variables loaded from a file and restarts on failure while avoiding credential leakage?","answer":"[{\"id\":\"a\",\"text\":\"Place environment variables directly in the systemd unit and set Restart=always, User=appuser.\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use EnvironmentFile=/etc/app.env with restricted permissions, and set Restart=on-failure, User=appuser, WorkingDirectory=/opt/app.\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Run the app as root and use Type=forking with a shell wrapper.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Store credentials in /etc/profile.d and rely on PAM to inject environment.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct option is B. Using EnvironmentFile loads environment variables from a dedicated file, keeping credentials separate from the unit file and allowing proper access controls. Restart=on-failure reduces unnecessary restarts on clean exits, and running as a non-privileged User improves security.\n\n## Why Other Options Are Wrong\n- A: While valid, embedding env vars in the unit is less maintainable and can expose secrets if the unit file is read by multiple admins; it also uses Restart=always which restarts even on clean exits.\n- C: Running as root and forking is insecure and unnecessary for most services; Type=simple is the default and safer when balanced with explicit startup behavior.\n- D: /etc/profile.d is not read by systemd services, so env vars would not be reliably loaded at service startup.\n\n## Key Concepts\n- Systemd EnvironmentFile mechanism\n- Service security and restart behavior\n\n## Real-World Application\n- Use EnvironmentFile for scalable, secure service configuration in production, with proper file permissions and a dedicated service user.","diagram":null,"difficulty":"intermediate","tags":["Linux","systemd","security","service-configuration","Kubernetes","AWS","Terraform","Logging","certification-mcq","domain-weight-20"],"channel":"linux-foundation-sysadmin","subChannel":"service-config","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:48:37.071Z","createdAt":"2026-01-12 13:48:37"},{"id":"linux-foundation-sysadmin-service-config-1768225717070-1","question":"To raise the per-service file descriptor limit for a high-traffic web server without editing the main unit file, which approach is best practice?","answer":"[{\"id\":\"a\",\"text\":\"Edit the nginx.service file to include LimitNOFILE=100000.\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Create a drop-in override at /etc/systemd/system/nginx.service.d/override.conf with [Service] LimitNOFILE=100000, then run systemctl daemon-reload and restart.\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Set a global limit in /etc/security/limits.conf for the nginx user.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use ulimit -n 100000 in the startup script and assume system-wide propagation.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct because a drop-in override preserves the main unit intact and applies the limit only to the specific service. It also ensures the change survives daemon reloads after systemd reloads its configuration.\n\n## Why Other Options Are Wrong\n- A: Editing the main unit is less maintainable and risks conflicts with package updates; overrides are preferred.\n- C: Global limits may behave unexpectedly for other services and require pam or login session handling, which is not targeted.\n- D: ulmit affects shells, not the systemd service manager, and may not apply reliably during service startup.\n\n## Key Concepts\n- Systemd drop-in overrides\n- LimitNOFILE and service-specific resource control\n\n## Real-World Application\n- Use drop-in overrides to fine-tune resource limits per service in production without altering packaged unit files.","diagram":null,"difficulty":"intermediate","tags":["Linux","systemd","Networking","Kubernetes","AWS","Terraform","Logging","NGINX","certification-mcq","domain-weight-20"],"channel":"linux-foundation-sysadmin","subChannel":"service-config","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:48:37.611Z","createdAt":"2026-01-12 13:48:38"},{"id":"linux-foundation-sysadmin-service-config-1768225717070-2","question":"You want to harden SSH so that password authentication is disabled but key-based logins are allowed, and root login is disabled. Which configuration snippet in /etc/ssh/sshd_config achieves this correctly?","answer":"[{\"id\":\"a\",\"text\":\"PasswordAuthentication no and PermitRootLogin prohibit-password, then restart sshd.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"PasswordAuthentication no, but PermitRootLogin yes, and rely on PAM to block passwords.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"PasswordAuthentication yes and PermitRootLogin prohibit-password to force non-root logins.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Disable SSH entirely and rely on physical access.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A is correct because it explicitly disables password-based authentication while allowing key-based logins via the SSH daemon and prohibits root SSH logins. Restarting sshd applies the changes.\n\n## Why Other Options Are Wrong\n- B: Root login is still allowed if PermitRootLogin is not properly set, and PAM alone cannot reliably enforce key-based authentication for all cases.\n- C: Enabling password authentication defeats the goal of password-less login, and prohibit-password for root does not cover non-root accounts.\n- D: Disabling SSH entirely is unacceptable for remote administration.\n\n## Key Concepts\n- SSH hardening best practices\n- SSHD configuration directives: PasswordAuthentication, PermitRootLogin\n\n## Real-World Application\n- Enabling key-based access while restricting root and password login is a standard security hardening step for servers exposed to untrusted networks.","diagram":null,"difficulty":"intermediate","tags":["Linux","SSH","Security","Access-Management","Kubernetes","AWS","Terraform","Logging","certification-mcq","domain-weight-20"],"channel":"linux-foundation-sysadmin","subChannel":"service-config","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:48:38.130Z","createdAt":"2026-01-12 13:48:38"},{"id":"linux-foundation-sysadmin-service-config-1768225717070-3","question":"A service writes high-volume logs to stdout and you want systemd to forward these logs to rsyslog for centralized logging, with log rotation handling. Which configuration sequence is correct?","answer":"[{\"id\":\"a\",\"text\":\"Enable ForwardToSyslog in /etc/systemd/journald.conf, configure rsyslog to write to /var/log/app/app.log, and set up logrotate for that file.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Direct the application to write to /var/log/app/app.log and configure rsyslog only, skipping journald.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Disable journald and write logs directly to /var/log/messages.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Only configure rsyslog without systemd journal forwarding or log rotation.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A is correct because enabling ForwardToSyslog allows systemd journals to forward entries to rsyslog, enabling centralized logging. Configuring rsyslog to write to a dedicated file under /var/log/app and applying logrotate ensures manageable log sizes and rotations.\n\n## Why Other Options Are Wrong\n- B: Directing logs to a file via the application bypasses systemd/journald forwarding and misses unified log handling.\n- C: Disabling journald reduces the reliability of centralized logging and can complicate troubleshooting.\n- D: Without journald forwarding, you may lose structured metadata systemd provides; combining journald forwarding with rsyslog is preferred.\n\n## Key Concepts\n- Journald forwarding to syslog\n- rsyslog configuration and logrotate\n\n## Real-World Application\n- Centralized, rotated logging is critical for observable systems in production and for compliance audits.","diagram":null,"difficulty":"intermediate","tags":["Linux","systemd","Logging","rsyslog","logrotate","Kubernetes","AWS","Terraform","certification-mcq","domain-weight-20"],"channel":"linux-foundation-sysadmin","subChannel":"service-config","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:48:38.318Z","createdAt":"2026-01-12 13:48:38"},{"id":"linux-foundation-sysadmin-service-config-1768225717070-4","question":"Your application service depends on a PostgreSQL database running on localhost:5432. You want startup to wait until the database is reachable before starting the app. Which combination is considered best practice?","answer":"[{\"id\":\"a\",\"text\":\"Add After=postgresql.service to the unit file.\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Add After=postgresql.service and ExecStartPre=/usr/local/bin/wait-for-db.sh localhost 5432 30.\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Rely on systemd’s network-online.target to guarantee DB readiness.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use a separate restart policy that restarts the app if DB is unavailable.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct because it combines a dependency on the database service with an explicit startup wait script that checks actual connectivity before launching the app, ensuring startup only proceeds when the DB is reachable.\n\n## Why Other Options Are Wrong\n- A: After=postgresql.service ensures ordering but does not guarantee the DB is ready to accept connections.\n- C: network-online.target only ensures network availability, not database readiness.\n- D: A restart policy does not guarantee initial startup sequencing or DB reachability.\n\n## Key Concepts\n- Systemd service dependencies\n- ExecStartPre health checks\n\n## Real-World Application\n- Implementing health checks at startup prevents race conditions between services in critical environments.","diagram":null,"difficulty":"intermediate","tags":["Linux","systemd","PostgreSQL","Kubernetes","AWS","Terraform","Database","certification-mcq","domain-weight-20"],"channel":"linux-foundation-sysadmin","subChannel":"service-config","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:48:38.498Z","createdAt":"2026-01-12 13:48:38"},{"id":"linux-foundation-sysadmin-storage-management-1768245789444-0","question":"You have a system with a single volume group (vg0) that hosts an ext4 root LV named root on /dev/vg0/root. You added a second disk and prepared a new PV on /dev/sdb1, then extended vg0 with that PV. Which sequence of commands correctly extends the LV and filesystem to utilize the new space?","answer":"[{\"id\":\"a\",\"text\":\"pvcreate /dev/sdb1; vgextend vg0 /dev/sdb1; lvextend -l +%FREE /dev/vg0/root; resize2fs /dev/vg0/root\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"pvcreate /dev/sdb2; vgextend vg0 /dev/sdb2; lvextend -l +%FREE /dev/vg0/root; resize2fs /dev/vg0/root\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"pvcreate /dev/sdb1; vgextend vg0 /dev/sdb1; lvextend -L 100G /dev/vg0/root\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"vgextend vg0 /dev/sdb1; lvextend -l +%FREE /dev/vg0/root; resize2fs\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA is correct because it follows the proper sequence: create the new physical volume, extend the volume group, extend the logical volume using all available space, and resize the filesystem accordingly.\n\n## Why Other Options Are Wrong\n- B uses /dev/sdb2 which may not exist or be the intended second disk, so it would fail.\n- C extends by a fixed amount (100G) rather than using all free space, which may not utilize the entire newly available space.\n- D omits the final resize operation, leaving the filesystem unchanged even though the LV was extended.\n\n## Key Concepts\n- LVM: pvcreate, vgextend, lvextend, resize2fs\n- Filesystem grow on the mounted volume (ext4 supports online resize)\n\n## Real-World Application\nThis pattern is common when expanding storage for root or data volumes in production servers while minimizing downtime.\n","diagram":null,"difficulty":"intermediate","tags":["Linux","Storage","LVM","AWS","Kubernetes","Terraform","certification-mcq","domain-weight-13"],"channel":"linux-foundation-sysadmin","subChannel":"storage-management","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T19:23:09.446Z","createdAt":"2026-01-12 19:23:09"},{"id":"linux-foundation-sysadmin-storage-management-1768245789444-1","question":"You have a degraded RAID1 array /dev/md0 consisting of /dev/sda1 and /dev/sdb1. A drive has failed and you plan to replace it with a new disk /dev/sdc1. Which sequence of commands correctly marks the failed member, removes it, adds the new disk, and initiates a resync?","answer":"[{\"id\":\"a\",\"text\":\"mdadm --manage /dev/md0 --fail /dev/sdb1; mdadm --manage /dev/md0 --remove /dev/sdb1; replace disk, partition on /dev/sdc1; mdadm --manage /dev/md0 --add /dev/sdc1; mdadm --detail /dev/md0\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"mdadm --stop /dev/md0; mdadm --assemble --force /dev/md0 /dev/sda1 /dev/sdc1\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"mdadm --zero-superblock /dev/sdb1; mdadm --purge /dev/sdb1; mdadm --add /dev/sdb1 /dev/md0\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"mdadm --grow /dev/md0 --raid-devices=1; mdadm --detail /dev/md0\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA correctly marks the failed member, removes it from the array, replaces it with the new disk, and adds the new disk to the array to start resync.\n\n## Why Other Options Are Wrong\n- B stops the array and attempts a rebuild in a way that is not appropriate for a degraded RAID1 and may not succeed.\n- C erases the disk metadata and uses an incorrect add operation; it does not follow the standard degraded rebuild workflow.\n- D uses --grow with 1 raid device, which is invalid for a degraded RAID1 and does not perform a replacement.\n\n## Key Concepts\n- mdadm degraded array management, --fail, --remove, --add, resync behavior\n\n## Real-World Application\nReplacing failed drives in RAID arrays is a frequent maintenance task in production storage pools to maintain redundancy with minimal downtime.\n","diagram":null,"difficulty":"intermediate","tags":["Linux","Storage","mdadm","AWS","Kubernetes","Terraform","certification-mcq","domain-weight-13"],"channel":"linux-foundation-sysadmin","subChannel":"storage-management","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T19:23:09.974Z","createdAt":"2026-01-12 19:23:10"},{"id":"linux-foundation-sysadmin-storage-management-1768245789444-2","question":"You want to take a consistent snapshot of an active Logical Volume for backup without downtime. Which command creates a proper read-only snapshot of /dev/vg0/root to /dev/vg0/root-snap?","answer":"[{\"id\":\"a\",\"text\":\"lvcreate -s -n root-snap -L 5G /dev/vg0/root\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"lvcreate -m -n root-snap -L 5G /dev/vg0/root\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"lvcreate -s -n root-snap -l 100%FREE /dev/vg0/root\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"lvcreate -s -n snap1 -L 5G /dev/vg0/backup\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA creates a proper LVM snapshot with the -s flag, naming the snapshot root-snap and sizing it appropriately for backup.\n\n## Why Other Options Are Wrong\n- B uses -m which is for mirroring, not LVM snapshots.\n- C uses -l 100%FREE, which is valid for some LVM commands but the proper syntax for a snapshot uses -L for size, and the target must be the LV being snapshotted, not using 100%FREE here.\n- D snapshots a different LV (/dev/vg0/backup) instead of the root LV, which defeats the purpose of the backup.\n\n## Key Concepts\n- LVM snapshot creation using lvcreate with -s\n- Snapshot sizing considerations for backups\n\n## Real-World Application\nSnapshots allow consistent backups of live filesystems without need for unmounting services, enabling offline or off-hours backups with minimal impact.\n","diagram":null,"difficulty":"intermediate","tags":["Linux","Storage","LVM","AWS","Kubernetes","Terraform","certification-mcq","domain-weight-13"],"channel":"linux-foundation-sysadmin","subChannel":"storage-management","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T19:23:10.505Z","createdAt":"2026-01-12 19:23:10"},{"id":"linux-foundation-sysadmin-storage-management-1768245789444-3","question":"An ext4 filesystem on /home is to be quota-enabled for both users and groups. Which sequence correctly enables quotas on the filesystem and activates quota accounting?","answer":"[{\"id\":\"a\",\"text\":\"Edit /etc/fstab to add usrquota, grpquota; remount /home; quotacheck -avugm; quotaon -v /home\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"quotaon -v /home; edquota -u user; edquota -g group\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"edquota -a; quotaon -v /home; quotaoff /home\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"xfs_quota -u on /home; xfs_quota -g on /home\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA correctly enables quotas by updating /etc/fstab (usrquota and grpquota), remounting, running quotacheck to create quota files, and turning quotas on with quotaon.\n\n## Why Other Options Are Wrong\n- B attempts to enable quotas without the filesystem being prepared and without quota files present; edquota applies to limits after quotas exist.\n- C uses edquota before quotas are enabled and then turns quotas off, which negates the change.\n- D applies to XFS quota tools; the scenario specifies ext4 quotas, and the commands differ.\n\n## Key Concepts\n- /etc/fstab quota options, quotacheck, quotaon\n- Distinction between ext4 quotas vs XFS quota tooling\n\n## Real-World Application\nEnabling quotas helps enforce storage limits per user/group, critical in multi-tenant environments to prevent single users from consuming all space.\n","diagram":null,"difficulty":"intermediate","tags":["Linux","Storage","Quotas","AWS","Kubernetes","Terraform","certification-mcq","domain-weight-13"],"channel":"linux-foundation-sysadmin","subChannel":"storage-management","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T19:23:10.681Z","createdAt":"2026-01-12 19:23:10"},{"id":"linux-foundation-sysadmin-storage-management-1768245789445-4","question":"You have a LUKS-encrypted data partition on /dev/sdd1 and want to automatically unlock it at boot using a keyfile stored on an unencrypted small partition. Which approach correctly configures this behavior for a system using systemd?","answer":"[{\"id\":\"a\",\"text\":\"cryptsetup luksOpen /dev/sdd1 data; echo 'data PARTUUID=XXXX-XXXX none' >> /etc/crypttab; update-initramfs -u\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"cryptsetup format /dev/sdd1; cryptsetup luksOpen /dev/sdd1 data\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"mkinitcpio -p linux; cryptsetup luksOpen /dev/sdd1 data\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"mount /dev/sdd1 /mnt; echo 'KEYFILE' > /mnt/keyfile\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA outlines the correct approach: open the encrypted device, configure /etc/crypttab to reference the keyfile, and update initramfs to include the unlock during early boot.\n\n## Why Other Options Are Wrong\n- B uses cryptsetup format, which erases the existing LUKS metadata and is destructive.\n- C uses a different initramfs tooling (mkinitcpio) and leverages a distribution-specific mechanism not universally applicable; the step to tie the keyfile into crypttab and initramfs is missing.\n- D mounts the device and writes a keyfile, which does not enable automatic unlock at boot.\n\n## Key Concepts\n- crypttab configuration, initramfs integration, and LUKS keyfile usage for auto-unlock\n\n## Real-World Application\nAutomated unlock at boot is essential for servers requiring encrypted storage with minimal manual intervention during startup, especially in headless deployments.\n","diagram":null,"difficulty":"intermediate","tags":["Linux","Storage","LUKS","AWS","Kubernetes","Terraform","certification-mcq","domain-weight-13"],"channel":"linux-foundation-sysadmin","subChannel":"storage-management","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T19:23:10.870Z","createdAt":"2026-01-12 19:23:10"},{"id":"linux-foundation-sysadmin-user-group-1768286458643-0","question":"To add user 'alice' to the supplementary group 'developers' without changing her primary group, which command should you run?","answer":"[{\"id\":\"a\",\"text\":\"usermod -g developers alice\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"gpasswd -a alice developers\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"useradd -G developers alice\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"setfacl -m g:developers:rw- /home/alice\",\"isCorrect\":false}]","explanation":"## Correct Answer\nB. gpasswd -a alice developers\n\nThis adds alice to the supplementary group without altering her primary group. The other options are incorrect for this scenario: A would change alice's primary group, C would attempt to add an existing user with a new group list (not appropriate for an existing user), and D manipulates ACLs on a file/directory rather than adding the user to a system group.\n\n## Why Other Options Are Wrong\n- Option A: Changes the primary group, not the supplementary groups, which is not what was requested.\n- Option C: Attempts to create a user or modify a user in a context that doesn’t apply to adding an existing user to a group.\n- Option D: ACL modification on a path does not modify the user's group membership.\n\n## Key Concepts\n- Primary vs supplementary groups\n- Correct use of group management commands (usermod, gpasswd)\n- How to grant access via group membership without altering primary group\n\n## Real-World Application\n- When onboarding a contractor or contractor-equivalent user, you often grant access via additional groups without altering existing primary roles, preserving existing permissions and audit trails.","diagram":null,"difficulty":"intermediate","tags":["linux","user-management","aws","kubernetes","terraform","security","certification-mcq","domain-weight-10"],"channel":"linux-foundation-sysadmin","subChannel":"user-group","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T06:40:58.644Z","createdAt":"2026-01-13 06:40:58"},{"id":"linux-foundation-sysadmin-user-group-1768286458643-1","question":"In a shared project directory /srv/project, you want new files to inherit the directory's group ownership automatically. Which action accomplishes this by default?","answer":"[{\"id\":\"a\",\"text\":\"chmod g+s /srv/project\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"chgrp projectgrp /srv/project\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"setfacl -m d:g:projectgrp:rwX /srv/project\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"chown root:root /srv/project\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA. chmod g+s /srv/project\n\nSetting the setgid bit on a directory causes files and subdirectories created within to inherit the directory's group, ensuring group ownership propagates as intended. The other options modify ownership or ACLs but do not provide the automatic group inheritance behavior for new files.\n\n## Why Other Options Are Wrong\n- Option B: Changes the directory's group but does not enforce inheritance for new files.\n- Option C: Applies a default ACL, which is not the same as the traditional setgid inheritance and may not apply uniformly.\n- Option D: Changes ownership but not the group inheritance behavior.\n\n## Key Concepts\n- setgid bit behavior on directories\n- Inheritance of group ownership for new files\n- Distinction between traditional permissions and ACLs\n\n## Real-World Application\n- Teams sharing a common project directory rely on setgid to ensure that new artifacts remain grouped with the project’s collaboration group, simplifying permission management.","diagram":null,"difficulty":"intermediate","tags":["linux","permissions","aws","kubernetes","terraform","security","certification-mcq","domain-weight-10"],"channel":"linux-foundation-sysadmin","subChannel":"user-group","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T06:40:59.006Z","createdAt":"2026-01-13 06:40:59"},{"id":"linux-foundation-sysadmin-user-group-1768286458643-2","question":"To require a password change at first login for a new user 'bob', which command would enforce that behavior?","answer":"[{\"id\":\"a\",\"text\":\"chage -d 0 bob\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"passwd -e bob\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"usermod -e 20260101 bob\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"useradd -m bob\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA. chage -d 0 bob\n\nSetting the password aging data to 0 forces bob to change the password on next login, which enforces a first-login password change. Other options either expire the password in a different way or are unrelated to first-login behavior.\n\n## Why Other Options Are Wrong\n- Option B: Forces a password change on next login but does not guarantee it occurs on the first login in all PAM configurations.\n- Option C: Sets an account expiration date, not specifically a first-login password change.\n- Option D: Creates a new user; does not address password change behavior.\n\n## Key Concepts\n- Password aging and first-login requirements\n- chage command usage for enforcing password changes\n- Distinction between account aging and password aging\n\n## Real-World Application\n- Enforcing first-login password changes on new hires is a common security control to ensure initial credential integrity.","diagram":null,"difficulty":"intermediate","tags":["linux","authentication","aws","kubernetes","terraform","security","certification-mcq","domain-weight-10"],"channel":"linux-foundation-sysadmin","subChannel":"user-group","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T06:40:59.366Z","createdAt":"2026-01-13 06:40:59"},{"id":"linux-foundation-sysadmin-user-group-1768286458643-3","question":"Which command sequence correctly creates a new group named 'devops' with GID 2001 and adds user 'alice' to it?","answer":"[{\"id\":\"a\",\"text\":\"groupadd -g 2001 devops; usermod -a -G devops alice\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"addgroup -g 2001 devops; useradd alice\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"groupadd devops -g 2001\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"usermod -G 2001 alice\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA. groupadd -g 2001 devops; usermod -a -G devops alice\n\nFirst create the group with the specified GID, then append the user alice to that group without removing other supplementary groups. Options B and C either use a non-portable tool or misstate the command syntax, and D attempts to modify GIDs directly without ensuring the group exists.\n\n## Why Other Options Are Wrong\n- Option B: Uses a non-portable tool and lacks proper user-group association in a portable way.\n- Option C: Group creation syntax is incorrect for setting a specific GID in many distros.\n- Option D: Modifying -G without ensuring the group exists or without -a can alter existing supplementary groups in unintended ways.\n\n## Key Concepts\n- Group creation with specific GID\n- Correct use of usermod to append to a group\n- Distinction between primary group creation and user membership\n\n## Real-World Application\n- When aligning onboarding or project-based access, you often predefine a group with a fixed GID and attach users to it to standardize permissions across servers.","diagram":null,"difficulty":"intermediate","tags":["linux","groups","aws","kubernetes","terraform","security","certification-mcq","domain-weight-10"],"channel":"linux-foundation-sysadmin","subChannel":"user-group","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T06:40:59.494Z","createdAt":"2026-01-13 06:40:59"},{"id":"linux-foundation-sysadmin-user-group-1768286458643-4","question":"After a contractor leaves, you need to ensure they cannot log in using any method while preserving their home directory for audit. Which action provides a definitive lock?","answer":"[{\"id\":\"a\",\"text\":\"passwd -l contractor\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"chage -E 0 contractor\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"userdel contractor\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"usermod -s /sbin/nologin contractor\",\"isCorrect\":true}]","explanation":"## Correct Answer\nD. usermod -s /sbin/nologin contractor\n\nSetting the login shell to /sbin/nologin definitively prevents any login attempt (password or SSH keys) while preserving the home directory for audit. Password lock (A) and account expiration (B) may still allow certain login methods in some configurations (e.g., key-based logins) and thus are not definitive in all environments. Deleting the user (C) would remove the account entirely, which is not preserving the home directory for audit.\n\n## Why Other Options Are Wrong\n- Option A: Locks only the password; if SSH keys or other methods exist, login could still be possible.\n- Option B: Expiring the account; some authentication methods may still be able to log in depending on PAM/SSH configuration.\n- Option C: Deletes the user; this contradicts the requirement to preserve the home directory for audit.\n\n## Key Concepts\n- Definitive login denial strategies\n- Distinction between password locks, account expiration, and shell-based lockdowns\n- Auditing considerations when employees depart\n\n## Real-World Application\n- In offboarding, ensuring no access while keeping data for audits is a common security control; using a nologin shell provides a robust, audit-friendly lock without data loss.","diagram":null,"difficulty":"intermediate","tags":["linux","security","aws","kubernetes","terraform","compliance","certification-mcq","domain-weight-10"],"channel":"linux-foundation-sysadmin","subChannel":"user-group","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T06:40:59.627Z","createdAt":"2026-01-13 06:40:59"}],"subChannels":["essential-commands","general","networking","operation-running","service-config","storage-management","user-group"],"companies":["Apple","Databricks","DoorDash","Goldman Sachs","Hugging Face","Meta","Microsoft","MongoDB","Plaid","Snowflake","Tesla","Twitter","Two Sigma"],"stats":{"total":33,"beginner":1,"intermediate":28,"advanced":4,"newThisWeek":33}}