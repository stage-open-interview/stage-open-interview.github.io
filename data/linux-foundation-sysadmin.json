{"questions":[{"id":"q-1044","question":"On a Linux host running multiple tenant services, peak I/O from a data ingestion daemon saturates the disk, causing latency spikes for others. Propose a production plan to isolate and throttle disk I/O across tenants using systemd slices and cgroup v2 io.max. Include concrete unit and slice snippets, per-device limits for a SATA HDD and NVMe, and a test plan using fio and iostat to verify tail latency improvements under concurrent workloads?","answer":"Implement per-tenant I/O isolation using systemd slices and cgroup v2 io.max. Create tenant-<name>.slice, assign services to it, and throttle devices (e.g., 8:0 rbps=50M wbps=50M). Include per-device ","explanation":"## Why This Is Asked\nTests ability to design resource isolation and practical configuration with systemd and cgroups in a multi-tenant environment, focusing on I/O pressure rather than CPU/memory alone.\n\n## Key Concepts\n- cgroup v2 io.max throttling\n- systemd slices and per-service isolation\n- per-device I/O limits and hierarchy\n- reproducible load testing with fio and iostat\n\n## Code Example\n```javascript\n# Example: set per-device I/O throttling for tenant-a.slice (pseudo)\nsudo mkdir -p /sys/fs/cgroup/tenant-a.slice\necho \"8:0 rbps=50M wbps=50M\" | sudo tee /sys/fs/cgroup/tenant-a.slice/io.max\n```\n\n## Follow-up Questions\n- How would you adapt if the workload alternates between read-heavy and write-heavy?\n- What are potential pitfalls with block layer throttling and caches?","diagram":null,"difficulty":"advanced","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Microsoft","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T20:30:42.832Z","createdAt":"2026-01-12T20:30:42.832Z"},{"id":"q-1056","question":"On a Linux host, a TLS proxy reads its certificate from /etc/ssl/certs/app.crt and currently reloads by restarting, causing brief downtime during renewal. Propose a zero-downtime rotation using a systemd.path trigger that fires on a new cert symlink, with a separate service unit for ExecReload and an atomic update scheme (swap in a new cert, then switch a symlink). Include concrete unit snippets and a test plan?","answer":"Propose a zero-downtime TLS cert rotation using systemd.path to trigger on an updated certificate symlink, plus a dedicated systemd service that ExecReloads the proxy. Use atomic cert updates (place n","explanation":"## Why This Is Asked\nTests practical mastery of systemd path activation, service reloads, and atomic file updates for TLS without downtime. It also validates understanding of race-free certificate rotation and testability.\n\n## Key Concepts\n- systemd.path and PathChanged/PathModified\n- ExecReload vs Restart semantics\n- Atomic file replacement via symlinks\n- Reproducible test plan for cert rotation\n\n## Code Example\n```ini\n# app-cert.path\n[Unit]\nDescription=Trigger TLS cert rotation on new cert\n\n[Path]\nPathChanged=/etc/ssl/certs/app.crt\nUnit=app-cert.service\n\n[Install]\nWantedBy=multi-user.target\n```\n```ini\n# app-cert.service\n[Unit]\nDescription=Reload TLS proxy with new certificate\n\n[Service]\nType=oneshot\nExecStart=/usr/local/bin/app-cert-rotate-reload.sh\n```\n```bash\n# app-cert-rotate-reload.sh\n#!/bin/sh\nset -e\n# Swap in the new cert and reload the proxy\nmv -f /etc/ssl/certs/app.crt.new /etc/ssl/certs/app.crt\nsystemctl reload app-proxy.service\n```\n```\n\n## Follow-up Questions\n- How to handle reload failure and ensure idempotence?\n- How to monitor for stale symlinks or missing new certs?\n- How would you extend this for multiple certificates or canary rollouts?","diagram":"flowchart TD\n  A[Cert renewal arrives] --> B[Write app.crt.new]\n  B --> C[symlink swap via script]\n  C --> D{Reload trigger}\n  D --> E[app-proxy reloads without downtime]","difficulty":"advanced","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T21:18:27.923Z","createdAt":"2026-01-12T21:18:27.923Z"},{"id":"q-1122","question":"On a Linux host running multiple ML model servers in separate systemd slices using cgroup v2, a spike in one tenant consumes memory and triggers host memory pressure despite per-tenant limits. Propose a production plan to enforce strict isolation and backpressure using memory.max and memory.high, enable group OOM behavior, and implement eviction/playback strategies. Include concrete unit and cgroup settings and a test plan with a reproducible spike and validation steps?","answer":"Implement per-tenant slices with MemoryMax and MemoryHigh in a cgroup v2 setup; attach each model server to its slice. Enable memory.oom_group for group-based OOM handling and configure a hard cap wit","explanation":"## Why This Is Asked\nTests the ability to design strict memory isolation in a multi-tenant Linux environment, covering cgroup v2 memory controls, systemd slices, and OOM behavior under load.\n\n## Key Concepts\n- cgroup v2 memory.max and memory.high for hard/soft limits\n- memory.oom_group for group-level OOM decisions\n- systemd slices and per-tenant unit configurations\n- observability: cgroup events, dmesg, and host health verification\n\n## Code Example\n```javascript\n# Example: tenant-A.slice\n[Slice]\nMemoryHigh=6G\nMemoryMax=8G\n\n# Example: tenant-A service\n[Service]\nSlice=tenant-A.slice\nExecStart=/usr/bin/modelA_server\nRestart=on-failure\n```\n\n## Follow-up Questions\n- How would you automate scale-out when new tenants are added?\n- What metrics and alerts would you surface to detect pressure early?","diagram":"flowchart TD\n  A[Tenant A] --> B[Cgroup v2 memory.max]\n  A --> C[Cgroup v2 memory.high]\n  B --> D[Host stability checks]\n  C --> D","difficulty":"advanced","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","MongoDB","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T23:29:53.881Z","createdAt":"2026-01-12T23:29:53.881Z"},{"id":"q-1230","question":"On a Linux host with a multi-tenant workload sharing a single 10GbE NIC configured with multiple receive queues, one tenant bursts UDP traffic and starves others, causing increased latency and packet loss. Propose a production plan to enforce tenant isolation and fairness using NIC multi-queue, RSS/XPS mapping, IRQ affinity, and per-tenant cgroups (v2) with io.max and tc shaping. Include concrete steps and a test plan with realistic traffic?","answer":"Plan to isolate tenants by binding NIC RX queues to per-tenant CPUs, use RSS/XPS to map flows, create per-tenant systemd slices with per-tenant CPU budgets, apply io.max quotas, and shape egress with ","explanation":"## Why This Is Asked\nAssesses practical network isolation skills in a multi-tenant Linux environment, focusing on NIC multi-queue tuning, IRQ affinity, and per-tenant resource containers to prevent bursts from one tenant affecting others.\n\n## Key Concepts\n- NIC multi-queue, RSS, XPS\n- IRQ affinity per-tenant isolation\n- cgroups v2 with io.max and per-tenant slices\n- tc shaping and fq_codel for fairness\n- Observability with iperf3/pktgen, iostat\n\n## Code Example\n```bash\n# Enable 4 RX queues\nethtool -L eth0 rx 4\n# Bind a few IRQs to CPUs (example)\necho 2-3,6-7 > /proc/irq/46/smp_affinity_list\n```\n\n## Follow-up Questions\n- How would you validate fairness under concurrent tenants?\n- What are NUMA pitfalls and how would you mitigate them?","diagram":"flowchart TD\n A[Tenants] --> B[RX queues]\n A --> C[CPU cores]\n B --> D[Mapping rules]\n C --> E[Isolation boundary]","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","MongoDB","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T05:40:43.739Z","createdAt":"2026-01-13T05:40:43.739Z"},{"id":"q-1327","question":"On a Linux host running multiple tenants via containers, a CPU-intensive workload from one tenant starves others. Propose a production plan to enforce CPU isolation using cgroup v2 slices, systemd integration, and per-tenant quotas; include concrete settings (cpu.max, cpu.weight), scheduling options (cpuset and isolcpus), and a test plan with realistic workloads and verification steps?","answer":"Create per-tenant cgroup v2 slices and assign each container to its slice. Use cpu.max and cpu.weight to cap and weight CPU usage (e.g., tenantA 250000 1000000, tenants B, C, D 150000 1000000; weights","explanation":"## Why This Is Asked\nAssesses practical mastery of modern Linux isolation (cgroup v2, systemd slices, cpuset) and real-world tradeoffs between throughput and fairness.\n\n## Key Concepts\n- cgroup v2 CPU controller: cpu.max, cpu.weight\n- systemd slices and container assignment\n- cpuset/isolation of CPUs; isolcpus boot option\n- realistic workloads: fio, sysbench; tail latency tracking\n\n## Code Example\n```bash\n# Illustrative: create and configure tenant slices (pseudo-commands)\nsystemd-run --unit=tenantA.slice --property=CPUQuota=250000 --property=CPUQuotaPeriodUS=1000000 --slice=tenantA.slice sleep 1h\nsystemd-run --unit=tenantB.slice --property=CPUQuota=150000 --property=CPUQuotaPeriodUS=1000000 --slice=tenantB.slice sleep 1h\n# Bind slices to CPUs via cpuset (illustrative)\nchmod 600 /sys/fs/cgroup/unified/tenantA.slice/cpuset.cpus\n# Attach containers to their slices as they spawn\n```\n\n## Follow-up Questions\n- How would you monitor CPU-steal and detect fairness violations in production?\n- What are risks of over-isolating CPUs, and how would you safely rebalance during load spikes?","diagram":null,"difficulty":"advanced","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Google","IBM"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T11:37:36.271Z","createdAt":"2026-01-13T11:37:36.271Z"},{"id":"q-1346","question":"On a production Linux host serving multiple ephemeral monitoring daemons, a heartbeat worker must emit a 5-second ping to a central collector. During peak load, the heartbeat misses several intervals, delaying alerts. Propose a concrete production plan to guarantee cadence and resilience using: (a) per-service CPU isolation and pinning, (b) high-priority timer/RT scheduling or systemd timers with precise tuning, (c) a fallback mechanism for missed heartbeats, and (d) a validation strategy with a controlled load. Include concrete config snippets for systemd, cpuset, and timer settings?","answer":"Plan pins heartbeat to a dedicated CPU set, uses a systemd timer with 5s cadence and AccuracySec=1s, and a high-priority heartbeat service with CPUQuota=50% and SCHED_FIFO-like behavior. Add a fallbac","explanation":"## Why This Is Asked\n\nTests knowledge of per-service isolation, timer accuracy, fault tolerance, and measurable validation.\n\n## Key Concepts\n\n- systemd slices timers CPU affinity\n- cpuset pinning and CPUQuota/AccuracySec\n- RT priority and fallback mechanisms\n- end-to-end verification under load\n\n## Code Example\n\n```\n# See /etc/systemd/system/heartbeat.* for concrete files\n```\n\n## Follow-up Questions\n\n- How would you detect missed heartbeats and trigger alerts?\n- What are the trade-offs of increasing CPU isolation vs. responsiveness?","diagram":"flowchart TD\n  A[Create heartbeat.slice] --> B[Pin heartbeat service]\n  B --> C[Configure 5s timer]\n  C --> D[Add fallback heartbeat]\n  D --> E[Test under load]","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","LinkedIn"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T13:05:27.792Z","createdAt":"2026-01-13T13:05:27.792Z"},{"id":"q-1385","question":"On a Linux host running ephemeral build jobs managed by systemd, a rogue job floods /tmp with tiny files, exhausting inodes and delaying builds. Propose a production plan to isolate and bound /tmp usage and auto-clean stale files. Include: (a) per-service PrivateTmp and tmpfs /tmp with a size cap; (b) systemd-tmpfiles cleanup rules; (c) inode usage monitoring; (d) a reproducible test that proves isolation and cleanup. How would you implement this?","answer":"Configure per-service isolation: set PrivateTmp=true and PrivateDevices=true for build services; mount /tmp as tmpfs with size=128M and mode=1777; add systemd-tmpfiles cleanup rules to prune stale fil","explanation":"## Why This Is Asked\nAssesses practical isolation of temporary storage for multi-tenant build workloads and how to enforce cleanups under bursty IO.\n\n## Key Concepts\n- systemd PrivateTmp and PrivateDevices\n- tmpfs sizing and mounting options\n- inode monitoring and alerting (df -i, thresholds)\n- systemd-tmpfiles cleanup rules and scheduling\n\n## Code Example\n```javascript\n// Example pseudo-configuration snippets for illustration\n// systemd unit: PrivateTmp=true\n// /etc/fstab: tmpfs /tmp tmpfs size=128M,mode=1777 0 0\n```\n\n## Follow-up Questions\n- How would you scale this across a cluster of CI workers?\n- What are potential edge cases with cleanup timing and race conditions?","diagram":null,"difficulty":"beginner","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Coinbase","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T14:47:50.803Z","createdAt":"2026-01-13T14:47:50.803Z"},{"id":"q-1490","question":"On a Linux server, a small web app runs as a systemd service and reads its port and log level from /etc/myapp/config.yaml. Changes to this file must apply without dropping connections. Design a production-safe hot-reload mechanism using systemd ExecReload, a SIGHUP-based reload, and a small validation script. Include unit file snippets and a test plan with a reproducible config change and verification steps?","answer":"Use systemd ExecReload to send SIGHUP to the daemon and a small validator script. Create /usr/local/bin/myapp_reload to validate /etc/myapp/config.yaml (e.g., ensure .port and .log_level exist) and th","explanation":"## Why This Is Asked\nTests hot-reload capability and safe config changes without dropping connections.\n\n## Key Concepts\n- systemd ExecReload\n- SIGHUP reload semantics\n- config validation (YAML)\n- graceful reload vs restart\n- test plan design\n\n## Code Example\n```javascript\n// Implementation sketch\n```\n\n## Follow-up Questions\n- How would you handle a daemon that does not support SIGHUP?\n- How would you verify that in-flight requests survive a reload?","diagram":"flowchart TD\n  A[config.yaml change] --> B[validate YAML]\n  B --> C{valid?}\n  C -->|yes| D[send SIGHUP]\n  D --> E[service reload]\n  E --> F[verify endpoints]\n  C -->|no| G[abort reload]","difficulty":"beginner","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T19:03:23.816Z","createdAt":"2026-01-13T19:03:23.816Z"},{"id":"q-1497","question":"On a Linux host running multiple tenants in systemd services sharing a single NVMe SSD and a 25Gbps NIC, a new requirement enforces strict I/O isolation to prevent a noisy tenant from stalling others. Propose a production plan to enforce per-tenant I/O isolation and backpressure using: (a) cgroup v2 io.max per service; (b) per-tenant filesystem isolation via PrivateMounts and PrivateDevices with a dedicated data path; (c) I/O scheduler tuning (none/deadline) and per-tenant block IO queuing discipline with tc; (d) a deterministic test plan with a reproducible burst scenario and rollback steps. Include concrete config snippets for systemd units and cgroups?","answer":"Plan: use per-tenant cgroup v2 io.max to cap disk writes, create tenant.slice with PrivateMounts for isolation, assign each service to a sub-cgroup, attach per-tenant tc qdisc to shape writes, and iso","explanation":"## Why This Is Asked\nTests hands-on mastery of Linux IO isolation primitives under multi-tenant workloads and production risk management.\n\n## Key Concepts\n- cgroup v2 io.max\n- systemd Slice and isolation\n- tc/bpf/qdisc for per-tenant IO shaping\n- PrivateMounts/PrivateDevices for filesystem isolation\n- Reproducible test methodology with fio\n\n## Code Example\n```javascript\n# systemd unit\n[Unit]\nDescription=Tenant A worker\nAfter=network-online.target\n\n[Service]\nSlice=tenant-A.slice\nPrivateMounts=yes\nPrivateDevices=yes\nExecStart=/usr/local/bin/tenant-a-worker\n```\n\n```javascript\n# cgroup v2 IO limit (pseudo)\nmkdir -p /sys/fs/cgroup/tenant-A\necho \"8:0 100M\" > /sys/fs/cgroup/tenant-A/io.max\n```\n\n## Follow-up Questions\n- How would you roll back if another tenant regressed?\n- What metrics and alerting would you install to detect drift?","diagram":null,"difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T19:32:24.815Z","createdAt":"2026-01-13T19:32:24.816Z"},{"id":"q-1588","question":"On a dual-socket Linux host running PostgreSQL and a data-processor ETL job, both services contend for memory and cache across NUMA nodes; design an advanced plan to enforce NUMA-aware isolation with cpuset/memcg, disable NUMA balancing, set per-node IRQ affinity, and validate via targeted benchmarks. Include concrete commands and a test plan?","answer":"Pin two services to separate NUMA nodes using cpuset and memory binding; PostgreSQL on node0, ETL on node1. Disable NUMA balancing, set per-node IRQ affinity, and ensure memory policies via memcg. Validate isolation through targeted benchmarks measuring memory locality, cache coherence, and latency under mixed workload conditions.","explanation":"## Why This Is Asked\nNUMA awareness is critical for predictable latency in multi-socket systems; this question tests practical implementation of resource isolation, memory binding, and performance verification techniques.\n\n## Key Concepts\n- NUMA topology and memory affinity\n- cpuset and memory controller (memcg) binding\n- NUMA balancing disable and IRQ affinity configuration\n- Performance validation under mixed workload scenarios\n\n## Code Example\n```bash\n# Detect NUMA topology and create cpusets\nlscpu | grep NUMA\nsudo mkdir -p /sys/fs/cgroup/cpuset/postgres\nsudo bash -c 'echo 0-15 > /sys/fs/cgroup/cpuset/postgres/cpuset.cpus'\nsudo bash -c 'echo 0 > /sys/fs/cgroup/cpuset/postgres/cpuset.mems'\n# Similar setup for ETL on node1\n```\n\n## Follow-up Considerations\nMonitor NUMA statistics via `numastat` and validate memory locality using `perf` and custom benchmarks measuring cross-NUMA traffic impact.","diagram":null,"difficulty":"advanced","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T04:28:01.530Z","createdAt":"2026-01-13T22:54:02.606Z"},{"id":"q-1665","question":"On a Linux server hosting a latency-sensitive database and a heavy batch analytics job that share a single 2-socket NUMA node and HDD I/O, propose a production plan to guarantee tail latency (P95/P99) for the database while allowing analytics to finish within a bounded window. Include NUMA memory binding, CPU pinning via cpuset/systemd slices, I/O throttling via cgroup v2 io.max, IRQ affinity, and a validation test plan with realistic workloads?","answer":"Pin the latency-sensitive database to CPUs 0-3 on NUMA node 0 with a dedicated systemd slice and cpuset; bind memory local to node 0 and set memory.low to favor the DB. Run analytics on CPUs 4-7 with ","explanation":"## Why This Is Asked\nTests ability to reason about cross-domain isolation (CPU, memory, I/O) and NUMA-awareness to guarantee tail latency under mixed workloads.\n\n## Key Concepts\n- NUMA locality and memory policies\n- cpuset and systemd Slice-based isolation\n- cgroup v2 io.max and memory.high for throttling\n- IRQ affinity and CPU affinity for stable paths\n- Validation with realistic, reproducible workloads\n\n## Code Example\n```javascript\n// systemd cpuset and io.max example (illustrative)\n[Unit]\nDescription=Latency DB\n\n[Service]\nSlice=db.slice\nCPUS=0-3\nMemoryHigh=0\n\n```\n\n## Follow-up Questions\n- How would you adjust if analytics occasionally spikes beyond expected bounds?\n- How would you monitor to detect regressive behavior after updates?","diagram":null,"difficulty":"advanced","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T05:49:28.663Z","createdAt":"2026-01-14T05:49:28.663Z"},{"id":"q-1700","question":"On a Linux server running several io_uring-based data ingestors, one worker's backlog bursts, triggering tail latency spikes for others. Propose a production plan to guarantee per-worker latency and fairness using: (a) per-worker io_uring ring sizing and SQ depth; (b) per-worker cgroups v2 with io.max and task limits; (c) backpressure and pacing (budget tokens, per-worker deadlines); (d) monitoring and a reproducible test plan to validate latency under burst?","answer":"Configure per-worker io_uring rings with fixed SQ depth (e.g., 256), pin workers to dedicated CPUs via cpuset, set per-cgroup io.max (e.g., 8MB/s), implement pacing in ingestion to cap outstanding I/O","explanation":"## Why This Is Asked\nTests ability to design per-worker I/O isolation for io_uring workloads, a realistic pattern as data ingestion scales, and how to enforce fairness without sacrificing throughput.\n\n## Key Concepts\n- io_uring ring isolation and SQ budgeting\n- cgroup v2 io.max and per-task constraints\n- cpuset or task-pinning for CPU affinity\n- backpressure/pacing mechanisms in async I/O\n- instrumentation: latency percentiles, eBPF-based tracing\n\n## Code Example\n```javascript\n# Example cgroup setup (bash-like)\nsudo mkdir -p /sys/fs/cgroup/myio/worker1\necho 10485760 > /sys/fs/cgroup/myio/worker1/io.max\n# add pid\necho <pid> > /sys/fs/cgroup/myio/worker1/cgroup.procs\n```\n\n## Follow-up Questions\n- How would you extend to dynamic reallocation of rings if a worker consistently underperforms?\n- What are the risks of strict per-worker quotas on bursty workloads and how would you mitigate them?","diagram":"flowchart TD\n  A[Worker] --> B[io_uring ring]\n  B --> C[Submit IOs]\n  C --> D[Latency Monitor]\n  D --> E{Budget OK?}\n  E -->|Yes| F[Continue]\n  E -->|No| G[Apply Backpressure]\n  G --> H[Ballast Queue]\n  H --> F","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T07:36:58.713Z","createdAt":"2026-01-14T07:36:58.713Z"},{"id":"q-1774","question":"On a Linux host, a file-watcher daemon uses inotify to monitor a large directory tree. As load grows, events start being missed and the daemon lags. Propose a production plan to keep event delivery reliable: tune inotify limits, raise file descriptor limits for the service, add a polling fallback or batching, and validate with a reproducible test that simulates churn?","answer":"Increase inotify capacity and harden limits; implement a polling fallback for missed events; add a watchdog health check to detect lag and restart gracefully. Steps: 1) set fs.inotify.max_user_watches","explanation":"## Why This Is Asked\nTests understanding of Linux kernel parameters, systemd service limits, and practical reliability strategies for event-driven workloads.\n\n## Key Concepts\n- Inotify limits (fs.inotify.*) and kernel tuning\n- Per-service file descriptor limits (LimitNOFILE, ulimit)\n- Fault-tolerant design (polling fallback, batching)\n- Observability and testing with churn scenarios\n\n## Code Example\n```ini\n# Watch daemon systemd unit override\n[Service]\nLimitNOFILE=512000\n```\n\n```bash\n# Increase inotify capacity (runtime)\nsysctl -w fs.inotify.max_user_watches=300000\n```\n\n```bash\n# Simple churn test (pseudo)\nfor i in {1..100000}; do touch /watch/dir/file_$i; done\n```\n\n## Follow-up Questions\n- How would you measure inotify queue length and event lag in production?\n- How would you ensure compatibility across kernel versions and containers?","diagram":"flowchart TD\n  A[Inotify Watch] --> B[Event]\n  B --> C[Worker]\n  C --> D[Lag Monitor]\n  D --> E{Lag > threshold}\n  E -->|Yes| F[Switch to Polling]\n  E -->|No| G[Normal]\n","difficulty":"beginner","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","Oracle","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T10:36:39.086Z","createdAt":"2026-01-14T10:36:39.086Z"},{"id":"q-1807","question":"On a Linux host running a hot-reload deployment workflow for a critical service, mid-deploy tampering could swap the binary between copy and start, breaking integrity. Design a production-grade binary integrity strategy using Linux IMA with runtime attestation. Include how to sign binaries in CI, a signed binary repository, systemd integration (ExecStartPre), policy placement, rollback tests, and how you'd validate against tampering during deployment?","answer":"Use Linux IMA to enforce signed binaries for hot-reload. Sign binaries in CI, publish to a signed repo, enable IMA-based attestation, and require the service to be measured before load. Integrate with","explanation":"## Why This Is Asked\nSecurity-critical deployments require verifiable runtime integrity. This tests knowledge of kernel integrity (IMA), secure deployment workflows, and integration with systemd and CI.\n\n## Key Concepts\n- Linux IMA and runtime attestation\n- Signed binaries in CI and secure repositories\n- systemd integration (ExecStartPre) and rollback mechanisms\n- Tamper-detection tests and observability\n\n## Code Example\n```bash\n# validate-binaries.sh (conceptual)\nset -euo pipefail\nEXPECTED_HASH=$(cat /etc/ci/signatures/service.sha256)\nACTUAL_HASH=$(sha256sum /usr/local/bin/myservice | awk '{print $1}')\nif [ \"$ACTUAL_HASH\" != \"$EXPECTED_HASH\" ]; then\n  echo \"Signature mismatch\" >&2\n  exit 1\nfi\n```\n\n```ini\n# /etc/systemd/system/myservice.service (snippet)\n[Unit]\nDescription=My Critical Service\n\n[Service]\nExecStart=/usr/local/bin/myservice\nExecStartPre=/usr/local/bin/validate-binaries.sh\n```\n\n## Follow-up Questions\n- How would you handle CI/CD with multiple binary variants across environments?\n- How would you monitor and alert on attestation failures in production?","diagram":"flowchart TD\n  A[Signed Binary Repo] --> B[IMA Attestation]\n  B --> C[systemd ExecStartPre Verifies]\n  C --> D[Service Starts]\n  E[Tamper Attempt] --> F[Attestation Failure & Alert]","difficulty":"advanced","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T11:42:10.365Z","createdAt":"2026-01-14T11:42:10.365Z"},{"id":"q-1939","question":"On a Linux host running a web stack (Nginx proxy to a Node/Go app), the app writes to /var/log/webapp/app.log and /var/log/webapp/access.log. Bursts fill the disk and logrotation sometimes fails because the app keeps logs open. Propose a production-grade plan to ensure reliable log rotation with no data loss and no disk-full events. Include concrete logrotate config (copytruncate vs postrotate), systemd unit tweaks (Restart, ExecReload), filesystem layout advice, and a test plan to reproduce a burst and verify rotation completes without downtime?","answer":"Configure logrotate with copytruncate (or a postrotate that signals the process to reopen logs) and rotate daily or at 100M, keeping 7 archives. Move logs to a dedicated partition; enable Restart=on-f","explanation":"## Why This Is Asked\nTests practical log management skills: integrating logrotate with systemd, handling open-file rotation issues, and ensuring disk-safety with verifiable tests.\n\n## Key Concepts\n- logrotate behavior: copytruncate vs postrotate\n- systemd unit settings: Restart, ExecReload\n- filesystem layout and quotas for logs\n- reproducible test plan to simulate burst and confirm rotation\n\n## Code Example\n```javascript\n// logrotate config snippet (illustrative)\n// Real config lives in /etc/logrotate.d/webapp\n```\n\n## Follow-up Questions\n- What are the tradeoffs of copytruncate vs postrotate in data integrity?\n- How would you monitor disk usage and set automated alerts for log growth?","diagram":null,"difficulty":"beginner","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Stripe","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T17:51:38.423Z","createdAt":"2026-01-14T17:51:38.423Z"},{"id":"q-1954","question":"On a Linux host running three CI runners as systemd services sharing a single NVMe and a 10 Gb NIC, a long-running build on one runner starves CPU and I/O, delaying others. Propose a production plan to guarantee fair CPU and I/O while preserving peak throughput: (a) per-service CPU limits via cgroup v2 and CPU affinity; (b) per-service I/O throttling with io.max and a suitable I/O scheduler (BFQ/mq-deadline); (c) CPU pinning and IRQ isolation; (d) validation with bursts and latency metrics?","answer":"Plan using cgroup v2: create ci.slice with per-service sub-slices (ci-runner-A/B/C). Assign cpu.max and io.max per service; pin CPUs via cpuset and set IRQ affinity. Use BFQ for per-service I/O schedu","explanation":"## Why This Is Asked\nTests practical mastery of isolating CPU and I/O in a multi-tenant CI environment under hardware contention, a common production challenge.\n\n## Key Concepts\n- cgroup v2: cpu and io controllers for per-service quotas.\n- systemd slices: grouping services under a parent for unified controls.\n- CPU affinity and cpuset: bind runners to dedicated CPUs.\n- I/O schedulers: BFQ or mq-deadline for per-service throttling.\n- Observability: latency, throughput, tail latency, and backlog.\n\n## Code Example\n```javascript\n# Systemd override example (pseudo)\n[Slice]\nCPUAccounting=true\n\n[Service]\nSlice=ci.slice\nCPUQuota=33%\n```\n```javascript\n# cgroup v2 quota (pseudo)\necho 0.33 > /sys/fs/cgroup/ci.slice/ci-runner-A/cpu.max\n```\n\n## Follow-up Questions\n- How would you monitor and adapt quotas during a bursty workload?\n- What rollback strategy would you use if a quota regresses build reliability?\n","diagram":"flowchart TD\n  CI[ci.slice] --> A[ci-runner-A]\n  CI --> B[ci-runner-B]\n  CI --> C[ci-runner-C]\n  A --> CPU[CPU max]\n  A --> IO[io max]\n  B --> CPU2[CPU max]\n  B --> IO2[io max]\n  C --> CPU3[CPU max]\n  C --> IO3[io max]","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Apple","Databricks"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T18:51:30.714Z","createdAt":"2026-01-14T18:51:30.714Z"},{"id":"q-1987","question":"On a Linux host running CI jobs in rootless containers (e.g., Podman) shared across teams, a misconfigured container could attempt host escape. Propose a production hardening plan using user namespaces, rootless mode, Seccomp and AppArmor, and a tight per-job capset + cgroupv2 quotas. Include concrete commands and a rollback plan?","answer":"Run each CI worker in rootless containers with user namespaces, drop all capabilities, and enforce seccomp + AppArmor. Bind a tight cap set and per-job quotas via cgroupv2 memory.max, cpu.max, and pid","explanation":"## Why This Is Asked\nRealistic containment for containerized CI workloads is critical; this tests practical hardening, not theory.\n\n## Key Concepts\n- Rootless containers and user namespaces\n- Seccomp and AppArmor profiles\n- Capability bounding (cap_set)\n- cgroupv2 quotas (memory.max, cpu.max, pids.max)\n- Monitoring and rollback\n\n## Code Example\n```bash\npodman run --rm --name ci-job \\\n  --security-opt seccomp=seccomp.json \\\n  --security-opt apparmor=ci-job \\\n  --cap-drop=ALL --memory=512m --cpu-quota=50000 --cpu-period=100000 \\\n  --uidmap 0:1:1 --uidmap 1:2:1000 \\\n  localhost/ci-image\n```\n\n## Follow-up Questions\n- How would you audit for privilege escalations at runtime?\n- How would you handle failed containment and rollback without losing work?","diagram":"flowchart TD\nCIJob[CI Job Container] -->|uses| UserNS[User Namespaces]\nCIJob -->|enforced by| Seccomp[Seccomp Profile]\nCIJob -->|enforced by| AppArmor[AppArmor]\nCIJob -->|cgroups| CG[CG v2 quotas]\nCG --> Host[Host Kernel]","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","Square","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T19:42:42.923Z","createdAt":"2026-01-14T19:42:42.923Z"},{"id":"q-2014","question":"On a Linux host running multiple GPU-accelerated services in separate systemd slices under containerized workloads, occasional CPU contention causes tail latency spikes and timeouts for one service. Design a production plan to bound latency and preserve throughput without service interruption. Include per-slice isolation (isolcpus, cpuset), systemd slice configuration, cpu.max, and a kernel I/O scheduler tuning (mq-deadline), plus a reproducible test plan with mixed workloads and latency verification steps?","answer":"Bind each GPU service to a dedicated CPU set and a separate systemd slice; pin IRQs and NIC interrupts; enable isolcpus on cores; cap CPU usage with cpu.max; tune I/O scheduler to mq-deadline with per","explanation":"## Why This Is Asked\n\nThis question probes practical orchestration of CPU, memory, and IO isolation under contention with production-grade controls and observability.\n\n## Key Concepts\n\n- systemd slices and cpuset for resource isolation\n- cgroup v2 cpu.max, cpu.weight, and per-slice quotas\n- isolcpus and IRQ affinity to prevent interrupts from disturbing critical slices\n- I/O scheduler tuning (mq-deadline vs BFQ) and per-tenant quotas\n- reproducible load generation and latency verification with fio/stress-ng and ftrace/perf\n\n## Code Example\n\n```bash\n#!/bin/bash\n# Repro: spawn workloads pinned to CPU pools 0-3 and 4-7\nset -e\n# Start GPU service in background\ntaskset -c 0-3 /usr/local/bin/gpu-service &\n# Generate CPU contention on non-critical cores\nstress-ng --cpu 4 --timeout 60s &\nwait\n```\n\n## Follow-up Questions\n\n- How would you validate that latency stays within SLA under a quarterly load spike?\n- What observability would you add to detect when a slice is starved?","diagram":"flowchart TD\n  A[Identify contention] --> B[Isolate with slices & cpuset]\n  B --> C[Pin IRQs & cap resources with cpu.max]\n  C --> D[Tune I/O scheduler (mq-deadline)]\n  D --> E[Test with mixed workloads & measure tail latency]","difficulty":"advanced","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Instacart","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T20:51:49.017Z","createdAt":"2026-01-14T20:51:49.018Z"},{"id":"q-2167","question":"On a KVM host running dozens of VMs with memory ballooning enabled, a burst in guest memory growth triggers host memory pressure and occasional OOM events. Propose a production-safe plan to cap balloon aggressiveness, enforce host backpressure, and prevent thrash, using libvirt XML (memory.size/currentMemory, memoryBacking, balloon device) and cgroup v2 memory.max/memory.high, plus a test plan with a reproducible spike and verification steps?","answer":"Per-VM caps and headroom via cgroup v2: memory.max and memory.high per VM plus a low vm.swappiness to delay swap pressure. Keep ballooning enabled but hard-cap host memory pressure so reclamation happ","explanation":"## Why This Is Asked\nProbe for practical knowledge of KVM memory management, cgroup v2, ballooning trade-offs, and test scaffolding under real production-like conditions.\n\n## Key Concepts\n- KVM memory ballooning and how guest memory maps to host resources\n- cgroup v2 limits (memory.max, memory.high) for backpressure\n- libvirt XML configuration for per-VM memory caps\n- Observability and test plan for reproducible spikes\n\n## Code Example\n```xml\n<domain type='kvm'>\n  <name>vm1</name>\n  <memory unit='KiB'>4294967296</memory>\n  <currentMemory unit='KiB'>4294967296</currentMemory>\n  <memoryBacking>\n    <hugepages/>\n  </memoryBacking>\n  <devices>\n    <memballoon model='virtio'/>\n  </devices>\n</domain>\n```\n\n## Follow-up Questions\n- How would you monitor and alert on host memory pressure events?\n- What are the risks of disabling ballooning on some VMs?","diagram":null,"difficulty":"advanced","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Instacart","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T05:43:50.597Z","createdAt":"2026-01-15T05:43:50.598Z"},{"id":"q-2223","question":"On a Linux host running multiple CI jobs in containers, CPU contention causes builds to time out. Propose a production-ready plan to bound CPU for CI workloads using cgroup v2 and systemd slices, including concrete unit settings and a test plan with a CPU-bound load?","answer":"Bound CPU for CI work using cgroup v2 and systemd. Create a dedicated CI.slice and move CI containers/services into it. Set cpu.max to 50% (period 100000) and restrict to a subset of CPUs with Allowed","explanation":"## Why This Is Asked\n\nTests practical CPU isolation using cgroup v2 and systemd, not theory. It also checks how to validate under load, monitor, and adjust configurations for fairness.\n\n## Key Concepts\n\n- cgroup v2 CPU controller (cpu.max)\n- systemd Slice and resource controls (Slice=, CPUQuota, AllowedCPUs)\n- container isolation in CI workloads (Docker/K8s)\n- Observability (systemd-cgtop, pidstat, stress-ng)\n\n## Code Example\n\n```bash\n# Example setup (illustrative)\n# Create and configure a dedicated CI.slice with CPU limit\nsudo mkdir -p /sys/fs/cgroup/CI.slice\necho \"50000000 100000000\" | sudo tee /sys/fs/cgroup/CI.slice/cpu.max\n# Or via systemd properties for the slice\nsudo systemctl set-property CI.slice CPUQuota=50% CPUQuotaPeriodSec=1\n```\n\n## Follow-up Questions\n\n- How would you adapt for bursty CI workloads?\n- How do you monitor and auto-tune CPU limits as workloads evolve?","diagram":"flowchart TD\n  A[Start] --> B[Create CI.slice]\n  B --> C[Bind CI jobs to CI.slice]\n  C --> D[Configure cpu.max and AllowedCPUs]\n  D --> E[Run CPU-bound test to validate isolation]\n  E --> F[Monitor via systemd-cgtop and metrics]","difficulty":"beginner","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Snowflake","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T07:50:41.156Z","createdAt":"2026-01-15T07:50:41.156Z"},{"id":"q-2294","question":"On a Linux host running multi-tenant, rootless containers for isolation, a rogue tenant's DNS resolver leaks queries and inflates latency for others. Propose a production plan to enforce strict per-tenant DNS isolation without compromising performance. Include concrete steps for: (a) per-tenant network namespaces and DNS forwarders, (b) nftables rules to confine DNS to tenant resolvers, (c) container runtime config to prevent cross-tenant DNS leakage, and (d) a reproducible test plan to validate isolation under bursty DNS load?","answer":"Deploy per-tenant DNS forwarders (e.g., Unbound) inside isolated net namespaces and route each tenantâ€™s containers to its local resolver at 127.0.0.1:5353. Enforce nftables rules to allow DNS queries ","explanation":"## Why This Is Asked\nReal-world readiness to prevent cross-tenant DNS interference in multi-tenant Linux environments, especially with rootless containers. The answer should show practical separation and testability.\n\n## Key Concepts\n- Per-tenant network namespaces and local DNS forwarders\n- nftables-based DNS isolation rules and drop-all-untagged traffic\n- Container runtimes (Podman/CRIO) configuring per-tenant resolv.conf\n- Reproducible test plan with burst DNS load and latency verification\n\n## Code Example\n```bash\n# Example nftables: restrict DNS to tenant resolver\ntable inet filter {\n  chain forward {\n    tcp dport 53 ct state new accept\n    udp dport 53 ct state new accept\n    counter\n  }\n}\n```\n```ini\n# Per-tenant Unbound config snippet (tenant1-unbound.conf)\nserver:\n  directory: /var/cache/unbound\n  interface: 127.0.0.1\n  port: 5353\n  do-not-query-localhost: no\n  cache-min-ttl: 60\n```\n\n## Follow-up Questions\n- How would you monitor per-tenant DNS latency and cache efficiency?\n- How would you handle DNSSEC validation in this setup?","diagram":"flowchart TD\n  TenantA_NS(Tenant A Namespace) --> ResolverA[Resolver A (127.0.0.1:5353)]\n  TenantB_NS(Tenant B Namespace) --> ResolverB[Resolver B (127.0.0.1:5353)]\n  TenantA_Containers[Containers A] --> ResolverA\n  TenantB_Containers[Containers B] --> ResolverB\n  HostFirewall[Host nftables] --> ResolverA\n  HostFirewall --> ResolverB","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Citadel","Cloudflare"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T10:55:39.543Z","createdAt":"2026-01-15T10:55:39.543Z"},{"id":"q-2515","question":"On a Linux host running a multi-tenant container workload sharing a single 25 GbE NIC, one tenant suddenly bursts writes to a shared storage and causes CPU and FD contention, threatening SLA for others. Propose a production plan to enforce strict per-tenant isolation using cgroup v2 (cpu, memory, pids, io.max) and systemd.slice, plus network QoS with tc.clsact and per-tenant TX queues. Include concrete unit snippets and a test plan with reproducible surge and SLA verification?","answer":"Per-tenant quotas via cgroup v2: cpu.max, memory.max, memory.high, pids.max, io.max; place each tenant in its own systemd.slice. Use tc clsact with per-tenant egress queues and BPF-based enforcement. ","explanation":"## Why This Is Asked\n\nTests ability to design practical, scalable OS-level isolation for multi-tenant workloads, covering CPU/memory/pids/io and network QoS, plus concrete validation steps.\n\n## Key Concepts\n\n- cgroup v2 resource controllers and unified hierarchy\n- systemd.slice for tenant-level isolation\n- tc clsact, qdisc shaping, and BPF enforcement for per-tenant QoS\n- Validation: reproducible bursts, SLA metrics, rollback plan\n\n## Code Example\n\n```ini\n; tenantA.slice (example fragment)\n[Slice]\nCPUQuota=50%\nMemoryMax=4G\nMemoryHigh=3.5G\nIPCS=1\nPidsMax=1000\nIOWeight=500\n```\n\n```bash\n# minimal test harness (conceptual)\nip netns add tenantA\nip link add ifA type dummy\n# Bind tenantA to its own slice and netns, run burst test, verify SLA\n```\n\n## Follow-up Questions\n\n- How would you roll back quotas if a tenant misbehaves?\n- How would you monitor per-tenant metrics and alert on SLA drift?","diagram":"flowchart TD\n  A[Tenant] --> B[Cgroup v2] \n  B --> C[Resource Controls] \n  A --> D[Network QoS] \n  D --> E[SLA Monitoring]","difficulty":"advanced","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Discord","Instacart"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T21:00:55.131Z","createdAt":"2026-01-15T21:00:55.131Z"},{"id":"q-2538","question":"On a Linux host running a multi-tenant data ingestion pipeline and a separate real-time alerting service, ingestion bursts cause alert latency. Propose a production plan to guarantee low-latency alerts while preserving ingestion throughput using: (a) per-service CPU isolation with systemd slices and cpuset, (b) RT scheduling for the alerting process, (c) CPU affinity and NUMA bindings, (d) a validation plan with burst traffic and SLA checks?","answer":"Partition CPU resources using CGroups v2: allocate cores 0-3 for data ingestion and cores 4-5 for alerting; configure alerting within a dedicated systemd.slice with CPUAffinity=4-5 and CPUQuota=60%; execute alerting with real-time scheduling (chrt -f 90) on the isolated cores to ensure predictable latency during ingestion bursts.","explanation":"## Why This Is Asked\nTests practical CPU isolation and real-time scheduling implementation in multi-tenant environments.\n\n## Key Concepts\n- CGroups v2 CPU isolation and per-service systemd slices\n- Real-Time scheduling (SCHED_FIFO) for latency-critical workloads\n- CPU affinity and NUMA binding for deterministic performance\n- Validation methodology using burst traffic patterns and SLA compliance\n\n## Code Example\n```bash\n# Conceptual: RT scheduling and CPU isolation for alerting\nsudo systemctl edit alerting.service << 'EOF'\n[Service]\nCPUAffinity=4-5\nCPUQuota=60%\nEOF\n```\n\n## Follow-up Questions\n-","diagram":"flowchart TD\n  Ingestion --> CPU_Pools\n  Alerting --> CPU_Pools\n  CPU_Pools --> SLA_Metrics\n  SLA_Metrics --> Validated","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T05:34:36.230Z","createdAt":"2026-01-15T21:46:48.385Z"},{"id":"q-2573","question":"On a Linux host running a real-time audio processing pipeline that must meet deterministic latency, a single 8-core CPU shows occasional dropouts during peak input. Design a production plan to guarantee latency isolation for the critical path: specify hardware topology, kernel boot params (isolcpus, nohz_full), IRQ affinity, CPU sets, cgroups v2 with a dedicated rt.slice using SCHED_FIFO priorities, memory locking, and a test plan with cyclictest and a synthetic workload simulating 16-channel 192 kHz audio. What exact steps would you take and how would you verify?","answer":"Configure the system with dedicated CPU isolation: boot with `isolcpus=4-7 nohz_full=4-7`, disable irqbalance service and pin network/controller IRQs to CPUs 0-3. Create a dedicated `rt.slice` cgroup v2 slice, assign real-time audio tasks exclusively to CPUs 4-7 with CPU sets, configure SCHED_FIFO priority 99 for the audio processing threads, and enable memory locking with `memlock` unlimited to prevent page swapping. Verify deterministic performance using cyclictest alongside a synthetic 16-channel 192 kHz audio workload that simulates the production processing pipeline.","explanation":"## Why This Is Asked\nThis question evaluates practical expertise in real-time Linux systems engineering, specifically CPU isolation, interrupt management, and deterministic scheduling under load. It assesses a candidate's ability to design hardware topology-aware solutions that guarantee audio processing latency requirements.\n\n## Key Concepts\n- CPU isolation with isolcpus and nohz_full for deterministic execution\n- IRQ affinity management and irqbalance service control\n- cgroups v2 hierarchy with dedicated real-time slices\n- SCHED_FIFO real-time scheduler with priority management\n- Memory locking strategies for preventing cache misses and swapping\n- Performance validation using cyclictest and synthetic workloads\n\n## Code Example\n```bash\n# GRUB configuration for CPU isolation\nGRUB_CMDLINE_LINUX=\"isolcpus=4-7 nohz_full=4-7\"\n\n# Systemd rt.slice configuration\n[Slice]\nCPUQuota=100%\nCPUAffinity=4 5 6 7\nMemoryAccounting=yes\nMemoryLimit=infinity\n\n# Real-time service configuration\n[Service]\nSlice=rt.slice\nCPUSchedulingPolicy=fifo\nCPUSchedulingPriority=99\nLimitMEMLOCK=infinity\n```","diagram":"flowchart TD\n  A[Real-time task] --> B[Isolated CPUs 4-7]\n  B --> C[SCHED_FIFO 99]\n  C --> D[MLock / memlockall]\n  D --> E[Cyclictest + audio workload test]","difficulty":"advanced","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Square","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T05:18:04.809Z","createdAt":"2026-01-15T23:33:51.532Z"},{"id":"q-2632","question":"On a Linux host running a UDP telemetry ingest pipeline at high pps, tail latency spikes under peak load threaten alerts. Design a production plan to measure end-to-end latency with eBPF (tracepoints, kprobes, uprobes) and BPF maps, identify bottlenecks (kernel vs user-space), implement mitigations (backpressure, NIC tuning, IRQ affinity, NUMA pinning), and validate with a reproducible load test and SLA targets?","answer":"Use eBPF to timestamp per-packet across the receive path and ingest app, aggregate latency histograms, and pinpoint whether tail latency is kernel, NIC, or user-space bottlenecks. Mitigate with deeper","explanation":"## Why This Is Asked\nTests hands-on observability and low-latency tuning skills.\n\n## Key Concepts\n- eBPF instrumentation across kernel and user space\n- end-to-end and tail latency analysis\n- NIC queue tuning, IRQ affinity, NUMA awareness\n- backpressure and load-testing validation\n\n## Code Example\n```javascript\n// example: placeholder for eBPF tracepoint probes and histograms\n```\n\n## Follow-up Questions\n- How would you automate this in CI?\n- How would you extend for IPv6 and multi-tenant isolation?","diagram":"flowchart TD\n  A[Start] --> B[Attach eBPF probes]\n  B --> C[Capture per-packet timestamps]\n  C --> D[Build latency histograms]\n  D --> E[Triage kernel vs user-space]\n  E --> F[Apply mitigations]\n  F --> G[Validate with load test]\n  G --> H[ SLA verification]","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","OpenAI","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T04:15:24.673Z","createdAt":"2026-01-16T04:15:24.673Z"},{"id":"q-2708","question":"A Linux server hosting a small web app sees frequent 'No space left on device' errors despite disk space being free; df -h shows space but df -i shows inode exhaustion. Outline a beginner-friendly plan to diagnose and fix inode exhaustion, including exact commands to identify hotspots, recommended cleanup or restructuring, and a simple test plan to reproduce and verify the fix?","answer":"Run inode check: df -i to confirm exhaustion. Locate hotspots with: for d in /var/log /home /tmp; do echo $d; find $d -type f | wc -l; done. If /var/log dominates inodes, purge stale files and enable ","explanation":"## Why This Is Asked\nInodes exhaustion is a common beginner pitfall that halts file creation even when space remains. It tests practical diagnosis and remediation with real commands.\n\n## Key Concepts\n- inode vs space\n- hotspot identification\n- log management and separate volumes\n- safe cleanup and testing\n\n## Code Example\n\n```bash\n# example commands\ndf -i\nfor d in /var/log /home /tmp; do echo $d; du -sh $d || true; done\n```\n\n## Follow-up Questions\n- What are inode-safe log rotation strategies?\n- How would you prevent recurrence in a multi-tenant setup?","diagram":"flowchart TD\n  A[Inode Exhaustion] --> B[Diagnose Hotspots]\n  B --> C[Purge/Rotate]\n  C --> D[Move Logs]\n  D --> E[Test Validation]","difficulty":"beginner","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Instacart","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T07:41:34.278Z","createdAt":"2026-01-16T07:41:34.278Z"},{"id":"q-2787","question":"On a Linux host running several KVM guests, one guest issues heavy bursts to a shared virtio-block device, causing host scheduler starvation and jitter in the management plane. Propose a production plan to guarantee host responsiveness and deterministic per-guest I/O latency under mixed workloads. Include host CPU pinning, NUMA affinity, VFIO isolation, IRQ steering, per-guest cgroups (cpu, io.max), and storage QoS with an appropriate I/O scheduler (bfq or mq-deadline), plus a test plan using fio and cyclictest?","answer":"Pin virtio-blk to dedicated cores, pin QEMU processes with a fixed CPU set, enable NUMA awareness and large pages for guests, isolate devices with vfio-pci, assign IRQs to those CPUs, carve per-guest ","explanation":"## Why This Is Asked\nTests practical mastery of real-world multi-tenant virtualization, CPU and I/O isolation, and QoS tuning under bursty workloads. It differentiates candidates who understand end-to-end latency guarantees from those who know only theory.\n\n## Key Concepts\n- KVM guest isolation and VFIO device binding\n- NUMA awareness and large pages for performance\n- CPU pinning, IRQ steering, and per-guest cgroups (cpu, io.max)\n- Storage QoS and I/O schedulers (bfq, mq-deadline)\n\n## Code Example\n````bash\n# Bind QEMU to CPUs 2-5 (example)\npid=$(pidof qemu-system-x86_64)\ntaskset -cp 2-5 $pid\n````\n\n## Follow-up Questions\n- How would you handle a sudden surge across multiple guests without starving the management plane?\n- What monitoring hooks and SLIs would you deploy to detect and auto-remediate degradation?\n","diagram":"flowchart TD\n  Host[Host System]\n  G1[Guest A]\n  G2[Guest B]\n  Disk[Shared Virtio Disk]\n  Host --> G1\n  Host --> G2\n  G1 -- I/O Burst --> Disk\n  G2 -- I/O Wait --> Disk","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Hashicorp"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T11:43:31.458Z","createdAt":"2026-01-16T11:43:31.458Z"},{"id":"q-2931","question":"On a Linux server hosting a web app, the /var partition has space but inode usage on /var/lib/app/cache is near exhaustion due to many tiny files created daily. Propose a production plan to prevent inode exhaustion without downtime: redesign cache storage (hashed dirs), implement cleanup/rotation, enable tmpfiles purge, and a lightweight watchdog. Include concrete commands and config snippets, plus a reproducible test plan?","answer":"Plan: 1) migrate cache to hashed dir layout using sha1/first2 chars (e.g., /var/lib/app/cache/aa/abcd...). 2) add a daily cleanup script and rotate/compress logs; delete files older than 14 days. 3) e","explanation":"## Why This Is Asked\n\nTests ability to reason about inode pressure, not just disk space. Shows practical fixes that avoid downtime and maintain SLA.\n\n## Key Concepts\n\n- Inodes vs disk space\n- Cache storage layout and HA safety\n- Automated cleanup and tmpfiles\n- Lightweight watchdogs and test plans\n\n## Code Example\n\n```bash\n# migrate: create hashed dirs and move files\nmkdir -p /var/lib/app/cache/aa\n# example migration placeholder\n```\n\n## Follow-up Questions\n\n- How would you generalize this for multiple tenants?\n- What are potential pitfalls of hashed-cache layouts?","diagram":null,"difficulty":"beginner","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Snowflake","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T17:53:05.675Z","createdAt":"2026-01-16T17:53:05.675Z"},{"id":"q-2937","question":"On a fleet of 12 Linux hosts serving a high-traffic API behind a load balancer, implement zero-downtime kernel updates using live patching (kpatch/kgraft or distro Livepatch). Design a production plan covering patch discovery, staging, canary validation, rollout strategy, rollback criteria, and instrumentation to ensure SLA adherence during updates?","answer":"Design a zero-downtime kernel update plan using live patching (kpatch/kgraft or distro Livepatch). Include patch discovery, staged canary, automated validation (crash-free boot, perf tests), rolling r","explanation":"Why This Is Asked\n\nAssesses practical mastery of live patching in prod: discovery, validation, and safe rollout with rollback under strict SLAs.\n\nKey Concepts\n\n- Live patching mechanisms (kpatch, kgraft, distro Livepatch)\n- Canary and rolling rollout strategies\n- Validation: crash-free boot, perf/load tests, failover checks\n- Telemetry: MTTR, patch success rate, SLA metrics\n\nCode Example\n\n```bash\n# High-level patch flow (illustrative)\nPATCH_DIR=/patches\nkgraft apply $PATCH_DIR/patch1.kpatch\nsystemctl restart api-service\n```\n\nFollow-up Questions\n\n- How would you validate rollback under patch failure scenarios? \n- Which metrics determine patch readiness across all nodes and how would you automate alerting?","diagram":"flowchart TD\n  A[Detect patch available] --> B[Validate patch on canary]\n  B --> C[Rollout to subset]\n  C --> D[Full rollout]\n  D --> E[Monitor metrics]\n  E --> F{Issue?}\n  F -->|Yes| G[Rollback patch]\n  F -->|No| H[Continue]","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","DoorDash","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T18:43:41.436Z","createdAt":"2026-01-16T18:43:41.437Z"},{"id":"q-3013","question":"On a Linux host running a multi-tenant container stack over a single 25 GbE NIC, one tenant deploys a high-volume eBPF-based load balancer that exhausts kernel memory via per-connection maps, risking SLA for others. Design a production plan to cap per-tenant BPF map quotas, enforce cgroup v2 memory.max, io.max, isolate CPUs, and validate with reproducible traffic and memory graphs. Include concrete steps and a test plan?","answer":"Implement per-tenant BPF resource isolation using cgroup v2 quotas, CPU pinning, and bpffs namespaces. Create dedicated cgroups for each tenant with memory.max and io.max limits, establish per-tenant bpffs mount points, attach BPF programs to tenant cgroups, and enforce CPU isolation through systemd or cpusets. Monitor resource usage with bpftool, perf, and cgroup metrics.","explanation":"## Why This Is Asked\nDesigning per-tenant resource isolation for kernel-space workloads (BPF) on a NIC-limited host tests practical knowledge of cgroup v2, bpffs, CPU isolation, and observabilityâ€”critical skills for multi-tenant container environments.\n\n## Key Concepts\n- BPF map quotas per tenant\n- cgroup v2 memory.max and io.max enforcement\n- bpffs per-tenant namespaces\n- CPU isolation and pinning strategies\n- Observability with bpftool and perf\n\n### Code Example\n```bash\n# Create per-tenant cgroup with quotas\nmkdir -p /sys/fs/cgroup/unified/tenantA\necho 1024M > /sys/fs/cgroup/unified/tenantA/memory.max\necho 500M > /sys/fs/cgroup/unified/tenantA/io.max\n```","diagram":"flowchart TD\n  A[Start] --> B[Identify BPF resource usage per tenant]\n  B --> C[Create per-tenant cgroups and bpffs roots]\n  C --> D[Apply memory.max and io.max per tenant]\n  D --> E[Isolate CPUs and pin workloads]\n  E --> F[Test plan and monitoring]\n  F --> G[Validate SLA]","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Apple","Bloomberg"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T06:04:14.235Z","createdAt":"2026-01-16T21:32:20.492Z"},{"id":"q-3168","question":"On a Linux host running multiple longâ€‘lived services, a single process intermittently hits the file descriptor limit during peak traffic, breaking log shipping. Design a production plan to prevent FD exhaustion: enforce perâ€‘process limits (LimitNOFILE), raise fs.file-max, enable systemd TasksMax, isolate services with dedicated cgroups, add a watchdog that restarts on quota breach, and implement a burst FD test to verify. What concrete steps would you take?","answer":"Per-process and system-wide FD hardening plan: set LimitNOFILE high on each unit, raise fs.file-max, enable systemd TasksMax, isolate services with dedicated cgroups, add a watchdog that restarts on q","explanation":"## Why This Is Asked\n\nTests ability to translate FD limits into concrete production controls: per-unit limits, system-wide caps, and containment via cgroups. It also probes validation under bursty conditions and automated remediation.\n\n## Key Concepts\n\n- fs.file-max and perâ€‘process limits (LimitNOFILE)\n- systemd TasksMax and cgroups\n- watchdog, auto-remediation, monitoring\n\n## Code Example\n\n```bash\n# Example: per-service FD limit\n[Service]\nLimitNOFILE=1048576\n```\n\n## Follow-up Questions\n\n- How would you test burst scenarios and verify SLA under peak?\n- How would you audit changes across services and roll back?\n","diagram":"flowchart TD\n  A[FD limit breach] --> B[Detect]\n  B --> C[Enforce per-process limits]\n  C --> D[Isolate via cgroups]\n  D --> E[Watchdog: restart/recycle]\n  E --> F[Burst FD test & verification]","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Robinhood","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T05:32:42.285Z","createdAt":"2026-01-17T05:32:42.285Z"},{"id":"q-3309","question":"On a Linux host running multiple tenants' containers sharing a single 40Gbps NIC, design a production plan for deterministic per-tenant network performance: telemetry and enforcement without sacrificing throughput. Propose a plan using XDP/eBPF to (a) tag/redirect packets to per-tenant maps, (b) enforce per-tenant egress rate via tc or XDP, (c) minimize overhead with high flow counts, (d) test with synthetic traffic. Include kernel config, BPF maps, and verification steps?","answer":"Use XDP with a per-tenant BPF map keyed by tenant_id to maintain counters and enforce rate via a token-bucket limiter. Attach an XDP program to the primary NIC to tag packets and steer excess traffic ","explanation":"## Why This Is Asked\n\nThis question tests practical XDP/eBPF planning for multi-tenant networks.\n\n## Key Concepts\n\n- XDP, eBPF maps, per-tenant isolation, rate limiting, queueing disciplines\n- tc vs XDP-based enforcement, multi-queue NICs, low overhead\n- Observability and verification with synthetic traffic\n\n## Code Example\n\n```c\n#include <linux/bpf.h>\n#include <bpf/bpf_helpers.h>\n\nSEC(\"xdp\")\nint xdp_prog(struct xdp_md *ctx) {\n  // skeleton: tenant_id extraction and rate enforcement\n  return XDP_PASS;\n}\n\nchar _license[] SEC(\"license\") = \\\"GPL\\\";\n```\n\n## Follow-up Questions\n\n- How would you scale maps for 10k tenants?\n- What failure modes and mitigations exist with XDP in multi-tenant env?\n","diagram":null,"difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T10:44:48.860Z","createdAt":"2026-01-17T10:44:48.861Z"},{"id":"q-3393","question":"On a Linux host acting as a 40 Gbps edge router, implement per-tenant ingress rate limiting and DDoS protection using an XDP-based fast path. Design a BPF program with per-tenant maps (tenant_id -> token bucket), dynamic quota reloads, and safe memory management. Explain topology, attach mode, safety against OOM, and a test plan with multi-tenant traffic to validate fairness and latency?","answer":"Use XDP in native mode at ingress with a per-tenant BPF hash map (tenant_id -> token bucket). Forward if bucket>0 (atomically decremented); otherwise XDP_DROP or rate-limit. Update quotas live from us","explanation":"## Why This Is Asked\\nThis question probes practical, production-grade use of eBPF/XDP for high-throughput, low-latency traffic control with per-tenant isolation.\\n\\n## Key Concepts\\n- eBPF maps and per-tenant state\\n- XDP, native mode, and fast path decisions\\n- Token bucket rate limiting and atomic operations\\n- Live map updates and safety/OOM considerations\\n- Observability: bpftool, perf, and counters\\n\\n## Code Example\\n```c\\n// Skeleton XDP program structure\\n#include <linux/bpf.h>\\n#include <bpf/bpf_helpers.h>\\n\\nstruct bpf_map_def SEC(\\\"maps\\\"){ /* legacy */ };\\n\\nSEC(\\\"xdp\\\")\\nint xdp_prog(struct xdp_md *ctx){\\n    // lookup tenant_id, fetch bucket, decrement atomically, forward or drop\\n    return XDP_PASS;\\n}\\n\\nchar _license[] SEC(\\\"license\\\") = \\\"GPL\\\";\\n```\\n\\n## Follow-up Questions\\n- How would you handle tenant churn and map growth?\\n- How would you rate-limit control traffic while preserving low latency for legitimate flows?","diagram":null,"difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Amazon","LinkedIn"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T14:34:22.953Z","createdAt":"2026-01-17T14:34:22.953Z"},{"id":"q-3522","question":"On a Linux host running a high-volume NAT gateway with multiple containers behind a single 100Gb NIC, a surge in new connections triggers nf_conntrack contention and latency spikes. Design a production plan to guarantee sub-2ms connection-setup latency at the 99th percentile: tune nf_conntrack (max, hashsize), enable per-namespace conntrack tables, adopt an eBPF/XDP path for early filtering, isolate tenants with separate netns and tc policing, and instrument with bpftrace. Include exact steps and a test plan with 10k concurrent connections?","answer":"Raise nf_conntrack_max and nf_conntrack_buckets; enable per-namespace conntrack tables; deploy a small eBPF/XDP program to pre-filter invalid/duplicate connections; pin NAT/conntrack processing to ded","explanation":"## Why This Is Asked\n\nTests practical networking tuning for multi-tenant NAT gateways, focusing on conntrack, eBPF/XDP, and per-tenant isolation.\n\n## Key Concepts\n\n- nf_conntrack tuning (max, buckets)\n- eBPF/XDP for early filtering\n- NetNS isolation and tc policing\n- High-concurrency performance verification\n\n## Code Example\n\n```c\n// XDP skeleton\n#include <linux/bpf.h>\n#include <bpf/bpf_helpers.h>\nSEC(\"xdp\")\nint xdp_prog(struct xdp_md *ctx) {\n    // placeholder\n    return XDP_PASS;\n}\n```\n\n## Follow-up Questions\n\n- What are risks of per-namespace conntrack tables?\n- How would you monitor regressions post-rollout?","diagram":"flowchart TD\n  A[Connection Surge] --> B[NF Conntrack Check]\n  B --> C[XDP Early Filtering]\n  C --> D[Per-Namespace NAT]\n  D --> E[Tenant Shaping]\n  E --> F[Metrics & Rollback]","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","DoorDash","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T19:35:43.645Z","createdAt":"2026-01-17T19:35:43.645Z"},{"id":"q-3570","question":"On a Linux host running multiple high-sensitivity services in a NUMA system, memory pressure from one service causes paging and latency spikes for others. Propose a production plan to strictly bound per-service memory usage and preserve throughput. Include concrete steps: enabling cgroup v2 quotas (memory.max, memory.high, memory.swap.max), per-service NUMA binding, kernel tunables (vm.min_free_kbytes, vm.swappiness), and a test plan with 10k allocations and 60-minute stability checks?","answer":"Implement per-service memory quotas via cgroup v2: create dedicated slices for each service; enforce memory.max as a hard cap, memory.high as a soft target, and memory.swap.max to limit swap usage. Bind processes to specific NUMA nodes using numactl to ensure local memory allocation, configure kernel tunables (vm.min_free_kbytes to reserve system memory, vm.swappiness to control paging behavior), and validate with a comprehensive test plan.","explanation":"## Why This Is Asked\nTests depth in memory isolation, NUMA awareness, and production readiness. It targets real-world constraints: per-service quotas, NUMA local allocations, and careful kernel tuning to avoid spillover under pressure.\n\n## Key Concepts\n- cgroup v2 memory controls: memory.max, memory.high, memory.swap.max\n- NUMA binding and local memory allocation (numactl)\n- Kernel tunables: vm.min_free_kbytes, vm.swappiness, THP impact\n- Observability: per-slice memory usage and latency metrics\n\n## Code Example\n```\n# /etc/systemd/system/serviceA.slice\n[Slice]\nCPUAccounting=true\nMemoryAccounting=true\nMemoryMax=8G\nMemoryHigh=7G\nMemorySwapMax=1G\n```\n\n## Implementation Steps\n1. Create systemd slices with memory quotas\n2. Configure NUMA binding via numactl or systemd CPUAffinity\n3. Set kernel tunables: vm.min_free_kbytes=1G, vm.swappiness=10\n4. Deploy monitoring: pressure stall information, per-cgroup stats\n5. Execute test plan: 10k allocations, 60-minute stability validation","diagram":"flowchart TD\n  A[Service] --> B[Quota: memory.max]\n  B --> C[NUMA binding (numactl)]\n  C --> D[Soft limit: memory.high]\n  D --> E[Swap cap: memory.swap.max]","difficulty":"advanced","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Databricks","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T05:43:38.022Z","createdAt":"2026-01-17T21:36:45.544Z"},{"id":"q-3662","question":"On a Linux server running a high-volume data ingestion service that ships logs to a cloud sink, sporadic tail latency spikes occur under burst I/O. Design a production plan to guarantee deterministic latency for the ingestion path: specify hardware topology (NUMA affinities), kernel boot params (isolcpus, nohz_full), IRQ affinity, CPU sets, and a containers/systemd v2 cgroup layout with a dedicated rt.slice or critical.slice using io.max and a CPU quota, memory locking, and a concrete test plan with fio and cyclictest simulating burst I/O. What exact steps would you take and how would you verify?","answer":"Pin ingestion workers to a dedicated NUMA node, isolate CPUs with isolcpus/nohz_full, bind IRQs to that node, and place the process in a v2 cgroup (critical.slice) with cpu.max, io.max, and memlock, p","explanation":"## Why This Is Asked\nTests ability to design end-to-end latency isolation on a single host using NUMA awareness, kernel boot parameters, IRQ affinity, and precise cgroup isolation. It examines practical trade-offs between throughput, isolation, and complexity.\n\n## Key Concepts\n- NUMA-aware CPU/Memory Placement\n- Kernel isolation: isolcpus, nohz_full, and CPU shielding\n- IRQ affinity and CPU sets in a cgrouped environment\n- cgroup v2: cpu.max, io.max, memory.high, memlock\n- Latency testing: fio burst I/O and cyclictest tail latency\n\n## Code Example\n```bash\n# Example: create a dedicated NUMA node and isolate a slice\n# 1) Isolate CPUs 2-5 on NUMA node 1\nsudo isolcpus=2-5 nohz_full=2-5 bootparams\n# 2) Create critical.slice and assign limits\nsystemd-run --unit=critical.slice --slice=critical.slice --property=CPUQuota=25%\n# 3) Bind ingestion process to the slice and CPUs\ntaskset -c 2-5 ./ingest_service\n```\n\n## Follow-up Questions\n- How would you extend this plan to a multi-node cluster with live failover?\n- Which monitoring hooks (perf, ftrace, iostat, pidstat) would you wire for ongoing validation?\n","diagram":null,"difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Stripe","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T04:15:39.538Z","createdAt":"2026-01-18T04:15:39.538Z"},{"id":"q-3726","question":"On a Linux host running an in-memory database that uses hugepages, intermittent THP defragmentation spikes cause tail latency regressions. Design a production plan to eliminate THP pauses while preserving memory efficiency. Include exact steps to disable THP, reserve hugepages for the DB, pin memory (memlock), NUMA binding and per-service cgroups, and a test plan proving p95 latency stays under target at 50k qps; add rollback?","answer":"Disable THP completely for the DB path, reserve dedicated hugepages, pin memory (memlock) via rlimits/mlockall, bind the process to specific NUMA nodes, and isolate it in its own cgroup. Verify p95 la","explanation":"## Why This Is Asked\nTail latency in latency-sensitive DB workloads can be dominated by kernel memory management; THP defrag can cause long stalls. A deterministic plan is needed for production.\n\n## Key Concepts\n- Transparent Huge Pages (THP) and defragmentation impact on latency\n- Hugepages reservation and NUMA affinity\n- memlock/mlockall, per-service cgroups, and resource isolation\n- Validation metrics and rollback procedures\n\n## Code Example\n```javascript\n// Pseudo test harness outline for latency validation in JS\nfunction testLatency(targetQps, p95) { /* implement synthetic workload and monitor latency */ }\n```\n\n## Follow-up Questions\n- How would you monitor THP activity in production?\n- What are the trade-offs of reserving hugepages vs. sharing them across processes?\n","diagram":"flowchart TD\n  A[DB process] --> B[Disable THP]\n  B --> C[Reserve HugePages]\n  C --> D[Pin Memory (mlock)]\n  D --> E[NUMA & CGroups isolation]\n  E --> F[Test & validate latency]\n  F --> G[Rollback path if needed]","difficulty":"advanced","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Lyft","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T07:27:54.478Z","createdAt":"2026-01-18T07:27:54.478Z"},{"id":"q-3866","question":"On a Linux host running multiple KVM guests with shared storage, a noisy neighbor VM spikes I/O and delays CI workloads on a critical VM. Propose a practical plan to guarantee sub-5ms tail I/O latency for that VM, including concrete steps to (a) switch to BFQ I/O scheduler, (b) enforce per-VM I/O limits with cgroup v2 io.weight/io.max, (c) isolate disks or queues for the critical VM, and (d) a test plan with reproducible load and latency verification?","answer":"Enable BFQ as the IO scheduler for the disks backing VMs; create per-VM cgroups under /sys/fs/cgroup/io, set io.weight higher for the critical VM and io.max for others, and consider dedicating a disk/","explanation":"## Why This Is Asked\n\nTests practical I/O isolation on Linux, combining kernel schedulers, cgroup v2 IO controls, and virtualization realities.\n\n## Key Concepts\n\n- BFQ I/O scheduler for per-VM isolation\n- cgroup v2 io.weight and io.max for per-VM controls\n- Virtualization impact on disk I/O and storage tiering\n- Realistic testing with fio/VDbench and latency targets\n\n## Code Example\n\n```javascript\n# Example commands (illustrative)\necho bfq | sudo tee /sys/block/sda/queue/scheduler\nsudo mkdir -p /sys/fs/cgroup/io/critical_vm\nsudo bash -c 'echo 256 > /sys/fs/cgroup/io/critical_vm/io.weight'\n```\n\n## Follow-up Questions\n\n- How would you adapt this plan to a Kubernetes node with per-pod I/O isolation?\n- What monitoring would you add to detect I/O issues before CI workloads are affected?","diagram":null,"difficulty":"beginner","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","LinkedIn","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T13:07:01.474Z","createdAt":"2026-01-18T13:07:01.474Z"},{"id":"q-3875","question":"On a Linux host handling high-velocity production workloads, implement a low-overhead tracing plan to debug intermittent latency spikes without impacting throughput? Design an approach using eBPF/BPFtrace to capture key latency paths, isolate tenants, and trigger alerts for SLA breaches. Include what to instrument, data structures, sampling strategy, and a test plan?","answer":"Use low-overhead eBPF tracing (BCC/BPFtrace) to instrument the critical latency path. Attach kprobes/tracepoints to scheduler, block IO, and network receive paths; filter by cgroup/docker container ID","explanation":"## Why This Is Asked\nIntermittent latency spikes are common in prod. This question asks for a practical, low-overhead tracing strategy that scales with tenants and provides actionable data.\n\n## Key Concepts\n- eBPF tracing\n- kprobes/tracepoints\n- per-tenant isolation via cgroups\n- ring buffers and histogram maps\n- alerting and baselining\n\n## Code Example\n```c\n// skeleton BPF program snippet\nBPF_HASH(latency, u32, u64);\n```\n\n## Follow-up Questions\n- How would you handle multi-tenant fairness if one tenant dominates traces?\n- How would you measure instrumentation overhead and bound it?\n","diagram":"flowchart TD\n  A[Start tracing] --> B{Source}\n  B --> C[kprobes/tracepoints]\n  C --> D[Filter by cgroup]\n  D --> E[Per-CPU buffers]\n  E --> F[Histograms map]\n  F --> G[Alerts]","difficulty":"advanced","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","Plaid","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T13:38:40.690Z","createdAt":"2026-01-18T13:38:40.691Z"},{"id":"q-3908","question":"On a Linux host running a high-throughput log ingestion stack (rsyslog -> local NVME store -> analytic pipeline) experiencing intermittent 100â€“300 ms I/O latency under peak writes. Design a production plan to diagnose and eliminate storage I/O bottlenecks while preserving log reliability. Include: I/O schedulers, per-disk vs per-tenant I/O shaping (cgroups v2 io.max), test plan with fio/workloads, and validation steps?","answer":"Plan to reproduce, bench, and isolate I/O bottlenecks: 1) instrument with iostat -dx, ioping, and blktrace; 2) compare IO schedulers (deadline vs bfq) and two queue depths; 3) implement per-tenant io.","explanation":"## Why This Is Asked\nTests the ability to translate real-world I/O path issues into concrete controls and validation, balancing throughput with reliability.\n\n## Key Concepts\n- I/O scheduling and devices tuning\n- cgroups v2 io.max for per-tenant shaping\n- Observability with iostat, blktrace, ioping, fio\n- SLA-driven latency validation and backpressure handling\n\n## Code Example\n```bash\n# sample fio job (simplified)\n[write_job]\nrw=write\nbs=4k\nsize=1000M\nioengine=libaio\n```\n\n## Follow-up Questions\n- How would caching and filesystem flags impact results?\n- How would you automate regression tests for future hardware changes?","diagram":"flowchart TD\n  A[Storage Bottleneck] --> B[Measure Metrics]\n  B --> C[Test Schedulers & Queue Depths]\n  C --> D[CGroup io.max per Tenant]\n  D --> E[Run fio workloads]\n  E --> F[Validate SLA & Backlog Avoidance]","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T14:43:00.557Z","createdAt":"2026-01-18T14:43:00.557Z"},{"id":"q-3958","question":"On a Linux host running both a real-time data-feed and a batch analytics job sharing one NVMe pool, bursts trigger 2â€“6 ms stalls. Design a plan to guarantee deterministic tail latency for the data-feed: per-tenant IO isolation (io.max), CPU pinning, NUMA-aware memory, appropriate I/O scheduler, and a test harness with mixed READ/WRITE fio workloads to verify p99 latency and failure modes?","answer":"Plan: enforce per-tenant I/O limits with cgroups v2 io.max, pin data-feed to a dedicated CPU set on a single NUMA node, use an NVMe-friendly mq-deadline scheduler, prefer io_uring paths, isolate workl","explanation":"## Why This Is Asked\n\nAssesses practical storage isolation and latency guarantees under bursty multi-tenant workloads on NVMe, a common real-world constraint in quant-heavy environments.\n\n## Key Concepts\n\n- cgroups v2 io.max and io.weight for per-tenant isolation\n- NUMA-aware CPU and memory binding to minimize cross-node latency\n- NVMe-optimized I/O schedulers (mq-deadline, bfq considerations)\n- IO paths: io_uring vs AIO in high-throughput scenarios\n- Reproducible testing with fio and latency KPIs (p99) and failover checks\n\n## Code Example\n\n```javascript\n// Example: quick fio test config (illustrative, not executable)\nconst job = { rw: 'randrw', bs: '4k', iodepth: 32, ioengine: 'io_uring' };\n```\n\n## Follow-up Questions\n\n- How would you validate recovery when a disk goes offline or a rebuild starts?\n- How would you monitor per-tenant latency in production and automate spillover to a separate pool?","diagram":null,"difficulty":"advanced","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Plaid","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T17:27:38.921Z","createdAt":"2026-01-18T17:27:38.921Z"},{"id":"q-4020","question":"On a Linux host running a multi-tenant data-collection service, tail latency spikes appear under 99th percentile during bursts. Outline a production plan to diagnose and mitigate root causes using eBPF tracing (bpftrace), per-tenant metrics, and strict QoS isolation. Include steps to install tracing, collect sched/net/block latency per tenant, enforce per-tenant cgroups v2 with cpu.max and io.max, and validate with synthetic bursts. What exact steps would you take and how would you revert if latency worsens?","answer":"Instrument with eBPF (bpftrace) to trace per-tenant sched_latency, net_xmit, and block IO; collect p95/p99 latency per tenant; enforce per-tenant cgroups v2 with cpu.max and io.max; throttle noisy ten","explanation":"## Why This Is Asked\\n\\nThis tests real-world ability to diagnose tail latency in multi-tenant Linux environments using advanced tracing, QoS, and safe production changes.\\n\\n## Key Concepts\\n- eBPF tracing (bpftrace)\\n- per-tenant QoS with cgroups v2\\n- latency metrics (p95/p99) and synthetic workloads\\n\\n## Code Example\\n```javascript\\n// illustrative bpftrace snippet (not runnable)\\nBEGIN { /* setup */ }\\nEND { /* teardown */ }\\n```\\n\\n## Follow-up Questions\\n- How would you adapt if tenants share a single NVMe?\\n- How would you calibrate p99 thresholds and alerting?","diagram":null,"difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","Snowflake","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T20:31:47.246Z","createdAt":"2026-01-18T20:31:47.248Z"},{"id":"q-4151","question":"On a Linux host running a high-fidelity trading UI service that must meet a 5 ms end-to-end latency SLA under mixed workloads, design a production plan to guarantee deterministic latency for the critical path. Specify hardware topology (NUMA, CPU pinning), kernel params (isolcpus, nohz_full, rcu_nocb), per-group scheduling using SCHED_DEADLINE with deadlines, IRQ affinity, cgroups v2 (rt.slice) and memory bindings, plus a test plan using cyclictest and a synthetic tick workload. What exact steps would you take and how would you verify?","answer":"Partition NUMA node 0 for the critical path; pin the UI worker to CPUs 0â€“3; boot with isolcpus=0-3, nohz_full=0-3, rcu_nocb; use SCHED_DEADLINE on rt.slice with period=5ms, runtime=4ms, slack=1ms; bin","explanation":"## Why This Is Asked\nThis probes real-time scheduling, NUMA binding, and cgroup isolation under mixed workloads, focusing on concrete tunables rather than theory.\n\n## Key Concepts\n- NUMA-aware CPU/memory binding\n- Kernel boot knobs: isolcpus, nohz_full, rcu_nocb\n- SCHED_DEADLINE configuration per group\n- Cgroups v2 rt.slice and memory bindings\n- IRQ affinity and memory locking considerations\n\n## Code Example\n```javascript\nconst sched = {\n  policy: 'SCHED_DEADLINE',\n  periodNs: 5000000,\n  runtimeNs: 4000000,\n  deadlineNs: 5000000\n};\n```\n\n## Follow-up Questions\n- How would you monitor for deadline misses in production?\n- What would you adjust if a single non-critical process causes cache pressure?\n","diagram":"flowchart TD\n  A[Hardware topology] --> B[Kernel isolation]\n  B --> C[SCHED_DEADLINE cfg]\n  C --> D[Cgroups v2 rt.slice]\n  D --> E[IRQ/mem binding]\n  E --> F[Test & verify]","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Salesforce","Snap","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T05:42:39.623Z","createdAt":"2026-01-19T05:42:39.623Z"},{"id":"q-4264","question":"On a fleet of Linux CI workers, bursts from a single job spike CPU and memory, starving other jobs. Propose a beginner-friendly plan to isolate each job using a dedicated systemd slice and cgroup v2, enforce per-job CPU quotas, limit PIDs, and ensure quick reversion if a burst is detected. Include concrete unit settings, a test plan with a 5-minute burst, and commands to verify outcomes?","answer":"Create a dedicated ci.slice with CPUQuota=50%, CPUQuotaPeriodSec=100ms, MemoryLimit=2G, and TasksMax=200. Use a service template ci-job@.service with Slice=ci.slice and per-job limits. Test by launchi","explanation":"## Why This Is Asked\nTests practical, beginner-friendly systemd/cgroup isolation for CI workers.\n\n## Key Concepts\n- systemd slices and resource controls (CPUQuota, MemoryLimit, TasksMax)\n- cgroup v2 awareness and per-job isolation\n- test harness for bursts and verification\n\n## Code Example\n```ini\n# /etc/systemd/system/ci.slice\n[Unit]\nDescription=CI Job Sandbox\n\n[Slice]\nCPUAccounting=true\nCPUQuota=50%\nCPUQuotaPeriodSec=100ms\nMemoryLimit=2G\nTasksMax=200\n```\n```ini\n# /etc/systemd/system/ci-job@.service\n[Unit]\nDescription=CI Job %I\n\n[Service]\nSlice=ci.slice\nExecStart=/bin/sleep infinity\n```\n\n## Follow-up Questions\n- How would you extend this for GPU-accelerated jobs?\n- How to monitor and automatically eject misbehaving jobs between bursts?","diagram":"flowchart TD\n  A[Job Submission] --> B[ci.slice]\n  B --> C[ci-job@.service]\n  C --> D{Limits Enforced}\n  D -->|Yes| E[Job runs within quotas]\n  D -->|No| F[Violation response]","difficulty":"beginner","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Discord"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T10:55:44.662Z","createdAt":"2026-01-19T10:55:44.662Z"},{"id":"q-4307","question":"On a Linux host, a long-running batch job scheduled via a systemd timer occasionally drifts under load, causing data window misses. Propose a production plan to ensure timely execution and reliability. Include concrete timer settings (OnCalendar, Persistent, AccuracySec, RandomizedDelaySec), service isolation (LimitNOFILE, CPUQuota, PrivateTmp, ProtectSystem), a lightweight monitoring/alerting approach, and a rollback path if drift worsens. Provide exact commands and config snippets?","answer":"Hardening focuses on deterministic cadence and isolation: use OnCalendar with a fixed cadence and Persistent=true; AccuracySec=1s; RandomizedDelaySec=30s. Enforce CPUQuota and LimitNOFILE; PrivateTmp;","explanation":"## Why This Is Asked\nTests knowledge of systemd timers, isolation, monitoring, and rollback in production.\n\n## Key Concepts\n- systemd timers: OnCalendar, Persistent, AccuracySec, RandomizedDelaySec\n- Service isolation: CPUQuota, LimitNOFILE, PrivateTmp, ProtectSystem\n- Observability: journald alerts, watchdog\n- Rollback plan\n\n## Code Example\n```ini\n# /etc/systemd/system/batch.timer\n[Timer]\nOnCalendar=hourly\nPersistent=true\nAccuracySec=1s\nRandomizedDelaySec=30s\n\n[Install]\nWantedBy=timers.target\n```\n```ini\n# /etc/systemd/system/batch.service\n[Service]\nUser=batch\nGroup=batch\nExecStart=/usr/local/bin/batch-runner\nLimitNOFILE=1048576\nCPUQuota=50%\nPrivateTmp=true\nProtectSystem=full\n```\n\n## Follow-up Questions\n- How would you validate timer drift under simulated load?\n- How to implement a safe rollback if drift increases?","diagram":null,"difficulty":"beginner","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T13:11:13.189Z","createdAt":"2026-01-19T13:11:13.190Z"},{"id":"q-4400","question":"On a Linux host, a periodic cleanup script used by a systemd timer occasionally fails because it relies on a writable /tmp that can be cleaned out during bursts. Propose a production-safe migration to a systemd-timer+service with isolation and idempotence. Include concrete unit settings (RuntimeDirectory, PrivateTmp, User/Group, WorkingDirectory, Restart), a minimal script pattern, and a test plan to reproduce and validate reliability?","answer":"Implement timer+service as a dedicated cleanup user with PrivateTmp and RuntimeDirectory=cleanup, set WorkingDirectory=/var/lib/cleanup, and Restart=on-failure. The script uses a /var/lib/cleanup/.don","explanation":"## Why This Is Asked\nThis tests practical systemd isolation, idempotence, and testability for regular tasks.\n\n## Key Concepts\n- systemd timer and service\n- RuntimeDirectory and PrivateTmp\n- Idempotent scripts with marker files\n- Journald-based verification and dry-run testing\n\n## Code Example\n```ini\n[Unit]\nDescription=Periodic cleanup service\n\n[Service]\nUser=cleanup\nGroup=cleanup\nRuntimeDirectory=cleanup\nPrivateTmp=true\nWorkingDirectory=/var/lib/cleanup\nExecStart=/usr/local/bin/cleanup.sh --marker /var/lib/cleanup/.done\nRestart=on-failure\nRemainAfterExit=yes\n```\n\n```ini\n[Timer]\nOnBootSec=5min\nOnUnitActiveSec=1h\nUnit=cleanup.service\n[Install]\nWantedBy=timers.target\n```\n\n## Follow-up Questions\n- How would you audit this over time?\n- What if the cleanup must run as root?","diagram":null,"difficulty":"beginner","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Instacart","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T17:32:50.910Z","createdAt":"2026-01-19T17:32:50.910Z"},{"id":"q-4478","question":"On a Linux server hosting a lightweight web app, a bursty logging workflow writes many small files under /var/log/app and, during peak bursts, inode exhaustion occurs even though disk space remains. Propose a practical plan to prevent inode pressure and keep logs reliable. Include concrete steps for quick triage, log rotation strategies, filesystem choices or partitioning, and a test plan to reproduce bursts and verify stability?","answer":"Audit inode usage with df -i /var/log/app and du -a /var/log/app | wc -l. Immediate fix: enable daily logrotate with compress, maxsize, and maxage; ensure /var/log is on a partition with ample inodes.","explanation":"## Why This Is Asked\n\nThis question probes practical inode and log management basics in a Linux production context. It checks triage speed, persistence of fixes, and understanding of how logs impact inodes, not just disk space.\n\n## Key Concepts\n\n- inode pressure detection and interpretation\n- log rotation, compression, and retention\n- filesystem inode density and partitioning\n- monitoring and alerting for resource exhaustion\n\n## Code Example\n\n```javascript\n// Quick audit (conceptual)\nconst commands = [\n  \"df -i /var/log/app\",\n  \"du -a /var/log/app | wc -l\"\n];\n```\n\n## Follow-up Questions\n\n- How would you implement a surge test for the log path?\n- Which filesystems and inode-density considerations would drive your partition plan?","diagram":"flowchart TD\n  A[Burst log write] --> B{Inodes <= threshold?}\n  B -- No --> C[Continue]\n  B -- Yes --> D[Run quick triage & rotate logs]\n  D --> E[Move to dedicated FS or enable quotas]\n","difficulty":"beginner","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Microsoft","NVIDIA","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T20:41:11.695Z","createdAt":"2026-01-19T20:41:11.695Z"},{"id":"q-4679","question":"On a Linux-based distributed analytics cluster spanning data-centers, time synchronization is critical for event ordering. A misalignment >200 Âµs breaks correlation across logs and triggers alert drift. Design a production-ready time-sync plan using IEEE 1588v2 (PTP) with hardware timestamping. Include transport choice (dedicated network vs grandmaster), NIC/PHC setup, daemon config (ptp4l/chronyd), fault tolerance, monitoring, and a test plan with cyclictest and log correlation?","answer":"Plan: IEEE 1588v2 with hardware timestamping. Use PHC on NICs, map to /dev/ptp0, run linuxptp ptp4l as a boundary clock with a robust grandmaster, and chronyd in PT mode for precise discipline. Prefer","explanation":"## Why This Is Asked\nTime coherence across nodes is essential for correct event ordering in distributed analytics. 1588v2 with hardware timestamping reduces software jitter and ensures deterministic clocks during high-throughput processing.\n\n## Key Concepts\n- IEEE 1588v2 (PTP) with hardware timestamping (PHC)\n- Grandmaster vs boundary clock topology\n- NIC support (PHC capability) and /dev/ptp* mapping\n- ptp4l configuration and chronyd PT mode\n- Fault tolerance, failover, and monitoring latency/offset\n\n## Code Example\n```ini\n# ptp4l config (partial)\n[global]\nstep_threshold 0\ndelay_req_interval 12\nclock_class 248\n\n[phc0]\n# NIC mapped PHC\n```\n\n## Follow-up Questions\n- How would you validate drift under sudden link outages?\n- What metrics would trigger automated failover and how would you test rollback?","diagram":null,"difficulty":"advanced","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","Hugging Face","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T07:48:32.149Z","createdAt":"2026-01-20T07:48:32.149Z"},{"id":"q-4752","question":"On a Linux host serving a GPU-accelerated real-time inference pipeline, multiple workloads share a single 2-socket system. Occasional stalls increase tail latency of the critical path under peak load. Design a production plan to guarantee deterministic latency for the critical path. Include hardware topology (NUMA binding), kernel/driver knobs (nohz_full, isolcpus, IRQ affinity), per-workload scheduling (SCHED_DEADLINE with deadlines), cgroups v2 (io.max, memory.min), and a validation plan with synthetic bursts. What exact steps would you take and how would you verify?","answer":"Isolate a NUMA node for the critical CUDA workers; pin them to CPUs 4â€“7; enable nohz_full and isolcpus on those CPUs; bind the GPU PCIe device to that node via IOMMU; set IRQ affinity away from isolat","explanation":"## Why This Is Asked\nTests hands-on ability to guarantee deterministic latency in GPU-backed real-time pipelines by combining OS-level isolation (NUMA, CPU pinning, IRQ affinity) with schedulers and cgroups, plus a practical validation path.\n\n## Key Concepts\n- NUMA binding and CPU isolation (isolcpus, nohz_full)\n- IRQ affinity and PCIe device binding to a dedicated node\n- SCHED_DEADLINE and rt.slice usage for hard real-time tasks\n- cgroups v2 controls (io.max, memory.min, cpuset) for resource guarantees\n- Validation with synthetic bursts and tail-latency measurements\n\n## Code Example\n```bash\n# Example: isolate CPUs 4-7 and bind GPU to NUMA node 0 (conceptual)\nnumactl --cpunodebind=0 --physcpubind=4-7 <your-infer-program>\n```\n\n## Follow-up Questions\n- How would you monitor real-time latency and detect deadlocks?\n- What failure modes would you test, and how would you revert changes if needed?","diagram":null,"difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","NVIDIA","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T10:52:18.371Z","createdAt":"2026-01-20T10:52:18.371Z"},{"id":"q-4919","question":"On a Linux host running a stateful daemon that handles financial transaction streams, design a zero-downtime in-place upgrade plan for a new binary and on-disk format change. The system uses systemd, a local WAL, and in-memory indices; describe upgrade steps, migration strategy, feature flags, testing plan, and rollback criteria?","answer":"Implement a rolling, in-place upgrade using two binaries and feature flags. Use systemd with a canary path: gradually shift traffic to v2 while the old process migrates its on-disk format via an atomi","explanation":"## Why This Is Asked\n\nAssess the ability to design safe, production-grade upgrades for stateful services with strict uptime requirements.\n\n## Key Concepts\n\n- Zero-downtime, in-place upgrade patterns\n- WAL-driven on-disk format migration\n- Feature flags and canary rollout strategy\n- Rollback criteria, data integrity checks, and observability\n\n## Code Example\n\n```bash\n# Pseudocode for upgrade flow\n# 1) Build v2\n# 2) Prepare migration (WAL replay, schema changes)\n# 3) Swap binaries behind a feature flag\n# 4) Incrementally route traffic to v2\n# 5) Validate metrics; if OK, complete rollout\n```\n\n## Follow-up Questions\n\n- How would you guarantee no active connections are dropped during the swap?\n- What metrics and thresholds signal a safe rollback, and how would you automate it?","diagram":"flowchart TD\n  A[Prepare v2] --> B[Run WAL migration]\n  B --> C[Canary start: route small % of traffic]\n  C --> D{OK?}\n  D -->|Yes| E[Full rollout to v2]\n  D -->|No| F[Rollback to v1]\n  E --> G[Monitor post-rollout metrics]\n  F --> H[Restore binaries and index state]","difficulty":"advanced","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","PayPal","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T19:09:58.221Z","createdAt":"2026-01-20T19:09:58.221Z"},{"id":"q-5030","question":"On a Linux host acting as a multi-tenant CI runner farm (Docker/K8s), one project triggers heavy memory pressure causing host swapping and spillover latency. Design a production plan to enforce per-tenant memory quotas and isolation: cgroups v2 memory.max/memory.swap.max, memory.high defaults, OOMScoreAdjust, page-cache pressure, and fair reclaim; include test plan with synthetic tenants, burst scenarios, and a rollback path if latency degrades?","answer":"Implement cgroups v2 with per-tenant slices, configure MemoryMax and MemorySwapMax for hard limits, set MemoryHigh for soft limits, pin tenants to dedicated CPUs, and configure io.max and cpu.max for fair resource distribution. Set OOMScoreAdjust to -1000 for critical system processes and higher values for tenant workloads to ensure proper OOM handling.","explanation":"## Why This Is Asked\nAssesses practical resource isolation under multi-tenant load using modern cgroup2 controls and robust rollback strategies.\n\n## Key Concepts\n- cgroups v2 memory controls: memory.max, memory.swap.max, memory.high\n- Per-tenant slices with CPU pinning and I/O throttling\n- OOMScoreAdjust tuning, swap minimization, burst handling\n- Fair reclaim algorithms and page-cache pressure management\n\n## Code Example\n```bash\n# Create per-tenant slice with memory limits\nsystemd-run --slice=tenant-A.slice \\\n  --property=MemoryMax=2G \\\n  --property=MemorySwapMax=1G \\\n  --property=MemoryHigh=1.8G \\\n  --property=CPUQuota=50% \\\n  --property=IOReadBandwidthMax=\"/dev/sda 10M\" \\\n  --property=IOWriteBandwidthMax=\"/dev/sda 10M\" \\\n  /usr/bin/docker run --rm tenant-workload\n\n# Configure OOM score adjustment\necho 500 > /proc/$(pidof tenant-workload)/oom_score_adj\necho -1000 > /proc/$(pidof systemd)/oom_score_adj\n```\n\n## Test Plan\n- Deploy synthetic tenants with varying memory profiles\n- Simulate burst scenarios with stress testing tools\n- Monitor swap usage, OOM events, and latency metrics\n- Validate rollback path via systemd slice removal","diagram":"flowchart TD\n  A[Tenant A] --> B[MemoryMax]\n  A --> C[MemorySwapMax]\n  A --> D[MemoryHigh]\n  Host --> E[vm.swappiness]\n  F[IO/CPU quotas] --> G[Fair Reclaim]","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T05:15:09.290Z","createdAt":"2026-01-21T02:31:12.912Z"},{"id":"q-5127","question":"On a fleet of Linux servers running a proprietary, signed kernel module for hardware offload, a critical security fix requires patching without reboot. Outline a production plan to apply a signed live patch (kpatch, livepatch) or hotfix module replacement, including patch prep, module signing (CONFIG_MODULE_SIG), orchestration, validation, and rollback steps?","answer":"Prepare patch from vendor, sign it with the kernel signing key (CONFIG_MODULE_SIG), load via kpatch or kernel livepatch with a canary, roll out through Ansible, and monitor health. Validate with the p","explanation":"## Why This Is Asked\nThis tests real-world lifecycle for hotpatching kernel modules on fleets with minimal downtime, enforcing module signing, and safe rollback.\n\n## Key Concepts\n- Kernel module signing and lockdown\n- Live patching tools: kpatch, kernel livepatch\n- Canary deployments and rollback mechanisms\n- Orchestration and validation pipelines\n\n## Code Example\n```bash\n# High-level steps (not production-ready)\ngrep CONFIG_MODULE_SIG /boot/config-$(uname -r)\nkpatch create fix.patch --base baseline.ko --patch patch.ko\nopenssl dgst -sha256 -sign private.pem -out patch.sig patch.ko\nkpatch load patch.ko\n# health check\ncurl -f http://localhost/health\n```\n```\n\n## Follow-up Questions\n- How would you handle a patch that causes instability on the canary?\n- How would you validate performance impact (latency/throughput) before full rollout?","diagram":null,"difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Instacart","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T07:15:21.032Z","createdAt":"2026-01-21T07:15:21.032Z"},{"id":"q-5158","question":"On a Linux host serving a multi-tenant artifact cache, hundreds of tenants perform bursts of tiny file creates/deletes in a shared directory, causing inode pressure and directory metadata contention that spikes latency to 100â€“300 ms. Design a production plan to diagnose and mitigate metadata bottlenecks without sacrificing correctness. Include filesystem strategy, per-tenant isolation, tuning, and a test/rollback plan?","answer":"Isolate per-tenant metadata by giving each tenant its own subvolume/file system (ext4/XFS) with dir_index enabled and, if feasible, separate inode tables; tune inode density, raise fs.file-max and ino","explanation":"## Why This Is Asked\nInterviews hinge on metadata bottlenecks in multi-tenant FS; this tests isolation, inode management, and practical test plans.\n\n## Key Concepts\n- Inode density and metadata locality\n- Per-tenant namespaces (subvols) and quotas\n- Dir_index, separate inode tables, and validation\n\n## Code Example\n```javascript\n// placeholder for demonstration; real steps in explanation\n```\n\n## Follow-up Questions\n- How would you rollback if metadata regression occurs?\n- How would this scale to thousands of tenants?","diagram":"flowchart TD\n  A[Tenant workloads] --> B{Metadata bottleneck}\n  B --> C[Per-tenant subvols/namespaces]\n  B --> D[Enable dir_index and adjust inode density]\n  C --> E[Separate partitions or pools]\n  D --> F[Monitoring: inode usage, dir cache hits]\n  E --> G[Test plan: ioping, fio metadata patterns]\n  F --> H[Rollback by merging subvols]","difficulty":"advanced","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","PayPal","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T08:50:28.634Z","createdAt":"2026-01-21T08:50:28.634Z"},{"id":"q-5269","question":"On a Linux host serving a high-throughput, multi-tenant telemetry ingestion pipeline (UDP, many tenants with bursts), tail latency spikes during peak load. Design a production plan to guarantee per-tenant latency isolation for the ingestion path using XDP/BPF, per-tenant queues, and cgroups v2. Include NIC offloads, isolcpus, and a test plan with netperf?","answer":"Pin a small CPU set to the ingest path (isolcpus, nohz_full) and dedicate a NIC queue per tenant via XDP_REDIRECT to per-tenant BPF rings. Use an eBPF program that routes flows to tenant maps, enforce","explanation":"## Why This Is Asked\nTests ability to design isolation in software-defined networking within a Linux multi-tenant ingest stack, focusing on practical BPF/XDP, NIC tuning, and QoS.\n\n## Key Concepts\n- XDP/BPF for flow-level tenant routing\n- Per-tenant queuing and pacing\n- cgroups v2 IO control and CPU affinity\n- NIC offloads and kernel tuning for latency\n\n## Code Example\n```c\n// Skeleton: BPF program outline for per-tenant redirect\n#include <linux/bpf.h>\n#include <bpf/bpf_helpers.h>\nSEC(\"xdp\")\nint tenant_redirect(struct xdp_md *ctx) {\n  // extract tenant from hash of 5-tuple and redirect to per-tenant queue\n  return XDP_REDIRECT; // placeholder\n}\n\nchar _license[] SEC(\"license\") = \"GPL\";\n```\n\n## Follow-up Questions\n- How would you measure and bound tail latency under 99th percentile SLA?\n- What failure paths require automatic rollback and how would you implement it?","diagram":null,"difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Goldman Sachs","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T14:39:31.732Z","createdAt":"2026-01-21T14:39:31.734Z"},{"id":"q-5566","question":"On a Linux host running multiple services (nginx and a custom worker) with shared config under /etc and logs under /var, design a minimal AppArmor confinement plan to sandbox the two binaries. Provide steps for enabling enforce mode, generating/refining profiles with aa-genprof, applying aa-enforce, and a rollback path if issues occur. Include concrete commands and a practical test plan?","answer":"Enable AppArmor confinement for two services: nginx and a custom worker. Commands: sudo apt-get update && sudo apt-get install apparmor apparmor-utils; sudo systemctl enable apparmor; sudo aa-status; ","explanation":"## Why This Is Asked\n\nThis checks practical containment skills using AppArmor in a beginner-to-intermediate context, focusing on safe rollout and rollback.\n\n## Key Concepts\n\n- AppArmor enforcement vs complain mode\n- aa-genprof, aa-status, aa-enforce\n- rollback testing and audit review\n\n## Code Example\n\n```javascript\n// No code required; AppArmor profile is in /etc/apparmor.d/\n```\n\n## Follow-up Questions\n\n- What would you do if nginx requires additional file access during startup?\n- How would you verify no privilege escalation paths remain?","diagram":null,"difficulty":"beginner","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T06:50:38.462Z","createdAt":"2026-01-22T06:50:38.462Z"},{"id":"q-5604","question":"On a Linux host serving a multi-tenant ML inference service, tenants share a single NIC and kernel stack. Bursty traffic causes tail latency spikes for some tenants. Design a production plan to enforce per-tenant network isolation and fairness using XDP/eBPF (maps for per-tenant quotas), tc with clsact, and cgroup v2 networking. Include topology, per-tenant queues, policing/dropping rules, rollback steps, and a concrete validation plan?","answer":"Engineer a per-tenant network control path using XDP/eBPF on the host NIC with maps keyed by tenant_id to enforce rate limits and drop policy; pair each tenant to dedicated veth pairs and attach a cls","explanation":"## Why This Is Asked\nThis tests practical, production-grade network isolation under burst traffic for multi-tenant workloads.\n\n## Key Concepts\n- XDP/eBPF maps for per-tenant quotas\n- tc clsact and qdisc shaping\n- cgroup v2 networking isolation\n- validated rollback strategies\n\n## Code Example\n```javascript\n// Placeholder: show wiring of map lookups and attach XDP prog\n```\n\n## Follow-up Questions\n- How would you monitor for misconfigurations and roll back safely?\n- How would you extend to support dynamic tenant onboarding?","diagram":"flowchart TD\n  A[Tenants & NIC] --> B[XDP policing]\n  B --> C[tc/qdisc shaping]\n  C --> D[per-tenant queues]\n  D --> E[validation & metrics]","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Lyft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T08:00:55.598Z","createdAt":"2026-01-22T08:00:55.598Z"},{"id":"q-5697","question":"On a Linux host running dozens of concurrent CI agents as systemd services, each agent writes logs to /var/log/ci/tenant-<id> and bursts cause IO contention and intermittent latency. Design a practical plan to guarantee per-tenant log isolation, implement per-tenant log rotation, and bound I/O without changing agent code. Include systemd unit changes, logrotate config, quota plan, and a test/rollback procedure?","answer":"Implement per-agent isolation by running each CI agent in its own systemd unit with PrivateDevices, PrivateTmp, and ReadOnlyPaths; mount /var/log/ci as a dedicated volume with quotas; use logrotate pe","explanation":"## Why This Is Asked\nMulti-tenant logging can cause IO contention and latency spikes when many small writers share a single path. This question tests practical isolation, rotation, and QoS strategies without changing app code.\n\n## Key Concepts\n- Systemd sandboxing: PrivateTmp, PrivateDevices, ReadOnlyPaths\n- Per-tenant log isolation and rotation using logrotate\n- Disk I/O control: ionice and IO scheduling\n- Quotas: per-tenant quotas on a dedicated log volume (e.g., project quotas on XFS)\n- Test plan: simulate bursts and verify rollback safety\n\n## Code Example\n```ini\n; /etc/systemd/system/ci-agent@.service\n[Unit]\nDescription=CI Agent for tenant %i\n\n[Service]\nExecStart=/usr/local/bin/ci-agent --tenant=%i\nPrivateTmp=true\nPrivateDevices=true\nReadOnlyPaths=/var/log/ci\nAmbientCapabilities=CAP_SYSLOG\nIOSchedulingClass=idle\nIOSchedulingPriority=7\n```\n\n```bash\n; /etc/logrotate.d/ci-agent-%i\n/var/log/ci/tenant-*/agent.log {\n  size 50M\n  rotate 7\n  copytruncate\n  missingok\n  notifempty\n}\n```\n\n## Follow-up Questions\n- How would you validate per-tenant isolation under peak bursts? \n- Which metrics would you monitor to detect drift or misconfigurations, and how would you roll back safely?","diagram":"flowchart TD\n  A[Tenant isolation] --> B[Per-tenant log dirs]\n  B --> C[Quota & IO scheduling]\n  C --> D[Test bursts]\n  D --> E[Rollback if needed]","difficulty":"beginner","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Discord","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T11:44:12.048Z","createdAt":"2026-01-22T11:44:12.048Z"},{"id":"q-5725","question":"On a Linux host running a multi-tenant streaming ingest service (shared NVMe pool), sporadic 1â€“2s pauses occur under bursts. Design a production plan to diagnose and mitigate using eBPF-based observability and per-tenant I/O throttling. Specify: probes (tracepoints/kprobes), metrics (per-tenant latency, stall duration, queue depth), tenancy mapping (cgroup v2 io, io.max), dashboards, and a safe rollback strategy. Include a test script using fio and cyclictest to validate latency isolation?","answer":"Plan uses eBPF: attach probes to blk_start_request, blk_mq_request_issue, and iowait; tag events by cgroup v2 tenant IDs; emit per-tenant latency, stall duration, and queue depth to Prometheus. Enforc","explanation":"## Why This Is Asked\nTests practical use of intrusive observability (eBPF) and real I/O isolation in a multi-tenant setting, plus a rollback strategy.\n\n## Key Concepts\n- eBPF tracing (tracepoints/kprobes) for block layer I/O\n- Per-tenant mapping via cgroup v2 identifiers\n- IO throttling with io.max and optional BPF-based throttling\n- Metrics pipeline to Prometheus/Grafana\n- Safe rollback and minimal overhead\n\n## Code Example\n```c\n// pseudo eBPF sketch (conceptual)\nstruct bpf_map_def SEC(\"maps\") tenant_latency = {\n  .type = BPF_MAP_TYPE_HASH,\n  .key_size = sizeof(u32),\n  .value_size = sizeof(struct metrics),\n  .max_entries = 1024,\n};\n```\n\n## Follow-up Questions\n- How would you adapt quotas for tenants with bursty workloads?\n- How do you quantify eBPF overhead and ensure it remains under control?","diagram":"flowchart TD\n  A[Ingest Service] --> B[eBPF Probes]\n  B --> C[Per-Tenant Metrics]\n  C --> D[Prometheus/Grafana]\n  D --> E[IO Quotas (io.max)]\n  E --> F[Test & Rollback]\n  F --> G[Back to Baseline]","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Lyft","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T13:19:59.923Z","createdAt":"2026-01-22T13:19:59.923Z"},{"id":"q-5792","question":"On a Linux host that runs a cloud-native CI agent pool sharing an NFS-backed build cache, bursts of parallel builds cause cache latency spikes and occasional timeouts. Propose a production plan to isolate and throttle the critical path across tenants using per-tenant I/O quotas, CPU pinning, and NUMA awareness. Include kernel/FS choices, a test plan with fio and cyclictest, and a rollback path?","answer":"Isolate per-tenant I/O and CPU on a shared NFS-backed build cache: create a cgroup v2 tree for tenants, set io.max to throttle each, assign CI workers to a dedicated CPU set, pin processes with a syst","explanation":"## Why This Is Asked\n\nTests ability to design per-tenant I/O isolation and CPU pinning in a real-world CI/cache scenario.\n\n## Key Concepts\n\n- cgroup v2 io.max and CPU quotas\n- cpuset / systemd slice for per-tenant isolation\n- NUMA awareness and memory locality\n- Validation with fio, iostat, cyclictest\n\n## Code Example\n\n```bash\n# Example commands for isolation (illustrative)\nsystemd-run --slice=buildcache-tenantA.slice --unit=ci-agent-A --scope /usr/local/bin/ci-agent --config tenantA.yaml\n```\n\n## Follow-up Questions\n\n- How would you extend to multiple NICs with IRQ affinity?\n- How would you detect and handle ballooning by the host?","diagram":"flowchart TD\n  BurstLoad[Burst Load] --> Isolation[Isolate I/O & CPU per tenant]\n  Isolation --> Test[Run fio, iostat, cyclictest]\n  Test --> Validation{SLA met?}\n  Validation -->|Yes| Deploy\n  Validation -->|No| Rollback","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Microsoft","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T16:59:46.290Z","createdAt":"2026-01-22T16:59:46.290Z"},{"id":"q-5877","question":"On a Linux host running a multi-tenant streaming pipeline where logs must be strictly time-ordered across containers, design a production plan to achieve precise per-container time synchronization using hardware timestamping and PTP 1588. Include NIC support, ptp4l/phc2sys, chrony integration, per-namespace clock exposure, and a verification plan with drift tests?","answer":"Enable hardware timestamping on a NIC with a PHC, run ptp4l to lock the hardware clock, use phc2sys to sync system time, and configure chrony as a local master for container clocks. Expose per-namespa","explanation":"## Why This Is Asked\nDemonstrates depth in low-latency timekeeping, hardware timestamping, and container clock isolation; tests ability to architect across NICs, kernel, and user-space.\n\n## Key Concepts\n- IEEE 1588 PTP and hardware timestamping\n- PHC, ptp4l, phc2sys\n- chrony with local master\n- per-container clocks and namespaces\n- verification: drift measurement, synthetic bursts\n\n## Code Example\n```javascript\n// Example: ptp4l and phc2sys commands\nptp4l -i eth0 -m\nphc2sys -s eth0 -w -v\n```\n\n## Follow-up Questions\n- How would you handle NICs without hardware timestamps?\n- How to scale to 1000+ containers?","diagram":"flowchart TD\n  A[PHC device] --> B[ptp4l]\n  B --> C[phc2sys]\n  C --> D[chrony local master]\n  A --> E[Container clocks exposure]","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Citadel"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T20:38:46.143Z","createdAt":"2026-01-22T20:38:46.145Z"},{"id":"q-6020","question":"On a production fleet of Linux servers, apply a critical security kernel patch without reboot. Design a zero-downtime rollout plan using live patching across 100+ hosts. Describe validation, canary strategy, phased rollout, monitoring, and rollback. Include module compatibility checks and SLA impact for latency-sensitive services?","answer":"Plan: validate patch in CI against prod-like workloads; deploy to a canary group (5%), run automated smoke and latency tests for several hours; then roll to 20%, 50%, and 100%. Monitor patch state, SL","explanation":"## Why This Is Asked\n\nTests ability to plan production-grade live kernel patch rollouts without downtime and with measurable SLAs.\n\n## Key Concepts\n\n- Live patching tooling (kpatch, kernel livepatch)\n- Canary deployments and phased rollouts\n- Validation, monitoring, and automated rollback\n- Module compatibility and reboot safety\n\n## Code Example\n\n```bash\n# illustrative commands\nkpatch load /patches/patch-2026-01-01.ko\n```\n\n## Follow-up Questions\n\n- How would you handle heterogeneous kernel versions? \n- What metrics define a safe rollback threshold?","diagram":"flowchart TD\n  A[CI tests patch] --> B[Canary rollout]\n  B --> C[Telemetry & tests]\n  C --> D[Staged rollout]\n  D --> E[Full rollout]\n  E --> F[Monitor & SLA]\n  F --> G[Rollback if needed]","difficulty":"advanced","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Oracle","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T05:36:17.542Z","createdAt":"2026-01-23T05:36:17.544Z"},{"id":"q-6103","question":"On a Linux host running mixed workloads (real-time audio path on NUMA node0 and analytics on node1), tail latency spikes occur due to cross-NUMA memory access and interconnect contention. Design a production plan to enforce strict NUMA locality and interconnect fairness: hardware topology checks, BIOS NUMA settings, kernel boot params (isolcpus, nohz_full), IRQ affinity, CPU/memory binding, cgroups v2 usage for CPU/membind, and a test plan with cyclictest and synthetic multi-channel load. What exact steps would you take and how would you verify?","answer":"Isolate the RT path on NUMA node0 and bind CPUs/memory tightly. Boot with isolcpus=0-7 nohz_full=0-7; route IRQ to node0; disable irqbalance. Put RT tasks in rt.slice with SCHED_FIFO 99. Bind with num","explanation":"## Why This Is Asked\nInterleaved workloads on multi-NUMA systems cause tail latency spikes in real-time paths. This question probes practical NUMA-aware isolation, IRQ routing, and disciplined binding under production pressures.\n\n## Key Concepts\n- NUMA locality and memory binding\n- IRQ affinity and interrupt steering\n- isolcpus and nohz_full for RT isolation\n- cgroups v2 for resource containment\n- scheduler choices (SCHED_FIFO) and real-time guarantees\n\n## Code Example\n```bash\n# Example bindings (illustrative)\n# Boot: isolcpus=0-7 nohz_full=0-7\n# Bind RT task:\nnumactl --physcpubind=0-7 --membind=0 -p <rt_pid>\n```\n\n## Follow-up Questions\n- How would you adapt if node0 has degraded memory bandwidth?\n- What instrumentation would you add to detect regression after kernel upgrades?","diagram":"flowchart TD\n  RT[RT Path] --> Node0[NUMA Node0]\n  RT --> Node1[NUMA Node1]\n  Node0 --> Isolate[Isolated CPUs 0-7]\n  Node0 --> Mem[Memory bound to node0]\n  RT --> IRQ[IRQ affinity to node0]","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","Snap","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T08:56:15.018Z","createdAt":"2026-01-23T08:56:15.019Z"},{"id":"q-6157","question":"On a Linux host hosting multiple services, journald is saturating CPU when high-rate logs are produced. Design a beginner-friendly plan to reduce log noise while preserving critical events, using journald tuning and a TLS-backed central log collector. Include concrete steps with journald.conf changes, a simple rsyslog forwarder config, and a basic test plan?","answer":"Limit and forward logs to central server with TLS. Steps: 1) journald.conf: RateLimitInterval=1s; RateLimitBurst=100; SystemMaxUse=2G; ForwardToSyslog=yes. 2) rsyslog: enable imjournal, configure TLS ","explanation":"## Why This Is Asked\n\nTests practical knowledge of log throughput, journald.conf tuning, and TLS log forwarding, with emphasis on preserving essential messages while reducing noise.\n\n## Key Concepts\n\n- journald rate limiting (RateLimitInterval, RateLimitBurst)\n- log rotation and storage (SystemMaxUse)\n- rsyslog/imjournal forwarding and TLS\n- testing log pipelines with bursts and verification\n\n## Code Example\n\n```javascript\n# Example journald.conf snippet\n[Journal]\nSystemMaxUse=2G\nRateLimitInterval=1s\nRateLimitBurst=100\nForwardToSyslog=yes\n```\n\n## Follow-up Questions\n\n- How would you validate no loss of critical alerts during bursts?\n- How would you handle central log server unavailability and retry behavior?","diagram":null,"difficulty":"beginner","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","IBM","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T11:10:17.978Z","createdAt":"2026-01-23T11:10:17.978Z"},{"id":"q-6210","question":"On a Linux host acting as a shared build farm for multiple tenants, each tenant runs their jobs inside rootless containers; two tenants recently observed cross-tenant file descriptor leakage under heavy I/O bursts. Design a production plan to enforce strict isolation: use user namespaces, drop capabilities, seccomp-bpf filters, AppArmor/SELinux, and cgroup v2 with per-tenant slices and io.max; include a concrete test plan with synthetic tenants and rollback?","answer":"Implement strict isolation: run each tenant in a per-tenant user namespace with rootless containers, drop all capabilities, enforce a seccomp-bpf filter and AppArmor/SELinux profile, and place each te","explanation":"## Why This Is Asked\n\nThis question probes practical containment and isolation in shared Linux build farms, focusing on namespaces, security policies, and per-tenant QoS.\n\n## Key Concepts\n\n- User namespaces and rootless containers\n- Seccomp-bpf and AppArmor/SELinux\n- Cgroup v2 slices and io.max\n- Capability dropping and read-only rootfs\n- Monitoring with eBPF and rollback strategies\n\n## Code Example\n\n```bash\n# Example: run a tenant job with strict isolation\npodman run --rm --security-opt seccomp=seccomp.json --cap-drop=ALL --security-opt apparmor=tenantA /tenant-image\n```\n\n## Follow-up Questions\n\n- How would you validate leakage is prevented in production?\n- How would you rollback if isolation fails?\n","diagram":null,"difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Instacart","LinkedIn"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T14:43:20.608Z","createdAt":"2026-01-23T14:43:20.608Z"},{"id":"q-6244","question":"On a Linux host serving a multi-tenant FaaS platform where tenants execute untrusted code in lightweight sandboxes, design a production plan to enforce strict per-tenant filesystem and process isolation while preserving latency. Specify: 1) namespace strategy (user, mount, pid, network), 2) per-tenant root layout (chroot/bind mounts/overlay), 3) sandbox escape mitigations (disable setns, tighten CAPs, seccomp-bpf), 4) auditing with auditd and eBPF, 5) test plan including a brute-force escape attempt and rollback, 6) a concrete per-tenant policy example and a runbook. How would you implement this?","answer":"Implement per-tenant user, mount, pid, and network namespaces; mount a private root via a read-only bind of a tenant base plus an overlay for writable state. Drop capabilities, enable seccomp-bpf, and","explanation":"## Why This Is Asked\nTests practical, scalable tenant isolation on Linux hosts, balancing security with latency. It demands concrete, implementable namespace layouts, filesystem strategies, and verification steps.\n\n## Key Concepts\n- Namespaces: user, mount, pid, net\n- Filesystem isolation: per-tenant roots via bind mounts/overlay\n- Security: restricted CAPs, seccomp-bpf, disable setns, module signing\n- Observability: auditd, BPF tracing, runbooks\n\n## Code Example\n```javascript\n// Placeholder: concept sketch only\n```\n\n## Follow-up Questions\n- How would you adapt this for a mixed multi-tenant and dedicated-tenant model? \n- What metrics indicate isolation integrity is compromised, and how would you trigger a rollback?","diagram":null,"difficulty":"advanced","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","DoorDash","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T16:08:06.356Z","createdAt":"2026-01-23T16:08:06.356Z"},{"id":"q-6257","question":"On a Linux host running a multi-tenant CI build farm using ZFS on Linux, builds intermittently pause for 50-200ms during peak bursts. The culprit appears to be ARC and L2ARC pressure from aggressive metadata caching. Design a production plan to diagnose and mitigate caching bottlenecks without compromising data integrity. Include ZFS tunables, per-tenant memory isolation, test plan, and rollback?","answer":"Begin by instrumenting ARC/L2ARC with kstat to track hits, misses, and per tenant memory usage. Then tune zfs_arc_min and zfs_arc_max to a safe fraction of RAM, cap L2ARC size, and consider disabling ","explanation":"## Why This Is Asked\nThe question probes practical cache tuning under multi-tenant load, not basic disk I/O.\n\n## Key Concepts\n- ARC/L2ARC behavior in ZoL\n- tunables: zfs_arc_min/max, zfs_prefetch, vfs_cache_pressure\n- per-tenant isolation via cgroups\n- test plan with synthetic workloads and rollback\n\n## Code Example\n```javascript\n// Example approach to adjust ARC settings\n// (run with caution on live systems)\n```\n\n## Follow-up Questions\n- How would you validate no data loss during rollback?\n- What metrics decide success of tuning?","diagram":"flowchart TD\n  ARC pressure --> Latency spike\n  Latency spike --> Tune ARC/L2ARC + per-tenant isolation\n  Tune ARC/L2ARC + per-tenant isolation --> Validate\n  Validate --> Rollback","difficulty":"advanced","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Google","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T16:45:39.890Z","createdAt":"2026-01-23T16:45:39.890Z"},{"id":"q-6463","question":"On a Linux host running a multi-tenant data-processing cluster with containers pinned to NUMA nodes, peak load causes latency spikes due to memory locality. Design a production plan to diagnose and fix NUMA-related bottlenecks while preserving tenant isolation. Include topology validation (lscpu, lstopo), per-tenant binding (cpuset, memory.high), memory interleaving vs per-node binding strategy, kernel settings (numa_balancing, numactl bias), and a test/rollback plan with SLA targets and controlled rollout?","answer":"Plan: verify NUMA topology with lscpu/lstopo, identify hot memory regions and tenant-node mapping, then enforce cpuset-based CPU affinity and per-tenant memory.high with cgroup v2. Prefer node-local allocation for latency-sensitive tenants, use interleaved memory for throughput-focused workloads. Configure numa_balancing=0 for predictable behavior, apply numactl bias where needed, and implement per-tenant monitoring with p95 latency targets.","explanation":"## Why This Is Asked\n\nExplores NUMA awareness, container isolation, and dynamic reconfiguration in production, highlighting practical trade-offs and rollback safety.\n\n## Key Concepts\n\n- NUMA topology, memory locality, and cross-node access costs\n- cpuset and cgroups v2 memory controllers (memory.high, memory.min)\n- Per-tenant binding strategies: node-local vs interleaved allocations\n- numactl/libnuma for biased allocations; topology discovery with lscpu, lstopo\n- Instrumentation: latency/throughput metrics (p95, tail latency), perf tools, synthetic workloads\n\n## Code Example\n\n```bash\n# Bind a","diagram":null,"difficulty":"advanced","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","NVIDIA","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T04:38:16.011Z","createdAt":"2026-01-24T02:38:47.601Z"},{"id":"q-6488","question":"On a Linux host running a PostgreSQL cluster with streaming replication, nightly backups cause I/O spikes that degrade primary write latency. Design a production plan to isolate backups from the main I/O path while preserving data coherence. Include storage topology, IO isolation (cgroups v2 io.max), per-task scheduling, WAL archiving strategy, and a test/rollback plan with realistic workloads?","answer":"Plan: isolate backups to a dedicated block device pool and throttle with cgroups v2 io.max; pin backup tasks to a reserved CPU set and use a fixed I/O scheduler (deadline); separate WAL archiving path","explanation":"## Why This Is Asked\nTests ability to design production-grade I/O isolation for mixed PostgreSQL workloads, balancing backup safety with primary latency.\n\n## Key Concepts\n- I/O isolation with cgroups v2 io.max\n- Separate storage tiers for WAL/DB and archives\n- Per-task CPU pinning and fixed I/O scheduling\n- WAL archiving strategy and coherence guarantees\n- Realistic benchmarking (fio) and rollback procedures\n\n## Code Example\n```javascript\n// Pseudo-commands illustrating setup (use appropriate paths and syntax for your env)\n// 1) create dedicated IO cgroup for backups\n// 2) set io.max to limit bandwidth for that group\n// 3) pin backup process to a reserved CPU set\n```\n\n## Follow-up Questions\n- How would you monitor and alert if backup I/O encroachment reappears?\n- What changes if backups run on a replica instead of primary?","diagram":null,"difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T04:27:48.212Z","createdAt":"2026-01-24T04:27:48.212Z"},{"id":"q-6523","question":"On a Linux host executing a shared CI workload for multiple tenants, a bursty CPU-bound job from Tenant A intermittently starves others. Propose a beginner-friendly plan to enforce fair CPU usage using cgroup v2; create per-tenant slices, apply cpu.max quotas, move jobs into slices, and validate with reproducible bursts. Include concrete commands and rollback steps?","answer":"Create a cgroup v2 per-tenant slice and cap CPU with cpu.max. For four tenants, set cpu.max to 25000 100000 in each slice and move each job PID into its slice via cgroup.procs. Run parallel CPU-bound ","explanation":"## Why This Is Asked\n\nTests practical understanding of Linux resource isolation using cgroup v2, not just theory. It checks ability to translate a multi-tenant burst problem into per-tenant quotas, safe process placement, and rollback plans.\n\n## Key Concepts\n\n- cgroup v2 unified hierarchy and cpu.max\n- per-tenant slices and cgroup.procs\n- quota math for fair share (e.g., 4 tenants, 25% each)\n- reproducible benchmarking and rollback safety\n\n## Code Example\n\n```bash\n# setup per-tenant slices and quotas\nmkdir -p /sys/fs/cgroup/tenant-A\nmkdir -p /sys/fs/cgroup/tenant-B\necho '25000 100000' > /sys/fs/cgroup/tenant-A/cpu.max\necho '25000 100000' > /sys/fs/cgroup/tenant-B/cpu.max\n# attach a running PID to Tenant A\necho 12345 > /sys/fs/cgroup/tenant-A/cgroup.procs\n```\n\n## Follow-up Questions\n\n- How would you monitor and verify fairness in production?\n- How would you handle a tenant requiring bursts above quota?","diagram":null,"difficulty":"beginner","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T06:52:32.097Z","createdAt":"2026-01-24T06:52:32.098Z"},{"id":"q-6705","question":"On a Linux host running thousands of multi-tenant containers, you want to instrument per-tenant kernel observability with eBPF, while keeping overhead under 5% and guaranteeing tenant isolation. Design a production plan to deploy a per-tenant, secure eBPF tracing agent that collects stack samples and I/O patterns. Include kernel prerequisites, BPF program layout, how to map tenants to per-tenant maps, data aggregation, security controls, testing and rollback?","answer":"Use unprivileged BPF via libbpf+bpftool, with per-tenant maps keyed by tenant_id; attach only a few tracepoints and small kprobes to minimize overhead; use ring buffers for per-tenant samples and rate","explanation":"## Why This Is Asked\nThis question probes practical use of eBPF for tenant-isolated observability with minimal overhead.\n\n## Key Concepts\n- eBPF, unprivileged BPF, per-tenant maps, libbpf, rate limiting, CPU pinning, security isolation.\n\n## Code Example\n```javascript\n// pseudo: bpftool load and attach, per-tenant map creation and usage\n```\n\n## Follow-up Questions\n- How would you measure per-tenant overhead and enforce a budget?\n- How would you handle dynamic tenant churn and map cleanup?\n","diagram":"flowchart TD\n  Start[Start] --> CheckPrereqs[Check kernel prereqs]\n  CheckPrereqs --> Deploy[Deploy per-tenant BPF harness]\n  Deploy --> Collect[Collect & aggregate by tenant]\n  Collect --> Verify[Verify isolation & latency goals]\n  Verify --> Rollback[Rollback strategy if targets missed]","difficulty":"advanced","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","Plaid","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T14:03:41.157Z","createdAt":"2026-01-24T14:03:41.157Z"},{"id":"q-6743","question":"On a Linux host running a multi-tenant artifact/cache service backed by a single NVMe pool, peak bursts from one tenant trigger heavy small writes that push the critical artifact path into tail latency spikes. Design a production plan to guarantee deterministic write latency for the critical path while preserving tenant isolation and data correctness. Include storage topology (per-tenant namespaces, pools, or separate mounts), I/O policy (io.max, per-tenant queues), CPU/memory isolation (cgroups v2, memory.policy), and a test plan with synthetic tenants and rollback/roll-forward criteria?","answer":"Create a dedicated NVMe namespace or pool for the critical artifact path; assign each tenant a fixed io.max in a separate cgroup2 subtree; pin the critical write processes to a reserved CPU core set; ","explanation":"## Why This Is Asked\nTests ability to design precise storage isolation and per-tenant QoS for high-throughput multi-tenant workloads, with concrete controls (NVMe namespaces, cgroupv2 io.max, CPU pinning, and I/O schedulers) and a realistic rollback path.\n\n## Key Concepts\n- Per-tenant I/O QoS using cgroup v2 io.max\n- Storage topology choices for isolation (NVMe namespaces, pools, separate mounts)\n- Latency measurement and rollback strategies\n\n## Code Example\n\n```bash\n# sample per-tenant io.max configuration (pseudo-example)\nmkdir -p /sys/fs/cgroup/io/tenant-a\necho 8:0 1048576 > /sys/fs/cgroup/io/tenant-a/io.max\n# attach critical writer to tenant-a cgroup\necho <pid> > /sys/fs/cgroup/io/tenant-a/cgroup.procs\n```\n\n## Follow-up Questions\n- How would you monitor and alert on tail latency spikes?\n- What changes would you make to handle multi-tenant bursts dynamically without manual intervention?","diagram":"flowchart TD\n  A[Tenant burst] --> B[Apply I/O isolation]\n  B --> C[Measure latency]\n  C --> D{Within SLA?}\n  D -- Yes --> E[Continue operation]\n  D -- No --> F[Rollback/adjust quotas]\n  F --> G[Notify and report]","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Databricks","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T15:36:15.655Z","createdAt":"2026-01-24T15:36:15.655Z"},{"id":"q-6816","question":"On a Linux host hosting a multi-tenant container cluster, tail startup latency spikes during bursts despite ample CPU; design a production plan to identify and mitigate startup latency using low-overhead tracing (eBPF) to profile container lifecycle events (namespace creation, cgroup setup, mounts, network namespace) and enforce per-tenant budgets? Include data collection, tenant-scoped tracing, rollout criteria, and rollback strategy?","answer":"Develop a low-overhead eBPF tracing plan to profile container lifecycle events (namespace creation, cgroup setup, mounts, network namespace) during startup bursts. Tag events with per-tenant identifie","explanation":"## Why This Is Asked\nTests ability to design advanced observability with safe overhead, tenant isolation, and rollback.\n\n## Key Concepts\n- eBPF tracepoints, container lifecycle, per-tenant tagging, tail-latency analysis, rollout and rollback.\n\n## Code Example\n```javascript\n// Pseudo-eBPF sketch: tag events with tenant and record startup latency\ntracepoint(\"container_start\", (ctx) => {\n  const tenant = getCurrentTenantId();\n  recordLatency(tenant, now() - startTime);\n});\n```\n\n## Follow-up Questions\n- How would you validate that tracing does not perturb startup times under load?\n- What metrics and dashboards would you expose to operators?","diagram":"flowchart TD\n  A[Start Burst] --> B[Trace container_start]\n  B --> C[Tag by tenant]\n  C --> D[Aggregate latency]\n  D --> E[Apply budgets or throttling]\n  E --> F[Rollout or rollback]","difficulty":"advanced","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","LinkedIn"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T18:49:41.238Z","createdAt":"2026-01-24T18:49:41.238Z"},{"id":"q-6831","question":"On a single Linux host with a shared workspace at /shared/workspaces used by multiple tenants for CI builds, there is sudden churn of small files causing uneven disk usage and risk of one tenant exhausting space. Design a beginner-friendly plan to implement per-tenant quotas on a shared filesystem (ext4 or XFS), covering enabling quotas, deciding user vs group quotas, setting soft/hard limits, monitoring, and a rollback/test plan?","answer":"Enable per-tenant quotas on the shared filesystem (ext4/XFS). Add usrquota,grpquota in /etc/fstab for /shared and remount; run quotacheck -cug /shared and quotaon /shared. Use setquota -u tenantA 5000","explanation":"## Why This Is Asked\n\nTests knowledge of filesystem quotas, per-tenant isolation, and practical admin steps to implement access control on shared storage.\n\n## Key Concepts\n\n- Disk quotas (user/group) on ext4/XFS\n- /etc/fstab changes and remounts\n- quota utilities: quotacheck, quotaon, edquota, setquota, repquota\n- Monitoring, alerting, and rollback procedures\n\n## Code Example\n\n```bash\n# Determine filesystem\ndf -T /shared\n\n# Enable quotas\nsudo sed -i '/\\/shared\\s.* / s|defaults|defaults,usrquota,grpquota|' /etc/fstab\nsudo mount -o remount /shared\nsudo quotacheck -cug /shared\nsudo quotaon /shared\n# Set quota for tenantA (KB)\nsudo setquota -u tenantA 50000000 60000000 0 0 /shared\n# Verify\nsudo repquota -a\n```\n\n## Follow-up Questions\n\n- What are trade-offs between per-user vs per-group quotas? How would you automate the quota config with config management?\n- How would you handle tenants with multiple accounts or shared workspaces across volumes?\n- How would you alert on quota approaching limits and enforce graceful degradation?","diagram":"flowchart TD\n  A[Identify tenants] --> B[Enable quotas on /shared]\n  B --> C[Set limits per tenant]\n  C --> D[Monitor usage]\n  D --> E[Rollback plan]","difficulty":"beginner","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","IBM","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T19:24:02.337Z","createdAt":"2026-01-24T19:24:02.337Z"},{"id":"q-6937","question":"On a Linux host running a multi-tenant microservices platform (Docker/K8s, hundreds of tenants), tail latency spikes under mixed workloads due to CPU/cache contention. Design a production plan to achieve deterministic observability and per-tenant isolation using eBPF, cgroups v2, and perf tooling. Include per-tenant latency attribution, overhead budgeting, instrumentation points, test plan, rollout and rollback?","answer":"Implement per-tenant latency tracing with eBPF: attach to sched_switch and key events, map samples to tenants via cgroup v2 paths; accumulate p95/p99 latency in a BPF map keyed by tenant ID; keep overhead under 2% CPU and 1% memory through sampling and map size limits.","explanation":"## Why This Is Asked\nTo assess capability to implement low-overhead, tenant-aware observability on a production Linux platform using modern tracing (eBPF), understand trade-offs between accuracy and overhead, and plan safe rollouts.\n\n## Key Concepts\n- eBPF instrumentation hygiene and overhead budgeting\n- cgroups v2 tenant attribution for per-tenant metrics\n- Rollout strategies (canary, feature flags, rapid rollback)\n- Data pipelines for metrics and dashboards\n- Validation and non-disruptive testing\n\n## Code Example\n```javascript\n// Skeleton eBPF program skeleton (C in practice)\nSEC(\"tracepoint\"","diagram":null,"difficulty":"advanced","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Bloomberg","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T05:50:53.558Z","createdAt":"2026-01-24T23:36:31.982Z"},{"id":"q-6985","question":"On a Linux host powering a multi-tenant build farm, intermittent 100â€“300 ms tail latencies arise under bursts. Design a production plan to achieve deterministic scheduling and per-tenant isolation using SCHED_DEADLINE, cpuset/cgroup, isolcpus/nohz_full, and IRQ affinity. Include concrete steps for deployment, test workloads (cyclictest, fio), and a rollback strategy?","answer":"Plan to guarantee deterministic scheduling on a multi-tenant build host by enforcing per-tenant cpuset and memory quotas, pinning build tasks with SCHED_DEADLINE, isolating CPUs with isolcpus and nohz","explanation":"## Why This Is Asked\\nAims to assess ability to architect deterministic scheduling under multi-tenant pressures, balancing performance, isolation, and safety.\\n\\n## Key Concepts\\n- SCHED_DEADLINE for hard real-time-like guarantees\\n- CPU isolation: isolcpus, nohz_full, rcu_nocb\\n- cpuset and memory limits per tenant via cgroups v2\\n- IRQ affinity and timer tick management\\n- Latency validation: cyclictest, fio, perf/bpftrace\\n\\n## Code Example\\n```bash\n# Pin a process to dedicated CPUs 2-3\ntaskset -c 2-3 <pid>\n```\\n\\n## Follow-up Questions\\n- How would you validate no regressions for other tenants?\\n- What failure modes would you simulate and how would you rollback?","diagram":null,"difficulty":"advanced","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Salesforce","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T04:24:13.459Z","createdAt":"2026-01-25T04:24:13.460Z"},{"id":"q-7205","question":"On a Linux host running multiple LXC containers sharing an NVMe drive, one container occasionally stalls during startup under peak I/O. Design a practical plan to diagnose and mitigate per-container I/O bottlenecks while preserving isolation: baseline metrics, map I/O to containers via cgroup io.max or blkio.weight, trace latency with blktrace if needed, apply per-container limits and a suitable disk scheduler, and validate with burst tests plus a rollback path?","answer":"On a Linux host with several LXC containers sharing an NVMe drive, one container stalls during startup under burst I/O. Collect baseline I/O with iostat -dx 1 60; map I/O to containers via cgroup io.m","explanation":"## Why This Is Asked\nTests practical ability to diagnose per-container I/O bottlenecks and apply isolation-aware mitigations in real environments.\n\n## Key Concepts\n- I/O scheduling, cgroups blkio, per-container limits, blktrace/blkparse, NVMe contention\n\n## Code Example\n```javascript\n# baseline\niostat -dx 1 60\n# per-container limit (example for v2)\necho 'k' > /sys/fs/cgroup/io/container_id/io.max\n```\n\n## Follow-up Questions\n- How would you scale this to many containers without starving core services?\n- What are the trade-offs of strict io.max versus dynamic backfill?","diagram":"flowchart TD\n  A[Start] --> B[Baseline metrics]\n  B --> C[Map I/O to containers]\n  C --> D[Apply per-container limits]\n  D --> E[Test bursts]\n  E --> F[Rollback if needed]","difficulty":"beginner","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Goldman Sachs","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T13:46:06.823Z","createdAt":"2026-01-25T13:46:06.823Z"},{"id":"q-7233","question":"On a Linux host with a shared NVMe pool backing multiple tenants, bursts of I/O cause latency spikes of 100-300ms for metadata and seconds for data ops. Design a practical, beginner-friendly plan to diagnose and mitigate I/O scheduling bottlenecks without sacrificing data integrity. Include concrete steps for testing schedulers (mq-deadline vs kyber), enabling multi-queue, per-tenant I/O controls (io.max/io.weight in cgroup v2), and a rollback plan with measurable success criteria?","answer":"Run iostat -dx 1 to identify high await and queue depth, and use blktrace/blkparse to attribute latency to devices. Switch I/O scheduler between mq-deadline and kyber by writing to /sys/block/<dev>/qu","explanation":"## Why This Is Asked\nThis tests practical, beginner-friendly ability to diagnose and mitigate I/O scheduling bottlenecks in a multi-tenant Linux environment using standard observability tools and per-tenant controls, with a safe rollback plan.\n\n## Key Concepts\n- I/O scheduling on NVMe devices (mq-deadline, kyber)\n- cgroup v2 I/O controls (io.max, io.weight)\n- Multi-queue block devices and NUMA awareness\n- Observability: iostat, iotop, blktrace, fio\n- Safe rollback and validation criteria\n\n## Code Example\n```bash\n# switch scheduler\necho kyber > /sys/block/nvme0n1/queue/scheduler\n# verify\ncat /sys/block/nvme0n1/queue/scheduler\n```\n\n## Follow-up Questions\n- How would you verify the per-tenant isolation remains intact under burst?\n- What instrumentation would you add to prevent regressions in future bursts?","diagram":null,"difficulty":"beginner","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","Robinhood","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T14:47:33.609Z","createdAt":"2026-01-25T14:47:33.609Z"},{"id":"q-7391","question":"On a Linux server with three NICs handling a real-time proxy at 10 Gbps, bursts trigger RX IRQ storms and tail-latency spikes in the critical path. Design a production plan to bound latency by isolating interrupts and shaping traffic: dedicate CPU cores to NIC RX queues, pin IRQs, enable RPS/RFS, deploy XDP-based drop/shaping, and implement per-tenant tc filters. Include concrete steps and a test plan with latency measurements?","answer":"Bind RX queues to dedicated CPU cores per NIC, disable IRQ migration by setting appropriate smp_affinity masks for relevant IRQs, enable RPS (Receive Packet Steering) and RFS (Receive Flow Steering) on each NIC to distribute packet processing, implement XDP with a simple pass/drop policy to maintain low latency, and apply tc (traffic control) filters for per-tenant traffic shaping and fair queuing.","explanation":"## Why This Is Asked\nThe scenario probes practical network-path isolation, low-latency design, and production observability in multi-NIC environments handling high-throughput real-time traffic.\n\n## Key Concepts\n- IRQ affinity and RPS/RFS to cap interrupt impact\n- XDP/BPF-based shaping for fast path handling\n- Per-tenant tc filters for fair shaping\n- Observability: perf, ftrace, latency benchmarks\n- Safe rollback and metrics-driven validation\n\n## Code Example\n```bash\n# Example: bind IRQs to CPU mask 0xFF (illustrative)\necho 255 > /proc/irq/123/smp_affinity\n```\n\n## Follow-up Questions\n- How would you validate the latency improvements in production?\n- What monitoring tools would you use to detect IRQ storms?\n- How do you ensure graceful degradation during overload conditions?","diagram":"flowchart TD\n  TB[Traffic Burst] --> RX[RX IRQ Storm]\n  RX --> CPU[Per-CPU Isolation]\n  CPU --> XDP[XDP Shaping]\n  XDP --> TFilters[Per-tenant tc Filters]\n  TFilters --> Mon[Observability & Tests]","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","Lyft","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T06:54:34.851Z","createdAt":"2026-01-25T21:29:14.169Z"},{"id":"q-7450","question":"On a Linux host running a production multi-tenant data-collection stack, a critical kernel vulnerability patch must be applied with zero downtime. Propose a concrete production plan to deploy a live patch (kgraft/kpatch/Ksplice) without reboot, covering patch selection, staging tests, compatibility checks with glibc, CPU/NUMA considerations, monitoring, and rollback?","answer":"Patch plan: Utilize a distribution-supported live patching tool (kpatch/kgraft/Ksplice) that maintains compatibility with the exact kernel version and glibc. Execute initial deployment on a staging environment replicating production workloads; perform comprehensive 24-hour load and latency testing to validate stability before production rollout.","explanation":"## Why This Is Asked\n\nTests ability to plan zero-downtime kernel patching with reproducible validation and safe rollback.\n\n## Key Concepts\n\n- Live patching tools (kpatch/kgraft/Ksplice)\n- Kernel-glibc compatibility and symbol versioning\n- Staging environments mirroring production workload\n- Rollback strategies and monitoring\n\n## Code Example\n\n```bash\n# Example patch sequence\nkpatch apply /patches/critical.patch\nkpatch status\njournalctl -k -b | tail -n 20\n```\n\n## Follow-up Questions\n\n- How would you validate patch under peak load without impacting users?\n- What failure modes require immediate rollback?","diagram":null,"difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Meta","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T05:48:55.501Z","createdAt":"2026-01-25T23:52:41.749Z"},{"id":"q-7461","question":"On a Linux host running a TLS-terminating reverse proxy farm for a multi-tenant SaaS, a surge in client TLS handshakes causes tail latency spikes that degrade backend API latency. Design a production plan to guarantee bounded TLS handshake latency for the critical path while preserving throughput. Include: CPU/core isolation strategy (nohz_full/isolcpus), per-path cpuset and systemd slice (tls.slice), IRQ affinity for NICs, memory locking, and a concrete test plan with a TLS-burst workload and TLS session resumption strategy. What exact steps would you take and how would you verify?","answer":"Implement a dedicated 4-core TLS processing path with nohz_full/isolcpus isolation on cores 4-7, configure NIC IRQ affinity to these isolated cores, deploy TLS workers within a tls.slice systemd slice using SCHED_FIFO priority 80, apply memory locking with memlockall and per-cgroup constraints via memory.max/memory.swap.max, and enforce I/O limits through io.max configurations.","explanation":"## Why This Is Asked\nThis question evaluates your ability to design comprehensive isolation strategies for latency-sensitive paths under burst conditions, requiring practical knowledge of CPU isolation, real-time scheduling, cgroups v2, NIC optimization, and systematic verification approaches.\n\n## Key Concepts\n- CPU isolation techniques (nohz_full/isolcpus)\n- Real-time scheduling with SCHED_FIFO\n- cgroups v2 resource management\n- NIC IRQ affinity optimization\n- Memory locking and swap prevention\n- I/O throttling mechanisms\n- TLS session resumption strategies\n- Production testing and verification methodologies\n\n## Code Example\n```bash\n# TLS slice systemd unit configuration\n[Unit]\nDescription=TLS worker path slice\nAfter=systemd-cgroups-agent.service\n\n[Slice]\nCPUAccounting=true\nCPUQuota=400%\nMemoryAccounting=true\nMemoryMax=4G\nMemorySwapMax=0\nIOAccounting=true\n```\n\n## Follow-up Questions\n- How would you monitor tail latency in production and define rollback triggers?\n- What modifications are needed if some tenants require different TLS configurations?\n- How would this design scale across multiple proxy instances?","diagram":null,"difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Databricks","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T04:28:48.269Z","createdAt":"2026-01-26T02:31:55.971Z"},{"id":"q-7702","question":"A multi-tenant Linux node runs a shared NFSv4.1 export consumed by hundreds of pods. During peak load, file operations on the export stall 2â€“5 seconds sporadically, while network and CPU show no obvious bottlenecks. Design a production plan to diagnose and mitigate NFS stall latency without sacrificing consistency. Include tracing plan, mount/fs options, caching/isolation strategies, per-tenant QoS, and a test and rollback plan?","answer":"Use nfsiostat and perf plus eBPF to trace stall sources and confirm if NFS lease contention or server I/O is the culprit. Mitigations: split into per-tenant subexports to isolate locks; upgrade to NFS","explanation":"## Why This Is Asked\nTests practical diagnosis of production NFS latency in a multi-tenant Kubernetes context, focusing on real observability and safe isolation without breaking data correctness.\n\n## Key Concepts\n- NFSv4.1 leases, locking, and per-export isolation\n- Observability: nfsiostat, iostat, perf, and eBPF tracing\n- Tenant QoS via per-export subtrees and server quotas\n- Safe rollback and staged testing with realistic bursts\n\n## Code Example\n```javascript\nnfsiostat -m -d /mnt/nfs-export\n```\n\n## Follow-up Questions\n- How would you extend this to a mixed on-prem/cloud environment?\n- What metrics would you alert on to catch regressions early?","diagram":"flowchart TD\n  A[Stalls on NFS export] --> B[Run nfsiostat / perf]\n  B --> C{Source?}\n  C --> D[Client lease contention]\n  C --> E[Server I/O bottleneck]\n  D --> F[Create per-tenant subexports]\n  E --> G[Apply quotas and bind mounts]\n  F --> H[Test with fio bursts]\n  G --> I[Monitor and rollback plan]\n","difficulty":"advanced","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Instacart","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T14:37:26.023Z","createdAt":"2026-01-26T14:37:26.026Z"},{"id":"q-7809","question":"On a Linux host running a high-rate security event pipeline (packet capture -> real-time anomaly detection) with a single 28-core NUMA server, peak traffic causes intermittent event drops. Design a production plan to guarantee deterministic latency for the critical path. Include hardware topology, NUMA affinity, IRQ/RPS/RFS tuning, per-queue CPU isolation with cgroups v2, an XDP/BPF-accelerated path, memory locking, and a concrete test plan using traffic generators and latency probes, plus rollback?","answer":"Implement fixed-core data path: dedicate 6â€“8 cores on a NUMA node for capture and detection; isolate NIC IRQs and set RPS/RFS to those CPUs; pin XDP/BPF processing to this CPU set; run the critical pa","explanation":"This question probes practical, end-to-end latency guarantees in a real-time Linux data path. It touches NUMA-aware topology, IRQ affinity, kernel network tuning, BPF/XDP acceleration, memory locking, and robust testing with rollback. It requires concrete configuration choices and a repeatable validation strategy.","diagram":null,"difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","Two Sigma","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T19:00:29.628Z","createdAt":"2026-01-26T19:00:29.629Z"},{"id":"q-7827","question":"On a Linux host serving a multi-tenant S3-compatible gateway fronting a local object store, bursts from a single tenant cause long tail I/O latency for others. Design a production plan to guarantee per-tenant IO isolation and preserve throughput: hardware topology, NUMA binding for IO workers, per-tenant cgroup v2 io.max, IO scheduler tuning (BFQ or Kyber), per-tenant cache sizing, and a test/rollback plan using synthetic tenants with burst workloads and latency targets?","answer":"Implement per-tenant QoS with cgroups v2: create tenant.slice and assign io.max quotas (bandwidth and IOPS); pin IO workers to fixed CPUs and NUMA nodes; use BFQ (or Kyber) as the IO scheduler; isolat","explanation":"## Why This Is Asked\nGauges ability to design IO isolation in multi-tenant Linux storage hosts.\n\n## Key Concepts\n- per-tenant cgroups v2 io.max quotas for bandwidth/IOPS\n- IO schedulers (BFQ or Kyber) for fairness under bursts\n- NUMA-bound IO workers and CPU affinity\n- per-tenant caching and data tiering to prevent cache leakage\n- observability, test plan, and safe rollback strategy\n\n## Code Example\n```javascript\n# Conceptual example: apply per-tenant IO quotas (pseudo)\n# mkdir /sys/fs/cgroup/tenant-a\n# echo 'read_bps=1048576;write_bps=1048576' > /sys/fs/cgroup/tenant-a/io.max\n```\n\n## Follow-up Questions\n- How would you adapt quotas for NVMe-oF or remote storage?\n- What failure modes if a tenant ignores quotas and overwhelms the pool?\n- How would you monitor fairness and automatically adjust quotas over time?","diagram":"flowchart TD\n  A[Tenant] --> B[io.max QoS]\n  B --> C[NUMA-bound IO threads]\n  C --> D[Scheduler: BFQ/Kyber]\n  D --> E[Cache/Tier isolation]\n  E --> F[Monitoring & Rollback]","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","Microsoft","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T19:34:39.395Z","createdAt":"2026-01-26T19:34:39.395Z"},{"id":"q-7943","question":"On a production Linux host running a mission-critical service, a hot patch via kpatch is deployed to fix a security issue. Under heavy NUMA pressure the patch causes intermittent stalls and hangs. Design a production plan to validate, monitor, and rollback with minimal downtime. Include staged rollout, NUMA-aware testing, performance baselines, rollback criteria, and post-mortem observability?","answer":"Plan a staged kpatch rollout: test on a hot spare kernel, run sustained workload with NUMA pinning, monitor latency, stalls, and panics, and define clear rollback criteria (patch load success, latency","explanation":"## Why This Is Asked\nLive patching in production carries regression risk under NUMA pressure. The candidate must design safe, testable rollback and observability.\n\n## Key Concepts\n- Live kernel patching (kpatch)\n- NUMA-aware testing\n- Canary and rollback mechanisms\n- Observability and rollback criteria\n\n## Code Example\n```bash\n# Apply patch (example)\nkpatch apply patch.ko\n# Rollback\nkpatch remove patch.ko\n```\n\n## Follow-up Questions\n- How would you measure patch impact and decide to rollback in under 30 minutes?\n- What telemetry would you add to detect subtle NUMA skew issues after patching?","diagram":"flowchart TD\n  A[Start Patch] --> B{Test Replica}\n  B --> C[Perf/NUMA Tests]\n  C --> D{Rollout}\n  D -->|Success| E[Live Patch]\n  D -->|Failure| F[Rollback]","difficulty":"advanced","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Hugging Face","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T23:57:02.397Z","createdAt":"2026-01-26T23:57:02.397Z"},{"id":"q-881","question":"On a Linux host, a long-running daemon writes to `/var/log/myapp.log` and is managed by systemd, but log rotation occasionally causes logging to stop after rotation. Propose a practical fix to ensure logging continues after rotation without restarting the daemon. Include the exact approach and a sample logrotate config snippet and testing steps?","answer":"Configure logrotate to reopen the log file after rotation instead of copying or truncating. After rotating, signal the daemon to reopen logs (e.g., SIGHUP). The config should include a postrotate that","explanation":"## Why This Is Asked\nTests practical handling of log rotation, signals, and ensuring service continuity without downtime. It checks knowledge of logrotate postrotate scripts, choosing the right approach (signal vs copytruncate), and how to validate in a controlled test.\n\n## Key Concepts\n- logrotate configuration fields and postrotate scripts\n- signaling daemons (SIGHUP) to reopen logs\n- copytruncate vs signaling trade-offs\n- testing routine for rotation\n\n## Code Example\n```bash\n/var/log/myapp.log {\n  rotate 5\n  weekly\n  missingok\n  notifempty\n  postrotate\n    kill -HUP `cat /var/run/myapp.pid` 2>/dev/null || true\n  endscript\n}\n```\n\n## Follow-up Questions\n- What are the trade-offs of copytruncate vs signaling?\n- How would you monitor and alert if log rotation fails to reopen logs?","diagram":null,"difficulty":"beginner","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Meta","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:59:55.582Z","createdAt":"2026-01-12T13:59:55.582Z"},{"id":"q-888","question":"A production Linux host runs a data-backup agent that uses /var/lib/backup/backup.lock to enforce a single instance. Sometimes a stale lock remains after a crash, blocking new runs; the agent also leaves non-terminating children on stop, risking partial backups. Propose a systemdâ€‘based lifecycle fix: ensure one instance, auto-clean stale lock, and graceful stop with timeout and fallback to kill. Include concrete unit snippets and verification steps?","answer":"Use a systemd unit with Type=forking, PIDFile, and ExecStartPre that checks and clears a stale lock: if /var/lib/backup/backup.lock exists and its PID is not running, delete it. Stop uses KillMode=con","explanation":"## Why This Is Asked\nTests understanding of robust service lifecycle management with systemd, handling singleton constraints, and clean termination of complex processes.\n\n## Key Concepts\n- systemd lifecycle: ExecStartPre, ExecStop, KillMode, TimeoutStopSec\n- singleton enforcement via lock files\n- graceful termination vs. forceful kill for child processes\n\n## Code Example\n```ini\n; /etc/systemd/system/backup-agent.service\n[Unit]\nDescription=Backup Agent\nAfter=network.target\n\n[Service]\nType=forking\nPIDFile=/var/run/backup/backup.pid\nExecStartPre=/bin/sh -c 'LOCK=/var/lib/backup/backup.lock; if [ -e \"$LOCK\" ]; then pid=$(cat \"$LOCK\"); if [ -d /proc/$pid ]; then exit 1; else rm -f \"$LOCK\"; fi; fi'\nExecStart=/usr/local/bin/backup-agent\nExecStop=/bin/kill -TERM $MAINPID\nExecStopPost=/bin/rm -f /var/lib/backup/backup.lock\nTimeoutStopSec=120s\nKillMode=control-group\nRestart=on-failure\n```\n\n## Follow-up Questions\n- How would you test idempotency for consecutive startups?\n- How would you adapt if the agent uses a PID file instead of a lock file?","diagram":null,"difficulty":"advanced","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T14:27:44.056Z","createdAt":"2026-01-12T14:27:44.056Z"},{"id":"q-961","question":"On a Linux host, a memory-hungry log-harvester daemon managed by systemd sporadically triggers the kernel OOM killer during peak load, crashing the service and delaying alerts. Propose a production-safe plan to prevent OOM termination while preserving throughput. Include concrete systemd settings (MemoryLimit, MemorySwapMax, OOMScoreAdjust, Restart), kernel tuning (swap, swappiness), and a test plan with a reproducible high-memory scenario and verification steps?","answer":"Apply per-service memory controls and OOM protection. Set MemoryLimit=2G, MemorySwapMax=2G, OOMScoreAdjust=-100, and Restart=on-failure with a 5s backoff. Tune vm.swappiness=10 and ensure swap is enab","explanation":"## Why This Is Asked\nTests memory pressure handling, systemd tuning, and safe recovery without service disruption.\n\n## Key Concepts\n- Kernel OOM killer and oom_score_adj\n- systemd memory constraints (MemoryLimit, MemorySwapMax)\n- Restart strategies and timeouts\n- memory tuning and swap behavior\n\n## Code Example\n```ini\n[Unit]\nDescription=Log Harvester\n\n[Service]\nExecStart=/usr/local/bin/logharvester\nType=simple\nMemoryLimit=2G\nMemorySwapMax=2G\nOOMScoreAdjust=-100\nRestart=on-failure\nRestartSec=5s\n```\n\n## Follow-up Questions\n- How would you observe and alert on OOM events?\n- How would you adjust for multiple high-memory services competing for swap?","diagram":"flowchart TD\n  A[Memory Pressure] --> B{OOM killer?}\n  B -->|Yes| C[Adjust OOMScore/MemoryLimit]\n  B -->|No| D[Normal Operation]\n  C --> E[Daemon Survives, backlog Drains]","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T17:22:50.353Z","createdAt":"2026-01-12T17:22:50.353Z"},{"id":"linux-foundation-sysadmin-networking-1768260262823-4","question":"You suspect ARP storms on a host with interface eth1. To quickly verify ARP traffic and identify abnormal ARP activity on that interface, which command should you run?","answer":"To quickly verify ARP traffic and identify abnormal ARP activity on interface eth1, run 'tcpdump -i eth1 arp'. This command captures and displays ARP packets specifically on the eth1 interface, allowing you to monitor for ARP storms.","explanation":"## Correct Answer\nA. tcpdump with the arp filter on the specific interface captures ARP requests/replies, which is exactly what you need to observe ARP storms.\n\n## Why Other Options Are Wrong\n- Option B: ICMP captures are unrelated to ARP activity.\n- Option C: TCP traffic does not reveal ARP behavior.\n- Option D: arp -a shows ARP table entries but not live ARP traffic, making storms harder to observe in real time.\n\n## Key Concepts\n- ARP monitoring with packet captures\n- Interface-specific traffic analysis\n- Distinguishing ARP storms from normal ARP chatter\n\n## Real-World Application\n- Proactively diagnosing network broadcast storms and ARP-related outages in data-center or campus networks.","diagram":null,"difficulty":"intermediate","tags":["Networking","ARP","Linux","Kubernetes","certification-mcq","domain-weight-12"],"channel":"linux-foundation-sysadmin","subChannel":"networking","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T03:32:44.361Z","createdAt":"2026-01-12 23:24:24"}],"subChannels":["general","networking"],"companies":["Adobe","Airbnb","Amazon","Anthropic","Apple","Bloomberg","Citadel","Cloudflare","Coinbase","Databricks","Discord","DoorDash","Goldman Sachs","Google","Hashicorp","Hugging Face","IBM","Instacart","LinkedIn","Lyft","Meta","Microsoft","MongoDB","NVIDIA","Netflix","OpenAI","Oracle","PayPal","Plaid","Robinhood","Salesforce","Scale Ai","Slack","Snap","Snowflake","Square","Stripe","Tesla","Twitter","Two Sigma","Uber","Zoom"],"stats":{"total":88,"beginner":20,"intermediate":39,"advanced":29,"newThisWeek":37}}