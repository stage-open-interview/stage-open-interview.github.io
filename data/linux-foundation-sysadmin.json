{"questions":[{"id":"q-1044","question":"On a Linux host running multiple tenant services, peak I/O from a data ingestion daemon saturates the disk, causing latency spikes for others. Propose a production plan to isolate and throttle disk I/O across tenants using systemd slices and cgroup v2 io.max. Include concrete unit and slice snippets, per-device limits for a SATA HDD and NVMe, and a test plan using fio and iostat to verify tail latency improvements under concurrent workloads?","answer":"Implement per-tenant I/O isolation using systemd slices and cgroup v2 io.max. Create tenant-<name>.slice, assign services to it, and throttle devices (e.g., 8:0 rbps=50M wbps=50M). Include per-device ","explanation":"## Why This Is Asked\nTests ability to design resource isolation and practical configuration with systemd and cgroups in a multi-tenant environment, focusing on I/O pressure rather than CPU/memory alone.\n\n## Key Concepts\n- cgroup v2 io.max throttling\n- systemd slices and per-service isolation\n- per-device I/O limits and hierarchy\n- reproducible load testing with fio and iostat\n\n## Code Example\n```javascript\n# Example: set per-device I/O throttling for tenant-a.slice (pseudo)\nsudo mkdir -p /sys/fs/cgroup/tenant-a.slice\necho \"8:0 rbps=50M wbps=50M\" | sudo tee /sys/fs/cgroup/tenant-a.slice/io.max\n```\n\n## Follow-up Questions\n- How would you adapt if the workload alternates between read-heavy and write-heavy?\n- What are potential pitfalls with block layer throttling and caches?","diagram":null,"difficulty":"advanced","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Microsoft","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T20:30:42.832Z","createdAt":"2026-01-12T20:30:42.832Z"},{"id":"q-1056","question":"On a Linux host, a TLS proxy reads its certificate from /etc/ssl/certs/app.crt and currently reloads by restarting, causing brief downtime during renewal. Propose a zero-downtime rotation using a systemd.path trigger that fires on a new cert symlink, with a separate service unit for ExecReload and an atomic update scheme (swap in a new cert, then switch a symlink). Include concrete unit snippets and a test plan?","answer":"Propose a zero-downtime TLS cert rotation using systemd.path to trigger on an updated certificate symlink, plus a dedicated systemd service that ExecReloads the proxy. Use atomic cert updates (place n","explanation":"## Why This Is Asked\nTests practical mastery of systemd path activation, service reloads, and atomic file updates for TLS without downtime. It also validates understanding of race-free certificate rotation and testability.\n\n## Key Concepts\n- systemd.path and PathChanged/PathModified\n- ExecReload vs Restart semantics\n- Atomic file replacement via symlinks\n- Reproducible test plan for cert rotation\n\n## Code Example\n```ini\n# app-cert.path\n[Unit]\nDescription=Trigger TLS cert rotation on new cert\n\n[Path]\nPathChanged=/etc/ssl/certs/app.crt\nUnit=app-cert.service\n\n[Install]\nWantedBy=multi-user.target\n```\n```ini\n# app-cert.service\n[Unit]\nDescription=Reload TLS proxy with new certificate\n\n[Service]\nType=oneshot\nExecStart=/usr/local/bin/app-cert-rotate-reload.sh\n```\n```bash\n# app-cert-rotate-reload.sh\n#!/bin/sh\nset -e\n# Swap in the new cert and reload the proxy\nmv -f /etc/ssl/certs/app.crt.new /etc/ssl/certs/app.crt\nsystemctl reload app-proxy.service\n```\n```\n\n## Follow-up Questions\n- How to handle reload failure and ensure idempotence?\n- How to monitor for stale symlinks or missing new certs?\n- How would you extend this for multiple certificates or canary rollouts?","diagram":"flowchart TD\n  A[Cert renewal arrives] --> B[Write app.crt.new]\n  B --> C[symlink swap via script]\n  C --> D{Reload trigger}\n  D --> E[app-proxy reloads without downtime]","difficulty":"advanced","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T21:18:27.923Z","createdAt":"2026-01-12T21:18:27.923Z"},{"id":"q-1122","question":"On a Linux host running multiple ML model servers in separate systemd slices using cgroup v2, a spike in one tenant consumes memory and triggers host memory pressure despite per-tenant limits. Propose a production plan to enforce strict isolation and backpressure using memory.max and memory.high, enable group OOM behavior, and implement eviction/playback strategies. Include concrete unit and cgroup settings and a test plan with a reproducible spike and validation steps?","answer":"Implement per-tenant slices with MemoryMax and MemoryHigh in a cgroup v2 setup; attach each model server to its slice. Enable memory.oom_group for group-based OOM handling and configure a hard cap wit","explanation":"## Why This Is Asked\nTests the ability to design strict memory isolation in a multi-tenant Linux environment, covering cgroup v2 memory controls, systemd slices, and OOM behavior under load.\n\n## Key Concepts\n- cgroup v2 memory.max and memory.high for hard/soft limits\n- memory.oom_group for group-level OOM decisions\n- systemd slices and per-tenant unit configurations\n- observability: cgroup events, dmesg, and host health verification\n\n## Code Example\n```javascript\n# Example: tenant-A.slice\n[Slice]\nMemoryHigh=6G\nMemoryMax=8G\n\n# Example: tenant-A service\n[Service]\nSlice=tenant-A.slice\nExecStart=/usr/bin/modelA_server\nRestart=on-failure\n```\n\n## Follow-up Questions\n- How would you automate scale-out when new tenants are added?\n- What metrics and alerts would you surface to detect pressure early?","diagram":"flowchart TD\n  A[Tenant A] --> B[Cgroup v2 memory.max]\n  A --> C[Cgroup v2 memory.high]\n  B --> D[Host stability checks]\n  C --> D","difficulty":"advanced","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","MongoDB","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T23:29:53.881Z","createdAt":"2026-01-12T23:29:53.881Z"},{"id":"q-1230","question":"On a Linux host with a multi-tenant workload sharing a single 10GbE NIC configured with multiple receive queues, one tenant bursts UDP traffic and starves others, causing increased latency and packet loss. Propose a production plan to enforce tenant isolation and fairness using NIC multi-queue, RSS/XPS mapping, IRQ affinity, and per-tenant cgroups (v2) with io.max and tc shaping. Include concrete steps and a test plan with realistic traffic?","answer":"Plan to isolate tenants by binding NIC RX queues to per-tenant CPUs, use RSS/XPS to map flows, create per-tenant systemd slices with per-tenant CPU budgets, apply io.max quotas, and shape egress with ","explanation":"## Why This Is Asked\nAssesses practical network isolation skills in a multi-tenant Linux environment, focusing on NIC multi-queue tuning, IRQ affinity, and per-tenant resource containers to prevent bursts from one tenant affecting others.\n\n## Key Concepts\n- NIC multi-queue, RSS, XPS\n- IRQ affinity per-tenant isolation\n- cgroups v2 with io.max and per-tenant slices\n- tc shaping and fq_codel for fairness\n- Observability with iperf3/pktgen, iostat\n\n## Code Example\n```bash\n# Enable 4 RX queues\nethtool -L eth0 rx 4\n# Bind a few IRQs to CPUs (example)\necho 2-3,6-7 > /proc/irq/46/smp_affinity_list\n```\n\n## Follow-up Questions\n- How would you validate fairness under concurrent tenants?\n- What are NUMA pitfalls and how would you mitigate them?","diagram":"flowchart TD\n A[Tenants] --> B[RX queues]\n A --> C[CPU cores]\n B --> D[Mapping rules]\n C --> E[Isolation boundary]","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","MongoDB","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T05:40:43.739Z","createdAt":"2026-01-13T05:40:43.739Z"},{"id":"q-1327","question":"On a Linux host running multiple tenants via containers, a CPU-intensive workload from one tenant starves others. Propose a production plan to enforce CPU isolation using cgroup v2 slices, systemd integration, and per-tenant quotas; include concrete settings (cpu.max, cpu.weight), scheduling options (cpuset and isolcpus), and a test plan with realistic workloads and verification steps?","answer":"Create per-tenant cgroup v2 slices and assign each container to its slice. Use cpu.max and cpu.weight to cap and weight CPU usage (e.g., tenantA 250000 1000000, tenants B, C, D 150000 1000000; weights","explanation":"## Why This Is Asked\nAssesses practical mastery of modern Linux isolation (cgroup v2, systemd slices, cpuset) and real-world tradeoffs between throughput and fairness.\n\n## Key Concepts\n- cgroup v2 CPU controller: cpu.max, cpu.weight\n- systemd slices and container assignment\n- cpuset/isolation of CPUs; isolcpus boot option\n- realistic workloads: fio, sysbench; tail latency tracking\n\n## Code Example\n```bash\n# Illustrative: create and configure tenant slices (pseudo-commands)\nsystemd-run --unit=tenantA.slice --property=CPUQuota=250000 --property=CPUQuotaPeriodUS=1000000 --slice=tenantA.slice sleep 1h\nsystemd-run --unit=tenantB.slice --property=CPUQuota=150000 --property=CPUQuotaPeriodUS=1000000 --slice=tenantB.slice sleep 1h\n# Bind slices to CPUs via cpuset (illustrative)\nchmod 600 /sys/fs/cgroup/unified/tenantA.slice/cpuset.cpus\n# Attach containers to their slices as they spawn\n```\n\n## Follow-up Questions\n- How would you monitor CPU-steal and detect fairness violations in production?\n- What are risks of over-isolating CPUs, and how would you safely rebalance during load spikes?","diagram":null,"difficulty":"advanced","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Google","IBM"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T11:37:36.271Z","createdAt":"2026-01-13T11:37:36.271Z"},{"id":"q-1346","question":"On a production Linux host serving multiple ephemeral monitoring daemons, a heartbeat worker must emit a 5-second ping to a central collector. During peak load, the heartbeat misses several intervals, delaying alerts. Propose a concrete production plan to guarantee cadence and resilience using: (a) per-service CPU isolation and pinning, (b) high-priority timer/RT scheduling or systemd timers with precise tuning, (c) a fallback mechanism for missed heartbeats, and (d) a validation strategy with a controlled load. Include concrete config snippets for systemd, cpuset, and timer settings?","answer":"Plan pins heartbeat to a dedicated CPU set, uses a systemd timer with 5s cadence and AccuracySec=1s, and a high-priority heartbeat service with CPUQuota=50% and SCHED_FIFO-like behavior. Add a fallbac","explanation":"## Why This Is Asked\n\nTests knowledge of per-service isolation, timer accuracy, fault tolerance, and measurable validation.\n\n## Key Concepts\n\n- systemd slices timers CPU affinity\n- cpuset pinning and CPUQuota/AccuracySec\n- RT priority and fallback mechanisms\n- end-to-end verification under load\n\n## Code Example\n\n```\n# See /etc/systemd/system/heartbeat.* for concrete files\n```\n\n## Follow-up Questions\n\n- How would you detect missed heartbeats and trigger alerts?\n- What are the trade-offs of increasing CPU isolation vs. responsiveness?","diagram":"flowchart TD\n  A[Create heartbeat.slice] --> B[Pin heartbeat service]\n  B --> C[Configure 5s timer]\n  C --> D[Add fallback heartbeat]\n  D --> E[Test under load]","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","LinkedIn"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T13:05:27.792Z","createdAt":"2026-01-13T13:05:27.792Z"},{"id":"q-1385","question":"On a Linux host running ephemeral build jobs managed by systemd, a rogue job floods /tmp with tiny files, exhausting inodes and delaying builds. Propose a production plan to isolate and bound /tmp usage and auto-clean stale files. Include: (a) per-service PrivateTmp and tmpfs /tmp with a size cap; (b) systemd-tmpfiles cleanup rules; (c) inode usage monitoring; (d) a reproducible test that proves isolation and cleanup. How would you implement this?","answer":"Configure per-service isolation: set PrivateTmp=true and PrivateDevices=true for build services; mount /tmp as tmpfs with size=128M and mode=1777; add systemd-tmpfiles cleanup rules to prune stale fil","explanation":"## Why This Is Asked\nAssesses practical isolation of temporary storage for multi-tenant build workloads and how to enforce cleanups under bursty IO.\n\n## Key Concepts\n- systemd PrivateTmp and PrivateDevices\n- tmpfs sizing and mounting options\n- inode monitoring and alerting (df -i, thresholds)\n- systemd-tmpfiles cleanup rules and scheduling\n\n## Code Example\n```javascript\n// Example pseudo-configuration snippets for illustration\n// systemd unit: PrivateTmp=true\n// /etc/fstab: tmpfs /tmp tmpfs size=128M,mode=1777 0 0\n```\n\n## Follow-up Questions\n- How would you scale this across a cluster of CI workers?\n- What are potential edge cases with cleanup timing and race conditions?","diagram":null,"difficulty":"beginner","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Coinbase","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T14:47:50.803Z","createdAt":"2026-01-13T14:47:50.803Z"},{"id":"q-1490","question":"On a Linux server, a small web app runs as a systemd service and reads its port and log level from /etc/myapp/config.yaml. Changes to this file must apply without dropping connections. Design a production-safe hot-reload mechanism using systemd ExecReload, a SIGHUP-based reload, and a small validation script. Include unit file snippets and a test plan with a reproducible config change and verification steps?","answer":"Use systemd ExecReload to send SIGHUP to the daemon and a small validator script. Create /usr/local/bin/myapp_reload to validate /etc/myapp/config.yaml (e.g., ensure .port and .log_level exist) and th","explanation":"## Why This Is Asked\nTests hot-reload capability and safe config changes without dropping connections.\n\n## Key Concepts\n- systemd ExecReload\n- SIGHUP reload semantics\n- config validation (YAML)\n- graceful reload vs restart\n- test plan design\n\n## Code Example\n```javascript\n// Implementation sketch\n```\n\n## Follow-up Questions\n- How would you handle a daemon that does not support SIGHUP?\n- How would you verify that in-flight requests survive a reload?","diagram":"flowchart TD\n  A[config.yaml change] --> B[validate YAML]\n  B --> C{valid?}\n  C -->|yes| D[send SIGHUP]\n  D --> E[service reload]\n  E --> F[verify endpoints]\n  C -->|no| G[abort reload]","difficulty":"beginner","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T19:03:23.816Z","createdAt":"2026-01-13T19:03:23.816Z"},{"id":"q-1497","question":"On a Linux host running multiple tenants in systemd services sharing a single NVMe SSD and a 25Gbps NIC, a new requirement enforces strict I/O isolation to prevent a noisy tenant from stalling others. Propose a production plan to enforce per-tenant I/O isolation and backpressure using: (a) cgroup v2 io.max per service; (b) per-tenant filesystem isolation via PrivateMounts and PrivateDevices with a dedicated data path; (c) I/O scheduler tuning (none/deadline) and per-tenant block IO queuing discipline with tc; (d) a deterministic test plan with a reproducible burst scenario and rollback steps. Include concrete config snippets for systemd units and cgroups?","answer":"Plan: use per-tenant cgroup v2 io.max to cap disk writes, create tenant.slice with PrivateMounts for isolation, assign each service to a sub-cgroup, attach per-tenant tc qdisc to shape writes, and iso","explanation":"## Why This Is Asked\nTests hands-on mastery of Linux IO isolation primitives under multi-tenant workloads and production risk management.\n\n## Key Concepts\n- cgroup v2 io.max\n- systemd Slice and isolation\n- tc/bpf/qdisc for per-tenant IO shaping\n- PrivateMounts/PrivateDevices for filesystem isolation\n- Reproducible test methodology with fio\n\n## Code Example\n```javascript\n# systemd unit\n[Unit]\nDescription=Tenant A worker\nAfter=network-online.target\n\n[Service]\nSlice=tenant-A.slice\nPrivateMounts=yes\nPrivateDevices=yes\nExecStart=/usr/local/bin/tenant-a-worker\n```\n\n```javascript\n# cgroup v2 IO limit (pseudo)\nmkdir -p /sys/fs/cgroup/tenant-A\necho \"8:0 100M\" > /sys/fs/cgroup/tenant-A/io.max\n```\n\n## Follow-up Questions\n- How would you roll back if another tenant regressed?\n- What metrics and alerting would you install to detect drift?","diagram":null,"difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T19:32:24.815Z","createdAt":"2026-01-13T19:32:24.816Z"},{"id":"q-1588","question":"On a dual-socket Linux host running PostgreSQL and a data-processor ETL job, both services contend for memory and cache across NUMA nodes; design an advanced plan to enforce NUMA-aware isolation with cpuset/memcg, disable NUMA balancing, set per-node IRQ affinity, and validate via targeted benchmarks. Include concrete commands and a test plan?","answer":"Pin two services to separate NUMA nodes using cpuset and memory binding; PostgreSQL on node0, ETL on node1. Disable NUMA balancing, set per-node IRQ affinity, and ensure memory policies via memcg. Validate isolation through targeted benchmarks measuring memory locality, cache coherence, and latency under mixed workload conditions.","explanation":"## Why This Is Asked\nNUMA awareness is critical for predictable latency in multi-socket systems; this question tests practical implementation of resource isolation, memory binding, and performance verification techniques.\n\n## Key Concepts\n- NUMA topology and memory affinity\n- cpuset and memory controller (memcg) binding\n- NUMA balancing disable and IRQ affinity configuration\n- Performance validation under mixed workload scenarios\n\n## Code Example\n```bash\n# Detect NUMA topology and create cpusets\nlscpu | grep NUMA\nsudo mkdir -p /sys/fs/cgroup/cpuset/postgres\nsudo bash -c 'echo 0-15 > /sys/fs/cgroup/cpuset/postgres/cpuset.cpus'\nsudo bash -c 'echo 0 > /sys/fs/cgroup/cpuset/postgres/cpuset.mems'\n# Similar setup for ETL on node1\n```\n\n## Follow-up Considerations\nMonitor NUMA statistics via `numastat` and validate memory locality using `perf` and custom benchmarks measuring cross-NUMA traffic impact.","diagram":null,"difficulty":"advanced","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T04:28:01.530Z","createdAt":"2026-01-13T22:54:02.606Z"},{"id":"q-1665","question":"On a Linux server hosting a latency-sensitive database and a heavy batch analytics job that share a single 2-socket NUMA node and HDD I/O, propose a production plan to guarantee tail latency (P95/P99) for the database while allowing analytics to finish within a bounded window. Include NUMA memory binding, CPU pinning via cpuset/systemd slices, I/O throttling via cgroup v2 io.max, IRQ affinity, and a validation test plan with realistic workloads?","answer":"Pin the latency-sensitive database to CPUs 0-3 on NUMA node 0 with a dedicated systemd slice and cpuset; bind memory local to node 0 and set memory.low to favor the DB. Run analytics on CPUs 4-7 with ","explanation":"## Why This Is Asked\nTests ability to reason about cross-domain isolation (CPU, memory, I/O) and NUMA-awareness to guarantee tail latency under mixed workloads.\n\n## Key Concepts\n- NUMA locality and memory policies\n- cpuset and systemd Slice-based isolation\n- cgroup v2 io.max and memory.high for throttling\n- IRQ affinity and CPU affinity for stable paths\n- Validation with realistic, reproducible workloads\n\n## Code Example\n```javascript\n// systemd cpuset and io.max example (illustrative)\n[Unit]\nDescription=Latency DB\n\n[Service]\nSlice=db.slice\nCPUS=0-3\nMemoryHigh=0\n\n```\n\n## Follow-up Questions\n- How would you adjust if analytics occasionally spikes beyond expected bounds?\n- How would you monitor to detect regressive behavior after updates?","diagram":null,"difficulty":"advanced","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T05:49:28.663Z","createdAt":"2026-01-14T05:49:28.663Z"},{"id":"q-1700","question":"On a Linux server running several io_uring-based data ingestors, one worker's backlog bursts, triggering tail latency spikes for others. Propose a production plan to guarantee per-worker latency and fairness using: (a) per-worker io_uring ring sizing and SQ depth; (b) per-worker cgroups v2 with io.max and task limits; (c) backpressure and pacing (budget tokens, per-worker deadlines); (d) monitoring and a reproducible test plan to validate latency under burst?","answer":"Configure per-worker io_uring rings with fixed SQ depth (e.g., 256), pin workers to dedicated CPUs via cpuset, set per-cgroup io.max (e.g., 8MB/s), implement pacing in ingestion to cap outstanding I/O","explanation":"## Why This Is Asked\nTests ability to design per-worker I/O isolation for io_uring workloads, a realistic pattern as data ingestion scales, and how to enforce fairness without sacrificing throughput.\n\n## Key Concepts\n- io_uring ring isolation and SQ budgeting\n- cgroup v2 io.max and per-task constraints\n- cpuset or task-pinning for CPU affinity\n- backpressure/pacing mechanisms in async I/O\n- instrumentation: latency percentiles, eBPF-based tracing\n\n## Code Example\n```javascript\n# Example cgroup setup (bash-like)\nsudo mkdir -p /sys/fs/cgroup/myio/worker1\necho 10485760 > /sys/fs/cgroup/myio/worker1/io.max\n# add pid\necho <pid> > /sys/fs/cgroup/myio/worker1/cgroup.procs\n```\n\n## Follow-up Questions\n- How would you extend to dynamic reallocation of rings if a worker consistently underperforms?\n- What are the risks of strict per-worker quotas on bursty workloads and how would you mitigate them?","diagram":"flowchart TD\n  A[Worker] --> B[io_uring ring]\n  B --> C[Submit IOs]\n  C --> D[Latency Monitor]\n  D --> E{Budget OK?}\n  E -->|Yes| F[Continue]\n  E -->|No| G[Apply Backpressure]\n  G --> H[Ballast Queue]\n  H --> F","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T07:36:58.713Z","createdAt":"2026-01-14T07:36:58.713Z"},{"id":"q-1774","question":"On a Linux host, a file-watcher daemon uses inotify to monitor a large directory tree. As load grows, events start being missed and the daemon lags. Propose a production plan to keep event delivery reliable: tune inotify limits, raise file descriptor limits for the service, add a polling fallback or batching, and validate with a reproducible test that simulates churn?","answer":"Increase inotify capacity and harden limits; implement a polling fallback for missed events; add a watchdog health check to detect lag and restart gracefully. Steps: 1) set fs.inotify.max_user_watches","explanation":"## Why This Is Asked\nTests understanding of Linux kernel parameters, systemd service limits, and practical reliability strategies for event-driven workloads.\n\n## Key Concepts\n- Inotify limits (fs.inotify.*) and kernel tuning\n- Per-service file descriptor limits (LimitNOFILE, ulimit)\n- Fault-tolerant design (polling fallback, batching)\n- Observability and testing with churn scenarios\n\n## Code Example\n```ini\n# Watch daemon systemd unit override\n[Service]\nLimitNOFILE=512000\n```\n\n```bash\n# Increase inotify capacity (runtime)\nsysctl -w fs.inotify.max_user_watches=300000\n```\n\n```bash\n# Simple churn test (pseudo)\nfor i in {1..100000}; do touch /watch/dir/file_$i; done\n```\n\n## Follow-up Questions\n- How would you measure inotify queue length and event lag in production?\n- How would you ensure compatibility across kernel versions and containers?","diagram":"flowchart TD\n  A[Inotify Watch] --> B[Event]\n  B --> C[Worker]\n  C --> D[Lag Monitor]\n  D --> E{Lag > threshold}\n  E -->|Yes| F[Switch to Polling]\n  E -->|No| G[Normal]\n","difficulty":"beginner","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","Oracle","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T10:36:39.086Z","createdAt":"2026-01-14T10:36:39.086Z"},{"id":"q-1807","question":"On a Linux host running a hot-reload deployment workflow for a critical service, mid-deploy tampering could swap the binary between copy and start, breaking integrity. Design a production-grade binary integrity strategy using Linux IMA with runtime attestation. Include how to sign binaries in CI, a signed binary repository, systemd integration (ExecStartPre), policy placement, rollback tests, and how you'd validate against tampering during deployment?","answer":"Use Linux IMA to enforce signed binaries for hot-reload. Sign binaries in CI, publish to a signed repo, enable IMA-based attestation, and require the service to be measured before load. Integrate with","explanation":"## Why This Is Asked\nSecurity-critical deployments require verifiable runtime integrity. This tests knowledge of kernel integrity (IMA), secure deployment workflows, and integration with systemd and CI.\n\n## Key Concepts\n- Linux IMA and runtime attestation\n- Signed binaries in CI and secure repositories\n- systemd integration (ExecStartPre) and rollback mechanisms\n- Tamper-detection tests and observability\n\n## Code Example\n```bash\n# validate-binaries.sh (conceptual)\nset -euo pipefail\nEXPECTED_HASH=$(cat /etc/ci/signatures/service.sha256)\nACTUAL_HASH=$(sha256sum /usr/local/bin/myservice | awk '{print $1}')\nif [ \"$ACTUAL_HASH\" != \"$EXPECTED_HASH\" ]; then\n  echo \"Signature mismatch\" >&2\n  exit 1\nfi\n```\n\n```ini\n# /etc/systemd/system/myservice.service (snippet)\n[Unit]\nDescription=My Critical Service\n\n[Service]\nExecStart=/usr/local/bin/myservice\nExecStartPre=/usr/local/bin/validate-binaries.sh\n```\n\n## Follow-up Questions\n- How would you handle CI/CD with multiple binary variants across environments?\n- How would you monitor and alert on attestation failures in production?","diagram":"flowchart TD\n  A[Signed Binary Repo] --> B[IMA Attestation]\n  B --> C[systemd ExecStartPre Verifies]\n  C --> D[Service Starts]\n  E[Tamper Attempt] --> F[Attestation Failure & Alert]","difficulty":"advanced","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T11:42:10.365Z","createdAt":"2026-01-14T11:42:10.365Z"},{"id":"q-1939","question":"On a Linux host running a web stack (Nginx proxy to a Node/Go app), the app writes to /var/log/webapp/app.log and /var/log/webapp/access.log. Bursts fill the disk and logrotation sometimes fails because the app keeps logs open. Propose a production-grade plan to ensure reliable log rotation with no data loss and no disk-full events. Include concrete logrotate config (copytruncate vs postrotate), systemd unit tweaks (Restart, ExecReload), filesystem layout advice, and a test plan to reproduce a burst and verify rotation completes without downtime?","answer":"Configure logrotate with copytruncate (or a postrotate that signals the process to reopen logs) and rotate daily or at 100M, keeping 7 archives. Move logs to a dedicated partition; enable Restart=on-f","explanation":"## Why This Is Asked\nTests practical log management skills: integrating logrotate with systemd, handling open-file rotation issues, and ensuring disk-safety with verifiable tests.\n\n## Key Concepts\n- logrotate behavior: copytruncate vs postrotate\n- systemd unit settings: Restart, ExecReload\n- filesystem layout and quotas for logs\n- reproducible test plan to simulate burst and confirm rotation\n\n## Code Example\n```javascript\n// logrotate config snippet (illustrative)\n// Real config lives in /etc/logrotate.d/webapp\n```\n\n## Follow-up Questions\n- What are the tradeoffs of copytruncate vs postrotate in data integrity?\n- How would you monitor disk usage and set automated alerts for log growth?","diagram":null,"difficulty":"beginner","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Stripe","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T17:51:38.423Z","createdAt":"2026-01-14T17:51:38.423Z"},{"id":"q-1954","question":"On a Linux host running three CI runners as systemd services sharing a single NVMe and a 10 Gb NIC, a long-running build on one runner starves CPU and I/O, delaying others. Propose a production plan to guarantee fair CPU and I/O while preserving peak throughput: (a) per-service CPU limits via cgroup v2 and CPU affinity; (b) per-service I/O throttling with io.max and a suitable I/O scheduler (BFQ/mq-deadline); (c) CPU pinning and IRQ isolation; (d) validation with bursts and latency metrics?","answer":"Plan using cgroup v2: create ci.slice with per-service sub-slices (ci-runner-A/B/C). Assign cpu.max and io.max per service; pin CPUs via cpuset and set IRQ affinity. Use BFQ for per-service I/O schedu","explanation":"## Why This Is Asked\nTests practical mastery of isolating CPU and I/O in a multi-tenant CI environment under hardware contention, a common production challenge.\n\n## Key Concepts\n- cgroup v2: cpu and io controllers for per-service quotas.\n- systemd slices: grouping services under a parent for unified controls.\n- CPU affinity and cpuset: bind runners to dedicated CPUs.\n- I/O schedulers: BFQ or mq-deadline for per-service throttling.\n- Observability: latency, throughput, tail latency, and backlog.\n\n## Code Example\n```javascript\n# Systemd override example (pseudo)\n[Slice]\nCPUAccounting=true\n\n[Service]\nSlice=ci.slice\nCPUQuota=33%\n```\n```javascript\n# cgroup v2 quota (pseudo)\necho 0.33 > /sys/fs/cgroup/ci.slice/ci-runner-A/cpu.max\n```\n\n## Follow-up Questions\n- How would you monitor and adapt quotas during a bursty workload?\n- What rollback strategy would you use if a quota regresses build reliability?\n","diagram":"flowchart TD\n  CI[ci.slice] --> A[ci-runner-A]\n  CI --> B[ci-runner-B]\n  CI --> C[ci-runner-C]\n  A --> CPU[CPU max]\n  A --> IO[io max]\n  B --> CPU2[CPU max]\n  B --> IO2[io max]\n  C --> CPU3[CPU max]\n  C --> IO3[io max]","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Apple","Databricks"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T18:51:30.714Z","createdAt":"2026-01-14T18:51:30.714Z"},{"id":"q-1987","question":"On a Linux host running CI jobs in rootless containers (e.g., Podman) shared across teams, a misconfigured container could attempt host escape. Propose a production hardening plan using user namespaces, rootless mode, Seccomp and AppArmor, and a tight per-job capset + cgroupv2 quotas. Include concrete commands and a rollback plan?","answer":"Run each CI worker in rootless containers with user namespaces, drop all capabilities, and enforce seccomp + AppArmor. Bind a tight cap set and per-job quotas via cgroupv2 memory.max, cpu.max, and pid","explanation":"## Why This Is Asked\nRealistic containment for containerized CI workloads is critical; this tests practical hardening, not theory.\n\n## Key Concepts\n- Rootless containers and user namespaces\n- Seccomp and AppArmor profiles\n- Capability bounding (cap_set)\n- cgroupv2 quotas (memory.max, cpu.max, pids.max)\n- Monitoring and rollback\n\n## Code Example\n```bash\npodman run --rm --name ci-job \\\n  --security-opt seccomp=seccomp.json \\\n  --security-opt apparmor=ci-job \\\n  --cap-drop=ALL --memory=512m --cpu-quota=50000 --cpu-period=100000 \\\n  --uidmap 0:1:1 --uidmap 1:2:1000 \\\n  localhost/ci-image\n```\n\n## Follow-up Questions\n- How would you audit for privilege escalations at runtime?\n- How would you handle failed containment and rollback without losing work?","diagram":"flowchart TD\nCIJob[CI Job Container] -->|uses| UserNS[User Namespaces]\nCIJob -->|enforced by| Seccomp[Seccomp Profile]\nCIJob -->|enforced by| AppArmor[AppArmor]\nCIJob -->|cgroups| CG[CG v2 quotas]\nCG --> Host[Host Kernel]","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","Square","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T19:42:42.923Z","createdAt":"2026-01-14T19:42:42.923Z"},{"id":"q-2014","question":"On a Linux host running multiple GPU-accelerated services in separate systemd slices under containerized workloads, occasional CPU contention causes tail latency spikes and timeouts for one service. Design a production plan to bound latency and preserve throughput without service interruption. Include per-slice isolation (isolcpus, cpuset), systemd slice configuration, cpu.max, and a kernel I/O scheduler tuning (mq-deadline), plus a reproducible test plan with mixed workloads and latency verification steps?","answer":"Bind each GPU service to a dedicated CPU set and a separate systemd slice; pin IRQs and NIC interrupts; enable isolcpus on cores; cap CPU usage with cpu.max; tune I/O scheduler to mq-deadline with per","explanation":"## Why This Is Asked\n\nThis question probes practical orchestration of CPU, memory, and IO isolation under contention with production-grade controls and observability.\n\n## Key Concepts\n\n- systemd slices and cpuset for resource isolation\n- cgroup v2 cpu.max, cpu.weight, and per-slice quotas\n- isolcpus and IRQ affinity to prevent interrupts from disturbing critical slices\n- I/O scheduler tuning (mq-deadline vs BFQ) and per-tenant quotas\n- reproducible load generation and latency verification with fio/stress-ng and ftrace/perf\n\n## Code Example\n\n```bash\n#!/bin/bash\n# Repro: spawn workloads pinned to CPU pools 0-3 and 4-7\nset -e\n# Start GPU service in background\ntaskset -c 0-3 /usr/local/bin/gpu-service &\n# Generate CPU contention on non-critical cores\nstress-ng --cpu 4 --timeout 60s &\nwait\n```\n\n## Follow-up Questions\n\n- How would you validate that latency stays within SLA under a quarterly load spike?\n- What observability would you add to detect when a slice is starved?","diagram":"flowchart TD\n  A[Identify contention] --> B[Isolate with slices & cpuset]\n  B --> C[Pin IRQs & cap resources with cpu.max]\n  C --> D[Tune I/O scheduler (mq-deadline)]\n  D --> E[Test with mixed workloads & measure tail latency]","difficulty":"advanced","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Instacart","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T20:51:49.017Z","createdAt":"2026-01-14T20:51:49.018Z"},{"id":"q-2167","question":"On a KVM host running dozens of VMs with memory ballooning enabled, a burst in guest memory growth triggers host memory pressure and occasional OOM events. Propose a production-safe plan to cap balloon aggressiveness, enforce host backpressure, and prevent thrash, using libvirt XML (memory.size/currentMemory, memoryBacking, balloon device) and cgroup v2 memory.max/memory.high, plus a test plan with a reproducible spike and verification steps?","answer":"Per-VM caps and headroom via cgroup v2: memory.max and memory.high per VM plus a low vm.swappiness to delay swap pressure. Keep ballooning enabled but hard-cap host memory pressure so reclamation happ","explanation":"## Why This Is Asked\nProbe for practical knowledge of KVM memory management, cgroup v2, ballooning trade-offs, and test scaffolding under real production-like conditions.\n\n## Key Concepts\n- KVM memory ballooning and how guest memory maps to host resources\n- cgroup v2 limits (memory.max, memory.high) for backpressure\n- libvirt XML configuration for per-VM memory caps\n- Observability and test plan for reproducible spikes\n\n## Code Example\n```xml\n<domain type='kvm'>\n  <name>vm1</name>\n  <memory unit='KiB'>4294967296</memory>\n  <currentMemory unit='KiB'>4294967296</currentMemory>\n  <memoryBacking>\n    <hugepages/>\n  </memoryBacking>\n  <devices>\n    <memballoon model='virtio'/>\n  </devices>\n</domain>\n```\n\n## Follow-up Questions\n- How would you monitor and alert on host memory pressure events?\n- What are the risks of disabling ballooning on some VMs?","diagram":null,"difficulty":"advanced","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Instacart","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T05:43:50.597Z","createdAt":"2026-01-15T05:43:50.598Z"},{"id":"q-2223","question":"On a Linux host running multiple CI jobs in containers, CPU contention causes builds to time out. Propose a production-ready plan to bound CPU for CI workloads using cgroup v2 and systemd slices, including concrete unit settings and a test plan with a CPU-bound load?","answer":"Bound CPU for CI work using cgroup v2 and systemd. Create a dedicated CI.slice and move CI containers/services into it. Set cpu.max to 50% (period 100000) and restrict to a subset of CPUs with Allowed","explanation":"## Why This Is Asked\n\nTests practical CPU isolation using cgroup v2 and systemd, not theory. It also checks how to validate under load, monitor, and adjust configurations for fairness.\n\n## Key Concepts\n\n- cgroup v2 CPU controller (cpu.max)\n- systemd Slice and resource controls (Slice=, CPUQuota, AllowedCPUs)\n- container isolation in CI workloads (Docker/K8s)\n- Observability (systemd-cgtop, pidstat, stress-ng)\n\n## Code Example\n\n```bash\n# Example setup (illustrative)\n# Create and configure a dedicated CI.slice with CPU limit\nsudo mkdir -p /sys/fs/cgroup/CI.slice\necho \"50000000 100000000\" | sudo tee /sys/fs/cgroup/CI.slice/cpu.max\n# Or via systemd properties for the slice\nsudo systemctl set-property CI.slice CPUQuota=50% CPUQuotaPeriodSec=1\n```\n\n## Follow-up Questions\n\n- How would you adapt for bursty CI workloads?\n- How do you monitor and auto-tune CPU limits as workloads evolve?","diagram":"flowchart TD\n  A[Start] --> B[Create CI.slice]\n  B --> C[Bind CI jobs to CI.slice]\n  C --> D[Configure cpu.max and AllowedCPUs]\n  D --> E[Run CPU-bound test to validate isolation]\n  E --> F[Monitor via systemd-cgtop and metrics]","difficulty":"beginner","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Snowflake","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T07:50:41.156Z","createdAt":"2026-01-15T07:50:41.156Z"},{"id":"q-2294","question":"On a Linux host running multi-tenant, rootless containers for isolation, a rogue tenant's DNS resolver leaks queries and inflates latency for others. Propose a production plan to enforce strict per-tenant DNS isolation without compromising performance. Include concrete steps for: (a) per-tenant network namespaces and DNS forwarders, (b) nftables rules to confine DNS to tenant resolvers, (c) container runtime config to prevent cross-tenant DNS leakage, and (d) a reproducible test plan to validate isolation under bursty DNS load?","answer":"Deploy per-tenant DNS forwarders (e.g., Unbound) inside isolated net namespaces and route each tenantâ€™s containers to its local resolver at 127.0.0.1:5353. Enforce nftables rules to allow DNS queries ","explanation":"## Why This Is Asked\nReal-world readiness to prevent cross-tenant DNS interference in multi-tenant Linux environments, especially with rootless containers. The answer should show practical separation and testability.\n\n## Key Concepts\n- Per-tenant network namespaces and local DNS forwarders\n- nftables-based DNS isolation rules and drop-all-untagged traffic\n- Container runtimes (Podman/CRIO) configuring per-tenant resolv.conf\n- Reproducible test plan with burst DNS load and latency verification\n\n## Code Example\n```bash\n# Example nftables: restrict DNS to tenant resolver\ntable inet filter {\n  chain forward {\n    tcp dport 53 ct state new accept\n    udp dport 53 ct state new accept\n    counter\n  }\n}\n```\n```ini\n# Per-tenant Unbound config snippet (tenant1-unbound.conf)\nserver:\n  directory: /var/cache/unbound\n  interface: 127.0.0.1\n  port: 5353\n  do-not-query-localhost: no\n  cache-min-ttl: 60\n```\n\n## Follow-up Questions\n- How would you monitor per-tenant DNS latency and cache efficiency?\n- How would you handle DNSSEC validation in this setup?","diagram":"flowchart TD\n  TenantA_NS(Tenant A Namespace) --> ResolverA[Resolver A (127.0.0.1:5353)]\n  TenantB_NS(Tenant B Namespace) --> ResolverB[Resolver B (127.0.0.1:5353)]\n  TenantA_Containers[Containers A] --> ResolverA\n  TenantB_Containers[Containers B] --> ResolverB\n  HostFirewall[Host nftables] --> ResolverA\n  HostFirewall --> ResolverB","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Citadel","Cloudflare"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T10:55:39.543Z","createdAt":"2026-01-15T10:55:39.543Z"},{"id":"q-2515","question":"On a Linux host running a multi-tenant container workload sharing a single 25 GbE NIC, one tenant suddenly bursts writes to a shared storage and causes CPU and FD contention, threatening SLA for others. Propose a production plan to enforce strict per-tenant isolation using cgroup v2 (cpu, memory, pids, io.max) and systemd.slice, plus network QoS with tc.clsact and per-tenant TX queues. Include concrete unit snippets and a test plan with reproducible surge and SLA verification?","answer":"Per-tenant quotas via cgroup v2: cpu.max, memory.max, memory.high, pids.max, io.max; place each tenant in its own systemd.slice. Use tc clsact with per-tenant egress queues and BPF-based enforcement. ","explanation":"## Why This Is Asked\n\nTests ability to design practical, scalable OS-level isolation for multi-tenant workloads, covering CPU/memory/pids/io and network QoS, plus concrete validation steps.\n\n## Key Concepts\n\n- cgroup v2 resource controllers and unified hierarchy\n- systemd.slice for tenant-level isolation\n- tc clsact, qdisc shaping, and BPF enforcement for per-tenant QoS\n- Validation: reproducible bursts, SLA metrics, rollback plan\n\n## Code Example\n\n```ini\n; tenantA.slice (example fragment)\n[Slice]\nCPUQuota=50%\nMemoryMax=4G\nMemoryHigh=3.5G\nIPCS=1\nPidsMax=1000\nIOWeight=500\n```\n\n```bash\n# minimal test harness (conceptual)\nip netns add tenantA\nip link add ifA type dummy\n# Bind tenantA to its own slice and netns, run burst test, verify SLA\n```\n\n## Follow-up Questions\n\n- How would you roll back quotas if a tenant misbehaves?\n- How would you monitor per-tenant metrics and alert on SLA drift?","diagram":"flowchart TD\n  A[Tenant] --> B[Cgroup v2] \n  B --> C[Resource Controls] \n  A --> D[Network QoS] \n  D --> E[SLA Monitoring]","difficulty":"advanced","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Discord","Instacart"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T21:00:55.131Z","createdAt":"2026-01-15T21:00:55.131Z"},{"id":"q-2538","question":"On a Linux host running a multi-tenant data ingestion pipeline and a separate real-time alerting service, ingestion bursts cause alert latency. Propose a production plan to guarantee low-latency alerts while preserving ingestion throughput using: (a) per-service CPU isolation with systemd slices and cpuset, (b) RT scheduling for the alerting process, (c) CPU affinity and NUMA bindings, (d) a validation plan with burst traffic and SLA checks?","answer":"Partition CPU resources using CGroups v2: allocate cores 0-3 for data ingestion and cores 4-5 for alerting; configure alerting within a dedicated systemd.slice with CPUAffinity=4-5 and CPUQuota=60%; execute alerting with real-time scheduling (chrt -f 90) on the isolated cores to ensure predictable latency during ingestion bursts.","explanation":"## Why This Is Asked\nTests practical CPU isolation and real-time scheduling implementation in multi-tenant environments.\n\n## Key Concepts\n- CGroups v2 CPU isolation and per-service systemd slices\n- Real-Time scheduling (SCHED_FIFO) for latency-critical workloads\n- CPU affinity and NUMA binding for deterministic performance\n- Validation methodology using burst traffic patterns and SLA compliance\n\n## Code Example\n```bash\n# Conceptual: RT scheduling and CPU isolation for alerting\nsudo systemctl edit alerting.service << 'EOF'\n[Service]\nCPUAffinity=4-5\nCPUQuota=60%\nEOF\n```\n\n## Follow-up Questions\n-","diagram":"flowchart TD\n  Ingestion --> CPU_Pools\n  Alerting --> CPU_Pools\n  CPU_Pools --> SLA_Metrics\n  SLA_Metrics --> Validated","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T05:34:36.230Z","createdAt":"2026-01-15T21:46:48.385Z"},{"id":"q-2573","question":"On a Linux host running a real-time audio processing pipeline that must meet deterministic latency, a single 8-core CPU shows occasional dropouts during peak input. Design a production plan to guarantee latency isolation for the critical path: specify hardware topology, kernel boot params (isolcpus, nohz_full), IRQ affinity, CPU sets, cgroups v2 with a dedicated rt.slice using SCHED_FIFO priorities, memory locking, and a test plan with cyclictest and a synthetic workload simulating 16-channel 192 kHz audio. What exact steps would you take and how would you verify?","answer":"Configure the system with dedicated CPU isolation: boot with `isolcpus=4-7 nohz_full=4-7`, disable irqbalance service and pin network/controller IRQs to CPUs 0-3. Create a dedicated `rt.slice` cgroup v2 slice, assign real-time audio tasks exclusively to CPUs 4-7 with CPU sets, configure SCHED_FIFO priority 99 for the audio processing threads, and enable memory locking with `memlock` unlimited to prevent page swapping. Verify deterministic performance using cyclictest alongside a synthetic 16-channel 192 kHz audio workload that simulates the production processing pipeline.","explanation":"## Why This Is Asked\nThis question evaluates practical expertise in real-time Linux systems engineering, specifically CPU isolation, interrupt management, and deterministic scheduling under load. It assesses a candidate's ability to design hardware topology-aware solutions that guarantee audio processing latency requirements.\n\n## Key Concepts\n- CPU isolation with isolcpus and nohz_full for deterministic execution\n- IRQ affinity management and irqbalance service control\n- cgroups v2 hierarchy with dedicated real-time slices\n- SCHED_FIFO real-time scheduler with priority management\n- Memory locking strategies for preventing cache misses and swapping\n- Performance validation using cyclictest and synthetic workloads\n\n## Code Example\n```bash\n# GRUB configuration for CPU isolation\nGRUB_CMDLINE_LINUX=\"isolcpus=4-7 nohz_full=4-7\"\n\n# Systemd rt.slice configuration\n[Slice]\nCPUQuota=100%\nCPUAffinity=4 5 6 7\nMemoryAccounting=yes\nMemoryLimit=infinity\n\n# Real-time service configuration\n[Service]\nSlice=rt.slice\nCPUSchedulingPolicy=fifo\nCPUSchedulingPriority=99\nLimitMEMLOCK=infinity\n```","diagram":"flowchart TD\n  A[Real-time task] --> B[Isolated CPUs 4-7]\n  B --> C[SCHED_FIFO 99]\n  C --> D[MLock / memlockall]\n  D --> E[Cyclictest + audio workload test]","difficulty":"advanced","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Square","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T05:18:04.809Z","createdAt":"2026-01-15T23:33:51.532Z"},{"id":"q-2632","question":"On a Linux host running a UDP telemetry ingest pipeline at high pps, tail latency spikes under peak load threaten alerts. Design a production plan to measure end-to-end latency with eBPF (tracepoints, kprobes, uprobes) and BPF maps, identify bottlenecks (kernel vs user-space), implement mitigations (backpressure, NIC tuning, IRQ affinity, NUMA pinning), and validate with a reproducible load test and SLA targets?","answer":"Use eBPF to timestamp per-packet across the receive path and ingest app, aggregate latency histograms, and pinpoint whether tail latency is kernel, NIC, or user-space bottlenecks. Mitigate with deeper","explanation":"## Why This Is Asked\nTests hands-on observability and low-latency tuning skills.\n\n## Key Concepts\n- eBPF instrumentation across kernel and user space\n- end-to-end and tail latency analysis\n- NIC queue tuning, IRQ affinity, NUMA awareness\n- backpressure and load-testing validation\n\n## Code Example\n```javascript\n// example: placeholder for eBPF tracepoint probes and histograms\n```\n\n## Follow-up Questions\n- How would you automate this in CI?\n- How would you extend for IPv6 and multi-tenant isolation?","diagram":"flowchart TD\n  A[Start] --> B[Attach eBPF probes]\n  B --> C[Capture per-packet timestamps]\n  C --> D[Build latency histograms]\n  D --> E[Triage kernel vs user-space]\n  E --> F[Apply mitigations]\n  F --> G[Validate with load test]\n  G --> H[ SLA verification]","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","OpenAI","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T04:15:24.673Z","createdAt":"2026-01-16T04:15:24.673Z"},{"id":"q-2708","question":"A Linux server hosting a small web app sees frequent 'No space left on device' errors despite disk space being free; df -h shows space but df -i shows inode exhaustion. Outline a beginner-friendly plan to diagnose and fix inode exhaustion, including exact commands to identify hotspots, recommended cleanup or restructuring, and a simple test plan to reproduce and verify the fix?","answer":"Run inode check: df -i to confirm exhaustion. Locate hotspots with: for d in /var/log /home /tmp; do echo $d; find $d -type f | wc -l; done. If /var/log dominates inodes, purge stale files and enable ","explanation":"## Why This Is Asked\nInodes exhaustion is a common beginner pitfall that halts file creation even when space remains. It tests practical diagnosis and remediation with real commands.\n\n## Key Concepts\n- inode vs space\n- hotspot identification\n- log management and separate volumes\n- safe cleanup and testing\n\n## Code Example\n\n```bash\n# example commands\ndf -i\nfor d in /var/log /home /tmp; do echo $d; du -sh $d || true; done\n```\n\n## Follow-up Questions\n- What are inode-safe log rotation strategies?\n- How would you prevent recurrence in a multi-tenant setup?","diagram":"flowchart TD\n  A[Inode Exhaustion] --> B[Diagnose Hotspots]\n  B --> C[Purge/Rotate]\n  C --> D[Move Logs]\n  D --> E[Test Validation]","difficulty":"beginner","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Instacart","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T07:41:34.278Z","createdAt":"2026-01-16T07:41:34.278Z"},{"id":"q-2787","question":"On a Linux host running several KVM guests, one guest issues heavy bursts to a shared virtio-block device, causing host scheduler starvation and jitter in the management plane. Propose a production plan to guarantee host responsiveness and deterministic per-guest I/O latency under mixed workloads. Include host CPU pinning, NUMA affinity, VFIO isolation, IRQ steering, per-guest cgroups (cpu, io.max), and storage QoS with an appropriate I/O scheduler (bfq or mq-deadline), plus a test plan using fio and cyclictest?","answer":"Pin virtio-blk to dedicated cores, pin QEMU processes with a fixed CPU set, enable NUMA awareness and large pages for guests, isolate devices with vfio-pci, assign IRQs to those CPUs, carve per-guest ","explanation":"## Why This Is Asked\nTests practical mastery of real-world multi-tenant virtualization, CPU and I/O isolation, and QoS tuning under bursty workloads. It differentiates candidates who understand end-to-end latency guarantees from those who know only theory.\n\n## Key Concepts\n- KVM guest isolation and VFIO device binding\n- NUMA awareness and large pages for performance\n- CPU pinning, IRQ steering, and per-guest cgroups (cpu, io.max)\n- Storage QoS and I/O schedulers (bfq, mq-deadline)\n\n## Code Example\n````bash\n# Bind QEMU to CPUs 2-5 (example)\npid=$(pidof qemu-system-x86_64)\ntaskset -cp 2-5 $pid\n````\n\n## Follow-up Questions\n- How would you handle a sudden surge across multiple guests without starving the management plane?\n- What monitoring hooks and SLIs would you deploy to detect and auto-remediate degradation?\n","diagram":"flowchart TD\n  Host[Host System]\n  G1[Guest A]\n  G2[Guest B]\n  Disk[Shared Virtio Disk]\n  Host --> G1\n  Host --> G2\n  G1 -- I/O Burst --> Disk\n  G2 -- I/O Wait --> Disk","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Hashicorp"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T11:43:31.458Z","createdAt":"2026-01-16T11:43:31.458Z"},{"id":"q-2931","question":"On a Linux server hosting a web app, the /var partition has space but inode usage on /var/lib/app/cache is near exhaustion due to many tiny files created daily. Propose a production plan to prevent inode exhaustion without downtime: redesign cache storage (hashed dirs), implement cleanup/rotation, enable tmpfiles purge, and a lightweight watchdog. Include concrete commands and config snippets, plus a reproducible test plan?","answer":"Plan: 1) migrate cache to hashed dir layout using sha1/first2 chars (e.g., /var/lib/app/cache/aa/abcd...). 2) add a daily cleanup script and rotate/compress logs; delete files older than 14 days. 3) e","explanation":"## Why This Is Asked\n\nTests ability to reason about inode pressure, not just disk space. Shows practical fixes that avoid downtime and maintain SLA.\n\n## Key Concepts\n\n- Inodes vs disk space\n- Cache storage layout and HA safety\n- Automated cleanup and tmpfiles\n- Lightweight watchdogs and test plans\n\n## Code Example\n\n```bash\n# migrate: create hashed dirs and move files\nmkdir -p /var/lib/app/cache/aa\n# example migration placeholder\n```\n\n## Follow-up Questions\n\n- How would you generalize this for multiple tenants?\n- What are potential pitfalls of hashed-cache layouts?","diagram":null,"difficulty":"beginner","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Snowflake","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T17:53:05.675Z","createdAt":"2026-01-16T17:53:05.675Z"},{"id":"q-2937","question":"On a fleet of 12 Linux hosts serving a high-traffic API behind a load balancer, implement zero-downtime kernel updates using live patching (kpatch/kgraft or distro Livepatch). Design a production plan covering patch discovery, staging, canary validation, rollout strategy, rollback criteria, and instrumentation to ensure SLA adherence during updates?","answer":"Design a zero-downtime kernel update plan using live patching (kpatch/kgraft or distro Livepatch). Include patch discovery, staged canary, automated validation (crash-free boot, perf tests), rolling r","explanation":"Why This Is Asked\n\nAssesses practical mastery of live patching in prod: discovery, validation, and safe rollout with rollback under strict SLAs.\n\nKey Concepts\n\n- Live patching mechanisms (kpatch, kgraft, distro Livepatch)\n- Canary and rolling rollout strategies\n- Validation: crash-free boot, perf/load tests, failover checks\n- Telemetry: MTTR, patch success rate, SLA metrics\n\nCode Example\n\n```bash\n# High-level patch flow (illustrative)\nPATCH_DIR=/patches\nkgraft apply $PATCH_DIR/patch1.kpatch\nsystemctl restart api-service\n```\n\nFollow-up Questions\n\n- How would you validate rollback under patch failure scenarios? \n- Which metrics determine patch readiness across all nodes and how would you automate alerting?","diagram":"flowchart TD\n  A[Detect patch available] --> B[Validate patch on canary]\n  B --> C[Rollout to subset]\n  C --> D[Full rollout]\n  D --> E[Monitor metrics]\n  E --> F{Issue?}\n  F -->|Yes| G[Rollback patch]\n  F -->|No| H[Continue]","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","DoorDash","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T18:43:41.436Z","createdAt":"2026-01-16T18:43:41.437Z"},{"id":"q-3013","question":"On a Linux host running a multi-tenant container stack over a single 25 GbE NIC, one tenant deploys a high-volume eBPF-based load balancer that exhausts kernel memory via per-connection maps, risking SLA for others. Design a production plan to cap per-tenant BPF map quotas, enforce cgroup v2 memory.max, io.max, isolate CPUs, and validate with reproducible traffic and memory graphs. Include concrete steps and a test plan?","answer":"Implement per-tenant BPF resource isolation using cgroup v2 quotas, CPU pinning, and bpffs namespaces. Create dedicated cgroups for each tenant with memory.max and io.max limits, establish per-tenant bpffs mount points, attach BPF programs to tenant cgroups, and enforce CPU isolation through systemd or cpusets. Monitor resource usage with bpftool, perf, and cgroup metrics.","explanation":"## Why This Is Asked\nDesigning per-tenant resource isolation for kernel-space workloads (BPF) on a NIC-limited host tests practical knowledge of cgroup v2, bpffs, CPU isolation, and observabilityâ€”critical skills for multi-tenant container environments.\n\n## Key Concepts\n- BPF map quotas per tenant\n- cgroup v2 memory.max and io.max enforcement\n- bpffs per-tenant namespaces\n- CPU isolation and pinning strategies\n- Observability with bpftool and perf\n\n### Code Example\n```bash\n# Create per-tenant cgroup with quotas\nmkdir -p /sys/fs/cgroup/unified/tenantA\necho 1024M > /sys/fs/cgroup/unified/tenantA/memory.max\necho 500M > /sys/fs/cgroup/unified/tenantA/io.max\n```","diagram":"flowchart TD\n  A[Start] --> B[Identify BPF resource usage per tenant]\n  B --> C[Create per-tenant cgroups and bpffs roots]\n  C --> D[Apply memory.max and io.max per tenant]\n  D --> E[Isolate CPUs and pin workloads]\n  E --> F[Test plan and monitoring]\n  F --> G[Validate SLA]","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Apple","Bloomberg"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T06:04:14.235Z","createdAt":"2026-01-16T21:32:20.492Z"},{"id":"q-3168","question":"On a Linux host running multiple longâ€‘lived services, a single process intermittently hits the file descriptor limit during peak traffic, breaking log shipping. Design a production plan to prevent FD exhaustion: enforce perâ€‘process limits (LimitNOFILE), raise fs.file-max, enable systemd TasksMax, isolate services with dedicated cgroups, add a watchdog that restarts on quota breach, and implement a burst FD test to verify. What concrete steps would you take?","answer":"Per-process and system-wide FD hardening plan: set LimitNOFILE high on each unit, raise fs.file-max, enable systemd TasksMax, isolate services with dedicated cgroups, add a watchdog that restarts on q","explanation":"## Why This Is Asked\n\nTests ability to translate FD limits into concrete production controls: per-unit limits, system-wide caps, and containment via cgroups. It also probes validation under bursty conditions and automated remediation.\n\n## Key Concepts\n\n- fs.file-max and perâ€‘process limits (LimitNOFILE)\n- systemd TasksMax and cgroups\n- watchdog, auto-remediation, monitoring\n\n## Code Example\n\n```bash\n# Example: per-service FD limit\n[Service]\nLimitNOFILE=1048576\n```\n\n## Follow-up Questions\n\n- How would you test burst scenarios and verify SLA under peak?\n- How would you audit changes across services and roll back?\n","diagram":"flowchart TD\n  A[FD limit breach] --> B[Detect]\n  B --> C[Enforce per-process limits]\n  C --> D[Isolate via cgroups]\n  D --> E[Watchdog: restart/recycle]\n  E --> F[Burst FD test & verification]","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Robinhood","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T05:32:42.285Z","createdAt":"2026-01-17T05:32:42.285Z"},{"id":"q-3309","question":"On a Linux host running multiple tenants' containers sharing a single 40Gbps NIC, design a production plan for deterministic per-tenant network performance: telemetry and enforcement without sacrificing throughput. Propose a plan using XDP/eBPF to (a) tag/redirect packets to per-tenant maps, (b) enforce per-tenant egress rate via tc or XDP, (c) minimize overhead with high flow counts, (d) test with synthetic traffic. Include kernel config, BPF maps, and verification steps?","answer":"Use XDP with a per-tenant BPF map keyed by tenant_id to maintain counters and enforce rate via a token-bucket limiter. Attach an XDP program to the primary NIC to tag packets and steer excess traffic ","explanation":"## Why This Is Asked\n\nThis question tests practical XDP/eBPF planning for multi-tenant networks.\n\n## Key Concepts\n\n- XDP, eBPF maps, per-tenant isolation, rate limiting, queueing disciplines\n- tc vs XDP-based enforcement, multi-queue NICs, low overhead\n- Observability and verification with synthetic traffic\n\n## Code Example\n\n```c\n#include <linux/bpf.h>\n#include <bpf/bpf_helpers.h>\n\nSEC(\"xdp\")\nint xdp_prog(struct xdp_md *ctx) {\n  // skeleton: tenant_id extraction and rate enforcement\n  return XDP_PASS;\n}\n\nchar _license[] SEC(\"license\") = \\\"GPL\\\";\n```\n\n## Follow-up Questions\n\n- How would you scale maps for 10k tenants?\n- What failure modes and mitigations exist with XDP in multi-tenant env?\n","diagram":null,"difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T10:44:48.860Z","createdAt":"2026-01-17T10:44:48.861Z"},{"id":"q-3393","question":"On a Linux host acting as a 40 Gbps edge router, implement per-tenant ingress rate limiting and DDoS protection using an XDP-based fast path. Design a BPF program with per-tenant maps (tenant_id -> token bucket), dynamic quota reloads, and safe memory management. Explain topology, attach mode, safety against OOM, and a test plan with multi-tenant traffic to validate fairness and latency?","answer":"Use XDP in native mode at ingress with a per-tenant BPF hash map (tenant_id -> token bucket). Forward if bucket>0 (atomically decremented); otherwise XDP_DROP or rate-limit. Update quotas live from us","explanation":"## Why This Is Asked\\nThis question probes practical, production-grade use of eBPF/XDP for high-throughput, low-latency traffic control with per-tenant isolation.\\n\\n## Key Concepts\\n- eBPF maps and per-tenant state\\n- XDP, native mode, and fast path decisions\\n- Token bucket rate limiting and atomic operations\\n- Live map updates and safety/OOM considerations\\n- Observability: bpftool, perf, and counters\\n\\n## Code Example\\n```c\\n// Skeleton XDP program structure\\n#include <linux/bpf.h>\\n#include <bpf/bpf_helpers.h>\\n\\nstruct bpf_map_def SEC(\\\"maps\\\"){ /* legacy */ };\\n\\nSEC(\\\"xdp\\\")\\nint xdp_prog(struct xdp_md *ctx){\\n    // lookup tenant_id, fetch bucket, decrement atomically, forward or drop\\n    return XDP_PASS;\\n}\\n\\nchar _license[] SEC(\\\"license\\\") = \\\"GPL\\\";\\n```\\n\\n## Follow-up Questions\\n- How would you handle tenant churn and map growth?\\n- How would you rate-limit control traffic while preserving low latency for legitimate flows?","diagram":null,"difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Amazon","LinkedIn"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T14:34:22.953Z","createdAt":"2026-01-17T14:34:22.953Z"},{"id":"q-3522","question":"On a Linux host running a high-volume NAT gateway with multiple containers behind a single 100Gb NIC, a surge in new connections triggers nf_conntrack contention and latency spikes. Design a production plan to guarantee sub-2ms connection-setup latency at the 99th percentile: tune nf_conntrack (max, hashsize), enable per-namespace conntrack tables, adopt an eBPF/XDP path for early filtering, isolate tenants with separate netns and tc policing, and instrument with bpftrace. Include exact steps and a test plan with 10k concurrent connections?","answer":"Raise nf_conntrack_max and nf_conntrack_buckets; enable per-namespace conntrack tables; deploy a small eBPF/XDP program to pre-filter invalid/duplicate connections; pin NAT/conntrack processing to ded","explanation":"## Why This Is Asked\n\nTests practical networking tuning for multi-tenant NAT gateways, focusing on conntrack, eBPF/XDP, and per-tenant isolation.\n\n## Key Concepts\n\n- nf_conntrack tuning (max, buckets)\n- eBPF/XDP for early filtering\n- NetNS isolation and tc policing\n- High-concurrency performance verification\n\n## Code Example\n\n```c\n// XDP skeleton\n#include <linux/bpf.h>\n#include <bpf/bpf_helpers.h>\nSEC(\"xdp\")\nint xdp_prog(struct xdp_md *ctx) {\n    // placeholder\n    return XDP_PASS;\n}\n```\n\n## Follow-up Questions\n\n- What are risks of per-namespace conntrack tables?\n- How would you monitor regressions post-rollout?","diagram":"flowchart TD\n  A[Connection Surge] --> B[NF Conntrack Check]\n  B --> C[XDP Early Filtering]\n  C --> D[Per-Namespace NAT]\n  D --> E[Tenant Shaping]\n  E --> F[Metrics & Rollback]","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","DoorDash","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T19:35:43.645Z","createdAt":"2026-01-17T19:35:43.645Z"},{"id":"q-3570","question":"On a Linux host running multiple high-sensitivity services in a NUMA system, memory pressure from one service causes paging and latency spikes for others. Propose a production plan to strictly bound per-service memory usage and preserve throughput. Include concrete steps: enabling cgroup v2 quotas (memory.max, memory.high, memory.swap.max), per-service NUMA binding, kernel tunables (vm.min_free_kbytes, vm.swappiness), and a test plan with 10k allocations and 60-minute stability checks?","answer":"Implement per-service memory quotas via cgroup v2: create dedicated slices for each service; enforce memory.max as a hard cap, memory.high as a soft target, and memory.swap.max to limit swap usage. Bind processes to specific NUMA nodes using numactl to ensure local memory allocation, configure kernel tunables (vm.min_free_kbytes to reserve system memory, vm.swappiness to control paging behavior), and validate with a comprehensive test plan.","explanation":"## Why This Is Asked\nTests depth in memory isolation, NUMA awareness, and production readiness. It targets real-world constraints: per-service quotas, NUMA local allocations, and careful kernel tuning to avoid spillover under pressure.\n\n## Key Concepts\n- cgroup v2 memory controls: memory.max, memory.high, memory.swap.max\n- NUMA binding and local memory allocation (numactl)\n- Kernel tunables: vm.min_free_kbytes, vm.swappiness, THP impact\n- Observability: per-slice memory usage and latency metrics\n\n## Code Example\n```\n# /etc/systemd/system/serviceA.slice\n[Slice]\nCPUAccounting=true\nMemoryAccounting=true\nMemoryMax=8G\nMemoryHigh=7G\nMemorySwapMax=1G\n```\n\n## Implementation Steps\n1. Create systemd slices with memory quotas\n2. Configure NUMA binding via numactl or systemd CPUAffinity\n3. Set kernel tunables: vm.min_free_kbytes=1G, vm.swappiness=10\n4. Deploy monitoring: pressure stall information, per-cgroup stats\n5. Execute test plan: 10k allocations, 60-minute stability validation","diagram":"flowchart TD\n  A[Service] --> B[Quota: memory.max]\n  B --> C[NUMA binding (numactl)]\n  C --> D[Soft limit: memory.high]\n  D --> E[Swap cap: memory.swap.max]","difficulty":"advanced","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Databricks","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T05:43:38.022Z","createdAt":"2026-01-17T21:36:45.544Z"},{"id":"q-3662","question":"On a Linux server running a high-volume data ingestion service that ships logs to a cloud sink, sporadic tail latency spikes occur under burst I/O. Design a production plan to guarantee deterministic latency for the ingestion path: specify hardware topology (NUMA affinities), kernel boot params (isolcpus, nohz_full), IRQ affinity, CPU sets, and a containers/systemd v2 cgroup layout with a dedicated rt.slice or critical.slice using io.max and a CPU quota, memory locking, and a concrete test plan with fio and cyclictest simulating burst I/O. What exact steps would you take and how would you verify?","answer":"Pin ingestion workers to a dedicated NUMA node, isolate CPUs with isolcpus/nohz_full, bind IRQs to that node, and place the process in a v2 cgroup (critical.slice) with cpu.max, io.max, and memlock, p","explanation":"## Why This Is Asked\nTests ability to design end-to-end latency isolation on a single host using NUMA awareness, kernel boot parameters, IRQ affinity, and precise cgroup isolation. It examines practical trade-offs between throughput, isolation, and complexity.\n\n## Key Concepts\n- NUMA-aware CPU/Memory Placement\n- Kernel isolation: isolcpus, nohz_full, and CPU shielding\n- IRQ affinity and CPU sets in a cgrouped environment\n- cgroup v2: cpu.max, io.max, memory.high, memlock\n- Latency testing: fio burst I/O and cyclictest tail latency\n\n## Code Example\n```bash\n# Example: create a dedicated NUMA node and isolate a slice\n# 1) Isolate CPUs 2-5 on NUMA node 1\nsudo isolcpus=2-5 nohz_full=2-5 bootparams\n# 2) Create critical.slice and assign limits\nsystemd-run --unit=critical.slice --slice=critical.slice --property=CPUQuota=25%\n# 3) Bind ingestion process to the slice and CPUs\ntaskset -c 2-5 ./ingest_service\n```\n\n## Follow-up Questions\n- How would you extend this plan to a multi-node cluster with live failover?\n- Which monitoring hooks (perf, ftrace, iostat, pidstat) would you wire for ongoing validation?\n","diagram":null,"difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Stripe","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T04:15:39.538Z","createdAt":"2026-01-18T04:15:39.538Z"},{"id":"q-3726","question":"On a Linux host running an in-memory database that uses hugepages, intermittent THP defragmentation spikes cause tail latency regressions. Design a production plan to eliminate THP pauses while preserving memory efficiency. Include exact steps to disable THP, reserve hugepages for the DB, pin memory (memlock), NUMA binding and per-service cgroups, and a test plan proving p95 latency stays under target at 50k qps; add rollback?","answer":"Disable THP completely for the DB path, reserve dedicated hugepages, pin memory (memlock) via rlimits/mlockall, bind the process to specific NUMA nodes, and isolate it in its own cgroup. Verify p95 la","explanation":"## Why This Is Asked\nTail latency in latency-sensitive DB workloads can be dominated by kernel memory management; THP defrag can cause long stalls. A deterministic plan is needed for production.\n\n## Key Concepts\n- Transparent Huge Pages (THP) and defragmentation impact on latency\n- Hugepages reservation and NUMA affinity\n- memlock/mlockall, per-service cgroups, and resource isolation\n- Validation metrics and rollback procedures\n\n## Code Example\n```javascript\n// Pseudo test harness outline for latency validation in JS\nfunction testLatency(targetQps, p95) { /* implement synthetic workload and monitor latency */ }\n```\n\n## Follow-up Questions\n- How would you monitor THP activity in production?\n- What are the trade-offs of reserving hugepages vs. sharing them across processes?\n","diagram":"flowchart TD\n  A[DB process] --> B[Disable THP]\n  B --> C[Reserve HugePages]\n  C --> D[Pin Memory (mlock)]\n  D --> E[NUMA & CGroups isolation]\n  E --> F[Test & validate latency]\n  F --> G[Rollback path if needed]","difficulty":"advanced","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Lyft","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T07:27:54.478Z","createdAt":"2026-01-18T07:27:54.478Z"},{"id":"q-3866","question":"On a Linux host running multiple KVM guests with shared storage, a noisy neighbor VM spikes I/O and delays CI workloads on a critical VM. Propose a practical plan to guarantee sub-5ms tail I/O latency for that VM, including concrete steps to (a) switch to BFQ I/O scheduler, (b) enforce per-VM I/O limits with cgroup v2 io.weight/io.max, (c) isolate disks or queues for the critical VM, and (d) a test plan with reproducible load and latency verification?","answer":"Enable BFQ as the IO scheduler for the disks backing VMs; create per-VM cgroups under /sys/fs/cgroup/io, set io.weight higher for the critical VM and io.max for others, and consider dedicating a disk/","explanation":"## Why This Is Asked\n\nTests practical I/O isolation on Linux, combining kernel schedulers, cgroup v2 IO controls, and virtualization realities.\n\n## Key Concepts\n\n- BFQ I/O scheduler for per-VM isolation\n- cgroup v2 io.weight and io.max for per-VM controls\n- Virtualization impact on disk I/O and storage tiering\n- Realistic testing with fio/VDbench and latency targets\n\n## Code Example\n\n```javascript\n# Example commands (illustrative)\necho bfq | sudo tee /sys/block/sda/queue/scheduler\nsudo mkdir -p /sys/fs/cgroup/io/critical_vm\nsudo bash -c 'echo 256 > /sys/fs/cgroup/io/critical_vm/io.weight'\n```\n\n## Follow-up Questions\n\n- How would you adapt this plan to a Kubernetes node with per-pod I/O isolation?\n- What monitoring would you add to detect I/O issues before CI workloads are affected?","diagram":null,"difficulty":"beginner","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","LinkedIn","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T13:07:01.474Z","createdAt":"2026-01-18T13:07:01.474Z"},{"id":"q-3875","question":"On a Linux host handling high-velocity production workloads, implement a low-overhead tracing plan to debug intermittent latency spikes without impacting throughput? Design an approach using eBPF/BPFtrace to capture key latency paths, isolate tenants, and trigger alerts for SLA breaches. Include what to instrument, data structures, sampling strategy, and a test plan?","answer":"Use low-overhead eBPF tracing (BCC/BPFtrace) to instrument the critical latency path. Attach kprobes/tracepoints to scheduler, block IO, and network receive paths; filter by cgroup/docker container ID","explanation":"## Why This Is Asked\nIntermittent latency spikes are common in prod. This question asks for a practical, low-overhead tracing strategy that scales with tenants and provides actionable data.\n\n## Key Concepts\n- eBPF tracing\n- kprobes/tracepoints\n- per-tenant isolation via cgroups\n- ring buffers and histogram maps\n- alerting and baselining\n\n## Code Example\n```c\n// skeleton BPF program snippet\nBPF_HASH(latency, u32, u64);\n```\n\n## Follow-up Questions\n- How would you handle multi-tenant fairness if one tenant dominates traces?\n- How would you measure instrumentation overhead and bound it?\n","diagram":"flowchart TD\n  A[Start tracing] --> B{Source}\n  B --> C[kprobes/tracepoints]\n  C --> D[Filter by cgroup]\n  D --> E[Per-CPU buffers]\n  E --> F[Histograms map]\n  F --> G[Alerts]","difficulty":"advanced","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","Plaid","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T13:38:40.690Z","createdAt":"2026-01-18T13:38:40.691Z"},{"id":"q-3908","question":"On a Linux host running a high-throughput log ingestion stack (rsyslog -> local NVME store -> analytic pipeline) experiencing intermittent 100â€“300 ms I/O latency under peak writes. Design a production plan to diagnose and eliminate storage I/O bottlenecks while preserving log reliability. Include: I/O schedulers, per-disk vs per-tenant I/O shaping (cgroups v2 io.max), test plan with fio/workloads, and validation steps?","answer":"Plan to reproduce, bench, and isolate I/O bottlenecks: 1) instrument with iostat -dx, ioping, and blktrace; 2) compare IO schedulers (deadline vs bfq) and two queue depths; 3) implement per-tenant io.","explanation":"## Why This Is Asked\nTests the ability to translate real-world I/O path issues into concrete controls and validation, balancing throughput with reliability.\n\n## Key Concepts\n- I/O scheduling and devices tuning\n- cgroups v2 io.max for per-tenant shaping\n- Observability with iostat, blktrace, ioping, fio\n- SLA-driven latency validation and backpressure handling\n\n## Code Example\n```bash\n# sample fio job (simplified)\n[write_job]\nrw=write\nbs=4k\nsize=1000M\nioengine=libaio\n```\n\n## Follow-up Questions\n- How would caching and filesystem flags impact results?\n- How would you automate regression tests for future hardware changes?","diagram":"flowchart TD\n  A[Storage Bottleneck] --> B[Measure Metrics]\n  B --> C[Test Schedulers & Queue Depths]\n  C --> D[CGroup io.max per Tenant]\n  D --> E[Run fio workloads]\n  E --> F[Validate SLA & Backlog Avoidance]","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T14:43:00.557Z","createdAt":"2026-01-18T14:43:00.557Z"},{"id":"q-3958","question":"On a Linux host running both a real-time data-feed and a batch analytics job sharing one NVMe pool, bursts trigger 2â€“6 ms stalls. Design a plan to guarantee deterministic tail latency for the data-feed: per-tenant IO isolation (io.max), CPU pinning, NUMA-aware memory, appropriate I/O scheduler, and a test harness with mixed READ/WRITE fio workloads to verify p99 latency and failure modes?","answer":"Plan: enforce per-tenant I/O limits with cgroups v2 io.max, pin data-feed to a dedicated CPU set on a single NUMA node, use an NVMe-friendly mq-deadline scheduler, prefer io_uring paths, isolate workl","explanation":"## Why This Is Asked\n\nAssesses practical storage isolation and latency guarantees under bursty multi-tenant workloads on NVMe, a common real-world constraint in quant-heavy environments.\n\n## Key Concepts\n\n- cgroups v2 io.max and io.weight for per-tenant isolation\n- NUMA-aware CPU and memory binding to minimize cross-node latency\n- NVMe-optimized I/O schedulers (mq-deadline, bfq considerations)\n- IO paths: io_uring vs AIO in high-throughput scenarios\n- Reproducible testing with fio and latency KPIs (p99) and failover checks\n\n## Code Example\n\n```javascript\n// Example: quick fio test config (illustrative, not executable)\nconst job = { rw: 'randrw', bs: '4k', iodepth: 32, ioengine: 'io_uring' };\n```\n\n## Follow-up Questions\n\n- How would you validate recovery when a disk goes offline or a rebuild starts?\n- How would you monitor per-tenant latency in production and automate spillover to a separate pool?","diagram":null,"difficulty":"advanced","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Plaid","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T17:27:38.921Z","createdAt":"2026-01-18T17:27:38.921Z"},{"id":"q-4020","question":"On a Linux host running a multi-tenant data-collection service, tail latency spikes appear under 99th percentile during bursts. Outline a production plan to diagnose and mitigate root causes using eBPF tracing (bpftrace), per-tenant metrics, and strict QoS isolation. Include steps to install tracing, collect sched/net/block latency per tenant, enforce per-tenant cgroups v2 with cpu.max and io.max, and validate with synthetic bursts. What exact steps would you take and how would you revert if latency worsens?","answer":"Instrument with eBPF (bpftrace) to trace per-tenant sched_latency, net_xmit, and block IO; collect p95/p99 latency per tenant; enforce per-tenant cgroups v2 with cpu.max and io.max; throttle noisy ten","explanation":"## Why This Is Asked\\n\\nThis tests real-world ability to diagnose tail latency in multi-tenant Linux environments using advanced tracing, QoS, and safe production changes.\\n\\n## Key Concepts\\n- eBPF tracing (bpftrace)\\n- per-tenant QoS with cgroups v2\\n- latency metrics (p95/p99) and synthetic workloads\\n\\n## Code Example\\n```javascript\\n// illustrative bpftrace snippet (not runnable)\\nBEGIN { /* setup */ }\\nEND { /* teardown */ }\\n```\\n\\n## Follow-up Questions\\n- How would you adapt if tenants share a single NVMe?\\n- How would you calibrate p99 thresholds and alerting?","diagram":null,"difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","Snowflake","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T20:31:47.246Z","createdAt":"2026-01-18T20:31:47.248Z"},{"id":"q-881","question":"On a Linux host, a long-running daemon writes to `/var/log/myapp.log` and is managed by systemd, but log rotation occasionally causes logging to stop after rotation. Propose a practical fix to ensure logging continues after rotation without restarting the daemon. Include the exact approach and a sample logrotate config snippet and testing steps?","answer":"Configure logrotate to reopen the log file after rotation instead of copying or truncating. After rotating, signal the daemon to reopen logs (e.g., SIGHUP). The config should include a postrotate that","explanation":"## Why This Is Asked\nTests practical handling of log rotation, signals, and ensuring service continuity without downtime. It checks knowledge of logrotate postrotate scripts, choosing the right approach (signal vs copytruncate), and how to validate in a controlled test.\n\n## Key Concepts\n- logrotate configuration fields and postrotate scripts\n- signaling daemons (SIGHUP) to reopen logs\n- copytruncate vs signaling trade-offs\n- testing routine for rotation\n\n## Code Example\n```bash\n/var/log/myapp.log {\n  rotate 5\n  weekly\n  missingok\n  notifempty\n  postrotate\n    kill -HUP `cat /var/run/myapp.pid` 2>/dev/null || true\n  endscript\n}\n```\n\n## Follow-up Questions\n- What are the trade-offs of copytruncate vs signaling?\n- How would you monitor and alert if log rotation fails to reopen logs?","diagram":null,"difficulty":"beginner","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Meta","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T13:59:55.582Z","createdAt":"2026-01-12T13:59:55.582Z"},{"id":"q-888","question":"A production Linux host runs a data-backup agent that uses /var/lib/backup/backup.lock to enforce a single instance. Sometimes a stale lock remains after a crash, blocking new runs; the agent also leaves non-terminating children on stop, risking partial backups. Propose a systemdâ€‘based lifecycle fix: ensure one instance, auto-clean stale lock, and graceful stop with timeout and fallback to kill. Include concrete unit snippets and verification steps?","answer":"Use a systemd unit with Type=forking, PIDFile, and ExecStartPre that checks and clears a stale lock: if /var/lib/backup/backup.lock exists and its PID is not running, delete it. Stop uses KillMode=con","explanation":"## Why This Is Asked\nTests understanding of robust service lifecycle management with systemd, handling singleton constraints, and clean termination of complex processes.\n\n## Key Concepts\n- systemd lifecycle: ExecStartPre, ExecStop, KillMode, TimeoutStopSec\n- singleton enforcement via lock files\n- graceful termination vs. forceful kill for child processes\n\n## Code Example\n```ini\n; /etc/systemd/system/backup-agent.service\n[Unit]\nDescription=Backup Agent\nAfter=network.target\n\n[Service]\nType=forking\nPIDFile=/var/run/backup/backup.pid\nExecStartPre=/bin/sh -c 'LOCK=/var/lib/backup/backup.lock; if [ -e \"$LOCK\" ]; then pid=$(cat \"$LOCK\"); if [ -d /proc/$pid ]; then exit 1; else rm -f \"$LOCK\"; fi; fi'\nExecStart=/usr/local/bin/backup-agent\nExecStop=/bin/kill -TERM $MAINPID\nExecStopPost=/bin/rm -f /var/lib/backup/backup.lock\nTimeoutStopSec=120s\nKillMode=control-group\nRestart=on-failure\n```\n\n## Follow-up Questions\n- How would you test idempotency for consecutive startups?\n- How would you adapt if the agent uses a PID file instead of a lock file?","diagram":null,"difficulty":"advanced","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T14:27:44.056Z","createdAt":"2026-01-12T14:27:44.056Z"},{"id":"q-961","question":"On a Linux host, a memory-hungry log-harvester daemon managed by systemd sporadically triggers the kernel OOM killer during peak load, crashing the service and delaying alerts. Propose a production-safe plan to prevent OOM termination while preserving throughput. Include concrete systemd settings (MemoryLimit, MemorySwapMax, OOMScoreAdjust, Restart), kernel tuning (swap, swappiness), and a test plan with a reproducible high-memory scenario and verification steps?","answer":"Apply per-service memory controls and OOM protection. Set MemoryLimit=2G, MemorySwapMax=2G, OOMScoreAdjust=-100, and Restart=on-failure with a 5s backoff. Tune vm.swappiness=10 and ensure swap is enab","explanation":"## Why This Is Asked\nTests memory pressure handling, systemd tuning, and safe recovery without service disruption.\n\n## Key Concepts\n- Kernel OOM killer and oom_score_adj\n- systemd memory constraints (MemoryLimit, MemorySwapMax)\n- Restart strategies and timeouts\n- memory tuning and swap behavior\n\n## Code Example\n```ini\n[Unit]\nDescription=Log Harvester\n\n[Service]\nExecStart=/usr/local/bin/logharvester\nType=simple\nMemoryLimit=2G\nMemorySwapMax=2G\nOOMScoreAdjust=-100\nRestart=on-failure\nRestartSec=5s\n```\n\n## Follow-up Questions\n- How would you observe and alert on OOM events?\n- How would you adjust for multiple high-memory services competing for swap?","diagram":"flowchart TD\n  A[Memory Pressure] --> B{OOM killer?}\n  B -->|Yes| C[Adjust OOMScore/MemoryLimit]\n  B -->|No| D[Normal Operation]\n  C --> E[Daemon Survives, backlog Drains]","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T17:22:50.353Z","createdAt":"2026-01-12T17:22:50.353Z"},{"id":"linux-foundation-sysadmin-networking-1768260262823-4","question":"You suspect ARP storms on a host with interface eth1. To quickly verify ARP traffic and identify abnormal ARP activity on that interface, which command should you run?","answer":"To quickly verify ARP traffic and identify abnormal ARP activity on interface eth1, run 'tcpdump -i eth1 arp'. This command captures and displays ARP packets specifically on the eth1 interface, allowing you to monitor for ARP storms.","explanation":"## Correct Answer\nA. tcpdump with the arp filter on the specific interface captures ARP requests/replies, which is exactly what you need to observe ARP storms.\n\n## Why Other Options Are Wrong\n- Option B: ICMP captures are unrelated to ARP activity.\n- Option C: TCP traffic does not reveal ARP behavior.\n- Option D: arp -a shows ARP table entries but not live ARP traffic, making storms harder to observe in real time.\n\n## Key Concepts\n- ARP monitoring with packet captures\n- Interface-specific traffic analysis\n- Distinguishing ARP storms from normal ARP chatter\n\n## Real-World Application\n- Proactively diagnosing network broadcast storms and ARP-related outages in data-center or campus networks.","diagram":null,"difficulty":"intermediate","tags":["Networking","ARP","Linux","Kubernetes","certification-mcq","domain-weight-12"],"channel":"linux-foundation-sysadmin","subChannel":"networking","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T03:32:44.361Z","createdAt":"2026-01-12 23:24:24"}],"subChannels":["general","networking"],"companies":["Adobe","Airbnb","Amazon","Anthropic","Apple","Bloomberg","Citadel","Cloudflare","Coinbase","Databricks","Discord","DoorDash","Goldman Sachs","Google","Hashicorp","Hugging Face","IBM","Instacart","LinkedIn","Lyft","Meta","Microsoft","MongoDB","NVIDIA","Netflix","OpenAI","Oracle","PayPal","Plaid","Robinhood","Salesforce","Scale Ai","Snap","Snowflake","Square","Stripe","Tesla","Twitter","Two Sigma","Uber","Zoom"],"stats":{"total":46,"beginner":9,"intermediate":21,"advanced":16,"newThisWeek":46}}