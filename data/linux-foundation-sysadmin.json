{"questions":[{"id":"q-1044","question":"On a Linux host running multiple tenant services, peak I/O from a data ingestion daemon saturates the disk, causing latency spikes for others. Propose a production plan to isolate and throttle disk I/O across tenants using systemd slices and cgroup v2 io.max. Include concrete unit and slice snippets, per-device limits for a SATA HDD and NVMe, and a test plan using fio and iostat to verify tail latency improvements under concurrent workloads?","answer":"Implement per-tenant I/O isolation using systemd slices and cgroup v2 io.max. Create tenant-<name>.slice, assign services to it, and throttle devices (e.g., 8:0 rbps=50M wbps=50M). Include per-device ","explanation":"## Why This Is Asked\nTests ability to design resource isolation and practical configuration with systemd and cgroups in a multi-tenant environment, focusing on I/O pressure rather than CPU/memory alone.\n\n## Key Concepts\n- cgroup v2 io.max throttling\n- systemd slices and per-service isolation\n- per-device I/O limits and hierarchy\n- reproducible load testing with fio and iostat\n\n## Code Example\n```javascript\n# Example: set per-device I/O throttling for tenant-a.slice (pseudo)\nsudo mkdir -p /sys/fs/cgroup/tenant-a.slice\necho \"8:0 rbps=50M wbps=50M\" | sudo tee /sys/fs/cgroup/tenant-a.slice/io.max\n```\n\n## Follow-up Questions\n- How would you adapt if the workload alternates between read-heavy and write-heavy?\n- What are potential pitfalls with block layer throttling and caches?","diagram":null,"difficulty":"advanced","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Microsoft","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T20:30:42.832Z","createdAt":"2026-01-12T20:30:42.832Z"},{"id":"q-1056","question":"On a Linux host, a TLS proxy reads its certificate from /etc/ssl/certs/app.crt and currently reloads by restarting, causing brief downtime during renewal. Propose a zero-downtime rotation using a systemd.path trigger that fires on a new cert symlink, with a separate service unit for ExecReload and an atomic update scheme (swap in a new cert, then switch a symlink). Include concrete unit snippets and a test plan?","answer":"Propose a zero-downtime TLS cert rotation using systemd.path to trigger on an updated certificate symlink, plus a dedicated systemd service that ExecReloads the proxy. Use atomic cert updates (place n","explanation":"## Why This Is Asked\nTests practical mastery of systemd path activation, service reloads, and atomic file updates for TLS without downtime. It also validates understanding of race-free certificate rotation and testability.\n\n## Key Concepts\n- systemd.path and PathChanged/PathModified\n- ExecReload vs Restart semantics\n- Atomic file replacement via symlinks\n- Reproducible test plan for cert rotation\n\n## Code Example\n```ini\n# app-cert.path\n[Unit]\nDescription=Trigger TLS cert rotation on new cert\n\n[Path]\nPathChanged=/etc/ssl/certs/app.crt\nUnit=app-cert.service\n\n[Install]\nWantedBy=multi-user.target\n```\n```ini\n# app-cert.service\n[Unit]\nDescription=Reload TLS proxy with new certificate\n\n[Service]\nType=oneshot\nExecStart=/usr/local/bin/app-cert-rotate-reload.sh\n```\n```bash\n# app-cert-rotate-reload.sh\n#!/bin/sh\nset -e\n# Swap in the new cert and reload the proxy\nmv -f /etc/ssl/certs/app.crt.new /etc/ssl/certs/app.crt\nsystemctl reload app-proxy.service\n```\n```\n\n## Follow-up Questions\n- How to handle reload failure and ensure idempotence?\n- How to monitor for stale symlinks or missing new certs?\n- How would you extend this for multiple certificates or canary rollouts?","diagram":"flowchart TD\n  A[Cert renewal arrives] --> B[Write app.crt.new]\n  B --> C[symlink swap via script]\n  C --> D{Reload trigger}\n  D --> E[app-proxy reloads without downtime]","difficulty":"advanced","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T21:18:27.923Z","createdAt":"2026-01-12T21:18:27.923Z"},{"id":"q-1122","question":"On a Linux host running multiple ML model servers in separate systemd slices using cgroup v2, a spike in one tenant consumes memory and triggers host memory pressure despite per-tenant limits. Propose a production plan to enforce strict isolation and backpressure using memory.max and memory.high, enable group OOM behavior, and implement eviction/playback strategies. Include concrete unit and cgroup settings and a test plan with a reproducible spike and validation steps?","answer":"Implement per-tenant slices with MemoryMax and MemoryHigh in a cgroup v2 setup; attach each model server to its slice. Enable memory.oom_group for group-based OOM handling and configure a hard cap wit","explanation":"## Why This Is Asked\nTests the ability to design strict memory isolation in a multi-tenant Linux environment, covering cgroup v2 memory controls, systemd slices, and OOM behavior under load.\n\n## Key Concepts\n- cgroup v2 memory.max and memory.high for hard/soft limits\n- memory.oom_group for group-level OOM decisions\n- systemd slices and per-tenant unit configurations\n- observability: cgroup events, dmesg, and host health verification\n\n## Code Example\n```javascript\n# Example: tenant-A.slice\n[Slice]\nMemoryHigh=6G\nMemoryMax=8G\n\n# Example: tenant-A service\n[Service]\nSlice=tenant-A.slice\nExecStart=/usr/bin/modelA_server\nRestart=on-failure\n```\n\n## Follow-up Questions\n- How would you automate scale-out when new tenants are added?\n- What metrics and alerts would you surface to detect pressure early?","diagram":"flowchart TD\n  A[Tenant A] --> B[Cgroup v2 memory.max]\n  A --> C[Cgroup v2 memory.high]\n  B --> D[Host stability checks]\n  C --> D","difficulty":"advanced","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","MongoDB","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T23:29:53.881Z","createdAt":"2026-01-12T23:29:53.881Z"},{"id":"q-1230","question":"On a Linux host with a multi-tenant workload sharing a single 10GbE NIC configured with multiple receive queues, one tenant bursts UDP traffic and starves others, causing increased latency and packet loss. Propose a production plan to enforce tenant isolation and fairness using NIC multi-queue, RSS/XPS mapping, IRQ affinity, and per-tenant cgroups (v2) with io.max and tc shaping. Include concrete steps and a test plan with realistic traffic?","answer":"Plan to isolate tenants by binding NIC RX queues to per-tenant CPUs, use RSS/XPS to map flows, create per-tenant systemd slices with per-tenant CPU budgets, apply io.max quotas, and shape egress with ","explanation":"## Why This Is Asked\nAssesses practical network isolation skills in a multi-tenant Linux environment, focusing on NIC multi-queue tuning, IRQ affinity, and per-tenant resource containers to prevent bursts from one tenant affecting others.\n\n## Key Concepts\n- NIC multi-queue, RSS, XPS\n- IRQ affinity per-tenant isolation\n- cgroups v2 with io.max and per-tenant slices\n- tc shaping and fq_codel for fairness\n- Observability with iperf3/pktgen, iostat\n\n## Code Example\n```bash\n# Enable 4 RX queues\nethtool -L eth0 rx 4\n# Bind a few IRQs to CPUs (example)\necho 2-3,6-7 > /proc/irq/46/smp_affinity_list\n```\n\n## Follow-up Questions\n- How would you validate fairness under concurrent tenants?\n- What are NUMA pitfalls and how would you mitigate them?","diagram":"flowchart TD\n A[Tenants] --> B[RX queues]\n A --> C[CPU cores]\n B --> D[Mapping rules]\n C --> E[Isolation boundary]","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","MongoDB","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T05:40:43.739Z","createdAt":"2026-01-13T05:40:43.739Z"},{"id":"q-1327","question":"On a Linux host running multiple tenants via containers, a CPU-intensive workload from one tenant starves others. Propose a production plan to enforce CPU isolation using cgroup v2 slices, systemd integration, and per-tenant quotas; include concrete settings (cpu.max, cpu.weight), scheduling options (cpuset and isolcpus), and a test plan with realistic workloads and verification steps?","answer":"Create per-tenant cgroup v2 slices and assign each container to its slice. Use cpu.max and cpu.weight to cap and weight CPU usage (e.g., tenantA 250000 1000000, tenants B, C, D 150000 1000000; weights","explanation":"## Why This Is Asked\nAssesses practical mastery of modern Linux isolation (cgroup v2, systemd slices, cpuset) and real-world tradeoffs between throughput and fairness.\n\n## Key Concepts\n- cgroup v2 CPU controller: cpu.max, cpu.weight\n- systemd slices and container assignment\n- cpuset/isolation of CPUs; isolcpus boot option\n- realistic workloads: fio, sysbench; tail latency tracking\n\n## Code Example\n```bash\n# Illustrative: create and configure tenant slices (pseudo-commands)\nsystemd-run --unit=tenantA.slice --property=CPUQuota=250000 --property=CPUQuotaPeriodUS=1000000 --slice=tenantA.slice sleep 1h\nsystemd-run --unit=tenantB.slice --property=CPUQuota=150000 --property=CPUQuotaPeriodUS=1000000 --slice=tenantB.slice sleep 1h\n# Bind slices to CPUs via cpuset (illustrative)\nchmod 600 /sys/fs/cgroup/unified/tenantA.slice/cpuset.cpus\n# Attach containers to their slices as they spawn\n```\n\n## Follow-up Questions\n- How would you monitor CPU-steal and detect fairness violations in production?\n- What are risks of over-isolating CPUs, and how would you safely rebalance during load spikes?","diagram":null,"difficulty":"advanced","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Google","IBM"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T11:37:36.271Z","createdAt":"2026-01-13T11:37:36.271Z"},{"id":"q-1346","question":"On a production Linux host serving multiple ephemeral monitoring daemons, a heartbeat worker must emit a 5-second ping to a central collector. During peak load, the heartbeat misses several intervals, delaying alerts. Propose a concrete production plan to guarantee cadence and resilience using: (a) per-service CPU isolation and pinning, (b) high-priority timer/RT scheduling or systemd timers with precise tuning, (c) a fallback mechanism for missed heartbeats, and (d) a validation strategy with a controlled load. Include concrete config snippets for systemd, cpuset, and timer settings?","answer":"Plan pins heartbeat to a dedicated CPU set, uses a systemd timer with 5s cadence and AccuracySec=1s, and a high-priority heartbeat service with CPUQuota=50% and SCHED_FIFO-like behavior. Add a fallbac","explanation":"## Why This Is Asked\n\nTests knowledge of per-service isolation, timer accuracy, fault tolerance, and measurable validation.\n\n## Key Concepts\n\n- systemd slices timers CPU affinity\n- cpuset pinning and CPUQuota/AccuracySec\n- RT priority and fallback mechanisms\n- end-to-end verification under load\n\n## Code Example\n\n```\n# See /etc/systemd/system/heartbeat.* for concrete files\n```\n\n## Follow-up Questions\n\n- How would you detect missed heartbeats and trigger alerts?\n- What are the trade-offs of increasing CPU isolation vs. responsiveness?","diagram":"flowchart TD\n  A[Create heartbeat.slice] --> B[Pin heartbeat service]\n  B --> C[Configure 5s timer]\n  C --> D[Add fallback heartbeat]\n  D --> E[Test under load]","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","LinkedIn"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T13:05:27.792Z","createdAt":"2026-01-13T13:05:27.792Z"},{"id":"q-1385","question":"On a Linux host running ephemeral build jobs managed by systemd, a rogue job floods /tmp with tiny files, exhausting inodes and delaying builds. Propose a production plan to isolate and bound /tmp usage and auto-clean stale files. Include: (a) per-service PrivateTmp and tmpfs /tmp with a size cap; (b) systemd-tmpfiles cleanup rules; (c) inode usage monitoring; (d) a reproducible test that proves isolation and cleanup. How would you implement this?","answer":"Configure per-service isolation: set PrivateTmp=true and PrivateDevices=true for build services; mount /tmp as tmpfs with size=128M and mode=1777; add systemd-tmpfiles cleanup rules to prune stale fil","explanation":"## Why This Is Asked\nAssesses practical isolation of temporary storage for multi-tenant build workloads and how to enforce cleanups under bursty IO.\n\n## Key Concepts\n- systemd PrivateTmp and PrivateDevices\n- tmpfs sizing and mounting options\n- inode monitoring and alerting (df -i, thresholds)\n- systemd-tmpfiles cleanup rules and scheduling\n\n## Code Example\n```javascript\n// Example pseudo-configuration snippets for illustration\n// systemd unit: PrivateTmp=true\n// /etc/fstab: tmpfs /tmp tmpfs size=128M,mode=1777 0 0\n```\n\n## Follow-up Questions\n- How would you scale this across a cluster of CI workers?\n- What are potential edge cases with cleanup timing and race conditions?","diagram":null,"difficulty":"beginner","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Coinbase","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T14:47:50.803Z","createdAt":"2026-01-13T14:47:50.803Z"},{"id":"q-1490","question":"On a Linux server, a small web app runs as a systemd service and reads its port and log level from /etc/myapp/config.yaml. Changes to this file must apply without dropping connections. Design a production-safe hot-reload mechanism using systemd ExecReload, a SIGHUP-based reload, and a small validation script. Include unit file snippets and a test plan with a reproducible config change and verification steps?","answer":"Use systemd ExecReload to send SIGHUP to the daemon and a small validator script. Create /usr/local/bin/myapp_reload to validate /etc/myapp/config.yaml (e.g., ensure .port and .log_level exist) and th","explanation":"## Why This Is Asked\nTests hot-reload capability and safe config changes without dropping connections.\n\n## Key Concepts\n- systemd ExecReload\n- SIGHUP reload semantics\n- config validation (YAML)\n- graceful reload vs restart\n- test plan design\n\n## Code Example\n```javascript\n// Implementation sketch\n```\n\n## Follow-up Questions\n- How would you handle a daemon that does not support SIGHUP?\n- How would you verify that in-flight requests survive a reload?","diagram":"flowchart TD\n  A[config.yaml change] --> B[validate YAML]\n  B --> C{valid?}\n  C -->|yes| D[send SIGHUP]\n  D --> E[service reload]\n  E --> F[verify endpoints]\n  C -->|no| G[abort reload]","difficulty":"beginner","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T19:03:23.816Z","createdAt":"2026-01-13T19:03:23.816Z"},{"id":"q-1497","question":"On a Linux host running multiple tenants in systemd services sharing a single NVMe SSD and a 25Gbps NIC, a new requirement enforces strict I/O isolation to prevent a noisy tenant from stalling others. Propose a production plan to enforce per-tenant I/O isolation and backpressure using: (a) cgroup v2 io.max per service; (b) per-tenant filesystem isolation via PrivateMounts and PrivateDevices with a dedicated data path; (c) I/O scheduler tuning (none/deadline) and per-tenant block IO queuing discipline with tc; (d) a deterministic test plan with a reproducible burst scenario and rollback steps. Include concrete config snippets for systemd units and cgroups?","answer":"Plan: use per-tenant cgroup v2 io.max to cap disk writes, create tenant.slice with PrivateMounts for isolation, assign each service to a sub-cgroup, attach per-tenant tc qdisc to shape writes, and iso","explanation":"## Why This Is Asked\nTests hands-on mastery of Linux IO isolation primitives under multi-tenant workloads and production risk management.\n\n## Key Concepts\n- cgroup v2 io.max\n- systemd Slice and isolation\n- tc/bpf/qdisc for per-tenant IO shaping\n- PrivateMounts/PrivateDevices for filesystem isolation\n- Reproducible test methodology with fio\n\n## Code Example\n```javascript\n# systemd unit\n[Unit]\nDescription=Tenant A worker\nAfter=network-online.target\n\n[Service]\nSlice=tenant-A.slice\nPrivateMounts=yes\nPrivateDevices=yes\nExecStart=/usr/local/bin/tenant-a-worker\n```\n\n```javascript\n# cgroup v2 IO limit (pseudo)\nmkdir -p /sys/fs/cgroup/tenant-A\necho \"8:0 100M\" > /sys/fs/cgroup/tenant-A/io.max\n```\n\n## Follow-up Questions\n- How would you roll back if another tenant regressed?\n- What metrics and alerting would you install to detect drift?","diagram":null,"difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T19:32:24.815Z","createdAt":"2026-01-13T19:32:24.816Z"},{"id":"q-1588","question":"On a dual-socket Linux host running PostgreSQL and a data-processor ETL job, both services contend for memory and cache across NUMA nodes; design an advanced plan to enforce NUMA-aware isolation with cpuset/memcg, disable NUMA balancing, set per-node IRQ affinity, and validate via targeted benchmarks. Include concrete commands and a test plan?","answer":"Pin two services to separate NUMA nodes using cpuset and memory binding; PostgreSQL on node0, ETL on node1. Disable NUMA balancing, set per-node IRQ affinity, and ensure memory policies via memcg. Validate isolation through targeted benchmarks measuring memory locality, cache coherence, and latency under mixed workload conditions.","explanation":"## Why This Is Asked\nNUMA awareness is critical for predictable latency in multi-socket systems; this question tests practical implementation of resource isolation, memory binding, and performance verification techniques.\n\n## Key Concepts\n- NUMA topology and memory affinity\n- cpuset and memory controller (memcg) binding\n- NUMA balancing disable and IRQ affinity configuration\n- Performance validation under mixed workload scenarios\n\n## Code Example\n```bash\n# Detect NUMA topology and create cpusets\nlscpu | grep NUMA\nsudo mkdir -p /sys/fs/cgroup/cpuset/postgres\nsudo bash -c 'echo 0-15 > /sys/fs/cgroup/cpuset/postgres/cpuset.cpus'\nsudo bash -c 'echo 0 > /sys/fs/cgroup/cpuset/postgres/cpuset.mems'\n# Similar setup for ETL on node1\n```\n\n## Follow-up Considerations\nMonitor NUMA statistics via `numastat` and validate memory locality using `perf` and custom benchmarks measuring cross-NUMA traffic impact.","diagram":null,"difficulty":"advanced","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T04:28:01.530Z","createdAt":"2026-01-13T22:54:02.606Z"},{"id":"q-1665","question":"On a Linux server hosting a latency-sensitive database and a heavy batch analytics job that share a single 2-socket NUMA node and HDD I/O, propose a production plan to guarantee tail latency (P95/P99) for the database while allowing analytics to finish within a bounded window. Include NUMA memory binding, CPU pinning via cpuset/systemd slices, I/O throttling via cgroup v2 io.max, IRQ affinity, and a validation test plan with realistic workloads?","answer":"Pin the latency-sensitive database to CPUs 0-3 on NUMA node 0 with a dedicated systemd slice and cpuset; bind memory local to node 0 and set memory.low to favor the DB. Run analytics on CPUs 4-7 with ","explanation":"## Why This Is Asked\nTests ability to reason about cross-domain isolation (CPU, memory, I/O) and NUMA-awareness to guarantee tail latency under mixed workloads.\n\n## Key Concepts\n- NUMA locality and memory policies\n- cpuset and systemd Slice-based isolation\n- cgroup v2 io.max and memory.high for throttling\n- IRQ affinity and CPU affinity for stable paths\n- Validation with realistic, reproducible workloads\n\n## Code Example\n```javascript\n// systemd cpuset and io.max example (illustrative)\n[Unit]\nDescription=Latency DB\n\n[Service]\nSlice=db.slice\nCPUS=0-3\nMemoryHigh=0\n\n```\n\n## Follow-up Questions\n- How would you adjust if analytics occasionally spikes beyond expected bounds?\n- How would you monitor to detect regressive behavior after updates?","diagram":null,"difficulty":"advanced","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T05:49:28.663Z","createdAt":"2026-01-14T05:49:28.663Z"},{"id":"q-1700","question":"On a Linux server running several io_uring-based data ingestors, one worker's backlog bursts, triggering tail latency spikes for others. Propose a production plan to guarantee per-worker latency and fairness using: (a) per-worker io_uring ring sizing and SQ depth; (b) per-worker cgroups v2 with io.max and task limits; (c) backpressure and pacing (budget tokens, per-worker deadlines); (d) monitoring and a reproducible test plan to validate latency under burst?","answer":"Configure per-worker io_uring rings with fixed SQ depth (e.g., 256), pin workers to dedicated CPUs via cpuset, set per-cgroup io.max (e.g., 8MB/s), implement pacing in ingestion to cap outstanding I/O","explanation":"## Why This Is Asked\nTests ability to design per-worker I/O isolation for io_uring workloads, a realistic pattern as data ingestion scales, and how to enforce fairness without sacrificing throughput.\n\n## Key Concepts\n- io_uring ring isolation and SQ budgeting\n- cgroup v2 io.max and per-task constraints\n- cpuset or task-pinning for CPU affinity\n- backpressure/pacing mechanisms in async I/O\n- instrumentation: latency percentiles, eBPF-based tracing\n\n## Code Example\n```javascript\n# Example cgroup setup (bash-like)\nsudo mkdir -p /sys/fs/cgroup/myio/worker1\necho 10485760 > /sys/fs/cgroup/myio/worker1/io.max\n# add pid\necho <pid> > /sys/fs/cgroup/myio/worker1/cgroup.procs\n```\n\n## Follow-up Questions\n- How would you extend to dynamic reallocation of rings if a worker consistently underperforms?\n- What are the risks of strict per-worker quotas on bursty workloads and how would you mitigate them?","diagram":"flowchart TD\n  A[Worker] --> B[io_uring ring]\n  B --> C[Submit IOs]\n  C --> D[Latency Monitor]\n  D --> E{Budget OK?}\n  E -->|Yes| F[Continue]\n  E -->|No| G[Apply Backpressure]\n  G --> H[Ballast Queue]\n  H --> F","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T07:36:58.713Z","createdAt":"2026-01-14T07:36:58.713Z"},{"id":"q-1774","question":"On a Linux host, a file-watcher daemon uses inotify to monitor a large directory tree. As load grows, events start being missed and the daemon lags. Propose a production plan to keep event delivery reliable: tune inotify limits, raise file descriptor limits for the service, add a polling fallback or batching, and validate with a reproducible test that simulates churn?","answer":"Increase inotify capacity and harden limits; implement a polling fallback for missed events; add a watchdog health check to detect lag and restart gracefully. Steps: 1) set fs.inotify.max_user_watches","explanation":"## Why This Is Asked\nTests understanding of Linux kernel parameters, systemd service limits, and practical reliability strategies for event-driven workloads.\n\n## Key Concepts\n- Inotify limits (fs.inotify.*) and kernel tuning\n- Per-service file descriptor limits (LimitNOFILE, ulimit)\n- Fault-tolerant design (polling fallback, batching)\n- Observability and testing with churn scenarios\n\n## Code Example\n```ini\n# Watch daemon systemd unit override\n[Service]\nLimitNOFILE=512000\n```\n\n```bash\n# Increase inotify capacity (runtime)\nsysctl -w fs.inotify.max_user_watches=300000\n```\n\n```bash\n# Simple churn test (pseudo)\nfor i in {1..100000}; do touch /watch/dir/file_$i; done\n```\n\n## Follow-up Questions\n- How would you measure inotify queue length and event lag in production?\n- How would you ensure compatibility across kernel versions and containers?","diagram":"flowchart TD\n  A[Inotify Watch] --> B[Event]\n  B --> C[Worker]\n  C --> D[Lag Monitor]\n  D --> E{Lag > threshold}\n  E -->|Yes| F[Switch to Polling]\n  E -->|No| G[Normal]\n","difficulty":"beginner","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","Oracle","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T10:36:39.086Z","createdAt":"2026-01-14T10:36:39.086Z"},{"id":"q-1807","question":"On a Linux host running a hot-reload deployment workflow for a critical service, mid-deploy tampering could swap the binary between copy and start, breaking integrity. Design a production-grade binary integrity strategy using Linux IMA with runtime attestation. Include how to sign binaries in CI, a signed binary repository, systemd integration (ExecStartPre), policy placement, rollback tests, and how you'd validate against tampering during deployment?","answer":"Use Linux IMA to enforce signed binaries for hot-reload. Sign binaries in CI, publish to a signed repo, enable IMA-based attestation, and require the service to be measured before load. Integrate with","explanation":"## Why This Is Asked\nSecurity-critical deployments require verifiable runtime integrity. This tests knowledge of kernel integrity (IMA), secure deployment workflows, and integration with systemd and CI.\n\n## Key Concepts\n- Linux IMA and runtime attestation\n- Signed binaries in CI and secure repositories\n- systemd integration (ExecStartPre) and rollback mechanisms\n- Tamper-detection tests and observability\n\n## Code Example\n```bash\n# validate-binaries.sh (conceptual)\nset -euo pipefail\nEXPECTED_HASH=$(cat /etc/ci/signatures/service.sha256)\nACTUAL_HASH=$(sha256sum /usr/local/bin/myservice | awk '{print $1}')\nif [ \"$ACTUAL_HASH\" != \"$EXPECTED_HASH\" ]; then\n  echo \"Signature mismatch\" >&2\n  exit 1\nfi\n```\n\n```ini\n# /etc/systemd/system/myservice.service (snippet)\n[Unit]\nDescription=My Critical Service\n\n[Service]\nExecStart=/usr/local/bin/myservice\nExecStartPre=/usr/local/bin/validate-binaries.sh\n```\n\n## Follow-up Questions\n- How would you handle CI/CD with multiple binary variants across environments?\n- How would you monitor and alert on attestation failures in production?","diagram":"flowchart TD\n  A[Signed Binary Repo] --> B[IMA Attestation]\n  B --> C[systemd ExecStartPre Verifies]\n  C --> D[Service Starts]\n  E[Tamper Attempt] --> F[Attestation Failure & Alert]","difficulty":"advanced","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T11:42:10.365Z","createdAt":"2026-01-14T11:42:10.365Z"},{"id":"q-1939","question":"On a Linux host running a web stack (Nginx proxy to a Node/Go app), the app writes to /var/log/webapp/app.log and /var/log/webapp/access.log. Bursts fill the disk and logrotation sometimes fails because the app keeps logs open. Propose a production-grade plan to ensure reliable log rotation with no data loss and no disk-full events. Include concrete logrotate config (copytruncate vs postrotate), systemd unit tweaks (Restart, ExecReload), filesystem layout advice, and a test plan to reproduce a burst and verify rotation completes without downtime?","answer":"Configure logrotate with copytruncate (or a postrotate that signals the process to reopen logs) and rotate daily or at 100M, keeping 7 archives. Move logs to a dedicated partition; enable Restart=on-f","explanation":"## Why This Is Asked\nTests practical log management skills: integrating logrotate with systemd, handling open-file rotation issues, and ensuring disk-safety with verifiable tests.\n\n## Key Concepts\n- logrotate behavior: copytruncate vs postrotate\n- systemd unit settings: Restart, ExecReload\n- filesystem layout and quotas for logs\n- reproducible test plan to simulate burst and confirm rotation\n\n## Code Example\n```javascript\n// logrotate config snippet (illustrative)\n// Real config lives in /etc/logrotate.d/webapp\n```\n\n## Follow-up Questions\n- What are the tradeoffs of copytruncate vs postrotate in data integrity?\n- How would you monitor disk usage and set automated alerts for log growth?","diagram":null,"difficulty":"beginner","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Stripe","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T17:51:38.423Z","createdAt":"2026-01-14T17:51:38.423Z"},{"id":"q-1954","question":"On a Linux host running three CI runners as systemd services sharing a single NVMe and a 10 Gb NIC, a long-running build on one runner starves CPU and I/O, delaying others. Propose a production plan to guarantee fair CPU and I/O while preserving peak throughput: (a) per-service CPU limits via cgroup v2 and CPU affinity; (b) per-service I/O throttling with io.max and a suitable I/O scheduler (BFQ/mq-deadline); (c) CPU pinning and IRQ isolation; (d) validation with bursts and latency metrics?","answer":"Plan using cgroup v2: create ci.slice with per-service sub-slices (ci-runner-A/B/C). Assign cpu.max and io.max per service; pin CPUs via cpuset and set IRQ affinity. Use BFQ for per-service I/O schedu","explanation":"## Why This Is Asked\nTests practical mastery of isolating CPU and I/O in a multi-tenant CI environment under hardware contention, a common production challenge.\n\n## Key Concepts\n- cgroup v2: cpu and io controllers for per-service quotas.\n- systemd slices: grouping services under a parent for unified controls.\n- CPU affinity and cpuset: bind runners to dedicated CPUs.\n- I/O schedulers: BFQ or mq-deadline for per-service throttling.\n- Observability: latency, throughput, tail latency, and backlog.\n\n## Code Example\n```javascript\n# Systemd override example (pseudo)\n[Slice]\nCPUAccounting=true\n\n[Service]\nSlice=ci.slice\nCPUQuota=33%\n```\n```javascript\n# cgroup v2 quota (pseudo)\necho 0.33 > /sys/fs/cgroup/ci.slice/ci-runner-A/cpu.max\n```\n\n## Follow-up Questions\n- How would you monitor and adapt quotas during a bursty workload?\n- What rollback strategy would you use if a quota regresses build reliability?\n","diagram":"flowchart TD\n  CI[ci.slice] --> A[ci-runner-A]\n  CI --> B[ci-runner-B]\n  CI --> C[ci-runner-C]\n  A --> CPU[CPU max]\n  A --> IO[io max]\n  B --> CPU2[CPU max]\n  B --> IO2[io max]\n  C --> CPU3[CPU max]\n  C --> IO3[io max]","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Apple","Databricks"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T18:51:30.714Z","createdAt":"2026-01-14T18:51:30.714Z"},{"id":"q-1987","question":"On a Linux host running CI jobs in rootless containers (e.g., Podman) shared across teams, a misconfigured container could attempt host escape. Propose a production hardening plan using user namespaces, rootless mode, Seccomp and AppArmor, and a tight per-job capset + cgroupv2 quotas. Include concrete commands and a rollback plan?","answer":"Run each CI worker in rootless containers with user namespaces, drop all capabilities, and enforce seccomp + AppArmor. Bind a tight cap set and per-job quotas via cgroupv2 memory.max, cpu.max, and pid","explanation":"## Why This Is Asked\nRealistic containment for containerized CI workloads is critical; this tests practical hardening, not theory.\n\n## Key Concepts\n- Rootless containers and user namespaces\n- Seccomp and AppArmor profiles\n- Capability bounding (cap_set)\n- cgroupv2 quotas (memory.max, cpu.max, pids.max)\n- Monitoring and rollback\n\n## Code Example\n```bash\npodman run --rm --name ci-job \\\n  --security-opt seccomp=seccomp.json \\\n  --security-opt apparmor=ci-job \\\n  --cap-drop=ALL --memory=512m --cpu-quota=50000 --cpu-period=100000 \\\n  --uidmap 0:1:1 --uidmap 1:2:1000 \\\n  localhost/ci-image\n```\n\n## Follow-up Questions\n- How would you audit for privilege escalations at runtime?\n- How would you handle failed containment and rollback without losing work?","diagram":"flowchart TD\nCIJob[CI Job Container] -->|uses| UserNS[User Namespaces]\nCIJob -->|enforced by| Seccomp[Seccomp Profile]\nCIJob -->|enforced by| AppArmor[AppArmor]\nCIJob -->|cgroups| CG[CG v2 quotas]\nCG --> Host[Host Kernel]","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","Square","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T19:42:42.923Z","createdAt":"2026-01-14T19:42:42.923Z"},{"id":"q-2014","question":"On a Linux host running multiple GPU-accelerated services in separate systemd slices under containerized workloads, occasional CPU contention causes tail latency spikes and timeouts for one service. Design a production plan to bound latency and preserve throughput without service interruption. Include per-slice isolation (isolcpus, cpuset), systemd slice configuration, cpu.max, and a kernel I/O scheduler tuning (mq-deadline), plus a reproducible test plan with mixed workloads and latency verification steps?","answer":"Bind each GPU service to a dedicated CPU set and a separate systemd slice; pin IRQs and NIC interrupts; enable isolcpus on cores; cap CPU usage with cpu.max; tune I/O scheduler to mq-deadline with per","explanation":"## Why This Is Asked\n\nThis question probes practical orchestration of CPU, memory, and IO isolation under contention with production-grade controls and observability.\n\n## Key Concepts\n\n- systemd slices and cpuset for resource isolation\n- cgroup v2 cpu.max, cpu.weight, and per-slice quotas\n- isolcpus and IRQ affinity to prevent interrupts from disturbing critical slices\n- I/O scheduler tuning (mq-deadline vs BFQ) and per-tenant quotas\n- reproducible load generation and latency verification with fio/stress-ng and ftrace/perf\n\n## Code Example\n\n```bash\n#!/bin/bash\n# Repro: spawn workloads pinned to CPU pools 0-3 and 4-7\nset -e\n# Start GPU service in background\ntaskset -c 0-3 /usr/local/bin/gpu-service &\n# Generate CPU contention on non-critical cores\nstress-ng --cpu 4 --timeout 60s &\nwait\n```\n\n## Follow-up Questions\n\n- How would you validate that latency stays within SLA under a quarterly load spike?\n- What observability would you add to detect when a slice is starved?","diagram":"flowchart TD\n  A[Identify contention] --> B[Isolate with slices & cpuset]\n  B --> C[Pin IRQs & cap resources with cpu.max]\n  C --> D[Tune I/O scheduler (mq-deadline)]\n  D --> E[Test with mixed workloads & measure tail latency]","difficulty":"advanced","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Instacart","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T20:51:49.017Z","createdAt":"2026-01-14T20:51:49.018Z"},{"id":"q-2167","question":"On a KVM host running dozens of VMs with memory ballooning enabled, a burst in guest memory growth triggers host memory pressure and occasional OOM events. Propose a production-safe plan to cap balloon aggressiveness, enforce host backpressure, and prevent thrash, using libvirt XML (memory.size/currentMemory, memoryBacking, balloon device) and cgroup v2 memory.max/memory.high, plus a test plan with a reproducible spike and verification steps?","answer":"Per-VM caps and headroom via cgroup v2: memory.max and memory.high per VM plus a low vm.swappiness to delay swap pressure. Keep ballooning enabled but hard-cap host memory pressure so reclamation happ","explanation":"## Why This Is Asked\nProbe for practical knowledge of KVM memory management, cgroup v2, ballooning trade-offs, and test scaffolding under real production-like conditions.\n\n## Key Concepts\n- KVM memory ballooning and how guest memory maps to host resources\n- cgroup v2 limits (memory.max, memory.high) for backpressure\n- libvirt XML configuration for per-VM memory caps\n- Observability and test plan for reproducible spikes\n\n## Code Example\n```xml\n<domain type='kvm'>\n  <name>vm1</name>\n  <memory unit='KiB'>4294967296</memory>\n  <currentMemory unit='KiB'>4294967296</currentMemory>\n  <memoryBacking>\n    <hugepages/>\n  </memoryBacking>\n  <devices>\n    <memballoon model='virtio'/>\n  </devices>\n</domain>\n```\n\n## Follow-up Questions\n- How would you monitor and alert on host memory pressure events?\n- What are the risks of disabling ballooning on some VMs?","diagram":null,"difficulty":"advanced","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Instacart","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T05:43:50.597Z","createdAt":"2026-01-15T05:43:50.598Z"},{"id":"q-881","question":"On a Linux host, a long-running daemon writes to `/var/log/myapp.log` and is managed by systemd, but log rotation occasionally causes logging to stop after rotation. Propose a practical fix to ensure logging continues after rotation without restarting the daemon. Include the exact approach and a sample logrotate config snippet and testing steps?","answer":"Configure logrotate to reopen the log file after rotation instead of copying or truncating. After rotating, signal the daemon to reopen logs (e.g., SIGHUP). The config should include a postrotate that","explanation":"## Why This Is Asked\nTests practical handling of log rotation, signals, and ensuring service continuity without downtime. It checks knowledge of logrotate postrotate scripts, choosing the right approach (signal vs copytruncate), and how to validate in a controlled test.\n\n## Key Concepts\n- logrotate configuration fields and postrotate scripts\n- signaling daemons (SIGHUP) to reopen logs\n- copytruncate vs signaling trade-offs\n- testing routine for rotation\n\n## Code Example\n```bash\n/var/log/myapp.log {\n  rotate 5\n  weekly\n  missingok\n  notifempty\n  postrotate\n    kill -HUP `cat /var/run/myapp.pid` 2>/dev/null || true\n  endscript\n}\n```\n\n## Follow-up Questions\n- What are the trade-offs of copytruncate vs signaling?\n- How would you monitor and alert if log rotation fails to reopen logs?","diagram":null,"difficulty":"beginner","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Meta","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T13:59:55.582Z","createdAt":"2026-01-12T13:59:55.582Z"},{"id":"q-888","question":"A production Linux host runs a data-backup agent that uses /var/lib/backup/backup.lock to enforce a single instance. Sometimes a stale lock remains after a crash, blocking new runs; the agent also leaves non-terminating children on stop, risking partial backups. Propose a systemdâ€‘based lifecycle fix: ensure one instance, auto-clean stale lock, and graceful stop with timeout and fallback to kill. Include concrete unit snippets and verification steps?","answer":"Use a systemd unit with Type=forking, PIDFile, and ExecStartPre that checks and clears a stale lock: if /var/lib/backup/backup.lock exists and its PID is not running, delete it. Stop uses KillMode=con","explanation":"## Why This Is Asked\nTests understanding of robust service lifecycle management with systemd, handling singleton constraints, and clean termination of complex processes.\n\n## Key Concepts\n- systemd lifecycle: ExecStartPre, ExecStop, KillMode, TimeoutStopSec\n- singleton enforcement via lock files\n- graceful termination vs. forceful kill for child processes\n\n## Code Example\n```ini\n; /etc/systemd/system/backup-agent.service\n[Unit]\nDescription=Backup Agent\nAfter=network.target\n\n[Service]\nType=forking\nPIDFile=/var/run/backup/backup.pid\nExecStartPre=/bin/sh -c 'LOCK=/var/lib/backup/backup.lock; if [ -e \"$LOCK\" ]; then pid=$(cat \"$LOCK\"); if [ -d /proc/$pid ]; then exit 1; else rm -f \"$LOCK\"; fi; fi'\nExecStart=/usr/local/bin/backup-agent\nExecStop=/bin/kill -TERM $MAINPID\nExecStopPost=/bin/rm -f /var/lib/backup/backup.lock\nTimeoutStopSec=120s\nKillMode=control-group\nRestart=on-failure\n```\n\n## Follow-up Questions\n- How would you test idempotency for consecutive startups?\n- How would you adapt if the agent uses a PID file instead of a lock file?","diagram":null,"difficulty":"advanced","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T14:27:44.056Z","createdAt":"2026-01-12T14:27:44.056Z"},{"id":"q-961","question":"On a Linux host, a memory-hungry log-harvester daemon managed by systemd sporadically triggers the kernel OOM killer during peak load, crashing the service and delaying alerts. Propose a production-safe plan to prevent OOM termination while preserving throughput. Include concrete systemd settings (MemoryLimit, MemorySwapMax, OOMScoreAdjust, Restart), kernel tuning (swap, swappiness), and a test plan with a reproducible high-memory scenario and verification steps?","answer":"Apply per-service memory controls and OOM protection. Set MemoryLimit=2G, MemorySwapMax=2G, OOMScoreAdjust=-100, and Restart=on-failure with a 5s backoff. Tune vm.swappiness=10 and ensure swap is enab","explanation":"## Why This Is Asked\nTests memory pressure handling, systemd tuning, and safe recovery without service disruption.\n\n## Key Concepts\n- Kernel OOM killer and oom_score_adj\n- systemd memory constraints (MemoryLimit, MemorySwapMax)\n- Restart strategies and timeouts\n- memory tuning and swap behavior\n\n## Code Example\n```ini\n[Unit]\nDescription=Log Harvester\n\n[Service]\nExecStart=/usr/local/bin/logharvester\nType=simple\nMemoryLimit=2G\nMemorySwapMax=2G\nOOMScoreAdjust=-100\nRestart=on-failure\nRestartSec=5s\n```\n\n## Follow-up Questions\n- How would you observe and alert on OOM events?\n- How would you adjust for multiple high-memory services competing for swap?","diagram":"flowchart TD\n  A[Memory Pressure] --> B{OOM killer?}\n  B -->|Yes| C[Adjust OOMScore/MemoryLimit]\n  B -->|No| D[Normal Operation]\n  C --> E[Daemon Survives, backlog Drains]","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"channel":"linux-foundation-sysadmin","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T17:22:50.353Z","createdAt":"2026-01-12T17:22:50.353Z"},{"id":"linux-foundation-sysadmin-networking-1768260262823-4","question":"You suspect ARP storms on a host with interface eth1. To quickly verify ARP traffic and identify abnormal ARP activity on that interface, which command should you run?","answer":"To quickly verify ARP traffic and identify abnormal ARP activity on interface eth1, run 'tcpdump -i eth1 arp'. This command captures and displays ARP packets specifically on the eth1 interface, allowing you to monitor for ARP storms.","explanation":"## Correct Answer\nA. tcpdump with the arp filter on the specific interface captures ARP requests/replies, which is exactly what you need to observe ARP storms.\n\n## Why Other Options Are Wrong\n- Option B: ICMP captures are unrelated to ARP activity.\n- Option C: TCP traffic does not reveal ARP behavior.\n- Option D: arp -a shows ARP table entries but not live ARP traffic, making storms harder to observe in real time.\n\n## Key Concepts\n- ARP monitoring with packet captures\n- Interface-specific traffic analysis\n- Distinguishing ARP storms from normal ARP chatter\n\n## Real-World Application\n- Proactively diagnosing network broadcast storms and ARP-related outages in data-center or campus networks.","diagram":null,"difficulty":"intermediate","tags":["Networking","ARP","Linux","Kubernetes","certification-mcq","domain-weight-12"],"channel":"linux-foundation-sysadmin","subChannel":"networking","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T03:32:44.361Z","createdAt":"2026-01-12 23:24:24"}],"subChannels":["general","networking"],"companies":["Adobe","Anthropic","Apple","Bloomberg","Citadel","Cloudflare","Coinbase","Databricks","Discord","DoorDash","Goldman Sachs","Google","Hugging Face","IBM","Instacart","LinkedIn","Meta","Microsoft","MongoDB","OpenAI","Oracle","Plaid","Snap","Snowflake","Square","Stripe","Tesla","Twitter","Two Sigma","Uber"],"stats":{"total":23,"beginner":5,"intermediate":8,"advanced":10,"newThisWeek":23}}