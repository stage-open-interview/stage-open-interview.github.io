{"questions":[{"id":"q-1222","question":"Design a real-time ad-click analytics pipeline in GCP that ingests Pub/Sub events via Dataflow into BigQuery, while implementing 90-day TTL on PII data, exporting audits to Cloud Storage, and handling schema evolution, late data, dedup, and rollback. Provide concrete architecture, data models, and operational steps?","answer":"Two-tier streaming: a BigQuery hot table partitioned by ingestion_date with 90-day partition expiration for PII, plus a daily Cloud Storage Parquet archive for audits. Dataflow templates ingest Pub/Su","explanation":"## Why This Is Asked\nEvaluates real-world trade-offs in latency, cost, and governance for streaming pipelines on GCP.\n\n## Key Concepts\n- Pub/Sub -> Dataflow streaming\n- BigQuery partition expiration (90 days)\n- Parquet archive in Cloud Storage\n- Schema evolution with nested fields\n- Deduplication and late data handling\n- Rollback/versioned templates\n\n## Code Example\n```javascript\n// Pseudo Beam skeleton: dedupe by event_id and write to two sinks\n```\n\n## Follow-up Questions\n- How to validate TTL and archiving with tests?\n- What monitoring and alerting would you add for duty-cycle failures?","diagram":"flowchart TD\n  PubSub[Pub/Sub] --> Dataflow[Dataflow Template]\n  Dataflow --> BigQueryHot[BigQuery: Ingest (90d TTL)]\n  Dataflow --> Archive[Cloud Storage: Parquet Archive]\n  BigQueryHot --> Monitoring[Monitoring & Audits]","difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","NVIDIA","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T05:33:44.453Z","createdAt":"2026-01-13T05:33:44.453Z"},{"id":"q-1310","question":"You operate a streaming ingestion pipeline: Pub/Sub -> Dataflow -> BigQuery. Daily 50k events with fields user_id, event_type, amount (which can be string). Propose a simple ingestion-time data quality plan that validates JSON schema, coerces types, and routes invalid records to a dead-letter Pub/Sub topic without stopping the pipeline. Include testing with a small sample and how you monitor invalid counts?","answer":"Parse Pub/Sub messages with a simple Beam DoFn that validates required fields (user_id, event_type) and coerces amount to float. Valid records go to BigQuery; invalid ones emit to a dead-letter Pub/Su","explanation":"## Why This Is Asked\n\nAssess data quality at ingestion and the practicality of dead-letter routing in a real pipeline.\n\n## Key Concepts\n\n- Data quality at ingestion\n- Side outputs / dead-letter routing\n- Type coercion and validation\n- Beam/Dataflow to BigQuery integration\n- Monitoring\n\n## Code Example\n\n```javascript\n// Pseudo Beam DoFn for ingestion validation\nfunction processElement(element) {\n  let obj;\n  try { obj = JSON.parse(element); } catch (e) { return {error: 'invalid_json'}; }\n  if (!obj.user_id || !obj.event_type) return {error: 'missing_fields'};\n  const amount = parseFloat(obj.amount);\n  if (Number.isNaN(amount)) return {error: 'invalid_amount'};\n  return {user_id: obj.user_id, event_type: obj.event_type, amount: amount};\n}\n```\n\n## Follow-up Questions\n\n- How would you test dead-letter routing under load?\n- How would you tune retry/backoff for DLQ messages?\n","diagram":null,"difficulty":"beginner","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","PayPal","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T10:36:08.797Z","createdAt":"2026-01-13T10:36:08.797Z"},{"id":"q-1319","question":"Design a compliant streaming data platform in GCP for a fintech app that ingests transaction events from Pub/Sub through Dataflow into BigQuery. New fields may appear over time; PII must be detected and masked before analytics, while raw data is retained with restricted access. Outline the end-to-end architecture, data models, schema evolution strategy, PII handling, lineage, retention, and operational monitoring with concrete steps and trade-offs?","answer":"Implement a streaming Dataflow pipeline from Pub/Sub to a partitioned, clustered BigQuery table with schema registry in Data Catalog for forward/backward compatibility. Validate records, redact PII vi","explanation":"## Why This Is Asked\nReal-world streaming data governance, including schema evolution, PII handling, and lineage, is critical in fintech.\n\n## Key Concepts\n- Streaming ingestion with Pub/Sub and Dataflow\n- BigQuery partitioning and clustering\n- Schema registry and compatibility management\n- PII detection and masking (DLP)\n- Data lineage and metadata (Data Catalog)\n- Retention TTLs and access controls\n- Monitoring and alerting (Cloud Monitoring)\n\n## Code Example\n```python\ndef transform(record, registry_schema):\n    if not validate(record, registry_schema):\n        raise ValueError('Schema mismatch')\n    pii = ['acct_number', 'ssn']\n    masked = mask_pii(record, pii)\n    return masked\n```\n\n## Follow-up Questions\n- How would you test end-to-end TTL retention and schema evolution without downtime?\n- How would you handle cross-producer schema drift and multiple versions simultaneously?","diagram":"flowchart TD\n  A[Pub/Sub] --> B[Dataflow]\n  B --> C[BigQuery Table]\n  C --> D[Masked Analytics Views]\n  B --> E[Data Catalog (Lineage)]\n  C --> F[Raw Data with TTL Policy]","difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Plaid","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T11:31:53.811Z","createdAt":"2026-01-13T11:31:53.811Z"},{"id":"q-1389","question":"You receive streaming JSON events from Pub/Sub. Design a beginner-friendly GCP pipeline using Dataflow (Beam) to parse/flatten events, write to a partitioned BigQuery table (partition by event_date), and deduplicate by event_id. Late data allowed up to 6 hours. Outline architecture, data model, schema-change handling, and monitoring?","answer":"Design a Dataflow (Beam) streaming job that reads Pub/Sub JSON, flattens fields into a simple schema, writes to a partitioned BigQuery table by event_date, and deduplicates on event_id within a 5-minu","explanation":"## Why This Is Asked\n\nTests a practical, beginner-friendly data ingestion design on GCP, covering streaming, dedup, partitioning, late data handling, and basic schema evolution.\n\n## Key Concepts\n\n- Dataflow (Beam) streaming pipeline\n- Pub/Sub as the source\n- BigQuery partitioned tables and upserts\n- Deduplication by event_id\n- Lightweight schema-change and monitoring\n\n## Code Example\n\n```javascript\n// Pseudo-Beam-like snippet (conceptual)\nfunction parseEvent(raw) {\n  const obj = JSON.parse(raw);\n  return { event_id: obj.id, event_date: obj.date, payload: obj.payload };\n}\n```\n\n## Follow-up Questions\n\n- How would you test the dedup logic in Dataflow?\n- How would you monitor latency and late-data rates in this pipeline?","diagram":null,"difficulty":"beginner","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","Snap","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T14:50:08.738Z","createdAt":"2026-01-13T14:50:08.738Z"},{"id":"q-1415","question":"You are building a multi-source data platform where raw data arrives as Parquet in Google Cloud Storage, Avro via Pub/Sub, and JSON through a partner API; design a two-stage pipeline that normalizes all formats into a canonical Parquet dataset, ingested into a partitioned BigQuery table, with schema drift handling, data validation, and end-to-end lineage. Include how you would implement idempotent ingests, rollback, and cross-team access controls?","answer":"Implement a two-stage pipeline: stage1 normalizes Parquet(JSON/Avro) to a canonical Parquet schema, stage2 loads to a partitioned BigQuery table. Use a central schema registry and field-level nulls fo","explanation":"## Why This Is Asked\nDesign a multi-source ingestion pattern with schema drift handling, cross-format canonicalization, and governance; evaluate practical trade-offs.\n\n## Key Concepts\n- Multi-format normalization and canonical schema\n- Dataflow/Beam idempotent ingestion and exactly-once semantics\n- Schema evolution, drift handling, and validation\n- Data lineage, access controls, and rollback mechanisms\n\n## Code Example\n```javascript\nfunction toCanonical(rec){\n  // normalize fields, coerce types, assign stable id\n  return { id: rec.id ?? generateId(), ts: new Date(rec.ts), payload: rec.payload ?? null };\n}\n```\n\n## Follow-up Questions\n- How would you test schema drift and automate backwards-compatible changes?\n- How would you monitor latency and data skew across sources?","diagram":"flowchart TD\n  A[Ingest Parquet (GCS)] --> B[Normalize to Canonical Parquet]\n  C[Ingest Avro (Pub/Sub)] --> B\n  D[Ingest JSON (API)] --> B\n  B --> E[BigQuery (Partitioned)]\n  E --> F[Lineage & Auditing]","difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Snowflake","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T15:53:31.239Z","createdAt":"2026-01-13T15:53:31.239Z"},{"id":"q-1449","question":"Design a streaming enrichment pipeline: Pub/Sub → Dataflow → BigQuery, with an inline governance layer enforcing per-record rules (required fields, value ranges, cross-field consistency) at ingestion. Records failing rules go to a quarantine Cloud Storage bucket; good records load to a partitioned BigQuery table. Describe data model, rule engine approach, idempotence, backpressure, and testing/monitoring plan?","answer":"Implement a DoFn in Dataflow that validates each event against a schema (required fields, ranges, cross-field constraints). Use tagged outputs: main for valid records, quarantine for invalid. Write ma","explanation":"## Why This Is Asked\n\nAssesses ability to embed governance in streaming, handling bad data without blocking analytics, and operationalize quarantine and observability.\n\n## Key Concepts\n\n- Ingestion-time data quality checks\n- Dataflow ParDo with side outputs for quarantines\n- Idempotent BigQuery writes and record versioning\n- Backpressure and dead-letter handling\n- Observability and testing strategy\n\n## Code Example\n\n```python\nimport apache_beam as beam\n\nclass Govern(beam.DoFn):\n    def process(self, e):\n        if not e.get('user_id') or e.get('age', 0) < 0:\n            yield beam.pvalue.TaggedOutput('quarantine', e)\n        else:\n            yield e\n\nwith beam.Pipeline(...) as p:\n    raw = p | 'Read' >> beam.io.ReadFromPubSub(topic=...)  \n    main, bad = (raw | 'Govern' >> beam.ParDo(Govern()).with_outputs('quarantine', main='main'))\n    main | 'BQ' >> beam.io.WriteToBigQuery(table=..., schema=...)\n    bad | 'Quarantine' >> beam.io.WriteToText('gs://bucket/quarantine/')\n```\n\n## Follow-up Questions\n\n- How would you handle schema evolution in the governance layer without breaking downstream?\n- How to quantify and alert on data quality drift over time?\n- What tests would you add for end-to-end validation and rollback scenarios?","diagram":"flowchart TD\n  PubSub[Pub/Sub] --> Dataflow[Dataflow]\n  Dataflow --> BigQuery[BigQuery]\n  Dataflow --> Quarantine[GCS Quarantine]\n  Quarantine --> Audits[Audit Logs]\n  BigQuery --> Reports[BI Dashboards]","difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Instacart","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T17:45:34.896Z","createdAt":"2026-01-13T17:45:34.896Z"},{"id":"q-1473","question":"In a beginner friendly GCP data pipeline Pub/Sub -> Dataflow -> BigQuery design robust observability and recovery. Describe how to instrument Dataflow with metrics backlog throughput processed failed, implement a dead letter policy that dumps failed JSON messages to Cloud Storage, set retry backoff, and outline a replay path to re ing est from Cloud Storage. Include a concrete alert rule backlog throughput greater than 0.2 for 5 minutes and a brief example snippet?","answer":"Instrument Dataflow with metrics backlog, throughput, processed, and failed; emit structured logs to Cloud Logging. Route failed messages to a per topic Cloud Storage DLQ with 24h retention. Use expon","explanation":"## Why This Is Asked\nTests practical observability and recoverability for a streaming pipeline in fintech-like constraints, focusing on actionable metrics and a concrete DLQ/replay path.\n\n## Key Concepts\n- Dataflow metrics and structured logging\n- Dead-letter queues and DLQ retention\n- Retry/backoff strategies\n- Replay/re-ingestion pipelines and validation\n\n## Code Example\n```python\n# Pseudocode: DLQ handling in a Beam DoFn\nclass Process(DoFn):\n  def process(self, element):\n    try:\n      yield element\n    except Exception:\n      write_to_dlq(element)\n```\n\n## Follow-up Questions\n- How would you test the DLQ path with simulated failures?\n- What are the cost and complexity trade-offs of DLQ retention and replay?\n","diagram":"flowchart TD\n  A[Pub/Sub] --> B[Dataflow]\n  B --> C[BigQuery]\n  subgraph DLQ\n    D[Cloud Storage DLQ]\n  end\n  D --> E[Replay Job]","difficulty":"beginner","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","IBM"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T18:48:00.248Z","createdAt":"2026-01-13T18:48:00.248Z"},{"id":"q-1546","question":"In GCP, ingest daily Pub/Sub JSON events through Dataflow into BigQuery. Implement basic observability and reliability: 1) emit metrics for ingested, processed, and failed events; 2) export metrics to Cloud Monitoring and alert on failures for three consecutive checks; 3) deduplicate by event_id; 4) log audits to Cloud Storage. Outline architecture, data model, and concrete steps?","answer":"Implement a Dataflow (Apache Beam) pipeline that reads Pub/Sub JSON messages, parses them with a DoFn that emits metrics for ingested, processed, and failed events, then writes to BigQuery via streaming inserts with insertId set to event_id for deduplication. Export metrics to Cloud Monitoring and create alert policies for three consecutive failures. Log audit events to Cloud Storage for compliance tracking.","explanation":"## Why This Is Asked\nThis tests practical observability and data reliability skills in a real-world, beginner-friendly setting.\n\n## Key Concepts\n- Beam Metrics and Cloud Monitoring integration\n- Idempotent streaming writes with insertId\n- Lightweight auditing and error handling\n- Alerting on data quality metrics\n\n## Code Example\n```javascript\n// Pseudo-beam sketch (JavaScript/TypeScript syntax used for illustration)\nclass ParseEvent extends DoFn {\n  @ProcessElement\n  process(@Element json, output) {\n    const event = JSON.parse(json.image);\n    this.ingested.inc();\n    // ... validation and transformation\n    this.processed.inc();\n    output(event);\n  }\n  \n  @Setup\n  setup() {\n    this.ingested = Metrics.counter('pipeline', 'ingested_events');\n    this.processed = Metrics.counter('pipeline', 'processed_events');\n    this.failed = Metrics.counter('pipeline', 'failed_events');\n  }\n}\n```","diagram":"flowchart TD\n  PubSub[Pub/Sub] --> Dataflow[Dataflow / Beam]\n  Dataflow --> BigQuery[BigQuery]\n  Dataflow --> Monitor[Cloud Monitoring]\n  Dataflow --> Logs[Cloud Logging]\n  BigQuery --> Audit[Audit Logs (Cloud Storage)]","difficulty":"beginner","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Microsoft","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T06:31:05.145Z","createdAt":"2026-01-13T21:34:41.250Z"},{"id":"q-1581","question":"In Pub/Sub, daily JSON events contain PHI fields. Design a beginner-friendly GCP pipeline using Dataflow (Beam) to redact PHI with Cloud DLP before loading into BigQuery, and emit a separate audit log to Cloud Storage. Outline architecture, data model, and validation steps, including how you’d monitor masking failures?","answer":"Implement a Dataflow (Apache Beam) streaming job that reads JSON events from Pub/Sub, applies Cloud DLP masking to PHI fields (name, DOB, SSN), writes the redacted records to a BigQuery dataset, and emits audit logs to Cloud Storage. The pipeline includes error handling, dead-letter queues, and monitoring for masking failures.","explanation":"## Why This Is Asked\n\nTests practical data masking and data lake hygiene using familiar GCP tools. It ensures candidates can architect a compliant pipeline without overcomplicating a beginner task.\n\n## Key Concepts\n\n- Dataflow (Beam) streaming ingestion\n- Cloud DLP masking of PHI\n- BigQuery storage of redacted data\n- Cloud Storage audit logs and monitoring\n\n## Code Example\n\n```javascript\n// Pseudo Beam skeleton: read Pub/Sub, redact PHI with DLP, write to BigQuery, emit audit to GCS\n```\n\n## Follow-up Questions\n\n- How would you test masking coverage and false negatives in this pipeline?\n- What monitoring metrics would you track to ensure compliance?","diagram":null,"difficulty":"beginner","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T05:50:38.131Z","createdAt":"2026-01-13T22:48:46.197Z"},{"id":"q-1596","question":"Design a streaming pipeline: Pub/Sub → Dataflow (Beam) → BigQuery. Core: a dynamic data quality gate using a Firestore-stored policy engine that validates fields/types at ingest; invalid records serialized to Cloud Storage as JSONL with metadata; valid records load to BigQuery; publish quality metrics to Cloud Monitoring; handle drift/backpressure without downtime. Explain architecture, data model, and operational steps?","answer":"Design a streaming pipeline with Pub/Sub → Dataflow (Beam) → BigQuery, featuring a dynamic data quality gate using a Firestore-stored policy engine for field/type validation at ingest. Invalid records are serialized to Cloud Storage as JSONL with metadata, while valid records load to BigQuery. Quality metrics are published to Cloud Monitoring, and the system handles schema drift and backpressure without downtime through adaptive processing and buffering strategies.","explanation":"## Why This Is Asked\n\nTests ability to design a robust, observable streaming pipeline with runtime policy-based validation and safe failure handling at scale.\n\n## Key Concepts\n\n- Streaming ingestion with Pub/Sub and Dataflow (Beam)\n- Dynamic policy engine (Firestore) for field/type validation\n- Dead-letter/quarantine in Cloud Storage with metadata\n- Custom metrics in Cloud Monitoring for data quality\n- Schema drift resilience and backpressure handling\n\n## Code Example\n\n```python\n# Example: skeleton DoFn using Firestore policy to validate records\nclass ValidateWithPolicy(DoFn):\n    def setup(self):\n        self.firestore_client = firestore.Client()\n        self.policy_cache = {}\n    \n    def process(self, element):\n        # Load policy from Firestore with caching\n        policy = self.get_policy(element.get('schema_version'))\n        \n        # Validate fields/types against policy\n        if self.validate_record(element, policy):\n            yield element\n        else:\n            # Send invalid records to dead-letter sink\n            yield pvalue.TaggedOutput('invalid', {\n                'record': element,\n                'metadata': {\n                    'timestamp': datetime.utcnow().isoformat(),\n                    'validation_errors': self.get_validation_errors(element, policy)\n                }\n            })\n```","diagram":null,"difficulty":"advanced","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Hashicorp","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T05:34:10.860Z","createdAt":"2026-01-14T02:24:24.969Z"},{"id":"q-1722","question":"Design a cross-region, real-time analytics pipeline for a fintech on GCP. Ingest 100 GB/day of JSON events from Pub/Sub into BigQuery while ensuring per-tenant data isolation, a 90-day TTL on PII, and a rollback path. Show the architecture, data models, and operational steps for schema evolution, late data, dedup, and cost control?","answer":"Ingest via Pub/Sub -> Dataflow (Streaming) -> BigQuery (partitioned by day) with per-tenant views; store raw Parquet in Cloud Storage for audits; implement 90-day TTL by a scheduled Dataflow job that ","explanation":"## Why This Is Asked\nTests ability to design cross-region, scalable analytics with data governance, TTL, and schema evolution.\n\n## Key Concepts\n- Cross-region ingestion, storage\n- PII TTL and redaction\n- Tenant isolation with Authorized Views/IAM\n- Schema evolution and drift handling\n- Late data, dedup, and cost control\n\n## Code Example\n```javascript\n// Dataflow streaming pseudo-template\nconst events = pipeline\n  .apply('ReadPubSub', PubsubIO.readStrings().fromTopic('projects/..../topics/events'))\n  .apply('Parse', ParDo.of(new ParseFn()))\n  .apply('Deduplicate', /* ... */)\n  .apply('WriteBQ', BigQueryIO.writeTableRows().to('project.dataset.table$YYYYMMDD'))\n```\n\n## Follow-up Questions\n- How would you secure data in transit and at rest?\n- How would you monitor costs and adjust partitions?","diagram":null,"difficulty":"advanced","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Oracle","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T08:40:44.958Z","createdAt":"2026-01-14T08:40:44.959Z"},{"id":"q-1754","question":"Design a beginner-friendly GCP pipeline to load daily 2-5 GB of newline-delimited JSON logs from Cloud Storage into BigQuery. Keep only the latest 45 days in a partitioned table (partition by load_date, clustered by record_id). Ensure dedup by record_id, handle optional new fields with a permissive schema, allow late data up to 24 hours, and provide basic monitoring. Choose between Dataflow (Beam) or Cloud Functions, justify, and outline data model, transformations, error handling, and testing strategies?","answer":"Use Dataflow (Beam) in batch mode to read daily JSONL from Cloud Storage, parse and flatten, write to a staging table, then MERGE into a final partitioned table (load_date) clustered by record_id for ","explanation":"## Why This Is Asked\nThis question probes how a junior data engineer designs an end-to-end GCP pipeline for file-based ingestion, dedup, and schema evolution—a common real-world task that isn't covered by Pub/Sub-focused questions.\n\n## Key Concepts\n- Data ingestion from Cloud Storage using Dataflow (Beam) batch processing\n- BigQuery partitioning by load_date and clustering by record_id\n- Dedup via MERGE into final table; staging table usage\n- Permissive schema + schema evolution; late data handling (24h)\n- Monitoring: Cloud Monitoring dashboards and Logs-based metrics\n\n## Code Example\n```javascript\n// Example data map for a row\nfunction toRecord(line) {\n  const obj = JSON.parse(line);\n  return {\n    record_id: obj.id,\n    load_date: obj.date || new Date().toISOString().slice(0,10),\n    fieldA: obj.fieldA ?? null\n  };\n}\n```\n\n## Follow-up Questions\n- How would you test with synthetic JSON files and validate dedup?\n- How would you handle schema changes that remove fields?\n- What are cost trade-offs between staging vs final tables and clustering?","diagram":"flowchart TD\n  A[Cloud Storage JSONL] --> B[Dataflow Batch]\n  B --> C[Staging BigQuery]\n  C --> D[Final Table (load_date, cluster by record_id)]\n  D --> E[Monitoring & Alerts]","difficulty":"beginner","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","Lyft","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T09:41:21.218Z","createdAt":"2026-01-14T09:41:21.218Z"},{"id":"q-1787","question":"Design a data quality framework for a streaming pipeline ingesting 500 GB/day of JSON events from Pub/Sub into BigQuery. Specify in-flight schema validation, per-field checks (types, nullability, ranges), quarantining bad records, and automatic schema evolution with Data Catalog, plus end-to-end monitoring, alerting, and rollback procedures?","answer":"Leverage Dataflow (Beam) with a Python DoFn validating against an external Avro/JSON schema; route valid events to BigQuery staging and malformed ones to a quarantine table with error codes. Enforce p","explanation":"## Why This Is Asked\nTests practical data quality controls in streaming pipelines on GCP, combining validation, governance, and recoverability.\n\n## Key Concepts\n- In-flight validation against a schema\n- Per-field checks: type, nullability, ranges\n- Quarantine/fallback handling for bad records\n- Data Catalog tagging for schema versions\n- Drift detection with BigQuery ML or SQL\n- Rollback and idempotent sinks\n\n## Code Example\n```python\n# Beam DoFn sketch for in-flight validation\nimport json\nfrom apache_beam import DoFn, pvalue\n\nclass ValidateRecordDoFn(DoFn):\n    def process(self, element, *args, **kwargs):\n        try:\n            obj = json.loads(element)\n            if not isinstance(obj.get(\"user_id\"), str):\n                yield pvalue.TaggedOutput(\"bad\", element)\n                return\n            if obj.get(\"age\", 0) < 0:\n                yield pvalue.TaggedOutput(\"bad\", element)\n                return\n            yield obj\n        except Exception:\n            yield pvalue.TaggedOutput(\"bad\", element)\n```\n\n## Follow-up Questions\n- How would you test this in CI/CD?\n- How would you monitor quality trends and alert on drift?","diagram":"flowchart TD\n  A[Pub/Sub] --> B[Dataflow (Beam)]\n  B --> C[BigQuery Staging]\n  B --> D[Quarantine]\n  C --> E[BigQuery Production]\n  F[Data Catalog Tags] --> C\n  F --> E","difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Goldman Sachs","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T10:49:14.020Z","createdAt":"2026-01-14T10:49:14.020Z"},{"id":"q-1834","question":"Design a cross-tenant data governance pipeline on GCP for 1 TB/day of JSON user activity ingested from Pub/Sub into BigQuery in two regions. Enforce per-tenant privacy with field-level masking, auto-generate end-to-end data lineage with Data Catalog, support backward-compatible schema evolution, and export immutable audit logs to Cloud Storage. Include architecture, data models, rollback plan per-tenant (within 24h), and testing strategy?","answer":"Ingest via Pub/Sub -> Dataflow -> BigQuery in two regions; apply per-tenant masking for PII with DLP before load; tag lineage in Data Catalog for each field and table, and surface per-tenant access vi","explanation":"## Why This Is Asked\nThis question probes architectural rigor in multi-tenant governance, data lineage, privacy controls, and rollback strategy at scale on GCP. It emphasizes secure isolation, schema evolution, and auditable data flows.\n\n## Key Concepts\n- Data governance and lineage with Data Catalog\n- Per-tenant privacy masking with DLP or equivalent\n- Cross-region BigQuery data sharing via Authorized Views\n- Schema evolution with table templates and backward compatibility\n- Immutable audit logging and rollback workflows\n\n## Code Example\n```javascript\n// Example pseudocode illustrating a Dataflow pipeline segment wiring Pub/Sub -> BigQuery\nconst pipeline = ...\n.pipe(PubSubSource())\n  .transform(maskPIIPerTenant)\n  .to(BigQuerySink({region: 'us-central1'}))\n```\n\n## Follow-up Questions\n- How would you test tenant isolation failures?\n- How would you handle late-arriving data affecting lineage labels?","diagram":"flowchart TD\n  A[Pub/Sub] --> B[Dataflow]\n  B --> C[BigQuery (Region 1)]\n  B --> D[BigQuery (Region 2)]\n  C --> E[Data Catalog Tags]\n  D --> E\n  C --> F[Audit Logs Cloud Storage]\n  D --> F","difficulty":"advanced","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T13:16:15.203Z","createdAt":"2026-01-14T13:16:15.203Z"},{"id":"q-1869","question":"Design a GCP streaming pipeline for 2 TB/day of multi-tenant JSON events from Pub/Sub into BigQuery. Create a schema registry in Cloud Storage, validate with a Beam DoFn, quarantine nonconforming records to an invalid dataset, and apply per-tenant DLP masking. Enforce row-level access with BigQuery policies, capture lineage in Data Catalog, and provide rollback and testing plan?","answer":"Design a GCP streaming pipeline for 2 TB/day of multi-tenant JSON events from Pub/Sub into BigQuery. Build a schema registry in Cloud Storage, validate with a Beam DoFn, quarantine nonconforming recor","explanation":"## Why This Is Asked\nProbes multi-tenant data integrity, privacy, and governance at scale, including schema drift handling, masking, and lineage.\n\n## Key Concepts\n- Schema registry in Cloud Storage and Drift detection in Dataflow/Beam\n- Row-level security with BigQuery policies for per-tenant isolation\n- Per-tenant DLP masking and quarantine of invalid data\n- Data Catalog lineage and end-to-end observability\n\n## Code Example\n```javascript\n// Pseudo-Beam DoFn skeleton for schema validation\nclass ValidateSchema extends DoFn {\n  processElement(context) {\n    const record = context.element;\n    // load canonical schema from GCS\n    // validate fields, types, requireds\n    if (valid(record)) {\n      context.output(record);\n    } else {\n      context.output(NOT_VALID, record);\n    }\n  }\n}\n```\n\n## Follow-up Questions\n- How would you test schema drift in production without impacting traffic?\n- How would you implement rollback across tenants if a schema change breaks downstream queries?","diagram":null,"difficulty":"advanced","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Snap","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T14:56:51.986Z","createdAt":"2026-01-14T14:56:51.986Z"},{"id":"q-1906","question":"Ingest a daily batch of JSON records stored in Cloud Storage into BigQuery via a Dataflow (Beam) job. Build a beginner-friendly pipeline that validates a minimal schema (tenant_id, event_id, event_ts), filters out records missing required fields, and deduplicates by (tenant_id, event_id). Load valid data into a partitioned BigQuery table; write invalid records to a separate GCS errors bucket. Include basic health counters and a simple template?","answer":"Batch Dataflow (Beam) reads daily JSON lines from Cloud Storage, validates required fields (tenant_id, event_id, event_ts), filters invalid records, and deduplicates by (tenant_id, event_id). Valid ro","explanation":"## Why This Is Asked\n- Tests ability to implement a reliable, beginner-friendly batch ingestion with data quality checks, idempotent deduplication, and observable health signals in GCP.\n\n## Key Concepts\n- Dataflow/Beam batch pipelines, JSON parsing, and error routing\n- Simple schema validation and field presence checks\n- Deduplication strategy for (tenant_id, event_id)\n- BigQuery partitioning and error data landfill\n- Basic monitoring with counters\n\n## Code Example\n```javascript\n// Java-like pseudocode for Beam; real syntax depends on the SDK (Python/Java)\nclass ValidateAndKey(DoFn):\n  def process(self, elem):\n    required = ['tenant_id','event_id','event_ts']\n    if all(k in elem and elem[k] is not None for k in required):\n      yield ((elem['tenant_id'], elem['event_id']), elem)\n    else:\n      yield ('invalid', elem)\n```\n\n## Follow-up Questions\n- How would you scale this to 10x data and ensure low tail latency?\n- How would you adapt for evolving event schemas and backward compatibility?","diagram":"flowchart TD\n  A[Cloud Storage: daily.json] --> B[Dataflow (Beam)]\n  B --> C{Records Valid?}\n  C -->|Yes| D[BigQuery: partitioned table]\n  C -->|No| E[Errors: GCS bucket]","difficulty":"beginner","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T16:51:30.533Z","createdAt":"2026-01-14T16:51:30.533Z"},{"id":"q-1953","question":"Design a GCP data pipeline for a multinational SaaS product that ingests 200 GB/day of JSON telemetry from Pub/Sub into BigQuery across two regions. Requirements: multi-tenant isolation, automatic schema evolution, per-tenant TTL, data quality checks (schema conformance, nulls, duplicates), and an automated rollback path for schema changes. Include data model sketches, partitioning strategy, testing, and rollback steps?","answer":"Use per-tenant datasets in BigQuery and two-region Dataflow pipelines with a unified Avro/JSON schema registry. Ingest with event_id for dedup, partition by ingestion_date, TTL per tenant (30/90 days)","explanation":"## Why This Is Asked\nAssesses ability to design a compliant, scalable, two-region data pipeline with strong data quality and schema governance, plus a robust rollback path.\n\n## Key Concepts\n- Multi-tenant isolation with per-tenant datasets\n- Two-region active-active ingestion\n- Schema registry and automatic evolution\n- TTL/retention controls per tenant\n- Data quality: schema conformance, null checks, dedup\n- Rollback strategy: versioned schemas and reprocessing window\n- Metadata and lineage via Data Catalog\n\n## Code Example\n```python\n# Pseudocode: Beam DoFn for basic validation and dedup tagging\nclass ValidateEventDoFn(beam.DoFn):\n    def process(self, element):\n        data = json.loads(element)\n        if not data.get('tenant_id') or not data.get('event_id'):\n            yield beam.pvalue.TaggedOutput('invalid', data)\n            return\n        # type checks and masking placeholders\n        yield data\n```\n\n## Follow-up Questions\n- How would you test schema evolution across regions without downtime?\n- What metrics and alerts would you set for quality/rollback events?","diagram":null,"difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Hashicorp","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T18:50:45.651Z","createdAt":"2026-01-14T18:50:45.651Z"},{"id":"q-1997","question":"Ingest daily 1 TB JSON exports dumped into Cloud Storage by partner apps. Design a beginner-friendly GCP batch pipeline to load into BigQuery with per-tenant isolation (datasets), ingestion-date partitioning, and a simple schema-evolution strategy (new fields added as nullable columns). Include data-quality checks (nulls, types) and a rollback path to revert a day’s ingest within 24h. Outline architecture and testing?","answer":"Ingest 1 TB/day JSON exports from Cloud Storage with a batch Dataflow job. Use per-tenant datasets and ingestion-date partitioned tables. Flatten/parse JSON and map new fields to nullable columns to s","explanation":"## Why This Is Asked\nThis question tests practical batch data ingestion on GCP, focusing on beginner-friendly patterns that still show depth around tenant isolation, partitioning, and recoverability.\n\n## Key Concepts\n- Batch processing with Dataflow or Beam\n- Tenant isolation via separate datasets and IAMs\n- BigQuery ingestion-date partitioning\n- Lightweight schema evolution (nullable new fields)\n- Basic data quality checks (nullability, type validation)\n- Rollback strategy within 24h\n\n## Code Example\n\n```javascript\n// Pseudo Dataflow skeleton (illustrative)\nimport apache_beam as beam\nfrom apache_beam.options.pipeline_options import PipelineOptions\n# ... omitted for brevity\n```\n\n## Follow-up Questions\n- How would you test this pipeline end-to-end with failing records?\n- How would you scale tenant isolation when tenants grow into thousands?","diagram":null,"difficulty":"beginner","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Meta","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T20:31:34.562Z","createdAt":"2026-01-14T20:31:34.562Z"},{"id":"q-2073","question":"On GCP, design an observability-driven pipeline that detects schema drift and data-quality anomalies in near real-time for 1 TB/day of JSON events ingested from Pub/Sub into BigQuery across two regions. Include per-tenant lineage, automatic schema evolution, drift-triggered alerts, a self-healing rollback path, and export of audit trails to Cloud Storage. Provide architecture, data models, tests, and operational playbooks?","answer":"Ingest via Pub/Sub to Dataflow (Apache Beam) deployed across two regions, writing to region-partitioned BigQuery tables with tenant isolation. Implement drift detection by comparing incoming event schemas against a canonical schema stored in Data Catalog; on drift detection, trigger automated alerts via Cloud Monitoring and initiate schema evolution workflows. Maintain per-tenant lineage through Data Catalog tags and export audit trails to Cloud Storage for compliance. Include self-healing rollback capabilities using BigQuery time travel and versioned schema definitions.","explanation":"## Why This Is Asked\nThis probes practical observability, schema governance, drift handling, and rollback in a multi-region data pipeline, integrating Dataflow, BigQuery, Data Catalog, and Cloud Storage audits.\n\n## Key Concepts\n- Streaming ingestion and schema drift detection\n- Data Catalog lineage and tenant isolation\n- Schema evolution and rollback mechanics\n- Alerting and self-healing strategies\n\n## Code Example\n```javascript\n// Pseudocode drift detector skeleton\nfunction detectDrift(record, catalog) {\n  // Compare incoming fields with catalog schema\n  return driftDetected;\n}\n```\n\n## Follow","diagram":null,"difficulty":"advanced","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Databricks","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T05:33:18.450Z","createdAt":"2026-01-14T23:26:10.805Z"},{"id":"q-2153","question":"Design a cost-aware multi-tenant GCP data pipeline for streaming telemetry that ingests 150 GB/day of JSON from Pub/Sub into BigQuery across two regions. Enforce per-tenant access with BigQuery row level security and per-tenant dataset versioning; support backward compatible schema evolution; and export versioned snapshots to Cloud Storage. Include data models, partitioning, testing, rollback, and synthetic-tenant generation?","answer":"Ingest via Dataflow from Pub/Sub to two regional BigQuery datasets. Use ingestion-time partitioned tables, cluster by tenant_id, and apply row-level security so each tenant sees only their data. Keep ","explanation":"## Why This Is Asked\nTests real-world multi-tenant data governance, per-tenant isolation, and cost-conscious architecture across regions. It also covers schema evolution, rollback, and data export for audits.\n\n## Key Concepts\n- BigQuery row-level security and per-tenant access\n- Ingestion-time partitioning and tenant clustering\n- Dataset/versioning strategy and backward-compatible schema evolution\n- Cross-region replication and cost controls\n- Synthetic tenant generation and rollback testing\n\n## Code Example\n```sql\n-- example RLAC for tenant isolation (conceptual)\nCREATE ROW ACCESS POLICY tenant_rlac\nON `project.dataset.table`\nUSING (tenant_id = SESSION_USER())\nWITH CHECK (tenant_id = SESSION_USER());\n```\n\n## Follow-up Questions\n- How would you handle schema drift across regions and tenants?\n- What monitoring would you add to detect cost spikes from multi-tenant queries?","diagram":"flowchart TD\n  A[Pub/Sub] --> B[Dataflow]\n  B --> C[BigQuery (Region 1, Tenant data)]\n  B --> D[BigQuery (Region 2, Tenant data)]\n  C --> E[RLAC policies per tenant]\n  D --> E\n  C --> F[Snapshots to Cloud Storage (versioned)]\n  D --> F","difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["NVIDIA","Oracle","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T05:35:12.126Z","createdAt":"2026-01-15T05:35:12.126Z"},{"id":"q-2227","question":"Design a real-time data ingestion pipeline on GCP for a game analytics platform: 60 GB/day of JSON events from Pub/Sub into BigQuery across two regions. Enforce per-tenant isolation with field-level masking, automatic schema evolution, and a per-tenant canary rollout with a controlled rollback path. Include data models, testing strategy, lineage via Data Catalog, and cost controls. Provide a concrete implementation plan?","answer":"Streaming pipeline: Pub/Sub → Dataflow → BigQuery across two regions with per-tenant isolation and field-level masking. Enable tenant-scoped schema evolution via versioned schemas in Data Catalog, plu","explanation":"## Why This Is Asked\nEvaluates real-time ingestion, tenant isolation, and controlled schema changes with governance across regions.\n\n## Key Concepts\n- Real-time streaming with Pub/Sub and Dataflow\n- Per-tenant masking and isolation\n- Canary deployments and rollback automation\n- Data Catalog lineage and versioned schemas\n\n## Code Example\n```javascript\n// Skeleton: Beam transform applying masking using a per-tenant function\nimport apache_beam as beam\nclass MaskPerTenant(beam.DoFn):\n  def process(self, element, *args, **kwargs):\n    # tenant_id = element['tenant_id']\n    # mask fields per tenant\n    return [element]\n```\n\n## Follow-up Questions\n- How would you implement automatic backfill for late data?\n- How would you monitor data quality and roll back a breaking change?","diagram":"flowchart TD\n  A[Pub/Sub: Events] --> B[Dataflow: Transform]\n  B --> C[BigQuery: Region 1]\n  B --> D[BigQuery: Region 2]\n  C --> E[Data Catalog: Lineage]\n  D --> E","difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Meta","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T08:43:19.512Z","createdAt":"2026-01-15T08:43:19.512Z"},{"id":"q-2364","question":"Design a cross-tenant data exchange on GCP: ingest 1 TB/day of JSON telemetry from Pub/Sub into two BigQuery regions. Implement per-tenant isolation via dataset-level access controls and masked views, and use Data Catalog policy tags to enforce visibility. Model a pragmatic two-layer architecture: raw + curated; ensure end-to-end lineage; support controlled data sharing via explicit contracts. Provide a 24h per-tenant rollback plan and testing strategy?","answer":"Route 1 TB/day via Dataflow to BigQuery in Region A and Region B. Implement per-tenant isolation with dataset-level access and masked views; enforce visibility with Data Catalog policy tags. Build raw","explanation":"## Why This Is Asked\nExplores data-contract-driven governance and cross-region sharing, a real-world need for large, multi-tenant analytics.\n\n## Key Concepts\n- Data contracts in Data Catalog\n- Per-tenant isolation via views/policies\n- End-to-end lineage\n- Two-region replication and cost-aware design\n- Rollback strategy with time travel and canaries\n\n## Code Example\n```python\n# pseudo-contract validator\ndef validate_contract(contract):\n  required_fields = {\"tenant_id\",\"schema\"}\n  return required_fields.issubset(contract.keys())\n```\n\n## Follow-up Questions\n- How would you test contract violations in CI/CD?\n- How would you handle schema evolution across tenants without breaking sharing contracts?","diagram":null,"difficulty":"advanced","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","NVIDIA","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T14:54:28.599Z","createdAt":"2026-01-15T14:54:28.599Z"},{"id":"q-2410","question":"Design an end-to-end GCP data ingestion pipeline for 1-2 TB/day of JSON telemetry streamed from Pub/Sub into BigQuery with per-tenant isolation and strict data residency policies. The solution must mask PII per tenant at ingestion (field-level), use CMEK, and employ DLP where needed. Provide two-region architecture, automatic schema evolution, late data handling, and a rollback/backfill plan, plus end-to-end data lineage in Data Catalog and audit exports. Include architecture, data model, testing, and runbook details?","answer":"Pub/Sub → Dataflow streaming; per-tenant isolation via separate datasets and partitioned tables; field-level masking at parse time driven by a central policy store; CMEK encryption on BigQuery; DLP fo","explanation":"## Why This Is Asked\nTests ability to design a privacy-aware, multi-region data pipeline with governance and testing in a realistic setting.\n\n## Key Concepts\n- Pub/Sub streaming\n- Multi-tenant masking and isolation\n- CMEK and DLP integration\n- Data Catalog lineage\n- Schema evolution and backfills\n- Two-region resilience and audits\n\n## Code Example\n```javascript\n// Dataflow DoFn: mask PII based on tenant policy\nclass MaskPII {\n  process(record, ctx) {\n    const policy = getPolicy(ctx.tenantId);\n    return maskRecord(record, policy);\n  }\n}\n```\n\n## Follow-up Questions\n- How would you test end-to-end lineage accuracy? \n- How would you handle policy updates without downtime?","diagram":null,"difficulty":"advanced","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Hugging Face","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T16:59:12.833Z","createdAt":"2026-01-15T16:59:12.833Z"},{"id":"q-2523","question":"On GCP, design a data mesh for 3 domains (Sales, Product, Ops) where each domain owns its BigQuery data products, publishes a standard schema, and uses Data Catalog and IAM for cross-domain governance. Include an automated lineage from source events to data products, a policy-driven access layer with per-domain permissions, and a testing/rollback plan for schema drift. Provide concrete data models and workflow?","answer":"Implement a data mesh architecture with domain-owned BigQuery datasets for Sales, Product, and Operations. Each domain manages its own data products using standardized schemas, while Data Catalog provides centralized metadata management with tag templates for automated lineage tracking. Establish per-domain IAM roles and Authorized Views to enable secure cross-domain data access. Deploy CI/CD pipelines with schema validation tests to detect and prevent drift, with automated rollback capabilities for schema changes.","explanation":"## Why This Is Asked\nThis evaluates practical implementation of data mesh principles, governance frameworks, and lineage automation on Google Cloud Platform.\n\n## Key Concepts\n- Domain-driven data ownership with BigQuery datasets\n- Data Catalog metadata management and automated lineage\n- IAM-based access control with Authorized Views\n- CI/CD pipeline integration for schema drift prevention\n- Cross-domain governance with policy-driven permissions\n\n## Code Example\n```javascript\nfunction validateSchema(expectedSchema, actualSchema) {\n  const expected = JSON.stringify(expectedSchema);\n  const actual = JSON.stringify(actualSchema);\n  return expected === actual;\n}\n```","diagram":"flowchart TD\n  A[Source Events] --> B[Dataflow/Beam] \n  B --> C[Domain Datasets] \n  C --> D[Data Catalog & Lineage] \n  D --> E[BI & ML Serving]","difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Netflix","Snowflake","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T05:46:15.342Z","createdAt":"2026-01-15T21:34:05.475Z"},{"id":"q-2602","question":"Design a multi-tenant, streaming data platform on GCP for a financial app that ingests 1 TB/day of JSON events via Pub/Sub, processes them with Apache Beam on Dataflow, and writes per-tenant data products to BigQuery across three regions. Explain per-tenant isolation, automatic schema evolution, late data handling, data quality checks, rollback paths, cross-region replication, Data Catalog lineage, and testing strategy?","answer":"Per-tenant isolation: distinct Pub/Sub topics and BigQuery datasets per tenant; Dataflow pipelines with exactly-once writes; centralized schema registry with Data Catalog for evolution; QC checks (sch","explanation":"## Why This Is Asked\nEvaluates real-world ability to design a scalable, multi-tenant streaming pipeline on GCP with strict data governance, schema evolution, cross-region durability, and robust testing.\n\n## Key Concepts\n- Multi-tenant isolation across Pub/Sub and BigQuery\n- Dataflow (Beam) streaming pipelines with exactly-once semantics\n- Centralized schema evolution using a registry and Data Catalog lineage\n- Data quality checks: schema conformance, dedup, nulls\n- Late data handling and watermarking\n- Cross-region replication and rollback/backfill strategies\n- Testing: end-to-end, drift, canaries\n\n## Code Example\n```python\nimport apache_beam as beam\nfrom apache_beam.options.pipeline_options import PipelineOptions\nimport json\n\nclass ParseJsonDoFn(beam.DoFn):\n    def process(self, element):\n        yield json.loads(element)\n\noptions = PipelineOptions(streaming=True)\nwith beam.Pipeline(options=options) as p:\n    (p\n     | 'ReadPubSub' >> beam.io.ReadFromPubSub(subscription='projects/PROJ/subscriptions/tenant-sub')\n     | 'Parse' >> beam.ParDo(ParseJsonDoFn())\n     | 'ToBQ' >> beam.io.WriteToBigQuery(\n         table='tenant_ds.tenant_table',\n         dataset='tenant_ds',\n         project='PROJECT',\n         write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND,\n         create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED))\n```\n\n## Follow-up Questions\n- How would you implement idempotent writes to BigQuery across regions?\n- What monitoring and alerting would you add for schema drift per tenant?","diagram":"flowchart TD\n  PubSub[Pub/Sub per-tenant topics]\n  Dataflow[Dataflow streaming pipelines]\n  BigQuery[BigQuery cross-region tables]\n  Catalog[Data Catalog lineage]\n  Access[Per-tenant IAM]\n  QC[Quality checks]\n  Rollback[Rollback plan]\n  \n  PubSub --> Dataflow\n  Dataflow --> BigQuery\n  BigQuery --> Catalog\n  Catalog --> Access\n  QC --> Rollback","difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Microsoft","Robinhood","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T02:32:08.185Z","createdAt":"2026-01-16T02:32:08.185Z"},{"id":"q-931","question":"A GCP pipeline ingests 1 TB of JSON user activity daily from Pub/Sub into BigQuery via Dataflow. New fields appear over time; you must evolve the schema without downtime. What approach would you use for schema drift and nested fields, and outline concrete steps to implement it?","answer":"Use a two-layer scheme: a stable BigQuery table with known fields and a separate extra_json column to capture drift. The Dataflow job maps incoming JSON to the known schema; unknown fields are stored ","explanation":"## Why This Is Asked\nThis question probes practical handling of evolving data in a streaming pipeline, balancing query stability with schema growth.\n\n## Key Concepts\n- Schema evolution in BigQuery\n- Semi-structured data (JSON)\n- Dataflow transforms for parsing\n- Backward compatibility\n- Metadata and cataloging\n\n## Code Example\n```javascript\nfunction parseEvent(json) {\n  const data = JSON.parse(json);\n  const known = {\n    userId: data.user_id || null,\n    eventTime: data.event_time || null,\n    country: data.country || null\n  };\n  const extraKeys = Object.keys(data).filter(k => !(k in known));\n  const extra = JSON.stringify(extraKeys.reduce((acc, k) => { acc[k] = data[k]; return acc; }, {}));\n  return { ...known, extra_json: extra };\n}\n```\n\n## Follow-up Questions\n- How would you migrate existing data to the new table version with minimal downtime?\n- How would you monitor for schema drift and alert when new fields appear?","diagram":null,"difficulty":"beginner","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Discord","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T15:41:21.360Z","createdAt":"2026-01-12T15:41:21.361Z"},{"id":"q-962","question":"In a GCP streaming pipeline, Pub/Sub feeds millions of events into BigQuery via Dataflow. Out-of-order arrivals and duplicates occur. Design an idempotent sink using a staging table and a final partitioned table, leveraging insertId for dedup and a MERGE strategy to upsert into the final table. Outline concrete steps and trade-offs to implement?","answer":"Use a staging BigQuery table where Dataflow writes with insertId derived from a stable key (user_id + event_id + event_time). Periodically MERGE from staging into a final partitioned table, deduplicat","explanation":"## Why This Is Asked\n\nTests practical data quality, idempotent sinks, and operational design using Dataflow + BigQuery features.\n\n## Key Concepts\n\n- Idempotent sink\n- insertId dedup\n- staging vs final table\n- MERGE in BigQuery\n- allowed lateness and partitioning\n- quotas and monitoring\n\n## Code Example\n\n```sql\nMERGE INTO dataset.final AS F\nUSING dataset.staging AS S\nON F.user_id = S.user_id AND F.event_time = S.event_time\nWHEN NOT MATCHED THEN INSERT (user_id, event_time, event_id, action, payload)\nWHEN MATCHED THEN UPDATE SET action = S.action, payload = S.payload;\n```\n\n## Follow-up Questions\n\n- How to handle tombstones?\n- How to scale MERGE with sharded partitions?\n- How would you instrument observability for dedup latency?","diagram":"flowchart TD\n  PubSub[Pub/Sub] --> Dataflow[Dataflow]\n  Dataflow --> Staging[StagingBQ]\n  Staging --> Final[FinalBQ]\n  Staging -->|MERGE| Final","difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Google","Instacart"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T17:23:31.180Z","createdAt":"2026-01-12T17:23:31.180Z"}],"subChannels":["general"],"companies":["Amazon","Anthropic","Apple","Bloomberg","Citadel","Cloudflare","Coinbase","Databricks","Discord","Goldman Sachs","Google","Hashicorp","Hugging Face","IBM","Instacart","Lyft","Meta","Microsoft","MongoDB","NVIDIA","Netflix","OpenAI","Oracle","PayPal","Plaid","Robinhood","Salesforce","Scale Ai","Snap","Snowflake","Square","Stripe","Tesla","Twitter","Two Sigma","Uber","Zoom"],"stats":{"total":27,"beginner":9,"intermediate":11,"advanced":7,"newThisWeek":27}}