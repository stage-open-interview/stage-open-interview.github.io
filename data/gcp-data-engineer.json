{"questions":[{"id":"q-1222","question":"Design a real-time ad-click analytics pipeline in GCP that ingests Pub/Sub events via Dataflow into BigQuery, while implementing 90-day TTL on PII data, exporting audits to Cloud Storage, and handling schema evolution, late data, dedup, and rollback. Provide concrete architecture, data models, and operational steps?","answer":"Two-tier streaming: a BigQuery hot table partitioned by ingestion_date with 90-day partition expiration for PII, plus a daily Cloud Storage Parquet archive for audits. Dataflow templates ingest Pub/Su","explanation":"## Why This Is Asked\nEvaluates real-world trade-offs in latency, cost, and governance for streaming pipelines on GCP.\n\n## Key Concepts\n- Pub/Sub -> Dataflow streaming\n- BigQuery partition expiration (90 days)\n- Parquet archive in Cloud Storage\n- Schema evolution with nested fields\n- Deduplication and late data handling\n- Rollback/versioned templates\n\n## Code Example\n```javascript\n// Pseudo Beam skeleton: dedupe by event_id and write to two sinks\n```\n\n## Follow-up Questions\n- How to validate TTL and archiving with tests?\n- What monitoring and alerting would you add for duty-cycle failures?","diagram":"flowchart TD\n  PubSub[Pub/Sub] --> Dataflow[Dataflow Template]\n  Dataflow --> BigQueryHot[BigQuery: Ingest (90d TTL)]\n  Dataflow --> Archive[Cloud Storage: Parquet Archive]\n  BigQueryHot --> Monitoring[Monitoring & Audits]","difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","NVIDIA","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T05:33:44.453Z","createdAt":"2026-01-13T05:33:44.453Z"},{"id":"q-1310","question":"You operate a streaming ingestion pipeline: Pub/Sub -> Dataflow -> BigQuery. Daily 50k events with fields user_id, event_type, amount (which can be string). Propose a simple ingestion-time data quality plan that validates JSON schema, coerces types, and routes invalid records to a dead-letter Pub/Sub topic without stopping the pipeline. Include testing with a small sample and how you monitor invalid counts?","answer":"Parse Pub/Sub messages with a simple Beam DoFn that validates required fields (user_id, event_type) and coerces amount to float. Valid records go to BigQuery; invalid ones emit to a dead-letter Pub/Su","explanation":"## Why This Is Asked\n\nAssess data quality at ingestion and the practicality of dead-letter routing in a real pipeline.\n\n## Key Concepts\n\n- Data quality at ingestion\n- Side outputs / dead-letter routing\n- Type coercion and validation\n- Beam/Dataflow to BigQuery integration\n- Monitoring\n\n## Code Example\n\n```javascript\n// Pseudo Beam DoFn for ingestion validation\nfunction processElement(element) {\n  let obj;\n  try { obj = JSON.parse(element); } catch (e) { return {error: 'invalid_json'}; }\n  if (!obj.user_id || !obj.event_type) return {error: 'missing_fields'};\n  const amount = parseFloat(obj.amount);\n  if (Number.isNaN(amount)) return {error: 'invalid_amount'};\n  return {user_id: obj.user_id, event_type: obj.event_type, amount: amount};\n}\n```\n\n## Follow-up Questions\n\n- How would you test dead-letter routing under load?\n- How would you tune retry/backoff for DLQ messages?\n","diagram":null,"difficulty":"beginner","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","PayPal","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T10:36:08.797Z","createdAt":"2026-01-13T10:36:08.797Z"},{"id":"q-1319","question":"Design a compliant streaming data platform in GCP for a fintech app that ingests transaction events from Pub/Sub through Dataflow into BigQuery. New fields may appear over time; PII must be detected and masked before analytics, while raw data is retained with restricted access. Outline the end-to-end architecture, data models, schema evolution strategy, PII handling, lineage, retention, and operational monitoring with concrete steps and trade-offs?","answer":"Implement a streaming Dataflow pipeline from Pub/Sub to a partitioned, clustered BigQuery table with schema registry in Data Catalog for forward/backward compatibility. Validate records, redact PII vi","explanation":"## Why This Is Asked\nReal-world streaming data governance, including schema evolution, PII handling, and lineage, is critical in fintech.\n\n## Key Concepts\n- Streaming ingestion with Pub/Sub and Dataflow\n- BigQuery partitioning and clustering\n- Schema registry and compatibility management\n- PII detection and masking (DLP)\n- Data lineage and metadata (Data Catalog)\n- Retention TTLs and access controls\n- Monitoring and alerting (Cloud Monitoring)\n\n## Code Example\n```python\ndef transform(record, registry_schema):\n    if not validate(record, registry_schema):\n        raise ValueError('Schema mismatch')\n    pii = ['acct_number', 'ssn']\n    masked = mask_pii(record, pii)\n    return masked\n```\n\n## Follow-up Questions\n- How would you test end-to-end TTL retention and schema evolution without downtime?\n- How would you handle cross-producer schema drift and multiple versions simultaneously?","diagram":"flowchart TD\n  A[Pub/Sub] --> B[Dataflow]\n  B --> C[BigQuery Table]\n  C --> D[Masked Analytics Views]\n  B --> E[Data Catalog (Lineage)]\n  C --> F[Raw Data with TTL Policy]","difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Plaid","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T11:31:53.811Z","createdAt":"2026-01-13T11:31:53.811Z"},{"id":"q-1389","question":"You receive streaming JSON events from Pub/Sub. Design a beginner-friendly GCP pipeline using Dataflow (Beam) to parse/flatten events, write to a partitioned BigQuery table (partition by event_date), and deduplicate by event_id. Late data allowed up to 6 hours. Outline architecture, data model, schema-change handling, and monitoring?","answer":"Design a Dataflow (Beam) streaming job that reads Pub/Sub JSON, flattens fields into a simple schema, writes to a partitioned BigQuery table by event_date, and deduplicates on event_id within a 5-minu","explanation":"## Why This Is Asked\n\nTests a practical, beginner-friendly data ingestion design on GCP, covering streaming, dedup, partitioning, late data handling, and basic schema evolution.\n\n## Key Concepts\n\n- Dataflow (Beam) streaming pipeline\n- Pub/Sub as the source\n- BigQuery partitioned tables and upserts\n- Deduplication by event_id\n- Lightweight schema-change and monitoring\n\n## Code Example\n\n```javascript\n// Pseudo-Beam-like snippet (conceptual)\nfunction parseEvent(raw) {\n  const obj = JSON.parse(raw);\n  return { event_id: obj.id, event_date: obj.date, payload: obj.payload };\n}\n```\n\n## Follow-up Questions\n\n- How would you test the dedup logic in Dataflow?\n- How would you monitor latency and late-data rates in this pipeline?","diagram":null,"difficulty":"beginner","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","Snap","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T14:50:08.738Z","createdAt":"2026-01-13T14:50:08.738Z"},{"id":"q-1415","question":"You are building a multi-source data platform where raw data arrives as Parquet in Google Cloud Storage, Avro via Pub/Sub, and JSON through a partner API; design a two-stage pipeline that normalizes all formats into a canonical Parquet dataset, ingested into a partitioned BigQuery table, with schema drift handling, data validation, and end-to-end lineage. Include how you would implement idempotent ingests, rollback, and cross-team access controls?","answer":"Implement a two-stage pipeline: stage1 normalizes Parquet(JSON/Avro) to a canonical Parquet schema, stage2 loads to a partitioned BigQuery table. Use a central schema registry and field-level nulls fo","explanation":"## Why This Is Asked\nDesign a multi-source ingestion pattern with schema drift handling, cross-format canonicalization, and governance; evaluate practical trade-offs.\n\n## Key Concepts\n- Multi-format normalization and canonical schema\n- Dataflow/Beam idempotent ingestion and exactly-once semantics\n- Schema evolution, drift handling, and validation\n- Data lineage, access controls, and rollback mechanisms\n\n## Code Example\n```javascript\nfunction toCanonical(rec){\n  // normalize fields, coerce types, assign stable id\n  return { id: rec.id ?? generateId(), ts: new Date(rec.ts), payload: rec.payload ?? null };\n}\n```\n\n## Follow-up Questions\n- How would you test schema drift and automate backwards-compatible changes?\n- How would you monitor latency and data skew across sources?","diagram":"flowchart TD\n  A[Ingest Parquet (GCS)] --> B[Normalize to Canonical Parquet]\n  C[Ingest Avro (Pub/Sub)] --> B\n  D[Ingest JSON (API)] --> B\n  B --> E[BigQuery (Partitioned)]\n  E --> F[Lineage & Auditing]","difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Snowflake","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T15:53:31.239Z","createdAt":"2026-01-13T15:53:31.239Z"},{"id":"q-1449","question":"Design a streaming enrichment pipeline: Pub/Sub → Dataflow → BigQuery, with an inline governance layer enforcing per-record rules (required fields, value ranges, cross-field consistency) at ingestion. Records failing rules go to a quarantine Cloud Storage bucket; good records load to a partitioned BigQuery table. Describe data model, rule engine approach, idempotence, backpressure, and testing/monitoring plan?","answer":"Implement a DoFn in Dataflow that validates each event against a schema (required fields, ranges, cross-field constraints). Use tagged outputs: main for valid records, quarantine for invalid. Write ma","explanation":"## Why This Is Asked\n\nAssesses ability to embed governance in streaming, handling bad data without blocking analytics, and operationalize quarantine and observability.\n\n## Key Concepts\n\n- Ingestion-time data quality checks\n- Dataflow ParDo with side outputs for quarantines\n- Idempotent BigQuery writes and record versioning\n- Backpressure and dead-letter handling\n- Observability and testing strategy\n\n## Code Example\n\n```python\nimport apache_beam as beam\n\nclass Govern(beam.DoFn):\n    def process(self, e):\n        if not e.get('user_id') or e.get('age', 0) < 0:\n            yield beam.pvalue.TaggedOutput('quarantine', e)\n        else:\n            yield e\n\nwith beam.Pipeline(...) as p:\n    raw = p | 'Read' >> beam.io.ReadFromPubSub(topic=...)  \n    main, bad = (raw | 'Govern' >> beam.ParDo(Govern()).with_outputs('quarantine', main='main'))\n    main | 'BQ' >> beam.io.WriteToBigQuery(table=..., schema=...)\n    bad | 'Quarantine' >> beam.io.WriteToText('gs://bucket/quarantine/')\n```\n\n## Follow-up Questions\n\n- How would you handle schema evolution in the governance layer without breaking downstream?\n- How to quantify and alert on data quality drift over time?\n- What tests would you add for end-to-end validation and rollback scenarios?","diagram":"flowchart TD\n  PubSub[Pub/Sub] --> Dataflow[Dataflow]\n  Dataflow --> BigQuery[BigQuery]\n  Dataflow --> Quarantine[GCS Quarantine]\n  Quarantine --> Audits[Audit Logs]\n  BigQuery --> Reports[BI Dashboards]","difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Instacart","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T17:45:34.896Z","createdAt":"2026-01-13T17:45:34.896Z"},{"id":"q-1473","question":"In a beginner friendly GCP data pipeline Pub/Sub -> Dataflow -> BigQuery design robust observability and recovery. Describe how to instrument Dataflow with metrics backlog throughput processed failed, implement a dead letter policy that dumps failed JSON messages to Cloud Storage, set retry backoff, and outline a replay path to re ing est from Cloud Storage. Include a concrete alert rule backlog throughput greater than 0.2 for 5 minutes and a brief example snippet?","answer":"Instrument Dataflow with metrics backlog, throughput, processed, and failed; emit structured logs to Cloud Logging. Route failed messages to a per topic Cloud Storage DLQ with 24h retention. Use expon","explanation":"## Why This Is Asked\nTests practical observability and recoverability for a streaming pipeline in fintech-like constraints, focusing on actionable metrics and a concrete DLQ/replay path.\n\n## Key Concepts\n- Dataflow metrics and structured logging\n- Dead-letter queues and DLQ retention\n- Retry/backoff strategies\n- Replay/re-ingestion pipelines and validation\n\n## Code Example\n```python\n# Pseudocode: DLQ handling in a Beam DoFn\nclass Process(DoFn):\n  def process(self, element):\n    try:\n      yield element\n    except Exception:\n      write_to_dlq(element)\n```\n\n## Follow-up Questions\n- How would you test the DLQ path with simulated failures?\n- What are the cost and complexity trade-offs of DLQ retention and replay?\n","diagram":"flowchart TD\n  A[Pub/Sub] --> B[Dataflow]\n  B --> C[BigQuery]\n  subgraph DLQ\n    D[Cloud Storage DLQ]\n  end\n  D --> E[Replay Job]","difficulty":"beginner","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","IBM"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T18:48:00.248Z","createdAt":"2026-01-13T18:48:00.248Z"},{"id":"q-1546","question":"In GCP, ingest daily Pub/Sub JSON events through Dataflow into BigQuery. Implement basic observability and reliability: 1) emit metrics for ingested, processed, and failed events; 2) export metrics to Cloud Monitoring and alert on failures for three consecutive checks; 3) deduplicate by event_id; 4) log audits to Cloud Storage. Outline architecture, data model, and concrete steps?","answer":"Implement a Dataflow (Apache Beam) pipeline that reads Pub/Sub JSON messages, parses them with a DoFn that emits metrics for ingested, processed, and failed events, then writes to BigQuery via streaming inserts with insertId set to event_id for deduplication. Export metrics to Cloud Monitoring and create alert policies for three consecutive failures. Log audit events to Cloud Storage for compliance tracking.","explanation":"## Why This Is Asked\nThis tests practical observability and data reliability skills in a real-world, beginner-friendly setting.\n\n## Key Concepts\n- Beam Metrics and Cloud Monitoring integration\n- Idempotent streaming writes with insertId\n- Lightweight auditing and error handling\n- Alerting on data quality metrics\n\n## Code Example\n```javascript\n// Pseudo-beam sketch (JavaScript/TypeScript syntax used for illustration)\nclass ParseEvent extends DoFn {\n  @ProcessElement\n  process(@Element json, output) {\n    const event = JSON.parse(json.image);\n    this.ingested.inc();\n    // ... validation and transformation\n    this.processed.inc();\n    output(event);\n  }\n  \n  @Setup\n  setup() {\n    this.ingested = Metrics.counter('pipeline', 'ingested_events');\n    this.processed = Metrics.counter('pipeline', 'processed_events');\n    this.failed = Metrics.counter('pipeline', 'failed_events');\n  }\n}\n```","diagram":"flowchart TD\n  PubSub[Pub/Sub] --> Dataflow[Dataflow / Beam]\n  Dataflow --> BigQuery[BigQuery]\n  Dataflow --> Monitor[Cloud Monitoring]\n  Dataflow --> Logs[Cloud Logging]\n  BigQuery --> Audit[Audit Logs (Cloud Storage)]","difficulty":"beginner","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Microsoft","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T06:31:05.145Z","createdAt":"2026-01-13T21:34:41.250Z"},{"id":"q-1581","question":"In Pub/Sub, daily JSON events contain PHI fields. Design a beginner-friendly GCP pipeline using Dataflow (Beam) to redact PHI with Cloud DLP before loading into BigQuery, and emit a separate audit log to Cloud Storage. Outline architecture, data model, and validation steps, including how you’d monitor masking failures?","answer":"Implement a Dataflow (Apache Beam) streaming job that reads JSON events from Pub/Sub, applies Cloud DLP masking to PHI fields (name, DOB, SSN), writes the redacted records to a BigQuery dataset, and emits audit logs to Cloud Storage. The pipeline includes error handling, dead-letter queues, and monitoring for masking failures.","explanation":"## Why This Is Asked\n\nTests practical data masking and data lake hygiene using familiar GCP tools. It ensures candidates can architect a compliant pipeline without overcomplicating a beginner task.\n\n## Key Concepts\n\n- Dataflow (Beam) streaming ingestion\n- Cloud DLP masking of PHI\n- BigQuery storage of redacted data\n- Cloud Storage audit logs and monitoring\n\n## Code Example\n\n```javascript\n// Pseudo Beam skeleton: read Pub/Sub, redact PHI with DLP, write to BigQuery, emit audit to GCS\n```\n\n## Follow-up Questions\n\n- How would you test masking coverage and false negatives in this pipeline?\n- What monitoring metrics would you track to ensure compliance?","diagram":null,"difficulty":"beginner","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T05:50:38.131Z","createdAt":"2026-01-13T22:48:46.197Z"},{"id":"q-1596","question":"Design a streaming pipeline: Pub/Sub → Dataflow (Beam) → BigQuery. Core: a dynamic data quality gate using a Firestore-stored policy engine that validates fields/types at ingest; invalid records serialized to Cloud Storage as JSONL with metadata; valid records load to BigQuery; publish quality metrics to Cloud Monitoring; handle drift/backpressure without downtime. Explain architecture, data model, and operational steps?","answer":"Design a streaming pipeline with Pub/Sub → Dataflow (Beam) → BigQuery, featuring a dynamic data quality gate using a Firestore-stored policy engine for field/type validation at ingest. Invalid records are serialized to Cloud Storage as JSONL with metadata, while valid records load to BigQuery. Quality metrics are published to Cloud Monitoring, and the system handles schema drift and backpressure without downtime through adaptive processing and buffering strategies.","explanation":"## Why This Is Asked\n\nTests ability to design a robust, observable streaming pipeline with runtime policy-based validation and safe failure handling at scale.\n\n## Key Concepts\n\n- Streaming ingestion with Pub/Sub and Dataflow (Beam)\n- Dynamic policy engine (Firestore) for field/type validation\n- Dead-letter/quarantine in Cloud Storage with metadata\n- Custom metrics in Cloud Monitoring for data quality\n- Schema drift resilience and backpressure handling\n\n## Code Example\n\n```python\n# Example: skeleton DoFn using Firestore policy to validate records\nclass ValidateWithPolicy(DoFn):\n    def setup(self):\n        self.firestore_client = firestore.Client()\n        self.policy_cache = {}\n    \n    def process(self, element):\n        # Load policy from Firestore with caching\n        policy = self.get_policy(element.get('schema_version'))\n        \n        # Validate fields/types against policy\n        if self.validate_record(element, policy):\n            yield element\n        else:\n            # Send invalid records to dead-letter sink\n            yield pvalue.TaggedOutput('invalid', {\n                'record': element,\n                'metadata': {\n                    'timestamp': datetime.utcnow().isoformat(),\n                    'validation_errors': self.get_validation_errors(element, policy)\n                }\n            })\n```","diagram":null,"difficulty":"advanced","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Hashicorp","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T05:34:10.860Z","createdAt":"2026-01-14T02:24:24.969Z"},{"id":"q-1722","question":"Design a cross-region, real-time analytics pipeline for a fintech on GCP. Ingest 100 GB/day of JSON events from Pub/Sub into BigQuery while ensuring per-tenant data isolation, a 90-day TTL on PII, and a rollback path. Show the architecture, data models, and operational steps for schema evolution, late data, dedup, and cost control?","answer":"Ingest via Pub/Sub -> Dataflow (Streaming) -> BigQuery (partitioned by day) with per-tenant views; store raw Parquet in Cloud Storage for audits; implement 90-day TTL by a scheduled Dataflow job that ","explanation":"## Why This Is Asked\nTests ability to design cross-region, scalable analytics with data governance, TTL, and schema evolution.\n\n## Key Concepts\n- Cross-region ingestion, storage\n- PII TTL and redaction\n- Tenant isolation with Authorized Views/IAM\n- Schema evolution and drift handling\n- Late data, dedup, and cost control\n\n## Code Example\n```javascript\n// Dataflow streaming pseudo-template\nconst events = pipeline\n  .apply('ReadPubSub', PubsubIO.readStrings().fromTopic('projects/..../topics/events'))\n  .apply('Parse', ParDo.of(new ParseFn()))\n  .apply('Deduplicate', /* ... */)\n  .apply('WriteBQ', BigQueryIO.writeTableRows().to('project.dataset.table$YYYYMMDD'))\n```\n\n## Follow-up Questions\n- How would you secure data in transit and at rest?\n- How would you monitor costs and adjust partitions?","diagram":null,"difficulty":"advanced","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Oracle","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T08:40:44.958Z","createdAt":"2026-01-14T08:40:44.959Z"},{"id":"q-1754","question":"Design a beginner-friendly GCP pipeline to load daily 2-5 GB of newline-delimited JSON logs from Cloud Storage into BigQuery. Keep only the latest 45 days in a partitioned table (partition by load_date, clustered by record_id). Ensure dedup by record_id, handle optional new fields with a permissive schema, allow late data up to 24 hours, and provide basic monitoring. Choose between Dataflow (Beam) or Cloud Functions, justify, and outline data model, transformations, error handling, and testing strategies?","answer":"Use Dataflow (Beam) in batch mode to read daily JSONL from Cloud Storage, parse and flatten, write to a staging table, then MERGE into a final partitioned table (load_date) clustered by record_id for ","explanation":"## Why This Is Asked\nThis question probes how a junior data engineer designs an end-to-end GCP pipeline for file-based ingestion, dedup, and schema evolution—a common real-world task that isn't covered by Pub/Sub-focused questions.\n\n## Key Concepts\n- Data ingestion from Cloud Storage using Dataflow (Beam) batch processing\n- BigQuery partitioning by load_date and clustering by record_id\n- Dedup via MERGE into final table; staging table usage\n- Permissive schema + schema evolution; late data handling (24h)\n- Monitoring: Cloud Monitoring dashboards and Logs-based metrics\n\n## Code Example\n```javascript\n// Example data map for a row\nfunction toRecord(line) {\n  const obj = JSON.parse(line);\n  return {\n    record_id: obj.id,\n    load_date: obj.date || new Date().toISOString().slice(0,10),\n    fieldA: obj.fieldA ?? null\n  };\n}\n```\n\n## Follow-up Questions\n- How would you test with synthetic JSON files and validate dedup?\n- How would you handle schema changes that remove fields?\n- What are cost trade-offs between staging vs final tables and clustering?","diagram":"flowchart TD\n  A[Cloud Storage JSONL] --> B[Dataflow Batch]\n  B --> C[Staging BigQuery]\n  C --> D[Final Table (load_date, cluster by record_id)]\n  D --> E[Monitoring & Alerts]","difficulty":"beginner","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","Lyft","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T09:41:21.218Z","createdAt":"2026-01-14T09:41:21.218Z"},{"id":"q-1787","question":"Design a data quality framework for a streaming pipeline ingesting 500 GB/day of JSON events from Pub/Sub into BigQuery. Specify in-flight schema validation, per-field checks (types, nullability, ranges), quarantining bad records, and automatic schema evolution with Data Catalog, plus end-to-end monitoring, alerting, and rollback procedures?","answer":"Leverage Dataflow (Beam) with a Python DoFn validating against an external Avro/JSON schema; route valid events to BigQuery staging and malformed ones to a quarantine table with error codes. Enforce p","explanation":"## Why This Is Asked\nTests practical data quality controls in streaming pipelines on GCP, combining validation, governance, and recoverability.\n\n## Key Concepts\n- In-flight validation against a schema\n- Per-field checks: type, nullability, ranges\n- Quarantine/fallback handling for bad records\n- Data Catalog tagging for schema versions\n- Drift detection with BigQuery ML or SQL\n- Rollback and idempotent sinks\n\n## Code Example\n```python\n# Beam DoFn sketch for in-flight validation\nimport json\nfrom apache_beam import DoFn, pvalue\n\nclass ValidateRecordDoFn(DoFn):\n    def process(self, element, *args, **kwargs):\n        try:\n            obj = json.loads(element)\n            if not isinstance(obj.get(\"user_id\"), str):\n                yield pvalue.TaggedOutput(\"bad\", element)\n                return\n            if obj.get(\"age\", 0) < 0:\n                yield pvalue.TaggedOutput(\"bad\", element)\n                return\n            yield obj\n        except Exception:\n            yield pvalue.TaggedOutput(\"bad\", element)\n```\n\n## Follow-up Questions\n- How would you test this in CI/CD?\n- How would you monitor quality trends and alert on drift?","diagram":"flowchart TD\n  A[Pub/Sub] --> B[Dataflow (Beam)]\n  B --> C[BigQuery Staging]\n  B --> D[Quarantine]\n  C --> E[BigQuery Production]\n  F[Data Catalog Tags] --> C\n  F --> E","difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Goldman Sachs","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T10:49:14.020Z","createdAt":"2026-01-14T10:49:14.020Z"},{"id":"q-931","question":"A GCP pipeline ingests 1 TB of JSON user activity daily from Pub/Sub into BigQuery via Dataflow. New fields appear over time; you must evolve the schema without downtime. What approach would you use for schema drift and nested fields, and outline concrete steps to implement it?","answer":"Use a two-layer scheme: a stable BigQuery table with known fields and a separate extra_json column to capture drift. The Dataflow job maps incoming JSON to the known schema; unknown fields are stored ","explanation":"## Why This Is Asked\nThis question probes practical handling of evolving data in a streaming pipeline, balancing query stability with schema growth.\n\n## Key Concepts\n- Schema evolution in BigQuery\n- Semi-structured data (JSON)\n- Dataflow transforms for parsing\n- Backward compatibility\n- Metadata and cataloging\n\n## Code Example\n```javascript\nfunction parseEvent(json) {\n  const data = JSON.parse(json);\n  const known = {\n    userId: data.user_id || null,\n    eventTime: data.event_time || null,\n    country: data.country || null\n  };\n  const extraKeys = Object.keys(data).filter(k => !(k in known));\n  const extra = JSON.stringify(extraKeys.reduce((acc, k) => { acc[k] = data[k]; return acc; }, {}));\n  return { ...known, extra_json: extra };\n}\n```\n\n## Follow-up Questions\n- How would you migrate existing data to the new table version with minimal downtime?\n- How would you monitor for schema drift and alert when new fields appear?","diagram":null,"difficulty":"beginner","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Discord","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T15:41:21.360Z","createdAt":"2026-01-12T15:41:21.361Z"},{"id":"q-962","question":"In a GCP streaming pipeline, Pub/Sub feeds millions of events into BigQuery via Dataflow. Out-of-order arrivals and duplicates occur. Design an idempotent sink using a staging table and a final partitioned table, leveraging insertId for dedup and a MERGE strategy to upsert into the final table. Outline concrete steps and trade-offs to implement?","answer":"Use a staging BigQuery table where Dataflow writes with insertId derived from a stable key (user_id + event_id + event_time). Periodically MERGE from staging into a final partitioned table, deduplicat","explanation":"## Why This Is Asked\n\nTests practical data quality, idempotent sinks, and operational design using Dataflow + BigQuery features.\n\n## Key Concepts\n\n- Idempotent sink\n- insertId dedup\n- staging vs final table\n- MERGE in BigQuery\n- allowed lateness and partitioning\n- quotas and monitoring\n\n## Code Example\n\n```sql\nMERGE INTO dataset.final AS F\nUSING dataset.staging AS S\nON F.user_id = S.user_id AND F.event_time = S.event_time\nWHEN NOT MATCHED THEN INSERT (user_id, event_time, event_id, action, payload)\nWHEN MATCHED THEN UPDATE SET action = S.action, payload = S.payload;\n```\n\n## Follow-up Questions\n\n- How to handle tombstones?\n- How to scale MERGE with sharded partitions?\n- How would you instrument observability for dedup latency?","diagram":"flowchart TD\n  PubSub[Pub/Sub] --> Dataflow[Dataflow]\n  Dataflow --> Staging[StagingBQ]\n  Staging --> Final[FinalBQ]\n  Staging -->|MERGE| Final","difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Google","Instacart"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T17:23:31.180Z","createdAt":"2026-01-12T17:23:31.180Z"}],"subChannels":["general"],"companies":["Amazon","Apple","Bloomberg","Citadel","Cloudflare","Coinbase","Databricks","Discord","Goldman Sachs","Google","Hashicorp","Hugging Face","IBM","Instacart","Lyft","Microsoft","MongoDB","NVIDIA","Oracle","PayPal","Plaid","Robinhood","Salesforce","Scale Ai","Snap","Snowflake","Square","Tesla","Uber"],"stats":{"total":15,"beginner":7,"intermediate":6,"advanced":2,"newThisWeek":15}}