{"questions":[{"id":"gcp-data-engineer-design-data-systems-1768195992962-0","question":"Your real-time analytics pipeline ingests events from Pub/Sub, processes them with a Dataflow streaming job, and writes per-event metrics to BigQuery for near real-time dashboards. You must guarantee exactly-once processing to avoid duplicate metrics due to retries. Which approach best achieves this?","answer":"[{\"id\":\"a\",\"text\":\"Use Dataflow with Pub/Sub and write to BigQuery using streaming inserts without any deduplication handling\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use Dataflow streaming with an insertId per row derived from the event's unique id to enable idempotent writes in BigQuery\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Use Pub/Sub with a Cloud Function that writes to BigQuery with insertId; duplicates may occur\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use a batch pipeline that reads Pub/Sub via pull subscriptions and writes to BigQuery in batch mode\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption b is correct because deriving a unique insertId for each event ensures idempotent streaming writes to BigQuery. If the Dataflow worker retries a failed write, BigQuery uses the insertId to deduplicate and avoid duplicates. The other options either rely on non-idempotent streaming writes (a), rely on a non-deduplicating path (c), or abandon streaming guarantees by using batch processing (d).\n\n## Why Other Options Are Wrong\n- Option A: Streaming inserts without deduplication cannot guarantee exactly-once semantics when retries occur.\n- Option C: Cloud Functions can produce duplicates on retries unless you implement idempotency manually, which is error-prone at scale.\n- Option D: Batch processing cannot provide real-time or near-real-time semantics and misses the exactly-once guarantees in a streaming context.\n\n## Key Concepts\n- Exactly-once semantics in streaming pipelines\n- Idempotent writes with insertId for BigQuery streaming inserts\n- Dataflow and Pub/Sub integration for real-time analytics\n\n## Real-World Application\n- Used in production to ensure metrics dashboards reflect unique events even under retry scenarios, preventing inflated counts or duplicates.","diagram":null,"difficulty":"intermediate","tags":["GCP","BigQuery","Dataflow","Pub/Sub","AWS","Kubernetes","Terraform","certification-mcq","domain-weight-22"],"channel":"gcp-data-engineer","subChannel":"design-data-systems","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T05:33:12.964Z","createdAt":"2026-01-12 05:33:13"},{"id":"gcp-data-engineer-design-data-systems-1768195992962-1","question":"A Dataflow job consumes clickstream data from Pub/Sub and computes per-minute aggregates. Some events arrive late by up to 20 minutes. To include late data without delaying current window results, which pattern should you implement?","answer":"[{\"id\":\"a\",\"text\":\"Enable allowed lateness of 20 minutes and use appropriate triggers to emit results after window end\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Switch to a batch job that reprocesses the previous day’s data every night\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Discard late data to maintain the lowest latency\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Run a separate corrective streaming job that reprocesses the late data after it arrives\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption a is correct because allowing lateness (e.g., 20 minutes) and configuring triggers (such as AfterWatermark or AfterProcessingTime) enables the pipeline to include late events in the appropriate windows without waiting for the next day. \n\n## Why Other Options Are Wrong\n- Option B: Batch reprocessing introduces higher latency and does not handle real-time late data in current windows.\n- Option C: Dropping late data sacrifices data completeness and analytics accuracy.\n- Option D: A separate corrective job adds complexity and potential drift; handling lateness within the streaming window is more robust.\n\n## Key Concepts\n- Event-time processing in streaming pipelines\n- Allowed lateness and windowing triggers in Beam/Dataflow\n- Balancing latency and data completeness\n\n## Real-World Application\n- Enables accurate near-real-time dashboards even when some events arrive late due to network or device issues.","diagram":null,"difficulty":"intermediate","tags":["GCP","Dataflow","Pub/Sub","BigQuery","AWS","Kubernetes","Terraform","certification-mcq","domain-weight-22"],"channel":"gcp-data-engineer","subChannel":"design-data-systems","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T05:33:13.472Z","createdAt":"2026-01-12 05:33:13"},{"id":"gcp-data-engineer-design-data-systems-1768195992962-2","question":"IoT telemetry daily volume reaches around 10 TB/day. You want long-term storage that keeps costs reasonable and enables fast ad-hoc analytics. Which BigQuery table design best supports time-series analysis at scale?","answer":"[{\"id\":\"a\",\"text\":\"Store in a single wide, non-partitioned BigQuery table\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Partition the table by ingestion_date and cluster on device_id, ideally using ingestion-time partitioning\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Store as a nested JSON string in a single column and parse at query time\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use Cloud Bigtable for analytics instead of BigQuery\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption b is correct because partitioning by date and clustering on device_id improves query performance and cost efficiency for high-volume time-series analytics. Ingestion-time partitioning helps automate partitioning for streaming loads. \n\n## Why Other Options Are Wrong\n- Option A: Non-partitioned tables incur higher scan costs and slower queries on large datasets.\n- Option C: Storing JSON strings requires parsing at query time, increasing latency and cost; nested/structured columns in BigQuery are preferred.\n- Option D: Cloud Bigtable is optimized for wide-column NoSQL workloads, not ad-hoc analytics; BigQuery is typically better for analytics workloads at scale.\n\n## Key Concepts\n- Time-series data modeling in BigQuery\n- Ingestion-time partitioning and clustering for performance/cost\n- Analytical workloads vs NoSQL storage trade-offs\n\n## Real-World Application\n- Enables fast, cost-effective analytics on high-volume IoT telemetry with scalable querying and efficient data management.","diagram":null,"difficulty":"intermediate","tags":["GCP","BigQuery","Dataflow","Pub/Sub","AWS","Kubernetes","Terraform","certification-mcq","domain-weight-22"],"channel":"gcp-data-engineer","subChannel":"design-data-systems","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T05:33:13.829Z","createdAt":"2026-01-12 05:33:13"},{"id":"gcp-data-engineer-ingest-process-1768158927214-0","question":"Which architecture best achieves reliable, deduplicated, near-real-time writes from Pub/Sub to BigQuery while tolerating late data?","answer":"[{\"id\":\"a\",\"text\":\"Ingest events to Cloud Storage and perform batch loads from Cloud Storage to BigQuery every 5 minutes.\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Publish events to Pub/Sub, process with Dataflow using per-key windows and insertId-based dedup, then write to BigQuery via streaming inserts.\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Publish events directly to BigQuery streaming inserts from client applications.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use Cloud Functions to buffer events in memory and periodically write to BigQuery.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct because Pub/Sub provides real-time ingestion, Dataflow enables per-key windowing and deduplication, and BigQuery streaming inserts support near-real-time writes alongside insertId-based dedup to minimize duplicates.\n\n## Why Other Options Are Wrong\n- Option A trades real-time visibility for batch latency and adds buffering complexity, delaying insight.\n- Option C cannot guarantee idempotent writes and raises risks of duplicates due to retries.\n- Option D relies on in-memory buffering which is unsafe for reliability and scaling and can lead to data loss on restarts.\n\n## Key Concepts\n- Streaming ingestion with Pub/Sub\n- Dataflow windowing and per-key processing\n- Deduplication with insertId in BigQuery streaming inserts\n- Handling late data via watermarking and windowing\n\n## Real-World Application\nUsed for real-time dashboards and anomaly detection where timely, deduplicated insights are critical.\n","diagram":null,"difficulty":"intermediate","tags":["GCP","Dataflow","Pub/Sub","BigQuery","Terraform","Kubernetes","AWS-S3","AWS-Lambda","AWS-Kinesis","certification-mcq","domain-weight-25"],"channel":"gcp-data-engineer","subChannel":"ingest-process","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T19:15:27.215Z","createdAt":"2026-01-11 19:15:27"},{"id":"gcp-data-engineer-ingest-process-1768158927214-1","question":"You're ingesting JSON sensors data with evolving schema into BigQuery. Which approach minimizes downtime and automatically updates the BigQuery table schema as new fields appear?","answer":"[{\"id\":\"a\",\"text\":\"Use BigQuery load jobs with auto-detect on every file and replace the table.\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Implement a Dataflow pipeline with BigQueryIO and enable schema update options to ALLOW_FIELD_ADDITION.\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Use a scheduled Cloud Storage-to-BigQuery Transfer with auto-detect.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Write to a new BigQuery table each day and merge manually.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct because Dataflow's BigQueryIO supports dynamic schema updates (e.g., ALLOW_FIELD_ADDITION), enabling automatic schema evolution as new fields appear in incoming JSON data, with minimal downtime.\n\n## Why Other Options Are Wrong\n- Option A can lead to table recreation or downtime and does not reliably handle ongoing schema evolution in streaming contexts.\n- Option C does not provide automated, reliable schema evolution for evolving JSON data in a streaming/batch mix.\n- Option D introduces unnecessary complexity and potential data fragmentation; merging would require custom tooling.\n\n## Key Concepts\n- BigQueryIO schema update options (ALLOW_FIELD_ADDITION)\n- Dynamic schema evolution in streaming pipelines\n- Dataflow as a flexible ingestion path for evolving schemas\n\n## Real-World Application\nCommon when IoT/wearable sensors introduce new fields over time, needing seamless table evolution without downtime.\n","diagram":null,"difficulty":"intermediate","tags":["GCP","Dataflow","BigQuery","Terraform","Kubernetes","AWS-S3","AWS-Lambda","AWS-Kinesis","certification-mcq","domain-weight-25"],"channel":"gcp-data-engineer","subChannel":"ingest-process","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T19:15:27.757Z","createdAt":"2026-01-11 19:15:28"},{"id":"gcp-data-engineer-ingest-process-1768158927214-2","question":"You need per-user event counts in 15-minute windows, with events potentially arriving late up to 10 minutes. Which Dataflow windowing/triggers setup is appropriate?","answer":"[{\"id\":\"a\",\"text\":\"Use 15-minute fixed windows with no lateness.\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use 15-minute fixed windows with allowed lateness 10 minutes and default trigger.\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Use 15-minute sliding windows with a 10-minute lag.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use a global window with incremental accumulation.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct because 15-minute fixed windows with allowed lateness of 10 minutes ensure late events are included up to the stated tolerance, while the default trigger finalizes the window after the lateness period.\n\n## Why Other Options Are Wrong\n- Option A drops late data, reducing correctness and insight.\n- Option C uses sliding windows which would create overlapping windows and complexity without necessity for per-15-minute aggregation.\n- Option D with a global window prevents per-interval aggregation and late data handling.\n\n## Key Concepts\n- Windowing and fixed windows\n- Allowed lateness and watermark behavior\n- Triggers and late data handling\n\n## Real-World Application\nUseful for dashboards showing per-user activity over recent intervals where data can arrive late due to network delays.\n","diagram":null,"difficulty":"intermediate","tags":["GCP","Dataflow","Pub/Sub","BigQuery","Terraform","Kubernetes","AWS-S3","AWS-Lambda","AWS-Kinesis","certification-mcq","domain-weight-25"],"channel":"gcp-data-engineer","subChannel":"ingest-process","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T19:15:28.234Z","createdAt":"2026-01-11 19:15:28"},{"id":"gcp-data-engineer-store-data-1768224509666-0","question":"A data lake stores daily Parquet logs in Cloud Storage and you want to query the most recent 24 months in BigQuery while keeping storage costs low. Which lifecycle strategy best balances cost and access?","answer":"[{\"id\":\"a\",\"text\":\"Keep all data in Standard storage with no lifecycle rules\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Move data older than 60 days to Nearline and data older than 365 days to Archive\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Move all data older than 30 days to Coldline\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Delete data after 24 months\",\"isCorrect\":false}]","explanation":"## Correct Answer\n- Option B is correct: use lifecycle rules to migrate data to cheaper storage as it ages, e.g., move data older than 60 days to Nearline and data older than 365 days to Archive, while keeping the most recent data in Standard for fast access.\n\n## Why Other Options Are Wrong\n- Option A keeps all data in Standard, increasing cost for rarely accessed older data.\n- Option C moves data to Coldline too aggressively for data that is still frequently accessed or queried, increasing retrieval costs.\n- Option D deletes data after 24 months, which would violate a 24-month retention requirement.\n\n## Key Concepts\n- Cloud Storage storage classes (Standard, Nearline, Coldline, Archive)\n- Lifecycle management to transition data across classes\n- Cost vs. access trade-offs in long-term storage\n\n## Real-World Application\n- Implement lifecycle rules in the Cloud Console or via gsutil to automatically move data based on age, ensuring last 24 months remain readily accessible while older data is cost-optimized.","diagram":null,"difficulty":"intermediate","tags":["GCP","CloudStorage","BigQuery","DataLifecycle","Terraform","certification-mcq","domain-weight-20"],"channel":"gcp-data-engineer","subChannel":"store-data","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:28:29.668Z","createdAt":"2026-01-12 13:28:30"},{"id":"gcp-data-engineer-store-data-1768224509666-1","question":"You have clickstream data with fields event_date (YYYY-MM-DD), country, user_id, and page. You load daily batches into a BigQuery table. You frequently query by a date range of the last 30 days and by country. Which table design yields the best cost to performance ratio?","answer":"[{\"id\":\"a\",\"text\":\"Partition the table by event_date only\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Partition the table by event_date and cluster by country\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Cluster the table by country only\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Partition by ingestion_time only\",\"isCorrect\":false}]","explanation":"## Correct Answer\n- Option B is correct: partition by event_date enables partition pruning for the 30-day window, and clustering by country localizes data for country-filtered queries, reducing scanned data and cost.\n\n## Why Other Options Are Wrong\n- Option A lacks clustering, so country filters may scan more data than necessary.\n- Option C lacks date-based partition pruning, leading to larger scanned ranges for date filters.\n- Option D (ingestion_time partitioning) does not align with business date filters and provides less effective pruning when querying by event_date.\n\n## Key Concepts\n- Partitioning by date for range queries\n- Clustering by frequently filtered fields (country)\n- Partition pruning reduces data scanned and cost\n\n## Real-World Application\n- Implement a BigQuery table with PARTITION BY event_date and CLUSTER BY country, then write queries with WHERE event_date BETWEEN ... AND ... AND country = ...","diagram":null,"difficulty":"intermediate","tags":["GCP","BigQuery","Partitioning","Clustering","DataModeling","certification-mcq","domain-weight-20"],"channel":"gcp-data-engineer","subChannel":"store-data","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:28:30.189Z","createdAt":"2026-01-12 13:28:30"},{"id":"gcp-data-engineer-store-data-1768224509666-2","question":"For regulatory compliance, you need to ensure that objects stored in a Cloud Storage bucket are immutable for a defined retention period. Which feature provides this capability?","answer":"[{\"id\":\"a\",\"text\":\"Enable object versioning\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Enable bucket-level lifecycle to delete after a retention period\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Enable Object Lock retention policies\",\"isCorrect\":true},{\"id\":\"d\",\"text\":\"Use signed URLs with long expiration\",\"isCorrect\":false}]","explanation":"## Correct Answer\n- Option C is correct: Object Lock retention policies enforce immutability for the objects for the specified retention period, satisfying regulatory retention requirements.\n\n## Why Other Options Are Wrong\n- Option A makes previous versions preservable but does not guarantee immutability of the current object during the retention window.\n- Option B can delete objects after a retention period but does not guarantee immutability during the period.\n- Option D does not address immutability or retention; it affects access control.\n\n## Key Concepts\n- Cloud Storage Object Lock\n- Retention policies for compliance\n- Immutable data guarantees\n\n## Real-World Application\n- Apply a retention policy to the bucket and enable Object Lock, set the retention period, and monitor to ensure compliance.","diagram":null,"difficulty":"intermediate","tags":["GCP","CloudStorage","Compliance","Terraform","certification-mcq","domain-weight-20"],"channel":"gcp-data-engineer","subChannel":"store-data","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:28:30.708Z","createdAt":"2026-01-12 13:28:30"},{"id":"gcp-data-engineer-store-data-1768224509666-3","question":"You manage a dataset containing binary assets (images/videos) and a separate metadata table in BigQuery for analytics. Which architecture provides efficient storage for assets and fast SQL querying of metadata together with scalable access to the assets?","answer":"[{\"id\":\"a\",\"text\":\"Store assets in Cloud Storage and metadata in BigQuery\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Store assets in BigQuery and metadata in Cloud Storage\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Store both assets and metadata in BigQuery\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Store assets in Filestore and metadata in Spanner\",\"isCorrect\":false}]","explanation":"## Correct Answer\n- Option A is correct: Cloud Storage is optimized for large binary objects, while BigQuery stores structured metadata and supports SQL analytics, enabling efficient joins and analytics on metadata while serving the assets from object storage.\n\n## Why Other Options Are Wrong\n- Option B reverses the roles, misusing BigQuery for binary storage which is not cost-effective.\n- Option C duplicates binary data in BigQuery, which is expensive and unnecessary.\n- Option D uses a mix of services not aligned with common analytics patterns for binary assets and metadata.\n\n## Key Concepts\n- Separation of binary assets (Cloud Storage) vs structured data (BigQuery)\n- Efficient analytics via SQL on metadata\n- Scalable asset access via Cloud Storage integration with metadata store\n\n## Real-World Application\n- Implement a metadata catalog in BigQuery referencing Cloud Storage object URIs for asset retrieval in downstream apps.","diagram":null,"difficulty":"intermediate","tags":["GCP","CloudStorage","BigQuery","DataCatalog","Terraform","certification-mcq","domain-weight-20"],"channel":"gcp-data-engineer","subChannel":"store-data","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:28:30.888Z","createdAt":"2026-01-12 13:28:30"},{"id":"gcp-data-engineer-store-data-1768224509666-4","question":"You stream events from multiple sources into Pub/Sub and want to store them into BigQuery with strong deduplication guarantees. Which approach best ensures idempotent writes into BigQuery?","answer":"[{\"id\":\"a\",\"text\":\"Rely on Pub/Sub exactly-once delivery and use BigQuery streaming inserts with insertId\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use Pub/Sub with a dead-letter queue and ignore duplicates in BigQuery\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Batch load from GCS every hour and MERGE into BigQuery\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use Dataflow with a deduplication step and MERGE into BigQuery\",\"isCorrect\":true}]","explanation":"## Correct Answer\n- Option D is correct: a Dataflow pipeline can implement a deduplication step, producing upserts into BigQuery (MERGE or equivalent) based on a stable unique key, ensuring idempotent writes in a streaming context.\n\n## Why Other Options Are Wrong\n- Option A relies on Pub/Sub delivery guarantees and BigQuery insertId deduplication, which may not be sufficient in all failure scenarios and is more brittle than a controlled pipeline.\n- Option B’s dead-letter approach does not prevent duplicates in the main sink.\n- Option C introduces latency and potential windowing complexities; it is not as robust for real-time deduplication as a streaming Dataflow solution.\n\n## Key Concepts\n- Idempotent ingestion patterns\n- Dataflow pipelines with MERGE into BigQuery\n- Deduplication keys and upserts\n\n## Real-World Application\n- Implement a streaming Dataflow job that reads from Pub/Sub, applies a unique-id-based deduplication, and writes to BigQuery using MERGE semantics to avoid duplicates.","diagram":null,"difficulty":"intermediate","tags":["GCP","Pub/Sub","Dataflow","BigQuery","ETL","certification-mcq","domain-weight-20"],"channel":"gcp-data-engineer","subChannel":"store-data","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:28:31.067Z","createdAt":"2026-01-12 13:28:31"}],"subChannels":["design-data-systems","ingest-process","store-data"],"companies":[],"stats":{"total":11,"beginner":0,"intermediate":11,"advanced":0,"newThisWeek":11}}