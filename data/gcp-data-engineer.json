{"questions":[{"id":"q-1222","question":"Design a real-time ad-click analytics pipeline in GCP that ingests Pub/Sub events via Dataflow into BigQuery, while implementing 90-day TTL on PII data, exporting audits to Cloud Storage, and handling schema evolution, late data, dedup, and rollback. Provide concrete architecture, data models, and operational steps?","answer":"Two-tier streaming: a BigQuery hot table partitioned by ingestion_date with 90-day partition expiration for PII, plus a daily Cloud Storage Parquet archive for audits. Dataflow templates ingest Pub/Su","explanation":"## Why This Is Asked\nEvaluates real-world trade-offs in latency, cost, and governance for streaming pipelines on GCP.\n\n## Key Concepts\n- Pub/Sub -> Dataflow streaming\n- BigQuery partition expiration (90 days)\n- Parquet archive in Cloud Storage\n- Schema evolution with nested fields\n- Deduplication and late data handling\n- Rollback/versioned templates\n\n## Code Example\n```javascript\n// Pseudo Beam skeleton: dedupe by event_id and write to two sinks\n```\n\n## Follow-up Questions\n- How to validate TTL and archiving with tests?\n- What monitoring and alerting would you add for duty-cycle failures?","diagram":"flowchart TD\n  PubSub[Pub/Sub] --> Dataflow[Dataflow Template]\n  Dataflow --> BigQueryHot[BigQuery: Ingest (90d TTL)]\n  Dataflow --> Archive[Cloud Storage: Parquet Archive]\n  BigQueryHot --> Monitoring[Monitoring & Audits]","difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","NVIDIA","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T05:33:44.453Z","createdAt":"2026-01-13T05:33:44.453Z"},{"id":"q-1310","question":"You operate a streaming ingestion pipeline: Pub/Sub -> Dataflow -> BigQuery. Daily 50k events with fields user_id, event_type, amount (which can be string). Propose a simple ingestion-time data quality plan that validates JSON schema, coerces types, and routes invalid records to a dead-letter Pub/Sub topic without stopping the pipeline. Include testing with a small sample and how you monitor invalid counts?","answer":"Parse Pub/Sub messages with a simple Beam DoFn that validates required fields (user_id, event_type) and coerces amount to float. Valid records go to BigQuery; invalid ones emit to a dead-letter Pub/Su","explanation":"## Why This Is Asked\n\nAssess data quality at ingestion and the practicality of dead-letter routing in a real pipeline.\n\n## Key Concepts\n\n- Data quality at ingestion\n- Side outputs / dead-letter routing\n- Type coercion and validation\n- Beam/Dataflow to BigQuery integration\n- Monitoring\n\n## Code Example\n\n```javascript\n// Pseudo Beam DoFn for ingestion validation\nfunction processElement(element) {\n  let obj;\n  try { obj = JSON.parse(element); } catch (e) { return {error: 'invalid_json'}; }\n  if (!obj.user_id || !obj.event_type) return {error: 'missing_fields'};\n  const amount = parseFloat(obj.amount);\n  if (Number.isNaN(amount)) return {error: 'invalid_amount'};\n  return {user_id: obj.user_id, event_type: obj.event_type, amount: amount};\n}\n```\n\n## Follow-up Questions\n\n- How would you test dead-letter routing under load?\n- How would you tune retry/backoff for DLQ messages?\n","diagram":null,"difficulty":"beginner","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","PayPal","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T10:36:08.797Z","createdAt":"2026-01-13T10:36:08.797Z"},{"id":"q-1319","question":"Design a compliant streaming data platform in GCP for a fintech app that ingests transaction events from Pub/Sub through Dataflow into BigQuery. New fields may appear over time; PII must be detected and masked before analytics, while raw data is retained with restricted access. Outline the end-to-end architecture, data models, schema evolution strategy, PII handling, lineage, retention, and operational monitoring with concrete steps and trade-offs?","answer":"Implement a streaming Dataflow pipeline from Pub/Sub to a partitioned, clustered BigQuery table with schema registry in Data Catalog for forward/backward compatibility. Validate records, redact PII vi","explanation":"## Why This Is Asked\nReal-world streaming data governance, including schema evolution, PII handling, and lineage, is critical in fintech.\n\n## Key Concepts\n- Streaming ingestion with Pub/Sub and Dataflow\n- BigQuery partitioning and clustering\n- Schema registry and compatibility management\n- PII detection and masking (DLP)\n- Data lineage and metadata (Data Catalog)\n- Retention TTLs and access controls\n- Monitoring and alerting (Cloud Monitoring)\n\n## Code Example\n```python\ndef transform(record, registry_schema):\n    if not validate(record, registry_schema):\n        raise ValueError('Schema mismatch')\n    pii = ['acct_number', 'ssn']\n    masked = mask_pii(record, pii)\n    return masked\n```\n\n## Follow-up Questions\n- How would you test end-to-end TTL retention and schema evolution without downtime?\n- How would you handle cross-producer schema drift and multiple versions simultaneously?","diagram":"flowchart TD\n  A[Pub/Sub] --> B[Dataflow]\n  B --> C[BigQuery Table]\n  C --> D[Masked Analytics Views]\n  B --> E[Data Catalog (Lineage)]\n  C --> F[Raw Data with TTL Policy]","difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Plaid","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T11:31:53.811Z","createdAt":"2026-01-13T11:31:53.811Z"},{"id":"q-1389","question":"You receive streaming JSON events from Pub/Sub. Design a beginner-friendly GCP pipeline using Dataflow (Beam) to parse/flatten events, write to a partitioned BigQuery table (partition by event_date), and deduplicate by event_id. Late data allowed up to 6 hours. Outline architecture, data model, schema-change handling, and monitoring?","answer":"Design a Dataflow (Beam) streaming job that reads Pub/Sub JSON, flattens fields into a simple schema, writes to a partitioned BigQuery table by event_date, and deduplicates on event_id within a 5-minu","explanation":"## Why This Is Asked\n\nTests a practical, beginner-friendly data ingestion design on GCP, covering streaming, dedup, partitioning, late data handling, and basic schema evolution.\n\n## Key Concepts\n\n- Dataflow (Beam) streaming pipeline\n- Pub/Sub as the source\n- BigQuery partitioned tables and upserts\n- Deduplication by event_id\n- Lightweight schema-change and monitoring\n\n## Code Example\n\n```javascript\n// Pseudo-Beam-like snippet (conceptual)\nfunction parseEvent(raw) {\n  const obj = JSON.parse(raw);\n  return { event_id: obj.id, event_date: obj.date, payload: obj.payload };\n}\n```\n\n## Follow-up Questions\n\n- How would you test the dedup logic in Dataflow?\n- How would you monitor latency and late-data rates in this pipeline?","diagram":null,"difficulty":"beginner","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","Snap","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T14:50:08.738Z","createdAt":"2026-01-13T14:50:08.738Z"},{"id":"q-1415","question":"You are building a multi-source data platform where raw data arrives as Parquet in Google Cloud Storage, Avro via Pub/Sub, and JSON through a partner API; design a two-stage pipeline that normalizes all formats into a canonical Parquet dataset, ingested into a partitioned BigQuery table, with schema drift handling, data validation, and end-to-end lineage. Include how you would implement idempotent ingests, rollback, and cross-team access controls?","answer":"Implement a two-stage pipeline: stage1 normalizes Parquet(JSON/Avro) to a canonical Parquet schema, stage2 loads to a partitioned BigQuery table. Use a central schema registry and field-level nulls fo","explanation":"## Why This Is Asked\nDesign a multi-source ingestion pattern with schema drift handling, cross-format canonicalization, and governance; evaluate practical trade-offs.\n\n## Key Concepts\n- Multi-format normalization and canonical schema\n- Dataflow/Beam idempotent ingestion and exactly-once semantics\n- Schema evolution, drift handling, and validation\n- Data lineage, access controls, and rollback mechanisms\n\n## Code Example\n```javascript\nfunction toCanonical(rec){\n  // normalize fields, coerce types, assign stable id\n  return { id: rec.id ?? generateId(), ts: new Date(rec.ts), payload: rec.payload ?? null };\n}\n```\n\n## Follow-up Questions\n- How would you test schema drift and automate backwards-compatible changes?\n- How would you monitor latency and data skew across sources?","diagram":"flowchart TD\n  A[Ingest Parquet (GCS)] --> B[Normalize to Canonical Parquet]\n  C[Ingest Avro (Pub/Sub)] --> B\n  D[Ingest JSON (API)] --> B\n  B --> E[BigQuery (Partitioned)]\n  E --> F[Lineage & Auditing]","difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Snowflake","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T15:53:31.239Z","createdAt":"2026-01-13T15:53:31.239Z"},{"id":"q-1449","question":"Design a streaming enrichment pipeline: Pub/Sub → Dataflow → BigQuery, with an inline governance layer enforcing per-record rules (required fields, value ranges, cross-field consistency) at ingestion. Records failing rules go to a quarantine Cloud Storage bucket; good records load to a partitioned BigQuery table. Describe data model, rule engine approach, idempotence, backpressure, and testing/monitoring plan?","answer":"Implement a DoFn in Dataflow that validates each event against a schema (required fields, ranges, cross-field constraints). Use tagged outputs: main for valid records, quarantine for invalid. Write ma","explanation":"## Why This Is Asked\n\nAssesses ability to embed governance in streaming, handling bad data without blocking analytics, and operationalize quarantine and observability.\n\n## Key Concepts\n\n- Ingestion-time data quality checks\n- Dataflow ParDo with side outputs for quarantines\n- Idempotent BigQuery writes and record versioning\n- Backpressure and dead-letter handling\n- Observability and testing strategy\n\n## Code Example\n\n```python\nimport apache_beam as beam\n\nclass Govern(beam.DoFn):\n    def process(self, e):\n        if not e.get('user_id') or e.get('age', 0) < 0:\n            yield beam.pvalue.TaggedOutput('quarantine', e)\n        else:\n            yield e\n\nwith beam.Pipeline(...) as p:\n    raw = p | 'Read' >> beam.io.ReadFromPubSub(topic=...)  \n    main, bad = (raw | 'Govern' >> beam.ParDo(Govern()).with_outputs('quarantine', main='main'))\n    main | 'BQ' >> beam.io.WriteToBigQuery(table=..., schema=...)\n    bad | 'Quarantine' >> beam.io.WriteToText('gs://bucket/quarantine/')\n```\n\n## Follow-up Questions\n\n- How would you handle schema evolution in the governance layer without breaking downstream?\n- How to quantify and alert on data quality drift over time?\n- What tests would you add for end-to-end validation and rollback scenarios?","diagram":"flowchart TD\n  PubSub[Pub/Sub] --> Dataflow[Dataflow]\n  Dataflow --> BigQuery[BigQuery]\n  Dataflow --> Quarantine[GCS Quarantine]\n  Quarantine --> Audits[Audit Logs]\n  BigQuery --> Reports[BI Dashboards]","difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Instacart","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T17:45:34.896Z","createdAt":"2026-01-13T17:45:34.896Z"},{"id":"q-1473","question":"In a beginner friendly GCP data pipeline Pub/Sub -> Dataflow -> BigQuery design robust observability and recovery. Describe how to instrument Dataflow with metrics backlog throughput processed failed, implement a dead letter policy that dumps failed JSON messages to Cloud Storage, set retry backoff, and outline a replay path to re ing est from Cloud Storage. Include a concrete alert rule backlog throughput greater than 0.2 for 5 minutes and a brief example snippet?","answer":"Instrument Dataflow with metrics backlog, throughput, processed, and failed; emit structured logs to Cloud Logging. Route failed messages to a per topic Cloud Storage DLQ with 24h retention. Use expon","explanation":"## Why This Is Asked\nTests practical observability and recoverability for a streaming pipeline in fintech-like constraints, focusing on actionable metrics and a concrete DLQ/replay path.\n\n## Key Concepts\n- Dataflow metrics and structured logging\n- Dead-letter queues and DLQ retention\n- Retry/backoff strategies\n- Replay/re-ingestion pipelines and validation\n\n## Code Example\n```python\n# Pseudocode: DLQ handling in a Beam DoFn\nclass Process(DoFn):\n  def process(self, element):\n    try:\n      yield element\n    except Exception:\n      write_to_dlq(element)\n```\n\n## Follow-up Questions\n- How would you test the DLQ path with simulated failures?\n- What are the cost and complexity trade-offs of DLQ retention and replay?\n","diagram":"flowchart TD\n  A[Pub/Sub] --> B[Dataflow]\n  B --> C[BigQuery]\n  subgraph DLQ\n    D[Cloud Storage DLQ]\n  end\n  D --> E[Replay Job]","difficulty":"beginner","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","IBM"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T18:48:00.248Z","createdAt":"2026-01-13T18:48:00.248Z"},{"id":"q-1546","question":"In GCP, ingest daily Pub/Sub JSON events through Dataflow into BigQuery. Implement basic observability and reliability: 1) emit metrics for ingested, processed, and failed events; 2) export metrics to Cloud Monitoring and alert on failures for three consecutive checks; 3) deduplicate by event_id; 4) log audits to Cloud Storage. Outline architecture, data model, and concrete steps?","answer":"Implement a Dataflow (Apache Beam) pipeline that reads Pub/Sub JSON messages, parses them with a DoFn that emits metrics for ingested, processed, and failed events, then writes to BigQuery via streaming inserts with insertId set to event_id for deduplication. Export metrics to Cloud Monitoring and create alert policies for three consecutive failures. Log audit events to Cloud Storage for compliance tracking.","explanation":"## Why This Is Asked\nThis tests practical observability and data reliability skills in a real-world, beginner-friendly setting.\n\n## Key Concepts\n- Beam Metrics and Cloud Monitoring integration\n- Idempotent streaming writes with insertId\n- Lightweight auditing and error handling\n- Alerting on data quality metrics\n\n## Code Example\n```javascript\n// Pseudo-beam sketch (JavaScript/TypeScript syntax used for illustration)\nclass ParseEvent extends DoFn {\n  @ProcessElement\n  process(@Element json, output) {\n    const event = JSON.parse(json.image);\n    this.ingested.inc();\n    // ... validation and transformation\n    this.processed.inc();\n    output(event);\n  }\n  \n  @Setup\n  setup() {\n    this.ingested = Metrics.counter('pipeline', 'ingested_events');\n    this.processed = Metrics.counter('pipeline', 'processed_events');\n    this.failed = Metrics.counter('pipeline', 'failed_events');\n  }\n}\n```","diagram":"flowchart TD\n  PubSub[Pub/Sub] --> Dataflow[Dataflow / Beam]\n  Dataflow --> BigQuery[BigQuery]\n  Dataflow --> Monitor[Cloud Monitoring]\n  Dataflow --> Logs[Cloud Logging]\n  BigQuery --> Audit[Audit Logs (Cloud Storage)]","difficulty":"beginner","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Microsoft","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T06:31:05.145Z","createdAt":"2026-01-13T21:34:41.250Z"},{"id":"q-1581","question":"In Pub/Sub, daily JSON events contain PHI fields. Design a beginner-friendly GCP pipeline using Dataflow (Beam) to redact PHI with Cloud DLP before loading into BigQuery, and emit a separate audit log to Cloud Storage. Outline architecture, data model, and validation steps, including how you’d monitor masking failures?","answer":"Implement a Dataflow (Apache Beam) streaming job that reads JSON events from Pub/Sub, applies Cloud DLP masking to PHI fields (name, DOB, SSN), writes the redacted records to a BigQuery dataset, and emits audit logs to Cloud Storage. The pipeline includes error handling, dead-letter queues, and monitoring for masking failures.","explanation":"## Why This Is Asked\n\nTests practical data masking and data lake hygiene using familiar GCP tools. It ensures candidates can architect a compliant pipeline without overcomplicating a beginner task.\n\n## Key Concepts\n\n- Dataflow (Beam) streaming ingestion\n- Cloud DLP masking of PHI\n- BigQuery storage of redacted data\n- Cloud Storage audit logs and monitoring\n\n## Code Example\n\n```javascript\n// Pseudo Beam skeleton: read Pub/Sub, redact PHI with DLP, write to BigQuery, emit audit to GCS\n```\n\n## Follow-up Questions\n\n- How would you test masking coverage and false negatives in this pipeline?\n- What monitoring metrics would you track to ensure compliance?","diagram":null,"difficulty":"beginner","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T05:50:38.131Z","createdAt":"2026-01-13T22:48:46.197Z"},{"id":"q-1596","question":"Design a streaming pipeline: Pub/Sub → Dataflow (Beam) → BigQuery. Core: a dynamic data quality gate using a Firestore-stored policy engine that validates fields/types at ingest; invalid records serialized to Cloud Storage as JSONL with metadata; valid records load to BigQuery; publish quality metrics to Cloud Monitoring; handle drift/backpressure without downtime. Explain architecture, data model, and operational steps?","answer":"Design a streaming pipeline with Pub/Sub → Dataflow (Beam) → BigQuery, featuring a dynamic data quality gate using a Firestore-stored policy engine for field/type validation at ingest. Invalid records are serialized to Cloud Storage as JSONL with metadata, while valid records load to BigQuery. Quality metrics are published to Cloud Monitoring, and the system handles schema drift and backpressure without downtime through adaptive processing and buffering strategies.","explanation":"## Why This Is Asked\n\nTests ability to design a robust, observable streaming pipeline with runtime policy-based validation and safe failure handling at scale.\n\n## Key Concepts\n\n- Streaming ingestion with Pub/Sub and Dataflow (Beam)\n- Dynamic policy engine (Firestore) for field/type validation\n- Dead-letter/quarantine in Cloud Storage with metadata\n- Custom metrics in Cloud Monitoring for data quality\n- Schema drift resilience and backpressure handling\n\n## Code Example\n\n```python\n# Example: skeleton DoFn using Firestore policy to validate records\nclass ValidateWithPolicy(DoFn):\n    def setup(self):\n        self.firestore_client = firestore.Client()\n        self.policy_cache = {}\n    \n    def process(self, element):\n        # Load policy from Firestore with caching\n        policy = self.get_policy(element.get('schema_version'))\n        \n        # Validate fields/types against policy\n        if self.validate_record(element, policy):\n            yield element\n        else:\n            # Send invalid records to dead-letter sink\n            yield pvalue.TaggedOutput('invalid', {\n                'record': element,\n                'metadata': {\n                    'timestamp': datetime.utcnow().isoformat(),\n                    'validation_errors': self.get_validation_errors(element, policy)\n                }\n            })\n```","diagram":null,"difficulty":"advanced","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Hashicorp","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T05:34:10.860Z","createdAt":"2026-01-14T02:24:24.969Z"},{"id":"q-1722","question":"Design a cross-region, real-time analytics pipeline for a fintech on GCP. Ingest 100 GB/day of JSON events from Pub/Sub into BigQuery while ensuring per-tenant data isolation, a 90-day TTL on PII, and a rollback path. Show the architecture, data models, and operational steps for schema evolution, late data, dedup, and cost control?","answer":"Ingest via Pub/Sub -> Dataflow (Streaming) -> BigQuery (partitioned by day) with per-tenant views; store raw Parquet in Cloud Storage for audits; implement 90-day TTL by a scheduled Dataflow job that ","explanation":"## Why This Is Asked\nTests ability to design cross-region, scalable analytics with data governance, TTL, and schema evolution.\n\n## Key Concepts\n- Cross-region ingestion, storage\n- PII TTL and redaction\n- Tenant isolation with Authorized Views/IAM\n- Schema evolution and drift handling\n- Late data, dedup, and cost control\n\n## Code Example\n```javascript\n// Dataflow streaming pseudo-template\nconst events = pipeline\n  .apply('ReadPubSub', PubsubIO.readStrings().fromTopic('projects/..../topics/events'))\n  .apply('Parse', ParDo.of(new ParseFn()))\n  .apply('Deduplicate', /* ... */)\n  .apply('WriteBQ', BigQueryIO.writeTableRows().to('project.dataset.table$YYYYMMDD'))\n```\n\n## Follow-up Questions\n- How would you secure data in transit and at rest?\n- How would you monitor costs and adjust partitions?","diagram":null,"difficulty":"advanced","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Oracle","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T08:40:44.958Z","createdAt":"2026-01-14T08:40:44.959Z"},{"id":"q-1754","question":"Design a beginner-friendly GCP pipeline to load daily 2-5 GB of newline-delimited JSON logs from Cloud Storage into BigQuery. Keep only the latest 45 days in a partitioned table (partition by load_date, clustered by record_id). Ensure dedup by record_id, handle optional new fields with a permissive schema, allow late data up to 24 hours, and provide basic monitoring. Choose between Dataflow (Beam) or Cloud Functions, justify, and outline data model, transformations, error handling, and testing strategies?","answer":"Use Dataflow (Beam) in batch mode to read daily JSONL from Cloud Storage, parse and flatten, write to a staging table, then MERGE into a final partitioned table (load_date) clustered by record_id for ","explanation":"## Why This Is Asked\nThis question probes how a junior data engineer designs an end-to-end GCP pipeline for file-based ingestion, dedup, and schema evolution—a common real-world task that isn't covered by Pub/Sub-focused questions.\n\n## Key Concepts\n- Data ingestion from Cloud Storage using Dataflow (Beam) batch processing\n- BigQuery partitioning by load_date and clustering by record_id\n- Dedup via MERGE into final table; staging table usage\n- Permissive schema + schema evolution; late data handling (24h)\n- Monitoring: Cloud Monitoring dashboards and Logs-based metrics\n\n## Code Example\n```javascript\n// Example data map for a row\nfunction toRecord(line) {\n  const obj = JSON.parse(line);\n  return {\n    record_id: obj.id,\n    load_date: obj.date || new Date().toISOString().slice(0,10),\n    fieldA: obj.fieldA ?? null\n  };\n}\n```\n\n## Follow-up Questions\n- How would you test with synthetic JSON files and validate dedup?\n- How would you handle schema changes that remove fields?\n- What are cost trade-offs between staging vs final tables and clustering?","diagram":"flowchart TD\n  A[Cloud Storage JSONL] --> B[Dataflow Batch]\n  B --> C[Staging BigQuery]\n  C --> D[Final Table (load_date, cluster by record_id)]\n  D --> E[Monitoring & Alerts]","difficulty":"beginner","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","Lyft","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T09:41:21.218Z","createdAt":"2026-01-14T09:41:21.218Z"},{"id":"q-1787","question":"Design a data quality framework for a streaming pipeline ingesting 500 GB/day of JSON events from Pub/Sub into BigQuery. Specify in-flight schema validation, per-field checks (types, nullability, ranges), quarantining bad records, and automatic schema evolution with Data Catalog, plus end-to-end monitoring, alerting, and rollback procedures?","answer":"Leverage Dataflow (Beam) with a Python DoFn validating against an external Avro/JSON schema; route valid events to BigQuery staging and malformed ones to a quarantine table with error codes. Enforce per-field checks using Pydantic models for type validation, nullability constraints, and value ranges. Implement schema evolution through Data Catalog API calls to register new schema versions and tag datasets with version metadata. Use BigQuery streaming inserts with write disposition WRITE_APPEND and idempotent keys for deduplication. Monitor pipeline health via Cloud Monitoring metrics (validation success rate, quarantine volume, schema drift alerts) and set up alerting thresholds. For rollback, maintain schema version history in Data Catalog and implement a blue-green deployment strategy with pipeline rollback capability.","explanation":"## Why This Is Asked\nTests practical data quality controls in streaming pipelines on GCP, combining validation, governance, and recoverability.\n\n## Key Concepts\n- In-flight validation against a schema\n- Per-field checks: type, nullability, ranges\n- Quarantine/fallback handling for bad records\n- Data Catalog tagging for schema versions\n- Drift detection with BigQuery ML or SQL\n- Rollback and idempotent sinks\n\n## Code Example\n```python\n# Beam DoFn sketch for in-flight validation\nimport json\nfrom apache_beam import DoFn, pvalue\nfrom pydantic import BaseModel, ValidationError\n\nclass EventSchema(BaseModel):\n    user_id: str\n    timestamp: int\n    event_type: str\n    value: float = None\n\nclass ValidateRecordDoFn(DoFn):\n    def process(self, element, *args, **kwargs):\n        try:\n            obj = json.loads(element)\n            EventSchema(**obj)  # Pydantic validation\n            yield pvalue.TaggedOutput(\"valid\", element)\n        except (json.JSONDecodeError, ValidationError) as e:\n            yield pvalue.TaggedOutput(\"bad\", {\n                \"record\": element,\n                \"error\": str(e),\n                \"timestamp\": time.time()\n            })\n```","diagram":"flowchart TD\n  A[Pub/Sub] --> B[Dataflow (Beam)]\n  B --> C[BigQuery Staging]\n  B --> D[Quarantine]\n  C --> E[BigQuery Production]\n  F[Data Catalog Tags] --> C\n  F --> E","difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Goldman Sachs","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":["data quality framework","streaming pipeline","schema validation","per-field checks","quarantine table","schema evolution","data catalog","cloud monitoring","rollback procedures","bigquery streaming","dataflow beam","pydantic models"],"voiceSuitable":true,"isNew":true,"lastUpdated":"2026-01-20T05:40:18.426Z","createdAt":"2026-01-14T10:49:14.020Z"},{"id":"q-1834","question":"Design a cross-tenant data governance pipeline on GCP for 1 TB/day of JSON user activity ingested from Pub/Sub into BigQuery in two regions. Enforce per-tenant privacy with field-level masking, auto-generate end-to-end data lineage with Data Catalog, support backward-compatible schema evolution, and export immutable audit logs to Cloud Storage. Include architecture, data models, rollback plan per-tenant (within 24h), and testing strategy?","answer":"Ingest via Pub/Sub -> Dataflow -> BigQuery in two regions; apply per-tenant masking for PII with DLP before load; tag lineage in Data Catalog for each field and table, and surface per-tenant access vi","explanation":"## Why This Is Asked\nThis question probes architectural rigor in multi-tenant governance, data lineage, privacy controls, and rollback strategy at scale on GCP. It emphasizes secure isolation, schema evolution, and auditable data flows.\n\n## Key Concepts\n- Data governance and lineage with Data Catalog\n- Per-tenant privacy masking with DLP or equivalent\n- Cross-region BigQuery data sharing via Authorized Views\n- Schema evolution with table templates and backward compatibility\n- Immutable audit logging and rollback workflows\n\n## Code Example\n```javascript\n// Example pseudocode illustrating a Dataflow pipeline segment wiring Pub/Sub -> BigQuery\nconst pipeline = ...\n.pipe(PubSubSource())\n  .transform(maskPIIPerTenant)\n  .to(BigQuerySink({region: 'us-central1'}))\n```\n\n## Follow-up Questions\n- How would you test tenant isolation failures?\n- How would you handle late-arriving data affecting lineage labels?","diagram":"flowchart TD\n  A[Pub/Sub] --> B[Dataflow]\n  B --> C[BigQuery (Region 1)]\n  B --> D[BigQuery (Region 2)]\n  C --> E[Data Catalog Tags]\n  D --> E\n  C --> F[Audit Logs Cloud Storage]\n  D --> F","difficulty":"advanced","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T13:16:15.203Z","createdAt":"2026-01-14T13:16:15.203Z"},{"id":"q-1869","question":"Design a GCP streaming pipeline for 2 TB/day of multi-tenant JSON events from Pub/Sub into BigQuery. Create a schema registry in Cloud Storage, validate with a Beam DoFn, quarantine nonconforming records to an invalid dataset, and apply per-tenant DLP masking. Enforce row-level access with BigQuery policies, capture lineage in Data Catalog, and provide rollback and testing plan?","answer":"Design a GCP streaming pipeline for 2 TB/day of multi-tenant JSON events from Pub/Sub into BigQuery. Build a schema registry in Cloud Storage, validate with a Beam DoFn, quarantine nonconforming recor","explanation":"## Why This Is Asked\nProbes multi-tenant data integrity, privacy, and governance at scale, including schema drift handling, masking, and lineage.\n\n## Key Concepts\n- Schema registry in Cloud Storage and Drift detection in Dataflow/Beam\n- Row-level security with BigQuery policies for per-tenant isolation\n- Per-tenant DLP masking and quarantine of invalid data\n- Data Catalog lineage and end-to-end observability\n\n## Code Example\n```javascript\n// Pseudo-Beam DoFn skeleton for schema validation\nclass ValidateSchema extends DoFn {\n  processElement(context) {\n    const record = context.element;\n    // load canonical schema from GCS\n    // validate fields, types, requireds\n    if (valid(record)) {\n      context.output(record);\n    } else {\n      context.output(NOT_VALID, record);\n    }\n  }\n}\n```\n\n## Follow-up Questions\n- How would you test schema drift in production without impacting traffic?\n- How would you implement rollback across tenants if a schema change breaks downstream queries?","diagram":null,"difficulty":"advanced","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Snap","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T14:56:51.986Z","createdAt":"2026-01-14T14:56:51.986Z"},{"id":"q-1906","question":"Ingest a daily batch of JSON records stored in Cloud Storage into BigQuery via a Dataflow (Beam) job. Build a beginner-friendly pipeline that validates a minimal schema (tenant_id, event_id, event_ts), filters out records missing required fields, and deduplicates by (tenant_id, event_id). Load valid data into a partitioned BigQuery table; write invalid records to a separate GCS errors bucket. Include basic health counters and a simple template?","answer":"Batch Dataflow (Beam) reads daily JSON lines from Cloud Storage, validates required fields (tenant_id, event_id, event_ts), filters invalid records, and deduplicates by (tenant_id, event_id). Valid ro","explanation":"## Why This Is Asked\n- Tests ability to implement a reliable, beginner-friendly batch ingestion with data quality checks, idempotent deduplication, and observable health signals in GCP.\n\n## Key Concepts\n- Dataflow/Beam batch pipelines, JSON parsing, and error routing\n- Simple schema validation and field presence checks\n- Deduplication strategy for (tenant_id, event_id)\n- BigQuery partitioning and error data landfill\n- Basic monitoring with counters\n\n## Code Example\n```javascript\n// Java-like pseudocode for Beam; real syntax depends on the SDK (Python/Java)\nclass ValidateAndKey(DoFn):\n  def process(self, elem):\n    required = ['tenant_id','event_id','event_ts']\n    if all(k in elem and elem[k] is not None for k in required):\n      yield ((elem['tenant_id'], elem['event_id']), elem)\n    else:\n      yield ('invalid', elem)\n```\n\n## Follow-up Questions\n- How would you scale this to 10x data and ensure low tail latency?\n- How would you adapt for evolving event schemas and backward compatibility?","diagram":"flowchart TD\n  A[Cloud Storage: daily.json] --> B[Dataflow (Beam)]\n  B --> C{Records Valid?}\n  C -->|Yes| D[BigQuery: partitioned table]\n  C -->|No| E[Errors: GCS bucket]","difficulty":"beginner","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T16:51:30.533Z","createdAt":"2026-01-14T16:51:30.533Z"},{"id":"q-1953","question":"Design a GCP data pipeline for a multinational SaaS product that ingests 200 GB/day of JSON telemetry from Pub/Sub into BigQuery across two regions. Requirements: multi-tenant isolation, automatic schema evolution, per-tenant TTL, data quality checks (schema conformance, nulls, duplicates), and an automated rollback path for schema changes. Include data model sketches, partitioning strategy, testing, and rollback steps?","answer":"Use per-tenant datasets in BigQuery and two-region Dataflow pipelines with a unified Avro/JSON schema registry. Ingest with event_id for dedup, partition by ingestion_date, TTL per tenant (30/90 days)","explanation":"## Why This Is Asked\nAssesses ability to design a compliant, scalable, two-region data pipeline with strong data quality and schema governance, plus a robust rollback path.\n\n## Key Concepts\n- Multi-tenant isolation with per-tenant datasets\n- Two-region active-active ingestion\n- Schema registry and automatic evolution\n- TTL/retention controls per tenant\n- Data quality: schema conformance, null checks, dedup\n- Rollback strategy: versioned schemas and reprocessing window\n- Metadata and lineage via Data Catalog\n\n## Code Example\n```python\n# Pseudocode: Beam DoFn for basic validation and dedup tagging\nclass ValidateEventDoFn(beam.DoFn):\n    def process(self, element):\n        data = json.loads(element)\n        if not data.get('tenant_id') or not data.get('event_id'):\n            yield beam.pvalue.TaggedOutput('invalid', data)\n            return\n        # type checks and masking placeholders\n        yield data\n```\n\n## Follow-up Questions\n- How would you test schema evolution across regions without downtime?\n- What metrics and alerts would you set for quality/rollback events?","diagram":null,"difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Hashicorp","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T18:50:45.651Z","createdAt":"2026-01-14T18:50:45.651Z"},{"id":"q-1997","question":"Ingest daily 1 TB JSON exports dumped into Cloud Storage by partner apps. Design a beginner-friendly GCP batch pipeline to load into BigQuery with per-tenant isolation (datasets), ingestion-date partitioning, and a simple schema-evolution strategy (new fields added as nullable columns). Include data-quality checks (nulls, types) and a rollback path to revert a day’s ingest within 24h. Outline architecture and testing?","answer":"Ingest 1 TB/day JSON exports from Cloud Storage with a batch Dataflow job. Use per-tenant datasets and ingestion-date partitioned tables. Flatten/parse JSON and map new fields to nullable columns to s","explanation":"## Why This Is Asked\nThis question tests practical batch data ingestion on GCP, focusing on beginner-friendly patterns that still show depth around tenant isolation, partitioning, and recoverability.\n\n## Key Concepts\n- Batch processing with Dataflow or Beam\n- Tenant isolation via separate datasets and IAMs\n- BigQuery ingestion-date partitioning\n- Lightweight schema evolution (nullable new fields)\n- Basic data quality checks (nullability, type validation)\n- Rollback strategy within 24h\n\n## Code Example\n\n```javascript\n// Pseudo Dataflow skeleton (illustrative)\nimport apache_beam as beam\nfrom apache_beam.options.pipeline_options import PipelineOptions\n# ... omitted for brevity\n```\n\n## Follow-up Questions\n- How would you test this pipeline end-to-end with failing records?\n- How would you scale tenant isolation when tenants grow into thousands?","diagram":null,"difficulty":"beginner","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Meta","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T20:31:34.562Z","createdAt":"2026-01-14T20:31:34.562Z"},{"id":"q-2073","question":"On GCP, design an observability-driven pipeline that detects schema drift and data-quality anomalies in near real-time for 1 TB/day of JSON events ingested from Pub/Sub into BigQuery across two regions. Include per-tenant lineage, automatic schema evolution, drift-triggered alerts, a self-healing rollback path, and export of audit trails to Cloud Storage. Provide architecture, data models, tests, and operational playbooks?","answer":"Ingest via Pub/Sub to Dataflow (Apache Beam) deployed across two regions, writing to region-partitioned BigQuery tables with tenant isolation. Implement drift detection by comparing incoming event schemas against a canonical schema stored in Data Catalog; on drift detection, trigger automated alerts via Cloud Monitoring and initiate schema evolution workflows. Maintain per-tenant lineage through Data Catalog tags and export audit trails to Cloud Storage for compliance. Include self-healing rollback capabilities using BigQuery time travel and versioned schema definitions.","explanation":"## Why This Is Asked\nThis probes practical observability, schema governance, drift handling, and rollback in a multi-region data pipeline, integrating Dataflow, BigQuery, Data Catalog, and Cloud Storage audits.\n\n## Key Concepts\n- Streaming ingestion and schema drift detection\n- Data Catalog lineage and tenant isolation\n- Schema evolution and rollback mechanics\n- Alerting and self-healing strategies\n\n## Code Example\n```javascript\n// Pseudocode drift detector skeleton\nfunction detectDrift(record, catalog) {\n  // Compare incoming fields with catalog schema\n  return driftDetected;\n}\n```\n\n## Follow","diagram":null,"difficulty":"advanced","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Databricks","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T05:33:18.450Z","createdAt":"2026-01-14T23:26:10.805Z"},{"id":"q-2153","question":"Design a cost-aware multi-tenant GCP data pipeline for streaming telemetry that ingests 150 GB/day of JSON from Pub/Sub into BigQuery across two regions. Enforce per-tenant access with BigQuery row level security and per-tenant dataset versioning; support backward compatible schema evolution; and export versioned snapshots to Cloud Storage. Include data models, partitioning, testing, rollback, and synthetic-tenant generation?","answer":"Ingest via Dataflow from Pub/Sub to two regional BigQuery datasets. Use ingestion-time partitioned tables, cluster by tenant_id, and apply row-level security so each tenant sees only their data. Keep ","explanation":"## Why This Is Asked\nTests real-world multi-tenant data governance, per-tenant isolation, and cost-conscious architecture across regions. It also covers schema evolution, rollback, and data export for audits.\n\n## Key Concepts\n- BigQuery row-level security and per-tenant access\n- Ingestion-time partitioning and tenant clustering\n- Dataset/versioning strategy and backward-compatible schema evolution\n- Cross-region replication and cost controls\n- Synthetic tenant generation and rollback testing\n\n## Code Example\n```sql\n-- example RLAC for tenant isolation (conceptual)\nCREATE ROW ACCESS POLICY tenant_rlac\nON `project.dataset.table`\nUSING (tenant_id = SESSION_USER())\nWITH CHECK (tenant_id = SESSION_USER());\n```\n\n## Follow-up Questions\n- How would you handle schema drift across regions and tenants?\n- What monitoring would you add to detect cost spikes from multi-tenant queries?","diagram":"flowchart TD\n  A[Pub/Sub] --> B[Dataflow]\n  B --> C[BigQuery (Region 1, Tenant data)]\n  B --> D[BigQuery (Region 2, Tenant data)]\n  C --> E[RLAC policies per tenant]\n  D --> E\n  C --> F[Snapshots to Cloud Storage (versioned)]\n  D --> F","difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["NVIDIA","Oracle","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T05:35:12.126Z","createdAt":"2026-01-15T05:35:12.126Z"},{"id":"q-2227","question":"Design a real-time data ingestion pipeline on GCP for a game analytics platform: 60 GB/day of JSON events from Pub/Sub into BigQuery across two regions. Enforce per-tenant isolation with field-level masking, automatic schema evolution, and a per-tenant canary rollout with a controlled rollback path. Include data models, testing strategy, lineage via Data Catalog, and cost controls. Provide a concrete implementation plan?","answer":"Streaming pipeline: Pub/Sub → Dataflow → BigQuery across two regions with per-tenant isolation and field-level masking. Enable tenant-scoped schema evolution via versioned schemas in Data Catalog, plu","explanation":"## Why This Is Asked\nEvaluates real-time ingestion, tenant isolation, and controlled schema changes with governance across regions.\n\n## Key Concepts\n- Real-time streaming with Pub/Sub and Dataflow\n- Per-tenant masking and isolation\n- Canary deployments and rollback automation\n- Data Catalog lineage and versioned schemas\n\n## Code Example\n```javascript\n// Skeleton: Beam transform applying masking using a per-tenant function\nimport apache_beam as beam\nclass MaskPerTenant(beam.DoFn):\n  def process(self, element, *args, **kwargs):\n    # tenant_id = element['tenant_id']\n    # mask fields per tenant\n    return [element]\n```\n\n## Follow-up Questions\n- How would you implement automatic backfill for late data?\n- How would you monitor data quality and roll back a breaking change?","diagram":"flowchart TD\n  A[Pub/Sub: Events] --> B[Dataflow: Transform]\n  B --> C[BigQuery: Region 1]\n  B --> D[BigQuery: Region 2]\n  C --> E[Data Catalog: Lineage]\n  D --> E","difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Meta","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T08:43:19.512Z","createdAt":"2026-01-15T08:43:19.512Z"},{"id":"q-2364","question":"Design a cross-tenant data exchange on GCP: ingest 1 TB/day of JSON telemetry from Pub/Sub into two BigQuery regions. Implement per-tenant isolation via dataset-level access controls and masked views, and use Data Catalog policy tags to enforce visibility. Model a pragmatic two-layer architecture: raw + curated; ensure end-to-end lineage; support controlled data sharing via explicit contracts. Provide a 24h per-tenant rollback plan and testing strategy?","answer":"Route 1 TB/day via Dataflow to BigQuery in Region A and Region B. Implement per-tenant isolation with dataset-level access and masked views; enforce visibility with Data Catalog policy tags. Build raw","explanation":"## Why This Is Asked\nExplores data-contract-driven governance and cross-region sharing, a real-world need for large, multi-tenant analytics.\n\n## Key Concepts\n- Data contracts in Data Catalog\n- Per-tenant isolation via views/policies\n- End-to-end lineage\n- Two-region replication and cost-aware design\n- Rollback strategy with time travel and canaries\n\n## Code Example\n```python\n# pseudo-contract validator\ndef validate_contract(contract):\n  required_fields = {\"tenant_id\",\"schema\"}\n  return required_fields.issubset(contract.keys())\n```\n\n## Follow-up Questions\n- How would you test contract violations in CI/CD?\n- How would you handle schema evolution across tenants without breaking sharing contracts?","diagram":null,"difficulty":"advanced","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","NVIDIA","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T14:54:28.599Z","createdAt":"2026-01-15T14:54:28.599Z"},{"id":"q-2410","question":"Design an end-to-end GCP data ingestion pipeline for 1-2 TB/day of JSON telemetry streamed from Pub/Sub into BigQuery with per-tenant isolation and strict data residency policies. The solution must mask PII per tenant at ingestion (field-level), use CMEK, and employ DLP where needed. Provide two-region architecture, automatic schema evolution, late data handling, and a rollback/backfill plan, plus end-to-end data lineage in Data Catalog and audit exports. Include architecture, data model, testing, and runbook details?","answer":"Pub/Sub → Dataflow streaming; per-tenant isolation via separate datasets and partitioned tables; field-level masking at parse time driven by a central policy store; CMEK encryption on BigQuery; DLP fo","explanation":"## Why This Is Asked\nTests ability to design a privacy-aware, multi-region data pipeline with governance and testing in a realistic setting.\n\n## Key Concepts\n- Pub/Sub streaming\n- Multi-tenant masking and isolation\n- CMEK and DLP integration\n- Data Catalog lineage\n- Schema evolution and backfills\n- Two-region resilience and audits\n\n## Code Example\n```javascript\n// Dataflow DoFn: mask PII based on tenant policy\nclass MaskPII {\n  process(record, ctx) {\n    const policy = getPolicy(ctx.tenantId);\n    return maskRecord(record, policy);\n  }\n}\n```\n\n## Follow-up Questions\n- How would you test end-to-end lineage accuracy? \n- How would you handle policy updates without downtime?","diagram":null,"difficulty":"advanced","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Hugging Face","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T16:59:12.833Z","createdAt":"2026-01-15T16:59:12.833Z"},{"id":"q-2523","question":"On GCP, design a data mesh for 3 domains (Sales, Product, Ops) where each domain owns its BigQuery data products, publishes a standard schema, and uses Data Catalog and IAM for cross-domain governance. Include an automated lineage from source events to data products, a policy-driven access layer with per-domain permissions, and a testing/rollback plan for schema drift. Provide concrete data models and workflow?","answer":"Implement a data mesh architecture with domain-owned BigQuery datasets for Sales, Product, and Operations. Each domain manages its own data products using standardized schemas, while Data Catalog provides centralized metadata management with tag templates for automated lineage tracking. Establish per-domain IAM roles and Authorized Views to enable secure cross-domain data access. Deploy CI/CD pipelines with schema validation tests to detect and prevent drift, with automated rollback capabilities for schema changes.","explanation":"## Why This Is Asked\nThis evaluates practical implementation of data mesh principles, governance frameworks, and lineage automation on Google Cloud Platform.\n\n## Key Concepts\n- Domain-driven data ownership with BigQuery datasets\n- Data Catalog metadata management and automated lineage\n- IAM-based access control with Authorized Views\n- CI/CD pipeline integration for schema drift prevention\n- Cross-domain governance with policy-driven permissions\n\n## Code Example\n```javascript\nfunction validateSchema(expectedSchema, actualSchema) {\n  const expected = JSON.stringify(expectedSchema);\n  const actual = JSON.stringify(actualSchema);\n  return expected === actual;\n}\n```","diagram":"flowchart TD\n  A[Source Events] --> B[Dataflow/Beam] \n  B --> C[Domain Datasets] \n  C --> D[Data Catalog & Lineage] \n  D --> E[BI & ML Serving]","difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Netflix","Snowflake","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T05:46:15.342Z","createdAt":"2026-01-15T21:34:05.475Z"},{"id":"q-2602","question":"Design a multi-tenant, streaming data platform on GCP for a financial app that ingests 1 TB/day of JSON events via Pub/Sub, processes them with Apache Beam on Dataflow, and writes per-tenant data products to BigQuery across three regions. Explain per-tenant isolation, automatic schema evolution, late data handling, data quality checks, rollback paths, cross-region replication, Data Catalog lineage, and testing strategy?","answer":"Per-tenant isolation: distinct Pub/Sub topics and BigQuery datasets per tenant; Dataflow pipelines with exactly-once writes; centralized schema registry with Data Catalog for evolution; QC checks (sch","explanation":"## Why This Is Asked\nEvaluates real-world ability to design a scalable, multi-tenant streaming pipeline on GCP with strict data governance, schema evolution, cross-region durability, and robust testing.\n\n## Key Concepts\n- Multi-tenant isolation across Pub/Sub and BigQuery\n- Dataflow (Beam) streaming pipelines with exactly-once semantics\n- Centralized schema evolution using a registry and Data Catalog lineage\n- Data quality checks: schema conformance, dedup, nulls\n- Late data handling and watermarking\n- Cross-region replication and rollback/backfill strategies\n- Testing: end-to-end, drift, canaries\n\n## Code Example\n```python\nimport apache_beam as beam\nfrom apache_beam.options.pipeline_options import PipelineOptions\nimport json\n\nclass ParseJsonDoFn(beam.DoFn):\n    def process(self, element):\n        yield json.loads(element)\n\noptions = PipelineOptions(streaming=True)\nwith beam.Pipeline(options=options) as p:\n    (p\n     | 'ReadPubSub' >> beam.io.ReadFromPubSub(subscription='projects/PROJ/subscriptions/tenant-sub')\n     | 'Parse' >> beam.ParDo(ParseJsonDoFn())\n     | 'ToBQ' >> beam.io.WriteToBigQuery(\n         table='tenant_ds.tenant_table',\n         dataset='tenant_ds',\n         project='PROJECT',\n         write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND,\n         create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED))\n```\n\n## Follow-up Questions\n- How would you implement idempotent writes to BigQuery across regions?\n- What monitoring and alerting would you add for schema drift per tenant?","diagram":"flowchart TD\n  PubSub[Pub/Sub per-tenant topics]\n  Dataflow[Dataflow streaming pipelines]\n  BigQuery[BigQuery cross-region tables]\n  Catalog[Data Catalog lineage]\n  Access[Per-tenant IAM]\n  QC[Quality checks]\n  Rollback[Rollback plan]\n  \n  PubSub --> Dataflow\n  Dataflow --> BigQuery\n  BigQuery --> Catalog\n  Catalog --> Access\n  QC --> Rollback","difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Microsoft","Robinhood","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T02:32:08.185Z","createdAt":"2026-01-16T02:32:08.185Z"},{"id":"q-2744","question":"Design a multi-tenant data pipeline on GCP that ingests 50–100 GB/day of JSON events from Pub/Sub into BigQuery across two regions. Implement per-tenant isolation with dedicated datasets and topics, a robust data quality observability layer that validates schema conformance, nulls, duplicates, and timeliness, and real-time alerts. Include an automated remediation path (schema bumps and selective re-ingestion) and a clear rollback strategy using time travel. Provide architecture, data models, partitioning, testing, and rollback steps?","answer":"Per-tenant isolation via per-tenant BigQuery datasets and Pub/Sub topics, a Dataflow streaming pipeline to both regions, and a concurrent Data Quality pipeline that validates schema conformance, null ","explanation":"## Why This Is Asked\n\nAssess capability to design scalable, multi-region, multi-tenant data pipelines with robust data quality and rollback strategies on GCP.\n\n## Key Concepts\n\n- Per-tenant isolation (datasets, topics) and multi-region BigQuery\n- Streaming ingestion with Dataflow\n- Data quality observability: schema conformance, nulls, duplicates, timeliness\n- Real-time alerts via Cloud Monitoring\n- Automated remediation: schema bumps + re-ingestion\n- Rollback: point-in-time re-ingestion windows; BigQuery time travel\n\n## Code Example\n\n```javascript\n// Pseudo Dataflow DoFn for schema validation\nfunction validate(record, schema) {\n  // check required fields and types\n}\n```\n\n## Follow-up Questions\n\n- How would you test cross-region schema drift and lineage?\n- What are your failure-handling strategies for late data and re-ingestion costs?","diagram":"flowchart TD\n  A[Pub/Sub Ingest] --> B[Dataflow Streaming]\n  B --> C1[BigQuery Region 1]\n  B --> C2[BigQuery Region 2]\n  D[Data Quality] --> E[Cloud Monitoring Alerts]\n  F[Remediation] --> G[Re-ingest & Schema Upgrade]","difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Google","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T09:52:41.399Z","createdAt":"2026-01-16T09:52:41.399Z"},{"id":"q-2755","question":"Design a two-region, cross-tenant data lakehouse on GCP for 2 TB/day of JSON events ingested via Pub/Sub, storing raw Parquet in regional Cloud Storage and tenant-scoped tables in BigQuery; include per-tenant isolation, automatic schema evolution, late-arrival handling, TTL, and end-to-end lineage with Data Catalog. Provide data model, workflows, and rollback strategy?","answer":"Streaming 2 TB/day of JSON via Pub/Sub; Dataflow writes per-tenant Parquet in regional Cloud Storage and updates tenant-scoped BigQuery partitions. Enforce isolation with IAM and row-level filters; ma","explanation":"## Why This Is Asked\nTests cross-region data lakehouse design with tenant isolation, schema evolution, data governance, and rollback.\n\n## Key Concepts\n- GCP data lakehouse pattern, Pub/Sub, Dataflow, BigQuery, Cloud Storage\n- Tenant isolation, schema drift, DLP masking, Data Catalog lineage\n- Canary rollbacks, TTL, lifecycle management\n\n## Code Example\n```javascript\n// Pseudocode: Dataflow streaming template wiring Pub/Sub to Parquet+BigQuery\n```\n\n## Follow-up Questions\n- How would you validate latency SLAs across regions?\n- How would you handle late data and out-of-order events?","diagram":"flowchart TD\n  PubSub --> Dataflow\n  Dataflow --> CloudStorageParquet[Parquet in regional buckets]\n  Dataflow --> BigQueryTables[Tenant BigQuery partitions]\n  BigQueryTables --> DataCatalog[Lineage in Data Catalog]","difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T10:42:34.046Z","createdAt":"2026-01-16T10:42:34.046Z"},{"id":"q-2855","question":"Design a real-time cross-tenant data pipeline on GCP ingesting 1-2 TB/day of JSON events from Pub/Sub into two regions. Implement per-tenant privacy via field-level masking and tokenization using DLP and Cloud KMS, enforce data residency rules, provide end-to-end lineage with Data Catalog, support forward/backward schema evolution, and build a testing strategy with synthetic tenants. Include a rollback plan for privacy policy changes within 24 hours?","answer":"A robust answer identifies Pub/Sub -> Dataflow with per-tenant masking using DLP and Cloud KMS, writes to regional BigQuery tables, ensures tenant-scoped lineage in Data Catalog, implements schema evo","explanation":"## Why This Is Asked\n\nEvaluates capability to design a scalable, privacy-conscious streaming pipeline across regions, with governance and rollback considerations.\n\n## Key Concepts\n\n- Real-time ingestion with Pub/Sub and Dataflow\n- Per-tenant privacy via DLP masking and Cloud KMS\n- Data residency controls and multi-region writes\n- End-to-end lineage in Data Catalog\n- Schema evolution and backward/forward compatibility\n- Synthetic data testing and 24-hour rollback planning\n\n## Code Example\n\n```javascript\n// Pseudo Dataflow sketch for masking a tenant field using DLP\n```\n\n## Follow-up Questions\n\n- How would you test privacy-policy rollbacks across tenants during peak load?\n- What monitoring would you add to detect masking failures or schema drift?","diagram":"flowchart TD\n  Pub[Pub/Sub Ingest] --> DF[Dataflow Masking]\n  DF --> BQ[BigQuery Regional Tables]\n  BQ --> Catalog[Data Catalog Lineage]\n  Catalog --> BI[BI Tools]","difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","NVIDIA","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T14:49:27.353Z","createdAt":"2026-01-16T14:49:27.353Z"},{"id":"q-2903","question":"Design a real-time cross-tenant telemetry pipeline on GCP for 5 TB/day of JSON events ingested via Pub/Sub and stored into per-tenant BigQuery tables across two regions. Requirements: strict tenant isolation, field-level privacy masking at ingestion (PII redaction with DLP), auto schema evolution with backward compatibility, late-arrival handling, per-tenant TTL retention, and immutable audit logs exported to Cloud Storage. Include end-to-end lineage via Data Catalog, rollback strategy, testing plan, and concrete data model sketches, Dataflow templates, and failure modes?","answer":"Use Pub/Sub → Dataflow Flex Template with per-tenant routing to BigQuery tables. Ingest masks PII using DLP in a streaming DoFn; write to partitioned, tenant-scoped BigQuery tables to ensure isolation","explanation":"## Why This Is Asked\nTests ability to design a scalable, compliant real-time pipeline across regions with strict tenant isolation, privacy masking, TTL, and lineage.\n\n## Key Concepts\n- Cross-region streaming data pipelines\n- Per-tenant isolation strategies (datasets, tables, or partitioning)\n- Ingestion-time privacy masking (DLP integration)\n- Schema evolution and backward compatibility\n- Late-arrival handling and watermarking\n- TTL and lifecycle policies\n- Data Catalog lineage and governance\n\n## Code Example\n```python\n# Pseudo Beam DoFn for PII masking at ingest\nclass MaskPIIFn(beam.DoFn):\n    def process(self, element):\n        # call to DLP or internal masking logic\n        yield masked_element\n```\n\n## Follow-up Questions\n- How would you test for schema drift and ensure backward compatibility?\n- What are the trade-offs between per-tenant datasets vs shared dataset with tenant_id partitioning?","diagram":"flowchart TD\n  PubSub[Pub/Sub Ingest] --> Dataflow[Dataflow Ingest]\n  Dataflow --> BigQuery[BigQuery Tenant Tables]\n  Dataflow --> DLP[DLP Masking]\n  BigQuery --> Catalog[Data Catalog Lineage]\n  Dataflow --> Audit[Cloud Storage Audit Logs]","difficulty":"advanced","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Netflix","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T16:54:37.246Z","createdAt":"2026-01-16T16:54:37.247Z"},{"id":"q-2958","question":"Design a real-time, cross-tenant analytics pipeline on GCP that ingests 4 TB/day of JSON clickstream from Pub/Sub across three regions, with per-tenant isolation: raw data stored regionally as Parquet, aggregated BI-ready tables in BigQuery; enforce field-level masking with DLP; automatic schema drift handling using Data Catalog and Dataplex; maintain end-to-end lineage; and implement a rollback plan for tenant-specific schema changes within 24 hours, including testing and failover playbooks?","answer":"Architect a 3-region cross-tenant streaming pipeline: 4 TB/day of JSON clickstream from Pub/Sub, Dataflow processing with per-tenant masking via DLP; regional raw Parquet in Cloud Storage; per-tenant ","explanation":"## Why This Is Asked\nAssesses ability to design real-time, multi-region data pipelines with privacy, governance, and operational resilience.\n\n## Key Concepts\n- Multi-region streaming and data locality\n- Tenant isolation and masking\n- Schema drift management with Data Catalog/Dataplex\n- Data lineage and rollback workflows\n- Canary testing and runbooks\n\n## Code Example\n```python\n# Minimal Apache Beam skeleton for Pub/Sub -> BigQuery with masking\nimport apache_beam as beam\nclass MaskFn(beam.DoFn):\n  def process(self, element):\n    # pseudocode: mask PII fields\n    return masked\nwith beam.Pipeline(options=... ) as p:\n  (p\n   | beam.io.ReadFromPubSub(subscription='projects/…/subscriptions/…')\n   | beam.ParDo(MaskFn())\n   | beam.io.WriteToBigQuery('project:dataset.table', write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE)\n  )\n```\n\n## Follow-up Questions\n- How would you implement per-tenant rollback isolation and testing without impacting other tenants?\n- What are your failover and data consistency guarantees across regions during a rollback?","diagram":"flowchart TD\n  P[Pub/Sub Ingest] --> D[Dataflow Processing]\n  D --> R[Regional Raw Parquet (Cloud Storage)]\n  D --> Q[BigQuery Tenant Tables]\n  Q --> L[Data Catalog / Dataplex Lineage]\n  P --> M[PII Masking with DLP]","difficulty":"advanced","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["PayPal","Tesla","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T18:59:53.849Z","createdAt":"2026-01-16T18:59:53.849Z"},{"id":"q-2982","question":"Design a per-tenant data-sharing microservice on GCP that ingests 100 GB/day of JSON activity from Pub/Sub into BigQuery, then serves tenant-scoped datasets to external partners via a Cloud Run API. Include per-tenant isolation with partitioned BigQuery tables, authorized views for external access, IAM/Endpoints-based OAuth, audit trails to Cloud Logging, and automatic schema drift handling with backward-compatible updates and rollback?","answer":"Architect a Pub/Sub -> Dataflow -> BigQuery pipeline with separate partitioned tables per tenant, and a Cloud Run API exposing tenant-scoped datasets via authorized views. Enforce per-tenant IAM roles","explanation":"## Why This Is Asked\nTests real-world multi-tenant data sharing, strict isolation, and governance. It also probes integration of Pub/Sub, BigQuery partitioning, Cloud Run APIs, Endpoints/IAM auth, and auditable logs, plus a practical approach to schema drift.\n\n## Key Concepts\n- Multi-tenant isolation via partitioned BigQuery tables\n- Authorized views for partner access\n- Cloud Endpoints with OAuth/IAM for API security\n- Immutable audit trails in Cloud Logging\n- Schema evolution with rollback semantics\n\n## Code Example\n```yaml\n# Pseudo-architecture snippet\npipeline:\n  - source: PubSub\n  - transform: Dataflow\n  - sink: BigQuery (tenant-partitioned)\napi:\n  - runtime: Cloud Run\n  - auth: Endpoints + IAM\nlogging:\n  - sink: Cloud Logging (audit)\n```\n\n## Follow-up Questions\n- How would you test schema drift transitions across tenants without impacting production?\n- How would you handle backfill for a new tenant with historical data?","diagram":"flowchart TD\n  A[Pub/Sub] --> B[Dataflow]\n  B --> C[BigQuery per-tenant partitions]\n  C --> D[Cloud Run API for partners]\n  D --> E[External Partners]\n  F[Audit Logs] --> G[Cloud Logging]","difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Instacart","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T19:46:06.972Z","createdAt":"2026-01-16T19:46:06.972Z"},{"id":"q-2999","question":"On GCP, design a multi-tenant streaming validation and quality framework for 10-20B events/day from Pub/Sub into BigQuery across regions. Each tenant has its own schema and masking rules. Describe the end-to-end data model, per-tenant schema registry, per-tenant masking/quality rules, lineage via Data Catalog, and a canary rollback plan to the last-good schema within 6 hours?","answer":"Design a multi-tenant streaming validation framework: route Pub/Sub messages by tenant via Dataflow templates; apply per-tenant masking and schema checks; write validated data to tenant-specific BigQu","explanation":"## Why This Is Asked\nEvaluates ability to design scalable, tenant-isolated streaming pipelines with per-tenant rules, robust quality checks, and rapid rollback.\n\n## Key Concepts\n- Per-tenant schema registry and masking rules\n- Streaming validation with Dataflow templates\n- Data Catalog lineage integration\n- Canary rollouts and fast rollback\n\n## Code Example\n```python\ndef mask_record(record, tenant_policy):\n    for field, rule in tenant_policy.items():\n        if field in record:\n            record[field] = apply_mask(record[field], rule)\n    return record\n```\n\n## Follow-up Questions\n- How would you test schema drift and trigger rollback automatically?\n- How would you scale the schema registry as tenants scale from 100 to 1000+?","diagram":"flowchart TD\n  PubSub[Pub/Sub Ingest] --> DF[Dataflow Validation]\n  DF --> BIQ[BigQuery Tenant Tables]\n  BIQ --> DAC[Data Catalog Lineage]\n  DF --> CAN[Canary Controller]\n  CAN --> GBK[Last-Good Schema Rollback]\n  BIQ --> LOG[Audit Logs in Cloud Storage]","difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T20:39:47.413Z","createdAt":"2026-01-16T20:39:47.414Z"},{"id":"q-3093","question":"Design a batch Dataflow pipeline that reads per-tenant JSON logs from Cloud Storage, validates against a schema.json in GCS, flattens nested fields, and writes to a BigQuery table partitioned by ingest_date with event_id dedup. Allow late-arriving files up to 24 hours via re-ingest, and emit an audit log to Cloud Storage plus a quick observability summary to Pub/Sub?","answer":"Build a batch Dataflow pipeline that reads per-tenant JSON logs from Cloud Storage, validates them against a schema.json file stored in GCS, flattens nested fields, and writes to a BigQuery table partitioned by ingest_date. Implement event_id-based deduplication to ensure idempotent processing, support a 24-hour late-arrival window with re-ingest capability, and generate audit logs to Cloud Storage with observability summaries published to Pub/Sub.","explanation":"## Why This Is Asked\nTests ability to design a scalable, tenant-aware batch pipeline that uses Dataflow, enforces schema validation at ingest, and handles late arrivals without complex orchestration.\n\n## Key Concepts\n- Dataflow batch pipelines and PTransforms\n- JSON schema validation stored in GCS\n- BigQuery partitioning by ingest_date and per-tenant isolation\n- Idempotent deduplication by event_id\n- Late-arrival handling with re-ingest and retries\n- Audit logs into Cloud Storage and Pub/Sub observability\n\n## Code Example\n```javascript\n// Pseudo Dataflow-like sketch: validate JSON against schema and flatten\nfunction processTenantLogs(pipeline) {\n  pipeline\n    .apply('ReadTenantLogs', GCSIO.read().from('gs://tenant-logs/*/*.json'))\n    .apply('ValidateSchema', ParDo.of(new SchemaValidator('gs://schemas/schema.json')))\n    .apply('FlattenFields', ParDo.of(new FieldFlattener()))\n    .apply('DeduplicateEvents', Distinct.create().withField('event_id'))\n    .apply('WriteToBigQuery', BigQueryIO.writeTable()\n      .to('project.dataset.events')\n      .withSchema(eventSchema)\n      .withWriteDisposition(BigQueryIO.Write.WriteDisposition.WRITE_APPEND)\n      .withCreateDisposition(BigQueryIO.Write.CreateDisposition.CREATE_IF_NEEDED)\n      .withTimePartitioning('DAY', 'ingest_date'))\n    .apply('GenerateAudit', ParDo.of(new AuditLogger('gs://audit-logs/')));\n}\n```","diagram":"flowchart TD\n  A[GCS: Daily JSON logs (tenant_id present)] --> B[Dataflow (Beam) - Parse & Flatten]\n  B --> C[BigQuery: dataset.table partitioned by ingest_date]\n  B --> D[BigQuery: deduplicate by event_id]\n  E[Schema: JSON schema file in GCS]\n  A --> B\n  E --> B\n  C --> F[Audit logs: Cloud Storage]","difficulty":"beginner","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","Slack","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T05:00:34.945Z","createdAt":"2026-01-17T02:16:35.301Z"},{"id":"q-3251","question":"You operate a multi-tenant analytics service on GCP with 50 tenants. 5 GB/day of JSON events arrive via Pub/Sub. Design a beginner-friendly pipeline that ingests events with Dataflow, stores them in a single BigQuery table partitioned by ingestion_date, and includes tenant_id. Enforce per-tenant access using BigQuery authorized views (or per-tenant filters), support additive schema evolution, deduplicate by event_id, handle late data up to 1 hour, and capture end-to-end lineage in Data Catalog. Provide architecture sketch and a minimal test plan?","answer":"Ingest Pub/Sub JSON with Dataflow into a single BigQuery table partitioned by ingestion_date, including tenant_id. Enforce per-tenant access via authorized views filtering on tenant_id. Deduplicate by","explanation":"## Why This Is Asked\n\nTests the ability to design a practical, beginner-friendly pipeline with tenant isolation, governance, and lineage on GCP.\n\n## Key Concepts\n\n- Dataflow streaming ingestion from Pub/Sub\n- BigQuery single table with partitioning by ingestion_date and tenant_id\n- Authorized views for per-tenant access control\n- Additive schema evolution with nullable fields\n- Deduplication by event_id\n- Late data handling (1 hour)\n- Data Catalog-based lineage\n\n## Code Example\n\n```javascript\n// Pseudo Dataflow-style transform (illustrative)\nfunction transform(eventJson){\n  const obj = JSON.parse(eventJson);\n  obj.ingestion_date = (obj.event_time ? obj.event_time : new Date()).toISOString().slice(0,10);\n  return obj;\n}\n```\n\n## Follow-up Questions\n\n- How would you test tenant isolation and access controls?\n- What are trade-offs between a single table vs per-tenant tables?\n- How would you roll out additive schema changes with minimal downtime?","diagram":"flowchart TD\n  PubSub[Pub/Sub] --> Dataflow[Dataflow Ingestion]\n  Dataflow --> BigQuery[BigQuery Table]\n  BigQuery --> Views[Authorized Views]\n  DataCatalog[Data Catalog] --> Lineage[Lineage]","difficulty":"beginner","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Twitter","Two Sigma","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T08:45:00.921Z","createdAt":"2026-01-17T08:45:00.921Z"},{"id":"q-3270","question":"Design a multi-tenant feature store on GCP for real-time ML inference using Vertex AI Feature Store with online/offline stores and BigQuery as the offline layer. Ingest features via Pub/Sub; enforce per-tenant isolation and versioned schemas; support canary rollouts and rollback, with testing and Data Catalog lineage. Outline data model, ingestion, validation, and monitoring?","answer":"Propose a two-store setup: Vertex AI Feature Store for online serving and BigQuery for offline analytics, with tenant-scoped namespaces. Ingest through Pub/Sub, tag features by tenant, and version sch","explanation":"## Why This Is Asked\n\nThis question probes the ability to design scalable, tenant-aware ML feature systems on GCP, integrating Vertex AI Feature Store, BigQuery, Pub/Sub, and Data Catalog, while addressing schema evolution, canary deployments, rollback, and testing.\n\n## Key Concepts\n\n- Vertex AI Feature Store; online/offline\n- Tenant isolation and schema versioning\n- Canary rollouts and rollback\n- Data validation, lineage, monitoring\n\n## Code Example\n\n````javascript\n// Pseudo ingestion sketch for versioned features\n````\n\n## Follow-up Questions\n\n- How would you implement per-tenant access control and data masking in Feature Store?\n- How would you validate and test rollback scenarios in production?","diagram":"flowchart TD\n  A[Tenant] --> B[Ingest via Pub/Sub]\n  B --> C[Vertex AI Online Store]\n  B --> D[BigQuery Offline Store]\n  C --> E[Serving Layer]\n  D --> F[BI/Analytics]","difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Microsoft","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T09:33:58.544Z","createdAt":"2026-01-17T09:33:58.544Z"},{"id":"q-3415","question":"New streaming data: 150 MB/day of JSON click events arrive via Pub/Sub and must be ingested into BigQuery in a single region. Design a beginner-friendly GCP pipeline using Dataflow (Beam) to parse, validate required fields, and enrich with a userId lookup; store results in a daily-partitioned BigQuery table; implement idempotent writes using insertId or MERGE; route malformed messages to a dead-letter Pub/Sub topic and archive originals to Cloud Storage; propose a 24-hour rollback plan and a basic data quality/monitoring approach?","answer":"Design Dataflow (Beam) to parse JSON, validate fields (event_id, user_id, ts), and enrich with a simple userId lookup. Write to a daily-partitioned BigQuery table using BigQueryIO with insertId on eve","explanation":"## Why This Is Asked\n\nTests ability to craft a practical streaming pipeline on GCP with beginner-friendly components: Pub/Sub ingestion, Dataflow parsing/validation, BigQuery partitioning, idempotent writes, dead-letter handling, and archival. Also touches data quality checks and rollback planning.\n\n## Key Concepts\n\n- Dataflow Beam parsing and validation\n- BigQuery streaming inserts with insertId or MERGE\n- Dead-letter queues and archival storage\n- Schema evolution, basic data quality checks, monitoring\n\n## Code Example\n\n```javascript\n// Example: pseudo-beam snippet illustrating insertId usage\n```\n\n## Follow-up Questions\n\n- How would you test the pipeline end-to-end?\n- What data quality metrics would you monitor?","diagram":"flowchart TD\n  A[Pub/Sub] --> B[Dataflow/Beam parse & enrich]\n  B --> C(BigQuery: daily partitions)\n  B --> D[Dead-letter Pub/Sub]\n  B --> E[Cloud Storage archive]\n  C --> F[Monitoring & tests]","difficulty":"beginner","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["NVIDIA","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T15:33:19.801Z","createdAt":"2026-01-17T15:33:19.801Z"},{"id":"q-3512","question":"On GCP, design a two-tenant data platform for an advertising analytics service ingesting 800 GB/day of JSON events via Pub/Sub. Each tenant has isolated dashboards and ML models. Build versioned data contracts in Data Catalog, enforce them via a registry, route data to per-tenant BigQuery tables and to Vertex AI Feature Store for ML, apply per-tenant PII masking, and maintain end-to-end lineage. Include testing, a canary rollout, and a per-tenant rollback window (6 hours)?","answer":"Establish per-tenant data contracts in Data Catalog with versioned schemas, enforce via Dataflow templates that validate JSON against the registry, mask PII with DLP before storage, route to tenant-sc","explanation":"## Why This Is Asked\nReal-world pattern combining data contracts, lineage, privacy, and ML integration; tests ability to design governance and practical rollback\n\n## Key Concepts\n- Versioned data contracts in Data Catalog\n- Dataflow-based validation against registry\n- PII masking via DLP and per-tenant isolation\n- BigQuery + Vertex AI Feature Store integration\n- End-to-end lineage and canary rollouts\n\n## Code Example\n```yaml\ncontracts:\n  - tenant: \"tenantA\"\n    schema_version: \"v1\"\n    fields:\n      - name: \"user_id\"\n        type: \"STRING\"\n      - name: \"event_type\"\n        type: \"STRING\"\n      - name: \"timestamp\"\n        type: \"TIMESTAMP\"\n    privacy:\n      mask_fields:\n        - \"user_id\"\n```\n\n## Follow-up Questions\n- How would you test schema evolution for existing tenants?\n- How would you implement canary rollout and rollback for contracts?","diagram":"flowchart TD\n  A[Pub/Sub Ingest] --> B[Dataflow Template]\n  B --> C[BigQuery (Tenant A)]\n  B --> D[BigQuery (Tenant B)]\n  B --> E[Vertex AI Feature Store]\n  C --> F[Data Catalog Lineage]\n  E --> F","difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Databricks","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T19:28:49.140Z","createdAt":"2026-01-17T19:28:49.140Z"},{"id":"q-3553","question":"On GCP, build a streaming ML feature pipeline that ingests 1 TB/day of JSON user events via Pub/Sub, computes features in Dataflow, stores in BigQuery and Vertex AI Feature Store, and traces end-to-end lineage for model training. Describe data models, feature store integration, drift detection, rollback strategy, and how you'd reproduce training with exact feature versions?","answer":"Pub/Sub -> Dataflow (Parse, normalize, compute features in fixed windows) -> BigQuery (raw events) + Vertex AI Feature Store (features with per-tenant tags). Track lineage via Data Catalog and feature","explanation":"## Why This Is Asked\nTests ability to design a scalable streaming feature pipeline with end-to-end lineage, versioned features for reproducible ML training, and robust rollback and drift detection.\n\n## Key Concepts\n- Streaming ingestion: Pub/Sub, Dataflow\n- Feature storage: Vertex AI Feature Store, BigQuery\n- Lineage: Data Catalog, feature versioning\n- Reproducibility: pinned feature versions per training run\n- Quality: drift detection, rollback strategies\n\n## Code Example\n```javascript\n// Pseudocode: Dataflow steps for feature computation and writes\n```\n\n## Follow-up Questions\n- How would you enforce per-tenant feature isolation in the feature store?\n- How would you implement end-to-end testing for training reproducibility across feature versions?","diagram":"flowchart TD\n  Ingest[Ingest Events] --> Process[Process with Dataflow]\n  Process --> RawBQ[BigQuery Raw]\n  Process --> FeatureStore[Feature Store]\n  RawBQ --> Train[Model Training]\n  FeatureStore --> Train","difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","Tesla","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T21:25:36.167Z","createdAt":"2026-01-17T21:25:36.167Z"},{"id":"q-3716","question":"Design a GCP-based multi-tenant analytics platform for 300 tenants ingesting 500 GB/day of JSON events via Pub/Sub. Build per-tenant data products in BigQuery with strict isolation (Authorized Views), enforce data contracts via Data Catalog, mask PII at ingest with DLP, and store immutable audit logs in Cloud Storage. Include automatic schema evolution, event-time processing with late arrivals (Dataflow), cross-region replication, and a 24h tenant rollback?","answer":"Design a GCP-based multi-tenant analytics pipeline for 300 tenants ingesting 500 GB/day of JSON events via Pub/Sub. Implement per-tenant BigQuery data products with Authorized Views, enforce data cont","explanation":"## Why This Is Asked\nTests experience designing scalable, governed data platforms with multi-tenant isolation, data contracts, and end-to-end lineage on GCP. It also probes handling late-arriving data, PII masking, and per-tenant rollback plans.\n\n## Key Concepts\n- Multi-tenant isolation in BigQuery with Authorized Views\n- Data contracts and lineage via Data Catalog\n- Ingestion-time PII masking with DLP\n- Event-time processing and late-arrivals in Dataflow\n- Cross-region replication and immutable audit logs\n- Schema evolution and per-tenant rollback strategy\n\n## Code Example\n```javascript\n// Pseudo-code: ingest path with DLP masking before storage\nconst ingest = (record) => {\n  const masked = dlp.maskPII(record);\n  pubsub.publish(masked);\n};\n```\n\n## Follow-up Questions\n- How would you test schema evolution across 300 tenants without downtime?\n- What metrics and alerting would you implement for cross-region replication health?","diagram":"flowchart TD\n  PubSub[Pub/Sub] --> Dataflow[Dataflow Ingest]\n  Dataflow --> RawStorage[Cloud Storage Raw Parquet]\n  Dataflow --> BigQuery[BigQuery Tenant Data Products]\n  DataCatalog[Data Catalog] --> BigQuery","difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Meta","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T06:51:32.769Z","createdAt":"2026-01-18T06:51:32.770Z"},{"id":"q-3753","question":"Design a cross-tenant analytics platform on GCP that ingests 2 TB/day of JSON events per tenant via Pub/Sub, stores raw Parquet in regional Cloud Storage, and serves per-tenant aggregated views in BigQuery with strict isolation. Implement per-tenant differential privacy budgets at query time using DP libraries and UDFs, track budgets in Data Catalog, and enable a 24h rollback of privacy budgets. Include data models, lineage, testing strategy, and rollback plan?","answer":"In-depth approach: ingest 2 TB/day via Pub/Sub, land raw Parquet in regional GCS, and publish per-tenant views in BigQuery with row-level isolation. Enforce differential privacy at query time using DP","explanation":"## Why This Is Asked\nThis new angle tests privacy-aware analytics design across GCP services, tenant isolation, and governance.\n\n## Key Concepts\n- Differential privacy budgets per tenant\n- BigQuery UDFs for DP noise\n- Dataflow DP transforms and lineage\n- Data Catalog metadata for budget tracking\n- Regional storage and per-tenant isolation\n- Rollback strategies and testing\n\n## Code Example\n```javascript\n// DP budget checkpoint (pseudo)\nfunction applyDP(value, epsilonBudget) {\n  const noise = sampleLaplace(1/epsilonBudget);\n  return value + noise;\n}\n```\n\n## Follow-up Questions\n- How would you test budget exhaustion and prevent cross-tenant leakage?\n- How would you handle late data affecting DP guarantees and rollback?","diagram":"flowchart TD\n  P[Pub/Sub Ingest] --> D[Dataflow DP]\n  D --> B[BigQuery per-tenant views]\n  P --> R[Raw Parquet in regional GCS]\n  B --> L[Data Catalog lineage]","difficulty":"advanced","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Apple","Instacart"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T08:38:39.531Z","createdAt":"2026-01-18T08:38:39.532Z"},{"id":"q-3795","question":"Design a GCP streaming ML feature pipeline for five tenants: ingest 250 GB/day of JSON events from Pub/Sub into Vertex AI Feature Store (online for real-time scoring and offline for batch) with per-tenant isolation, data contracts in Data Catalog, and PII masking at ingest. Include schema evolution, drift monitoring, cross-region replication, and a 24h rollback plan?","answer":"Five-tenant streaming ML feature pipeline: Pub/Sub → Dataflow → Vertex AI Feature Store (online for real-time scoring and offline for batch) with per-tenant isolation, data contracts in Data Catalog, ","explanation":"## Why This Is Asked\nTests ability to design end-to-end streaming ML pipelines on GCP with tenants separation, data contracts, and governance.\n\n## Key Concepts\n- Vertex AI Feature Store (online/offline)\n- Pub/Sub + Dataflow streaming\n- Data Catalog data contracts and schema evolution\n- Per-tenant isolation via IAM\n- Drift monitoring and rollback plans\n\n## Code Example\n```javascript\n// PII masking example at ingest\nfunction maskPII(record){ if(record.email){ record.email = hash(record.email); } return record; }\n```\n\n## Follow-up Questions\n- How would you validate schema evolution without breaking tenants?\n- How would you design drift alerts and rollbacks across regions?","diagram":"flowchart TD\n  PubSub[Pub/Sub Ingest] --> Dataflow[Dataflow Transform]\n  Dataflow --> Online[Vertex AI Feature Store (Online)]\n  Dataflow --> Offline[Vertex AI Feature Store (Offline)]\n  Online --> Serving[Model Serving]\n  Serving --> Alerts[Drift Alerts & Monitoring]\n  DataCatalog[Data Catalog Contracts] --> Dataflow\n  IAM[Tenant Isolation (IAM)] --> Dataflow","difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Salesforce","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T09:42:21.058Z","createdAt":"2026-01-18T09:42:21.058Z"},{"id":"q-3879","question":"Design a cost-aware, two-region multi-tenant analytics pipeline on GCP that ingests 1 TB/day of JSON events via Pub/Sub into BigQuery. Add tenant-scoped data quality scoring at ingest, per-tenant row-level isolation, and automated schema drift rollback with canary deployments. Include data models, partitioning, testing plan, and rollback strategy per tenant?","answer":"Implement a per-tenant, two-region pipeline: Pub/Sub -> Dataflow -> BigQuery per-tenant datasets. Enforce isolation with Row-Level Security and Authorized Views; compute tenant-scoped data quality sco","explanation":"## Why This Is Asked\nTests ability to design a cost-aware, two-region, multi-tenant pipeline with scalable data quality, robust isolation, and a clear rollback strategy. Requires practical knowledge of GCP services and governance.\n\n## Key Concepts\n- Two-region BigQuery with per-tenant datasets and isolation via RLS/Authorized Views\n- Ingest-time data quality scoring (conformance, completeness, duplicates)\n- Canary schema changes and per-tenant rollback within 24h\n- Data Catalog for lineage; Dataflow for streaming transformation\n\n## Code Example\n```python\n# Pseudo Dataflow transform: extract tenant, score quality, route to ok/bad lanes\nclass ProcessEvent(DoFn):\n    def process(self, element):\n        evt = json.loads(element)\n        tenant = evt['tenant_id']\n        score = score_quality(evt)\n        if score['ok']:\n            yield beam.window.TimestampedValue((tenant, evt), evt['ts'])\n        else:\n            yield beam.pvalue.TaggedOutput('bad', (tenant, evt, score))\n```\n\n## Follow-up Questions\n- How would you validate per-tenant rollback across all tenants without impacting live queries?\n- What metrics and alerts would you surface to detect schema drift per tenant?","diagram":null,"difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","Plaid","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T13:43:22.039Z","createdAt":"2026-01-18T13:43:22.039Z"},{"id":"q-3926","question":"Design a cost-aware, multi-tenant streaming pipeline: Pub/Sub feeds 1 TB/day of JSON events; Dataflow writes per-tenant, day-partitioned BigQuery tables (tenant_id, user_id). Run a shared Vertex AI anomaly model with per-tenant thresholds via side inputs; store scores in BigQuery and publish alerts to Pub/Sub. Archive older data to Cloud Storage Coldline; Data Catalog provides lineage. Rollback uses canary deployment and per-tenant guardrails; test with synthetic data and drift checks?","answer":"Design a cost-aware, multi-tenant streaming pipeline: Pub/Sub feeds 1 TB/day of JSON events; Dataflow writes per-tenant, day-partitioned BigQuery tables (tenant_id, user_id). Run a shared Vertex AI an","explanation":"## Why This Is Asked\nThis prompt tests building a scalable, cost-aware multi-tenant streaming solution with real-time ML scoring, strict data isolation, and robust rollback.\n\n## Key Concepts\n- Pub/Sub → Dataflow streaming\n- Per-tenant BigQuery storage and day partitioning\n- Vertex AI scoring with tenant-specific thresholds via side inputs\n- Data Catalog lineage and Cloud Storage lifecycle for archival\n- Canary deployments and per-tenant rollbacks\n\n## Code Example\n```javascript\n// Pseudo-config for per-tenant thresholds and canary rollout\nconst tenantConfig = {\n  \"tenantA\": { \"threshold\": 0.95, \"canary\": true },\n  \"tenantB\": { \"threshold\": 0.98, \"canary\": false }\n};\n```\n\n## Follow-up Questions\n- How would you monitor data skew and trigger rebalancing?\n- What failure modes exist during drift and how would you test rollbacks?","diagram":null,"difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Snap","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T15:42:04.001Z","createdAt":"2026-01-18T15:42:04.001Z"},{"id":"q-4022","question":"Design a two-region streaming pipeline on GCP for multi-tenant analytics (Pub/Sub → Dataflow → BigQuery). Enforce per-tenant isolation (Authorized Views), two-region active-active replication, exactly-once processing, late data handling, and per-tenant TTL (90 days). Support automatic schema evolution and provide a rollback plan using versioned tables and canary validation?","answer":"Configure a Dataflow streaming pipeline (Pub/Sub → Dataflow → BigQuery) with per-tenant datasets and Authorized Views, two-region replication, and exactly-once processing. Use TTL via time-partitioned","explanation":"## Why This Is Asked\\n\\nTests practical mastery of streaming pipelines, cross-region consistency, and tenant isolation with BigQuery, Dataflow, Pub/Sub. It also probes ability to design rollback and testing.\\n\\n## Key Concepts\\n- Streaming exactly-once semantics in Dataflow\\n- Tenant isolation with Authorized Views/BQ datasets\\n- Two-region replication and failover\\n- TTL and schema evolution strategies\\n- Canary rollbacks and testing\\n\\n## Code Example\\n```javascript\\n// Pseudo-Dataflow config (illustrative)\\nconst opts = {\\n  streaming: true,\\n  region: 'us-central1'\\n};\\n```\\n\\n## Follow-up Questions\\n- How would you test TTL compliance and canary rollback?\\n- How would you handle schema drift across regions?","diagram":null,"difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T20:34:35.603Z","createdAt":"2026-01-18T20:34:35.604Z"},{"id":"q-4317","question":"Design a beginner-friendly GCP pipeline that ingests 300 MB/day of JSON streaming events from Pub/Sub into BigQuery in a single region. Requirements: 1) parse and validate fields: eventId, deviceId, ts, metric, value; 2) normalize ts to UTC; 3) deduplicate by eventId; 4) allow up to 2 minutes lateness for a 1-minute sliding window; 5) produce a daily-partitioned BigQuery summary with average value per device/metric; 6) write raw JSON to Cloud Storage and publish malformed messages to a DLQ Pub/Sub; 7) provide a 24-hour rollback by deleting last day’s summary partition and reprocessing; 8) basic monitoring?","answer":"Use Dataflow streaming: Pub/Sub -> DoFn to parse/validate eventId, deviceId, ts, metric, value; UTC-normalize ts; emit invalid to a DLQ Pub/Sub. BigQueryIO writes to a daily-partitioned table with ins","explanation":"## Why This Is Asked\nTests end-to-end streaming design, deduplication, backfill tolerance, and rollback.\n\n## Key Concepts\n- Dataflow (Beam) streaming pipelines\n- Pub/Sub + DLQ handling\n- BigQuery insertId for idempotent writes\n- Windowing and late data handling\n- Cloud Storage archival and rollback strategies\n- Basic monitoring\n\n## Code Example\n```javascript\n// Beam skeleton showing Pub/Sub, DoFn, and BigQuery write with insertId\n```\n\n## Follow-up Questions\n- How would you test idempotence and rollback in CI/CD?\n- How would you adjust for bursty data or increasing eventId cardinality?","diagram":"flowchart TD\nA[Pub/Sub] --> B[Dataflow: Parse/Validate/Normalize]\nB --> C[BigQuery: daily-partitioned table (insertId=eventId)]\nB --> D[BigQuery: per-window averages to summary table]\nB --> E[Cloud Storage: Raw archive]\nB --> F[Pub/Sub: Dead-letter topic]\n","difficulty":"beginner","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Google","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-19T13:23:21.932Z","createdAt":"2026-01-19T13:23:21.932Z"},{"id":"q-4364","question":"Design a real-time, multi-tenant fraud-detection pipeline on GCP for 100 tenants, each with ~200k events/day via Pub/Sub; implement per-tenant Vertex AI Feature Store, streaming scoring with Dataflow, model versioning, explainability logs, and drift monitoring; enforce isolation with IAM and Authorized Views, ensure end-to-end lineage via Data Catalog, and provide a rollback to a previous model version within 2 hours plus a testing strategy?","answer":"Implement per-tenant isolation with IAM, Authorized Views, and separate feature stores; process Pub/Sub events in Dataflow, compute features in real-time, score with Vertex AI hosted model, and push r","explanation":"## Why This Is Asked\nTests the ability to design end-to-end real-time ML-enabled data pipelines with strict tenant isolation, governance, explainability, and rollback.\n\n## Key Concepts\n- Vertex AI Feature Store per-tenant\n- Dataflow streaming in/out\n- IAM + Authorized Views for multi-tenancy\n- Data Catalog lineage\n- Drift detection and explainability logging\n- Canary/rollback safety window and testing\n\n## Code Example\n\n```json\n{\n  \"tenant\": \"<tenant_id>\",\n  \"feature_columns\": [\"recent_amount\",\"velocity\",\"device_fingerprint\"],\n  \"model_version\": \"fraud_v1.2\"\n}\n```\n\n## Follow-up Questions\n- How would you automate cross-tenant drift alerts and rollback triggers?\n- What latency targets and monitoring would you implement for streaming scoring?","diagram":null,"difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Tesla","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-19T15:52:47.985Z","createdAt":"2026-01-19T15:52:47.985Z"},{"id":"q-4495","question":"Batch ingestion: 500k JSON events/day stored in Cloud Storage (gs://data/events/YYYY/MM/DD/*.json). Build a beginner-friendly Dataflow (Python) pipeline that parses JSON, validates required fields (user_id, event_type, ts), enriches with a user tier lookup from BigQuery analytics.user_traits, and writes to analytics.events$YYYYMMDD (daily partition) using insertId for idempotence. Route malformed records to gs://data/events/errors/YYYY-MM-DD/ and outline a 24h rollback approach plus basic data quality monitoring?","answer":"Use a batch Dataflow job with a DoFn to parse JSON and validate fields; load user_traits as a side input (small lookup) to add tier; write to BigQuery with insertId set to event_id into analytics.even","explanation":"## Why This Is Asked\n\nTests batch ingestion, simple enrichment, and idempotent writes in a minimal setup. It also checks error handling paths and basic rollback thinking.\n\n## Key Concepts\n\n- Dataflow (Beam Python)\n- BigQueryIO with daily partitioned tables\n- Side inputs for small lookups (analytics.user_traits)\n- insertId-based deduplication\n- Dead-letter: errors to Cloud Storage\n- Basic monitoring and rollback strategy\n\n## Code Example\n\n```python\nfrom apache_beam import DoFn, ParDo, PCollection\nimport json\n\nclass ParseEventDoFn(DoFn):\n  def process(self, element):\n    try:\n      data = json.loads(element)\n      # minimal validation\n      if all(k in data for k in (\"user_id\", \"event_type\", \"ts\")):\n        yield data\n      else:\n        yield None\n    except Exception:\n      yield None\n```\n\n## Follow-up Questions\n\n- How would you test this locally with DirectRunner and mock BigQuery side input?\n- How would you adapt for late-arriving events or schema evolution?","diagram":"flowchart TD\n  A[Read JSON from GCS] --> B[Parse & Validate]\n  B --> C{Valid}\n  C -->|Yes| D[Enrich with BigQuery lookup (side input)]\n  C -->|No| E[Write to errors bucket]\n  D --> F[Write to BigQuery analytics.events$YYYYMMDD]\n  F --> G[Metrics & Monitoring]","difficulty":"beginner","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Google","Lyft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-19T21:28:33.215Z","createdAt":"2026-01-19T21:28:33.218Z"},{"id":"q-4569","question":"Design a GCP streaming fraud-detection pipeline for ride-hailing that ingests 1–2 TB/day of JSON events via Pub/Sub into two regions. Enforce per-tenant contracts with schema validation, isolate tenants with dedicated datasets, compute real-time features and anomaly scores in Dataflow, and expose a low-latency BigQuery view. Include end-to-end lineage via Data Catalog, automatic drift rollback, and a concrete testing plan?","answer":"Design a GCP streaming fraud-detection pipeline for ride-hailing that ingests 1–2 TB/day of JSON events via Pub/Sub into two regions. Enforce per-tenant contracts with schema validation, isolate tenants with dedicated datasets, compute real-time features and anomaly scores in Dataflow, and expose a low-latency BigQuery view. Include end-to-end lineage via Data Catalog, automatic drift rollback, and a concrete testing plan.","explanation":"## Why This Is Asked\nTests ability to design cross-region streaming pipelines with strict per-tenant isolation, real-time feature extraction, and robust data governance.\n\n## Key Concepts\n- Streaming ingestion: Pub/Sub to Dataflow with windowed aggregations\n- Tenant isolation: separate datasets/views and access controls\n- Real-time features: feature store patterns in BigQuery/Dataflow\n- Data governance: Data Catalog lineage, schema validation, drift rollback\n\n## Code Example\n```python\n# Skeleton Apache Beam streaming pipeline (Python)\nimport apache_beam as beam\nfrom apache_beam.options.pipeline_options import PipelineOptions\nfrom apache_beam.io.gcp.bigquery import WriteToBigQuery\n\nclass FraudDetectionPipeline:\n    def __init__(self, project_id, region):\n        self.project_id = project_id\n        self.region = region\n    \n    def validate_schema(self, element):\n        # Enforce per-tenant schema contracts\n        required_fields = ['tenant_id', 'event_type', 'timestamp', 'user_id']\n        return all(field in element for field in required_fields)\n    \n    def compute_features(self, element):\n        # Real-time feature extraction\n        features = {\n            'tenant_id': element['tenant_id'],\n            'event_hour': beam.window.TimestampedValue.get_current_time(),\n            'anomaly_score': self.calculate_anomaly_score(element)\n        }\n        return features\n    \n    def calculate_anomaly_score(self, event):\n        # Simplified anomaly detection logic\n        return 0.0  # Replace with actual ML model scoring\n\ndef run_pipeline():\n    options = PipelineOptions([\n        '--project=your-project',\n        '--region=us-central1',\n        '--streaming=true',\n        '--temp_location=gs://your-bucket/temp/'\n    ])\n    \n    with beam.Pipeline(options=options) as p:\n        (p\n         | 'ReadFromPubSub' >> beam.io.ReadFromPubSub(\n             subscription='projects/your-project/subscriptions/fraud-events')\n         | 'ParseJSON' >> beam.Map(lambda x: json.loads(x))\n         | 'ValidateSchema' >> beam.Filter(FraudDetectionPipeline.validate_schema)\n         | 'TenantRouting' >> beam.Map(lambda x: (\n             f'tenant_{x[\"tenant_id\"]}', x))\n         | 'ComputeFeatures' >> beam.Map(FraudDetectionPipeline.compute_features)\n         | 'WriteToBigQuery' >> WriteToBigQuery(\n             table='your-project:fraud_detection.features',\n             schema='tenant_id:STRING,event_hour:TIMESTAMP,anomaly_score:FLOAT',\n             write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND))\n```\n\n## Testing Plan\n1. **Unit Tests**: Schema validation, feature computation logic\n2. **Integration Tests**: End-to-end Pub/Sub to BigQuery flow\n3. **Performance Tests**: 1–2 TB/day ingestion throughput validation\n4. **Tenant Isolation Tests**: Verify dataset separation and access controls\n5. **Drift Detection Tests**: Schema evolution and rollback scenarios\n6. **Disaster Recovery Tests**: Cross-region failover validation","diagram":null,"difficulty":"advanced","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","Lyft","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T06:00:13.918Z","createdAt":"2026-01-20T02:20:42.024Z"},{"id":"q-4631","question":"Design a GCP streaming pipeline for 1 TB/day of multi-tenant clickstream events ingested via Pub/Sub into per-tenant BigQuery datasets. Include isolated tenants with separate schemas, backward-compatible schema evolution, per-tenant data masking at ingest (DLP), end-to-end lineage via Data Catalog, and a rolling 24h rollback that replays data for affected tenants without touching others; also specify per-tenant cost controls with slot reservations. Provide architecture, data models, and testing strategy?","answer":"Load per-tenant events from Pub/Sub into dedicated BigQuery datasets via a Dataflow streaming template; each tenant uses a separate table with backward-compatible schema evolution managed by a central","explanation":"## Why This Is Asked\n\nTests the ability to design a production-grade streaming pipeline with strong tenant isolation, governance, and operational rollback on GCP.\n\n## Key Concepts\n\n- GCP streaming ETL: Pub/Sub, Dataflow\n- Tenant isolation: per-tenant datasets, IAM\n- Schema evolution: registry + backward compatibility\n- Data masking: DLP at ingest\n- Data lineage: Data Catalog tagging\n- Rollback strategy: point-in-time replay per tenant\n\n## Code Example\n\n```python\n# Pseudo Dataflow template snippet\ndef process(event, context):\n    masked = mask(event)\n    write_to_bq(masked, tenant=event['tenant'])\n```\n\n## Follow-up Questions\n\n- How would you test schema evolution impact across tenants?\n- How would you handle a bursty backlog during rollback?","diagram":"flowchart TD\n  A[Pub/Sub] --> B[Dataflow Template]\n  B --> C[BigQuery per-tenant]\n  C --> D[Data Catalog lineage]\n  C --> E[Rollback backlog replay]","difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Tesla","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T05:48:20.741Z","createdAt":"2026-01-20T05:48:20.741Z"},{"id":"q-4731","question":"Design a real-time, cross-tenant analytics pipeline on GCP ingesting 2 TB/day of JSON events via Pub/Sub into BigQuery across two regions. Enforce per-tenant isolation with field-level masking, support automatic schema evolution, and implement a tenant-scoped data replay (24h window) without impacting others. Add end-to-end lineage in Data Catalog and a Vertex AI-based data observability layer. Describe architecture, data models, rollback, and tests?","answer":"Ingest with Pub/Sub and Dataflow, deploy two regional pipelines feeding per-tenant BigQuery datasets. Use DLP + IAM for field-level masking, and BigQuery auto schema updates. Implement a tenant-scoped","explanation":"## Why This Is Asked\n\nEvaluate ability to design scalable, multi-region, tenant-isolated pipelines with governance and observability. The prompt tests streaming ETL, tenant-scoped replay, schema evolution strategies, privacy controls, automated lineage, and a data quality layer with ML-based anomaly detection. It also probes rollback plans and robust testing in production.\n\n## Key Concepts\n\n- Multi-region BigQuery reservoirs and Dataflow pipelines\n- Tenant isolation and per-tenant schemas\n- Replay semantics with idempotent upserts\n- Data Catalog lineage and DLP masking\n- Vertex AI monitoring and anomaly detection\n\n## Code Example\n\n```javascript\n// pseudo-code sketch\n```\n\n## Follow-up Questions\n\n- How would you validate replay correctness at scale?\n- How would you handle partial failures during replay across tenants?","diagram":null,"difficulty":"advanced","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Hugging Face","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T10:08:45.938Z","createdAt":"2026-01-20T10:08:45.938Z"},{"id":"q-4827","question":"Design a real-time, cross-region, multi-tenant analytics pipeline on GCP that ingests 3 TB/day of JSON events from Pub/Sub into BigQuery in two regions. Each tenant must be isolated (per-tenant datasets or authorized views), with field-level privacy masking at ingest, automatic schema evolution, and end-to-end lineage captured in Data Catalog. Include data model, partitioning / clustering, rollback plan per tenant within 24h, testing strategy, and how you'd monitor latency?","answer":"Streaming via Pub/Sub -> Dataflow templates; apply per-tenant masking with a DoFn and DLP; write to regionally replicated BigQuery datasets partitioned by ingest_date and clustered by tenant_id; Data ","explanation":"## Why This Is Asked\nProbes practical streaming architecture at scale, cross-region replication, and robust per-tenant privacy controls.\n\n## Key Concepts\n- Real-time ingestion: Pub/Sub to Dataflow templates\n- Multi-region data residency and consistency\n- Tenant isolation strategies: per-tenant datasets or authorized views\n- Field-level masking: DLP integration at ingest\n- Schema evolution in BigQuery with streaming\n- End-to-end lineage via Data Catalog\n- Immutable audit logs to Cloud Storage\n- Rollback using BigQuery time travel and per-tenant snapshots\n- Monitoring: Dataflow metrics, backlog, latency\n\n## Code Example\n```python\nimport apache_beam as beam\nfrom apache_beam.options.pipeline_options import PipelineOptions\n\nclass MaskPII(beam.DoFn):\n  def process(self, element, *args, **kwargs):\n    # pseudo masking logic per tenant\n    tenant = element.get('tenant_id')\n    if 'email' in element:\n      element['email'] = '***@masked.com'\n    # more field-level masking based on tenant policy\n    yield element\n```\n\n## Follow-up Questions\n- How would you handle schema drift across regions and tenants?\n- How would you validate rollback SLAs and rollback correctness under load?","diagram":"flowchart TD\n  PubSub[Pub/Sub: JSON events] --> T[Dataflow: ingest & transform]\n  T --> BQ1[BigQuery Region A: per-tenant datasets]\n  T --> BQ2[BigQuery Region B: per-tenant datasets]\n  BQ1 --> LC[Data Catalog: lineage]\n  BQ2 --> LC\n  T --> Audit[Audit Logs to Cloud Storage]\n  T --> Mask[Masking: per-tenant fields]\n  LC --> Monitor[Monitoring & Alerts]","difficulty":"advanced","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Google","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T15:08:37.317Z","createdAt":"2026-01-20T15:08:37.317Z"},{"id":"q-4866","question":"Incoming Pub/Sub messages contain a field 'events' which is an array of sub-events with 'id','type','value'. Build a beginner-friendly Dataflow (Beam) streaming pipeline to flatten into one row per sub-event, validate required fields at sub-event level, and enrich with a voyageId lookup; write to a daily-partitioned BigQuery table. Route malformed messages to a dead-letter Pub/Sub topic and archive originals to Cloud Storage. Provide architecture, data model, and a minimal test plan?","answer":"Use a streaming Dataflow pipeline with a ParDo that explodes the events array, validates sub-field presence, and emits valid rows to a BigQuery streaming insert into a daily-partitioned table using th","explanation":"## Why This Is Asked\nTests nested data handling, field validation, enrichment, and reliable DLQ/archival integration. Also checks BigQuery partition-aware writing and testability.\n\n## Key Concepts\n- Flatten nested arrays in Beam\n- Sub-event validation and error routing\n- Side-input enrichment\n- Daily-partitioned BigQuery writes\n- Pub/Sub Dead Letter and Cloud Storage archival\n- Beam TestStream for unit tests\n\n## Code Example\n```python\nimport json\nimport apache_beam as beam\nfrom apache_beam.options.pipeline_options import PipelineOptions\n\nclass FlattenSubEvents(beam.DoFn):\n    def process(self, element, voyage_map):\n        for ev in element.get('events', []):\n            if all(k in ev for k in ('id','type','value')):\n                ev['voyageId'] = voyage_map.get(element.get('voyageId'))\n                yield ev\n\ndef run():\n    opts = PipelineOptions()\n    with beam.Pipeline(options=opts) as p:\n        voyage_map = p | 'CreateVoyageMap' >> beam.Create({'V1': 'Voyage-1'})\n        _ = (\n            p\n            | 'ReadPubSub' >> beam.io.ReadFromPubSub(topic='projects/PROJECT/topics/TOPIC')\n            | 'ParseJson' >> beam.Map(lambda b: json.loads(b.decode('utf-8')))\n            | 'Flatten' >> beam.ParDo(FlattenSubEvents(), voyage_map=beam.pvalue.AsDict(voyage_map))\n            | 'ToBigQuery' >> beam.io.WriteToBigQuery(\n                table='PROJECT.DATASET.table$YYYYMMDD',\n                schema='SCHEMA_AUTODETECT',\n                write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE\n            )\n        )\n\nif __name__ == '__main__':\n    run()\n```\n\n## Follow-up Questions\n- How would you test late-arriving nested events and ensure they are processed in order?\n- What changes would you make if the events array grows large or becomes streaming-heavy?","diagram":"flowchart TD\n  PubSub[Pub/Sub] --> Dataflow[Dataflow (Beam)]\n  Dataflow --> BigQuery[BigQuery daily partition]\n  Dataflow --> DLQ[DLQ Pub/Sub]\n  Dataflow --> Archive[Cloud Storage Archive]","difficulty":"beginner","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["NVIDIA","Salesforce","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T16:55:31.325Z","createdAt":"2026-01-20T16:55:31.325Z"},{"id":"q-931","question":"A GCP pipeline ingests 1 TB of JSON user activity daily from Pub/Sub into BigQuery via Dataflow. New fields appear over time; you must evolve the schema without downtime. What approach would you use for schema drift and nested fields, and outline concrete steps to implement it?","answer":"Use a two-layer scheme: a stable BigQuery table with known fields and a separate extra_json column to capture drift. The Dataflow job maps incoming JSON to the known schema; unknown fields are stored ","explanation":"## Why This Is Asked\nThis question probes practical handling of evolving data in a streaming pipeline, balancing query stability with schema growth.\n\n## Key Concepts\n- Schema evolution in BigQuery\n- Semi-structured data (JSON)\n- Dataflow transforms for parsing\n- Backward compatibility\n- Metadata and cataloging\n\n## Code Example\n```javascript\nfunction parseEvent(json) {\n  const data = JSON.parse(json);\n  const known = {\n    userId: data.user_id || null,\n    eventTime: data.event_time || null,\n    country: data.country || null\n  };\n  const extraKeys = Object.keys(data).filter(k => !(k in known));\n  const extra = JSON.stringify(extraKeys.reduce((acc, k) => { acc[k] = data[k]; return acc; }, {}));\n  return { ...known, extra_json: extra };\n}\n```\n\n## Follow-up Questions\n- How would you migrate existing data to the new table version with minimal downtime?\n- How would you monitor for schema drift and alert when new fields appear?","diagram":null,"difficulty":"beginner","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Discord","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T15:41:21.360Z","createdAt":"2026-01-12T15:41:21.361Z"},{"id":"q-962","question":"In a GCP streaming pipeline, Pub/Sub feeds millions of events into BigQuery via Dataflow. Out-of-order arrivals and duplicates occur. Design an idempotent sink using a staging table and a final partitioned table, leveraging insertId for dedup and a MERGE strategy to upsert into the final table. Outline concrete steps and trade-offs to implement?","answer":"Use a staging BigQuery table where Dataflow writes with insertId derived from a stable key (user_id + event_id + event_time). Periodically MERGE from staging into a final partitioned table, deduplicat","explanation":"## Why This Is Asked\n\nTests practical data quality, idempotent sinks, and operational design using Dataflow + BigQuery features.\n\n## Key Concepts\n\n- Idempotent sink\n- insertId dedup\n- staging vs final table\n- MERGE in BigQuery\n- allowed lateness and partitioning\n- quotas and monitoring\n\n## Code Example\n\n```sql\nMERGE INTO dataset.final AS F\nUSING dataset.staging AS S\nON F.user_id = S.user_id AND F.event_time = S.event_time\nWHEN NOT MATCHED THEN INSERT (user_id, event_time, event_id, action, payload)\nWHEN MATCHED THEN UPDATE SET action = S.action, payload = S.payload;\n```\n\n## Follow-up Questions\n\n- How to handle tombstones?\n- How to scale MERGE with sharded partitions?\n- How would you instrument observability for dedup latency?","diagram":"flowchart TD\n  PubSub[Pub/Sub] --> Dataflow[Dataflow]\n  Dataflow --> Staging[StagingBQ]\n  Staging --> Final[FinalBQ]\n  Staging -->|MERGE| Final","difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Google","Instacart"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T17:23:31.180Z","createdAt":"2026-01-12T17:23:31.180Z"}],"subChannels":["general"],"companies":["Adobe","Airbnb","Amazon","Anthropic","Apple","Bloomberg","Citadel","Cloudflare","Coinbase","Databricks","Discord","Goldman Sachs","Google","Hashicorp","Hugging Face","IBM","Instacart","Lyft","Meta","Microsoft","MongoDB","NVIDIA","Netflix","OpenAI","Oracle","PayPal","Plaid","Robinhood","Salesforce","Scale Ai","Slack","Snap","Snowflake","Square","Stripe","Tesla","Twitter","Two Sigma","Uber","Zoom"],"stats":{"total":54,"beginner":15,"intermediate":26,"advanced":13,"newThisWeek":38}}