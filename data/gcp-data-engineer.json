{"questions":[{"id":"q-1222","question":"Design a real-time ad-click analytics pipeline in GCP that ingests Pub/Sub events via Dataflow into BigQuery, while implementing 90-day TTL on PII data, exporting audits to Cloud Storage, and handling schema evolution, late data, dedup, and rollback. Provide concrete architecture, data models, and operational steps?","answer":"Two-tier streaming: a BigQuery hot table partitioned by ingestion_date with 90-day partition expiration for PII, plus a daily Cloud Storage Parquet archive for audits. Dataflow templates ingest Pub/Su","explanation":"## Why This Is Asked\nEvaluates real-world trade-offs in latency, cost, and governance for streaming pipelines on GCP.\n\n## Key Concepts\n- Pub/Sub -> Dataflow streaming\n- BigQuery partition expiration (90 days)\n- Parquet archive in Cloud Storage\n- Schema evolution with nested fields\n- Deduplication and late data handling\n- Rollback/versioned templates\n\n## Code Example\n```javascript\n// Pseudo Beam skeleton: dedupe by event_id and write to two sinks\n```\n\n## Follow-up Questions\n- How to validate TTL and archiving with tests?\n- What monitoring and alerting would you add for duty-cycle failures?","diagram":"flowchart TD\n  PubSub[Pub/Sub] --> Dataflow[Dataflow Template]\n  Dataflow --> BigQueryHot[BigQuery: Ingest (90d TTL)]\n  Dataflow --> Archive[Cloud Storage: Parquet Archive]\n  BigQueryHot --> Monitoring[Monitoring & Audits]","difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","NVIDIA","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T05:33:44.453Z","createdAt":"2026-01-13T05:33:44.453Z"},{"id":"q-931","question":"A GCP pipeline ingests 1 TB of JSON user activity daily from Pub/Sub into BigQuery via Dataflow. New fields appear over time; you must evolve the schema without downtime. What approach would you use for schema drift and nested fields, and outline concrete steps to implement it?","answer":"Use a two-layer scheme: a stable BigQuery table with known fields and a separate extra_json column to capture drift. The Dataflow job maps incoming JSON to the known schema; unknown fields are stored ","explanation":"## Why This Is Asked\nThis question probes practical handling of evolving data in a streaming pipeline, balancing query stability with schema growth.\n\n## Key Concepts\n- Schema evolution in BigQuery\n- Semi-structured data (JSON)\n- Dataflow transforms for parsing\n- Backward compatibility\n- Metadata and cataloging\n\n## Code Example\n```javascript\nfunction parseEvent(json) {\n  const data = JSON.parse(json);\n  const known = {\n    userId: data.user_id || null,\n    eventTime: data.event_time || null,\n    country: data.country || null\n  };\n  const extraKeys = Object.keys(data).filter(k => !(k in known));\n  const extra = JSON.stringify(extraKeys.reduce((acc, k) => { acc[k] = data[k]; return acc; }, {}));\n  return { ...known, extra_json: extra };\n}\n```\n\n## Follow-up Questions\n- How would you migrate existing data to the new table version with minimal downtime?\n- How would you monitor for schema drift and alert when new fields appear?","diagram":null,"difficulty":"beginner","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Discord","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T15:41:21.360Z","createdAt":"2026-01-12T15:41:21.361Z"},{"id":"q-962","question":"In a GCP streaming pipeline, Pub/Sub feeds millions of events into BigQuery via Dataflow. Out-of-order arrivals and duplicates occur. Design an idempotent sink using a staging table and a final partitioned table, leveraging insertId for dedup and a MERGE strategy to upsert into the final table. Outline concrete steps and trade-offs to implement?","answer":"Use a staging BigQuery table where Dataflow writes with insertId derived from a stable key (user_id + event_id + event_time). Periodically MERGE from staging into a final partitioned table, deduplicat","explanation":"## Why This Is Asked\n\nTests practical data quality, idempotent sinks, and operational design using Dataflow + BigQuery features.\n\n## Key Concepts\n\n- Idempotent sink\n- insertId dedup\n- staging vs final table\n- MERGE in BigQuery\n- allowed lateness and partitioning\n- quotas and monitoring\n\n## Code Example\n\n```sql\nMERGE INTO dataset.final AS F\nUSING dataset.staging AS S\nON F.user_id = S.user_id AND F.event_time = S.event_time\nWHEN NOT MATCHED THEN INSERT (user_id, event_time, event_id, action, payload)\nWHEN MATCHED THEN UPDATE SET action = S.action, payload = S.payload;\n```\n\n## Follow-up Questions\n\n- How to handle tombstones?\n- How to scale MERGE with sharded partitions?\n- How would you instrument observability for dedup latency?","diagram":"flowchart TD\n  PubSub[Pub/Sub] --> Dataflow[Dataflow]\n  Dataflow --> Staging[StagingBQ]\n  Staging --> Final[FinalBQ]\n  Staging -->|MERGE| Final","difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Google","Instacart"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T17:23:31.180Z","createdAt":"2026-01-12T17:23:31.180Z"}],"subChannels":["general"],"companies":["Citadel","Cloudflare","Databricks","Discord","Google","Instacart","NVIDIA","Snap","Uber"],"stats":{"total":3,"beginner":1,"intermediate":2,"advanced":0,"newThisWeek":3}}