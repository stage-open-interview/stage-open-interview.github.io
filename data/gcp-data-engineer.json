{"questions":[{"id":"q-1222","question":"Design a real-time ad-click analytics pipeline in GCP that ingests Pub/Sub events via Dataflow into BigQuery, while implementing 90-day TTL on PII data, exporting audits to Cloud Storage, and handling schema evolution, late data, dedup, and rollback. Provide concrete architecture, data models, and operational steps?","answer":"Two-tier streaming: a BigQuery hot table partitioned by ingestion_date with 90-day partition expiration for PII, plus a daily Cloud Storage Parquet archive for audits. Dataflow templates ingest Pub/Su","explanation":"## Why This Is Asked\nEvaluates real-world trade-offs in latency, cost, and governance for streaming pipelines on GCP.\n\n## Key Concepts\n- Pub/Sub -> Dataflow streaming\n- BigQuery partition expiration (90 days)\n- Parquet archive in Cloud Storage\n- Schema evolution with nested fields\n- Deduplication and late data handling\n- Rollback/versioned templates\n\n## Code Example\n```javascript\n// Pseudo Beam skeleton: dedupe by event_id and write to two sinks\n```\n\n## Follow-up Questions\n- How to validate TTL and archiving with tests?\n- What monitoring and alerting would you add for duty-cycle failures?","diagram":"flowchart TD\n  PubSub[Pub/Sub] --> Dataflow[Dataflow Template]\n  Dataflow --> BigQueryHot[BigQuery: Ingest (90d TTL)]\n  Dataflow --> Archive[Cloud Storage: Parquet Archive]\n  BigQueryHot --> Monitoring[Monitoring & Audits]","difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","NVIDIA","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T05:33:44.453Z","createdAt":"2026-01-13T05:33:44.453Z"},{"id":"q-1310","question":"You operate a streaming ingestion pipeline: Pub/Sub -> Dataflow -> BigQuery. Daily 50k events with fields user_id, event_type, amount (which can be string). Propose a simple ingestion-time data quality plan that validates JSON schema, coerces types, and routes invalid records to a dead-letter Pub/Sub topic without stopping the pipeline. Include testing with a small sample and how you monitor invalid counts?","answer":"Parse Pub/Sub messages with a simple Beam DoFn that validates required fields (user_id, event_type) and coerces amount to float. Valid records go to BigQuery; invalid ones emit to a dead-letter Pub/Su","explanation":"## Why This Is Asked\n\nAssess data quality at ingestion and the practicality of dead-letter routing in a real pipeline.\n\n## Key Concepts\n\n- Data quality at ingestion\n- Side outputs / dead-letter routing\n- Type coercion and validation\n- Beam/Dataflow to BigQuery integration\n- Monitoring\n\n## Code Example\n\n```javascript\n// Pseudo Beam DoFn for ingestion validation\nfunction processElement(element) {\n  let obj;\n  try { obj = JSON.parse(element); } catch (e) { return {error: 'invalid_json'}; }\n  if (!obj.user_id || !obj.event_type) return {error: 'missing_fields'};\n  const amount = parseFloat(obj.amount);\n  if (Number.isNaN(amount)) return {error: 'invalid_amount'};\n  return {user_id: obj.user_id, event_type: obj.event_type, amount: amount};\n}\n```\n\n## Follow-up Questions\n\n- How would you test dead-letter routing under load?\n- How would you tune retry/backoff for DLQ messages?\n","diagram":null,"difficulty":"beginner","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","PayPal","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T10:36:08.797Z","createdAt":"2026-01-13T10:36:08.797Z"},{"id":"q-1319","question":"Design a compliant streaming data platform in GCP for a fintech app that ingests transaction events from Pub/Sub through Dataflow into BigQuery. New fields may appear over time; PII must be detected and masked before analytics, while raw data is retained with restricted access. Outline the end-to-end architecture, data models, schema evolution strategy, PII handling, lineage, retention, and operational monitoring with concrete steps and trade-offs?","answer":"Implement a streaming Dataflow pipeline from Pub/Sub to a partitioned, clustered BigQuery table with schema registry in Data Catalog for forward/backward compatibility. Validate records, redact PII vi","explanation":"## Why This Is Asked\nReal-world streaming data governance, including schema evolution, PII handling, and lineage, is critical in fintech.\n\n## Key Concepts\n- Streaming ingestion with Pub/Sub and Dataflow\n- BigQuery partitioning and clustering\n- Schema registry and compatibility management\n- PII detection and masking (DLP)\n- Data lineage and metadata (Data Catalog)\n- Retention TTLs and access controls\n- Monitoring and alerting (Cloud Monitoring)\n\n## Code Example\n```python\ndef transform(record, registry_schema):\n    if not validate(record, registry_schema):\n        raise ValueError('Schema mismatch')\n    pii = ['acct_number', 'ssn']\n    masked = mask_pii(record, pii)\n    return masked\n```\n\n## Follow-up Questions\n- How would you test end-to-end TTL retention and schema evolution without downtime?\n- How would you handle cross-producer schema drift and multiple versions simultaneously?","diagram":"flowchart TD\n  A[Pub/Sub] --> B[Dataflow]\n  B --> C[BigQuery Table]\n  C --> D[Masked Analytics Views]\n  B --> E[Data Catalog (Lineage)]\n  C --> F[Raw Data with TTL Policy]","difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Plaid","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T11:31:53.811Z","createdAt":"2026-01-13T11:31:53.811Z"},{"id":"q-1389","question":"You receive streaming JSON events from Pub/Sub. Design a beginner-friendly GCP pipeline using Dataflow (Beam) to parse/flatten events, write to a partitioned BigQuery table (partition by event_date), and deduplicate by event_id. Late data allowed up to 6 hours. Outline architecture, data model, schema-change handling, and monitoring?","answer":"Design a Dataflow (Beam) streaming job that reads Pub/Sub JSON, flattens fields into a simple schema, writes to a partitioned BigQuery table by event_date, and deduplicates on event_id within a 5-minu","explanation":"## Why This Is Asked\n\nTests a practical, beginner-friendly data ingestion design on GCP, covering streaming, dedup, partitioning, late data handling, and basic schema evolution.\n\n## Key Concepts\n\n- Dataflow (Beam) streaming pipeline\n- Pub/Sub as the source\n- BigQuery partitioned tables and upserts\n- Deduplication by event_id\n- Lightweight schema-change and monitoring\n\n## Code Example\n\n```javascript\n// Pseudo-Beam-like snippet (conceptual)\nfunction parseEvent(raw) {\n  const obj = JSON.parse(raw);\n  return { event_id: obj.id, event_date: obj.date, payload: obj.payload };\n}\n```\n\n## Follow-up Questions\n\n- How would you test the dedup logic in Dataflow?\n- How would you monitor latency and late-data rates in this pipeline?","diagram":null,"difficulty":"beginner","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","Snap","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T14:50:08.738Z","createdAt":"2026-01-13T14:50:08.738Z"},{"id":"q-1415","question":"You are building a multi-source data platform where raw data arrives as Parquet in Google Cloud Storage, Avro via Pub/Sub, and JSON through a partner API; design a two-stage pipeline that normalizes all formats into a canonical Parquet dataset, ingested into a partitioned BigQuery table, with schema drift handling, data validation, and end-to-end lineage. Include how you would implement idempotent ingests, rollback, and cross-team access controls?","answer":"Implement a two-stage pipeline: stage1 normalizes Parquet(JSON/Avro) to a canonical Parquet schema, stage2 loads to a partitioned BigQuery table. Use a central schema registry and field-level nulls fo","explanation":"## Why This Is Asked\nDesign a multi-source ingestion pattern with schema drift handling, cross-format canonicalization, and governance; evaluate practical trade-offs.\n\n## Key Concepts\n- Multi-format normalization and canonical schema\n- Dataflow/Beam idempotent ingestion and exactly-once semantics\n- Schema evolution, drift handling, and validation\n- Data lineage, access controls, and rollback mechanisms\n\n## Code Example\n```javascript\nfunction toCanonical(rec){\n  // normalize fields, coerce types, assign stable id\n  return { id: rec.id ?? generateId(), ts: new Date(rec.ts), payload: rec.payload ?? null };\n}\n```\n\n## Follow-up Questions\n- How would you test schema drift and automate backwards-compatible changes?\n- How would you monitor latency and data skew across sources?","diagram":"flowchart TD\n  A[Ingest Parquet (GCS)] --> B[Normalize to Canonical Parquet]\n  C[Ingest Avro (Pub/Sub)] --> B\n  D[Ingest JSON (API)] --> B\n  B --> E[BigQuery (Partitioned)]\n  E --> F[Lineage & Auditing]","difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Snowflake","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T15:53:31.239Z","createdAt":"2026-01-13T15:53:31.239Z"},{"id":"q-1449","question":"Design a streaming enrichment pipeline: Pub/Sub → Dataflow → BigQuery, with an inline governance layer enforcing per-record rules (required fields, value ranges, cross-field consistency) at ingestion. Records failing rules go to a quarantine Cloud Storage bucket; good records load to a partitioned BigQuery table. Describe data model, rule engine approach, idempotence, backpressure, and testing/monitoring plan?","answer":"Implement a DoFn in Dataflow that validates each event against a schema (required fields, ranges, cross-field constraints). Use tagged outputs: main for valid records, quarantine for invalid. Write ma","explanation":"## Why This Is Asked\n\nAssesses ability to embed governance in streaming, handling bad data without blocking analytics, and operationalize quarantine and observability.\n\n## Key Concepts\n\n- Ingestion-time data quality checks\n- Dataflow ParDo with side outputs for quarantines\n- Idempotent BigQuery writes and record versioning\n- Backpressure and dead-letter handling\n- Observability and testing strategy\n\n## Code Example\n\n```python\nimport apache_beam as beam\n\nclass Govern(beam.DoFn):\n    def process(self, e):\n        if not e.get('user_id') or e.get('age', 0) < 0:\n            yield beam.pvalue.TaggedOutput('quarantine', e)\n        else:\n            yield e\n\nwith beam.Pipeline(...) as p:\n    raw = p | 'Read' >> beam.io.ReadFromPubSub(topic=...)  \n    main, bad = (raw | 'Govern' >> beam.ParDo(Govern()).with_outputs('quarantine', main='main'))\n    main | 'BQ' >> beam.io.WriteToBigQuery(table=..., schema=...)\n    bad | 'Quarantine' >> beam.io.WriteToText('gs://bucket/quarantine/')\n```\n\n## Follow-up Questions\n\n- How would you handle schema evolution in the governance layer without breaking downstream?\n- How to quantify and alert on data quality drift over time?\n- What tests would you add for end-to-end validation and rollback scenarios?","diagram":"flowchart TD\n  PubSub[Pub/Sub] --> Dataflow[Dataflow]\n  Dataflow --> BigQuery[BigQuery]\n  Dataflow --> Quarantine[GCS Quarantine]\n  Quarantine --> Audits[Audit Logs]\n  BigQuery --> Reports[BI Dashboards]","difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Instacart","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T17:45:34.896Z","createdAt":"2026-01-13T17:45:34.896Z"},{"id":"q-1473","question":"In a beginner friendly GCP data pipeline Pub/Sub -> Dataflow -> BigQuery design robust observability and recovery. Describe how to instrument Dataflow with metrics backlog throughput processed failed, implement a dead letter policy that dumps failed JSON messages to Cloud Storage, set retry backoff, and outline a replay path to re ing est from Cloud Storage. Include a concrete alert rule backlog throughput greater than 0.2 for 5 minutes and a brief example snippet?","answer":"Instrument Dataflow with metrics backlog, throughput, processed, and failed; emit structured logs to Cloud Logging. Route failed messages to a per topic Cloud Storage DLQ with 24h retention. Use expon","explanation":"## Why This Is Asked\nTests practical observability and recoverability for a streaming pipeline in fintech-like constraints, focusing on actionable metrics and a concrete DLQ/replay path.\n\n## Key Concepts\n- Dataflow metrics and structured logging\n- Dead-letter queues and DLQ retention\n- Retry/backoff strategies\n- Replay/re-ingestion pipelines and validation\n\n## Code Example\n```python\n# Pseudocode: DLQ handling in a Beam DoFn\nclass Process(DoFn):\n  def process(self, element):\n    try:\n      yield element\n    except Exception:\n      write_to_dlq(element)\n```\n\n## Follow-up Questions\n- How would you test the DLQ path with simulated failures?\n- What are the cost and complexity trade-offs of DLQ retention and replay?\n","diagram":"flowchart TD\n  A[Pub/Sub] --> B[Dataflow]\n  B --> C[BigQuery]\n  subgraph DLQ\n    D[Cloud Storage DLQ]\n  end\n  D --> E[Replay Job]","difficulty":"beginner","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","IBM"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T18:48:00.248Z","createdAt":"2026-01-13T18:48:00.248Z"},{"id":"q-1546","question":"In GCP, ingest daily Pub/Sub JSON events through Dataflow into BigQuery. Implement basic observability and reliability: 1) emit metrics for ingested, processed, and failed events; 2) export metrics to Cloud Monitoring and alert on failures for three consecutive checks; 3) deduplicate by event_id; 4) log audits to Cloud Storage. Outline architecture, data model, and concrete steps?","answer":"Implement a Dataflow (Apache Beam) pipeline that reads Pub/Sub JSON messages, parses them with a DoFn that emits metrics for ingested, processed, and failed events, then writes to BigQuery via streaming inserts with insertId set to event_id for deduplication. Export metrics to Cloud Monitoring and create alert policies for three consecutive failures. Log audit events to Cloud Storage for compliance tracking.","explanation":"## Why This Is Asked\nThis tests practical observability and data reliability skills in a real-world, beginner-friendly setting.\n\n## Key Concepts\n- Beam Metrics and Cloud Monitoring integration\n- Idempotent streaming writes with insertId\n- Lightweight auditing and error handling\n- Alerting on data quality metrics\n\n## Code Example\n```javascript\n// Pseudo-beam sketch (JavaScript/TypeScript syntax used for illustration)\nclass ParseEvent extends DoFn {\n  @ProcessElement\n  process(@Element json, output) {\n    const event = JSON.parse(json.image);\n    this.ingested.inc();\n    // ... validation and transformation\n    this.processed.inc();\n    output(event);\n  }\n  \n  @Setup\n  setup() {\n    this.ingested = Metrics.counter('pipeline', 'ingested_events');\n    this.processed = Metrics.counter('pipeline', 'processed_events');\n    this.failed = Metrics.counter('pipeline', 'failed_events');\n  }\n}\n```","diagram":"flowchart TD\n  PubSub[Pub/Sub] --> Dataflow[Dataflow / Beam]\n  Dataflow --> BigQuery[BigQuery]\n  Dataflow --> Monitor[Cloud Monitoring]\n  Dataflow --> Logs[Cloud Logging]\n  BigQuery --> Audit[Audit Logs (Cloud Storage)]","difficulty":"beginner","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Microsoft","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T06:31:05.145Z","createdAt":"2026-01-13T21:34:41.250Z"},{"id":"q-1581","question":"In Pub/Sub, daily JSON events contain PHI fields. Design a beginner-friendly GCP pipeline using Dataflow (Beam) to redact PHI with Cloud DLP before loading into BigQuery, and emit a separate audit log to Cloud Storage. Outline architecture, data model, and validation steps, including how you’d monitor masking failures?","answer":"Implement a Dataflow (Apache Beam) streaming job that reads JSON events from Pub/Sub, applies Cloud DLP masking to PHI fields (name, DOB, SSN), writes the redacted records to a BigQuery dataset, and emits audit logs to Cloud Storage. The pipeline includes error handling, dead-letter queues, and monitoring for masking failures.","explanation":"## Why This Is Asked\n\nTests practical data masking and data lake hygiene using familiar GCP tools. It ensures candidates can architect a compliant pipeline without overcomplicating a beginner task.\n\n## Key Concepts\n\n- Dataflow (Beam) streaming ingestion\n- Cloud DLP masking of PHI\n- BigQuery storage of redacted data\n- Cloud Storage audit logs and monitoring\n\n## Code Example\n\n```javascript\n// Pseudo Beam skeleton: read Pub/Sub, redact PHI with DLP, write to BigQuery, emit audit to GCS\n```\n\n## Follow-up Questions\n\n- How would you test masking coverage and false negatives in this pipeline?\n- What monitoring metrics would you track to ensure compliance?","diagram":null,"difficulty":"beginner","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T05:50:38.131Z","createdAt":"2026-01-13T22:48:46.197Z"},{"id":"q-1596","question":"Design a streaming pipeline: Pub/Sub → Dataflow (Beam) → BigQuery. Core: a dynamic data quality gate using a Firestore-stored policy engine that validates fields/types at ingest; invalid records serialized to Cloud Storage as JSONL with metadata; valid records load to BigQuery; publish quality metrics to Cloud Monitoring; handle drift/backpressure without downtime. Explain architecture, data model, and operational steps?","answer":"Design a streaming pipeline with Pub/Sub → Dataflow (Beam) → BigQuery, featuring a dynamic data quality gate using a Firestore-stored policy engine for field/type validation at ingest. Invalid records are serialized to Cloud Storage as JSONL with metadata, while valid records load to BigQuery. Quality metrics are published to Cloud Monitoring, and the system handles schema drift and backpressure without downtime through adaptive processing and buffering strategies.","explanation":"## Why This Is Asked\n\nTests ability to design a robust, observable streaming pipeline with runtime policy-based validation and safe failure handling at scale.\n\n## Key Concepts\n\n- Streaming ingestion with Pub/Sub and Dataflow (Beam)\n- Dynamic policy engine (Firestore) for field/type validation\n- Dead-letter/quarantine in Cloud Storage with metadata\n- Custom metrics in Cloud Monitoring for data quality\n- Schema drift resilience and backpressure handling\n\n## Code Example\n\n```python\n# Example: skeleton DoFn using Firestore policy to validate records\nclass ValidateWithPolicy(DoFn):\n    def setup(self):\n        self.firestore_client = firestore.Client()\n        self.policy_cache = {}\n    \n    def process(self, element):\n        # Load policy from Firestore with caching\n        policy = self.get_policy(element.get('schema_version'))\n        \n        # Validate fields/types against policy\n        if self.validate_record(element, policy):\n            yield element\n        else:\n            # Send invalid records to dead-letter sink\n            yield pvalue.TaggedOutput('invalid', {\n                'record': element,\n                'metadata': {\n                    'timestamp': datetime.utcnow().isoformat(),\n                    'validation_errors': self.get_validation_errors(element, policy)\n                }\n            })\n```","diagram":null,"difficulty":"advanced","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Hashicorp","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T05:34:10.860Z","createdAt":"2026-01-14T02:24:24.969Z"},{"id":"q-1722","question":"Design a cross-region, real-time analytics pipeline for a fintech on GCP. Ingest 100 GB/day of JSON events from Pub/Sub into BigQuery while ensuring per-tenant data isolation, a 90-day TTL on PII, and a rollback path. Show the architecture, data models, and operational steps for schema evolution, late data, dedup, and cost control?","answer":"Ingest via Pub/Sub -> Dataflow (Streaming) -> BigQuery (partitioned by day) with per-tenant views; store raw Parquet in Cloud Storage for audits; implement 90-day TTL by a scheduled Dataflow job that ","explanation":"## Why This Is Asked\nTests ability to design cross-region, scalable analytics with data governance, TTL, and schema evolution.\n\n## Key Concepts\n- Cross-region ingestion, storage\n- PII TTL and redaction\n- Tenant isolation with Authorized Views/IAM\n- Schema evolution and drift handling\n- Late data, dedup, and cost control\n\n## Code Example\n```javascript\n// Dataflow streaming pseudo-template\nconst events = pipeline\n  .apply('ReadPubSub', PubsubIO.readStrings().fromTopic('projects/..../topics/events'))\n  .apply('Parse', ParDo.of(new ParseFn()))\n  .apply('Deduplicate', /* ... */)\n  .apply('WriteBQ', BigQueryIO.writeTableRows().to('project.dataset.table$YYYYMMDD'))\n```\n\n## Follow-up Questions\n- How would you secure data in transit and at rest?\n- How would you monitor costs and adjust partitions?","diagram":null,"difficulty":"advanced","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Oracle","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T08:40:44.958Z","createdAt":"2026-01-14T08:40:44.959Z"},{"id":"q-1754","question":"Design a beginner-friendly GCP pipeline to load daily 2-5 GB of newline-delimited JSON logs from Cloud Storage into BigQuery. Keep only the latest 45 days in a partitioned table (partition by load_date, clustered by record_id). Ensure dedup by record_id, handle optional new fields with a permissive schema, allow late data up to 24 hours, and provide basic monitoring. Choose between Dataflow (Beam) or Cloud Functions, justify, and outline data model, transformations, error handling, and testing strategies?","answer":"Use Dataflow (Beam) in batch mode to read daily JSONL from Cloud Storage, parse and flatten, write to a staging table, then MERGE into a final partitioned table (load_date) clustered by record_id for ","explanation":"## Why This Is Asked\nThis question probes how a junior data engineer designs an end-to-end GCP pipeline for file-based ingestion, dedup, and schema evolution—a common real-world task that isn't covered by Pub/Sub-focused questions.\n\n## Key Concepts\n- Data ingestion from Cloud Storage using Dataflow (Beam) batch processing\n- BigQuery partitioning by load_date and clustering by record_id\n- Dedup via MERGE into final table; staging table usage\n- Permissive schema + schema evolution; late data handling (24h)\n- Monitoring: Cloud Monitoring dashboards and Logs-based metrics\n\n## Code Example\n```javascript\n// Example data map for a row\nfunction toRecord(line) {\n  const obj = JSON.parse(line);\n  return {\n    record_id: obj.id,\n    load_date: obj.date || new Date().toISOString().slice(0,10),\n    fieldA: obj.fieldA ?? null\n  };\n}\n```\n\n## Follow-up Questions\n- How would you test with synthetic JSON files and validate dedup?\n- How would you handle schema changes that remove fields?\n- What are cost trade-offs between staging vs final tables and clustering?","diagram":"flowchart TD\n  A[Cloud Storage JSONL] --> B[Dataflow Batch]\n  B --> C[Staging BigQuery]\n  C --> D[Final Table (load_date, cluster by record_id)]\n  D --> E[Monitoring & Alerts]","difficulty":"beginner","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","Lyft","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T09:41:21.218Z","createdAt":"2026-01-14T09:41:21.218Z"},{"id":"q-1787","question":"Design a data quality framework for a streaming pipeline ingesting 500 GB/day of JSON events from Pub/Sub into BigQuery. Specify in-flight schema validation, per-field checks (types, nullability, ranges), quarantining bad records, and automatic schema evolution with Data Catalog, plus end-to-end monitoring, alerting, and rollback procedures?","answer":"Leverage Dataflow (Beam) with a Python DoFn validating against an external Avro/JSON schema; route valid events to BigQuery staging and malformed ones to a quarantine table with error codes. Enforce per-field checks using Pydantic models for type validation, nullability constraints, and value ranges. Implement schema evolution through Data Catalog API calls to register new schema versions and tag datasets with version metadata. Use BigQuery streaming inserts with write disposition WRITE_APPEND and idempotent keys for deduplication. Monitor pipeline health via Cloud Monitoring metrics (validation success rate, quarantine volume, schema drift alerts) and set up alerting thresholds. For rollback, maintain schema version history in Data Catalog and implement a blue-green deployment strategy with pipeline rollback capability.","explanation":"## Why This Is Asked\nTests practical data quality controls in streaming pipelines on GCP, combining validation, governance, and recoverability.\n\n## Key Concepts\n- In-flight validation against a schema\n- Per-field checks: type, nullability, ranges\n- Quarantine/fallback handling for bad records\n- Data Catalog tagging for schema versions\n- Drift detection with BigQuery ML or SQL\n- Rollback and idempotent sinks\n\n## Code Example\n```python\n# Beam DoFn sketch for in-flight validation\nimport json\nfrom apache_beam import DoFn, pvalue\nfrom pydantic import BaseModel, ValidationError\n\nclass EventSchema(BaseModel):\n    user_id: str\n    timestamp: int\n    event_type: str\n    value: float = None\n\nclass ValidateRecordDoFn(DoFn):\n    def process(self, element, *args, **kwargs):\n        try:\n            obj = json.loads(element)\n            EventSchema(**obj)  # Pydantic validation\n            yield pvalue.TaggedOutput(\"valid\", element)\n        except (json.JSONDecodeError, ValidationError) as e:\n            yield pvalue.TaggedOutput(\"bad\", {\n                \"record\": element,\n                \"error\": str(e),\n                \"timestamp\": time.time()\n            })\n```","diagram":"flowchart TD\n  A[Pub/Sub] --> B[Dataflow (Beam)]\n  B --> C[BigQuery Staging]\n  B --> D[Quarantine]\n  C --> E[BigQuery Production]\n  F[Data Catalog Tags] --> C\n  F --> E","difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Goldman Sachs","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":["data quality framework","streaming pipeline","schema validation","per-field checks","quarantine table","schema evolution","data catalog","cloud monitoring","rollback procedures","bigquery streaming","dataflow beam","pydantic models"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2026-01-20T05:40:18.426Z","createdAt":"2026-01-14T10:49:14.020Z"},{"id":"q-1834","question":"Design a cross-tenant data governance pipeline on GCP for 1 TB/day of JSON user activity ingested from Pub/Sub into BigQuery in two regions. Enforce per-tenant privacy with field-level masking, auto-generate end-to-end data lineage with Data Catalog, support backward-compatible schema evolution, and export immutable audit logs to Cloud Storage. Include architecture, data models, rollback plan per-tenant (within 24h), and testing strategy?","answer":"Ingest via Pub/Sub -> Dataflow -> BigQuery in two regions; apply per-tenant masking for PII with DLP before load; tag lineage in Data Catalog for each field and table, and surface per-tenant access vi","explanation":"## Why This Is Asked\nThis question probes architectural rigor in multi-tenant governance, data lineage, privacy controls, and rollback strategy at scale on GCP. It emphasizes secure isolation, schema evolution, and auditable data flows.\n\n## Key Concepts\n- Data governance and lineage with Data Catalog\n- Per-tenant privacy masking with DLP or equivalent\n- Cross-region BigQuery data sharing via Authorized Views\n- Schema evolution with table templates and backward compatibility\n- Immutable audit logging and rollback workflows\n\n## Code Example\n```javascript\n// Example pseudocode illustrating a Dataflow pipeline segment wiring Pub/Sub -> BigQuery\nconst pipeline = ...\n.pipe(PubSubSource())\n  .transform(maskPIIPerTenant)\n  .to(BigQuerySink({region: 'us-central1'}))\n```\n\n## Follow-up Questions\n- How would you test tenant isolation failures?\n- How would you handle late-arriving data affecting lineage labels?","diagram":"flowchart TD\n  A[Pub/Sub] --> B[Dataflow]\n  B --> C[BigQuery (Region 1)]\n  B --> D[BigQuery (Region 2)]\n  C --> E[Data Catalog Tags]\n  D --> E\n  C --> F[Audit Logs Cloud Storage]\n  D --> F","difficulty":"advanced","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T13:16:15.203Z","createdAt":"2026-01-14T13:16:15.203Z"},{"id":"q-1869","question":"Design a GCP streaming pipeline for 2 TB/day of multi-tenant JSON events from Pub/Sub into BigQuery. Create a schema registry in Cloud Storage, validate with a Beam DoFn, quarantine nonconforming records to an invalid dataset, and apply per-tenant DLP masking. Enforce row-level access with BigQuery policies, capture lineage in Data Catalog, and provide rollback and testing plan?","answer":"Design a GCP streaming pipeline for 2 TB/day of multi-tenant JSON events from Pub/Sub into BigQuery. Build a schema registry in Cloud Storage, validate with a Beam DoFn, quarantine nonconforming recor","explanation":"## Why This Is Asked\nProbes multi-tenant data integrity, privacy, and governance at scale, including schema drift handling, masking, and lineage.\n\n## Key Concepts\n- Schema registry in Cloud Storage and Drift detection in Dataflow/Beam\n- Row-level security with BigQuery policies for per-tenant isolation\n- Per-tenant DLP masking and quarantine of invalid data\n- Data Catalog lineage and end-to-end observability\n\n## Code Example\n```javascript\n// Pseudo-Beam DoFn skeleton for schema validation\nclass ValidateSchema extends DoFn {\n  processElement(context) {\n    const record = context.element;\n    // load canonical schema from GCS\n    // validate fields, types, requireds\n    if (valid(record)) {\n      context.output(record);\n    } else {\n      context.output(NOT_VALID, record);\n    }\n  }\n}\n```\n\n## Follow-up Questions\n- How would you test schema drift in production without impacting traffic?\n- How would you implement rollback across tenants if a schema change breaks downstream queries?","diagram":null,"difficulty":"advanced","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Snap","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T14:56:51.986Z","createdAt":"2026-01-14T14:56:51.986Z"},{"id":"q-1906","question":"Ingest a daily batch of JSON records stored in Cloud Storage into BigQuery via a Dataflow (Beam) job. Build a beginner-friendly pipeline that validates a minimal schema (tenant_id, event_id, event_ts), filters out records missing required fields, and deduplicates by (tenant_id, event_id). Load valid data into a partitioned BigQuery table; write invalid records to a separate GCS errors bucket. Include basic health counters and a simple template?","answer":"Batch Dataflow (Beam) reads daily JSON lines from Cloud Storage, validates required fields (tenant_id, event_id, event_ts), filters invalid records, and deduplicates by (tenant_id, event_id). Valid ro","explanation":"## Why This Is Asked\n- Tests ability to implement a reliable, beginner-friendly batch ingestion with data quality checks, idempotent deduplication, and observable health signals in GCP.\n\n## Key Concepts\n- Dataflow/Beam batch pipelines, JSON parsing, and error routing\n- Simple schema validation and field presence checks\n- Deduplication strategy for (tenant_id, event_id)\n- BigQuery partitioning and error data landfill\n- Basic monitoring with counters\n\n## Code Example\n```javascript\n// Java-like pseudocode for Beam; real syntax depends on the SDK (Python/Java)\nclass ValidateAndKey(DoFn):\n  def process(self, elem):\n    required = ['tenant_id','event_id','event_ts']\n    if all(k in elem and elem[k] is not None for k in required):\n      yield ((elem['tenant_id'], elem['event_id']), elem)\n    else:\n      yield ('invalid', elem)\n```\n\n## Follow-up Questions\n- How would you scale this to 10x data and ensure low tail latency?\n- How would you adapt for evolving event schemas and backward compatibility?","diagram":"flowchart TD\n  A[Cloud Storage: daily.json] --> B[Dataflow (Beam)]\n  B --> C{Records Valid?}\n  C -->|Yes| D[BigQuery: partitioned table]\n  C -->|No| E[Errors: GCS bucket]","difficulty":"beginner","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T16:51:30.533Z","createdAt":"2026-01-14T16:51:30.533Z"},{"id":"q-1953","question":"Design a GCP data pipeline for a multinational SaaS product that ingests 200 GB/day of JSON telemetry from Pub/Sub into BigQuery across two regions. Requirements: multi-tenant isolation, automatic schema evolution, per-tenant TTL, data quality checks (schema conformance, nulls, duplicates), and an automated rollback path for schema changes. Include data model sketches, partitioning strategy, testing, and rollback steps?","answer":"Use per-tenant datasets in BigQuery and two-region Dataflow pipelines with a unified Avro/JSON schema registry. Ingest with event_id for dedup, partition by ingestion_date, TTL per tenant (30/90 days)","explanation":"## Why This Is Asked\nAssesses ability to design a compliant, scalable, two-region data pipeline with strong data quality and schema governance, plus a robust rollback path.\n\n## Key Concepts\n- Multi-tenant isolation with per-tenant datasets\n- Two-region active-active ingestion\n- Schema registry and automatic evolution\n- TTL/retention controls per tenant\n- Data quality: schema conformance, null checks, dedup\n- Rollback strategy: versioned schemas and reprocessing window\n- Metadata and lineage via Data Catalog\n\n## Code Example\n```python\n# Pseudocode: Beam DoFn for basic validation and dedup tagging\nclass ValidateEventDoFn(beam.DoFn):\n    def process(self, element):\n        data = json.loads(element)\n        if not data.get('tenant_id') or not data.get('event_id'):\n            yield beam.pvalue.TaggedOutput('invalid', data)\n            return\n        # type checks and masking placeholders\n        yield data\n```\n\n## Follow-up Questions\n- How would you test schema evolution across regions without downtime?\n- What metrics and alerts would you set for quality/rollback events?","diagram":null,"difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Hashicorp","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T18:50:45.651Z","createdAt":"2026-01-14T18:50:45.651Z"},{"id":"q-1997","question":"Ingest daily 1 TB JSON exports dumped into Cloud Storage by partner apps. Design a beginner-friendly GCP batch pipeline to load into BigQuery with per-tenant isolation (datasets), ingestion-date partitioning, and a simple schema-evolution strategy (new fields added as nullable columns). Include data-quality checks (nulls, types) and a rollback path to revert a day’s ingest within 24h. Outline architecture and testing?","answer":"Ingest 1 TB/day JSON exports from Cloud Storage with a batch Dataflow job. Use per-tenant datasets and ingestion-date partitioned tables. Flatten/parse JSON and map new fields to nullable columns to s","explanation":"## Why This Is Asked\nThis question tests practical batch data ingestion on GCP, focusing on beginner-friendly patterns that still show depth around tenant isolation, partitioning, and recoverability.\n\n## Key Concepts\n- Batch processing with Dataflow or Beam\n- Tenant isolation via separate datasets and IAMs\n- BigQuery ingestion-date partitioning\n- Lightweight schema evolution (nullable new fields)\n- Basic data quality checks (nullability, type validation)\n- Rollback strategy within 24h\n\n## Code Example\n\n```javascript\n// Pseudo Dataflow skeleton (illustrative)\nimport apache_beam as beam\nfrom apache_beam.options.pipeline_options import PipelineOptions\n# ... omitted for brevity\n```\n\n## Follow-up Questions\n- How would you test this pipeline end-to-end with failing records?\n- How would you scale tenant isolation when tenants grow into thousands?","diagram":null,"difficulty":"beginner","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Meta","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T20:31:34.562Z","createdAt":"2026-01-14T20:31:34.562Z"},{"id":"q-2073","question":"On GCP, design an observability-driven pipeline that detects schema drift and data-quality anomalies in near real-time for 1 TB/day of JSON events ingested from Pub/Sub into BigQuery across two regions. Include per-tenant lineage, automatic schema evolution, drift-triggered alerts, a self-healing rollback path, and export of audit trails to Cloud Storage. Provide architecture, data models, tests, and operational playbooks?","answer":"Ingest via Pub/Sub to Dataflow (Apache Beam) deployed across two regions, writing to region-partitioned BigQuery tables with tenant isolation. Implement drift detection by comparing incoming event schemas against a canonical schema stored in Data Catalog; on drift detection, trigger automated alerts via Cloud Monitoring and initiate schema evolution workflows. Maintain per-tenant lineage through Data Catalog tags and export audit trails to Cloud Storage for compliance. Include self-healing rollback capabilities using BigQuery time travel and versioned schema definitions.","explanation":"## Why This Is Asked\nThis probes practical observability, schema governance, drift handling, and rollback in a multi-region data pipeline, integrating Dataflow, BigQuery, Data Catalog, and Cloud Storage audits.\n\n## Key Concepts\n- Streaming ingestion and schema drift detection\n- Data Catalog lineage and tenant isolation\n- Schema evolution and rollback mechanics\n- Alerting and self-healing strategies\n\n## Code Example\n```javascript\n// Pseudocode drift detector skeleton\nfunction detectDrift(record, catalog) {\n  // Compare incoming fields with catalog schema\n  return driftDetected;\n}\n```\n\n## Follow","diagram":null,"difficulty":"advanced","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Databricks","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T05:33:18.450Z","createdAt":"2026-01-14T23:26:10.805Z"},{"id":"q-2153","question":"Design a cost-aware multi-tenant GCP data pipeline for streaming telemetry that ingests 150 GB/day of JSON from Pub/Sub into BigQuery across two regions. Enforce per-tenant access with BigQuery row level security and per-tenant dataset versioning; support backward compatible schema evolution; and export versioned snapshots to Cloud Storage. Include data models, partitioning, testing, rollback, and synthetic-tenant generation?","answer":"Ingest via Dataflow from Pub/Sub to two regional BigQuery datasets. Use ingestion-time partitioned tables, cluster by tenant_id, and apply row-level security so each tenant sees only their data. Keep ","explanation":"## Why This Is Asked\nTests real-world multi-tenant data governance, per-tenant isolation, and cost-conscious architecture across regions. It also covers schema evolution, rollback, and data export for audits.\n\n## Key Concepts\n- BigQuery row-level security and per-tenant access\n- Ingestion-time partitioning and tenant clustering\n- Dataset/versioning strategy and backward-compatible schema evolution\n- Cross-region replication and cost controls\n- Synthetic tenant generation and rollback testing\n\n## Code Example\n```sql\n-- example RLAC for tenant isolation (conceptual)\nCREATE ROW ACCESS POLICY tenant_rlac\nON `project.dataset.table`\nUSING (tenant_id = SESSION_USER())\nWITH CHECK (tenant_id = SESSION_USER());\n```\n\n## Follow-up Questions\n- How would you handle schema drift across regions and tenants?\n- What monitoring would you add to detect cost spikes from multi-tenant queries?","diagram":"flowchart TD\n  A[Pub/Sub] --> B[Dataflow]\n  B --> C[BigQuery (Region 1, Tenant data)]\n  B --> D[BigQuery (Region 2, Tenant data)]\n  C --> E[RLAC policies per tenant]\n  D --> E\n  C --> F[Snapshots to Cloud Storage (versioned)]\n  D --> F","difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["NVIDIA","Oracle","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T05:35:12.126Z","createdAt":"2026-01-15T05:35:12.126Z"},{"id":"q-2227","question":"Design a real-time data ingestion pipeline on GCP for a game analytics platform: 60 GB/day of JSON events from Pub/Sub into BigQuery across two regions. Enforce per-tenant isolation with field-level masking, automatic schema evolution, and a per-tenant canary rollout with a controlled rollback path. Include data models, testing strategy, lineage via Data Catalog, and cost controls. Provide a concrete implementation plan?","answer":"Streaming pipeline: Pub/Sub → Dataflow → BigQuery across two regions with per-tenant isolation and field-level masking. Enable tenant-scoped schema evolution via versioned schemas in Data Catalog, plu","explanation":"## Why This Is Asked\nEvaluates real-time ingestion, tenant isolation, and controlled schema changes with governance across regions.\n\n## Key Concepts\n- Real-time streaming with Pub/Sub and Dataflow\n- Per-tenant masking and isolation\n- Canary deployments and rollback automation\n- Data Catalog lineage and versioned schemas\n\n## Code Example\n```javascript\n// Skeleton: Beam transform applying masking using a per-tenant function\nimport apache_beam as beam\nclass MaskPerTenant(beam.DoFn):\n  def process(self, element, *args, **kwargs):\n    # tenant_id = element['tenant_id']\n    # mask fields per tenant\n    return [element]\n```\n\n## Follow-up Questions\n- How would you implement automatic backfill for late data?\n- How would you monitor data quality and roll back a breaking change?","diagram":"flowchart TD\n  A[Pub/Sub: Events] --> B[Dataflow: Transform]\n  B --> C[BigQuery: Region 1]\n  B --> D[BigQuery: Region 2]\n  C --> E[Data Catalog: Lineage]\n  D --> E","difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Meta","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T08:43:19.512Z","createdAt":"2026-01-15T08:43:19.512Z"},{"id":"q-2364","question":"Design a cross-tenant data exchange on GCP: ingest 1 TB/day of JSON telemetry from Pub/Sub into two BigQuery regions. Implement per-tenant isolation via dataset-level access controls and masked views, and use Data Catalog policy tags to enforce visibility. Model a pragmatic two-layer architecture: raw + curated; ensure end-to-end lineage; support controlled data sharing via explicit contracts. Provide a 24h per-tenant rollback plan and testing strategy?","answer":"Route 1 TB/day via Dataflow to BigQuery in Region A and Region B. Implement per-tenant isolation with dataset-level access and masked views; enforce visibility with Data Catalog policy tags. Build raw","explanation":"## Why This Is Asked\nExplores data-contract-driven governance and cross-region sharing, a real-world need for large, multi-tenant analytics.\n\n## Key Concepts\n- Data contracts in Data Catalog\n- Per-tenant isolation via views/policies\n- End-to-end lineage\n- Two-region replication and cost-aware design\n- Rollback strategy with time travel and canaries\n\n## Code Example\n```python\n# pseudo-contract validator\ndef validate_contract(contract):\n  required_fields = {\"tenant_id\",\"schema\"}\n  return required_fields.issubset(contract.keys())\n```\n\n## Follow-up Questions\n- How would you test contract violations in CI/CD?\n- How would you handle schema evolution across tenants without breaking sharing contracts?","diagram":null,"difficulty":"advanced","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","NVIDIA","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T14:54:28.599Z","createdAt":"2026-01-15T14:54:28.599Z"},{"id":"q-2410","question":"Design an end-to-end GCP data ingestion pipeline for 1-2 TB/day of JSON telemetry streamed from Pub/Sub into BigQuery with per-tenant isolation and strict data residency policies. The solution must mask PII per tenant at ingestion (field-level), use CMEK, and employ DLP where needed. Provide two-region architecture, automatic schema evolution, late data handling, and a rollback/backfill plan, plus end-to-end data lineage in Data Catalog and audit exports. Include architecture, data model, testing, and runbook details?","answer":"Pub/Sub → Dataflow streaming; per-tenant isolation via separate datasets and partitioned tables; field-level masking at parse time driven by a central policy store; CMEK encryption on BigQuery; DLP fo","explanation":"## Why This Is Asked\nTests ability to design a privacy-aware, multi-region data pipeline with governance and testing in a realistic setting.\n\n## Key Concepts\n- Pub/Sub streaming\n- Multi-tenant masking and isolation\n- CMEK and DLP integration\n- Data Catalog lineage\n- Schema evolution and backfills\n- Two-region resilience and audits\n\n## Code Example\n```javascript\n// Dataflow DoFn: mask PII based on tenant policy\nclass MaskPII {\n  process(record, ctx) {\n    const policy = getPolicy(ctx.tenantId);\n    return maskRecord(record, policy);\n  }\n}\n```\n\n## Follow-up Questions\n- How would you test end-to-end lineage accuracy? \n- How would you handle policy updates without downtime?","diagram":null,"difficulty":"advanced","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Hugging Face","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T16:59:12.833Z","createdAt":"2026-01-15T16:59:12.833Z"},{"id":"q-2523","question":"On GCP, design a data mesh for 3 domains (Sales, Product, Ops) where each domain owns its BigQuery data products, publishes a standard schema, and uses Data Catalog and IAM for cross-domain governance. Include an automated lineage from source events to data products, a policy-driven access layer with per-domain permissions, and a testing/rollback plan for schema drift. Provide concrete data models and workflow?","answer":"Implement a data mesh architecture with domain-owned BigQuery datasets for Sales, Product, and Operations. Each domain manages its own data products using standardized schemas, while Data Catalog provides centralized metadata management with tag templates for automated lineage tracking. Establish per-domain IAM roles and Authorized Views to enable secure cross-domain data access. Deploy CI/CD pipelines with schema validation tests to detect and prevent drift, with automated rollback capabilities for schema changes.","explanation":"## Why This Is Asked\nThis evaluates practical implementation of data mesh principles, governance frameworks, and lineage automation on Google Cloud Platform.\n\n## Key Concepts\n- Domain-driven data ownership with BigQuery datasets\n- Data Catalog metadata management and automated lineage\n- IAM-based access control with Authorized Views\n- CI/CD pipeline integration for schema drift prevention\n- Cross-domain governance with policy-driven permissions\n\n## Code Example\n```javascript\nfunction validateSchema(expectedSchema, actualSchema) {\n  const expected = JSON.stringify(expectedSchema);\n  const actual = JSON.stringify(actualSchema);\n  return expected === actual;\n}\n```","diagram":"flowchart TD\n  A[Source Events] --> B[Dataflow/Beam] \n  B --> C[Domain Datasets] \n  C --> D[Data Catalog & Lineage] \n  D --> E[BI & ML Serving]","difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Netflix","Snowflake","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T05:46:15.342Z","createdAt":"2026-01-15T21:34:05.475Z"},{"id":"q-2602","question":"Design a multi-tenant, streaming data platform on GCP for a financial app that ingests 1 TB/day of JSON events via Pub/Sub, processes them with Apache Beam on Dataflow, and writes per-tenant data products to BigQuery across three regions. Explain per-tenant isolation, automatic schema evolution, late data handling, data quality checks, rollback paths, cross-region replication, Data Catalog lineage, and testing strategy?","answer":"Per-tenant isolation: distinct Pub/Sub topics and BigQuery datasets per tenant; Dataflow pipelines with exactly-once writes; centralized schema registry with Data Catalog for evolution; QC checks (sch","explanation":"## Why This Is Asked\nEvaluates real-world ability to design a scalable, multi-tenant streaming pipeline on GCP with strict data governance, schema evolution, cross-region durability, and robust testing.\n\n## Key Concepts\n- Multi-tenant isolation across Pub/Sub and BigQuery\n- Dataflow (Beam) streaming pipelines with exactly-once semantics\n- Centralized schema evolution using a registry and Data Catalog lineage\n- Data quality checks: schema conformance, dedup, nulls\n- Late data handling and watermarking\n- Cross-region replication and rollback/backfill strategies\n- Testing: end-to-end, drift, canaries\n\n## Code Example\n```python\nimport apache_beam as beam\nfrom apache_beam.options.pipeline_options import PipelineOptions\nimport json\n\nclass ParseJsonDoFn(beam.DoFn):\n    def process(self, element):\n        yield json.loads(element)\n\noptions = PipelineOptions(streaming=True)\nwith beam.Pipeline(options=options) as p:\n    (p\n     | 'ReadPubSub' >> beam.io.ReadFromPubSub(subscription='projects/PROJ/subscriptions/tenant-sub')\n     | 'Parse' >> beam.ParDo(ParseJsonDoFn())\n     | 'ToBQ' >> beam.io.WriteToBigQuery(\n         table='tenant_ds.tenant_table',\n         dataset='tenant_ds',\n         project='PROJECT',\n         write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND,\n         create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED))\n```\n\n## Follow-up Questions\n- How would you implement idempotent writes to BigQuery across regions?\n- What monitoring and alerting would you add for schema drift per tenant?","diagram":"flowchart TD\n  PubSub[Pub/Sub per-tenant topics]\n  Dataflow[Dataflow streaming pipelines]\n  BigQuery[BigQuery cross-region tables]\n  Catalog[Data Catalog lineage]\n  Access[Per-tenant IAM]\n  QC[Quality checks]\n  Rollback[Rollback plan]\n  \n  PubSub --> Dataflow\n  Dataflow --> BigQuery\n  BigQuery --> Catalog\n  Catalog --> Access\n  QC --> Rollback","difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Microsoft","Robinhood","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T02:32:08.185Z","createdAt":"2026-01-16T02:32:08.185Z"},{"id":"q-2744","question":"Design a multi-tenant data pipeline on GCP that ingests 50–100 GB/day of JSON events from Pub/Sub into BigQuery across two regions. Implement per-tenant isolation with dedicated datasets and topics, a robust data quality observability layer that validates schema conformance, nulls, duplicates, and timeliness, and real-time alerts. Include an automated remediation path (schema bumps and selective re-ingestion) and a clear rollback strategy using time travel. Provide architecture, data models, partitioning, testing, and rollback steps?","answer":"Per-tenant isolation via per-tenant BigQuery datasets and Pub/Sub topics, a Dataflow streaming pipeline to both regions, and a concurrent Data Quality pipeline that validates schema conformance, null ","explanation":"## Why This Is Asked\n\nAssess capability to design scalable, multi-region, multi-tenant data pipelines with robust data quality and rollback strategies on GCP.\n\n## Key Concepts\n\n- Per-tenant isolation (datasets, topics) and multi-region BigQuery\n- Streaming ingestion with Dataflow\n- Data quality observability: schema conformance, nulls, duplicates, timeliness\n- Real-time alerts via Cloud Monitoring\n- Automated remediation: schema bumps + re-ingestion\n- Rollback: point-in-time re-ingestion windows; BigQuery time travel\n\n## Code Example\n\n```javascript\n// Pseudo Dataflow DoFn for schema validation\nfunction validate(record, schema) {\n  // check required fields and types\n}\n```\n\n## Follow-up Questions\n\n- How would you test cross-region schema drift and lineage?\n- What are your failure-handling strategies for late data and re-ingestion costs?","diagram":"flowchart TD\n  A[Pub/Sub Ingest] --> B[Dataflow Streaming]\n  B --> C1[BigQuery Region 1]\n  B --> C2[BigQuery Region 2]\n  D[Data Quality] --> E[Cloud Monitoring Alerts]\n  F[Remediation] --> G[Re-ingest & Schema Upgrade]","difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Google","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T09:52:41.399Z","createdAt":"2026-01-16T09:52:41.399Z"},{"id":"q-2755","question":"Design a two-region, cross-tenant data lakehouse on GCP for 2 TB/day of JSON events ingested via Pub/Sub, storing raw Parquet in regional Cloud Storage and tenant-scoped tables in BigQuery; include per-tenant isolation, automatic schema evolution, late-arrival handling, TTL, and end-to-end lineage with Data Catalog. Provide data model, workflows, and rollback strategy?","answer":"Streaming 2 TB/day of JSON via Pub/Sub; Dataflow writes per-tenant Parquet in regional Cloud Storage and updates tenant-scoped BigQuery partitions. Enforce isolation with IAM and row-level filters; ma","explanation":"## Why This Is Asked\nTests cross-region data lakehouse design with tenant isolation, schema evolution, data governance, and rollback.\n\n## Key Concepts\n- GCP data lakehouse pattern, Pub/Sub, Dataflow, BigQuery, Cloud Storage\n- Tenant isolation, schema drift, DLP masking, Data Catalog lineage\n- Canary rollbacks, TTL, lifecycle management\n\n## Code Example\n```javascript\n// Pseudocode: Dataflow streaming template wiring Pub/Sub to Parquet+BigQuery\n```\n\n## Follow-up Questions\n- How would you validate latency SLAs across regions?\n- How would you handle late data and out-of-order events?","diagram":"flowchart TD\n  PubSub --> Dataflow\n  Dataflow --> CloudStorageParquet[Parquet in regional buckets]\n  Dataflow --> BigQueryTables[Tenant BigQuery partitions]\n  BigQueryTables --> DataCatalog[Lineage in Data Catalog]","difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T10:42:34.046Z","createdAt":"2026-01-16T10:42:34.046Z"},{"id":"q-2855","question":"Design a real-time cross-tenant data pipeline on GCP ingesting 1-2 TB/day of JSON events from Pub/Sub into two regions. Implement per-tenant privacy via field-level masking and tokenization using DLP and Cloud KMS, enforce data residency rules, provide end-to-end lineage with Data Catalog, support forward/backward schema evolution, and build a testing strategy with synthetic tenants. Include a rollback plan for privacy policy changes within 24 hours?","answer":"A robust answer identifies Pub/Sub -> Dataflow with per-tenant masking using DLP and Cloud KMS, writes to regional BigQuery tables, ensures tenant-scoped lineage in Data Catalog, implements schema evo","explanation":"## Why This Is Asked\n\nEvaluates capability to design a scalable, privacy-conscious streaming pipeline across regions, with governance and rollback considerations.\n\n## Key Concepts\n\n- Real-time ingestion with Pub/Sub and Dataflow\n- Per-tenant privacy via DLP masking and Cloud KMS\n- Data residency controls and multi-region writes\n- End-to-end lineage in Data Catalog\n- Schema evolution and backward/forward compatibility\n- Synthetic data testing and 24-hour rollback planning\n\n## Code Example\n\n```javascript\n// Pseudo Dataflow sketch for masking a tenant field using DLP\n```\n\n## Follow-up Questions\n\n- How would you test privacy-policy rollbacks across tenants during peak load?\n- What monitoring would you add to detect masking failures or schema drift?","diagram":"flowchart TD\n  Pub[Pub/Sub Ingest] --> DF[Dataflow Masking]\n  DF --> BQ[BigQuery Regional Tables]\n  BQ --> Catalog[Data Catalog Lineage]\n  Catalog --> BI[BI Tools]","difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","NVIDIA","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T14:49:27.353Z","createdAt":"2026-01-16T14:49:27.353Z"},{"id":"q-2903","question":"Design a real-time cross-tenant telemetry pipeline on GCP for 5 TB/day of JSON events ingested via Pub/Sub and stored into per-tenant BigQuery tables across two regions. Requirements: strict tenant isolation, field-level privacy masking at ingestion (PII redaction with DLP), auto schema evolution with backward compatibility, late-arrival handling, per-tenant TTL retention, and immutable audit logs exported to Cloud Storage. Include end-to-end lineage via Data Catalog, rollback strategy, testing plan, and concrete data model sketches, Dataflow templates, and failure modes?","answer":"Use Pub/Sub → Dataflow Flex Template with per-tenant routing to BigQuery tables. Ingest masks PII using DLP in a streaming DoFn; write to partitioned, tenant-scoped BigQuery tables to ensure isolation","explanation":"## Why This Is Asked\nTests ability to design a scalable, compliant real-time pipeline across regions with strict tenant isolation, privacy masking, TTL, and lineage.\n\n## Key Concepts\n- Cross-region streaming data pipelines\n- Per-tenant isolation strategies (datasets, tables, or partitioning)\n- Ingestion-time privacy masking (DLP integration)\n- Schema evolution and backward compatibility\n- Late-arrival handling and watermarking\n- TTL and lifecycle policies\n- Data Catalog lineage and governance\n\n## Code Example\n```python\n# Pseudo Beam DoFn for PII masking at ingest\nclass MaskPIIFn(beam.DoFn):\n    def process(self, element):\n        # call to DLP or internal masking logic\n        yield masked_element\n```\n\n## Follow-up Questions\n- How would you test for schema drift and ensure backward compatibility?\n- What are the trade-offs between per-tenant datasets vs shared dataset with tenant_id partitioning?","diagram":"flowchart TD\n  PubSub[Pub/Sub Ingest] --> Dataflow[Dataflow Ingest]\n  Dataflow --> BigQuery[BigQuery Tenant Tables]\n  Dataflow --> DLP[DLP Masking]\n  BigQuery --> Catalog[Data Catalog Lineage]\n  Dataflow --> Audit[Cloud Storage Audit Logs]","difficulty":"advanced","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Netflix","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T16:54:37.246Z","createdAt":"2026-01-16T16:54:37.247Z"},{"id":"q-2958","question":"Design a real-time, cross-tenant analytics pipeline on GCP that ingests 4 TB/day of JSON clickstream from Pub/Sub across three regions, with per-tenant isolation: raw data stored regionally as Parquet, aggregated BI-ready tables in BigQuery; enforce field-level masking with DLP; automatic schema drift handling using Data Catalog and Dataplex; maintain end-to-end lineage; and implement a rollback plan for tenant-specific schema changes within 24 hours, including testing and failover playbooks?","answer":"Architect a 3-region cross-tenant streaming pipeline: 4 TB/day of JSON clickstream from Pub/Sub, Dataflow processing with per-tenant masking via DLP; regional raw Parquet in Cloud Storage; per-tenant ","explanation":"## Why This Is Asked\nAssesses ability to design real-time, multi-region data pipelines with privacy, governance, and operational resilience.\n\n## Key Concepts\n- Multi-region streaming and data locality\n- Tenant isolation and masking\n- Schema drift management with Data Catalog/Dataplex\n- Data lineage and rollback workflows\n- Canary testing and runbooks\n\n## Code Example\n```python\n# Minimal Apache Beam skeleton for Pub/Sub -> BigQuery with masking\nimport apache_beam as beam\nclass MaskFn(beam.DoFn):\n  def process(self, element):\n    # pseudocode: mask PII fields\n    return masked\nwith beam.Pipeline(options=... ) as p:\n  (p\n   | beam.io.ReadFromPubSub(subscription='projects/…/subscriptions/…')\n   | beam.ParDo(MaskFn())\n   | beam.io.WriteToBigQuery('project:dataset.table', write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE)\n  )\n```\n\n## Follow-up Questions\n- How would you implement per-tenant rollback isolation and testing without impacting other tenants?\n- What are your failover and data consistency guarantees across regions during a rollback?","diagram":"flowchart TD\n  P[Pub/Sub Ingest] --> D[Dataflow Processing]\n  D --> R[Regional Raw Parquet (Cloud Storage)]\n  D --> Q[BigQuery Tenant Tables]\n  Q --> L[Data Catalog / Dataplex Lineage]\n  P --> M[PII Masking with DLP]","difficulty":"advanced","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["PayPal","Tesla","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T18:59:53.849Z","createdAt":"2026-01-16T18:59:53.849Z"},{"id":"q-2982","question":"Design a per-tenant data-sharing microservice on GCP that ingests 100 GB/day of JSON activity from Pub/Sub into BigQuery, then serves tenant-scoped datasets to external partners via a Cloud Run API. Include per-tenant isolation with partitioned BigQuery tables, authorized views for external access, IAM/Endpoints-based OAuth, audit trails to Cloud Logging, and automatic schema drift handling with backward-compatible updates and rollback?","answer":"Architect a Pub/Sub -> Dataflow -> BigQuery pipeline with separate partitioned tables per tenant, and a Cloud Run API exposing tenant-scoped datasets via authorized views. Enforce per-tenant IAM roles","explanation":"## Why This Is Asked\nTests real-world multi-tenant data sharing, strict isolation, and governance. It also probes integration of Pub/Sub, BigQuery partitioning, Cloud Run APIs, Endpoints/IAM auth, and auditable logs, plus a practical approach to schema drift.\n\n## Key Concepts\n- Multi-tenant isolation via partitioned BigQuery tables\n- Authorized views for partner access\n- Cloud Endpoints with OAuth/IAM for API security\n- Immutable audit trails in Cloud Logging\n- Schema evolution with rollback semantics\n\n## Code Example\n```yaml\n# Pseudo-architecture snippet\npipeline:\n  - source: PubSub\n  - transform: Dataflow\n  - sink: BigQuery (tenant-partitioned)\napi:\n  - runtime: Cloud Run\n  - auth: Endpoints + IAM\nlogging:\n  - sink: Cloud Logging (audit)\n```\n\n## Follow-up Questions\n- How would you test schema drift transitions across tenants without impacting production?\n- How would you handle backfill for a new tenant with historical data?","diagram":"flowchart TD\n  A[Pub/Sub] --> B[Dataflow]\n  B --> C[BigQuery per-tenant partitions]\n  C --> D[Cloud Run API for partners]\n  D --> E[External Partners]\n  F[Audit Logs] --> G[Cloud Logging]","difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Instacart","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T19:46:06.972Z","createdAt":"2026-01-16T19:46:06.972Z"},{"id":"q-2999","question":"On GCP, design a multi-tenant streaming validation and quality framework for 10-20B events/day from Pub/Sub into BigQuery across regions. Each tenant has its own schema and masking rules. Describe the end-to-end data model, per-tenant schema registry, per-tenant masking/quality rules, lineage via Data Catalog, and a canary rollback plan to the last-good schema within 6 hours?","answer":"Design a multi-tenant streaming validation framework: route Pub/Sub messages by tenant via Dataflow templates; apply per-tenant masking and schema checks; write validated data to tenant-specific BigQu","explanation":"## Why This Is Asked\nEvaluates ability to design scalable, tenant-isolated streaming pipelines with per-tenant rules, robust quality checks, and rapid rollback.\n\n## Key Concepts\n- Per-tenant schema registry and masking rules\n- Streaming validation with Dataflow templates\n- Data Catalog lineage integration\n- Canary rollouts and fast rollback\n\n## Code Example\n```python\ndef mask_record(record, tenant_policy):\n    for field, rule in tenant_policy.items():\n        if field in record:\n            record[field] = apply_mask(record[field], rule)\n    return record\n```\n\n## Follow-up Questions\n- How would you test schema drift and trigger rollback automatically?\n- How would you scale the schema registry as tenants scale from 100 to 1000+?","diagram":"flowchart TD\n  PubSub[Pub/Sub Ingest] --> DF[Dataflow Validation]\n  DF --> BIQ[BigQuery Tenant Tables]\n  BIQ --> DAC[Data Catalog Lineage]\n  DF --> CAN[Canary Controller]\n  CAN --> GBK[Last-Good Schema Rollback]\n  BIQ --> LOG[Audit Logs in Cloud Storage]","difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T20:39:47.413Z","createdAt":"2026-01-16T20:39:47.414Z"},{"id":"q-3093","question":"Design a batch Dataflow pipeline that reads per-tenant JSON logs from Cloud Storage, validates against a schema.json in GCS, flattens nested fields, and writes to a BigQuery table partitioned by ingest_date with event_id dedup. Allow late-arriving files up to 24 hours via re-ingest, and emit an audit log to Cloud Storage plus a quick observability summary to Pub/Sub?","answer":"Build a batch Dataflow pipeline that reads per-tenant JSON logs from Cloud Storage, validates them against a schema.json file stored in GCS, flattens nested fields, and writes to a BigQuery table partitioned by ingest_date. Implement event_id-based deduplication to ensure idempotent processing, support a 24-hour late-arrival window with re-ingest capability, and generate audit logs to Cloud Storage with observability summaries published to Pub/Sub.","explanation":"## Why This Is Asked\nTests ability to design a scalable, tenant-aware batch pipeline that uses Dataflow, enforces schema validation at ingest, and handles late arrivals without complex orchestration.\n\n## Key Concepts\n- Dataflow batch pipelines and PTransforms\n- JSON schema validation stored in GCS\n- BigQuery partitioning by ingest_date and per-tenant isolation\n- Idempotent deduplication by event_id\n- Late-arrival handling with re-ingest and retries\n- Audit logs into Cloud Storage and Pub/Sub observability\n\n## Code Example\n```javascript\n// Pseudo Dataflow-like sketch: validate JSON against schema and flatten\nfunction processTenantLogs(pipeline) {\n  pipeline\n    .apply('ReadTenantLogs', GCSIO.read().from('gs://tenant-logs/*/*.json'))\n    .apply('ValidateSchema', ParDo.of(new SchemaValidator('gs://schemas/schema.json')))\n    .apply('FlattenFields', ParDo.of(new FieldFlattener()))\n    .apply('DeduplicateEvents', Distinct.create().withField('event_id'))\n    .apply('WriteToBigQuery', BigQueryIO.writeTable()\n      .to('project.dataset.events')\n      .withSchema(eventSchema)\n      .withWriteDisposition(BigQueryIO.Write.WriteDisposition.WRITE_APPEND)\n      .withCreateDisposition(BigQueryIO.Write.CreateDisposition.CREATE_IF_NEEDED)\n      .withTimePartitioning('DAY', 'ingest_date'))\n    .apply('GenerateAudit', ParDo.of(new AuditLogger('gs://audit-logs/')));\n}\n```","diagram":"flowchart TD\n  A[GCS: Daily JSON logs (tenant_id present)] --> B[Dataflow (Beam) - Parse & Flatten]\n  B --> C[BigQuery: dataset.table partitioned by ingest_date]\n  B --> D[BigQuery: deduplicate by event_id]\n  E[Schema: JSON schema file in GCS]\n  A --> B\n  E --> B\n  C --> F[Audit logs: Cloud Storage]","difficulty":"beginner","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","Slack","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T05:00:34.945Z","createdAt":"2026-01-17T02:16:35.301Z"},{"id":"q-3251","question":"You operate a multi-tenant analytics service on GCP with 50 tenants. 5 GB/day of JSON events arrive via Pub/Sub. Design a beginner-friendly pipeline that ingests events with Dataflow, stores them in a single BigQuery table partitioned by ingestion_date, and includes tenant_id. Enforce per-tenant access using BigQuery authorized views (or per-tenant filters), support additive schema evolution, deduplicate by event_id, handle late data up to 1 hour, and capture end-to-end lineage in Data Catalog. Provide architecture sketch and a minimal test plan?","answer":"Ingest Pub/Sub JSON with Dataflow into a single BigQuery table partitioned by ingestion_date, including tenant_id. Enforce per-tenant access via authorized views filtering on tenant_id. Deduplicate by","explanation":"## Why This Is Asked\n\nTests the ability to design a practical, beginner-friendly pipeline with tenant isolation, governance, and lineage on GCP.\n\n## Key Concepts\n\n- Dataflow streaming ingestion from Pub/Sub\n- BigQuery single table with partitioning by ingestion_date and tenant_id\n- Authorized views for per-tenant access control\n- Additive schema evolution with nullable fields\n- Deduplication by event_id\n- Late data handling (1 hour)\n- Data Catalog-based lineage\n\n## Code Example\n\n```javascript\n// Pseudo Dataflow-style transform (illustrative)\nfunction transform(eventJson){\n  const obj = JSON.parse(eventJson);\n  obj.ingestion_date = (obj.event_time ? obj.event_time : new Date()).toISOString().slice(0,10);\n  return obj;\n}\n```\n\n## Follow-up Questions\n\n- How would you test tenant isolation and access controls?\n- What are trade-offs between a single table vs per-tenant tables?\n- How would you roll out additive schema changes with minimal downtime?","diagram":"flowchart TD\n  PubSub[Pub/Sub] --> Dataflow[Dataflow Ingestion]\n  Dataflow --> BigQuery[BigQuery Table]\n  BigQuery --> Views[Authorized Views]\n  DataCatalog[Data Catalog] --> Lineage[Lineage]","difficulty":"beginner","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Twitter","Two Sigma","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T08:45:00.921Z","createdAt":"2026-01-17T08:45:00.921Z"},{"id":"q-3270","question":"Design a multi-tenant feature store on GCP for real-time ML inference using Vertex AI Feature Store with online/offline stores and BigQuery as the offline layer. Ingest features via Pub/Sub; enforce per-tenant isolation and versioned schemas; support canary rollouts and rollback, with testing and Data Catalog lineage. Outline data model, ingestion, validation, and monitoring?","answer":"Propose a two-store setup: Vertex AI Feature Store for online serving and BigQuery for offline analytics, with tenant-scoped namespaces. Ingest through Pub/Sub, tag features by tenant, and version sch","explanation":"## Why This Is Asked\n\nThis question probes the ability to design scalable, tenant-aware ML feature systems on GCP, integrating Vertex AI Feature Store, BigQuery, Pub/Sub, and Data Catalog, while addressing schema evolution, canary deployments, rollback, and testing.\n\n## Key Concepts\n\n- Vertex AI Feature Store; online/offline\n- Tenant isolation and schema versioning\n- Canary rollouts and rollback\n- Data validation, lineage, monitoring\n\n## Code Example\n\n````javascript\n// Pseudo ingestion sketch for versioned features\n````\n\n## Follow-up Questions\n\n- How would you implement per-tenant access control and data masking in Feature Store?\n- How would you validate and test rollback scenarios in production?","diagram":"flowchart TD\n  A[Tenant] --> B[Ingest via Pub/Sub]\n  B --> C[Vertex AI Online Store]\n  B --> D[BigQuery Offline Store]\n  C --> E[Serving Layer]\n  D --> F[BI/Analytics]","difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Microsoft","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T09:33:58.544Z","createdAt":"2026-01-17T09:33:58.544Z"},{"id":"q-3415","question":"New streaming data: 150 MB/day of JSON click events arrive via Pub/Sub and must be ingested into BigQuery in a single region. Design a beginner-friendly GCP pipeline using Dataflow (Beam) to parse, validate required fields, and enrich with a userId lookup; store results in a daily-partitioned BigQuery table; implement idempotent writes using insertId or MERGE; route malformed messages to a dead-letter Pub/Sub topic and archive originals to Cloud Storage; propose a 24-hour rollback plan and a basic data quality/monitoring approach?","answer":"Design Dataflow (Beam) to parse JSON, validate fields (event_id, user_id, ts), and enrich with a simple userId lookup. Write to a daily-partitioned BigQuery table using BigQueryIO with insertId on eve","explanation":"## Why This Is Asked\n\nTests ability to craft a practical streaming pipeline on GCP with beginner-friendly components: Pub/Sub ingestion, Dataflow parsing/validation, BigQuery partitioning, idempotent writes, dead-letter handling, and archival. Also touches data quality checks and rollback planning.\n\n## Key Concepts\n\n- Dataflow Beam parsing and validation\n- BigQuery streaming inserts with insertId or MERGE\n- Dead-letter queues and archival storage\n- Schema evolution, basic data quality checks, monitoring\n\n## Code Example\n\n```javascript\n// Example: pseudo-beam snippet illustrating insertId usage\n```\n\n## Follow-up Questions\n\n- How would you test the pipeline end-to-end?\n- What data quality metrics would you monitor?","diagram":"flowchart TD\n  A[Pub/Sub] --> B[Dataflow/Beam parse & enrich]\n  B --> C(BigQuery: daily partitions)\n  B --> D[Dead-letter Pub/Sub]\n  B --> E[Cloud Storage archive]\n  C --> F[Monitoring & tests]","difficulty":"beginner","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["NVIDIA","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T15:33:19.801Z","createdAt":"2026-01-17T15:33:19.801Z"},{"id":"q-3512","question":"On GCP, design a two-tenant data platform for an advertising analytics service ingesting 800 GB/day of JSON events via Pub/Sub. Each tenant has isolated dashboards and ML models. Build versioned data contracts in Data Catalog, enforce them via a registry, route data to per-tenant BigQuery tables and to Vertex AI Feature Store for ML, apply per-tenant PII masking, and maintain end-to-end lineage. Include testing, a canary rollout, and a per-tenant rollback window (6 hours)?","answer":"Establish per-tenant data contracts in Data Catalog with versioned schemas, enforce via Dataflow templates that validate JSON against the registry, mask PII with DLP before storage, route to tenant-sc","explanation":"## Why This Is Asked\nReal-world pattern combining data contracts, lineage, privacy, and ML integration; tests ability to design governance and practical rollback\n\n## Key Concepts\n- Versioned data contracts in Data Catalog\n- Dataflow-based validation against registry\n- PII masking via DLP and per-tenant isolation\n- BigQuery + Vertex AI Feature Store integration\n- End-to-end lineage and canary rollouts\n\n## Code Example\n```yaml\ncontracts:\n  - tenant: \"tenantA\"\n    schema_version: \"v1\"\n    fields:\n      - name: \"user_id\"\n        type: \"STRING\"\n      - name: \"event_type\"\n        type: \"STRING\"\n      - name: \"timestamp\"\n        type: \"TIMESTAMP\"\n    privacy:\n      mask_fields:\n        - \"user_id\"\n```\n\n## Follow-up Questions\n- How would you test schema evolution for existing tenants?\n- How would you implement canary rollout and rollback for contracts?","diagram":"flowchart TD\n  A[Pub/Sub Ingest] --> B[Dataflow Template]\n  B --> C[BigQuery (Tenant A)]\n  B --> D[BigQuery (Tenant B)]\n  B --> E[Vertex AI Feature Store]\n  C --> F[Data Catalog Lineage]\n  E --> F","difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Databricks","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T19:28:49.140Z","createdAt":"2026-01-17T19:28:49.140Z"},{"id":"q-3553","question":"On GCP, build a streaming ML feature pipeline that ingests 1 TB/day of JSON user events via Pub/Sub, computes features in Dataflow, stores in BigQuery and Vertex AI Feature Store, and traces end-to-end lineage for model training. Describe data models, feature store integration, drift detection, rollback strategy, and how you'd reproduce training with exact feature versions?","answer":"Pub/Sub -> Dataflow (Parse, normalize, compute features in fixed windows) -> BigQuery (raw events) + Vertex AI Feature Store (features with per-tenant tags). Track lineage via Data Catalog and feature","explanation":"## Why This Is Asked\nTests ability to design a scalable streaming feature pipeline with end-to-end lineage, versioned features for reproducible ML training, and robust rollback and drift detection.\n\n## Key Concepts\n- Streaming ingestion: Pub/Sub, Dataflow\n- Feature storage: Vertex AI Feature Store, BigQuery\n- Lineage: Data Catalog, feature versioning\n- Reproducibility: pinned feature versions per training run\n- Quality: drift detection, rollback strategies\n\n## Code Example\n```javascript\n// Pseudocode: Dataflow steps for feature computation and writes\n```\n\n## Follow-up Questions\n- How would you enforce per-tenant feature isolation in the feature store?\n- How would you implement end-to-end testing for training reproducibility across feature versions?","diagram":"flowchart TD\n  Ingest[Ingest Events] --> Process[Process with Dataflow]\n  Process --> RawBQ[BigQuery Raw]\n  Process --> FeatureStore[Feature Store]\n  RawBQ --> Train[Model Training]\n  FeatureStore --> Train","difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","Tesla","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T21:25:36.167Z","createdAt":"2026-01-17T21:25:36.167Z"},{"id":"q-3716","question":"Design a GCP-based multi-tenant analytics platform for 300 tenants ingesting 500 GB/day of JSON events via Pub/Sub. Build per-tenant data products in BigQuery with strict isolation (Authorized Views), enforce data contracts via Data Catalog, mask PII at ingest with DLP, and store immutable audit logs in Cloud Storage. Include automatic schema evolution, event-time processing with late arrivals (Dataflow), cross-region replication, and a 24h tenant rollback?","answer":"Design a GCP-based multi-tenant analytics pipeline for 300 tenants ingesting 500 GB/day of JSON events via Pub/Sub. Implement per-tenant BigQuery data products with Authorized Views, enforce data cont","explanation":"## Why This Is Asked\nTests experience designing scalable, governed data platforms with multi-tenant isolation, data contracts, and end-to-end lineage on GCP. It also probes handling late-arriving data, PII masking, and per-tenant rollback plans.\n\n## Key Concepts\n- Multi-tenant isolation in BigQuery with Authorized Views\n- Data contracts and lineage via Data Catalog\n- Ingestion-time PII masking with DLP\n- Event-time processing and late-arrivals in Dataflow\n- Cross-region replication and immutable audit logs\n- Schema evolution and per-tenant rollback strategy\n\n## Code Example\n```javascript\n// Pseudo-code: ingest path with DLP masking before storage\nconst ingest = (record) => {\n  const masked = dlp.maskPII(record);\n  pubsub.publish(masked);\n};\n```\n\n## Follow-up Questions\n- How would you test schema evolution across 300 tenants without downtime?\n- What metrics and alerting would you implement for cross-region replication health?","diagram":"flowchart TD\n  PubSub[Pub/Sub] --> Dataflow[Dataflow Ingest]\n  Dataflow --> RawStorage[Cloud Storage Raw Parquet]\n  Dataflow --> BigQuery[BigQuery Tenant Data Products]\n  DataCatalog[Data Catalog] --> BigQuery","difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Meta","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T06:51:32.769Z","createdAt":"2026-01-18T06:51:32.770Z"},{"id":"q-3753","question":"Design a cross-tenant analytics platform on GCP that ingests 2 TB/day of JSON events per tenant via Pub/Sub, stores raw Parquet in regional Cloud Storage, and serves per-tenant aggregated views in BigQuery with strict isolation. Implement per-tenant differential privacy budgets at query time using DP libraries and UDFs, track budgets in Data Catalog, and enable a 24h rollback of privacy budgets. Include data models, lineage, testing strategy, and rollback plan?","answer":"In-depth approach: ingest 2 TB/day via Pub/Sub, land raw Parquet in regional GCS, and publish per-tenant views in BigQuery with row-level isolation. Enforce differential privacy at query time using DP","explanation":"## Why This Is Asked\nThis new angle tests privacy-aware analytics design across GCP services, tenant isolation, and governance.\n\n## Key Concepts\n- Differential privacy budgets per tenant\n- BigQuery UDFs for DP noise\n- Dataflow DP transforms and lineage\n- Data Catalog metadata for budget tracking\n- Regional storage and per-tenant isolation\n- Rollback strategies and testing\n\n## Code Example\n```javascript\n// DP budget checkpoint (pseudo)\nfunction applyDP(value, epsilonBudget) {\n  const noise = sampleLaplace(1/epsilonBudget);\n  return value + noise;\n}\n```\n\n## Follow-up Questions\n- How would you test budget exhaustion and prevent cross-tenant leakage?\n- How would you handle late data affecting DP guarantees and rollback?","diagram":"flowchart TD\n  P[Pub/Sub Ingest] --> D[Dataflow DP]\n  D --> B[BigQuery per-tenant views]\n  P --> R[Raw Parquet in regional GCS]\n  B --> L[Data Catalog lineage]","difficulty":"advanced","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Apple","Instacart"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T08:38:39.531Z","createdAt":"2026-01-18T08:38:39.532Z"},{"id":"q-3795","question":"Design a GCP streaming ML feature pipeline for five tenants: ingest 250 GB/day of JSON events from Pub/Sub into Vertex AI Feature Store (online for real-time scoring and offline for batch) with per-tenant isolation, data contracts in Data Catalog, and PII masking at ingest. Include schema evolution, drift monitoring, cross-region replication, and a 24h rollback plan?","answer":"Five-tenant streaming ML feature pipeline: Pub/Sub → Dataflow → Vertex AI Feature Store (online for real-time scoring and offline for batch) with per-tenant isolation, data contracts in Data Catalog, ","explanation":"## Why This Is Asked\nTests ability to design end-to-end streaming ML pipelines on GCP with tenants separation, data contracts, and governance.\n\n## Key Concepts\n- Vertex AI Feature Store (online/offline)\n- Pub/Sub + Dataflow streaming\n- Data Catalog data contracts and schema evolution\n- Per-tenant isolation via IAM\n- Drift monitoring and rollback plans\n\n## Code Example\n```javascript\n// PII masking example at ingest\nfunction maskPII(record){ if(record.email){ record.email = hash(record.email); } return record; }\n```\n\n## Follow-up Questions\n- How would you validate schema evolution without breaking tenants?\n- How would you design drift alerts and rollbacks across regions?","diagram":"flowchart TD\n  PubSub[Pub/Sub Ingest] --> Dataflow[Dataflow Transform]\n  Dataflow --> Online[Vertex AI Feature Store (Online)]\n  Dataflow --> Offline[Vertex AI Feature Store (Offline)]\n  Online --> Serving[Model Serving]\n  Serving --> Alerts[Drift Alerts & Monitoring]\n  DataCatalog[Data Catalog Contracts] --> Dataflow\n  IAM[Tenant Isolation (IAM)] --> Dataflow","difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Salesforce","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T09:42:21.058Z","createdAt":"2026-01-18T09:42:21.058Z"},{"id":"q-3879","question":"Design a cost-aware, two-region multi-tenant analytics pipeline on GCP that ingests 1 TB/day of JSON events via Pub/Sub into BigQuery. Add tenant-scoped data quality scoring at ingest, per-tenant row-level isolation, and automated schema drift rollback with canary deployments. Include data models, partitioning, testing plan, and rollback strategy per tenant?","answer":"Implement a per-tenant, two-region pipeline: Pub/Sub -> Dataflow -> BigQuery per-tenant datasets. Enforce isolation with Row-Level Security and Authorized Views; compute tenant-scoped data quality sco","explanation":"## Why This Is Asked\nTests ability to design a cost-aware, two-region, multi-tenant pipeline with scalable data quality, robust isolation, and a clear rollback strategy. Requires practical knowledge of GCP services and governance.\n\n## Key Concepts\n- Two-region BigQuery with per-tenant datasets and isolation via RLS/Authorized Views\n- Ingest-time data quality scoring (conformance, completeness, duplicates)\n- Canary schema changes and per-tenant rollback within 24h\n- Data Catalog for lineage; Dataflow for streaming transformation\n\n## Code Example\n```python\n# Pseudo Dataflow transform: extract tenant, score quality, route to ok/bad lanes\nclass ProcessEvent(DoFn):\n    def process(self, element):\n        evt = json.loads(element)\n        tenant = evt['tenant_id']\n        score = score_quality(evt)\n        if score['ok']:\n            yield beam.window.TimestampedValue((tenant, evt), evt['ts'])\n        else:\n            yield beam.pvalue.TaggedOutput('bad', (tenant, evt, score))\n```\n\n## Follow-up Questions\n- How would you validate per-tenant rollback across all tenants without impacting live queries?\n- What metrics and alerts would you surface to detect schema drift per tenant?","diagram":null,"difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","Plaid","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T13:43:22.039Z","createdAt":"2026-01-18T13:43:22.039Z"},{"id":"q-3926","question":"Design a cost-aware, multi-tenant streaming pipeline: Pub/Sub feeds 1 TB/day of JSON events; Dataflow writes per-tenant, day-partitioned BigQuery tables (tenant_id, user_id). Run a shared Vertex AI anomaly model with per-tenant thresholds via side inputs; store scores in BigQuery and publish alerts to Pub/Sub. Archive older data to Cloud Storage Coldline; Data Catalog provides lineage. Rollback uses canary deployment and per-tenant guardrails; test with synthetic data and drift checks?","answer":"Design a cost-aware, multi-tenant streaming pipeline: Pub/Sub feeds 1 TB/day of JSON events; Dataflow writes per-tenant, day-partitioned BigQuery tables (tenant_id, user_id). Run a shared Vertex AI an","explanation":"## Why This Is Asked\nThis prompt tests building a scalable, cost-aware multi-tenant streaming solution with real-time ML scoring, strict data isolation, and robust rollback.\n\n## Key Concepts\n- Pub/Sub → Dataflow streaming\n- Per-tenant BigQuery storage and day partitioning\n- Vertex AI scoring with tenant-specific thresholds via side inputs\n- Data Catalog lineage and Cloud Storage lifecycle for archival\n- Canary deployments and per-tenant rollbacks\n\n## Code Example\n```javascript\n// Pseudo-config for per-tenant thresholds and canary rollout\nconst tenantConfig = {\n  \"tenantA\": { \"threshold\": 0.95, \"canary\": true },\n  \"tenantB\": { \"threshold\": 0.98, \"canary\": false }\n};\n```\n\n## Follow-up Questions\n- How would you monitor data skew and trigger rebalancing?\n- What failure modes exist during drift and how would you test rollbacks?","diagram":null,"difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Snap","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T15:42:04.001Z","createdAt":"2026-01-18T15:42:04.001Z"},{"id":"q-4022","question":"Design a two-region streaming pipeline on GCP for multi-tenant analytics (Pub/Sub → Dataflow → BigQuery). Enforce per-tenant isolation (Authorized Views), two-region active-active replication, exactly-once processing, late data handling, and per-tenant TTL (90 days). Support automatic schema evolution and provide a rollback plan using versioned tables and canary validation?","answer":"Configure a Dataflow streaming pipeline (Pub/Sub → Dataflow → BigQuery) with per-tenant datasets and Authorized Views, two-region replication, and exactly-once processing. Use TTL via time-partitioned","explanation":"## Why This Is Asked\\n\\nTests practical mastery of streaming pipelines, cross-region consistency, and tenant isolation with BigQuery, Dataflow, Pub/Sub. It also probes ability to design rollback and testing.\\n\\n## Key Concepts\\n- Streaming exactly-once semantics in Dataflow\\n- Tenant isolation with Authorized Views/BQ datasets\\n- Two-region replication and failover\\n- TTL and schema evolution strategies\\n- Canary rollbacks and testing\\n\\n## Code Example\\n```javascript\\n// Pseudo-Dataflow config (illustrative)\\nconst opts = {\\n  streaming: true,\\n  region: 'us-central1'\\n};\\n```\\n\\n## Follow-up Questions\\n- How would you test TTL compliance and canary rollback?\\n- How would you handle schema drift across regions?","diagram":null,"difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T20:34:35.603Z","createdAt":"2026-01-18T20:34:35.604Z"},{"id":"q-4317","question":"Design a beginner-friendly GCP pipeline that ingests 300 MB/day of JSON streaming events from Pub/Sub into BigQuery in a single region. Requirements: 1) parse and validate fields: eventId, deviceId, ts, metric, value; 2) normalize ts to UTC; 3) deduplicate by eventId; 4) allow up to 2 minutes lateness for a 1-minute sliding window; 5) produce a daily-partitioned BigQuery summary with average value per device/metric; 6) write raw JSON to Cloud Storage and publish malformed messages to a DLQ Pub/Sub; 7) provide a 24-hour rollback by deleting last day’s summary partition and reprocessing; 8) basic monitoring?","answer":"Use Dataflow streaming: Pub/Sub -> DoFn to parse/validate eventId, deviceId, ts, metric, value; UTC-normalize ts; emit invalid to a DLQ Pub/Sub. BigQueryIO writes to a daily-partitioned table with ins","explanation":"## Why This Is Asked\nTests end-to-end streaming design, deduplication, backfill tolerance, and rollback.\n\n## Key Concepts\n- Dataflow (Beam) streaming pipelines\n- Pub/Sub + DLQ handling\n- BigQuery insertId for idempotent writes\n- Windowing and late data handling\n- Cloud Storage archival and rollback strategies\n- Basic monitoring\n\n## Code Example\n```javascript\n// Beam skeleton showing Pub/Sub, DoFn, and BigQuery write with insertId\n```\n\n## Follow-up Questions\n- How would you test idempotence and rollback in CI/CD?\n- How would you adjust for bursty data or increasing eventId cardinality?","diagram":"flowchart TD\nA[Pub/Sub] --> B[Dataflow: Parse/Validate/Normalize]\nB --> C[BigQuery: daily-partitioned table (insertId=eventId)]\nB --> D[BigQuery: per-window averages to summary table]\nB --> E[Cloud Storage: Raw archive]\nB --> F[Pub/Sub: Dead-letter topic]\n","difficulty":"beginner","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Google","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T13:23:21.932Z","createdAt":"2026-01-19T13:23:21.932Z"},{"id":"q-4364","question":"Design a real-time, multi-tenant fraud-detection pipeline on GCP for 100 tenants, each with ~200k events/day via Pub/Sub; implement per-tenant Vertex AI Feature Store, streaming scoring with Dataflow, model versioning, explainability logs, and drift monitoring; enforce isolation with IAM and Authorized Views, ensure end-to-end lineage via Data Catalog, and provide a rollback to a previous model version within 2 hours plus a testing strategy?","answer":"Implement per-tenant isolation with IAM, Authorized Views, and separate feature stores; process Pub/Sub events in Dataflow, compute features in real-time, score with Vertex AI hosted model, and push r","explanation":"## Why This Is Asked\nTests the ability to design end-to-end real-time ML-enabled data pipelines with strict tenant isolation, governance, explainability, and rollback.\n\n## Key Concepts\n- Vertex AI Feature Store per-tenant\n- Dataflow streaming in/out\n- IAM + Authorized Views for multi-tenancy\n- Data Catalog lineage\n- Drift detection and explainability logging\n- Canary/rollback safety window and testing\n\n## Code Example\n\n```json\n{\n  \"tenant\": \"<tenant_id>\",\n  \"feature_columns\": [\"recent_amount\",\"velocity\",\"device_fingerprint\"],\n  \"model_version\": \"fraud_v1.2\"\n}\n```\n\n## Follow-up Questions\n- How would you automate cross-tenant drift alerts and rollback triggers?\n- What latency targets and monitoring would you implement for streaming scoring?","diagram":null,"difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Tesla","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T15:52:47.985Z","createdAt":"2026-01-19T15:52:47.985Z"},{"id":"q-4495","question":"Batch ingestion: 500k JSON events/day stored in Cloud Storage (gs://data/events/YYYY/MM/DD/*.json). Build a beginner-friendly Dataflow (Python) pipeline that parses JSON, validates required fields (user_id, event_type, ts), enriches with a user tier lookup from BigQuery analytics.user_traits, and writes to analytics.events$YYYYMMDD (daily partition) using insertId for idempotence. Route malformed records to gs://data/events/errors/YYYY-MM-DD/ and outline a 24h rollback approach plus basic data quality monitoring?","answer":"Use a batch Dataflow job with a DoFn to parse JSON and validate fields; load user_traits as a side input (small lookup) to add tier; write to BigQuery with insertId set to event_id into analytics.even","explanation":"## Why This Is Asked\n\nTests batch ingestion, simple enrichment, and idempotent writes in a minimal setup. It also checks error handling paths and basic rollback thinking.\n\n## Key Concepts\n\n- Dataflow (Beam Python)\n- BigQueryIO with daily partitioned tables\n- Side inputs for small lookups (analytics.user_traits)\n- insertId-based deduplication\n- Dead-letter: errors to Cloud Storage\n- Basic monitoring and rollback strategy\n\n## Code Example\n\n```python\nfrom apache_beam import DoFn, ParDo, PCollection\nimport json\n\nclass ParseEventDoFn(DoFn):\n  def process(self, element):\n    try:\n      data = json.loads(element)\n      # minimal validation\n      if all(k in data for k in (\"user_id\", \"event_type\", \"ts\")):\n        yield data\n      else:\n        yield None\n    except Exception:\n      yield None\n```\n\n## Follow-up Questions\n\n- How would you test this locally with DirectRunner and mock BigQuery side input?\n- How would you adapt for late-arriving events or schema evolution?","diagram":"flowchart TD\n  A[Read JSON from GCS] --> B[Parse & Validate]\n  B --> C{Valid}\n  C -->|Yes| D[Enrich with BigQuery lookup (side input)]\n  C -->|No| E[Write to errors bucket]\n  D --> F[Write to BigQuery analytics.events$YYYYMMDD]\n  F --> G[Metrics & Monitoring]","difficulty":"beginner","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Google","Lyft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T21:28:33.215Z","createdAt":"2026-01-19T21:28:33.218Z"},{"id":"q-4569","question":"Design a GCP streaming fraud-detection pipeline for ride-hailing that ingests 1–2 TB/day of JSON events via Pub/Sub into two regions. Enforce per-tenant contracts with schema validation, isolate tenants with dedicated datasets, compute real-time features and anomaly scores in Dataflow, and expose a low-latency BigQuery view. Include end-to-end lineage via Data Catalog, automatic drift rollback, and a concrete testing plan?","answer":"Design a GCP streaming fraud-detection pipeline for ride-hailing that ingests 1–2 TB/day of JSON events via Pub/Sub into two regions. Enforce per-tenant contracts with schema validation, isolate tenants with dedicated datasets, compute real-time features and anomaly scores in Dataflow, and expose a low-latency BigQuery view. Include end-to-end lineage via Data Catalog, automatic drift rollback, and a concrete testing plan.","explanation":"## Why This Is Asked\nTests ability to design cross-region streaming pipelines with strict per-tenant isolation, real-time feature extraction, and robust data governance.\n\n## Key Concepts\n- Streaming ingestion: Pub/Sub to Dataflow with windowed aggregations\n- Tenant isolation: separate datasets/views and access controls\n- Real-time features: feature store patterns in BigQuery/Dataflow\n- Data governance: Data Catalog lineage, schema validation, drift rollback\n\n## Code Example\n```python\n# Skeleton Apache Beam streaming pipeline (Python)\nimport apache_beam as beam\nfrom apache_beam.options.pipeline_options import PipelineOptions\nfrom apache_beam.io.gcp.bigquery import WriteToBigQuery\n\nclass FraudDetectionPipeline:\n    def __init__(self, project_id, region):\n        self.project_id = project_id\n        self.region = region\n    \n    def validate_schema(self, element):\n        # Enforce per-tenant schema contracts\n        required_fields = ['tenant_id', 'event_type', 'timestamp', 'user_id']\n        return all(field in element for field in required_fields)\n    \n    def compute_features(self, element):\n        # Real-time feature extraction\n        features = {\n            'tenant_id': element['tenant_id'],\n            'event_hour': beam.window.TimestampedValue.get_current_time(),\n            'anomaly_score': self.calculate_anomaly_score(element)\n        }\n        return features\n    \n    def calculate_anomaly_score(self, event):\n        # Simplified anomaly detection logic\n        return 0.0  # Replace with actual ML model scoring\n\ndef run_pipeline():\n    options = PipelineOptions([\n        '--project=your-project',\n        '--region=us-central1',\n        '--streaming=true',\n        '--temp_location=gs://your-bucket/temp/'\n    ])\n    \n    with beam.Pipeline(options=options) as p:\n        (p\n         | 'ReadFromPubSub' >> beam.io.ReadFromPubSub(\n             subscription='projects/your-project/subscriptions/fraud-events')\n         | 'ParseJSON' >> beam.Map(lambda x: json.loads(x))\n         | 'ValidateSchema' >> beam.Filter(FraudDetectionPipeline.validate_schema)\n         | 'TenantRouting' >> beam.Map(lambda x: (\n             f'tenant_{x[\"tenant_id\"]}', x))\n         | 'ComputeFeatures' >> beam.Map(FraudDetectionPipeline.compute_features)\n         | 'WriteToBigQuery' >> WriteToBigQuery(\n             table='your-project:fraud_detection.features',\n             schema='tenant_id:STRING,event_hour:TIMESTAMP,anomaly_score:FLOAT',\n             write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND))\n```\n\n## Testing Plan\n1. **Unit Tests**: Schema validation, feature computation logic\n2. **Integration Tests**: End-to-end Pub/Sub to BigQuery flow\n3. **Performance Tests**: 1–2 TB/day ingestion throughput validation\n4. **Tenant Isolation Tests**: Verify dataset separation and access controls\n5. **Drift Detection Tests**: Schema evolution and rollback scenarios\n6. **Disaster Recovery Tests**: Cross-region failover validation","diagram":null,"difficulty":"advanced","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","Lyft","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T06:00:13.918Z","createdAt":"2026-01-20T02:20:42.024Z"},{"id":"q-4631","question":"Design a GCP streaming pipeline for 1 TB/day of multi-tenant clickstream events ingested via Pub/Sub into per-tenant BigQuery datasets. Include isolated tenants with separate schemas, backward-compatible schema evolution, per-tenant data masking at ingest (DLP), end-to-end lineage via Data Catalog, and a rolling 24h rollback that replays data for affected tenants without touching others; also specify per-tenant cost controls with slot reservations. Provide architecture, data models, and testing strategy?","answer":"Load per-tenant events from Pub/Sub into dedicated BigQuery datasets via a Dataflow streaming template; each tenant uses a separate table with backward-compatible schema evolution managed by a central","explanation":"## Why This Is Asked\n\nTests the ability to design a production-grade streaming pipeline with strong tenant isolation, governance, and operational rollback on GCP.\n\n## Key Concepts\n\n- GCP streaming ETL: Pub/Sub, Dataflow\n- Tenant isolation: per-tenant datasets, IAM\n- Schema evolution: registry + backward compatibility\n- Data masking: DLP at ingest\n- Data lineage: Data Catalog tagging\n- Rollback strategy: point-in-time replay per tenant\n\n## Code Example\n\n```python\n# Pseudo Dataflow template snippet\ndef process(event, context):\n    masked = mask(event)\n    write_to_bq(masked, tenant=event['tenant'])\n```\n\n## Follow-up Questions\n\n- How would you test schema evolution impact across tenants?\n- How would you handle a bursty backlog during rollback?","diagram":"flowchart TD\n  A[Pub/Sub] --> B[Dataflow Template]\n  B --> C[BigQuery per-tenant]\n  C --> D[Data Catalog lineage]\n  C --> E[Rollback backlog replay]","difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Tesla","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T05:48:20.741Z","createdAt":"2026-01-20T05:48:20.741Z"},{"id":"q-4731","question":"Design a real-time, cross-tenant analytics pipeline on GCP ingesting 2 TB/day of JSON events via Pub/Sub into BigQuery across two regions. Enforce per-tenant isolation with field-level masking, support automatic schema evolution, and implement a tenant-scoped data replay (24h window) without impacting others. Add end-to-end lineage in Data Catalog and a Vertex AI-based data observability layer. Describe architecture, data models, rollback, and tests?","answer":"Ingest with Pub/Sub and Dataflow, deploy two regional pipelines feeding per-tenant BigQuery datasets. Use DLP + IAM for field-level masking, and BigQuery auto schema updates. Implement a tenant-scoped","explanation":"## Why This Is Asked\n\nEvaluate ability to design scalable, multi-region, tenant-isolated pipelines with governance and observability. The prompt tests streaming ETL, tenant-scoped replay, schema evolution strategies, privacy controls, automated lineage, and a data quality layer with ML-based anomaly detection. It also probes rollback plans and robust testing in production.\n\n## Key Concepts\n\n- Multi-region BigQuery reservoirs and Dataflow pipelines\n- Tenant isolation and per-tenant schemas\n- Replay semantics with idempotent upserts\n- Data Catalog lineage and DLP masking\n- Vertex AI monitoring and anomaly detection\n\n## Code Example\n\n```javascript\n// pseudo-code sketch\n```\n\n## Follow-up Questions\n\n- How would you validate replay correctness at scale?\n- How would you handle partial failures during replay across tenants?","diagram":null,"difficulty":"advanced","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Hugging Face","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T10:08:45.938Z","createdAt":"2026-01-20T10:08:45.938Z"},{"id":"q-4827","question":"Design a real-time, cross-region, multi-tenant analytics pipeline on GCP that ingests 3 TB/day of JSON events from Pub/Sub into BigQuery in two regions. Each tenant must be isolated (per-tenant datasets or authorized views), with field-level privacy masking at ingest, automatic schema evolution, and end-to-end lineage captured in Data Catalog. Include data model, partitioning / clustering, rollback plan per tenant within 24h, testing strategy, and how you'd monitor latency?","answer":"Streaming via Pub/Sub -> Dataflow templates; apply per-tenant masking with a DoFn and DLP; write to regionally replicated BigQuery datasets partitioned by ingest_date and clustered by tenant_id; Data ","explanation":"## Why This Is Asked\nProbes practical streaming architecture at scale, cross-region replication, and robust per-tenant privacy controls.\n\n## Key Concepts\n- Real-time ingestion: Pub/Sub to Dataflow templates\n- Multi-region data residency and consistency\n- Tenant isolation strategies: per-tenant datasets or authorized views\n- Field-level masking: DLP integration at ingest\n- Schema evolution in BigQuery with streaming\n- End-to-end lineage via Data Catalog\n- Immutable audit logs to Cloud Storage\n- Rollback using BigQuery time travel and per-tenant snapshots\n- Monitoring: Dataflow metrics, backlog, latency\n\n## Code Example\n```python\nimport apache_beam as beam\nfrom apache_beam.options.pipeline_options import PipelineOptions\n\nclass MaskPII(beam.DoFn):\n  def process(self, element, *args, **kwargs):\n    # pseudo masking logic per tenant\n    tenant = element.get('tenant_id')\n    if 'email' in element:\n      element['email'] = '***@masked.com'\n    # more field-level masking based on tenant policy\n    yield element\n```\n\n## Follow-up Questions\n- How would you handle schema drift across regions and tenants?\n- How would you validate rollback SLAs and rollback correctness under load?","diagram":"flowchart TD\n  PubSub[Pub/Sub: JSON events] --> T[Dataflow: ingest & transform]\n  T --> BQ1[BigQuery Region A: per-tenant datasets]\n  T --> BQ2[BigQuery Region B: per-tenant datasets]\n  BQ1 --> LC[Data Catalog: lineage]\n  BQ2 --> LC\n  T --> Audit[Audit Logs to Cloud Storage]\n  T --> Mask[Masking: per-tenant fields]\n  LC --> Monitor[Monitoring & Alerts]","difficulty":"advanced","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Google","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T15:08:37.317Z","createdAt":"2026-01-20T15:08:37.317Z"},{"id":"q-4866","question":"Incoming Pub/Sub messages contain a field 'events' which is an array of sub-events with 'id','type','value'. Build a beginner-friendly Dataflow (Beam) streaming pipeline to flatten into one row per sub-event, validate required fields at sub-event level, and enrich with a voyageId lookup; write to a daily-partitioned BigQuery table. Route malformed messages to a dead-letter Pub/Sub topic and archive originals to Cloud Storage. Provide architecture, data model, and a minimal test plan?","answer":"Use a streaming Dataflow pipeline with a ParDo that explodes the events array, validates sub-field presence, and emits valid rows to a BigQuery streaming insert into a daily-partitioned table using th","explanation":"## Why This Is Asked\nTests nested data handling, field validation, enrichment, and reliable DLQ/archival integration. Also checks BigQuery partition-aware writing and testability.\n\n## Key Concepts\n- Flatten nested arrays in Beam\n- Sub-event validation and error routing\n- Side-input enrichment\n- Daily-partitioned BigQuery writes\n- Pub/Sub Dead Letter and Cloud Storage archival\n- Beam TestStream for unit tests\n\n## Code Example\n```python\nimport json\nimport apache_beam as beam\nfrom apache_beam.options.pipeline_options import PipelineOptions\n\nclass FlattenSubEvents(beam.DoFn):\n    def process(self, element, voyage_map):\n        for ev in element.get('events', []):\n            if all(k in ev for k in ('id','type','value')):\n                ev['voyageId'] = voyage_map.get(element.get('voyageId'))\n                yield ev\n\ndef run():\n    opts = PipelineOptions()\n    with beam.Pipeline(options=opts) as p:\n        voyage_map = p | 'CreateVoyageMap' >> beam.Create({'V1': 'Voyage-1'})\n        _ = (\n            p\n            | 'ReadPubSub' >> beam.io.ReadFromPubSub(topic='projects/PROJECT/topics/TOPIC')\n            | 'ParseJson' >> beam.Map(lambda b: json.loads(b.decode('utf-8')))\n            | 'Flatten' >> beam.ParDo(FlattenSubEvents(), voyage_map=beam.pvalue.AsDict(voyage_map))\n            | 'ToBigQuery' >> beam.io.WriteToBigQuery(\n                table='PROJECT.DATASET.table$YYYYMMDD',\n                schema='SCHEMA_AUTODETECT',\n                write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE\n            )\n        )\n\nif __name__ == '__main__':\n    run()\n```\n\n## Follow-up Questions\n- How would you test late-arriving nested events and ensure they are processed in order?\n- What changes would you make if the events array grows large or becomes streaming-heavy?","diagram":"flowchart TD\n  PubSub[Pub/Sub] --> Dataflow[Dataflow (Beam)]\n  Dataflow --> BigQuery[BigQuery daily partition]\n  Dataflow --> DLQ[DLQ Pub/Sub]\n  Dataflow --> Archive[Cloud Storage Archive]","difficulty":"beginner","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["NVIDIA","Salesforce","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T16:55:31.325Z","createdAt":"2026-01-20T16:55:31.325Z"},{"id":"q-5281","question":"Ingest 150 MB/day of JSON events via Pub/Sub into BigQuery in a single region. Build a beginner-friendly Dataflow pipeline that parses and validates required fields, enriches via a side-input mapping, and writes to a daily-partitioned BigQuery table using insertId for idempotence. Route invalid messages to a dead-letter Pub/Sub topic and archive originals to Cloud Storage. Maintain a streaming-aggregation table for daily active users and provide a 24-hour rollback plus basic monitoring?","answer":"Parse JSON events from Pub/Sub, validate required fields, enrich via a side-input mapping, and write to a daily-partitioned BigQuery table using insertId for idempotence. Route invalid messages to a d","explanation":"## Why This Is Asked\nTests ability to build a resilient, beginner-friendly streaming pipeline using common GCP primitives, with explicit data quality, enrichment, and observability requirements. It emphasizes idempotent writes, dead-letter handling, and rollback strategies—key for real-world data work.\n\n## Key Concepts\n- Dataflow (Beam) basic transforms: ParDo, DoFn, side inputs\n- Pub/Sub as source, BigQuery as sink (daily partitions)\n- Idempotent writes via insertId\n- Side-input enrichment\n- Dead-letter handling and Cloud Storage archival\n- Streaming aggregation in BigQuery\n- Rollback: replay from archive or Pub/Sub snapshot\n- Monitoring with Cloud Monitoring and Logging\n\n## Code Example\n```python\nimport apache_beam as beam\nfrom apache_beam.options.pipeline_options import PipelineOptions\n\ndef parse_validate(elem):\n    # simple example\n    return {'user_id': elem['user_id'], 'event': elem['event']}\n\nclass AddTier(beam.DoFn):\n    def __init__(self, tier_map):\n        self.tier_map = tier_map\n    def process(self, elem):\n        user = elem['user_id']\n        elem['tier'] = self.tier_map.get(user, 'unknown')\n        yield elem\n\ndef run():\n    options = PipelineOptions(...)\n    tier_map = {'u1':'gold','u2':'silver'}\n    with beam.Pipeline(options=options) as p:\n        (p\n         | 'Read PubSub' >> beam.io.ReadFromPubSub(topic='projects/.../topics/events')\n         | 'Decode' >> beam.Map(lambda b: json.loads(b.decode('utf-8')))\n         | 'Validate' >> beam.Filter(lambda r: 'user_id' in r and 'event' in r)\n         | 'Enrich' >> beam.ParDo(AddTier(tier_map))\n         | 'WriteBigQuery' >> beam.io.WriteToBigQuery(...)\n        )\n```\n\n## Follow-up Questions\n- How to test idempotence across restarts?\n- How to handle late data and out-of-order events?","diagram":null,"difficulty":"beginner","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Plaid","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T14:57:23.018Z","createdAt":"2026-01-21T14:57:23.018Z"},{"id":"q-5297","question":"On GCP, design a two-region, cross-tenant streaming analytics platform for 1 TB/day of JSON events ingested via Pub/Sub. Build per-tenant data contracts with drift detection and automatic schema evolution, plus per-tenant cost controls (BQ slot reservations) and SLA-bound latency. Include architecture, data models, rollback plan within 24h, and testing strategy?","answer":"Design a two-region streaming pipeline: 1 TB/day of JSON events from Pub/Sub per tenant ingested via Dataflow into region-local storage and per-tenant BigQuery datasets with Authorized Views. Enforce ","explanation":"## Why This Is Asked\nTests ability to design multi-region, cross-tenant contracts with automated drift detection, policy-driven masking, and cost controls, using core GCP services. Emphasizes production-readiness: rollback, testing, and governance.\n\n## Key Concepts\n- Data contracts per tenant and drift detection\n- Drift-aware schema evolution and automated upgrades\n- Cross-region replication, latency commitments, and late data handling\n- Per-tenant cost controls (BigQuery Reservations/slots) and SLA guarantees\n- Immutable audit logs and end-to-end data lineage via Data Catalog\n\n## Code Example\n```python\nimport apache_beam as beam\nclass MaskPII(beam.DoFn):\n  def process(self, element):\n    if isinstance(element, dict) and 'ssn' in element:\n      element['ssn'] = '***-**-****'\n    yield element\n```\n\n## Follow-up Questions\n- How would you detect and resolve schema drift across hundreds of tenants?\n- How would you model per-tenant cost budgets and alerting in real time?\n- What tests would you run to validate rollback correctness and data integrity?","diagram":"flowchart TD\n  PubSub[Pub/Sub] --> Ingest[Dataflow Ingest]\n  Ingest --> Raw[Regional Raw Parquet]\n  Raw --> BK[Tenant BigQuery Datasets]\n  BK --> Catalog[Data Catalog (Contracts)]\n  Catalog --> Lineage[End-to-End Lineage]\n  Ingest --> Masking[DLP Masking]\n  BK --> Audit[Immutable Audit Logs (Cloud Storage)]","difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Slack","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T15:43:32.502Z","createdAt":"2026-01-21T15:43:32.502Z"},{"id":"q-5358","question":"Design a GCP data fabric for multi-tenant analytics: ingest 2 TB/day of JSON events per tenant from Pub/Sub; store raw Parquet lands in regional GCS, and curated BigQuery tables live per tenant. Include Vertex AI Feature Store namespaces for online scoring, per-tenant isolation, and field-level masking via DLP at ingest. Ensure lineage in Data Catalog, schema evolution, drift alerts, and a 24h per-tenant rollback by replaying events. Use Dataflow and Composer?","answer":"Design a GCP data fabric where 2 TB/day of JSON from Pub/Sub is ingested per tenant, raw Parquet lands in regional GCS, and curated BigQuery tables live per tenant. Include Vertex AI Feature Store nam","explanation":"## Why This Is Asked\nTests ability to architect multi-tenant data pipelines with modern GCP components, balancing isolation, governance, data quality, and operational resilience including rollback and drift management.\n\n## Key Concepts\n- Multi-tenant isolation in BigQuery\n- Vertex AI Feature Store integration\n- Data Catalog lineage\n- DLP masking at ingestion\n- Schema evolution and drift\n- 24h rollback by event replay\n- Dataflow + Composer orchestration\n\n## Code Example\n```javascript\n// Placeholder: show a skeleton pipeline in Python/Java for real implementation\n```\n\n## Follow-up Questions\n- How would you implement per-tenant cost controls and quotas?\n- How would you validate the 24h rollback end-to-end with minimal tenant impact?","diagram":"flowchart TD\n  PubSub[Pub/Sub] --> Dataflow[Dataflow]\n  Dataflow --> RegionalGCS[Regional GCS Parquet]\n  Dataflow --> BigQuery[Per-tenant BigQuery]\n  BigQuery --> FeatureStore[Vertex AI Feature Store (tenant namespace)]\n  DataCatalog[Data Catalog] --> BigQuery\n  DLP[Field-level masking (DLP)] --> Dataflow","difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T19:11:00.167Z","createdAt":"2026-01-21T19:11:00.167Z"},{"id":"q-5537","question":"On GCP, design a scalable real-time multi-tenant streaming pipeline that ingests 2 TB/day of JSON events from Pub/Sub, routes each tenant's data through per-tenant Dataflow pipelines, and writes to per-tenant BigQuery tables. Introduce per-tenant data quality gates (schema conformance, value ranges, completeness) via a configurable rule engine; quarantine failed events to Cloud Storage with immutable audit logs; and provide auto-scaling, per-tenant quotas, and a 24h rollback plan by replaying Pub/Sub offsets?","answer":"Architect a real-time multi-tenant streaming pipeline on GCP: Pub/Sub -> per-tenant Dataflow pipelines -> BigQuery per-tenant tables. Enforce per-tenant quality gates (schema conformance, range checks","explanation":"## Why This Is Asked\nTests the ability to design per-tenant data quality, isolation, and operational controls in streaming workloads using GCP services. It emphasizes practical handling of quality gates, auditability, rollback, and cost controls.\n\n## Key Concepts\n- Multi-tenant streaming with per-tenant isolation\n- Data quality gates (schema conformance, value ranges, completeness)\n- Rule engine for per-tenant quality policies\n- Quarantine of bad data to Cloud Storage with immutable audit logs\n- Dynamic scaling in Dataflow and per-tenant BigQuery tables\n- Rollback strategy via Pub/Sub offset replay\n\n## Code Example\n```python\nimport json\nimport apache_beam as beam\nfrom apache_beam.options.pipeline_options import PipelineOptions\n\nclass QualityGate(beam.DoFn):\n    def process(self, element):\n        record = json.loads(element)\n        tenant = record.get(\"tenant\")\n        if not tenant:\n            yield beam.pvalue.TaggedOutput(\"bad\", record)\n            return\n        valid = (\n            \"event_time\" in record and isinstance(record[\"event_time\"], str) and\n            \"payload\" in record\n        )\n        if valid:\n            yield (tenant, record)\n        else:\n            yield beam.pvalue.TaggedOutput(\"bad\", record)\n\ndef run():\n    options = PipelineOptions(streaming=True)\n    with beam.Pipeline(options=options) as p:\n        input = p | \"Read PubSub\" >> beam.io.ReadFromPubSub(subscription=\"projects/.../subscriptions/tenant-sub\")\n        main, bad = (input | \"Parse\" >> beam.ParDo(QualityGate()).with_outputs(\"bad\", main=\"main\"))\n        # Write main to per-tenant BigQuery using dynamic destinations (pseudo-code)\n        # bad records go to Cloud Storage with immutability for auditing\n```\n\n## Follow-up Questions\n- How would you implement per-tenant rollback granularity and ensure no data loss when rolling back 24h?\n- How would you monitor data quality metrics and trigger alerts across tenants without causing alert fatigue?","diagram":"flowchart TD\n  A[Pub/Sub Ingest] --> B[Per-tenant Dataflow Pipelines]\n  B --> C[BigQuery per-tenant Tables]\n  B --> D[Quality Gates]\n  D --> E{Pass}\n  E --> C\n  E --> F[Quarantine in Cloud Storage]\n  F --> G[Audit Logs in Cloud Storage]","difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","IBM","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T04:36:46.151Z","createdAt":"2026-01-22T04:36:46.151Z"},{"id":"q-5582","question":"A beginner-friendly GCP data pipeline: 100 MB/day of JSON Pub/Sub events from a web app. Use Dataflow (Beam) to parse, validate required fields user_id, event_time, action, enrich via a side-input lookup from a small CSV in Cloud Storage (user_segment by user_id), and write valid rows to a daily-partitioned BigQuery table in a single region. Use insertId for idempotence. Malformed messages go to a dead-letter Pub/Sub topic and originals archived to Cloud Storage. Include basic monitoring (parsing error rate) and a simple test plan?","answer":"Approach: build a Dataflow pipeline that parses Pub/Sub JSON, validates user_id/event_time/action, enriches with a side-input from a small CSV in Cloud Storage (user_segment by user_id), and writes to","explanation":"## Why This Is Asked\nTests ability to design a beginner-friendly, end-to-end pipeline using core GCP components and practical data-quality controls without overwhelming complexity.\n\n## Key Concepts\n- Beam DoFn with side inputs\n- ReadFromPubSub and WriteToBigQuery with insertId\n- Side-input lookup from Cloud Storage CSV\n- Dead-letter Pub/Sub routing\n- Archiving originals to Cloud Storage\n- Cloud Monitoring for parse error rate\n\n## Code Example\n```python\n# Dataflow skeleton\nimport apache_beam as beam\nfrom apache_beam.options.pipeline_options import PipelineOptions\n\nclass ValidateAndEnrich(beam.DoFn):\n    def __init__(self, lookup_side):\n        self.lookup_side = lookup_side\n\n    def process(self, element, lookup):\n        import json\n        try:\n            rec = json.loads(element)\n            if 'user_id' not in rec or 'event_time' not in rec or 'action' not in rec:\n                raise ValueError('missing fields')\n            seg = lookup.get(rec['user_id'], '')\n            rec['user_segment'] = seg\n            yield rec\n        except Exception:\n            yield beam.pvalue.TaggedOutput('bad', element)\n\n# Pipeline construction would include:\n# - ReadFromPubSub, ParDo with side input, and WriteToBigQuery with insertId\n```\n\n## Follow-up Questions\n- How would you test the side-input enrichment locally?\n- How would you adjust for a growing lookup file in Cloud Storage?","diagram":"flowchart TD\n  PubSub[Pub/Sub] --> Dataflow[Dataflow (Beam)]\n  Dataflow --> BigQuery[BigQuery: daily partitioned]\n  Dataflow --> DLQ[DLQ Pub/Sub]\n  Dataflow --> Archive[Cloud Storage: originals]\n  LookupCSV[CSV lookup] --> Dataflow","difficulty":"beginner","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["OpenAI","Salesforce","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T07:13:04.524Z","createdAt":"2026-01-22T07:13:04.524Z"},{"id":"q-5605","question":"In a multi-tenant, GCP-based data platform, each tenant's PII must be encrypted with a tenant-specific CMEK and only accessible within its project. Describe an end-to-end design using Pub/Sub ingestion, Dataflow streaming, BigQuery, Cloud KMS, and Data Catalog that enforces per-tenant encryption, field-level masking, and per-tenant access controls. Include data models, key management strategy, end-to-end lineage, testing, and a 24-hour rollback plan per tenant?","answer":"Use per-tenant CMEK in Cloud KMS (one key per tenant, scoped to its project). Dataflow reads Pub/Sub, masks PII via a policy store, then writes to BigQuery tables encrypted with CMEK. Data Catalog tag","explanation":"## Why This Is Asked\nTests knowledge of CMEK-per-tenant, streaming masking, lineage, and rollback.\n\n## Key Concepts\n- Tenant-level KMS keys and project scoping\n- Field-level masking in streaming\n- Data Catalog lineage and access governance\n- Point-in-time rollback for multi-tenant data\n\n## Code Example\n```python\n# Pseudo Dataflow-like sketch: read Pub/Sub, mask, write to CMEK-protected BigQuery\n```\n\n## Follow-up Questions\n- How to rotate tenant keys without downtime?\n- How to validate end-to-end privacy guarantees in production?\n- How would you enforce per-tenant isolation across Pub/Sub topics and BigQuery datasets?","diagram":"flowchart TD\n  A[Pub/Sub Ingest] --> B[Dataflow Streaming]\n  B --> C[Encrypted BigQuery Tables (CMEK)]\n  B --> D[Masking Module]\n  C --> E[Data Catalog Lineage]\n  F[KMS Tenant Keys] --> C\n  F --> D","difficulty":"advanced","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","IBM","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T08:01:34.484Z","createdAt":"2026-01-22T08:01:34.484Z"},{"id":"q-5658","question":"Design a cross-tenant streaming pipeline on GCP ingesting 5 TB/day of JSON events from Pub/Sub into BigQuery across two regions. Enforce per-tenant isolation with tenant-scoped datasets, a central schema registry, and ingest-time quality checks (validation, masking, anomaly detection) with a live quality dashboard. Include date partitioning, late arrival handling, cross-region replication, Data Catalog lineage, and a per-tenant rollback within 24h?","answer":"Design a cross-tenant streaming pipeline on GCP ingesting 5 TB/day of JSON events from Pub/Sub into BigQuery across two regions. Enforce per-tenant isolation with tenant-scoped datasets, a central sch","explanation":"## Why This Is Asked\nThis question probes cross-region streaming design, per-tenant isolation, schema governance, and active data quality.\n\n## Key Concepts\n- GCP Dataflow, Pub/Sub, BigQuery\n- Tenant isolation, schema registry, Data Catalog lineage\n- Ingest-time validation, masking, quality dashboards\n- Late arrival handling, cross-region replication, versioned rollback\n\n## Code Example\n```javascript\n// Placeholder: show dataflow-like pipeline wiring for ingestion and validation\n```\n\n## Follow-up Questions\n- How to test per-tenant rollback without affecting others?\n- What metrics and alerts would you expose for data quality in the dashboard?","diagram":"flowchart TD\n  PubSub[Pub/Sub] --> Ingest[Ingest (Dataflow region A/B)]\n  Ingest -->BQ1[BigQuery Tenant Data (Region 1)]\n  Ingest -->BQ2[BigQuery Tenant Data (Region 2)]\n  BQ1 --> Lineage[Data Catalog Lineage]\n  BQ2 --> Lineage\n  Lineage --> Dashboard[Quality Dashboard]","difficulty":"advanced","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Discord","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T10:14:23.278Z","createdAt":"2026-01-22T10:14:23.278Z"},{"id":"q-5744","question":"Process 300k daily JSON events from Pub/Sub with nested attributes (user: {id, tier}, event: {type, ts, meta}, device: {os}) by enriching with a user-tier lookup from Cloud Bigtable, flattening to a flat schema, and writing to a daily-partitioned BigQuery table in a single region. Include idempotent writes, dead-letter handling, archiving originals to Cloud Storage, a 24-hour rollback, and a lightweight data quality dashboard?","answer":"Design a Beam pipeline in Python: read Pub/Sub JSON, flatten nested fields, enrich with a Cloud Bigtable user-tier lookup, validate required fields, and upsert to a daily-partitioned BigQuery table. U","explanation":"## Why This Is Asked\n\nTests the ability to design an end-to-end ingestion, enrichment, and quality pattern using familiar GCP primitives, with an approachable yet realistic complexity for a beginner.\n\n## Key Concepts\n\n- Dataflow/Beam transforms\n- Flattening nested JSON\n- Cloud Bigtable enrichment\n- Idempotent BigQuery writes (insertId / upserts)\n- Dead-lettering and archival\n- 24-hour rollback strategy\n- Lightweight data quality monitoring\n\n## Code Example\n\n```python\n# Skeleton Beam pipeline outline\nimport apache_beam as beam\nfrom apache_beam.options.pipeline_options import PipelineOptions\nimport json\n\nclass FlattenAndEnrich(beam.DoFn):\n    def process(self, elem):\n        # placeholder for flatten/enrich logic\n        return [elem]\n\ndef run():\n  with beam.Pipeline(options=PipelineOptions()) as p:\n      msgs = (\n        p\n        | \"Read\" >> beam.io.ReadFromPubSub(subscription=\"projects/PROJECT/subscriptions/SUB\")\n        | \"Parse\" >> beam.Map(lambda b: json.loads(b.decode(\"utf-8\")))\n        | \"Flatten\" >> beam.ParDo(FlattenAndEnrich())\n        | \"WriteBQ\" >> beam.io.WriteToBigQuery(\n            table=\"PROJECT:DATASET.TABLE\",\n            write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND,\n            create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED\n        )\n      )\n\nif __name__ == \"__main__\":\n  run()\n```\n\n## Follow-up Questions\n\n- How would you test this with unit tests and integration tests?\n- How would you handle Bigtable lookup latency failures and retries?","diagram":null,"difficulty":"beginner","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T14:53:45.750Z","createdAt":"2026-01-22T14:53:45.751Z"},{"id":"q-5834","question":"Scenario: 100–500 MB/day of JSON click events arrive via Pub/Sub; design a beginner-friendly Dataflow (Beam) pipeline to parse, validate required fields, and normalize country codes to ISO 3166-1 alpha-2. Write to a daily-partitioned BigQuery table using insertId for idempotence; route malformed messages to a Pub/Sub dead-letter topic and archive originals to Cloud Storage. Include unit tests?","answer":"Design a Dataflow (Beam) streaming job (Python) to parse Pub/Sub JSON click events, validate required fields, and normalize country codes to ISO 3166-1 alpha-2. Write to a daily-partitioned BigQuery t","explanation":"## Why This Is Asked\nTests practical beginner skills in building a robust, observable Dataflow pipeline with real-world data quality needs: field validation, data normalization, and controlled error handling.\n\n## Key Concepts\n- Dataflow (Beam) streaming basics\n- Field validation and data normalization (country codes)\n- BigQuery inserts with insertId for idempotence\n- Dead-letter Queues and Cloud Storage archival\n- Unit testing transforms\n\n## Code Example\n```python\nclass NormalizeCountryFn(beam.DoFn):\n    MAP = {'US':'US','GB':'GB','CA':'CA'}\n    def process(self, element):\n        c = (element.get('country') or '').upper()\n        element['country'] = self.MAP.get(c, 'UNKNOWN')\n        yield element\n```\n\n## Follow-up Questions\n- How would you implement unit tests for this transform?\n- How would you handle new country codes and schema drift over time?","diagram":null,"difficulty":"beginner","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["PayPal","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T18:53:15.467Z","createdAt":"2026-01-22T18:53:15.467Z"},{"id":"q-5875","question":"Ingest 50 GB/day of JSON logs via Pub/Sub and route two event types to separate BigQuery tables (clicks and impressions) with distinct schemas. Build a beginner-friendly Dataflow (Beam) pipeline that parses messages, validates required fields, normalizes timestamps, and writes to daily-partitioned tables. Use insertId for idempotency, a dead-letter Pub/Sub for malformed messages, and archive originals to Cloud Storage. Propose a 24h rollback and a simple schema-evolution approach for adding fields to clicks?","answer":"Design a single Dataflow (Beam) job that parses Pub/Sub JSON, validates required fields (userId, timestamp, eventType), and routes events to two BigQuery tables (clicks and impressions) with separate ","explanation":"## Why This Is Asked\n\nAssesses practical Dataflow design for multi-table ingestion, basic validation, and idempotent writes in a beginner context. Includes fault handling (DLQ) and a rollback strategy to backfill.\n\n## Key Concepts\n\n- Dataflow (Beam) fundamentals\n- Pub/Sub ingestion and DLQ routing\n- BigQuery daily partitions and separate schemas\n- Idempotent writes with insertId\n- Simple schema evolution and archival\n\n## Code Example\n\n```javascript\n// Pseudo-beam DoFn skeleton for parsing and routing\nclass RouteEventFn extends DoFn {\n  processElement(ctx) {\n    // parse JSON, validate fields, emit to per-type PCollections\n  }\n}\n```\n\n## Follow-up Questions\n\n- How would you test idempotency and DLQ handling?\n- How would you monitor data quality and backlog in production?","diagram":null,"difficulty":"beginner","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Instacart","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T19:52:41.950Z","createdAt":"2026-01-22T19:52:41.950Z"},{"id":"q-5920","question":"Design an end-to-end testing framework for a two-region GCP streaming analytics pipeline (Pub/Sub -> Dataflow -> BigQuery) handling 1 TB/day of JSON events. Include deterministic per-tenant data generation, staging dataflow run, idempotent upserts and exact-once validation, schema-change tests, fault-injection scenarios, and automatic backfill rollback using partitioned tables; verify via Data Catalog lineage?","answer":"Implement a CI/CD-driven end-to-end test harness that generates deterministic synthetic events per tenant, publishes them to Pub/Sub, executes a Dataflow pipeline in staging, and validates idempotent upserts with exact-once semantics. Utilize partitioned BigQuery tables for automatic backfill rollback, incorporate schema-change test scenarios, implement fault injection for cross-region resilience testing, and verify end-to-end lineage through Data Catalog metadata.","explanation":"## Why This Is Asked\nTests a candidate's ability to design robust, scalable testing for streaming data pipelines with real-world concerns: schema evolution, fault tolerance, and cross-region behavior.\n\n## Key Concepts\n- End-to-end testing of Pub/Sub/Dataflow/BigQuery flows\n- Deterministic synthetic data and per-tenant contracts\n- Idempotence, exactly-once semantics, and schema evolution\n- Fault injection and backfill rollback mechanisms\n- Data Catalog lineage verification\n\n## Code Example\n```javascript\n// Pseudo test harness sketch\n```\n\n## Follow-up Questions\n- How would you integrate this with existing CI/CD pipelines?\n- What monitoring and alerting would you implement for test failures?\n- How would you handle schema drift between test and production environments?","diagram":"flowchart TD\n  A[Generate synthetic events] --> B[Publish to Pub/Sub]\n  B --> C[Dataflow pipeline in staging]\n  C --> D[BigQuery validation]\n  D --> E[Data Catalog lineage check]","difficulty":"advanced","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T05:51:15.905Z","createdAt":"2026-01-22T22:04:42.757Z"},{"id":"q-5941","question":"Design a GCP streaming pipeline to detect per-tenant data drift in real time. Ingest 2 TB/day of JSON events from Pub/Sub, process with Dataflow across two regions, and write per-tenant feature vectors to BigQuery. Compute drift metrics (distribution shifts, KS tests) per tenant and auto-trigger Vertex AI retraining when thresholds are exceeded. Ensure Data Catalog lineage and a 24h rollback plan?","answer":"Implement a multi-region streaming pipeline with Pub/Sub ingestion, Dataflow processing across two regions, and per-tenant BigQuery tables for feature vectors. Compute real-time drift metrics including distribution shifts and Kolmogorov-Smirnov tests, storing per-tenant drift scores. Configure Vertex AI retraining triggers when drift thresholds are exceeded, maintain comprehensive Data Catalog lineage tracking, and establish a 24-hour rollback plan with versioned model deployments.","explanation":"## Why This Is Asked\nThis question evaluates real-time, tenant-isolated data drift detection and automated model retraining capabilities—a critical production challenge at scale.\n\n## Key Concepts\n- Real-time streaming with multi-region Dataflow\n- Per-tenant isolation in BigQuery\n- Data Catalog lineage tracking\n- Vertex AI automated retraining triggers\n- Drift metrics: distribution shifts and Kolmogorov-Smirnov tests\n\n## Code Example\n```javascript\n// Pseudo-code: evaluate drift score and trigger retraining\nfunction evaluateDriftAndRetrain(tenantId, driftScore, threshold) {\n  if (driftScore > threshold) {\n    triggerVertexAIRetraining(tenantId);\n  }\n}\n```","diagram":"flowchart TD\n  A[Pub/Sub] --> B[Dataflow (2 regions)]\n  B --> C[BigQuery: per-tenant tables]\n  C --> D[Drift store per tenant]\n  D --> E[Vertex AI retraining]\n  E --> F[Data Catalog lineage]","difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","Twitter","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T05:38:24.514Z","createdAt":"2026-01-22T22:50:49.467Z"},{"id":"q-5984","question":"Design a cross-region, cross-tenant analytics platform on GCP for 2 TB/day of JSON activity ingested via Pub/Sub. Implement per-tenant privacy with masking, share aggregated metrics with a partner via BigQuery Data Exchange, enforce data contracts in Data Catalog, and export end-to-end lineage and immutable audit logs to Cloud Storage. Include a 24h per-tenant rollback plan, testing strategy, and failure scenarios?","answer":"Two-region pipeline: Pub/Sub → Dataflow → regional BigQuery raw layer, with per-tenant masked analytics views. Leverage BigQuery Data Exchange to share aggregated metrics with partners, enforcing per-tenant contracts through Data Catalog. Export comprehensive lineage and immutable audit logs to Cloud Storage. Implement 24-hour per-tenant rollback via time travel and dedicated backup tables. Include robust testing strategy and failure scenario handling.","explanation":"## Why This Is Asked\n\nThis question evaluates end-to-end design capabilities for cross-region, multi-tenant data sharing on GCP, emphasizing privacy controls, data lineage, and rollback mechanisms.\n\n## Key Concepts\n\n- Cross-region BigQuery governance\n- Data Exchange and partner sharing\n- Data masking and per-tenant isolation\n- Data lineage and testing frameworks\n- Rollback and resilience patterns\n\n## Code Example\n\n```javascript\n// Simple masking helper\nfunction mask(str) { return str ? '***' : null; }\n```\n\n## Follow-up Questions\n\n- How would you implement per-tenant rollbacks if a tenant's data exceeds the 24-hour time travel window?\n- What strategies would you use to optimize cross-region data transfer costs?\n- How would you handle schema evolution while maintaining data contracts?","diagram":"flowchart TD\n  A[Pub/Sub] --> B[Dataflow]\n  B --> C[Regional BigQuery Raw]\n  C --> D[Per-Tenant Masked Views]\n  C --> E[Data Exchange Share]\n  D --> F[Cloud Storage (Lineage/Audit)]\n  E --> G[Partner Analytics]","difficulty":"advanced","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T04:54:32.468Z","createdAt":"2026-01-23T02:40:55.972Z"},{"id":"q-6001","question":"On GCP, design a real-time telemetry pipeline for a multi-tenant SaaS with 1-2 TB/day of JSON events ingested via Pub/Sub across two regions. Requirements: strict tenant isolation; per-field masking with policy tags; end-to-end lineage via Data Catalog; automatic schema drift detection with backward-compatible evolution; raw Parquet in regional Cloud Storage; per-tenant BigQuery datasets; late-arrival handling; 24-hour per-tenant rollback and test strategy; immutable audit logs to Cloud Storage; define data models, workflows, and rollback tests?","answer":"Implement Pub/Sub → Dataflow streaming with tenant isolation; write per-tenant BigQuery datasets and Parquet raw in regional Cloud Storage. Use Data Catalog policy tags for field masking, and a schema","explanation":"## Why This Is Asked\nTests ability to design a scalable, compliant streaming pipeline with tenant isolation, governance, and rollback capabilities in GCP.\n\n## Key Concepts\n- Multi-tenant data models and per-tenant governance\n- Streaming ETL with Dataflow and Pub/Sub\n- Data Catalog lineage and policy tags for masking\n- Schema registry and backward-compatible evolution\n- Per-tenant storage in BigQuery + Parquet in regional Cloud Storage\n- Rollback strategies, canary testing, and immutable audit logs\n\n## Code Example\n```python\n# simplified Dataflow mask example (pseudo)\nclass MaskFields(beam.DoFn):\n  def __init__(self, tenant_id):\n    self.tenant_id = tenant_id\n  def process(self, element):\n    record = json.loads(element)\n    if 'email' in record:\n      record['email'] = mask(record['email'])  # per-tenant policy\n    yield json.dumps(record).encode('utf-8')\n```\n\n## Follow-up Questions\n- How would you implement per-tenant schema drift detection and automatic evolution without breaking queries?\n- What testing strategy ensures safe 24h rollbacks under peak load?","diagram":"flowchart TD\n  A[Pub/Sub] --> B[Dataflow Streaming]\n  B --> C[BigQuery (per-tenant)]\n  B --> D[Parquet (regional Storage)]\n  A --> E[Data Catalog lineage]\n  C --> F[Audit Logs to Cloud Storage]\n  D --> F","difficulty":"advanced","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Apple","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T04:17:24.660Z","createdAt":"2026-01-23T04:17:24.660Z"},{"id":"q-6067","question":"Design a two-region, multi-tenant analytics pipeline on GCP for 2 TB/day of JSON events ingested via Pub/Sub. Implement per-tenant isolation with **CMEK** (Cloud KMS), separate datasets and authorized views. Enable cross-tenant aggregation via **BigQuery Data Exchange** and ensure end-to-end lineage with **Data Catalog**. Detail CMEK rotation, rollback plan (24h), testing, and failure modes?","answer":"Create per-tenant datasets and Authorized Views; use CMEK keys for each tenant in Cloud KMS and policy-based key rotation. Ingest via Pub/Sub into Dataflow, write to partitioned BigQuery tables; share","explanation":"## Why This Is Asked\nThis question probes secure multi-tenant data handling on GCP, focusing on customer-managed encryption keys, cross-region data sharing, and governance tooling.\n\n## Key Concepts\n- CMEK per-tenant using Cloud KMS\n- Tenant isolation via separate BigQuery datasets and Authorized Views\n- Cross-tenant sharing via BigQuery Data Exchange\n- End-to-end lineage with Data Catalog; IAM-based access controls\n- Rollback, testing strategies, and failure modes in a multi-region setup\n\n## Code Example\n```javascript\n// Example: binding a per-tenant KMS key to a BigQuery dataset writer in Dataflow (pseudo)\nconst kmsKey = \"projects/myproj/locations/global/keyRings/tenant-keys/cryptoKeys/tenantA\";\nwriter.setKmsKey(kmsKey);\n```\n\n## Follow-up Questions\n- How would you implement a 24h rollback for data already written?\n- How would you test CMEK rotation without affecting live data?","diagram":null,"difficulty":"advanced","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Square","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T07:36:15.508Z","createdAt":"2026-01-23T07:36:15.508Z"},{"id":"q-6201","question":"Design a beginner-friendly GCP data pipeline to ingest 50 MB/day of JSON API logs via Pub/Sub into BigQuery. Use Dataflow (Beam) to parse and validate fields (timestamp, user_id, endpoint). Propose a simple schema-evolution approach (core schema in BigQuery plus an extras JSON field or per-day tables). Ensure idempotent writes (insertId/MERGE), route malformed messages to a dead-letter Pub/Sub topic and archive originals to Cloud Storage, and outline a 24-hour rollback and basic monitoring plan?","answer":"Design a streaming Dataflow pipeline (Python) that reads Pub/Sub JSON logs, validates timestamp/user_id/endpoint, and writes to BigQuery with a fixed core schema; store extra fields in a JSON extras c","explanation":"## Why This Is Asked\n\nTests ability to implement a simple, reliable streaming ingest with validation, a practical approach to schema evolution, and basic operations like dead-lettering, archiving, rollback, and monitoring at beginner level.\n\n## Key Concepts\n\n- Pub/Sub as a source and dead-lettering for bad messages\n- Dataflow (Beam) for parsing/validation\n- BigQuery core schema + extras JSON field for evolution\n- Idempotent writes using insertId or MERGE\n- Rollback strategies using Pub/Sub replay or GCS archive\n- Basic monitoring and alerts for data quality\n\n## Code Example\n\n```javascript\n// Pseudo-beam style pseudocode\n// Read from Pub/Sub, parse JSON, validate required fields, emit rows to BigQuery\nclass ParseAndValidateDoFn extends DoFn {\n  processElement(ctx) {\n    const msg = JSON.parse(ctx.element);\n    if (!msg.timestamp || !msg.user_id || !msg.endpoint) return;\n    const row = {core_field1: msg.timestamp, core_field2: msg.user_id, core_field3: msg.endpoint, extras: JSON.stringify(msg.extras || {})};\n    yield row;\n  }\n}\n```\n\n## Follow-up Questions\n\n- How would you test schema-evolution choices and measure their impact on query performance?\n- What would you monitor to detect data quality regressions in this pipeline?","diagram":"flowchart TD\n  A[Pub/Sub Ingest] --> B[Dataflow: Parse & Validate]\n  B --> C[BigQuery: Core Schema]\n  B --> D[BigQuery: Extras JSON]\n  C --> E[(InsertId/MERGE)]\n  D --> E\n  A --> F[Archive Originals to Cloud Storage]\n  E --> G[Monitoring & Alerts]","difficulty":"beginner","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","Meta","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T13:45:37.548Z","createdAt":"2026-01-23T13:45:37.548Z"},{"id":"q-6319","question":"Ingest multi-app telemetry via Pub/Sub into per-app BigQuery datasets: design a beginner-friendly GCP streaming pipeline using Dataflow that validates required fields, writes to daily-partitioned per-app tables, uses idempotent upserts, routes malformed messages to a dead-letter Pub/Sub topic, archives originals to Cloud Storage, enforces per-app quotas via slot reservations, and supports a 24h rollback by replaying archived data. Outline architecture, schemas, and testing plan?","answer":"Propose a Dataflow streaming job that reads Pub/Sub, validates required fields (app, timestamp, event), applies per-app schemas, writes to daily-partitioned per-app BigQuery tables using MERGE/insertI","explanation":"## Why This Is Asked\nTests ability to design a practical, cost-aware multi-app ingest with isolation and rollback.\n\n## Key Concepts\n- Dataflow streaming from Pub/Sub\n- Per-app BigQuery schemas and datasets\n- Idempotent writes (MERGE/insertId)\n- Dead-letter Pub/Sub and archival\n- Per-app quotas and slot reservations\n- 24h rollback via replay from Cloud Storage\n\n## Code Example\n```javascript\n// Pseudo-Beam skeleton for Dataflow pipeline in JS-like syntax\nimport beamsdk from 'beam-js';\nconst p = new beamsdk.Pipeline();\n// ... validation, routing, and write steps\n```\n\n## Follow-up Questions\n- How would you test rollback without affecting other apps?\n- How would you monitor ingestion latency and quota usage?","diagram":"flowchart TD\n  PubSub[Pub/Sub] --> DF[Dataflow job]\n  DF --> BQ[BigQuery per-app tables]\n  DF --> Dead[Dead-Letter Pub/Sub]\n  DF --> Archive[Cloud Storage archive]\n  Archive --> Rollback[24h rollback by re-ingest]","difficulty":"beginner","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","NVIDIA","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T19:06:01.260Z","createdAt":"2026-01-23T19:06:01.260Z"},{"id":"q-6566","question":"Ingest 100 MB/day of JSON activity via Pub/Sub for multi-tenant analytics. Build a beginner Dataflow-based pipeline that flattens nested payloads, validates required fields, deduplicates on event_id, writes to a BigQuery table partitioned by ingestion_date and clustered by tenant_id, routes malformed messages to a dead-letter Pub/Sub topic, archives originals to Cloud Storage, and supports a per-tenant 24h rollback by replaying from staging. Include testing and basic monitoring?","answer":"Implement a Dataflow (Beam) pipeline that flattens nested JSON, validates required fields, and deduplicates on event_id. Write to a BigQuery table partitioned by ingestion_date and clustered by tenant","explanation":"## Why This Is Asked\nThis question tests practical data ingestion, de-dup logic, and per-tenant isolation in a real GCP pipeline, a common beginner-to-intermediate scenario.\n\n## Key Concepts\n- Dataflow (Beam) pipelines, Pub/Sub integration, BigQuery partitioning and clustering\n- Idempotent upserts with event_id, per-tenant isolation via tenant_id\n- Dead-letter handling and archival strategy, basic monitoring\n- Rollback using staged data and replay window, testing plan\n\n## Code Example\n```javascript\n// Pseudocode: dedupe by event_id within a fixed window\nfunction dedupe(events) {\n  const seen = new Set();\n  return events.filter(e => {\n    if (seen.has(e.event_id)) return false;\n    seen.add(e.event_id);\n    return true;\n  });\n}\n```\n\n## Follow-up Questions\n- How would you test idempotent writes and rollback reliably?\n- What are the cost implications of partitioning by day and clustering by tenant_id?","diagram":null,"difficulty":"beginner","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Google","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T08:36:48.211Z","createdAt":"2026-01-24T08:36:48.211Z"},{"id":"q-6723","question":"Design a beginner-friendly GCP streaming pipeline for multi-tenant JSON activity ingested via Pub/Sub into per-tenant BigQuery datasets. Implement tenant-level masking for sensitive fields with DLP at ingest, enforce per-tenant quotas, and provide a 24h rollback that replays raw data from Cloud Storage to affected tenants only. Outline architecture, data models, and a basic testing plan?","answer":"Ingest with Dataflow (Beam) reading Pub/Sub, route by tenant to per-tenant BigQuery datasets; create datasets on demand. Apply DLP masking for sensitive fields (email, phone) in the pipeline. Use a pe","explanation":"## Why This Is Asked\nThis angle explores multi-tenant isolation, masking, and rollback in a beginner-friendly way.\n\n## Key Concepts\n- Dataflow (Beam) streaming pipelines\n- Pub/Sub for ingestion\n- Per-tenant BigQuery datasets\n- Data Loss Prevention masking in ingest\n- Idempotent writes with MERGE\n- Dead-letter queues and raw data archiving\n- 24h rollback via replay from Cloud Storage\n- Observability through Dataflow metrics\n\n## Code Example\n```javascript\n// Dataflow masking sketch (pseudo)\nclass MaskPII {\n  process(element) {\n    if (element.email) element.email = '*****@*****.com';\n    if (element.phone) element.phone = '**********';\n    return element;\n  }\n}\n```\n\n## Follow-up Questions\n- How would you test data quality across tenants? \n- How would you enforce per-tenant quotas in BigQuery and Dataflow?","diagram":null,"difficulty":"beginner","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Meta","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T14:40:46.365Z","createdAt":"2026-01-24T14:40:46.365Z"},{"id":"q-6734","question":"On GCP, architect a fault-tolerant, multi-tenant streaming pipeline for 2 TB/day of JSON events ingested via Pub/Sub. Each tenant must have per-tenant backpressure budgets, ingestion-time masking using tenant-specific policies stored in Data Catalog, and regional data residency. Include a streaming path (Dataflow vs Dataflow Flex), per-tenant schema evolution, end-to-end data lineage, and immutable audit logs to Cloud Storage. Provide a 24h per-tenant rollback plan, a testing strategy, and failure scenarios?","answer":"Pub/Sub -> Dataflow with per-tenant partitioning; regional Dataflow workers bound to tenant region; ingestion-time masking driven by per-tenant policies stored in Data Catalog (masking fields, redacti","explanation":"## Why This Is Asked\n\nTests ability to design a scalable, compliant multi-tenant streaming pipeline with privacy, residency, rollback.\n\n## Key Concepts\n\n- Multi-tenant isolation and regional residency\n- Pub/Sub and Dataflow streaming ETL\n- Policy-driven masking via Data Catalog\n- End-to-end data lineage with Data Catalog\n- Immutable audit logs in Cloud Storage\n- Rollback strategy using BigQuery time travel and log replay\n\n## Code Example\n\n```javascript\nfunction maskByTenant(record, policy) {\n  // apply masking rules per tenant\n  // e.g., redact SSN if policy.requiresMask and field matches\n  return maskedRecord\n}\n```\n\n## Follow-up Questions\n\n- How would you test masking policy drift across tenants?\n- How would you scale masking latency to thousands of tenants while preserving SLA?","diagram":null,"difficulty":"advanced","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Goldman Sachs","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T15:28:50.023Z","createdAt":"2026-01-24T15:28:50.024Z"},{"id":"q-6806","question":"Design a beginner-friendly GCP data pipeline to ingest 100 MB/day of JSON logs from Pub/Sub into BigQuery using Dataflow. Flatten nested JSON, normalize timestamps, and enrich with a small country lookup table stored in BigQuery. Isolate tenants with separate datasets, implement idempotent writes, route malformed messages to a dead-letter topic, and implement a 24-hour rollback by re-ingesting archived JSON from Cloud Storage. Include architecture, data model, and testing plan?","answer":"Design a Dataflow (Python) streaming pipeline: Pub/Sub -> Dataflow -> per-tenant BigQuery datasets. Flatten nested JSON, normalize event_time, and enrich with a country lookup in BigQuery via a side i","explanation":"## Why This Is Asked\nTests ability to translate a beginner scenario into a repeatable streaming pipeline with basic features: parsing, enrichment, per-tenant isolation, dedup, DLQ, and archival for rollback. Ensures familiarity with Dataflow, Pub/Sub, and BigQuery integration.\n\n## Key Concepts\n- Dataflow DoFn for flattening and enrichment\n- Pub/Sub schema and message validation\n- Per-tenant isolation via separate BigQuery datasets\n- Idempotent writes with insertId or MERGE\n- Dead-letter handling for bad messages\n- Cloud Storage archival for rollback\n- 24h rollback workflow by re-ingesting archived data\n\n## Code Example\n```python\nimport apache_beam as beam\nimport json\n\nclass FlattenEnrich(beam.DoFn):\n    def process(self, element, country_lookup):\n        record = json.loads(element)\n        data = {\n            \"tenant\": record.get(\"tenant\"),\n            \"event_time\": record.get(\"event_time\"),\n            \"user_id\": record.get(\"user\", {}).get(\"id\"),\n            \"country\": country_lookup.get(record.get(\"region\"), \"unknown\")\n        }\n        yield data\n```\n\n## Follow-up Questions\n- How would you test idempotency and message deduplication in this pipeline?\n- How would you handle late or out-of-order data while preserving correct event-time semantics?","diagram":"flowchart TD\n  A[Pub/Sub] --> B[Dataflow]\n  B --> C[BigQuery (per-tenant)]\n  B --> D[Cloud Storage Archive]\n  B --> E[Dead-letter Pub/Sub]","difficulty":"beginner","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Meta","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T18:39:48.330Z","createdAt":"2026-01-24T18:39:48.333Z"},{"id":"q-6846","question":"Design a cross-region data platform on GCP for two tenants ingesting 3 TB/day of JSON telemetry via Pub/Sub. Each tenant has isolated BigQuery datasets and Vertex AI Feature Stores. Include per-tenant privacy controls (masking/DP), end-to-end lineage via Data Catalog, backward-compatible schema evolution, and a 24h rollback plan with backfill and testing. Outline architecture, data models, and failure scenarios?","answer":"Leverage Pub/Sub -> Dataflow pipelines to regional BigQuery datasets and per-tenant Vertex AI Feature Stores. Enforce isolation, per-tenant masking/DP, and end-to-end lineage via Data Catalog. Support","explanation":"## Why This Is Asked\n\nTests multi-tenant isolation, real-time streaming, and integration of Vertex AI Feature Stores with governance and privacy controls. Also probes rollback, testing, and failure handling across regions.\n\n## Key Concepts\n\n- Pub/Sub to Dataflow pipelines\n- Per-tenant BigQuery datasets and Vertex AI Feature Stores\n- Data Catalog lineage and masking/DP\n- Backward-compatible schema evolution\n- Point-in-time recovery and feature store snapshots for rollback\n\n## Code Example\n\n```yaml\n# high-level configuration sketch\ningest:\n  source: Pub/Sub\n  flow: Dataflow\ntenants:\n  - id: tenant1\n    bigquery: project/tenant1\n    feature_store: tenant1/fs\n  - id: tenant2\n    bigquery: project/tenant2\n    feature_store: tenant2/fs\nprivacy:\n  masking: true\n  differential_privacy: true\nlineage:\n  publisher: Data Catalog\n```\n\n## Follow-up Questions\n\n- How would you monitor per-tenant data freshness and latency?\n- How would you test schema drift across tenants and rollback safely?","diagram":"flowchart TD\n  Ingest[Pub/Sub] --> Dataflow[Dataflow]\n  Dataflow --> BIQ1[BigQuery - Tenant1]\n  Dataflow --> BIQ2[BigQuery - Tenant2]\n  BIQ1 --> FStore1[Vertex AI Feature Store - Tenant1]\n  BIQ2 --> FStore2[Vertex AI Feature Store - Tenant2]\n  BIQ1 --> Catalog[Data Catalog]\n  BIQ2 --> Catalog\n  FStore1 --> Catalog\n  FStore2 --> Catalog","difficulty":"advanced","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T19:40:08.843Z","createdAt":"2026-01-24T19:40:08.843Z"},{"id":"q-6912","question":"Design a real-time data quality and governance pipeline on GCP for multi-tenant IoT telemetry (1-2 TB/day) ingested from Pub/Sub into per-tenant BigQuery tables across two regions. Include ingestion-time anomaly detection, per-tenant quarantine, auto-remediation via rule engine, lineage to Data Catalog, and dashboards for data quality with rollback for quarantined data?","answer":"Route Pub/Sub streams through Dataflow streaming templates that shard by tenant and write to regional BigQuery datasets across two regions. Implement ingestion-time quality checks including schema drift validation, range verification, and null value detection. Failed records are routed to per-tenant dead-letter tables with auto-remediation via Cloud Functions rule engine. Maintain lineage in Data Catalog with custom metadata for quality metrics and quarantine status. Provide Looker dashboards for data quality monitoring and rollback capabilities for quarantined data restoration.","explanation":"## Why This Is Asked\n\nThis question probes end-to-end streaming data quality, per-tenant isolation, regional replication, and governance integration under real-world failure modes. It also tests operational thinking around dead-letter handling, auto-remediation, rollback, and observability.\n\n## Key Concepts\n\n- Streaming pipelines: Pub/Sub → Dataflow\n- Tenant-aware routing and regional BigQuery\n- Data quality gates: schema drift, range checks, null handling\n- Dead-letter queues and auto-remediation\n- Data Catalog lineage and access controls\n- Rollback and backfill strategies\n\n## Code Example\n\n```java\n// Dataflow pipeline with quality checks\nPipeline p = Pipeline.create(options);\n\np.apply(\"ReadFromPubSub\", PubsubIO.readStrings()\n    .fromTopic(\"projects/PROJECT/topics/iot-telemetry\"))\n .apply(\"ParseAndValidate\", ParDo.of(new ValidateAndEnrichFn()))\n .apply(\"TenantRouting\", ParDo.of(new TenantRouterFn()))\n .apply(\"QualityChecks\", ParDo.of(new DataQualityValidatorFn()))\n .apply(\"WriteToBigQuery\", BigQueryIO.writeTableRows()\n    .to(new DynamicTenantDestinations())\n    .withSchema(TELEMETRY_SCHEMA)\n    .withWriteDisposition(BigQueryIO.Write.WriteDisposition.WRITE_APPEND));\n```","diagram":null,"difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T04:58:24.854Z","createdAt":"2026-01-24T22:37:16.375Z"},{"id":"q-7044","question":"On GCP, design a multi-tenant streaming analytics pipeline to ingest 5 TB/day of JSON events via Pub/Sub, produce per-tenant aggregates in BigQuery with strict privacy (masking/tokenization) and end-to-end lineage via Data Catalog, while exposing aggregated metrics to a partner via Data Exchange. Include 24h per-tenant rollback, data contracts, and a testing/rollback strategy with concrete data models and failure scenarios?","answer":"Architect a tenant-scoped streaming pipeline: Pub/Sub -> Dataflow streaming with per-tenant transforms, write to partitioned BigQuery tables (tenant_id as partition). Mask PII in a dedicated DoFn and ","explanation":"## Why This Is Asked\n\nThe question probes end-to-end expertise in GCP streaming, privacy, governance, and rollback in a multi-tenant setup, focusing on real-time analytics.\n\n## Key Concepts\n\n- Pub/Sub, Dataflow streaming, BigQuery partitioned tables, Data Catalog data contracts, Data Exchange.\n\n- End-to-end lineage, masking/tokenization, per-tenant isolation, rollback strategy.\n\n## Code Example\n\n```python\n# Example (pseudo)\nclass MaskPII(DoFn):\n    def process(self, element):\n        tenant = element['tenant_id']\n        data = element.copy()\n        if 'email' in data: data['email'] = mask_email(data['email'])\n        if 'phone' in data: data['phone'] = mask_phone(data['phone'])\n        yield data\n```\n\n## Follow-up Questions\n\n- How would you test the rollback workflow per tenant and handle partial failures?\n- What metrics and alerting would validate end-to-end latency and privacy guarantees?","diagram":null,"difficulty":"advanced","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Netflix","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T07:02:38.695Z","createdAt":"2026-01-25T07:02:38.695Z"},{"id":"q-7191","question":"Design a beginner-friendly GCP ingestion pipeline for ~200 GB/day of CSV logs stored in Cloud Storage. The pipeline must read CSV, validate required fields (run_id, ts, user_id, action), deduplicate by run_id, and upsert into a daily-partitioned BigQuery table. Support a forward-compatible schema evolution (optional device_type). Implement basic monitoring with Cloud Monitoring alerts for failed files and load errors, and a 24h rollback plan (delete and reload last day’s partition). Provide concrete steps and trade-offs?","answer":"Use Cloud Dataflow (Python) batch pipeline to read GCS CSVs, validate fields, deduplicate by run_id, and upsert to a daily-partitioned BigQuery table via MERGE. Add optional device_type via a schema_v","explanation":"## Why This Is Asked\nThis question tests practical data ingestion basics: structured parsing, idempotent loads, and operational guardrails for a beginner-friendly GCP data-engineering task. It introduces schema evolution and rollback early to assess stability concerns.\n\n## Key Concepts\n- Batch vs streaming Dataflow on GCP\n- Deduplication by run_id and MERGE into BigQuery\n- Schema evolution with optional fields and versioning\n- Basic monitoring and alerting with Cloud Monitoring\n- 24h rollback strategy for partitioned tables\n\n## Code Example\n```javascript\n// Pseudocode: read from GCS, validate, deduplicate by run_id, write with MERGE\nfunction process(batch){\n  let dedup = batch.uniqueBy(r => r.run_id);\n  // upsert into BigQuery daily partition\n  bigQueryMERGE('dataset.table$${date}', dedup);\n}\n```\n\n## Follow-up Questions\n- How would you test late-arriving data?\n- How would you handle schema drift over time?","diagram":null,"difficulty":"beginner","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Scale Ai","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T13:22:49.066Z","createdAt":"2026-01-25T13:22:49.066Z"},{"id":"q-7241","question":"Design a beginner-friendly GCP data pipeline to load 5 GB/day of newline-delimited JSON logs stored in Cloud Storage (gs://logs/YYYY/MM/DD/part-*.json) into a single BigQuery table partitioned by ingestion date. Use Dataflow (Beam) to parse/validate fields event_id, user_id, ts, action, implement idempotent upserts via MERGE, route malformed records to a Cloud Storage errors bucket, archive originals, and propose a 24h rollback plan plus basic data quality checks and monitoring. How would you implement this?","answer":"Implement a Batch Dataflow job reading gs://logs/YYYY/MM/DD/part-*.json, validate event_id, user_id, ts, action, and upsert into a daily-partitioned BigQuery table using MERGE on event_id. Push malfor","explanation":"## Why This Is Asked\nTests batch ingestion, idempotent updates, and rollback strategies in GCP. It adds a batch-from-GCS angle not covered by Pub/Sub-centric questions, plus partitioned BigQuery writes and a partition-swap rollback.\n\n## Key Concepts\n- Batch Dataflow pipelines\n- BigQuery MERGE for upserts\n- GCS-based error handling\n- Daily partitioned tables\n- Rollback by partition swap\n- Basic data quality and monitoring\n\n## Code Example\n```python\nimport apache_beam as beam\nimport json\n\nclass ParseJson(beam.DoFn):\n    def process(self, element):\n        try:\n            obj = json.loads(element)\n            if all(k in obj for k in (\"event_id\",\"user_id\",\"ts\",\"action\")):\n                yield obj\n        except Exception:\n            pass\n```\n\n## Follow-up Questions\n- How would you test schema evolution in this setup?\n- How would you validate rollback correctness under partial failures?","diagram":"flowchart TD\n  GCSInput[GCS input: gs://logs/YYYY/MM/DD/part-*.json] --> DF[Dataflow batch job]\n  DF --> BQ[BigQuery daily partitions]\n  DF --> ERR[Errors bucket]\n  BQ --> ARCH[Archive originals]\n  BQ --> ROLLBACK[Rollback via 24h partition backup]","difficulty":"beginner","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Square","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T14:54:56.860Z","createdAt":"2026-01-25T14:54:56.860Z"},{"id":"q-7284","question":"On GCP, design a beginner-friendly data ingestion pipeline for 100 GB/day of CSV logs stored in Cloud Storage. Ingest into BigQuery (single region) using Dataflow (Beam) to parse/validate required fields, perform minimal transformations, and write to a daily partitioned table. Include dead-letter handling, basic data quality checks, and a 24h per-day rollback plan (delete and re-ingest that day's data)?","answer":"Ingest 100 GB/day CSVs from Cloud Storage into a daily-partitioned BigQuery table via a Dataflow pipeline. Validate schema, skip bad rows to a DLQ, deduplicate by a natural key, and implement a 24h ro","explanation":"## Why This Is Asked\nTests ability to design a simple, reproducible data ingestion flow leveraging Dataflow and BigQuery with basic data quality and rollback.\n\n## Key Concepts\n- Dataflow batch/stream patterns, BigQuery partitioning, DLQ, schema validation, idempotent writes, rollback by partition delete, basic monitoring.\n\n## Code Example\n```javascript\n// Pseudo Dataflow-like sketch: parse CSV, validate, write to BigQuery, emit DLQ\nconst rows = readCSV(inputFiles)\n  .map(parseRow)\n  .filter(r => r.isValid)\n  .map(r => ({...r, ingestTime: now()}))\n\nwriteToBigQuery(rows, 'project.dataset.table$YYYYMMDD')\n\n// DLQ handling for invalid rows\nconst bad = rows.filter(r => !r.isValid)\nwriteToDLQ(bad, 'dlq-topic')\n```\n\n## Follow-up Questions\n- How would you test the rollback script and ensure idempotence across re-ingestions?\n- What are simple data quality thresholds and how would you monitor them?\n","diagram":"flowchart TD\n  A[Cloud Storage CSVs] --> B[Dataflow Pipeline]\n  B --> C[BigQuery Daily Partition]\n  B --> D[Dead-letter Cloud Pub/Sub/Storage]\n  C --> E[Monitoring/Alerts]","difficulty":"beginner","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T16:51:11.351Z","createdAt":"2026-01-25T16:51:11.351Z"},{"id":"q-7390","question":"On GCP, 3 TB/day of nested JSON logs arrive in Cloud Storage. Design a beginner-friendly batch pipeline using Dataflow to flatten JSON, load to a partitioned BigQuery production table by ingestion_date, and upsert via MERGE from a staging table. Include (a) schema file or simple discovery, (b) per-field masking for PII, (c) basic data quality checks and alerts, and (d) a 24h rollback plan?","answer":"Dataflow batch job (Python) to parse nested JSON, flatten to a flat schema, and write to a staging BigQuery table partitioned by ingestion_date. Use a MERGE into production table by a stable id. Maintain schema files in Cloud Storage for consistency, apply field-level PII masking during transformation, implement data quality checks with Cloud Monitoring alerts, and maintain a 24-hour rollback strategy using table snapshots and version control.","explanation":"## Why This Is Asked\nTests practical batch ingestion, data quality checks, and rollback procedures in GCP.\n\n## Key Concepts\n- Dataflow batch transforms\n- BigQuery partitioning and MERGE\n- Schema management in Cloud Storage\n- PII masking\n- Cloud Monitoring alerts\n- Rollback strategies\n\n## Code Example\n```javascript\n// Pseudo Dataflow snippet: read from GCS, flatten, write to staging\n```\n\n## Follow-up Questions\n- How would you test schema drift across days?\n- How would you validate rollback integrity if staging also becomes unavailable?","diagram":null,"difficulty":"beginner","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","NVIDIA","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T06:54:55.127Z","createdAt":"2026-01-25T21:28:32.189Z"},{"id":"q-7422","question":"Design a beginner-friendly GCP data pipeline to ingest 400 GB/day of JSON events via Pub/Sub into BigQuery in a single region. Include per-tenant data isolation using daily partitions, a simple field-level masking step, idempotent writes with MERGE, a 24h Cloud Storage backup for rollback, and a lightweight data lineage log exported to Cloud Storage. Provide a basic test plan with synthetic data and rollback validation?","answer":"Implement a Dataflow streaming job that reads Pub/Sub JSON messages, applies field-level PII masking in a custom DoFn, and writes to daily-partitioned BigQuery tables using MERGE statements for idempotent upserts. Maintain 24-hour backups of raw Pub/Sub data in Cloud Storage for rollback capability, with per-tenant isolation enforced through partition pruning. Generate lightweight lineage logs tracking job metrics and schema changes, exported to Cloud Storage and optionally registered in Data Catalog.","explanation":"## Why This Is Asked\nTests practical familiarity with Pub/Sub, Dataflow, and BigQuery in a tenant-aware, beginner-friendly setup, plus rollback and lineage basics.\n\n## Key Concepts\n- Dataflow streaming pipelines and Beam transforms\n- BigQuery MERGE for idempotent upserts\n- Field-level masking in-flight\n- Daily partitions for tenant isolation\n- Cloud Storage backups for rollback\n- Lightweight lineage logs and Data Catalog basics\n\n## Code Example\n```javascript\nfunction maskPII(record) {\n  if (record.email) record.email = 'redacted@example.com';\n  if (record.phone) record.phone = '000-000-0000';\n}","diagram":null,"difficulty":"beginner","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Google","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T06:33:29.625Z","createdAt":"2026-01-25T22:34:59.403Z"},{"id":"q-7703","question":"On GCP, design a per-tenant streaming analytics pipeline where thousands of tenants emit JSON events to Pub/Sub with evolving schemas. Build a per-tenant self-healing schema registry, ingest with Dataflow, validate against registry, store raw events as Parquet in Cloud Storage, and write per-tenant aggregates in BigQuery. Include drift detection, rollback strategy within 24h, data contracts, Data Catalog lineage, and a testing plan?","answer":"Leverage a tenant-scoped Dataflow streaming pipeline. Ingest JSON from Pub/Sub, validate vs a per-tenant schema registry stored in Cloud Storage, and apply tenant-specific transforms. Write raw events","explanation":"## Why This Is Asked\n\nTests ability to design per-tenant schema evolution, data contracts, and end-to-end lineage in GCP.\n\n## Key Concepts\n\n- Streaming ingestion with Pub/Sub and Dataflow\n- Per-tenant schema registry and drift detection\n- BigQuery per-tenant aggregates; Parquet raw storage\n- Data Catalog lineage and DLQ handling\n\n## Code Example\n\n```javascript\n// pseudo: drift check sketch\nfunction driftCheck(registrySchema, eventKeys){\n  const missing = eventKeys.filter(k => !registrySchema.includes(k));\n  return missing.length === 0;\n}\n```\n\n## Follow-up Questions\n\n- How would you test schema drift in CI/CD?\n- How would you rollback a single tenant without affecting others?","diagram":null,"difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Citadel","Discord"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T14:38:58.537Z","createdAt":"2026-01-26T14:38:58.537Z"},{"id":"q-7731","question":"On GCP, design a cross-tenant streaming analytics pipeline for 4 TB/day of JSON events ingested via Pub/Sub with strict per-tenant isolation and CMEK-based encryption. Ingest raw data to Cloud Storage as Parquet, transform in Dataflow into per-tenant BigQuery datasets with per-tenant masking via Cloud DLP, and publish aggregates to a partner via BigQuery Data Exchange. Enforce per-tenant data contracts in Data Catalog with end-to-end lineage, and enable 24h per-tenant rollback by replaying Pub/Sub from a retention topic. Include cross-region failover, schema evolution, and cost controls?","answer":"Ingest 4 TB/day via Pub/Sub, store raw data to Cloud Storage as Parquet, and stream-transform to per-tenant BigQuery datasets with CMEK encryption. Apply per-tenant masking with Cloud DLP, publish agg","explanation":"## Why This Is Asked\nTests ability to design a scalable, privacy-conscious cross-tenant pipeline on GCP, integrating Pub/Sub, Dataflow, BigQuery, Data Catalog, Data Exchange, and CMEK/DLP. It also probes rollback, cross-region resilience, and cost governance.\n\n## Key Concepts\n- Cross-tenant isolation and CMEK key management\n- Real-time ingestion with Pub/Sub and Dataflow\n- Per-tenant masking via Cloud DLP and access controls\n- End-to-end lineage and data contracts in Data Catalog\n- Sharing aggregates via BigQuery Data Exchange\n- 24h per-tenant rollback via Pub/Sub retention topic\n- Cross-region failover and schema evolution\n- Cost governance and budget controls\n\n## Code Example\n```javascript\n// Pseudo: per-tenant masking placeholder (not production-ready)\nfunction maskTenantRow(row, tenantConfig) {\n  // Apply tenant-specific masking rules, preserving schema\n  const masked = { ...row };\n  if (tenantConfig.maskEmail) masked.email = redact(masked.email);\n  if (tenantConfig.maskPii) masked.ssn = null;\n  return masked;\n}\n```\n\n## Follow-up Questions\n- How would you verify rollback consistency for all tenants within 24h?\n- What are the key failure modes during cross-region failover and how would you mitigate them?","diagram":"flowchart TD\n  Ingest[Pub/Sub Ingest 4 TB/day] --> Raw[Raw Parquet in Cloud Storage]\n  Raw --> Transform[Dataflow per-tenant Transform]\n  Transform --> Mask[Per-tenant masking via DLP]\n  Transform --> Catalog[Data Catalog contracts & lineage]\n  Transform --> Exchange[Aggregates via Data Exchange]\n  Rollback[24h per-tenant rollback via Pub/Sub retention] --> Verify[Verification & tests]\n  Exchange --> Partner[Partner access & monitoring]","difficulty":"advanced","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Goldman Sachs","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T15:42:01.109Z","createdAt":"2026-01-26T15:42:01.109Z"},{"id":"q-7824","question":"On GCP, design a streaming data quality and observability pipeline for 5 TB/day of JSON events ingested from Pub/Sub across 3 regions. Build per-tenant quality gates, schema drift detection, and automated remediation triggers before data lands in BigQuery. Include how Data Catalog lineage, DLP masking, and monitoring integrate, and outline a rollback plan for corrupted batches?","answer":"Propose a streaming quality and observability layer: ingest 5 TB/day from Pub/Sub into Dataflow, route by tenant to per-tenant BigQuery partitions, apply per-tenant schema validation and quality gates","explanation":"## Why This Is Asked\n\nThis question probes practical design of a streaming quality and observability layer, cross-region tenant isolation, and integration with Data Catalog and DLP, plus rollback semantics.\n\n## Key Concepts\n\n- Real-time data quality gates\n- Schema drift detection\n- Data masking and privacy\n- Data lineage and observability\n- Per-tenant backfill and rollback\n\n## Code Example\n\n```python\nfrom apache_beam import DoFn\n\nclass QualityGate(DoFn):\n    def process(self, element, *args, **kwargs):\n        tenant = element.get('tenant_id')\n        if not tenant:\n            return\n        if element.get('field_x') is None:\n            return\n        yield element\n```\n\n## Follow-up Questions\n\n- How would you test drift detection without impacting production data?\n- How would you scale the backfill across regions while ensuring exactly-once semantics?","diagram":"flowchart TD\n  PubSub[Pub/Sub] --> QoG[Dataflow QoG]\n  QoG --> TenantBQ[Tenant BigQuery Tables]\n  QoG --> Mask[DLP Masking]\n  TenantBQ --> LG[Data Catalog Lineage]\n  LG --> Mon[Monitoring & Alerts]\n  Mon --> Roll[Rollback/Remediation]","difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Netflix","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T19:31:03.757Z","createdAt":"2026-01-26T19:31:03.759Z"},{"id":"q-7889","question":"On GCP, design a streaming data quality and lineage platform for 1 TB/day of JSON Pub/Sub events from 250 tenants. Include per-tenant schema validation and range checks in Dataflow, cross-field rules, per-tenant partitioned BigQuery, automatic backward-compatible schema evolution, Data Catalog lineage, and per-tenant dashboards in Looker Studio. Include alerting, late-data handling, and a backfill plan using an immutable audit log?","answer":"Implement a templated Dataflow pipeline (Apache Beam) that validates each JSON event per tenant against a per-tenant schema, enforces range and cross-field checks, and routes valid data to per-tenant partitioned BigQuery tables while invalid events go to dead-letter storage. Enable automatic backward-compatible schema evolution through versioned schemas stored in Cloud Storage, with Data Catalog capturing full lineage from Pub/Sub through Dataflow to BigQuery. Provide per-tenant Looker Studio dashboards with quality metrics, alerting via Cloud Monitoring, and late-data handling with watermarks. Support backfill through an immutable audit log in Cloud Storage or BigQuery that stores all raw events.","explanation":"## Why This Is Asked\nTests practical streaming data quality, end-to-end lineage, and tenant governance on GCP, integrating Pub/Sub, Dataflow, BigQuery, Data Catalog, and Looker Studio.\n\n## Key Concepts\n- Per-tenant schema validation with cross-field rules\n- Streaming quality gates with late-data handling\n- End-to-end lineage with Data Catalog\n- Schema evolution strategy and per-tenant partitioning\n- Observability through monitoring and alerting\n\n## Code Example\n```javascript\n// Pseudo-DoFn sketch for per-event validation\nfunction validateEvent(event, tenant) {\n  // validate schema, ranges, and","diagram":"flowchart TD\n  A[Pub/Sub] --> B[Dataflow (Beam)]\n  B --> C[BigQuery per-tenant partitions]\n  A --> D[Immutable Audit Log]\n  C --> E[Data Catalog Lineage]\n  E --> F[Looker Studio Dashboards]","difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T06:35:54.124Z","createdAt":"2026-01-26T21:43:46.922Z"},{"id":"q-7906","question":"On GCP, design a streaming analytics pipeline that ingests 8 TB/day of JSON clickstream via Pub/Sub, computes per-tenant 1-minute aggregates, stores raw events as Parquet in Cloud Storage, and tenant-scoped aggregates in BigQuery. Use Dataflow with autoscaling, enforce per-tenant isolation via IAM and separate datasets, define data contracts in Data Catalog, and implement a cost-optimized cross-region failover with event replay. Include 24h rollback per tenant and a testing strategy?","answer":"Pub/Sub → Dataflow tenant-scoped template → BigQuery per-tenant datasets with 1-minute windows; raw events Parquet in Cloud Storage; enforce Data Catalog contracts; strict IAM isolation; autoscaling with cross-region failover; event replay for 24h rollback per tenant.","explanation":"## Why This Is Asked\n\nThis question evaluates end-to-end streaming architecture at scale, focusing on tenant isolation, operational reliability, and cost optimization in multi-tenant GCP environments. It tests ability to balance performance, security, and recoverability while designing for 8 TB/day throughput.\n\n## Key Concepts\n\n- **Tenant Isolation**: Per-tenant BigQuery datasets and IAM policies ensuring strict data separation\n- **Streaming Processing**: Dataflow autoscaling with tenant-scoped templates for 1-minute aggregation windows\n- **Data Governance**: Data Catalog contracts and lineage tracking for schema enforcement\n- **Storage Strategy**: Raw events archived as Parquet in Cloud Storage for long-term retention\n- **Recovery Mechanisms**: 24-hour per-tenant rollback via event replay and idempotent aggregations\n- **Cost Optimization**: Cross-region failover with synthetic testing and resource-aware scaling\n\n## Code\n\nThis architecture demonstrates production-ready streaming analytics with enterprise-grade isolation and operational excellence.","diagram":"flowchart TD\n  Ingest[Pub/Sub] --> Process[Dataflow Tenant Template]\n  Process --> StoreBQ[BigQuery per-tenant]\n  Process --> Archive[Cloud Storage Parquet]\n  StoreBQ --> Metrics[Aggregates 1-min]\n  Archive --> Metrics\n  Metrics --> Catalog[Data Catalog Contracts]\n  Catalog --> Rollback[24h Rollback]\n  Rollback --> Test[Testing & Rollback Scenarios]\n  Test --> Failover[Cross-region Standby]","difficulty":"advanced","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Robinhood","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T04:58:34.460Z","createdAt":"2026-01-26T22:38:21.768Z"},{"id":"q-7965","question":"On GCP, design a cross-tenant real-time anomaly detection pipeline for 4 TB/day of JSON events ingested via Pub/Sub, computing per-tenant features in Dataflow, scoring with a Vertex AI model, storing per-tenant results in BigQuery, with per-tenant privacy controls via DLP masking and Data Catalog lineage, and sharing only aggregated metrics via Data Exchange. Include 24h per-tenant rollback, canary testing, and a failure-playbook?","answer":"Design a Dataflow streaming pipeline ingesting 4 TB/day from Pub/Sub with tenant-based partitioning, computing real-time streaming features, scoring with Vertex AI, and storing per-tenant results in BigQuery; implement DLP masking for privacy enforcement, maintain Data Catalog lineage for governance, and share only aggregated metrics via Data Exchange. Deploy with 24-hour per-tenant rollback capabilities using regional raw storage and replay functionality, integrate canary testing for safe deployments, and maintain a comprehensive failure-playbook for operational resilience.","explanation":"## Why This Is Asked\nEvaluates end-to-end proficiency in building scalable, privacy-preserving streaming analytics on GCP, including model scoring, data governance, and cross-tenant sharing. It assesses understanding of rollback strategies, testing discipline, and operational playbooks.\n\n## Key Concepts\n- Streaming pipeline architecture using Cloud Dataflow and Pub/Sub\n- Per-tenant isolation and privacy through DLP masking and Data Catalog lineage\n- Model scoring integration with Vertex AI and per-tenant BigQuery storage\n- Aggregated data sharing via Data Exchange\n- 24-hour per-tenant rollback using regional raw storage and replay capabilities\n- Canary testing for safe deployments\n- Comprehensive failure-playbook development for operational resilience","diagram":"flowchart TD\n  PubSub[Pub/Sub: 4 TB/day] --> DF[Dataflow Pipeline]\n  DF --> F[Feature Extraction per Tenant]\n  F --> BigQuery[BigQuery: per-tenant tables]\n  DF --> VA[Vertex AI: Scoring]\n  VA --> EX[Data Exchange: aggregated metrics]\n  BigQuery --> DataCatalog[Data Catalog: lineage]\n  RawStore[Regional Cloud Storage: 24h raw] --> Rollback[Rollback Job per Tenant]","difficulty":"advanced","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["NVIDIA","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T05:40:08.263Z","createdAt":"2026-01-27T02:47:19.716Z"},{"id":"q-8052","question":"On GCP, design a privacy-preserving, cross-tenant streaming analytics pipeline for 2 TB/day of JSON events arriving via Pub/Sub, targeting per-tenant BigQuery datasets. Implement per-tenant data contracts and field-level masking using DLP, maintain end-to-end lineage with Data Catalog, and support per-tenant 24h rollback. Include a real-time cross-tenant leakage detector, a testing strategy with synthetic tenants and canary rollouts, and plan for schema evolution and cost controls?","answer":"Ingest 2 TB/day via Pub/Sub; use Dataflow streaming with per-tenant routing; apply DLP masking at ingest; write to per-tenant BigQuery datasets with Authorized Views; maintain end-to-end lineage in Da","explanation":"## Why This Is Asked\nThe question tests practical mastery of privacy, lineage, and multi-tenant isolation in GCP with real-world constraints like rollback and testing.\n\n## Key Concepts\n- Pub/Sub -> Dataflow per-tenant routing\n- DLP masking at ingest\n- Data Catalog lineage across sources and sinks\n- Per-tenant BigQuery datasets with fine-grained access\n- 24h tenant rollback and canary schema evolution\n- Cross-tenant leakage detector; synthetic tenants for testing\n\n## Code Example\n```python\n# Example: quick field masking scaffold\ndef mask_fields(record, fields_to_mask):\n    for f in fields_to_mask:\n        if f in record:\n            record[f] = 'REDACTED'\n    return record\n```\n\n## Follow-up Questions\n- How would you validate leakage detector accuracy and false positives?\n- What latency/throughput trade-offs exist with masking depth?","diagram":null,"difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Robinhood","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T07:38:30.429Z","createdAt":"2026-01-27T07:38:30.429Z"},{"id":"q-8162","question":"On GCP, design a cost-aware, per-tenant analytics pipeline for 50 tenants ingesting 200 GB/day JSON events via Pub/Sub, with per-tenant quotas and cost attribution in Cloud Billing, and dynamic retention per tenant. Use Dataflow for streaming, BigQuery per-tenant data products with Authorized Views, Data Catalog tagging, and budget alarms in Cloud Monitoring. Include scaling and rollback strategies?","answer":"Per-tenant data product: each tenant uses its own BigQuery dataset with Authorized Views; a single Dataflow streaming job writes to tenant tables; resources are tagged in Data Catalog; Cloud Billing e","explanation":"## Why This Is Asked\n\nThis question probes cost governance, per-tenant isolation, policy enforcement, and rollback controls in a real GCP data platform.\n\n## Key Concepts\n\n- Cloud Billing cost attribution with resource labels and Data Catalog tagging\n- BigQuery per-tenant datasets and Authorized Views for isolation\n- Dataflow autoscaling and per-tenant quotas\n- Data retention policies and 24h rollback via time travel\n- Monitoring alerts and rollback testing\n\n## Code Example\n\n```sql\n-- Pseudo BigQuery per-tenant dataset and view (illustrative)\nCREATE SCHEMA tenant_123_dataset;\nCREATE TABLE tenant_123_dataset.events AS\nSELECT * FROM raw_events WHERE tenant_id = 123;\n```\n\n## Follow-up Questions\n\n- How would you validate budget alert thresholds at scale?\n- How would you test rollback under production load and ensure data integrity?","diagram":"flowchart TD\n  PubSub[Pub/Sub Ingest] --> Dataflow[Dataflow Streaming]\n  Dataflow --> BQ[BigQuery per-tenant Tables]\n  BQ --> Catalog[Data Catalog Tags]\n  Dataflow --> Billing[Billing Export]\n  Billing --> Alerts[Monitoring Alerts]\n  BQ --> Retention[Tenant Retention]\n  Dataflow --> Rollback[24h Rollback Window]","difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Cloudflare"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T11:51:51.477Z","createdAt":"2026-01-27T11:51:51.478Z"},{"id":"q-8285","question":"Design a beginner-friendly GCP pipeline that ingests 1 GB/day of JSON events via Pub/Sub, parses and validates the schema in Dataflow, enriches with a static lookup, and writes to a daily-partitioned BigQuery table. Implement per-tenant access using BigQuery Authorized Views that mask PII fields; include a 24h rollback plan and a basic test strategy with a small sample dataset?","answer":"Use a Dataflow streaming pipeline reading Pub/Sub JSON, validate against a fixed schema, apply a lookup to enrich fields, and upsert into a daily-partitioned BigQuery table using insertId for deduplic","explanation":"Why This Is Asked\n\nThis question tests a beginner-friendly, end-to-end pipeline design with core GCP components (Pub/Sub, Dataflow, BigQuery) plus governance via Authorized Views. It requires concrete rollback and test strategies without excessive complexity.\n\n### Key Concepts\n\n- Dataflow (Beam) streaming transform\n- Pub/Sub ingestion\n- BigQuery daily partitions for cost-effective querying\n- InsertId deduplication for at-least-once semantics\n- BigQuery Authorized Views for per-tenant masking\n- Rollback and test strategies\n\n### Code Example\n\n```python\n# Pseudocode for Dataflow pipeline\nfrom apache_beam import Pipeline\n\ndef run():\n  with Pipeline(options=...) as p:\n    msgs = p | 'ReadPubSub' >> beam.io.ReadFromPubSub(...) \n    parsed = msgs | 'Parse' >> beam.Map(parse_json)\n    validated = parsed | 'Validate' >> beam.Filter(valid_record)\n    enriched = validated | 'Enrich' >> beam.Map(enrich_lookup)\n    enriched | 'WriteBigQuery' >> beam.io.WriteToBigQuery(\n        table='dataset.table', schema=SCHEMA, insert_disposition='WRITE_APPEND'\n    )\n```\n\n### Follow-up Questions\n\n- How would you test idempotency and handle late data?\n- How would you verify Authorized Views masking without exposing real data?","diagram":null,"difficulty":"beginner","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Meta","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T18:04:49.447Z","createdAt":"2026-01-27T18:04:49.447Z"},{"id":"q-8479","question":"On GCP, design a 30-tenant, 4 TB/day real-time telemetry pipeline ingesting JSON via Pub/Sub, with a 3-tier architecture (raw, curated, analytics) stored regionally and in BigQuery. Enforce per-tenant privacy with masking/tokenization and tenant-key encryption in Cloud KMS; manage per-tenant data contracts in Data Catalog and share aggregates via Data Exchange. Include 24h rollback, schema versioning, testing/backfill, and end-to-end lineage?","answer":"Use a three-tier design: raw/curated/analytics with per-tenant schemas in BigQuery, regionally stored data in Cloud Storage, streamed via Dataflow. Enforce privacy with per-tenant masking, encryption/","explanation":"## Why This Is Asked\nTests end-to-end data governance, privacy, rollback, and cross-tenant sharing in a real-world GCP setup.\n\n## Key Concepts\n- Multi-tier data architecture (raw/curated/analytics)\n- Per-tenant privacy and key management (masking, KMS)\n- Data contracts in Data Catalog, sharing via Data Exchange\n- Rollback/backfill with event-sourcing and lineage\n\n## Code Example\n```javascript\n// Pseudo: mask fields in a Dataflow DoFn based on tenant config\nfunction maskRecord(record, tenantPolicy) {\n  // apply masking rules per tenant\n  return maskedRecord;\n}\n```\n\n## Follow-up Questions\n- How would you backfill a 24h window when a masking rule changes?\n- How to test data contracts against evolving schemas without breaking producers?","diagram":"flowchart TD\n  A[Pub/Sub Ingest] --> B[Dataflow]\n  B --> C[Regional Storage Raw]\n  B --> D[Curated BigQuery]\n  D --> E[Analytics BigQuery]\n  E --> F[Data Exchange]\n  C --> G[Data Catalog Lineage]","difficulty":"advanced","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","IBM","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-28T04:33:04.279Z","createdAt":"2026-01-28T04:33:04.280Z"},{"id":"q-8486","question":"On GCP, design a multi-tenant streaming analytics pipeline that ingests 5 TB/day of JSON events via Pub/Sub and writes per-tenant aggregates to BigQuery. Add a robust data quality layer: deduplication, schema drift detection, and per-tenant alerting using Data Loss Prevention and Data Catalog lineage. Include tenant-level cost accounting and a 24h rollback mechanism, plus a testing strategy that simulates late arrivals and out-of-order events?","answer":"Use a single Dataflow pipeline with per-tenant keys and stateful deduplication on (tenant_id, event_id). Ingest 5 TB/day via Pub/Sub, store raw events in Cloud Storage, and write per-tenant aggregates","explanation":"## Why This Is Asked\nTests practical design decisions for multi-tenant pipelines: dedup, schema drift, privacy, lineage, cost tracking, and rollback under real-world data flows.\n\n## Key Concepts\n- Per-tenant isolation and cost accounting\n- Stateful deduplication and event-time windowing with lateness\n- Schema drift detection and safe evolution\n- DLP masking and Data Catalog lineage\n- Reprocessing rollback strategy and per-tenant recovery\n\n## Code Example\n```javascript\n// Pseudo Dataflow style sketch\nfunction ProcessEvent(event) {\n  if (isDuplicate(event)) return; // stateful dedup\n  const masked = maskPII(event);\n  emitToBigQuery(masked);\n  recordLineage(event);\n}\n```\n\n## Follow-up Questions\n- How would you simulate and validate 24h tenant-rollbacks under high load?\n- What metrics and alerting would verify data quality and lineage completeness?","diagram":"flowchart TD\n  PubSub[Pub/Sub] --> Dataflow[Dataflow Pipeline]\n  Dataflow --> BigQuery[Per-tenant BigQuery Tables]\n  Dataflow --> CloudStorage[Raw Events in Cloud Storage]\n  Dataflow --> DataCatalog[Data Catalog Lineage]\n  BigQuery --> Billing[Cost Accounting]\n","difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Meta","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-28T05:40:50.934Z","createdAt":"2026-01-28T05:40:50.934Z"},{"id":"q-931","question":"A GCP pipeline ingests 1 TB of JSON user activity daily from Pub/Sub into BigQuery via Dataflow. New fields appear over time; you must evolve the schema without downtime. What approach would you use for schema drift and nested fields, and outline concrete steps to implement it?","answer":"Use a two-layer scheme: a stable BigQuery table with known fields and a separate extra_json column to capture drift. The Dataflow job maps incoming JSON to the known schema; unknown fields are stored ","explanation":"## Why This Is Asked\nThis question probes practical handling of evolving data in a streaming pipeline, balancing query stability with schema growth.\n\n## Key Concepts\n- Schema evolution in BigQuery\n- Semi-structured data (JSON)\n- Dataflow transforms for parsing\n- Backward compatibility\n- Metadata and cataloging\n\n## Code Example\n```javascript\nfunction parseEvent(json) {\n  const data = JSON.parse(json);\n  const known = {\n    userId: data.user_id || null,\n    eventTime: data.event_time || null,\n    country: data.country || null\n  };\n  const extraKeys = Object.keys(data).filter(k => !(k in known));\n  const extra = JSON.stringify(extraKeys.reduce((acc, k) => { acc[k] = data[k]; return acc; }, {}));\n  return { ...known, extra_json: extra };\n}\n```\n\n## Follow-up Questions\n- How would you migrate existing data to the new table version with minimal downtime?\n- How would you monitor for schema drift and alert when new fields appear?","diagram":null,"difficulty":"beginner","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Discord","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T15:41:21.360Z","createdAt":"2026-01-12T15:41:21.361Z"},{"id":"q-962","question":"In a GCP streaming pipeline, Pub/Sub feeds millions of events into BigQuery via Dataflow. Out-of-order arrivals and duplicates occur. Design an idempotent sink using a staging table and a final partitioned table, leveraging insertId for dedup and a MERGE strategy to upsert into the final table. Outline concrete steps and trade-offs to implement?","answer":"Use a staging BigQuery table where Dataflow writes with insertId derived from a stable key (user_id + event_id + event_time). Periodically MERGE from staging into a final partitioned table, deduplicat","explanation":"## Why This Is Asked\n\nTests practical data quality, idempotent sinks, and operational design using Dataflow + BigQuery features.\n\n## Key Concepts\n\n- Idempotent sink\n- insertId dedup\n- staging vs final table\n- MERGE in BigQuery\n- allowed lateness and partitioning\n- quotas and monitoring\n\n## Code Example\n\n```sql\nMERGE INTO dataset.final AS F\nUSING dataset.staging AS S\nON F.user_id = S.user_id AND F.event_time = S.event_time\nWHEN NOT MATCHED THEN INSERT (user_id, event_time, event_id, action, payload)\nWHEN MATCHED THEN UPDATE SET action = S.action, payload = S.payload;\n```\n\n## Follow-up Questions\n\n- How to handle tombstones?\n- How to scale MERGE with sharded partitions?\n- How would you instrument observability for dedup latency?","diagram":"flowchart TD\n  PubSub[Pub/Sub] --> Dataflow[Dataflow]\n  Dataflow --> Staging[StagingBQ]\n  Staging --> Final[FinalBQ]\n  Staging -->|MERGE| Final","difficulty":"intermediate","tags":["gcp-data-engineer"],"channel":"gcp-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Google","Instacart"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T17:23:31.180Z","createdAt":"2026-01-12T17:23:31.180Z"}],"subChannels":["general"],"companies":["Adobe","Airbnb","Amazon","Anthropic","Apple","Bloomberg","Citadel","Cloudflare","Coinbase","Databricks","Discord","DoorDash","Goldman Sachs","Google","Hashicorp","Hugging Face","IBM","Instacart","LinkedIn","Lyft","Meta","Microsoft","MongoDB","NVIDIA","Netflix","OpenAI","Oracle","PayPal","Plaid","Robinhood","Salesforce","Scale Ai","Slack","Snap","Snowflake","Square","Stripe","Tesla","Twitter","Two Sigma","Uber","Zoom"],"stats":{"total":94,"beginner":31,"intermediate":37,"advanced":26,"newThisWeek":40}}