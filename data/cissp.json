{"questions":[{"id":"cissp-security-architecture-1768210456003-0","question":"An organization hosts multi-tenant workloads in a public cloud and wants to adopt Zero Trust architecture. Which combination best achieves Zero Trust at scale?","answer":"[{\"id\":\"a\",\"text\":\"Perimeter firewall-only approach\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Implement Zero Trust with identity-based access, least privilege, device posture, micro-segmentation, and mutual TLS for service-to-service communication\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Rely solely on cloud IAM roles in the provider to enforce access\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Allow all traffic within a shared VPC and use security groups only for logging\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct answer is **B** because Zero Trust requires identity-based access controls, least privilege, device posture checks, micro-segmentation, and mutual TLS for service-to-service communication, which collectively remove implicit trust and enforce continuous verification.\n\n## Why Other Options Are Wrong\n- A: Perimeter firewall-only models assume trusted internal networks; fails to verify identity or enforce micro-segmentation.\n- C: Relying solely on cloud IAM roles misses micro-segmentation and device posture checks across workloads.\n- D: Trusting internal traffic within a VPC contradicts Zero Trust, which treats every access as potentially untrusted.\n\n## Key Concepts\n- Zero Trust Architecture\n- Micro-segmentation\n- Identity-based access control and least privilege\n- Mutual TLS (mTLS) for service communication\n\n## Real-World Application\n- Map all workloads into micro-segments; enforce per-service identity; deploy an IdP; implement device-health checks; require mTLS between services; monitor with a ZTA control plane.","diagram":null,"difficulty":"intermediate","tags":["Zero Trust","Cloud Security","AWS","IAM","Mutual TLS","Micro-segmentation","Kubernetes","certification-mcq","domain-weight-13"],"channel":"cissp","subChannel":"security-architecture","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T09:34:16.004Z","createdAt":"2026-01-12 09:34:16"},{"id":"cissp-security-architecture-1768210456003-1","question":"To manage cryptographic keys across cloud and on‑prem environments, which approach provides proper lifecycle management and separation of duties?","answer":"[{\"id\":\"a\",\"text\":\"Use a single master key stored in configuration files and rotate manually\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use a cloud KMS/HSM with separate key policies, strong access controls, automated rotation, and auditing; use envelope encryption for data\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Store keys in plaintext in environment variables and rely on automatic rotation\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use separate keystores per application with no central policy\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct answer is **B** because using a cloud KMS/HSM with separate key policies and automated rotation provides centralized lifecycle management, separation of duties, and auditable usage, while envelope encryption secures data with key material stored separately.\n\n## Why Other Options Are Wrong\n- A: Storing a master key in config files is insecure and hard to rotate; elevates risk of leakage.\n- C: Plaintext keys in environment variables expose keys in memory and logs; rotation does not mitigate exposure.\n- D: Per-app keystores without central policy create inconsistent controls and governance gaps.\n\n## Key Concepts\n- KMS and HSM integration\n- Envelope encryption\n- Key policy and access control\n- Auditing and rotation\n\n## Real-World Application\n- Implement CMKs managed by a centralized KMS, enforce strict IAM policies, enable audit logging, rotate keys on a schedule, and ensure data encryption uses envelope-derived keys across all workloads.","diagram":null,"difficulty":"intermediate","tags":["KMS","HSM","Envelope Encryption","AWS","Terraform","Security Governance","certification-mcq","domain-weight-13"],"channel":"cissp","subChannel":"security-architecture","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T09:34:16.560Z","createdAt":"2026-01-12 09:34:16"},{"id":"cissp-security-architecture-1768210456003-2","question":"To secure a software supply chain for Terraform-managed deployments to Kubernetes, which combination best ensures trusted deployment?","answer":"[{\"id\":\"a\",\"text\":\"Sign artifacts but skip SBOM generation and policy checks\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Integrate code signing in CI/CD, generate SBOMs, sign artifacts, use immutable infrastructure, and enforce pre-deployment policy checks\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Use rapid deployment without signing or SBOMs, relying only on runtime monitoring\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Store credentials in Vault and bypass artifact signing\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct answer is **B** because a secure supply chain requires verifiable provenance: code signing in CI/CD, SBOM generation, artifact signing, and policy gates before deployment to ensure integrity and traceability in Terraform/Kubernetes workflows.\n\n## Why Other Options Are Wrong\n- A: Signatures without SBOMs or policy checks do not prove provenance or component composition.\n- C: Omitting signing and SBOMs leaves supply-chain risks unmanaged and unverified.\n- D: Signing credentials in Vault alone does not guarantee artifact integrity or deployment safety.\n\n## Key Concepts\n- SBOM (Software Bill of Materials)\n- Code signing\n- Immutable infrastructure\n- Policy enforcement (e.g., OPA)\n\n## Real-World Application\n- Extend CI/CD with SBOM tooling, sign Terraform modules, verify signatures during plan/apply, and enforce module/version controls before Kubernetes deployments.","diagram":null,"difficulty":"intermediate","tags":["Terraform","Kubernetes","CI/CD","Code Signing","SBOM","OPA","certification-mcq","domain-weight-13"],"channel":"cissp","subChannel":"security-architecture","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T09:34:17.098Z","createdAt":"2026-01-12 09:34:17"},{"id":"cissp-security-risk-1768173507975-0","question":"When a control is cost-prohibitive and applying it would push residual risk beyond the organization's defined risk appetite, which risk response best aligns with standard risk management practice?","answer":"[{\"id\":\"a\",\"text\":\"Avoidance\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Transference\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Mitigation\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Acceptance\",\"isCorrect\":true}]","explanation":"## Correct Answer\nD — Acceptance. When residual risk remains within the organization’s risk appetite and the cost of mitigating exceeds tolerance, acceptance is appropriate.\n\n## Why Other Options Are Wrong\n- A: Avoidance eliminates the risk by changing scope, which is not feasible and contradicts the scenario where the asset must remain in scope.\n- B: Transference shifts risk (eg, insurance) but does not reduce likelihood or impact to acceptable levels in cybersecurity context.\n- C: Mitigation reduces risk but is cost-prohibitive here and would push risk beyond tolerance; acceptance is preferred when within appetite.\n\n## Key Concepts\n- Risk appetite and risk tolerance\n- Residual risk management\n- Cost-benefit analysis of controls\n\n## Real-World Application\n- A security program may accept residual risk for legacy systems after evaluating total cost of control implementation against business impact.","diagram":null,"difficulty":"intermediate","tags":["risk-management","CISSP","ISO-31000","AWS","BusinessContinuity","certification-mcq","domain-weight-15"],"channel":"cissp","subChannel":"security-risk","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T23:18:27.976Z","createdAt":"2026-01-11 23:18:28"},{"id":"cissp-security-risk-1768173507975-1","question":"During vendor onboarding, a third-party cloud service provider must be evaluated for data protection controls before a contract is signed. Which activity reduces third-party risk most effectively before onboarding?","answer":"[{\"id\":\"a\",\"text\":\"Conduct a detailed due diligence questionnaire and risk assessment\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Require SOC 2 Type II report after go-live\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Rely on marketing materials for assurances\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Implement no extra controls since provider is trusted\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA — Conduct a detailed due diligence questionnaire and risk assessment. This enables proactive identification of data protection controls, data flows, and residual risks before onboarding.\n\n## Why Other Options Are Wrong\n- B: SOC 2 Type II after go-live delays assurance and fails to prevent risk during onboarding.\n- C: Marketing materials are not a reliable basis for risk assessment due to potential bias.\n- D: Assuming trust without controls ignores shared responsibility and regulatory requirements.\n\n## Key Concepts\n- Third-party risk management\n- Due diligence and risk assessment\n- Shared responsibility model\n\n## Real-World Application\n- Organizations use pre-screening questionnaires to terminate or require mitigations before signing with cloud providers.","diagram":null,"difficulty":"intermediate","tags":["third-party-risk","cloud-security","AWS","Terraform","certification-mcq","domain-weight-15"],"channel":"cissp","subChannel":"security-risk","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T23:18:28.451Z","createdAt":"2026-01-11 23:18:28"},{"id":"cissp-security-risk-1768173507975-2","question":"A financial institution conducts a business impact analysis to determine RTO and RPO for mission-critical processes. Which component of BIA is most essential for establishing recovery objectives?","answer":"[{\"id\":\"a\",\"text\":\"Asset inventory\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Dependency mapping and criticality assessment\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Technical vulnerability scan\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Incident response playbook\",\"isCorrect\":false}]","explanation":"## Correct Answer\nB — Dependency mapping and criticality assessment. Identifying which processes depend on which resources and how critical they are drives the determination of RTOs and RPOs.\n\n## Why Other Options Are Wrong\n- A: Asset inventory supports asset management but doesn’t directly set recovery objectives.\n- C: Vulnerability scans inform technical risk, not recovery priorities in a BIA.\n- D: Incident response plans guide reactive actions, not objective recovery timelines.\n\n## Key Concepts\n- Business Impact Analysis components\n- RTO and RPO derivation\n- Process dependencies\n\n## Real-World Application\n- Banks map processes to systems and data flows to set realistic recovery targets during disruptive events.","diagram":null,"difficulty":"intermediate","tags":["BIA","business-continuity","AWS","Kubernetes","certification-mcq","domain-weight-15"],"channel":"cissp","subChannel":"security-risk","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T23:18:28.910Z","createdAt":"2026-01-11 23:18:28"}],"subChannels":["security-architecture","security-risk"],"companies":[],"stats":{"total":6,"beginner":0,"intermediate":6,"advanced":0,"newThisWeek":6}}