{"questions":[{"id":"aws-saa-design-performant-1768227966430-0","question":"A globally distributed e-commerce application experiences varying read traffic and needs low-latency data access while maintaining strong durability. Which design best achieves this?","answer":"[{\"id\":\"a\",\"text\":\"Use a single-region RDS with read replicas in other regions behind a latency-based Route 53 policy\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use DynamoDB Global Tables with DAX in each region, fronted by CloudFront for static assets, and Route 53 latency-based routing\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Use a single-region DynamoDB table with an in-memory cache in the application layer and a CDN for static assets\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Store all data in S3 objects and retrieve via a custom API across regions\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct because DynamoDB Global Tables provide multi-region, fully replicated data with low-latency reads, and DAX accelerates read performance within a region. CloudFront caches static assets at edge locations to reduce origin load, and Route 53 latency-based routing helps direct users to the nearest healthy region for optimal latency.\n\n## Why Other Options Are Wrong\n- A: Cross-region RDS read replicas can reduce latency but require more maintenance and do not provide the seamless multi-region write and global low-latency access that DynamoDB Global Tables offer.\n- C: A single-region DynamoDB with an app-layer cache does not guarantee global low-latency access for all regions, and the cache may become a bottleneck.\n- D: S3 is object storage and does not provide efficient queryable access patterns for dynamic data across regions; relying on S3 alone undermines latency for transactional-like reads.\n\n## Key Concepts\n- DynamoDB Global Tables\n- DynamoDB with DAX\n- CloudFront edge caching\n- Route 53 latency-based routing\n\n## Real-World Application\n- Use this pattern for global online retailers needing consistent performance across regions with resilient failover.\n","diagram":null,"difficulty":"intermediate","tags":["AWS","DynamoDB","DAX","CloudFront","Route53","certification-mcq","domain-weight-24"],"channel":"aws-saa","subChannel":"design-performant","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T14:26:06.431Z","createdAt":"2026-01-12 14:26:06"},{"id":"aws-saa-design-performant-1768227966430-1","question":"An application experiences irregular traffic spikes and must process background tasks without blocking user requests. Which design best enables decoupled, scalable processing?","answer":"[{\"id\":\"a\",\"text\":\"Use Kinesis Data Streams to ingest events and process them with EC2 workers behind an ASG\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use SQS to decouple producers and consumers and scale workers independently\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Use an RDS Multi-AZ database to absorb spikes with read replicas\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Expose a single API Gateway endpoint to directly trigger a Lambda for all work\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct because SQS provides durable, asynchronous messaging that decouples producers from consumers, allowing workers to scale independently based on queue depth.\n\n## Why Other Options Are Wrong\n- A: Kinesis is suitable for streaming data but adds complexity for typical background-job processing and requires more infrastructure to scale workers.\n- C: RDS Multi-AZ addresses database high availability, not decoupled background processing.\n- D: A single API-based trigger creates tight coupling and can throttle user-facing latency during spikes.\n\n## Key Concepts\n- SQS (Simple Queue Service)\n- Decoupled architectures\n- Event-driven processing\n\n## Real-World Application\n- Ideal for order processing pipelines, image processing queues, or any backlog-prone workflows.\n","diagram":null,"difficulty":"intermediate","tags":["AWS","SQS","Kinesis","Lambda","certification-mcq","domain-weight-24"],"channel":"aws-saa","subChannel":"design-performant","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T14:26:06.927Z","createdAt":"2026-01-12 14:26:07"},{"id":"aws-saa-design-performant-1768227966430-2","question":"A workload requires shared file storage accessible by a fleet of EC2 instances across multiple AZs with high throughput and low latency. Which AWS service best fits this requirement?","answer":"[{\"id\":\"a\",\"text\":\"EBS volumes attached to each EC2 instance\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"S3 object storage\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"EFS (Elastic File System)\",\"isCorrect\":true},{\"id\":\"d\",\"text\":\"FSx for Lustre\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption C is correct because EFS provides a scalable, shared filesystem that can be mounted across multiple EC2 instances in different AZs, offering high throughput and low latency for shared access.\n\n## Why Other Options Are Wrong\n- A: EBS is tied to a single AZ and cannot be shared across instances in different AZs.\n- B: S3 is object storage, not a POSIX-style shared filesystem for concurrent file access.\n- D: FSx for Lustre is optimized for HPC workloads and not the typical shared filesystem use case across general EC2 instances.\n\n## Key Concepts\n- EFS shared file system across AZs\n- POSIX-compliant storage for multiple EC2s\n\n## Real-World Application\n- Suitable for web servers needing shared logs, media uploads, or shared application state.\n","diagram":null,"difficulty":"intermediate","tags":["AWS","EFS","EBS","S3","certification-mcq","domain-weight-24"],"channel":"aws-saa","subChannel":"design-performant","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T14:26:07.385Z","createdAt":"2026-01-12 14:26:07"},{"id":"aws-saa-design-performant-1768227966430-3","question":"A highly-variable workload requires automatic, fine-grained scaling of a NoSQL database without manual capacity planning. Which DynamoDB capacity model best fit this requirement?","answer":"[{\"id\":\"a\",\"text\":\"Provisioned capacity with auto-scaling\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"On-demand capacity mode\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Reserved capacity with upfront payment\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Global secondary index capacity planning\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct because DynamoDB on-demand capacity automatically scales to accommodate request traffic without prior capacity planning, making it ideal for unpredictable workloads.\n\n## Why Other Options Are Wrong\n- A: Provisioned with auto-scaling still requires configuration and may not react quickly enough to sudden spikes.\n- C: Reserved capacity provides cost savings but is not suited for unpredictable workloads.\n- D: Global secondary indexes are for query patterns, not a capacity model.\n\n## Key Concepts\n- DynamoDB on-demand vs provisioned capacity\n- Auto-scaling considerations\n\n## Real-World Application\n- Use for new services with uncertain traffic patterns or seasonal spikes.\n","diagram":null,"difficulty":"intermediate","tags":["AWS","DynamoDB","On-Demand","Auto-Scaling","certification-mcq","domain-weight-24"],"channel":"aws-saa","subChannel":"design-performant","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T14:26:07.686Z","createdAt":"2026-01-12 14:26:07"},{"id":"aws-saa-design-performant-1768227966430-4","question":"Your stateless API must remain highly available across Availability Zones with minimal operational overhead. Which design is the most resilient and scalable choice?","answer":"[{\"id\":\"a\",\"text\":\"Deploy in two AZs behind an Application Load Balancer with a Multi-AZ RDS instance for the database\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Deploy in a single AZ with an EBS-backed instance and no load balancer\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Deploy across two regions with a single Lambda in each region and no data replication\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Deploy behind an API Gateway with a single Lambda in a single region\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A is correct because running across multiple AZs behind an Application Load Balancer distributes traffic and provides fault tolerance, while a Multi-AZ RDS instance ensures database availability and automatic failover within the region.\n\n## Why Other Options Are Wrong\n- B: Single-AZ deployment creates a single point of failure and is not resilient.\n- C: Two regions without data replication risks data inconsistency and operational overhead; also regional failures can impact availability.\n- D: A single-region, single-Lambda setup lacks multi-AZ traffic distribution and high availability guarantees.\n\n## Key Concepts\n- Availability Zones and Multi-AZ deployments\n- Application Load Balancer (ALB)\n- AWS managed databases (RDS) high availability\n\n## Real-World Application\n- This pattern is standard for production-grade stateless APIs requiring high availability with minimal manual operations.\n","diagram":null,"difficulty":"intermediate","tags":["AWS","ALB","RDS","High-Availability","certification-mcq","domain-weight-24"],"channel":"aws-saa","subChannel":"design-performant","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T14:26:07.850Z","createdAt":"2026-01-12 14:26:07"},{"id":"aws-saa-design-resilient-1768213442314-0","question":"You are designing a serverless web application that must remain available during a regional outage in us-east-1. The app uses DynamoDB to store user profiles and S3 to store assets. To minimize RPO and enable automatic failover between us-east-1 and us-west-2, which design pattern should you implement?","answer":"[{\"id\":\"a\",\"text\":\"DynamoDB Global Tables replicated across us-east-1 and us-west-2, S3 cross-region replication for assets, and Route 53 failover with health checks to route traffic to the healthy region automatically.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Maintain a single-region DynamoDB instance in us-east-1 with manual backups to us-west-2, then switch DNS to the backup region after an outage.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use an RDS Multi-AZ deployment and rely on the primary region failing over automatically across regions.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Deploy two completely independent stacks in us-east-1 and us-west-2 with separate DynamoDB tables and implement cross-region data sync via Lambda.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption a is correct: it provides true multi-region active data and automated routing for DR. DynamoDB Global Tables replicates data across regions to minimize RPO, S3 cross-region replication ensures asset availability, and Route 53 health checks enable automatic failover.\n\n## Why Other Options Are Wrong\n- Option B: relies on manual failover and single-region state; results in higher RPO and downtime.\n- Option C: RDS Multi-AZ is intra-region; does not protect against regional outages.\n- Option D: while feasible, it adds operational complexity and potential data divergence without an automated global failover strategy.\n\n## Key Concepts\n- DynamoDB Global Tables\n- S3 Cross-Region Replication\n- Route 53 Health Checks and Failover\n- Multi-Region Disaster Recovery\n\n## Real-World Application\nImplement a global architecture with tables replicated across regions, replicate assets, and configure DNS failover so traffic redirects automatically to the healthy region during outages.","diagram":null,"difficulty":"intermediate","tags":["AWS","DynamoDB","S3","Route53","certification-mcq","domain-weight-26"],"channel":"aws-saa","subChannel":"design-resilient","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T10:24:02.315Z","createdAt":"2026-01-12 10:24:02"},{"id":"aws-saa-design-resilient-1768213442314-1","question":"An organization runs a critical application in us-east-1 using EC2 Auto Scaling behind an Application Load Balancer. The service depends on an RDS primary instance in us-east-1. The organization wants to meet an RTO of 15 minutes and an RPO of less than 5 minutes in the event of a regional outage. Which DR pattern best achieves this?","answer":"[{\"id\":\"a\",\"text\":\"Create an RDS cross-region read replica in us-west-2 and configure Route 53 health checks with a failover to the us-west-2 endpoint; use a Lambda function to automatically promote the read replica if the primary region becomes unhealthy.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Enable RDS Multi-AZ in us-east-1 and rely on automatic failover to the standby within the same region.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Implement DynamoDB global tables across us-east-1 and us-west-2 for all data and route traffic accordingly.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use S3 cross-region replication to replicate configuration data and perform manual DNS failover.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption a is correct: RDS cross-region read replicas provide DR across regions with low RPO; automating promotion via a Lambda function helps achieve the target 15-minute RTO. B only covers intra-region failover and cannot survive regional outages. C introduces a data model mismatch for relational data. D relies on manual steps, increasing the outage duration.\n\n## Why Other Options Are Wrong\n- Option B: Intra-region failover does not protect against regional outages.\n- Option C: DynamoDB is not the primary data store for a relational RDS-based workload.\n- Option D: Manual DNS failover slows recovery and increases RTO.\n\n## Key Concepts\n- RDS cross-region read replicas\n- Automated failover using Lambda\n- Route 53 health checks\n- Disaster recovery objectives (RTO/RPO)\n\n## Real-World Application\nConfigure a cross-region read replica in the secondary region, monitor the primary region health, trigger promotion of the replica when unhealthy, and route traffic to the secondary region automatically.","diagram":null,"difficulty":"intermediate","tags":["AWS","RDS","Route53","DR","certification-mcq","domain-weight-26"],"channel":"aws-saa","subChannel":"design-resilient","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T10:24:02.848Z","createdAt":"2026-01-12 10:24:03"},{"id":"aws-saa-design-resilient-1768213442314-2","question":"To store 7 years of critical audit logs with minimal ongoing costs while ensuring immutability during the retention window, which storage approach is best?","answer":"[{\"id\":\"a\",\"text\":\"Store logs in S3 Standard and set a lifecycle rule to transition to Glacier Deep Archive after 30 days, and enable S3 Object Lock in compliance mode to ensure immutability for the retention period.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Store logs in S3 Standard-Infrequent Access with indefinite retention.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Store logs on EBS volumes attached to EC2 instances and snapshot them to S3.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use S3 Intelligent-Tiering with no lifecycle rules.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption a is correct: transitioning to Glacier Deep Archive reduces ongoing costs for long-term retention, while S3 Object Lock in compliance mode provides immutability to meet audit requirements.\n\n## Why Other Options Are Wrong\n- Option B: Standard-IA reduces costs but lacks guaranteed immutability and may still be more expensive than Glacier Deep Archive for long-term retention.\n- Option C: EBS snapshots are not ideal for long-term, cost-effective durable storage of large audit logs.\n- Option D: Without lifecycle rules, data may remain in a higher-cost tier longer than necessary, and Object Lock is not enforced without S3 Lock configurations.\n\n## Key Concepts\n- S3 Lifecycle transitions\n- Glacier Deep Archive storage class\n- S3 Object Lock (compliance mode)\n\n## Real-World Application\nConfigure a lifecycle policy that moves logs from S3 Standard to Glacier Deep Archive after 30 days and enable Object Lock in compliance mode for the retention window to ensure immutability.","diagram":null,"difficulty":"intermediate","tags":["AWS","S3","GlacierDeepArchive","ObjectLock","certification-mcq","domain-weight-26"],"channel":"aws-saa","subChannel":"design-resilient","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T10:24:03.366Z","createdAt":"2026-01-12 10:24:03"},{"id":"aws-saa-design-secure-1768173626470-0","question":"You are designing a new multi-tier web application on AWS. The requirements include a public endpoint with TLS termination at the edge, private subnets for application servers and the database, restricted outbound internet access for security updates, encryption of data at rest using KMS, and centralized secrets management. Which architecture best satisfies these requirements?","answer":"[{\"id\":\"a\",\"text\":\"Application Load Balancer in public subnets terminates TLS, application servers in private subnets, database in private subnets, a NAT Gateway for outbound updates, S3 and Secrets access via VPC endpoints, and CloudTrail and Config enabled.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Application Load Balancer in public subnets terminates TLS, all resources including the database in public subnets, no VPC endpoints, no Secrets Manager, and no CloudTrail configuration.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Application Load Balancer in private subnets with TLS termination at the edge, public endpoint provided via an Internet-facing front door in a different account.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"All resources placed in private subnets with no public endpoint; access is only via a VPN and there is no encryption at rest.\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nOption a) is correct because it satisfies all requirements: TLS termination at the edge via the ALB in public subnets; app servers and database in private subnets; outbound updates are allowed through a NAT Gateway in the public segment without exposing private resources; access to S3 and Secrets Manager can be restricted via VPC endpoints; data at rest for the database is encrypted with a KMS CMK; auditing is provided by CloudTrail and Config.\n\n## Why Other Options Are Wrong\n\n- Option B is incorrect because the database and all resources are placed in public subnets and there is no encryption at rest, no Secrets Manager, and no CloudTrail, which increases risk and fails governance requirements.\n- Option C is incorrect because it relies on a private ALB with no public endpoint, which fails the requirement for public accessibility.\n- Option D is incorrect because it describes an entirely private deployment with no public endpoint and no encryption governance, failing the primary design requirement.\n\n## Key Concepts\n\n- Public endpoint with TLS termination at the edge (ALB or CloudFront)\n- Private subnets for app and data layers\n- NAT Gateway for outbound updates\n- S3/VPC Endpoints to avoid internet traversal\n- Encryption at rest with AWS KMS\n- Secrets management with Secrets Manager\n- Auditability via CloudTrail and Config\n\n## Real-World Application\n\nThis pattern reflects a typical enterprise-ready web app deployment on AWS, balancing public accessibility with private data security, governance, and operational best practices.","diagram":null,"difficulty":"intermediate","tags":["AWS","VPC","ALB","RDS","SecretsManager","KMS","CloudTrail","Config","certification-mcq","domain-weight-30"],"channel":"aws-saa","subChannel":"design-secure","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T23:20:26.471Z","createdAt":"2026-01-11 23:20:26"},{"id":"aws-saa-design-secure-1768173626470-1","question":"An EKS cluster hosts multiple microservices in a single namespace. You want to ensure each service can call only its own limited set of AWS APIs, and you want to prevent lateral movement if a pod is compromised. Which design best achieves this?","answer":"[{\"id\":\"a\",\"text\":\"Create a dedicated IAM role for each Kubernetes ServiceAccount using IRSA (IAM Roles for Service Accounts), and implement Kubernetes NetworkPolicy to restrict pod-to-pod traffic and isolate namespaces.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Attach a single IAM role to every node in the cluster and grant broad permissions to all pods.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Rely on Kubernetes RBAC to control AWS API access for pods.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use a shared IAM role for all services and map roles using an external identity provider.\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nOption a) is correct because IRSA provides per-service-account least-privilege AWS credentials and NetworkPolicy enforces segmentation to limit lateral movement.\n\n## Why Other Options Are Wrong\n\n- B: Attaching a single IAM role to all nodes gives all pods broad permissions; violates least privilege and risk.\n- C: Kubernetes RBAC does not control AWS API permissions; not sufficient.\n- D: A shared IAM role for all services lacks per-service isolation and is insecure.\n\n## Key Concepts\n\n- IAM Roles for Service Accounts (IRSA)\n- Kubernetes ServiceAccounts\n- Kubernetes NetworkPolicy\n- Least privilege\n\n## Real-World Application\n\nEnables safe multi-team microservice ecosystems by preventing broad AWS access and limiting east-west movement in the cluster.","diagram":null,"difficulty":"intermediate","tags":["AWS","EKS","IAM","IRSA","NetworkPolicy","certification-mcq","domain-weight-30"],"channel":"aws-saa","subChannel":"design-secure","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T23:20:26.942Z","createdAt":"2026-01-11 23:20:27"},{"id":"aws-saa-design-secure-1768173626470-2","question":"To prevent data exfiltration when workloads access S3, you want to ensure traffic from your VPC to S3 does not traverse the public internet. Which configuration achieves this with least risk and simplest maintenance?","answer":"[{\"id\":\"a\",\"text\":\"Create a VPC Gateway Endpoint for S3, configure the bucket policy to allow access only from that endpoint, and enable SSE-KMS for encryption.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Access S3 over the public internet but restrict outbound traffic with a firewall.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Route S3 traffic through a NAT Gateway without enabling an S3 endpoint.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Place S3 in a separate private subaccount and use CloudFront to access.\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nOption a) is correct because a VPC Gateway Endpoint ensures S3 traffic stays on the AWS network, the bucket policy restricts access to that endpoint, and SSE-KMS provides encryption at rest.\n\n## Why Other Options Are Wrong\n\n- Option B: Exposes S3 traffic to the public internet, defeating the objective of keeping data private.\n- Option C: NAT Gateway does not provide a private path to S3 and traffic can still traverse the internet; no endpoint means exposure risks.\n- Option D: CloudFront does not inherently restrict S3 access from a VPC path and adds unnecessary complexity; S3 should be accessed via a VPC endpoint for isolation.\n\n## Key Concepts\n\n- VPC Gateway Endpoints for S3\n- Bucket policies tightly scoped to VPC endpoints\n- Server-Side Encryption with KMS (SSE-KMS)\n\n## Real-World Application\n\nEnables private, compliant access to S3 from workloads running in a VPC without exposing data to the public internet.","diagram":null,"difficulty":"intermediate","tags":["AWS","S3","VPC","S3Endpoint","KMS","certification-mcq","domain-weight-30"],"channel":"aws-saa","subChannel":"design-secure","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T23:20:27.421Z","createdAt":"2026-01-11 23:20:27"},{"id":"q-672","question":"Design a scalable, cost-aware data ingestion and processing pipeline on AWS for 1 TB/day of log data arriving from multiple on-prem and cloud sources. The pipeline must deliver raw data immutably for 90 days, provide near-real-time enrichment within 5 seconds, and support cross-region failover. Specify services, data flows, constraints, and trade-offs?","answer":"Use per-region Kinesis Data Firehose to S3 with Object Lock for immutability, enable SSE-KMS, and enable lifecycle for cost. Run Glue Streaming for enrichment, and DynamoDB to store offsets/state. DLQ","explanation":"## Why This Is Asked\nAssesses design of scalable, compliant pipelines with immutability, latency, DR, and cost in AWS.\n\n## Key Concepts\n- Kinesis Firehose, S3, Object Lock, SSE-KMS\n- Glue Streaming, Lambda, DynamoDB state\n- DLQ, cross-region replication, CloudWatch\n\n## Code Example\n```javascript\n// Pseudo-idempotent Lambda sketch\nconst AWS = require('aws-sdk');\nconst ddb = new AWS.DynamoDB.DocumentClient();\nexports.handler = async (evt) => {\n  const key = { id: evt.id };\n  const exists = await ddb.get({TableName:'Processed', Key:key}).promise();\n  if (exists.Item) return;\n  await ddb.put({TableName:'Processed', Item:{id: evt.id, t: Date.now()}}).promise();\n  // enrich data\n};\n```\n\n## Follow-up Questions\n- How would you validate data quality and schema evolution across regions?\n- How do you handle late-arriving data and reprocessing without duplicating work?","diagram":"flowchart TD\n  A[Ingest] --> B[Kinesis Firehose]\n  B --> C[S3 regional bucket]\n  C --> D[Object Lock: immutability]\n  D --> E[Glue Streaming / Lambda]\n  E --> F[DynamoDB state]\n  F --> G[CloudWatch alarms]","difficulty":"advanced","tags":["aws-saa"],"channel":"aws-saa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","IBM"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T14:44:21.525Z","createdAt":"2026-01-11T14:44:21.525Z"},{"id":"q-930","question":"An existing web service runs on EC2 in a single VPC with an ALB. Traffic surges cause latency spikes and occasional outages during AZ failures. Propose a beginner-friendly, cost-conscious HA setup using an Application Load Balancer, Auto Scaling across at least two AZs, and a relational database option. Include networking, health checks, a scaling policy, and a basic DB deployment choice with trade-offs. What would you implement first and why?","answer":"Create an ALB in front of an ASG spread across two AZs with a CPU-based target tracking policy (min 2, max 6). Use a Multi-AZ RDS or Aurora for the database. Enable CloudWatch alarms for latency and e","explanation":"## Why This Is Asked\n\nAssesses practical HA design, cost awareness, and fundamental AWS components for a resilient web service.\n\n## Key Concepts\n\n- Application Load Balancer\n- Auto Scaling Group across AZs\n- Multi-AZ RDS or Aurora\n- CloudWatch alarms\n- S3 + CloudFront for static assets\n- Cost trade-offs between on-demand instances vs reserved capacity\n\n## Code Example\n\n```yaml\n# Minimal CloudFormation sketch (illustrative only)\nResources:\n  MyALB:\n    Type: AWS::ElasticLoadBalancingV2::LoadBalancer\n    Properties:\n      Name: my-alb\n      Scheme: internet-facing\n  MyTargetGroup:\n    Type: AWS::ElasticLoadBalancingV2::TargetGroup\n    Properties:\n      VpcId: vpc-xxxx\n      Protocol: HTTP\n      Port: 80\n  MyASG:\n    Type: AWS::AutoScaling::AutoScalingGroup\n    Properties:\n      VPCZoneIdentifier:\n        - subnet-aaa\n        - subnet-bbb\n      TargetGroupARNs:\n        - !Ref MyTargetGroup\n      LaunchConfigurationName: !Ref MyLaunchConfig\n```\n\n## Follow-up Questions\n\n- Compare Aurora Provisioned vs Aurora Serverless for this pattern.\n- How would you monitor latency, and what would trigger scale-out vs scale-in?","diagram":null,"difficulty":"beginner","tags":["aws-saa"],"channel":"aws-saa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T15:40:08.464Z","createdAt":"2026-01-12T15:40:08.464Z"}],"subChannels":["design-performant","design-resilient","design-secure","general"],"companies":["Amazon","Hashicorp","IBM","MongoDB"],"stats":{"total":13,"beginner":1,"intermediate":11,"advanced":1,"newThisWeek":13}}