{"questions":[{"id":"aws-saa-design-cost-1768243188462-0","question":"A web application experiences variable traffic and needs to minimize cost while maintaining a 99.95% SLA. Which approach best achieves cost efficiency without compromising availability?","answer":"[{\"id\":\"a\",\"text\":\"Use only On-Demand instances with a single Auto Scaling group across all instances\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use Auto Scaling with a mixed fleet of On-Demand and Spot Instances and configure Spot interruptions to failover to On-Demand\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Use Reserved Instances exclusively and schedule workloads to run only during business hours\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Migrate the workload to Lambda for all compute tasks to avoid managed instances\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct. A mixed On-Demand and Spot strategy with Auto Scaling reduces cost while preserving capacity; configure Spot interruptions with automatic fallback to On-Demand to maintain SLA.\n\n## Why Other Options Are Wrong\n- Option A: All On-Demand is more expensive and ignores potential savings from Spot capacity.\n- Option C: RI-only pricing reduces flexibility and can lead to underutilized capacity if demand fluctuates.\n- Option D: While serverless can cut costs, it introduces cold-start risks and state-management challenges for a typical web app; not universally suitable.\n\n## Key Concepts\n- Spot Instances, On-Demand, Auto Scaling, interruption handling\n- Mixed instance policies (EC2 Fleet / ASG mixed instances)\n- SLA resilience under variable demand\n\n## Real-World Application\n- Implement an ASG with mixed instances, configure target capacity, enable Spot interruption handling, and set graceful On-Demand fallback to meet SLA during Spot interruptions.","diagram":null,"difficulty":"intermediate","tags":["AWS","EC2","Spot Instances","Auto Scaling","cost-optimization","certification-mcq","domain-weight-20"],"channel":"aws-saa","subChannel":"design-cost","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T18:39:48.463Z","createdAt":"2026-01-12 18:39:48"},{"id":"aws-saa-design-cost-1768243188462-1","question":"A data lake stores 100 TB of raw logs in S3 in us-east-1. Logs are accessed by analysts infrequently, with retrieval needed within 24 hours. Which storage strategy best minimizes cost while meeting the retrieval window?","answer":"[{\"id\":\"a\",\"text\":\"Keep all data in S3 Standard with no lifecycle management\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Move data older than 90 days to S3 Glacier Deep Archive via a lifecycle policy\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Move data older than 90 days to S3 Standard-IA only\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use S3 Intelligent-Tiering with no lifecycle rules\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct. Lifecycle transitions to S3 Glacier Deep Archive provide substantial cost savings for long-term retention while still supporting a 24-hour retrieval window (standard retrieval times are typically within hours). If faster retrieval is occasionally required, expedited options are available at higher cost.\n\n## Why Other Options Are Wrong\n- Option A: Keeps data in Standard, which is expensive for long-term infrequently accessed data.\n- Option C: Standard-IA is cheaper than Standard but not as cost-effective as Deep Archive for long-term retention, and frequent rehydration can add latency.\n- Option D: Intelligent-Tiering adds monitoring/transition overhead and may be more expensive for predictably cold data.\n\n## Key Concepts\n- S3 storage classes and lifecycle policies\n- Glacier Deep Archive retrieval options and costs\n- Cost-optimized data retention\n\n## Real-World Application\n- Implement a lifecycle policy: objects older than 90 days transition to Glacier Deep Archive; periodically validate that required quick-access data is still retrievable within organizational SLAs.","diagram":null,"difficulty":"intermediate","tags":["AWS","S3","Glacier","Storage-Class","cost-optimization","certification-mcq","domain-weight-20"],"channel":"aws-saa","subChannel":"design-cost","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T18:39:49.075Z","createdAt":"2026-01-12 18:39:49"},{"id":"aws-saa-design-cost-1768243188462-2","question":"An e-commerce API serves a global user base and must minimize cross-region data transfer costs while keeping latency reasonable. Which pattern best achieves this balance?","answer":"[{\"id\":\"a\",\"text\":\"Route all traffic to a single region using AWS Global Accelerator\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Cache all dynamic responses at the edge with CloudFront but serve writes from a central region\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use DynamoDB Global Tables to replicate data across regions and serve reads locally\",\"isCorrect\":true},{\"id\":\"d\",\"text\":\"Rely on cross-region RDS read replicas to serve reads closer to users\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption C is correct. DynamoDB Global Tables replicate data across regions, enabling local reads and reducing cross-region data transfer while preserving low-latency access for global users.\n\n## Why Other Options Are Wrong\n- Option A: Centralizing to one region increases latency for distant users and does not reduce data transfer costs.\n- Option B: Edge caching helps with static content but not with dynamic API responses and write consistency across regions.\n- Option D: Cross-region RDS replicas introduce ongoing cross-region replication costs and latency for writes.\n\n## Key Concepts\n- DynamoDB Global Tables, cross-region replication, low-latency reads\n- Edge caching vs. regional data placement\n\n## Real-World Application\n- Deploy Global Tables in US-East, EU-West, and AP-South regions; route reads to nearest regional replica; monitor cross-region replication costs and latency.","diagram":null,"difficulty":"intermediate","tags":["AWS","DynamoDB","Global Tables","latency","cost-optimization","certification-mcq","domain-weight-20"],"channel":"aws-saa","subChannel":"design-cost","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T18:39:49.541Z","createdAt":"2026-01-12 18:39:49"},{"id":"aws-saa-design-cost-1768243188462-3","question":"For long-term backup cost optimization across multiple AWS services, which approach best reduces storage expense while meeting retention needs?","answer":"[{\"id\":\"a\",\"text\":\"Store all backups in S3 Standard and delete after 1 year\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Backups are centralized to a single region using EBS snapshots only\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use AWS Backup with a lifecycle policy to move older backups to S3 Glacier Deep Archive\",\"isCorrect\":true},{\"id\":\"d\",\"text\":\"Rely on manual backups to on-prem storage for long-term retention\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption C is correct. AWS Backup can centralize backups and apply lifecycle policies to move older backups to Glacier Deep Archive, substantially reducing storage costs for long-term retention.\n\n## Why Other Options Are Wrong\n- Option A: Standard storage is expensive for backups kept long-term.\n- Option B: EBS snapshots alone are not a cost-efficient, scalable long-term retention strategy across multiple services.\n- Option D: On-prem retention adds operational overhead and potential risk; cloud-native lifecycle policies are typically cheaper and easier to manage.\n\n## Key Concepts\n- AWS Backup, lifecycle policies, cross-service backups\n- S3 Glacier Deep Archive cost advantages\n\n## Real-World Application\n- Configure AWS Backup plans with Vaults and a lifecycle rule: keep daily backups for 30 days, then move to Glacier Deep Archive; test retrieval from Deep Archive to verify RTO expectations.","diagram":null,"difficulty":"intermediate","tags":["AWS","Backup","Glacier","S3","cost-optimization","certification-mcq","domain-weight-20"],"channel":"aws-saa","subChannel":"design-cost","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T18:39:49.720Z","createdAt":"2026-01-12 18:39:49"},{"id":"aws-saa-design-cost-1768243188462-4","question":"A multi-account setup requires cost governance and accountability by department. Which approach provides the best visibility and ability to optimize costs?","answer":"[{\"id\":\"a\",\"text\":\"Use a single payer account and rely on budgets per department\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use AWS Organizations with per-account quotas and consolidated billing\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Enable Cost Allocation Tags and use AWS Cost Explorer with filters by tag\",\"isCorrect\":true},{\"id\":\"d\",\"text\":\"Rely on monthly invoices and manual cost reviews\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption C is correct. Enabling Cost Allocation Tags and using Cost Explorer allows per-department cost visibility and detailed analysis, enabling targeted optimization efforts.\n\n## Why Other Options Are Wrong\n- Option A: Lacks per-department visibility and accountability.\n- Option B: Consolidated billing helps overall costs but does not inherently allocate costs by department without tagging.\n- Option D: Inaccurate for proactive cost control and lacks granularity.\n\n## Key Concepts\n- Cost Allocation Tags, Cost Explorer, multi-account governance\n\n## Real-World Application\n- Tag resources by department, cost center, and project; use Cost Explorer dashboards to identify high-spend services by department and enforce budgets.","diagram":null,"difficulty":"intermediate","tags":["AWS","Cost-Explorer","Cost Allocation Tags","Organizations","cost-optimization","certification-mcq","domain-weight-20"],"channel":"aws-saa","subChannel":"design-cost","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T18:39:49.883Z","createdAt":"2026-01-12 18:39:49"},{"id":"aws-saa-design-performant-1768227966430-0","question":"A globally distributed e-commerce application experiences varying read traffic and needs low-latency data access while maintaining strong durability. Which design best achieves this?","answer":"[{\"id\":\"a\",\"text\":\"Use a single-region RDS with read replicas in other regions behind a latency-based Route 53 policy\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use DynamoDB Global Tables with DAX in each region, fronted by CloudFront for static assets, and Route 53 latency-based routing\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Use a single-region DynamoDB table with an in-memory cache in the application layer and a CDN for static assets\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Store all data in S3 objects and retrieve via a custom API across regions\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct because DynamoDB Global Tables provide multi-region, fully replicated data with low-latency reads, and DAX accelerates read performance within a region. CloudFront caches static assets at edge locations to reduce origin load, and Route 53 latency-based routing helps direct users to the nearest healthy region for optimal latency.\n\n## Why Other Options Are Wrong\n- A: Cross-region RDS read replicas can reduce latency but require more maintenance and do not provide the seamless multi-region write and global low-latency access that DynamoDB Global Tables offer.\n- C: A single-region DynamoDB with an app-layer cache does not guarantee global low-latency access for all regions, and the cache may become a bottleneck.\n- D: S3 is object storage and does not provide efficient queryable access patterns for dynamic data across regions; relying on S3 alone undermines latency for transactional-like reads.\n\n## Key Concepts\n- DynamoDB Global Tables\n- DynamoDB with DAX\n- CloudFront edge caching\n- Route 53 latency-based routing\n\n## Real-World Application\n- Use this pattern for global online retailers needing consistent performance across regions with resilient failover.\n","diagram":null,"difficulty":"intermediate","tags":["AWS","DynamoDB","DAX","CloudFront","Route53","certification-mcq","domain-weight-24"],"channel":"aws-saa","subChannel":"design-performant","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T14:26:06.431Z","createdAt":"2026-01-12 14:26:06"},{"id":"aws-saa-design-performant-1768227966430-1","question":"An application experiences irregular traffic spikes and must process background tasks without blocking user requests. Which design best enables decoupled, scalable processing?","answer":"[{\"id\":\"a\",\"text\":\"Use Kinesis Data Streams to ingest events and process them with EC2 workers behind an ASG\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use SQS to decouple producers and consumers and scale workers independently\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Use an RDS Multi-AZ database to absorb spikes with read replicas\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Expose a single API Gateway endpoint to directly trigger a Lambda for all work\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct because SQS provides durable, asynchronous messaging that decouples producers from consumers, allowing workers to scale independently based on queue depth.\n\n## Why Other Options Are Wrong\n- A: Kinesis is suitable for streaming data but adds complexity for typical background-job processing and requires more infrastructure to scale workers.\n- C: RDS Multi-AZ addresses database high availability, not decoupled background processing.\n- D: A single API-based trigger creates tight coupling and can throttle user-facing latency during spikes.\n\n## Key Concepts\n- SQS (Simple Queue Service)\n- Decoupled architectures\n- Event-driven processing\n\n## Real-World Application\n- Ideal for order processing pipelines, image processing queues, or any backlog-prone workflows.\n","diagram":null,"difficulty":"intermediate","tags":["AWS","SQS","Kinesis","Lambda","certification-mcq","domain-weight-24"],"channel":"aws-saa","subChannel":"design-performant","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T14:26:06.927Z","createdAt":"2026-01-12 14:26:07"},{"id":"aws-saa-design-performant-1768227966430-2","question":"A workload requires shared file storage accessible by a fleet of EC2 instances across multiple AZs with high throughput and low latency. Which AWS service best fits this requirement?","answer":"[{\"id\":\"a\",\"text\":\"EBS volumes attached to each EC2 instance\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"S3 object storage\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"EFS (Elastic File System)\",\"isCorrect\":true},{\"id\":\"d\",\"text\":\"FSx for Lustre\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption C is correct because EFS provides a scalable, shared filesystem that can be mounted across multiple EC2 instances in different AZs, offering high throughput and low latency for shared access.\n\n## Why Other Options Are Wrong\n- A: EBS is tied to a single AZ and cannot be shared across instances in different AZs.\n- B: S3 is object storage, not a POSIX-style shared filesystem for concurrent file access.\n- D: FSx for Lustre is optimized for HPC workloads and not the typical shared filesystem use case across general EC2 instances.\n\n## Key Concepts\n- EFS shared file system across AZs\n- POSIX-compliant storage for multiple EC2s\n\n## Real-World Application\n- Suitable for web servers needing shared logs, media uploads, or shared application state.\n","diagram":null,"difficulty":"intermediate","tags":["AWS","EFS","EBS","S3","certification-mcq","domain-weight-24"],"channel":"aws-saa","subChannel":"design-performant","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T14:26:07.385Z","createdAt":"2026-01-12 14:26:07"},{"id":"aws-saa-design-performant-1768227966430-3","question":"A highly-variable workload requires automatic, fine-grained scaling of a NoSQL database without manual capacity planning. Which DynamoDB capacity model best fit this requirement?","answer":"[{\"id\":\"a\",\"text\":\"Provisioned capacity with auto-scaling\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"On-demand capacity mode\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Reserved capacity with upfront payment\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Global secondary index capacity planning\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct because DynamoDB on-demand capacity automatically scales to accommodate request traffic without prior capacity planning, making it ideal for unpredictable workloads.\n\n## Why Other Options Are Wrong\n- A: Provisioned with auto-scaling still requires configuration and may not react quickly enough to sudden spikes.\n- C: Reserved capacity provides cost savings but is not suited for unpredictable workloads.\n- D: Global secondary indexes are for query patterns, not a capacity model.\n\n## Key Concepts\n- DynamoDB on-demand vs provisioned capacity\n- Auto-scaling considerations\n\n## Real-World Application\n- Use for new services with uncertain traffic patterns or seasonal spikes.\n","diagram":null,"difficulty":"intermediate","tags":["AWS","DynamoDB","On-Demand","Auto-Scaling","certification-mcq","domain-weight-24"],"channel":"aws-saa","subChannel":"design-performant","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T14:26:07.686Z","createdAt":"2026-01-12 14:26:07"},{"id":"aws-saa-design-performant-1768227966430-4","question":"Your stateless API must remain highly available across Availability Zones with minimal operational overhead. Which design is the most resilient and scalable choice?","answer":"[{\"id\":\"a\",\"text\":\"Deploy in two AZs behind an Application Load Balancer with a Multi-AZ RDS instance for the database\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Deploy in a single AZ with an EBS-backed instance and no load balancer\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Deploy across two regions with a single Lambda in each region and no data replication\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Deploy behind an API Gateway with a single Lambda in a single region\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A is correct because running across multiple AZs behind an Application Load Balancer distributes traffic and provides fault tolerance, while a Multi-AZ RDS instance ensures database availability and automatic failover within the region.\n\n## Why Other Options Are Wrong\n- B: Single-AZ deployment creates a single point of failure and is not resilient.\n- C: Two regions without data replication risks data inconsistency and operational overhead; also regional failures can impact availability.\n- D: A single-region, single-Lambda setup lacks multi-AZ traffic distribution and high availability guarantees.\n\n## Key Concepts\n- Availability Zones and Multi-AZ deployments\n- Application Load Balancer (ALB)\n- AWS managed databases (RDS) high availability\n\n## Real-World Application\n- This pattern is standard for production-grade stateless APIs requiring high availability with minimal manual operations.\n","diagram":null,"difficulty":"intermediate","tags":["AWS","ALB","RDS","High-Availability","certification-mcq","domain-weight-24"],"channel":"aws-saa","subChannel":"design-performant","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T14:26:07.850Z","createdAt":"2026-01-12 14:26:07"},{"id":"aws-saa-design-performant-1768293401856-0","question":"You run a globally distributed web application that demands sub-50 ms read latency for users worldwide and automatic disaster recovery across regions. Which design best meets these requirements?","answer":"[{\"id\":\"a\",\"text\":\"Deploy an Aurora Global Database across two regions and front the endpoints with AWS Global Accelerator and Route 53-based health-based failover.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Deploy a single-region RDS with Multi-AZ and configure cross-region read replicas in a second region behind a Global Load Balancer.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Migrate to DynamoDB Global Tables across regions and route reads to the nearest region via API Gateway and CloudFront.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Run the application in a single region with EC2 Auto Scaling and rely on a static front-end hosted in S3 for disaster recovery.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A. Aurora Global Database across regions provides cross-region replication with low-latency reads and automated disaster recovery, and when paired with Global Accelerator and Route 53, offers low latency and regional failover capabilities.\n\n## Why Other Options Are Wrong\n- Option B: Cross-region read replicas in a traditional RDS setup are not automatically promoted during regional failures, so DR is not truly automatic and latency/flavor of failover are less deterministic.\n- Option C: DynamoDB Global Tables support multi-region replication for non-relational workloads; for relational transactional workloads, Aurora Global Database generally provides better relational semantics and comparable latency for reads.\n- Option D: A single-region deployment lacks regional DR and cannot meet cross-region automatic recovery requirements.\n\n## Key Concepts\n- Aurora Global Database\n- AWS Global Accelerator\n- Route 53 health-based routing\n\n## Real-World Application\n- In production, design teams would deploy an Aurora Global Database spanning the primary and secondary regions, enable Global Accelerator to optimize user traffic to the closest region, and configure Route 53 to fail over to the healthy region automatically when regional endpoints become unhealthy.\n","diagram":null,"difficulty":"intermediate","tags":["AWS","Aurora","Global-Accelerator","Route53","certification-mcq","domain-weight-24"],"channel":"aws-saa","subChannel":"design-performant","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T08:36:41.857Z","createdAt":"2026-01-13 08:36:42"},{"id":"aws-saa-design-performant-1768293401856-1","question":"A data lake stores raw logs in S3; to minimize cost while retaining data for regulatory compliance, you want a lifecycle policy that moves older data to cheaper storage and eventually deletes it. Which policy achieves this?","answer":"[{\"id\":\"a\",\"text\":\"Transition objects older than 90 days to S3 Glacier Deep Archive and delete them after 7 years.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Transition objects to S3 Standard-IA after 60 days and delete after 1 year.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Keep objects in S3 Standard indefinitely and archive manually.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Move data to an on-prem archive and delete from S3.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A. A lifecycle policy that moves data older than 90 days to S3 Glacier Deep Archive and deletes after 7 years aligns with cost optimization for rarely accessed data while satisfying long-term retention requirements.\n\n## Why Other Options Are Wrong\n- Option B: Standard-IA is more expensive for long-term large-scale storage and not ideal for data that is rarely accessed yet must meet archival retention.\n- Option C: Manual archiving introduces operational overhead and risk of non-compliance due to delays.\n- Option D: Offloading to on-premises adds complexity and negates cloud-native lifecycle benefits.\n\n## Key Concepts\n- S3 Lifecycle policies\n- Glacier Deep Archive\n- Cost optimization for data lakes\n\n## Real-World Application\n- Data engineers implement lifecycle rules to automatically transition logs to cheaper storage after a defined period and set expiration policies to meet regulatory retention without manual intervention.\n","diagram":null,"difficulty":"intermediate","tags":["AWS","S3","Glacier","Lifecycle","certification-mcq","domain-weight-24"],"channel":"aws-saa","subChannel":"design-performant","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T08:36:42.340Z","createdAt":"2026-01-13 08:36:42"},{"id":"aws-saa-design-performant-1768293401856-2","question":"A batch-processing workload runs entirely inside a VPC and frequently accesses S3 and DynamoDB. To minimize data exposure and avoid any internet egress, which design provides private access to AWS services?","answer":"[{\"id\":\"a\",\"text\":\"Use a NAT Gateway to access public endpoints for S3 and DynamoDB.\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Create VPC endpoints for S3 (gateway) and DynamoDB (interface) to route traffic entirely within the AWS network.\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Establish a VPN connection to an on-premises network and access services through it.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Place resources in public subnets with an Internet Gateway for direct access.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B. VPC endpoints (S3 gateway endpoint and DynamoDB interface endpoint) allow private, region-traffic that never traverses the public Internet, reducing exposure and egress costs.\n\n## Why Other Options Are Wrong\n- Option A: NAT Gateway still uses public Internet egress to reach AWS services, exposing traffic.\n- Option C: VPN to on-premises introduces routing complexity and does not provide private AWS service endpoints within the VPC.\n- Option D: Public subnets expose resources to the Internet and undermine private access goals.\n\n## Key Concepts\n- VPC Endpoints (Gateway/Interface)\n- Private connectivity to AWS services\n\n## Real-World Application\n- Data pipelines that require secure, low-latency access to S3 and DynamoDB from within VPCs commonly rely on VPC endpoints to meet compliance and security requirements.\n","diagram":null,"difficulty":"intermediate","tags":["AWS","VPC-Endpoints","S3","DynamoDB","certification-mcq","domain-weight-24"],"channel":"aws-saa","subChannel":"design-performant","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T08:36:42.849Z","createdAt":"2026-01-13 08:36:42"},{"id":"aws-saa-design-performant-1768293401856-3","question":"To serve a globally distributed API with variable traffic, which design balances cost, scale, and latency most effectively?","answer":"[{\"id\":\"a\",\"text\":\"Deploy an EC2 Auto Scaling group behind an Application Load Balancer in multiple availability zones.\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use AWS Lambda with API Gateway and provisioned concurrency to achieve predictable, low latency at a low cost.\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Run microservices on ECS with Fargate and a global load balancer across regions.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use Lightsail instances behind a regional load balancer.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B. Lambda with API Gateway provides event-driven, serverless scaling with pay-per-use pricing, ideal for unpredictable global traffic. Provisioned concurrency can mitigate cold starts to maintain low latency, balancing cost and responsiveness for a global audience.\n\n## Why Other Options Are Wrong\n- Option A: EC2 Auto Scaling requires capacity planning, can incur higher costs for idle capacity, and may not achieve optimal latency globally without complex routing.\n- Option C: ECS/Fargate with a regional/global setup adds complexity and may not achieve the lowest latency for sporadic global traffic unless carefully tuned.\n- Option D: Lightsail is limited compared to the breadth of AWS services and networking features needed for a globally distributed API.\n\n## Key Concepts\n- AWS Lambda & API Gateway\n- Provisioned Concurrency\n- Serverless global scale\n\n## Real-World Application\n- Teams often pair Lambda with API Gateway for public APIs and use provisioned concurrency or Lambda@Edge for edge latency optimization, keeping operational overhead low while maintaining responsiveness at scale.\n","diagram":null,"difficulty":"intermediate","tags":["AWS","Lambda","API-Gateway","Provisioned-Concurrency","certification-mcq","domain-weight-24"],"channel":"aws-saa","subChannel":"design-performant","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T08:36:43.029Z","createdAt":"2026-01-13 08:36:43"},{"id":"aws-saa-design-performant-1768293401856-4","question":"You need to access a partner service privately from your VPC, without traversing the public Internet. Which approach provides secure private connectivity to the partner service?","answer":"[{\"id\":\"a\",\"text\":\"Expose the partner service publicly with TLS and use a public endpoint.\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Establish a VPN connection between your VPC and the partner's network.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use AWS PrivateLink to create an Interface Endpoint to the partner service inside your VPC.\",\"isCorrect\":true},{\"id\":\"d\",\"text\":\"Use AWS Direct Connect to connect to the partner's on-prem network.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption C. AWS PrivateLink creates a private endpoint (Interface Endpoint) in your VPC to access the partner service without traversing the public Internet, improving security and reducing exposure.\n\n## Why Other Options Are Wrong\n- Option A: Public endpoints re-expose traffic to the Internet and rely on Internet routing.\n- Option B: VPNs add complexity and do not provide the same seamless private endpoint experience within AWS.\n- Option D: Direct Connect is primarily for private connections to on-prem networks, not to a partner service hosted as a private AWS service.\n\n## Key Concepts\n- AWS PrivateLink\n- Interface Endpoints\n\n## Real-World Application\n- Enterprises integrate SaaS partners or partner-managed services via PrivateLink to ensure traffic stays on the AWS network boundary and meets security/compliance requirements.\n","diagram":null,"difficulty":"intermediate","tags":["AWS","PrivateLink","VPC","Interface-Endpoint","certification-mcq","domain-weight-24"],"channel":"aws-saa","subChannel":"design-performant","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T08:36:43.195Z","createdAt":"2026-01-13 08:36:43"},{"id":"aws-saa-design-resilient-1768213442314-0","question":"You are designing a serverless web application that must remain available during a regional outage in us-east-1. The app uses DynamoDB to store user profiles and S3 to store assets. To minimize RPO and enable automatic failover between us-east-1 and us-west-2, which design pattern should you implement?","answer":"[{\"id\":\"a\",\"text\":\"DynamoDB Global Tables replicated across us-east-1 and us-west-2, S3 cross-region replication for assets, and Route 53 failover with health checks to route traffic to the healthy region automatically.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Maintain a single-region DynamoDB instance in us-east-1 with manual backups to us-west-2, then switch DNS to the backup region after an outage.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use an RDS Multi-AZ deployment and rely on the primary region failing over automatically across regions.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Deploy two completely independent stacks in us-east-1 and us-west-2 with separate DynamoDB tables and implement cross-region data sync via Lambda.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption a is correct: it provides true multi-region active data and automated routing for DR. DynamoDB Global Tables replicates data across regions to minimize RPO, S3 cross-region replication ensures asset availability, and Route 53 health checks enable automatic failover.\n\n## Why Other Options Are Wrong\n- Option B: relies on manual failover and single-region state; results in higher RPO and downtime.\n- Option C: RDS Multi-AZ is intra-region; does not protect against regional outages.\n- Option D: while feasible, it adds operational complexity and potential data divergence without an automated global failover strategy.\n\n## Key Concepts\n- DynamoDB Global Tables\n- S3 Cross-Region Replication\n- Route 53 Health Checks and Failover\n- Multi-Region Disaster Recovery\n\n## Real-World Application\nImplement a global architecture with tables replicated across regions, replicate assets, and configure DNS failover so traffic redirects automatically to the healthy region during outages.","diagram":null,"difficulty":"intermediate","tags":["AWS","DynamoDB","S3","Route53","certification-mcq","domain-weight-26"],"channel":"aws-saa","subChannel":"design-resilient","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T10:24:02.315Z","createdAt":"2026-01-12 10:24:02"},{"id":"aws-saa-design-resilient-1768213442314-1","question":"An organization runs a critical application in us-east-1 using EC2 Auto Scaling behind an Application Load Balancer. The service depends on an RDS primary instance in us-east-1. The organization wants to meet an RTO of 15 minutes and an RPO of less than 5 minutes in the event of a regional outage. Which DR pattern best achieves this?","answer":"[{\"id\":\"a\",\"text\":\"Create an RDS cross-region read replica in us-west-2 and configure Route 53 health checks with a failover to the us-west-2 endpoint; use a Lambda function to automatically promote the read replica if the primary region becomes unhealthy.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Enable RDS Multi-AZ in us-east-1 and rely on automatic failover to the standby within the same region.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Implement DynamoDB global tables across us-east-1 and us-west-2 for all data and route traffic accordingly.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use S3 cross-region replication to replicate configuration data and perform manual DNS failover.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption a is correct: RDS cross-region read replicas provide DR across regions with low RPO; automating promotion via a Lambda function helps achieve the target 15-minute RTO. B only covers intra-region failover and cannot survive regional outages. C introduces a data model mismatch for relational data. D relies on manual steps, increasing the outage duration.\n\n## Why Other Options Are Wrong\n- Option B: Intra-region failover does not protect against regional outages.\n- Option C: DynamoDB is not the primary data store for a relational RDS-based workload.\n- Option D: Manual DNS failover slows recovery and increases RTO.\n\n## Key Concepts\n- RDS cross-region read replicas\n- Automated failover using Lambda\n- Route 53 health checks\n- Disaster recovery objectives (RTO/RPO)\n\n## Real-World Application\nConfigure a cross-region read replica in the secondary region, monitor the primary region health, trigger promotion of the replica when unhealthy, and route traffic to the secondary region automatically.","diagram":null,"difficulty":"intermediate","tags":["AWS","RDS","Route53","DR","certification-mcq","domain-weight-26"],"channel":"aws-saa","subChannel":"design-resilient","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T10:24:02.848Z","createdAt":"2026-01-12 10:24:03"},{"id":"aws-saa-design-resilient-1768213442314-2","question":"To store 7 years of critical audit logs with minimal ongoing costs while ensuring immutability during the retention window, which storage approach is best?","answer":"[{\"id\":\"a\",\"text\":\"Store logs in S3 Standard and set a lifecycle rule to transition to Glacier Deep Archive after 30 days, and enable S3 Object Lock in compliance mode to ensure immutability for the retention period.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Store logs in S3 Standard-Infrequent Access with indefinite retention.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Store logs on EBS volumes attached to EC2 instances and snapshot them to S3.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use S3 Intelligent-Tiering with no lifecycle rules.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption a is correct: transitioning to Glacier Deep Archive reduces ongoing costs for long-term retention, while S3 Object Lock in compliance mode provides immutability to meet audit requirements.\n\n## Why Other Options Are Wrong\n- Option B: Standard-IA reduces costs but lacks guaranteed immutability and may still be more expensive than Glacier Deep Archive for long-term retention.\n- Option C: EBS snapshots are not ideal for long-term, cost-effective durable storage of large audit logs.\n- Option D: Without lifecycle rules, data may remain in a higher-cost tier longer than necessary, and Object Lock is not enforced without S3 Lock configurations.\n\n## Key Concepts\n- S3 Lifecycle transitions\n- Glacier Deep Archive storage class\n- S3 Object Lock (compliance mode)\n\n## Real-World Application\nConfigure a lifecycle policy that moves logs from S3 Standard to Glacier Deep Archive after 30 days and enable Object Lock in compliance mode for the retention window to ensure immutability.","diagram":null,"difficulty":"intermediate","tags":["AWS","S3","GlacierDeepArchive","ObjectLock","certification-mcq","domain-weight-26"],"channel":"aws-saa","subChannel":"design-resilient","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T10:24:03.366Z","createdAt":"2026-01-12 10:24:03"},{"id":"aws-saa-design-resilient-1768279330800-0","question":"A global e-commerce application uses DynamoDB for product catalog and S3 for product images. The business requirement is to remain available with minimal data loss even if the us-east-1 region becomes completely unavailable for an extended period. Which design provides maximum resilience and minimal data loss?","answer":"[{\"id\":\"a\",\"text\":\"Replicate DynamoDB tables to another region using DynamoDB Global Tables, replicate S3 assets via Cross-Region Replication, and route traffic with Route 53 failover.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Rely on a single-region deployment with daily backups to S3.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use only DynamoDB on-demand backups without cross-region replication.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Rely solely on S3 Cross-Region Replication for all data.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA) Implement DynamoDB Global Tables for cross-region replication and S3 Cross-Region Replication, with Route 53 failover to route traffic to the healthy region.\n\n## Why Other Options Are Wrong\n- B: Single-region deployment cannot tolerate regional outages and will incur downtime.\n- C: Backups alone do not keep the application available during an outage across regions; there is no active cross-region data path.\n- D: S3 replication alone does not cover catalog data in DynamoDB and does not address DNS failover or active availability across regions.\n\n## Key Concepts\n- DynamoDB Global Tables for multi-region resilience\n- S3 Cross-Region Replication for asset availability\n- Route 53 health checks and failover routing\n\n## Real-World Application\nUse this pattern for global, highly available storefronts where regional outages must not disrupt user experience or cause data loss.\n","diagram":null,"difficulty":"intermediate","tags":["DynamoDB","DynamoDB-Global-Tables","Route53","S3","certification-mcq","domain-weight-26"],"channel":"aws-saa","subChannel":"design-resilient","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T04:42:10.801Z","createdAt":"2026-01-13 04:42:11"},{"id":"aws-saa-design-resilient-1768279330800-1","question":"A critical multi-region web service is deployed in us-east-1 and eu-west-1 with a multi-region RDS database and S3 assets. An entire region outage is possible. Which pattern best ensures continuity of service with minimal data loss during a regional outage?","answer":"[{\"id\":\"a\",\"text\":\"Configure Route 53 failover routing to a healthy region and maintain cross-region RDS read replicas.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Rely on a single-region deployment with local backups only.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Disable cross-region replication and route all traffic to one region.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use S3 replication without cross-region DB replication and no DNS failover.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA) Use Route 53 failover routing to a healthy region and maintain cross-region RDS read replicas to ensure data availability and low RTO/RPO during regional outages.\n\n## Why Other Options Are Wrong\n- B: Does not provide cross-region availability or rapid failover.\n- C: Removes critical cross-region resilience and DNS-directed recovery.\n- D: S3 replication alone does not address DB availability and DNS failover across regions.\n\n## Key Concepts\n- Route 53 failover routing\n- RDS cross-region read replicas\n- Cross-region data availability\n\n## Real-World Application\nHelps ensure customer-facing APIs stay responsive and data-consistent during regional disruptions.\n","diagram":null,"difficulty":"intermediate","tags":["Route53","RDS","S3","certification-mcq","domain-weight-26"],"channel":"aws-saa","subChannel":"design-resilient","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T04:42:11.160Z","createdAt":"2026-01-13 04:42:11"},{"id":"aws-saa-design-resilient-1768279330800-2","question":"An e-commerce application uses EC2 Auto Scaling behind an Application Load Balancer spanning multiple AZs. During traffic spikes, some instances launch slowly, causing user-visible latency. Which approach best improves resilience and reduces capacity shortfalls during bursts?","answer":"[{\"id\":\"a\",\"text\":\"Use a mixed instance policy with On-Demand and Spot Instances across multiple AZs and enable instance refresh.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Lock the deployment to a single AZ to reduce cross-AZ networking overhead.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Disable Health Checks in the Auto Scaling group to avoid premature termination.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Increase the size of a single instance type to handle spikes.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA) A mixed instance policy with On-Demand and Spot Instances across multiple AZs provides fault tolerance and cost efficiency, reducing the risk of capacity shortfalls during bursts.\n\n## Why Other Options Are Wrong\n- B: Reducing AZs decreases fault tolerance and resilience.\n- C: Disabling health checks leads to poorer fault detection and degraded stability.\n- D: Scaling up a single instance type limits flexibility and may not meet demand efficiently.\n\n## Key Concepts\n- Auto Scaling mixedInstancePolicy\n- Spot vs On-Demand instances\n- Multi-AZ resilience\n\n## Real-World Application\nUseful for cost-aware, highly available web front-ends facing unpredictable traffic patterns.\n","diagram":null,"difficulty":"intermediate","tags":["EC2","AutoScaling","ALB","certification-mcq","domain-weight-26"],"channel":"aws-saa","subChannel":"design-resilient","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T04:42:11.506Z","createdAt":"2026-01-13 04:42:11"},{"id":"aws-saa-design-resilient-1768279330800-3","question":"A Lambda-based API behind API Gateway experiences intermittent cold starts during sudden bursts, impacting latency. Which approach most effectively reduces cold starts and improves availability?","answer":"[{\"id\":\"a\",\"text\":\"Enable Provisioned Concurrency on the Lambda functions used by API Gateway.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Increase the Lambda function timeout to a very high value.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Move the API to EC2 to avoid Lambda cold starts altogether.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Enable caching in API Gateway to mask latency.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA) Provisioned Concurrency keeps a specified number of Lambda instances pre-initialized, eliminating cold starts for those invocations.\n\n## Why Other Options Are Wrong\n- B: Longer timeouts do not address cold start latency.\n- C: Moving to EC2 increases management overhead and reduces serverless benefits.\n- D: API Gateway caching helps response times but does not prevent cold starts at the function level.\n\n## Key Concepts\n- Lambda Provisioned Concurrency\n- Cold start fundamentals\n- API Gateway integration with Lambda\n\n## Real-World Application\nUseful for latency-sensitive APIs with burst traffic patterns where consistent response times are critical.\n","diagram":null,"difficulty":"intermediate","tags":["Lambda","API-Gateway","Serverless","certification-mcq","domain-weight-26"],"channel":"aws-saa","subChannel":"design-resilient","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T04:42:11.631Z","createdAt":"2026-01-13 04:42:11"},{"id":"aws-saa-design-resilient-1768279330800-4","question":"An organization stores regulated data in S3 and must enforce a data retention window that prevents deletion or overwriting for the initial 90 days. Which feature best satisfies this requirement?","answer":"[{\"id\":\"a\",\"text\":\"S3 Object Lock in Compliance mode with a 90-day retention period.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"S3 Versioning combined with a lifecycle rule to delete after 90 days.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"A bucket policy denying delete operations for all users.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"S3 Lifecycle policy to move objects to Glacier after 90 days.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA) S3 Object Lock in Compliance mode enforces immutability for a defined retention period, preventing deletions or overwrites during that window.\n\n## Why Other Options Are Wrong\n- B: Versioning helps recover earlier object versions but does not prevent deletion or overwriting during a retention window.\n- C: A bucket policy can block deletions but does not provide a time-bound immutability window and can be bypassed by policy changes.\n- D: Transitioning to Glacier is for cost, not immutability; it allows deletions under retention policies and does not enforce a fixed retention window.\n\n## Key Concepts\n- S3 Object Lock (Compliance mode)\n- Data immutability and regulatory retention\n\n## Real-World Application\nUsed in industries requiring strict data retention and tamper-evidence, such as finance and healthcare.\n","diagram":null,"difficulty":"intermediate","tags":["S3","Object-Lock","Compliance","certification-mcq","domain-weight-26"],"channel":"aws-saa","subChannel":"design-resilient","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T04:42:11.761Z","createdAt":"2026-01-13 04:42:11"},{"id":"aws-saa-design-secure-1768173626470-0","question":"You are designing a new multi-tier web application on AWS. The requirements include a public endpoint with TLS termination at the edge, private subnets for application servers and the database, restricted outbound internet access for security updates, encryption of data at rest using KMS, and centralized secrets management. Which architecture best satisfies these requirements?","answer":"[{\"id\":\"a\",\"text\":\"Application Load Balancer in public subnets terminates TLS, application servers in private subnets, database in private subnets, a NAT Gateway for outbound updates, S3 and Secrets access via VPC endpoints, and CloudTrail and Config enabled.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Application Load Balancer in public subnets terminates TLS, all resources including the database in public subnets, no VPC endpoints, no Secrets Manager, and no CloudTrail configuration.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Application Load Balancer in private subnets with TLS termination at the edge, public endpoint provided via an Internet-facing front door in a different account.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"All resources placed in private subnets with no public endpoint; access is only via a VPN and there is no encryption at rest.\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nOption a) is correct because it satisfies all requirements: TLS termination at the edge via the ALB in public subnets; app servers and database in private subnets; outbound updates are allowed through a NAT Gateway in the public segment without exposing private resources; access to S3 and Secrets Manager can be restricted via VPC endpoints; data at rest for the database is encrypted with a KMS CMK; auditing is provided by CloudTrail and Config.\n\n## Why Other Options Are Wrong\n\n- Option B is incorrect because the database and all resources are placed in public subnets and there is no encryption at rest, no Secrets Manager, and no CloudTrail, which increases risk and fails governance requirements.\n- Option C is incorrect because it relies on a private ALB with no public endpoint, which fails the requirement for public accessibility.\n- Option D is incorrect because it describes an entirely private deployment with no public endpoint and no encryption governance, failing the primary design requirement.\n\n## Key Concepts\n\n- Public endpoint with TLS termination at the edge (ALB or CloudFront)\n- Private subnets for app and data layers\n- NAT Gateway for outbound updates\n- S3/VPC Endpoints to avoid internet traversal\n- Encryption at rest with AWS KMS\n- Secrets management with Secrets Manager\n- Auditability via CloudTrail and Config\n\n## Real-World Application\n\nThis pattern reflects a typical enterprise-ready web app deployment on AWS, balancing public accessibility with private data security, governance, and operational best practices.","diagram":null,"difficulty":"intermediate","tags":["AWS","VPC","ALB","RDS","SecretsManager","KMS","CloudTrail","Config","certification-mcq","domain-weight-30"],"channel":"aws-saa","subChannel":"design-secure","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T23:20:26.471Z","createdAt":"2026-01-11 23:20:26"},{"id":"aws-saa-design-secure-1768173626470-1","question":"An EKS cluster hosts multiple microservices in a single namespace. You want to ensure each service can call only its own limited set of AWS APIs, and you want to prevent lateral movement if a pod is compromised. Which design best achieves this?","answer":"[{\"id\":\"a\",\"text\":\"Create a dedicated IAM role for each Kubernetes ServiceAccount using IRSA (IAM Roles for Service Accounts), and implement Kubernetes NetworkPolicy to restrict pod-to-pod traffic and isolate namespaces.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Attach a single IAM role to every node in the cluster and grant broad permissions to all pods.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Rely on Kubernetes RBAC to control AWS API access for pods.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use a shared IAM role for all services and map roles using an external identity provider.\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nOption a) is correct because IRSA provides per-service-account least-privilege AWS credentials and NetworkPolicy enforces segmentation to limit lateral movement.\n\n## Why Other Options Are Wrong\n\n- B: Attaching a single IAM role to all nodes gives all pods broad permissions; violates least privilege and risk.\n- C: Kubernetes RBAC does not control AWS API permissions; not sufficient.\n- D: A shared IAM role for all services lacks per-service isolation and is insecure.\n\n## Key Concepts\n\n- IAM Roles for Service Accounts (IRSA)\n- Kubernetes ServiceAccounts\n- Kubernetes NetworkPolicy\n- Least privilege\n\n## Real-World Application\n\nEnables safe multi-team microservice ecosystems by preventing broad AWS access and limiting east-west movement in the cluster.","diagram":null,"difficulty":"intermediate","tags":["AWS","EKS","IAM","IRSA","NetworkPolicy","certification-mcq","domain-weight-30"],"channel":"aws-saa","subChannel":"design-secure","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T23:20:26.942Z","createdAt":"2026-01-11 23:20:27"},{"id":"aws-saa-design-secure-1768173626470-2","question":"To prevent data exfiltration when workloads access S3, you want to ensure traffic from your VPC to S3 does not traverse the public internet. Which configuration achieves this with least risk and simplest maintenance?","answer":"[{\"id\":\"a\",\"text\":\"Create a VPC Gateway Endpoint for S3, configure the bucket policy to allow access only from that endpoint, and enable SSE-KMS for encryption.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Access S3 over the public internet but restrict outbound traffic with a firewall.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Route S3 traffic through a NAT Gateway without enabling an S3 endpoint.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Place S3 in a separate private subaccount and use CloudFront to access.\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nOption a) is correct because a VPC Gateway Endpoint ensures S3 traffic stays on the AWS network, the bucket policy restricts access to that endpoint, and SSE-KMS provides encryption at rest.\n\n## Why Other Options Are Wrong\n\n- Option B: Exposes S3 traffic to the public internet, defeating the objective of keeping data private.\n- Option C: NAT Gateway does not provide a private path to S3 and traffic can still traverse the internet; no endpoint means exposure risks.\n- Option D: CloudFront does not inherently restrict S3 access from a VPC path and adds unnecessary complexity; S3 should be accessed via a VPC endpoint for isolation.\n\n## Key Concepts\n\n- VPC Gateway Endpoints for S3\n- Bucket policies tightly scoped to VPC endpoints\n- Server-Side Encryption with KMS (SSE-KMS)\n\n## Real-World Application\n\nEnables private, compliant access to S3 from workloads running in a VPC without exposing data to the public internet.","diagram":null,"difficulty":"intermediate","tags":["AWS","S3","VPC","S3Endpoint","KMS","certification-mcq","domain-weight-30"],"channel":"aws-saa","subChannel":"design-secure","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T23:20:27.421Z","createdAt":"2026-01-11 23:20:27"},{"id":"aws-saa-design-secure-1768259819908-0","question":"In a VPC with public and private subnets, you want to ensure that instances in private subnets never reach the public Internet while still allowing access to AWS services. Which combination achieves this?","answer":"[{\"id\":\"a\",\"text\":\"Attach an Internet Gateway and route private subnets to it; rely on security groups to restrict traffic to AWS endpoints\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Create VPC Endpoints for the AWS services you need, route private subnets to those endpoints, and remove the 0.0.0.0/0 route from private subnets; do not configure NAT for private subnets\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Place all resources in a public subnet and disable NAT; rely on security groups\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use a NAT Gateway in a private subnet to allow internet access\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct option is B.\n\n## Why Other Options Are Wrong\n- A: An Internet Gateway and public egress would allow private subnets to reach the internet; VPC Endpoints are required for AWS service access without public internet.\n- C: Placing resources in a public subnet defeats private-subnet isolation and increases exposure.\n- D: NAT Gateways must be placed in public subnets; using NAT for private subnets would still create egress paths to the public internet, which contradicts the requirement.\n\n## Key Concepts\n- VPC Endpoints (PrivateLink)\n- Private subnets routing\n- NAT and Internet access controls\n\n## Real-World Application\n- This pattern is typical for compliance-heavy workloads that must access AWS services without exposing traffic to the public internet, such as data processing pipelines in regulated industries.","diagram":null,"difficulty":"intermediate","tags":["AWS","VPC","PrivateLink","NAT","Security","certification-mcq","domain-weight-30"],"channel":"aws-saa","subChannel":"design-secure","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T23:16:59.910Z","createdAt":"2026-01-12 23:17:00"},{"id":"aws-saa-design-secure-1768259819908-1","question":"Your organization hosts services behind a private REST API in API Gateway, accessible only from a specific VPC. To enforce access only from that VPC, which setup is correct?","answer":"[{\"id\":\"a\",\"text\":\"Public REST API with API Key and IP restrictions\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Private REST API with a VPC Endpoint in the consumer VPC and a resource policy restricting access to that endpoint\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Public REST API with WAF IP allowances to the VPC\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use Lambda authorizers to gate access by source VPC\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct.\n\n## Why Other Options Are Wrong\n- A: API Keys provide limited, easily bypassable protection and do not restrict by VPC boundaries.\n- C: WAF IP allowances applied to a public API do not inherently enforce access from a specific VPC and are brittle for dynamic environments.\n- D: Lambda authorizers operate at the API layer, not at the VPC boundary, and cannot reliably restrict by VPC without additional infrastructure.\n\n## Key Concepts\n- API Gateway Private REST APIs\n- VPC Endpoints (Interface endpoints)\n- Resource policies for API Gateway\n\n## Real-World Application\n- This approach enables secure service-to-service communication across accounts without exposing the API to the public internet.","diagram":null,"difficulty":"intermediate","tags":["AWS","API Gateway","VPC","PrivateLink","certification-mcq","domain-weight-30"],"channel":"aws-saa","subChannel":"design-secure","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T23:17:00.361Z","createdAt":"2026-01-12 23:17:00"},{"id":"aws-saa-design-secure-1768259819908-2","question":"You need to detect and block data exfiltration from your VPCs across multiple accounts. Which combination provides practical protection and visibility?","answer":"[{\"id\":\"a\",\"text\":\"Enable VPC Flow Logs to CloudWatch, enable GuardDuty, and rely on security groups to block exfiltration\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Enable VPC Flow Logs, enable GuardDuty, and deploy AWS Network Firewall with a central policy; Use Firewall Manager to enforce\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Only enable CloudTrail logs\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use CloudWatch Alarms on VPC metrics to notify security teams\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct.\n\n## Why Other Options Are Wrong\n- A: Security groups can mitigate some risk but are not reliable for comprehensive exfiltration detection and enforcement when used in isolation.\n- C: CloudTrail covers API calls, not network traffic in the VPC.\n- D: Alarms provide visibility but do not actively block exfiltration; they are reactive rather than preventive.\n\n## Key Concepts\n- VPC Flow Logs, GuardDuty, Network Firewall\n- Firewall Manager for centralized policy enforcement\n\n## Real-World Application\n- Large organizations use GuardDuty + network firewalls with centralized policy to detect and block unauthorized egress across many accounts.","diagram":null,"difficulty":"intermediate","tags":["AWS","GuardDuty","VPC","Network Firewall","Firewall Manager","certification-mcq","domain-weight-30"],"channel":"aws-saa","subChannel":"design-secure","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T23:17:00.773Z","createdAt":"2026-01-12 23:17:00"},{"id":"aws-saa-design-secure-1768259819908-3","question":"A microservices app running on EKS requires strong encryption in transit between services with minimal changes to application code. Which AWS feature best supports this out of the box?","answer":"[{\"id\":\"a\",\"text\":\"AWS App Mesh with mTLS enabled\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"AWS Shield with TLS enforcement\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"API Gateway with TLS termination for internal traffic\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"S3 encryption in transit only\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A is correct.\n\n## Why Other Options Are Wrong\n- B: Shield protects against DDoS, not service-to-service mTLS in a mesh.\n- C: API Gateway is not typically used for east-west traffic inside a cluster; it would require extra layers and changes.\n- D: S3 encryption in transit does not apply to service-to-service communication within the cluster.\n\n## Key Concepts\n- AWS App Mesh\n- mTLS for service mesh\n- East-west traffic security\n\n## Real-World Application\n- App Mesh standardizes mTLS across microservices with minimal changes to service code, improving security posture in multi-service deployments.","diagram":null,"difficulty":"intermediate","tags":["AWS","App Mesh","EKS","TLS","Service Mesh","certification-mcq","domain-weight-30"],"channel":"aws-saa","subChannel":"design-secure","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T23:17:00.911Z","createdAt":"2026-01-12 23:17:00"},{"id":"aws-saa-design-secure-1768259819908-4","question":"You need centralized secrets management for multiple accounts with automatic rotation and fine-grained access control. Which approach is best?","answer":"[{\"id\":\"a\",\"text\":\"AWS Secrets Manager with automatic rotation and cross-account access via resource policies, granting per-service IAM roles to retrieve the secret\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"SSM Parameter Store with no rotation and no cross-account access controls\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Store secrets as environment variables in each deployment and rotate manually\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Embed secrets in application code and version with no rotation\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A is correct.\n\n## Why Other Options Are Wrong\n- B: Parameter Store with no rotation fails rotation requirements and lacks robust cross-account access controls.\n- C: Environment variables expose secrets at runtime and are hard to rotate centrally.\n- D: Secret hard-coding in code is insecure and violates best practices for secret management.\n\n## Key Concepts\n- AWS Secrets Manager\n- Automatic rotation\n- Cross-account access via resource policies\n\n## Real-World Application\n- Centralized secret management with rotation is critical for multi-account environments and reduces blast radius during credential exposure incidents.","diagram":null,"difficulty":"intermediate","tags":["AWS","Secrets Manager","Cross-Account","IAM","certification-mcq","domain-weight-30"],"channel":"aws-saa","subChannel":"design-secure","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T23:17:01.048Z","createdAt":"2026-01-12 23:17:01"},{"id":"q-1047","question":"Design a multi-tenant analytics data lake on AWS for 3,000 tenants with strict data isolation. Propose an architecture using a shared S3 data lake with per-tenant prefixes, Lake Formation permissions, and ABAC via tenant tags. Outline governance (IAM, KMS, RAM), onboarding/offboarding automation, cost accounting, and cross-region DR. Include testing strategies to validate isolation and detect privilege escalation?","answer":"Architect a shared S3 data lake with per-tenant prefixes and Lake Formation permissions, plus ABAC via tenant tags and IAM conditions for fine-grained access. Use RAM for cross-account sharing, CMK en","explanation":"## Why This Is Asked\n\nAssesses ability to architect scalable per-tenant isolation in a data lake using Lake Formation, ABAC, and cross-region DR. Evaluates governance, automation, and cost controls.\n\n## Key Concepts\n\n- Shared data lake with per-tenant prefixes\n- Lake Formation fine-grained access controls\n- ABAC via tenant tags and IAM conditions\n- RAM cross-account sharing and CMK encryption\n- Automated onboarding/offboarding and cost accounting\n- Cross-region DR and isolation validation\n\n## Code Example\n\n```javascript\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\"s3:GetObject\"],\n      \"Resource\": \"arn:aws:s3:::mylake/tenant-*/data/*\",\n      \"Condition\": {\"StringEquals\": {\"s3:ExistingObjectTag/Tenant\": \"TenantA\"}}\n    }\n  ]\n}\n```\n\n## Follow-up Questions\n\n- How would you validate tenant isolation in non-prod environments without impacting customers?\n- What DR testing plan and failure scenarios would you run to ensure resilience?","diagram":null,"difficulty":"advanced","tags":["aws-saa"],"channel":"aws-saa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Robinhood","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T20:32:54.576Z","createdAt":"2026-01-12T20:32:54.576Z"},{"id":"q-1203","question":"A web app runs in a private subnet in a VPC with no Internet access. It must fetch a config.json from a single S3 bucket owned by the same account. Design the minimal IAM role for the EC2 instance, a bucket policy, and a VPC endpoint setup to allow access via the VPC endpoint while denying access to other buckets. What steps and policies would you implement?","answer":"Attach an IAM role to the EC2 instance with s3:GetObject on arn:aws:s3:::config-bucket/config.json; enable a S3 VPC Endpoint and route the private subnet to it; add a bucket policy Deny all s3 actions","explanation":"## Why This Is Asked\nTests practical use of least-privilege cross-service access in a private subnet, combining IAM roles, S3 bucket policies, and VPC endpoints.\n\n## Key Concepts\n- IAM roles attached to EC2 instances\n- S3 bucket policies with source VPCE conditions\n- VPC endpoints and route tables for private access\n- Deny/Allow policy composition and testing\n\n## Code Example\n```json\n// EC2 role (conceptual)\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\"Effect\": \"Allow\", \"Action\": \"s3:GetObject\", \"Resource\": \"arn:aws:s3:::config-bucket/config.json\"}\n  ]\n}\n```\n```json\n// VPCE policy (conceptual)\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\"s3:GetObject\"],\n      \"Resource\": \"arn:aws:s3:::config-bucket/config.json\",\n      \"Condition\": {\"StringEquals\": {\"aws:SourceVpce\": \"vpce-12345\"}}\n    }\n  ]\n}\n```\n```json\n// Bucket policy (conceptual)\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Deny\",\n      \"Principal\": \"*\",\n      \"Action\": \"s3:*\",\n      \"Resource\": [\"arn:aws:s3:::config-bucket\", \"arn:aws:s3:::config-bucket/*\"],\n      \"Condition\": {\"StringNotEquals\": {\"aws:SourceVpce\": \"vpce-12345\"}}\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": \"*\",\n      \"Action\": \"s3:GetObject\",\n      \"Resource\": \"arn:aws:s3:::config-bucket/config.json\",\n      \"Condition\": {\"StringEquals\": {\"aws:SourceVpce\": \"vpce-12345\"}}\n    }\n  ]\n}\n```\n\n## Follow-up Questions\n- How would you extend this to support multiple config files or buckets?\n- How would you monitor and alert on VPCE policy misconfigurations?","diagram":null,"difficulty":"beginner","tags":["aws-saa"],"channel":"aws-saa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","DoorDash","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T04:50:53.820Z","createdAt":"2026-01-13T04:50:53.820Z"},{"id":"q-672","question":"Design a scalable, cost-aware data ingestion and processing pipeline on AWS for 1 TB/day of log data arriving from multiple on-prem and cloud sources. The pipeline must deliver raw data immutably for 90 days, provide near-real-time enrichment within 5 seconds, and support cross-region failover. Specify services, data flows, constraints, and trade-offs?","answer":"Use per-region Kinesis Data Firehose to S3 with Object Lock for immutability, enable SSE-KMS, and enable lifecycle for cost. Run Glue Streaming for enrichment, and DynamoDB to store offsets/state. DLQ","explanation":"## Why This Is Asked\nAssesses design of scalable, compliant pipelines with immutability, latency, DR, and cost in AWS.\n\n## Key Concepts\n- Kinesis Firehose, S3, Object Lock, SSE-KMS\n- Glue Streaming, Lambda, DynamoDB state\n- DLQ, cross-region replication, CloudWatch\n\n## Code Example\n```javascript\n// Pseudo-idempotent Lambda sketch\nconst AWS = require('aws-sdk');\nconst ddb = new AWS.DynamoDB.DocumentClient();\nexports.handler = async (evt) => {\n  const key = { id: evt.id };\n  const exists = await ddb.get({TableName:'Processed', Key:key}).promise();\n  if (exists.Item) return;\n  await ddb.put({TableName:'Processed', Item:{id: evt.id, t: Date.now()}}).promise();\n  // enrich data\n};\n```\n\n## Follow-up Questions\n- How would you validate data quality and schema evolution across regions?\n- How do you handle late-arriving data and reprocessing without duplicating work?","diagram":"flowchart TD\n  A[Ingest] --> B[Kinesis Firehose]\n  B --> C[S3 regional bucket]\n  C --> D[Object Lock: immutability]\n  D --> E[Glue Streaming / Lambda]\n  E --> F[DynamoDB state]\n  F --> G[CloudWatch alarms]","difficulty":"advanced","tags":["aws-saa"],"channel":"aws-saa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","IBM"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T14:44:21.525Z","createdAt":"2026-01-11T14:44:21.525Z"},{"id":"q-930","question":"An existing web service runs on EC2 in a single VPC with an ALB. Traffic surges cause latency spikes and occasional outages during AZ failures. Propose a beginner-friendly, cost-conscious HA setup using an Application Load Balancer, Auto Scaling across at least two AZs, and a relational database option. Include networking, health checks, a scaling policy, and a basic DB deployment choice with trade-offs. What would you implement first and why?","answer":"Create an ALB in front of an ASG spread across two AZs with a CPU-based target tracking policy (min 2, max 6). Use a Multi-AZ RDS or Aurora for the database. Enable CloudWatch alarms for latency and e","explanation":"## Why This Is Asked\n\nAssesses practical HA design, cost awareness, and fundamental AWS components for a resilient web service.\n\n## Key Concepts\n\n- Application Load Balancer\n- Auto Scaling Group across AZs\n- Multi-AZ RDS or Aurora\n- CloudWatch alarms\n- S3 + CloudFront for static assets\n- Cost trade-offs between on-demand instances vs reserved capacity\n\n## Code Example\n\n```yaml\n# Minimal CloudFormation sketch (illustrative only)\nResources:\n  MyALB:\n    Type: AWS::ElasticLoadBalancingV2::LoadBalancer\n    Properties:\n      Name: my-alb\n      Scheme: internet-facing\n  MyTargetGroup:\n    Type: AWS::ElasticLoadBalancingV2::TargetGroup\n    Properties:\n      VpcId: vpc-xxxx\n      Protocol: HTTP\n      Port: 80\n  MyASG:\n    Type: AWS::AutoScaling::AutoScalingGroup\n    Properties:\n      VPCZoneIdentifier:\n        - subnet-aaa\n        - subnet-bbb\n      TargetGroupARNs:\n        - !Ref MyTargetGroup\n      LaunchConfigurationName: !Ref MyLaunchConfig\n```\n\n## Follow-up Questions\n\n- Compare Aurora Provisioned vs Aurora Serverless for this pattern.\n- How would you monitor latency, and what would trigger scale-out vs scale-in?","diagram":null,"difficulty":"beginner","tags":["aws-saa"],"channel":"aws-saa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T15:40:08.464Z","createdAt":"2026-01-12T15:40:08.464Z"},{"id":"q-944","question":"Design a multi-tenant SaaS on AWS that ingests telemetry from thousands of tenants daily, enforces strict data isolation, and provides cross-region DR. Outline data partitioning, access control, encryption strategy, immutable logging, and DR failover. Include services, trade-offs, and how you test isolation?","answer":"Partition data per-tenant using S3 prefixes and DynamoDB partitions; enforce per-tenant IAM with IRSA and tight policies. Use a dedicated KMS CMK per tenant for envelope encryption, and store immutabl","explanation":"## Why This Is Asked\nProbes ability to design a scalable, secure, DR-ready multi-tenant architecture on AWS with strict data isolation and auditable controls.\n\n## Key Concepts\n- Tenant isolation across data storage\n- Data partitioning strategies (prefixes, shards)\n- IAM/IRSA with least privilege\n- Envelope encryption with per-tenant KMS keys\n- Immutable logging (S3 Object Lock, CloudTrail)\n- Cross-region DR and failover strategies\n\n## Code Example\n```javascript\n// Example policy shape (illustrative)\nconst policy = {\n  Version: '2012-10-17',\n  Statement: [\n    { Effect: 'Allow', Action: ['s3:GetObject'], Resource: 'arn:aws:s3:::tenant-telemetry/*' }\n  ]\n}\n```\n\n## Follow-up Questions\n- How would you test tenant data isolation in production?\n- How would you handle tenant onboarding/offboarding and key rotation?","diagram":"flowchart TD\n  T[Tenant Data Partition] --> I[Identity & Access Control]\n  E[Encryption] --> D[Data Stores]\n  A[Audit Logs] --> C[CloudTrail]\n  DR[DR & Replication] --> R[Route 53 Failover]","difficulty":"advanced","tags":["aws-saa"],"channel":"aws-saa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Meta","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T16:33:20.453Z","createdAt":"2026-01-12T16:33:20.453Z"},{"id":"q-989","question":"Design a centralized security telemetry pipeline for 1,000 AWS accounts to detect anomalies in near real-time. Ingest VPC Flow Logs, CloudTrail, and GuardDuty findings into a central security account using AWS Organizations, implement least-privilege cross-account roles, normalize data into a common schema in S3, partition by source account and region, apply Lake Formation permissions, and set up alerting with EventBridge and security findings. Include scaling, cost, DR, and testing?","answer":"Use AWS Organizations for centralization, create per-account roles trusted by a dedicated security account, ingest logs via Kinesis Data Firehose into S3, normalize to a common JSON schema with a Lamb","explanation":"## Why This Is Asked\n\nAssesses multi-account security telemetry design, cross-account access, and practical data governance in AWS.\n\n## Key Concepts\n\n- AWS Organizations and SCPs for centralized control\n- Cross-account IAM roles with least privilege\n- Real-time ingestion (Kinesis Firehose) and normalization (Lambda)\n- S3 data lake with partitioning by account/region/date\n- Lake Formation permissions and data catalog hygiene\n- EventBridge integration with security findings\n- Cross-region DR and cost-aware scaling\n\n## Code Example\n\n```javascript\n// Example IAM trust policy for cross-account role\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\"AWS\": \"arn:aws:iam::SECURITY_ACC_ID:root\"},\n      \"Action\": \"sts:AssumeRole\"\n    }\n  ]\n}\n```\n\n## Follow-up Questions\n\n- How would you enforce data retention and deletion across accounts?\n- What monitoring would you implement to detect drift in cross-account permissions?","diagram":null,"difficulty":"intermediate","tags":["aws-saa"],"channel":"aws-saa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T18:33:59.850Z","createdAt":"2026-01-12T18:33:59.850Z"}],"subChannels":["design-cost","design-performant","design-resilient","design-secure","general"],"companies":["Amazon","Anthropic","Cloudflare","DoorDash","Google","Hashicorp","IBM","Meta","MongoDB","Plaid","Robinhood","Snap","Snowflake"],"stats":{"total":37,"beginner":2,"intermediate":32,"advanced":3,"newThisWeek":37}}