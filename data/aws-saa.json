{"questions":[{"id":"aws-saa-design-resilient-1768213442314-0","question":"You are designing a serverless web application that must remain available during a regional outage in us-east-1. The app uses DynamoDB to store user profiles and S3 to store assets. To minimize RPO and enable automatic failover between us-east-1 and us-west-2, which design pattern should you implement?","answer":"[{\"id\":\"a\",\"text\":\"DynamoDB Global Tables replicated across us-east-1 and us-west-2, S3 cross-region replication for assets, and Route 53 failover with health checks to route traffic to the healthy region automatically.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Maintain a single-region DynamoDB instance in us-east-1 with manual backups to us-west-2, then switch DNS to the backup region after an outage.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use an RDS Multi-AZ deployment and rely on the primary region failing over automatically across regions.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Deploy two completely independent stacks in us-east-1 and us-west-2 with separate DynamoDB tables and implement cross-region data sync via Lambda.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption a is correct: it provides true multi-region active data and automated routing for DR. DynamoDB Global Tables replicates data across regions to minimize RPO, S3 cross-region replication ensures asset availability, and Route 53 health checks enable automatic failover.\n\n## Why Other Options Are Wrong\n- Option B: relies on manual failover and single-region state; results in higher RPO and downtime.\n- Option C: RDS Multi-AZ is intra-region; does not protect against regional outages.\n- Option D: while feasible, it adds operational complexity and potential data divergence without an automated global failover strategy.\n\n## Key Concepts\n- DynamoDB Global Tables\n- S3 Cross-Region Replication\n- Route 53 Health Checks and Failover\n- Multi-Region Disaster Recovery\n\n## Real-World Application\nImplement a global architecture with tables replicated across regions, replicate assets, and configure DNS failover so traffic redirects automatically to the healthy region during outages.","diagram":null,"difficulty":"intermediate","tags":["AWS","DynamoDB","S3","Route53","certification-mcq","domain-weight-26"],"channel":"aws-saa","subChannel":"design-resilient","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T10:24:02.315Z","createdAt":"2026-01-12 10:24:02"},{"id":"aws-saa-design-resilient-1768213442314-1","question":"An organization runs a critical application in us-east-1 using EC2 Auto Scaling behind an Application Load Balancer. The service depends on an RDS primary instance in us-east-1. The organization wants to meet an RTO of 15 minutes and an RPO of less than 5 minutes in the event of a regional outage. Which DR pattern best achieves this?","answer":"[{\"id\":\"a\",\"text\":\"Create an RDS cross-region read replica in us-west-2 and configure Route 53 health checks with a failover to the us-west-2 endpoint; use a Lambda function to automatically promote the read replica if the primary region becomes unhealthy.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Enable RDS Multi-AZ in us-east-1 and rely on automatic failover to the standby within the same region.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Implement DynamoDB global tables across us-east-1 and us-west-2 for all data and route traffic accordingly.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use S3 cross-region replication to replicate configuration data and perform manual DNS failover.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption a is correct: RDS cross-region read replicas provide DR across regions with low RPO; automating promotion via a Lambda function helps achieve the target 15-minute RTO. B only covers intra-region failover and cannot survive regional outages. C introduces a data model mismatch for relational data. D relies on manual steps, increasing the outage duration.\n\n## Why Other Options Are Wrong\n- Option B: Intra-region failover does not protect against regional outages.\n- Option C: DynamoDB is not the primary data store for a relational RDS-based workload.\n- Option D: Manual DNS failover slows recovery and increases RTO.\n\n## Key Concepts\n- RDS cross-region read replicas\n- Automated failover using Lambda\n- Route 53 health checks\n- Disaster recovery objectives (RTO/RPO)\n\n## Real-World Application\nConfigure a cross-region read replica in the secondary region, monitor the primary region health, trigger promotion of the replica when unhealthy, and route traffic to the secondary region automatically.","diagram":null,"difficulty":"intermediate","tags":["AWS","RDS","Route53","DR","certification-mcq","domain-weight-26"],"channel":"aws-saa","subChannel":"design-resilient","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T10:24:02.848Z","createdAt":"2026-01-12 10:24:03"},{"id":"aws-saa-design-resilient-1768213442314-2","question":"To store 7 years of critical audit logs with minimal ongoing costs while ensuring immutability during the retention window, which storage approach is best?","answer":"[{\"id\":\"a\",\"text\":\"Store logs in S3 Standard and set a lifecycle rule to transition to Glacier Deep Archive after 30 days, and enable S3 Object Lock in compliance mode to ensure immutability for the retention period.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Store logs in S3 Standard-Infrequent Access with indefinite retention.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Store logs on EBS volumes attached to EC2 instances and snapshot them to S3.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use S3 Intelligent-Tiering with no lifecycle rules.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption a is correct: transitioning to Glacier Deep Archive reduces ongoing costs for long-term retention, while S3 Object Lock in compliance mode provides immutability to meet audit requirements.\n\n## Why Other Options Are Wrong\n- Option B: Standard-IA reduces costs but lacks guaranteed immutability and may still be more expensive than Glacier Deep Archive for long-term retention.\n- Option C: EBS snapshots are not ideal for long-term, cost-effective durable storage of large audit logs.\n- Option D: Without lifecycle rules, data may remain in a higher-cost tier longer than necessary, and Object Lock is not enforced without S3 Lock configurations.\n\n## Key Concepts\n- S3 Lifecycle transitions\n- Glacier Deep Archive storage class\n- S3 Object Lock (compliance mode)\n\n## Real-World Application\nConfigure a lifecycle policy that moves logs from S3 Standard to Glacier Deep Archive after 30 days and enable Object Lock in compliance mode for the retention window to ensure immutability.","diagram":null,"difficulty":"intermediate","tags":["AWS","S3","GlacierDeepArchive","ObjectLock","certification-mcq","domain-weight-26"],"channel":"aws-saa","subChannel":"design-resilient","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T10:24:03.366Z","createdAt":"2026-01-12 10:24:03"},{"id":"aws-saa-design-secure-1768173626470-0","question":"You are designing a new multi-tier web application on AWS. The requirements include a public endpoint with TLS termination at the edge, private subnets for application servers and the database, restricted outbound internet access for security updates, encryption of data at rest using KMS, and centralized secrets management. Which architecture best satisfies these requirements?","answer":"[{\"id\":\"a\",\"text\":\"Application Load Balancer in public subnets terminates TLS, application servers in private subnets, database in private subnets, a NAT Gateway for outbound updates, S3 and Secrets access via VPC endpoints, and CloudTrail and Config enabled.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Application Load Balancer in public subnets terminates TLS, all resources including the database in public subnets, no VPC endpoints, no Secrets Manager, and no CloudTrail configuration.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Application Load Balancer in private subnets with TLS termination at the edge, public endpoint provided via an Internet-facing front door in a different account.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"All resources placed in private subnets with no public endpoint; access is only via a VPN and there is no encryption at rest.\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nOption a) is correct because it satisfies all requirements: TLS termination at the edge via the ALB in public subnets; app servers and database in private subnets; outbound updates are allowed through a NAT Gateway in the public segment without exposing private resources; access to S3 and Secrets Manager can be restricted via VPC endpoints; data at rest for the database is encrypted with a KMS CMK; auditing is provided by CloudTrail and Config.\n\n## Why Other Options Are Wrong\n\n- Option B is incorrect because the database and all resources are placed in public subnets and there is no encryption at rest, no Secrets Manager, and no CloudTrail, which increases risk and fails governance requirements.\n- Option C is incorrect because it relies on a private ALB with no public endpoint, which fails the requirement for public accessibility.\n- Option D is incorrect because it describes an entirely private deployment with no public endpoint and no encryption governance, failing the primary design requirement.\n\n## Key Concepts\n\n- Public endpoint with TLS termination at the edge (ALB or CloudFront)\n- Private subnets for app and data layers\n- NAT Gateway for outbound updates\n- S3/VPC Endpoints to avoid internet traversal\n- Encryption at rest with AWS KMS\n- Secrets management with Secrets Manager\n- Auditability via CloudTrail and Config\n\n## Real-World Application\n\nThis pattern reflects a typical enterprise-ready web app deployment on AWS, balancing public accessibility with private data security, governance, and operational best practices.","diagram":null,"difficulty":"intermediate","tags":["AWS","VPC","ALB","RDS","SecretsManager","KMS","CloudTrail","Config","certification-mcq","domain-weight-30"],"channel":"aws-saa","subChannel":"design-secure","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T23:20:26.471Z","createdAt":"2026-01-11 23:20:26"},{"id":"aws-saa-design-secure-1768173626470-1","question":"An EKS cluster hosts multiple microservices in a single namespace. You want to ensure each service can call only its own limited set of AWS APIs, and you want to prevent lateral movement if a pod is compromised. Which design best achieves this?","answer":"[{\"id\":\"a\",\"text\":\"Create a dedicated IAM role for each Kubernetes ServiceAccount using IRSA (IAM Roles for Service Accounts), and implement Kubernetes NetworkPolicy to restrict pod-to-pod traffic and isolate namespaces.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Attach a single IAM role to every node in the cluster and grant broad permissions to all pods.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Rely on Kubernetes RBAC to control AWS API access for pods.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use a shared IAM role for all services and map roles using an external identity provider.\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nOption a) is correct because IRSA provides per-service-account least-privilege AWS credentials and NetworkPolicy enforces segmentation to limit lateral movement.\n\n## Why Other Options Are Wrong\n\n- B: Attaching a single IAM role to all nodes gives all pods broad permissions; violates least privilege and risk.\n- C: Kubernetes RBAC does not control AWS API permissions; not sufficient.\n- D: A shared IAM role for all services lacks per-service isolation and is insecure.\n\n## Key Concepts\n\n- IAM Roles for Service Accounts (IRSA)\n- Kubernetes ServiceAccounts\n- Kubernetes NetworkPolicy\n- Least privilege\n\n## Real-World Application\n\nEnables safe multi-team microservice ecosystems by preventing broad AWS access and limiting east-west movement in the cluster.","diagram":null,"difficulty":"intermediate","tags":["AWS","EKS","IAM","IRSA","NetworkPolicy","certification-mcq","domain-weight-30"],"channel":"aws-saa","subChannel":"design-secure","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T23:20:26.942Z","createdAt":"2026-01-11 23:20:27"},{"id":"aws-saa-design-secure-1768173626470-2","question":"To prevent data exfiltration when workloads access S3, you want to ensure traffic from your VPC to S3 does not traverse the public internet. Which configuration achieves this with least risk and simplest maintenance?","answer":"[{\"id\":\"a\",\"text\":\"Create a VPC Gateway Endpoint for S3, configure the bucket policy to allow access only from that endpoint, and enable SSE-KMS for encryption.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Access S3 over the public internet but restrict outbound traffic with a firewall.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Route S3 traffic through a NAT Gateway without enabling an S3 endpoint.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Place S3 in a separate private subaccount and use CloudFront to access.\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nOption a) is correct because a VPC Gateway Endpoint ensures S3 traffic stays on the AWS network, the bucket policy restricts access to that endpoint, and SSE-KMS provides encryption at rest.\n\n## Why Other Options Are Wrong\n\n- Option B: Exposes S3 traffic to the public internet, defeating the objective of keeping data private.\n- Option C: NAT Gateway does not provide a private path to S3 and traffic can still traverse the internet; no endpoint means exposure risks.\n- Option D: CloudFront does not inherently restrict S3 access from a VPC path and adds unnecessary complexity; S3 should be accessed via a VPC endpoint for isolation.\n\n## Key Concepts\n\n- VPC Gateway Endpoints for S3\n- Bucket policies tightly scoped to VPC endpoints\n- Server-Side Encryption with KMS (SSE-KMS)\n\n## Real-World Application\n\nEnables private, compliant access to S3 from workloads running in a VPC without exposing data to the public internet.","diagram":null,"difficulty":"intermediate","tags":["AWS","S3","VPC","S3Endpoint","KMS","certification-mcq","domain-weight-30"],"channel":"aws-saa","subChannel":"design-secure","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T23:20:27.421Z","createdAt":"2026-01-11 23:20:27"},{"id":"q-672","question":"Design a scalable, cost-aware data ingestion and processing pipeline on AWS for 1 TB/day of log data arriving from multiple on-prem and cloud sources. The pipeline must deliver raw data immutably for 90 days, provide near-real-time enrichment within 5 seconds, and support cross-region failover. Specify services, data flows, constraints, and trade-offs?","answer":"Use per-region Kinesis Data Firehose to S3 with Object Lock for immutability, enable SSE-KMS, and enable lifecycle for cost. Run Glue Streaming for enrichment, and DynamoDB to store offsets/state. DLQ","explanation":"## Why This Is Asked\nAssesses design of scalable, compliant pipelines with immutability, latency, DR, and cost in AWS.\n\n## Key Concepts\n- Kinesis Firehose, S3, Object Lock, SSE-KMS\n- Glue Streaming, Lambda, DynamoDB state\n- DLQ, cross-region replication, CloudWatch\n\n## Code Example\n```javascript\n// Pseudo-idempotent Lambda sketch\nconst AWS = require('aws-sdk');\nconst ddb = new AWS.DynamoDB.DocumentClient();\nexports.handler = async (evt) => {\n  const key = { id: evt.id };\n  const exists = await ddb.get({TableName:'Processed', Key:key}).promise();\n  if (exists.Item) return;\n  await ddb.put({TableName:'Processed', Item:{id: evt.id, t: Date.now()}}).promise();\n  // enrich data\n};\n```\n\n## Follow-up Questions\n- How would you validate data quality and schema evolution across regions?\n- How do you handle late-arriving data and reprocessing without duplicating work?","diagram":"flowchart TD\n  A[Ingest] --> B[Kinesis Firehose]\n  B --> C[S3 regional bucket]\n  C --> D[Object Lock: immutability]\n  D --> E[Glue Streaming / Lambda]\n  E --> F[DynamoDB state]\n  F --> G[CloudWatch alarms]","difficulty":"advanced","tags":["aws-saa"],"channel":"aws-saa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","IBM"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T14:44:21.525Z","createdAt":"2026-01-11T14:44:21.525Z"}],"subChannels":["design-resilient","design-secure","general"],"companies":["Hashicorp","IBM"],"stats":{"total":7,"beginner":0,"intermediate":6,"advanced":1,"newThisWeek":7}}