{"questions":[{"id":"azure-developer-connect-consume-1768256547964-0","question":"You have an Azure Function that must read a database connection string stored securely in Azure Key Vault. You want to avoid embedding secrets in code or configuration files. Which approach is best?","answer":"[{\"id\":\"a\",\"text\":\"Store the connection string in application settings in plaintext\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use a system-assigned managed identity to access Key Vault and retrieve the secret at runtime\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Store the connection string in a blob and read it from code\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Pass the connection string as an environment variable during deployment\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nThe **correct** answer is that you should use a system-assigned managed identity to access Key Vault and retrieve the secret at runtime. This avoids embedding secrets in code or config and leverages Azure RBAC to grant access only to the required secret.\n\n## Why Other Options Are Wrong\n\n- Option A: plaintext in application settings exposes secret and violates least privilege.\n- Option C: storing the secret in a blob with code access poses exposure risk and requires extra steps.\n- Option D: environment variables still expose secrets in deployment artifacts and memory; not ideal for secret rotation.\n\n## Key Concepts\n\n- Managed identities eliminate hard-coded credentials.\n- Azure Key Vault stores and controls access to secrets.\n- Secret retrieval via the Key Vault SDK enforces access policies.\n\n## Real-World Application\n\nThis pattern is common when deploying serverless functions that need to read connection strings or API keys without embedding them in the codebase, enabling centralized secret rotation and auditing.","diagram":null,"difficulty":"intermediate","tags":["Azure","AzureKeyVault","ManagedIdentity","AKS","S3","Terraform","certification-mcq","domain-weight-15"],"channel":"azure-developer","subChannel":"connect-consume","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T22:22:27.965Z","createdAt":"2026-01-12 22:22:28"},{"id":"azure-developer-connect-consume-1768256547964-1","question":"An AKS workload needs to access an Azure SQL Database and call a third-party REST API. You want to avoid embedding credentials in images or code and minimize credential management overhead. Which approach best achieves this?","answer":"[{\"id\":\"a\",\"text\":\"Use a Kubernetes Secret with credentials mounted as environment variables\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use Azure AD Workload Identity to attach a Kubernetes service account to an Azure AD pod identity, enabling Azure SQL authentication and OAuth 2.0 tokens for the third-party API\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Store credentials in a ConfigMap and mount it as a file in the container\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Embed a static API key in the container image and rotate manually\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nThe **correct** answer is to use Azure AD Workload Identity to attach a Kubernetes service account to an Azure AD pod identity, allowing the pod to obtain Azure SQL authentication tokens and OAuth 2.0 tokens for the third-party API without hard-coding credentials.\n\n## Why Other Options Are Wrong\n\n- Option A: Secrets in Kubernetes Secrets can be base64-encoded and may be exposed; not ideal without additional safeguards.\n- Option C: ConfigMaps are not meant for secret data and can be exposed to pods that mount them.\n- Option D: Hard-coding or embedding static keys leads to secret sprawl and rotation challenges.\n\n## Key Concepts\n\n- Workload Identity for AKS enables pod-level access to Azure AD-secured resources without secret files.\n- RBAC and least privilege govern access to both Azure SQL and external APIs.\n\n## Real-World Application\n\nThis pattern is common for microservices deployed to AKS that must consume both Azure resources and external SaaS APIs securely at runtime.","diagram":null,"difficulty":"intermediate","tags":["AKS","AzureAD","WorkloadIdentity","AzureSQL","Terraform","AWS","certification-mcq","domain-weight-15"],"channel":"azure-developer","subChannel":"connect-consume","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T22:22:28.601Z","createdAt":"2026-01-12 22:22:29"},{"id":"azure-developer-connect-consume-1768256547964-2","question":"An application running in Azure App Service must read from a Storage Blob container without exposing storage keys. Which approach is recommended?","answer":"[{\"id\":\"a\",\"text\":\"Store the storage account key in App Settings in plaintext\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use a user-assigned managed identity with Storage Blob Data Contributor role and access via DefaultAzureCredential\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Generate a SAS token and embed it in code\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Enable public access to the container and read via URL\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nThe **correct** answer is to use a user-assigned managed identity with the Storage Blob Data Contributor role and access via DefaultAzureCredential. This avoids secret keys and supports automatic rotation and principle-based access.\n\n## Why Other Options Are Wrong\n\n- Option A: storing keys in App Settings is insecure and exposes keys.\n- Option C: SAS tokens embedded in code can be leaked and may not rotate automatically.\n- Option D: public container access removes access control and is not secure.\n\n## Key Concepts\n\n- Managed identities provide credential-free access to Azure resources.\n- RBAC grants fine-grained access to storage data planes.\n\n## Real-World Application\n\nThis pattern is standard for apps needing to read or write blobs securely without embedding keys in code or configuration.","diagram":null,"difficulty":"intermediate","tags":["Azure","AzureStorage","ManagedIdentity","AKS","Terraform","certification-mcq","domain-weight-15"],"channel":"azure-developer","subChannel":"connect-consume","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T22:22:29.150Z","createdAt":"2026-01-12 22:22:29"},{"id":"azure-developer-connect-consume-1768256547964-3","question":"You need to authenticate to a third-party API that uses OAuth 2.0 client credentials. You want to rotate credentials and avoid hard-coding secrets. Which approach should you implement?","answer":"[{\"id\":\"a\",\"text\":\"Store the client secret in code and implement token refresh logic\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Store the client secret in Azure Key Vault and access it using a managed identity, then obtain access tokens via OAuth 2.0 client credentials flow\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Use a static API key that never changes\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Publish the API key to a public endpoint for dynamic retrieval\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nThe **correct** answer is to store the client secret in Azure Key Vault and access it using a managed identity, then obtain access tokens via the OAuth 2.0 client credentials flow. This enables secret rotation without embedding credentials in code.\n\n## Why Other Options Are Wrong\n\n- Option A: secret in code is vulnerable to exposure and complicates rotation.\n- Option C: static API keys do not support rotation or least privilege.\n- Option D: publicly exposing API keys introduces significant risk.\n\n## Key Concepts\n\n- Secret management via Key Vault supports rotation and access auditing.\n- Managed identities enable secure secret access without embedding credentials.\n\n## Real-World Application\n\nThis pattern is common when a service principal or app must securely obtain tokens for a partner API without embedding credentials in the application.","diagram":null,"difficulty":"intermediate","tags":["Azure","AzureKeyVault","MSAL","OAuth","AWS","certification-mcq","domain-weight-15"],"channel":"azure-developer","subChannel":"connect-consume","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T22:22:29.317Z","createdAt":"2026-01-12 22:22:29"},{"id":"azure-developer-connect-consume-1768256547964-4","question":"A backend service must securely access multiple resources: Azure SQL Database and a SaaS CRM API. You want to avoid embedding credentials in code and centralize secret management. Which design best achieves this?","answer":"[{\"id\":\"a\",\"text\":\"Use a single set of credentials stored in application configuration for both resources\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use a dedicated user-assigned managed identity for the service to access Azure SQL with RBAC and fetch CRM API credentials from Azure Key Vault, rotating secrets as needed\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Store separate API keys for both resources directly in code\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use environment variables to hold secrets for both resources\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nThe **correct** answer is to use a dedicated user-assigned managed identity for the service to access Azure SQL with RBAC and to fetch CRM API credentials from Azure Key Vault, rotating secrets as needed. This provides least privilege access and centralized secret management.\n\n## Why Other Options Are Wrong\n\n- Option A: a single credential set for disparate resources increases blast radius and complicates rotation.\n- Option C: embedding keys in code creates secret exposure risks.\n- Option D: environment variables still expose secrets in runtime and deployment artifacts.\n\n## Key Concepts\n\n- Separate identities per resource support scope-limited access.\n- Centralized secret management via Key Vault simplifies rotation and auditing.\n\n## Real-World Application\n\nThis pattern is common in services that must talk to both Azure and external SaaS APIs while maintaining strong security posture and auditability.","diagram":null,"difficulty":"intermediate","tags":["Azure","AzureSQL","AzureKeyVault","WorkloadIdentity","Terraform","AWS","certification-mcq","domain-weight-15"],"channel":"azure-developer","subChannel":"connect-consume","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T22:22:29.482Z","createdAt":"2026-01-12 22:22:29"},{"id":"azure-developer-develop-compute-1768162939034-0","question":"You are designing a high-throughput event-driven API on Azure Functions that must connect to a third-party service requiring a fixed outbound IP. Which compute option best meets latency, scale, and IP allow-list requirements?","answer":"[{\"id\":\"a\",\"text\":\"Azure Functions Consumption plan with dynamic outbound IP\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Azure Functions Premium plan with pre-warmed instances and VNET integration\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Azure App Service on Standard plan\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Azure Container Instances with a single container\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B. Azure Functions Premium plan with pre-warmed instances and VNET integration reduces cold starts and provides secure, controllable egress when connecting to external services, which helps meet latency and network-security requirements. For strict fixed outbound IPs, additional egress controls (such as a NAT or dedicated egress) may be used, but Premium still offers the best balance of latency, scale, and network control for this scenario.\n\n## Why Other Options Are Wrong\n- Option A: Consumption plan experiences cold starts under load and outbound IPs can change, which worsens latency and IP allow-list reliability.\n- Option C: App Service on Standard does not guarantee pre-warmed instances and has less flexible networking options for controlled egress.\n- Option D: ACI with a single container does not provide the auto-scaling, pacing, or integrated networking features needed for high-throughput, multi-tenant workloads.\n\n## Key Concepts\n- Serverless compute options: Consumption vs Premium plans\n- VNET integration for Azure Functions\n- Egress control and outbound IP considerations\n- When to apply NAT or dedicated egress patterns\n\n## Real-World Application\nUsed when building a serverless integration that must call partner APIs with IP allow-list constraints and predictable latency, enabling reliable scaling without cold starts.","diagram":null,"difficulty":"intermediate","tags":["Azure","Kubernetes","AWS","Terraform","AKS","certification-mcq","domain-weight-25"],"channel":"azure-developer","subChannel":"develop-compute","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T20:22:19.035Z","createdAt":"2026-01-11 20:22:19"},{"id":"azure-developer-develop-compute-1768162939034-1","question":"During a migration of a microservices app to AKS, your team requires zero-downtime deployments, per-service autoscaling, and straightforward routing. Which approach best satisfies these needs in AKS?","answer":"[{\"id\":\"a\",\"text\":\"Deploy each service as a Kubernetes Deployment using RollingUpdate with readiness probes and per-service HorizontalPodAutoscaler\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Use VM Scale Sets behind a load balancer\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Move all services to Azure Functions for event-driven processing\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use App Service Environment with slot swapping\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A. Deploying each microservice as a Kubernetes Deployment with a RollingUpdate strategy, plus readiness probes and per-service HorizontalPodAutoscalers (HPA), enables zero-downtime deployments and automatic scaling. While canary/blue-green patterns can be added with service mesh tooling, the core AKS pattern for safe updates and scalable services is rolling updates combined with readiness probes and per-service HPAs.\n\n## Why Other Options Are Wrong\n- Option B: VM Scale Sets can offer scale but do not provide native per-service rolling updates with Kubernetes-level health checks and routing complexity.\n- Option C: Azure Functions is not a direct substitute for a distributed microservices architecture on AKS and does not address zero-downtime rolling updates across multiple services.\n- Option D: Slot swapping is an App Service concept and does not apply to AKS traffic routing and rolling deployments.\n\n## Key Concepts\n- Kubernetes Deployment rolling updates\n- Readiness probes and liveness checks\n- HorizontalPodAutoscaler per service\n- AKS deployment strategies for zero-downtime upgrades\n\n## Real-World Application\nApplies when migrating microservices to AKS and you need smooth, automatic updates across many services without traffic disruption, while preserving scalable capacity per service.","diagram":null,"difficulty":"intermediate","tags":["Azure","Kubernetes","AWS","Terraform","AKS","certification-mcq","domain-weight-25"],"channel":"azure-developer","subChannel":"develop-compute","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T20:22:19.617Z","createdAt":"2026-01-11 20:22:20"},{"id":"azure-developer-develop-compute-1768162939034-2","question":"In an event-driven Azure architecture, you want to guarantee exactly-once processing of messages from a queue into a downstream store. Which pattern best achieves this?","answer":"[{\"id\":\"a\",\"text\":\"Azure Service Bus with sessions and content-based deduplication and an idempotent consumer\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Azure Storage Queues with DeleteMessage after processing\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Azure Event Grid with automatic retries until success\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Cosmos DB change feed with eventual consistency handling\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A. Azure Service Bus supports per-message deduplication and sessions for ordered processing, and when combined with an idempotent consumer (the downstream store logic is designed to handle duplicates gracefully), you can approach exactly-once semantics. Other options either rely on at-least-once processing guarantees (Storage Queues, Event Grid retries) or require complex idempotency handling beyond what change feeds provide by default.\n\n## Why Other Options Are Wrong\n- Option B: Storage Queues can deliver messages at-least-once; duplicates are possible without additional dedup logic.\n- Option C: Event Grid retries do not guarantee exactly-once delivery semantics for each event across all handlers.\n- Option D: Cosmos DB change feed provides strong ordering and consistency models for data changes but does not inherently enforce exactly-once processing for external downstream writes.\n\n## Key Concepts\n- Exactly-once processing patterns\n- Service Bus deduplication and sessions\n- Idempotent consumer design\n- Event-driven architecture trade-offs\n\n## Real-World Application\nUseful when integrating microservices that ingest events from a message broker and must avoid duplicate writes to a downstream data store, such as financial or inventory systems where duplicates have real consequences.","diagram":null,"difficulty":"intermediate","tags":["Azure","Kubernetes","Terraform","AWS","Service Bus","certification-mcq","domain-weight-25"],"channel":"azure-developer","subChannel":"develop-compute","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T20:22:20.149Z","createdAt":"2026-01-11 20:22:20"},{"id":"azure-developer-develop-storage-1768225332421-0","question":"You have a web app that lets users upload profile pictures to a private blob container. You cannot expose account keys to the client. Which approach ensures least privilege, revocability, and no key exposure?","answer":"[{\"id\":\"a\",\"text\":\"Generate a user delegation SAS (via Azure AD) with write permission and short expiry, served to client from backend\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Make the container publicly accessible and have the client upload\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Embed the account key in the client to authorize the upload\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use a CORS policy alone to enable uploads\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nOption a is correct. A user delegation SAS (or a per-user SAS backed by Azure AD) provides scoped access to blob resources without sharing account keys and can be revoked by expiring the token or via policy updates.\n\n## Why Other Options Are Wrong\n\n- Option B: Public container access would allow anyone to upload or enumerate blobs, defeating security and revocation controls.\n- Option C: Embedding the account key in client code is effectively sharing the key and is insecure.\n- Option D: CORS controls cross-origin requests, not authentication or granular authorization for blob operations.\n\n## Key Concepts\n\n- Shared Access Signatures (SAS)\n- User delegation SAS and Azure AD integration\n- Principle of least privilege\n- Token expiry and revocation\n\n## Real-World Application\n\nImplement a backend endpoint that issues short-lived SAS tokens per user or per operation, enforce permission scope (write/list), and rotate tokens frequently. Integrate this with the frontend to fetch tokens before upload and validate server-side usage logs.\n","diagram":null,"difficulty":"intermediate","tags":["AzureBlobStorage","SAS","RBAC","AKS","Terraform","AWS-S3","certification-mcq","domain-weight-15"],"channel":"azure-developer","subChannel":"develop-storage","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:42:12.422Z","createdAt":"2026-01-12 13:42:13"},{"id":"azure-developer-develop-storage-1768225332421-1","question":"Logs stored in a container must be archived after 180 days and purged after seven years; which feature and example rules would implement this?","answer":"[{\"id\":\"a\",\"text\":\"Storage lifecycle management: MoveToArchive after 180 days; Delete after 7 years\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Set container default tier to Archive\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Enable hot tier only\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Write a custom script to move older blobs manually\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nOption a is correct. Lifecycle management policies can automatically transition blobs to Archive after a specified period and delete blobs after a retention window, enabling automated long-term data retention without manual scripting.\n\n## Why Other Options Are Wrong\n\n- Option B: There is no global default tier policy that automatically migrates existing blobs; lifecycle rules are required.\n- Option C: Restricting to hot tier ignores the archival/mass-retention requirements.\n- Option D: Manual scripting defeats the benefits of automated policy-based retention and is error-prone.\n\n## Key Concepts\n\n- Lifecycle management rules\n- MoveToArchive action\n- Delete action with prefixes and age criteria\n\n## Real-World Application\n\nConfigure a lifecycle policy on the storage account targeting the logs container with a 180-day move-to-archive rule and a 7-year delete rule, then validate with sample blobs and monitor policy execution via storage analytics.\n","diagram":null,"difficulty":"intermediate","tags":["AzureStorage","LifecycleManagement","Archive","Terraform","Kubernetes","AWS-S3","certification-mcq","domain-weight-15"],"channel":"azure-developer","subChannel":"develop-storage","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:42:13.174Z","createdAt":"2026-01-12 13:42:13"},{"id":"azure-developer-develop-storage-1768225332421-2","question":"You plan to implement a data lake using ADLS Gen2 and you enable hierarchical namespace (HNS) on a storage account. Which outcomes are true?","answer":"[{\"id\":\"a\",\"text\":\"Enables directory-level ACLs and POSIX permissions\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Disables blob APIs\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Automatically moves data to Archive\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Forbids soft delete\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nOption a is correct. Enabling HNS provides directory/file semantics with POSIX-like ACLs, enabling granular access control for data lake workloads.\n\n## Why Other Options Are Wrong\n\n- Option B: ADLS Gen2 uses blob storage APIs; it does not disable them.\n- Option C: HNS does not automatically move data to Archive.\n- Option D: Soft delete remains an independent feature; HNS does not forbid it.\n\n## Key Concepts\n\n- Hierarchical namespace (HNS)\n- Directory/file semantics\n- POSIX ACLs and RBAC interaction\n\n## Real-World Application\n\nDesign lake folders and assign ACLs to data engineers and data scientists, then use path-based access controls combined with RBAC to govern access to datasets.\n","diagram":null,"difficulty":"intermediate","tags":["ADLSGen2","HierarchicalNamespace","ACL","POSIX","Kubernetes","Terraform","certification-mcq","domain-weight-15"],"channel":"azure-developer","subChannel":"develop-storage","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:42:13.693Z","createdAt":"2026-01-12 13:42:13"},{"id":"azure-developer-develop-storage-1768225332421-3","question":"You need to upload a 2 GB video from a browser to blob storage; which approach is best to enable resumable uploads and avoid memory pressure?","answer":"[{\"id\":\"a\",\"text\":\"Use BlockBlobClient to upload in chunks via stageBlock and commitBlockList\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Use AppendBlob and append blocks\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use PageBlob and incremental copy\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Upload as a single put blob with the entire file\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nOption a is correct. Block blobs support chunked uploads via stageBlock and commitBlockList, enabling resumable transfers and avoiding large memory usage in the browser.\n\n## Why Other Options Are Wrong\n\n- Option B: AppendBlob is not suited for large generic file uploads and lacks broad browser support for resumable uploads.\n- Option C: PageBlob is intended for random read/write scenarios and is less practical for streaming browser uploads.\n- Option D: A single large upload can exhaust memory and is brittle in unreliable networks.\n\n## Key Concepts\n\n- Block blobs\n- StageBlock/CommitBlockList pattern\n- Resumable uploads and client-side chunking\n\n## Real-World Application\n\nImplement a frontend that chunks the file (e.g., 4–8 MB per block) and a backend or SDK flow that stages blocks and commits the list, providing progress UI and retry logic for disrupted connections.\n","diagram":null,"difficulty":"intermediate","tags":["BlockBlob","LargeFileUpload","AzureBlobStorage","AKS","Terraform","certification-mcq","domain-weight-15"],"channel":"azure-developer","subChannel":"develop-storage","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:42:13.875Z","createdAt":"2026-01-12 13:42:13"},{"id":"azure-developer-develop-storage-1768225332421-4","question":"An Azure Function needs to write to a private blob container, but you want to avoid long-lived keys in code. You have a managed identity. Which RBAC role should you assign?","answer":"[{\"id\":\"a\",\"text\":\"Storage Blob Data Contributor\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Storage Account Contributor\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Reader\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Storage Blob Data Owner\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nOption a is correct. Storage Blob Data Contributor provides write (and read) access to blob data, granting the least privilege needed for the function to write to the container when using a managed identity.\n\n## Why Other Options Are Wrong\n\n- Option B: Storage Account Contributor is a management-plane role and grants broad permissions not needed for blob data operations.\n- Option C: Reader cannot write to blobs.\n- Option D: Data Owner is more privileged than required for typical write operations.\n\n## Key Concepts\n\n- Managed identity\n- Azure RBAC for data plane\n- Least privilege for blob data operations\n\n## Real-World Application\n\nAssign the function's managed identity Storage Blob Data Contributor, test write/read to the target container, and monitor audit logs for access events.\n","diagram":null,"difficulty":"intermediate","tags":["ManagedIdentity","RBAC","StorageBlobDataContributor","AzureFunctions","Kubernetes","certification-mcq","domain-weight-15"],"channel":"azure-developer","subChannel":"develop-storage","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:42:14.056Z","createdAt":"2026-01-12 13:42:14"},{"id":"q-1006","question":"Design a real-time telemetry ingestion pipeline for a fleet of autonomous vehicles on Azure. Events arrive at high volume per region; you must store compact per-vehicle summaries in Cosmos DB and archive raw events to Data Lake Gen2. How would you achieve exactly-once processing for aggregates, sub-200 ms latency, and zero data loss on transient failures? Propose architecture using Event Hubs, Functions, Databricks, and cross-region replication; justify idempotency and retry strategies?","answer":"Implement a pipeline: Ingest events via Event Hubs, process with Databricks Structured Streaming to produce per-vehicle aggregates on a sliding window with watermarking; write to Cosmos DB using idemp","explanation":"## Why This Is Asked\nTests ability to design scalable, fault-tolerant ingest with exactly-once semantics across regions.\n\n## Key Concepts\n- Ingestion via Event Hubs; streaming processing with Databricks; idempotent sinks in Cosmos DB; archive to ADLS Gen2; multi-region writes; retries; dead-lettering.\n\n## Code Example\n```javascript\n// Idempotent upsert for aggregate sink\nconst key = vehicleId + '|' + windowEnd;\nawait container.items.upsert({ id: key, vehicleId, windowEnd, sum, count, ts: Date.now() });\n```\n\n## Follow-up Questions\n- How would you test exactly-once semantics under backpressure? \n- What monitoring would you add for cross-region latency and data loss?","diagram":"flowchart TD\nA[Event Hubs ingress] --> B[Databricks Structured Streaming]\nB --> C[Cosmos DB upsert: id=vehicleId|windowEnd]\nA --> D[ADLS Gen2 Archive via Capture]\nC --> E[Multi-region Cosmos Writes]","difficulty":"advanced","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","Slack","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T18:51:22.684Z","createdAt":"2026-01-12T18:51:22.684Z"},{"id":"q-1136","question":"Design an end-to-end telemetry ingestion pipeline for 1M devices/min delivering messages {vehicleId, ts, lat, lon, speed}. Ingest via HTTPS into Event Hubs with vehicleId as partition key, process with a Function app (Event Hubs trigger) using batchSize=100; deduplicate per vehicle with Durable Entity and upsert to Cosmos DB multi-region. Explain data model, idempotency, Change Feed, backpressure, and monitoring?","answer":"Event Hubs partitioned by vehicleId; Functions with Event Hubs trigger (batchSize 100, prefetch 300) writes upserts to Cosmos DB (multi-region) and uses a per-vehicle Durable Entity to deduplicate, pr","explanation":"## Why This Is Asked\nTests ability to design scalable, fault-tolerant ingest with exactly-once semantics on Cosmos DB.\n\n## Key Concepts\n- Event Hubs partitioning, throughput units\n- Durable Entities for per-vehicle state\n- Cosmos DB multi-region upserts\n- Change Feed for read models and analytics\n- Backpressure, retries, monitoring\n\n## Code Example\n```javascript\n// Pseudo: durable entity for dedupe per vehicle\nclass VehicleState {\n  apply(event) { /* debounce and upsert once */ }\n}\n```\n\n## Follow-up Questions\n- How would you handle schema evolution for vehicle telemetry?\n- How would you validate end-to-end latency under peak loads?","diagram":"flowchart TD\n  Ingest[HTTPS Telemetry] --> EH[Event Hubs]\n  EH --> Fn[Functions (E/H trigger)]\n  Fn --> Cosmos[Cosmos DB (upsert)]\n  Cosmos --> Read[Read Model (Change Feed)]","difficulty":"advanced","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Amazon","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T01:25:35.864Z","createdAt":"2026-01-13T01:25:35.864Z"},{"id":"q-1248","question":"Design an end-to-end Azure ingestion pipeline for multi-tenant IoT events: thousands of devices per region send JSON to a gateway, per-tenant aggregates stored in Cosmos DB, raw data archived to Data Lake Gen2. Explain chosen services (Event Hub, Function/Durable Function, Cosmos DB with TTL, Data Lake), how you enforce per-tenant isolation and auditability, and how you achieve exactly-once processing and retry semantics?","answer":"Use Event Hubs in a regional namespace to ingest; route to Durable Functions that implement idempotent writes using a composite key (tenantId, messageId). Cosmos DB with partitionKey=tenantId stores a","explanation":"## Why This Is Asked\nThis tests practical Azure data-pipeline design, multi-tenancy, and reliability in production.\n\n## Key Concepts\n- Event Hubs, Durable Functions, Cosmos DB, Data Lake Gen2\n- Per-tenant partitioning, RBAC, Managed Identities\n- Deduplication, idempotent writes, retry strategies\n\n## Code Example\n```javascript\n// Pseudo Durable Function outline for idempotent processing\n```\n\n## Follow-up Questions\n- How would you test for cross-tenant data leakage?\n- How would you monitor latency and backpressure across regions?","diagram":null,"difficulty":"intermediate","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Citadel","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T06:42:46.713Z","createdAt":"2026-01-13T06:42:46.713Z"},{"id":"q-1277","question":"Design a real-time multi-tenant feature-store pipeline on Azure for a high-velocity AI platform. Ingest telemetry events via Event Hubs (tenantId, featureName, value, ts). Build end-to-end streaming with exactly-once semantics, isolation by tenant, and low-latency online reads. Specify concrete components (Event Hubs, Spark Structured Streaming, Cosmos DB with tenantId partition, Redis online store), auditability, TTL, and testing strategy?","answer":"Ingress: Event Hubs (tenantId key). Processing: Spark Structured Streaming with strict checkpointing for exactly-once; writes to Cosmos DB partitioned by tenantId (upsert for idempotence) plus a Redis","explanation":"## Why This Is Asked\nAssess ability to architect multi-tenant streaming systems on Azure with strong data isolation, idempotence, and end-to-end correctness.\n\n## Key Concepts\n- Event Hubs + Spark Structured Streaming\n- Cosmos DB partitioning by tenantId\n- Upsert semantics for idempotence\n- Redis as online store for latency\n- Audit/logging, TTL, RBAC and data isolation\n\n## Code Example\n```javascript\n// Pseudocode for upsert into Cosmos DB to preserve idempotence\nfunction saveFeature(tenantId, key, value, timestamp) {\n  const doc = { id: `${tenantId}:${key}`, tenantId, key, value, ts: timestamp };\n  cosmos.upsertItem(doc);\n}\n```\n\n## Follow-up Questions\n- How would you test end-to-end with replay and fault injection?\n- How would you enforce data retention, privacy, and tenant isolation in practice?","diagram":"flowchart TD\n  A[Event Ingest] --> B[Spark Structured Streaming]\n  B --> C[Cosmos DB (tenantId PK)]\n  B --> D[Redis Online Store]\n  C --> E[Audit Log / Changelog]\n  D --> E","difficulty":"advanced","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Hugging Face","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T07:40:49.401Z","createdAt":"2026-01-13T07:40:49.401Z"},{"id":"q-891","question":"Design a beginner-friendly serverless image-upload API on Azure: clients POST JPEGs to an HTTP-triggered Function, which saves the file to Blob Storage, enqueues a processing job, and stores a metadata record in Cosmos DB. How would you ensure idempotency, handle retries with back-off, and keep costs predictable on a Consumption plan?","answer":"Implement idempotency via an idempotency-key header; persist seen keys in Cosmos DB; on duplicates, return 200 and skip work. Decouple with a queue; HTTP function enqueues, blob saved. Processing func","explanation":"## Why This Is Asked\n\nTests practical serverless data flow, idempotency, and cost control in Azure. It also touches inter-service communication and observability.\n\n## Key Concepts\n\n- Idempotent HTTP endpoints using an idempotency key\n- Decoupling with HTTP → Blob → Queue → Worker\n- Cosmos DB for idempotency store and metadata\n- Retry/backoff and dead-lettering\n- Cost-conscious design on Consumption plan\n\n## Code Example\n\n```javascript\n// Skeleton: HTTP trigger checks idempotency key, writes blob, enqueues, and stores metadata\nmodule.exports = async function (context, req) {\n  const key = req.headers[\"x-idempotency-key\"];\n  // look up key in Cosmos DB; if exists, return 200\n  // otherwise, save blob, enqueue job, write metadata, and return 202\n};\n```\n\n## Follow-up Questions\n\n- How would you test idempotency guarantees in this flow?\n- What metrics would you collect to validate retry/backoff behavior and cost control?","diagram":null,"difficulty":"beginner","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Robinhood","Salesforce","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T14:31:34.107Z","createdAt":"2026-01-12T14:31:34.107Z"},{"id":"q-973","question":"Case: You’re building a beginner-friendly Azure API that accepts events from mobile apps. Each event includes userId, eventType, and timestamp. The API should write a compact summary to Cosmos DB and stream raw events to Event Hubs for analytics. On a Consumption plan, outline the minimal architecture, bindings, and error handling to ensure low latency, safe retries, and no data loss during transient outages?","answer":"HTTP-triggered Azure Function validates payload (userId, eventType, timestamp). Store a compact summary in Cosmos DB using a composite id (userId+timestamp) to enforce idempotency, and emit the full e","explanation":"## Why This Is Asked\nTests knowledge of a realistic ingestion path: HTTP input, idempotent storage, and streaming analytics with Azure services, plus the constraints of a Consumption plan.\n\n## Key Concepts\n- HTTP trigger in Azure Functions\n- Cosmos DB best practices for idempotent keys\n- Event Hubs for scalable intake of streams\n- Retry/backoff strategies on Consumption plan\n- Bindings and error handling in serverless architectures\n\n## Code Example\n```javascript\nmodule.exports = async function(context, req) {\n  const body = req.body;\n  // basic validation\n  if (!body?.userId || !body?.eventType || !body?.timestamp) {\n    context.res = { status: 400, body: 'Invalid payload' };\n    return;\n  }\n  const id = `${body.userId}|${body.timestamp}`;\n  // write summary to Cosmos DB (idempotent key)\n  context.bindings.cosmosDoc = { id, userId: body.userId, timestamp: body.timestamp, eventType: body.eventType };\n  // publish raw event to Event Hubs\n  context.bindings.outputEventHub = body;\n  context.res = { status: 202, body: 'Accepted' };\n};\n```\n\n## Follow-up Questions\n- How would you validate and test idempotency in this flow? \n- What metrics would you observe to ensure latency stays low during bursts?","diagram":null,"difficulty":"beginner","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["PayPal","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T17:37:30.997Z","createdAt":"2026-01-12T17:37:30.997Z"},{"id":"azure-developer-implement-api-1768279150081-0","question":"How would you implement a granular per-subscription rate limit in API Management (APIM)?","answer":"[{\"id\":\"a\",\"text\":\"Use a quota-by-key policy in inbound with renewal-period and counter-key\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use a rate-limit-by-key policy in inbound with renewal-period and counter-key\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Use a limit-by-header policy in outbound with a fixed window\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use a global rate limit on the gateway without a key\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct answer is b. Use a rate-limit-by-key policy in inbound with renewal-period and a counter-key derived from the subscription (e.g., context.Subscription.Id) to enforce per-subscription limits. While quota-by-key (option a) can enforce limits, it targets a per-period cap and is not as precise for ordinary per-second or per-minute rate limiting. Option c references a non-existent limit-by-header policy, and option d would apply globally to all callers rather than per-subscription, failing the requirement.\n\n## Why Other Options Are Wrong\n- Option A: quota-by-key can enforce per-period quotas but is less suitable for granular per-subscription rate limiting and may complicate eviction semantics.\n- Option C: no limit-by-header policy exists in APIM.\n- Option D: applies globally and ignores the per-subscription granularity.\n\n## Key Concepts\n- APIM policies\n- rate-limit-by-key\n- counter-key\n- renewal-period\n\n## Real-World Application\n- Prevents abuse by external consumers while preserving fair usage per subscriber in partner or public APIs.","diagram":null,"difficulty":"intermediate","tags":["AzureAPIManagement","Terraform","Kubernetes","AWS","AzureFunctions","certification-mcq","domain-weight-10"],"channel":"azure-developer","subChannel":"implement-api","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T04:39:10.083Z","createdAt":"2026-01-13 04:39:10"},{"id":"azure-developer-implement-api-1768279150081-1","question":"Which configuration enables API Management to reliably validate Azure Active Directory JWT tokens for your API?","answer":"[{\"id\":\"a\",\"text\":\"Use validate-jwt policy with issuer https://login.microsoftonline.com/{tenantId}/v2.0 and audience api://{appId}\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Validate tokens at the backend service and pass through any token\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use check-header policy to ensure Authorization header exists\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use oauth2 policy requiring client credentials flow\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct answer is a. The validate-jwt policy validates the token's issuer and audience against your Azure AD app registration, ensuring the token is issued by your tenant and intended for your API. Options b, c, and d do not perform this validation at the API gateway level and therefore do not guarantee token integrity before reaching backend services.\n\n## Why Other Options Are Wrong\n- Option B: Relies on a downstream service for validation, which defeats the APIM gateway's protective role.\n- Option C: Only checks for presence of an Authorization header, not the token's validity.\n- Option D: OAuth2 flow is for obtaining tokens, not validating incoming tokens at APIM.\n\n## Key Concepts\n- validate-jwt policy\n- issuer and audience verification\n- Azure AD/JWT validation\n\n## Real-World Application\n- Secures API surface exposed via APIM by validating tokens at the gateway before backend processing.","diagram":null,"difficulty":"intermediate","tags":["AzureAPIManagement","Terraform","Kubernetes","AWS","AzureFunctions","certification-mcq","domain-weight-10"],"channel":"azure-developer","subChannel":"implement-api","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T04:39:10.442Z","createdAt":"2026-01-13 04:39:10"},{"id":"azure-developer-implement-api-1768279150081-2","question":"To expose an internal API to external developers while enforcing paid subscription access, which APIM configuration is appropriate?","answer":"[{\"id\":\"a\",\"text\":\"Create a product that contains the API, mark subscription-required, and publish to the developer portal\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Enable IP restriction at gateway\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Require client certificate in inbound policy\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use a backend service to verify subscription manually\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct answer is a. In APIM, grouping APIs into a Product and requiring a subscription ensures external developers must subscribe to access the APIs, enabling monetization or tiered access. IP restriction, client certificates, or back-end verification alone do not provide integrated, scalable subscription management.\n\n## Why Other Options Are Wrong\n- Option B: IP restrictions do not enforce a subscription model.\n- Option C: Client certificates can restrict access but do not provide per-subscriber entitlements or usage metering.\n- Option D: Back-end verification bypasses APIM’s built-in subscription management and analytics.\n\n## Key Concepts\n- APIM products and subscriptions\n- Developer portal publishing\n- Access control via products\n\n## Real-World Application\n- Exposes internal APIs to external partners with monetization and tiered access.","diagram":null,"difficulty":"intermediate","tags":["AzureAPIManagement","Terraform","Kubernetes","AWS","AzureFunctions","certification-mcq","domain-weight-10"],"channel":"azure-developer","subChannel":"implement-api","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T04:39:10.787Z","createdAt":"2026-01-13 04:39:10"},{"id":"azure-developer-implement-api-1768279150081-3","question":"You need to expose API Management behind a custom domain with TLS termination; which approach is correct?","answer":"[{\"id\":\"a\",\"text\":\"Configure a custom domain on the APIM instance and upload a TLS certificate; map hostname; configure DNS\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Create a new API for each region\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use a policy to rewrite URLs to backend\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use a global policy to block certificates\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct answer is a. To serve APIM under a custom domain with TLS termination, configure a custom domain on the APIM instance, upload the TLS certificate, map the hostname, and configure the DNS entry accordingly. The other options do not address custom domain and TLS termination requirements.\n\n## Why Other Options Are Wrong\n- Option B: Regional API instances are not about domain naming or TLS termination.\n- Option C: URL rewriting is unrelated to domain and TLS termination.\n- Option D: Blocking certificates is counterproductive and irrelevant to domain customization.\n\n## Key Concepts\n- Custom domain configuration\n- TLS termination on APIM\n- DNS mapping\n\n## Real-World Application\n- Enables brand-consistent endpoints and accepted TLS practices for external clients.","diagram":null,"difficulty":"intermediate","tags":["AzureAPIManagement","Terraform","Kubernetes","AWS","AzureFunctions","certification-mcq","domain-weight-10"],"channel":"azure-developer","subChannel":"implement-api","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T04:39:10.913Z","createdAt":"2026-01-13 04:39:10"},{"id":"azure-developer-implement-api-1768279150081-4","question":"Which mechanism do you use in APIM to manage multiple API versions without duplicating gateways?","answer":"[{\"id\":\"a\",\"text\":\"Create an API Version Set and set versioning scheme to SegmentedPath; deploy versions under that set\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Hardcode version in backend URL and handle routing in code\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use a single version and rely on deprecation banners in docs\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use a separate APIM instance per version\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct answer is a. APIM provides API Version Sets to group versions of an API and choose a versioning scheme (e.g., SegmentedPath) so different versions can be exposed under the same gateway without duplicating deployments. Options B, C, and D either complicate routing, rely on manual deprecation, or require multiple gateways, which is less scalable.\n\n## Why Other Options Are Wrong\n- Option B: Hardcoding versioning in backend reduces flexibility and breaks centralized API management.\n- Option C: Deprecation banners do not provide proper version isolation or lifecycle management.\n- Option D: Separate APIM instances duplicate gateways and increase maintenance without necessity.\n\n## Key Concepts\n- API Version Set\n- Versioning schemes (SegmentedPath, Query, Header)\n- Version lifecycle management\n\n## Real-World Application\n- Smoothly deprecate old APIs while introducing new versions without downtime.","diagram":null,"difficulty":"intermediate","tags":["AzureAPIManagement","Terraform","Kubernetes","AWS","AzureFunctions","certification-mcq","domain-weight-10"],"channel":"azure-developer","subChannel":"implement-api","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T04:39:11.039Z","createdAt":"2026-01-13 04:39:11"},{"id":"azure-developer-implement-security-1768202831629-0","question":"A web application runs in Azure App Service. To securely manage a credential used to call a third-party service, you want centralized storage, automatic rotation, and minimal exposure in code. Which approach best achieves this?","answer":"[{\"id\":\"a\",\"text\":\"Store the key in App Service Settings as a plain text value and rotate manually in the portal\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Store the key in an Azure Key Vault and grant the app's managed identity read access to secrets; retrieve the key at runtime; enable versioned secrets for rotation\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Store the key as a secret in a Kubernetes Secret used by the App Service container\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Embed the key in code and load it from a configuration file at runtime\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct because using Azure Key Vault with a managed identity provides centralized secret management, runtime retrieval, and secret versioning for rotation without embedding credentials in code or configuration. \n\n## Why Other Options Are Wrong\n- A: Storing in App Service settings exposes secrets in the portal and requires manual rotation, increasing risk. \n- C: Kubernetes Secrets are not appropriate for App Service deployments and can leak if not carefully managed. \n- D: Embedding secrets in code or config files ties rotation to code changes and risks exposure. \n\n## Key Concepts\n- Managed identities for Azure resources\n- Azure Key Vault secret management and versioning \n- Secret rotation without code changes \n\n## Real-World Application\nImplement a Key Vault, grant the App Service's identity Get/List on secrets, retrieve the secret at startup or per-call, and rotate the secret by creating a new version in Key Vault without touching the application code.","diagram":null,"difficulty":"intermediate","tags":["AzureSecurity","KeyVault","ManagedIdentity","Kubernetes","Terraform","AWS IAM","certification-mcq","domain-weight-20"],"channel":"azure-developer","subChannel":"implement-security","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T07:27:11.631Z","createdAt":"2026-01-12 07:27:12"},{"id":"azure-developer-implement-security-1768202831629-1","question":"To enforce that all new Storage Accounts use customer-managed keys (CMK) for encryption at rest, and to prevent non-compliant configurations, which approach provides the least friction while ensuring enforcement?","answer":"[{\"id\":\"a\",\"text\":\"Rely on Microsoft-managed keys for encryption by default and monitor for CMK usage\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Create an Azure Policy that requires encryption with customer-managed keys and denies creation of storage accounts without a CMK\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Audit storage accounts quarterly and remediate non-compliant ones manually\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Store CMKs in Key Vault and rotate them manually; apply changes after resource creation\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct because an Azure Policy can enforce CMK usage at creation time and deny non-compliant storage accounts, providing automatic, scalable enforcement across the subscription. \n\n## Why Other Options Are Wrong\n- A: Microsoft-managed keys do not enforce CMK usage and do not prevent non-compliant configurations. \n- C: Auditing without enforcement allows non-compliant resources to exist. \n- D: Manual rotation/remediation is error-prone and lacks scalable enforcement. \n\n## Key Concepts\n- Azure Policy for enforcement\n- Customer-managed keys in Key Vault for encryption at rest\n- Deny assignments to prevent non-compliant resource creation\n\n## Real-World Application\nDefine a policy initiative that requires CMK for storage accounts and assign it at the subscription level; configure remediation tasks to address non-compliant resources when detected.","diagram":null,"difficulty":"intermediate","tags":["AzureSecurity","KeyVault","AzurePolicy","Terraform","Kubernetes","AWS IAM","certification-mcq","domain-weight-20"],"channel":"azure-developer","subChannel":"implement-security","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T07:27:12.158Z","createdAt":"2026-01-12 07:27:12"},{"id":"azure-developer-implement-security-1768202831629-2","question":"In an AKS cluster, you need pods to access Azure Key Vault secrets without embedding credentials in Kubernetes manifests. Which approach best enables fine-grained, workload-scoped access from specific pods while avoiding secret leakage?","answer":"[{\"id\":\"a\",\"text\":\"Create a Kubernetes Secret containing a service principal and mount it to the pods\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use Azure AD Workload Identity Federation to map a Kubernetes service account to an Azure AD identity and grant it access to Key Vault\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Attach a managed identity to the AKS node pool and share it across all pods to access Key Vault\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Embed the key in the application code and fetch it from Key Vault using a client secret\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct because Azure AD Workload Identity Federations (or Azure AD Pod Identity) enable Kubernetes service accounts to obtain short-lived tokens that grant pods scoped access to Key Vault, eliminating per-pod secret management. \n\n## Why Other Options Are Wrong\n- A: Kubernetes Secrets can be leaked or exposed in pod specs and are not ideal for dynamic workloads. \n- C: Node-scoped identities do not provide fine-grained, workload-level access control and are not principle-of-least-privilege. \n- D: Embedding secrets in code is insecure and breaks rotation guarantees. \n\n## Key Concepts\n- Azure AD Workload Identity Federation / Pod Identity\n- Fine-grained access control to Key Vault\n- Short-lived tokens and avoiding embedded secrets\n\n## Real-World Application\nConfigure a Kubernetes service account bound to an Azure identity, grant that identity a Key Vault access policy (or RBAC), and modify the pod deployment to use the bound service account so secrets are retrieved securely at runtime.","diagram":null,"difficulty":"intermediate","tags":["AzureSecurity","AKS","KeyVault","Kubernetes","Terraform","AWS IAM","certification-mcq","domain-weight-20"],"channel":"azure-developer","subChannel":"implement-security","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T07:27:12.672Z","createdAt":"2026-01-12 07:27:12"},{"id":"azure-developer-monitor-troubleshoot-1768238739192-0","question":"A web API hosted on Azure App Service experiences intermittent latency under load. You need to determine whether latency is caused by cold starts across instances or by resource contention. Which monitoring approach least ambiguously identifies the cause?","answer":"[{\"id\":\"a\",\"text\":\"Check CPU usage on the App Service plan in the primary region\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use Application Insights to correlate request durations with operation_Id across instances\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Review Network Security Group flow logs for inbound traffic\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Enable diagnostic settings to push platform logs to Log Analytics and review overall request rate\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct because Application Insights provides distributed tracing across instances, allowing you to correlate request durations with the same operation_Id to identify whether latency is due to cold starts (new instances taking longer) or sustained resource contention.\n\n## Why Other Options Are Wrong\n- A only shows CPU usage in the aggregate and does not reveal cross-instance latency patterns.\n- C captures network traffic but not application latency or cross-instance behavior.\n- D provides raw platform logs but lacks targeted correlation of latency with specific requests and instances.\n\n## Key Concepts\n- Distributed tracing with Application Insights\n- Cross-instance request correlation\n- Distinguishing cold starts from sustained load effects\n\n## Real-World Application\n- In production, enable Application Insights for your API, ensure request correlation is enabled, and build dashboards that show per-request latency across instances to quickly identify the root cause during load spikes.","diagram":null,"difficulty":"intermediate","tags":["Azure","AWS","Kubernetes","Terraform","AzureMonitor","certification-mcq","domain-weight-15"],"channel":"azure-developer","subChannel":"monitor-troubleshoot","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T17:25:39.194Z","createdAt":"2026-01-12 17:25:39"},{"id":"azure-developer-monitor-troubleshoot-1768238739192-1","question":"A Kubernetes cluster on AKS hosts multiple microservices and you observe occasional pod restarts during node pressure events. Which Azure Monitor feature best helps you identify misbehaving pods across nodes?","answer":"[{\"id\":\"a\",\"text\":\"Azure Monitor for Containers to view per-node and per-pod metrics\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Azure SQL Analytics\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Traffic Manager\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Azure Advisor\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A is correct because Azure Monitor for Containers (Container Insights) provides per-node and per-pod metrics, enabling you to pinpoint which pods or nodes are under pressure and correlate with restarts.\n\n## Why Other Options Are Wrong\n- B focuses on SQL workloads, not Kubernetes containers.\n- C Traffic Manager is a global DNS load balancer and doesn’t give per-pod insights.\n- D Advisor offers best-practice recommendations but not granular pod-level telemetry.\n\n## Key Concepts\n- Container Insights in Azure Monitor\n- Node-level and pod-level telemetry\n- Proactive troubleshooting for AKS\n\n## Real-World Application\n- Use Container Insights dashboards to track CPU/memory pressure and pod restarts, enabling targeted fixes such as resource requests/limits or node pool adjustments.","diagram":null,"difficulty":"intermediate","tags":["Azure","AWS","Kubernetes","Terraform","AzureMonitor","certification-mcq","domain-weight-15"],"channel":"azure-developer","subChannel":"monitor-troubleshoot","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T17:25:39.812Z","createdAt":"2026-01-12 17:25:40"},{"id":"azure-developer-monitor-troubleshoot-1768238739192-2","question":"Your HTTP-triggered Azure Functions experience sporadic latency during bursts. You want to reduce cold-start latency while managing cost. Which approach provides the best balance?","answer":"[{\"id\":\"a\",\"text\":\"Move the function to a Premium plan and enable Always On with a small number of pre-warmed instances\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Increase memory in the Consumption plan\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Enable Proxies to cache responses at the edge\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Move the function to an App Service plan\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A is correct because the Premium plan with Always On and pre-warmed instances reduces cold-start latency and provides predictable performance under bursty load, while still offering cost control compared to always-on scaling on other plans.\n\n## Why Other Options Are Wrong\n- B may reduce some latency but does not guarantee pre-warmed instances and can still suffer cold starts.\n- C Proxies do not address cold-start latency for functions and can add caching layers that complicate behavior.\n- D App Service plan removes serverless benefits and may increase cost without guaranteeing reduced cold starts for functions.\n\n## Key Concepts\n- Azure Functions hosting plans (Consumption vs Premium)\n- Always On and pre-warmed instances\n- Cost-performance trade-offs\n\n## Real-World Application\n- For unpredictable traffic, migrate to Premium with Always On and configure enough pre-warmed instances to meet peak bursts, then monitor latency and cost in the same dashboard.","diagram":null,"difficulty":"intermediate","tags":["Azure","AWS","Kubernetes","Terraform","AzureMonitor","certification-mcq","domain-weight-15"],"channel":"azure-developer","subChannel":"monitor-troubleshoot","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T17:25:40.393Z","createdAt":"2026-01-12 17:25:40"},{"id":"azure-developer-monitor-troubleshoot-1768238739192-3","question":"You operate a Cosmos DB account with multi-region writes enabled. To ensure cross-region replication lag is minimized and to respond to latency increases, which approach best leverages Azure Monitor and autoscale features?","answer":"[{\"id\":\"a\",\"text\":\"Create a Cosmos DB metrics alert on replication lag and enable automatic RU/s autoscale\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Rely on Traffic Manager to route traffic away from lagging regions\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use Application Insights to monitor client-side latency only\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Disable multi-region writes to avoid replication lag\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A is correct because Azure Monitor alerts can surface replication lag via Cosmos DB metrics, and enabling RU/s autoscale allows the database to elastically adjust throughput in response to latency or demand, helping minimize lag.\n\n## Why Other Options Are Wrong\n- B Traffic Manager operates at the DNS level and does not directly address Cosmos DB replication lag.\n- C Focuses on client-side latency and misses server-side replication dynamics.\n- D Disables the feature you rely on to reduce latency across regions and is not suitable for production needs.\n\n## Key Concepts\n- Cosmos DB multi-region write telemetry\n- Azure Monitor metrics alerts\n- Throughput autoscale (RU/s)\n\n## Real-World Application\n- Implement alerts for replication lag and pair with autoscale to automatically drive throughput changes during spikes, maintaining lower latency for users in distant regions.","diagram":null,"difficulty":"intermediate","tags":["Azure","AWS","Kubernetes","Terraform","AzureMonitor","certification-mcq","domain-weight-15"],"channel":"azure-developer","subChannel":"monitor-troubleshoot","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T17:25:40.584Z","createdAt":"2026-01-12 17:25:40"},{"id":"azure-developer-monitor-troubleshoot-1768238739192-4","question":"Azure SQL Database reports frequent deadlocks during peak hours. You want to identify root causes and implement safeguards with minimal downtime. Which combined approach is most effective?","answer":"[{\"id\":\"a\",\"text\":\"Enable Query Store, review wait statistics, and force a good execution plan when needed; leverage Intelligent Insights for anomaly detection\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Enable only SQL Audit logs and monitor for unusual activity\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use Application Insights to trace client requests\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Attach a SQL Server Profiler trace to the database\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A is correct because Query Store captures runtime statistics and plan choices, allowing you to identify and fix problematic plans; wait stats highlight blocking patterns, and Intelligent Insights helps surface anomalies that contribute to deadlocks.\n\n## Why Other Options Are Wrong\n- B Audit logs don’t provide the performance insight needed to diagnose deadlocks.\n- C Client tracing does not reveal server-side deadlock causes.\n- D Profiler is deprecated for Azure SQL Database in favor of Query Store and has performance risks.\n\n## Key Concepts\n- Query Store and wait statistics\n- Plan forcing and stability\n- Intelligent Insights for operational anomalies\n\n## Real-World Application\n- Regularly review Query Store reports during peak hours, apply forced plans when necessary, and enable Intelligent Insights to proactively detect deadlock-prone patterns and mitigate them before they impact users.","diagram":null,"difficulty":"intermediate","tags":["Azure","AWS","Kubernetes","Terraform","AzureMonitor","certification-mcq","domain-weight-15"],"channel":"azure-developer","subChannel":"monitor-troubleshoot","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T17:25:40.763Z","createdAt":"2026-01-12 17:25:40"}],"subChannels":["connect-consume","develop-compute","develop-storage","general","implement-api","implement-security","monitor-troubleshoot"],"companies":["Airbnb","Amazon","Citadel","Google","Hugging Face","Lyft","OpenAI","PayPal","Robinhood","Salesforce","Scale Ai","Slack","Tesla"],"stats":{"total":32,"beginner":2,"intermediate":27,"advanced":3,"newThisWeek":32}}