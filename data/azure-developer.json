{"questions":[{"id":"azure-developer-develop-compute-1768162939034-0","question":"You are designing a high-throughput event-driven API on Azure Functions that must connect to a third-party service requiring a fixed outbound IP. Which compute option best meets latency, scale, and IP allow-list requirements?","answer":"[{\"id\":\"a\",\"text\":\"Azure Functions Consumption plan with dynamic outbound IP\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Azure Functions Premium plan with pre-warmed instances and VNET integration\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Azure App Service on Standard plan\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Azure Container Instances with a single container\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B. Azure Functions Premium plan with pre-warmed instances and VNET integration reduces cold starts and provides secure, controllable egress when connecting to external services, which helps meet latency and network-security requirements. For strict fixed outbound IPs, additional egress controls (such as a NAT or dedicated egress) may be used, but Premium still offers the best balance of latency, scale, and network control for this scenario.\n\n## Why Other Options Are Wrong\n- Option A: Consumption plan experiences cold starts under load and outbound IPs can change, which worsens latency and IP allow-list reliability.\n- Option C: App Service on Standard does not guarantee pre-warmed instances and has less flexible networking options for controlled egress.\n- Option D: ACI with a single container does not provide the auto-scaling, pacing, or integrated networking features needed for high-throughput, multi-tenant workloads.\n\n## Key Concepts\n- Serverless compute options: Consumption vs Premium plans\n- VNET integration for Azure Functions\n- Egress control and outbound IP considerations\n- When to apply NAT or dedicated egress patterns\n\n## Real-World Application\nUsed when building a serverless integration that must call partner APIs with IP allow-list constraints and predictable latency, enabling reliable scaling without cold starts.","diagram":null,"difficulty":"intermediate","tags":["Azure","Kubernetes","AWS","Terraform","AKS","certification-mcq","domain-weight-25"],"channel":"azure-developer","subChannel":"develop-compute","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T20:22:19.035Z","createdAt":"2026-01-11 20:22:19"},{"id":"azure-developer-develop-compute-1768162939034-1","question":"During a migration of a microservices app to AKS, your team requires zero-downtime deployments, per-service autoscaling, and straightforward routing. Which approach best satisfies these needs in AKS?","answer":"[{\"id\":\"a\",\"text\":\"Deploy each service as a Kubernetes Deployment using RollingUpdate with readiness probes and per-service HorizontalPodAutoscaler\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Use VM Scale Sets behind a load balancer\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Move all services to Azure Functions for event-driven processing\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use App Service Environment with slot swapping\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A. Deploying each microservice as a Kubernetes Deployment with a RollingUpdate strategy, plus readiness probes and per-service HorizontalPodAutoscalers (HPA), enables zero-downtime deployments and automatic scaling. While canary/blue-green patterns can be added with service mesh tooling, the core AKS pattern for safe updates and scalable services is rolling updates combined with readiness probes and per-service HPAs.\n\n## Why Other Options Are Wrong\n- Option B: VM Scale Sets can offer scale but do not provide native per-service rolling updates with Kubernetes-level health checks and routing complexity.\n- Option C: Azure Functions is not a direct substitute for a distributed microservices architecture on AKS and does not address zero-downtime rolling updates across multiple services.\n- Option D: Slot swapping is an App Service concept and does not apply to AKS traffic routing and rolling deployments.\n\n## Key Concepts\n- Kubernetes Deployment rolling updates\n- Readiness probes and liveness checks\n- HorizontalPodAutoscaler per service\n- AKS deployment strategies for zero-downtime upgrades\n\n## Real-World Application\nApplies when migrating microservices to AKS and you need smooth, automatic updates across many services without traffic disruption, while preserving scalable capacity per service.","diagram":null,"difficulty":"intermediate","tags":["Azure","Kubernetes","AWS","Terraform","AKS","certification-mcq","domain-weight-25"],"channel":"azure-developer","subChannel":"develop-compute","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T20:22:19.617Z","createdAt":"2026-01-11 20:22:20"},{"id":"azure-developer-develop-compute-1768162939034-2","question":"In an event-driven Azure architecture, you want to guarantee exactly-once processing of messages from a queue into a downstream store. Which pattern best achieves this?","answer":"[{\"id\":\"a\",\"text\":\"Azure Service Bus with sessions and content-based deduplication and an idempotent consumer\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Azure Storage Queues with DeleteMessage after processing\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Azure Event Grid with automatic retries until success\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Cosmos DB change feed with eventual consistency handling\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A. Azure Service Bus supports per-message deduplication and sessions for ordered processing, and when combined with an idempotent consumer (the downstream store logic is designed to handle duplicates gracefully), you can approach exactly-once semantics. Other options either rely on at-least-once processing guarantees (Storage Queues, Event Grid retries) or require complex idempotency handling beyond what change feeds provide by default.\n\n## Why Other Options Are Wrong\n- Option B: Storage Queues can deliver messages at-least-once; duplicates are possible without additional dedup logic.\n- Option C: Event Grid retries do not guarantee exactly-once delivery semantics for each event across all handlers.\n- Option D: Cosmos DB change feed provides strong ordering and consistency models for data changes but does not inherently enforce exactly-once processing for external downstream writes.\n\n## Key Concepts\n- Exactly-once processing patterns\n- Service Bus deduplication and sessions\n- Idempotent consumer design\n- Event-driven architecture trade-offs\n\n## Real-World Application\nUseful when integrating microservices that ingest events from a message broker and must avoid duplicate writes to a downstream data store, such as financial or inventory systems where duplicates have real consequences.","diagram":null,"difficulty":"intermediate","tags":["Azure","Kubernetes","Terraform","AWS","Service Bus","certification-mcq","domain-weight-25"],"channel":"azure-developer","subChannel":"develop-compute","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T20:22:20.149Z","createdAt":"2026-01-11 20:22:20"},{"id":"azure-developer-develop-storage-1768225332421-0","question":"You have a web app that lets users upload profile pictures to a private blob container. You cannot expose account keys to the client. Which approach ensures least privilege, revocability, and no key exposure?","answer":"[{\"id\":\"a\",\"text\":\"Generate a user delegation SAS (via Azure AD) with write permission and short expiry, served to client from backend\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Make the container publicly accessible and have the client upload\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Embed the account key in the client to authorize the upload\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use a CORS policy alone to enable uploads\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nOption a is correct. A user delegation SAS (or a per-user SAS backed by Azure AD) provides scoped access to blob resources without sharing account keys and can be revoked by expiring the token or via policy updates.\n\n## Why Other Options Are Wrong\n\n- Option B: Public container access would allow anyone to upload or enumerate blobs, defeating security and revocation controls.\n- Option C: Embedding the account key in client code is effectively sharing the key and is insecure.\n- Option D: CORS controls cross-origin requests, not authentication or granular authorization for blob operations.\n\n## Key Concepts\n\n- Shared Access Signatures (SAS)\n- User delegation SAS and Azure AD integration\n- Principle of least privilege\n- Token expiry and revocation\n\n## Real-World Application\n\nImplement a backend endpoint that issues short-lived SAS tokens per user or per operation, enforce permission scope (write/list), and rotate tokens frequently. Integrate this with the frontend to fetch tokens before upload and validate server-side usage logs.\n","diagram":null,"difficulty":"intermediate","tags":["AzureBlobStorage","SAS","RBAC","AKS","Terraform","AWS-S3","certification-mcq","domain-weight-15"],"channel":"azure-developer","subChannel":"develop-storage","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:42:12.422Z","createdAt":"2026-01-12 13:42:13"},{"id":"azure-developer-develop-storage-1768225332421-1","question":"Logs stored in a container must be archived after 180 days and purged after seven years; which feature and example rules would implement this?","answer":"[{\"id\":\"a\",\"text\":\"Storage lifecycle management: MoveToArchive after 180 days; Delete after 7 years\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Set container default tier to Archive\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Enable hot tier only\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Write a custom script to move older blobs manually\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nOption a is correct. Lifecycle management policies can automatically transition blobs to Archive after a specified period and delete blobs after a retention window, enabling automated long-term data retention without manual scripting.\n\n## Why Other Options Are Wrong\n\n- Option B: There is no global default tier policy that automatically migrates existing blobs; lifecycle rules are required.\n- Option C: Restricting to hot tier ignores the archival/mass-retention requirements.\n- Option D: Manual scripting defeats the benefits of automated policy-based retention and is error-prone.\n\n## Key Concepts\n\n- Lifecycle management rules\n- MoveToArchive action\n- Delete action with prefixes and age criteria\n\n## Real-World Application\n\nConfigure a lifecycle policy on the storage account targeting the logs container with a 180-day move-to-archive rule and a 7-year delete rule, then validate with sample blobs and monitor policy execution via storage analytics.\n","diagram":null,"difficulty":"intermediate","tags":["AzureStorage","LifecycleManagement","Archive","Terraform","Kubernetes","AWS-S3","certification-mcq","domain-weight-15"],"channel":"azure-developer","subChannel":"develop-storage","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:42:13.174Z","createdAt":"2026-01-12 13:42:13"},{"id":"azure-developer-develop-storage-1768225332421-2","question":"You plan to implement a data lake using ADLS Gen2 and you enable hierarchical namespace (HNS) on a storage account. Which outcomes are true?","answer":"[{\"id\":\"a\",\"text\":\"Enables directory-level ACLs and POSIX permissions\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Disables blob APIs\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Automatically moves data to Archive\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Forbids soft delete\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nOption a is correct. Enabling HNS provides directory/file semantics with POSIX-like ACLs, enabling granular access control for data lake workloads.\n\n## Why Other Options Are Wrong\n\n- Option B: ADLS Gen2 uses blob storage APIs; it does not disable them.\n- Option C: HNS does not automatically move data to Archive.\n- Option D: Soft delete remains an independent feature; HNS does not forbid it.\n\n## Key Concepts\n\n- Hierarchical namespace (HNS)\n- Directory/file semantics\n- POSIX ACLs and RBAC interaction\n\n## Real-World Application\n\nDesign lake folders and assign ACLs to data engineers and data scientists, then use path-based access controls combined with RBAC to govern access to datasets.\n","diagram":null,"difficulty":"intermediate","tags":["ADLSGen2","HierarchicalNamespace","ACL","POSIX","Kubernetes","Terraform","certification-mcq","domain-weight-15"],"channel":"azure-developer","subChannel":"develop-storage","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:42:13.693Z","createdAt":"2026-01-12 13:42:13"},{"id":"azure-developer-develop-storage-1768225332421-3","question":"You need to upload a 2 GB video from a browser to blob storage; which approach is best to enable resumable uploads and avoid memory pressure?","answer":"[{\"id\":\"a\",\"text\":\"Use BlockBlobClient to upload in chunks via stageBlock and commitBlockList\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Use AppendBlob and append blocks\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use PageBlob and incremental copy\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Upload as a single put blob with the entire file\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nOption a is correct. Block blobs support chunked uploads via stageBlock and commitBlockList, enabling resumable transfers and avoiding large memory usage in the browser.\n\n## Why Other Options Are Wrong\n\n- Option B: AppendBlob is not suited for large generic file uploads and lacks broad browser support for resumable uploads.\n- Option C: PageBlob is intended for random read/write scenarios and is less practical for streaming browser uploads.\n- Option D: A single large upload can exhaust memory and is brittle in unreliable networks.\n\n## Key Concepts\n\n- Block blobs\n- StageBlock/CommitBlockList pattern\n- Resumable uploads and client-side chunking\n\n## Real-World Application\n\nImplement a frontend that chunks the file (e.g., 4–8 MB per block) and a backend or SDK flow that stages blocks and commits the list, providing progress UI and retry logic for disrupted connections.\n","diagram":null,"difficulty":"intermediate","tags":["BlockBlob","LargeFileUpload","AzureBlobStorage","AKS","Terraform","certification-mcq","domain-weight-15"],"channel":"azure-developer","subChannel":"develop-storage","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:42:13.875Z","createdAt":"2026-01-12 13:42:13"},{"id":"azure-developer-develop-storage-1768225332421-4","question":"An Azure Function needs to write to a private blob container, but you want to avoid long-lived keys in code. You have a managed identity. Which RBAC role should you assign?","answer":"[{\"id\":\"a\",\"text\":\"Storage Blob Data Contributor\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Storage Account Contributor\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Reader\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Storage Blob Data Owner\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nOption a is correct. Storage Blob Data Contributor provides write (and read) access to blob data, granting the least privilege needed for the function to write to the container when using a managed identity.\n\n## Why Other Options Are Wrong\n\n- Option B: Storage Account Contributor is a management-plane role and grants broad permissions not needed for blob data operations.\n- Option C: Reader cannot write to blobs.\n- Option D: Data Owner is more privileged than required for typical write operations.\n\n## Key Concepts\n\n- Managed identity\n- Azure RBAC for data plane\n- Least privilege for blob data operations\n\n## Real-World Application\n\nAssign the function's managed identity Storage Blob Data Contributor, test write/read to the target container, and monitor audit logs for access events.\n","diagram":null,"difficulty":"intermediate","tags":["ManagedIdentity","RBAC","StorageBlobDataContributor","AzureFunctions","Kubernetes","certification-mcq","domain-weight-15"],"channel":"azure-developer","subChannel":"develop-storage","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:42:14.056Z","createdAt":"2026-01-12 13:42:14"},{"id":"q-891","question":"Design a beginner-friendly serverless image-upload API on Azure: clients POST JPEGs to an HTTP-triggered Function, which saves the file to Blob Storage, enqueues a processing job, and stores a metadata record in Cosmos DB. How would you ensure idempotency, handle retries with back-off, and keep costs predictable on a Consumption plan?","answer":"Implement idempotency via an idempotency-key header; persist seen keys in Cosmos DB; on duplicates, return 200 and skip work. Decouple with a queue; HTTP function enqueues, blob saved. Processing func","explanation":"## Why This Is Asked\n\nTests practical serverless data flow, idempotency, and cost control in Azure. It also touches inter-service communication and observability.\n\n## Key Concepts\n\n- Idempotent HTTP endpoints using an idempotency key\n- Decoupling with HTTP → Blob → Queue → Worker\n- Cosmos DB for idempotency store and metadata\n- Retry/backoff and dead-lettering\n- Cost-conscious design on Consumption plan\n\n## Code Example\n\n```javascript\n// Skeleton: HTTP trigger checks idempotency key, writes blob, enqueues, and stores metadata\nmodule.exports = async function (context, req) {\n  const key = req.headers[\"x-idempotency-key\"];\n  // look up key in Cosmos DB; if exists, return 200\n  // otherwise, save blob, enqueue job, write metadata, and return 202\n};\n```\n\n## Follow-up Questions\n\n- How would you test idempotency guarantees in this flow?\n- What metrics would you collect to validate retry/backoff behavior and cost control?","diagram":null,"difficulty":"beginner","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Robinhood","Salesforce","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T14:31:34.107Z","createdAt":"2026-01-12T14:31:34.107Z"},{"id":"azure-developer-implement-security-1768202831629-0","question":"A web application runs in Azure App Service. To securely manage a credential used to call a third-party service, you want centralized storage, automatic rotation, and minimal exposure in code. Which approach best achieves this?","answer":"[{\"id\":\"a\",\"text\":\"Store the key in App Service Settings as a plain text value and rotate manually in the portal\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Store the key in an Azure Key Vault and grant the app's managed identity read access to secrets; retrieve the key at runtime; enable versioned secrets for rotation\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Store the key as a secret in a Kubernetes Secret used by the App Service container\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Embed the key in code and load it from a configuration file at runtime\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct because using Azure Key Vault with a managed identity provides centralized secret management, runtime retrieval, and secret versioning for rotation without embedding credentials in code or configuration. \n\n## Why Other Options Are Wrong\n- A: Storing in App Service settings exposes secrets in the portal and requires manual rotation, increasing risk. \n- C: Kubernetes Secrets are not appropriate for App Service deployments and can leak if not carefully managed. \n- D: Embedding secrets in code or config files ties rotation to code changes and risks exposure. \n\n## Key Concepts\n- Managed identities for Azure resources\n- Azure Key Vault secret management and versioning \n- Secret rotation without code changes \n\n## Real-World Application\nImplement a Key Vault, grant the App Service's identity Get/List on secrets, retrieve the secret at startup or per-call, and rotate the secret by creating a new version in Key Vault without touching the application code.","diagram":null,"difficulty":"intermediate","tags":["AzureSecurity","KeyVault","ManagedIdentity","Kubernetes","Terraform","AWS IAM","certification-mcq","domain-weight-20"],"channel":"azure-developer","subChannel":"implement-security","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T07:27:11.631Z","createdAt":"2026-01-12 07:27:12"},{"id":"azure-developer-implement-security-1768202831629-1","question":"To enforce that all new Storage Accounts use customer-managed keys (CMK) for encryption at rest, and to prevent non-compliant configurations, which approach provides the least friction while ensuring enforcement?","answer":"[{\"id\":\"a\",\"text\":\"Rely on Microsoft-managed keys for encryption by default and monitor for CMK usage\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Create an Azure Policy that requires encryption with customer-managed keys and denies creation of storage accounts without a CMK\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Audit storage accounts quarterly and remediate non-compliant ones manually\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Store CMKs in Key Vault and rotate them manually; apply changes after resource creation\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct because an Azure Policy can enforce CMK usage at creation time and deny non-compliant storage accounts, providing automatic, scalable enforcement across the subscription. \n\n## Why Other Options Are Wrong\n- A: Microsoft-managed keys do not enforce CMK usage and do not prevent non-compliant configurations. \n- C: Auditing without enforcement allows non-compliant resources to exist. \n- D: Manual rotation/remediation is error-prone and lacks scalable enforcement. \n\n## Key Concepts\n- Azure Policy for enforcement\n- Customer-managed keys in Key Vault for encryption at rest\n- Deny assignments to prevent non-compliant resource creation\n\n## Real-World Application\nDefine a policy initiative that requires CMK for storage accounts and assign it at the subscription level; configure remediation tasks to address non-compliant resources when detected.","diagram":null,"difficulty":"intermediate","tags":["AzureSecurity","KeyVault","AzurePolicy","Terraform","Kubernetes","AWS IAM","certification-mcq","domain-weight-20"],"channel":"azure-developer","subChannel":"implement-security","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T07:27:12.158Z","createdAt":"2026-01-12 07:27:12"},{"id":"azure-developer-implement-security-1768202831629-2","question":"In an AKS cluster, you need pods to access Azure Key Vault secrets without embedding credentials in Kubernetes manifests. Which approach best enables fine-grained, workload-scoped access from specific pods while avoiding secret leakage?","answer":"[{\"id\":\"a\",\"text\":\"Create a Kubernetes Secret containing a service principal and mount it to the pods\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use Azure AD Workload Identity Federation to map a Kubernetes service account to an Azure AD identity and grant it access to Key Vault\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Attach a managed identity to the AKS node pool and share it across all pods to access Key Vault\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Embed the key in the application code and fetch it from Key Vault using a client secret\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct because Azure AD Workload Identity Federations (or Azure AD Pod Identity) enable Kubernetes service accounts to obtain short-lived tokens that grant pods scoped access to Key Vault, eliminating per-pod secret management. \n\n## Why Other Options Are Wrong\n- A: Kubernetes Secrets can be leaked or exposed in pod specs and are not ideal for dynamic workloads. \n- C: Node-scoped identities do not provide fine-grained, workload-level access control and are not principle-of-least-privilege. \n- D: Embedding secrets in code is insecure and breaks rotation guarantees. \n\n## Key Concepts\n- Azure AD Workload Identity Federation / Pod Identity\n- Fine-grained access control to Key Vault\n- Short-lived tokens and avoiding embedded secrets\n\n## Real-World Application\nConfigure a Kubernetes service account bound to an Azure identity, grant that identity a Key Vault access policy (or RBAC), and modify the pod deployment to use the bound service account so secrets are retrieved securely at runtime.","diagram":null,"difficulty":"intermediate","tags":["AzureSecurity","AKS","KeyVault","Kubernetes","Terraform","AWS IAM","certification-mcq","domain-weight-20"],"channel":"azure-developer","subChannel":"implement-security","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T07:27:12.672Z","createdAt":"2026-01-12 07:27:12"}],"subChannels":["develop-compute","develop-storage","general","implement-security"],"companies":["Robinhood","Salesforce","Tesla"],"stats":{"total":12,"beginner":1,"intermediate":11,"advanced":0,"newThisWeek":12}}