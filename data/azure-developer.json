{"questions":[{"id":"q-1006","question":"Design a real-time telemetry ingestion pipeline for a fleet of autonomous vehicles on Azure. Events arrive at high volume per region; you must store compact per-vehicle summaries in Cosmos DB and archive raw events to Data Lake Gen2. How would you achieve exactly-once processing for aggregates, sub-200 ms latency, and zero data loss on transient failures? Propose architecture using Event Hubs, Functions, Databricks, and cross-region replication; justify idempotency and retry strategies?","answer":"Implement a pipeline: Ingest events via Event Hubs, process with Databricks Structured Streaming to produce per-vehicle aggregates on a sliding window with watermarking; write to Cosmos DB using idemp","explanation":"## Why This Is Asked\nTests ability to design scalable, fault-tolerant ingest with exactly-once semantics across regions.\n\n## Key Concepts\n- Ingestion via Event Hubs; streaming processing with Databricks; idempotent sinks in Cosmos DB; archive to ADLS Gen2; multi-region writes; retries; dead-lettering.\n\n## Code Example\n```javascript\n// Idempotent upsert for aggregate sink\nconst key = vehicleId + '|' + windowEnd;\nawait container.items.upsert({ id: key, vehicleId, windowEnd, sum, count, ts: Date.now() });\n```\n\n## Follow-up Questions\n- How would you test exactly-once semantics under backpressure? \n- What monitoring would you add for cross-region latency and data loss?","diagram":"flowchart TD\nA[Event Hubs ingress] --> B[Databricks Structured Streaming]\nB --> C[Cosmos DB upsert: id=vehicleId|windowEnd]\nA --> D[ADLS Gen2 Archive via Capture]\nC --> E[Multi-region Cosmos Writes]","difficulty":"advanced","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","Slack","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T18:51:22.684Z","createdAt":"2026-01-12T18:51:22.684Z"},{"id":"q-1136","question":"Design an end-to-end telemetry ingestion pipeline for 1M devices/min delivering messages {vehicleId, ts, lat, lon, speed}. Ingest via HTTPS into Event Hubs with vehicleId as partition key, process with a Function app (Event Hubs trigger) using batchSize=100; deduplicate per vehicle with Durable Entity and upsert to Cosmos DB multi-region. Explain data model, idempotency, Change Feed, backpressure, and monitoring?","answer":"Event Hubs partitioned by vehicleId; Functions with Event Hubs trigger (batchSize 100, prefetch 300) writes upserts to Cosmos DB (multi-region) and uses a per-vehicle Durable Entity to deduplicate, pr","explanation":"## Why This Is Asked\nTests ability to design scalable, fault-tolerant ingest with exactly-once semantics on Cosmos DB.\n\n## Key Concepts\n- Event Hubs partitioning, throughput units\n- Durable Entities for per-vehicle state\n- Cosmos DB multi-region upserts\n- Change Feed for read models and analytics\n- Backpressure, retries, monitoring\n\n## Code Example\n```javascript\n// Pseudo: durable entity for dedupe per vehicle\nclass VehicleState {\n  apply(event) { /* debounce and upsert once */ }\n}\n```\n\n## Follow-up Questions\n- How would you handle schema evolution for vehicle telemetry?\n- How would you validate end-to-end latency under peak loads?","diagram":"flowchart TD\n  Ingest[HTTPS Telemetry] --> EH[Event Hubs]\n  EH --> Fn[Functions (E/H trigger)]\n  Fn --> Cosmos[Cosmos DB (upsert)]\n  Cosmos --> Read[Read Model (Change Feed)]","difficulty":"advanced","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Amazon","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T01:25:35.864Z","createdAt":"2026-01-13T01:25:35.864Z"},{"id":"q-1248","question":"Design an end-to-end Azure ingestion pipeline for multi-tenant IoT events: thousands of devices per region send JSON to a gateway, per-tenant aggregates stored in Cosmos DB, raw data archived to Data Lake Gen2. Explain chosen services (Event Hub, Function/Durable Function, Cosmos DB with TTL, Data Lake), how you enforce per-tenant isolation and auditability, and how you achieve exactly-once processing and retry semantics?","answer":"Use Event Hubs in a regional namespace to ingest; route to Durable Functions that implement idempotent writes using a composite key (tenantId, messageId). Cosmos DB with partitionKey=tenantId stores a","explanation":"## Why This Is Asked\nThis tests practical Azure data-pipeline design, multi-tenancy, and reliability in production.\n\n## Key Concepts\n- Event Hubs, Durable Functions, Cosmos DB, Data Lake Gen2\n- Per-tenant partitioning, RBAC, Managed Identities\n- Deduplication, idempotent writes, retry strategies\n\n## Code Example\n```javascript\n// Pseudo Durable Function outline for idempotent processing\n```\n\n## Follow-up Questions\n- How would you test for cross-tenant data leakage?\n- How would you monitor latency and backpressure across regions?","diagram":null,"difficulty":"intermediate","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Citadel","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T06:42:46.713Z","createdAt":"2026-01-13T06:42:46.713Z"},{"id":"q-1277","question":"Design a real-time multi-tenant feature-store pipeline on Azure for a high-velocity AI platform. Ingest telemetry events via Event Hubs (tenantId, featureName, value, ts). Build end-to-end streaming with exactly-once semantics, isolation by tenant, and low-latency online reads. Specify concrete components (Event Hubs, Spark Structured Streaming, Cosmos DB with tenantId partition, Redis online store), auditability, TTL, and testing strategy?","answer":"Ingress: Event Hubs (tenantId key). Processing: Spark Structured Streaming with strict checkpointing for exactly-once; writes to Cosmos DB partitioned by tenantId (upsert for idempotence) plus a Redis","explanation":"## Why This Is Asked\nAssess ability to architect multi-tenant streaming systems on Azure with strong data isolation, idempotence, and end-to-end correctness.\n\n## Key Concepts\n- Event Hubs + Spark Structured Streaming\n- Cosmos DB partitioning by tenantId\n- Upsert semantics for idempotence\n- Redis as online store for latency\n- Audit/logging, TTL, RBAC and data isolation\n\n## Code Example\n```javascript\n// Pseudocode for upsert into Cosmos DB to preserve idempotence\nfunction saveFeature(tenantId, key, value, timestamp) {\n  const doc = { id: `${tenantId}:${key}`, tenantId, key, value, ts: timestamp };\n  cosmos.upsertItem(doc);\n}\n```\n\n## Follow-up Questions\n- How would you test end-to-end with replay and fault injection?\n- How would you enforce data retention, privacy, and tenant isolation in practice?","diagram":"flowchart TD\n  A[Event Ingest] --> B[Spark Structured Streaming]\n  B --> C[Cosmos DB (tenantId PK)]\n  B --> D[Redis Online Store]\n  C --> E[Audit Log / Changelog]\n  D --> E","difficulty":"advanced","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Hugging Face","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T07:40:49.401Z","createdAt":"2026-01-13T07:40:49.401Z"},{"id":"q-1359","question":"A small API running on Azure Functions must securely retrieve a database connection string from Azure Key Vault at startup and refresh it periodically without restarting the function. Propose a beginner-friendly, low-latency approach using Managed Identity and Key Vault, including caching strategy, rotation handling, and error fallback?","answer":"Use a single Function App with a managed identity to fetch the connection string from Key Vault via SecretClient. Cache the secret in-memory with a short TTL (5–10 minutes) and refresh via a timer-bas","explanation":"## Why This Is Asked\nTests ability to combine cloud identity, secret management, and cache-based freshness in a simple serverless setup. It also probes handling secret rotation with minimal downtime.\n\n## Key Concepts\n- Managed Identity and Key Vault access\n- Secret retrieval patterns and in-memory caching\n- Cache invalidation and rotation handling\n- Error fallback and observability\n\n## Code Example\n```javascript\nimport { SecretClient } from \"@azure/keyvault-secrets\";\nimport { DefaultAzureCredential } from \"@azure/identity\";\n\nconst credential = new DefaultAzureCredential();\nconst vaultUrl = process.env.KEY_VAULT_URL;\nconst client = new SecretClient(vaultUrl, credential);\nlet cachedConnStr = null;\nlet cacheExpiry = 0;\n\nasync function loadConnStr() {\n  const secret = await client.getSecret(\"DbConnectionString\");\n  cachedConnStr = secret.value;\n  cacheExpiry = Date.now() + 10 * 60 * 1000; // 10 minutes\n  return cachedConnStr;\n}\n```\n\n## Follow-up Questions\n- How would you simulate and test secret rotation without impacting users?\n- How would you scale this in a multi-instance Function App deployment?\n- What changes if Key Vault is temporarily unavailable?","diagram":null,"difficulty":"beginner","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["OpenAI","Salesforce","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T13:13:54.965Z","createdAt":"2026-01-13T13:13:54.965Z"},{"id":"q-1397","question":"Design a multi-region, per-tenant Azure API with data residency, deterministic retries, and exactly-once semantics at scale. Propose services (APIM, Functions + Durable Functions, Cosmos DB per-tenant, Front Door, Private Endpoints) and explain how per-tenant isolation, data residency, and retry determinism are achieved?","answer":"Use APIM in each region in front of a Durable Functions orchestrator that enforces idempotency via a per-tenant idempotency key. Route writes to Cosmos DB with per-tenant partition keys and multi-regi","explanation":"## Why This Is Asked\n\nAssesses practical Azure design for data residency, regional routing, and robust exactly-once processing at scale. Expected to justify service choices, tenancy isolation, and fault-tolerant retry strategies.\n\n## Key Concepts\n\n- Idempotency keys and per-tenant isolation\n- Durable Functions orchestrations for reliable retries\n- Cosmos DB per-tenant partitions with controlled multi-region writes\n- Data residency via region-specific routing and Private Endpoints\n- Global routing with Front Door and secure exposure\n\n## Code Example\n\n```javascript\n// Pseudo: acquire idempotency key, check store, and proceed with orchestrator\nconst id = req.headers['x-idempotency-key'];\nconst seen = await cosmos.find({ id, partitionKey: idTenant(req) });\nif (seen) return { status: 200, body: seen.result };\nawait durableClient.startNew('Orchestrator', { id, tenant: req.tenant, payload: req.body });\n```\n\n## Follow-up Questions\n\n- How would you test idempotency guarantees across regions?\n- Which failure scenarios require compensating actions and how would you implement them?","diagram":"flowchart TD\n  A[Client Calls API] --> B[APIM Region A]\n  A --> C[APIM Region B]\n  B --> D[Durable Functions Orchestrator]\n  C --> D\n  D --> E[Cosmos DB per-Tenant in Residency Region]\n  D --> F[Event Grid for Tenant Events]\n  E --> G[Private Endpoints & Front Door]\n  F --> G","difficulty":"intermediate","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Oracle","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T15:37:44.877Z","createdAt":"2026-01-13T15:37:44.877Z"},{"id":"q-1539","question":"Design a real-time, multi-tenant analytics pipeline for a chat platform: events arrive at 5-10k msgs/sec total via Azure Event Hubs; process with Azure Databricks on Delta Lake stored in ADLS Gen2; governance via Unity Catalog; ensure strict per-tenant isolation, auditability, and exactly-once processing; TTL/retention; cross-region read; cost constraints. Describe architecture decisions, data layouts, and failure scenarios?","answer":"Proposed architecture: Ingest events from Azure Event Hubs into Delta Lake tables on ADLS Gen2 using Databricks Structured Streaming. Ensure exactly-once processing through idempotent writes leveraging unique event_id identifiers. Enforce strict tenant isolation by implementing dedicated schemas per tenant within Unity Catalog, complemented by row-level security using tenant_id columns. Configure TTL and retention through Delta Lake's OPTIMIZE and VACUUM operations, enable cross-region reads via Azure's geo-redundant storage, and optimize costs with Databricks autoscaling and spot instances. Implement comprehensive audit logging through Unity Catalog's access controls and Delta Lake's transaction logs.","explanation":"## Why This Is Asked\nThis tests knowledge of real-time data pipelines, Azure Databricks, and data governance in multi-tenant contexts.\n\n## Key Concepts\n- Durable ingestion and exactly-once processing with Structured Streaming and Delta Lake\n- Unity Catalog RBAC and per-tenant isolation (tenant_id, dedicated schemas)\n- Row-level security and auditability with Unity Catalog logs\n- TTL/retention, Vacuum, and cost-aware autoscaling\n\n## Code Example\n```sql\nMERGE INTO delta_table AS target\nUSING staging AS source\nON target.event_id = source.event_id\nWHEN MATCHED THEN UPDATE SET *\nWHEN NOT MATCHED THEN INSERT *\n```","diagram":"flowchart TD\n  A[Event Hubs] --> B[Databricks Structured Streaming]\n  B --> C[Delta Lake (ADLS Gen2)]\n  C --> D[Unity Catalog Governance]\n  D --> E[RBAC/RLS per Tenant]\n  C --> F[Audit Logs]\n  F --> G[TTL & Vacuum]","difficulty":"advanced","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Discord","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T06:32:48.345Z","createdAt":"2026-01-13T20:54:39.547Z"},{"id":"q-1565","question":"Design a zero-trust, per-tenant access model to a private Azure SQL Database from a microservices mesh. Use Azure AD, Managed Identities, Row-Level Security, and dynamic data masking. Explain how you enforce least privilege, tenant isolation, auditing, and how you validate access policies in CI/CD?","answer":"Implement Row-Level Security (RLS) on Azure SQL with a predicate function that returns TRUE when @TenantId = USER_CONTEXT(N'TenantId'); map each service identity to a SQL user restricted to its tenant's rows. Enable dynamic data masking on sensitive columns, configure Azure AD authentication with Managed Identities, and set up SQL Database auditing to log all access attempts. Enforce least privilege by granting only necessary permissions to each service identity and validate access policies through automated tests in CI/CD pipelines.","explanation":"## Why This Is Asked\nTests ability to implement per-tenant isolation, security controls, and CI/CD validation in a real Azure SQL setup.\n\n## Key Concepts\n- Zero-trust with per-tenant isolation\n- Row-Level Security and USER_CONTEXT()\n- Managed Identities and least privilege\n- Dynamic data masking and SQL auditing\n- CI/CD policy checks and automated tests\n\n## Code Example\n```sql\nCREATE FUNCTION dbo.fnTenantPredicate(@TenantId int)\nRETURNS TABLE\nWITH SCHEMABINDING\nAS RETURN SELECT 1 AS access WHERE @TenantId = CAST(USER_CONTEXT(N'TenantId') AS int);\n\nCREATE SECURITY POLICY dbo.TenantPolicy\nADD FILTER PREDICATE dbo.fnTenantPredicate(TenantId) ON dbo.YourTable,\nADD BLOCK PREDICATE dbo.fnTenantPredicate(TenantId) ON dbo.YourTable;\n```","diagram":"flowchart TD\n  A[Service] --> B[Managed Identity]\n  B --> C[Azure SQL with RLS]\n  C --> D[Audit Log]\n  C --> E[Masked Columns]","difficulty":"intermediate","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Google","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T06:18:28.116Z","createdAt":"2026-01-13T21:50:51.430Z"},{"id":"q-1594","question":"You're building a real-time, multi-tenant feature-flag platform on Azure to serve traffic across three regions with sub-50ms evaluation latency. Each tenant has per-flag rules, canary/A/B experiments, and strict audit requirements. Outline end-to-end design: data model for flags and experiments, evaluation path, storage choices (Cosmos DB vs SQL), caching, event sourcing, cross-region synchronization, security (Managed Identities, Key Vault, RBAC), rollback strategy, and failure modes. Include how you'd test canary safety and ensure tenant isolation?","answer":"Two-tier architecture: fast in-region evaluation with central governance. Store versioned flags in Cosmos DB with multi-region replication; cache per-region in Redis Enterprise; evaluate via deterministic hash-based routing for canary experiments.","explanation":"## Why This Is Asked\nTests ability to design low-latency, multi-tenant feature-flag systems on Azure, integrating multiple services and making informed trade-offs.\n\n## Key Concepts\n- Low-latency, region-local evaluation path\n- Multi-region replication and canary experiments\n- Versioned data model for flags/experiments\n- Secure access via Managed Identities and Key Vault\n- Auditability with Event Hubs to ADLS Gen2\n\n## Code Example\n```javascript\nfunction selectFlag(userId, tenantId, flagVersion, canary) { \n  const key = `${tenantId}:${userId}:${flagVersion}`;\n  const bucket = hash(key) % 100;\n```","diagram":"flowchart TD\n A[Client Request] --> B[API Gateway]\n B --> C[Region Redis Cache]\n C --> D[Flag Service (Cosmos DB)]\n D --> E[Evaluation Engine]\n E --> F[Response]","difficulty":"advanced","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","DoorDash"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T05:34:33.777Z","createdAt":"2026-01-13T23:29:11.837Z"},{"id":"q-1620","question":"You're architecting a **multi-region telemetry ingestion** pipeline for a real-time fraud-detection service. Edge devices per region push JSON events to **Azure IoT Hub**; you must ingest, partition by tenant, store raw and processed results with exact-once semantics, and enforce strict per-tenant isolation and auditability within tight cost constraints. Propose architecture choices (IoT Hub, **Event Hubs**, **Delta Lake**, **Unity Catalog**, **Cosmos DB**), data layout, and failure modes; how would you test end-to-end dedup and cross-region replayability?","answer":"Use a shared Event Hub with tenantId as the partition key; cross-region replication via paired namespaces; Spark Structured Streaming into Delta Lake with transactionId dedup for exactly-once; store raw events in Delta Lake with tenant-specific paths, processed results in Cosmos DB with tenant isolation, and enforce governance through Unity Catalog with row-level security.","explanation":"Why This Is Asked\n- Tests end-to-end thinking for cross-region telemetry pipelines with strict isolation, auditability, and exactly-once guarantees.\n- Evaluates choice of Azure services, data layouts, and governance patterns under cost constraints.\n\nKey Concepts\n- Tenant isolation and data governance in a shared data plane\n- Exactly-once processing across streaming and lakehouse layers\n- Partitioning strategy and cross-region replication\n- Durable deduplication and auditing mechanisms\n\nCode Example\n```python\n# PySpark pseudo-code for deduplication in a streaming Delta Lake sink\nfrom pyspark.sql.functions import col\nfrom delta.tables import DeltaTable\n\ndef dedup_streaming_batch(df, batch_id):\n    delta_table = DeltaTable.forPath(spark, \"/raw/telemetry\")\n    (delta_table.alias(\"target\")\n     .merge(df.alias(\"source\"), \"source.transactionId = target.transactionId\")\n     .whenNotMatchedInsertAll()\n     .execute())\n```","diagram":null,"difficulty":"advanced","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Hugging Face"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T04:02:16.999Z","createdAt":"2026-01-14T02:45:10.285Z"},{"id":"q-1668","question":"You’re building a beginner Azure Function (HTTP trigger) that calls a 3rd‑party API. Do not store API keys in code or app settings. How would you securely fetch the API key at runtime from Azure Key Vault using a managed identity? Outline the steps to enable the identity, grant access, and provide a minimal code snippet to read the secret?","answer":"Enable the Function App's system-assigned managed identity and grant it Key Vault Secrets Reader on the vault. In code, read the secret with DefaultAzureCredential and SecretClient, then use it in the","explanation":"## Why This Is Asked\nThis tests practical credential management in Azure: using managed identities, granting least privilege, and runtime secret retrieval to avoid embedding keys.\n\n## Key Concepts\n- Managed Identity (system-assigned)\n- Azure Key Vault Secrets Reader\n- Azure.Identity DefaultAzureCredential\n- Azure.Security.KeyVault.Secrets SecretClient\n- Avoiding secret provisioning in config\n\n## Code Example\n```csharp\nusing Azure.Identity;\nusing Azure.Security.KeyVault.Secrets;\nvar cred = new DefaultAzureCredential();\nvar client = new SecretClient(new Uri(\"https://<vault>.vault.azure.net/\"), cred);\nvar apiKey = (await client.GetSecretAsync(\"ApiKey\")).Value.Value;\n```\n\n## Follow-up Questions\n- How would you handle secret rotation and cache expiration?\n- How would you secure the Key Vault access policy for multiple environments (dev/stage/prod)?","diagram":null,"difficulty":"beginner","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T05:51:55.453Z","createdAt":"2026-01-14T05:51:55.453Z"},{"id":"q-1727","question":"You're building a polyglot Azure-based service with HTTP APIs, background workers, and a data lake. You need end-to-end tracing across Functions, AKS, and Databricks jobs using OpenTelemetry and a single trace across services. Describe how you'd implement tracing, propagate context, and collect/export to Azure Monitor, including sampling strategy and validation steps?","answer":"Instrument Functions, AKS, and Databricks with OpenTelemetry. Use W3C TraceContext propagation; carry traceparent across HTTP, queues, and Delta Lake jobs. Export spans via OTLP to Azure Monitor (Log ","explanation":"## Why This Is Asked\nEvaluate ability to implement observability across heterogeneous runtimes in Azure, with end-to-end tracing and cross-service propagation.\n\n## Key Concepts\n- OpenTelemetry across Functions, AKS, Databricks\n- W3C TraceContext; OTLP exporting to Azure Monitor\n- Context propagation through queues and Delta Lake\n- Sampling strategies and trace retention\n\n## Code Example\n```javascript\nconst { trace, context, propagation } = require('@opentelemetry/api');\nconst { NodeTracerProvider } = require('@opentelemetry/sdk-trace-node');\nconst { SimpleSpanProcessor } = require('@opentelemetry/sdk-trace-base');\nconst { OTLPTraceExporter } = require('@opentelemetry/exporter-otlp-http');\n\nconst provider = new NodeTracerProvider();\nconst exporter = new OTLPTraceExporter({ url: 'https://<region>.monitor.azure.com/v1/traces' });\nprovider.addSpanProcessor(new SimpleSpanProcessor(exporter));\nprovider.register();\n\nmodule.exports = async function (context, req) {\n  const tracer = trace.getTracer('example-function');\n  const span = tracer.startSpan('http-request');\n  // downstream calls propagate context automatically when using OpenTelemetry API\n  span.end();\n  return { status: 200, body: 'OK' };\n};\n```\n\n## Follow-up Questions\n- How would you test end-to-end traces across services?\n- How would you adjust sampling in production without outages?","diagram":"flowchart TD\n  A[Azure Function] --> B[OpenTelemetry Span]\n  B --> C[OTLP Exporter]\n  A --> D[AKS services]\n  D --> E[Databricks jobs]","difficulty":"intermediate","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Meta","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T08:46:16.131Z","createdAt":"2026-01-14T08:46:16.131Z"},{"id":"q-1791","question":"You're building an Azure-native, multi-tenant data platform for real-time payments used by PayPal and Tesla. Ingest via Event Hubs, process with AKS and Functions, store in Delta Lake on ADLS Gen2, expose a data API. How would you enforce strict per-tenant isolation, achieve end-to-end exactly-once semantics across services, and implement a shadow-traffic ML model deployment with safe rollback and audit trails? Include architecture choices, data layouts, and failure modes?","answer":"Per-tenant isolation via Delta Lake databases per tenant managed by Unity Catalog with VNet-protected ADLS Gen2; exactly-once via outbox pattern: commit row then deduplicated sink to Delta Lake, struc","explanation":"## Why This Is Asked\nInterview context explanation.\n\n## Key Concepts\n- Per-tenant isolation with Delta Lake, Unity Catalog, and network controls\n- Exactly-once semantics via outbox + idempotent sinks + structured streaming\n- Shadow ML deployment with traffic-splitting and rollback\n- Governance with Purview lineage\n\n## Code Example\n```javascript\n// Pseudo outbox transaction\nasync function handleEvent(evt) {\n  await db.insert({table: 'outbox', ...evt});\n  await writeToDeltaLake(evt); // idempotent\n}\n```\n\n## Follow-up Questions\n- How would you validate end-to-end exactly-once guarantees?\n- How would you enforce tenant RBAC across all data paths?","diagram":"flowchart TD\nA[Event Hubs intake] --> B[AKS/Functions processing]\nB --> C[Delta Lake per tenant] --> D[Data API]\nD --> E[Purview lineage]\nF[Azure ML Shadow Model] --> G[Traffic split 5-10%]\nG --> H[Latency/Accuracy metrics]\nH --> I[Promote or rollback]","difficulty":"advanced","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["PayPal","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T10:52:31.948Z","createdAt":"2026-01-14T10:52:31.948Z"},{"id":"q-1839","question":"You're building an Azure IoT telemetry platform for a global fleet. Devices send 20–30k events/sec to IoT Hub. You need per-tenant isolation, real-time enrichment (geolocation, device type), and fan-out to three sinks: Delta Lake on ADLS Gen2, Azure Data Explorer dashboards, and an AI inference service on AKS. Must guarantee at-least-once semantics, handle out-of-order data, and support cross-region DR with measurable RPO. Compare two architectures: (A) serverless micro-batch: IoT Hub → Event Hubs → Functions for lightweight enrichment; Spark Structured Streaming writes to Delta Lake on ADLS Gen2; sinks feed Data Explorer and AKS inference. (B) pure streaming: Event Hubs → Spark Structured Streaming with larger cluster, stricter SLAs, and end-to-end exactly-once semantics. Explain data models, failure modes, and governance?","answer":"Two architectures: (A) micro-batch: IoT Hub → Event Hubs → Functions for lightweight enrichment; Spark Structured Streaming to Delta Lake on ADLS Gen2; feeds to Data Explorer and AKS inference; per-te","explanation":"## Why This Is Asked\nTests the ability to design scalable IoT pipelines with per-tenant isolation and multi-sink consumption. It also probes idempotent processing, dedup, and cross-region DR.\n\n## Key Concepts\n- IoT Hub, Event Hubs, and Spark Structured Streaming\n- Delta Lake on ADLS Gen2, Azure Data Explorer, AKS inference\n- Per-tenant isolation via managed identities and resource tagging\n- Exactly-once vs at-least-once semantics, dedup, watermarking\n- DR and RPO considerations\n\n## Code Example\n```javascript\n// Example foreachBatch to ensure idempotent writes\nfunction foreachBatch(batch, batchId) {\n  const dedup = batch.dropDuplicates(['event_id'])\n  dedup.write.format('delta').mode('append').save('/mnt/delta/events')\n}\n```\n\n## Follow-up Questions\n- How to handle out of order events? \n- How would you implement per-tenant data segregation in storage and compute?","diagram":"flowchart TD\n  A[IoT Hub Ingest] --> B[Event Hubs]\n  B --> C[Enrichment (Functions / Spark)]\n  C --> D1[Delta Lake (ADLS Gen2)]\n  C --> D2[Azure Data Explorer]\n  C --> D3[AKS Inference Service]\n  D1 --> E[Cross-region DR]\n  D2 --> E\n  D3 --> E","difficulty":"intermediate","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T13:24:17.847Z","createdAt":"2026-01-14T13:24:17.847Z"},{"id":"q-1962","question":"You’re building a beginner Azure Function (HTTP trigger) that receives event payloads and stores them in a data container. Design a lightweight, auditable approach to log every write without slowing latency. Include how you would generate an immutable audit trail and what storage pattern you’d use. Provide a minimal code snippet to append an audit line with timestamp, eventId, and userId derived from Authorization header?","answer":"Use an HTTP-triggered Function on the Consumption plan that writes to a data container and appends a separate line to an immutable audit blob in the same storage account. Extract userId from a JWT in ","explanation":"## Why This Is Asked\nTests the ability to design lightweight auditing with low latency using Append Blobs and JWT parsing; demonstrates immutability, data-audit separation, and basic error handling.\n\n## Key Concepts\n- Append Blob usage for immutable, append-only logs\n- JWT payload extraction from Authorization header\n- Per-event audit entries with timestamp and eventId\n- Resilient writes with basic retry/backoff\n\n## Code Example\n```javascript\n// Minimal Azure Function snippet (TypeScript)\nimport { ContainerClient, AppendBlobClient } from \"@azure/storage-blob\";\nconst containerClient = new ContainerClient(\"<storage-url>\", new DefaultAzureCredential());\nexport async function run(context, req){\n  const auth = req.headers[\"authorization\"] ?? \"\";\n  const token = auth.split(\" \")[1] ?? \"\";\n  const payload = JSON.parse(Buffer.from(token.split(\".\")[1], 'base64').toString());\n  const userId = payload?.sub ?? \"anonymous\";\n  const eventId = req.body?.eventId ?? crypto.randomUUID();\n  const line = JSON.stringify({ ts: new Date().toISOString(), eventId, userId, status: 'received' }) + \"\\n\";\n  const audit = containerClient.getAppendBlobClient(\"audit.log\");\n  await audit.appendBlock(line, line.length);\n  context.res = { status: 200, body: { eventId } };\n}\n```\n\n## Follow-up Questions\n- How would you extend this to handle audit log rotation or retention?\n- How would you enforce per-tenant isolation for audits?\n","diagram":"flowchart TD\nA[HTTP Trigger] --> B[Write Data Blob]\nA --> C[Append Audit Log]\nC --> D[Audit blob in same storage account]\nB --> E[Return eventId]","difficulty":"beginner","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T18:56:50.374Z","createdAt":"2026-01-14T18:56:50.374Z"},{"id":"q-1995","question":"Describe how you would implement a timer-triggered Azure Function that runs every 15 minutes to poll an on‑prem REST endpoint and write a daily aggregation blob to Azure Blob Storage. How would you guarantee idempotent writes to avoid duplicates across retries, including blob naming strategy and a minimal check-then-write code pattern?","answer":"Use a TimerTrigger Azure Function (every 15 minutes) to fetch 15‑minute metrics, compute a window key like 20260114-0915, and write a single blob named agg-20260114-0915.json. Before writing, check fo","explanation":"## Why This Is Asked\nThis probes practical use of Timer triggers, external data polling, and robust idempotent writes to object storage, a common beginner scenario with real-world reliability concerns.\n\n## Key Concepts\n- Timer-triggered functions and external REST calls\n- Idempotent writes and atomic blob operations\n- Blob naming for time-windowed aggregates\n- Concurrency, retries, and race condition handling\n\n## Code Example\n```csharp\nvar container = blobServiceClient.GetBlobContainerClient(\"metrics\");\nawait container.CreateIfNotExistsAsync();\nstring blobName = $\"agg-{start:yyyyMMdd-HHmm}.json\";\nBlobClient blob = container.GetBlobClient(blobName);\nif (!await blob.ExistsAsync())\n{\n    using var stream = new MemoryStream(Encoding.UTF8.GetBytes(payload));\n    await blob.UploadAsync(stream, new BlobHttpHeaders { ContentType = \"application/json\" });\n}\n```\n\n## Follow-up Questions\n- How would you handle idempotency if multiple function instances run concurrently?\n- How would you ensure correct window alignment across time zones and daylight saving changes?","diagram":null,"difficulty":"beginner","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Plaid","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T19:50:29.731Z","createdAt":"2026-01-14T19:50:29.731Z"},{"id":"q-2059","question":"Design an Azure-native data export service for a fintech that must export per-customer data on demand and on a schedule, with strict data residency, consent checks, and an audit trail. Use ADLS Gen2, Cosmos DB, Event Grid, Durable Functions or Functions, and Key Vault. Explain data partitioning, encryption, access control, idempotency, and failure modes including retries, outages, and rollback. Provide a concrete data model and flow?","answer":"A Durable Functions orchestrator triggers per-customer export jobs after validating consent in Cosmos DB. Data is read from source tables, streamed to ADLS Gen2 under tenant-specific prefixes, encrypted with customer-managed keys from Key Vault, and logged to an immutable audit trail. The system ensures idempotency through request deduplication and status tracking, handles failures with exponential backoff and circuit breakers, and maintains data residency through region-specific deployments.","explanation":"## Why This Is Asked\nThis question evaluates real-world enterprise concerns: per-customer data exports, consent management, data residency compliance, auditability, and failure handling within Azure-native architectures.\n\n## Key Concepts\n- Durable Functions orchestration patterns and idempotency\n- Customer-managed keys (CMK) with Azure Key Vault\n- Consent management and compliance workflows\n- Immutable audit logging and cross-region replication\n- Tenant-based data partitioning and AAD access control\n\n## Code Example\n```javascript\n// Pseudo: orchestrator outline\n```\n\n## Follow-up Questions\n- How would you handle large-scale concurrent exports?\n- What monitoring and alerting strategies would you implement?\n- How do you ensure data consistency during partial failures?\n- What strategies would you use for cost optimization?","diagram":"flowchart TD\n  A[Request export] --> B{Consent?}\n  B -- Yes --> C[Orchestrator (Durable Functions)]\n  C --> D[Read Source Data]\n  D --> E[Write to ADLS Gen2 (tenant prefix)]\n  E --> F[Encrypt with CMK (Key Vault)]\n  F --> G[Audit in Cosmos + immutable logs]\n  B --> H[Handle Denial]\n  G --> I[Retry/rollback]","difficulty":"intermediate","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Robinhood","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T05:50:43.792Z","createdAt":"2026-01-14T22:47:07.627Z"},{"id":"q-2109","question":"You're building a real-time moderation pipeline for a global social app on Azure. Ingest flows via Azure Event Hubs at 20-40k msgs/sec, then you apply NLP classification in a chain of Functions (or Durable Functions) to flag policy-violating messages, store results in Cosmos DB with per-tenant isolation, and index metadata in Azure Cognitive Search for fast queries. How would you design for latency under 150ms per event, strict per-tenant data isolation, idempotent retries, and auditable trails across services? Include error handling and rollback strategies?","answer":"Leverage Azure Event Hubs with Durable Functions orchestrator to sequence the pipeline: classify, persist, and index. Partition Cosmos DB by tenant for strict data isolation, and create per-tenant Azure Cognitive Search indexes. Implement messageId-based idempotency and establish cross-service error handling with dead-letter queues for failed events.","explanation":"## Why This Is Asked\nTests practical multi-service orchestration, data isolation, and end-to-end reliability in Azure cloud environments.\n\n## Key Concepts\n- Durable Functions orchestrations for reliable workflow management\n- Idempotent writes using messageId-based deduplication\n- Per-tenant partitioning for strict data isolation\n- Cross-service error handling with dead-letter queues\n- Event-driven architecture with proper retry mechanisms\n\n## Code Example\n```javascript\n// Azure Function snippet: upsertCosmos(message)\nconst { CosmosClient } = require('@azure/cosmos');\nconst client = new CosmosClient(process.env.COSMOS_ENDPOINT);\n```","diagram":"flowchart TD\n  EH[Event Hubs] --> OF[Durable Functions orchestrator]\n  OF --> CD[Cosmos DB (Tenant partition key)]\n  OF --> CS[Search Index per tenant]\n  CD --> AUD[Audit log]","difficulty":"intermediate","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Amazon","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T04:58:58.598Z","createdAt":"2026-01-15T02:21:13.266Z"},{"id":"q-2155","question":"You're building a tenant-aware API gateway on Azure that serves dozens of microservices for hundreds of tenants. Implement per-tenant quotas, latency budgets, and canary rollouts. Describe a concrete architecture using Azure API Management, Azure Front Door, Redis for rate-limiting, and per-tenant state in Cosmos DB or Redis; explain failure modes, rollback, and testing strategies under traffic spikes?","answer":"Implement a tenant-aware API gateway using Azure API Management with a rate-limit-by-key policy keyed on tenant-id, backed by Redis token-bucket counters and Azure Front Door for global routing. Persi","explanation":"## Why This Is Asked\n\nAssesses ability to design a robust, Azure-native API gateway handling per-tenant isolation, dynamic canary deployments, and precise quota controls under load.\n\n## Key Concepts\n\n- Tenant isolation in API Management with per-tenant rate limits\n- Redis-based token-bucket or sliding-window throttling\n- Canary deployments via APIM revisions and feature flags\n- Global routing with Azure Front Door\n- Observability and audit trails in Cosmos DB and Application Insights\n\n## Code Example\n\n```xml\n<policies>\n  <inbound>\n    <rate-limit-by-key calls=\"1000\" renewal-period=\"60\" counter-key=\"@(context.Variables.GetValueOrDefault('tenantId','anonymous'))\" />\n  </inbound>\n</policies>\n```\n\n```lua\n-- Redis Lua example (tenant-scoped bucket)\nlocal key = KEYS[1]\nlocal now = tonumber(ARGV[1])\nlocal capacity = tonumber(ARGV[2])\nlocal refill = tonumber(ARGV[3])\nlocal tokens = tonumber(redis.call('GET', key) or capacity)\nif tokens <= 0 then\n  return {err=\"rate-limited\"}\nelse\n  redis.call('SET', key, tokens-1, 'EX', 60)\n  return {ok, tokens-1}\nend\n```\n\n## Follow-up Questions\n\n- How would you test race conditions in quota counters under high concurrency?\n- How would you monitor and alert on tenant quota breaches without noisy alerts?","diagram":"flowchart TD\n  Client(Client) --> FrontDoor(Azure Front Door)\n  FrontDoor --> APIM(API Management)\n  APIM --> Redis(Redis rate limiter per tenant)\n  Redis --> Backend[Backend microservices]\n  APIM --> CosmosAudit(Audit & events store)\n","difficulty":"intermediate","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T05:36:19.391Z","createdAt":"2026-01-15T05:36:19.392Z"},{"id":"q-2176","question":"You're building a beginner Azure-based image processing workflow: a user uploads a photo to Azure Blob Storage. A Function App should trigger, resize the image into three thumbnails, store metadata in Cosmos DB with per-user partition keys, and publish a status message to Azure Service Bus. How would you implement idempotent processing, retries, and per-user isolation, with a minimal code sketch showing how to deduplicate on blob name and initiate the three resizes?","answer":"Use a Blob-triggered Durable Function orchestrator. Derive imageId from blob name for dedup. Check Cosmos DB for existing record before upsert; upsert with partition key userId and id=imageId to ensur","explanation":"## Why This Is Asked\nTests understanding of idempotence, per-user isolation, and basic orchestration in Azure Functions using Durable Functions.\n\n## Key Concepts\n- Durable Functions orchestrator and fan-out/fan-in\n- Idempotent processing using a stable id (blob name)\n- Cosmos DB upsert with per-user partitioning\n- Service Bus message deduplication via MessageId\n\n## Code Example\n```javascript\n// Pseudo-durable orchestrator sketch\nconst df = require('durable-functions');\nmodule.exports = df.app();\nasync function orchestrator(context) {\n  const blobName = context.bindingData.name;\n  const imageId = blobName; // dedupe key\n  // check existing in Cosmos, then upsert\n  // fan-out to Resize activities\n  const outputs = await context.df.Task.all([\n    context.df.callActivity('ResizeA', {imageId}),\n    context.df.callActivity('ResizeB', {imageId}),\n    context.df.callActivity('ResizeC', {imageId})\n  ]);\n  await context.df.callActivity('UpsertMetadata', {imageId, userId: context.bindingData.userId});\n  await context.df.callActivity('PublishStatus', {imageId});\n  return outputs;\n}\n```\n\n## Follow-up Questions\n- How would you handle partial failures during the fan-out?\n- How would you test idempotency across replays without corrupting data?","diagram":"flowchart TD\n  A[Blob Upload] --> B[Blob Trigger Function]\n  B --> C[Durable Orchestrator]\n  C --> D[ResizeA]\n  C --> E[ResizeB]\n  C --> F[ResizeC]\n  D --> G[Cosmos Upsert]\n  E --> G\n  F --> G\n  G --> H[Publish to Service Bus]","difficulty":"beginner","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Cloudflare"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T06:45:41.190Z","createdAt":"2026-01-15T06:45:41.190Z"},{"id":"q-2203","question":"Design a privacy-preserving, auditable streaming pipeline for real-time telemetry in a multi-tenant SaaS on Azure. Ingest via Azure Event Hubs at ~60k msgs/sec; run per-tenant aggregation and anomaly detection in Durable Functions; store per-tenant data in Cosmos DB with strict isolation; enforce data residency per tenant region and provide rollback for feature-flag changes. Describe architecture, data model, exactly-once guarantees, audit trails, and failure modes?","answer":"Leverage Event Hubs intake per tenant (60k msgs/sec), dedupe by (tenantId,eventId), and orchestrate with Durable Functions. Persist to Cosmos DB with tenant-scoped partitions and region-bound accounts","explanation":"## Why This Is Asked\nTests ability to design a compliant, scalable, and auditable streaming pipeline across Azure services with strict tenancy, data residency, and rollback needs.\n\n## Key Concepts\n- Event deduplication using tenantId and eventId\n- Exactly-once processing via Durable Functions and durable state\n- Tenant-scoped Cosmos partitions and region-bound accounts for residency\n- Tamper-evident logging and versioned rollback strategy\n\n## Code Example\n```javascript\n// Pseudocode: dedupe then persist per tenant\nasync function handleEvent(ctx) {\n  const {tenantId, eventId} = ctx.bindingData;\n  const exists = await cosmos.read(`dedup/${tenantId}/${eventId}`);\n  if (exists) return;\n  await cosmos.upsert(`dedup/${tenantId}/${eventId}`, true);\n  // write main event data to tenant partition\n}\n```\n\n## Follow-up Questions\n- How would you verify exactly-once semantics under backpressure?\n- How would you validate data residency and audit trails across tenants?","diagram":"flowchart TD\n  A[Event Ingest (Event Hubs)] --> B[Dedup / Route]\n  B --> C[Durable Functions Orchestrator]\n  C --> D[Cosmos DB (tenant partitions)]\n  D --> E[Audit / Dashboards (immutable storage)]","difficulty":"advanced","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Plaid","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T07:34:40.634Z","createdAt":"2026-01-15T07:34:40.634Z"},{"id":"q-2235","question":"Design a real-time, multi-tenant feature-flag platform on Azure for a geo-distributed microservices stack. Tenant isolation must be enforced; flag evaluation latency < 200 ms at peak. Use API Management, Functions, Cosmos DB (partitioned by tenant), Redis (near the API layer), and Event Grid. Describe data model, cache strategy, canary rollout, rollback plan, auditing, and failure handling?","answer":"Architect a global flag service with per-tenant Cosmos DB partitions and near-api Redis cache; evaluate by hashing tenantId+flagName to pick a shard, ensuring <200 ms latency. Invalidate cache on upda","explanation":"## Why This Is Asked\nTests ability to design scalable, compliant flag systems across regions with strict latency.\n\n## Key Concepts\n- Azure API Management, Functions, Cosmos DB per-tenant partitioning\n- Redis caching and cache invalidation\n- Canary rollouts, versioning, Event Grid\n- Observability and failure handling\n\n## Code Example\n```javascript\n// Flag evaluation sketch\nasync function getFlag(tenantId, flagName) {\n  const cacheKey = `${tenantId}:${flagName}`;\n  let value = await redis.get(cacheKey);\n  if (value != null) return JSON.parse(value);\n  const flagDoc = await cosmos.Flags.findOne({ tenantId, flagName });\n  value = flagDoc?.value ?? false;\n  await redis.set(cacheKey, JSON.stringify(value), { ttl: 60 });\n  return value;\n}\n```\n\n## Follow-up Questions\n- How would you validate latency across regions?\n- How would you handle cache misses and partial failures?","diagram":"flowchart TD\n  A[Client Request] --> B[API Management]\n  B --> C[Flag Eval Function]\n  C --> D[Cosmos DB (per-tenant)]\n  D --> E[Redis Cache Near API]\n  E --> F[Flag Value]","difficulty":"advanced","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Stripe","Uber","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T08:51:32.233Z","createdAt":"2026-01-15T08:51:32.233Z"},{"id":"q-2307","question":"You're building a privacy-preserving analytics marketplace on Azure: ingest telemetry via Event Hubs; anonymize with differential privacy during ingestion or enrichment; store tenant-scoped data in Delta Lake on ADLS Gen2 with strict per-tenant partitioning; catalog lineage in Azure Purview; and share results through per-tenant REST APIs with RBAC and data-sharing controls. Outline the architecture, data flow, and trade-offs to meet privacy, latency, and cost targets?","answer":"Implement ingestion via Event Hubs, perform DP during or after enrichment, store tenant-scoped data in Delta Lake on ADLS Gen2 with partitions per tenant, catalog lineage in Purview, and expose per-te","explanation":"## Why This Is Asked\nAssesses ability to design privacy-aware analytics with governance, cross-service data flow, and cost controls at scale.\n\n## Key Concepts\n- Differential privacy and when to apply it\n- Delta Lake partitioning and tenant isolation\n- Azure Purview for catalog and lineage\n- RBAC, API Management, and per-tenant data sharing\n- Consent management and auditing\n\n## Code Example\n```javascript\n// Pseudocode: simple DP-style anonymization before write\nfunction applyDP(record, epsilon=1.0){\n  const noise = laplace(0, 1/epsilon);\n  record.value += noise;\n  return record;\n}\n```\n\n## Follow-up Questions\n- How would you validate DP parameters to meet regulatory requirements?\n- How would you implement dynamic masking and tenant-aware data sharing budgets?","diagram":"flowchart TD\n  Ingest[Event Hubs] --> Anon[Differential Privacy]\n  Anon --> Store[Delta Lake (ADLS Gen2), tenant-partitioned]\n  Store --> Catalog[Azure Purview]\n  Catalog --> API[Per-tenant APIs (RBAC)]\n  API --> Monitor[Azure Monitor / OpenTelemetry]","difficulty":"advanced","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Oracle","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T11:35:53.627Z","createdAt":"2026-01-15T11:35:53.627Z"},{"id":"q-2379","question":"Build a beginner Azure REST API using Azure Functions (HTTP trigger) to manage product data for multiple tenants. Each tenant must be isolated, configs sourced from Azure App Configuration, and telemetry sent to Application Insights. Describe the authentication model, per-tenant data isolation strategy, and a minimal test plan, plus a small code sketch showing API key validation and tenant extraction?","answer":"Use an HTTP-triggered Function that reads x-api-key and tenantId headers. Validate the key against a value from Azure App Configuration via a Managed Identity. Use Cosmos DB with partitionKey = tenant","explanation":"## Why This Is Asked\n\nThis checks practical Azure capabilities: serverless API, tenant isolation, config via App Configuration, and observability.\n\n## Key Concepts\n\n- HTTP-triggered Azure Functions\n- Azure App Configuration\n- Managed Identity\n- Cosmos DB partitioning\n- Application Insights telemetry\n\n## Code Example\n\n```javascript\n// Pseudo-code: read headers and validate API key against App Configuration\n```\n\n## Follow-up Questions\n\n- How would you rotate API keys and invalidate old ones without downtime?\n- How would you test multi-tenant isolation end-to-end?","diagram":"flowchart TD\n  A[Client] --> B[Azure Function HTTP]\n  B --> C[App Configuration]\n  B --> D[Cosmos DB (tenantId partition)]\n  B --> E[Application Insights]","difficulty":"beginner","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Hashicorp","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T15:44:09.087Z","createdAt":"2026-01-15T15:44:09.087Z"},{"id":"q-2484","question":"Design a multi-tenant telemetry pipeline on Azure: events arrive through Event Hubs per-tenant, processed by Durable Functions, stored in per-tenant Cosmos DB, and surfaced via Azure Data Explorer dashboards. How would you implement end-to-end tracing with OpenTelemetry across all components, ensure per-tenant correlation, minimize overhead, handle retries idempotently, and preserve privacy controls (pseudonymization, access controls)?","answer":"Implement OpenTelemetry across Event Hubs, Durable Functions, Cosmos DB, and ADX; propagate traceparent/tracestate and tenant_id in message properties; route traces to Application Insights and data ex","explanation":"## Why This Is Asked\nAssess distributed tracing across serverless, event-driven Azure services; evaluate OpenTelemetry integration, trace propagation, and tenant-scoped correlation; inspect handling of retries, idempotency, and privacy controls.\n\n## Key Concepts\n- OpenTelemetry across Event Hubs, Durable Functions, Cosmos DB, ADX\n- Trace context propagation (traceparent/tracestate)\n- Tenant isolation and RBAC, per-tenant partitioning\n- Privacy controls and sampling\n\n## Code Example\n```javascript\n// Startup: initialize OTLP exporter and tracer\nimport { NodeTracerProvider } from '@opentelemetry/sdk-trace-node';\nimport { SimpleSpanProcessor } from '@opentelemetry/sdk-trace-base';\nimport { OTLPTraceExporter } from '@opentelemetry/exporter-otlp-http';\nconst provider = new NodeTracerProvider();\nprovider.addSpanProcessor(new SimpleSpanProcessor(new OTLPTraceExporter({url:'https://collector.example/otlp'})));\nprovider.register();\n```\n\n## Follow-up Questions\n- How would you verify end-to-end trace availability under partial failures?\n- How would you enforce per-tenant sampling rates and preserve privacy?","diagram":"flowchart TD\n  A[Event Hubs] --> B[Durable Functions]\n  B --> C[Cosmos DB]\n  A --> D[OpenTelemetry Collector]\n  C --> E[Azure Data Explorer]","difficulty":"intermediate","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Apple","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T19:46:22.442Z","createdAt":"2026-01-15T19:46:22.442Z"},{"id":"q-2679","question":"Build a real-time fraud scoring pipeline on Azure for a multi-tenant fintech. Ingest 60k msgs/sec via Event Hubs, run scoring in Azure ML, store per-tenant results in Cosmos DB with region-bound writes, and publish to downstream systems. How would you enforce per-tenant data residency, hit sub-200ms latency, support hot model updates, and ensure end-to-end auditability and replay safety? Include regional failover and RBAC?","answer":"Leverage regional Event Hubs with per-tenant Cosmos DB partitions, a Durable Functions orchestrator for end-to-end flow, and idempotent writes to ensure replay safety. Route traffic to the nearest reg","explanation":"## Why This Is Asked\n\nTests ability to design for data residency, latency, model updates, and auditable traces in Azure.\n\n## Key Concepts\n\n- Multi-region data residency\n- Durable Functions orchestration\n- Idempotency and replay safety\n- Canary model updates and feature flags\n- OpenTelemetry tracing and RBAC\n\n## Code Example\n\n```python\n# Pseudo Durable Functions orchestrator sketch (illustrative)\nfrom azure.durable_functions import Orchestrator, Task\ndef orchestrator(context: OrchestratorContext):\n    event = yield context.call_activity('IngestEvent', context.get_input())\n    score = yield context.call_activity('ScoreEvent', event)\n    yield context.call_activity('StoreResult', {'tenant': event['tenantId'], 'score': score})\n```\n\n## Follow-up Questions\n\n- How would you monitor drift between model versions?\n- How would you test region failover scenarios?","diagram":"flowchart TD\n  Ingest[Event Hubs - regional] --> Orchestrator[Durable Functions]\n  Orchestrator --> Score[Azure ML]\n  Score --> Store[Cosmos DB (per-tenant)]\n  Store --> Downstream[Downstream systems]\n  Store --> Audit[Audit trails (ADLS/Purview)]\n  Audit --> ModelUpdate[Canary model updates via CI/CD]","difficulty":"intermediate","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","OpenAI","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T06:52:30.476Z","createdAt":"2026-01-16T06:52:30.476Z"},{"id":"q-2724","question":"You're building a cross-cloud data onboarding and sharing pipeline on Azure: per-tenant data lands in ADLS Gen2, governance is enforced via Purview with per-tenant contracts, then data is published to Snowflake (external stage) and Databricks (Delta Sharing) for analytics. How would you implement data contracts, isolation, lineage, access control, and cost accounting end-to-end, including failure modes?","answer":"Use Azure Purview to encode per-tenant contracts and lineage; land data in ADLS Gen2 with per-tenant partitions; publish to Snowflake via a private external stage and to Databricks via Delta Sharing; ","explanation":"## Why This Is Asked\nAssesses ability to design governance, isolation, and cross-cloud sharing at scale, including failure handling and cost accountability.\n\n## Key Concepts\n- Cross-cloud data onboarding\n- Per-tenant contracts and data contracts\n- Data lineage and governance with Purview\n- Cross-platform publishing (Snowflake external stage, Delta Sharing)\n- Access control via Azure AD and managed identities\n- Cost accounting and governance\n\n## Code Example\n```python\n# Pseudocode: register dataset in Purview and publish to Snowflake/Databricks\npurview.register_dataset(tenant_id, dataset_id, schema)\nsnowflake.publish_external_stage(tenant_id, dataset_id, source_path)\ndatabricks.delta_sharing.share(tenant_id, dataset_id, delta_table)\n```\n\n## Follow-up Questions\n- How would you validate data contracts during onboarding and monitor drift?\n- What are your failure modes and retry strategies across ADLS, Purview, Snowflake, and Databricks, including observability?","diagram":"flowchart TD\n  Ingest[Ingest to ADLS Gen2] --> Catalog[Purview governance & per-tenant contracts]\n  Catalog --> Snowflake[External stage to Snowflake]\n  Catalog --> Databricks[Delta Sharing to Databricks]\n  Access[Azure AD-based access control] --> Audit[Audit logs & lineage]","difficulty":"advanced","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Snowflake","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T09:37:34.809Z","createdAt":"2026-01-16T09:37:34.809Z"},{"id":"q-2796","question":"You're designing a globe-spanning telemetry pipeline for an industrial platform: devices push MQTT data to IoT Hub, events feed into Event Hubs at up to 200k msgs/min, Durable Functions compute windowed aggregates, and results are written to Cosmos DB with tenantId partition keys. How would you meet sub-200ms tail latency, strict per-tenant isolation, idempotent retries, cross-region reads, private endpoints, auditable traces, plus DR and cost controls?","answer":"Design a globe-spanning telemetry pipeline: MQTT to IoT Hub, events to Event Hubs, Durable Functions for windowed analytics, per-tenant Cosmos DB with tenantId partitioning. Target sub-200ms tails, st","explanation":"## Why This Is Asked\nThis question probes end-to-end latency, multi-region replication, data isolation, idempotency, observability, and cost.\n\n## Key Concepts\n- IoT Hub, Event Hubs, Durable Functions\n- Cosmos DB per-tenant partitioning and access control\n- Cross-region replication, private endpoints, auditing\n- Disaster recovery and cost optimization\n\n## Code Example\n```javascript\n// Pseudo: deduplicate by messageId in Durable Function\nasync function process(event) {\n  const key = event.messageId;\n  if (await storage.exists(key)) return;\n  await storage.put(key, true);\n  // further processing\n}\n```\n\n## Follow-up Questions\n- How would you test end-to-end latency under burst traffic?\n- How would you implement schema evolution and backward compatibility across tenants?","diagram":"flowchart TD\n  IoTHub([IoT Hub]) --> EventHub([Event Hubs])\n  EventHub --> DF([Durable Functions])\n  DF --> Cosmos([Cosmos DB])\n  Cosmos --> Dash([Dashboards/Power BI])","difficulty":"intermediate","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T13:03:22.603Z","createdAt":"2026-01-16T13:03:22.604Z"},{"id":"q-2877","question":"You're building a real-time fraud-detection pipeline for a global fintech app on Azure. Ingest at 100k msgs/sec via Event Hubs; orchestrate with Durable Functions; call a low-latency Azure ML scoring endpoint; persist per-tenant results in Cosmos DB; classify data with Purview; ensure backpressure, idempotent retries, DLQ, and cross-region audit trails. How would you design and what are the key failure modes and mitigations?","answer":"Design a latency-aware pipeline with a Durable Functions orchestrator, fan-out to an Azure ML scoring endpoint, per-tenant Cosmos DB writes, Purview tagging, and cross-region audit logs. Include backp","explanation":"## Why This Is Asked\n\nEvaluates end-to-end throughput, fault tolerance, and data governance in a distributed Azure workflow, with real-world constraints like per-tenant isolation and cross-region auditing.\n\n## Key Concepts\n\n- Event Hubs throughput and backpressure\n- Durable Functions orchestration patterns\n- Azure ML scoring latency and scaling\n- Cosmos DB per-tenant isolation and consistency\n- Purview data governance and tagging\n- DLQ, retries, auditing, and cross-region compliance\n\n## Code Example\n\n```javascript\n// Pseudo-DnD orchestrator (Durable Functions)\nmodule.exports = async function(context) {\n  const event = context.df.getInput();\n  const score = await context.df.callActivity('MLInfer', event);\n  await context.df.callActivity('CosmosUpsert', { tenant: event.tenant, score });\n  context.df.setCustomStatus(`scored:${score}`);\n  return score;\n}\n```\n\n## Follow-up Questions\n\n- How would you tune for cold-start latency and cold-start penalties in Azure Functions?\n- What strategies ensure per-tenant isolation at scale in Cosmos DB and how would you test them?","diagram":"flowchart TD\n  A[Event Hubs Ingest] --> B[Durable Functions Orchestrator]\n  B --> C[Azure ML Scoring]\n  C --> D[Cosmos DB (per-tenant)]\n  D --> E[Purview Tagging / Audit]\n  E --> F[Monitoring & Alerts]","difficulty":"advanced","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Scale Ai","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T15:45:54.590Z","createdAt":"2026-01-16T15:45:54.590Z"},{"id":"q-2932","question":"You're building a real-time, multi-tenant order routing pipeline on Azure. Ingest ~60k–120k events/sec from mobile/web clients via Azure Event Hubs; apply per-tenant business rules in Durable Functions; route to multiple fulfillment providers; persist per-tenant order state in Cosmos DB; expose a low-latency query API. How would you design to guarantee exactly-once processing, strict per-tenant isolation, auditable lineage, and resilience to provider outages, including error handling and rollback strategies?","answer":"Use a Durable Functions orchestrator with per-tenant suborchestrations; dedupe with a composite key (tenantId, orderId, eventId) and upsert idempotently into Cosmos DB; rely on Event Hubs partitioning","explanation":"## Why This Is Asked\nAssesses ability to design real-time, multi-tenant Azure pipelines with exactly-once semantics, cross-provider resilience, and auditable lineage.\n\n## Key Concepts\n- Durable Functions orchestration and per-tenant suborchestrations\n- Idempotent upserts in Cosmos DB using composite keys\n- Event Hubs partitioning, checkpointing, and ordering guarantees\n- Saga pattern for cross-provider rollback\n- Auditability via immutable logs and cross-region replication\n\n## Code Example\n```javascript\n// Pseudo-upsert with idempotency using tenant+order+event keys\nasync function upsertOrder(ctx, item){\n  const key = `${item.tenantId}:${item.orderId}:${item.eventId}`;\n  await cosmosClient.upsert(\"Orders\", { id: key, data: item, ts: Date.now() });\n}\n```\n\n## Follow-up Questions\n- How would you implement schema evolution without breaking consumers?\n- How do you test failure scenarios across regions and providers?","diagram":"flowchart TD\nA[Event Hubs] --> B[Durable Functions Orchestrator]\nB --> C[Per-Tenant Suborchestrations]\nC --> D[Cosmos DB Upserts]\nB --> E[Routing to Providers]\nE --> F[Audit Trail Storage]","difficulty":"intermediate","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","IBM","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T17:53:44.846Z","createdAt":"2026-01-16T17:53:44.846Z"},{"id":"q-2949","question":"Design a beginner Azure data pipeline: land per-tenant CSVs in Azure Blob via Data Factory, create per-tenant Delta tables under Unity Catalog for isolation, and implement MERGE-based upserts in PySpark (using event_id as the key). Include simple validation, audit logging, and retry handling; outline data model, steps, and provide a minimal PySpark MERGE snippet?","answer":"Land tenant CSVs into Blob via ADF, create per-tenant Delta tables under Unity Catalog, and perform MERGE upserts on event_id in PySpark. Add schema validation, lightweight audit logs, and exponential","explanation":"## Why This Is Asked\nTests practical Azure data engineering basics for multi-tenant isolation, Delta Lake usage, and data pipeline reliability.\n\n## Key Concepts\n- Azure Data Factory orchestration\n- Delta Lake MERGE upserts\n- Unity Catalog per-tenant isolation\n- PySpark dataframes and schema validation\n\n## Code Example\n```python\nfrom delta.tables import DeltaTable\n\n# assume df is the new batch with columns: tenant_id, event_id, ...\npath = f\"dbfs:/delta/{tenant_id}/events\"\ndelta_table = DeltaTable.forPath(spark, path)\ndelta_table.alias('t').merge(\n    source=df.alias('s'),\n    condition=\"t.event_id = s.event_id\"\n).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n```\n\n## Follow-up Questions\n- How would you test idempotency across retries?\n- How would you scale to many tenants and manage catalog permissions?","diagram":null,"difficulty":"beginner","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","NVIDIA","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T18:52:48.690Z","createdAt":"2026-01-16T18:52:48.690Z"},{"id":"q-3021","question":"You're building a real-time fraud detection pipeline for a global rideshare app on Azure. Ingest 50-100k events/sec via Event Hubs; perform per-tenant feature extraction and ML scoring in a per-tenant isolated runtime (Durable Functions or AKS); persist scores in Cosmos DB with tenant isolation; expose a low-latency REST API; ensure exact-once delivery, drift-aware model updates, and auditable trails. How would you design and test this end-to-end?","answer":"Implement an idempotent consumer on Event Hubs keyed by (tenantId, eventId) with checkpointing; feature extraction and scoring run in per-tenant isolated runtimes; store scores and features in Cosmos DB with tenant-specific containers; expose results through a low-latency API Gateway with caching; ensure exact-once delivery via idempotency keys and deduplication; handle model drift with automated monitoring and blue-green deployments; maintain auditable trails through Azure Purview integration.","explanation":"## Why This Is Asked\nThis scenario tests real-time processing, per-tenant isolation, latency requirements, drift management, and auditable data trails in a compliant Azure ecosystem.\n\n## Key Concepts\n- Event Hubs throughput optimization, deduplication strategies, and checkpoint management\n- Idempotent processing patterns for exactly-once semantics\n- Per-tenant partitioning and isolation in Cosmos DB\n- Azure ML model registry with blue/green deployment strategies\n- Automated drift detection and rollback mechanisms\n- Comprehensive auditing with Azure Purview integration\n\n## Code Example\n```java","diagram":"flowchart TD\n  Ingest(Event Hubs) --> FeatureExtraction[Feature Extraction]\n  FeatureExtraction --> Scoring[ML Scoring]\n  Scoring --> CosmosDB[Cosmos DB (per-tenant)]\n  CosmosDB --> RESTAPI[REST API]\n  ModelRegistry --> DriftWatch[Drift & Rollback]\n  AuditTrail --> Purview[Azure Purview]","difficulty":"intermediate","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","DoorDash","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T06:04:48.936Z","createdAt":"2026-01-16T21:40:01.945Z"},{"id":"q-3212","question":"In a multi-tenant telemetry platform on Azure, events arrive at 60-100k msgs/sec via Event Hubs. Design an end-to-end pipeline that preserves strict per-tenant isolation, supports idempotent retries, and provides auditable trails for audits. Use a Durable Functions orchestrator for per-tenant enrichment, Cosmos DB with tenant partition keys, and surface analytics via Cognitive Search. Include disaster recovery, cost controls, and governance considerations?","answer":"Ingest with Event Hubs; use a Durable Functions orchestrator to route per-tenant, perform enrichment, then upsert into Cosmos DB with tenantId as partition key. Implement the outbox pattern to guarant","explanation":"## Why This Is Asked\nTests real-world Azure streaming, per-tenant isolation, idempotency, and auditable trails across services with DR and cost controls.\n\n## Key Concepts\n- Event Hubs throughput and consumer groups\n- Durable Functions orchestration and per-tenant state\n- Cosmos DB partitioning and multi-region replication\n- Outbox pattern for exactly-once delivery\n- Azure Monitor, Purview, RBAC, autoscale\n\n## Code Example\n```javascript\n// outline of Durable Functions orchestrator handling tenant-scoped workflow\n```\n\n## Follow-up Questions\n- How would you implement idempotent retries and exactly-once semantics across the outbox and downstream systems?\n- What are latency and cost trade-offs of cross-region Cosmos DB writes?","diagram":"flowchart TD\n  EH[Event Hubs] --> OR[Durable Functions Orchestrator]\n  OR --> CS[Cosmos DB (tenant PK)]\n  CS --> SEARCH[Cognitive Search]\n  CS --> PUR[Purview/Governance]\n  OR --> LOG[Audit Logs]\n  LOG --> MON[Log Analytics]","difficulty":"intermediate","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T07:26:48.970Z","createdAt":"2026-01-17T07:26:48.970Z"},{"id":"q-3279","question":"Privacy-first cross-tenant data export service on Azure: tenants configure export jobs to move selected telemetry from a central data lake to their own storage destinations (Azure Blob or S3-compatible) in real time or near-real-time. Data ingested through Azure Event Hubs, processed by a chain of Functions and Durable Functions to apply redaction/pseudonymization rules stored per-tenant in Cosmos DB, then written to the destination with per-tenant encryption keys from Key Vault. How would you design end-to-end flow, guarantees (idempotence, exactly-once), security boundaries, data residency, and auditability, including failure handling and SLA targets? Include concrete components and data formats?","answer":"Event Hubs → Functions → Durable orchestrator; per-tenant redaction in Cosmos DB and KEKs in Key Vault; export to Azure Blob or S3 with tenant-scoped encryption; deterministic path tenant_export_times","explanation":"## Why This Is Asked\n\nAssesses ability to design a privacy-focused, multi-tenant export pipeline on Azure, balancing security, compliance, and reliability across components.\n\n## Key Concepts\n- End-to-end data flow with Event Hubs, Functions, Durable Functions\n- Per-tenant policy and key management (Cosmos DB, Key Vault)\n- Idempotence, exactly-once semantics, and rollback strategies\n- Data residency, cross-region export, and auditability\n- Observability with distributed tracing and telemetry\n\n## Code Example\n```csharp\n// Pseudo Durable Orchestrator skeleton\n[FunctionName(\"ExportOrchestrator\")]\npublic static async Task Run([OrchestrationTrigger] IDurableOrchestrationContext ctx){\n  var payload = ctx.GetInput<string>();\n  // activities: redact, encrypt, write\n}\n```\n\n## Follow-up Questions\n- How would you validate end-to-end latency and failure modes?\n- How would you test the guarantee of exactly-once exports in retries?","diagram":null,"difficulty":"advanced","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","MongoDB","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T09:43:01.960Z","createdAt":"2026-01-17T09:43:01.960Z"},{"id":"q-3360","question":"You’re building a beginner Azure Function HTTP webhook receiver that ingests events from multiple partner vendors. Each vendor has a tenantId and a shared secret stored in Key Vault. Outline a secure flow: signature validation via Key Vault secrets (Managed Identity), per-tenant rate limiting using Redis, idempotent Cosmos DB writes with tenantId as partitionKey and eventId as id, and App Insights telemetry. Provide a minimal code sketch for tenant extraction and signature check?","answer":"Use an HTTP-triggered Function; get tenantId from a header; fetch the vendor secret from Key Vault via DefaultAzureCredential; validate the request with an HMAC SHA-256 signature over the body using t","explanation":"## Why This Is Asked\n\nTests a secure, tenant-aware webhook ingestion flow with observable telemetry and fault tolerance.\n\n## Key Concepts\n\n- HTTP trigger, signature validation, Key Vault, Managed Identity\n- Per-tenant rate limiting using Redis\n- Cosmos DB idempotent writes with composite keys\n- App Insights telemetry\n\n## Code Example\n\n```javascript\n// tenant extraction and signature check (pseudo)\nconst tenantId = req.headers['x-tenant-id'];\nconst secret = await getSecretFromKeyVault(tenantId);\nconst valid = verifyHMAC(req.body, secret, req.headers['x-signature']);\nif (!valid) return { status: 401 };\nawait cosmos.upsert({ tenantId, eventId: req.headers['x-event-id'], ...payload });\n```\n\n## Follow-up Questions\n\n- How would you test this locally with a mock Key Vault and Redis?\n- How to handle secret rotation without downtime?","diagram":"flowchart TD\n  WebhookReceiver[Webhook Receiver] --> SigCheck[Signature Validation]\n  SigCheck --> RateLimit[Per-Tenant Rate Limiting]\n  RateLimit --> Cosmos[Cosmos DB Upsert (tenant partition)]\n  Cosmos --> Insights[App Insights Telemetry]","difficulty":"beginner","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","OpenAI","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T13:35:56.991Z","createdAt":"2026-01-17T13:35:56.991Z"},{"id":"q-3580","question":"Design a multi-tenant telemetry analytics stack on Azure for devices generating 30k events/sec. Ingest via Azure Event Hubs, enrich with Durable Functions for dedup and schema normalization, and store per-tenant data in Azure Data Explorer with fast queries. Outline data model, exactly-once processing, per-tenant RBAC, TTL retention, cross-region reads, and cost controls; include failure modes and audit trails?","answer":"Ingest 30,000 events per second through Azure Event Hubs; leverage Durable Functions to deduplicate by TenantId+MessageId and normalize schemas; sink to Azure Data Explorer with tenant-scoped databases or a shared table partitioned by TenantId for optimal query performance. Implement exactly-once processing using idempotent operations and deduplication keys. Enforce per-tenant RBAC through Azure AD integration and database-level permissions. Configure TTL retention policies per tenant and enable cross-region reads with follower databases. Apply cost controls via auto-scaling, capacity reservations, and monitoring alerts. Include comprehensive failure mode handling with dead-letter queues, retry policies, and detailed audit trails through Azure Monitor and Log Analytics.","explanation":"## Why This Is Asked\n\nTests the ability to design a scalable, compliant multi-tenant pipeline with strong isolation, latency awareness, and cost governance using Azure primitives. It also probes stateful orchestration, exactly-once guarantees, and cross-region considerations.\n\n## Key Concepts\n\n- Event Hubs ingestion and durable orchestration\n- Exactly-once semantics with deduplication keys\n- Azure Data Explorer data modeling and tenant isolation\n- RBAC, retention, and budget controls\n- Cross-region replication and auditing\n\n## Code Example\n\n```javascript\n// Pseudo ingestion function demonstrating key concepts\nasync function processTelemetry(events) {\n  // Deduplicate using TenantId+MessageId composite key\n  const uniqueEvents = await deduplicate(events, \n    (e) => `${e.tenantId}:${e.messageId}`);\n  \n  // Normalize schema across tenants\n  const normalized = uniqueEvents.map(normalizeSchema);\n  \n  // Enrich with tenant-specific metadata\n  const enriched = await enrichWithTenantData(normalized);\n  \n  // Write to ADX with idempotent operations\n  await writeToADX(enriched, { \n    table: getTenantTable(enriched[0].tenantId),\n    idempotencyKey: 'messageId'\n  });\n}\n```","diagram":"flowchart TD\n  A[Device] --> B(Event Hubs)\n  B --> C[Enrichment (Durable Functions)]\n  C --> D[ADX ingest (tenant-scoped)]\n  D --> E[RBAC & retention]\n  E --> F[Dashboards/alerts]\n  F --> G[Cross-region replicas]","difficulty":"intermediate","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Tesla","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T05:36:25.492Z","createdAt":"2026-01-17T22:31:37.428Z"},{"id":"q-3630","question":"Design a fintech telemetry pipeline on Azure to ingest 50k msgs/sec of client telemetry via Event Hubs, process with Spark Structured Streaming on Databricks, redact PII per-tenant, and store redacted data in ADLS Gen2 Parquet partitions by tenant/date. Expose per-tenant aggregates in Cosmos DB for dashboards, ensure exactly-once processing, cross-region replication, retention, audit trails, and security (Managed Identity + Key Vault). What is your implementation plan and key trade-offs?","answer":"Design a fintech telemetry pipeline to ingest 50,000 messages per second through Azure Event Hubs, process with Spark Structured Streaming on Azure Databricks, apply per-tenant PII redaction with data isolation, store redacted data in ADLS Gen2 using tenant/date-partitioned Parquet files, expose per-tenant aggregates in Cosmos DB for dashboard consumption, ensure exactly-once processing semantics, implement cross-region replication, establish data retention policies, maintain comprehensive audit trails, and enforce enterprise-grade security through Managed Identity and Key Vault integration.","explanation":"## Why This Is Asked\nTests ability to design compliant, scalable telemetry pipelines with tenant isolation and exactly-once semantics, including cross-region replication and auditable trails.\n\n## Key Concepts\n- Azure Event Hubs, Spark Structured Streaming, Parquet on ADLS Gen2, Cosmos DB per-tenant containers\n- Exactly-once semantics, idempotent sinks, checkpointing, deduplication strategies\n- PII redaction, per-tenant isolation, data retention policies, cross-region replication\n- Security: Managed Identity, Key Vault; cost controls and monitoring\n\n## Code Example\n```python\n# Pseudo Spark Stru","diagram":"flowchart TD\n  A[Client Telemetry] --> B[Azure Event Hubs]\n  B --> C[Spark Structured Streaming on Databricks]\n  C --> D[PII Redaction & Per-Tenant Isolation]\n  D --> E[Parquet in ADLS Gen2 (tenant/date)]\n  E --> F[Cosmos DB (per-tenant) for dashboards]\n  F --> G[Cross-Region Replication]\n  G --> H[Azure Monitor Alerts]\n","difficulty":"advanced","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","LinkedIn","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T05:07:18.681Z","createdAt":"2026-01-18T02:32:07.234Z"},{"id":"q-3752","question":"You're building a beginner Azure Functions HTTP API to manage a tenant-scoped product catalog. Data lives in Azure Table Storage with PartitionKey=tenantId. To reduce latency, add a read-through cache using Azure Cache for Redis. Describe the end-to-end flow and TTL strategy, and provide a minimal code snippet showing GET /catalog/{tenantId}/{productId} that checks Redis first, then Table Storage, and caches the result. How would you implement this?","answer":"Check Redis first with key `${tenantId}:${productId}`; on miss fetch from Table Storage (PartitionKey=tenantId, RowKey=productId); if found, return and cache the item in Redis with TTL (e.g., 300s). U","explanation":"## Why This Is Asked\nTests understanding of read-through caching, per-tenant data isolation, and latency optimization in a serverless context. It also surfaces familiarity with common Azure data services and simple failure handling.\n\n## Key Concepts\n- Read-through caching with Redis in serverless Functions\n- Per-tenant data isolation using Table Storage PartitionKey\n- TTL strategy to balance freshness and performance\n- Cache stampede mitigation and basic error handling\n\n## Code Example\n```csharp\n// Minimal illustration for GET endpoint\npublic async Task<IActionResult> GetCatalog(string tenantId, string productId)\n{\n    var key = $\"{tenantId}:{productId}\";\n    var cached = await _cache.GetStringAsync(key);\n    if (cached != null) return new OkObjectResult(JsonConvert.DeserializeObject<Product>(cached));\n\n    var tbl = _table.GetTableReference(\"Catalog\");\n    var op = TableOperation.Retrieve<ProductEntity>(tenantId, productId);\n    var res = await tbl.ExecuteAsync(op);\n    var item = (ProductEntity)res.Result;\n    if (item == null) return new NotFoundResult();\n\n    var prod = item.ToProduct();\n    await _cache.SetStringAsync(key, JsonConvert.SerializeObject(prod), new DistributedCacheEntryOptions { AbsoluteExpirationRelativeToNow = TimeSpan.FromSeconds(300) });\n    return new OkObjectResult(prod);\n}\n```\n\n## Follow-up Questions\n- How would you invalidate the cache after an update to a product?\n- How would you test latency and cache miss/hit scenarios locally?","diagram":null,"difficulty":"beginner","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Square","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T08:37:29.701Z","createdAt":"2026-01-18T08:37:29.701Z"},{"id":"q-3823","question":"Design a compliant, multi-tenant data ingestion path on Azure for a fintech app: Ingest 100k msgs/sec via Event Hubs, orchestrate with Durable Functions, use per-tenant keys from Key Vault, store isolated data in Cosmos DB, and emit auditable logs to immutable blob storage with tracing. How would you ensure data isolation, idempotence, auditability, and deletion requests?","answer":"Use Event Hubs ingestion with tenant-scoped consumer groups, a Durable Functions orchestrator for step sequencing, per-tenant Cosmos DB partitions, and Key Vault-based customer-managed keys. Emit immu","explanation":"## Why This Is Asked\nTests ability to design scalable, compliant multi-tenant ingestions on Azure with strict isolation, auditability, and deletion workflows, plus integration of security and observability primitives.\n\n## Key Concepts\n- Azure Event Hubs with tenant-level isolation\n- Durable Functions orchestration\n- Cosmos DB partitioning per tenant\n- Key Vault customer-managed keys\n- Immutable audit logs (Blob; WORM)\n- Idempotence and exactly-once-like semantics\n- OpenTelemetry tracing and tenant RBAC\n- Data deletion subject requests\n\n## Code Example\n```javascript\n// Durable Functions skeleton\nimport * as df from 'durable-functions';\nexport const orchestrator = df.orchestrator(function* (context) {\n  const tenantId = context.bindingData.tenantId;\n  // idempotency check\n  // orchestrate steps: ingest -> enrich -> persist -> audit\n});\n```\n\n## Follow-up Questions\n- How would you implement deletion requests across all stores while preserving audit integrity?\n- What are potential cost and latency trade-offs of per-tenant KMS keys and cross-region replication?","diagram":"flowchart TD\n  Ingest[Ingest via Event Hubs] --> Orchestrate[Durable Functions Orchestrator]\n  Orchestrate --> Persist[Persist to Cosmos DB (per-tenant)]\n  Persist --> Audit[Audit logs to Immutable Blob Storage]\n  Audit --> Trace[OpenTelemetry tracing]","difficulty":"intermediate","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Goldman Sachs","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T10:46:15.421Z","createdAt":"2026-01-18T10:46:15.421Z"},{"id":"q-3825","question":"You're designing a global, multi-tenant fintech data pipeline on Azure. Ingest 100k events/sec from mobile and web via Azure Event Hubs, process with Azure Databricks Spark into Delta Lake on ADLS Gen2, with strict per-tenant isolation. Implement data masking for PII, audit trails via Azure Purview, TTL retention, and cost-aware autoscaling. How would you structure data layouts, governance, failure modes, and testing?","answer":"Use Event Hubs -> Structured Streaming in Databricks to write to tenant-scoped Delta Lake partitions (tenant_id/date) on ADLS Gen2. Enforce per-tenant RBAC (Unity Catalog), apply masking via Spark SQL","explanation":"## Why This Is Asked\\n\\nThis probes end-to-end data pipeline design with multi-tenant governance, masking, and auditability on Azure; it tests practicality, not theory.\\n\\n## Key Concepts\\n\\n- Azure Event Hubs, Databricks, Delta Lake, ADLS Gen2\\n- Azure Purview governance and lineage\\n- Per-tenant isolation, masking, RBAC, audit trails\\n- TTL retention, idempotent writes, autoscaling, cost control\\n\\n## Code Example\\n```python\\n# Pseudo: read from Event Hubs, write to Delta Lake with partition by tenant_id/date\\n```\\n\\n## Follow-up Questions\\n\\n- How would you test data masking rules without leaking PII?\\n- How would you handle cross-region DR for Purview and Delta Lake?\\n","diagram":null,"difficulty":"intermediate","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T11:23:56.387Z","createdAt":"2026-01-18T11:23:56.389Z"},{"id":"q-3929","question":"You're building a real-time, multi-tenant ML inference stack on Azure for 100k device events/sec. Ingest via Azure Event Hubs, batch via Durable Functions per tenant, call a single Azure ML endpoint for inference, and store per-tenant results in Cosmos DB with TTL. How would you ensure end-to-end latency under 200ms, strict isolation, idempotent retries, and auditable trails, plus testing strategies?","answer":"Ingest 100k/sec via Event Hubs, partition by tenantId, batch with Durable Functions per tenant (64–128 events), call a single Azure ML endpoint for inference, then store results in per-tenant Cosmos D","explanation":"## Why This Is Asked\n\nTests real-time, multi-tenant pipelines with ML inference, coverage of latency budgets, data isolation, retries, and observability.\n\n## Key Concepts\n\n- Event Hubs partitioning by tenantId for isolation and parallelism\n- Durable Functions orchestration for deterministic batching\n- Azure ML endpoint for scalable scoring\n- Cosmos DB per-tenant containers with TTL\n- Observability: Log Analytics, auditing, and RBAC\n\n## Code Example\n\n```javascript\n// Pseudo-batching and inference flow\nasync function batchInfer(events, endpoint) {\n  const batch = events.slice(0, 128);\n  const payload = batch.map(e => ({ tenantId: e.tenantId, data: e.payload }));\n  const res = await axios.post(endpoint, payload);\n  return res.data;\n}\n```\n\n## Follow-up Questions\n\n- How would you validate latency under peak load with synthetic traffic?\n- How would you roll back partial failures and preserve exactly-once semantics?","diagram":null,"difficulty":"intermediate","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T15:44:17.706Z","createdAt":"2026-01-18T15:44:17.707Z"},{"id":"q-3987","question":"You're designing a secure, scalable data integration between Salesforce and an Azure Databricks lakehouse. Enable CDC for accounts and opportunities, stream changes through Event Hubs into Databricks, and upsert into Delta Lake with per-tenant isolation. How would you implement incremental upserts, PII masking, GDPR deletions, data cataloging/audit via Purview, and robust replay/error handling? Include testing and rollback plan?","answer":"Use Databricks Structured Streaming consuming from Event Hubs, and upsert into Delta Lake with tenantId and id as the key. Mask PII during streaming via a UDF. Handle Salesforce deletions with tombsto","explanation":"## Why This Is Asked\nAssess end-to-end data integration between Salesforce and Databricks, focusing on CDC, per-tenant isolation, and governance.\n\n## Key Concepts\n- Salesforce Change Data Capture and Event Hubs\n- Delta Lake MERGE semantics and idempotence\n- Per-tenant data isolation via partitioning\n- PII masking in streaming\n- GDPR delete/tombstones and purge strategy\n- Purview lineage and data cataloging\n- Replayable streaming and robust error handling\n\n## Code Example\n```python\nfrom delta.tables import DeltaTable\n# PySpark example (conceptual)\ndelta = DeltaTable.forPath(spark, \"/mnt/delta/tenant_data\")\nmerge_df = spark.readStream.format(\"json\").load(\"/mnt/bronze/tenant_src\")\ndelta.alias(\"t\").merge(\n  merge_df.alias(\"s\"),\n  \"t.tenantId = s.tenantId AND t.id = s.id\"\n).whenMatchedUpdate(set = {\"name\": \"s.name\", \"emailMasked\": \"maskPII(s.email)\"})\n .whenNotMatchedInsertAll().execute()\n```\n\n## Follow-up Questions\n- How would you test failure modes and ensure idempotent replays? \n- What are the monitoring and alerting knobs you’d add for data freshness and governance?","diagram":"flowchart TD\n  A[Salesforce CDC events] --> B[Event Hubs]\n  B --> C[Databricks Structured Streaming]\n  C --> D[Delta Lake (tenant)]\n  D --> E[Purview lineage]\n  D --> F[PII masked views]\n  D --> G[Time Travel / Rollback]","difficulty":"intermediate","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T18:47:45.619Z","createdAt":"2026-01-18T18:47:45.619Z"},{"id":"q-3999","question":"You're operating a global multi-tenant event pipeline on Azure: Event Hubs ingest, Azure Functions processing, and Cosmos DB storage with per-tenant isolation. How would you implement end-to-end observability using OpenTelemetry and Azure Monitor to achieve tenant-aware tracing across components, with correlation IDs, dynamic sampling, and no cross-tenant leakage? Describe instrumentation points, data retention, and costs?","answer":"Instrument Event Hubs, Functions, and Cosmos DB with OpenTelemetry. Propagate tenantId and traceId across boundaries, push to Azure Monitor via the OpenTelemetry collector, and store traces in a per-t","explanation":"## Why This Is Asked\nAssesses practical knowledge of end-to-end observability in Azure for multi-tenant data pipelines, including cross-service tracing, privacy, and cost considerations.\n\n## Key Concepts\n- OpenTelemetry integration with Azure Functions and Event Hubs\n- Trace context propagation (tenantId, traceId)\n- Azure Monitor / Log Analytics backend for traces\n- Per-tenant isolation in logs and data paths\n- Dynamic sampling and cost-aware tracing, privacy controls\n\n## Code Example\n```javascript\n// Node.js (simplified sketch)\nconst { trace, context, propagation } = require('@opentelemetry/api');\nconst { registerInstrumentations } = require('@opentelemetry/instrumentation');\nconst { HttpInstrumentation } = require('@opentelemetry/instrumentation-http');\n\nregisterInstrumentations({ instrumentations: [new HttpInstrumentation()], tracerProvider: /*...*/ });\n\nmodule.exports = async function (context, req) {\n  const tenantId = req.headers['x-tenant-id'];\n  const span = trace.getTracer('azure-trace').startSpan('function.process', {\n    attributes: { tenantId }\n  });\n  return await context.bindings?.invoke(req, { parent: span.context() });\n};\n```\n\n## Follow-up Questions\n- How would you validate dynamic sampling across tenants with different load patterns?\n- How would you detect and remediate telemetry leaks that cross tenant boundaries?","diagram":"flowchart TD\n  A[Event Hubs Ingest] --> B[Azure Functions]\n  B --> C[Cosmos DB]\n  C --> D[Azure Monitor Logs]\\n\n  A -- trace --> D\n  B -- trace --> D\n  C -- trace --> D","difficulty":"intermediate","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T19:21:50.944Z","createdAt":"2026-01-18T19:21:50.946Z"},{"id":"q-4034","question":"You're building a beginner Azure Functions HTTP API to serve per-tenant feature flags. Flags live in Azure App Configuration, with per-tenant overrides stored in Azure Cosmos DB; the API must resolve a tenant's flags, cache results in Azure Redis Cache for 60s, and gracefully fallback to defaults if a tenant is missing. Implement GET /flags/{tenantId}?name={flagName} and provide a minimal Node.js snippet that reads from Redis, then App Configuration, with a default if not present?","answer":"Use a layered approach where the API resolves a tenant's flag by (1) validating the tenantId; (2) checking Redis for cached feature flags; (3) if cache miss, fetching the base flag from Azure App Configuration; (4) applying tenant-specific overrides from Cosmos DB; (5) caching the result with 60s TTL; (6) falling back to default values if the tenant doesn't exist or services are unavailable.","explanation":"## Why This Is Asked\nTests practical Azure knowledge: multi-tenant flag resolution, per-tenant overrides, and cache usage.\n\n## Key Concepts\n- Azure Functions, Azure App Configuration, Azure Cosmos DB, Redis Cache\n- Tenant isolation, TTL caching, fallback defaults, basic error handling\n\n## Code Example\n```javascript\n// Pseudo-snippet: resolve flag with Redis cache, App Configuration, and Cosmos overrides\n```\n\n## Follow-up Questions\n- How would you test cache invalidation when App Configuration changes?\n- How would you handle partial flag data or Cosmos outages?","diagram":null,"difficulty":"beginner","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Google","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T07:04:26.912Z","createdAt":"2026-01-18T20:45:34.685Z"},{"id":"q-4104","question":"You're building a beginner Azure per-tenant image-processing pipeline. A user uploads an image to a tenant-scoped blob container; an Event Grid BlobCreated event triggers an Azure Function to generate a 300x300 thumbnail, store it in a per-tenant thumbnails container, and log an audit entry in Azure Table Storage. Enforce per-tenant isolation, idempotent retries, and dead-lettering via a Storage Queue. Outline the end-to-end flow and provide a minimal Node.js blob-trigger function using sharp to resize and save the thumbnail?","answer":"Use Event Grid BlobCreated to trigger an Azure Function; derive tenantId from the blob path; resize to 300x300 with sharp; write thumbnail to tenants/{tenantId}/thumbnails; insert audit row with PartitionKey for tenant isolation.","explanation":"## Why This Is Asked\n\nTests event-driven processing basics on Azure, per-tenant isolation, idempotency, and dead-lettering in a tangible, beginner-friendly scenario.\n\n## Key Concepts\n\n- Event Grid and Blob storage integration\n- Per-tenant isolation via path-based or partitioned data\n- Idempotent processing and deduplication\n- Dead-letter queues and robust retry policies\n- Lightweight auditing via Table Storage or Cosmos\n\n## Code Example\n\n```javascript\nmodule.exports = async function(context, blob) {\n  const name = context.bindingData.name; // e.g. tenants/{tenant}/source/{file}\n  const parts = name.split('/');\n  const tenantId = parts[1];\n  const fileName = parts[3];\n  \n  // Idempotency check\n  const thumbnailPath = `tenants/${tenantId}/thumbnails/${fileName}`;\n  \n  try {\n    // Resize with sharp\n    const sharp = require('sharp');\n    const thumbnail = await sharp(blob).resize(300, 300).toBuffer();\n    \n    // Save thumbnail\n    context.bindings.outputBlob = thumbnail;\n    \n    // Audit entry\n    context.bindings.auditTable = {\n      PartitionKey: tenantId,\n      RowKey: `${Date.now()}-${fileName}`,\n      Timestamp: new Date().toISOString(),\n      Status: 'processed'\n    };\n    \n  } catch (error) {\n    // Dead-letter on failure\n    context.bindings.deadLetterQueue = {\n      tenantId,\n      fileName,\n      error: error.message,\n      timestamp: new Date().toISOString()\n    };\n    throw error;\n  }\n};\n```\n\n## End-to-End Flow\n\n1. User uploads image to `tenants/{tenantId}/source/{filename}`\n2. Event Grid emits BlobCreated event\n3. Azure Function triggers, extracts tenantId from path\n4. Function resizes image to 300x300 using sharp\n5. Thumbnail saved to `tenants/{tenantId}/thumbnails/{filename}`\n6. Audit entry logged with tenantId as PartitionKey\n7. On failure, message sent to dead-letter queue for retry","diagram":"flowchart TD\n  A[BlobCreated Event] --> B[Function Trigger]\n  B --> C[Parse tenant from path]\n  C --> D[Read source blob]\n  D --> E[Resize with sharp]\n  E --> F[Write thumbnail to tenant container]\n  F --> G[Audit entry in Table Storage]\n  G --> H[Successful end]\n  H --> I[DLQ on failure]","difficulty":"beginner","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T05:31:52.422Z","createdAt":"2026-01-19T02:29:34.804Z"},{"id":"q-4243","question":"You're building a beginner Azure Functions HTTP API to accept per-tenant document uploads (PDFs/images) for Instacart partners. Store files in Blob Storage under tenantId/docs/, generate thumbnails, and keep metadata in Cosmos DB with partitionKey=tenantId. How would you implement secure uploads, per-tenant isolation, simple virus scanning, and cost-aware scaling?","answer":"Generate a per-tenant SAS URL for direct upload to Blob Storage at tenantId/docs/{filename}, then persist metadata in Cosmos DB with partitionKey=tenantId. Use Event Grid to trigger a thumbnail/virus-","explanation":"## Why This Is Asked\nThis checks practical use of serverless primitives with multi-tenant isolation, scalable I/O, and basic security.\n\n## Key Concepts\n- Azure Functions HTTP API\n- Blob Storage SAS tokens\n- Cosmos DB partitioning per tenant\n- Event Grid triggers and serverless workers\n- Virus scanning and TTL\n\n## Code Example\n```javascript\n// Node.js pseudo: issue SAS for tenant's container\nconst { BlobServiceClient, generateBlobSASQueryParameters, BlobSASPermissions } = require(\"@azure/storage-blob\");\n\n// Placeholder for generating SAS (simplified)\n```\n\n## Follow-up Questions\n- How would you test per-tenant isolation and failure scenarios?\n- How would you monitor costs and throttle tenants with spikes?","diagram":"flowchart TD\n  A(Client) --> B[Upload API]\n  B --> C[Blob Storage - tenant/docs]\n  C --> D[Cosmos DB - tenantId]\n  D --> E[Event Grid]\n  E --> F[Thumbnail/Scan Functions]","difficulty":"beginner","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T09:55:45.145Z","createdAt":"2026-01-19T09:55:45.145Z"},{"id":"q-4461","question":"Describe an end-to-end per-tenant data path: ingest via Event Hubs, tenant-scoped routing using message headers, per-tenant durable functions orchestration, and ADLS Gen2 folders /tenants/{tenantId}/. Store to Delta tables with per-tenant partitions, govern with Purview, enforce retention, and tag costs. Include isolation, retries, audit trails, and testing strategies?","answer":"Describe an end-to-end per-tenant data path: ingest via Event Hubs, tenant-scoped routing using message headers, per-tenant durable functions orchestration, and ADLS Gen2 folders /tenants/{tenantId}/.","explanation":"## Why This Is Asked\n\nTests ability to design for strict per-tenant isolation, governance, and operational discipline in a real Azure data platform.\n\n## Key Concepts\n\n- Per-tenant data layout on ADLS Gen2 with explicit tenant folders and Delta Lake partitions\n- Tenant-scoped routing using message headers in Event Hubs\n- Durable Functions orchestration for per-tenant pipelines\n- Purview for governance and retention policies\n- Cost tagging, RBAC, and audit trails\n\n## Code Example\n\n```javascript\n// Simple helper to derive a per-tenant path\nfunction tenantPath(tenantId){\n  return `/tenants/${tenantId}/`;\n}\n```\n\n## Follow-up Questions\n\n- How would you test tenant isolation under burst traffic and ensure no cross-tenant data leakage?\n- What failure modes require circuit breakers and how would you implement them across the pipeline?","diagram":"flowchart TD\n  Ingest(EventHub) --> Route{Tenant Header Present?}\n  Route -- Yes --> Orchestrator[Durable Functions per Tenant]\n  Orchestrator --> Sink[(ADLS Gen2 /tenants/{tenantId}/)]\n  Sink --> Catalog[Purview Governance & Retention]\n  Catalog --> RBAC[Azure AD RBAC & Cost Tags]\n  RBAC --> AuditLogs[Audit Trails & Alerts]","difficulty":"intermediate","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Salesforce","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T19:44:59.134Z","createdAt":"2026-01-19T19:44:59.134Z"},{"id":"q-4522","question":"You’re building a global multi-tenant analytics pipeline on Azure for a SaaS product. Ingest via Event Hubs, processing with Synapse Spark, data stored per-tenant in ADLS Gen2, queried via serverless SQL. How would you ensure strict per-tenant isolation, dynamic masking at query time, and auditable lineage across Purview, while controlling costs and handling late data? Include testing and rollback strategies?","answer":"Implement per-tenant storage partitions in ADLS Gen2 and enforce RBAC through Synapse serverless SQL with tenantId filters; apply dynamic masking during Spark ETL using user attributes, and register lineage in Azure Purview with automatic dataset registration. Deploy auto-scaling Spark pools with per-tenant quotas for cost control, and utilize Event Hubs capture with watermarks for late data handling. For testing, establish tenant-isolated test environments with synthetic data generation to validate masking policies. Deploy via Azure DevOps using blue-green deployments with automatic rollback triggers based on performance thresholds.","explanation":"## Why This Is Asked\n\nThis question evaluates practical design skills for multi-tenant analytics on Azure, balancing isolation, governance, and cost within a lakehouse architecture while ensuring operational reliability.\n\n## Key Concepts\n\n- **Tenant isolation**: Partitioning strategies, RBAC enforcement, filter-pushdown optimization, per-tenant resource quotas\n- **Data masking and privacy**: Dynamic masking implementation in Spark and SQL with attribute-based access controls\n- **Data governance**: Purview lineage tracking and data catalog integration with automated registration workflows\n- **Cost management**: Auto-scaling Spark pools, per-tenant quotas, and Event Hubs capture optimization\n- **Operational reliability**: Testing with synthetic data, blue-green deployments, and automated rollback mechanisms\n\n## Expected Response\n\nThe response should demonstrate comprehensive understanding of Azure's data ecosystem while addressing the specific challenges of multi-tenant environments including strict isolation, dynamic privacy controls, and auditable data lineage.","diagram":null,"difficulty":"intermediate","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Snowflake","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T07:05:59.364Z","createdAt":"2026-01-19T22:31:42.507Z"},{"id":"q-4567","question":"You're building a global, real-time feature-flag evaluation service on Azure. End users require a response in ~40ms; flags are stored in Azure App Configuration with per-tenant labels and dynamic rules. How would you design data flow, caching, eviction, audit logging, and failover to meet latency and correctness under partial outages?","answer":"Implement a cache-aside pattern using Azure Cache for Redis with tenant-scoped keys (tenantId:flagName). Authoritative flag configurations reside in Azure App Configuration with per-tenant labels and dynamic rules. On read requests, first check Redis; on cache miss, query App Configuration directly, populate the cache with appropriate TTL, and return the result. Leverage Event Grid for real-time cache invalidation when configurations change, with fallback to direct App Configuration queries during Redis outages to maintain availability.","explanation":"## Why This Is Asked\n\nAssesses real-time flag evaluation, caching strategy, per-tenant isolation, and resiliency under partial outages.\n\n## Key Concepts\n\n- Cache-aside with Azure Cache for Redis\n- Azure App Configuration tenant scoping and labels\n- TTL, eviction, prefetching, and cache warmup\n- Event Grid change notifications for invalidation\n- Auditing and tracing with Azure Monitor/OpenTelemetry\n\n## Code Example\n\n```javascript\nasync function getFlag(tenantId, flagName) {\n  const key = `${tenantId}:${flagName}`;\n  let val = await redis.get(key);\n  if (val) return JSON.parse(val);\n  const flag = await appConfig.getFlag(tenantId, flagName);\n  await redis.setex(key, flag.ttl, JSON.stringify(flag));\n  return flag;\n}\n```","diagram":"flowchart TD\n  A[FlagRequest] --> B[Redis Cache]\n  B -- Hit --> C[Return Flag]\n  B -- Miss --> D[App Configuration]\n  D --> E[Evaluate Rules]\n  E --> F[Cache Update]\n  F --> C","difficulty":"intermediate","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T06:01:10.990Z","createdAt":"2026-01-20T00:04:49.454Z"},{"id":"q-4611","question":"You’re building a global edge-accelerated API platform for AI features across tenants. Use Azure API Management in front of a multi-region AKS model-service stack; store per-tenant state in Cosmos DB with tenantId as the partition key and enforce per-tenant quotas with Redis; ensure data residency by region pinning; describe end-to-end latency targets, fault-tolerance, testing, and rollback plans?","answer":"Design with APIM policies enforcing per-tenant quotas, route to multi-region AKS hosting the model service, and store per-tenant state in Cosmos DB using tenantId as the partition key with geo-replica","explanation":"## Why This Is Asked\nTests multi-region workloads, tenancy isolation, and per-tenant QoS in a realistic Azure SaaS, plus data residency and observability.\n\n## Key Concepts\n- Per-tenant QoS with Redis counters\n- Geo-distributed Cosmos DB and data residency\n- APIM policies, circuit breakers, canary testing\n- Observability with tracing and audits\n\n## Code Example\n```javascript\n// APIM policy (high level)\nconst policy = {\n  name: 'ValidateTenant',\n  onRequest: function(ctx) {\n    if (!ctx.request.headers['X-Tenant-Id']) {\n      throw new Error('Missing tenant')\n    }\n  }\n}\n```\n\n## Follow-up Questions\n- How would you test data residency guarantees at scale?\n- How do you handle per-tenant rollback in a multi-region deployment?","diagram":"flowchart TD\nA[Client] --> B[Azure API Management]\nB --> C[AKS (multi-region)]\nC --> D[Cosmos DB (tenant partition)]\nC --> E[Redis (quotas)]","difficulty":"advanced","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","OpenAI","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T04:22:18.146Z","createdAt":"2026-01-20T04:22:18.146Z"},{"id":"q-4622","question":"You're building a global, multi-tenant AI feature store on Azure for tenants across major platforms (e.g., Meta, DoorDash, Hugging Face). Ingest telemetry from mobile/web via Azure Event Hubs, process in Azure Databricks to derive features, store raw data in ADLS Gen2, and publish derived features to a real-time feature store in Azure SQL/Delta Lake with per-tenant isolation. How would you design data contracts, enforce tenant-level access, support cross-region replication, ensure privacy controls, and validate disaster recovery and cost constraints?","answer":"Partition data by tenant in Delta Lake on ADLS Gen2, secured by Unity Catalog and per-tenant RBAC; ingest through Event Hubs with tenant-based partitions; Databricks notebooks enforce per-tenant acces","explanation":"## Why This Is Asked\n\nTests multi-tenant data isolation, cross-region DR, and cost governance in a realistic feature store pipeline on Azure.\n\n## Key Concepts\n\n- Tenant isolation with Delta Lake and Unity Catalog\n- Event Hubs partitioning and Databricks RBAC\n- Cross-region DR and geo-redundant storage\n- Cost control via autoscale and budgets\n- Data contracts for features and lineage with Purview\n\n## Code Example\n\n```javascript\n// Pseudo-implementation of tenant-scoped feature access\nfunction getFeatures(tenantId, user) {\n  if (user.tenant !== tenantId) throw new Error(\"access denied\");\n  // read from Delta Lake with RBAC context\n}\n```\n\n## Follow-up Questions\n\n- How would you test tenant isolation under bursty load?\n- How would you implement feature versioning and backward compatibility?","diagram":null,"difficulty":"advanced","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Hugging Face","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T05:37:29.115Z","createdAt":"2026-01-20T05:37:29.115Z"},{"id":"q-4726","question":"Design a per-tenant API gateway pattern on Azure for a multi-tenant SaaS: API Management fronts microservices, routes requests by tenant, enforces per-tenant quotas, provides isolation, and logs governance data. How would you implement routing, rate limiting, authentication, and observability to support 5k+ rps with low latency?","answer":"Design a per-tenant API gateway on Azure: API Management fronts microservices, routes by tenant, and enforces per-tenant quotas via policy-based rate limiting backed by Redis. Use Azure AD/OIDC for RB","explanation":"## Why This Is Asked\nThis tests practical multi-tenant API surface design, policy enforcement choices, and performance/security trade-offs across Azure services.\n\n## Key Concepts\n- API gateway choices: APIM vs Front Door vs service mesh\n- Per-tenant routing and data isolation\n- Rate limiting, quotas, circuit breakers, and resilience\n- Authentication/Authorization with Azure AD/OIDC\n- Observability: audit logs, telemetry, governance\n\n## Code Example\n```xml\n<!-- APIM inbound policy for per-tenant rate limiting -->\n<rate-limit-by-key calls=\"1000\" renewal-period=\"60\" counter-key=\"@(context.Variables[\\\"tenantId\\\"])\" />\n```\n\n## Follow-up Questions\n- How would you test tenant isolation and quota enforcement at scale?\n- What strategies minimize downtime when updating routing rules or policies?\n- How would you integrate with a governance catalog (Purview) for lineage and compliance?","diagram":null,"difficulty":"intermediate","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Citadel","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T10:02:10.725Z","createdAt":"2026-01-20T10:02:10.725Z"},{"id":"q-4901","question":"You're building a beginner Azure Functions timer job to enforce per-tenant TTL on logs stored in ADLS Gen2. TTLs are stored in Azure App Configuration per tenant. Describe how you would implement a timer-triggered Python function that reads TTL for each tenant, lists /logs/{tenantId}/ blobs, deletes those older than TTL, and logs actions for auditing. Include safety, idempotency, concurrency considerations, and testing. Also provide a minimal code snippet to delete a single blob given tenantId and blobPath?","answer":"Describe a timer-triggered Python Azure Function that reads per-tenant TTLs from Azure App Configuration, lists ADLS Gen2 paths /logs/{tenantId}/, deletes blobs older than TTL, and logs actions for au","explanation":"## Why This Is Asked\nThis checks practical understanding of timer-triggered functions, per-tenant configuration, and data lifecycle in ADLS Gen2. It also covers safety controls and auditing for automated deletions.\n\n## Key Concepts\n- Timer-triggered Azure Functions (Python)\n- Azure App Configuration with per-tenant keys\n- Azure Data Lake Storage Gen2 (list and delete blobs)\n- TTL-based retention and idempotent deletions\n- Dry-run mode, audit logging, and testing strategies\n\n## Code Example\n```python\n# minimal helper to delete a single blob (tenant scoped)\nfrom azure.identity import ManagedIdentityCredential\nfrom azure.storage.filedatalake import DataLakeServiceClient\n\ndef delete_blob(dls, tenant_id, blob_path, dry_run=False):\n    filesystem = dls.get_file_system_client(file_system='logs')\n    file_client = filesystem.get_file_client(f\"{tenant_id}/{blob_path}\")\n    if dry_run:\n        print(f'DRY-RUN: would delete {tenant_id}/{blob_path}')\n        return\n    file_client.delete_file()\n```\n\n## Follow-up Questions\n- How would you test dry-run mode end-to-end? \n- How would you handle concurrency when many tenants are processed simultaneously? \n- How would you monitor deletions and rollback in case of accidental deletion?","diagram":"flowchart TD\n  A[Timer Trigger] --> B[Load TTLs from App Configuration]\n  B --> C[List /logs/{tenantId}/ for all tenants]\n  C --> D{Blob older than TTL?}\n  D -- Yes --> E[Delete blob]\n  E --> F[Log deletion]\n  D -- No --> G[Skip]\n  F --> H[Next blob]\n  H --> D","difficulty":"beginner","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T17:58:45.913Z","createdAt":"2026-01-20T17:58:45.913Z"},{"id":"q-4965","question":"You're designing a real-time anti-fraud platform for a multi-tenant fintech on Azure. Ingest 500k events/sec from payment gateways via Azure Event Hubs, process with Spark into Delta Lake on ADLS Gen2, enforce per-tenant isolation. Build ML-based scoring in Azure ML, served through AKS with an online feature store in Redis, provide SHAP explanations and drift detection. How would you design data framing, model lifecycle, observability, rollback, and cost controls?","answer":"Partition Delta Lake by tenantId for strict isolation, implement per-tenant feature stores in Redis with TTL-based cache invalidation strategies. Version models in Azure ML registry and deploy to AKS with autoscaling; return SHAP explanations alongside predictions while logging comprehensive telemetry for drift detection.","explanation":"## Why This Is Asked\nTests end-to-end ownership of a high-throughput, multi-tenant fraud system on Azure, including data architecture, ML lifecycle, explainability, drift handling, observability, and cost controls.\n\n## Key Concepts\n- Delta Lake partitioning by tenantId for isolation and query performance\n- Online feature store in Redis with TTL and invalidation strategies\n- Azure ML model registry and AKS deployment with autoscaling\n- SHAP explainability and drift detection triggers\n- Observability via Purview logging and telemetry\n- Cost controls: auto-scaling, data retention policies, and resource optimization","diagram":"flowchart TD\n  Ingest[Event Hubs Ingest] --> Process[Spark Delta Lake] --> Store[Delta Lake on ADLS Gen2]\n  Process --> FeatureStore[Redis Online Feature Store] --> Scoring[AKS Scoring Service]\n  Scoring --> Explain[SHAP Explanations]\n  Explain --> Registry[Azure ML Registry]\n  Registry --> Audit[Purview Auditing]","difficulty":"advanced","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Lyft","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-21T06:06:34.503Z","createdAt":"2026-01-20T21:46:00.217Z"},{"id":"q-5046","question":"Build a beginner, tenant-aware image processing pipeline on Azure. When a user uploads an image to a tenant-specific blob container, trigger a Function that creates a 128x128 thumbnail using Pillow, saves it to a per-tenant folder, and writes status to Cosmos DB. Include per-tenant isolation (tenantId in paths), retry logic, and observable logging with Application Insights. Provide a minimal Python blob-trigger function and outline the end-to-end flow?","answer":"Create a blob-triggered Python Azure Function that processes uploaded images by resizing them to 128x128 thumbnails using Pillow, saves results to tenant-specific storage paths, and updates Cosmos DB with processing status. Implement tenant isolation via path-based segregation (tenant/{tenantId}/thumbnails/), incorporate exponential backoff retry logic for fault tolerance, and enable comprehensive observability through Application Insights logging.","explanation":"## Why This Is Asked\nThis scenario evaluates practical Azure serverless development skills including blob storage triggers, image processing workflows, multi-tenant architecture patterns, error handling strategies, and observability implementation in a production-ready context.\n\n## Key Concepts\n- Azure Blob Storage triggers with tenant-scoped container paths\n- Python image processing using Pillow library\n- Cosmos DB integration with tenant-based partitioning\n- Idempotent operations and exponential backoff retry mechanisms\n- Application Insights integration for end-to-end monitoring\n\n## Code Example\n```python\n# Python Azure Function - blob trigger\nimport logging\nfrom PIL import Image\nfrom io import BytesIO\nimport azure.functions as func\n```","diagram":null,"difficulty":"beginner","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Instacart"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-21T05:07:59.771Z","createdAt":"2026-01-21T02:59:35.292Z"},{"id":"q-5322","question":"You're building a beginner Azure Functions HTTP API to serve per-tenant pricing for a product catalog. Implement an HTTP GET endpoint /pricing/{tenantId}/{productId} that reads tenantId from either a request header X-Tenant-ID or path parameter, checks Redis for a cached price pricing:{tenantId}:{productId} with a 60s TTL, and on miss fetches basePrice from Cosmos DB (PartitionKey=tenantId, id=productId) and discount from Azure App Configuration (key: pricing:discount:tenantId). Apply discount, fall back to global defaults if missing, write result to Redis, and return {price, currency, discountApplied}. Include error handling?","answer":"Use a cache-first flow in Azure Functions (Node.js). Read tenantId from header X-Tenant-ID or route parameter. Check Redis key pricing:{tenantId}:{productId} with 60s TTL. On miss, fetch basePrice from Cosmos DB (PartitionKey=tenantId, id=productId) and discount from Azure App Configuration (key: pricing:discount:tenantId). Apply discount, fall back to global defaults if missing, write result to Redis with 60s TTL, and return {price, currency, discountApplied}. Include comprehensive error handling for Redis failures, Cosmos DB timeouts, and missing configuration values.","explanation":"## Why This Is Asked\nTests practical use of per-tenant data isolation, cache strategy, and basic Azure services integration in a beginner-friendly task.\n\n## Key Concepts\n- Azure Functions HTTP trigger with route parameters\n- Azure Redis Cache: TTL-based caching and error handling\n- Cosmos DB: PartitionKey per tenant for isolation with retry logic\n- Azure App Configuration for per-tenant feature/data with fallbacks\n- Graceful degradation and circuit breaker patterns\n\n## Code Example\n```javascript\nconst { CosmosClient } = require('@azure/cosmos');\nconst { AppConfigurationClient } = require('@azure/app-configuration');\nconst redis = require('redis');\n\nmodule.exports = async function (context, req) {\n  const tenantId = req.headers['x-tenant-id'] || req.params.tenantId;\n  const productId = req.params.productId;\n  const cacheKey = `pricing:${tenantId}:${productId}`;\n  \n  try {\n    // Check Redis cache first\n    const cached = await getCachedPrice(cacheKey);\n    if (cached) {\n      return { status: 200, body: cached };\n    }\n    \n    // Cache miss - fetch from Cosmos DB\n    const basePrice = await fetchBasePrice(tenantId, productId);\n    \n    // Get discount from App Config with fallback\n    const discount = await fetchDiscount(tenantId);\n    \n    // Calculate final price\n    const finalPrice = basePrice.price * (1 - discount);\n    const result = {\n      price: finalPrice,\n      currency: basePrice.currency || 'USD',\n      discountApplied: discount\n    };\n    \n    // Cache result for 60s\n    await setCachedPrice(cacheKey, result, 60);\n    \n    return { status: 200, body: result };\n    \n  } catch (error) {\n    context.log.error('Pricing endpoint error:', error);\n    return { \n      status: 500, \n      body: { error: 'Unable to fetch pricing information' } \n    };\n  }\n};\n\nasync function getCachedPrice(key) {\n  try {\n    const client = redis.createClient(process.env.REDIS_CONNECTION_STRING);\n    const cached = await client.get(key);\n    return cached ? JSON.parse(cached) : null;\n  } catch (error) {\n    console.error('Redis cache read failed:', error);\n    return null; // Continue without cache\n  }\n}\n\nasync function setCachedPrice(key, value, ttlSeconds) {\n  try {\n    const client = redis.createClient(process.env.REDIS_CONNECTION_STRING);\n    await client.setEx(key, ttlSeconds, JSON.stringify(value));\n  } catch (error) {\n    console.error('Redis cache write failed:', error);\n    // Continue without caching\n  }\n}\n\nasync function fetchBasePrice(tenantId, productId) {\n  const client = new CosmosClient(process.env.COSMOS_CONNECTION_STRING);\n  const container = client.database('catalog').container('products');\n  \n  try {\n    const { resource } = await container.item(productId, tenantId).read();\n    if (!resource) throw new Error('Product not found');\n    return resource;\n  } catch (error) {\n    // Fallback to default pricing structure\n    return { price: 100.0, currency: 'USD' };\n  }\n}\n\nasync function fetchDiscount(tenantId) {\n  const client = new AppConfigurationClient(process.env.APP_CONFIG_CONNECTION_STRING);\n  \n  try {\n    const { value } = await client.getConfigurationSetting(\n      { key: `pricing:discount:${tenantId}` }\n    );\n    return parseFloat(value) || 0;\n  } catch (error) {\n    try {\n      // Fallback to global discount\n      const { value } = await client.getConfigurationSetting(\n        { key: 'pricing:discount:global' }\n      );\n      return parseFloat(value) || 0;\n    } catch (globalError) {\n      return 0; // No discount available\n    }\n  }\n}\n```\n\n## Follow-up Questions\n- How would you test cache invalidation and TTL expiry?\n- How would you handle concurrent cache misses to avoid duplicate DB reads?\n- What monitoring would you add to track cache hit/miss ratios?","diagram":"flowchart TD\n  A(Client Request)-->B[Azure Functions HTTP]\n  B-->C{Redis Cache?}\n  C--Yes-->D[Return cached price]\n  C--No-->E[Cosmos DB lookup (tenantId, productId)]\n  E-->F[App Configuration lookup (tenantId)]\n  F-->G[Compute final price]\n  G-->H[Write to Redis (60s TTL)]\n  H-->I[Return response]\n","difficulty":"beginner","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","LinkedIn","Lyft"],"eli5":null,"relevanceScore":null,"voiceKeywords":["azure functions http","per-tenant pricing","cache-first flow","redis cache ttl","cosmos db partitionkey","azure app configuration","route parameters","request header","error handling","graceful degradation","circuit breaker"],"voiceSuitable":true,"isNew":true,"lastUpdated":"2026-01-25T05:10:15.140Z","createdAt":"2026-01-21T17:17:23.574Z"},{"id":"q-5505","question":"You're building a beginner Azure Functions HTTP API to onboard tenants. Implement POST /tenants to onboard a new tenant: validate payload fields tenantId, name, region; read tenantId from header X-Tenant-ID or body; check Redis cache tenants:{tenantId} with 120s TTL; on miss write to Cosmos DB (PartitionKey=tenantId, id=tenantId) with {tenantId, name, region}; publish a message to Service Bus topic onboardingNotifications; ensure idempotency for duplicate requests and return 201 with the tenant record, or 4xx/5xx on error?","answer":"Proposed approach: implement an HTTP trigger with route POST /tenants; parse request body, require tenantId, name, and region fields; prioritize X-Tenant-ID header when present, falling back to body; check Redis hash tenants:{tenantId} for cached tenant data with 120s TTL; on cache miss, perform Cosmos DB upsert using tenantId as both PartitionKey and id; publish onboarding notification to Service Bus topic; implement idempotency by checking cache/database before writes; return 201 status with tenant record on success, or appropriate 4xx/5xx error responses.","explanation":"## Why This Is Asked\n\nThis question evaluates Azure Functions expertise for a practical tenant onboarding workflow, integrating stateless HTTP processing, Redis caching, Cosmos DB persistence, and Service Bus messaging. It also tests understanding of idempotency patterns, comprehensive error handling, and data validation in a distributed system context.\n\n## Key Concepts\n\n- Azure Functions HTTP trigger configuration and routing\n- Redis caching implementation with TTL management\n- Cosmos DB upsert operations using PartitionKey strategy\n- Azure Service Bus topic publishing for event-driven architecture\n- Idempotency patterns for duplicate request handling\n- Robust error handling and appropriate HTTP status codes\n- Request validation and data transformation\n\n## Code Example\n\n```javascript\nmodule.exports = async function (context, req) {\n  // Implementation outline\n  const tenantId = req.headers['x-tenant-id'] || req.body?.tenantId;\n  const { name, region } = req.body;\n  \n  // Validate required fields\n  if (!tenantId || !name || !region) {\n    return { status: 400, body: { error: 'Missing required fields' } };\n  }\n  \n  // Check Redis cache\n  const cacheKey = `tenants:${tenantId}`;\n  let cached = await redis.get(cacheKey);\n  \n  if (!cached) {\n    // Cache miss - write to Cosmos DB\n    await cosmos.upsert({\n      id: tenantId,\n      PartitionKey: tenantId,\n      tenantId,\n      name,\n      region\n    });\n    \n    // Publish to Service Bus\n    await serviceBus.publish('onboardingNotifications', {\n      tenantId,\n      name,\n      region,\n      timestamp: new Date().toISOString()\n    });\n    \n    // Cache with 120s TTL\n    await redis.setex(cacheKey, 120, JSON.stringify({ tenantId, name, region }));\n  }\n  \n  return {\n    status: 201,\n    body: JSON.parse(cached) || { tenantId, name, region }\n  };\n};\n```","diagram":null,"difficulty":"beginner","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T04:06:15.216Z","createdAt":"2026-01-22T02:47:15.601Z"},{"id":"q-5517","question":"You're building a beginner Azure Functions HTTP API to accept per-tenant CSV data uploads. Implement POST /import/{tenantId} (or read tenantId from header X-Tenant-ID) that validates a CSV with headers id,name,timestamp, streams the file to a per-tenant Azure Blob Storage container data/{tenantId}/uploads, and deduplicates by hashing file contents to skip reuploads within 24 hours. Return {status, uploadedBytes, duplicatesSkipped}. Include a minimal Node.js snippet showing the CSV validation, Blob upload, and a simple SHA-256 hash to detect duplicates?","answer":"Read tenantId from the path or X-Tenant-ID header, validate CSV headers (id,name,timestamp), stream to per-tenant blob container data/{tenantId}/uploads, and deduplicate by hashing file contents to sk","explanation":"## Why This Is Asked\nTests practical per-tenant data handling in Azure Functions, including HTTP routing, CSV validation, and blob storage with a simple dedup mechanism.\n\n## Key Concepts\n- HTTP routing and header/parameter extraction\n- CSV validation and streaming\n- Azure Blob Storage for per-tenant isolation\n- Content hashing for idempotency (SHA-256)\n- Lightweight dedup store with TTL (e.g., Redis)\n- Basic error handling (400, 401, 500)\n\n## Code Example\n```javascript\n// Minimal illustrative snippet (pseudo)\nconst { BlobServiceClient } = require('@azure/storage-blob');\nconst crypto = require('crypto');\nmodule.exports = async function(context, req) {\n  const tenantId = (req.params?.tenantId) || req.headers['x-tenant-id'];\n  // validate CSV headers and stream upload to blob\n  // compute hash and check TTL store for duplicates\n  // return summary object\n};\n```\n\n## Follow-up Questions\n- How would you test idempotency and duplicates at scale?\n- How would you handle very large CSV uploads (streaming vs buffering) and backpressure?","diagram":"flowchart TD\n  A[HTTP POST /import/{tenantId}] --> B[Extract tenantId from path/header]\n  B --> C[Validate CSV headers]\n  C --> D[Stream to blob data/{tenantId}/uploads]\n  D --> E[Compute SHA-256 and check 24h TTL store]\n  E --> F{Duplicate?}\n  F -->|Yes| G[Skip upload]\n  F -->|No| H[Upload to Blob]\n  H --> I[Return summary]","difficulty":"beginner","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Oracle","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T04:16:31.762Z","createdAt":"2026-01-22T04:16:31.762Z"},{"id":"q-5575","question":"You're building an Azure-based multi-tenant feature-flag API. Implement GET /feature/{tenantId}/{featureName} returning enabled/disabled with: tenant flags in Cosmos DB, global defaults in App Configuration, 30s Redis cache, regional rollout override, deterministic A/B bucketing via X-Experiment, and per-tenant audit logs to ADLS Gen2. Include error handling and tests?","answer":"Serverless API design: use Azure Functions with a Redis cache (feat:{tenant}:{feature}) for 30s. On miss, fetch tenant flag from Cosmos (PartitionKey=tenant), fallback to App Configuration defaults, t","explanation":"## Why This Is Asked\n\nAssess cross-service integration, tenant isolation, caching strategy, deterministic experimentation, compliance/audit path, and robust failure handling in a scalable azure stack.\n\n## Key Concepts\n\n- Azure Functions HTTP trigger\n- Cosmos DB per-tenant pattern\n- App Configuration for defaults\n- Redis caching with TTL\n- Regional rollout overrides\n- Deterministic A/B bucketing via seed RNG\n- Audit logging to ADLS Gen2\n- Observability and testing\n\n## Code Example\n\n```typescript\nimport { AzureFunction, Context, HttpRequest } from \"@azure/functions\";\n\nconst httpTrigger: AzureFunction = async function (context: Context, req: HttpRequest): Promise<void> {\n  // Skeleton: parse tenantId/featureName, cache, fetch, compute AB bucket, audit, respond\n  context.res = { status: 200, body: { enabled: true } };\n};\nexport default httpTrigger;\n```\n\n## Follow-up Questions\n\n- How would you implement deterministic AB bucketing across tenants with changing rollout percentages?\n- How would you test failure scenarios and ensure audit completeness?","diagram":"flowchart TD\nA[Client Request] --> B[Redis feat:{tenant}:{feature}]\nB -- Miss --> C[Cosmos: tenant flag (PartitionKey=tenant, id=feature)]\nC -- Found --> D[Apply rollout + AB bucket]\nC -- Not Found --> E[App Configuration default]\nD --> F[Audit to ADLS Gen2; cache 30s]\nE --> F\nF --> G[Return result]","difficulty":"advanced","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Salesforce","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T07:05:25.318Z","createdAt":"2026-01-22T07:05:25.318Z"},{"id":"q-5741","question":"You're building a real-time ride dispatch pipeline for a multi-city fleet (Lyft/Uber) on Azure. Ingest GPS pings and trip events at high throughput via Azure Event Hubs, make ETA and dispatch decisions with Durable Functions, store per-city state in Cosmos DB (cityId as PartitionKey), and publish updates to Redis for driver apps and dashboards. How would you design to meet sub-100ms ETA latency, per-city isolation, idempotent retries, and auditable trails across services?","answer":"Partition per city in Cosmos DB (cityId) and use Durable Functions for stateful dispatch. Ingest via Event Hubs with a city-scoped consumer group; compute ETA/dispatch in a low-latency path; cache lat","explanation":"Why This Is Asked\nTests practical Azure design skills for low-latency, scalable dispatch with strict data isolation across cities.\n\nKey Concepts\n- Event Hubs with city-scoped consumers, Durable Functions for workflow state, Cosmos DB partitioning by city\n- Low-latency data path and per-city caching in Redis; idempotent processing via dedup keys\n- Auditing and observability through Cosmos logs and Application Insights; backpressure and DLQ handling\n\nCode Example\n```javascript\n// Pseudo: deduplicate and process ETA update\nasync function processEvent(evt) {\n  const id = `dedup:${evt.eventId}`;\n  if (await redis.exists(id)) return;\n  await redis.set(id, 1, 'EX', 300);\n  // compute ETA, persist, publish to Redis channels\n}\n```\n\nFollow-up Questions\n- How would you test end-to-end latency under burst load and ensure per-city SLA adherence?\n- What tracing and failure-rollback strategies would you add for reliability under partial outages?","diagram":"flowchart TD\n  EH[Event Hubs (city-scoped group)] --> OF[Durable Functions orchestrator]\n  OF --> Redis[Redis (city namespace cache)]\n  OF --> Cosmos[Cosmos DB (city partition)]\n  Cosmos --> Audit[Audit log (Cosmos/Application Insights)]\n  Redis --> Clients[Driver/Rider UIs]","difficulty":"intermediate","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T14:51:02.845Z","createdAt":"2026-01-22T14:51:02.845Z"},{"id":"q-5764","question":"You're building a real-time content recommendation service for a global video conferencing app. Ingest 200k events/sec via Azure Event Hubs, route to tenant-scoped models on Azure ML hosted in AKS, and serve inferences with <100ms latency. Store embeddings in Redis (per-tenant namespace) and metadata in Cosmos DB (PK=tenantId). How would you implement routing, data isolation, drift detection, canary rollouts, and rollback strategies to meet latency and cost goals?","answer":"Route by X-Tenant-ID through a tenant-scoped gateway on AKS, selecting a per-tenant model version from Azure ML. Canary 5-10% traffic to newer versions, increment on success. Store embeddings in Redis","explanation":"## Why This Is Asked\nTests ability to design tenant-isolated, low-latency inference with real-time routing, drift monitoring, and safe rollbacks in a mixed Azure stack.\n\n## Key Concepts\n- Tenant isolation and routing\n- Canary deployments and canary traffic shaping\n- Real-time feature storage and model management\n- Drift detection and rollback triggers\n- Observability and latency budgeting\n\n## Code Example\n```javascript\n// Pseudo-code: choose model version per tenant and route inference\nfunction routeInference(tenantId, event) {\n  const modelVersion = selectVersionForTenant(tenantId);\n  const embedding = redis.get(` Embedding:${tenantId}`);\n  const score = mlService.infer(modelVersion, embedding, event);\n  return score;\n}\n```\n\n## Follow-up Questions\n- How would you test canary rollouts under burst load?\n- How would you handle cross-region data residency and failover?","diagram":"flowchart TD\n  A[Event Hubs Ingest] --> B[AKS Tenant Gateway]\n  B --> C[Azure ML per-tenant model]\n  C --> D[Redis Embeddings (tenant)]\n  D --> E[Cosmos DB (tenant metadata)]\n  C --> F[Drift Alerts (Azure ML)]\n  F --> G[Rollback via Feature Flags]\n  B --> H[Application Insights (latency, metrics)]","difficulty":"intermediate","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T15:45:51.659Z","createdAt":"2026-01-22T15:45:51.659Z"},{"id":"q-5789","question":"Design a GPU-accelerated, multi-tenant AI inference platform on Azure for Nvidia/OpenAI workloads. Deploy containerized model servers on AKS with NVIDIA GPU scheduling, enforcing per-tenant quotas, strict data isolation in logs, and cost-aware autoscaling. Outline the data/model namespace design, GPU partitioning, model lifecycle, monitoring, and rollback/failover strategies. How would you test cross-tenant leakage and latency under load?","answer":"Assign each tenant a dedicated Triton inference server pod on AKS with GPU limits; isolate logs with tenant IDs; use separate model repositories per tenant; enforce quotas via ResourceQuotas and Limit","explanation":"## Why This Is Asked\nThis question probes multi-tenant GPU workloads, isolation, lifecycle, and cost-aware scaling in Azure, a realistic challenge for AI platforms at scale.\n\n## Key Concepts\n- Kubernetes GPU scheduling (NVIDIA device plugin)\n- Tenant isolation (namespaces, secrets, logs)\n- Model lifecycle & canary rollouts\n- Autoscaling strategies and cost controls\n- Observability and auditing across tenants\n\n## Code Example\n```yaml\n# Example per-tenant Triton deployment\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: triton-tenant-a\n  namespace: tenant-a\nspec:\n  replicas: 2\n  template:\n    spec:\n      containers:\n      - name: triton\n        image: nvcr.io/nvidia/tritonserver:22.11-py3\n        resources:\n          limits:\n            nvidia.com/gpu: 1\n        args: [\"tritonserver\", \"--model-repository=/models/tenant-a\"]\n```\n\n## Follow-up Questions\n- How to verify no cross-tenant leakage in logs and artifacts?\n- What metrics and tests validate latency under bursty load?","diagram":"flowchart TD\n  A[Ingress] --> B[Auth & Tenant Routing]\n  B --> C[Tenant Namespace]\n  C --> D[Triton Pod with GPU]\n  D --> E[Telemetry & Logs]\n  E --> F[Azure Monitor & OpenTelemetry]","difficulty":"advanced","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["NVIDIA","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T16:52:24.703Z","createdAt":"2026-01-22T16:52:24.703Z"},{"id":"q-5846","question":"You're building a secure, Azure-native multi-tenant ETL pipeline that ingests logs at scale (20k events/sec per tenant) via Azure Event Hubs, processes with Spark on Databricks into a Delta Lake on ADLS Gen2, and enforces per-tenant retention, masking, and legal holds. Design governance with Purview, per-tenant export controls, and a rolling-key encryption strategy. Outline data layout, failure modes, and testing?","answer":"Architect a multi-tenant ETL: Event Hubs -> Databricks Spark -> Delta Lake on ADLS Gen2; enforce per-tenant isolation, masking, TTL retention, and legal holds. Use envelope encryption with Key Vault B","explanation":"## Why This Is Asked\n\nThis question probes real-world Azure data engineering at scale, including tenant isolation, data governance, and operational testing.\n\n## Key Concepts\n\n- Per-tenant isolation and partitioning\n- Delta Lake on ADLS Gen2\n- Purview governance, encryption with Key Vault BYOK\n- Data masking and retention policies; legal holds\n- End-to-end testing: chaos, restore, erase\n\n## Code Example\n\n```javascript\n// Example masking sketch\nfunction mask(val){ /* masking logic */ return val }\n```\n\n## Follow-up Questions\n\n- How would you test cross-region failover of Delta Lake data?\n- How would you validate complete data erasure across all stores?","diagram":"flowchart TD\n  EH[Event Hubs Ingest] --> DS[Databricks Spark]\n  DS --> DL[Delta Lake on ADLS Gen2]\n  DL --> PUR[Purview Governance]\n  PUR --> END[Analytics/Exports]","difficulty":"advanced","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Oracle","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T19:08:03.756Z","createdAt":"2026-01-22T19:08:03.756Z"},{"id":"q-5867","question":"In a multi-tenant Azure data plane, tenants request GDPR-compliant data export and erasure. Design an end-to-end process using Azure Data Lake Gen2, Cosmos DB, and Purview to execute per-tenant exports and deletions with exactly-once semantics, idempotence, and auditable lineage. Include error handling, retries, and performance implications?","answer":"Leverage a Durable Functions orchestrator per tenant to trigger export and delete jobs. Scan ADLS Gen2 and Cosmos DB with tenant scope, write per-tenant exports, and tombstone markers for deletions. U","explanation":"## Why This Is Asked\n\nInterviews GDPR-compliance and data-plane design across storage, databases, and governance, emphasizing end-to-end guarantees, per-tenant isolation, and auditable lineage.\n\n## Key Concepts\n\n- Durable Functions orchestration\n- Exactly-once semantics and idempotency\n- Per-tenant data isolation in ADLS Gen2 and Cosmos DB\n- Data governance, lineage with Purview\n- Export/delete workflows, retries, and rollback strategies\n\n## Code Example\n\n```javascript\n// Pseudo Durable Functions orchestrator sketch for export/delete\n```\n\n## Follow-up Questions\n\n- How would you test end-to-end guarantees?\n- How would you handle large tenants with partial failures?\n","diagram":null,"difficulty":"advanced","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Two Sigma","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T19:47:04.723Z","createdAt":"2026-01-22T19:47:04.723Z"},{"id":"q-5964","question":"You're implementing a global, low-latency feature-flag evaluation service on Azure for 1,000+ tenants. Peak traffic: 200k requests/sec region-wide. Flags live in Cosmos DB with per-tenant partitions; hot path cached in Redis; edge caching via Front Door. Canary rollouts per tenant. How would you design for sub-20ms tail latency, strict isolation, idempotent retries, and auditable change trails? Include testing and rollback?","answer":"Implement a two-tier caching architecture with Redis as the hot path and Cosmos DB as the source of truth using tenantId as the partition key. Front the service with Azure Front Door integrated with API Management for edge caching and global distribution. Evaluate flags within a Durable Functions orchestrator that first queries Redis, falls back to Cosmos DB on cache misses, and atomically updates Redis. Ensure exactly-once semantics using idempotency keys with Redis TTL. Enforce per-tenant isolation through rate limiting and circuit breakers. Enable canary rollouts via feature flag metadata with gradual traffic splitting. Audit all changes through Event Grid to Log Analytics with immutable storage for compliance.","explanation":"## Why This Is Asked\nThis tests your ability to design a global, low-latency multi-tenant feature flag evaluation service with strict isolation requirements and comprehensive audit trails.\n\n## Key Concepts\n- Two-tier caching strategy (Redis hot path, Cosmos DB source of truth)\n- Edge caching via Azure Front Door for global distribution\n- Exactly-once semantics using idempotency keys and TTL\n- Durable Functions orchestration for complex evaluation workflows\n- Per-tenant isolation through rate limiting and circuit breakers\n- Canary deployment capabilities with gradual traffic splitting\n- Comprehensive audit trails through Event Grid and Log Analytics","diagram":null,"difficulty":"intermediate","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Tesla","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T05:09:28.227Z","createdAt":"2026-01-22T23:50:31.291Z"},{"id":"q-6066","question":"You're designing a per-tenant isolation architecture for a real-time vehicle telemetry analytics service on Azure: use AKS namespaces or a tenant-scoped ML workspace, Event Hubs ingestion, Durable Functions preprocessing, Delta Lake on ADLS Gen2 for tenant data, per-tenant feature flags via App Configuration, data masking in ETL, Purview lineage/audit, TTL policies, and a per-tenant canary rollout with traffic shifting and rollback plan. Include tests and fault-injection?","answer":"Design should enforce tenant isolation via AKS namespaces or dedicated ML workspaces, with Event Hubs for ingestion, Durable Functions for preprocessing, and Delta Lake on ADLS Gen2 for per-tenant dat","explanation":"## Why This Is Asked\n\nThis question probes advanced Azure architecture skills across multi-tenant isolation, streaming ingestion, lakehouse storage, feature flag governance, data masking, and controlled deployment strategies. It also tests testability and disaster recovery planning in a realistic setting.\n\n## Key Concepts\n\n- Per-tenant isolation via AKS namespaces or tenant-scoped ML workspace\n- Ingestion with Event Hubs and event-driven orchestration with Durable Functions\n- Delta Lake on ADLS Gen2 for tenant-scoped storage\n- Feature flags per tenant using Azure App Configuration\n- Data masking during ETL and privacy-by-design\n- Data lineage and audits with Azure Purview\n- TTL policies for data retention\n- Canary rollouts and rollback strategies per tenant\n\n## Code Example\n\n```javascript\n// Pseudo: canary rollout gate per tenant\nfunction shouldRouteToCanary(tenantId, rolloutConfig) {\n  const rate = rolloutConfig[tenantId] || 0;\n  return Math.random() < rate;\n}\n```\n\n## Follow-up Questions\n\n- How would you test tenant isolation boundaries under burst load and simulate data leakage risk?\n- Describe a failure mode and automated rollback process for a tenant during a canary release.","diagram":null,"difficulty":"advanced","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T07:35:11.493Z","createdAt":"2026-01-23T07:35:11.495Z"},{"id":"q-6105","question":"You're building a beginner Azure Functions HTTP API to deliver per-tenant greetings with rate limiting. Implement GET /greet/{tenantId} that reads tenantId from the path or header X-Tenant-ID, validates the tenant, and enforces a per-tenant 1-request-per-second limit using Azure Redis Cache. Use key rate:{tenantId}:count with TTL 1s; return 429 with Retry-After: 1 if limit exceeded; otherwise respond with {\\\"message\\\": \\\"Hello, {tenantId}!\\\"}. Include basic error handling?","answer":"Read tenantId from the path or X-Tenant-ID header, validate against an allowlist, then increment rate:{tenantId}:count in Redis with TTL 1s. If the counter exceeds 1, return 429 with Retry-After: 1; o","explanation":"## Why This Is Asked\nThe task tests basic serverless HTTP handling, per-tenant isolation, and simple rate-limiting logic using Azure Redis Cache. It touches deployment concerns (bindings, configuration) and error handling in a beginner-friendly way.\n\n## Key Concepts\n- Azure Functions HTTP trigger in JavaScript/TypeScript\n- Redis INCR with TTL-based rate limiting\n- Per-tenant isolation and validation\n- Basic error handling and logging\n\n## Code Example\n```javascript\n// Pseudo-code sketch\nmodule.exports = async function (context, req) {\n  const tenantId = req.params.tenantId || req.headers['x-tenant-id'];\n  if (!tenantId) return { status: 400, body: 'Missing tenantId' };\n  // validate allowlist (omitted for brevity)\n  const key = `rate:${tenantId}:count`;\n  const count = await redis.incr(key);\n  if (count === 1) await redis.expire(key, 1);\n  if (count > 1) return { status: 429, headers: { 'Retry-After': '1' }, body: '{\"error\":\"Rate limit\"}' };\n  return { status: 200, body: '{\"message\":\"Hello, '+tenantId+'!\"}' };\n};\n```\n\n## Follow-up Questions\n- How would you test this rate limiter under concurrent requests?\n- How would you adapt this for bursty traffic and distributed tenants?","diagram":"flowchart TD\n  A[Client Request] --> B[Extract tenantId]\n  B --> C{Validate tenant}\n  C -->|invalid| D[Return 400]\n  C -->|valid| E[Redis INCR rate:{tenantId}:count]\n  E --> F{count > 1}?\n  F -->|yes| G[Return 429 Retry-After]\n  F -->|no| H[Return 200 greeting]","difficulty":"beginner","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","NVIDIA","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T08:58:06.451Z","createdAt":"2026-01-23T08:58:06.451Z"},{"id":"q-6208","question":"You're designing a global, multi-tenant fraud-detection pipeline on Azure. Ingest 100k events/sec via Event Hubs, process with Databricks Spark into Delta Lake on ADLS Gen2, enforce per-tenant isolation (schema or partitioning), run a real-time scoring model, and expose an API to fetch recent scores with provenance. How would you architect data layout, model deployment, governance, testing, and failover to meet latency and isolation targets?","answer":"Architect a per-tenant Delta Lake schema (tenant_id partition), enforce isolation via Unity Catalog RBAC, ingest 100k events/sec from Event Hubs into Databricks Spark, compute real-time fraud scores w","explanation":"## Why This Is Asked\nTests experience building scalable, isolated multi-tenant pipelines with real-time scoring, governance, and reliability on Azure. It probes data layout decisions, model deployment, and end-to-end testing in a production context.\n\n## Key Concepts\n- Delta Lake partitioning by tenant_id for isolation\n- Unity Catalog RBAC for governance\n- Event Hubs + Databricks Structured Streaming for low-latency ingest\n- MLflow model versioning and deployment in Databricks\n- Cosmos DB per-tenant storage for scores\n- Redis caching for hot scores and PII masking\n- Purview lineage and OpenTelemetry tracing for end-to-end observability\n- Idempotent retries and chaos testing for resilience\n\n## Code Example\n```python\n# PySpark pseudo-code for streaming ingest to Delta Lake\ndf = spark.readStream.format(\"eventhubs\").load()\ndf2 = df.selectExpr(\"cast(body as string) as message\", \"enqueuedTime\")\ndf2.writeStream.format(\"delta\").option(\"checkpointLocation\", \"/checkpoints/ft\").start(\"/delta/scores\")\n```\n\n## Follow-up Questions\n- How would you detect and respond to model drift across tenants?\n- How would you implement tenant-level SLAs and alerting with cost controls?","diagram":"flowchart TD\n  A[Event Hubs] --> B[Databricks Structured Streaming]\n  B --> C[Delta Lake (tenant partition)]\n  C --> D[Cosmos DB (tenant scores)]\n  D --> E[API layer (provenance)]","difficulty":"advanced","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Databricks"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T14:39:27.986Z","createdAt":"2026-01-23T14:39:27.986Z"},{"id":"q-6295","question":"You're building a multi-tenant telemetry stack for autonomous delivery robots deployed by Instacart-like, Nvidia-like, and Amazon-like customers on Azure. Robots stream telemetry events (position, battery, fault codes) at ~50k-100k events/sec per tenant via IoT Hub to Event Hubs, then into Functions or Spark for enrichment. Design a per-tenant pipeline that delivers sub-200ms fault alerts, strict isolation, exactly-once semantics, cross-region reads, and cost controls. Include data model, windowing, governance, testing, and rollback?","answer":"Outline a design with per-tenant isolation in Cosmos DB (tenantId as partition key), Event Hubs as ingestion, Functions for enrichment, and a real-time alert path (change feed + durable function) targ","explanation":"## Why This Is Asked\nTests real-time telemetry, multi-tenant isolation, and governance under cost constraints.\n\n## Key Concepts\n- IoT Hub, Event Hubs, Functions, Durable Functions\n- Cosmos DB partitions and cross-region replication\n- Change Feed, windowing, exactly-once processing\n- Data governance with Purview, cost controls, rollback strategies\n\n## Code Example\n```json\n{ \"partitionKey\": \"tenantId\" }\n```\n\n## Follow-up Questions\n- How to implement idempotent writes and exactly-once sinks across regions?\n- How would you test canary rollouts and rollback strategies for schema changes?","diagram":"flowchart TD\n  IotHub[IoT Hub] --> EH[Event Hub]\n  EH --> Fn[Functions / Spark]\n  Fn --> CCS[Cosmos DB (per-tenant)]\n  CCS --> Syn[Azure Synapse / Data Explorer]\n  Governance --> CCS","difficulty":"intermediate","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Instacart","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T18:01:44.714Z","createdAt":"2026-01-23T18:01:44.714Z"},{"id":"q-6380","question":"You're building a beginner Azure Functions HTTP API to issue time-limited SAS tokens for per-tenant blob storage access. Implement GET /token/{tenantId}/{container}/{blobName} that reads tenantId from header X-Tenant-ID or path, checks Redis for a cached token with key token:{tenantId}:{container}:{blobName} TTL 300s, and on miss generate a SAS token with read permission for the blob, valid for 5 minutes. Return {token, url, expiry}. Include error handling and fallback to generating a new token if Redis fails?","answer":"I would implement an Azure Function using JavaScript/TypeScript with a GET route that accepts the path parameters `tenantId`, `container`, and `blobName`. First, I'd extract the tenantId from either the `X-Tenant-ID` header or the path parameter as a fallback. Then I'd query Redis for a cached token using the key pattern `token:{tenantId}:{container}:{blobName}` with a 300-second TTL. If found, I'd return the cached token. If not found or if Redis fails, I'd generate a new SAS token with read permissions for the specific blob, valid for 5 minutes, cache it in Redis, and return the response containing the token, URL, and expiry time.","explanation":"## Why This Is Asked\nThis question evaluates practical Azure serverless development skills, including multi-tenant architecture design, caching strategies, and secure Azure Storage integration.\n\n## Key Concepts\n- Azure Functions HTTP triggers and routing\n- Redis caching with TTL management\n- Azure Blob Storage SAS token generation\n- Per-tenant isolation patterns\n- Error handling and fault tolerance\n- Caching fallback strategies\n\n## Code Example\n```javascript\n// Azure Function with Redis caching and SAS generation\nconst { BlobServiceClient } = require('@azure/storage-blob');\nconst redis = require('redis');\n\nmodule.exports = async function (context, req) {\n  const { tenantId, container, blobName } = req.params;\n  const headerTenantId = req.headers['x-tenant-id'];\n  const effectiveTenantId = headerTenantId || tenantId;\n  \n  const cacheKey = `token:${effectiveTenantId}:${container}:${blobName}`;\n  \n  try {\n    // Try Redis cache first\n    const cached = await redis.get(cacheKey);\n    if (cached) {\n      return { status: 200, body: JSON.parse(cached) };\n    }\n  } catch (redisError) {\n    context.log.warn('Redis unavailable, generating new token');\n  }\n  \n  // Generate new SAS token\n  const blobServiceClient = BlobServiceClient.fromConnectionString(process.env.AZURE_STORAGE_CONNECTION);\n  const containerClient = blobServiceClient.getContainerClient(container);\n  const blobClient = containerClient.getBlobClient(blobName);\n  \n  const sasToken = await blobClient.generateBlobSasUrl({\n    permissions: 'r',\n    expiresOn: new Date(Date.now() + 5 * 60 * 1000)\n  });\n  \n  const response = {\n    token: sasToken,\n    url: blobClient.url,\n    expiry: new Date(Date.now() + 5 * 60 * 1000).toISOString()\n  };\n  \n  // Cache the token\n  try {\n    await redis.setex(cacheKey, 300, JSON.stringify(response));\n  } catch (cacheError) {\n    context.log.warn('Failed to cache token');\n  }\n  \n  return { status: 200, body: response };\n};\n```","diagram":"flowchart TD\n  A[HTTP Request] --> B[Resolve tenant]\n  B --> C[Check Redis cache]\n  C --> D{Hit?}\n  D -->|Yes| E[Return cached token]\n  D -->|No| F[Generate SAS]\n  F --> G[Cache token]\n  G --> H[Return token]","difficulty":"beginner","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Microsoft","Plaid","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T05:34:26.357Z","createdAt":"2026-01-23T21:41:54.199Z"},{"id":"q-6606","question":"You're building a beginner Azure Functions HTTP gateway for a multi-tenant SaaS app. Implement GET /authorize?tenantId={tenantId}&action={action} that reads tenantId from the X-Tenant-ID header or query param, checks Redis for a policy decision at policies:{tenantId}:{action} with a 60s TTL, and on a miss fetches allowedActions from Cosmos DB (PartitionKey=tenantId, id='policies'), evaluates, caches the result, and returns {allowed: true|false, reason}. Include error handling?","answer":"Implementation outline: extract tenantId from X-Tenant-ID header or query param; read action from query; check Redis key policies:{tenantId}:{action} with 60s TTL; on miss fetch allowedActions from Co","explanation":"## Why This Is Asked\nTests practical use of caching, cross-store data access, and error handling in a beginner Azure Functions context.\n\n## Key Concepts\n- Azure Functions HTTP trigger\n- Redis caching with TTL\n- Cosmos DB per-tenant data isolation\n- Basic authz decision logic and retries\n\n## Code Example\n```javascript\n// Pseudocode showing the operation flow\n```\n\n## Follow-up Questions\n- How would you test per-tenant isolation and cache invalidation?\n- How would you simulate Redis/Cosmos failures and ensure resilience?","diagram":"flowchart TD\n  A[HTTP request] --> B{TenantId in header or query}\n  B --> C[Extract tenantId]\n  C --> D[Redis lookup policies:{tenant}:{action} TTL60]\n  D --> E{Cache hit?}\n  E -->|Yes| F[Return decision]\n  E -->|No| G[Fetch from Cosmos DB]\n  G --> H[Cache result]\n  H --> F\n  F --> I[Return response]\n","difficulty":"beginner","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","Instacart","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T09:57:00.558Z","createdAt":"2026-01-24T09:57:00.558Z"},{"id":"q-6677","question":"You're building a CDC-style pipeline on Azure for a multi-tenant platform. Ingest via Azure Event Hubs, emit tenant-scoped changes, and apply Delta Lake MERGE upserts partitioned by tenantId in ADLS Gen2. Implement per-tenant masking on reads, TTL via VACUUM, schema evolution via a central registry, and audit trails via Purview. Ensure idempotent processing with offsets and test with backfill, late data, and drift scenarios. How would you architect data layout, governance, and testing?","answer":"Design a CDC-style pipeline: ingest via Event Hubs, emit tenant-scoped changes, and apply Delta Lake MERGE upserts partitioned by tenantId in ADLS Gen2; apply per-tenant masking on reads, TTL via VACU","explanation":"## Why This Is Asked\nTests CDC handling, per-tenant isolation, and governance across Azure services.\n\n## Key Concepts\n- Change data capture with Event Hubs\n- Delta Lake MERGE upserts and tenant partitioning\n- Schema evolution with a central registry\n- Data masking, TTL, Purview auditing\n- Idempotency, late-arriving data, end-to-end testing\n\n## Code Example\n```javascript\n// Pseudo: MERGE into Delta table by tenantId to upsert events\n```\n\n## Follow-up Questions\n- How would you validate schema evolution compatibility across tenants?\n- How would you simulate backfill and ensure no cross-tenant leakage?","diagram":"flowchart TD\n  A[Event Hubs] --> B[Databricks Spark Job]\n  B --> C[Delta Lake on ADLS Gen2]\n  C --> D[Tenant-sealed tables by tenantId]\n  D --> E[Purview auditing & masking on read]\n  C --> F[TTL/VACUUM]\n","difficulty":"advanced","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Google","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T13:17:34.487Z","createdAt":"2026-01-24T13:17:34.487Z"},{"id":"q-6721","question":"You're designing a global multi-tenant telemetry analytics system on Azure. Ingest events via Event Hubs, apply differential privacy at ingestion, compute per-tenant aggregates with Spark in Azure Synapse, store results in Delta Lake on ADLS Gen2, and surface dashboards in Power BI. How would you manage privacy budgets, tenant isolation, testing, and scaling under load?","answer":"Implement per-tenant differential privacy at ingestion, track budgets per tenant in Redis, apply Laplace noise to aggregates, use Spark with tenant_id partitioning in Azure Synapse, store DP-protected","explanation":"## Why This Is Asked\n\nThis question probes practical implementation of privacy-preserving analytics in a multi-tenant Azure environment, focusing on DP budget accounting, isolation, testing, and scalable data paths.\n\n## Key Concepts\n\n- Differential privacy budgets per tenant and accounting\n- Per-tenant data isolation in Delta Lake/ADLS Gen2\n- Scalable ingestion and compute (Event Hubs, Synapse Spark)\n- Observability and auditing with Purview\n\n## Code Example\n\n```javascript\n// pseudo budget allocator (high level)\nfunction allocateNoise(tenantId, desiredDP) {\n  // fetch budget, compare, adjust\n  // return actual noise level\n}\n```\n\n## Follow-up Questions\n\n- How would you validate DP guarantees under burst traffic?\n- What testing strategy ensures no cross-tenant leakage during schema evolution?","diagram":null,"difficulty":"advanced","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Cloudflare"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T14:39:19.931Z","createdAt":"2026-01-24T14:39:19.931Z"},{"id":"q-6769","question":"You're building a real-time risk analytics pipeline for a multi-tenant fintech on Azure. Ingest 100k events/sec from payment gateways via Azure Event Hubs; compute per-tenant risk scores with Databricks Structured Streaming; store raw and enriched data in Delta Lake on ADLS Gen2 with per-tenant partitioning; expose scores via a per-tenant API backed by Synapse. Describe architecture, data model, idempotency, schema evolution, governance, testing, and failover?","answer":"Architect a per-tenant real-time risk pipeline: ingest 100k events/sec from Event Hubs; run Databricks Structured Streaming to compute risk scores per tenant; write raw and enriched data to Delta Lake","explanation":"## Why This Is Asked\nThis probes end-to-end thinking across ingestion, processing, storage, and exposure, with real-world concerns like tenant isolation, idempotency, and fault tolerance.\n\n## Key Concepts\n- Throughput and backpressure handling in Event Hubs and Spark Structured Streaming\n- Delta Lake partitioning by tenant and schema evolution\n- Per-tenant RBAC and data masking in views for governance\n- API surface via Synapse and serverless query\n- Testing strategies and cross-region failover\n\n## Code Example\n```python\n# Pseudo-upsert in Databricks Delta Lake\nfrom delta.tables import DeltaTable\ndef upsert_batch(batch_df, batch_id):\n  delta = DeltaTable.forPath(spark, '/mnt/delta/risk')\n  delta.alias('t').merge(batch_df.alias('s'), 't.event_id = s.event_id AND t.tenant_id = s.tenant_id')\\\n    .whenMatchedUpdate(set={'score':'s.score','ts':'s.ts'})\\\n    .whenNotMatchedInsertAll().execute()\n```\n\n## Follow-up Questions\n- How would you handle late data and watermarking to preserve per-tenant isolation?\n- What monitoring and alerting would you put in place for latency SLAs across components?","diagram":"flowchart TD\n  A[Event Hubs] --> B[Databricks Structured Streaming]\n  B --> C[Delta Lake (ADLS Gen2) - per-tenant partitions]\n  C --> D[Synapse API Layer (per-tenant)]\n  D --> E[BI / Dashboards]\n  E --> F[Data Governance (Purview)]","difficulty":"intermediate","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Databricks"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T16:56:26.323Z","createdAt":"2026-01-24T16:56:26.323Z"},{"id":"q-6982","question":"In a multi-tenant SaaS on Azure, tenants select a primary region for their data. How would you design a residency and disaster-recovery strategy using Cosmos DB (per-tenant partition key), region pairs, and cross-region analytics, ensuring strict per-tenant isolation, defined RPO/RTO, automated failover, and compliant data egress controls?","answer":"Use Cosmos DB with tenantId as the partition key, enable multi-region writes in a primary region with a configured DR region and explicit failover priorities. Target RPO of minutes and RTO under 1 hou","explanation":"## Why This Is Asked\nThis question probes data residency, disaster recovery, and per-tenant isolation in a real SaaS on Azure. It tests practical choices and tradeoffs for uptime, data locality, and governance.\n\n## Key Concepts\n- Cosmos DB multi-region writes with per-tenant isolation\n- Failover priorities and DR testing\n- RPO/RTO targets and cross-region analytics\n- Data egress controls and RBAC\n- Backups, compliance labeling, and DR drills\n\n## Code Example\n```yaml\n# Example residency config (conceptual)\nc CosmosAccount:\n  primaryRegion: \"eastus\"\n  failoverRegion: \"westeurope\"\n  policy: \"regionFailoverPriority\"\n```\n\n## Follow-up Questions\n- How would you automate DR failover with minimal manual steps?\n- What metrics and alerts verify RPO/RTO during drills?\n","diagram":"flowchart TD\n  Tenant[Tenant] --> API[API Layer]\n  API --> Cosmos[(Cosmos DB: tenantId PK)]\n  Cosmos --> DRRegion[DR Region Replicas]\n  API --> Redis[Cache / RBAC]\n  Cosmos --> Analytics[(ADLS Gen2 DR)]","difficulty":"intermediate","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Google","Instacart"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T04:22:31.281Z","createdAt":"2026-01-25T04:22:31.281Z"},{"id":"q-7032","question":"You're building a beginner Azure-native telemetry intake for a fintech platform (Plaid/Stripe-like). Create an Azure Functions HTTP API (POST /telemetry/{tenantId}) that accepts JSON payload up to 1KB, validates tenantId format, ensures per-tenant isolation by partition key, writes a compact summary to Cosmos DB and stores the raw payload in Blob Storage under telemetry/{tenantId}/YYYY/MM/DD/ with a deterministic filename derived from payload hash. Ensure idempotent writes and include basic tests plan?","answer":"Implement an HTTP-triggered Function that validates tenantId, computes a SHA256 hash of the payload, upserts a summary {tenantId, timestamp, hash} in Cosmos DB with partitionKey=tenantId, and writes t","explanation":"## Why This Is Asked\n\nTests ability to build a small, production‑macing, Azure-native ingestion path with explicit per‑tenant isolation, idempotency, and testability using familiar services.\n\n## Key Concepts\n\n- Azure Functions HTTP trigger for a simple API surface\n- Per-tenant isolation via partitionKey in Cosmos DB\n- Idempotent writes using payload hash as idempotency key\n- Blob Storage for raw payloads with date-based pathing\n- Managed Identity for secure service-to-service access\n- Lightweight retry strategy and testability\n\n## Code Example\n\n```javascript\n// Minimal Node.js Azure Function skeleton\nmodule.exports = async function (context, req) {\n  const { tenantId } = req.params;\n  if (!/^[a-zA-Z0-9_-]+$/.test(tenantId)) {\n    context.res = { status: 400, body: 'Invalid tenantId' };\n    return;\n  }\n  const payload = req.body;\n  if (!payload || JSON.stringify(payload).length > 1024) {\n    context.res = { status: 400, body: 'Invalid payload size' };\n    return;\n  }\n  // compute hash (pseudo)\n  const crypto = require('crypto');\n  const hash = crypto.createHash('sha256').update(JSON.stringify(payload)).digest('hex');\n  const ts = new Date().toISOString();\n  // write to Cosmos DB (partitionKey=tenantId, id=hash)\n  // write to Blob Storage at telemetry/{tenantId}/{YYYY/MM/DD}/{hash}.json\n  context.res = { status: 202, body: 'accepted' };\n};\n```\n\n## Follow-up Questions\n\n- How would you test idempotency and race conditions between Cosmos and Blob writes?\n- How would you adjust for larger payloads or higher throughput?","diagram":"flowchart TD\n  A[HTTP POST /telemetry/{tenantId}] --> B[Validate tenantId]\n  B --> C[Hash payload]\n  C --> D{Upsert Cosmos summary}\n  C --> E[Write raw payload to Blob]\n  D --> F[Success]\n  E --> F\n","difficulty":"beginner","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Plaid","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T06:54:26.240Z","createdAt":"2026-01-25T06:54:26.240Z"},{"id":"q-7077","question":"You're building a cross-tenant incident routing system integrating Salesforce and Slack events into an Azure-driven workflow. Ingest events from Salesforce and Slack via Azure Event Hubs (peak ~60k msgs/sec), deduplicate, and route to per-tenant queues. Orchestrate triage with Durable Functions, persist state in Cosmos DB (tenantId as PartitionKey), and log auditable trails. How would you design for latency, fault tolerance, data residency, idempotency, and end-to-end observability across services?","answer":"Use per-tenant Cosmos DB partitioning (tenantId), Event Hubs dedupe by eventId, and a Durable Functions orchestrator that fans out to tenant queues. Enrich in-flight with metadata from Salesforce/Slac","explanation":"## Why This Is Asked\n\nTests ability to design a cross-service, multi-tenant incident routing flow with real-time streaming, serverless orchestration, and strict data isolation. Emphasizes correctness, observability, and rollback in a pragmatic Azure context.\n\n## Key Concepts\n\n- Event deduplication across Event Hubs\n- Durable Functions orchestrations and fan-out\n- Tenant data isolation in Cosmos DB\n- Auditing and traceability with App Insights/OpenTelemetry\n\n## Code Example\n\n```javascript\n// Pseudo snippet: idempotent event handler\n```\n\n## Follow-up Questions\n\n- How would you test idempotency under replay scenarios?\n- How would you implement tenant-specific rate limits and backpressure?","diagram":"flowchart TD\n  Ingest[Ingress: Salesforce/Slack Events via Event Hubs] --> Route[Routing & Deduplication]\n  Route --> Orchestrator[Durable Functions Orchestrator]\n  Orchestrator --> Cosmos[Cosmos DB (tenantId)]\n  Cosmos --> Audit[Auditable Trails]\n  Orchestrator --> Queues[Tenant Queues]\n  Ingest --> Metadata[Enrichment: Salesforce/Slack via Managed Identity]\n  Queues --> End[Incident Triage & Notification]","difficulty":"intermediate","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Salesforce","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T08:40:21.657Z","createdAt":"2026-01-25T08:40:21.657Z"},{"id":"q-7273","question":"You're designing a two-region, active-active telemetry pipeline for a multi-tenant SaaS app. Ingest via Event Hubs, process with Azure Functions, store per-tenant data in Cosmos DB multi-region write with TTL and masking, and expose metrics via Power BI. How would you ensure cross-region consistency, per-tenant isolation, disaster recovery, and cost control while supporting hot failover and audit trails?","answer":"Implement paired Event Hubs and Cosmos DB with tenant partition keys, enable multi-region writes and bounded-staleness, mask PII at ingestion, TTL enabled, and use App Configuration for per-tenant fea","explanation":"## Why This Is Asked\nTests knowledge of multi-region Azure architecture, per-tenant isolation, and DR/QA practices beyond basic streaming.\n\n## Key Concepts\n- Cross-region replication and consistency in Cosmos DB\n- Event Hubs paired namespaces and disaster recovery\n- Tenant-aware masking, TTL, feature flags, cost controls\n- Chaos testing and resilient replay strategies\n\n## Code Example\n```javascript\n// pseudo\n// function to mask PII and write to tenant partition\n```\n\n## Follow-up Questions\n- How would you monitor cross-region replication lag?\n- How do you test failover without impacting tenants?","diagram":"flowchart TD\n  Ingest[Event Hubs] --> Process[Azure Functions]\n  Process --> Store[Cosmos DB (Tenant PK)]\n  Store --> Route[Front Door / App Config]\n  Route --> Metrics[Power BI]","difficulty":"advanced","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T16:39:07.117Z","createdAt":"2026-01-25T16:39:07.117Z"},{"id":"q-7433","question":"You're designing a real-time analytics pipeline on Azure for a multi-tenant fintech app. Ingest 100k events/sec via Azure Event Hubs, process with Functions and Durable Functions writing Delta Lake on ADLS Gen2, ensuring per-tenant isolation. Implement tenant-scoped encryption using customer-managed keys in Azure Key Vault, envelope encryption in the write path, and rotate keys with automated policies. How would you design key governance, access control, and latency considerations while maintaining observability and auditability?","answer":"Implement tenant-scoped customer-managed keys (CMKs) in Azure Key Vault, accessed via managed identities from Azure Functions and Durable Functions. Apply envelope encryption: generate per-tenant data encryption keys (DEKs), encrypt events with the DEK, then wrap the DEK with the tenant's CMK for storage. Establish automated key rotation policies and enforce access controls through role-based access control (RBAC) and managed identities. Optimize latency by caching DEKs in memory with TTL, implementing batch writes to Delta Lake, and using Event Hubs throughput units. Ensure observability through Azure Monitor metrics for encryption operations, Purview for data lineage, and comprehensive audit logging in Key Vault and storage accounts.","explanation":"## Why This Is Asked\nTests secure, scalable key governance for a multi-tenant real-time analytics pipeline on Azure.\n\n## Key Concepts\n- Tenant-scoped CMKs in Azure Key Vault\n- Envelope encryption with per-tenant DEKs\n- Automated key rotation and governance policies\n- RBAC and managed identity-based access control\n- Latency optimization through caching and batching\n- Observability via Azure Monitor and Purview\n- Comprehensive audit trails and compliance logging\n\n## Code Example\n```javascript\n// Envelope encryption implementation\nimport { KeyClient } from '@azure/keyvault-keys';\nimport { DefaultAzureCredential } from '@azure/identity';\n\nconst keyClient = new KeyClient(\n  process.env.KEY_VAULT_URI,\n  new DefaultAzureCredential()\n);\n\nasync function encryptTenantData(tenantId, data) {\n  // Generate per-tenant DEK\n  const dek = await crypto.subtle.generateKey(\n    { name: 'AES-GCM', length: 256 },\n    true,\n    ['encrypt', 'decrypt']\n  );\n  \n  // Encrypt data with DEK\n  const encryptedData = await crypto.subtle.encrypt(\n    { name: 'AES-GCM', iv: crypto.getRandomValues(new Uint8Array(12)) },\n    dek,\n    new TextEncoder().encode(data)\n  );\n  \n  // Wrap DEK with tenant's CMK\n  const cmk = await keyClient.getKey(`tenant-${tenantId}-cmk`);\n  const wrappedDek = await crypto.subtle.wrapKey(\n    'AES-GCM',\n    dek,\n    cmk,\n    { name: 'AES-GCM' }\n  );\n  \n  return { encryptedData, wrappedDek };\n}\n```","diagram":"flowchart TD\n  EH[Event Hubs] --> FW[Functions/Durable Functions]\n  FW --> DL[Delta Lake (ADLS Gen2)]\n  KV[Key Vault (CMKs)] --> DL\n  DL --> PUR[Purview Audit Trail]\n  RBAC[IAM/RBAC] --> FW","difficulty":"advanced","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T06:01:36.862Z","createdAt":"2026-01-25T22:47:17.175Z"},{"id":"q-7441","question":"You are building a beginner Azure Functions HTTP API for per-tenant image uploads. Each tenant uploads images to a tenant/{tenantId}/images/ blob path and receives a SAS URL for direct upload. Validate tenantId from a header, enforce 2 MB max file size, and return the SAS URL or an error. Include a minimal Node.js SAS snippet and a test for isolation?","answer":"Implement an HTTP-triggered Azure Function that extracts the tenantId from the request header, constructs a tenant-scoped blob path (tenant/{tenantId}/images/), validates that the requested file size does not exceed 2 MB, and generates a SAS URL with write permissions for direct client upload.","explanation":"## Why This Is Asked\nThis question tests practical Azure Functions implementation skills, demonstrates understanding of multi-tenant architecture through proper data isolation, and showcases knowledge of secure upload patterns using SAS tokens without requiring complex backend services.\n\n## Key Concepts\n- Azure Functions HTTP triggers and request processing\n- Tenant-based data isolation through blob path scoping\n- Shared Access Signature (SAS) tokens for secure client-side uploads\n- File size validation and comprehensive error handling\n- Proper authentication and authorization patterns\n\n## Code Implementation\n\nThe solution requires:\n1. HTTP trigger function with tenant validation\n2. Blob storage client for SAS token generation\n3. Request validation for file size limits\n4. Error handling for unauthorized or invalid requests\n5. Tenant-specific path construction for data isolation\n\nThis approach ensures secure, scalable image uploads while maintaining strict tenant separation and resource limits.","diagram":"flowchart TD\n  A[Client Request] --> B[Validate tenantId header]\n  B --> C[Compute path tenant-{id}-images/{filename}]\n  C --> D[Generate SAS token]\n  D --> E[Return SAS URL to client]","difficulty":"beginner","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Scale Ai","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T05:55:47.829Z","createdAt":"2026-01-25T23:38:15.612Z"},{"id":"q-7480","question":"You're building a geo-telemetry processing pipeline for a multi-tenant ride-hailing platform on Azure. Ingest 150k events/sec from vehicles via Azure Event Hubs, enrich with Azure Maps in a Durable Functions chain, aggregate per-tenant metrics in Cosmos DB, and serve dashboards via a Web API. How would you design for per-tenant isolation, exactly-once processing, scaling, failure handling, and cost control? Include governance and testing plan?","answer":"Partition Event Hubs by tenant; use a Durable Functions orchestrator to chain enrichment and per-tenant aggregation; store results in Cosmos DB with tenantId as PartitionKey; implement idempotency through event deduplication using Event Hub sequence numbers; apply backpressure via consumer group scaling; enforce per-tenant rate limits; utilize Azure Cost Management alerts; implement circuit breakers for external services; establish data retention policies per tenant; create comprehensive integration and load testing plans.","explanation":"## Why This Is Asked\nAssess real-world telemetry pipelines, per-tenant isolation, and exactly-once semantics under high throughput.\n\n## Key Concepts\n- Event Hubs partitioning by tenant\n- Durable Functions orchestration\n- Per-tenant Cosmos DB partitioning\n- Idempotency and exactly-once processing\n- Backpressure and scaling, cost controls\n- Data governance and testing\n\n## Code Example\n```typescript\n// Pseudo-Durable Functions orchestrator\nconst orchestrator = function*(context) {\n  const evt = context.getInput();\n  const enriched = yield context.callActivity('EnrichGeo', evt);\n  const agg = yield context.callActivity('AggregateMetrics', enriched);\n  return agg;\n};\n```","diagram":"flowchart TD\n  A[Vehicle Telemetry] --> B[Event Hubs (tenant partition)]\n  B --> C[Durable Functions Orchestrator]\n  C --> D[Azure Maps Enrichment]\n  D --> E[Cosmos DB (tenant partition)]\n  E --> F[Real-time Dashboard API]","difficulty":"intermediate","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Scale Ai","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T05:22:43.080Z","createdAt":"2026-01-26T02:59:24.325Z"},{"id":"q-7513","question":"You're designing a global, multi-tenant real-time anomaly detection service for industrial IoT on Azure. Ingest from sites via Azure IoT Hub, run per-tenant models loaded from Blob storage, and serve sub-100 ms inferences at scale. Describe how you'd achieve strict per-tenant isolation, model versioning and canary rollouts, cost-aware autoscaling, and auditable governance. Include latency tests, security (MSI/RBAC), and testing plans?","answer":"Isolate per-tenant with separate Cosmos DB containers and Blob storage; versioned models in Blob with a Redis registry; hot-swap via tenant flags and canary shards; autoscale inference with Functions ","explanation":"## Why This Is Asked\nShows ability to design scalable, secure multi-tenant real-time inference with governance and cost controls.\n\n## Key Concepts\n- Tenant isolation via storage and compute boundaries\n- Model versioning and canary rollouts per tenant\n- Sub-100 ms latency, caching, and warm pools\n- Observability, auditing, and secure access\n\n## Code Example\n```javascript\n// Skeleton: tenant model registry access\nconst version = registry[tenantId] || 'latest';\nconst model = loadModelIfCached(tenantId, version);\n```\n\n## Follow-up Questions\n- How would you test and roll out canaries across tenants safely?\n- How would you handle drift or failure without impacting other tenants?","diagram":null,"difficulty":"advanced","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","NVIDIA","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T05:40:26.730Z","createdAt":"2026-01-26T05:40:26.731Z"},{"id":"q-7710","question":"You're building a multi-tenant Azure analytics pipeline for IoT telemetry from thousands of devices across regions. Ingest via Azure Event Hubs, process with Spark on Synapse for near-real-time enrichment, store per-tenant data in Delta Lake on ADLS Gen2 with strict isolation, and expose fast queries via Synapse or Cognitive Search. How would you guarantee end-to-end latency <200ms, per-tenant isolation, schema evolution handling, audit trails, and cost control with autoscale? Include testing and rollback plans?","answer":"Design with tenantId as the partition key across ingestion, processing, and Delta Lake storage. Use Structured Streaming with watermarking and backpressure to target sub-200ms end-to-end latency; isol","explanation":"## Why This Is Asked\nAssess end-to-end telemetry pipeline design under tenancy, latency, and governance pressures in Azure.\n\n## Key Concepts\n- Tenant isolation across Event Hubs, Spark, and Delta Lake\n- End-to-end latency targets and backpressure handling\n- Data governance, lineage, and encryption with CMK\n- Auto-scaling, testing strategies, and rollback plans\n\n## Code Example\n```javascript\n// Pseudo Spark Structured Streaming setup (Scala/Java-like syntax in JS for brevity)\nconst stream = spark.readStream...\nstream.writeStream.format('delta').option('path','/datalake/tenantId=...').start()\n```\n\n## Follow-up Questions\n- How would you test schema evolution and backward compatibility in this lakehouse?\n- How would you detect and recover from a burst-induced backpressure event?","diagram":null,"difficulty":"intermediate","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Meta","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T14:45:34.478Z","createdAt":"2026-01-26T14:45:34.478Z"},{"id":"q-7744","question":"You're building a global, multi-region analytics lake on Azure for per-tenant data. Ingest via Event Hubs, store Delta Lake on ADLS Gen2 in Region A and Region B with cross-region replication. Describe data partitioning, per-tenant isolation, cross-region ACID guarantees, governance via Purview, and a coordinated failover plan to meet RPO/RTO targets. What challenges and trade-offs would you surface?","answer":"Two active Azure regions, each hosting Delta Lake on ADLS Gen2 with tenantId partitions. Ingest via Event Hubs and mirror data with cross-region Delta sharing and ADLS replication. Purview catalogs pe","explanation":"## Why This Is Asked\n\nThis design probes cross-region DR, per-tenant isolation, and governance integration in a lakehouse, plus operational testing.\n\n## Key Concepts\n\n- Delta Lake ACID across regions\n- Cross-region replication and latency\n- Per-tenant isolation and dynamic masking\n- Purview governance and lineage\n- DR orchestration with RPO/RTO targets\n\n## Code Example\n\n```python\n# Pseudocode: tenant-filter for isolation in Spark\nfrom pyspark.sql import functions as F\n\ndef tenant_view(df, tenant_id):\n    return df.filter(F.col('tenant_id') == F.lit(tenant_id))\n```\n\n## Follow-up Questions\n\n- How would you validate replication lag and cross-tenant leakage during failover?\n- What testing strategies ensure data integrity after region failover?","diagram":"flowchart TD\n  Ingest[Event Hubs] --> RegionA[Delta Lake Region A]\n  Ingest2[Event Hubs] --> RegionB[Delta Lake Region B]\n  RegionA --> DR[DR Orchestrator]\n  RegionB --> DR\n  DR --> Analytics[Analytics workloads]","difficulty":"advanced","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T15:53:04.715Z","createdAt":"2026-01-26T15:53:04.716Z"},{"id":"q-7987","question":"You're building a beginner Azure-native image-upload service. A mobile app uploads photos to ADLS Gen2 under per-tenant folders tenantId/images/. Create an HTTP function that issues a SAS token allowing a single blob upload to tenantId/images/{filename}, expiring in 15 minutes, scoped to that tenant. Also require server-side audit log to a per-tenant blob. Provide a brief Node.js code sketch to issue the SAS and outline tests for tenant isolation and token expiry?","answer":"Implement a SAS token generator for a single blob in tenantId/images/{filename}, expiring in 15 minutes, scoped to the tenant path. Use BlobServiceClient with a StorageSharedKeyCredential, generate a ","explanation":"## Why This Is Asked\n\nTests understanding of per-tenant data isolation, Azure Storage SAS scope, and minimal server-side auditing in a beginner setting. It also touches Node.js SDK usage and basic security considerations without heavy infrastructure.\n\n## Key Concepts\n\n- Azure Storage SAS for scoped permissions\n- Per-tenant folder structure in ADLS Gen2\n- Lightweight auditing via blob/log container\n- Time-limited tokens and basic validation\n\n## Code Example\n\n```javascript\n// Minimal SAS snippet (conceptual)\nconst { BlobServiceClient, StorageSharedKeyCredential, generateBlobSASQueryParameters, BlobSASPermissions } = require('@azure/storage-blob');\nconst account = process.env.ACCOUNT_NAME;\nconst accountKey = process.env.ACCOUNT_KEY;\nconst cred = new StorageSharedKeyCredential(account, accountKey);\nconst blobName = `${tenantId}/images/${filename}`;\nconst expiresOn = new Date(Date.now() + 15 * 60 * 1000);\nconst sas = generateBlobSASQueryParameters({ containerName: tenantId, blobName, permissions: BlobSASPermissions.parse('w'), expiresOn }, cred).toString();\n// Return SAS to client for direct upload\n```\n\n## Follow-up Questions\n\n- How would you rotate keys and migrate to managed identity for SAS generation?\n- How would you test for cross-tenant leakage and SAS revocation?","diagram":"flowchart TD\n  Client[Mobile App] --> Fn[HTTP Trigger Function]\n  Fn --> SAS[Generate SAS for tenant/images/{filename}]\n  SAS --> Upload[Client uploads to ADLS Gen2]\n  Upload --> Audit[Audit log container (per-tenant)]\n  Audit --> Monitor[Diagnostics/Alerts]","difficulty":"beginner","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T04:23:31.222Z","createdAt":"2026-01-27T04:23:31.223Z"},{"id":"q-8069","question":"You're building a global IoT telemetry pipeline. Devices stream to Azure IoT Hub in multiple regions; offline devices use edge modules to batch data before upload. Implement strict per-tenant isolation, storing analytics-ready data in Delta Lake on ADLS Gen2 with encryption at rest, and ensure secure transit. Design end-to-end processing, schema evolution, retention, failure modes, and testing strategy?","answer":"Adopt per-tenant namespaces in Delta Lake (PartitionKey/tenantId) with ADLS Gen2 access via Managed Identities; encrypt data using CMK in Key Vault; encrypt in transit with TLS; ingest via IoT Hub -> ","explanation":"## Why This Is Asked\n\nTests end-to-end IoT data governance, multi-region isolation, and production-grade processing with Azure services, plus practical storage and security constraints.\n\n## Key Concepts\n- IoT Hub and Edge processing for offline devices\n- Tenant isolation in Delta Lake on ADLS Gen2\n- Encryption at rest (CMK) and in transit (TLS)\n- Ingestion chain: IoT Hub -> Event Hubs -> Databricks Spark\n- Schema evolution, retention, idempotency, and checkpointing\n\n## Code Example\n\n```javascript\n// Pseudocode: idempotent upsert in Spark streaming\nval df = readStream(...)\nval deduped = df.withWatermark(\"timestamp\", \"1h\").dropDuplicates(\"tenantId\", \"eventId\")\ndeduped.writeStream.format(\"delta\").option(\"checkpointLocation\", \"csLocation\").start(\"deltaLakePath\")\n```\n\n## Follow-up Questions\n- How would you validate tenant data isolation in testing? \n- What strategies for schema evolution and rolling upgrades would you use in Delta Lake?\n","diagram":"flowchart TD\n  A[Devices] --> B[IoT Hub] \n  B --> C[Event Hubs] \n  C --> D[Databricks Delta Lake] \n  D --> E[Delta Store / Access Policy]\n  E --> F[Purview & Key Vault (CMK) & Monitoring]\n  D --> G[Edge Gateway]","difficulty":"advanced","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Lyft","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T07:58:58.535Z","createdAt":"2026-01-27T07:58:58.535Z"},{"id":"q-8128","question":"You're building a minimal per-tenant inventory API on Azure Functions using Cosmos DB. Implement POST /items with body {itemId,name,qty} and Idempotency-Key header to upsert a document with id = tenantId-itemId and partition key tenantId; GET /items/{tenantId} to list items. Explain data model, idempotency, retry/backoff, and testing strategy?","answer":"Implement a per-tenant inventory API with Azure Functions and Cosmos DB. Use partition key tenantId, POST /items with body {itemId,name,qty} and Idempotency-Key header; perform upsert with id = tenant","explanation":"## Why This Is Asked\nTests basic serverless API design, tenant isolation, and resilience in Azure.\n\n## Key Concepts\n- Azure Functions HTTP triggers\n- Cosmos DB partitioning and upsert\n- Idempotency with Idempotency-Key header\n- Retry/backoff strategies and testability\n\n## Code Example\n```javascript\n// Node.js example for upsert using Cosmos DB SDK\nconst { CosmosClient } = require(\"@azure/cosmos\");\n\nasync function upsertItem(client, tenantId, item) {\n  const id = `${tenantId}-${item.itemId}`;\n  const { resource } = await client\n    .database(\"InventoryDB\")\n    .container(\"Items\")\n    .item(id, tenantId)\n    .replace({ id, tenantId, ...item });\n  return resource;\n}\n```\n\n## Follow-up Questions\n- How would you validate idempotency keys and handle duplicates?\n- How would you test isolation across tenants?","diagram":null,"difficulty":"beginner","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","IBM","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T10:56:47.144Z","createdAt":"2026-01-27T10:56:47.144Z"},{"id":"q-8254","question":"Design a global per-tenant feature-flag and pricing experiment platform on Azure. Ingest 200k events/sec from client apps via Azure Event Hubs; evaluate feature flags in near-real-time using Durable Functions orchestration or a small AKS service; store per-tenant variant results in Cosmos DB; expose decisions via CDN. Ensure deterministic traffic split by tenant, strict data residency, audit trails, and safe rollback with canary updates. What would you implement and why?","answer":"Implement a multi-region Azure stack: ingest events via Event Hubs, orchestrate with Durable Functions, store per-tenant results in Cosmos DB, and gate traffic with a tenant-hash in Redis for determin","explanation":"## Why This Is Asked\n\nAssess real-world architecture for per-tenant experimentation with strict isolation and rollback.\n\n## Key Concepts\n\n- Event-driven design with durable orchestration\n- Deterministic tenant-level traffic splitting\n- Per-tenant data isolation in Cosmos DB\n- Canary updates and rollback semantics\n- Observability and governance via Purview/Insights\n\n## Code Example\n\n```javascript\n// Durable Functions orchestrator skeleton\nmodule.exports = async function(context) {\n  const input = context.df.getInput();\n  // route to feature variant evaluation\n  const variant = await context.df.callActivity('EvaluateFlags', input);\n  return variant;\n};\n```\n\n## Follow-up Questions\n\n- How would you ensure deterministic A/B split across tenants at scale?\n- How would you test rollback of a new feature flag release?","diagram":null,"difficulty":"intermediate","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T17:00:05.498Z","createdAt":"2026-01-27T17:00:05.498Z"},{"id":"q-8392","question":"You're building a beginner Azure-native appointment-scheduler API for multi-tenant clinics. Tenants call POST /appointments with date/time. Use Azure Functions (JavaScript) HTTP trigger, store bookings in Cosmos DB with partitionKey tenantId and id tenantId|appointmentTime, implement idempotency, and use Redis for deduplication. Explain data model, idempotency, and failure handling. Provide a minimal code snippet for idempotent function?","answer":"Implement an HTTP-triggered Azure Function that receives {tenantId, appointmentTime}. Use Cosmos DB with partitionKey = tenantId and id = tenantId|appointmentTime to ensure per-tenant isolation and id","explanation":"## Why This Is Asked\nTests practical Azure knowledge: serverless HTTP endpoints, multi-tenant data isolation, idempotency, and simple deduplication.\n\n## Key Concepts\n- Cosmos DB partitionKey design for per-tenant isolation\n- Idempotent writes using a composite id tenantId|appointmentTime\n- Lightweight deduplication with Redis TTL\n- Basic error handling for conflicts and failures\n\n## Code Example\n```javascript\nmodule.exports = async function (context, req) {\n  const { tenantId, appointmentTime } = req.body || {};\n  const id = `${tenantId}_${appointmentTime}`;\n  // Pseudo: check Redis cache for id\n  // if exists return {status: 409}\n  // else write to Cosmos DB with {id, tenantId, appointmentTime} and set Redis key with TTL 60\n  context.res = { status: 201, body: { bookingId: id } };\n}\n```\n\n## Follow-up Questions\n- How would clock skew affect idempotency and how to mitigate?\n- How would you extend for partial failures (e.g., Redis write succeeds, Cosmos write fails)?","diagram":"flowchart TD\n  A[HTTP POST /appointments] --> B[Validate input & tenantId]\n  B --> C[Compute id = tenantId|appointmentTime]\n  C --> D{Redis has id?}\n  D -->|yes| E[Return 409]\n  D -->|no| F[Insert into Cosmos DB with partitionKey=tenantId]\n  F --> G[Set Redis key with TTL 60s]\n  G --> H[Return 201 with bookingId]\n","difficulty":"beginner","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Google","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T22:49:43.878Z","createdAt":"2026-01-27T22:49:43.878Z"},{"id":"q-8438","question":"You're building a beginner Azure Functions API for a multi-tenant app where each tenant uses its own schema in Azure SQL Database. Implement GET /tenant/{tenantId}/users to return active users for that tenant. Describe how you map tenantId to a schema, ensure isolation, how you configure connections, and how you cache responses in Redis with a TTL. Provide a minimal Node.js snippet that demonstrates selecting the schema and a parameterized query to fetch users?","answer":"Map each tenantId to a dedicated schema in Azure SQL Database. Validate schema names against an allowlist to prevent injection, then use connection pooling with Managed Identity authentication. Execute parameterized queries with dynamic schema selection: `SELECT Id, Username, Email FROM [${schema}].Users WHERE IsActive = 1`. Cache results in Redis using tenant-specific keys with a 5-minute TTL: `redis.setex('users:${tenantId}', 300, JSON.stringify(results))`.","explanation":"## Why This Is Asked\n\nThis question evaluates fundamental multi-tenant architecture skills including data isolation, dynamic SQL execution, and caching strategies in serverless applications.\n\n## Key Concepts\n\n- Per-tenant schema mapping for data isolation\n- Safe dynamic SQL with schema validation\n- Azure Functions HTTP API implementation\n- Managed Identity for secure database access\n- Redis caching with TTL optimization\n- Parameterized queries preventing SQL injection\n- Connection pooling for performance\n\n## Code Example\n\n```javascript\n// Minimal snippet demonstrating schema selection and query\nasync function getTenantUsers(tenantId) {\n  const schema = validateTenantSchema(tenantId);\n  const cacheKey = `users:${tenantId}`;\n  \n  // Check Redis cache first\n  const cached = await redis.get(cacheKey);\n  if (cached) return JSON.parse(cached);\n  \n  // Execute parameterized query with dynamic schema\n  const query = `SELECT Id, Username, Email FROM [${schema}].Users WHERE IsActive = 1`;\n  const results = await sql.query(query);\n  \n  // Cache results for 5 minutes\n  await redis.setex(cacheKey, 300, JSON.stringify(results));\n  return results;\n}\n```","diagram":"flowchart TD\n  A[Client request] --> B{Tenant mapping}\n  B --> C[Validate schema]\n  C --> D[SQL query]\n  D --> E[Redis cache]\n  E --> F[Return results]","difficulty":"beginner","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","Meta","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-28T05:52:15.881Z","createdAt":"2026-01-28T02:31:16.192Z"},{"id":"q-8476","question":"You're building a beginner Azure-native webhook receiver for three partners. An Azure Functions HTTP API ingests events, stores per-tenant metadata in Cosmos DB with partitionKey=tenantId, and enqueues processing to a Service Bus topic for durable work queues. Design authentication using Managed Identity, per-tenant isolation, idempotent processing using eventId, and a testing strategy for failure modes and data consistency?","answer":"Create an HTTP-triggered Azure Function that reads tenantId from a header, authenticates using Managed Identity to access Cosmos DB and Service Bus, and writes per-tenant metadata to Cosmos DB with pa","explanation":"## Why This Is Asked\n\nGauges practical Azure-native workflow design, tenant isolation, and basic reliability patterns in a beginner-friendly setting.\n\n## Key Concepts\n\n- Azure Functions HTTP trigger and managed identity\n- Cosmos DB partitioning by tenantId\n- Service Bus for durable, decoupled processing\n- Idempotency with eventId\n- Basic testing: unit, integration, failure-mode checks\n\n## Code Example\n\n```javascript\nmodule.exports = async function (context, req) {\n  const tenantId = req.headers['x-tenant-id'];\n  // authenticate via Managed Identity and route accordingly\n  // persist tenant data to Cosmos DB with partitionKey = tenantId\n  // publish eventId-enabled message to Service Bus\n  context.res = { status: 200, body: 'ok' };\n}\n```\n\n## Follow-up Questions\n\n- How would you implement deduplication across multiple function instances?\n- What metrics would you collect to monitor latency and reliability?","diagram":"flowchart TD\n  A[HTTP Ingest] --> B[Validate tenantId header]\n  B --> C{Tenant authorized?}\n  C -- Yes --> D[Persist in Cosmos DB (partitionKey=tenantId)]\n  C -- Yes --> E[Publish to Service Bus]\n  E --> F[Worker processes]\n  C -- No --> G[Return 401]","difficulty":"beginner","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Lyft","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-28T04:31:08.318Z","createdAt":"2026-01-28T04:31:08.318Z"},{"id":"q-891","question":"Design a beginner-friendly serverless image-upload API on Azure: clients POST JPEGs to an HTTP-triggered Function, which saves the file to Blob Storage, enqueues a processing job, and stores a metadata record in Cosmos DB. How would you ensure idempotency, handle retries with back-off, and keep costs predictable on a Consumption plan?","answer":"Implement idempotency via an idempotency-key header; persist seen keys in Cosmos DB; on duplicates, return 200 and skip work. Decouple with a queue; HTTP function enqueues, blob saved. Processing func","explanation":"## Why This Is Asked\n\nTests practical serverless data flow, idempotency, and cost control in Azure. It also touches inter-service communication and observability.\n\n## Key Concepts\n\n- Idempotent HTTP endpoints using an idempotency key\n- Decoupling with HTTP → Blob → Queue → Worker\n- Cosmos DB for idempotency store and metadata\n- Retry/backoff and dead-lettering\n- Cost-conscious design on Consumption plan\n\n## Code Example\n\n```javascript\n// Skeleton: HTTP trigger checks idempotency key, writes blob, enqueues, and stores metadata\nmodule.exports = async function (context, req) {\n  const key = req.headers[\"x-idempotency-key\"];\n  // look up key in Cosmos DB; if exists, return 200\n  // otherwise, save blob, enqueue job, write metadata, and return 202\n};\n```\n\n## Follow-up Questions\n\n- How would you test idempotency guarantees in this flow?\n- What metrics would you collect to validate retry/backoff behavior and cost control?","diagram":null,"difficulty":"beginner","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Robinhood","Salesforce","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T14:31:34.107Z","createdAt":"2026-01-12T14:31:34.107Z"},{"id":"q-973","question":"Case: You’re building a beginner-friendly Azure API that accepts events from mobile apps. Each event includes userId, eventType, and timestamp. The API should write a compact summary to Cosmos DB and stream raw events to Event Hubs for analytics. On a Consumption plan, outline the minimal architecture, bindings, and error handling to ensure low latency, safe retries, and no data loss during transient outages?","answer":"HTTP-triggered Azure Function validates payload (userId, eventType, timestamp). Store a compact summary in Cosmos DB using a composite id (userId+timestamp) to enforce idempotency, and emit the full e","explanation":"## Why This Is Asked\nTests knowledge of a realistic ingestion path: HTTP input, idempotent storage, and streaming analytics with Azure services, plus the constraints of a Consumption plan.\n\n## Key Concepts\n- HTTP trigger in Azure Functions\n- Cosmos DB best practices for idempotent keys\n- Event Hubs for scalable intake of streams\n- Retry/backoff strategies on Consumption plan\n- Bindings and error handling in serverless architectures\n\n## Code Example\n```javascript\nmodule.exports = async function(context, req) {\n  const body = req.body;\n  // basic validation\n  if (!body?.userId || !body?.eventType || !body?.timestamp) {\n    context.res = { status: 400, body: 'Invalid payload' };\n    return;\n  }\n  const id = `${body.userId}|${body.timestamp}`;\n  // write summary to Cosmos DB (idempotent key)\n  context.bindings.cosmosDoc = { id, userId: body.userId, timestamp: body.timestamp, eventType: body.eventType };\n  // publish raw event to Event Hubs\n  context.bindings.outputEventHub = body;\n  context.res = { status: 202, body: 'Accepted' };\n};\n```\n\n## Follow-up Questions\n- How would you validate and test idempotency in this flow? \n- What metrics would you observe to ensure latency stays low during bursts?","diagram":null,"difficulty":"beginner","tags":["azure-developer"],"channel":"azure-developer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["PayPal","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T17:37:30.997Z","createdAt":"2026-01-12T17:37:30.997Z"}],"subChannels":["general"],"companies":["Adobe","Airbnb","Amazon","Anthropic","Apple","Bloomberg","Citadel","Cloudflare","Coinbase","Databricks","Discord","DoorDash","Goldman Sachs","Google","Hashicorp","Hugging Face","IBM","Instacart","LinkedIn","Lyft","Meta","Microsoft","MongoDB","NVIDIA","Netflix","OpenAI","Oracle","PayPal","Plaid","Robinhood","Salesforce","Scale Ai","Slack","Snap","Snowflake","Square","Stripe","Tesla","Two Sigma","Uber","Zoom"],"stats":{"total":93,"beginner":29,"intermediate":34,"advanced":30,"newThisWeek":36}}