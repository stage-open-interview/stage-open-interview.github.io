{"questions":[{"id":"q-1100","question":"In a Databricks data pipeline, ingest JSON events from S3 into Delta Lake with Auto Loader. Each event has a nested user object (id, email) and an optional pages array. Some events miss user.email. How would you design the pipeline to safely flatten nested fields, handle missing values, and upsert the latest user state into a Delta Silver table using an idempotent MERGE?","answer":"Use Auto Loader with schema evolution, flatten nested fields, and a MERGE for idempotent upserts. ReadStream via cloudFiles, inferColumnTypes true, mergeSchema true. Flatten: user_id = user.id, email ","explanation":"## Why This Is Asked\n\nTests practical mastery of ingesting semi-structured data, flattening nested JSON, handling optional fields, and performing idempotent upserts in Delta Lake using Databricks primitives.\n\n## Key Concepts\n\n- Auto Loader with cloudFiles options for JSON with schema evolution\n- Flattening nested structs and handling missing fields safely\n- Upserts with MERGE to ensure idempotent state in Delta Lake\n- Null handling with COALESCE and robust array expansion (explode_outer)\n\n## Code Example\n\n```javascript\n// Databricks PySpark-like sketch (syntax-highlighted as javascript)\nval df = spark.readStream.format(\"cloudFiles\")\n  .option(\"cloudFiles.format\",\"json\")\n  .option(\"cloudFiles.inferColumnTypes\",\"true\")\n  .option(\"cloudFiles.mergeSchema\",\"true\")\n  .load(\"s3a://bucket/events/\")\n\nval flat = df.selectExpr(\"user.id as user_id\",\n                       \"coalesce(user.email, '') as email\",\n                       \"ts as event_ts\",\n                       \"explode_outer(pages) as page\")\n```\n\n## Follow-up Questions\n\n- How would you test this pipeline end-to-end, including schema evolution scenarios?\n- How would you handle new nested fields added to user in future deployments?","diagram":null,"difficulty":"beginner","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T22:31:52.482Z","createdAt":"2026-01-12T22:31:52.482Z"},{"id":"q-1112","question":"Scenario: In a Databricks Delta Live Tables pipeline, ingest JSON events from a Kafka source into a Bronze table, where nested fields drift over time (e.g., payload.user.locale is added later, some events miss payload.user). You then transform to a Silver table used by billing and marketing. How would you implement a robust schema-evolution strategy and gating so new fields are captured without breaking existing downstream queries, and how would you test this in a run?","answer":"Enable schema evolution in DLT by merging new fields into Bronze and keeping downstream schema backward compatible; treat optional nested fields as nullable and use defaults; gate with non-null expect","explanation":"## Why This Is Asked\nTests practical schema evolution and data quality gating in a real Delta Live Tables pipeline, focusing on drift in nested JSON and downstream stability.\n\n## Key Concepts\n- Delta Live Tables schema evolution\n- Nested JSON drift handling\n- Downstream backward compatibility\n- Data quality gates/expectations\n- End-to-end testing with mixed schemas\n\n## Code Example\n```python\n# PySpark-like sketch for Bronze -> Silver\nbronze = spark.readStream.format(\"cloudFiles\").option(\"cloudFiles.format\",\"json\").load(\"/bronze\")\nsilver = bronze.selectExpr(\"payload.user.id as user_id\", \"payload.user.locale as user_locale\", \"payload.action as action\", \"timestamp\")\nsilver = silver.withColumn(\"user_locale\", F.coalesce(col(\"user_locale\"), F.lit(\"unknown\")))\n```\n\n## Follow-up Questions\n- How would you roll schema changes from experimental to production with minimal downtime?\n- How would you monitor for schema drift in production and trigger alerts?","diagram":null,"difficulty":"intermediate","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Microsoft","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T23:23:17.885Z","createdAt":"2026-01-12T23:23:17.885Z"},{"id":"q-1144","question":"In a Databricks streaming pipeline ingesting tenant-scoped events from Kafka into Delta Lake, events may arrive late by up to 10 minutes. Describe a concrete end-to-end approach to upsert the latest state per (tenant_id, user_id) into a Silver table while preserving the full event history. Include: data model, CDC logic, watermarking and late data handling, an idempotent MERGE strategy, schema evolution handling, and governance considerations with Unity Catalog RBAC and data lineage?","answer":"Use Bronze‚ÜíSilver CDC pattern with Delta Lake. Silver holds latest state per (tenant_id, user_id); History preserves all changes. Use a 10-minute watermark to bound late data; MERGE INTO Silver WHEN M","explanation":"## Why This Is Asked\nThis tests practical CDC design, late-arrival handling, and Delta Lake upserts in a multi-tenant setting.\n\n## Key Concepts\n- Delta Lake MERGE for upserts\n- Watermarks and late data handling in streaming\n- Bronze-Silver-History data modeling\n- Unity Catalog RBAC and data lineage\n\n## Code Example\n```javascript\n-- Spark SQL (illustrative)\nMERGE INTO silver_latest AS s\nUSING bronze AS b\nON s.tenant_id = b.tenant_id AND s.user_id = b.user_id\nWHEN MATCHED THEN UPDATE SET\n  s.state = b.state,\n  s.last_updated = current_timestamp(),\n  s.version = b.version\nWHEN NOT MATCHED THEN INSERT (tenant_id, user_id, state, last_updated, version)\nVALUES (b.tenant_id, b.user_id, b.state, current_timestamp(), b.version);\n```\n\n## Follow-up Questions\n- How would you validate late-data handling under bursty traffic?\n- How would you monitor and alert on data-skew and drift across tenants?","diagram":null,"difficulty":"advanced","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Cloudflare","Databricks"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T01:30:54.604Z","createdAt":"2026-01-13T01:30:54.604Z"},{"id":"q-1182","question":"In a multi-tenant Databricks lakehouse ingesting per-tenant telemetry from Kafka and S3 into a unified Silver Delta table, describe an end-to-end CDC strategy to implement SCD2-like history per tenant with idempotent MERGE, handle late data with a watermark, and enforce governance through Unity Catalog RBAC and dynamic PII masking. Include data model, CDC logic, testing plan, and how you‚Äôd validate lineage?","answer":"Design a CDC-driven SCD2 history per tenant in a Delta table. Ingest via Delta Live Tables into a Silver table partitioned by tenant_id and timestamp. On MERGE, close previous history rows by setting ","explanation":"## Why This Is Asked\n\nTests the ability to design a CDC-based SCD2 history in a multi-tenant lakehouse with governance and masking requirements, using Delta Live Tables and MERGE semantics.\n\n## Key Concepts\n\n- CDC and SCD2 patterns in Delta Lake\n- Delta Live Tables and MERGE-based upserts\n- Watermarking and late-data handling\n- Unity Catalog RBAC and dynamic data masking\n- Data lineage and governance with Delta sharing\n\n## Code Example\n\n```sql\nMERGE INTO silver AS tgt\nUSING staging AS src\nON tgt.tenant_id = src.tenant_id AND tgt.key = src.key\nWHEN MATCHED AND (src.attributes_hash <> tgt.attributes_hash OR src.timestamp <> tgt.end_date) THEN\n  UPDATE SET end_date = current_timestamp()\nWHEN NOT MATCHED THEN\n  INSERT (tenant_id, key, attributes_hash, start_date, end_date)\n  VALUES (src.tenant_id, src.key, src.attributes_hash, current_timestamp(), NULL)\n```\n\n## Follow-up Questions\n\n- How would you test idempotency of MERGE under replayed CDC events?\n- How would you implement per-tenant masking policies in Unity Catalog and verify lineage against dashboards?","diagram":null,"difficulty":"advanced","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Google","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T03:43:29.150Z","createdAt":"2026-01-13T03:43:29.150Z"},{"id":"q-1226","question":"In a Databricks Delta Live Tables pipeline that ingests clickstream events from a cloud bucket into a Delta table, data quality checks are defined via expectations. A batch arrives with corrupted event_time values that would fail the run. Outline a concrete approach to quarantine bad data, feed downstream tables only from valid rows, and store invalids for audit, while still handling late data and deduplication?","answer":"Create a staging raw_events table, apply a light validation to tag rows as valid/invalid, store invalid rows in invalid_events, and feed downstream clean_events from valid rows only. Use a watermark o","explanation":"## Why This Is Asked\nDemonstrates practical, production-ready handling of partial data quality failures in a Delta Live Tables pipeline, balancing correctness with availability.\n\n## Key Concepts\n- Staging vs downstream separation\n- Expectations-based quality checks\n- Quarantine and audit of invalid data\n- Late data handling with watermarking\n- Deduplication strategies across micro-batches\n\n## Code Example\n```javascript\nimport dlt\nimport pyspark.sql.functions as F\n\n@dlt.table\ndef raw_events():\n  return spark.read.format(\"cloudFiles\").option(\"cloudFiles.format\",\"json\").load(\"/mnt/events/raw\")\n\n@dlt.table\ndef valid_events():\n  df = dlt.read(\"raw_events\")\n  return df.filter(F.col(\"event_time\").isNotNull() & F.col(\"user_id\").isNotNull())\n\n@dlt.table\ndef invalid_events():\n  raw = dlt.read(\"raw_events\")\n  valid_ids = dlt.read(\"valid_events\").select(\"event_id\").distinct()\n  return raw.join(valid_ids, \"event_id\", \"left_anti\")\n\n@dlt.table\ndef clean_events():\n  return dlt.read(\"valid_events\")\n```\n\n## Follow-up Questions\n- How would you monitor and alert if invalid_events growth spikes?\n- What changes would you make to support reprocessing of invalid data after fixes?","diagram":"flowchart TD\n  A[Ingest Raw Events] --> B[DLT Validation Tagging]\n  B --> C{Validity}\n  C -->|Valid| D[Feed Clean Events]\n  C -->|Invalid| E[Store Invalid Events]\n  D --> F[Analytics & BI]\n  E --> G[Audit & Reprocessing]\n  H[Late Data via Watermark] --> B","difficulty":"intermediate","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T05:38:03.397Z","createdAt":"2026-01-13T05:38:03.397Z"},{"id":"q-1264","question":"In a Databricks job ingesting daily Parquet files from S3 into Delta Lake, a new field 'campaign_id' may appear in newer files while older ones do not. Describe a concrete, beginner-friendly approach to ingest without data loss, allowing downstream joins, and handling the schema change gracefully. Include how to enable Delta schema evolution (mergeSchema/autoMerge), define a compatible target schema, and implement a minimal validation to drop rows missing essential fields?","answer":"Enable Delta schema evolution by using mergeSchema on write and enabling auto-merge: spark.conf.set('spark.databricks.delta.schema.autoMerge.enabled','true'); df.write.format('delta').option('mergeSch","explanation":"## Why This Is Asked\n\nTests understanding of practical schema evolution in Delta Lake during basic batch ingestion and how to keep data accessible for joins.\n\n## Key Concepts\n\n- Delta Lake schema evolution\n- mergeSchema and autoMerge\n- Nullability and backward compatibility\n- Data quality validation\n\n## Code Example\n\n```javascript\n// Ingestion example with schema evolution\ndf.write.format(\"delta\").option(\"mergeSchema\",\"true\").mode(\"append\").save(\"/delta/user_events\")\n```\n\n## Follow-up Questions\n\n- How would you validate that campaign_id is correctly populated in downstream queries?\n- How would you backfill the historical table to reflect a new campaigns dimension without downtime?","diagram":null,"difficulty":"beginner","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Coinbase","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T07:31:13.696Z","createdAt":"2026-01-13T07:31:13.696Z"},{"id":"q-1320","question":"You're designing a Delta Live Tables workflow ingesting data from Kafka and an S3 landing zone, with a downstream customer dimension in Delta Lake that uses SCD Type 2. How would you implement idempotent MERGE-based upserts, handle schema drift, and preserve late-arriving data while auditing invalid events and ensuring downstream BI reads only current rows?","answer":"Implement Bronze‚ÜíSilver‚ÜíGold: dedupe by customer_id with max(event_time) in Silver; allowMissingFields for schema drift. In Gold, MERGE into customer_dim: on match with changes, set old row valid_to a","explanation":"## Why This Is Asked\nTests practical mastery of Delta Live Tables patterns: CDC, SCD2, and schema drift handling in production-like streams.\n\n## Key Concepts\n- SCD Type 2 implementation via MERGE\n- Schema drift resilience with safe evolution\n- Late data handling using watermarks and auditing\n- Data lineage and is_current filtering for dashboards\n\n## Code Example\n```sql\nMERGE INTO gold.customer_dim AS t\nUSING silver.customer_stage AS s\nON t.customer_id = s.customer_id AND t.is_current = true\nWHEN MATCHED AND (t.name <> s.name OR t.email <> s.email) THEN\n  UPDATE SET t.valid_to = current_timestamp(), t.is_current = false\nWHEN NOT MATCHED THEN\n  INSERT (customer_id, name, email, valid_from, valid_to, is_current)\n  VALUES (s.customer_id, s.name, s.email, current_timestamp(), NULL, true);\n```\n\n## Follow-up Questions\n- How would you test idempotency across pipeline restarts?\n- How would you validate schema drift without failing runs?\n- How would you structure audits to avoid data loss during failures?","diagram":null,"difficulty":"intermediate","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["NVIDIA","Robinhood","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T11:32:27.379Z","createdAt":"2026-01-13T11:32:27.379Z"},{"id":"q-1375","question":"In a Databricks streaming job, a Kafka topic emits JSON events for many tenants. The payload schema drifts with new fields; you want a stable Silver Delta table with a canonical schema and history. Describe a beginner-friendly approach to map events to the canonical schema, handle new fields without breaking downstream joins, and perform an idempotent MERGE into Silver by (tenant_id, event_id). Include a concrete mapping rule set and a small MERGE example?","answer":"Read Bronze with a permissive schema; map to a canonical Silver schema: tenant_id, event_id, event_ts, and payload (struct). Unknown fields go into payload to avoid drift. Use a MERGE into Silver on (","explanation":"## Why This Is Asked\nTests practical skill in handling schema drift and idempotent upserts in streaming Databricks pipelines.\n\n## Key Concepts\n- Schema drift and canonical schema design\n- Struct payload encoding\n- Idempotent MERGE pattern\n- Streaming boundaries and data quality\n\n## Code Example\n```javascript\nMERGE INTO Silver AS s\nUSING (SELECT tenant_id, event_id, event_ts, payload FROM Bronze) AS b\nON s.tenant_id = b.tenant_id AND s.event_id = b.event_id\nWHEN MATCHED THEN UPDATE SET event_ts = b.event_ts, payload = b.payload\nWHEN NOT MATCHED THEN INSERT (tenant_id, event_id, event_ts, payload) VALUES (b.tenant_id, b.event_id, b.event_ts, b.payload);\n```\n\n## Follow-up Questions\n- How would you test duplicate avoidance and drift handling end-to-end?\n- How would you evolve the canonical schema as new tenant-specific fields appear?","diagram":null,"difficulty":"beginner","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","Oracle","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T14:38:09.108Z","createdAt":"2026-01-13T14:38:09.108Z"},{"id":"q-1409","question":"Describe an end-to-end approach for a fraud-detection streaming pipeline using two Kafka topics (transactions, account_updates): Bronze ingest, join to a versioned SCD2 customer_dim, compute risk in Silver, upsert via MERGE with a deterministic key, watermark late data, handle schema drift with Delta Lake evolution, and enforce governance with Unity Catalog RBAC and lineage. Include testing with synthetic late-arriving data?","answer":"Ingest two Kafka topics into Bronze, join to a versioned SCD2 customer_dim, compute per-event risk in Silver, and upsert into Silver with MERGE on tx_id. Use watermarking for late data, enable Delta L","explanation":"## Why This Is Asked\nAssesses ability to design multi-source streaming with versioned references, robust late-data handling, idempotent upserts, schema evolution, and governance.\n\n## Key Concepts\n- Multi-source streaming from Kafka into Bronze\n- Slowly changing dimension (SCD2) for customer_dim\n- Idempotent MERGE semantics on deterministic keys\n- Watermarking and late data handling\n- Delta Lake schema evolution for drift\n- Unity Catalog RBAC and data lineage\n\n## Code Example\n```javascript\n// Pseudo-sample: MERGE into Silver risk table from Bronze transactions\nspark.sql(`MERGE INTO silver_risk AS s\nUSING bronze_transactions AS b\nON s.tx_id = b.tx_id\nWHEN MATCHED THEN UPDATE SET s.score = b.score, s.updated_at = current_timestamp()\nWHEN NOT MATCHED THEN INSERT (tx_id, score, updated_at) VALUES (b.tx_id, b.score, current_timestamp())`)\n```\n\n## Follow-up Questions\n- How would you test end-to-end with synthetic late-arriving data?\n- What metrics would you monitor to detect backpressure or data skew in this design?","diagram":null,"difficulty":"advanced","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","Scale Ai","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T15:50:17.661Z","createdAt":"2026-01-13T15:50:17.661Z"},{"id":"q-1451","question":"In a Databricks notebook, you need to join a 10M-row Delta Lake 'customers' table with a 10k-row 'segments' reference table to produce a daily marketing audience feed. How would you implement an efficient join strategy in Spark/Delta to minimize shuffle and cost, ensure correctness if segments update, and maintain lineage? Include: join type and hints, caching strategy, refresh cadence, and where to store results?","answer":"Use a broadcast join and caching. Load the 10M-row fact table as df_big and the 10k-row segments as df_small, then join with F.broadcast(df_small) on customer_id. Cache df_big for the daily run. Write","explanation":"## Why This Is Asked\nTests practical Spark optimization and Delta Lake usage for daily pipelines, focusing on join strategy, caching, and governance.\n\n## Key Concepts\n- Broadcast joins and data skew management\n- Caching large DataFrames for repeated reads\n- Delta Lake partitioning for incremental writes\n- Upserts with MERGE and change logging\n- Lineage and RBAC in governance\n\n## Code Example\n```python\nfrom pyspark.sql import functions as F\n\ndf_big = spark.read.format(\"delta\").table(\"db.customers\")\ndf_small = spark.read.format(\"delta\").table(\"db.segments\")\n\ndf_enriched = df_big.join(F.broadcast(df_small), \"customer_id\", \"left\")\ndf_enriched = df_enriched.cache()\n\ndf_enriched.write.format(\"delta\").mode(\"append\").partitionBy(\"order_date\").saveAsTable(\"db.marketing_events_enriched\")\n```\n\n## Follow-up Questions\n- How would you handle frequent updates to the segments table during the day?\n- How would you quantify and reduce shuffle cost in this pipeline?","diagram":"flowchart TD\n  A[Read big delta table] --> B[Read small segments table]\n  B --> C[Broadcast join A and B on customer_id]\n  C --> D[Cache enriched df]\n  D --> E[Write to Delta table partitioned by date]\n  E --> F[Register lineage in Unity Catalog]","difficulty":"beginner","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Google","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T17:46:52.463Z","createdAt":"2026-01-13T17:46:52.463Z"},{"id":"q-1540","question":"In a Databricks streaming pipeline ingesting order events from Kafka into Delta Lake, implement a scalable SCD Type 2 for a customer_dim table to preserve history while handling late-arriving updates up to 15 minutes. Describe the data model (bronze/silver), CDC logic using MERGE, watermarking, and schema evolution, plus Unity Catalog RBAC and lineage considerations. Include a minimal code sketch of the MERGE closing an old row and inserting a new version?","answer":"Design a Delta Live Tables pipeline with a bronze Kafka stream feeding a silver customer_dim SCD2. Implement CDC via a MERGE that closes the active row (set end_date, current=false) and inserts a new row with start_date and current=true. Use watermarking for 15-minute late data handling, enable schema evolution, and implement Unity Catalog RBAC with lineage tracking.","explanation":"## Why This Is Asked\nTests mastery of real-world data governance and slowly changing dimensions in streaming pipelines on Databricks.\n\n## Key Concepts\n- Delta Live Tables, MERGE-based CDC, SCD2, watermarking for late data, schema evolution, Unity Catalog RBAC, data lineage.\n\n## Code Example\n```sql\nMERGE INTO silver.customer_dim AS t\nUSING staging.customer_dim AS s\nON t.customer_id = s.customer_id AND t.current = true\nWHEN MATCHED THEN UPDATE SET t.end_date = CURRENT_TIMESTAMP, t.current = false\nWHEN NOT MATCHED THEN INSERT (customer_id, name, address, start_date, end_date, current)\nVALUES (s.customer_id, s.name, s.address, CURRENT_TIMESTAMP, NULL, true)\n```","diagram":null,"difficulty":"advanced","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Hugging Face"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T06:32:33.205Z","createdAt":"2026-01-13T20:55:12.361Z"},{"id":"q-1555","question":"In a Databricks pipeline ingesting 20 TB/day of Parquet logs on S3 into Delta Lake, design a practical optimization plan to improve read latency for near-real-time dashboards using Delta features like OPTIMIZE, ZORDER, Data Skipping, caching, and Photon. Discuss partitioning strategy, trade-offs, and how you'd validate gains with benchmark metrics?","answer":"Leverage OPTIMIZE with ZORDER on frequently filtered columns (date, user_id), enable Data Skipping, and maintain hot partitions in memory cache. Reevaluate daily versus hourly partitioning to balance compaction overhead against query performance, and validate improvements through comprehensive benchmark metrics.","explanation":"## Why This Is Asked\nThis question evaluates practical optimization strategies for Databricks pipelines handling large-scale Delta Lake workloads, requiring balance between read latency, cost efficiency, and operational maintainability.\n\n## Key Concepts\n- Delta Lake OPTIMIZE and ZORDER for optimal data layout\n- Data skipping and Photon-accelerated query processing\n- Strategic partitioning and intelligent caching\n- Maintenance trade-offs: vacuum operations, compaction, write amplification\n- Performance metrics: query runtime, bytes scanned, cache hit rates, cost per query\n\n## Code Example\n```sql","diagram":null,"difficulty":"intermediate","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","IBM","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T06:25:12.266Z","createdAt":"2026-01-13T21:41:51.915Z"},{"id":"q-1643","question":"In Databricks, ingest streaming data from Kafka into Delta Lake for 10k IoT devices emitting multiple sensor types (temperature, humidity, pressure). Build a Silver table with the latest per-device per-sensor-type state while preserving full history. The source schema will evolve (new sensors added, some removed). Outline a robust end-to-end approach: data model, CDC/Upsert logic, watermarking for late data, schema evolution strategy, idempotent MERGE, and governance with Unity Catalog RBAC and lineage. Include concrete examples of Bronze->Silver handoff and a dynamic per-tenant view?","answer":"Use a Silver table keyed by (device_id, sensor_type) storing the latest value per key, while preserving full history in a separate history mechanism. Upsert Bronze to Silver via MERGE with a 2-minute ","explanation":"## Why This Is Asked\nTests end-to-end thinking: streaming CDC, evolving schemas, and governance in a high-cardinality IoT pipeline.\n\n## Key Concepts\n- Streaming CDC with MERGE into Delta Lake Silver\n- Handling schema evolution (auto-evolve) for new sensors\n- Watermarking and late data management\n- Modeling latest state vs full history\n- Unity Catalog RBAC and lineage for per‚Äëtenant views\n\n## Code Example\n```javascript\n-- Bronze to Silver MERGE (pseudo-SQL)\nMERGE INTO silver_sensor AS s\nUSING batch_view AS b\nON s.device_id = b.device_id AND s.sensor_type = b.sensor_type\nWHEN MATCHED THEN UPDATE SET\n  s.value = b.value,\n  s.ts = b.ts\nWHEN NOT MATCHED THEN INSERT (device_id, sensor_type, value, ts) VALUES (b.device_id, b.sensor_type, b.value, b.ts);\n```\n\n```javascript\n-- Streaming pipeline skeleton with watermark (pseudo-Python)\nbronze = spark.readStream.format('kafka')... \nbronze = bronze.selectExpr('CAST(value AS STRING) as json', 'timestamp')\n# parse and write to Bronze Delta, then MERGE to Silver in micro-batches with a watermark\n```\n\n## Follow-up Questions\n- How would you validate correctness under high late-arrival rates and schema drift?\n- How would you implement per-tenant data access using dynamic views and RBAC without leaking data across tenants?","diagram":null,"difficulty":"advanced","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T04:29:04.600Z","createdAt":"2026-01-14T04:29:04.601Z"},{"id":"q-1658","question":"Design a secure external data sharing workflow in a Databricks environment using Unity Catalog and Delta Sharing to expose aggregated metrics derived from production Delta tables to external partners while maintaining tenant isolation and governance. Include data model, masking strategy, per partner quotas, refresh cadence, and how to monitor and revoke access?","answer":"Use a Delta Sharing producer dataset based on a Silver view (tenant_id, metric, value) with masking on sensitive fields. Publish as a Delta Share to a consumer role with read access only. Enforce per ","explanation":"## Why This Is Asked\nThis tests Delta Sharing, Unity Catalog governance, masking, quotas, and revocation in real-world partner data sharing.\n\n## Key Concepts\n- Delta Sharing producer/consumer model\n- Unity Catalog RBAC and quotas\n- Dynamic data masking policies\n- Data freshness and incremental refresh\n- Audit logs and compliance\n\n## Code Example\n```sql\n-- Create share and grant access to partner\nCREATE SHARE partner_agg_share;\nALTER SHARE partner_agg_share ADD CONSUMER 'partner_catalog' WITH 'us-east-1';\nGRANT SELECT ON database.shared_view TO SHARE partner_agg_share;\n```\n\n## Follow-up Questions\n- How would you handle schema drift in the shared view without breaking consumers?\n- What monitoring metrics indicate misuse or quota breaches?","diagram":null,"difficulty":"advanced","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Oracle","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T05:41:50.396Z","createdAt":"2026-01-14T05:41:50.397Z"},{"id":"q-1683","question":"Design a multi-tenant, Databricks-based data pipeline ingesting 1 TB/day of JSON events from Kafka into Delta Lake. Tenants share storage but must be completely isolated; dashboards must mask PII fields per-tenant. Propose an end-to-end pattern using Unity Catalog RBAC, dynamic data masking, Delta Live Tables, and Photon-enabled reads. Include data model, masking rules, handling schema evolution, and how you validate governance and lineage?","answer":"Three-layer lakehouse: Bronze (Kafka ‚Üí Delta), Silver (per-tenant masking via Unity Catalog dynamic masking with tenant scoping), Gold (dashboards + features). Enforce RBAC, use DLT with schema evolut","explanation":"Why This Is Asked\n- Tests multi-tenant governance, masking, and lineage in a real Databricks setup. \n- Evaluates practical use of Unity Catalog RBAC, dynamic masking, and DLT for evolving schemas. \n- Probes performance considerations with Photon and partitioning strategies.\n\nKey Concepts\n- Unity Catalog RBAC and access controls\n- Dynamic data masking for PII per tenant\n- Delta Live Tables for reliable, incremental transforms\n- Photon-accelerated reads and Delta caching\n- Schema evolution handling and time travel for audits\n- Data lineage and governance validation\n\nCode Example\n```sql\n-- Pseudo example: define a masking policy and apply to Silver table\nCREATE MASKING POLICY tenant_masking AS (tenant_id STRING) RETURNS STRING ->\n  CASE WHEN CURRENT_USER() = tenant_id THEN tenant_id ELSE 'REDACTED' END;\nALTER TABLE silver APPLY MASKING tenant_masking ON (tenant_id);\n```\n\nFollow-up Questions\n- How would you test for cross-tenant data leakage during schema evolution?\n- Which metrics and dashboards would you instrument to monitor governance and lineage health across tenants?","diagram":null,"difficulty":"advanced","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","Netflix","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T06:55:17.136Z","createdAt":"2026-01-14T06:55:17.136Z"},{"id":"q-1721","question":"Databricks beginner scenario: A streaming pipeline reads 2 TB/month of JSON events from S3 via Autoloader into Delta Lake. A downstream dashboard shows active_users by hour. Late events arrive up to 10 minutes. Design a practical plan to maintain accurate hourly active_user counts with minimal duplication, including: schema/partitioning for streaming, watermark-based late data handling, an idempotent MERGE into a Silver table, and a simple end-to-end test using synthetic late events. Also outline monitoring steps?","answer":"Implement an Autoloader Bronze layer, upsert hourly aggregates into Silver with a MERGE on (user_id, hour). Use a 10-minute watermark for late data; partition Silver by hour; ensure idempotence with a","explanation":"## Why This Is Asked\n\nTests practical use of streaming with late data, Delta MERGE upserts, and end-to-end validation in a beginner-friendly context.\n\n## Key Concepts\n\n- Autoloader and Delta Lake Bronze-Silver pattern\n- Watermarks and late data handling in structured streaming\n- Idempotent MERGE-based upserts for counts\n- End-to-end testing with synthetic late events\n- Monitoring latency and data quality\n\n## Code Example\n\n```sql\n-- MERGE into Silver hourly active_users\nMERGE INTO silver.active_users AS s\nUSING (\n  SELECT user_id,\n         date_trunc('hour', event_time) AS hour,\n         COUNT(*) AS cnt\n  FROM bronze.events\n  GROUP BY user_id, date_trunc('hour', event_time)\n) AS b\nON s.user_id = b.user_id AND s.hour = b.hour\nWHEN MATCHED THEN UPDATE SET s.count = b.cnt\nWHEN NOT MATCHED THEN INSERT (user_id, hour, count) VALUES (b.user_id, b.hour, b.cnt)\n```\n\n## Follow-up Questions\n\n- How would you adjust for bursty late data or skewed user activity?\n- What metrics would you monitor to ensure SLA compliance for dashboards?","diagram":"flowchart TD\n  S[Source: S3 Autoloader] --> B[Bronze Layer]\n  B --> S2[Silver Layer with MERGE]\n  S2 --> D[Dashboards]\n  D --> M[Monitoring + Alerts]","difficulty":"beginner","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Instacart","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T07:55:55.291Z","createdAt":"2026-01-14T07:55:55.291Z"},{"id":"q-1844","question":"In a Databricks streaming pipeline ingesting events from Kafka into Delta Lake for a 50+ TB/day workload, design a CDC-based upsert to a Silver table with per-tenant sharding, late data handling, and schema evolution. Describe the data model, idempotent MERGE keys, watermark latency model, and Unity Catalog RBAC considerations; propose validation metrics and a minimal reproducible code skeleton?","answer":"Use a two-table CDC pattern: Bronze stream ingests events from Kafka, writes (tenant_id, event_id, ts, payload). Silver MERGEs into a partitioned table by tenant_id/date, upserting on (tenant_id, even","explanation":"## Why This Is Asked\nTests CDC mastery, multi-tenant partitioning, late-data handling, schema evolution, and governance in a large Databricks setup.\n\n## Key Concepts\n- CDC MERGE across Bronze to Silver\n- Per-tenant partitioning and idempotent keys\n- Watermarking and late-arrival handling\n- Schema evolution in Delta Lake\n- Unity Catalog RBAC and lineage tracking\n\n## Code Example\n```sql\nMERGE INTO Silver s\nUSING staged_b e b\nON (s.tenant_id = b.tenant_id AND s.event_id = b.event_id)\nWHEN MATCHED THEN UPDATE SET s.value = b.value, s.ts = b.ts\nWHEN NOT MATCHED THEN INSERT (tenant_id, event_id, value, ts) VALUES (b.tenant_id, b.event_id, b.value, b.ts);\n```\n\n## Follow-up Questions\n- How would you validate idempotence with late data?\n- How to evolve schema while preserving history and lineage?","diagram":null,"difficulty":"advanced","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Instacart","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T13:29:00.353Z","createdAt":"2026-01-14T13:29:00.353Z"},{"id":"q-1853","question":"In a Databricks job ingesting 50 GB/day of Avro logs from S3 into Delta Lake, design a beginner-friendly Delta Live Tables pipeline to produce Bronze (raw) and Silver (flattened) tables. Include how you flatten the nested field user (id and tier) into Silver, enforce a simple data quality gate (NOT NULL user_id, user_tier in {'free','standard','premium'}), and choose a partitioning strategy by file_date. Explain testing and how you'll measure improvements?","answer":"Use a Delta Live Tables pipeline with Bronze (raw Avro from S3) and Silver (flattened). Bronze loads from s3://bucket/logs/daily/, Silver computes user_id = user.id and user_tier = user.tier, plus ess","explanation":"## Why This Is Asked\nThis tests practical DLT usage, Bronze/Silver modeling, nested field flattening, basic data quality gates, and partitioning in a real-ish beginner scenario.\n\n## Key Concepts\n- Delta Live Tables (DLT)\n- Bronze/Silver data modeling\n- Flattening nested structs in Spark\n- Simple data quality gates\n- Partitioning strategy and schema evolution\n\n## Code Example\n```javascript\nimport dlt\n@dlt.table\ndef bronze_logs():\n  return spark.read.format(\"avro\").load(\"s3://bucket/logs/daily/\")\n\n@dlt.table\ndef silver_logs():\n  b = dlt.read(\"bronze_logs\")\n  s = b.selectExpr(\"user.id as user_id\",\"user.tier as user_tier\", \"*\")\n  return s.filter(\"user_id IS NOT NULL AND user_tier IN ('free','standard','premium')\")\n```\n\n## Follow-up Questions\n- How would you test drift between Bronze and Silver? \n- How would you handle an unseen user_tier value (e.g., 'enterprise')?\n","diagram":null,"difficulty":"beginner","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Robinhood","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T14:39:02.124Z","createdAt":"2026-01-14T14:39:02.124Z"},{"id":"q-1944","question":"Design a Delta Live Tables pipeline that streams per-tenant user activity from Kafka into Bronze, then updates a per-tenant Silver table implementing SCD Type 2 on a tenant-scoped customer_dim, using a deterministic MERGE for upserts. Tenants can emit out-of-order data and schema drift occurs. Propose concrete config for watermarking, per-tenant constraints, and schema evolution, plus RBAC in Unity Catalog. Include synthetic late-data tests and metrics to validate latency and correctness?","answer":"Propose using DLT with per-tenant watermarks, deterministic MERGE keys, and row-level constraints. Bronze ingest from Kafka, then Silver with per-tenant SCD2 and a deterministic MERGE into tenant-scop","explanation":"## Why This Is Asked\n\nAssesses practical mastery of per-tenant data pipelines, late data handling, schema evolution, and governance in Databricks DLTs. Requires concrete trade-offs, deterministic keys, and test strategies beyond generic concepts.\n\n## Key Concepts\n\n- Delta Live Tables and MERGE upserts\n- Per-tenant governance with Unity Catalog RBAC\n- Watermarking and late data handling in streaming\n- SCD Type 2 on Delta Lake\n- Schema evolution and data quality constraints\n- Synthetic testing for latency and correctness\n\n## Code Example\n\n```javascript\n// Pseudo-skeleton illustrating per-tenant pipeline steps\nsetup() {\n  // configure per-tenant watermarking\n  // define Bronze from Kafka\n  // define Silver with SCD2 on tenant_id\n  // upsert with deterministic keys into tenant-scoped table\n  // enable schema evolution\n  // apply RBAC\n}\n```\n\n## Follow-up Questions\n\n- How would you validate idempotency under restart scenarios?\n- What metrics would you surface in dashboards to monitor per-tenant data quality?","diagram":null,"difficulty":"intermediate","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Citadel","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T17:57:46.590Z","createdAt":"2026-01-14T17:57:46.591Z"},{"id":"q-1990","question":"Beginner: Adobe wants a minimal, end-to-end Databricks pipeline to ingest daily JSON event logs from S3 into Delta Lake. Design the workflow using Auto Loader for a Bronze table, then flatten to a date-partitioned Silver table. Include simple schema-evolution handling, a lightweight data-quality check (required fields, duplicates), and a basic unit test that validates the Silver schema and daily row count?","answer":"Use Auto Loader to land JSON into Bronze Delta, flatten into Silver with a date partition, enable mergeSchema for drift, and apply a lightweight quality guard for non-null fields and a unique key. Imp","explanation":"## Why This Is Asked\nThis question tests a beginner-friendly, end-to-end Databricks pipeline design, focusing on Bronze/Silver layering and basic quality checks rather than complex streaming or governance. It reveals understanding of ingestion, schema drift, partitioning, and simple test strategies.\n\n## Key Concepts\n- Auto Loader ingestion\n- Delta Lake Bronze/Silver layers\n- Schema evolution with mergeSchema\n- Lightweight data quality checks\n- Unit tests for schema and row counts\n\n## Code Example\n```javascript\n# Pseudo-Spark-like illustration\nbronze = spark.read.format('delta').load('s3://bucket/bronze')\nsilver = bronze.selectExpr('*', 'CAST(event_time AS DATE) as date')\nsilver.write.format('delta').mode('append').partitionBy('date').save('s3://bucket/silver')\n```\n\n## Follow-up Questions\n- How would you test nested JSON changes?\n- How would you run these tests in a CI/CD pipeline?","diagram":null,"difficulty":"beginner","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T19:44:43.880Z","createdAt":"2026-01-14T19:44:43.880Z"},{"id":"q-2033","question":"In a Databricks workflow, ingest 50 GB/day of JSON web logs from S3 into a Bronze Delta table. Propose a beginner-friendly pipeline to populate a Silver table that upserts the latest event per session_id, handles 2-minute late data with a watermark, and validates data quality before write. Include partitioning strategy, a MERGE-based CDC, and Unity Catalog RBAC considerations?","answer":"Design a pipeline using Autoloader to land JSON into Bronze, then upsert into Silver with MERGE on session_id using the latest timestamp. Apply a 2-minute watermark for late data, partition Silver by date for query performance, implement data quality validation rules before writing, and establish Unity Catalog RBAC with appropriate privileges for Bronze/Silver access.","explanation":"Why This Is Asked\n- Tests practical Delta Lake upsert and late data handling with realistic data volume\n- Evaluates ability to translate business rules into robust, auditable pipelines\n- Assesses understanding of medallion architecture and enterprise data governance\n\nKey Concepts\n- Autoloader ingestion, Bronze to Silver MERGE CDC, watermarking for late data\n- Partitioning for query efficiency, schema quality gates, Unity Catalog RBAC\n- Practical validation without overengineering\n\nCode Example\n```javascript\n// Illustrative MERGE logic (conceptual)\nMERGE INTO silver_sessions AS s\nUSING (\n  SELECT session_id, MAX(ts) AS ts, MAX(event_type) AS event_type\n  FROM bronze_events\n  WHERE ts >= current_timestamp() - INTERVAL 2 MINUTES\n  GROUP BY session_id\n) AS b\nON s.session_id = b.session_id\nWHEN MATCHED THEN UPDATE SET *\nWHEN NOT MATCHED THEN INSERT *\n```","diagram":null,"difficulty":"beginner","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Bloomberg","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T06:16:38.809Z","createdAt":"2026-01-14T21:39:33.380Z"},{"id":"q-2070","question":"Design an end-to-end Databricks pipeline to ingest a daily JSON feed from an on-prem FTP drop into Delta Lake. Use Auto Loader with schema evolution for changing fields (user_id, event_type, event_ts, ip_address, device, etc.). Describe Bronze to Silver upserts via MERGE, data quality tests, IP masking, and Unity Catalog RBAC with lineage. Be concrete about steps and trade-offs?","answer":"Design an end-to-end Databricks pipeline to ingest a daily JSON feed from an on-prem FTP drop into Delta Lake. Use Auto Loader with schema evolution for changing fields (user_id, event_type, event_ts, ip_address, device, etc.). Describe Bronze to Silver upserts via MERGE, data quality tests, IP masking, and Unity Catalog RBAC with lineage. Be concrete about steps and trade-offs.\n\n**Architecture Overview:**\n1. **Bronze Layer**: Raw ingestion using Auto Loader with cloudFiles format\n2. **Silver Layer**: Cleaned, deduplicated data with PII masking\n3. **Gold Layer**: Business-ready aggregated tables\n\n**Step-by-Step Implementation:**\n\n**1. Bronze Layer - Auto Loader Ingestion**\n```python\n# Bronze streaming job\nbronze_df = (spark.readStream\n    .format(\"cloudFiles\")\n    .option(\"cloudFiles.format\", \"json\")\n    .option(\"cloudFiles.schemaLocation\", \"/schemas/bronze_events\")\n    .option(\"mergeSchema\", \"true\")\n    .load(\"/mnt/ftp/daily_drop/\"))\n\n# Write to Bronze Delta table\n(bronze_df.writeStream\n    .format(\"delta\")\n    .option(\"checkpointLocation\", \"/checkpoints/bronze_events\")\n    .table(\"bronze_events\"))\n```\n\n**2. Silver Layer - MERGE with Data Quality**\n```sql\n-- Silver upsert with data quality checks\nMERGE INTO silver_events AS target\nUSING (\n    SELECT *,\n        CASE \n            WHEN user_id IS NULL OR user_id = '' THEN 'INVALID_USER_ID'\n            WHEN event_ts IS NULL THEN 'INVALID_TIMESTAMP'\n            WHEN NOT is_valid_ip(ip_address) THEN 'INVALID_IP'\n        END AS quality_flag,\n        mask_ip(ip_address) AS masked_ip\n    FROM bronze_events\n    WHERE ingestion_date >= CURRENT_DATE - 1\n) AS source\nON target.user_id = source.user_id \n   AND target.event_ts = source.event_ts\nWHEN MATCHED AND source.quality_flag IS NULL THEN\n    UPDATE SET *\nWHEN NOT MATCHED AND source.quality_flag IS NULL THEN\n    INSERT *\nWHEN MATCHED AND source.quality_flag IS NOT NULL THEN\n    DELETE;\n```\n\n**3. Data Quality & PII Functions**\n```python\nfrom pyspark.sql.functions import udf, regexp_replace\nfrom pyspark.sql.types import BooleanType, StringType\n\n@udf(BooleanType())\ndef is_valid_ip(ip):\n    import ipaddress\n    try:\n        ipaddress.ip_address(ip)\n        return True\n    except:\n        return False\n\n@udf(StringType())\ndef mask_ip(ip):\n    parts = ip.split('.')\n    return f\"{parts[0]}.{parts[1]}.***.***\"\n```\n\n**4. Unity Catalog RBAC & Lineage**\n```sql\n-- Create catalog and set permissions\nCREATE CATALOG IF NOT EXISTS event_catalog;\n\n-- Grant roles\nCREATE ROLE IF NOT EXISTS data_engineers;\nCREATE ROLE IF NOT EXISTS data_analysts;\nCREATE ROLE IF NOT EXISTS data_scientists;\n\n-- Set permissions\nGRANT USE CATALOG ON CATALOG event_catalog TO ROLE data_engineers;\nGRANT USE SCHEMA ON SCHEMA event_catalog.bronze TO ROLE data_engineers;\nGRANT SELECT ON TABLE event_catalog.bronze.bronze_events TO ROLE data_analysts;\n\n-- Add lineage tags\nALTER TABLE event_catalog.silver.silver_events \nSET TBLPROPERTIES (\n    'lineage.source' = 'bronze_events',\n    'lineage.transformation' = 'MERGE_with_data_quality',\n    'pii.masked_fields' = 'ip_address'\n);\n```\n\n**Trade-offs & Considerations:**\n\n**Schema Evolution Trade-offs:**\n- ‚úÖ Handles new fields automatically\n- ‚ùå May introduce data type conflicts requiring manual intervention\n- üí° Monitor schema changes with Delta Lake schema history\n\n**MERGE Performance:**\n- ‚úÖ Ensures idempotent processing\n- ‚ùå Higher compute cost vs simple append\n- üí° Optimize with Z-ordering on join keys (user_id, event_ts)\n\n**Data Quality Approach:**\n- ‚úÖ Prevents bad data propagation\n- ‚ùå May reject valid edge cases\n- üí° Implement quarantine table for rejected records\n\n**IP Masking Strategy:**\n- ‚úÖ Complies with privacy regulations\n- ‚ùå Loses analytical value of geolocation\n- üí° Store hash of original IP in secure vault for audit","explanation":"This question tests practical ingestion, drift handling, and governance in Databricks. Candidates must demonstrate proficiency with Auto Loader for incremental JSON ingestion, schema evolution techniques for handling evolving data structures, MERGE-based upserts for maintaining data consistency while preserving history, basic data quality validation and PII masking, and Unity Catalog RBAC with data lineage tracking.\n\n**Key Assessment Areas:**\n- **Auto Loader Implementation**: Understanding cloudFiles format, schema location management, and mergeSchema configuration\n- **Schema Evolution Handling**: Knowledge of how Delta Lake manages schema changes and potential conflicts\n- **MERGE Operations**: Ability to write efficient upsert logic with proper join conditions and handling of duplicates\n- **Data Quality Framework**: Implementation of validation rules and error handling strategies\n- **PII Protection**: Understanding of masking techniques and privacy compliance requirements\n- **Unity Catalog Governance**: Role-based access control, permission management, and lineage tracking\n\n**What Makes a Strong Answer:**\n- Concrete code examples showing practical implementation\n- Discussion of performance trade-offs and optimization strategies\n- Understanding of production considerations like error handling and monitoring\n- Knowledge of security and compliance best practices\n- Ability to explain the 'why' behind architectural decisions","diagram":null,"difficulty":"beginner","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T05:33:41.927Z","createdAt":"2026-01-14T22:55:34.382Z"},{"id":"q-2199","question":"How would you implement a compliant, zero-duplication deletion workflow in a Databricks streaming pipeline that ingests user events from Kafka into Delta Lake? A deletions log triggers row-level removals across Bronze/Silver, handling late data with tombstones, ensuring idempotent MERGE, and preserving an auditable deletion history via a separate Delta table. Describe architecture, data flow, and validation?","answer":"Implement a deletion workflow with a DeletionLog stream; when a deletion request arrives, publish user_id and deletion_ts to a Delta table and a separate DeletionLog; on Bronze and Silver, use MERGE t","explanation":"## Why This Is Asked\n\nTests the ability to design compliant, end-to-end data deletion in a streaming Delta Lake pipeline. It covers change data capture semantics, idempotency, auditability, and cross-layer consistency (Bronze to Silver), which are critical at scale.\n\n## Key Concepts\n\n- Delta Lake MERGE for row-level deletions\n- Deletion log and tombstone handling\n- Idempotent operations and deleted_at auditing\n- Data lineage, governance, and RBAC in Unity Catalog\n\n## Code Example\n\n```javascript\n// Pseudo-Delta MERGE for deletion\nconst stmt = `\nMERGE INTO BronzeDelta AS b\nUSING DeletionLog AS d\nON b.user_id = d.user_id\nWHEN MATCHED THEN DELETE\n`;\ndb.run(stmt);\n```\n\n## Follow-up Questions\n\n- How would you test end-to-end deletion latency and ensure no residual copies in backups? \n- How would you scale the deletion workflow when deletions are frequent and from multiple sources?","diagram":"flowchart TD\n  A[Kafka Stream] --> B[Bronze Delta]\n  B --> C[Silver Delta]\n  D[Deletion Log] --> C\n  C --> E[Audit Table]","difficulty":"advanced","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Databricks","Goldman Sachs"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T07:06:54.769Z","createdAt":"2026-01-15T07:06:54.769Z"},{"id":"q-2279","question":"In a Databricks job ingesting 10 GB/day of user event JSON from S3 into Bronze Delta Lake, design a beginner-friendly Silver pipeline that upserts latest user state by user_id using MERGE, enforces Delta constraints (NOT NULL user_id, age > 0), and fails the pipeline on any quality violation. How would you implement and test this end-to-end?","answer":"Use Autoloader to ingest JSON into Bronze Delta Lake, then Silver via MERGE on user_id to upsert latest state. Enforce Delta constraints: NOT NULL user_id, age > 0, and non-empty payload. Add a Spark ","explanation":"## Why This Is Asked\nThis question probes practical data ingestion, safe upserts, and basic data quality controls in Databricks for a real-world, beginner-friendly scenario.\n\n## Key Concepts\n- Delta Lake constraints (NOT NULL, CHECK-like rules)\n- MERGE upserts for incremental state updates\n- Auto Loader for resilient, schema-evolving ingestion\n- Simple data quality gates that fail jobs on violations\n\n## Code Example\n```sql\n-- MERGE example (pseudo for illustration)\nMERGE INTO silver_table AS s\nUSING bronze_table AS b\nON s.user_id = b.user_id\nWHEN MATCHED THEN UPDATE SET *\nWHEN NOT MATCHED THEN INSERT (*)\n```\n\n```python\n# Pseudo Spark check (counts violations)\nviolations = bronze_df.filter(col('user_id').isNull() | (col('age') <= 0) | (col('payload').eqNullSafe(''))).count()\nif violations > 0:\n  raise Exception(f\"Data quality violation count: {violations}\")\n```\n\n## Follow-up Questions\n- How would you handle late-arriving data or schema evolution?\n- How would you test quality gates at scale across partitions?","diagram":null,"difficulty":"beginner","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Netflix","Salesforce","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T10:39:53.066Z","createdAt":"2026-01-15T10:39:53.066Z"},{"id":"q-2397","question":"In a Databricks data pipeline, ingest telemetry from two sources: S3 JSONs (Auto Loader) and a Kafka topic, with evolving schema (device_id, ts, metrics...). Build Bronze, Silver (dedupe by event_id using MERGE CDC), Gold. Implement 2-minute late data watermark, PII masking, and Unity Catalog RBAC with lineage across Bronze/Silver/Gold. Provide concrete steps, partitioning, and a lightweight test plan?","answer":"Ingest via Auto Loader and structured streaming, Bronze stores raw JSON, Silver uses MERGE on event_id and upserts latest per event, a 2-minute watermark for late data, Gold computes hourly device agg","explanation":"## Why This Is Asked\nThis question probes end-to-end data engineering with multi-source ingestion, schema evolution, upsert-based CDC, governance, and testing in Databricks.\n\n## Key Concepts\n- Multi-source streaming ingestion (S3 Auto Loader + Kafka)\n- Delta Lake Bronze/Silver/Gold with MERGE CDC\n- Watermarks for late data\n- PII masking and data masking strategies\n- Unity Catalog RBAC and lineage across layers\n- Lightweight data quality tests\n\n## Code Example\n```sql\nMERGE INTO silver.telemetry AS t\nUSING bronze.raw AS s\nON t.event_id = s.event_id\nWHEN MATCHED THEN UPDATE SET t.* = s.*\nWHEN NOT MATCHED THEN INSERT *;\n```\n\n## Follow-up Questions\n- How would you validate watermark correctness and handle out-of-order events in production?\n- How would you extend RBAC to allow per-device-group access without leaking sensitive fields?","diagram":"flowchart TD\n  A(Source: S3 & Kafka) --> B(Bronze: Raw JSON)\n  B --> C(Silver: Deduped + Masked)\n  C --> D(Gold: Hourly Aggregates)\n  C --> E(Lineage & RBAC)","difficulty":"intermediate","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Goldman Sachs","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T16:46:28.084Z","createdAt":"2026-01-15T16:46:28.084Z"},{"id":"q-2435","question":"Design a Databricks pipeline ingesting telemetry from S3 (Auto Loader) and a Kafka topic with evolving schema. Build Bronze/Silver/Gold with CDC dedupe by event_id; add a drift-detection layer that scores per-field distribution changes and quarantines anomalous records. Mask PII before Silver/Gold, enforce Unity Catalog RBAC with lineage. Provide concrete steps, partitioning, and a lightweight test plan?","answer":"Design uses Auto Loader for S3 and Kafka in Bronze, Silver performs CDC MERGE by event_id, Gold is the consumable layer. Add a drift-detection pass that scores per-field distribution changes and quara","explanation":"## Why This Is Asked\nTests multi-source ingestion with evolving schemas, real-time anomaly handling, and governance at scale. Probes drift scoring, quarantine routing, late-data handling, PII masking, and Unity Catalog RBAC with lineage across layers.\n\n## Key Concepts\n- Multi-source ingestion (S3 Auto Loader, Kafka)\n- CDC dedupe by event_id\n- Delta schema evolution and drift scoring\n- Two-minute watermark for late data\n- PII masking strategies\n- Unity Catalog RBAC and lineage\n- Quarantine routing and tests\n\n## Code Example\n```javascript\n// drift-score helper (illustrative)\nfunction driftScore(oldRecord, newRecord, fields){\n  let s = 0\n  fields.forEach(f => {\n    if (typeof oldRecord[f] !== typeof newRecord[f]) s += 1\n  })\n  return s\n}\n```\n\n## Follow-up Questions\n- How to tune drift thresholds and adapt to schema evolution?\n- How would you validate quarantine routing and lineage exports across environments?","diagram":"flowchart TD\n  Bronze[Bronze Layer: S3 + Kafka] --> Silver[Silver Layer: CDC MERGE by event_id]\n  Silver --> Gold[Gold Layer: masked, consumable]\n  Silver --> Quarantine[Quarantine Layer: anomalies]\n  Quarantine --> Silver","difficulty":"advanced","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Microsoft","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T17:53:16.495Z","createdAt":"2026-01-15T17:53:16.495Z"},{"id":"q-2577","question":"In a Databricks Delta Live Tables (DLT) pipeline, ingest telemetry from two sources: S3 Auto Loader JSON with evolving schema and a Kafka stream. Implement multi-source dedup by a composite key (device_id, event_id), support up to 5-minute late data with a watermark, mask PII fields, and publish lineage via Unity Catalog RBAC across Bronze Silver Gold. Provide concrete steps, test plan, and rollback path?","answer":"Design a Delta Live Tables pipeline that ingests data from two sources: S3 Auto Loader for JSON files with evolving schema and a Kafka stream. Implement composite-key deduplication using device_id and event_id, configure a 5-minute watermark for late data handling, apply field-level PII masking to sensitive fields like ip_address, email, and phone, and establish data lineage through Unity Catalog RBAC across Bronze, Silver, and Gold layers.","explanation":"## Why This Is Asked\nThis question evaluates end-to-end data engineering capabilities including multi-source ingestion, deduplication strategies, late data handling, data privacy compliance, and governance implementation in a comprehensive DLT workflow.\n\n## Key Concepts\n- Delta Live Tables multi-source ingestion patterns\n- Composite-key deduplication strategies\n- Watermarking for late data tolerance\n- Field-level PII masking implementation\n- Unity Catalog RBAC and lineage management\n- Bronze-Silver-Gold medallion architecture\n\n## Code Example\n```javascript\n// Pseudo DLT skeleton for Bronze->Silver with dedupe and masking\n@dlt.table\ndef bronze(raw: DataFrame) -> DataFrame:\n  return raw\n\n@dlt.table\ndef silver(bronze: DataFrame) -> DataFrame:\n  val = bronze.dropDuplicates([\"device_id\", \"event_id\"])\\\n    .withWatermark(\"timestamp\", \"5 minutes\")\\\n    .mask_pii([\"ip_address\", \"email\", \"phone\"])\n  return val\n```\n\n## Implementation Approach\n1. Configure S3 Auto Loader with schema evolution\n2. Set up Kafka stream ingestion\n3. Implement composite-key deduplication logic\n4. Configure watermarking for late data\n5. Apply PII masking transformations\n6. Establish Unity Catalog permissions and lineage\n\n## Testing Strategy\n- Validate data ingestion from both sources\n- Test deduplication accuracy\n- Verify late data handling with watermark\n- Confirm PII masking effectiveness\n- Audit Unity Catalog permissions and lineage\n\n## Rollback Plan\n- Maintain versioned DLT pipeline definitions\n- Preserve raw data in landing zones\n- Document rollback procedures for each layer\n- Establish monitoring for pipeline health","diagram":"flowchart TD\n  A[S3 Auto Loader JSON] --> Bronze\n  B[Kafka Stream] --> Bronze\n  Bronze --> Silver\n  Silver --> Gold","difficulty":"advanced","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","NVIDIA","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T05:15:32.236Z","createdAt":"2026-01-15T23:37:55.308Z"},{"id":"q-2662","question":"In a Databricks Delta Lake pipeline ingesting telemetry from two sources (S3 JSONs via Auto Loader with evolving schema and a streaming Kafka feed), design a GDPR-style purge workflow to delete or mask PII across Bronze, Silver, and Gold while preserving aggregates and lineage. Describe concrete steps using MERGE/DELETE, specify retention, audit logging, and how you test downstream dashboards and time-travel reads. Include RBAC considerations?","answer":"Trigger a purge workflow that masks PII and deletes records across Bronze, Silver, and Gold using a MERGE keyed by event_id or user_id when a deletion request arrives; set pii columns to NULL, flag ro","explanation":"## Why This Is Asked\nTests privacy controls, cross-layer purge, and governance in a realistic Databricks setup.\n\n## Key Concepts\n- Delta Lake MERGE for in-place masking/deletes\n- Time travel for validation and rollback\n- Governance with Unity Catalog RBAC and lineage\n- VACUUM retention and audit-trail design\n\n## Code Example\n```sql\n-- Example purge MERGE across Bronze\nMERGE INTO bronze.telemetry AS b\nUSING purge_queue AS p\nON b.event_id = p.event_id\nWHEN MATCHED THEN UPDATE SET\n  pii_email = NULL,\n  pii_phone = NULL,\n  purged = TRUE;\n```\n\n## Follow-up Questions\n- How would you validate purge correctness across Bronze/Silver/Gold with synthetic data?\n- How do you handle partial deletes for multi-tenant data while preserving aggregates?","diagram":null,"difficulty":"intermediate","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T05:45:56.550Z","createdAt":"2026-01-16T05:45:56.550Z"},{"id":"q-2743","question":"Design a multi-tenant Databricks pipeline for a SaaS IoT product. Ingest streaming telemetry from an MQTT bridge and a batch Postgres metadata feed. Build Bronze (raw), Silver (device_id normalization, unit standardization, PII masking), Gold (per-tenant aggregates). Implement schema evolution, a 2-minute late-data watermark, RBAC via Unity Catalog with per-tenant lineage, and a lightweight QA suite. Provide concrete steps, partitioning, and validation plan?","answer":"Implement with Delta Live Tables: Bronze ingests from MQTT bridge and Postgres batch feed; Silver normalizes device_id, standardizes units, masks PII; Gold computes per-tenant aggregates (devices_onli","explanation":"## Why This Is Asked\nTests multi-tenant governance, IoT ingestion, schema evolution, and late data handling; evaluates real-world trade-offs across Bronze/Silver/Gold and privacy masking.\n\n## Key Concepts\n- Delta Live Tables with Bronze/Silver/Gold\n- MQTT ingestion and batch Postgres feed\n- Schema evolution, 2-minute watermark\n- PII masking, Unity Catalog RBAC, tenant lineage\n- Validation and QA hooks\n\n## Code Example\n```python\n# Example DLT Python skeleton\nimport dlt\n@dlt.table\ndef bronze_raw():\n  return spark.read...  # placeholder\n\n@dlt.table\ndef silver_normalize():\n  ...\n```\n\n## Follow-up Questions\n- How would you handle schema drift across tenants?\n- How would you validate late-data handling without impacting live tenants?","diagram":"flowchart TD\n  Bronze[Bronze - Raw MQTT & Postgres]</n  Silver[Silver - Normalized, PII masked]</n  Gold[Gold - Per-tenant aggregates]\n  Bronze --> Silver\n  Silver --> Gold\n  TenantRBAC(Unity Catalog) --> Silver\n  TenantRBAC(Unity Catalog) --> Gold","difficulty":"advanced","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Databricks","Discord"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T09:52:06.223Z","createdAt":"2026-01-16T09:52:06.223Z"},{"id":"q-2794","question":"In a Databricks environment, you receive a daily JSON feed of fintech transactions from a partner S3 bucket. Build a three-table pipeline: Bronze Delta with Auto Loader and schema evolution, Silver Delta that upserts deduplicating by transaction_id, and Gold that aggregates daily totals by customer_id. The JSON evolves (new fields like merchant_id and currency). Explain how you'd implement schema evolution, a MERGE based upsert, and partitioning by transaction_date. Include a lightweight data quality test plan and PII masking in outputs?","answer":"Use Auto Loader to load JSON into Bronze with schema evolution for new fields merchant_id and currency. Silver handles dedup via MERGE on transaction_id, with partitioning by transaction_date. Gold ag","explanation":"## Why This Is Asked\n\nThis question tests practical data engineering for evolving schemas, bronze-silver-gold layering, and robust upserts in a Databricks workflow. It also covers data quality and PII masking‚Äîessential beginner skills in real-world pipelines.\n\n## Key Concepts\n\n- Auto Loader with schema evolution for JSON feeds\n- Delta Lake MERGE for upserts and deduplication\n- Partitioning by transaction_date to optimize queries\n- Basic data quality checks (NOT NULL, amount > 0, valid timestamps)\n- PII masking in downstream views\n\n## Code Example\n\n```javascript\n// Pseudo-Spark SQL example for Bronze to Silver MERGE\nconst bronze = spark.read.format(\"delta\").load(\"/mnt/partner/transactions/bronze\")\nbronze.createOrReplaceTempView(\"bronze\")\n\nspark.sql(`\nMERGE INTO silver AS s\nUSING bronze AS b\nON s.transaction_id = b.transaction_id\nWHEN MATCHED THEN UPDATE SET s.amount = b.amount, s.event_ts = b.event_ts, s.merchant_id = b.merchant_id, s.currency = b.currency\nWHEN NOT MATCHED THEN INSERT (transaction_id, user_id, amount, event_ts, merchant_id, currency, transaction_date) VALUES (b.transaction_id, b.user_id, b.amount, b.event_ts, b.merchant_id, b.currency, b.transaction_date)\n`)\n```\n\n## Follow-up Questions\n\n- How would you test schema evolution with a feed that adds merchant_id midstream? \n- Which monitoring checks would you add to detect MERGE conflicts or late data issues and ensure masking stays intact?","diagram":null,"difficulty":"beginner","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","Netflix","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T13:01:19.972Z","createdAt":"2026-01-16T13:01:19.972Z"},{"id":"q-2987","question":"Design a Databricks data pipeline that ingests telemetry from two sources: a streaming Kafka topic and a batch REST API, both with evolving schemas. Build Bronze (raw), Silver (mask PII and implement a 2-minute watermark), and Gold (Customer SCD2) layers, with CDC merges. Enforce Unity Catalog RBAC with lineage across layers, and outline concrete partitioning, test plans, and recovery strategies?","answer":"Ingest from Kafka (stream) and REST API (batch) into Bronze with Auto Loader and schema evolution, then Silver masking PII and applying a 2-minute watermark. Use MERGE CDC to upsert into Gold where Cu","explanation":"## Why This Is Asked\n\nTests ability to integrate streaming and batch sources with schema evolution, implement SCD2, masking, and governance across lakehouse layers; evaluates practical decisions on partitioning, watermarking, and RBAC.\n\n## Key Concepts\n\n- Multi-source ingestion and schema evolution\n- Silver masking and 2-minute watermark\n- Gold: Slowly Changing Dimension Type 2\n- Unity Catalog RBAC and lineage\n\n## Code Example\n\n```javascript\n// Pseudo steps for MERGE-based SCD2 in Bronze->Silver->Gold\n```\n\n## Follow-up Questions\n\n- How would you handle late data beyond the watermark?\n- How would you validate end-to-end data lineage across Bronze, Silver, and Gold?","diagram":"flowchart TD\n  A[Ingest Kafka (Bronze)] --> B[Bronze: Kafka raw]\n  C[Ingest REST API (Bronze)] --> D[Bronze: REST raw]\n  B --> E[Silver: Clean & PII masked]\n  D --> E\n  E --> F[Gold: Customer SCD2]\n  F --> G[Lineage: Unity Catalog]","difficulty":"intermediate","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T20:30:45.198Z","createdAt":"2026-01-16T20:30:45.199Z"},{"id":"q-3011","question":"Design a cross-tenant Databricks Delta Live Tables pipeline that shares Gold metrics with external partners via Delta Sharing, while enforcing per-tenant isolation with Unity Catalog. Ingest telemetry from S3 JSON with evolving schema and a high-throughput Kafka topic. Build Bronze (raw), Silver (dedupe by event_id with a 2-minute watermark), and Gold (per-tenant aggregates). Include masking for PII, partition strategy, tests, and RBAC plan?","answer":"Set up a Delta Live Tables pipeline with Bronze ingest from S3 (Auto Loader) and Kafka, Silver deduped by event_id using MERGE CDC with a 2-minute watermark, and Gold with per-tenant aggregates. Enfor","explanation":"## Why This Is Called\nTests ability to design multi-tenant data products with external sharing, RBAC, masking, and data quality in Databricks.\n\n## Key Concepts\n- Delta Live Tables orchestration for Bronze/Silver/Gold\n- Delta Sharing for external partner access\n- Unity Catalog RBAC with per-tenant isolation\n- Dynamic masking for PII fields\n- Schema evolution on JSON/S3 + Kafka ingestion\n- 2-minute late data watermark and CDC dedupe\n\n## Code Example\n```python\n# Pseudo masking example\ndef mask_pii(value):\n  if value is None:\n    return None\n  return 'REDACTED'\n```\n\n## Follow-up Questions\n- How would you test multi-tenant access and masking across Delta Sharing shares?\n- How would you monitor lineage from Bronze to Gold and ensure compliant RBAC changes propagate?","diagram":null,"difficulty":"advanced","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T21:29:53.549Z","createdAt":"2026-01-16T21:29:53.550Z"},{"id":"q-3115","question":"Design a Databricks Delta Live Tables pipeline for a drone delivery fleet. Ingest two streams: a) S3-stored JSON drone events (Bronze) with evolving fields (drone_id, ts, lat, lon, battery, sensor), b) PostgreSQL CDC drone metadata (model, region, maint_status). Build Bronze, Silver, Gold with a MERGE CDC to upsert Silver and support schema evolution, and a Gold layer with enriched metrics. Implement a 2-minute late-data watermark, mask operator_id in Bronze, and enforce Unity Catalog RBAC with lineage across layers. Include partitioning strategy, testing plan, and trade-offs?","answer":"Leverage Delta Live Tables with two streams: Bronze from S3 Auto Loader and PostgreSQL CDC Bronze. Silver uses MERGE CDC to dedupe and apply schema evolution; Gold computes enriched metrics. Implement","explanation":"## Why This Is Asked\nTests real-world multi-stream ingestion, CDC upserts, dynamic schema evolution, data masking, and lineage governance in a single pipeline. It also probes RBAC and testing discipline for production readiness.\n\n## Key Concepts\n- Delta Live Tables, MERGE CDC, schema evolution\n- Watermarking for late data\n- Data masking of PII fields\n- Unity Catalog RBAC and lineage capture across Bronze/Silver/Gold\n- Multi-stream joins and partitioning strategies\n\n## Code Example\n```javascript\n// Pseudo-DDT: mask operator_id in Bronze before Silver\n// This is a conceptual snippet; actual DLT Python/SQL syntax will vary by project\nimport pyspark.sql.functions as F\nbronze = spark.readStream.format('delta').load('s3://bucket/drone/bronze')\nsilver = bronze.withColumn('operator_id_masked', F.sha2(F.col('operator_id'), 256))\n```\n\n## Follow-up Questions\n- How would you test schema evolution with a new field added to the drone events?\n- How would you validate lineage and RBAC across all three layers in Unity Catalog?","diagram":null,"difficulty":"intermediate","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T04:00:03.867Z","createdAt":"2026-01-17T04:00:03.867Z"},{"id":"q-3155","question":"Design a production ready Databricks data pipeline for telemetry arriving from S3 Auto Loader and a Kafka topic with an evolving schema. Build Bronze Silver Gold with Silver deduplicating on event_id using MERGE CDC, a 2 minute watermark for late data, and PII masking. Enforce Unity Catalog RBAC with lineage. Propose a Great Expectations CI/CD test harness with synthetic data and a rollback strategy?","answer":"Use Delta Live Tables to orchestrate Bronze (S3 Auto Loader) and Silver/Gold, with Kafka as a Bronze input. Implement dedupe on event_id in Silver via MERGE CDC, enforce a 2-minute watermark for late ","explanation":"## Why This Is Asked\nTests ability to design end-to-end data quality, governance, and testing in a real Databricks environment with evolving schemas and multi-source ingestion.\n\n## Key Concepts\n- DLTs, Auto Loader, structured streaming\n- MERGE CDC deduplication\n- watermarking for late data\n- PII masking and Unity Catalog RBAC\n- Great Expectations CI/CD\n\n## Code Example\n```python\n# skeleton: create expectations for schema and data quality\nimport great_expectations as ge\n\n# define a simple expectation suite for Bronze->Silver\nsuite = ge.from_pandas(bronze_df.head())\nsuite.expect_column_values_to_not_be_null(\"event_id\")\nsuite.expect_column_values_to_be_of_type(\"ts\", \"datetime64[ns]\")\n```\n\n## Follow-up Questions\n- How would you test schema drift across evolving fields and flag PRs automatically?\n- What rollback considerations exist if a downstream consumer caches Gold results?","diagram":"flowchart TD\n  A[Ingest Bronze: S3 Auto Loader + Kafka] --> B[Silver: dedupe by event_id (MERGE CDC)]\n  B --> C[Gold: analytics layer]\n  A --> D[PII masking in Bronze/Silver]\n  B --> E[ lineage via Unity Catalog RBAC ]","difficulty":"intermediate","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","LinkedIn","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T04:50:08.964Z","createdAt":"2026-01-17T04:50:08.964Z"},{"id":"q-3250","question":"Design a Databricks streaming pipeline ingesting S3 Auto Loader JSONs and a Kafka topic with evolving schema. Build Bronze/Silver/Gold with event_id dedupe via MERGE CDC and a 2-minute late data watermark. Add an observability layer: inline data drift detection for numeric fields and schema drift alerts with Slack alerts; enforce Unity Catalog RBAC with lineage. Provide concrete steps and a minimal test plan?","answer":"Propose Bronze from S3 Auto Loader and Kafka, handle evolving schema; Silver dedupe by event_id with MERGE CDC; Gold computes per-user metrics. 2-minute watermark. Observability layer: rolling window ","explanation":"## Why This Is Asked\n\nThis question probes how a candidate adds observability to a streaming Databricks pipeline, not just correctness. It requires practical design choices for drift checks, schema evolution, and RBAC in Unity Catalog, plus how to validate and test the setup.\n\n## Key Concepts\n\n- Delta Lake streaming with evolving schema and MERGE CDC\n- 2-minute watermark for late data\n- Data drift and schema drift detection across numeric fields\n- Slack alerting and lightweight test plan\n- Unity Catalog RBAC with lineage across Bronze/Silver/Gold\n\n## Code Example\n\n```python\n# Drift check sketch (pseudo)\nfrom pyspark.sql import functions as F\n# compute rolling stats and compare to baseline; trigger alert on threshold breach\n```\n\n## Follow-up Questions\n\n- How would you test drift thresholds and alert reliability?\n- How to scale drift checks across many numeric fields and tenants?","diagram":null,"difficulty":"intermediate","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Robinhood","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T08:44:24.519Z","createdAt":"2026-01-17T08:44:24.519Z"},{"id":"q-3325","question":"In a Databricks data-engineer exercise, ingest daily Parquet logs of customer events from S3 into Delta Lake; events originate from several microservices with event_time in different time zones. Describe a beginner-friendly pipeline that uses Auto Loader, Bronze/Silver with a simple MERGE to upsert the latest per customer_id, standardizes timestamps to UTC, and validates with a lightweight data quality check (not null, reasonable ranges). Include a partitioning plan and a quick validation approach?","answer":"Use Auto Loader to ingest S3 Parquet into Bronze. In Silver, cast event_time to UTC (from per-service TZ via to_timestamp(event_time, tz)). Upsert latest per customer_id with MERGE into Silver; partit","explanation":"## Why This Is Asked\n\nAssesses ability to handle multi-time-zone data with a simple Bronze/Silver pattern, using MERGE for upserts and basic data quality checks in a beginner-friendly way.\n\n## Key Concepts\n\n- Delta Lake Bronze/Silver pattern\n- Auto Loader ingestion\n- Timezone normalization to UTC\n- MERGE upserts for latest events\n- Lightweight data quality checks\n\n## Code Example\n\n```python\n# PySpark sketch (conceptual)\nfrom pyspark.sql import functions as F\n\nbronze = spark.read.format(\"cloudFiles\").option(\"cloudFiles.format\",\"parquet\").load(\"s3://bucket/bronze/\")\n# Normalize to UTC using per-service timezone column 'tz'\nsilver = bronze.withColumn(\"event_time_utc\", F.to_utc_timestamp(F.col(\"event_time\"), F.col(\"tz\")))\nlatest = silver.groupBy(\"customer_id\").agg(F.max(\"event_time_utc\").alias(\"event_time_utc\"))\nupsert = silver.alias(\"s\").join(latest.alias(\"l\"), [\"customer_id\", \"event_time_utc\"]).select(\"s.*\")\n# In real code, MERGE into Silver table with upserts\n```\n\n## Follow-up Questions\n\n- How would you validate timezone normalization across multiple sources?\n- How would you monitor and alert on data quality regressions in this pipeline?","diagram":"flowchart TD\n  A[Ingest: S3 Parquet] --> B[Bronze Delta Table]\n  B --> C[Transform: UTC normalization]\n  C --> D[Silver Delta Table with MERGE upsert]\n  D --> E[Quality checks & partitioning]\n","difficulty":"beginner","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T11:34:10.631Z","createdAt":"2026-01-17T11:34:10.631Z"},{"id":"q-3468","question":"In a multi-tenant Databricks data platform, design a Delta Live Tables pipeline that ingests telemetry from two sources: S3 Auto Loader JSON drops and a Kafka topic, with per-tenant isolation. Build Bronze, Silver, and Gold layers; deduplicate in Silver by event_id using MERGE CDC; apply per-tenant data masking for PII; enforce Unity Catalog RBAC with lineage; handle per-tenant schema evolution and provide a rollback path via Delta Time Travel. Include synthetic data testing plan?","answer":"Design a multi-tenant Delta Live Tables pipeline: ingest from S3 Auto Loader and Kafka; Bronze raw with tenant_id, ts, event_id; Silver dedups by event_id via MERGE CDC and masks PII per tenant; Gold ","explanation":"## Why This Is Asked\nTests multi-tenant data governance, per-tenant masking, and end-to-end DLT with lineage and rollback.\n\n## Key Concepts\n- Delta Live Tables, Bronze/Silver/Gold layering\n- MERGE CDC for per-tenant dedup\n- Per-tenant PII masking logic\n- Unity Catalog RBAC and lineage\n- Per-tenant schema evolution and Delta Time Travel rollbacks\n- Synthetic data CI validation\n\n## Code Example\n```python\nimport dlt\n@dlt.table\ndef bronze_raw():\n    pass\n@dlt.table\ndef silver_dedup():\n    pass\n@dlt.table\ndef gold_agg():\n    pass\n```\n\n## Follow-up Questions\n- How would you monitor per-tenant SLAs and data quality?\n- How do you handle cross-tenant schema drift conflicts?\n- What test strategies validate RBAC isolation across tenants?\n","diagram":null,"difficulty":"advanced","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Bloomberg","DoorDash"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T17:33:00.167Z","createdAt":"2026-01-17T17:33:00.167Z"},{"id":"q-3604","question":"In a Databricks pipeline ingesting 10 GB/day of Parquet user events from S3 into a Bronze Delta table via Auto Loader, design a beginner-friendly Silver layer that deduplicates by event_id, handles 2-minute late data with a watermark, and masks PII (email) at query time using a simple hashing or masking approach. Outline concrete steps including partitioning, a MERGE-based CDC, Unity Catalog RBAC, and a lightweight test plan. How would you implement the masking view?","answer":"Design a Silver layer that ingests from the Bronze table using MERGE statements to deduplicate by event_id, implements a 2-minute watermark for late data handling, partitions by date for optimal performance, and creates a masked view that hashes email fields using SHA2(256) to protect PII while maintaining data relationships.","explanation":"## Why This Is Asked\n\nTests ability to design an end-to-end Databricks pipeline at a beginner level, including deduplication, late data handling, data privacy, and governance.\n\n## Key Concepts\n\n- Bronze-Silver-Gold data modeling in Delta Lake\n- MERGE for idempotent upserts and CDC\n- Structured Streaming watermark for late data\n- Data masking via views (hashing sensitive fields)\n- Unity Catalog RBAC for data access control\n\n## Code Example\n\n```sql\n-- Masked view example\nCREATE OR REPLACE VIEW silver_masked AS\nSELECT event_id,\n       user_id,\n       SHA2(email, 256) AS email_hash,\n       timestamp,\n       event_type,\n       event_data\nFROM silver_events;\n```","diagram":null,"difficulty":"beginner","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Meta","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T05:29:24.991Z","createdAt":"2026-01-17T23:29:47.300Z"},{"id":"q-3667","question":"Design a Databricks data pipeline that ingests 5 TB/day of semi-structured JSON events from S3 into a Bronze Delta table, then propagates changes to Silver and Gold marts using Delta Change Data Feed (CDF) without re-reading full history. Implement 2-minute late data handling, PII masking at query time, Unity Catalog RBAC, and referential integrity across marts. Outline CDC strategy, partitioning, and a practical test plan?","answer":"Enable Delta CDF on the Bronze table for event_id. Use MERGE into Silver to apply Inserts/Updates and tombstones from Bronze. Build Gold via MERGE from Silver, preserving referential keys. Apply a lig","explanation":"## Why This Is Asked\n\nTests deep knowledge of CDC patterns, late data handling, and data governance in a realistic Databricks setup.\n\n## Key Concepts\n\n- Delta Lake Change Data Feed (CDF)\n- MERGE CDC for Silver\n- Gold derivation and referential integrity\n- Query-time masking\n- Unity Catalog RBAC and lineage\n- Late data windowing\n\n## Code Example\n\n```python\n# Enable CDF\nspark.sql(\"ALTER TABLE bronze SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\")\n# Pseudo-merges shown conceptually; implement with exact match/conditions in production\n# MERGE Bronze -> Silver on event_id with updates and deletes\n```\n\n## Follow-up Questions\n\n- How would you handle schema evolution across Bronze to downstream without breaking queries?\n- What metrics would you monitor to ensure CDC latency remains within target?","diagram":null,"difficulty":"advanced","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Hashicorp"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T04:18:56.001Z","createdAt":"2026-01-18T04:18:56.001Z"},{"id":"q-3854","question":"In a Databricks data platform for a retail chain, ingest 3 sources: S3 Parquet orders, S3 JSON customers, and Kafka click events, into Bronze, Silver, and Gold using Delta Live Tables. Build a centralized data-contract registry with versioned schemas and field-level expectations, enforce it via contract-aware backfills: bump a contract version, backfill Bronze, then cascade to Silver/Gold. Outline promotion flow, compatibility rules, testing plan, and Unity Catalog RBAC considerations?","answer":"Implement a centralized data-contract registry with versioned schemas and field-level expectations; enforce via Delta Live Tables contract checks. On version bumps, trigger Bronze backfill, then casca","explanation":"## Why This Is Asked\nThis question probes data contract governance, backfill strategies, and lineage in a real Databricks setup.\n\n## Key Concepts\n- Data contracts, versioning, optional vs required fields\n- Delta Live Tables, Bronze/Silver/Gold patterns\n- Backfill workflows and PR promotions\n- Unity Catalog RBAC and lineage\n\n## Code Example\n```python\n# Pseudo-code illustrating contract enforcement concept\ndef enforce_contract(df, contract_schema):\n    if df.schema != contract_schema:\n        raise ValueError(\"Schema mismatch: expected {}\".format(contract_schema))\n    return df\n```\n\n## Follow-up Questions\n- How would you optimize backfill performance on large Bronze datasets?\n- How would you test contract changes in CI/CD and handle rollbacks if downstream data is affected?\n","diagram":null,"difficulty":"intermediate","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T12:59:40.513Z","createdAt":"2026-01-18T12:59:40.514Z"},{"id":"q-3914","question":"In a beginner Databricks pipeline, ingest 5 GB/day JSON clickstream from S3 into Bronze via Auto Loader; flatten nested payloads into Silver, deduplicate by event_id with MERGE CDC, and apply a 2-minute watermark. Build Gold as daily active users by country. Partition by date; enforce Unity Catalog RBAC. Provide a lightweight test plan and RBAC considerations?","answer":"Ingest 5 GB/day JSON clickstream from S3 to Bronze via Auto Loader; flatten nested payloads into Silver, deduplicate by event_id with MERGE CDC, and apply a 2-minute watermark. Build Gold as daily act","explanation":"## Why This Is Asked\nTests ability to handle nested JSON, streaming ingestion with Auto Loader, deduplication via MERGE CDC, late data handling with a watermark, and governance via Unity Catalog RBAC. Also evaluates end-to-end testing mindset.\n\n## Key Concepts\n- Auto Loader streaming ingestion\n- Flattening nested JSON payloads\n- MERGE CDC for event_id dedup\n- 2-minute watermark for late data\n- Bronze/Silver/Gold data lake pattern\n- Unity Catalog RBAC and lineage\n\n## Code Example\n```python\n# PySpark flattening example\nraw = spark.readStream.format(\"cloudFiles\").option(\"cloudFiles.format\",\"json\").load(\"s3://bucket/clickstream/\")\nsilver = raw.selectExpr(\"event_id\", \"cast(ts as timestamp) as ts\", \"payload.user_id as user_id\", \"payload.country as country\", \"payload.actions as actions\")\n```\n\n## Follow-up Questions\n- How would you monitor late data and alert on stalls?\n- What are the trade-offs between MERGE CDC vs UPDATE-only approaches?","diagram":null,"difficulty":"beginner","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Instacart","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T14:52:04.054Z","createdAt":"2026-01-18T14:52:04.054Z"},{"id":"q-3991","question":"Design a Databricks ingestion path for streaming clickstream: Bronze from S3 JSON (Auto Loader) and Kafka; Silver computes per-user features (last_seen, total_events, avg_session_len) with a windowed watermark; Gold registers features in the Databricks Feature Store for model training. Include schema evolution, Unity Catalog RBAC, and a CI/CD test harness with synthetic data, drift checks, and Delta time-travel backfills with rollback?","answer":"Bronze from S3 JSON (Auto Loader) and Kafka; Silver computes per-user features (last_seen, total_events, avg_session_len) with a 3-minute watermark; Gold registers features in the Databricks Feature S","explanation":"## Why This Is Asked\n\nTests end-to-end design: multi-source ingestion, windowed feature engineering, feature store integration, and governance with RBAC. It also probes backfill, schema evolution, drift detection, and rollback in CI/CD.\n\n## Key Concepts\n- Bronze/Silver/Gold data path\n- Auto Loader + Kafka integration\n- Windowed aggregations and event-time watermark\n- Databricks Feature Store and model training integration\n- Delta Lake schema evolution and constraints\n- Unity Catalog RBAC and data lineage\n- CI/CD with synthetic data, drift checks, and Delta time travel backfills\n\n## Code Example\n```python\n# Placeholder: showcase how you might register a feature in the store (pseudo)\nfrom databricks.feature_store import FeatureStoreClient\nfs = FeatureStoreClient()\nfeature = {\"name\": \"last_seen\",\n           \"type\": \"timestamp\",\n           \"entity\": \"user_id\"}\nfs.create_feature_store_feature(feature)\n```\n\n## Follow-up Questions\n- How would you backfill Silver when a new feature is added without breaking existing models?\n- What drift detection threshold would you use and how would you alert it in CI/CD?","diagram":"flowchart TD\n  Bronze[Bronze: Ingest from S3 JSON + Kafka]\n  Silver[Silver: Per-user features (last_seen, total_events, avg_session_len) with 3m watermark]\n  Gold[Gold: Feature Store for model training]\n  Bronze --> Silver\n  Silver --> Gold","difficulty":"intermediate","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T18:50:19.874Z","createdAt":"2026-01-18T18:50:19.875Z"},{"id":"q-4073","question":"In a Databricks environment ingest 100 GB/day of Parquet user events from S3 and 5 MB/s real-time orders from Kafka. Design Bronze-Silver-Gold pipelines and add a versioned Feature Store with source lineage and drift monitoring. Include: schema evolution strategy, Unity Catalog RBAC, and a rollback/refresh plan for features. How would you implement feature retrieval for model scoring with time-travel to feature versions?","answer":"Use Delta Live Tables for Bronze/Silver/Gold and a versioned Databricks Feature Store with Unity Catalog RBAC. Ingest S3 Parquet and Kafka into Bronze; Silver deduplicates by event_id and handles sche","explanation":"## Why This Is Asked\nExplores a new angle: a versioned, lineage‚Äëaware Feature Store with drift monitoring and rollback, beyond basic Bronze/Silver/Gold patterns.\n\n## Key Concepts\n- Delta Live Tables\n- Unity Catalog RBAC\n- Databricks Feature Store versioning\n- Drift detection and rollback\n- Time travel for reproducible scoring\n\n## Code Example\n```\nSELECT * FROM feature_store.user_session_features AS OF TIMESTAMP '2026-01-18 12:00:00';\n```\n\n## Follow-up Questions\n- How would you tune drift thresholds and rollback triggers?\n- How do you roll back a feature refresh without impacting ongoing scoring?\n","diagram":null,"difficulty":"intermediate","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Lyft","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T22:43:30.937Z","createdAt":"2026-01-18T22:43:30.937Z"},{"id":"q-4086","question":"In a Databricks project ingesting 5 GB/day of JSON event data from two APIs into Delta tables on S3, design a beginner-friendly pipeline that unifies the schemas, builds Bronze, Silver, and Gold layers, and handles 2-minute late data with a watermark. Implement simple per-field normalization, a PII masking approach at query time, and Unity Catalog RBAC restricting access to Silver and Gold. Outline concrete steps including partitioning, schema evolution, and a lightweight test plan?","answer":"Bronze: ingest both API feeds via Auto Loader into Delta Bronze (unified schema). Silver: MERGE-CDC dedup by event_id, normalize data types, apply 2-minute watermark. Mask emails in a read view (hash ","explanation":"## Why This Is Asked\n\nThis question tests practical Databricks ingestion, schema unification, late data handling, and basic data privacy with RBAC, all at a beginner level. It emphasizes concrete steps over theory.\n\n## Key Concepts\n\n- Auto Loader Bronze landing\n- Silver dedupe with MERGE CDC\n- Watermarking for late data\n- Simple PII masking via read-view\n- Unity Catalog RBAC\n\n## Code Example\n\n```javascript\n// Pseudo: masking view example\nCREATE VIEW masked_emails AS SELECT ..., hash_email(email) AS email_masked FROM silver_table;\n```\n\n## Follow-up Questions\n\n- How would you validate that late data does not violate late-arrival guarantees?\n- What tests would you add for schema drift across API updates?","diagram":"flowchart TD\n  BronzeBronze[ Bronze Ingest ] --> SilverSilver[ Silver Layer ]\n  SilverSilver --> GoldGold[ Gold Layer ]\n  subgraph Masking\n    MaskView[ Masking View ]\n  end\n  SilverSilver --> MaskView\n","difficulty":"beginner","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Plaid","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T23:30:39.469Z","createdAt":"2026-01-18T23:30:39.469Z"},{"id":"q-4118","question":"Scenario: Ingest 200 GB/day of JSON transactional events from two e-commerce APIs into Delta Lake on S3. Design a Bronze‚ÜíSilver‚ÜíGold pipeline with Auto Loader, handle evolving schemas (promo_code, device_type, customer_segment), implement per-field normalization, a 2-minute late-data watermark, and a query-time PII masking view. Enforce Unity Catalog RBAC restricting Silver/Gold, optimize partitioning by date and region, define a schema-evolution policy, and outline a lightweight test plan plus a rollback strategy for failed deploys?","answer":"Design a Bronze‚ÜíSilver‚ÜíGold pipeline using Auto Loader to ingest 200 GB/day of JSON events from two e-commerce APIs into Delta Lake on S3. Bronze layer uses Auto Loader with schema inference and cloudFiles to land raw data. Silver layer standardizes field names and types, converts timestamps to UTC, deduplicates by natural key, and handles schema evolution for new fields like promo_code, device_type, and customer_segment. Gold layer aggregates per-user daily metrics. Implement a 2-minute watermark for late data handling and create a query-time PII masking view. Enforce Unity Catalog RBAC to restrict Silver/Gold access, optimize partitioning by date and region, define a schema-evolution policy, and establish a lightweight test plan with rollback strategy for failed deployments.","explanation":"## Why This Is Asked\nTests ability to handle schema drift, governance, and reliability in a multi-layer Delta Lake architecture.\n\n## Key Concepts\n- Auto Loader with Bronze-Silver-Gold pattern on Delta Lake/S3\n- Schema drift handling and evolution policy\n- 2-minute watermark for late data processing\n- Query-time masking view for PII protection\n- Unity Catalog RBAC with column-level security\n- Partitioning optimization by date/region\n- Time-travel rollback capabilities\n\n## Code Example\n```sql\n-- PII masking view example\nCREATE OR REPLACE VIEW masked_users AS\nSELECT user_id, REGEXP_REPLACE(email, '^[^@]+', '***') AS email_mask\nFROM silver_table;\n```\n\n## Follow-up Questions\n- How would you handle backfill for historical data?\n- What monitoring would you implement for pipeline health?\n- How do you ensure data quality across layers?","diagram":null,"difficulty":"intermediate","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Lyft","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-19T04:07:46.704Z","createdAt":"2026-01-19T02:51:03.300Z"},{"id":"q-906","question":"Scenario: A daily Delta Live Tables (DLT) pipeline ingests clickstream JSONs into a Bronze Delta table and then enriches with a users_dim to a Silver table. Build a beginner-friendly pipeline that: deduplicates by event_id, validates user_id via users_dim, filters to business hours 08:00‚Äì18:00, enriches with user fields, and writes to Silver partitioned by event_date. Outline minimal steps and rationale?","answer":"Proposed approach: Use a DLT pipeline with Bronze ingest from JSON, then Silver derived from Bronze. Deduplicate on event_id, keeping the latest by ingest_time. Join to users_dim on user_id to validat","explanation":"## Why This Is Asked\n\nTests ability to design an end-to-end DL pipeline focused on data quality, deduplication, enrichment, and partitioned storage for performance.\n\n## Key Concepts\n\n- Delta Live Tables basics and table dependencies\n- Deduplication by event_id using window or primary-key approaches\n- Referential integrity via dimension lookups during enrichment\n- Time-based filtering for business hours\n- Partitioning Silver by event_date for efficient queries\n\n## Code Example\n\n```javascript\n// Pseudo-DLT sketch (not runnable)\nBronze = read_json('s3://bucket/clicks/bronze/')\nSilver = Bronze\n  .dropDuplicates(['event_id'])\n  .join(users_dim, Bronze.user_id == users_dim.user_id, 'left')\n  .filter(hour(Bronze.event_timestamp) >= 8 and hour(Bronze.event_timestamp) <= 18)\n  .withColumn('event_date', to_date(Bronze.event_timestamp))\n  .writePartitionBy('event_date')\n```\n\n## Follow-up Questions\n\n- How would you handle late-arriving events or backfills in this pipeline?\n- What tests would you add to validate deduplication and the user_id join behavior?","diagram":null,"difficulty":"beginner","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Netflix","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T14:44:28.732Z","createdAt":"2026-01-12T14:44:28.732Z"}],"subChannels":["general"],"companies":["Adobe","Airbnb","Amazon","Anthropic","Apple","Bloomberg","Citadel","Cloudflare","Coinbase","Databricks","Discord","DoorDash","Goldman Sachs","Google","Hashicorp","Hugging Face","IBM","Instacart","LinkedIn","Lyft","Meta","Microsoft","MongoDB","NVIDIA","Netflix","OpenAI","Oracle","Plaid","Robinhood","Salesforce","Scale Ai","Snap","Snowflake","Square","Stripe","Tesla","Twitter","Two Sigma","Uber","Zoom"],"stats":{"total":46,"beginner":16,"intermediate":15,"advanced":15,"newThisWeek":46}}