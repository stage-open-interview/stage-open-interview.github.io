{"questions":[{"id":"q-1100","question":"In a Databricks data pipeline, ingest JSON events from S3 into Delta Lake with Auto Loader. Each event has a nested user object (id, email) and an optional pages array. Some events miss user.email. How would you design the pipeline to safely flatten nested fields, handle missing values, and upsert the latest user state into a Delta Silver table using an idempotent MERGE?","answer":"Use Auto Loader with schema evolution, flatten nested fields, and a MERGE for idempotent upserts. ReadStream via cloudFiles, inferColumnTypes true, mergeSchema true. Flatten: user_id = user.id, email ","explanation":"## Why This Is Asked\n\nTests practical mastery of ingesting semi-structured data, flattening nested JSON, handling optional fields, and performing idempotent upserts in Delta Lake using Databricks primitives.\n\n## Key Concepts\n\n- Auto Loader with cloudFiles options for JSON with schema evolution\n- Flattening nested structs and handling missing fields safely\n- Upserts with MERGE to ensure idempotent state in Delta Lake\n- Null handling with COALESCE and robust array expansion (explode_outer)\n\n## Code Example\n\n```javascript\n// Databricks PySpark-like sketch (syntax-highlighted as javascript)\nval df = spark.readStream.format(\"cloudFiles\")\n  .option(\"cloudFiles.format\",\"json\")\n  .option(\"cloudFiles.inferColumnTypes\",\"true\")\n  .option(\"cloudFiles.mergeSchema\",\"true\")\n  .load(\"s3a://bucket/events/\")\n\nval flat = df.selectExpr(\"user.id as user_id\",\n                       \"coalesce(user.email, '') as email\",\n                       \"ts as event_ts\",\n                       \"explode_outer(pages) as page\")\n```\n\n## Follow-up Questions\n\n- How would you test this pipeline end-to-end, including schema evolution scenarios?\n- How would you handle new nested fields added to user in future deployments?","diagram":null,"difficulty":"beginner","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T22:31:52.482Z","createdAt":"2026-01-12T22:31:52.482Z"},{"id":"q-1112","question":"Scenario: In a Databricks Delta Live Tables pipeline, ingest JSON events from a Kafka source into a Bronze table, where nested fields drift over time (e.g., payload.user.locale is added later, some events miss payload.user). You then transform to a Silver table used by billing and marketing. How would you implement a robust schema-evolution strategy and gating so new fields are captured without breaking existing downstream queries, and how would you test this in a run?","answer":"Enable schema evolution in DLT by merging new fields into Bronze and keeping downstream schema backward compatible; treat optional nested fields as nullable and use defaults; gate with non-null expect","explanation":"## Why This Is Asked\nTests practical schema evolution and data quality gating in a real Delta Live Tables pipeline, focusing on drift in nested JSON and downstream stability.\n\n## Key Concepts\n- Delta Live Tables schema evolution\n- Nested JSON drift handling\n- Downstream backward compatibility\n- Data quality gates/expectations\n- End-to-end testing with mixed schemas\n\n## Code Example\n```python\n# PySpark-like sketch for Bronze -> Silver\nbronze = spark.readStream.format(\"cloudFiles\").option(\"cloudFiles.format\",\"json\").load(\"/bronze\")\nsilver = bronze.selectExpr(\"payload.user.id as user_id\", \"payload.user.locale as user_locale\", \"payload.action as action\", \"timestamp\")\nsilver = silver.withColumn(\"user_locale\", F.coalesce(col(\"user_locale\"), F.lit(\"unknown\")))\n```\n\n## Follow-up Questions\n- How would you roll schema changes from experimental to production with minimal downtime?\n- How would you monitor for schema drift in production and trigger alerts?","diagram":null,"difficulty":"intermediate","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Microsoft","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T23:23:17.885Z","createdAt":"2026-01-12T23:23:17.885Z"},{"id":"q-1144","question":"In a Databricks streaming pipeline ingesting tenant-scoped events from Kafka into Delta Lake, events may arrive late by up to 10 minutes. Describe a concrete end-to-end approach to upsert the latest state per (tenant_id, user_id) into a Silver table while preserving the full event history. Include: data model, CDC logic, watermarking and late data handling, an idempotent MERGE strategy, schema evolution handling, and governance considerations with Unity Catalog RBAC and data lineage?","answer":"Use Bronze‚ÜíSilver CDC pattern with Delta Lake. Silver holds latest state per (tenant_id, user_id); History preserves all changes. Use a 10-minute watermark to bound late data; MERGE INTO Silver WHEN M","explanation":"## Why This Is Asked\nThis tests practical CDC design, late-arrival handling, and Delta Lake upserts in a multi-tenant setting.\n\n## Key Concepts\n- Delta Lake MERGE for upserts\n- Watermarks and late data handling in streaming\n- Bronze-Silver-History data modeling\n- Unity Catalog RBAC and data lineage\n\n## Code Example\n```javascript\n-- Spark SQL (illustrative)\nMERGE INTO silver_latest AS s\nUSING bronze AS b\nON s.tenant_id = b.tenant_id AND s.user_id = b.user_id\nWHEN MATCHED THEN UPDATE SET\n  s.state = b.state,\n  s.last_updated = current_timestamp(),\n  s.version = b.version\nWHEN NOT MATCHED THEN INSERT (tenant_id, user_id, state, last_updated, version)\nVALUES (b.tenant_id, b.user_id, b.state, current_timestamp(), b.version);\n```\n\n## Follow-up Questions\n- How would you validate late-data handling under bursty traffic?\n- How would you monitor and alert on data-skew and drift across tenants?","diagram":null,"difficulty":"advanced","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Cloudflare","Databricks"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T01:30:54.604Z","createdAt":"2026-01-13T01:30:54.604Z"},{"id":"q-1182","question":"In a multi-tenant Databricks lakehouse ingesting per-tenant telemetry from Kafka and S3 into a unified Silver Delta table, describe an end-to-end CDC strategy to implement SCD2-like history per tenant with idempotent MERGE, handle late data with a watermark, and enforce governance through Unity Catalog RBAC and dynamic PII masking. Include data model, CDC logic, testing plan, and how you‚Äôd validate lineage?","answer":"Design a CDC-driven SCD2 history per tenant in a Delta table. Ingest via Delta Live Tables into a Silver table partitioned by tenant_id and timestamp. On MERGE, close previous history rows by setting ","explanation":"## Why This Is Asked\n\nTests the ability to design a CDC-based SCD2 history in a multi-tenant lakehouse with governance and masking requirements, using Delta Live Tables and MERGE semantics.\n\n## Key Concepts\n\n- CDC and SCD2 patterns in Delta Lake\n- Delta Live Tables and MERGE-based upserts\n- Watermarking and late-data handling\n- Unity Catalog RBAC and dynamic data masking\n- Data lineage and governance with Delta sharing\n\n## Code Example\n\n```sql\nMERGE INTO silver AS tgt\nUSING staging AS src\nON tgt.tenant_id = src.tenant_id AND tgt.key = src.key\nWHEN MATCHED AND (src.attributes_hash <> tgt.attributes_hash OR src.timestamp <> tgt.end_date) THEN\n  UPDATE SET end_date = current_timestamp()\nWHEN NOT MATCHED THEN\n  INSERT (tenant_id, key, attributes_hash, start_date, end_date)\n  VALUES (src.tenant_id, src.key, src.attributes_hash, current_timestamp(), NULL)\n```\n\n## Follow-up Questions\n\n- How would you test idempotency of MERGE under replayed CDC events?\n- How would you implement per-tenant masking policies in Unity Catalog and verify lineage against dashboards?","diagram":null,"difficulty":"advanced","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Google","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T03:43:29.150Z","createdAt":"2026-01-13T03:43:29.150Z"},{"id":"q-1226","question":"In a Databricks Delta Live Tables pipeline that ingests clickstream events from a cloud bucket into a Delta table, data quality checks are defined via expectations. A batch arrives with corrupted event_time values that would fail the run. Outline a concrete approach to quarantine bad data, feed downstream tables only from valid rows, and store invalids for audit, while still handling late data and deduplication?","answer":"Create a staging raw_events table, apply a light validation to tag rows as valid/invalid, store invalid rows in invalid_events, and feed downstream clean_events from valid rows only. Use a watermark o","explanation":"## Why This Is Asked\nDemonstrates practical, production-ready handling of partial data quality failures in a Delta Live Tables pipeline, balancing correctness with availability.\n\n## Key Concepts\n- Staging vs downstream separation\n- Expectations-based quality checks\n- Quarantine and audit of invalid data\n- Late data handling with watermarking\n- Deduplication strategies across micro-batches\n\n## Code Example\n```javascript\nimport dlt\nimport pyspark.sql.functions as F\n\n@dlt.table\ndef raw_events():\n  return spark.read.format(\"cloudFiles\").option(\"cloudFiles.format\",\"json\").load(\"/mnt/events/raw\")\n\n@dlt.table\ndef valid_events():\n  df = dlt.read(\"raw_events\")\n  return df.filter(F.col(\"event_time\").isNotNull() & F.col(\"user_id\").isNotNull())\n\n@dlt.table\ndef invalid_events():\n  raw = dlt.read(\"raw_events\")\n  valid_ids = dlt.read(\"valid_events\").select(\"event_id\").distinct()\n  return raw.join(valid_ids, \"event_id\", \"left_anti\")\n\n@dlt.table\ndef clean_events():\n  return dlt.read(\"valid_events\")\n```\n\n## Follow-up Questions\n- How would you monitor and alert if invalid_events growth spikes?\n- What changes would you make to support reprocessing of invalid data after fixes?","diagram":"flowchart TD\n  A[Ingest Raw Events] --> B[DLT Validation Tagging]\n  B --> C{Validity}\n  C -->|Valid| D[Feed Clean Events]\n  C -->|Invalid| E[Store Invalid Events]\n  D --> F[Analytics & BI]\n  E --> G[Audit & Reprocessing]\n  H[Late Data via Watermark] --> B","difficulty":"intermediate","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T05:38:03.397Z","createdAt":"2026-01-13T05:38:03.397Z"},{"id":"q-1264","question":"In a Databricks job ingesting daily Parquet files from S3 into Delta Lake, a new field 'campaign_id' may appear in newer files while older ones do not. Describe a concrete, beginner-friendly approach to ingest without data loss, allowing downstream joins, and handling the schema change gracefully. Include how to enable Delta schema evolution (mergeSchema/autoMerge), define a compatible target schema, and implement a minimal validation to drop rows missing essential fields?","answer":"Enable Delta schema evolution by using mergeSchema on write and enabling auto-merge: spark.conf.set('spark.databricks.delta.schema.autoMerge.enabled','true'); df.write.format('delta').option('mergeSch","explanation":"## Why This Is Asked\n\nTests understanding of practical schema evolution in Delta Lake during basic batch ingestion and how to keep data accessible for joins.\n\n## Key Concepts\n\n- Delta Lake schema evolution\n- mergeSchema and autoMerge\n- Nullability and backward compatibility\n- Data quality validation\n\n## Code Example\n\n```javascript\n// Ingestion example with schema evolution\ndf.write.format(\"delta\").option(\"mergeSchema\",\"true\").mode(\"append\").save(\"/delta/user_events\")\n```\n\n## Follow-up Questions\n\n- How would you validate that campaign_id is correctly populated in downstream queries?\n- How would you backfill the historical table to reflect a new campaigns dimension without downtime?","diagram":null,"difficulty":"beginner","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Coinbase","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T07:31:13.696Z","createdAt":"2026-01-13T07:31:13.696Z"},{"id":"q-1320","question":"You're designing a Delta Live Tables workflow ingesting data from Kafka and an S3 landing zone, with a downstream customer dimension in Delta Lake that uses SCD Type 2. How would you implement idempotent MERGE-based upserts, handle schema drift, and preserve late-arriving data while auditing invalid events and ensuring downstream BI reads only current rows?","answer":"Implement Bronze‚ÜíSilver‚ÜíGold: dedupe by customer_id with max(event_time) in Silver; allowMissingFields for schema drift. In Gold, MERGE into customer_dim: on match with changes, set old row valid_to a","explanation":"## Why This Is Asked\nTests practical mastery of Delta Live Tables patterns: CDC, SCD2, and schema drift handling in production-like streams.\n\n## Key Concepts\n- SCD Type 2 implementation via MERGE\n- Schema drift resilience with safe evolution\n- Late data handling using watermarks and auditing\n- Data lineage and is_current filtering for dashboards\n\n## Code Example\n```sql\nMERGE INTO gold.customer_dim AS t\nUSING silver.customer_stage AS s\nON t.customer_id = s.customer_id AND t.is_current = true\nWHEN MATCHED AND (t.name <> s.name OR t.email <> s.email) THEN\n  UPDATE SET t.valid_to = current_timestamp(), t.is_current = false\nWHEN NOT MATCHED THEN\n  INSERT (customer_id, name, email, valid_from, valid_to, is_current)\n  VALUES (s.customer_id, s.name, s.email, current_timestamp(), NULL, true);\n```\n\n## Follow-up Questions\n- How would you test idempotency across pipeline restarts?\n- How would you validate schema drift without failing runs?\n- How would you structure audits to avoid data loss during failures?","diagram":null,"difficulty":"intermediate","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["NVIDIA","Robinhood","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T11:32:27.379Z","createdAt":"2026-01-13T11:32:27.379Z"},{"id":"q-1375","question":"In a Databricks streaming job, a Kafka topic emits JSON events for many tenants. The payload schema drifts with new fields; you want a stable Silver Delta table with a canonical schema and history. Describe a beginner-friendly approach to map events to the canonical schema, handle new fields without breaking downstream joins, and perform an idempotent MERGE into Silver by (tenant_id, event_id). Include a concrete mapping rule set and a small MERGE example?","answer":"Read Bronze with a permissive schema; map to a canonical Silver schema: tenant_id, event_id, event_ts, and payload (struct). Unknown fields go into payload to avoid drift. Use a MERGE into Silver on (","explanation":"## Why This Is Asked\nTests practical skill in handling schema drift and idempotent upserts in streaming Databricks pipelines.\n\n## Key Concepts\n- Schema drift and canonical schema design\n- Struct payload encoding\n- Idempotent MERGE pattern\n- Streaming boundaries and data quality\n\n## Code Example\n```javascript\nMERGE INTO Silver AS s\nUSING (SELECT tenant_id, event_id, event_ts, payload FROM Bronze) AS b\nON s.tenant_id = b.tenant_id AND s.event_id = b.event_id\nWHEN MATCHED THEN UPDATE SET event_ts = b.event_ts, payload = b.payload\nWHEN NOT MATCHED THEN INSERT (tenant_id, event_id, event_ts, payload) VALUES (b.tenant_id, b.event_id, b.event_ts, b.payload);\n```\n\n## Follow-up Questions\n- How would you test duplicate avoidance and drift handling end-to-end?\n- How would you evolve the canonical schema as new tenant-specific fields appear?","diagram":null,"difficulty":"beginner","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","Oracle","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T14:38:09.108Z","createdAt":"2026-01-13T14:38:09.108Z"},{"id":"q-1409","question":"Describe an end-to-end approach for a fraud-detection streaming pipeline using two Kafka topics (transactions, account_updates): Bronze ingest, join to a versioned SCD2 customer_dim, compute risk in Silver, upsert via MERGE with a deterministic key, watermark late data, handle schema drift with Delta Lake evolution, and enforce governance with Unity Catalog RBAC and lineage. Include testing with synthetic late-arriving data?","answer":"Ingest two Kafka topics into Bronze, join to a versioned SCD2 customer_dim, compute per-event risk in Silver, and upsert into Silver with MERGE on tx_id. Use watermarking for late data, enable Delta L","explanation":"## Why This Is Asked\nAssesses ability to design multi-source streaming with versioned references, robust late-data handling, idempotent upserts, schema evolution, and governance.\n\n## Key Concepts\n- Multi-source streaming from Kafka into Bronze\n- Slowly changing dimension (SCD2) for customer_dim\n- Idempotent MERGE semantics on deterministic keys\n- Watermarking and late data handling\n- Delta Lake schema evolution for drift\n- Unity Catalog RBAC and data lineage\n\n## Code Example\n```javascript\n// Pseudo-sample: MERGE into Silver risk table from Bronze transactions\nspark.sql(`MERGE INTO silver_risk AS s\nUSING bronze_transactions AS b\nON s.tx_id = b.tx_id\nWHEN MATCHED THEN UPDATE SET s.score = b.score, s.updated_at = current_timestamp()\nWHEN NOT MATCHED THEN INSERT (tx_id, score, updated_at) VALUES (b.tx_id, b.score, current_timestamp())`)\n```\n\n## Follow-up Questions\n- How would you test end-to-end with synthetic late-arriving data?\n- What metrics would you monitor to detect backpressure or data skew in this design?","diagram":null,"difficulty":"advanced","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","Scale Ai","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T15:50:17.661Z","createdAt":"2026-01-13T15:50:17.661Z"},{"id":"q-1451","question":"In a Databricks notebook, you need to join a 10M-row Delta Lake 'customers' table with a 10k-row 'segments' reference table to produce a daily marketing audience feed. How would you implement an efficient join strategy in Spark/Delta to minimize shuffle and cost, ensure correctness if segments update, and maintain lineage? Include: join type and hints, caching strategy, refresh cadence, and where to store results?","answer":"Use a broadcast join and caching. Load the 10M-row fact table as df_big and the 10k-row segments as df_small, then join with F.broadcast(df_small) on customer_id. Cache df_big for the daily run. Write","explanation":"## Why This Is Asked\nTests practical Spark optimization and Delta Lake usage for daily pipelines, focusing on join strategy, caching, and governance.\n\n## Key Concepts\n- Broadcast joins and data skew management\n- Caching large DataFrames for repeated reads\n- Delta Lake partitioning for incremental writes\n- Upserts with MERGE and change logging\n- Lineage and RBAC in governance\n\n## Code Example\n```python\nfrom pyspark.sql import functions as F\n\ndf_big = spark.read.format(\"delta\").table(\"db.customers\")\ndf_small = spark.read.format(\"delta\").table(\"db.segments\")\n\ndf_enriched = df_big.join(F.broadcast(df_small), \"customer_id\", \"left\")\ndf_enriched = df_enriched.cache()\n\ndf_enriched.write.format(\"delta\").mode(\"append\").partitionBy(\"order_date\").saveAsTable(\"db.marketing_events_enriched\")\n```\n\n## Follow-up Questions\n- How would you handle frequent updates to the segments table during the day?\n- How would you quantify and reduce shuffle cost in this pipeline?","diagram":"flowchart TD\n  A[Read big delta table] --> B[Read small segments table]\n  B --> C[Broadcast join A and B on customer_id]\n  C --> D[Cache enriched df]\n  D --> E[Write to Delta table partitioned by date]\n  E --> F[Register lineage in Unity Catalog]","difficulty":"beginner","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Google","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T17:46:52.463Z","createdAt":"2026-01-13T17:46:52.463Z"},{"id":"q-1540","question":"In a Databricks streaming pipeline ingesting order events from Kafka into Delta Lake, implement a scalable SCD Type 2 for a customer_dim table to preserve history while handling late-arriving updates up to 15 minutes. Describe the data model (bronze/silver), CDC logic using MERGE, watermarking, and schema evolution, plus Unity Catalog RBAC and lineage considerations. Include a minimal code sketch of the MERGE closing an old row and inserting a new version?","answer":"Design a Delta Live Tables pipeline with a bronze Kafka stream feeding a silver customer_dim SCD2. Implement CDC via a MERGE that closes the active row (set end_date, current=false) and inserts a new row with start_date and current=true. Use watermarking for 15-minute late data handling, enable schema evolution, and implement Unity Catalog RBAC with lineage tracking.","explanation":"## Why This Is Asked\nTests mastery of real-world data governance and slowly changing dimensions in streaming pipelines on Databricks.\n\n## Key Concepts\n- Delta Live Tables, MERGE-based CDC, SCD2, watermarking for late data, schema evolution, Unity Catalog RBAC, data lineage.\n\n## Code Example\n```sql\nMERGE INTO silver.customer_dim AS t\nUSING staging.customer_dim AS s\nON t.customer_id = s.customer_id AND t.current = true\nWHEN MATCHED THEN UPDATE SET t.end_date = CURRENT_TIMESTAMP, t.current = false\nWHEN NOT MATCHED THEN INSERT (customer_id, name, address, start_date, end_date, current)\nVALUES (s.customer_id, s.name, s.address, CURRENT_TIMESTAMP, NULL, true)\n```","diagram":null,"difficulty":"advanced","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Hugging Face"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T06:32:33.205Z","createdAt":"2026-01-13T20:55:12.361Z"},{"id":"q-1555","question":"In a Databricks pipeline ingesting 20 TB/day of Parquet logs on S3 into Delta Lake, design a practical optimization plan to improve read latency for near-real-time dashboards using Delta features like OPTIMIZE, ZORDER, Data Skipping, caching, and Photon. Discuss partitioning strategy, trade-offs, and how you'd validate gains with benchmark metrics?","answer":"Leverage OPTIMIZE with ZORDER on frequently filtered columns (date, user_id), enable Data Skipping, and maintain hot partitions in memory cache. Reevaluate daily versus hourly partitioning to balance compaction overhead against query performance, and validate improvements through comprehensive benchmark metrics.","explanation":"## Why This Is Asked\nThis question evaluates practical optimization strategies for Databricks pipelines handling large-scale Delta Lake workloads, requiring balance between read latency, cost efficiency, and operational maintainability.\n\n## Key Concepts\n- Delta Lake OPTIMIZE and ZORDER for optimal data layout\n- Data skipping and Photon-accelerated query processing\n- Strategic partitioning and intelligent caching\n- Maintenance trade-offs: vacuum operations, compaction, write amplification\n- Performance metrics: query runtime, bytes scanned, cache hit rates, cost per query\n\n## Code Example\n```sql","diagram":null,"difficulty":"intermediate","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","IBM","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T06:25:12.266Z","createdAt":"2026-01-13T21:41:51.915Z"},{"id":"q-1643","question":"In Databricks, ingest streaming data from Kafka into Delta Lake for 10k IoT devices emitting multiple sensor types (temperature, humidity, pressure). Build a Silver table with the latest per-device per-sensor-type state while preserving full history. The source schema will evolve (new sensors added, some removed). Outline a robust end-to-end approach: data model, CDC/Upsert logic, watermarking for late data, schema evolution strategy, idempotent MERGE, and governance with Unity Catalog RBAC and lineage. Include concrete examples of Bronze->Silver handoff and a dynamic per-tenant view?","answer":"Use a Silver table keyed by (device_id, sensor_type) storing the latest value per key, while preserving full history in a separate history mechanism. Upsert Bronze to Silver via MERGE with a 2-minute ","explanation":"## Why This Is Asked\nTests end-to-end thinking: streaming CDC, evolving schemas, and governance in a high-cardinality IoT pipeline.\n\n## Key Concepts\n- Streaming CDC with MERGE into Delta Lake Silver\n- Handling schema evolution (auto-evolve) for new sensors\n- Watermarking and late data management\n- Modeling latest state vs full history\n- Unity Catalog RBAC and lineage for per‚Äëtenant views\n\n## Code Example\n```javascript\n-- Bronze to Silver MERGE (pseudo-SQL)\nMERGE INTO silver_sensor AS s\nUSING batch_view AS b\nON s.device_id = b.device_id AND s.sensor_type = b.sensor_type\nWHEN MATCHED THEN UPDATE SET\n  s.value = b.value,\n  s.ts = b.ts\nWHEN NOT MATCHED THEN INSERT (device_id, sensor_type, value, ts) VALUES (b.device_id, b.sensor_type, b.value, b.ts);\n```\n\n```javascript\n-- Streaming pipeline skeleton with watermark (pseudo-Python)\nbronze = spark.readStream.format('kafka')... \nbronze = bronze.selectExpr('CAST(value AS STRING) as json', 'timestamp')\n# parse and write to Bronze Delta, then MERGE to Silver in micro-batches with a watermark\n```\n\n## Follow-up Questions\n- How would you validate correctness under high late-arrival rates and schema drift?\n- How would you implement per-tenant data access using dynamic views and RBAC without leaking data across tenants?","diagram":null,"difficulty":"advanced","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T04:29:04.600Z","createdAt":"2026-01-14T04:29:04.601Z"},{"id":"q-1658","question":"Design a secure external data sharing workflow in a Databricks environment using Unity Catalog and Delta Sharing to expose aggregated metrics derived from production Delta tables to external partners while maintaining tenant isolation and governance. Include data model, masking strategy, per partner quotas, refresh cadence, and how to monitor and revoke access?","answer":"Use a Delta Sharing producer dataset based on a Silver view (tenant_id, metric, value) with masking on sensitive fields. Publish as a Delta Share to a consumer role with read access only. Enforce per ","explanation":"## Why This Is Asked\nThis tests Delta Sharing, Unity Catalog governance, masking, quotas, and revocation in real-world partner data sharing.\n\n## Key Concepts\n- Delta Sharing producer/consumer model\n- Unity Catalog RBAC and quotas\n- Dynamic data masking policies\n- Data freshness and incremental refresh\n- Audit logs and compliance\n\n## Code Example\n```sql\n-- Create share and grant access to partner\nCREATE SHARE partner_agg_share;\nALTER SHARE partner_agg_share ADD CONSUMER 'partner_catalog' WITH 'us-east-1';\nGRANT SELECT ON database.shared_view TO SHARE partner_agg_share;\n```\n\n## Follow-up Questions\n- How would you handle schema drift in the shared view without breaking consumers?\n- What monitoring metrics indicate misuse or quota breaches?","diagram":null,"difficulty":"advanced","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Oracle","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T05:41:50.396Z","createdAt":"2026-01-14T05:41:50.397Z"},{"id":"q-1683","question":"Design a multi-tenant, Databricks-based data pipeline ingesting 1 TB/day of JSON events from Kafka into Delta Lake. Tenants share storage but must be completely isolated; dashboards must mask PII fields per-tenant. Propose an end-to-end pattern using Unity Catalog RBAC, dynamic data masking, Delta Live Tables, and Photon-enabled reads. Include data model, masking rules, handling schema evolution, and how you validate governance and lineage?","answer":"Three-layer lakehouse: Bronze (Kafka ‚Üí Delta), Silver (per-tenant masking via Unity Catalog dynamic masking with tenant scoping), Gold (dashboards + features). Enforce RBAC, use DLT with schema evolut","explanation":"Why This Is Asked\n- Tests multi-tenant governance, masking, and lineage in a real Databricks setup. \n- Evaluates practical use of Unity Catalog RBAC, dynamic masking, and DLT for evolving schemas. \n- Probes performance considerations with Photon and partitioning strategies.\n\nKey Concepts\n- Unity Catalog RBAC and access controls\n- Dynamic data masking for PII per tenant\n- Delta Live Tables for reliable, incremental transforms\n- Photon-accelerated reads and Delta caching\n- Schema evolution handling and time travel for audits\n- Data lineage and governance validation\n\nCode Example\n```sql\n-- Pseudo example: define a masking policy and apply to Silver table\nCREATE MASKING POLICY tenant_masking AS (tenant_id STRING) RETURNS STRING ->\n  CASE WHEN CURRENT_USER() = tenant_id THEN tenant_id ELSE 'REDACTED' END;\nALTER TABLE silver APPLY MASKING tenant_masking ON (tenant_id);\n```\n\nFollow-up Questions\n- How would you test for cross-tenant data leakage during schema evolution?\n- Which metrics and dashboards would you instrument to monitor governance and lineage health across tenants?","diagram":null,"difficulty":"advanced","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","Netflix","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T06:55:17.136Z","createdAt":"2026-01-14T06:55:17.136Z"},{"id":"q-1721","question":"Databricks beginner scenario: A streaming pipeline reads 2 TB/month of JSON events from S3 via Autoloader into Delta Lake. A downstream dashboard shows active_users by hour. Late events arrive up to 10 minutes. Design a practical plan to maintain accurate hourly active_user counts with minimal duplication, including: schema/partitioning for streaming, watermark-based late data handling, an idempotent MERGE into a Silver table, and a simple end-to-end test using synthetic late events. Also outline monitoring steps?","answer":"Implement an Autoloader Bronze layer, upsert hourly aggregates into Silver with a MERGE on (user_id, hour). Use a 10-minute watermark for late data; partition Silver by hour; ensure idempotence with a","explanation":"## Why This Is Asked\n\nTests practical use of streaming with late data, Delta MERGE upserts, and end-to-end validation in a beginner-friendly context.\n\n## Key Concepts\n\n- Autoloader and Delta Lake Bronze-Silver pattern\n- Watermarks and late data handling in structured streaming\n- Idempotent MERGE-based upserts for counts\n- End-to-end testing with synthetic late events\n- Monitoring latency and data quality\n\n## Code Example\n\n```sql\n-- MERGE into Silver hourly active_users\nMERGE INTO silver.active_users AS s\nUSING (\n  SELECT user_id,\n         date_trunc('hour', event_time) AS hour,\n         COUNT(*) AS cnt\n  FROM bronze.events\n  GROUP BY user_id, date_trunc('hour', event_time)\n) AS b\nON s.user_id = b.user_id AND s.hour = b.hour\nWHEN MATCHED THEN UPDATE SET s.count = b.cnt\nWHEN NOT MATCHED THEN INSERT (user_id, hour, count) VALUES (b.user_id, b.hour, b.cnt)\n```\n\n## Follow-up Questions\n\n- How would you adjust for bursty late data or skewed user activity?\n- What metrics would you monitor to ensure SLA compliance for dashboards?","diagram":"flowchart TD\n  S[Source: S3 Autoloader] --> B[Bronze Layer]\n  B --> S2[Silver Layer with MERGE]\n  S2 --> D[Dashboards]\n  D --> M[Monitoring + Alerts]","difficulty":"beginner","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Instacart","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T07:55:55.291Z","createdAt":"2026-01-14T07:55:55.291Z"},{"id":"q-1844","question":"In a Databricks streaming pipeline ingesting events from Kafka into Delta Lake for a 50+ TB/day workload, design a CDC-based upsert to a Silver table with per-tenant sharding, late data handling, and schema evolution. Describe the data model, idempotent MERGE keys, watermark latency model, and Unity Catalog RBAC considerations; propose validation metrics and a minimal reproducible code skeleton?","answer":"Use a two-table CDC pattern: Bronze stream ingests events from Kafka, writes (tenant_id, event_id, ts, payload). Silver MERGEs into a partitioned table by tenant_id/date, upserting on (tenant_id, even","explanation":"## Why This Is Asked\nTests CDC mastery, multi-tenant partitioning, late-data handling, schema evolution, and governance in a large Databricks setup.\n\n## Key Concepts\n- CDC MERGE across Bronze to Silver\n- Per-tenant partitioning and idempotent keys\n- Watermarking and late-arrival handling\n- Schema evolution in Delta Lake\n- Unity Catalog RBAC and lineage tracking\n\n## Code Example\n```sql\nMERGE INTO Silver s\nUSING staged_b e b\nON (s.tenant_id = b.tenant_id AND s.event_id = b.event_id)\nWHEN MATCHED THEN UPDATE SET s.value = b.value, s.ts = b.ts\nWHEN NOT MATCHED THEN INSERT (tenant_id, event_id, value, ts) VALUES (b.tenant_id, b.event_id, b.value, b.ts);\n```\n\n## Follow-up Questions\n- How would you validate idempotence with late data?\n- How to evolve schema while preserving history and lineage?","diagram":null,"difficulty":"advanced","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Instacart","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T13:29:00.353Z","createdAt":"2026-01-14T13:29:00.353Z"},{"id":"q-1853","question":"In a Databricks job ingesting 50 GB/day of Avro logs from S3 into Delta Lake, design a beginner-friendly Delta Live Tables pipeline to produce Bronze (raw) and Silver (flattened) tables. Include how you flatten the nested field user (id and tier) into Silver, enforce a simple data quality gate (NOT NULL user_id, user_tier in {'free','standard','premium'}), and choose a partitioning strategy by file_date. Explain testing and how you'll measure improvements?","answer":"Use a Delta Live Tables pipeline with Bronze (raw Avro from S3) and Silver (flattened). Bronze loads from s3://bucket/logs/daily/, Silver computes user_id = user.id and user_tier = user.tier, plus ess","explanation":"## Why This Is Asked\nThis tests practical DLT usage, Bronze/Silver modeling, nested field flattening, basic data quality gates, and partitioning in a real-ish beginner scenario.\n\n## Key Concepts\n- Delta Live Tables (DLT)\n- Bronze/Silver data modeling\n- Flattening nested structs in Spark\n- Simple data quality gates\n- Partitioning strategy and schema evolution\n\n## Code Example\n```javascript\nimport dlt\n@dlt.table\ndef bronze_logs():\n  return spark.read.format(\"avro\").load(\"s3://bucket/logs/daily/\")\n\n@dlt.table\ndef silver_logs():\n  b = dlt.read(\"bronze_logs\")\n  s = b.selectExpr(\"user.id as user_id\",\"user.tier as user_tier\", \"*\")\n  return s.filter(\"user_id IS NOT NULL AND user_tier IN ('free','standard','premium')\")\n```\n\n## Follow-up Questions\n- How would you test drift between Bronze and Silver? \n- How would you handle an unseen user_tier value (e.g., 'enterprise')?\n","diagram":null,"difficulty":"beginner","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Robinhood","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T14:39:02.124Z","createdAt":"2026-01-14T14:39:02.124Z"},{"id":"q-1944","question":"Design a Delta Live Tables pipeline that streams per-tenant user activity from Kafka into Bronze, then updates a per-tenant Silver table implementing SCD Type 2 on a tenant-scoped customer_dim, using a deterministic MERGE for upserts. Tenants can emit out-of-order data and schema drift occurs. Propose concrete config for watermarking, per-tenant constraints, and schema evolution, plus RBAC in Unity Catalog. Include synthetic late-data tests and metrics to validate latency and correctness?","answer":"Propose using DLT with per-tenant watermarks, deterministic MERGE keys, and row-level constraints. Bronze ingest from Kafka, then Silver with per-tenant SCD2 and a deterministic MERGE into tenant-scop","explanation":"## Why This Is Asked\n\nAssesses practical mastery of per-tenant data pipelines, late data handling, schema evolution, and governance in Databricks DLTs. Requires concrete trade-offs, deterministic keys, and test strategies beyond generic concepts.\n\n## Key Concepts\n\n- Delta Live Tables and MERGE upserts\n- Per-tenant governance with Unity Catalog RBAC\n- Watermarking and late data handling in streaming\n- SCD Type 2 on Delta Lake\n- Schema evolution and data quality constraints\n- Synthetic testing for latency and correctness\n\n## Code Example\n\n```javascript\n// Pseudo-skeleton illustrating per-tenant pipeline steps\nsetup() {\n  // configure per-tenant watermarking\n  // define Bronze from Kafka\n  // define Silver with SCD2 on tenant_id\n  // upsert with deterministic keys into tenant-scoped table\n  // enable schema evolution\n  // apply RBAC\n}\n```\n\n## Follow-up Questions\n\n- How would you validate idempotency under restart scenarios?\n- What metrics would you surface in dashboards to monitor per-tenant data quality?","diagram":null,"difficulty":"intermediate","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Citadel","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T17:57:46.590Z","createdAt":"2026-01-14T17:57:46.591Z"},{"id":"q-1990","question":"Beginner: Adobe wants a minimal, end-to-end Databricks pipeline to ingest daily JSON event logs from S3 into Delta Lake. Design the workflow using Auto Loader for a Bronze table, then flatten to a date-partitioned Silver table. Include simple schema-evolution handling, a lightweight data-quality check (required fields, duplicates), and a basic unit test that validates the Silver schema and daily row count?","answer":"Use Auto Loader to land JSON into Bronze Delta, flatten into Silver with a date partition, enable mergeSchema for drift, and apply a lightweight quality guard for non-null fields and a unique key. Imp","explanation":"## Why This Is Asked\nThis question tests a beginner-friendly, end-to-end Databricks pipeline design, focusing on Bronze/Silver layering and basic quality checks rather than complex streaming or governance. It reveals understanding of ingestion, schema drift, partitioning, and simple test strategies.\n\n## Key Concepts\n- Auto Loader ingestion\n- Delta Lake Bronze/Silver layers\n- Schema evolution with mergeSchema\n- Lightweight data quality checks\n- Unit tests for schema and row counts\n\n## Code Example\n```javascript\n# Pseudo-Spark-like illustration\nbronze = spark.read.format('delta').load('s3://bucket/bronze')\nsilver = bronze.selectExpr('*', 'CAST(event_time AS DATE) as date')\nsilver.write.format('delta').mode('append').partitionBy('date').save('s3://bucket/silver')\n```\n\n## Follow-up Questions\n- How would you test nested JSON changes?\n- How would you run these tests in a CI/CD pipeline?","diagram":null,"difficulty":"beginner","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T19:44:43.880Z","createdAt":"2026-01-14T19:44:43.880Z"},{"id":"q-2033","question":"In a Databricks workflow, ingest 50 GB/day of JSON web logs from S3 into a Bronze Delta table. Propose a beginner-friendly pipeline to populate a Silver table that upserts the latest event per session_id, handles 2-minute late data with a watermark, and validates data quality before write. Include partitioning strategy, a MERGE-based CDC, and Unity Catalog RBAC considerations?","answer":"Design a pipeline using Autoloader to land JSON into Bronze, then upsert into Silver with MERGE on session_id using the latest timestamp. Apply a 2-minute watermark for late data, partition Silver by date for query performance, implement data quality validation rules before writing, and establish Unity Catalog RBAC with appropriate privileges for Bronze/Silver access.","explanation":"Why This Is Asked\n- Tests practical Delta Lake upsert and late data handling with realistic data volume\n- Evaluates ability to translate business rules into robust, auditable pipelines\n- Assesses understanding of medallion architecture and enterprise data governance\n\nKey Concepts\n- Autoloader ingestion, Bronze to Silver MERGE CDC, watermarking for late data\n- Partitioning for query efficiency, schema quality gates, Unity Catalog RBAC\n- Practical validation without overengineering\n\nCode Example\n```javascript\n// Illustrative MERGE logic (conceptual)\nMERGE INTO silver_sessions AS s\nUSING (\n  SELECT session_id, MAX(ts) AS ts, MAX(event_type) AS event_type\n  FROM bronze_events\n  WHERE ts >= current_timestamp() - INTERVAL 2 MINUTES\n  GROUP BY session_id\n) AS b\nON s.session_id = b.session_id\nWHEN MATCHED THEN UPDATE SET *\nWHEN NOT MATCHED THEN INSERT *\n```","diagram":null,"difficulty":"beginner","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Bloomberg","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T06:16:38.809Z","createdAt":"2026-01-14T21:39:33.380Z"},{"id":"q-2070","question":"Design an end-to-end Databricks pipeline to ingest a daily JSON feed from an on-prem FTP drop into Delta Lake. Use Auto Loader with schema evolution for changing fields (user_id, event_type, event_ts, ip_address, device, etc.). Describe Bronze to Silver upserts via MERGE, data quality tests, IP masking, and Unity Catalog RBAC with lineage. Be concrete about steps and trade-offs?","answer":"Design an end-to-end Databricks pipeline to ingest a daily JSON feed from an on-prem FTP drop into Delta Lake. Use Auto Loader with schema evolution for changing fields (user_id, event_type, event_ts, ip_address, device, etc.). Describe Bronze to Silver upserts via MERGE, data quality tests, IP masking, and Unity Catalog RBAC with lineage. Be concrete about steps and trade-offs.\n\n**Architecture Overview:**\n1. **Bronze Layer**: Raw ingestion using Auto Loader with cloudFiles format\n2. **Silver Layer**: Cleaned, deduplicated data with PII masking\n3. **Gold Layer**: Business-ready aggregated tables\n\n**Step-by-Step Implementation:**\n\n**1. Bronze Layer - Auto Loader Ingestion**\n```python\n# Bronze streaming job\nbronze_df = (spark.readStream\n    .format(\"cloudFiles\")\n    .option(\"cloudFiles.format\", \"json\")\n    .option(\"cloudFiles.schemaLocation\", \"/schemas/bronze_events\")\n    .option(\"mergeSchema\", \"true\")\n    .load(\"/mnt/ftp/daily_drop/\"))\n\n# Write to Bronze Delta table\n(bronze_df.writeStream\n    .format(\"delta\")\n    .option(\"checkpointLocation\", \"/checkpoints/bronze_events\")\n    .table(\"bronze_events\"))\n```\n\n**2. Silver Layer - MERGE with Data Quality**\n```sql\n-- Silver upsert with data quality checks\nMERGE INTO silver_events AS target\nUSING (\n    SELECT *,\n        CASE \n            WHEN user_id IS NULL OR user_id = '' THEN 'INVALID_USER_ID'\n            WHEN event_ts IS NULL THEN 'INVALID_TIMESTAMP'\n            WHEN NOT is_valid_ip(ip_address) THEN 'INVALID_IP'\n        END AS quality_flag,\n        mask_ip(ip_address) AS masked_ip\n    FROM bronze_events\n    WHERE ingestion_date >= CURRENT_DATE - 1\n) AS source\nON target.user_id = source.user_id \n   AND target.event_ts = source.event_ts\nWHEN MATCHED AND source.quality_flag IS NULL THEN\n    UPDATE SET *\nWHEN NOT MATCHED AND source.quality_flag IS NULL THEN\n    INSERT *\nWHEN MATCHED AND source.quality_flag IS NOT NULL THEN\n    DELETE;\n```\n\n**3. Data Quality & PII Functions**\n```python\nfrom pyspark.sql.functions import udf, regexp_replace\nfrom pyspark.sql.types import BooleanType, StringType\n\n@udf(BooleanType())\ndef is_valid_ip(ip):\n    import ipaddress\n    try:\n        ipaddress.ip_address(ip)\n        return True\n    except:\n        return False\n\n@udf(StringType())\ndef mask_ip(ip):\n    parts = ip.split('.')\n    return f\"{parts[0]}.{parts[1]}.***.***\"\n```\n\n**4. Unity Catalog RBAC & Lineage**\n```sql\n-- Create catalog and set permissions\nCREATE CATALOG IF NOT EXISTS event_catalog;\n\n-- Grant roles\nCREATE ROLE IF NOT EXISTS data_engineers;\nCREATE ROLE IF NOT EXISTS data_analysts;\nCREATE ROLE IF NOT EXISTS data_scientists;\n\n-- Set permissions\nGRANT USE CATALOG ON CATALOG event_catalog TO ROLE data_engineers;\nGRANT USE SCHEMA ON SCHEMA event_catalog.bronze TO ROLE data_engineers;\nGRANT SELECT ON TABLE event_catalog.bronze.bronze_events TO ROLE data_analysts;\n\n-- Add lineage tags\nALTER TABLE event_catalog.silver.silver_events \nSET TBLPROPERTIES (\n    'lineage.source' = 'bronze_events',\n    'lineage.transformation' = 'MERGE_with_data_quality',\n    'pii.masked_fields' = 'ip_address'\n);\n```\n\n**Trade-offs & Considerations:**\n\n**Schema Evolution Trade-offs:**\n- ‚úÖ Handles new fields automatically\n- ‚ùå May introduce data type conflicts requiring manual intervention\n- üí° Monitor schema changes with Delta Lake schema history\n\n**MERGE Performance:**\n- ‚úÖ Ensures idempotent processing\n- ‚ùå Higher compute cost vs simple append\n- üí° Optimize with Z-ordering on join keys (user_id, event_ts)\n\n**Data Quality Approach:**\n- ‚úÖ Prevents bad data propagation\n- ‚ùå May reject valid edge cases\n- üí° Implement quarantine table for rejected records\n\n**IP Masking Strategy:**\n- ‚úÖ Complies with privacy regulations\n- ‚ùå Loses analytical value of geolocation\n- üí° Store hash of original IP in secure vault for audit","explanation":"This question tests practical ingestion, drift handling, and governance in Databricks. Candidates must demonstrate proficiency with Auto Loader for incremental JSON ingestion, schema evolution techniques for handling evolving data structures, MERGE-based upserts for maintaining data consistency while preserving history, basic data quality validation and PII masking, and Unity Catalog RBAC with data lineage tracking.\n\n**Key Assessment Areas:**\n- **Auto Loader Implementation**: Understanding cloudFiles format, schema location management, and mergeSchema configuration\n- **Schema Evolution Handling**: Knowledge of how Delta Lake manages schema changes and potential conflicts\n- **MERGE Operations**: Ability to write efficient upsert logic with proper join conditions and handling of duplicates\n- **Data Quality Framework**: Implementation of validation rules and error handling strategies\n- **PII Protection**: Understanding of masking techniques and privacy compliance requirements\n- **Unity Catalog Governance**: Role-based access control, permission management, and lineage tracking\n\n**What Makes a Strong Answer:**\n- Concrete code examples showing practical implementation\n- Discussion of performance trade-offs and optimization strategies\n- Understanding of production considerations like error handling and monitoring\n- Knowledge of security and compliance best practices\n- Ability to explain the 'why' behind architectural decisions","diagram":null,"difficulty":"beginner","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T05:33:41.927Z","createdAt":"2026-01-14T22:55:34.382Z"},{"id":"q-2199","question":"How would you implement a compliant, zero-duplication deletion workflow in a Databricks streaming pipeline that ingests user events from Kafka into Delta Lake? A deletions log triggers row-level removals across Bronze/Silver, handling late data with tombstones, ensuring idempotent MERGE, and preserving an auditable deletion history via a separate Delta table. Describe architecture, data flow, and validation?","answer":"Implement a deletion workflow with a DeletionLog stream; when a deletion request arrives, publish user_id and deletion_ts to a Delta table and a separate DeletionLog; on Bronze and Silver, use MERGE t","explanation":"## Why This Is Asked\n\nTests the ability to design compliant, end-to-end data deletion in a streaming Delta Lake pipeline. It covers change data capture semantics, idempotency, auditability, and cross-layer consistency (Bronze to Silver), which are critical at scale.\n\n## Key Concepts\n\n- Delta Lake MERGE for row-level deletions\n- Deletion log and tombstone handling\n- Idempotent operations and deleted_at auditing\n- Data lineage, governance, and RBAC in Unity Catalog\n\n## Code Example\n\n```javascript\n// Pseudo-Delta MERGE for deletion\nconst stmt = `\nMERGE INTO BronzeDelta AS b\nUSING DeletionLog AS d\nON b.user_id = d.user_id\nWHEN MATCHED THEN DELETE\n`;\ndb.run(stmt);\n```\n\n## Follow-up Questions\n\n- How would you test end-to-end deletion latency and ensure no residual copies in backups? \n- How would you scale the deletion workflow when deletions are frequent and from multiple sources?","diagram":"flowchart TD\n  A[Kafka Stream] --> B[Bronze Delta]\n  B --> C[Silver Delta]\n  D[Deletion Log] --> C\n  C --> E[Audit Table]","difficulty":"advanced","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Databricks","Goldman Sachs"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T07:06:54.769Z","createdAt":"2026-01-15T07:06:54.769Z"},{"id":"q-2279","question":"In a Databricks job ingesting 10 GB/day of user event JSON from S3 into Bronze Delta Lake, design a beginner-friendly Silver pipeline that upserts latest user state by user_id using MERGE, enforces Delta constraints (NOT NULL user_id, age > 0), and fails the pipeline on any quality violation. How would you implement and test this end-to-end?","answer":"Use Autoloader to ingest JSON into Bronze Delta Lake, then Silver via MERGE on user_id to upsert latest state. Enforce Delta constraints: NOT NULL user_id, age > 0, and non-empty payload. Add a Spark ","explanation":"## Why This Is Asked\nThis question probes practical data ingestion, safe upserts, and basic data quality controls in Databricks for a real-world, beginner-friendly scenario.\n\n## Key Concepts\n- Delta Lake constraints (NOT NULL, CHECK-like rules)\n- MERGE upserts for incremental state updates\n- Auto Loader for resilient, schema-evolving ingestion\n- Simple data quality gates that fail jobs on violations\n\n## Code Example\n```sql\n-- MERGE example (pseudo for illustration)\nMERGE INTO silver_table AS s\nUSING bronze_table AS b\nON s.user_id = b.user_id\nWHEN MATCHED THEN UPDATE SET *\nWHEN NOT MATCHED THEN INSERT (*)\n```\n\n```python\n# Pseudo Spark check (counts violations)\nviolations = bronze_df.filter(col('user_id').isNull() | (col('age') <= 0) | (col('payload').eqNullSafe(''))).count()\nif violations > 0:\n  raise Exception(f\"Data quality violation count: {violations}\")\n```\n\n## Follow-up Questions\n- How would you handle late-arriving data or schema evolution?\n- How would you test quality gates at scale across partitions?","diagram":null,"difficulty":"beginner","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Netflix","Salesforce","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":["databricks job","bronze delta lake","silver pipeline","auto loader","merge statement","delta constraints","not null","data quality","user state","end-to-end testing","quality violation"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2026-01-22T05:04:11.108Z","createdAt":"2026-01-15T10:39:53.066Z"},{"id":"q-2397","question":"In a Databricks data pipeline, ingest telemetry from two sources: S3 JSONs (Auto Loader) and a Kafka topic, with evolving schema (device_id, ts, metrics...). Build Bronze, Silver (dedupe by event_id using MERGE CDC), Gold. Implement 2-minute late data watermark, PII masking, and Unity Catalog RBAC with lineage across Bronze/Silver/Gold. Provide concrete steps, partitioning, and a lightweight test plan?","answer":"Ingest via Auto Loader and structured streaming, Bronze stores raw JSON, Silver uses MERGE on event_id and upserts latest per event, a 2-minute watermark for late data, Gold computes hourly device agg","explanation":"## Why This Is Asked\nThis question probes end-to-end data engineering with multi-source ingestion, schema evolution, upsert-based CDC, governance, and testing in Databricks.\n\n## Key Concepts\n- Multi-source streaming ingestion (S3 Auto Loader + Kafka)\n- Delta Lake Bronze/Silver/Gold with MERGE CDC\n- Watermarks for late data\n- PII masking and data masking strategies\n- Unity Catalog RBAC and lineage across layers\n- Lightweight data quality tests\n\n## Code Example\n```sql\nMERGE INTO silver.telemetry AS t\nUSING bronze.raw AS s\nON t.event_id = s.event_id\nWHEN MATCHED THEN UPDATE SET t.* = s.*\nWHEN NOT MATCHED THEN INSERT *;\n```\n\n## Follow-up Questions\n- How would you validate watermark correctness and handle out-of-order events in production?\n- How would you extend RBAC to allow per-device-group access without leaking sensitive fields?","diagram":"flowchart TD\n  A(Source: S3 & Kafka) --> B(Bronze: Raw JSON)\n  B --> C(Silver: Deduped + Masked)\n  C --> D(Gold: Hourly Aggregates)\n  C --> E(Lineage & RBAC)","difficulty":"intermediate","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Goldman Sachs","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T16:46:28.084Z","createdAt":"2026-01-15T16:46:28.084Z"},{"id":"q-2435","question":"Design a Databricks pipeline ingesting telemetry from S3 (Auto Loader) and a Kafka topic with evolving schema. Build Bronze/Silver/Gold with CDC dedupe by event_id; add a drift-detection layer that scores per-field distribution changes and quarantines anomalous records. Mask PII before Silver/Gold, enforce Unity Catalog RBAC with lineage. Provide concrete steps, partitioning, and a lightweight test plan?","answer":"Design uses Auto Loader for S3 and Kafka in Bronze, Silver performs CDC MERGE by event_id, Gold is the consumable layer. Add a drift-detection pass that scores per-field distribution changes and quara","explanation":"## Why This Is Asked\nTests multi-source ingestion with evolving schemas, real-time anomaly handling, and governance at scale. Probes drift scoring, quarantine routing, late-data handling, PII masking, and Unity Catalog RBAC with lineage across layers.\n\n## Key Concepts\n- Multi-source ingestion (S3 Auto Loader, Kafka)\n- CDC dedupe by event_id\n- Delta schema evolution and drift scoring\n- Two-minute watermark for late data\n- PII masking strategies\n- Unity Catalog RBAC and lineage\n- Quarantine routing and tests\n\n## Code Example\n```javascript\n// drift-score helper (illustrative)\nfunction driftScore(oldRecord, newRecord, fields){\n  let s = 0\n  fields.forEach(f => {\n    if (typeof oldRecord[f] !== typeof newRecord[f]) s += 1\n  })\n  return s\n}\n```\n\n## Follow-up Questions\n- How to tune drift thresholds and adapt to schema evolution?\n- How would you validate quarantine routing and lineage exports across environments?","diagram":"flowchart TD\n  Bronze[Bronze Layer: S3 + Kafka] --> Silver[Silver Layer: CDC MERGE by event_id]\n  Silver --> Gold[Gold Layer: masked, consumable]\n  Silver --> Quarantine[Quarantine Layer: anomalies]\n  Quarantine --> Silver","difficulty":"advanced","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Microsoft","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T17:53:16.495Z","createdAt":"2026-01-15T17:53:16.495Z"},{"id":"q-2577","question":"In a Databricks Delta Live Tables (DLT) pipeline, ingest telemetry from two sources: S3 Auto Loader JSON with evolving schema and a Kafka stream. Implement multi-source dedup by a composite key (device_id, event_id), support up to 5-minute late data with a watermark, mask PII fields, and publish lineage via Unity Catalog RBAC across Bronze Silver Gold. Provide concrete steps, test plan, and rollback path?","answer":"Design a comprehensive Delta Live Tables pipeline with multi-source ingestion, composite-key deduplication, late data handling, PII masking, and Unity Catalog governance. The pipeline ingests from S3 Auto Loader (JSON with evolving schema) and Kafka streams, applies deduplication using device_id and event_id, supports 5-minute late data with watermarking, masks PII fields, and maintains lineage across Bronze-Silver-Gold layers with Unity Catalog RBAC.","explanation":"## Why This Is Asked\nThis question evaluates end-to-end data engineering capabilities including multi-source ingestion patterns, deduplication strategies, late data handling, data privacy compliance, and governance implementation in a comprehensive DLT workflow.\n\n## Key Concepts\n- Delta Live Tables multi-source ingestion patterns\n- Composite-key deduplication with ExpectValidFrom and DropDuplicates\n- Watermarking for late data tolerance (5-minute window)\n- Field-level PII masking using DLT apply_changes\n- Unity Catalog RBAC and lineage management\n- Bronze-Silver-Gold medallion architecture\n\n## Code Example\n```python\n# Bronze Layer - Multi-source ingestion\n@dlt.table(\n    comment=\"Raw telemetry from S3 and Kafka sources\",\n    table_properties={\n        \"delta.autoOptimize.optimizeWrite\": \"true\",\n        \"delta.autoOptimize.autoCompact\": \"true\"\n    }\n)\ndef bronze_telemetry_s3() -> DataFrame:\n    return (\n        spark.readStream.format(\"cloudFiles\")\n        .option(\"cloudFiles.format\", \"json\")\n        .option(\"cloudFiles.schemaLocation\", \"/tmp/schema/s3_telemetry\")\n        .option(\"cloudFiles.inferColumnTypes\", \"true\")\n        .load(\"/databricks-datasets/telemetry/s3/\")\n        .withColumn(\"source\", lit(\"s3\"))\n        .withColumn(\"ingest_time\", current_timestamp())\n    )\n\n@dlt.table(\n    comment=\"Raw telemetry from Kafka stream\",\n    table_properties={\n        \"delta.autoOptimize.optimizeWrite\": \"true\"\n    }\n)\ndef bronze_telemetry_kafka() -> DataFrame:\n    return (\n        spark.readStream.format(\"kafka\")\n        .option(\"kafka.bootstrap.servers\", \"kafka-broker:9092\")\n        .option(\"subscribe\", \"telemetry-topic\")\n        .option(\"startingOffsets\", \"latest\")\n        .load()\n        .select(from_json(col(\"value\").cast(\"string\"), telemetry_schema).alias(\"data\"))\n        .select(\"data.*\")\n        .withColumn(\"source\", lit(\"kafka\"))\n        .withColumn(\"ingest_time\", current_timestamp())\n    )\n\n# Silver Layer - Deduplication and PII masking\n@dlt.table(\n    comment=\"Deduplicated telemetry with PII masking\",\n    table_properties={\n        \"delta.enableChangeDataFeed\": \"true\",\n        \"delta.isolationLevel\": \"SERIALIZABLE\"\n    }\n)\ndef silver_telemetry_deduped() -> DataFrame:\n    # Combine both sources\n    combined = (\n        dlt.read(\"bronze_telemetry_s3\")\n        .unionByName(dlt.read(\"bronze_telemetry_kafka\"))\n    )\n    \n    # Apply watermark for late data handling\n    watermarked = (\n        combined\n        .withWatermark(\"event_timestamp\", \"5 minutes\")\n    )\n    \n    # Composite key deduplication\n    deduped = (\n        watermarked\n        .dropDuplicates([\"device_id\", \"event_id\"])\n    )\n    \n    # PII masking\n    pii_masked = (\n        deduped\n        .withColumn(\"ip_address\", regexp_replace(col(\"ip_address\"), r\"(\\d+\\.\\d+\\.\\d+)\\.\", r\"\\1.***\"))\n        .withColumn(\"email\", regexp_replace(col(\"email\"), r\"(.{2}).*(@.*)\", r\"\\1***\\2\"))\n        .withColumn(\"phone\", regexp_replace(col(\"phone\"), r\"(\\d{3})\\d{4}(\\d{4})\", r\"\\1****\\2\"))\n    )\n    \n    return pii_masked\n\n# Gold Layer - Aggregated analytics\n@dlt.table(\n    comment=\"Aggregated telemetry metrics\",\n    table_properties={\n        \"delta.optimize.write\": \"true\"\n    }\n)\ndef gold_telemetry_metrics() -> DataFrame:\n    return (\n        dlt.read(\"silver_telemetry_deduped\")\n        .groupBy(\n            window(col(\"event_timestamp\"), \"1 hour\"),\n            col(\"device_type\")\n        )\n        .agg(\n            count(\"event_id\").alias(\"event_count\"),\n            avg(\"cpu_usage\").alias(\"avg_cpu\"),\n            max(\"memory_usage\").alias(\"max_memory\")\n        )\n    )\n\n# Unity Catalog RBAC Configuration\nCREATE CATALOG IF NOT EXISTS telemetry_catalog;\nCREATE SCHEMA IF NOT EXISTS telemetry_catalog.bronze;\nCREATE SCHEMA IF NOT EXISTS telemetry_catalog.silver;\nCREATE SCHEMA IF NOT EXISTS telemetry_catalog.gold;\n\n-- Grant permissions\nGRANT USE CATALOG ON CATALOG telemetry_catalog TO `data_engineers`;\nGRANT USE SCHEMA ON SCHEMA telemetry_catalog.bronze TO `data_engineers`;\nGRANT SELECT ON ALL TABLES IN SCHEMA telemetry_catalog.bronze TO `data_analysts`;\nGRANT MODIFY ON ALL TABLES IN SCHEMA telemetry_catalog.silver TO `data_engineers`;\nGRANT SELECT ON ALL TABLES IN SCHEMA telemetry_catalog.gold TO `business_users`;\n\n-- Lineage setup\nALTER TABLE telemetry_catalog.bronze.bronze_telemetry_s3 SET TBLPROPERTIES (\n  'delta.lineage' = 's3_source->bronze->silver->gold'\n);\n```\n\n## Test Plan\n1. **Unit Tests**: Validate deduplication logic with test datasets containing duplicate device_id/event_id pairs\n2. **Integration Tests**: Test multi-source ingestion with mock S3 and Kafka endpoints\n3. **Late Data Tests**: Inject delayed events (3-7 minutes late) to verify watermark behavior\n4. **PII Masking Tests**: Verify regex patterns correctly mask IP, email, and phone fields\n5. **Performance Tests**: Load test with 1M events/hour to validate pipeline throughput\n6. **RBAC Tests**: Verify Unity Catalog permissions enforce proper access controls\n\n## Rollback Path\n1. **Pipeline Rollback**: Use DLT versioning to revert to previous pipeline version\n2. **Data Recovery**: Restore from Delta Lake time travel using `VERSION AS OF` syntax\n3. **Schema Rollback**: Maintain schema evolution history in Unity Catalog\n4. **Configuration Rollback**: Store pipeline configs in Git with tagged releases\n5. **Monitoring**: Set up alerts for data quality metrics degradation","diagram":"flowchart TD\n  A[S3 Auto Loader JSON] --> Bronze\n  B[Kafka Stream] --> Bronze\n  Bronze --> Silver\n  Silver --> Gold","difficulty":"advanced","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","NVIDIA","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":["delta live tables","multi-source ingestion","composite key deduplication","late data handling","watermarking strategy","pii field masking","unity catalog rbac","medallion architecture","s3 auto loader","kafka stream processing","data lineage tracking","evolving schema support"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2026-01-19T05:06:10.056Z","createdAt":"2026-01-15T23:37:55.308Z"},{"id":"q-2662","question":"In a Databricks Delta Lake pipeline ingesting telemetry from two sources (S3 JSONs via Auto Loader with evolving schema and a streaming Kafka feed), design a GDPR-style purge workflow to delete or mask PII across Bronze, Silver, and Gold while preserving aggregates and lineage. Describe concrete steps using MERGE/DELETE, specify retention, audit logging, and how you test downstream dashboards and time-travel reads. Include RBAC considerations?","answer":"Trigger a purge workflow that masks PII and deletes records across Bronze, Silver, and Gold using a MERGE keyed by event_id or user_id when a deletion request arrives; set pii columns to NULL, flag ro","explanation":"## Why This Is Asked\nTests privacy controls, cross-layer purge, and governance in a realistic Databricks setup.\n\n## Key Concepts\n- Delta Lake MERGE for in-place masking/deletes\n- Time travel for validation and rollback\n- Governance with Unity Catalog RBAC and lineage\n- VACUUM retention and audit-trail design\n\n## Code Example\n```sql\n-- Example purge MERGE across Bronze\nMERGE INTO bronze.telemetry AS b\nUSING purge_queue AS p\nON b.event_id = p.event_id\nWHEN MATCHED THEN UPDATE SET\n  pii_email = NULL,\n  pii_phone = NULL,\n  purged = TRUE;\n```\n\n## Follow-up Questions\n- How would you validate purge correctness across Bronze/Silver/Gold with synthetic data?\n- How do you handle partial deletes for multi-tenant data while preserving aggregates?","diagram":null,"difficulty":"intermediate","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T05:45:56.550Z","createdAt":"2026-01-16T05:45:56.550Z"},{"id":"q-2743","question":"Design a multi-tenant Databricks pipeline for a SaaS IoT product. Ingest streaming telemetry from an MQTT bridge and a batch Postgres metadata feed. Build Bronze (raw), Silver (device_id normalization, unit standardization, PII masking), Gold (per-tenant aggregates). Implement schema evolution, a 2-minute late-data watermark, RBAC via Unity Catalog with per-tenant lineage, and a lightweight QA suite. Provide concrete steps, partitioning, and validation plan?","answer":"Implement with Delta Live Tables: Bronze ingests from MQTT bridge and Postgres batch feed; Silver normalizes device_id, standardizes units, masks PII; Gold computes per-tenant aggregates (devices_onli","explanation":"## Why This Is Asked\nTests multi-tenant governance, IoT ingestion, schema evolution, and late data handling; evaluates real-world trade-offs across Bronze/Silver/Gold and privacy masking.\n\n## Key Concepts\n- Delta Live Tables with Bronze/Silver/Gold\n- MQTT ingestion and batch Postgres feed\n- Schema evolution, 2-minute watermark\n- PII masking, Unity Catalog RBAC, tenant lineage\n- Validation and QA hooks\n\n## Code Example\n```python\n# Example DLT Python skeleton\nimport dlt\n@dlt.table\ndef bronze_raw():\n  return spark.read...  # placeholder\n\n@dlt.table\ndef silver_normalize():\n  ...\n```\n\n## Follow-up Questions\n- How would you handle schema drift across tenants?\n- How would you validate late-data handling without impacting live tenants?","diagram":"flowchart TD\n  Bronze[Bronze - Raw MQTT & Postgres]</n  Silver[Silver - Normalized, PII masked]</n  Gold[Gold - Per-tenant aggregates]\n  Bronze --> Silver\n  Silver --> Gold\n  TenantRBAC(Unity Catalog) --> Silver\n  TenantRBAC(Unity Catalog) --> Gold","difficulty":"advanced","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Databricks","Discord"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T09:52:06.223Z","createdAt":"2026-01-16T09:52:06.223Z"},{"id":"q-2794","question":"In a Databricks environment, you receive a daily JSON feed of fintech transactions from a partner S3 bucket. Build a three-table pipeline: Bronze Delta with Auto Loader and schema evolution, Silver Delta that upserts deduplicating by transaction_id, and Gold that aggregates daily totals by customer_id. The JSON evolves (new fields like merchant_id and currency). Explain how you'd implement schema evolution, a MERGE based upsert, and partitioning by transaction_date. Include a lightweight data quality test plan and PII masking in outputs?","answer":"Use Auto Loader to load JSON into Bronze with schema evolution for new fields merchant_id and currency. Silver handles dedup via MERGE on transaction_id, with partitioning by transaction_date. Gold ag","explanation":"## Why This Is Asked\n\nThis question tests practical data engineering for evolving schemas, bronze-silver-gold layering, and robust upserts in a Databricks workflow. It also covers data quality and PII masking‚Äîessential beginner skills in real-world pipelines.\n\n## Key Concepts\n\n- Auto Loader with schema evolution for JSON feeds\n- Delta Lake MERGE for upserts and deduplication\n- Partitioning by transaction_date to optimize queries\n- Basic data quality checks (NOT NULL, amount > 0, valid timestamps)\n- PII masking in downstream views\n\n## Code Example\n\n```javascript\n// Pseudo-Spark SQL example for Bronze to Silver MERGE\nconst bronze = spark.read.format(\"delta\").load(\"/mnt/partner/transactions/bronze\")\nbronze.createOrReplaceTempView(\"bronze\")\n\nspark.sql(`\nMERGE INTO silver AS s\nUSING bronze AS b\nON s.transaction_id = b.transaction_id\nWHEN MATCHED THEN UPDATE SET s.amount = b.amount, s.event_ts = b.event_ts, s.merchant_id = b.merchant_id, s.currency = b.currency\nWHEN NOT MATCHED THEN INSERT (transaction_id, user_id, amount, event_ts, merchant_id, currency, transaction_date) VALUES (b.transaction_id, b.user_id, b.amount, b.event_ts, b.merchant_id, b.currency, b.transaction_date)\n`)\n```\n\n## Follow-up Questions\n\n- How would you test schema evolution with a feed that adds merchant_id midstream? \n- Which monitoring checks would you add to detect MERGE conflicts or late data issues and ensure masking stays intact?","diagram":null,"difficulty":"beginner","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","Netflix","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T13:01:19.972Z","createdAt":"2026-01-16T13:01:19.972Z"},{"id":"q-2987","question":"Design a Databricks data pipeline that ingests telemetry from two sources: a streaming Kafka topic and a batch REST API, both with evolving schemas. Build Bronze (raw), Silver (mask PII and implement a 2-minute watermark), and Gold (Customer SCD2) layers, with CDC merges. Enforce Unity Catalog RBAC with lineage across layers, and outline concrete partitioning, test plans, and recovery strategies?","answer":"Ingest from Kafka (stream) and REST API (batch) into Bronze with Auto Loader and schema evolution, then Silver masking PII and applying a 2-minute watermark. Use MERGE CDC to upsert into Gold where Cu","explanation":"## Why This Is Asked\n\nTests ability to integrate streaming and batch sources with schema evolution, implement SCD2, masking, and governance across lakehouse layers; evaluates practical decisions on partitioning, watermarking, and RBAC.\n\n## Key Concepts\n\n- Multi-source ingestion and schema evolution\n- Silver masking and 2-minute watermark\n- Gold: Slowly Changing Dimension Type 2\n- Unity Catalog RBAC and lineage\n\n## Code Example\n\n```javascript\n// Pseudo steps for MERGE-based SCD2 in Bronze->Silver->Gold\n```\n\n## Follow-up Questions\n\n- How would you handle late data beyond the watermark?\n- How would you validate end-to-end data lineage across Bronze, Silver, and Gold?","diagram":"flowchart TD\n  A[Ingest Kafka (Bronze)] --> B[Bronze: Kafka raw]\n  C[Ingest REST API (Bronze)] --> D[Bronze: REST raw]\n  B --> E[Silver: Clean & PII masked]\n  D --> E\n  E --> F[Gold: Customer SCD2]\n  F --> G[Lineage: Unity Catalog]","difficulty":"intermediate","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T20:30:45.198Z","createdAt":"2026-01-16T20:30:45.199Z"},{"id":"q-3011","question":"Design a cross-tenant Databricks Delta Live Tables pipeline that shares Gold metrics with external partners via Delta Sharing, while enforcing per-tenant isolation with Unity Catalog. Ingest telemetry from S3 JSON with evolving schema and a high-throughput Kafka topic. Build Bronze (raw), Silver (dedupe by event_id with a 2-minute watermark), and Gold (per-tenant aggregates). Include masking for PII, partition strategy, tests, and RBAC plan?","answer":"Set up a Delta Live Tables pipeline with Bronze ingest from S3 (Auto Loader) and Kafka, Silver deduped by event_id using MERGE CDC with a 2-minute watermark, and Gold with per-tenant aggregates. Enfor","explanation":"## Why This Is Called\nTests ability to design multi-tenant data products with external sharing, RBAC, masking, and data quality in Databricks.\n\n## Key Concepts\n- Delta Live Tables orchestration for Bronze/Silver/Gold\n- Delta Sharing for external partner access\n- Unity Catalog RBAC with per-tenant isolation\n- Dynamic masking for PII fields\n- Schema evolution on JSON/S3 + Kafka ingestion\n- 2-minute late data watermark and CDC dedupe\n\n## Code Example\n```python\n# Pseudo masking example\ndef mask_pii(value):\n  if value is None:\n    return None\n  return 'REDACTED'\n```\n\n## Follow-up Questions\n- How would you test multi-tenant access and masking across Delta Sharing shares?\n- How would you monitor lineage from Bronze to Gold and ensure compliant RBAC changes propagate?","diagram":null,"difficulty":"advanced","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T21:29:53.549Z","createdAt":"2026-01-16T21:29:53.550Z"},{"id":"q-3115","question":"Design a Databricks Delta Live Tables pipeline for a drone delivery fleet. Ingest two streams: a) S3-stored JSON drone events (Bronze) with evolving fields (drone_id, ts, lat, lon, battery, sensor), b) PostgreSQL CDC drone metadata (model, region, maint_status). Build Bronze, Silver, Gold with a MERGE CDC to upsert Silver and support schema evolution, and a Gold layer with enriched metrics. Implement a 2-minute late-data watermark, mask operator_id in Bronze, and enforce Unity Catalog RBAC with lineage across layers. Include partitioning strategy, testing plan, and trade-offs?","answer":"Leverage Delta Live Tables with two streams: Bronze from S3 Auto Loader and PostgreSQL CDC Bronze. Silver uses MERGE CDC to dedupe and apply schema evolution; Gold computes enriched metrics. Implement","explanation":"## Why This Is Asked\nTests real-world multi-stream ingestion, CDC upserts, dynamic schema evolution, data masking, and lineage governance in a single pipeline. It also probes RBAC and testing discipline for production readiness.\n\n## Key Concepts\n- Delta Live Tables, MERGE CDC, schema evolution\n- Watermarking for late data\n- Data masking of PII fields\n- Unity Catalog RBAC and lineage capture across Bronze/Silver/Gold\n- Multi-stream joins and partitioning strategies\n\n## Code Example\n```javascript\n// Pseudo-DDT: mask operator_id in Bronze before Silver\n// This is a conceptual snippet; actual DLT Python/SQL syntax will vary by project\nimport pyspark.sql.functions as F\nbronze = spark.readStream.format('delta').load('s3://bucket/drone/bronze')\nsilver = bronze.withColumn('operator_id_masked', F.sha2(F.col('operator_id'), 256))\n```\n\n## Follow-up Questions\n- How would you test schema evolution with a new field added to the drone events?\n- How would you validate lineage and RBAC across all three layers in Unity Catalog?","diagram":null,"difficulty":"intermediate","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T04:00:03.867Z","createdAt":"2026-01-17T04:00:03.867Z"},{"id":"q-3155","question":"Design a production ready Databricks data pipeline for telemetry arriving from S3 Auto Loader and a Kafka topic with an evolving schema. Build Bronze Silver Gold with Silver deduplicating on event_id using MERGE CDC, a 2 minute watermark for late data, and PII masking. Enforce Unity Catalog RBAC with lineage. Propose a Great Expectations CI/CD test harness with synthetic data and a rollback strategy?","answer":"Use Delta Live Tables to orchestrate Bronze (S3 Auto Loader) and Silver/Gold, with Kafka as a Bronze input. Implement dedupe on event_id in Silver via MERGE CDC, enforce a 2-minute watermark for late ","explanation":"## Why This Is Asked\nTests ability to design end-to-end data quality, governance, and testing in a real Databricks environment with evolving schemas and multi-source ingestion.\n\n## Key Concepts\n- DLTs, Auto Loader, structured streaming\n- MERGE CDC deduplication\n- watermarking for late data\n- PII masking and Unity Catalog RBAC\n- Great Expectations CI/CD\n\n## Code Example\n```python\n# skeleton: create expectations for schema and data quality\nimport great_expectations as ge\n\n# define a simple expectation suite for Bronze->Silver\nsuite = ge.from_pandas(bronze_df.head())\nsuite.expect_column_values_to_not_be_null(\"event_id\")\nsuite.expect_column_values_to_be_of_type(\"ts\", \"datetime64[ns]\")\n```\n\n## Follow-up Questions\n- How would you test schema drift across evolving fields and flag PRs automatically?\n- What rollback considerations exist if a downstream consumer caches Gold results?","diagram":"flowchart TD\n  A[Ingest Bronze: S3 Auto Loader + Kafka] --> B[Silver: dedupe by event_id (MERGE CDC)]\n  B --> C[Gold: analytics layer]\n  A --> D[PII masking in Bronze/Silver]\n  B --> E[ lineage via Unity Catalog RBAC ]","difficulty":"intermediate","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","LinkedIn","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T04:50:08.964Z","createdAt":"2026-01-17T04:50:08.964Z"},{"id":"q-3250","question":"Design a Databricks streaming pipeline ingesting S3 Auto Loader JSONs and a Kafka topic with evolving schema. Build Bronze/Silver/Gold with event_id dedupe via MERGE CDC and a 2-minute late data watermark. Add an observability layer: inline data drift detection for numeric fields and schema drift alerts with Slack alerts; enforce Unity Catalog RBAC with lineage. Provide concrete steps and a minimal test plan?","answer":"Propose Bronze from S3 Auto Loader and Kafka, handle evolving schema; Silver dedupe by event_id with MERGE CDC; Gold computes per-user metrics. 2-minute watermark. Observability layer: rolling window ","explanation":"## Why This Is Asked\n\nThis question probes how a candidate adds observability to a streaming Databricks pipeline, not just correctness. It requires practical design choices for drift checks, schema evolution, and RBAC in Unity Catalog, plus how to validate and test the setup.\n\n## Key Concepts\n\n- Delta Lake streaming with evolving schema and MERGE CDC\n- 2-minute watermark for late data\n- Data drift and schema drift detection across numeric fields\n- Slack alerting and lightweight test plan\n- Unity Catalog RBAC with lineage across Bronze/Silver/Gold\n\n## Code Example\n\n```python\n# Drift check sketch (pseudo)\nfrom pyspark.sql import functions as F\n# compute rolling stats and compare to baseline; trigger alert on threshold breach\n```\n\n## Follow-up Questions\n\n- How would you test drift thresholds and alert reliability?\n- How to scale drift checks across many numeric fields and tenants?","diagram":null,"difficulty":"intermediate","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Robinhood","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T08:44:24.519Z","createdAt":"2026-01-17T08:44:24.519Z"},{"id":"q-3325","question":"In a Databricks data-engineer exercise, ingest daily Parquet logs of customer events from S3 into Delta Lake; events originate from several microservices with event_time in different time zones. Describe a beginner-friendly pipeline that uses Auto Loader, Bronze/Silver with a simple MERGE to upsert the latest per customer_id, standardizes timestamps to UTC, and validates with a lightweight data quality check (not null, reasonable ranges). Include a partitioning plan and a quick validation approach?","answer":"Use Auto Loader to ingest S3 Parquet into Bronze. In Silver, cast event_time to UTC (from per-service TZ via to_timestamp(event_time, tz)). Upsert latest per customer_id with MERGE into Silver; partit","explanation":"## Why This Is Asked\n\nAssesses ability to handle multi-time-zone data with a simple Bronze/Silver pattern, using MERGE for upserts and basic data quality checks in a beginner-friendly way.\n\n## Key Concepts\n\n- Delta Lake Bronze/Silver pattern\n- Auto Loader ingestion\n- Timezone normalization to UTC\n- MERGE upserts for latest events\n- Lightweight data quality checks\n\n## Code Example\n\n```python\n# PySpark sketch (conceptual)\nfrom pyspark.sql import functions as F\n\nbronze = spark.read.format(\"cloudFiles\").option(\"cloudFiles.format\",\"parquet\").load(\"s3://bucket/bronze/\")\n# Normalize to UTC using per-service timezone column 'tz'\nsilver = bronze.withColumn(\"event_time_utc\", F.to_utc_timestamp(F.col(\"event_time\"), F.col(\"tz\")))\nlatest = silver.groupBy(\"customer_id\").agg(F.max(\"event_time_utc\").alias(\"event_time_utc\"))\nupsert = silver.alias(\"s\").join(latest.alias(\"l\"), [\"customer_id\", \"event_time_utc\"]).select(\"s.*\")\n# In real code, MERGE into Silver table with upserts\n```\n\n## Follow-up Questions\n\n- How would you validate timezone normalization across multiple sources?\n- How would you monitor and alert on data quality regressions in this pipeline?","diagram":"flowchart TD\n  A[Ingest: S3 Parquet] --> B[Bronze Delta Table]\n  B --> C[Transform: UTC normalization]\n  C --> D[Silver Delta Table with MERGE upsert]\n  D --> E[Quality checks & partitioning]\n","difficulty":"beginner","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T11:34:10.631Z","createdAt":"2026-01-17T11:34:10.631Z"},{"id":"q-3468","question":"In a multi-tenant Databricks data platform, design a Delta Live Tables pipeline that ingests telemetry from two sources: S3 Auto Loader JSON drops and a Kafka topic, with per-tenant isolation. Build Bronze, Silver, and Gold layers; deduplicate in Silver by event_id using MERGE CDC; apply per-tenant data masking for PII; enforce Unity Catalog RBAC with lineage; handle per-tenant schema evolution and provide a rollback path via Delta Time Travel. Include synthetic data testing plan?","answer":"Design a multi-tenant Delta Live Tables pipeline: ingest from S3 Auto Loader and Kafka; Bronze raw with tenant_id, ts, event_id; Silver dedups by event_id via MERGE CDC and masks PII per tenant; Gold ","explanation":"## Why This Is Asked\nTests multi-tenant data governance, per-tenant masking, and end-to-end DLT with lineage and rollback.\n\n## Key Concepts\n- Delta Live Tables, Bronze/Silver/Gold layering\n- MERGE CDC for per-tenant dedup\n- Per-tenant PII masking logic\n- Unity Catalog RBAC and lineage\n- Per-tenant schema evolution and Delta Time Travel rollbacks\n- Synthetic data CI validation\n\n## Code Example\n```python\nimport dlt\n@dlt.table\ndef bronze_raw():\n    pass\n@dlt.table\ndef silver_dedup():\n    pass\n@dlt.table\ndef gold_agg():\n    pass\n```\n\n## Follow-up Questions\n- How would you monitor per-tenant SLAs and data quality?\n- How do you handle cross-tenant schema drift conflicts?\n- What test strategies validate RBAC isolation across tenants?\n","diagram":null,"difficulty":"advanced","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Bloomberg","DoorDash"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T17:33:00.167Z","createdAt":"2026-01-17T17:33:00.167Z"},{"id":"q-3604","question":"In a Databricks pipeline ingesting 10 GB/day of Parquet user events from S3 into a Bronze Delta table via Auto Loader, design a beginner-friendly Silver layer that deduplicates by event_id, handles 2-minute late data with a watermark, and masks PII (email) at query time using a simple hashing or masking approach. Outline concrete steps including partitioning, a MERGE-based CDC, Unity Catalog RBAC, and a lightweight test plan. How would you implement the masking view?","answer":"Design a Silver layer that ingests from the Bronze table using MERGE statements to deduplicate by event_id, implements a 2-minute watermark for late data handling, partitions by date for optimal performance, and creates a masked view that hashes email fields using SHA2(256) to protect PII while maintaining data relationships.","explanation":"## Why This Is Asked\n\nTests ability to design an end-to-end Databricks pipeline at a beginner level, including deduplication, late data handling, data privacy, and governance.\n\n## Key Concepts\n\n- Bronze-Silver-Gold data modeling in Delta Lake\n- MERGE for idempotent upserts and CDC\n- Structured Streaming watermark for late data\n- Data masking via views (hashing sensitive fields)\n- Unity Catalog RBAC for data access control\n\n## Code Example\n\n```sql\n-- Masked view example\nCREATE OR REPLACE VIEW silver_masked AS\nSELECT event_id,\n       user_id,\n       SHA2(email, 256) AS email_hash,\n       timestamp,\n       event_type,\n       event_data\nFROM silver_events;\n```","diagram":null,"difficulty":"beginner","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Meta","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T05:29:24.991Z","createdAt":"2026-01-17T23:29:47.300Z"},{"id":"q-3667","question":"Design a Databricks data pipeline that ingests 5 TB/day of semi-structured JSON events from S3 into a Bronze Delta table, then propagates changes to Silver and Gold marts using Delta Change Data Feed (CDF) without re-reading full history. Implement 2-minute late data handling, PII masking at query time, Unity Catalog RBAC, and referential integrity across marts. Outline CDC strategy, partitioning, and a practical test plan?","answer":"Enable Delta CDF on the Bronze table for event_id. Use MERGE into Silver to apply Inserts/Updates and tombstones from Bronze. Build Gold via MERGE from Silver, preserving referential keys. Apply a lig","explanation":"## Why This Is Asked\n\nTests deep knowledge of CDC patterns, late data handling, and data governance in a realistic Databricks setup.\n\n## Key Concepts\n\n- Delta Lake Change Data Feed (CDF)\n- MERGE CDC for Silver\n- Gold derivation and referential integrity\n- Query-time masking\n- Unity Catalog RBAC and lineage\n- Late data windowing\n\n## Code Example\n\n```python\n# Enable CDF\nspark.sql(\"ALTER TABLE bronze SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\")\n# Pseudo-merges shown conceptually; implement with exact match/conditions in production\n# MERGE Bronze -> Silver on event_id with updates and deletes\n```\n\n## Follow-up Questions\n\n- How would you handle schema evolution across Bronze to downstream without breaking queries?\n- What metrics would you monitor to ensure CDC latency remains within target?","diagram":null,"difficulty":"advanced","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Hashicorp"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T04:18:56.001Z","createdAt":"2026-01-18T04:18:56.001Z"},{"id":"q-3854","question":"In a Databricks data platform for a retail chain, ingest 3 sources: S3 Parquet orders, S3 JSON customers, and Kafka click events, into Bronze, Silver, and Gold using Delta Live Tables. Build a centralized data-contract registry with versioned schemas and field-level expectations, enforce it via contract-aware backfills: bump a contract version, backfill Bronze, then cascade to Silver/Gold. Outline promotion flow, compatibility rules, testing plan, and Unity Catalog RBAC considerations?","answer":"Implement a centralized data-contract registry with versioned schemas and field-level expectations; enforce via Delta Live Tables contract checks. On version bumps, trigger Bronze backfill, then casca","explanation":"## Why This Is Asked\nThis question probes data contract governance, backfill strategies, and lineage in a real Databricks setup.\n\n## Key Concepts\n- Data contracts, versioning, optional vs required fields\n- Delta Live Tables, Bronze/Silver/Gold patterns\n- Backfill workflows and PR promotions\n- Unity Catalog RBAC and lineage\n\n## Code Example\n```python\n# Pseudo-code illustrating contract enforcement concept\ndef enforce_contract(df, contract_schema):\n    if df.schema != contract_schema:\n        raise ValueError(\"Schema mismatch: expected {}\".format(contract_schema))\n    return df\n```\n\n## Follow-up Questions\n- How would you optimize backfill performance on large Bronze datasets?\n- How would you test contract changes in CI/CD and handle rollbacks if downstream data is affected?\n","diagram":null,"difficulty":"intermediate","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T12:59:40.513Z","createdAt":"2026-01-18T12:59:40.514Z"},{"id":"q-3914","question":"In a beginner Databricks pipeline, ingest 5 GB/day JSON clickstream from S3 into Bronze via Auto Loader; flatten nested payloads into Silver, deduplicate by event_id with MERGE CDC, and apply a 2-minute watermark. Build Gold as daily active users by country. Partition by date; enforce Unity Catalog RBAC. Provide a lightweight test plan and RBAC considerations?","answer":"Ingest 5 GB/day JSON clickstream from S3 to Bronze via Auto Loader; flatten nested payloads into Silver, deduplicate by event_id with MERGE CDC, and apply a 2-minute watermark. Build Gold as daily act","explanation":"## Why This Is Asked\nTests ability to handle nested JSON, streaming ingestion with Auto Loader, deduplication via MERGE CDC, late data handling with a watermark, and governance via Unity Catalog RBAC. Also evaluates end-to-end testing mindset.\n\n## Key Concepts\n- Auto Loader streaming ingestion\n- Flattening nested JSON payloads\n- MERGE CDC for event_id dedup\n- 2-minute watermark for late data\n- Bronze/Silver/Gold data lake pattern\n- Unity Catalog RBAC and lineage\n\n## Code Example\n```python\n# PySpark flattening example\nraw = spark.readStream.format(\"cloudFiles\").option(\"cloudFiles.format\",\"json\").load(\"s3://bucket/clickstream/\")\nsilver = raw.selectExpr(\"event_id\", \"cast(ts as timestamp) as ts\", \"payload.user_id as user_id\", \"payload.country as country\", \"payload.actions as actions\")\n```\n\n## Follow-up Questions\n- How would you monitor late data and alert on stalls?\n- What are the trade-offs between MERGE CDC vs UPDATE-only approaches?","diagram":null,"difficulty":"beginner","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Instacart","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T14:52:04.054Z","createdAt":"2026-01-18T14:52:04.054Z"},{"id":"q-3991","question":"Design a Databricks ingestion path for streaming clickstream: Bronze from S3 JSON (Auto Loader) and Kafka; Silver computes per-user features (last_seen, total_events, avg_session_len) with a windowed watermark; Gold registers features in the Databricks Feature Store for model training. Include schema evolution, Unity Catalog RBAC, and a CI/CD test harness with synthetic data, drift checks, and Delta time-travel backfills with rollback?","answer":"Bronze from S3 JSON (Auto Loader) and Kafka; Silver computes per-user features (last_seen, total_events, avg_session_len) with a 3-minute watermark; Gold registers features in the Databricks Feature S","explanation":"## Why This Is Asked\n\nTests end-to-end design: multi-source ingestion, windowed feature engineering, feature store integration, and governance with RBAC. It also probes backfill, schema evolution, drift detection, and rollback in CI/CD.\n\n## Key Concepts\n- Bronze/Silver/Gold data path\n- Auto Loader + Kafka integration\n- Windowed aggregations and event-time watermark\n- Databricks Feature Store and model training integration\n- Delta Lake schema evolution and constraints\n- Unity Catalog RBAC and data lineage\n- CI/CD with synthetic data, drift checks, and Delta time travel backfills\n\n## Code Example\n```python\n# Placeholder: showcase how you might register a feature in the store (pseudo)\nfrom databricks.feature_store import FeatureStoreClient\nfs = FeatureStoreClient()\nfeature = {\"name\": \"last_seen\",\n           \"type\": \"timestamp\",\n           \"entity\": \"user_id\"}\nfs.create_feature_store_feature(feature)\n```\n\n## Follow-up Questions\n- How would you backfill Silver when a new feature is added without breaking existing models?\n- What drift detection threshold would you use and how would you alert it in CI/CD?","diagram":"flowchart TD\n  Bronze[Bronze: Ingest from S3 JSON + Kafka]\n  Silver[Silver: Per-user features (last_seen, total_events, avg_session_len) with 3m watermark]\n  Gold[Gold: Feature Store for model training]\n  Bronze --> Silver\n  Silver --> Gold","difficulty":"intermediate","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T18:50:19.874Z","createdAt":"2026-01-18T18:50:19.875Z"},{"id":"q-4073","question":"In a Databricks environment ingest 100 GB/day of Parquet user events from S3 and 5 MB/s real-time orders from Kafka. Design Bronze-Silver-Gold pipelines and add a versioned Feature Store with source lineage and drift monitoring. Include: schema evolution strategy, Unity Catalog RBAC, and a rollback/refresh plan for features. How would you implement feature retrieval for model scoring with time-travel to feature versions?","answer":"Implement Delta Live Tables for Bronze-Silver-Gold pipelines with a versioned Databricks Feature Store integrated with Unity Catalog RBAC. Ingest S3 Parquet files and Kafka streams into Bronze tables; apply Silver layer transformations including deduplication by event_id and schema evolution using MERGE operations with AUTO_MERGE enabled; create Gold layer aggregations that populate the Feature Store. Configure Unity Catalog RBAC for granular data access control, enable Feature Store versioning with comprehensive source lineage tracking, and implement drift monitoring through statistical distribution comparisons with automated alerting. Establish rollback procedures by maintaining feature version history and implementing refresh mechanisms that can revert to previous versions while ensuring continuous model scoring operations.","explanation":"## Why This Is Asked\nEvaluates advanced data engineering capabilities beyond standard Bronze-Silver-Gold patterns, specifically focusing on versioned Feature Stores with lineage tracking, drift monitoring, and rollback mechanisms.\n\n## Key Concepts\n- Delta Live Tables for pipeline orchestration\n- Unity Catalog RBAC for access control\n- Databricks Feature Store versioning and lineage\n- Statistical drift detection and alerting\n- Time travel for reproducible model scoring\n- Rollback and refresh strategies\n\n## Code Example\n```\nSELECT * FROM feature_store.user_session_features \nAS OF TIMESTAMP '2026-01-19 10:00:00'\nWHERE feature_version = 'v2.1'\n```","diagram":null,"difficulty":"intermediate","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Lyft","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T05:53:17.361Z","createdAt":"2026-01-18T22:43:30.937Z"},{"id":"q-4086","question":"In a Databricks project ingesting 5 GB/day of JSON event data from two APIs into Delta tables on S3, design a beginner-friendly pipeline that unifies the schemas, builds Bronze, Silver, and Gold layers, and handles 2-minute late data with a watermark. Implement simple per-field normalization, a PII masking approach at query time, and Unity Catalog RBAC restricting access to Silver and Gold. Outline concrete steps including partitioning, schema evolution, and a lightweight test plan?","answer":"Bronze: Ingest both API feeds using Auto Loader into Delta Bronze tables with unified schema handling. Silver: Apply MERGE-CDC to deduplicate by event_id, normalize data types, and implement a 2-minute watermark for late data handling. Create a read view with email masking using hash functions for PII protection. Gold: Generate aggregated business metrics from the cleansed Silver data. Implement Unity Catalog RBAC to restrict access to Silver and Gold layers. Partition tables by date and event_type for optimal query performance. Enable schema evolution to accommodate changing API structures. Test with sample datasets and validate watermark behavior.","explanation":"## Why This Is Asked\n\nThis question evaluates practical Databricks implementation skills including data ingestion, schema unification, late data handling, basic data privacy measures, and access control‚Äîall at a beginner-friendly level. It emphasizes actionable implementation steps over theoretical concepts.\n\n## Key Concepts\n\n- Auto Loader for Bronze layer ingestion\n- Silver layer deduplication using MERGE-CDC\n- Watermarking for late data management\n- PII masking through read views\n- Unity Catalog RBAC for access control\n\n## Code Example\n\n```javascript\n// PII masking view example\nCREATE VIEW masked_events AS\nSELECT \n    event_id,\n    event_timestamp,\n    event_type,\n    sha2(email, 256) as masked_email,\n    other_fields\nFROM silver_events;\n```","diagram":"flowchart TD\n  BronzeBronze[ Bronze Ingest ] --> SilverSilver[ Silver Layer ]\n  SilverSilver --> GoldGold[ Gold Layer ]\n  subgraph Masking\n    MaskView[ Masking View ]\n  end\n  SilverSilver --> MaskView\n","difficulty":"beginner","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Plaid","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T05:49:49.594Z","createdAt":"2026-01-18T23:30:39.469Z"},{"id":"q-4118","question":"Scenario: Ingest 200 GB/day of JSON transactional events from two e-commerce APIs into Delta Lake on S3. Design a Bronze‚ÜíSilver‚ÜíGold pipeline with Auto Loader, handle evolving schemas (promo_code, device_type, customer_segment), implement per-field normalization, a 2-minute late-data watermark, and a query-time PII masking view. Enforce Unity Catalog RBAC restricting Silver/Gold, optimize partitioning by date and region, define a schema-evolution policy, and outline a lightweight test plan plus a rollback strategy for failed deploys?","answer":"Design a Bronze‚ÜíSilver‚ÜíGold pipeline using Auto Loader to ingest 200 GB/day of JSON events from two e-commerce APIs into Delta Lake on S3. The Bronze layer uses Auto Loader with schema inference and cloudFiles to land raw data. The Silver layer standardizes field names and types, converts timestamps to UTC, deduplicates by natural key, and handles schema evolution for new fields like promo_code, device_type, and customer_segment. The Gold layer aggregates per-user daily metrics. Implement a 2-minute watermark for late data handling and create a query-time PII masking view. Enforce Unity Catalog RBAC to restrict Silver/Gold access, optimize partitioning by date and region, define a schema-evolution policy, and establish a lightweight test plan with rollback strategy for failed deployments.","explanation":"## Why This Is Asked\nTests ability to handle schema drift, governance, and reliability in a multi-layer Delta Lake architecture.\n\n## Key Concepts\n- Auto Loader with Bronze-Silver-Gold pattern on Delta Lake/S3\n- Schema drift handling and evolution policy\n- 2-minute watermark for late data processing\n- Query-time masking view for PII protection\n- Unity Catalog RBAC with column-level security\n- Partitioning optimization by date/region\n- Time-travel rollback capabilities\n\n## Code Example\n```sql\n-- PII masking view example\nCREATE OR REPLACE VIEW masked_users AS\nSELECT user_id, REGEXP_REPLACE(email, '[^@].*@', '***@') AS masked_email,\n       REGEXP_REPLACE(phone, '\\d(?=\\d{4})', '*') AS masked_phone\nFROM users;\n```\n\n## Implementation Notes\n- Use `spark.readStream.format('cloudFiles')` with `schemaEvolutionMode='addNewColumns'`\n- Configure `watermark='2 minutes'` in streaming queries\n- Apply `OPTIMIZE` and `Z-ORDER` on partition columns for performance\n- Implement Unity Catalog grants: `GRANT SELECT ON silver_catalog TO data_analysts`\n- Set up automated testing with `dbt test` and integration validation","diagram":null,"difficulty":"intermediate","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Lyft","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":["auto loader","schema evolution","delta lake","unity catalog","pii masking","late data","partitioning optimization","rollback strategy","bronze silver gold","watermark handling","column-level security","natural key deduplication"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2026-01-19T05:08:55.570Z","createdAt":"2026-01-19T02:51:03.300Z"},{"id":"q-4253","question":"Design a multi-tenant Databricks pipeline to ingest 5-20 TB/day of mobile events from S3 via Auto Loader, with per-tenant isolation and evolving schemas. Build Bronze, Silver, Gold with: 2-minute watermark, dedupe by event_id using MERGE CDC, and schema evolution; per-tenant dynamic masking for email via Unity Catalog; data residency by tenant_id/region partitions; RBAC; cost optimization with Photon and caching; and a lightweight test plan. How would you implement and test this end-to-end?","answer":"Ingest via Auto Loader to Bronze; Silver uses MERGE CDC on event_id with a 2-minute watermark and schema evolution; Gold aggregates per tenant. Implement per-tenant dynamic masking for email via Unity","explanation":"## Why This Is Asked\nTests multi-tenant isolation, masking policies, and end-to-end governance in a real Databricks setup.\n\n## Key Concepts\n- Auto Loader, Delta Lake schema evolution, MERGE CDC\n- Unity Catalog dynamic masking policies\n- Tenant_id/region partitioning for residency\n- RBAC, lineage, Photon costs\n\n## Code Example\n```python\n# PySpark sketch: write to silver with MERGE CDC (pseudo)\n# masking handled in a view using masking policy on email\n```\n\n## Follow-up Questions\n- How would you test masking correctness across tenants without data leakage?\n- What would you monitor to catch schema drift early and auto-rollback changes?","diagram":null,"difficulty":"advanced","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Robinhood","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T10:39:23.930Z","createdAt":"2026-01-19T10:39:23.930Z"},{"id":"q-4301","question":"In a Databricks data platform, ingest 50 TB/day of JSON web logs from S3 into a Bronze Delta table using Auto Loader. Build Silver with deduplication by event_id via MERGE CDC, a 24-hour watermark for late data, and a query-time IP masking view. Implement cross-region DR: replicate Bronze and Silver to a secondary region with consistent CDC, schema evolution, and mirrored Unity Catalog RBAC. Outline concrete steps, partitioning, test plan, and DR failover procedure?","answer":"Configure Auto Loader to ingest 50 TB/day JSON logs into Bronze; partition by date. Silver uses MERGE CDC on event_id with a 24h watermark; expose a masking view that hashes IPs at query time. DR: mir","explanation":"## Why This Is Asked\nThis question probes cross-region replication, CDC in Silver, and governance in Unity Catalog under DR, a realistic production concern for large-scale Databricks platforms.\n\n## Key Concepts\n- Auto Loader, partitioning by date\n- MERGE CDC for deduping event_id\n- Watermark for late data\n- Query-time IP masking view\n- Cross-region replication and DR\n- Unity Catalog RBAC, schema evolution checks\n\n## Code Example\n```javascript\n// masking view example (SQL in a JS block for illustration)\nCREATE VIEW silver.mask_ip AS\nSELECT event_id, user_id, event_ts,\n       SHA2(ip_address, 256) AS ip_masked\nFROM bronze.events;\n```\n\n## Follow-up Questions\n- How would you validate DR lag and data consistency end-to-end?\n- What are the pitfalls of cross-region replication for Delta Lake metadata?","diagram":null,"difficulty":"advanced","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["NVIDIA","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T13:06:23.042Z","createdAt":"2026-01-19T13:06:23.044Z"},{"id":"q-4330","question":"In a Databricks data lake for a fintech app, ingest 8-12 TB/day JSON events from S3 via Auto Loader and a streaming CDC feed from Postgres. Build Bronze/Silver/Gold with strict per-record RBAC via Unity Catalog, implement field-level masking for SSN at query time, and introduce a progressive data quality gate that halts Silver if latency P95 exceeds threshold. Include schema evolution, partitioning, and a lightweight test plan?","answer":"Use Auto Loader to ingest 8-12 TB/day JSON into Bronze; Silver deduplicates on event_id with MERGE CDC, and uses a 2-minute watermark. Mask SSN at query time via a masking VIEW over Silver. Enforce pe","explanation":"## Why This Is Asked\nTests practical data governance, schema evolution, and quality gates in a multi-tenant lakehouse. It emphasizes RBAC at the data layer, query-time masking, and production-safe progression from Bronze to Gold with latency guards.\n\n## Key Concepts\n- Delta Lake Bronze-Silver-Gold modeling\n- Unity Catalog RBAC and row/column masking\n- MERGE CDC and watermarking for late data\n- Delta Live Tables expectations for data quality\n\n## Code Example\n```javascript\n// Example placeholder snippet for masking view\n```\n\n## Follow-up Questions\n- How would you test masking performance under high cardinality SSN values?\n- How would you evolve the schema with minimal downtime while keeping downstream compatibility?","diagram":null,"difficulty":"advanced","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Snowflake","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T14:40:31.773Z","createdAt":"2026-01-19T14:40:31.773Z"},{"id":"q-4387","question":"Scenario: Databricks Bronze‚ÜíSilver‚ÜíGold pipeline ingesting 200 GB/day of product events from S3 via Auto Loader, with Silver dedupe on event_id and a 5-minute watermark. Propose a scalable observability and drift-detection layer: metrics schema (latency, completeness, duplicates, schema changes), anomaly detection approach, alerting, rollback/backfill plan, and Unity Catalog RBAC for dashboards and lineage. Include test plan?","answer":"Instrument per-batch metrics: total events, duplicates, nulls, late data, latency; use Delta table stats for schema drift; apply KS-test on key fields vs baseline; publish to a central Databricks SQL ","explanation":"## Why This Is Asked\n\nThis question tests practical observability, drift detection, RBAC, and operational readiness beyond basic ETL design.\n\n## Key Concepts\n\n- Data quality metrics\n- Drift detection (schema and distributions)\n- Delta Lake stats\n- Unity Catalog RBAC\n- Alerting and rollback\n\n## Code Example\n\n```python\n# Pseudo drift check\ndef is_drifted(current, baseline, p=0.05):\n    from scipy.stats import ks_2samp\n    stat, pval = ks_2samp(current, baseline)\n    return pval < p\n```\n\n## Follow-up Questions\n\n- How would you test drift detection with synthetic data?\n- How would you scale dashboards to 10k users?","diagram":null,"difficulty":"intermediate","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Robinhood","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T16:52:36.184Z","createdAt":"2026-01-19T16:52:36.184Z"},{"id":"q-4588","question":"In a Databricks setup, design a Databricks Feature Store pipeline: ingest Kafka real-time events and S3 batch catalog changes, build Bronze and Silver via Delta Live Tables, publish offline features to Delta tables and online features to the Databricks Feature Store, implement versioned feature definitions and drift detection, and enforce Unity Catalog RBAC. Outline the concrete steps and a test plan with synthetic data to validate online/offline parity?","answer":"Design a comprehensive Databricks Feature Store pipeline that ingests Kafka real-time events and S3 batch catalog changes into Bronze tables via Delta Live Tables, computes versioned Silver layer features, publishes offline features to Delta tables and online features to the Databricks Feature Store, implements drift detection with automated alerts, and enforces Unity Catalog RBAC for secure access control. The solution includes a robust test plan using synthetic data to validate online/offline feature parity through canary deployments and consistency checks.","explanation":"## Why This Is Asked\nThis question evaluates comprehensive feature engineering capabilities in Databricks, testing real-time and batch ingestion strategies, feature lifecycle management, governance frameworks, and ML operational readiness. It assesses practical integration of Delta Live Tables with the Databricks Feature Store while implementing enterprise-grade security controls.\n\n## Key Concepts\n- **Hybrid Ingestion**: Kafka streams for real-time events + S3 batch processing for catalog updates\n- **Medallion Architecture**: Bronze (raw) and Silver (processed) layers using Delta Live Tables\n- **Feature Distribution**: Offline storage (Delta tables) vs online serving (Feature Store)\n- **Feature Governance**: Versioned definitions with automated drift detection and alerting\n- **Access Control**: Unity Catalog RBAC for granular publish/read permissions\n- **Quality Assurance**: Synthetic data generation, canary deployments, and parity validation between online/offline feature stores\n\n## Code Example","diagram":"flowchart TD\n  A[Kafka: real-time events] --> B[Bronze via DLT]\n  C[S3: batch catalog] --> D[Bronze via DLT]\n  D --> E[Feature Engineering]\n  E --> F[Offline Feature Table]\n  E --> G[Online Feature Store]\n  F --> H[Model Training]\n  G --> H\n  H --> I[Monitoring]\n  I --> J[Alerts]","difficulty":"advanced","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T05:54:05.203Z","createdAt":"2026-01-20T02:44:09.828Z"},{"id":"q-4666","question":"Design a Databricks data pipeline for a multi-region fintech app that ingests 1 TB/day JSON transaction events from S3, with strongly evolving schemas and strict data contracts. Build Bronze, Silver, Gold using Auto Loader and Delta Live Tables, but focus on cross-region replication, time-travel correctness, and secure data sharing with Unity Catalog. Explain how you'd enforce row-level access, implement schema drift handling without downtime, and validate with a lightweight test harness?","answer":"Use Auto Loader to ingest 1 TB/day JSON events from S3 into Bronze; Silver enforces a contract via MERGE CDC on event_id with a 2-minute watermark; Gold derives per-session aggregates. Mirror Delta ta","explanation":"## Why This Is Asked\n\nThis question probes cross-region DR, evolving schemas, and strict data contracts in real-time Databricks workloads, plus governance via Unity Catalog.\n\n## Key Concepts\n\n- Auto Loader for scalable ingestion with schema evolution\n- Delta Live Tables for pipeline quality, CDC, and contracts\n- Cross-region replication and Delta time travel\n- Unity Catalog RBAC and row-level security\n- Schema drift handling with zero-downtime strategies\n- Lightweight test harness with synthetic data\n\n## Code Example\n\n```javascript\n// Pseudo contract check in a Delta Live Tables-like stage\nfunction checkContract(row) {\n  if (!row.event_id || !row.schema_version) {\n    throw new Error(\"Contract violation\");\n  }\n  return row;\n}\n```\n\n## Follow-up Questions\n\n- How would you detect drift and auto-heal without downtime?\n- How would you rollback a run while preserving lineage and metadata?","diagram":"flowchart TD\n  A[Bronze: Ingest from S3] --> B[Silver: schema contract, dedupe, watermark]\n  B --> C[Gold: aggregates]\n  D[DR region: cross-region mirrors] --> A\n  E[Time travel & lineage] --> B\n  F[Unity Catalog RBAC & RLS] --> B\n  F --> C","difficulty":"intermediate","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Coinbase","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T07:10:32.591Z","createdAt":"2026-01-20T07:10:32.591Z"},{"id":"q-4694","question":"Scenario: In a Databricks pipeline ingesting 100 GB/day of JSON clickstream from S3 plus nightly REST API enrichment, design a Silver layer that builds user_session features (counts, dwell, funnels) via windowed aggregates, uses a dynamic schema registry to handle evolving event fields (no CDC merges), and enforces Unity Catalog RBAC for multi-tenant access. Include partitioning, clustering, and a light validation/test plan?","answer":"Propose a Silver table that builds per-session features (session_id, user_id, event_ts, clicks, dwell, funnel_step) using 1h sliding windows, with a dynamic schema registry to absorb new event fields ","explanation":"## Why This Is Asked\nAssess ability to design scalable, governance-aware Silver layers that adapt to evolving schemas without CDC merges, while enforcing RBAC across tenants.\n\n## Key Concepts\n- Dynamic schema handling; no strict CDC\n- Windowed feature extraction; per-session metrics\n- Unity Catalog RBAC for multi-tenant access\n- Partitioning and clustering for cost and query performance\n\n## Code Example\n```python\n# PySpark sketch: compute per-session features with sliding windows\nsilver = bronze_df.groupBy('tenant_id','session_id')\\\n  .agg(F.count('event_id').alias('events'), F.max('event_ts').alias('last_event_ts'))\n```\n\n## Follow-up Questions\n- How would you validate schema drift and test RBAC changes? \n- What are trade-offs of 1h windows vs 5m windows in this scenario?","diagram":null,"difficulty":"intermediate","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Hugging Face","Lyft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T08:48:10.067Z","createdAt":"2026-01-20T08:48:10.067Z"},{"id":"q-4798","question":"In a Databricks pipeline, you ingest 1 TB/day of Parquet logs into a Delta table via Auto Loader. The table has many small files causing frequent small-file problems, and downstream queries occasionally misreport counts due to late data and churn. Propose a safe, automated nightly compaction strategy that preserves CDC correctness, avoids breaking streaming readers, and uses ZORDER. Outline concrete steps, toggles, and a lightweight test plan?","answer":"Nightly two-phase compaction: Phase A creates a staging Delta table by reading the Bronze with read-consistency, rewrites small Parquet files into larger ones (target ~128 MB) using OPTIMIZE with ZORD","explanation":"## Why This Is Asked\nTests practical mastery of Delta Lake maintenance, streaming safety, and data correctness under schema/file churn. It probes operational thinking beyond basic pipeline design.\n\n## Key Concepts\n- Small-file problem and file size optimization\n- OPTIMIZE with ZORDER for read performance\n- CDC preservation via MERGE back into live table\n- Safety for streaming readers during maintenance\n- Rollback via Delta time travel and test verification\n\n## Code Example\n```sql\n-- Phase A: stage compacted data\nCREATE OR REPLACE TABLE bronze_staging AS\nSELECT * FROM delta.`/bronze/logs`;\nOPTIMIZE delta.`/bronze/logs` ZORDER BY ingestion_key;\n\n-- Phase B: merge back to preserve CDC\nMERGE INTO delta.`/bronze/logs` AS live\nUSING delta.`/bronze/logs_staging` AS stage\nON live.id = stage.id\nWHEN MATCHED THEN UPDATE SET *\nWHEN NOT MATCHED THEN INSERT *;\n```\n\n## Follow-up Questions\n- How would you monitor compaction impact on streaming queries?\n- What toggles would you expose to operators (e.g., enable/disable, dry-run)?","diagram":null,"difficulty":"intermediate","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T13:20:00.686Z","createdAt":"2026-01-20T13:20:00.686Z"},{"id":"q-4915","question":"In a Databricks pipeline, 10‚Äì20 GB/day of Parquet clickstream data lands in S3 and is loaded into a Bronze Delta table via Auto Loader, with a Silver layer that must cope with weekly schema drift (new fields, renamed columns) and perform event_id deduplication, before a Gold aggregation layer used for dashboards. Describe an implementation that enables robust Delta Lake schema evolution, preserves downstream query compatibility, and includes a lightweight test plan to validate drift handling across environments. How would you guard against breaking changes while keeping retroactive queries correct?","answer":"Adopt Delta schema evolution with mergeSchema enabled and a target schema version. Silver casts/renames to the target, while Bronze stores original driftable columns. Use event_id dedupe via MERGE CDC","explanation":"## Why This Is Asked\nThis question probes practical schema evolution management, drift testing, and end-to-end stability in a Databricks lakehouse.\n\n## Key Concepts\n- Delta schema evolution and mergeSchema\n- Silver/Gld layering with drift handling\n- Backward/forward compatibility views\n- Lightweight drift test strategy\n\n## Code Example\n```javascript\n// Enable auto-merge of schemas in Delta Lake\nspark.conf.set(\"spark.databricks.delta.mergeSchema.autoMerge.enabled\",\"true\")\n```\n\n## Follow-up Questions\n- How would you roll back a drift that breaks downstream dashboards?\n- What tests would you automate to guard future schema changes?","diagram":"flowchart TD\n  A[Ingest: Auto Loader] --> B[Bronze Delta]\n  B --> C[Silver: Dedupe + Schema Evolution]\n  C --> D[Gold: Aggregations]\n  B -- drift --> C","difficulty":"intermediate","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T19:05:23.857Z","createdAt":"2026-01-20T19:05:23.857Z"},{"id":"q-4952","question":"In a Databricks pipeline ingesting 100 GB/day of Parquet web logs from S3 and a REST API returning JSON, design a Bronze-Silver-Gold schema that deduplicates on a composite key (user_id, event_ts) because event_id is optional. Implement a 3-minute watermark for late data, enforce per-tenant isolation with tenant-specific views (no RBAC changes), and handle schema drift with Delta constraints and a simple schema_version column. Provide concrete steps, partitioning, and a lightweight test plan?","answer":"Deduplicate with composite key (user_id, event_ts) in Silver using MERGE to upsert latest per key, applying a 3-minute watermark for late data. Bronze ingests from S3 Parquet and REST JSON. Gold store","explanation":"## Why This Is Asked\nExplores robust dedup when event identifiers are missing, multi-source integration, late data handling, and tenant isolation without reworking RBAC.\n\n## Key Concepts\n- Composite-key dedup with MERGE and windowing\n- Watermark-based late data processing\n- Delta constraints for schema drift\n- Tenant isolation via dynamic views (no RBAC changes)\n- Lightweight synthetic-data tests for drift and QC\n\n## Code Example\n```sql\nMERGE INTO silver AS s\nUSING (\n  SELECT user_id, event_ts, MAX(event_seq) AS seq\n  FROM bronze\n  GROUP BY user_id, event_ts\n) AS b\nON s.user_id = b.user_id AND s.event_ts = b.event_ts\nWHEN MATCHED THEN UPDATE SET s.seq = b.seq\nWHEN NOT MATCHED THEN INSERT (user_id, event_ts, seq) VALUES (b.user_id, b.event_ts, b.seq);\n```\n\n```sql\nCREATE OR REPLACE VIEW tenant_view AS\nSELECT * FROM silver\nWHERE tenant_id = current_setting('tenant_id');\n```\n\n## Follow-up Questions\n- How would you test drift between Bronze and Silver during a run?\n- How would you monitor late data and alert on schema drift?","diagram":"flowchart TD\n  Bronze[Bronze Ingestion: S3 Parquet + REST JSON]\n  Silver[Silver: Dedup by (user_id, event_ts) + 3m watermark]\n  Gold[Gold: Latest per user_id]\n  Views[Tenant-specific views via current_setting('tenant_id')]\n  Bronze --> Silver --> Gold\n  Silver --> Views","difficulty":"intermediate","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","LinkedIn"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T20:48:17.645Z","createdAt":"2026-01-20T20:48:17.645Z"},{"id":"q-5035","question":"In a Databricks data pipeline, ingest 3 GB/day Parquet logs from S3 via Auto Loader into a Bronze Delta table. Build a Silver layer that enriches each event with a small static dimension table customers (customer_id, country, segment) through a left join, deduplicates by event_id, and supports 2-minute late data with a watermark. Partition by event_date; broadcast the small dimension; enforce Unity Catalog RBAC and a lightweight test plan. How would you implement the enrichment join and tests?","answer":"Load 3 GB/day Parquet logs from S3 into a Bronze Delta table using Auto Loader. Build a Silver layer that enriches each event by left joining with the small static dimension table customers (customer_id, country, segment) on customer_id, deduplicates by event_id, and supports 2-minute late data using a watermark. Partition by event_date; broadcast the small dimension table; enforce Unity Catalog RBAC; and implement a lightweight test plan.","explanation":"## Why This Is Asked\n\nAssess practical enrichment of streaming events with a dimensional table, and how to implement simple data quality and governance controls in Databricks for a beginner.\n\n## Key Concepts\n\n- Auto Loader, Bronze/Silver tiers, Delta Lake merges\n- Join strategies with small dimensions (broadcast)\n- Watermarks for late data, deduplication\n- Unity Catalog RBAC and test plans\n\n## Code Example\n\n```sql\n-- Example Silver enrichment\nCREATE OR REPLACE VIEW silver.enriched_events AS\nSELECT e.*, c.country, c.segment\nFROM bronze.events e\nLEFT JOIN dimensions.customers c\n  ON e.customer_id = c.customer_id\nWHERE e.event_date = CURRENT_DATE()\n```\n\n## Implementation Approach\n\nFor the enrichment join, use Delta Lake's streaming capabilities with Auto Loader to ingest the raw Parquet files, then apply a broadcast join with the small customers dimension table. Implement watermarking to handle late data within the 2-minute window and deduplicate using event_id. Partition the Silver table by event_date for optimal query performance. Ensure Unity Catalog permissions are properly configured and create basic validation tests for data quality.","diagram":null,"difficulty":"beginner","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","Salesforce","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T05:12:56.024Z","createdAt":"2026-01-21T02:42:20.118Z"},{"id":"q-5119","question":"In a Databricks pipeline ingesting 3 GB/day of Parquet web events from S3 and a JSON API feed into a Bronze Delta table, design a beginner-friendly Silver that normalizes event_time and user_id, aggregates per-user daily activity, applies a 5-minute watermark for late data, enforces a fixed set of actions, and adds a simple daily-row-count alert; outline steps and provide a lightweight test plan?","answer":"Normalize event_time to a timestamp, standardize user_id as string, and map action to a fixed set. Silver computes per-user daily activity: date, user_id, sessions, page_views. Apply a 5-minute waterm","explanation":"## Why This Is Asked\nTests ability to design a beginner-friendly Silver layer with basic data quality and observability for a Databricks pipeline.\n\n## Key Concepts\n- Bronze/Silver/Gold layering, schema normalization, watermarking, and basic deduping\n- Query-time data quality checks and simple alerting\n- Lightweight tests and observable metrics\n\n## Code Example\n```javascript\n// Pseudo PySpark-like normalization (for illustration)\ndf = bronze_events\n  .withColumn(\"ts\", to_timestamp(col(\"event_time_epoch\")/1000))\n  .withColumn(\"user_id\", col(\"user_id\").cast(\"string\"))\n  .withColumn(\"action\", when(col(\"action\").isin(\"page_view\",\"click\",\"purchase\"), col(\"action\")).otherwise(\"other\"))\n```\n\n## Follow-up Questions\n- How would you test deduplication if event_id is missing in some records?\n- How would you implement a minimal alert for the daily row count difference between Bronze and Silver?","diagram":null,"difficulty":"beginner","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Instacart","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T07:07:17.778Z","createdAt":"2026-01-21T07:07:17.778Z"},{"id":"q-5193","question":"In a Databricks data platform ingesting 30 TB/day of Avro logs from an on-prem gateway into a Bronze Delta table via Auto Loader, design a Silver layer that isolates per-tenant data with dynamic partition pruning, handles evolving schema with MERGE-based CDC, and enforces per-tenant Unity Catalog RBAC plus audit trails across Bronze -> Silver -> Gold. Outline concrete steps, DR considerations, and a lightweight test plan?","answer":"Propose a Silver that creates per-tenant partitions, uses MERGE CDC to dedupe and apply schema changes, and uses a tenant-scoped VIEW for masked analytics. Enable Unity Catalog RBAC at Bronze and Silv","explanation":"## Why This Is Asked\nTests ability to design tenant-aware data platforms with partitioning, CDC, RBAC, and DR across regions in Databricks, while maintaining governance and testability.\n\n## Key Concepts\n- Per-tenant partitioning and dynamic pruning for performance\n- MERGE-based CDC to handle dedup and schema evolution\n- Unity Catalog RBAC and lineage/audit trails across layers\n- DR considerations: regional replicas and failover\n\n## Code Example\n```sql\n-- Pseudocode: per-tenant MERGE CDC\nMERGE INTO silver.tenant_events AS target\nUSING bronze.raw_events AS source\nON target.tenant_id = source.tenant_id AND target.event_id = source.event_id\nWHEN MATCHED THEN UPDATE SET target.col1 = source.col1, target.col2 = source.col2\nWHEN NOT MATCHED THEN INSERT (tenant_id, event_id, col1, col2) VALUES (source.tenant_id, source.event_id, source.col1, source.col2);\n```\n\n## Follow-up Questions\n- How would you test data drift across tenants and ensure RBAC constraints are enforced in dashboards?\n- What metrics and alerts would you implement to monitor schema evolution and partition pruning effectiveness?","diagram":null,"difficulty":"advanced","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T10:18:59.686Z","createdAt":"2026-01-21T10:18:59.686Z"},{"id":"q-5268","question":"Design a Databricks pipeline that ingests 20 GB/day of Parquet data from S3 into Bronze, then Silver with 5-minute watermark, and a Gold dataset. Expose the Gold dataset to an external partner via Delta Sharing to Google BigQuery. Outline Unity Catalog RBAC, the sharing setup, and a lightweight test plan; include trade-offs?","answer":"Ingest Bronze from S3 via Auto Loader; Silver with 5-minute watermark and event_id dedupe; Gold as the shared dataset. Enable Delta Sharing to Google BigQuery, grant Unity Catalog RBAC roles for inter","explanation":"## Why This Is Asked\nTests cross-tenant data sharing, Delta Sharing, and RBAC in Unity Catalog, plus end-to-end Bronze/Silver/Gold pipeline design.\n\n## Key Concepts\n- Delta Live Tables Bronze/Silver/Gold pattern\n- Watermarking and deduplication\n- Delta Sharing to Google BigQuery\n- Unity Catalog RBAC and data lineage\n- Monitoring and testing\n\n## Code Example\n```sql\n-- Illustrative placeholder for sharing setup\nCREATE SHARE gold_share FOR GOLD_TABLE;\nGRANT USAGE ON DATABASE gold TO partner_username;\n```\n\n## Follow-up Questions\n- How would you test for permission drift and revoke access quickly?\n- What are latency implications when sharing stale Gold data?","diagram":"flowchart TD\n  Bronze[Bronze: Ingest Parquet from S3]\n  Silver[Silver: Deduplicate by event_id; 5-min watermark]\n  Gold[Gold: Curated for sharing]\n  Sharing[Delta Sharing to Google BigQuery]\n\nBronze --> Silver\nSilver --> Gold\nGold --> Sharing","difficulty":"intermediate","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T13:33:03.655Z","createdAt":"2026-01-21T13:33:03.655Z"},{"id":"q-5346","question":"In a Databricks ETL ingesting 15 TB/day of JSON clickstream from S3 and a streaming REST feed, design Bronze/Silver/Gold with event_id dedupe via MERGE CDC and a 5-minute watermark. Propose a rollback strategy to a known-good Delta version on data-quality failure, plus a lightweight test plan and Unity Catalog RBAC considerations?","answer":"Use a MERGE CDC on Silver using event_id, enforce a 5-minute watermark, and track last_good_version in a small Delta table. On failure, time-travel Silver/Gold to that version and flip an alias to re-","explanation":"## Why This Is Asked\nTests practical handling of mixed batch/streaming ingestion, late data, and real-world failure recovery without downtime. It also probes governance, test discipline, and rollback operational habits.\n\n## Key Concepts\n- Delta Time Travel for safe rollback\n- MERGE CDC in Silver layer\n- Watermarking for late data handling\n- Rollback orchestration with table aliases\n- Unity Catalog RBAC and lineage\n\n## Code Example\n```javascript\n-- Pseudocode: record last_good_version after a successful run\nINSERT INTO last_good_version (version, timestamp) VALUES (CURRENT_VERSION(), NOW());\n```\n\n## Follow-up Questions\n- How would you automate canary rollouts and automated rollback when a failure is detected?\n- How would you test rollback paths using synthetic faults and data skew injections?","diagram":null,"difficulty":"intermediate","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Hugging Face","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T19:00:44.570Z","createdAt":"2026-01-21T19:00:44.570Z"},{"id":"q-5448","question":"Ingest 2 GB/day of CSV product catalog data via SFTP into a Bronze Delta table on S3. Design a beginner-friendly pipeline with Bronze (landing), Silver (normalized, typed, deduplicated by product_id using MERGE-based CDC), and Gold (category-level stock and price KPIs). Implement 2-minute late data via a watermark, and a read-time PII masking for supplier_contact (simple hashing or partial redaction) using a masking view. Outline concrete partitioning, schema validation, a lightweight test plan, and Unity Catalog RBAC considerations. Include how you‚Äôd implement the masking view?","answer":"Ingest 2 GB/day of CSV product catalog data via SFTP into a Bronze Delta table on S3. Design a beginner-friendly pipeline with Bronze (landing), Silver (normalized, typed, deduplicated by product_id using MERGE-based CDC), and Gold (category-level stock and price KPIs). Implement 2-minute late data via a watermark, and a read-time PII masking for supplier_contact (simple hashing or partial redaction) using a masking view. Outline concrete partitioning, schema validation, a lightweight test plan, and Unity Catalog RBAC considerations. Include how you'd implement the masking view?","explanation":"## Why This Is Asked\nTests practical, end-to-end data pipeline skills: landing, transformation, and management of late data with clear, auditable steps. Emphasizes data quality, RBAC, and data masking in a beginner-friendly setting.\n\n## Key Concepts\n- Bronze/Silver/Gold layering for CSV ingestion\n- MERGE-based CDC to deduplicate on product_id\n- Watermark for late data handling\n- Read-time PII masking with a masking view\n- Unity Catalog RBAC and basic lineage awareness\n\n## Code Example\n```python\n# Pseudo Python sketch for masking at read time\nfrom pyspark.sql.functions import when, col, regexp_\n```","diagram":"flowchart TD\n  A[Ingest CSV via SFTP] --> B[Bronze Delta (S3)]\n  B --> C[Silver: normalize, cast, dedupe via product_id]\n  C --> D[Gold: category KPIs]\n  D --> E[BI/Reports / Access via Unity Catalog]","difficulty":"beginner","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T05:59:06.987Z","createdAt":"2026-01-21T22:49:56.743Z"},{"id":"q-5562","question":"Scenario: In a Databricks pipeline ingesting 10 GB/day of Parquet web logs from S3 via Auto Loader into Bronze, design a beginner-friendly Silver layer that deduplicates by event_id, applies a 1-hour watermark, and adds an AuditDelta table to track per-file ingest metadata (path, ingest_ts, file_size). Outline concrete steps for MERGE CDC into Silver, Unity Catalog RBAC granting read to Silver and AuditDelta, and a lightweight test plan with synthetic data?","answer":"Describe implementing Silver with dedupe on event_id via MERGE, a 1-hour watermark for late data, and an AuditDelta table capturing per-file ingest metadata (path, ingest_ts, file_size). Explain MERGE","explanation":"## Why This Is Asked\n\nTests practical Databricks data pipeline skills: implementing a robust Silver layer with deduplication, late-data handling, and governance hooks via an audit table.\n\n## Key Concepts\n\n- Delta Lake MERGE for CDC\n- Auto Loader watermarking\n- Audit/metadata table design\n- Unity Catalog RBAC for read access\n- Lightweight data quality tests with synthetic data\n\n## Code Example\n\n```sql\n-- Example MERGE for Silver dedupe\nMERGE INTO silver_web_logs AS s\nUSING bronze_web_logs AS b\nON s.event_id = b.event_id\nWHEN MATCHED THEN UPDATE SET s.* = b.*\nWHEN NOT MATCHED THEN INSERT VALUES (b.*);\n```\n\n## Follow-up Questions\n\n- How would you validate watermark correctness with synthetic late-arriving data?\n- What tests would ensure the AuditDelta table accurately reflects ingested files?","diagram":null,"difficulty":"beginner","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T06:47:31.638Z","createdAt":"2026-01-22T06:47:31.640Z"},{"id":"q-5651","question":"In a Databricks pipeline consuming 10 TB/day of JSON logs from S3 via Auto Loader, design a per-tenant lineage graph: Bronze (raw), Silver (tenant_id dedupe by event_id using MERGE CDC), Gold (feature-ready). Tenants may evolve schemas independently. How would you implement per-tenant schema evolution, Unity Catalog RBAC, and a drift/test plan?","answer":"Deliver a per-tenant data graph: Bronze ingests S3 JSON logs, Silver partitions by tenant_id and deduplicates by event_id with MERGE CDC, and Gold yields feature-ready tables per tenant. Use Delta Lak","explanation":"## Why This Is Asked\nTests ability to design tenant-isolated pipelines with schema drift handling and governance.\n\n## Key Concepts\n- Delta Lake 2.0 schema evolution\n- MERGE CDC and per-tenant isolation\n- Unity Catalog RBAC and tenancy\n- Drift testing with synthetic tenants\n\n## Code Example\n```javascript\n// Pseudo-logic showing a per-tenant Silver view creation\nCREATE OR REPLACE VIEW silver_tenant AS SELECT * FROM silver WHERE tenant_id = :tenant\n```\n\n## Follow-up Questions\n- How would you monitor cross-tenant data drift and auto-generate test cases?\n- How does Unity Catalog RBAC scale with N tenants, and what automation would you implement to provision policies?","diagram":null,"difficulty":"advanced","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T10:05:46.805Z","createdAt":"2026-01-22T10:05:46.805Z"},{"id":"q-5737","question":"In a Databricks lakehouse used by Instacart, Twitter, and Slack, you need to securely share a curated Silver dataset with an external partner via Delta Sharing. Design a minimal, production-ready flow that (a) defines a strict Unity Catalog RBAC model, (b) partitions data to minimize exposure, (c) logs access events to an AuditShare table, and (d) handles schema evolution so the partner can still read legacy fields. Outline concrete steps and a lightweight synthetic-test plan with synthetic data?","answer":"Design a Delta Sharing flow exposing a curated Silver dataset via Unity Catalog. Key steps: restrict data with a dedicated Catalog/Schema and column-scoped views; create a Delta Sharing Share for that","explanation":"## Why This Is Asked\nTests Delta Sharing, Unity Catalog RBAC, and data-auditing in a multi-tenant setup with schema evolution.\n\n## Key Concepts\n- Delta Sharing, Unity Catalog, RBAC\n- Data minimization and per-share governance\n- Audit logging for data access\n- Schema evolution handling with legacy views\n\n## Code Example\n```javascript\n-- Pseudo steps for setup (not executable in this snippet)\nCREATE CATALOG partner_share_catalog;\nCREATE SCHEMA partner_share_schema IN partner_share_catalog;\nCREATE VIEW partner_share_schema.silver_view AS SELECT * FROM silver_table; -- restricted columns\nCREATE SHARE partner_silver_share FOR partner_share_schema;\nGRANT SELECT ON SHARE partner_silver_share TO external_partner_role;\n```\n\n## Follow-up Questions\n- How would you revoke access and verify revocation propagates? \n- How would you test schema evolution without breaking the partner‚Äôs consumer code?","diagram":"flowchart TD\n  A[Ingest Bronze] --> B[Create Silver View]\n  B --> C[Delta Sharing Share]\n  C --> D[Partner Reads Silver]\n  D --> E[AuditShare Logs]","difficulty":"intermediate","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","Slack","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T14:47:10.776Z","createdAt":"2026-01-22T14:47:10.777Z"},{"id":"q-5860","question":"Ingest ~8 GB/day of JSON product events from S3 via Auto Loader into Bronze. Design a beginner-friendly Silver that validates required fields (event_id, product_id, user_id, price), enforces types, and guards against schema drift; build a Gold denormalized view with daily total_sales by product. Include a lightweight data quality test plan and Unity Catalog RBAC: analysts read Silver/Gold, engineers access Bronze. How would you implement the validation?","answer":"Bronze -> Silver path with validation: require event_id, product_id, user_id, price not null, price >= 0, and correct types. Use a MERGE CDC from Bronze to Silver. Gold denormalizes to daily total_sal","explanation":"## Why This Is Asked\n\nTests data quality awareness, schema drift handling, and basic governance in a Databricks flow. It also probes familiarity with Silver/Gold layering and RBAC separation.\n\n## Key Concepts\n\n- Auto Loader Bronze‚ÜíSilver path with validation\n- MERGE CDC from Bronze to Silver\n- Simple schema drift guards and type enforcement\n- Gold denormalization: daily totals\n- Unity Catalog RBAC for data access\n\n## Code Example\n\n```sql\n-- Bronze to Silver validation\nSELECT *\nFROM Bronze\nWHERE event_id IS NOT NULL\n  AND product_id IS NOT NULL\n  AND user_id IS NOT NULL\n  AND price IS NOT NULL\n  AND price >= 0;\n```\n\n```sql\n-- Gold daily aggregation\nCREATE OR REPLACE VIEW Silver.daily_sales AS\nSELECT product_id, DATE(event_ts) AS day, SUM(price) AS total_sales\nFROM Silver\nGROUP BY product_id, DATE(event_ts);\n```\n\n## Follow-up Questions\n\n- How would you automate these data quality checks (notebooks, tests, alerts)?\n- How would you handle late-arriving data and evolving schemas in this setup?","diagram":null,"difficulty":"beginner","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T19:39:27.866Z","createdAt":"2026-01-22T19:39:27.866Z"},{"id":"q-5931","question":"In a Databricks pipeline ingesting 5 GB/day of JSON click events from an API and a Kafka topic into Delta tables on S3, design a beginner-friendly Bronze‚ÜíSilver‚ÜíGold flow with a 2-minute late-data watermark. Add an anomaly gate: flag users whose per-user event rate in 5-minute windows exceeds a threshold and persist results to an Anomalies Delta table (user_id, window_start, window_end, rate, score). Outline schema unification, a MERGE CDC into Silver, Unity Catalog RBAC, day partitioning, and a lightweight synthetic test plan to validate bursts. How would you implement the anomaly gate and queries to review recent anomalies?","answer":"Design a Bronze‚ÜíSilver‚ÜíGold pipeline for ingesting 5 GB/day of JSON click events from both an API and Kafka topic into Delta tables on S3, implementing a 2-minute late-data watermark and an anomaly gate that flags users whose per-user event rate in 5-minute windows exceeds a defined threshold. Persist anomaly results to an Anomalies Delta table with schema (user_id, window_start, window_end, rate, score). Include schema unification across sources, MERGE CDC for Silver updates, Unity Catalog RBAC controls, day-based partitioning, and a lightweight synthetic test plan to validate burst scenarios.","explanation":"## Why This Is Asked\nThis question evaluates practical data engineering skills including multi-source ingestion, data quality monitoring, and real-time anomaly detection in a Databricks environment. It tests understanding of medallion architecture, streaming processing with watermarks, change data capture, and governance while emphasizing maintainable, beginner-friendly implementations.\n\n## Key Concepts\n- Bronze-Silver-Gold medallion architecture with Delta Lake\n- Multi-source ingestion (API + Kafka) with schema unification\n- Streaming watermarks and windowed aggregations\n- Anomaly detection with threshold-based gating\n- MERGE operations for CDC into Silver layer\n- Unity Catalog for RBAC and data lineage\n- Partitioning strategies for query performance\n- Synthetic data generation for testing\n\n## Code Example\n```python\n# Example Spark snippet for anomaly gate implementation\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\n# Read from Bronze table with watermark\nbronze_df = spark.readStream.table(\"catalog.bronze.click_events\")\\\n    .withWatermark(\"event_time\", \"2 minutes\")\n\n# Calculate 5-minute per-user event rates\nanomaly_df = bronze_df.groupBy(\n    F.window(F.col(\"event_time\"), \"5 minutes\"),\n    F.col(\"user_id\")\n).agg(\n    F.count(\"*\").alias(\"event_count\"),\n    F.min(\"event_time\").alias(\"window_start\"),\n    F.max(\"event_time\").alias(\"window_end\")\n).withColumn(\n    \"rate\", F.col(\"event_count\") / 300  # events per second\n).withColumn(\n    \"score\", F.when(F.col(\"rate\") > F.lit(threshold), 1.0).otherwise(0.0)\n).filter(F.col(\"score\") > 0)\n\n# Write to Anomalies table\nanomaly_df.writeStream.format(\"delta\")\\\n    .option(\"checkpointLocation\", \"/checkpoints/anomalies\")\\\n    .table(\"catalog.gold.anomalies\")\n```","diagram":null,"difficulty":"beginner","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","Lyft","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T04:42:17.609Z","createdAt":"2026-01-22T22:37:53.509Z"},{"id":"q-5974","question":"In a Databricks pipeline ingesting multi-tenant SaaS events from Kafka and S3 Parquet into Bronze, design a Silver and Gold layer with per-tenant isolation via Unity Catalog dynamic views (row-level predicates). Implement a dynamic predicate using current_user() to restrict rows by tenant_id in Silver/Gold, while preserving CDC via MERGE and handling schema drift. Provide concrete steps, a sample dynamic-view SQL, and a lightweight test plan with synthetic tenants?","answer":"Leverage Unity Catalog Dynamic Views to enforce per-tenant access. Bronze‚ÜíSilver deduplication by event_id with MERGE CDC; create Silver_Tenant as a dynamic view filtered by tenant_id using current_user(). Gold layer aggregates tenant-specific metrics while maintaining isolation through the same dynamic predicate pattern.","explanation":"## Why This Is Asked\n\nTests per-tenant data isolation in a multi-source Delta pipeline, plus dynamic views and schema drift handling in Unity Catalog.\n\n## Key Concepts\n\n- Unity Catalog Dynamic Views\n- current_user() predicates\n- MERGE CDC with schema drift\n- RBAC for cross-tenant access\n\n## Code Example\n\n```sql\n-- SQL sample for Silver_Tenant dynamic view\nCREATE OR REPLACE VIEW Silver_Tenant AS\nSELECT * FROM Silver\nWHERE tenant_id = current_user();\n```\n\n## Follow-up Questions\n\n- How would you audit and test dynamic view predicates?\n- How do you onboard new tenants without redeploying views?","diagram":null,"difficulty":"advanced","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T05:01:58.532Z","createdAt":"2026-01-23T02:27:55.109Z"},{"id":"q-6188","question":"In a Databricks pipeline ingesting 40 GB/day of clickstream data from a REST API (JSON with evolving schema) and Parquet logs from S3, design Bronze‚ÄëSilver‚ÄëGold. Silver deduplicates by event_id, handles schema drift with Delta Lake schema evolution, applies a 1-hour watermark, and adds a GovernanceView masking sensitive fields via Unity Catalog RBAC. Outline concrete steps for MERGE CDC into Silver, enable safe schema evolution, and a drift-test plan using synthetic data?","answer":"Enable Delta schema auto-merge for Silver and implement MERGE CDC from Bronze to Silver on event_id. Apply a 1-hour watermark; use schema evolution handling for new fields via ALTER TABLE ADD COLUMN. ","explanation":"## Why This Is Asked\n\nTests ability to integrate CDC, schema drift handling, and governance in a Databricks lakehouse with realistic data complexities.\n\n## Key Concepts\n\n- Delta schema evolution and auto-merge\n- MERGE CDC from Bronze to Silver\n- Watermarks for late-arriving data\n- Unity Catalog RBAC and governance views\n- Drift testing with synthetic data\n\n## Code Example\n\n```sql\nMERGE INTO Silver AS s\nUSING Bronze AS b\nON s.event_id = b.event_id\nWHEN MATCHED THEN UPDATE SET ...\nWHEN NOT MATCHED THEN INSERT (...);\n```\n\n## Follow-up Questions\n\n- How would you CI-test schema drift and auto-merge behavior?\n- How to handle incompatible type changes in downstream models?","diagram":"flowchart TD\n  Ingest[Ingest] --> Bronze[Bronze]\n  Bronze --> Silver[Silver]\n  Silver --> Gold[Gold]\n  Silver --> GovernanceView[GovernanceView]\n  UnityCatalog[Unity Catalog RBAC] --> Read[Read access]","difficulty":"intermediate","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Square","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T13:07:39.434Z","createdAt":"2026-01-23T13:07:39.434Z"},{"id":"q-6277","question":"In a Databricks pipeline ingesting 5 GB/day of Parquet session data from S3 into Bronze, design a beginner-friendly Silver layer that deduplicates by event_id, validates a new 'country' field against a whitelist table, and records per-batch data quality metrics in a Delta Quality table. Outline concrete steps for Auto Loader, schema enforcement, a MERGE-based CDC into Silver, Unity Catalog RBAC, and a lightweight test plan. How would you implement the country whitelist join in Silver?","answer":"Use Auto Loader to ingest Parquet into Bronze with schema enforcement. Merge into Silver by event_id to deduplicate. In Silver, join with a small country_whitelist delta table on country; create a boo","explanation":"Why This Is Asked\nTests ability to add data quality checks alongside standard Bronze-Silver pipelines, a practical beginner task that shows Databricks skills and Unity Catalog RBAC awareness.\n\nKey Concepts\n- Auto Loader with schema enforcement\n- MERGE-based deduplication into Silver\n- Join to country_whitelist for validation\n- DataQuality Delta table for batch metrics\n- Unity Catalog RBAC and view-level access\n\nCode Example\n```sql\n-- SQL sketch showing whitelist join and invalid flag\nSELECT b.*,\n       CASE WHEN w.country IS NULL THEN true ELSE false END AS invalid_country\nFROM bronze.events b\nLEFT JOIN country_whitelist w ON b.country = w.country;\n```\n\nFollow-up Questions\n- How would you test the DQ metrics and handle drift in the whitelist?\n- How would you adjust for late-arriving records affecting batch metrics?","diagram":"flowchart TD\n  A[Ingest with Auto Loader] --> B[Bronze Delta]\n  B --> C[Silver: dedupe by event_id]\n  C --> D[Country whitelist join; invalid_country flag]\n  D --> E[DataQuality Delta: batch metrics]\n  E --> F[Unity Catalog RBAC governs Silver and DQ]","difficulty":"beginner","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T17:34:03.783Z","createdAt":"2026-01-23T17:34:03.784Z"},{"id":"q-6313","question":"Context: In a Databricks data platform ingesting 40 TB/day of JSON web events from S3 via Auto Loader into Bronze, design a Silver layer that handles evolving schema, performs CDC by session_id with MERGE, enforces a 5-minute watermark for late data, masks IP and email at query time via a Unity Catalog masking policy, and logs per-file ingest metadata to an AuditLog table (path, ingest_ts, file_size). Outline concrete RBAC, cross-region replication, and a lightweight synthetic-data test plan?","answer":"Use Auto Loader with schema evolution for Bronze; Silver uses MERGE CDC on session_id; set a 5-minute watermark for late data; implement Unity Catalog masking policy to redact IP and email at query ti","explanation":"## Why This Is Asked\n\nTests the ability to combine ingestion patterns (Auto Loader, schema evolution), CDC with MERGE, watermark tuning, data governance (masking policies), and audit/logging. It also probes RBAC design, cross-region DR, and practical testing with synthetic data.\n\n## Key Concepts\n\n- Auto Loader schema evolution\n- MERGE CDC on session_id\n- Watermarking and late data handling\n- Unity Catalog masking policies\n- AuditLog lineage and RBAC\n- Cross-region replication\n\n## Code Example\n\n```sql\n-- Example Unity masking policy\nCREATE MASKING POLICY mask_ip AS (val STRING) RETURNS STRING -> STRING\nRETURN CASE WHEN true THEN 'REDACTED' ELSE val END;\n```\n\n## Follow-up Questions\n\n- How would you test masking correctness without exposing real PII?\n- How would you monitor CDC lag and watermark delays in production?","diagram":null,"difficulty":"advanced","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Meta","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T19:00:55.310Z","createdAt":"2026-01-23T19:00:55.310Z"},{"id":"q-6415","question":"In a Databricks data pipeline ingesting 200 GB/day of JSON clickstream logs from S3 via Auto Loader, design a three-layer architecture (Bronze, Silver, Gold) that deduplicates by a composite key (user_id, event_id) using MERGE CDC, implements a 15-minute watermark to accommodate late data, and adds a per-file AuditDelta table with path, ingest_ts, file_size, and file_count. Specify Unity Catalog RBAC for Silver and AuditDelta, and implement a masking view for PII with role-based access. Outline a lightweight test plan using synthetic data to verify CDC, watermark, masking, and schema evolution handling without breaking existing workloads?","answer":"Bronze layer ingests raw JSON clickstream from S3 using Auto Loader with schema inference; Silver layer performs MERGE CDC operations on composite key (user_id, event_id) with 15-minute watermark tolerance for late data; Gold layer aggregates processed data for analytics. AuditDelta table tracks per-file metadata (path, ingest_ts, file_size, file_count) for lineage and monitoring. Unity Catalog RBAC grants SELECT privileges on Silver and AuditDelta tables to analytics role, with PII masking view implemented using dynamic masking functions for role-based access control.","explanation":"## Why This Is Asked\nTests end-to-end data engineering capabilities: late data handling strategies, CDC implementation in Delta Lake, governance frameworks via Unity Catalog, and PII masking across organizational roles.\n\n### Key Concepts\n- MERGE CDC operations using composite keys for deduplication\n- 15-minute watermark configuration for handling late-arriving data\n- AuditDelta implementation for comprehensive file-level lineage\n- Unity Catalog RBAC with granular privilege management\n- Dynamic masking views for PII protection\n- Schema evolution handling and comprehensive testing strategies\n\n### Code Example\n```python\n# Dynamic masking helper for PII protection\ndef mask_pii_field(value, field_type):\n    if not value:\n        return value\n    if field_type == 'email' and '@' in value:\n        local, domain = value.split('@', 1)\n        return f\"{local[0]}***@{domain}\"\n    elif field_type == 'phone':\n        return value[:3] + '***' + value[-4:] if len(value) > 7 else '***'\n    return value\n```\n\n### Follow-up Questions\n- How would you handle schema drift in the clickstream events?\n- What monitoring alerts would you configure for pipeline health?\n- How do you optimize Delta Lake file sizing for 200GB/day workload?","diagram":null,"difficulty":"intermediate","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Google","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T04:24:07.979Z","createdAt":"2026-01-23T22:45:07.986Z"},{"id":"q-6490","question":"Ingest 3 GB/day of Parquet customer_events from S3 into Bronze with Auto Loader. Design a beginner-friendly Silver layer implementing an SCD Type 2 Customer dimension with a surrogate key, begin_date, end_date, and a current_flag. Use MERGE CDC to upsert on changes, support simple schema evolution for new fields, apply a 1-day late-data watermark, and enforce Unity Catalog RBAC on Silver. Outline concrete steps for partitioning, CDC, testing, and a lightweight backfill plan?","answer":"Create Silver.Customer_SCD2 with surrogate_key, customer_id, name, email, begin_date, end_date, current_flag. On each event, MERGE into Silver: if same customer_id and attributes changed, end_date of ","explanation":"## Why This Is Asked\nTests ability to implement SCD Type 2 in a beginner-friendly way using MERGE CDC, while addressing late data, schema evolution, and RBAC.\n\n## Key Concepts\n- SCD Type 2 with surrogate keys\n- MERGE CDC pattern\n- Delta Lake schema evolution\n- Unity Catalog RBAC\n- Watermark for late data and a simple backfill plan\n\n## Code Example\n```sql\nMERGE INTO Silver.Customer_SCD2 AS tgt\nUSING Bronze.customer_events AS src\nON tgt.customer_id = src.customer_id AND tgt.end_date = DATE '9999-12-31'\nWHEN MATCHED AND (src.name <> tgt.name OR src.email <> tgt.email) THEN\n  UPDATE SET end_date = CURRENT_DATE(), current_flag = 'N'\nWHEN NOT MATCHED THEN\n  INSERT (surrogate_key, customer_id, name, email, begin_date, end_date, current_flag)\n  VALUES (uuid(), src.customer_id, src.name, src.email, CURRENT_DATE(), DATE '9999-12-31', 'Y');\n```\n\n## Follow-up Questions\n- How would you validate that historical rows are preserved while the latest view reflects current data?\n- How would you handle schema evolution when a new field is added to the source? ","diagram":null,"difficulty":"beginner","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Google","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T04:30:01.342Z","createdAt":"2026-01-24T04:30:01.342Z"},{"id":"q-6620","question":"In a Databricks data platform ingesting 50 TB/day of Parquet web logs from S3 via Auto Loader and streaming telemetry from a Kafka topic, design an advanced Silver layer implemented with Delta Live Tables that deduplicates by (tenant_id, event_id) using CDC, handles late data with a 5-minute watermark, enforces schema evolution with Delta constraints, and populates an AuditDelta table recording path, ingest_ts, file_size, and source_name per file. Include Unity Catalog RBAC for Silver and AuditDelta. Outline concrete steps, partitioning strategy, test plan with synthetic data, and how to observe CDC and lineage. How would you implement and verify?","answer":"Use Delta Live Tables with two inputs: Bronze (S3 Auto Loader) and Kafka. Silver dedupes on (tenant_id, event_id) via MERGE CDC; apply a 5-minute watermark on event_time for late data; enforce schema ","explanation":"## Why This Is Asked\nAssesses multi-source CDC design, DLT orchestration, and data governance in a practical Databricks setting.\n\n## Key Concepts\n- Delta Live Tables, CDC across Bronze and Kafka\n- Composite primary key dedupe, upserts\n- Watermarking and schema-evolution constraints\n- AuditDelta per-file metadata, Unity Catalog RBAC\n\n## Code Example\n```javascript\n-- Pseudo SQL/DLT pattern showing MERGE CDC into Silver\n```\n\n## Follow-up Questions\n- How would you test with noisy late data and schema drift?\n- How do you monitor CDC latency and audit completeness?","diagram":"flowchart TD\n  Bronze1[Bronze: S3 Auto Loader] --> Silver[Silver: DLT CDC]\n  Kafka[Kafka stream] --> Silver\n  Silver --> AuditDelta[AuditDelta: per-file metadata]\n  Silver --> DataConsumers[Consumers]\n","difficulty":"advanced","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Scale Ai","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T10:37:16.720Z","createdAt":"2026-01-24T10:37:16.720Z"},{"id":"q-6787","question":"In a Databricks pipeline ingesting 50 GB/week of Parquet web logs from S3 via Auto Loader and 25 GB/week of JSON events from a REST-driven stream, design a three-layer Lakehouse (Bronze/Silver/Gold) that deduplicates by a canonical event_id across sources, uses a 15-minute watermark for late data, and performs a CDC MERGE into Silver. Add a per-file AuditLineage table recording path, ingest_ts, file_size, source_name. Implement Unity Catalog RBAC for Silver and AuditLineage. Outline a lightweight synthetic-data test plan to verify CDC, watermark, schema-evolution, and RBAC?","answer":"Proposed approach: Use Auto Loader for Parquet Bronze and streaming REST JSON into Bronze; derive canonical_id from event_id+source; CDC MERGE Bronze‚ÜíSilver keyed by canonical_id; 15-minute watermark;","explanation":"## Why This Is Asked\nTests end-to-end design for multi-source ingestion, cross-source dedup, and governance in Databricks, plus practical test planning.\n\n## Key Concepts\n- Auto Loader and structured streaming\n- Delta Lake CDC MERGE\n- Watermarking and schema evolution\n- Unity Catalog RBAC and data lineage\n- Synthetic-data tests\n\n## Code Example\n```javascript\n// Pseudo-Delta MERGE for CDC\nspark.sql(`MERGE INTO silver_table AS s\nUSING bronze_table AS b\nON s.canonical_id = b.canonical_id\nWHEN MATCHED THEN UPDATE SET s.* = b.*\nWHEN NOT MATCHED THEN INSERT *`)\n```\n\n## Follow-up Questions\n- How would you implement schema drift handling across sources?\n- What metrics would you surface in a data quality dashboard?","diagram":"flowchart TD\n  S3Parquet[Parquet on S3] --> Bronze\n  RESTJSON[REST JSON] --> Bronze\n  Bronze --> Silver\n  Silver --> Gold\n  Bronze --> AuditLineage\n  Silver --> AuditLineage","difficulty":"intermediate","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Oracle","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T17:33:13.929Z","createdAt":"2026-01-24T17:33:13.929Z"},{"id":"q-6826","question":"In a Databricks pipeline ingesting 8-10 GB/day of Parquet logs from S3 into Bronze via Auto Loader, design a beginner-friendly extension to automatically quarantine corrupted files detected during ingestion, route them to a Delta quarantine table, and ensure healthy files proceed. Outline file-level validation, ingestion routing, and a light test plan with synthetic corrupt files; include Unity Catalog RBAC assigning Bronze and quarantine permissions?","answer":"Implement a two-branch flow: during Bronze ingestion, run a quick pre-check pass that lists files in the S3 prefix and validates Parquet footer/schema against a lightweight schema snapshot. If a file ","explanation":"## Why This Is Asked\nThis question probes practical data quality controls at ingest, error isolation, and RBAC in a real Databricks workflow.\n\n## Key Concepts\n- Ingest validation, quarantine pattern, and per-file metadata\n- Delta Lake partitioning, quarantine table, and RBAC\n- Lightweight test plan with synthetic corrupt files\n\n## Code Example\n```javascript\n// pseudo pre-check and quarantine snippet\n```\n## Follow-up Questions\n- How would you scale the pre-check to 10k files/day?\n- How do you monitor quarantine backlog and automate alerts?","diagram":"flowchart TD\nA[Ingest Parquet] --> B[Pre-check files]\nB --> C{Corrupt?}\nC -- Yes --> D[Quarantine]\nC -- No --> E[Load to Bronze]","difficulty":"beginner","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","LinkedIn"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T19:00:05.675Z","createdAt":"2026-01-24T19:00:05.675Z"},{"id":"q-6882","question":"In a Databricks data platform serving two SaaS tenants, each with a separate S3 folder, design a Bronze-Silver-Gold pipeline using Auto Loader and Delta Live Tables that enforces per-tenant isolation with Unity Catalog RBAC and row-level security on tenant_id, and records per-file ingest metadata in an AuditDelta table. Include a 2-minute watermark for late data, CDC-like behavior in Silver, and DLT expectations for data quality. Outline testing with synthetic tenants and how to verify lineage?","answer":"Design a multi-tenant Bronze-Silver-Gold pipeline: Bronze layer ingests from two tenant-specific S3 folders using Auto Loader with cloudFiles; Silver layer enforces per-tenant isolation through Unity Catalog RBAC combined with row-level security on tenant_id, maintains per-file ingest metadata in AuditDelta, implements CDC-like behavior with merge operations, and includes DLT expectations for data quality and schema evolution; Gold layer provides tenant-aware aggregations with a 2-minute watermark for late data handling.","explanation":"## Why This Is Asked\nValidates expertise in implementing enterprise-grade multi-tenant data pipelines with robust isolation, governance, and observability using Databricks' modern data stack.\n\n## Key Concepts\n- Unity Catalog RBAC with Row-Level Security on tenant_id\n- Delta Live Tables (DLT) with expectations and CDC patterns\n- Auto Loader for scalable Bronze ingestion from tenant-partitioned S3\n- AuditDelta for comprehensive per-file lineage tracking\n- Watermarking for late data handling\n- Synthetic tenant testing strategies and lineage verification","diagram":"flowchart TD\n  A[Bronze Ingest] --> B[Silver (tenant_id RLS)]\n  B --> C[AuditDelta per-file]\n  C --> D[Gold Aggregates]\n  A --> D","difficulty":"intermediate","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T06:17:38.628Z","createdAt":"2026-01-24T21:34:03.076Z"},{"id":"q-6953","question":"In a Databricks pipeline ingesting 200 GB/day of Parquet logs from multiple SaaS sources via Auto Loader into Bronze, design a Silver layer that handles per-source dynamic schema evolution and deduplicates by (source_id, event_id) using MERGE CDC. Implement an Anomalies table to log per-file drift (path, ingest_ts, source, drift_type). Outline steps, Unity Catalog RBAC, and a synthetic-drift test plan?","answer":"The Silver layer implements per-source dynamic schema evolution using Delta Lake's schema evolution capabilities, supported by a registry that tracks column types across sources. We employ MERGE CDC operations with composite keys (source_id, event_id) to perform deduplication and targeted updates for changed records. The Anomalies table captures per-file drift with comprehensive lineage: path, ingest_ts, source, and drift_type, maintaining traceability back to Bronze files. Unity Catalog RBAC is configured with distinct groups for ingestion, transformation, and monitoring roles to ensure proper access controls. Synthetic drift testing is implemented through schema mutation scripts and drift injection utilities to validate the pipeline's resilience to schema changes.","explanation":"Why This Is Asked\n\nThis question evaluates the ability to design a robust data architecture that handles heterogeneous sources with evolving schemas while maintaining data quality and governance. It tests understanding of Delta Lake features, CDC patterns, and observability in enterprise Databricks environments.\n\nKey Concepts\n\n- Delta Lake schema evolution with per-source handling\n- MERGE CDC for efficient deduplication and updates\n- Auto Loader drift detection and anomaly tracking\n- Anomalies table design with comprehensive lineage\n- Unity Catalog RBAC for granular access control\n- Synthetic testing for schema drift validation","diagram":null,"difficulty":"intermediate","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","PayPal","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T05:28:26.571Z","createdAt":"2026-01-25T02:34:12.349Z"},{"id":"q-7148","question":"Design a beginner-friendly Databricks pipeline that ingests 20 GB/day of Parquet clickstream logs from S3 via Auto Loader into Bronze, builds a Silver layer that tolerates schema drift with a minimal schema registry, evolves Silver with new fields without breaking downstream, and produces a Gold aggregator. Include (a) schema-drift detection with automatic column addition and defaults, (b) a MERGE-based CDC into Silver, (c) a 2-minute watermark for late data, (d) Unity Catalog RBAC restricting Silver/Gold and a masking view for PII, and (e) a lightweight synthetic-data test plan?","answer":"Bronze: Auto Loader from S3 Parquet. Silver uses a lightweight schema registry (registry with field_name, data_type, schema_version). On ingest, detect drift; if new columns appear, ALTER TABLE ADD CO","explanation":"## Why This Is Asked\n\nThis tests practical handling of schema drift, CDC, watermarking, and masking in a Databricks Delta Lake pipeline, plus Unity Catalog RBAC. \n\n## Key Concepts\n\n- Schema drift and light schema registry\n- Delta Lake schema evolution and ALTER TABLE\n- MERGE-based CDC into Silver\n- Watermarking for late data\n- Masking views and RBAC\n\n## Code Example\n\n```javascript\n# Pseudocode for drift detection and schema evolution\nbronze = spark.read.format(\"parquet\").load(\"s3://bucket/path\")\nincoming_schema = bronze.schema\nregistry = spark.table(\"schema_registry\").select(\"fields\").collect()\nif incoming_schema != registry.current_fields:\n    spark.sql(\"ALTER TABLE silver ADD COLUMNS (...) DEFAULT NULL\")\n```\n\n## Follow-up Questions\n\n- How would you handle non-nullable added columns? \n- How would you test schema drift with real-time data? ","diagram":null,"difficulty":"beginner","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Meta","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T11:25:51.330Z","createdAt":"2026-01-25T11:25:51.332Z"},{"id":"q-7271","question":"Ingest 40 TB/day JSON telemetry from Kafka and 20 TB/day Parquet logs from S3. Design a four-layer Lakehouse (Bronze/Silver/Gold/Analytics) where Silver deduplicates on a canonical_id, uses CDC into Silver, and enforces per-field masking on PII using Unity Catalog column-level security. Include an AuditLog capturing ingestion_ts, source, batch_id, and file_count per micro-batch. Outline testing with synthetic data, late data, and schema evolution; also discuss RBAC and lineage?","answer":"Propose Bronze from Kafka JSON and S3 Parquet, Silver with CDC MERGE on canonical_id, 15-minute watermark for late data, and per-field masking on PII via Unity Catalog column-level security. Include a","explanation":"## Why This Is Asked\nAssesses ability to architect multi-source CDC, governance, and testing in Databricks with Unity Catalog, RBAC, and lineage across banks.\n\n## Key Concepts\n- Multi-source CDC across Kafka JSON and S3 Parquet\n- Four-layer Lakehouse (Bronze/Silver/Gold/Analytics)\n- Canonical_id dedup via MERGE\n- Unity Catalog column-level masking for PII\n- AuditLog per micro-batch\n- RBAC, lineage, and data-skimming costs\n\n## Code Example\n```javascript\n// Pseudo-CDC MERGE into Silver\ndeltaTable.alias('t').merge(\n  source.alias('s'),\n  \"t.canonical_id = s.canonical_id\"\n).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n```\n\n## Follow-up Questions\n- How would you test masking at scale and verify lineage accuracy?\n- How would schema evolution affect downstream dashboards and how to mitigate?","diagram":null,"difficulty":"advanced","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Hashicorp","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T16:35:45.022Z","createdAt":"2026-01-25T16:35:45.024Z"},{"id":"q-7362","question":"In a Databricks data platform serving two tenants (Hugging Face and Meta) with separate Unity Catalog schemas, design a Delta Live Tables pipeline that ingests streaming event data from Kafka into Bronze, performs per-tenant CDC-like upserts into Silver, and materializes a tenant-scoped Gold table for model features. Enforce per-tenant isolation with RBAC and row-level security on tenant_id, maintain an AuditDelta table recording per-file ingest metadata, and implement a 3-minute watermark for late data. Outline testing with synthetic tenants and a plan to verify end-to-end lineage and cost considerations?","answer":"Implement a Delta Live Tables pipeline with two Unity Catalog schemas (tenants). Ingest Kafka streams into Bronze, upsert into Silver per (tenant_id, event_id) with a 3-minute watermark, and materiali","explanation":"## Why This Is Asked\nTests multi-tenant governance, CDC-like upserts, streaming, and lineage in a realistic Databricks setup.\n\n## Key Concepts\n- Delta Live Tables, Kafka streaming, Unity Catalog RBAC\n- Tenant isolation, row-level security, AuditDelta\n- Watermarks, lineage verification, cost considerations\n\n## Code Example\n```text\n// Not provided in interview; expect pseudo-commands for DLT, RBAC, and AuditDelta schema\n```\n\n## Follow-up Questions\n- How would you test schema evolution across Silver/Gold without breaking downstream models?\n- How would you monitor CDC latency and cost in production?","diagram":"flowchart TD\nA[Kafka Bronze] --> B[Silver per tenant CDC upserts]\nB --> C[Gold tenant features]\nA --> D[AuditDelta ingestion logs]\nC --> E[Model serving]","difficulty":"intermediate","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T19:50:20.248Z","createdAt":"2026-01-25T19:50:20.248Z"},{"id":"q-7386","question":"In a Databricks lakehouse with Bronze, Silver, Gold layers ingesting Parquet/JSON from S3 via Auto Loader, design a forward-compatibility layer that enforces Delta constraints to prevent breaking schema evolutions, adds a data-quality fence that blocks CDC MERGE on nulls or out-of-range values, and implements per-file AuditDelta; include Unity Catalog RBAC for Silver/Gold and per-file AuditDelta write privileges; give concrete steps and a lightweight synthetic test plan?","answer":"Enforce Delta constraints on Silver and Gold layers (NOT NULL for event_id, canonical_id, ingest_ts; strict type validation) to block schema evolutions that conflict with downstream marts. Implement a QualityGuard before CDC MERGE operations that rejects null values and out-of-range data, with per-file AuditDelta tables for lineage tracking. Configure Unity Catalog RBAC with principle-based privileges for Silver/Gold access and granular write permissions on AuditDelta tables.","explanation":"## Why This Is Asked\nTests mastery of Delta constraints, safe schema evolution, and data governance in a multi-layer lakehouse. Also probes RBAC setup and quality gating in CDC workflows.\n\n## Key Concepts\n- Delta constraints (NOT NULL, CHECK) to block incompatible evolutions\n- Schema evolution safety and versioned rollbacks\n- Data-quality gate before MERGE (nulls, ranges, type checks)\n- Unity Catalog RBAC for Silver/Gold and AuditDelta write privileges\n\n## Code Example\n```sql\n-- Pseudo SQL: enforce constraints on Silver/Gold\nALTER TABLE silver ADD CONSTRAINT not_null_event_id CHECK (event_id IS NOT NULL);\nALTER TABLE silver ADD CONSTRAINT valid_timestamp CHECK (ingest_ts > '2020-01-01');\n\n-- QualityGuard before CDC MERGE\nCREATE OR REPLACE TEMPORARY VIEW quality_guard AS\nSELECT * FROM bronze_staging \nWHERE event_id IS NOT NULL \n  AND canonical_id IS NOT NULL\n  AND ingest_ts IS NOT NULL\n  AND value_range BETWEEN 0 AND 1000;\n\n-- Unity Catalog RBAC setup\nCREATE ROLE silver_reader;\nCREATE ROLE gold_writer;\nGRANT SELECT ON silver TO silver_reader;\nGRANT MODIFY ON gold TO gold_writer;\nGRANT WRITE ON audit_delta TO audit_writer;\n```\n\n## Implementation Steps\n1. Define constraint policies for Silver/Gold layers\n2. Create QualityGuard validation functions\n3. Set up per-file AuditDelta tables with lineage metadata\n4. Configure Unity Catalog roles and privileges\n5. Implement synthetic test data with edge cases\n6. Validate constraint enforcement and audit trails","diagram":null,"difficulty":"advanced","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Google","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T06:56:46.142Z","createdAt":"2026-01-25T20:59:35.468Z"},{"id":"q-7557","question":"In a Databricks pipeline ingesting 100 GB/day Parquet web logs from S3 and 80 GB/day JSON telemetry from Kafka, design a Drift-Detection and Auto-Recovery Silver workflow. Use a CDC-based Silver keyed by (source, event_id). Implement a numeric-field drift detector against a baseline histogram; on drift, quarantine affected files to a DriftDelta table and rollback affected Silver partitions. Enforce Unity Catalog RBAC; propose a canary/test plan and observability in UI?","answer":"Propose a Drift-Detection and Auto-Recovery pattern: a Silver table CDC by (source, event_id); a lightweight drift detector on latency_ms compared to a stored histogram; on drift, quarantine the affec","explanation":"## Why This Is Asked\nDesign for resilience and governance in mixed batch/streaming pipelines, with drift handling and recoverability.\n\n## Key Concepts\n- Drift detection on numeric fields with baselines\n- File quarantine and partition rollback\n- CDC MERGE semantics and Delta Lake constraints\n- Unity Catalog RBAC for data and quarantine tables\n- Canary datasets and UI observability\n\n## Code Example\n```scala\n// Pseudo: compare histogram of latency_ms with baseline; flag drift if p-value < 0.01\n```\n\n## Follow-up Questions\n- How would you tune thresholds to balance false positives/negatives?\n- How would you extend to multi-tenant RBAC with per-tenant baselines?","diagram":"flowchart TD\n  Bronze[Bronze Layer] --> Silver[Silver CDC Layer]\n  Silver --> Gold[Gold Layer]\n  Silver --> Quarantine[DriftDelta (Quarantine)]\n  Quarantine --> Silver\n  RBAC[Unity Catalog RBAC] --> Silver\n  UIObserv[Observability UI] --> Lineage[Lineage & Recovery]\n","difficulty":"advanced","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Hugging Face","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T07:38:07.378Z","createdAt":"2026-01-26T07:38:07.378Z"},{"id":"q-7630","question":"In a Databricks data platform, ingest 200 TB/week Parquet logs from S3 and 50 TB/week JSON telemetry from a REST API, multi-tenant. Build a Bronze-Silver-Gold lakehouse with a canonical_id dedup via MERGE CDC, and a 20-minute watermark for late data. Add per-file AuditDelta (path, ingest_ts, file_size, source_name) and enforce Unity Catalog RBAC on Silver/AuditDelta. Enable Delta Sharing for Gold with tenant-scoped access. Outline concrete steps, data-quality rules, and a lightweight synthetic-data test plan to verify CDC, watermark, RBAC, and sharing?","answer":"Leverage Delta Lake with a 3-layer Lakehouse; Bronze ingests Parquet/JSON from S3 and REST; Silver normalizes to a canonical schema and performs MERGE CDC on (canonical_id, event_time_bucket); late da","explanation":"## Why This Is Asked\nTests end-to-end design skill for Databricks: multi-source ingestion, CDC semantics, watermarking for late data, per-file auditing, and robust governance via Unity Catalog and Delta Sharing.\n\n## Key Concepts\n- Delta Lake CDC with MERGE\n- Watermarking and late data handling\n- AuditDelta and file-level lineage\n- Unity Catalog RBAC and Delta Sharing\n- Synthetic data for validation\n\n## Code Example\n```javascript\n// No code required for the interview prompt\n```\n\n## Follow-up Questions\n- How would you handle schema evolution while preserving CDC guarantees?\n- How do you validate RBAC in automated tests?","diagram":null,"difficulty":"advanced","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["NVIDIA","Oracle","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T10:38:23.407Z","createdAt":"2026-01-26T10:38:23.408Z"},{"id":"q-7656","question":"In a Databricks data platform serving multiple SaaS tenants, design a scalable error-handling pattern for streaming and batch ingestion. Use Auto Loader to populate Bronze, Delta Live Tables to Silver/Gold, and a dedicated ErrorEvents Delta table capturing file_path, tenant_id, error_code, error_message, ingest_ts. Describe how you isolate tenants with Unity Catalog RBAC, implement per-file quarantining with a heartbeat/backpressure strategy, and validate lineage and re-ingestibility with synthetic faults?","answer":"Route ingestion errors to a dedicated ErrorEvents Delta table and quarantine failing files, while Bronze, Silver, Gold streams continue. Use Auto Loader with file-level error handling, capture tenant_","explanation":"## Why This Is Asked\nThis question probes real-world resilience: error routing, backpressure, per-tenant isolation, lineage, and re-ingestibility.\n\n## Key Concepts\n- Auto Loader file-level error handling\n- Delta Live Tables error handling and upserts\n- Unity Catalog RBAC and per-tenant isolation\n- Audit/lineage and re-ingestibility tests\n- Synthetic fault generation\n\n## Code Example\n```javascript\n// Pseudo DLDT pattern for error handling\n// This is illustrative; actual syntax uses Python/SQL in DLDT\n```\n\n## Follow-up Questions\n- How would you test late-arriving error files?\n- How would you ensure idempotent re-ingestion?","diagram":null,"difficulty":"intermediate","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T11:35:41.852Z","createdAt":"2026-01-26T11:35:41.852Z"},{"id":"q-7736","question":"In a Databricks data platform serving multiple tenants, design a streaming Bronze-Silver-Gold pipeline that ingests JSON events from a multi-tenant API gateway into Delta Lake, using Delta Live Tables (DLT) in streaming mode (not Auto Loader). Enforce per-tenant isolation with Unity Catalog RBAC and tenant_id row-level security, and record per-batch ingest metadata in an AuditTrail table. Implement a CDC-like MERGE from Bronze to Silver, support 2-minute late data with a watermark, and handle schema evolution. Propose concrete file layout, testing plan with synthetic tenants, and end-to-end lineage verification?","answer":"Design a streaming DLT pipeline: Bronze landing per-tenant JSON from API gateway; Silver with event_id dedupe and CDC-style MERGE; Gold aggregates per-tenant. Enforce Unity Catalog RBAC and tenant_id ","explanation":"## Why This Is Asked\nTests ability to design a multi-tenant streaming lakehouse with precise RBAC boundaries, lineage controls, and CDC-like semantics in DLTs.\n\n## Key Concepts\n- Multi-tenant isolation via Unity Catalog and tenant_id RLS\n- Delta Live Tables streaming pipelines and MERGE CDC\n- Watermarking for late data; schema evolution handling\n- AuditTrail for per-batch ingest metadata; lineage verification\n- Synthetic tenant testing and end-to-end checks\n\n## Code Example\n```python\nimport dlt\nimport pyspark.sql.functions as F\n\n@dlt.table\ndef Bronze_raw():\n    df = spark.read.json(\"/mnt/api-gateway/tenant_id=*/*\")\n    return df\n\n@dlt.table\ndef Silver_cdc():\n    s = dlt.read(\"Bronze_raw\").alias(\"b\")\n    # dedupe + CDC-like MERGE\n    return s\n```\n\n## Follow-up Questions\n- How would you test RBAC policy changes without affecting production data?\n- How would you validate lineage when the source evolves its schema?","diagram":"flowchart TD\n  API_Gateway --> Bronze\n  Bronze --> Silver\n  Silver --> Gold\n  Silver --> AuditTrail","difficulty":"intermediate","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Google","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T15:44:48.910Z","createdAt":"2026-01-26T15:44:48.910Z"},{"id":"q-7845","question":"In a two-tenant Databricks setup ingesting 400 GB/day Parquet clickstream from S3 and 100 GB/day JSON events from a REST API into Bronze via Auto Loader, design a Silver layer that enforces tenant isolation via Unity Catalog RBAC and a per-tenant policy, performs MERGE CDC from Bronze to Silver, adds an AuditDelta per file, and builds a Gold layer with aggregated per-tenant metrics. Provide concrete steps for RBAC, tenant-scoped policies, schema evolution handling, and a lightweight synthetic data test plan to validate CDC, isolation, and lineage. How would you approach?","answer":"Use Unity Catalog RBAC to grant Silver access per tenant and implement a row-level policy that filters by tenant_id = current_tenant(). In Silver, perform MERGE CDC from Bronze on (tenant_id, event_id","explanation":"## Why This Is Asked\nThis probes practical multi-tenant lakehouse design, per-tenant RBAC, CDC accuracy, and lineage.\n\n## Key Concepts\n- Unity Catalog RBAC and per-tenant row-level security\n- MERGE CDC from Bronze to Silver\n- per-file AuditDelta tracking and schema evolution handling\n- Gold: per-tenant aggregations with correct lineage\n\n## Code Example\n```sql\n-- Example: per-tenant policy (pseudocode)\nCREATE OR REPLACE POLICY tenant_rls AS\n  (tenant_id = current_user_tenant()) ON Silver.table;\n```\n\n## Follow-up Questions\n- How would you monitor CDC latency and RBAC drift?\n- How to handle new tenants without redeploying policies?","diagram":null,"difficulty":"advanced","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Discord"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T19:51:13.936Z","createdAt":"2026-01-26T19:51:13.936Z"},{"id":"q-7876","question":"Design a DR strategy for a multi-region SaaS data platform (Airbnb/Stripe-scale) using Databricks Bronze-Silver-Gold with Delta Live Tables. Outline cross-region replication from Region A to Region B, per-tenant isolation via Unity Catalog RBAC, a global truth table for late-arriving data, and CDC-style reconciliation. Include testing with synthetic tenants and how to verify lineage during failover?","answer":"Adopt active DR with Region A as primary and Region B as secondary. Mirror Bronze in Region B using cross-region Auto Loader and a DR-enabled DLT pipeline; enforce per-tenant isolation via Unity Catal","explanation":"## Why This Is Asked\nThis question probes DR design, cross-region data consistency, tenant isolation, and lineage verification in a Databricks lakehouse.\n\n## Key Concepts\n- Cross-region replication with Auto Loader and DLT\n- Unity Catalog RBAC for per-tenant isolation\n- Global truth table for late-arriving data\n- CDC-style reconciliation with MERGE\n- Lineage verification and testing\n\n## Code Example\n```python\n# Pseudo-Delta Live Tables setup for DR\nimport dlt\n@dlt.table\ndef bronze_region_a():\n    ...\n\n@dlt.table\ndef bronze_region_b():\n    ...\n```\n\n## Follow-up Questions\n- How would you monitor replication latency and alerting?\n- How to test end-to-end failover with synthetic tenants?","diagram":null,"difficulty":"intermediate","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T21:33:26.352Z","createdAt":"2026-01-26T21:33:26.352Z"},{"id":"q-906","question":"Scenario: A daily Delta Live Tables (DLT) pipeline ingests clickstream JSONs into a Bronze Delta table and then enriches with a users_dim to a Silver table. Build a beginner-friendly pipeline that: deduplicates by event_id, validates user_id via users_dim, filters to business hours 08:00‚Äì18:00, enriches with user fields, and writes to Silver partitioned by event_date. Outline minimal steps and rationale?","answer":"Proposed approach: Use a DLT pipeline with Bronze ingest from JSON, then Silver derived from Bronze. Deduplicate on event_id, keeping the latest by ingest_time. Join to users_dim on user_id to validat","explanation":"## Why This Is Asked\n\nTests ability to design an end-to-end DL pipeline focused on data quality, deduplication, enrichment, and partitioned storage for performance.\n\n## Key Concepts\n\n- Delta Live Tables basics and table dependencies\n- Deduplication by event_id using window or primary-key approaches\n- Referential integrity via dimension lookups during enrichment\n- Time-based filtering for business hours\n- Partitioning Silver by event_date for efficient queries\n\n## Code Example\n\n```javascript\n// Pseudo-DLT sketch (not runnable)\nBronze = read_json('s3://bucket/clicks/bronze/')\nSilver = Bronze\n  .dropDuplicates(['event_id'])\n  .join(users_dim, Bronze.user_id == users_dim.user_id, 'left')\n  .filter(hour(Bronze.event_timestamp) >= 8 and hour(Bronze.event_timestamp) <= 18)\n  .withColumn('event_date', to_date(Bronze.event_timestamp))\n  .writePartitionBy('event_date')\n```\n\n## Follow-up Questions\n\n- How would you handle late-arriving events or backfills in this pipeline?\n- What tests would you add to validate deduplication and the user_id join behavior?","diagram":null,"difficulty":"beginner","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Netflix","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T14:44:28.732Z","createdAt":"2026-01-12T14:44:28.732Z"}],"subChannels":["general"],"companies":["Adobe","Airbnb","Amazon","Anthropic","Apple","Bloomberg","Citadel","Cloudflare","Coinbase","Databricks","Discord","DoorDash","Goldman Sachs","Google","Hashicorp","Hugging Face","IBM","Instacart","LinkedIn","Lyft","Meta","Microsoft","MongoDB","NVIDIA","Netflix","OpenAI","Oracle","PayPal","Plaid","Robinhood","Salesforce","Scale Ai","Slack","Snap","Snowflake","Square","Stripe","Tesla","Twitter","Two Sigma","Uber","Zoom"],"stats":{"total":88,"beginner":26,"intermediate":33,"advanced":29,"newThisWeek":37}}