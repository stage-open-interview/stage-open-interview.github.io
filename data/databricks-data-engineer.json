{"questions":[{"id":"q-1100","question":"In a Databricks data pipeline, ingest JSON events from S3 into Delta Lake with Auto Loader. Each event has a nested user object (id, email) and an optional pages array. Some events miss user.email. How would you design the pipeline to safely flatten nested fields, handle missing values, and upsert the latest user state into a Delta Silver table using an idempotent MERGE?","answer":"Use Auto Loader with schema evolution, flatten nested fields, and a MERGE for idempotent upserts. ReadStream via cloudFiles, inferColumnTypes true, mergeSchema true. Flatten: user_id = user.id, email ","explanation":"## Why This Is Asked\n\nTests practical mastery of ingesting semi-structured data, flattening nested JSON, handling optional fields, and performing idempotent upserts in Delta Lake using Databricks primitives.\n\n## Key Concepts\n\n- Auto Loader with cloudFiles options for JSON with schema evolution\n- Flattening nested structs and handling missing fields safely\n- Upserts with MERGE to ensure idempotent state in Delta Lake\n- Null handling with COALESCE and robust array expansion (explode_outer)\n\n## Code Example\n\n```javascript\n// Databricks PySpark-like sketch (syntax-highlighted as javascript)\nval df = spark.readStream.format(\"cloudFiles\")\n  .option(\"cloudFiles.format\",\"json\")\n  .option(\"cloudFiles.inferColumnTypes\",\"true\")\n  .option(\"cloudFiles.mergeSchema\",\"true\")\n  .load(\"s3a://bucket/events/\")\n\nval flat = df.selectExpr(\"user.id as user_id\",\n                       \"coalesce(user.email, '') as email\",\n                       \"ts as event_ts\",\n                       \"explode_outer(pages) as page\")\n```\n\n## Follow-up Questions\n\n- How would you test this pipeline end-to-end, including schema evolution scenarios?\n- How would you handle new nested fields added to user in future deployments?","diagram":null,"difficulty":"beginner","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T22:31:52.482Z","createdAt":"2026-01-12T22:31:52.482Z"},{"id":"q-1112","question":"Scenario: In a Databricks Delta Live Tables pipeline, ingest JSON events from a Kafka source into a Bronze table, where nested fields drift over time (e.g., payload.user.locale is added later, some events miss payload.user). You then transform to a Silver table used by billing and marketing. How would you implement a robust schema-evolution strategy and gating so new fields are captured without breaking existing downstream queries, and how would you test this in a run?","answer":"Enable schema evolution in DLT by merging new fields into Bronze and keeping downstream schema backward compatible; treat optional nested fields as nullable and use defaults; gate with non-null expect","explanation":"## Why This Is Asked\nTests practical schema evolution and data quality gating in a real Delta Live Tables pipeline, focusing on drift in nested JSON and downstream stability.\n\n## Key Concepts\n- Delta Live Tables schema evolution\n- Nested JSON drift handling\n- Downstream backward compatibility\n- Data quality gates/expectations\n- End-to-end testing with mixed schemas\n\n## Code Example\n```python\n# PySpark-like sketch for Bronze -> Silver\nbronze = spark.readStream.format(\"cloudFiles\").option(\"cloudFiles.format\",\"json\").load(\"/bronze\")\nsilver = bronze.selectExpr(\"payload.user.id as user_id\", \"payload.user.locale as user_locale\", \"payload.action as action\", \"timestamp\")\nsilver = silver.withColumn(\"user_locale\", F.coalesce(col(\"user_locale\"), F.lit(\"unknown\")))\n```\n\n## Follow-up Questions\n- How would you roll schema changes from experimental to production with minimal downtime?\n- How would you monitor for schema drift in production and trigger alerts?","diagram":null,"difficulty":"intermediate","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Microsoft","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T23:23:17.885Z","createdAt":"2026-01-12T23:23:17.885Z"},{"id":"q-1144","question":"In a Databricks streaming pipeline ingesting tenant-scoped events from Kafka into Delta Lake, events may arrive late by up to 10 minutes. Describe a concrete end-to-end approach to upsert the latest state per (tenant_id, user_id) into a Silver table while preserving the full event history. Include: data model, CDC logic, watermarking and late data handling, an idempotent MERGE strategy, schema evolution handling, and governance considerations with Unity Catalog RBAC and data lineage?","answer":"Use Bronze→Silver CDC pattern with Delta Lake. Silver holds latest state per (tenant_id, user_id); History preserves all changes. Use a 10-minute watermark to bound late data; MERGE INTO Silver WHEN M","explanation":"## Why This Is Asked\nThis tests practical CDC design, late-arrival handling, and Delta Lake upserts in a multi-tenant setting.\n\n## Key Concepts\n- Delta Lake MERGE for upserts\n- Watermarks and late data handling in streaming\n- Bronze-Silver-History data modeling\n- Unity Catalog RBAC and data lineage\n\n## Code Example\n```javascript\n-- Spark SQL (illustrative)\nMERGE INTO silver_latest AS s\nUSING bronze AS b\nON s.tenant_id = b.tenant_id AND s.user_id = b.user_id\nWHEN MATCHED THEN UPDATE SET\n  s.state = b.state,\n  s.last_updated = current_timestamp(),\n  s.version = b.version\nWHEN NOT MATCHED THEN INSERT (tenant_id, user_id, state, last_updated, version)\nVALUES (b.tenant_id, b.user_id, b.state, current_timestamp(), b.version);\n```\n\n## Follow-up Questions\n- How would you validate late-data handling under bursty traffic?\n- How would you monitor and alert on data-skew and drift across tenants?","diagram":null,"difficulty":"advanced","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Cloudflare","Databricks"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T01:30:54.604Z","createdAt":"2026-01-13T01:30:54.604Z"},{"id":"q-1182","question":"In a multi-tenant Databricks lakehouse ingesting per-tenant telemetry from Kafka and S3 into a unified Silver Delta table, describe an end-to-end CDC strategy to implement SCD2-like history per tenant with idempotent MERGE, handle late data with a watermark, and enforce governance through Unity Catalog RBAC and dynamic PII masking. Include data model, CDC logic, testing plan, and how you’d validate lineage?","answer":"Design a CDC-driven SCD2 history per tenant in a Delta table. Ingest via Delta Live Tables into a Silver table partitioned by tenant_id and timestamp. On MERGE, close previous history rows by setting ","explanation":"## Why This Is Asked\n\nTests the ability to design a CDC-based SCD2 history in a multi-tenant lakehouse with governance and masking requirements, using Delta Live Tables and MERGE semantics.\n\n## Key Concepts\n\n- CDC and SCD2 patterns in Delta Lake\n- Delta Live Tables and MERGE-based upserts\n- Watermarking and late-data handling\n- Unity Catalog RBAC and dynamic data masking\n- Data lineage and governance with Delta sharing\n\n## Code Example\n\n```sql\nMERGE INTO silver AS tgt\nUSING staging AS src\nON tgt.tenant_id = src.tenant_id AND tgt.key = src.key\nWHEN MATCHED AND (src.attributes_hash <> tgt.attributes_hash OR src.timestamp <> tgt.end_date) THEN\n  UPDATE SET end_date = current_timestamp()\nWHEN NOT MATCHED THEN\n  INSERT (tenant_id, key, attributes_hash, start_date, end_date)\n  VALUES (src.tenant_id, src.key, src.attributes_hash, current_timestamp(), NULL)\n```\n\n## Follow-up Questions\n\n- How would you test idempotency of MERGE under replayed CDC events?\n- How would you implement per-tenant masking policies in Unity Catalog and verify lineage against dashboards?","diagram":null,"difficulty":"advanced","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Google","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T03:43:29.150Z","createdAt":"2026-01-13T03:43:29.150Z"},{"id":"q-1226","question":"In a Databricks Delta Live Tables pipeline that ingests clickstream events from a cloud bucket into a Delta table, data quality checks are defined via expectations. A batch arrives with corrupted event_time values that would fail the run. Outline a concrete approach to quarantine bad data, feed downstream tables only from valid rows, and store invalids for audit, while still handling late data and deduplication?","answer":"Create a staging raw_events table, apply a light validation to tag rows as valid/invalid, store invalid rows in invalid_events, and feed downstream clean_events from valid rows only. Use a watermark o","explanation":"## Why This Is Asked\nDemonstrates practical, production-ready handling of partial data quality failures in a Delta Live Tables pipeline, balancing correctness with availability.\n\n## Key Concepts\n- Staging vs downstream separation\n- Expectations-based quality checks\n- Quarantine and audit of invalid data\n- Late data handling with watermarking\n- Deduplication strategies across micro-batches\n\n## Code Example\n```javascript\nimport dlt\nimport pyspark.sql.functions as F\n\n@dlt.table\ndef raw_events():\n  return spark.read.format(\"cloudFiles\").option(\"cloudFiles.format\",\"json\").load(\"/mnt/events/raw\")\n\n@dlt.table\ndef valid_events():\n  df = dlt.read(\"raw_events\")\n  return df.filter(F.col(\"event_time\").isNotNull() & F.col(\"user_id\").isNotNull())\n\n@dlt.table\ndef invalid_events():\n  raw = dlt.read(\"raw_events\")\n  valid_ids = dlt.read(\"valid_events\").select(\"event_id\").distinct()\n  return raw.join(valid_ids, \"event_id\", \"left_anti\")\n\n@dlt.table\ndef clean_events():\n  return dlt.read(\"valid_events\")\n```\n\n## Follow-up Questions\n- How would you monitor and alert if invalid_events growth spikes?\n- What changes would you make to support reprocessing of invalid data after fixes?","diagram":"flowchart TD\n  A[Ingest Raw Events] --> B[DLT Validation Tagging]\n  B --> C{Validity}\n  C -->|Valid| D[Feed Clean Events]\n  C -->|Invalid| E[Store Invalid Events]\n  D --> F[Analytics & BI]\n  E --> G[Audit & Reprocessing]\n  H[Late Data via Watermark] --> B","difficulty":"intermediate","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T05:38:03.397Z","createdAt":"2026-01-13T05:38:03.397Z"},{"id":"q-1264","question":"In a Databricks job ingesting daily Parquet files from S3 into Delta Lake, a new field 'campaign_id' may appear in newer files while older ones do not. Describe a concrete, beginner-friendly approach to ingest without data loss, allowing downstream joins, and handling the schema change gracefully. Include how to enable Delta schema evolution (mergeSchema/autoMerge), define a compatible target schema, and implement a minimal validation to drop rows missing essential fields?","answer":"Enable Delta schema evolution by using mergeSchema on write and enabling auto-merge: spark.conf.set('spark.databricks.delta.schema.autoMerge.enabled','true'); df.write.format('delta').option('mergeSch","explanation":"## Why This Is Asked\n\nTests understanding of practical schema evolution in Delta Lake during basic batch ingestion and how to keep data accessible for joins.\n\n## Key Concepts\n\n- Delta Lake schema evolution\n- mergeSchema and autoMerge\n- Nullability and backward compatibility\n- Data quality validation\n\n## Code Example\n\n```javascript\n// Ingestion example with schema evolution\ndf.write.format(\"delta\").option(\"mergeSchema\",\"true\").mode(\"append\").save(\"/delta/user_events\")\n```\n\n## Follow-up Questions\n\n- How would you validate that campaign_id is correctly populated in downstream queries?\n- How would you backfill the historical table to reflect a new campaigns dimension without downtime?","diagram":null,"difficulty":"beginner","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Coinbase","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T07:31:13.696Z","createdAt":"2026-01-13T07:31:13.696Z"},{"id":"q-1320","question":"You're designing a Delta Live Tables workflow ingesting data from Kafka and an S3 landing zone, with a downstream customer dimension in Delta Lake that uses SCD Type 2. How would you implement idempotent MERGE-based upserts, handle schema drift, and preserve late-arriving data while auditing invalid events and ensuring downstream BI reads only current rows?","answer":"Implement Bronze→Silver→Gold: dedupe by customer_id with max(event_time) in Silver; allowMissingFields for schema drift. In Gold, MERGE into customer_dim: on match with changes, set old row valid_to a","explanation":"## Why This Is Asked\nTests practical mastery of Delta Live Tables patterns: CDC, SCD2, and schema drift handling in production-like streams.\n\n## Key Concepts\n- SCD Type 2 implementation via MERGE\n- Schema drift resilience with safe evolution\n- Late data handling using watermarks and auditing\n- Data lineage and is_current filtering for dashboards\n\n## Code Example\n```sql\nMERGE INTO gold.customer_dim AS t\nUSING silver.customer_stage AS s\nON t.customer_id = s.customer_id AND t.is_current = true\nWHEN MATCHED AND (t.name <> s.name OR t.email <> s.email) THEN\n  UPDATE SET t.valid_to = current_timestamp(), t.is_current = false\nWHEN NOT MATCHED THEN\n  INSERT (customer_id, name, email, valid_from, valid_to, is_current)\n  VALUES (s.customer_id, s.name, s.email, current_timestamp(), NULL, true);\n```\n\n## Follow-up Questions\n- How would you test idempotency across pipeline restarts?\n- How would you validate schema drift without failing runs?\n- How would you structure audits to avoid data loss during failures?","diagram":null,"difficulty":"intermediate","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["NVIDIA","Robinhood","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T11:32:27.379Z","createdAt":"2026-01-13T11:32:27.379Z"},{"id":"q-1375","question":"In a Databricks streaming job, a Kafka topic emits JSON events for many tenants. The payload schema drifts with new fields; you want a stable Silver Delta table with a canonical schema and history. Describe a beginner-friendly approach to map events to the canonical schema, handle new fields without breaking downstream joins, and perform an idempotent MERGE into Silver by (tenant_id, event_id). Include a concrete mapping rule set and a small MERGE example?","answer":"Read Bronze with a permissive schema; map to a canonical Silver schema: tenant_id, event_id, event_ts, and payload (struct). Unknown fields go into payload to avoid drift. Use a MERGE into Silver on (","explanation":"## Why This Is Asked\nTests practical skill in handling schema drift and idempotent upserts in streaming Databricks pipelines.\n\n## Key Concepts\n- Schema drift and canonical schema design\n- Struct payload encoding\n- Idempotent MERGE pattern\n- Streaming boundaries and data quality\n\n## Code Example\n```javascript\nMERGE INTO Silver AS s\nUSING (SELECT tenant_id, event_id, event_ts, payload FROM Bronze) AS b\nON s.tenant_id = b.tenant_id AND s.event_id = b.event_id\nWHEN MATCHED THEN UPDATE SET event_ts = b.event_ts, payload = b.payload\nWHEN NOT MATCHED THEN INSERT (tenant_id, event_id, event_ts, payload) VALUES (b.tenant_id, b.event_id, b.event_ts, b.payload);\n```\n\n## Follow-up Questions\n- How would you test duplicate avoidance and drift handling end-to-end?\n- How would you evolve the canonical schema as new tenant-specific fields appear?","diagram":null,"difficulty":"beginner","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","Oracle","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T14:38:09.108Z","createdAt":"2026-01-13T14:38:09.108Z"},{"id":"q-1409","question":"Describe an end-to-end approach for a fraud-detection streaming pipeline using two Kafka topics (transactions, account_updates): Bronze ingest, join to a versioned SCD2 customer_dim, compute risk in Silver, upsert via MERGE with a deterministic key, watermark late data, handle schema drift with Delta Lake evolution, and enforce governance with Unity Catalog RBAC and lineage. Include testing with synthetic late-arriving data?","answer":"Ingest two Kafka topics into Bronze, join to a versioned SCD2 customer_dim, compute per-event risk in Silver, and upsert into Silver with MERGE on tx_id. Use watermarking for late data, enable Delta L","explanation":"## Why This Is Asked\nAssesses ability to design multi-source streaming with versioned references, robust late-data handling, idempotent upserts, schema evolution, and governance.\n\n## Key Concepts\n- Multi-source streaming from Kafka into Bronze\n- Slowly changing dimension (SCD2) for customer_dim\n- Idempotent MERGE semantics on deterministic keys\n- Watermarking and late data handling\n- Delta Lake schema evolution for drift\n- Unity Catalog RBAC and data lineage\n\n## Code Example\n```javascript\n// Pseudo-sample: MERGE into Silver risk table from Bronze transactions\nspark.sql(`MERGE INTO silver_risk AS s\nUSING bronze_transactions AS b\nON s.tx_id = b.tx_id\nWHEN MATCHED THEN UPDATE SET s.score = b.score, s.updated_at = current_timestamp()\nWHEN NOT MATCHED THEN INSERT (tx_id, score, updated_at) VALUES (b.tx_id, b.score, current_timestamp())`)\n```\n\n## Follow-up Questions\n- How would you test end-to-end with synthetic late-arriving data?\n- What metrics would you monitor to detect backpressure or data skew in this design?","diagram":null,"difficulty":"advanced","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","Scale Ai","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T15:50:17.661Z","createdAt":"2026-01-13T15:50:17.661Z"},{"id":"q-1451","question":"In a Databricks notebook, you need to join a 10M-row Delta Lake 'customers' table with a 10k-row 'segments' reference table to produce a daily marketing audience feed. How would you implement an efficient join strategy in Spark/Delta to minimize shuffle and cost, ensure correctness if segments update, and maintain lineage? Include: join type and hints, caching strategy, refresh cadence, and where to store results?","answer":"Use a broadcast join and caching. Load the 10M-row fact table as df_big and the 10k-row segments as df_small, then join with F.broadcast(df_small) on customer_id. Cache df_big for the daily run. Write","explanation":"## Why This Is Asked\nTests practical Spark optimization and Delta Lake usage for daily pipelines, focusing on join strategy, caching, and governance.\n\n## Key Concepts\n- Broadcast joins and data skew management\n- Caching large DataFrames for repeated reads\n- Delta Lake partitioning for incremental writes\n- Upserts with MERGE and change logging\n- Lineage and RBAC in governance\n\n## Code Example\n```python\nfrom pyspark.sql import functions as F\n\ndf_big = spark.read.format(\"delta\").table(\"db.customers\")\ndf_small = spark.read.format(\"delta\").table(\"db.segments\")\n\ndf_enriched = df_big.join(F.broadcast(df_small), \"customer_id\", \"left\")\ndf_enriched = df_enriched.cache()\n\ndf_enriched.write.format(\"delta\").mode(\"append\").partitionBy(\"order_date\").saveAsTable(\"db.marketing_events_enriched\")\n```\n\n## Follow-up Questions\n- How would you handle frequent updates to the segments table during the day?\n- How would you quantify and reduce shuffle cost in this pipeline?","diagram":"flowchart TD\n  A[Read big delta table] --> B[Read small segments table]\n  B --> C[Broadcast join A and B on customer_id]\n  C --> D[Cache enriched df]\n  D --> E[Write to Delta table partitioned by date]\n  E --> F[Register lineage in Unity Catalog]","difficulty":"beginner","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Google","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T17:46:52.463Z","createdAt":"2026-01-13T17:46:52.463Z"},{"id":"q-1540","question":"In a Databricks streaming pipeline ingesting order events from Kafka into Delta Lake, implement a scalable SCD Type 2 for a customer_dim table to preserve history while handling late-arriving updates up to 15 minutes. Describe the data model (bronze/silver), CDC logic using MERGE, watermarking, and schema evolution, plus Unity Catalog RBAC and lineage considerations. Include a minimal code sketch of the MERGE closing an old row and inserting a new version?","answer":"Design a Delta Live Tables pipeline with a bronze Kafka stream feeding a silver customer_dim SCD2. Implement CDC via a MERGE that closes the active row (set end_date, current=false) and inserts a new row with start_date and current=true. Use watermarking for 15-minute late data handling, enable schema evolution, and implement Unity Catalog RBAC with lineage tracking.","explanation":"## Why This Is Asked\nTests mastery of real-world data governance and slowly changing dimensions in streaming pipelines on Databricks.\n\n## Key Concepts\n- Delta Live Tables, MERGE-based CDC, SCD2, watermarking for late data, schema evolution, Unity Catalog RBAC, data lineage.\n\n## Code Example\n```sql\nMERGE INTO silver.customer_dim AS t\nUSING staging.customer_dim AS s\nON t.customer_id = s.customer_id AND t.current = true\nWHEN MATCHED THEN UPDATE SET t.end_date = CURRENT_TIMESTAMP, t.current = false\nWHEN NOT MATCHED THEN INSERT (customer_id, name, address, start_date, end_date, current)\nVALUES (s.customer_id, s.name, s.address, CURRENT_TIMESTAMP, NULL, true)\n```","diagram":null,"difficulty":"advanced","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Hugging Face"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T06:32:33.205Z","createdAt":"2026-01-13T20:55:12.361Z"},{"id":"q-1555","question":"In a Databricks pipeline ingesting 20 TB/day of Parquet logs on S3 into Delta Lake, design a practical optimization plan to improve read latency for near-real-time dashboards using Delta features like OPTIMIZE, ZORDER, Data Skipping, caching, and Photon. Discuss partitioning strategy, trade-offs, and how you'd validate gains with benchmark metrics?","answer":"Leverage OPTIMIZE with ZORDER on frequently filtered columns (date, user_id), enable Data Skipping, and maintain hot partitions in memory cache. Reevaluate daily versus hourly partitioning to balance compaction overhead against query performance, and validate improvements through comprehensive benchmark metrics.","explanation":"## Why This Is Asked\nThis question evaluates practical optimization strategies for Databricks pipelines handling large-scale Delta Lake workloads, requiring balance between read latency, cost efficiency, and operational maintainability.\n\n## Key Concepts\n- Delta Lake OPTIMIZE and ZORDER for optimal data layout\n- Data skipping and Photon-accelerated query processing\n- Strategic partitioning and intelligent caching\n- Maintenance trade-offs: vacuum operations, compaction, write amplification\n- Performance metrics: query runtime, bytes scanned, cache hit rates, cost per query\n\n## Code Example\n```sql","diagram":null,"difficulty":"intermediate","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","IBM","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T06:25:12.266Z","createdAt":"2026-01-13T21:41:51.915Z"},{"id":"q-1643","question":"In Databricks, ingest streaming data from Kafka into Delta Lake for 10k IoT devices emitting multiple sensor types (temperature, humidity, pressure). Build a Silver table with the latest per-device per-sensor-type state while preserving full history. The source schema will evolve (new sensors added, some removed). Outline a robust end-to-end approach: data model, CDC/Upsert logic, watermarking for late data, schema evolution strategy, idempotent MERGE, and governance with Unity Catalog RBAC and lineage. Include concrete examples of Bronze->Silver handoff and a dynamic per-tenant view?","answer":"Use a Silver table keyed by (device_id, sensor_type) storing the latest value per key, while preserving full history in a separate history mechanism. Upsert Bronze to Silver via MERGE with a 2-minute ","explanation":"## Why This Is Asked\nTests end-to-end thinking: streaming CDC, evolving schemas, and governance in a high-cardinality IoT pipeline.\n\n## Key Concepts\n- Streaming CDC with MERGE into Delta Lake Silver\n- Handling schema evolution (auto-evolve) for new sensors\n- Watermarking and late data management\n- Modeling latest state vs full history\n- Unity Catalog RBAC and lineage for per‑tenant views\n\n## Code Example\n```javascript\n-- Bronze to Silver MERGE (pseudo-SQL)\nMERGE INTO silver_sensor AS s\nUSING batch_view AS b\nON s.device_id = b.device_id AND s.sensor_type = b.sensor_type\nWHEN MATCHED THEN UPDATE SET\n  s.value = b.value,\n  s.ts = b.ts\nWHEN NOT MATCHED THEN INSERT (device_id, sensor_type, value, ts) VALUES (b.device_id, b.sensor_type, b.value, b.ts);\n```\n\n```javascript\n-- Streaming pipeline skeleton with watermark (pseudo-Python)\nbronze = spark.readStream.format('kafka')... \nbronze = bronze.selectExpr('CAST(value AS STRING) as json', 'timestamp')\n# parse and write to Bronze Delta, then MERGE to Silver in micro-batches with a watermark\n```\n\n## Follow-up Questions\n- How would you validate correctness under high late-arrival rates and schema drift?\n- How would you implement per-tenant data access using dynamic views and RBAC without leaking data across tenants?","diagram":null,"difficulty":"advanced","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T04:29:04.600Z","createdAt":"2026-01-14T04:29:04.601Z"},{"id":"q-1658","question":"Design a secure external data sharing workflow in a Databricks environment using Unity Catalog and Delta Sharing to expose aggregated metrics derived from production Delta tables to external partners while maintaining tenant isolation and governance. Include data model, masking strategy, per partner quotas, refresh cadence, and how to monitor and revoke access?","answer":"Use a Delta Sharing producer dataset based on a Silver view (tenant_id, metric, value) with masking on sensitive fields. Publish as a Delta Share to a consumer role with read access only. Enforce per ","explanation":"## Why This Is Asked\nThis tests Delta Sharing, Unity Catalog governance, masking, quotas, and revocation in real-world partner data sharing.\n\n## Key Concepts\n- Delta Sharing producer/consumer model\n- Unity Catalog RBAC and quotas\n- Dynamic data masking policies\n- Data freshness and incremental refresh\n- Audit logs and compliance\n\n## Code Example\n```sql\n-- Create share and grant access to partner\nCREATE SHARE partner_agg_share;\nALTER SHARE partner_agg_share ADD CONSUMER 'partner_catalog' WITH 'us-east-1';\nGRANT SELECT ON database.shared_view TO SHARE partner_agg_share;\n```\n\n## Follow-up Questions\n- How would you handle schema drift in the shared view without breaking consumers?\n- What monitoring metrics indicate misuse or quota breaches?","diagram":null,"difficulty":"advanced","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Oracle","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T05:41:50.396Z","createdAt":"2026-01-14T05:41:50.397Z"},{"id":"q-1683","question":"Design a multi-tenant, Databricks-based data pipeline ingesting 1 TB/day of JSON events from Kafka into Delta Lake. Tenants share storage but must be completely isolated; dashboards must mask PII fields per-tenant. Propose an end-to-end pattern using Unity Catalog RBAC, dynamic data masking, Delta Live Tables, and Photon-enabled reads. Include data model, masking rules, handling schema evolution, and how you validate governance and lineage?","answer":"Three-layer lakehouse: Bronze (Kafka → Delta), Silver (per-tenant masking via Unity Catalog dynamic masking with tenant scoping), Gold (dashboards + features). Enforce RBAC, use DLT with schema evolut","explanation":"Why This Is Asked\n- Tests multi-tenant governance, masking, and lineage in a real Databricks setup. \n- Evaluates practical use of Unity Catalog RBAC, dynamic masking, and DLT for evolving schemas. \n- Probes performance considerations with Photon and partitioning strategies.\n\nKey Concepts\n- Unity Catalog RBAC and access controls\n- Dynamic data masking for PII per tenant\n- Delta Live Tables for reliable, incremental transforms\n- Photon-accelerated reads and Delta caching\n- Schema evolution handling and time travel for audits\n- Data lineage and governance validation\n\nCode Example\n```sql\n-- Pseudo example: define a masking policy and apply to Silver table\nCREATE MASKING POLICY tenant_masking AS (tenant_id STRING) RETURNS STRING ->\n  CASE WHEN CURRENT_USER() = tenant_id THEN tenant_id ELSE 'REDACTED' END;\nALTER TABLE silver APPLY MASKING tenant_masking ON (tenant_id);\n```\n\nFollow-up Questions\n- How would you test for cross-tenant data leakage during schema evolution?\n- Which metrics and dashboards would you instrument to monitor governance and lineage health across tenants?","diagram":null,"difficulty":"advanced","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","Netflix","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T06:55:17.136Z","createdAt":"2026-01-14T06:55:17.136Z"},{"id":"q-1721","question":"Databricks beginner scenario: A streaming pipeline reads 2 TB/month of JSON events from S3 via Autoloader into Delta Lake. A downstream dashboard shows active_users by hour. Late events arrive up to 10 minutes. Design a practical plan to maintain accurate hourly active_user counts with minimal duplication, including: schema/partitioning for streaming, watermark-based late data handling, an idempotent MERGE into a Silver table, and a simple end-to-end test using synthetic late events. Also outline monitoring steps?","answer":"Implement an Autoloader Bronze layer, upsert hourly aggregates into Silver with a MERGE on (user_id, hour). Use a 10-minute watermark for late data; partition Silver by hour; ensure idempotence with a","explanation":"## Why This Is Asked\n\nTests practical use of streaming with late data, Delta MERGE upserts, and end-to-end validation in a beginner-friendly context.\n\n## Key Concepts\n\n- Autoloader and Delta Lake Bronze-Silver pattern\n- Watermarks and late data handling in structured streaming\n- Idempotent MERGE-based upserts for counts\n- End-to-end testing with synthetic late events\n- Monitoring latency and data quality\n\n## Code Example\n\n```sql\n-- MERGE into Silver hourly active_users\nMERGE INTO silver.active_users AS s\nUSING (\n  SELECT user_id,\n         date_trunc('hour', event_time) AS hour,\n         COUNT(*) AS cnt\n  FROM bronze.events\n  GROUP BY user_id, date_trunc('hour', event_time)\n) AS b\nON s.user_id = b.user_id AND s.hour = b.hour\nWHEN MATCHED THEN UPDATE SET s.count = b.cnt\nWHEN NOT MATCHED THEN INSERT (user_id, hour, count) VALUES (b.user_id, b.hour, b.cnt)\n```\n\n## Follow-up Questions\n\n- How would you adjust for bursty late data or skewed user activity?\n- What metrics would you monitor to ensure SLA compliance for dashboards?","diagram":"flowchart TD\n  S[Source: S3 Autoloader] --> B[Bronze Layer]\n  B --> S2[Silver Layer with MERGE]\n  S2 --> D[Dashboards]\n  D --> M[Monitoring + Alerts]","difficulty":"beginner","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Instacart","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T07:55:55.291Z","createdAt":"2026-01-14T07:55:55.291Z"},{"id":"q-906","question":"Scenario: A daily Delta Live Tables (DLT) pipeline ingests clickstream JSONs into a Bronze Delta table and then enriches with a users_dim to a Silver table. Build a beginner-friendly pipeline that: deduplicates by event_id, validates user_id via users_dim, filters to business hours 08:00–18:00, enriches with user fields, and writes to Silver partitioned by event_date. Outline minimal steps and rationale?","answer":"Proposed approach: Use a DLT pipeline with Bronze ingest from JSON, then Silver derived from Bronze. Deduplicate on event_id, keeping the latest by ingest_time. Join to users_dim on user_id to validat","explanation":"## Why This Is Asked\n\nTests ability to design an end-to-end DL pipeline focused on data quality, deduplication, enrichment, and partitioned storage for performance.\n\n## Key Concepts\n\n- Delta Live Tables basics and table dependencies\n- Deduplication by event_id using window or primary-key approaches\n- Referential integrity via dimension lookups during enrichment\n- Time-based filtering for business hours\n- Partitioning Silver by event_date for efficient queries\n\n## Code Example\n\n```javascript\n// Pseudo-DLT sketch (not runnable)\nBronze = read_json('s3://bucket/clicks/bronze/')\nSilver = Bronze\n  .dropDuplicates(['event_id'])\n  .join(users_dim, Bronze.user_id == users_dim.user_id, 'left')\n  .filter(hour(Bronze.event_timestamp) >= 8 and hour(Bronze.event_timestamp) <= 18)\n  .withColumn('event_date', to_date(Bronze.event_timestamp))\n  .writePartitionBy('event_date')\n```\n\n## Follow-up Questions\n\n- How would you handle late-arriving events or backfills in this pipeline?\n- What tests would you add to validate deduplication and the user_id join behavior?","diagram":null,"difficulty":"beginner","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Netflix","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T14:44:28.732Z","createdAt":"2026-01-12T14:44:28.732Z"}],"subChannels":["general"],"companies":["Adobe","Airbnb","Amazon","Anthropic","Apple","Cloudflare","Coinbase","Databricks","Discord","DoorDash","Google","Hashicorp","Hugging Face","IBM","Instacart","Lyft","Microsoft","MongoDB","NVIDIA","Netflix","OpenAI","Oracle","Plaid","Robinhood","Scale Ai","Snap","Snowflake","Square","Twitter","Two Sigma","Uber","Zoom"],"stats":{"total":17,"beginner":6,"intermediate":4,"advanced":7,"newThisWeek":17}}