{"questions":[{"id":"databricks-data-engineer-elt-spark-1768169996346-0","question":"In a Databricks Delta Lake ELT pipeline, a Delta table db.sales is frequently queried with filters on region and sale_date. The dataset is large and files are small, causing slow scans. Which of the following changes would most improve query performance for these filters?","answer":"[{\"id\":\"a\",\"text\":\"Run OPTIMIZE delta.`db.sales` ZORDER BY (region, sale_date)\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Partition the table by region at write time to improve filtering\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Cache the entire table in memory before running queries\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Convert to a single large Parquet file per partition to reduce file fragmentation\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption a is correct. OPTIMIZE with ZORDER BY co-locates data based on frequently filtered columns (region, sale_date), reducing data scanned for common predicates and speeding up queries on large Delta tables.\n\n## Why Other Options Are Wrong\n- Option b: Rewriting partitioning after the fact is non-trivial for large datasets and does not guarantee improved performance for the common region/date predicates; ZORDER targets data layout for efficient pruning.\n- Option c: Caching a very large table yields diminishing returns and can exhaust cluster memory, offering transient benefits at best.\n- Option d: Creating a single large Parquet file per partition reduces parallelism and generally hurts read performance for distributed engines.\n\n## Key Concepts\n- Delta Lake OPTIMIZE\n- ZORDER BY for data locality\n\n## Real-World Application\n- Improves latency for frequent region/date range filters on large fact tables.\n```sql\nOPTIMIZE delta.`db.sales` ZORDER BY (region, sale_date)\n```\n","diagram":null,"difficulty":"intermediate","tags":["Delta-Lake","Spark-SQL","Python","ETL","Databricks","AWS-S3","certification-mcq","domain-weight-29"],"channel":"databricks-data-engineer","subChannel":"elt-spark","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T22:19:56.347Z","createdAt":"2026-01-11 22:19:56"},{"id":"databricks-data-engineer-elt-spark-1768169996346-1","question":"During an upsert from a staging table stg_sales to a Delta table dim_sales, you need to deduplicate on sale_id and apply inserts/updates idempotently. Which approach ensures ACID upserts and preserves history?","answer":"[{\"id\":\"a\",\"text\":\"Use a Spark SQL join and overwrite the target table\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use Delta Lake MERGE statement to upsert from staging into target\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Use INSERT with DISTINCT and append\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use REPLACE TABLE with the staging data\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption b is correct. The Delta MERGE statement provides ACID upserts, handles insert/update semantics in a single atomic operation, and preserves the transaction log/history, making it ideal for deduping on a natural key like sale_id.\n\n## Why Other Options Are Wrong\n- Option a: Upserting via a join and overwrite can lose row-level history and is not atomic for concurrent writers.\n- Option c: INSERT with DISTINCT does not handle updates to existing rows and can miss deduplication if staging contains updated rows for existing keys.\n- Option d: REPLACE TABLE discards the previous table metadata and history, breaking ACID guarantees.\n\n## Key Concepts\n- Delta Lake MERGE for upserts\n- ACID transactions and transaction log\n\n## Real-World Application\n- Safely applying daily upserts from a staging layer into a production dimension/fact table.\n```sql\nMERGE INTO delta.`db.dim_sales` AS t\nUSING delta.`db.stg_sales` AS s\nON t.sale_id = s.sale_id\nWHEN MATCHED THEN UPDATE SET t.amount = s.amount, t.sale_date = s.sale_date\nWHEN NOT MATCHED THEN INSERT (sale_id, amount, sale_date) VALUES (s.sale_id, s.amount, s.sale_date);\n```\n","diagram":null,"difficulty":"intermediate","tags":["Delta-Lake","Spark-SQL","Python","ETL","Databricks","AWS-S3","certification-mcq","domain-weight-29"],"channel":"databricks-data-engineer","subChannel":"elt-spark","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T22:19:56.832Z","createdAt":"2026-01-11 22:19:57"},{"id":"databricks-data-engineer-elt-spark-1768169996346-2","question":"A daily Python-based ETL load ingests JSON payloads that introduce a new field customer_segment not present in the existing Delta table schema. You want to automatically evolve the schema on write. Which write option enables this behavior?","answer":"[{\"id\":\"a\",\"text\":\"Disable schema enforcement and append to the table\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Write with option mergeSchema=true to enable schema evolution\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Pre-scan files and run ALTER TABLE to add new columns before writing\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Fail the job if a new field is detected\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption b is correct. Setting mergeSchema=true enables Delta Lake to evolve the schema on write, allowing new fields like customer_segment to be added automatically during append operations.\n\n## Why Other Options Are Wrong\n- Option a: Disabling schema enforcement can lead to inconsistent schemas and runtime errors when reading.\n- Option c: Pre-scanning and altering the schema before every write adds complexity and delay; schema evolution should be automatic.\n- Option d: Failing on new fields halts the pipeline and prevents automatic data ingestion.\n\n## Key Concepts\n- Delta Lake schema evolution\n- mergeSchema option in DataFrameWriter\n\n## Real-World Application\n- Ingesting semi-structured JSON that evolves over time without manual schema changes.\n```python\ndf.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").save(\"/mnt/delta/dim_sales\")\n```\n","diagram":null,"difficulty":"intermediate","tags":["Delta-Lake","Spark-SQL","Python","ETL","Databricks","AWS-S3","certification-mcq","domain-weight-29"],"channel":"databricks-data-engineer","subChannel":"elt-spark","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T22:19:57.298Z","createdAt":"2026-01-11 22:19:57"},{"id":"databricks-data-engineer-lakehouse-arch-1768210219223-0","question":"In Delta Live Tables, you want to enforce data quality on a dataset of employees by ensuring age is between 0 and 120 and that department_id exists in the departments dimension before loading into the Silver layer. Which mechanism should you use to declaratively enforce these rules within the pipeline?","answer":"[{\"id\":\"a\",\"text\":\"Use a Python UDF with a filter to drop bad rows.\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use a try/except around the write operation to skip invalid rows.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Define 'expect' constraints on the DLT table definitions.\",\"isCorrect\":true},{\"id\":\"d\",\"text\":\"Move quality checks into a separate nightly job.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption C: Define 'expect' constraints on the DLT table definitions.\n\nDLT provides built-in data quality constraints called expects that let you declare validation rules as part of the pipeline. This makes quality checks auditable, repeatable, and tied to the data lineage.\n\n## Why Other Options Are Wrong\n- Option A: Using a Python UDF with a filter to drop bad rows is ad-hoc and bypasses structured quality definitions, making it harder to maintain and monitor.\n- Option B: try/except handling may drop or skip bad rows unpredictably and doesn't enforce ongoing quality guarantees.\n- Option D: A separate nightly job delays enforcement and breaks end-to-end data quality guarantees.\n\n## Key Concepts\n- Delta Live Tables expects constraints for data quality\n- Declarative data validation integrates with lineage and monitoring\n\n## Real-World Application\n- In an HR data pipeline, enforce age 0-120 and department_id validity via DLT expects to ensure downstream analytics always receive clean Silver layer data.","diagram":null,"difficulty":"intermediate","tags":["Databricks","Delta-Live-Tables","Auto Loader","Unity Catalog","AWS","S3","Kubernetes","Terraform","certification-mcq","domain-weight-24"],"channel":"databricks-data-engineer","subChannel":"lakehouse-arch","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T09:30:19.226Z","createdAt":"2026-01-12 09:30:19"},{"id":"databricks-data-engineer-lakehouse-arch-1768210219223-1","question":"You ingest JSON event data into Delta Lake using Auto Loader; new fields are added over time. Which Auto Loader setting enables automatic schema evolution by merging new columns into the target schema?","answer":"[{\"id\":\"a\",\"text\":\"cloudFiles.schemaEvolutionMode = mergeSchema\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"cloudFiles.inferColumnTypes = true\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"cloudFiles.schemaEvolutionMode = addNewColumns\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"spark.databricks.delta.autoMerge.enabled = true\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A: cloudFiles.schemaEvolutionMode = mergeSchema.\n\nThis Auto Loader setting enables schema evolution by merging new fields from the source into the target Delta table, avoiding a full table rewrite when new columns appear.\n\n## Why Other Options Are Wrong\n- Option B: inferColumnTypes may infer types but does not control schema evolution for new columns.\n- Option C: addNewColumns can add columns but does not guarantee schema merging behavior in all cases.\n- Option D: Delta auto-merge is not the mechanism for Auto Loader schema evolution.\n\n## Key Concepts\n- Auto Loader schema evolution\n- Merging new columns into Delta schema\n\n## Real-World Application\n- Ingesting evolving JSON event logs without manual schema updates; the schema evolves automatically as new fields appear.","diagram":null,"difficulty":"intermediate","tags":["Databricks","Auto Loader","Delta-Lake","AWS","S3","Terraform","certification-mcq","domain-weight-24"],"channel":"databricks-data-engineer","subChannel":"lakehouse-arch","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T09:30:19.798Z","createdAt":"2026-01-12 09:30:20"},{"id":"databricks-data-engineer-lakehouse-arch-1768210219223-2","question":"You want to restrict access to a Delta table by region in a multi-team environment using Databricks Unity Catalog. What is the recommended scalable approach to implement row-level access without duplicating data?","answer":"[{\"id\":\"a\",\"text\":\"Enable built-in row-level security in Delta Lake.\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Create a view with a region predicate and grant access to the view instead of the base table.\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Create separate tables per region and grant access to the appropriate copies.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Hard-code region filters in every SQL/BI query.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B: Create a view with a region predicate and grant access to the view instead of the base table.\n\nUnity Catalog does not provide a native per-row security feature; using views with embedded predicates is the scalable pattern to enforce row-level access while keeping a single source of truth.\n\n## Why Other Options Are Wrong\n- Option A: Delta Lake does not offer built-in per-row RLS; this would require custom workarounds.\n- Option C: Duplicating data is not scalable and increases storage and maintenance overhead.\n- Option D: Hard-coding filters in queries is brittle and does not enforce security consistently.\n\n## Key Concepts\n- Unity Catalog access control via views\n- Row-level access via predicate-based views\n\n## Real-World Application\n- Restricting customer data by region for regional sales teams without data duplication.","diagram":null,"difficulty":"intermediate","tags":["Databricks","Unity Catalog","RLS","Delta-Lake","AWS","S3","Kubernetes","Terraform","certification-mcq","domain-weight-24"],"channel":"databricks-data-engineer","subChannel":"lakehouse-arch","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T09:30:20.318Z","createdAt":"2026-01-12 09:30:20"}],"subChannels":["elt-spark","lakehouse-arch"],"companies":[],"stats":{"total":6,"beginner":0,"intermediate":6,"advanced":0,"newThisWeek":6}}