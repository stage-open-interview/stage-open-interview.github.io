{"questions":[{"id":"q-1100","question":"In a Databricks data pipeline, ingest JSON events from S3 into Delta Lake with Auto Loader. Each event has a nested user object (id, email) and an optional pages array. Some events miss user.email. How would you design the pipeline to safely flatten nested fields, handle missing values, and upsert the latest user state into a Delta Silver table using an idempotent MERGE?","answer":"Use Auto Loader with schema evolution, flatten nested fields, and a MERGE for idempotent upserts. ReadStream via cloudFiles, inferColumnTypes true, mergeSchema true. Flatten: user_id = user.id, email ","explanation":"## Why This Is Asked\n\nTests practical mastery of ingesting semi-structured data, flattening nested JSON, handling optional fields, and performing idempotent upserts in Delta Lake using Databricks primitives.\n\n## Key Concepts\n\n- Auto Loader with cloudFiles options for JSON with schema evolution\n- Flattening nested structs and handling missing fields safely\n- Upserts with MERGE to ensure idempotent state in Delta Lake\n- Null handling with COALESCE and robust array expansion (explode_outer)\n\n## Code Example\n\n```javascript\n// Databricks PySpark-like sketch (syntax-highlighted as javascript)\nval df = spark.readStream.format(\"cloudFiles\")\n  .option(\"cloudFiles.format\",\"json\")\n  .option(\"cloudFiles.inferColumnTypes\",\"true\")\n  .option(\"cloudFiles.mergeSchema\",\"true\")\n  .load(\"s3a://bucket/events/\")\n\nval flat = df.selectExpr(\"user.id as user_id\",\n                       \"coalesce(user.email, '') as email\",\n                       \"ts as event_ts\",\n                       \"explode_outer(pages) as page\")\n```\n\n## Follow-up Questions\n\n- How would you test this pipeline end-to-end, including schema evolution scenarios?\n- How would you handle new nested fields added to user in future deployments?","diagram":null,"difficulty":"beginner","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T22:31:52.482Z","createdAt":"2026-01-12T22:31:52.482Z"},{"id":"q-1112","question":"Scenario: In a Databricks Delta Live Tables pipeline, ingest JSON events from a Kafka source into a Bronze table, where nested fields drift over time (e.g., payload.user.locale is added later, some events miss payload.user). You then transform to a Silver table used by billing and marketing. How would you implement a robust schema-evolution strategy and gating so new fields are captured without breaking existing downstream queries, and how would you test this in a run?","answer":"Enable schema evolution in DLT by merging new fields into Bronze and keeping downstream schema backward compatible; treat optional nested fields as nullable and use defaults; gate with non-null expect","explanation":"## Why This Is Asked\nTests practical schema evolution and data quality gating in a real Delta Live Tables pipeline, focusing on drift in nested JSON and downstream stability.\n\n## Key Concepts\n- Delta Live Tables schema evolution\n- Nested JSON drift handling\n- Downstream backward compatibility\n- Data quality gates/expectations\n- End-to-end testing with mixed schemas\n\n## Code Example\n```python\n# PySpark-like sketch for Bronze -> Silver\nbronze = spark.readStream.format(\"cloudFiles\").option(\"cloudFiles.format\",\"json\").load(\"/bronze\")\nsilver = bronze.selectExpr(\"payload.user.id as user_id\", \"payload.user.locale as user_locale\", \"payload.action as action\", \"timestamp\")\nsilver = silver.withColumn(\"user_locale\", F.coalesce(col(\"user_locale\"), F.lit(\"unknown\")))\n```\n\n## Follow-up Questions\n- How would you roll schema changes from experimental to production with minimal downtime?\n- How would you monitor for schema drift in production and trigger alerts?","diagram":null,"difficulty":"intermediate","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Microsoft","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T23:23:17.885Z","createdAt":"2026-01-12T23:23:17.885Z"},{"id":"q-1144","question":"In a Databricks streaming pipeline ingesting tenant-scoped events from Kafka into Delta Lake, events may arrive late by up to 10 minutes. Describe a concrete end-to-end approach to upsert the latest state per (tenant_id, user_id) into a Silver table while preserving the full event history. Include: data model, CDC logic, watermarking and late data handling, an idempotent MERGE strategy, schema evolution handling, and governance considerations with Unity Catalog RBAC and data lineage?","answer":"Use Bronze→Silver CDC pattern with Delta Lake. Silver holds latest state per (tenant_id, user_id); History preserves all changes. Use a 10-minute watermark to bound late data; MERGE INTO Silver WHEN M","explanation":"## Why This Is Asked\nThis tests practical CDC design, late-arrival handling, and Delta Lake upserts in a multi-tenant setting.\n\n## Key Concepts\n- Delta Lake MERGE for upserts\n- Watermarks and late data handling in streaming\n- Bronze-Silver-History data modeling\n- Unity Catalog RBAC and data lineage\n\n## Code Example\n```javascript\n-- Spark SQL (illustrative)\nMERGE INTO silver_latest AS s\nUSING bronze AS b\nON s.tenant_id = b.tenant_id AND s.user_id = b.user_id\nWHEN MATCHED THEN UPDATE SET\n  s.state = b.state,\n  s.last_updated = current_timestamp(),\n  s.version = b.version\nWHEN NOT MATCHED THEN INSERT (tenant_id, user_id, state, last_updated, version)\nVALUES (b.tenant_id, b.user_id, b.state, current_timestamp(), b.version);\n```\n\n## Follow-up Questions\n- How would you validate late-data handling under bursty traffic?\n- How would you monitor and alert on data-skew and drift across tenants?","diagram":null,"difficulty":"advanced","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Cloudflare","Databricks"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T01:30:54.604Z","createdAt":"2026-01-13T01:30:54.604Z"},{"id":"q-1182","question":"In a multi-tenant Databricks lakehouse ingesting per-tenant telemetry from Kafka and S3 into a unified Silver Delta table, describe an end-to-end CDC strategy to implement SCD2-like history per tenant with idempotent MERGE, handle late data with a watermark, and enforce governance through Unity Catalog RBAC and dynamic PII masking. Include data model, CDC logic, testing plan, and how you’d validate lineage?","answer":"Design a CDC-driven SCD2 history per tenant in a Delta table. Ingest via Delta Live Tables into a Silver table partitioned by tenant_id and timestamp. On MERGE, close previous history rows by setting ","explanation":"## Why This Is Asked\n\nTests the ability to design a CDC-based SCD2 history in a multi-tenant lakehouse with governance and masking requirements, using Delta Live Tables and MERGE semantics.\n\n## Key Concepts\n\n- CDC and SCD2 patterns in Delta Lake\n- Delta Live Tables and MERGE-based upserts\n- Watermarking and late-data handling\n- Unity Catalog RBAC and dynamic data masking\n- Data lineage and governance with Delta sharing\n\n## Code Example\n\n```sql\nMERGE INTO silver AS tgt\nUSING staging AS src\nON tgt.tenant_id = src.tenant_id AND tgt.key = src.key\nWHEN MATCHED AND (src.attributes_hash <> tgt.attributes_hash OR src.timestamp <> tgt.end_date) THEN\n  UPDATE SET end_date = current_timestamp()\nWHEN NOT MATCHED THEN\n  INSERT (tenant_id, key, attributes_hash, start_date, end_date)\n  VALUES (src.tenant_id, src.key, src.attributes_hash, current_timestamp(), NULL)\n```\n\n## Follow-up Questions\n\n- How would you test idempotency of MERGE under replayed CDC events?\n- How would you implement per-tenant masking policies in Unity Catalog and verify lineage against dashboards?","diagram":null,"difficulty":"advanced","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Google","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T03:43:29.150Z","createdAt":"2026-01-13T03:43:29.150Z"},{"id":"q-1226","question":"In a Databricks Delta Live Tables pipeline that ingests clickstream events from a cloud bucket into a Delta table, data quality checks are defined via expectations. A batch arrives with corrupted event_time values that would fail the run. Outline a concrete approach to quarantine bad data, feed downstream tables only from valid rows, and store invalids for audit, while still handling late data and deduplication?","answer":"Create a staging raw_events table, apply a light validation to tag rows as valid/invalid, store invalid rows in invalid_events, and feed downstream clean_events from valid rows only. Use a watermark o","explanation":"## Why This Is Asked\nDemonstrates practical, production-ready handling of partial data quality failures in a Delta Live Tables pipeline, balancing correctness with availability.\n\n## Key Concepts\n- Staging vs downstream separation\n- Expectations-based quality checks\n- Quarantine and audit of invalid data\n- Late data handling with watermarking\n- Deduplication strategies across micro-batches\n\n## Code Example\n```javascript\nimport dlt\nimport pyspark.sql.functions as F\n\n@dlt.table\ndef raw_events():\n  return spark.read.format(\"cloudFiles\").option(\"cloudFiles.format\",\"json\").load(\"/mnt/events/raw\")\n\n@dlt.table\ndef valid_events():\n  df = dlt.read(\"raw_events\")\n  return df.filter(F.col(\"event_time\").isNotNull() & F.col(\"user_id\").isNotNull())\n\n@dlt.table\ndef invalid_events():\n  raw = dlt.read(\"raw_events\")\n  valid_ids = dlt.read(\"valid_events\").select(\"event_id\").distinct()\n  return raw.join(valid_ids, \"event_id\", \"left_anti\")\n\n@dlt.table\ndef clean_events():\n  return dlt.read(\"valid_events\")\n```\n\n## Follow-up Questions\n- How would you monitor and alert if invalid_events growth spikes?\n- What changes would you make to support reprocessing of invalid data after fixes?","diagram":"flowchart TD\n  A[Ingest Raw Events] --> B[DLT Validation Tagging]\n  B --> C{Validity}\n  C -->|Valid| D[Feed Clean Events]\n  C -->|Invalid| E[Store Invalid Events]\n  D --> F[Analytics & BI]\n  E --> G[Audit & Reprocessing]\n  H[Late Data via Watermark] --> B","difficulty":"intermediate","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T05:38:03.397Z","createdAt":"2026-01-13T05:38:03.397Z"},{"id":"q-1264","question":"In a Databricks job ingesting daily Parquet files from S3 into Delta Lake, a new field 'campaign_id' may appear in newer files while older ones do not. Describe a concrete, beginner-friendly approach to ingest without data loss, allowing downstream joins, and handling the schema change gracefully. Include how to enable Delta schema evolution (mergeSchema/autoMerge), define a compatible target schema, and implement a minimal validation to drop rows missing essential fields?","answer":"Enable Delta schema evolution by using mergeSchema on write and enabling auto-merge: spark.conf.set('spark.databricks.delta.schema.autoMerge.enabled','true'); df.write.format('delta').option('mergeSch","explanation":"## Why This Is Asked\n\nTests understanding of practical schema evolution in Delta Lake during basic batch ingestion and how to keep data accessible for joins.\n\n## Key Concepts\n\n- Delta Lake schema evolution\n- mergeSchema and autoMerge\n- Nullability and backward compatibility\n- Data quality validation\n\n## Code Example\n\n```javascript\n// Ingestion example with schema evolution\ndf.write.format(\"delta\").option(\"mergeSchema\",\"true\").mode(\"append\").save(\"/delta/user_events\")\n```\n\n## Follow-up Questions\n\n- How would you validate that campaign_id is correctly populated in downstream queries?\n- How would you backfill the historical table to reflect a new campaigns dimension without downtime?","diagram":null,"difficulty":"beginner","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Coinbase","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T07:31:13.696Z","createdAt":"2026-01-13T07:31:13.696Z"},{"id":"q-906","question":"Scenario: A daily Delta Live Tables (DLT) pipeline ingests clickstream JSONs into a Bronze Delta table and then enriches with a users_dim to a Silver table. Build a beginner-friendly pipeline that: deduplicates by event_id, validates user_id via users_dim, filters to business hours 08:00–18:00, enriches with user fields, and writes to Silver partitioned by event_date. Outline minimal steps and rationale?","answer":"Proposed approach: Use a DLT pipeline with Bronze ingest from JSON, then Silver derived from Bronze. Deduplicate on event_id, keeping the latest by ingest_time. Join to users_dim on user_id to validat","explanation":"## Why This Is Asked\n\nTests ability to design an end-to-end DL pipeline focused on data quality, deduplication, enrichment, and partitioned storage for performance.\n\n## Key Concepts\n\n- Delta Live Tables basics and table dependencies\n- Deduplication by event_id using window or primary-key approaches\n- Referential integrity via dimension lookups during enrichment\n- Time-based filtering for business hours\n- Partitioning Silver by event_date for efficient queries\n\n## Code Example\n\n```javascript\n// Pseudo-DLT sketch (not runnable)\nBronze = read_json('s3://bucket/clicks/bronze/')\nSilver = Bronze\n  .dropDuplicates(['event_id'])\n  .join(users_dim, Bronze.user_id == users_dim.user_id, 'left')\n  .filter(hour(Bronze.event_timestamp) >= 8 and hour(Bronze.event_timestamp) <= 18)\n  .withColumn('event_date', to_date(Bronze.event_timestamp))\n  .writePartitionBy('event_date')\n```\n\n## Follow-up Questions\n\n- How would you handle late-arriving events or backfills in this pipeline?\n- What tests would you add to validate deduplication and the user_id join behavior?","diagram":null,"difficulty":"beginner","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Netflix","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T14:44:28.732Z","createdAt":"2026-01-12T14:44:28.732Z"}],"subChannels":["general"],"companies":["Airbnb","Anthropic","Apple","Cloudflare","Coinbase","Databricks","Discord","DoorDash","Google","Hashicorp","Microsoft","MongoDB","Netflix","Oracle","Scale Ai","Snap","Snowflake","Uber","Zoom"],"stats":{"total":7,"beginner":3,"intermediate":2,"advanced":2,"newThisWeek":7}}