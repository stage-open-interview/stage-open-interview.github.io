{"questions":[{"id":"databricks-data-engineer-elt-spark-1768169996346-0","question":"In a Databricks Delta Lake ELT pipeline, a Delta table db.sales is frequently queried with filters on region and sale_date. The dataset is large and files are small, causing slow scans. Which of the following changes would most improve query performance for these filters?","answer":"[{\"id\":\"a\",\"text\":\"Run OPTIMIZE delta.`db.sales` ZORDER BY (region, sale_date)\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Partition the table by region at write time to improve filtering\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Cache the entire table in memory before running queries\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Convert to a single large Parquet file per partition to reduce file fragmentation\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption a is correct. OPTIMIZE with ZORDER BY co-locates data based on frequently filtered columns (region, sale_date), reducing data scanned for common predicates and speeding up queries on large Delta tables.\n\n## Why Other Options Are Wrong\n- Option b: Rewriting partitioning after the fact is non-trivial for large datasets and does not guarantee improved performance for the common region/date predicates; ZORDER targets data layout for efficient pruning.\n- Option c: Caching a very large table yields diminishing returns and can exhaust cluster memory, offering transient benefits at best.\n- Option d: Creating a single large Parquet file per partition reduces parallelism and generally hurts read performance for distributed engines.\n\n## Key Concepts\n- Delta Lake OPTIMIZE\n- ZORDER BY for data locality\n\n## Real-World Application\n- Improves latency for frequent region/date range filters on large fact tables.\n```sql\nOPTIMIZE delta.`db.sales` ZORDER BY (region, sale_date)\n```\n","diagram":null,"difficulty":"intermediate","tags":["Delta-Lake","Spark-SQL","Python","ETL","Databricks","AWS-S3","certification-mcq","domain-weight-29"],"channel":"databricks-data-engineer","subChannel":"elt-spark","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T22:19:56.347Z","createdAt":"2026-01-11 22:19:56"},{"id":"databricks-data-engineer-elt-spark-1768169996346-1","question":"During an upsert from a staging table stg_sales to a Delta table dim_sales, you need to deduplicate on sale_id and apply inserts/updates idempotently. Which approach ensures ACID upserts and preserves history?","answer":"[{\"id\":\"a\",\"text\":\"Use a Spark SQL join and overwrite the target table\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use Delta Lake MERGE statement to upsert from staging into target\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Use INSERT with DISTINCT and append\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use REPLACE TABLE with the staging data\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption b is correct. The Delta MERGE statement provides ACID upserts, handles insert/update semantics in a single atomic operation, and preserves the transaction log/history, making it ideal for deduping on a natural key like sale_id.\n\n## Why Other Options Are Wrong\n- Option a: Upserting via a join and overwrite can lose row-level history and is not atomic for concurrent writers.\n- Option c: INSERT with DISTINCT does not handle updates to existing rows and can miss deduplication if staging contains updated rows for existing keys.\n- Option d: REPLACE TABLE discards the previous table metadata and history, breaking ACID guarantees.\n\n## Key Concepts\n- Delta Lake MERGE for upserts\n- ACID transactions and transaction log\n\n## Real-World Application\n- Safely applying daily upserts from a staging layer into a production dimension/fact table.\n```sql\nMERGE INTO delta.`db.dim_sales` AS t\nUSING delta.`db.stg_sales` AS s\nON t.sale_id = s.sale_id\nWHEN MATCHED THEN UPDATE SET t.amount = s.amount, t.sale_date = s.sale_date\nWHEN NOT MATCHED THEN INSERT (sale_id, amount, sale_date) VALUES (s.sale_id, s.amount, s.sale_date);\n```\n","diagram":null,"difficulty":"intermediate","tags":["Delta-Lake","Spark-SQL","Python","ETL","Databricks","AWS-S3","certification-mcq","domain-weight-29"],"channel":"databricks-data-engineer","subChannel":"elt-spark","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T22:19:56.832Z","createdAt":"2026-01-11 22:19:57"},{"id":"databricks-data-engineer-elt-spark-1768169996346-2","question":"A daily Python-based ETL load ingests JSON payloads that introduce a new field customer_segment not present in the existing Delta table schema. You want to automatically evolve the schema on write. Which write option enables this behavior?","answer":"[{\"id\":\"a\",\"text\":\"Disable schema enforcement and append to the table\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Write with option mergeSchema=true to enable schema evolution\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Pre-scan files and run ALTER TABLE to add new columns before writing\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Fail the job if a new field is detected\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption b is correct. Setting mergeSchema=true enables Delta Lake to evolve the schema on write, allowing new fields like customer_segment to be added automatically during append operations.\n\n## Why Other Options Are Wrong\n- Option a: Disabling schema enforcement can lead to inconsistent schemas and runtime errors when reading.\n- Option c: Pre-scanning and altering the schema before every write adds complexity and delay; schema evolution should be automatic.\n- Option d: Failing on new fields halts the pipeline and prevents automatic data ingestion.\n\n## Key Concepts\n- Delta Lake schema evolution\n- mergeSchema option in DataFrameWriter\n\n## Real-World Application\n- Ingesting semi-structured JSON that evolves over time without manual schema changes.\n```python\ndf.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").save(\"/mnt/delta/dim_sales\")\n```\n","diagram":null,"difficulty":"intermediate","tags":["Delta-Lake","Spark-SQL","Python","ETL","Databricks","AWS-S3","certification-mcq","domain-weight-29"],"channel":"databricks-data-engineer","subChannel":"elt-spark","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T22:19:57.298Z","createdAt":"2026-01-11 22:19:57"},{"id":"q-906","question":"Scenario: A daily Delta Live Tables (DLT) pipeline ingests clickstream JSONs into a Bronze Delta table and then enriches with a users_dim to a Silver table. Build a beginner-friendly pipeline that: deduplicates by event_id, validates user_id via users_dim, filters to business hours 08:00â€“18:00, enriches with user fields, and writes to Silver partitioned by event_date. Outline minimal steps and rationale?","answer":"Proposed approach: Use a DLT pipeline with Bronze ingest from JSON, then Silver derived from Bronze. Deduplicate on event_id, keeping the latest by ingest_time. Join to users_dim on user_id to validat","explanation":"## Why This Is Asked\n\nTests ability to design an end-to-end DL pipeline focused on data quality, deduplication, enrichment, and partitioned storage for performance.\n\n## Key Concepts\n\n- Delta Live Tables basics and table dependencies\n- Deduplication by event_id using window or primary-key approaches\n- Referential integrity via dimension lookups during enrichment\n- Time-based filtering for business hours\n- Partitioning Silver by event_date for efficient queries\n\n## Code Example\n\n```javascript\n// Pseudo-DLT sketch (not runnable)\nBronze = read_json('s3://bucket/clicks/bronze/')\nSilver = Bronze\n  .dropDuplicates(['event_id'])\n  .join(users_dim, Bronze.user_id == users_dim.user_id, 'left')\n  .filter(hour(Bronze.event_timestamp) >= 8 and hour(Bronze.event_timestamp) <= 18)\n  .withColumn('event_date', to_date(Bronze.event_timestamp))\n  .writePartitionBy('event_date')\n```\n\n## Follow-up Questions\n\n- How would you handle late-arriving events or backfills in this pipeline?\n- What tests would you add to validate deduplication and the user_id join behavior?","diagram":null,"difficulty":"beginner","tags":["databricks-data-engineer"],"channel":"databricks-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Netflix","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T14:44:28.732Z","createdAt":"2026-01-12T14:44:28.732Z"},{"id":"databricks-data-engineer-incremental-processing-1768225886749-0","question":"In a Databricks Delta Lake incremental ETL scenario, you stream data from Kafka into a Delta table with a primary key on id. You need to upsert each micro-batch into the target table and handle late arriving updates efficiently. Which approach is recommended?","answer":"[{\"id\":\"a\",\"text\":\"writeStream with append mode to a new Delta location\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"foreachBatch with MERGE INTO target_table USING updates ON t.id = s.id WHEN MATCHED THEN UPDATE SET ... WHEN NOT MATCHED THEN INSERT ...\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"perform a daily batch upsert by running MERGE on the last 24h data\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"a static batch job that overwrites the target Delta table\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe recommended approach is to use foreachBatch to perform a MERGE INTO on the target Delta table, upserting the incoming micro-batch by its primary key. This pattern preserves incremental state and minimizes window latency, while correctly applying inserts and updates in a scalable way.\n\n## Why Other Options Are Wrong\n- Option A: Appending to a new Delta location does not upsert into the existing table and will create duplicates.\n- Option C: Upserting on a daily cadence undermines the real-time incremental goal and increases latency.\n- Option D: Overwriting the target discards existing data and breaks incremental semantics.\n\n## Key Concepts\n- Delta Lake MERGE for upserts\n- Structured Streaming with foreachBatch\n- Exactly-once semantics in micro-batch pipelines\n- Handling late data via deterministic upsert logic\n\n## Real-World Application\n- Use case: streaming real-time orders from Kafka that require upserts to keep the current order state up-to-date without duplicates.","diagram":null,"difficulty":"intermediate","tags":["Delta Lake","Apache Spark","Databricks","Kafka","AWS S3","Kubernetes","Terraform","Delta-Live-Tables","CDC","certification-mcq","domain-weight-22"],"channel":"databricks-data-engineer","subChannel":"incremental-processing","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:51:26.750Z","createdAt":"2026-01-12 13:51:27"},{"id":"databricks-data-engineer-incremental-processing-1768225886749-1","question":"During incremental data ingestion, a new source occasionally adds new columns not present in the target Delta table. You want Delta to automatically evolve the schema without failing the write. Which option enables this behavior?","answer":"[{\"id\":\"a\",\"text\":\"Disable strict schema enforcement on the Delta table\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Set option mergeSchema = true on the write/merge path\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Manually alter the Delta table to add columns before writes\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Cast all incoming fields to the existing schema only\",\"isCorrect\":false}]","explanation":"## Correct Answer\nSet option mergeSchema = true on the write/merge path to enable automatic schema evolution when new columns appear in incoming data. This allows Delta Lake to adapt without failing writes.\n\n## Why Other Options Are Wrong\n- Option A: There is no generic switch called 'strict schema enforcement'; manually disabling is not standard practice and can have risks.\n- Option C: Manual schema changes are brittle and slow for streaming schemas.\n- Option D: Casting everything constrains evolution and loses new data.\n\n## Key Concepts\n- Delta Lake schema evolution\n- mergeSchema option\n- Ingestion of evolving JSON/Parquet schemas\n\n## Real-World Application\n- Ingesting JSON logs where new fields appear over time; automatically adapt without downtime.","diagram":null,"difficulty":"intermediate","tags":["Delta Lake","Apache Spark","Databricks","Schema Evolution","ETL","Kubernetes","certification-mcq","domain-weight-22"],"channel":"databricks-data-engineer","subChannel":"incremental-processing","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:51:27.571Z","createdAt":"2026-01-12 13:51:27"},{"id":"databricks-data-engineer-incremental-processing-1768225886749-2","question":"You are streaming events with an event_time field and you want to bound stateful memory and drop late events beyond a certain threshold. Which technique ensures progress and bounded state?","answer":"[{\"id\":\"a\",\"text\":\"Apply withWatermark on event_time with a defined delay\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Use dropDuplicates on id\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Enable checkpointing only\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Disable watermarking and rely on micro-batch intervals\",\"isCorrect\":false}]","explanation":"## Correct Answer\nApply withWatermark on the event_time column with a defined delay to bound state and drop late data beyond the allowed lateness, ensuring the streaming query progresses.\n\n## Why Other Options Are Wrong\n- Option B: Deduplication doesn't bound state or control late data arrival.\n- Option C: Checkpointing tracks progress but does not bound state.\n- Option D: Removing watermarking eliminates a key control that allows bounded memory.\n\n## Key Concepts\n- Watermarking in Structured Streaming\n- Event-time processing\n- State pruning for bounded streams\n\n## Real-World Application\n- IoT telemetry with late-arriving packets; ensures timely results.","diagram":null,"difficulty":"intermediate","tags":["Apache Spark","Databricks","Structured Streaming","Watermark","Kafka","AWS-S3","Kubernetes","certification-mcq","domain-weight-22"],"channel":"databricks-data-engineer","subChannel":"incremental-processing","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:51:28.092Z","createdAt":"2026-01-12 13:51:28"},{"id":"databricks-data-engineer-incremental-processing-1768225886749-3","question":"For large-scale incremental data pipelines with data quality checks and dependency modeling, which Databricks feature helps manage incremental processing and quality expectations?","answer":"[{\"id\":\"a\",\"text\":\"Delta TableTime Travel\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Delta Live Tables (DLT) with expectations\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Autoloader with incremental ingestion\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Spark Structured Streaming\",\"isCorrect\":false}]","explanation":"## Correct Answer\nDelta Live Tables (DLT) provides a managed pipeline experience with built-in data quality checks (expectations) and incremental processing, ideal for production-grade data engineering tasks.\n\n## Why Other Options Are Wrong\n- Option A: Time Travel helps revert to previous table versions, not pipeline quality management.\n- Option C: Autoloader focuses on ingestion rather than end-to-end pipeline quality checks.\n- Option D: Spark Structured Streaming is the engine, but lacks the declarative quality/CI-like pipeline management of DLT.\n\n## Key Concepts\n- Delta Live Tables\n- Data quality with expectations\n- Incremental data processing\n\n## Real-World Application\n- Building a reliable data pipeline with automated quality gates.","diagram":null,"difficulty":"intermediate","tags":["Delta Live Tables","Databricks","Data Quality","ETL","CI/CD","Kubernetes","certification-mcq","domain-weight-22"],"channel":"databricks-data-engineer","subChannel":"incremental-processing","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:51:28.293Z","createdAt":"2026-01-12 13:51:28"},{"id":"databricks-data-engineer-incremental-processing-1768225886749-4","question":"In a multi-source streaming ingestion scenario with delete Tombstone messages, which approach ensures deletes are applied to the Delta table while preserving existing records?","answer":"[{\"id\":\"a\",\"text\":\"Use MERGE with an is_deleted flag and DELETE WHEN MATCHED AND source.is_deleted = true\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Ignore tombstones and only insert new rows\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Overwrite the Delta table on every batch\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use a bulk UPDATE to set all rows as deleted\",\"isCorrect\":false}]","explanation":"## Correct Answer\nUse MERGE with an is_deleted flag and DELETE WHEN MATCHED AND source.is_deleted = true to apply deletes while preserving existing non-deleted records.\n\n## Why Other Options Are Wrong\n- Option A is correct; Option B ignores deletes, causing data to accumulate. Option C overwrites and loses history. Option D would affect all rows, not just targeted deletes.\n\n## Key Concepts\n- Delta MERGE for deletes\n- Tombstone handling in CDC scenarios\n- Idempotent streaming with deletes\n\n## Real-World Application\n- CDC from multiple sources where deletes must propagate to the downstream table.","diagram":null,"difficulty":"intermediate","tags":["Delta Lake","Databricks","CDC","MERGE","Kubernetes","AWS-S3","certification-mcq","domain-weight-22"],"channel":"databricks-data-engineer","subChannel":"incremental-processing","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:51:28.484Z","createdAt":"2026-01-12 13:51:28"},{"id":"databricks-data-engineer-lakehouse-arch-1768210219223-0","question":"In Delta Live Tables, you want to enforce data quality on a dataset of employees by ensuring age is between 0 and 120 and that department_id exists in the departments dimension before loading into the Silver layer. Which mechanism should you use to declaratively enforce these rules within the pipeline?","answer":"[{\"id\":\"a\",\"text\":\"Use a Python UDF with a filter to drop bad rows.\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use a try/except around the write operation to skip invalid rows.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Define 'expect' constraints on the DLT table definitions.\",\"isCorrect\":true},{\"id\":\"d\",\"text\":\"Move quality checks into a separate nightly job.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption C: Define 'expect' constraints on the DLT table definitions.\n\nDLT provides built-in data quality constraints called expects that let you declare validation rules as part of the pipeline. This makes quality checks auditable, repeatable, and tied to the data lineage.\n\n## Why Other Options Are Wrong\n- Option A: Using a Python UDF with a filter to drop bad rows is ad-hoc and bypasses structured quality definitions, making it harder to maintain and monitor.\n- Option B: try/except handling may drop or skip bad rows unpredictably and doesn't enforce ongoing quality guarantees.\n- Option D: A separate nightly job delays enforcement and breaks end-to-end data quality guarantees.\n\n## Key Concepts\n- Delta Live Tables expects constraints for data quality\n- Declarative data validation integrates with lineage and monitoring\n\n## Real-World Application\n- In an HR data pipeline, enforce age 0-120 and department_id validity via DLT expects to ensure downstream analytics always receive clean Silver layer data.","diagram":null,"difficulty":"intermediate","tags":["Databricks","Delta-Live-Tables","Auto Loader","Unity Catalog","AWS","S3","Kubernetes","Terraform","certification-mcq","domain-weight-24"],"channel":"databricks-data-engineer","subChannel":"lakehouse-arch","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T09:30:19.226Z","createdAt":"2026-01-12 09:30:19"},{"id":"databricks-data-engineer-lakehouse-arch-1768210219223-1","question":"You ingest JSON event data into Delta Lake using Auto Loader; new fields are added over time. Which Auto Loader setting enables automatic schema evolution by merging new columns into the target schema?","answer":"[{\"id\":\"a\",\"text\":\"cloudFiles.schemaEvolutionMode = mergeSchema\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"cloudFiles.inferColumnTypes = true\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"cloudFiles.schemaEvolutionMode = addNewColumns\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"spark.databricks.delta.autoMerge.enabled = true\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A: cloudFiles.schemaEvolutionMode = mergeSchema.\n\nThis Auto Loader setting enables schema evolution by merging new fields from the source into the target Delta table, avoiding a full table rewrite when new columns appear.\n\n## Why Other Options Are Wrong\n- Option B: inferColumnTypes may infer types but does not control schema evolution for new columns.\n- Option C: addNewColumns can add columns but does not guarantee schema merging behavior in all cases.\n- Option D: Delta auto-merge is not the mechanism for Auto Loader schema evolution.\n\n## Key Concepts\n- Auto Loader schema evolution\n- Merging new columns into Delta schema\n\n## Real-World Application\n- Ingesting evolving JSON event logs without manual schema updates; the schema evolves automatically as new fields appear.","diagram":null,"difficulty":"intermediate","tags":["Databricks","Auto Loader","Delta-Lake","AWS","S3","Terraform","certification-mcq","domain-weight-24"],"channel":"databricks-data-engineer","subChannel":"lakehouse-arch","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T09:30:19.798Z","createdAt":"2026-01-12 09:30:20"},{"id":"databricks-data-engineer-lakehouse-arch-1768210219223-2","question":"You want to restrict access to a Delta table by region in a multi-team environment using Databricks Unity Catalog. What is the recommended scalable approach to implement row-level access without duplicating data?","answer":"[{\"id\":\"a\",\"text\":\"Enable built-in row-level security in Delta Lake.\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Create a view with a region predicate and grant access to the view instead of the base table.\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Create separate tables per region and grant access to the appropriate copies.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Hard-code region filters in every SQL/BI query.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B: Create a view with a region predicate and grant access to the view instead of the base table.\n\nUnity Catalog does not provide a native per-row security feature; using views with embedded predicates is the scalable pattern to enforce row-level access while keeping a single source of truth.\n\n## Why Other Options Are Wrong\n- Option A: Delta Lake does not offer built-in per-row RLS; this would require custom workarounds.\n- Option C: Duplicating data is not scalable and increases storage and maintenance overhead.\n- Option D: Hard-coding filters in queries is brittle and does not enforce security consistently.\n\n## Key Concepts\n- Unity Catalog access control via views\n- Row-level access via predicate-based views\n\n## Real-World Application\n- Restricting customer data by region for regional sales teams without data duplication.","diagram":null,"difficulty":"intermediate","tags":["Databricks","Unity Catalog","RLS","Delta-Lake","AWS","S3","Kubernetes","Terraform","certification-mcq","domain-weight-24"],"channel":"databricks-data-engineer","subChannel":"lakehouse-arch","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T09:30:20.318Z","createdAt":"2026-01-12 09:30:20"}],"subChannels":["elt-spark","general","incremental-processing","lakehouse-arch"],"companies":["Apple","Netflix","Scale Ai"],"stats":{"total":12,"beginner":1,"intermediate":11,"advanced":0,"newThisWeek":12}}