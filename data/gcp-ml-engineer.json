{"questions":[{"id":"gcp-ml-engineer-automate-orchestrate-1768158985475-0","question":"Which approach best ensures reproducible experiment runs across dev, staging, and prod environments by parameterizing pipelines, isolating artifacts, and recording lineage?","answer":"[{\"id\":\"a\",\"text\":\"Use a single hard-coded pipeline with a global artifact bucket shared by all environments\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use parameterized Vertex AI Pipelines with environment-specific configuration files, separate artifact storage per environment, and Vertex AI Metadata for experiment lineage\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Run the pipeline once and reuse results across environments without environment isolation\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use manual steps per environment with no centralized configuration\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct option is B because parameterizing Vertex AI Pipelines with environment-specific configuration, separate artifact storage per environment, and Vertex AI Metadata enables reproducible runs and traceability across dev/stage/prod.\n\n## Why Other Options Are Wrong\n- A is incorrect because sharing a single global artifact bucket and not parameterizing pipelines leads to cross-environment contamination and non-reproducible runs.\n- C is incorrect because reusing results across environments without isolation increases drift and undermines reproducibility.\n- D is incorrect because manual, ad-hoc steps introduce variability and prevent centralized configuration.\n\n## Key Concepts\n- Parameterized Vertex AI Pipelines\n- Environment-specific configuration management\n- Artifact storage isolation per environment\n- Vertex AI Metadata for lineage\n\n## Real-World Application\nMaintain separate GCS buckets per environment, version-control all per-environment configs, and drive deployments via a centralized CI/CD workflow that injects environment parameters. Use Vertex AI Metadata to capture lineage across runs.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","Terraform","Vertex AI","Cloud Composer","Dataflow","BigQuery","Airflow","certification-mcq","domain-weight-21"],"channel":"gcp-ml-engineer","subChannel":"automate-orchestrate","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T19:16:25.476Z","createdAt":"2026-01-11 19:16:25"},{"id":"gcp-ml-engineer-automate-orchestrate-1768158985475-1","question":"Which deployment strategy provides safe rollback and minimal downtime when releasing a new model version to a Vertex AI Endpoint?","answer":"[{\"id\":\"a\",\"text\":\"Deploy new model to a separate endpoint and switch traffic via DNS\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use a canary deployment on Vertex AI Endpoints with traffic-splitting and an automated rollback if latency or accuracy drift exceeds threshold\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Immediately swap traffic to the new model on the same endpoint and delete the old version\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Create a new endpoint per deployment and keep old endpoints\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct option is B because canary deployments with traffic splitting allow progressive rollout and automatic rollback triggers based on latency or accuracy drift, minimizing downtime and risk.\n\n## Why Other Options Are Wrong\n- A defines a DNS-based switch which can introduce routing complexity and lacks automated rollback tied to model performance.\n- C causes potential downtime and provides no gradual validation.\n- D multiplies endpoints, increasing resource usage and routing complexity without inherent rollback guarantees.\n\n## Key Concepts\n- Canary deployments\n- Traffic splitting on Vertex AI Endpoints\n- Telemetry and automatic rollback logic\n\n## Real-World Application\nConfigure an initial traffic ramp (e.g., 10-20%), monitor latency, error rate, and drift metrics, and automatically roll back if thresholds are breached; once validated, shift remaining traffic to the new model.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","Terraform","Vertex AI","Cloud Composer","Dataflow","BigQuery","Airflow","certification-mcq","domain-weight-21"],"channel":"gcp-ml-engineer","subChannel":"automate-orchestrate","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T19:16:25.963Z","createdAt":"2026-01-11 19:16:26"},{"id":"gcp-ml-engineer-automate-orchestrate-1768158985475-2","question":"You're building a data processing and feature store pipeline. You need to incrementally materialize features from BigQuery to Vertex AI Feature Store while handling late data and ensuring idempotent writes. Which design best achieves this?","answer":"[{\"id\":\"a\",\"text\":\"Full re-ingestion daily, ignoring late data\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Streaming ingestion with Pub/Sub feeding Feature Store with upsert semantics\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Incremental batch pipeline using Dataflow that fetches last_update timestamp and upserts to Feature Store, with dedup and idempotent writes\",\"isCorrect\":true},{\"id\":\"d\",\"text\":\"Manual data transfer using a single batch job\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct option is C because an incremental batch Dataflow pipeline that uses last_update timestamp and idempotent writes to Vertex AI Feature Store ensures correct handling of late data and avoids duplicates.\n\n## Why Other Options Are Wrong\n- A full daily re-ingestion ignores late data and wastes compute.\n- B Streaming ingestion with Pub/Sub can provide near-real-time updates, but dedup semantics for Feature Store can be complex and may require additional logic.\n- D Manual data transfer lacks automation and reproducibility.\n\n## Key Concepts\n- Dataflow incremental batch processing\n- Last_update timestamp-based ingestion\n- Vertex AI Feature Store upserts and idempotent writes\n- Late-arriving data handling\n\n## Real-World Application\nImplement a Dataflow pipeline that queries BigQuery with a windowed last_update filter, performs dedup, and upserts to Feature Store; integrate with Cloud Composer to schedule and monitor retries and alerts.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","Terraform","Vertex AI","Cloud Composer","Dataflow","BigQuery","Airflow","certification-mcq","domain-weight-21"],"channel":"gcp-ml-engineer","subChannel":"automate-orchestrate","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T19:16:26.440Z","createdAt":"2026-01-11 19:16:26"},{"id":"gcp-ml-engineer-serving-scaling-1768199588961-0","question":"An online inference endpoint for a sentiment-analysis model receives sudden surges during a marketing campaign. The team wants to maintain median latency under 120 ms at up to 2k QPS while avoiding over-provisioning. You have a Vertex AI Endpoint. Which configuration best achieves this?","answer":"[{\"id\":\"a\",\"text\":\"Disable autoscaling and fix the endpoint to 10 replicas\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Configure automatic scaling with min_replica = 2, max_replica = 50, and target_cpu_utilization = 60%\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Deploy a separate replica in a distant region to reduce latency\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use batch predictions during bursts to handle high traffic\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe best approach is to configure automatic scaling with sensible min and max replicas and a target CPU utilization. This lets Vertex AI scale in response to actual load, keeping latency under control while avoiding over-provisioning.\n\n## Why Other Options Are Wrong\n- Option A: Manual scaling cannot adapt to spikes and risks under- or over-provisioning.\n- Option C: Regional distribution may affect latency but does not address per-request scaling during spikes and increases complexity.\n- Option D: Batch predictions are not suitable for real-time latency guarantees.\n\n## Key Concepts\n- Vertex AI endpoint automatic scaling\n- min_replica and max_replica configuration\n- target_cpu_utilization as a scaling signal\n\n## Real-World Application\n- Apply to a live inference endpoint for real-time user requests during campaigns; monitor latency and adjust min/max replicas as needed.","diagram":null,"difficulty":"intermediate","tags":["Vertex AI","Kubernetes","AWS","Terraform","GKE","certification-mcq","domain-weight-19"],"channel":"gcp-ml-engineer","subChannel":"serving-scaling","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T06:33:08.963Z","createdAt":"2026-01-12 06:33:09"},{"id":"gcp-ml-engineer-serving-scaling-1768199588961-1","question":"During a model update, a data-science team wants to canary-test a new version while continuing to serve existing traffic with minimal risk. Which approach should they use in Vertex AI?","answer":"[{\"id\":\"a\",\"text\":\"Create a new endpoint and deploy only the new model version; route traffic there later\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Deploy both versions to the same endpoint and use traffic_split to gradually shift traffic to the new version\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Stop traffic to the old version and roll out the new version immediately\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Deploy the new version to a separate endpoint and disable the older endpoint\",\"isCorrect\":false}]","explanation":"## Correct Answer\nDeploy both versions to the same endpoint and use traffic_split to gradually shift traffic to the new version. This enables canary testing with real traffic while maintaining the current version as a safety fallback.\n\n## Why Other Options Are Wrong\n- Option A: A new endpoint would split management and make gradual traffic control harder.\n- Option C: Stopping all traffic to old version risks service disruption.\n- Option D: A separate endpoint increases coordination overhead and is less safe for canary tests.\n\n## Key Concepts\n- Vertex AI endpoint traffic_split across deployed_models\n- Canary testing with phased traffic\n- Endpoint-level deployment management\n\n## Real-World Application\n- Roll out a model update with 10–20% canary traffic, monitor KPIs, then increase if metrics are satisfactory.","diagram":null,"difficulty":"intermediate","tags":["Vertex AI","Kubernetes","AWS","Terraform","GKE","certification-mcq","domain-weight-19"],"channel":"gcp-ml-engineer","subChannel":"serving-scaling","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T06:33:09.508Z","createdAt":"2026-01-12 06:33:09"},{"id":"gcp-ml-engineer-serving-scaling-1768199588961-2","question":"A model deployed on Vertex AI Endpoint exhibits high cold-start latency for the first request after deployment. Which practice is recommended to minimize cold-start latency in production?","answer":"[{\"id\":\"a\",\"text\":\"Set min_replica to a non-zero value to keep instances warm\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Increase max_replica to a very large number to ensure enough instances\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Pre-warm by sending test requests during deployment without relying on autoscaling\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Switch to batch predictions to avoid online latency\",\"isCorrect\":false}]","explanation":"## Correct Answer\nSetting min_replica to a non-zero value keeps at least some instances warm, reducing cold-start latency for the first requests after deployment.\n\n## Why Other Options Are Wrong\n- Option B: Over-provisioning by raising max_replica increases costs and may not reduce cold-start latency meaningfully.\n- Option C: Pre-warming can help but Vertex AI does not guarantee consistent warm-start without non-zero min_replica; it’s less reliable than always-warm replicas.\n- Option D: Batch predictions do not serve real-time latency-sensitive requests.\n\n## Key Concepts\n- Warm-start vs cold-start latency\n- min_replica as a stabilizing factor\n- Online predictions vs batch predictions\n\n## Real-World Application\n- After deploying critical inference endpoints, configure min_replica to maintain service level KPIs during traffic peaks.","diagram":null,"difficulty":"intermediate","tags":["Vertex AI","Kubernetes","AWS","Terraform","GKE","certification-mcq","domain-weight-19"],"channel":"gcp-ml-engineer","subChannel":"serving-scaling","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T06:33:10.029Z","createdAt":"2026-01-12 06:33:10"}],"subChannels":["automate-orchestrate","serving-scaling"],"companies":[],"stats":{"total":6,"beginner":0,"intermediate":6,"advanced":0,"newThisWeek":6}}