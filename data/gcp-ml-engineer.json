{"questions":[{"id":"gcp-ml-engineer-automate-orchestrate-1768158985475-0","question":"Which approach best ensures reproducible experiment runs across dev, staging, and prod environments by parameterizing pipelines, isolating artifacts, and recording lineage?","answer":"[{\"id\":\"a\",\"text\":\"Use a single hard-coded pipeline with a global artifact bucket shared by all environments\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use parameterized Vertex AI Pipelines with environment-specific configuration files, separate artifact storage per environment, and Vertex AI Metadata for experiment lineage\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Run the pipeline once and reuse results across environments without environment isolation\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use manual steps per environment with no centralized configuration\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct option is B because parameterizing Vertex AI Pipelines with environment-specific configuration, separate artifact storage per environment, and Vertex AI Metadata enables reproducible runs and traceability across dev/stage/prod.\n\n## Why Other Options Are Wrong\n- A is incorrect because sharing a single global artifact bucket and not parameterizing pipelines leads to cross-environment contamination and non-reproducible runs.\n- C is incorrect because reusing results across environments without isolation increases drift and undermines reproducibility.\n- D is incorrect because manual, ad-hoc steps introduce variability and prevent centralized configuration.\n\n## Key Concepts\n- Parameterized Vertex AI Pipelines\n- Environment-specific configuration management\n- Artifact storage isolation per environment\n- Vertex AI Metadata for lineage\n\n## Real-World Application\nMaintain separate GCS buckets per environment, version-control all per-environment configs, and drive deployments via a centralized CI/CD workflow that injects environment parameters. Use Vertex AI Metadata to capture lineage across runs.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","Terraform","Vertex AI","Cloud Composer","Dataflow","BigQuery","Airflow","certification-mcq","domain-weight-21"],"channel":"gcp-ml-engineer","subChannel":"automate-orchestrate","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T19:16:25.476Z","createdAt":"2026-01-11 19:16:25"},{"id":"gcp-ml-engineer-automate-orchestrate-1768158985475-1","question":"Which deployment strategy provides safe rollback and minimal downtime when releasing a new model version to a Vertex AI Endpoint?","answer":"[{\"id\":\"a\",\"text\":\"Deploy new model to a separate endpoint and switch traffic via DNS\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use a canary deployment on Vertex AI Endpoints with traffic-splitting and an automated rollback if latency or accuracy drift exceeds threshold\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Immediately swap traffic to the new model on the same endpoint and delete the old version\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Create a new endpoint per deployment and keep old endpoints\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct option is B because canary deployments with traffic splitting allow progressive rollout and automatic rollback triggers based on latency or accuracy drift, minimizing downtime and risk.\n\n## Why Other Options Are Wrong\n- A defines a DNS-based switch which can introduce routing complexity and lacks automated rollback tied to model performance.\n- C causes potential downtime and provides no gradual validation.\n- D multiplies endpoints, increasing resource usage and routing complexity without inherent rollback guarantees.\n\n## Key Concepts\n- Canary deployments\n- Traffic splitting on Vertex AI Endpoints\n- Telemetry and automatic rollback logic\n\n## Real-World Application\nConfigure an initial traffic ramp (e.g., 10-20%), monitor latency, error rate, and drift metrics, and automatically roll back if thresholds are breached; once validated, shift remaining traffic to the new model.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","Terraform","Vertex AI","Cloud Composer","Dataflow","BigQuery","Airflow","certification-mcq","domain-weight-21"],"channel":"gcp-ml-engineer","subChannel":"automate-orchestrate","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T19:16:25.963Z","createdAt":"2026-01-11 19:16:26"},{"id":"gcp-ml-engineer-automate-orchestrate-1768158985475-2","question":"You're building a data processing and feature store pipeline. You need to incrementally materialize features from BigQuery to Vertex AI Feature Store while handling late data and ensuring idempotent writes. Which design best achieves this?","answer":"[{\"id\":\"a\",\"text\":\"Full re-ingestion daily, ignoring late data\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Streaming ingestion with Pub/Sub feeding Feature Store with upsert semantics\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Incremental batch pipeline using Dataflow that fetches last_update timestamp and upserts to Feature Store, with dedup and idempotent writes\",\"isCorrect\":true},{\"id\":\"d\",\"text\":\"Manual data transfer using a single batch job\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct option is C because an incremental batch Dataflow pipeline that uses last_update timestamp and idempotent writes to Vertex AI Feature Store ensures correct handling of late data and avoids duplicates.\n\n## Why Other Options Are Wrong\n- A full daily re-ingestion ignores late data and wastes compute.\n- B Streaming ingestion with Pub/Sub can provide near-real-time updates, but dedup semantics for Feature Store can be complex and may require additional logic.\n- D Manual data transfer lacks automation and reproducibility.\n\n## Key Concepts\n- Dataflow incremental batch processing\n- Last_update timestamp-based ingestion\n- Vertex AI Feature Store upserts and idempotent writes\n- Late-arriving data handling\n\n## Real-World Application\nImplement a Dataflow pipeline that queries BigQuery with a windowed last_update filter, performs dedup, and upserts to Feature Store; integrate with Cloud Composer to schedule and monitor retries and alerts.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","Terraform","Vertex AI","Cloud Composer","Dataflow","BigQuery","Airflow","certification-mcq","domain-weight-21"],"channel":"gcp-ml-engineer","subChannel":"automate-orchestrate","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T19:16:26.440Z","createdAt":"2026-01-11 19:16:26"},{"id":"q-882","question":"You run a real-time fraud-detection model on GCP at ~25k QPS with sub-20 ms P95 latency. Design a production pipeline using Vertex AI, Feature Store, Pub/Sub, and Dataflow that ingests events, materializes features, serves online predictions, and supports canary rollouts. Include data/model drift monitoring, automated rollback, and cost considerations?","answer":"To meet those goals, route events via Pub/Sub to Dataflow for feature prep, push online features to Vertex AI Feature Store, deploy models in Vertex AI with 10–20% traffic to a canary, and monitor dri","explanation":"## Why This Is Asked\nTests ability to design end-to-end ML platform on GCP at scale, covering real-time ingestion, feature stores, model registry, canary deployments, monitoring, and cost control.\n\n## Key Concepts\n- Realtime ingestion via Pub/Sub\n- Feature Store online/offline usage\n- Vertex AI deployment and model monitoring\n- Canary traffic splitting and rollback\n- Drift detection and alerting\n- Cost-aware scaling\n\n## Code Example\n\n```python\n# Pseudo-code: canary traffic split and monitoring start\n```\n\n## Follow-up Questions\n- How would you set drift thresholds and alerts?\n- How would you evolve feature schemas without breaking serving?","diagram":null,"difficulty":"advanced","tags":["gcp-ml-engineer"],"channel":"gcp-ml-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T14:00:47.597Z","createdAt":"2026-01-12T14:00:47.597Z"},{"id":"q-905","question":"You operate a multi-tenant content recommender service on GCP used by advertisers. Each tenant has its own feature schema and data retention. Design a production ML pipeline (training and online serving) using Vertex AI, Feature Store, Pub/Sub, and Dataflow that supports **per-tenant namespaces**, **policy-based access**, **drift monitoring**, **automated rollback**, and **cost isolation**. Include data leakage prevention, schema evolution, and canary rollouts across regions?","answer":"Use tenant-scoped Feature Stores and separate training runs per tenant via Vertex AI Pipelines. Route streaming data through Pub/Sub and Dataflow into per-tenant online/offline stores, with IAM per-te","explanation":"## Why This Is Asked\nThis question probes how candidates architect multi-tenant ML workflows with governance, isolation, and reliability. It emphasizes production concerns beyond single-tenant pipelines.\n\n## Key Concepts\n- Multi-tenant feature store namespaces\n- IAM and per-tenant isolation\n- Drift detection and monitoring per tenant\n- Canary rollouts and automated rollbacks\n- Cost isolation via quotas and budgets\n- Schema evolution and data leakage prevention\n- Cross-region delivery and auditability\n\n## Code Example\n```javascript\nfunction featureStorePath(tenantId, project, location = 'us-central1') {\n  return `projects/${project}/locations/${location}/featurestores/${tenantId}_fs`;\n}\n```\n\n## Follow-up Questions\n- How would you validate cross-tenant feature leakage guarantees? \n- How would you implement tenant-specific canary traffic and rollbacks at scale?","diagram":"flowchart TD\nA[Tenant] --> B[Feature Store Namespace]\nB --> C[Training Pipeline]\nC --> D[Online Serving]\nD --> E[Canary Manager]\nE --> F[Region Failover]","difficulty":"advanced","tags":["gcp-ml-engineer"],"channel":"gcp-ml-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Scale Ai","Tesla","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T14:43:57.232Z","createdAt":"2026-01-12T14:43:57.232Z"},{"id":"q-922","question":"Design a real-time content moderation inference path on GCP that adds a Redis-based in‑memory cache in front of a Vertex AI online endpoint to reduce latency and cost. Outline what you cache (embeddings vs predictions), key schema (e.g., user_id, model_version, language), TTL and eviction policy, and how you invalidate cache on model deploy or feature updates. Include data privacy considerations and a plan for monitoring latency, cache hit rate, and drift. Provide a small Python sketch of the cache lookup?","answer":"Add a Redis cache in front of the Vertex AI online endpoint. Cache embeddings or prediction results keyed by user_id, model_version, language. Use a short TTL (5–15 min) with invalidation on new deplo","explanation":"## Why This Is Asked\n\nTests practical cache design between online model serving, latency, and cost, plus how to keep data private and fresh.\n\n## Key Concepts\n\n- In‑memory caching in front of online endpoints\n- Cache keys tied to user_id, language, and model_version\n- Invalidation triggers on model deploys or feature updates\n- Data privacy: hashing identifiers before caching\n- Observability: latency, cache hit rate, drift checks\n\n## Code Example\n\n```python\nimport redis\nimport hashlib\n\ncache = redis.Redis(host='redis-host', port=6379)\n\ndef cache_key(user_id, model_version, language):\n    return f\"{hashlib.sha256(user_id.encode()).hexdigest()}:{model_version}:{language}\"\n\ndef get_inference(user_id, model_version, language, compute_embedding, ttl=900):\n    key = cache_key(user_id, model_version, language)\n    val = cache.get(key)\n    if val is not None:\n        return val\n    value = compute_embedding(user_id)\n    cache.setex(key, ttl, value)\n    return value\n```\n\n## Follow-up Questions\n\n- How would you validate cache correctness when model_version changes?\n- How would you scale Redis for bursty traffic while preventing stale data?","diagram":"flowchart TD\n  Client[Client] --> Cache[Redis Cache]\n  Cache --> Endpoint[Vertex AI Endpoint]\n  Endpoint --> Cache\n  Cache --> FeatureStore[Feature Store]","difficulty":"beginner","tags":["gcp-ml-engineer"],"channel":"gcp-ml-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Discord","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T15:33:12.035Z","createdAt":"2026-01-12T15:33:12.035Z"},{"id":"gcp-ml-engineer-scaling-automation-1768224572525-0","question":"You’ve prototyped a ML model locally and want to scale it to production with traffic-based canary rollout to Vertex AI. Which deployment pattern best supports gradual traffic shift and autoscaling?","answer":"[{\"id\":\"a\",\"text\":\"Deploy as a single endpoint with fixed replicas\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Deploy two model revisions behind a single Vertex AI Endpoint and split traffic 10/90\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Deploy the new model to a completely separate Endpoint and switch DNS\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use batch predictions only\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct answer is B because Vertex AI Endpoints support traffic splitting and autoscaling, which enables canary deployments for gradual rollout while monitoring live metrics.\n\n## Why Other Options Are Wrong\n- A deploys a single endpoint with fixed replicas, which lacks dynamic autoscaling and traffic-based canary control.\n- C relies on DNS switch between endpoints, which is slow and brittle for real-time traffic routing.\n- D uses batch predictions, which are not suitable for live online inference at scale.\n\n## Key Concepts\n- Vertex AI Endpoints\n- Traffic splitting / canary deployments\n- Autoscaling\n\n## Real-World Application\n- In production, start with 10% traffic to the new model, monitor latency and accuracy, then progressively increase traffic and remove the old revision once metrics are satisfactory.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","Terraform","AWS S3","AWS EC2","GKE","Vertex AI","Kubeflow","certification-mcq","domain-weight-18"],"channel":"gcp-ml-engineer","subChannel":"scaling-automation","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:29:32.526Z","createdAt":"2026-01-12 13:29:32"},{"id":"gcp-ml-engineer-scaling-automation-1768224572525-1","question":"Your prototype needs sub-second scoring with regional low latency; you have traffic variability across regions. Which approach is most appropriate to ensure low latency and scalable inference?","answer":"[{\"id\":\"a\",\"text\":\"A single Vertex AI Endpoint in a single region and route through a CDN\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Multiple Vertex AI Endpoints deployed in several regions with a global load balancer directing traffic\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Use Cloud Functions to proxy requests to a single endpoint\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use batch predictions only\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct answer is B because deploying endpoints in multiple regions and using a global load balancer directs requests to the nearest region, reducing latency while preserving scalability.\n\n## Why Other Options Are Wrong\n- A centralizes in one region, increasing latency for distant users.\n- C adds an extra hop and is not optimized for real-time, high-throughput inference.\n- D batch predictions are not suitable for sub-second, per-request scoring.\n\n## Key Concepts\n- Multi-region deployment\n- Global load balancing\n- Latency-sensitive inference\n\n## Real-World Application\n- Use this pattern when serving a worldwide user base to maintain consistent SLA across regions.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","Terraform","AWS S3","AWS EC2","GKE","Vertex AI","Kubeflow","certification-mcq","domain-weight-18"],"channel":"gcp-ml-engineer","subChannel":"scaling-automation","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:29:33.064Z","createdAt":"2026-01-12 13:29:33"},{"id":"gcp-ml-engineer-scaling-automation-1768224572525-2","question":"You want reproducibility and governance of ML experiments going to production; what pipeline approach ensures lineage, reproducibility, and auditability?","answer":"[{\"id\":\"a\",\"text\":\"Jupyter notebooks stored locally\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Vertex AI Pipelines with metadata store and artifact lineage\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Manual script logs in a storage bucket\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Spreading experiments across multiple Git branches only\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct answer is B because Vertex AI Pipelines provides end-to-end workflow orchestration with metadata and lineage, enabling reproducibility and auditability.\n\n## Why Other Options Are Wrong\n- A lacks systematic reproducibility and governance.\n- C stores logs without explicit lineage and auditability.\n- D disperses experiments without centralized metadata or lineage tracking.\n\n## Key Concepts\n- Vertex AI Pipelines / Kubeflow Pipelines\n- Metadata store and artifact lineage\n\n## Real-World Application\n- Use pipelines to version data preprocessing, training, and deployment steps with attached lineage for audits.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","Terraform","AWS S3","AWS EC2","GKE","Vertex AI","Kubeflow","certification-mcq","domain-weight-18"],"channel":"gcp-ml-engineer","subChannel":"scaling-automation","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:29:33.590Z","createdAt":"2026-01-12 13:29:33"},{"id":"gcp-ml-engineer-scaling-automation-1768224572525-3","question":"To optimize costs for ephemeral prototypes with sporadic traffic, which deployment pattern best balances cost and availability?","answer":"[{\"id\":\"a\",\"text\":\"Keep all models loaded on a fixed number of replicas\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use Vertex AI Endpoints with autoscaling (minReplicas and maxReplicas) and canary traffic\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Keep warm instances always on in a single region\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Convert to batch inference only\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct answer is B because autoscaling with defined min and max replicas balances cost during idle periods while ensuring responsiveness during traffic spikes.\n\n## Why Other Options Are Wrong\n- A maintains constant resources, increasing cost.\n- C keeps resources on regardless of demand, wasting capacity during quiet periods.\n- D reduces real-time serving capabilities, which harms user experience for active deployments.\n\n## Key Concepts\n- Autoscaling endpoints\n- Min/max replicas\n- Canary traffic\n\n## Real-World Application\n- For models with irregular traffic, leverage autoscaling to scale down during idle times and ramp up during events.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","Terraform","AWS S3","AWS EC2","GKE","Vertex AI","Kubeflow","certification-mcq","domain-weight-18"],"channel":"gcp-ml-engineer","subChannel":"scaling-automation","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:29:33.772Z","createdAt":"2026-01-12 13:29:33"},{"id":"gcp-ml-engineer-scaling-automation-1768224572525-4","question":"You need to monitor data drift and trigger retraining automatically after deployment; which approach provides automated drift detection and end-to-end retraining pipeline?","answer":"[{\"id\":\"a\",\"text\":\"Manual quarterly reviews\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Vertex AI Model Monitoring with drift detection and a retraining pipeline triggered by alerts\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Rely on evaluation dataset accuracy monitoring only\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Hard-code drift thresholds in application code\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct answer is B because Model Monitoring detects drift in production data and can trigger retraining pipelines, enabling automated governance for production models.\n\n## Why Other Options Are Wrong\n- A delays detection and retraining opportunities.\n- C relies only on historical evaluation metrics and may miss real-time drift.\n- D thresholds embedded in code can become stale and miss evolving data patterns.\n\n## Key Concepts\n- Vertex AI Model Monitoring\n- Drift detection\n- Retraining pipelines\n\n## Real-World Application\n- Configure drift alerts to automatically start a retraining workflow with updated data and validated pipelines.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","Terraform","AWS S3","AWS EC2","GKE","Vertex AI","Kubeflow","certification-mcq","domain-weight-18"],"channel":"gcp-ml-engineer","subChannel":"scaling-automation","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:29:33.954Z","createdAt":"2026-01-12 13:29:34"},{"id":"gcp-ml-engineer-serving-scaling-1768199588961-0","question":"An online inference endpoint for a sentiment-analysis model receives sudden surges during a marketing campaign. The team wants to maintain median latency under 120 ms at up to 2k QPS while avoiding over-provisioning. You have a Vertex AI Endpoint. Which configuration best achieves this?","answer":"[{\"id\":\"a\",\"text\":\"Disable autoscaling and fix the endpoint to 10 replicas\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Configure automatic scaling with min_replica = 2, max_replica = 50, and target_cpu_utilization = 60%\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Deploy a separate replica in a distant region to reduce latency\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use batch predictions during bursts to handle high traffic\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe best approach is to configure automatic scaling with sensible min and max replicas and a target CPU utilization. This lets Vertex AI scale in response to actual load, keeping latency under control while avoiding over-provisioning.\n\n## Why Other Options Are Wrong\n- Option A: Manual scaling cannot adapt to spikes and risks under- or over-provisioning.\n- Option C: Regional distribution may affect latency but does not address per-request scaling during spikes and increases complexity.\n- Option D: Batch predictions are not suitable for real-time latency guarantees.\n\n## Key Concepts\n- Vertex AI endpoint automatic scaling\n- min_replica and max_replica configuration\n- target_cpu_utilization as a scaling signal\n\n## Real-World Application\n- Apply to a live inference endpoint for real-time user requests during campaigns; monitor latency and adjust min/max replicas as needed.","diagram":null,"difficulty":"intermediate","tags":["Vertex AI","Kubernetes","AWS","Terraform","GKE","certification-mcq","domain-weight-19"],"channel":"gcp-ml-engineer","subChannel":"serving-scaling","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T06:33:08.963Z","createdAt":"2026-01-12 06:33:09"},{"id":"gcp-ml-engineer-serving-scaling-1768199588961-1","question":"During a model update, a data-science team wants to canary-test a new version while continuing to serve existing traffic with minimal risk. Which approach should they use in Vertex AI?","answer":"[{\"id\":\"a\",\"text\":\"Create a new endpoint and deploy only the new model version; route traffic there later\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Deploy both versions to the same endpoint and use traffic_split to gradually shift traffic to the new version\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Stop traffic to the old version and roll out the new version immediately\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Deploy the new version to a separate endpoint and disable the older endpoint\",\"isCorrect\":false}]","explanation":"## Correct Answer\nDeploy both versions to the same endpoint and use traffic_split to gradually shift traffic to the new version. This enables canary testing with real traffic while maintaining the current version as a safety fallback.\n\n## Why Other Options Are Wrong\n- Option A: A new endpoint would split management and make gradual traffic control harder.\n- Option C: Stopping all traffic to old version risks service disruption.\n- Option D: A separate endpoint increases coordination overhead and is less safe for canary tests.\n\n## Key Concepts\n- Vertex AI endpoint traffic_split across deployed_models\n- Canary testing with phased traffic\n- Endpoint-level deployment management\n\n## Real-World Application\n- Roll out a model update with 10–20% canary traffic, monitor KPIs, then increase if metrics are satisfactory.","diagram":null,"difficulty":"intermediate","tags":["Vertex AI","Kubernetes","AWS","Terraform","GKE","certification-mcq","domain-weight-19"],"channel":"gcp-ml-engineer","subChannel":"serving-scaling","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T06:33:09.508Z","createdAt":"2026-01-12 06:33:09"},{"id":"gcp-ml-engineer-serving-scaling-1768199588961-2","question":"A model deployed on Vertex AI Endpoint exhibits high cold-start latency for the first request after deployment. Which practice is recommended to minimize cold-start latency in production?","answer":"[{\"id\":\"a\",\"text\":\"Set min_replica to a non-zero value to keep instances warm\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Increase max_replica to a very large number to ensure enough instances\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Pre-warm by sending test requests during deployment without relying on autoscaling\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Switch to batch predictions to avoid online latency\",\"isCorrect\":false}]","explanation":"## Correct Answer\nSetting min_replica to a non-zero value keeps at least some instances warm, reducing cold-start latency for the first requests after deployment.\n\n## Why Other Options Are Wrong\n- Option B: Over-provisioning by raising max_replica increases costs and may not reduce cold-start latency meaningfully.\n- Option C: Pre-warming can help but Vertex AI does not guarantee consistent warm-start without non-zero min_replica; it’s less reliable than always-warm replicas.\n- Option D: Batch predictions do not serve real-time latency-sensitive requests.\n\n## Key Concepts\n- Warm-start vs cold-start latency\n- min_replica as a stabilizing factor\n- Online predictions vs batch predictions\n\n## Real-World Application\n- After deploying critical inference endpoints, configure min_replica to maintain service level KPIs during traffic peaks.","diagram":null,"difficulty":"intermediate","tags":["Vertex AI","Kubernetes","AWS","Terraform","GKE","certification-mcq","domain-weight-19"],"channel":"gcp-ml-engineer","subChannel":"serving-scaling","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T06:33:10.029Z","createdAt":"2026-01-12 06:33:10"}],"subChannels":["automate-orchestrate","general","scaling-automation","serving-scaling"],"companies":["Airbnb","Discord","Goldman Sachs","Scale Ai","Snap","Tesla","Twitter"],"stats":{"total":14,"beginner":1,"intermediate":11,"advanced":2,"newThisWeek":14}}