{"questions":[{"id":"gcp-ml-engineer-automate-orchestrate-1768158985475-0","question":"Which approach best ensures reproducible experiment runs across dev, staging, and prod environments by parameterizing pipelines, isolating artifacts, and recording lineage?","answer":"[{\"id\":\"a\",\"text\":\"Use a single hard-coded pipeline with a global artifact bucket shared by all environments\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use parameterized Vertex AI Pipelines with environment-specific configuration files, separate artifact storage per environment, and Vertex AI Metadata for experiment lineage\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Run the pipeline once and reuse results across environments without environment isolation\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use manual steps per environment with no centralized configuration\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct option is B because parameterizing Vertex AI Pipelines with environment-specific configuration, separate artifact storage per environment, and Vertex AI Metadata enables reproducible runs and traceability across dev/stage/prod.\n\n## Why Other Options Are Wrong\n- A is incorrect because sharing a single global artifact bucket and not parameterizing pipelines leads to cross-environment contamination and non-reproducible runs.\n- C is incorrect because reusing results across environments without isolation increases drift and undermines reproducibility.\n- D is incorrect because manual, ad-hoc steps introduce variability and prevent centralized configuration.\n\n## Key Concepts\n- Parameterized Vertex AI Pipelines\n- Environment-specific configuration management\n- Artifact storage isolation per environment\n- Vertex AI Metadata for lineage\n\n## Real-World Application\nMaintain separate GCS buckets per environment, version-control all per-environment configs, and drive deployments via a centralized CI/CD workflow that injects environment parameters. Use Vertex AI Metadata to capture lineage across runs.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","Terraform","Vertex AI","Cloud Composer","Dataflow","BigQuery","Airflow","certification-mcq","domain-weight-21"],"channel":"gcp-ml-engineer","subChannel":"automate-orchestrate","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T19:16:25.476Z","createdAt":"2026-01-11 19:16:25"},{"id":"gcp-ml-engineer-automate-orchestrate-1768158985475-1","question":"Which deployment strategy provides safe rollback and minimal downtime when releasing a new model version to a Vertex AI Endpoint?","answer":"[{\"id\":\"a\",\"text\":\"Deploy new model to a separate endpoint and switch traffic via DNS\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use a canary deployment on Vertex AI Endpoints with traffic-splitting and an automated rollback if latency or accuracy drift exceeds threshold\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Immediately swap traffic to the new model on the same endpoint and delete the old version\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Create a new endpoint per deployment and keep old endpoints\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct option is B because canary deployments with traffic splitting allow progressive rollout and automatic rollback triggers based on latency or accuracy drift, minimizing downtime and risk.\n\n## Why Other Options Are Wrong\n- A defines a DNS-based switch which can introduce routing complexity and lacks automated rollback tied to model performance.\n- C causes potential downtime and provides no gradual validation.\n- D multiplies endpoints, increasing resource usage and routing complexity without inherent rollback guarantees.\n\n## Key Concepts\n- Canary deployments\n- Traffic splitting on Vertex AI Endpoints\n- Telemetry and automatic rollback logic\n\n## Real-World Application\nConfigure an initial traffic ramp (e.g., 10-20%), monitor latency, error rate, and drift metrics, and automatically roll back if thresholds are breached; once validated, shift remaining traffic to the new model.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","Terraform","Vertex AI","Cloud Composer","Dataflow","BigQuery","Airflow","certification-mcq","domain-weight-21"],"channel":"gcp-ml-engineer","subChannel":"automate-orchestrate","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T19:16:25.963Z","createdAt":"2026-01-11 19:16:26"},{"id":"gcp-ml-engineer-automate-orchestrate-1768158985475-2","question":"You're building a data processing and feature store pipeline. You need to incrementally materialize features from BigQuery to Vertex AI Feature Store while handling late data and ensuring idempotent writes. Which design best achieves this?","answer":"[{\"id\":\"a\",\"text\":\"Full re-ingestion daily, ignoring late data\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Streaming ingestion with Pub/Sub feeding Feature Store with upsert semantics\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Incremental batch pipeline using Dataflow that fetches last_update timestamp and upserts to Feature Store, with dedup and idempotent writes\",\"isCorrect\":true},{\"id\":\"d\",\"text\":\"Manual data transfer using a single batch job\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct option is C because an incremental batch Dataflow pipeline that uses last_update timestamp and idempotent writes to Vertex AI Feature Store ensures correct handling of late data and avoids duplicates.\n\n## Why Other Options Are Wrong\n- A full daily re-ingestion ignores late data and wastes compute.\n- B Streaming ingestion with Pub/Sub can provide near-real-time updates, but dedup semantics for Feature Store can be complex and may require additional logic.\n- D Manual data transfer lacks automation and reproducibility.\n\n## Key Concepts\n- Dataflow incremental batch processing\n- Last_update timestamp-based ingestion\n- Vertex AI Feature Store upserts and idempotent writes\n- Late-arriving data handling\n\n## Real-World Application\nImplement a Dataflow pipeline that queries BigQuery with a windowed last_update filter, performs dedup, and upserts to Feature Store; integrate with Cloud Composer to schedule and monitor retries and alerts.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","Terraform","Vertex AI","Cloud Composer","Dataflow","BigQuery","Airflow","certification-mcq","domain-weight-21"],"channel":"gcp-ml-engineer","subChannel":"automate-orchestrate","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T19:16:26.440Z","createdAt":"2026-01-11 19:16:26"},{"id":"gcp-ml-engineer-scaling-automation-1768224572525-0","question":"You’ve prototyped a ML model locally and want to scale it to production with traffic-based canary rollout to Vertex AI. Which deployment pattern best supports gradual traffic shift and autoscaling?","answer":"[{\"id\":\"a\",\"text\":\"Deploy as a single endpoint with fixed replicas\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Deploy two model revisions behind a single Vertex AI Endpoint and split traffic 10/90\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Deploy the new model to a completely separate Endpoint and switch DNS\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use batch predictions only\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct answer is B because Vertex AI Endpoints support traffic splitting and autoscaling, which enables canary deployments for gradual rollout while monitoring live metrics.\n\n## Why Other Options Are Wrong\n- A deploys a single endpoint with fixed replicas, which lacks dynamic autoscaling and traffic-based canary control.\n- C relies on DNS switch between endpoints, which is slow and brittle for real-time traffic routing.\n- D uses batch predictions, which are not suitable for live online inference at scale.\n\n## Key Concepts\n- Vertex AI Endpoints\n- Traffic splitting / canary deployments\n- Autoscaling\n\n## Real-World Application\n- In production, start with 10% traffic to the new model, monitor latency and accuracy, then progressively increase traffic and remove the old revision once metrics are satisfactory.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","Terraform","AWS S3","AWS EC2","GKE","Vertex AI","Kubeflow","certification-mcq","domain-weight-18"],"channel":"gcp-ml-engineer","subChannel":"scaling-automation","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:29:32.526Z","createdAt":"2026-01-12 13:29:32"},{"id":"gcp-ml-engineer-scaling-automation-1768224572525-1","question":"Your prototype needs sub-second scoring with regional low latency; you have traffic variability across regions. Which approach is most appropriate to ensure low latency and scalable inference?","answer":"[{\"id\":\"a\",\"text\":\"A single Vertex AI Endpoint in a single region and route through a CDN\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Multiple Vertex AI Endpoints deployed in several regions with a global load balancer directing traffic\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Use Cloud Functions to proxy requests to a single endpoint\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use batch predictions only\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct answer is B because deploying endpoints in multiple regions and using a global load balancer directs requests to the nearest region, reducing latency while preserving scalability.\n\n## Why Other Options Are Wrong\n- A centralizes in one region, increasing latency for distant users.\n- C adds an extra hop and is not optimized for real-time, high-throughput inference.\n- D batch predictions are not suitable for sub-second, per-request scoring.\n\n## Key Concepts\n- Multi-region deployment\n- Global load balancing\n- Latency-sensitive inference\n\n## Real-World Application\n- Use this pattern when serving a worldwide user base to maintain consistent SLA across regions.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","Terraform","AWS S3","AWS EC2","GKE","Vertex AI","Kubeflow","certification-mcq","domain-weight-18"],"channel":"gcp-ml-engineer","subChannel":"scaling-automation","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:29:33.064Z","createdAt":"2026-01-12 13:29:33"},{"id":"gcp-ml-engineer-scaling-automation-1768224572525-2","question":"You want reproducibility and governance of ML experiments going to production; what pipeline approach ensures lineage, reproducibility, and auditability?","answer":"[{\"id\":\"a\",\"text\":\"Jupyter notebooks stored locally\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Vertex AI Pipelines with metadata store and artifact lineage\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Manual script logs in a storage bucket\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Spreading experiments across multiple Git branches only\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct answer is B because Vertex AI Pipelines provides end-to-end workflow orchestration with metadata and lineage, enabling reproducibility and auditability.\n\n## Why Other Options Are Wrong\n- A lacks systematic reproducibility and governance.\n- C stores logs without explicit lineage and auditability.\n- D disperses experiments without centralized metadata or lineage tracking.\n\n## Key Concepts\n- Vertex AI Pipelines / Kubeflow Pipelines\n- Metadata store and artifact lineage\n\n## Real-World Application\n- Use pipelines to version data preprocessing, training, and deployment steps with attached lineage for audits.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","Terraform","AWS S3","AWS EC2","GKE","Vertex AI","Kubeflow","certification-mcq","domain-weight-18"],"channel":"gcp-ml-engineer","subChannel":"scaling-automation","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:29:33.590Z","createdAt":"2026-01-12 13:29:33"},{"id":"gcp-ml-engineer-scaling-automation-1768224572525-3","question":"To optimize costs for ephemeral prototypes with sporadic traffic, which deployment pattern best balances cost and availability?","answer":"[{\"id\":\"a\",\"text\":\"Keep all models loaded on a fixed number of replicas\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use Vertex AI Endpoints with autoscaling (minReplicas and maxReplicas) and canary traffic\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Keep warm instances always on in a single region\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Convert to batch inference only\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct answer is B because autoscaling with defined min and max replicas balances cost during idle periods while ensuring responsiveness during traffic spikes.\n\n## Why Other Options Are Wrong\n- A maintains constant resources, increasing cost.\n- C keeps resources on regardless of demand, wasting capacity during quiet periods.\n- D reduces real-time serving capabilities, which harms user experience for active deployments.\n\n## Key Concepts\n- Autoscaling endpoints\n- Min/max replicas\n- Canary traffic\n\n## Real-World Application\n- For models with irregular traffic, leverage autoscaling to scale down during idle times and ramp up during events.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","Terraform","AWS S3","AWS EC2","GKE","Vertex AI","Kubeflow","certification-mcq","domain-weight-18"],"channel":"gcp-ml-engineer","subChannel":"scaling-automation","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:29:33.772Z","createdAt":"2026-01-12 13:29:33"},{"id":"gcp-ml-engineer-scaling-automation-1768224572525-4","question":"You need to monitor data drift and trigger retraining automatically after deployment; which approach provides automated drift detection and end-to-end retraining pipeline?","answer":"[{\"id\":\"a\",\"text\":\"Manual quarterly reviews\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Vertex AI Model Monitoring with drift detection and a retraining pipeline triggered by alerts\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Rely on evaluation dataset accuracy monitoring only\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Hard-code drift thresholds in application code\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct answer is B because Model Monitoring detects drift in production data and can trigger retraining pipelines, enabling automated governance for production models.\n\n## Why Other Options Are Wrong\n- A delays detection and retraining opportunities.\n- C relies only on historical evaluation metrics and may miss real-time drift.\n- D thresholds embedded in code can become stale and miss evolving data patterns.\n\n## Key Concepts\n- Vertex AI Model Monitoring\n- Drift detection\n- Retraining pipelines\n\n## Real-World Application\n- Configure drift alerts to automatically start a retraining workflow with updated data and validated pipelines.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","Terraform","AWS S3","AWS EC2","GKE","Vertex AI","Kubeflow","certification-mcq","domain-weight-18"],"channel":"gcp-ml-engineer","subChannel":"scaling-automation","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:29:33.954Z","createdAt":"2026-01-12 13:29:34"},{"id":"gcp-ml-engineer-serving-scaling-1768199588961-0","question":"An online inference endpoint for a sentiment-analysis model receives sudden surges during a marketing campaign. The team wants to maintain median latency under 120 ms at up to 2k QPS while avoiding over-provisioning. You have a Vertex AI Endpoint. Which configuration best achieves this?","answer":"[{\"id\":\"a\",\"text\":\"Disable autoscaling and fix the endpoint to 10 replicas\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Configure automatic scaling with min_replica = 2, max_replica = 50, and target_cpu_utilization = 60%\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Deploy a separate replica in a distant region to reduce latency\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use batch predictions during bursts to handle high traffic\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe best approach is to configure automatic scaling with sensible min and max replicas and a target CPU utilization. This lets Vertex AI scale in response to actual load, keeping latency under control while avoiding over-provisioning.\n\n## Why Other Options Are Wrong\n- Option A: Manual scaling cannot adapt to spikes and risks under- or over-provisioning.\n- Option C: Regional distribution may affect latency but does not address per-request scaling during spikes and increases complexity.\n- Option D: Batch predictions are not suitable for real-time latency guarantees.\n\n## Key Concepts\n- Vertex AI endpoint automatic scaling\n- min_replica and max_replica configuration\n- target_cpu_utilization as a scaling signal\n\n## Real-World Application\n- Apply to a live inference endpoint for real-time user requests during campaigns; monitor latency and adjust min/max replicas as needed.","diagram":null,"difficulty":"intermediate","tags":["Vertex AI","Kubernetes","AWS","Terraform","GKE","certification-mcq","domain-weight-19"],"channel":"gcp-ml-engineer","subChannel":"serving-scaling","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T06:33:08.963Z","createdAt":"2026-01-12 06:33:09"},{"id":"gcp-ml-engineer-serving-scaling-1768199588961-1","question":"During a model update, a data-science team wants to canary-test a new version while continuing to serve existing traffic with minimal risk. Which approach should they use in Vertex AI?","answer":"[{\"id\":\"a\",\"text\":\"Create a new endpoint and deploy only the new model version; route traffic there later\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Deploy both versions to the same endpoint and use traffic_split to gradually shift traffic to the new version\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Stop traffic to the old version and roll out the new version immediately\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Deploy the new version to a separate endpoint and disable the older endpoint\",\"isCorrect\":false}]","explanation":"## Correct Answer\nDeploy both versions to the same endpoint and use traffic_split to gradually shift traffic to the new version. This enables canary testing with real traffic while maintaining the current version as a safety fallback.\n\n## Why Other Options Are Wrong\n- Option A: A new endpoint would split management and make gradual traffic control harder.\n- Option C: Stopping all traffic to old version risks service disruption.\n- Option D: A separate endpoint increases coordination overhead and is less safe for canary tests.\n\n## Key Concepts\n- Vertex AI endpoint traffic_split across deployed_models\n- Canary testing with phased traffic\n- Endpoint-level deployment management\n\n## Real-World Application\n- Roll out a model update with 10–20% canary traffic, monitor KPIs, then increase if metrics are satisfactory.","diagram":null,"difficulty":"intermediate","tags":["Vertex AI","Kubernetes","AWS","Terraform","GKE","certification-mcq","domain-weight-19"],"channel":"gcp-ml-engineer","subChannel":"serving-scaling","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T06:33:09.508Z","createdAt":"2026-01-12 06:33:09"},{"id":"gcp-ml-engineer-serving-scaling-1768199588961-2","question":"A model deployed on Vertex AI Endpoint exhibits high cold-start latency for the first request after deployment. Which practice is recommended to minimize cold-start latency in production?","answer":"[{\"id\":\"a\",\"text\":\"Set min_replica to a non-zero value to keep instances warm\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Increase max_replica to a very large number to ensure enough instances\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Pre-warm by sending test requests during deployment without relying on autoscaling\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Switch to batch predictions to avoid online latency\",\"isCorrect\":false}]","explanation":"## Correct Answer\nSetting min_replica to a non-zero value keeps at least some instances warm, reducing cold-start latency for the first requests after deployment.\n\n## Why Other Options Are Wrong\n- Option B: Over-provisioning by raising max_replica increases costs and may not reduce cold-start latency meaningfully.\n- Option C: Pre-warming can help but Vertex AI does not guarantee consistent warm-start without non-zero min_replica; it’s less reliable than always-warm replicas.\n- Option D: Batch predictions do not serve real-time latency-sensitive requests.\n\n## Key Concepts\n- Warm-start vs cold-start latency\n- min_replica as a stabilizing factor\n- Online predictions vs batch predictions\n\n## Real-World Application\n- After deploying critical inference endpoints, configure min_replica to maintain service level KPIs during traffic peaks.","diagram":null,"difficulty":"intermediate","tags":["Vertex AI","Kubernetes","AWS","Terraform","GKE","certification-mcq","domain-weight-19"],"channel":"gcp-ml-engineer","subChannel":"serving-scaling","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T06:33:10.029Z","createdAt":"2026-01-12 06:33:10"}],"subChannels":["automate-orchestrate","scaling-automation","serving-scaling"],"companies":[],"stats":{"total":11,"beginner":0,"intermediate":11,"advanced":0,"newThisWeek":11}}