{"questions":[{"id":"q-1010","question":"You're building a GPU-accelerated graph analytics pipeline that streams 1e9 edges. Design a cache coherence protocol (CCA) to keep per-vertex state consistent across 8 GPUs via a central directory. Choose directory vs snooping, invalidation vs update, and granularity. Describe data layout, coherence transitions, and a minimal update protocol with atomic operations; include trade-offs and performance tips?","answer":"Implement a directory-based CCA with per-vertex entries: owner, sharers bitmap, version, and a 64‑bit payload. Reads use atomic loads; writes perform a two‑phase commit: invalidate/upgrade relevant sh","explanation":"## Why This Is Asked\nAssess practical understanding of cache coherence in a high‑throughput, GPU‑accelerated setting.\n\n## Key Concepts\n- Directory‑based CCA\n- MESI‑like states and transitions\n- Atomic primitives (CAS, load, store)\n- Granularity vs traffic trade-offs\n\n## Code Example\n\n```javascript\n// Pseudo-layout for a vertex cache line\nclass VertexCacheLine {\n  constructor(owner, sharersMask, version, payload){\n    this.owner = owner; // int GPU id or -1\n    this.sharers = sharersMask; // bitmap\n    this.version = version; // int32\n    this.payload = payload; // 64-bit data blob\n  }\n}\n```\n\n## Follow-up Questions\n- How would you measure coherence traffic versus computation throughput?\n- How would you handle dynamic GPU membership (hot swap, failures)?","diagram":null,"difficulty":"intermediate","tags":["cca"],"channel":"cca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","LinkedIn","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T19:22:06.985Z","createdAt":"2026-01-12T19:22:06.985Z"},{"id":"q-1102","question":"You’re building a real-time 'cca' analytics service ingesting 10k events/sec from multiple services; it must provide low latency, deduplicate, and support backfill. Describe the architecture, data model, and exactly-once strategy, including how you’d implement dedup, transactional writes, and testing under node failures. What trade-offs do you consider?","answer":"Use a durable stream (Kafka) with partitioned topics; assign a stable id per event (topic+partition+offset). Achieve exactly-once semantics by using a transactional producer or idempotent upserts in t","explanation":"## Why This Is Asked\nTries to probe real-world streaming correctness, dedup, and fault tolerance in a scalable setting.\n\n## Key Concepts\n- Exactly-once processing\n- Idempotent writes and transactions\n- Deduplication and changelogs\n- Backpressure and observability\n\n## Code Example\n```javascript\n// Implementation code here\n```\n\n## Follow-up Questions\n- How would you test idempotence under partial failures?\n- How do you handle schema evolution in the event stream?","diagram":null,"difficulty":"intermediate","tags":["cca"],"channel":"cca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T22:33:20.733Z","createdAt":"2026-01-12T22:33:20.733Z"},{"id":"q-1106","question":"Design an end-to-end CDC pipeline that ingests change events from Salesforce and MongoDB and publishes to downstream consumers with at-least-once delivery. Explain your transport choice, deduplication, ordering across partitions, schema evolution, and strategies for backfills, replay, and rollbacks. Include monitoring, testing, and failover plans?","answer":"Leverage a Kafka-based CDC pipeline with Debezium connectors for Salesforce and MongoDB, emitting to per-entity topics. Use idempotent producers and a dedup key (event_id). Partition by entity_key to ","explanation":"## Why This Is Asked\nAssesses practical CDC design across real-world sources with production-grade guarantees and operations.\n\n## Key Concepts\n- CDC pipelines\n- Debezium and Kafka\n- Idempotency and deduplication\n- Schema evolution and backfills\n\n## Code Example\n```javascript\n// Debezium-like configuration sketch\n{\n  \"name\": \"salesforce-mongo-cdc\",\n  \"connector.class\": \"io.debezium.connector.mongodb.MongoDbConnector\",\n  \"tasks.max\": \"1\"\n}\n```\n\n## Follow-up Questions\n- How would you validate ordering guarantees across partitions?\n- How would you implement rollback and backfill strategies with minimal downtime?\n","diagram":null,"difficulty":"intermediate","tags":["cca"],"channel":"cca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T23:15:18.449Z","createdAt":"2026-01-12T23:15:18.449Z"},{"id":"q-1190","question":"You're building a real-time collaborative whiteboard for a chat/video platform at Discord/Airbnb/Netflix scale. Each of 5–10k rooms can have up to 200 concurrent editors and must stay highly available with <100 ms latency. Explain your stack decisions: transport (WebSocket vs gRPC streaming), per-room state partitioning, operation encoding, and conflict resolution (CRDT vs OT). How would you handle exactly-once delivery and failure recovery?","answer":"Use a WebSocket gateway multiplexing per-room channels; shard room state in Redis with CRDTs for concurrent edits (OR-Set, sequence CRDT). Persist snapshots to a durable store (DynamoDB). Deliver mess","explanation":"## Why This Is Asked\n\nTests system design for real-time collaboration at scale, including choice of transport, state partitioning, conflict resolution, and delivery guarantees.\n\n## Key Concepts\n\n- Real-time collaboration data models (CRDTs vs OT)\n- Transport choices (WebSocket vs gRPC)\n- Per-room sharding and snapshotting\n- Exactly-once vs at-least-once semantics\n\n## Code Example\n\n```javascript\n// Simple CRDT merge sketch\nclass ORSet {\n  constructor() { this.add = new Set(); this.remove = new Set(); }\n  apply(op) { if (op.type === 'add') this.add.add(op.id); else if (op.type === 'remove') this.remove.add(op.id); }\n  value() { return [...this.add].filter(id => !this.remove.has(id)); }\n  merge(other) { this.add = new Set([...this.add, ...other.add]); this.remove = new Set([...this.remove, ...other.remove]); }\n}\n```\n\n## Follow-up Questions\n\n- How would you horizontally scale 10k rooms with 200 concurrent editors?\n- How do you ensure client reconnection replays without duplication?","diagram":null,"difficulty":"intermediate","tags":["cca"],"channel":"cca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Discord","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T04:41:27.500Z","createdAt":"2026-01-13T04:41:27.500Z"},{"id":"q-1211","question":"In a multi-region service, each region maintains a local L1 cache and a shared L2 cache. How would you implement a robust cache coherence protocol to prevent stale reads while keeping latency low during write-heavy workloads? Include data paths, an invalidation strategy (push vs TTL), race-condition handling, and testing approaches?","answer":"I'd implement a write-through L1-L2 cache with per-key versioning and atomic updates. Writes update the L2 cache first under a global lease, then publish invalidations to all regional L1 caches via a ","explanation":"## Why This Is Asked\n\nThis question probes understanding of cross-region coherence, latency trade-offs, and robust invalidation strategies in production systems.\n\n## Key Concepts\n\n- Cache coherence\n- Invalidation strategies (push vs TTL)\n- Atomicity with Lua scripts\n- Testing via chaos engineering\n\n## Code Example\n\n```lua\nlocal key = KEYS[1]\nlocal v = ARGV[1]\nlocal cur = redis.call('GET', key)\nif not cur or tonumber(cur) < tonumber(v) then\n  redis.call('SET', key, v)\n  redis.call('PUBLISH','cache_invalidate', key)\n  return 1\nend\nreturn 0\n```\n\n## Follow-up Questions\n\n- How would you handle clock skew and partial failures?\n- How would you measure staleness and set TTLs?","diagram":null,"difficulty":"intermediate","tags":["cca"],"channel":"cca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T05:25:04.449Z","createdAt":"2026-01-13T05:25:04.449Z"},{"id":"q-1260","question":"You're building a real-time cca analytics service that ingests 20k-50k events/sec from multiple microservices and external partners. It must deliver per-user engagement scores with sub-second latency, handle out-of-order and late data, deduplicate events, and support backfill. Describe the end-to-end architecture, data model, and exactly-once strategy, including how you'd implement dedup, transactional writes, watermarking, and backfill testing under network partitions and clock skew?","answer":"Design a streaming pipeline with Kafka as the source, a per-user keyed Flink job, and a durable sink that supports exactly-once semantics. Deduplicate via an event_id cache in Redis or a transactional","explanation":"## Why This Is Asked\nThe question probes depth in real-time data systems, focusing on cca analytics under high throughput, with late data and dedup—areas critical at scale.\n\n## Key Concepts\n- Streaming architectures with per-user keys, watermarking, late-arrival handling\n- Exactly-once sinks, idempotent writes, dedup stores\n- Backfill/replay, fault-injection testing, clock skew\n\n## Code Example\n```javascript\n// Pseudo-code: dedup guard on incoming event\nfunction handleEvent(e) {\n  if (dedupStore.has(e.event_id)) return;\n  dedupStore.add(e.event_id);\n  sink.write(e); // idempotent downstream write\n}\n```\n\n## Follow-up Questions\n- How would you validate exactly-once guarantees under network partitions?\n- What are trade-offs of in-memory dedup vs persistent dedup stores?","diagram":null,"difficulty":"advanced","tags":["cca"],"channel":"cca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Uber","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T07:25:33.728Z","createdAt":"2026-01-13T07:25:33.728Z"},{"id":"q-1296","question":"Design a privacy-conscious extension of a real-time cca analytics pipeline that computes per-user engagement scores across multiple geo-regions for three partner firms (Lyft, NVIDIA, Instacart). The extension must minimize PII exposure, support synthetic data feeds for testing without leaking real PII, provide auditable data events for compliance, and preserve correctness under backpressure, partition rebalancing, and clock skew. Describe architecture, data model changes, masking strategies, and how you’d validate with synthetic data?","answer":"Architect a multi-geo streaming pipeline with a privacy layer that maps PII to surrogate IDs and applies field masking before ingestion. Compute per-user scores keyed by surrogate IDs; store an immuta","explanation":"## Why This Is Asked\n\nExplores privacy-preserving design, cross-geo data handling, and realistic testing strategies in real-time analytics.\n\n## Key Concepts\n\n- Privacy-preserving ID mapping and masking\n- Immutable audit/logging for compliance\n- Synthetic data generation with deterministic seeds\n- Correctness under backpressure, partition rebalancing, and clock skew\n- Data lineage and access controls across geographies\n\n## Code Example\n\n```javascript\n// Example: map PII to surrogate for privacy\nfunction toSurrogate(userId, pii) {\n  const surrogateId = hash(userId + 'salt');\n  const maskedPii = mask(pii);\n  return { surrogateId, maskedPii };\n}\n```\n\n## Follow-up Questions\n\n- How would you verify determinism of surrogate IDs across partitions and runs?\n- How would you perform backfill tests with synthetic data while preserving audit integrity?","diagram":"flowchart TD\n  A[Ingest] --> B[Privacy Layer]\n  B --> C[Surrogate DB]\n  C --> D[Compute Scores]\n  D --> E[Audit Store]\n  E --> F[Partner Feeds]","difficulty":"advanced","tags":["cca"],"channel":"cca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","Lyft","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T08:41:38.205Z","createdAt":"2026-01-13T08:41:38.205Z"},{"id":"q-1333","question":"Design a beginner-friendly data quality and observability pattern for a cca event ingestion pipeline. Ingest 1000–2000 events/sec from mobile and web sources. Specify a lightweight schema: user_id, event_type, ts, event_id. Implement at ingest: schema validation, DLQ for invalid records, per-field quality metrics, and a 60s watermark for late data. Describe implementation details and a concrete test plan with synthetic late and malformed events?","answer":"Implement a lightweight at-ingest validator for cca events enforcing a minimal schema (user_id, event_type, ts, event_id) and route invalid records to a DLQ. Track per-field quality metrics (missing_u","explanation":"## Why This Is Asked\nObservability and data quality are foundational for reliable analytics. This question probes practical patterns a junior engineer can implement end-to-end, including validation, DLQ routing, metrics, and watermarking.\n\n## Key Concepts\n- Schema validation at ingestion\n- Dead-letter queue for bad data\n- Per-field quality metrics\n- Watermarks and late data handling\n- Lightweight testing with synthetic data\n\n## Code Example\n```javascript\nfunction validateEvent(e) {\n  if (!e) return false;\n  const fields = ['user_id','event_type','ts','event_id'];\n  for (const f of fields) if (!(f in e)) return false;\n  const ts = Number(new Date(e.ts));\n  if (Number.isNaN(ts) || ts < 0) return false;\n  return true;\n}\n```\n\n## Follow-up Questions\n- How would you validate DLQ integrity under burst loads?\n- How would you extend to schema evolution without breaking dashboards?","diagram":null,"difficulty":"beginner","tags":["cca"],"channel":"cca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T11:41:37.640Z","createdAt":"2026-01-13T11:41:37.640Z"},{"id":"q-1344","question":"You're building a privacy-preserving, cross-tenant event ingestion and analytics service for a media analytics platform. Ingest 40k-120k events/sec from partner APIs and mobile SDKs, including PII fields. Design the end-to-end pipeline to enforce per-tenant isolation, field-level consent-based access, and auditability while preserving low-latency analytics. Include data model, masking, consent revocation handling, schema evolution, and testing strategy?","answer":"Adopt per-tenant data vaults in a centralized lake, tokenize PII with KMS-backed keys, and enforce field-level masks via a dynamic policy engine. Handle consent revocation by redacting affected fields","explanation":"## Why This Is Asked\nThis question probes multi-tenant privacy, data governance, and real-time compliance. It tests handling of PII, consent revocation, schema evolution, and auditability while maintaining latency.\n\n## Key Concepts\n- Multi-tenant isolation\n- PII masking and tokenization\n- Consent management and retroactive redaction\n- Streaming processing and schema evolution\n- Data lineage and end-to-end auditability\n\n## Code Example\n```python\n# Pseudocode for policy evaluation\ndef mask_fields(record, policy):\n    masked = record.copy()\n    for field in policy.mask_fields:\n        masked[field] = redact(masked[field])\n    return masked\n```\n\n## Follow-up Questions\n- How would you test consent revocation across partitions? \n- What metrics verify latency and correctness during backfills?","diagram":"flowchart TD\n  A[Ingest] --> B[PII Masking & Consent Check]\n  B --> C[Per-Tenant Store (Data Vault)]\n  C --> D[Analytics & Alerts]\n  D --> E[End-to-End Audit]\n  E --> F[Backfill & Validation]","difficulty":"intermediate","tags":["cca"],"channel":"cca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Meta","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T13:03:02.524Z","createdAt":"2026-01-13T13:03:02.526Z"},{"id":"q-840","question":"In a secure messaging service, ciphertexts are decrypted by a server with a decryption oracle exposed to clients, creating a potential CCA risk. Design a practical IND-CCA secure scheme for message confidentiality using existing primitives (e.g., OAEP, AES-GCM, MACs). Explain how you prevent chosen-ciphertext attacks, outline a concrete protocol, and discuss trade-offs?","answer":"Use Encrypt-then-MAC: ciphertext = AES-GCM(key, plaintext, nonce) plus a separate MAC over the ciphertext. Verify MAC before decryption and bound decryption attempts to prevent the oracle from learnin","explanation":"## Why This Is Asked\n\nThis checks understanding of IND-CCA security and how to build practical, protocol-level defenses in modern cryptographic systems.\n\n## Key Concepts\n\n- IND-CCA and chosen-ciphertext resistance\n- Encrypt-then-MAC vs MAC-then-Encrypt\n- Nonce management, associated data, and MAC verification order\n- Trade-offs: performance, ciphertext size, API surface\n\n## Code Example\n\n```javascript\nfunction encrypt(plaintext, key) {\n  const nonce = crypto.randomBytes(12);\n  const cipher = crypto.createCipheriv('aes-128-gcm', key, nonce);\n  const enc = Buffer.concat([cipher.update(plaintext, 'utf8'), cipher.final()]);\n  const tag = cipher.getAuthTag();\n  const payload = Buffer.concat([nonce, enc, tag]);\n  const mac = crypto.createHmac('sha256', key).update(payload).digest();\n  return { payload, mac };\n}\n\nfunction decrypt(payload, mac, key) {\n  if (!verifyMac(mac, payload, key)) throw new Error('MAC fail');\n  const nonce = payload.slice(0, 12);\n  const enc = payload.slice(12, -16);\n  const tag = payload.slice(-16);\n  const decipher = crypto.createDecipheriv('aes-128-gcm', key, nonce);\n  decipher.setAuthTag(tag);\n  return Buffer.concat([decipher.update(enc), decipher.final()]);\n}\n```\n\n## Follow-up Questions\n\n- How would you audit for decryption oracle leakage?\n- How would you enforce constant-time MAC checks and retry limits to prevent side channels?","diagram":null,"difficulty":"intermediate","tags":["cca"],"channel":"cca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Microsoft","Oracle","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T13:24:47.474Z","createdAt":"2026-01-12T13:24:47.474Z"},{"id":"q-879","question":"Describe a cross-region user preferences syncing protocol using MongoDB that tolerates regional partitions. Specify the data model (per-field version stamps), conflict resolution policy, and read/write configurations. Provide a concrete merge approach and an example conflict scenario?","answer":"Model a cross-region user preferences store in MongoDB where each user document carries per-field version stamps: {userId, prefs, version: {ts, clientId}}. Writes publish (ts, clientId). Conflict reso","explanation":"## Why This Is Asked\nThis question probes cross-region consistency, conflict resolution, and practical MongoDB usage.\n\n## Key Concepts\n- CAP trade-offs in distributed systems\n- Per-field versioning and conflict resolution\n- ReadConcern/WriteConcern in MongoDB\n\n## Code Example\n```javascript\nfunction mergePrefs(existing, incoming) {\n  const out = JSON.parse(JSON.stringify(existing || {}));\n  for (const key of Object.keys(incoming)) {\n    const inc = incoming[key];\n    const cur = out[key];\n    const tsInc = inc.__version?.ts ?? 0;\n    const tsCur = cur?.__version?.ts ?? -1;\n    if (tsInc > tsCur || (tsInc === tsCur && (inc.__version?.clientId ?? 0) < (cur?.__version?.clientId ?? 0))) {\n      out[key] = inc;\n    }\n  }\n  return out;\n}\n```\n\n## Follow-up Questions\n- How would you test this merge under simultaneous updates?\n- How would you adapt for field-level strong consistency needs?","diagram":null,"difficulty":"advanced","tags":["cca"],"channel":"cca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","MongoDB","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T13:58:27.264Z","createdAt":"2026-01-12T13:58:27.264Z"},{"id":"q-970","question":"You're shipping an E2E chat feature for an internal Meta–Microsoft product. An attacker may access a decryption oracle. Outline a concrete IND-CCA2 secure scheme for message exchange using public-key crypto, detailing padding (OAEP), a KEM/DEM split or AEAD wrapper, ephemeral keys, and how you bind metadata (timestamps, sender IDs) to prevent malleability. What are the failure modes and mitigations?","answer":"Propose ECIES-X25519 with AES-256-GCM as the DEM, using a fresh ephemeral key per message and OAEP-like padding on the KEK if using RSA. Bind metadata to the AEAD as AAD (sender, recipient, timestamp)","explanation":"## Why This Is Asked\nTests understanding of IND-CCA2 security in practical chat scenarios and how to harden PKE with AEAD wrappers.\n\n## Key Concepts\n- IND-CCA2 security, decryption-oracle threats\n- ECIES/KEM-DEM patterns, ephemeral keys\n- AEAD with AAD binding metadata\n- Padding and malleability considerations\n\n## Code Example\n```javascript\n// Pseudo: derive KEK with ephemeral ECDH, encrypt payload with AES-GCM using AAD\n```\n\n## Follow-up Questions\n- How would you audit for relicensing or side-channel risks in your scheme?\n- How do you rotate keys and handle forward secrecy at scale?","diagram":"flowchart TD\n  A[Sender] --> B[Encrypt with ephemeral KEK]\n  B --> C[Ciphertext with AAD]\n  C --> D[Transmit ciphertext]\n  D --> E[Receiver decrypts to plaintext]","difficulty":"advanced","tags":["cca"],"channel":"cca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T17:33:55.215Z","createdAt":"2026-01-12T17:33:55.215Z"}],"subChannels":["general"],"companies":["Airbnb","Amazon","Anthropic","Apple","Bloomberg","Coinbase","Discord","Google","Instacart","LinkedIn","Lyft","Meta","Microsoft","MongoDB","NVIDIA","Netflix","Oracle","Salesforce","Snowflake","Stripe","Uber","Zoom"],"stats":{"total":12,"beginner":1,"intermediate":7,"advanced":4,"newThisWeek":12}}