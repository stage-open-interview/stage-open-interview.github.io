{"questions":[{"id":"q-1010","question":"You're building a GPU-accelerated graph analytics pipeline that streams 1e9 edges. Design a cache coherence protocol (CCA) to keep per-vertex state consistent across 8 GPUs via a central directory. Choose directory vs snooping, invalidation vs update, and granularity. Describe data layout, coherence transitions, and a minimal update protocol with atomic operations; include trade-offs and performance tips?","answer":"Implement a directory-based CCA with per-vertex entries: owner, sharers bitmap, version, and a 64‑bit payload. Reads use atomic loads; writes perform a two‑phase commit: invalidate/upgrade relevant sh","explanation":"## Why This Is Asked\nAssess practical understanding of cache coherence in a high‑throughput, GPU‑accelerated setting.\n\n## Key Concepts\n- Directory‑based CCA\n- MESI‑like states and transitions\n- Atomic primitives (CAS, load, store)\n- Granularity vs traffic trade-offs\n\n## Code Example\n\n```javascript\n// Pseudo-layout for a vertex cache line\nclass VertexCacheLine {\n  constructor(owner, sharersMask, version, payload){\n    this.owner = owner; // int GPU id or -1\n    this.sharers = sharersMask; // bitmap\n    this.version = version; // int32\n    this.payload = payload; // 64-bit data blob\n  }\n}\n```\n\n## Follow-up Questions\n- How would you measure coherence traffic versus computation throughput?\n- How would you handle dynamic GPU membership (hot swap, failures)?","diagram":null,"difficulty":"intermediate","tags":["cca"],"channel":"cca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","LinkedIn","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T19:22:06.985Z","createdAt":"2026-01-12T19:22:06.985Z"},{"id":"q-1102","question":"You’re building a real-time 'cca' analytics service ingesting 10k events/sec from multiple services; it must provide low latency, deduplicate, and support backfill. Describe the architecture, data model, and exactly-once strategy, including how you’d implement dedup, transactional writes, and testing under node failures. What trade-offs do you consider?","answer":"Use a durable stream (Kafka) with partitioned topics; assign a stable id per event (topic+partition+offset). Achieve exactly-once semantics by using a transactional producer or idempotent upserts in t","explanation":"## Why This Is Asked\nTries to probe real-world streaming correctness, dedup, and fault tolerance in a scalable setting.\n\n## Key Concepts\n- Exactly-once processing\n- Idempotent writes and transactions\n- Deduplication and changelogs\n- Backpressure and observability\n\n## Code Example\n```javascript\n// Implementation code here\n```\n\n## Follow-up Questions\n- How would you test idempotence under partial failures?\n- How do you handle schema evolution in the event stream?","diagram":null,"difficulty":"intermediate","tags":["cca"],"channel":"cca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T22:33:20.733Z","createdAt":"2026-01-12T22:33:20.733Z"},{"id":"q-1106","question":"Design an end-to-end CDC pipeline that ingests change events from Salesforce and MongoDB and publishes to downstream consumers with at-least-once delivery. Explain your transport choice, deduplication, ordering across partitions, schema evolution, and strategies for backfills, replay, and rollbacks. Include monitoring, testing, and failover plans?","answer":"Leverage a Kafka-based CDC pipeline with Debezium connectors for Salesforce and MongoDB, emitting to per-entity topics. Use idempotent producers and a dedup key (event_id). Partition by entity_key to ","explanation":"## Why This Is Asked\nAssesses practical CDC design across real-world sources with production-grade guarantees and operations.\n\n## Key Concepts\n- CDC pipelines\n- Debezium and Kafka\n- Idempotency and deduplication\n- Schema evolution and backfills\n\n## Code Example\n```javascript\n// Debezium-like configuration sketch\n{\n  \"name\": \"salesforce-mongo-cdc\",\n  \"connector.class\": \"io.debezium.connector.mongodb.MongoDbConnector\",\n  \"tasks.max\": \"1\"\n}\n```\n\n## Follow-up Questions\n- How would you validate ordering guarantees across partitions?\n- How would you implement rollback and backfill strategies with minimal downtime?\n","diagram":null,"difficulty":"intermediate","tags":["cca"],"channel":"cca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T23:15:18.449Z","createdAt":"2026-01-12T23:15:18.449Z"},{"id":"q-1190","question":"You're building a real-time collaborative whiteboard for a chat/video platform at Discord/Airbnb/Netflix scale. Each of 5–10k rooms can have up to 200 concurrent editors and must stay highly available with <100 ms latency. Explain your stack decisions: transport (WebSocket vs gRPC streaming), per-room state partitioning, operation encoding, and conflict resolution (CRDT vs OT). How would you handle exactly-once delivery and failure recovery?","answer":"Use a WebSocket gateway multiplexing per-room channels; shard room state in Redis with CRDTs for concurrent edits (OR-Set, sequence CRDT). Persist snapshots to a durable store (DynamoDB). Deliver mess","explanation":"## Why This Is Asked\n\nTests system design for real-time collaboration at scale, including choice of transport, state partitioning, conflict resolution, and delivery guarantees.\n\n## Key Concepts\n\n- Real-time collaboration data models (CRDTs vs OT)\n- Transport choices (WebSocket vs gRPC)\n- Per-room sharding and snapshotting\n- Exactly-once vs at-least-once semantics\n\n## Code Example\n\n```javascript\n// Simple CRDT merge sketch\nclass ORSet {\n  constructor() { this.add = new Set(); this.remove = new Set(); }\n  apply(op) { if (op.type === 'add') this.add.add(op.id); else if (op.type === 'remove') this.remove.add(op.id); }\n  value() { return [...this.add].filter(id => !this.remove.has(id)); }\n  merge(other) { this.add = new Set([...this.add, ...other.add]); this.remove = new Set([...this.remove, ...other.remove]); }\n}\n```\n\n## Follow-up Questions\n\n- How would you horizontally scale 10k rooms with 200 concurrent editors?\n- How do you ensure client reconnection replays without duplication?","diagram":null,"difficulty":"intermediate","tags":["cca"],"channel":"cca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Discord","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T04:41:27.500Z","createdAt":"2026-01-13T04:41:27.500Z"},{"id":"q-1211","question":"In a multi-region service, each region maintains a local L1 cache and a shared L2 cache. How would you implement a robust cache coherence protocol to prevent stale reads while keeping latency low during write-heavy workloads? Include data paths, an invalidation strategy (push vs TTL), race-condition handling, and testing approaches?","answer":"I'd implement a write-through L1-L2 cache with per-key versioning and atomic updates. Writes update the L2 cache first under a global lease, then publish invalidations to all regional L1 caches via a ","explanation":"## Why This Is Asked\n\nThis question probes understanding of cross-region coherence, latency trade-offs, and robust invalidation strategies in production systems.\n\n## Key Concepts\n\n- Cache coherence\n- Invalidation strategies (push vs TTL)\n- Atomicity with Lua scripts\n- Testing via chaos engineering\n\n## Code Example\n\n```lua\nlocal key = KEYS[1]\nlocal v = ARGV[1]\nlocal cur = redis.call('GET', key)\nif not cur or tonumber(cur) < tonumber(v) then\n  redis.call('SET', key, v)\n  redis.call('PUBLISH','cache_invalidate', key)\n  return 1\nend\nreturn 0\n```\n\n## Follow-up Questions\n\n- How would you handle clock skew and partial failures?\n- How would you measure staleness and set TTLs?","diagram":null,"difficulty":"intermediate","tags":["cca"],"channel":"cca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T05:25:04.449Z","createdAt":"2026-01-13T05:25:04.449Z"},{"id":"q-1260","question":"You're building a real-time cca analytics service that ingests 20k-50k events/sec from multiple microservices and external partners. It must deliver per-user engagement scores with sub-second latency, handle out-of-order and late data, deduplicate events, and support backfill. Describe the end-to-end architecture, data model, and exactly-once strategy, including how you'd implement dedup, transactional writes, watermarking, and backfill testing under network partitions and clock skew?","answer":"Design a streaming pipeline with Kafka as the source, a per-user keyed Flink job, and a durable sink that supports exactly-once semantics. Deduplicate via an event_id cache in Redis or a transactional","explanation":"## Why This Is Asked\nThe question probes depth in real-time data systems, focusing on cca analytics under high throughput, with late data and dedup—areas critical at scale.\n\n## Key Concepts\n- Streaming architectures with per-user keys, watermarking, late-arrival handling\n- Exactly-once sinks, idempotent writes, dedup stores\n- Backfill/replay, fault-injection testing, clock skew\n\n## Code Example\n```javascript\n// Pseudo-code: dedup guard on incoming event\nfunction handleEvent(e) {\n  if (dedupStore.has(e.event_id)) return;\n  dedupStore.add(e.event_id);\n  sink.write(e); // idempotent downstream write\n}\n```\n\n## Follow-up Questions\n- How would you validate exactly-once guarantees under network partitions?\n- What are trade-offs of in-memory dedup vs persistent dedup stores?","diagram":null,"difficulty":"advanced","tags":["cca"],"channel":"cca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Uber","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T07:25:33.728Z","createdAt":"2026-01-13T07:25:33.728Z"},{"id":"q-1296","question":"Design a privacy-conscious extension of a real-time cca analytics pipeline that computes per-user engagement scores across multiple geo-regions for three partner firms (Lyft, NVIDIA, Instacart). The extension must minimize PII exposure, support synthetic data feeds for testing without leaking real PII, provide auditable data events for compliance, and preserve correctness under backpressure, partition rebalancing, and clock skew. Describe architecture, data model changes, masking strategies, and how you’d validate with synthetic data?","answer":"Architect a multi-geo streaming pipeline with a privacy layer that maps PII to surrogate IDs and applies field masking before ingestion. Compute per-user scores keyed by surrogate IDs; store an immuta","explanation":"## Why This Is Asked\n\nExplores privacy-preserving design, cross-geo data handling, and realistic testing strategies in real-time analytics.\n\n## Key Concepts\n\n- Privacy-preserving ID mapping and masking\n- Immutable audit/logging for compliance\n- Synthetic data generation with deterministic seeds\n- Correctness under backpressure, partition rebalancing, and clock skew\n- Data lineage and access controls across geographies\n\n## Code Example\n\n```javascript\n// Example: map PII to surrogate for privacy\nfunction toSurrogate(userId, pii) {\n  const surrogateId = hash(userId + 'salt');\n  const maskedPii = mask(pii);\n  return { surrogateId, maskedPii };\n}\n```\n\n## Follow-up Questions\n\n- How would you verify determinism of surrogate IDs across partitions and runs?\n- How would you perform backfill tests with synthetic data while preserving audit integrity?","diagram":"flowchart TD\n  A[Ingest] --> B[Privacy Layer]\n  B --> C[Surrogate DB]\n  C --> D[Compute Scores]\n  D --> E[Audit Store]\n  E --> F[Partner Feeds]","difficulty":"advanced","tags":["cca"],"channel":"cca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","Lyft","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T08:41:38.205Z","createdAt":"2026-01-13T08:41:38.205Z"},{"id":"q-1333","question":"Design a beginner-friendly data quality and observability pattern for a cca event ingestion pipeline. Ingest 1000–2000 events/sec from mobile and web sources. Specify a lightweight schema: user_id, event_type, ts, event_id. Implement at ingest: schema validation, DLQ for invalid records, per-field quality metrics, and a 60s watermark for late data. Describe implementation details and a concrete test plan with synthetic late and malformed events?","answer":"Implement a lightweight at-ingest validator for cca events enforcing a minimal schema (user_id, event_type, ts, event_id) and route invalid records to a DLQ. Track per-field quality metrics (missing_u","explanation":"## Why This Is Asked\nObservability and data quality are foundational for reliable analytics. This question probes practical patterns a junior engineer can implement end-to-end, including validation, DLQ routing, metrics, and watermarking.\n\n## Key Concepts\n- Schema validation at ingestion\n- Dead-letter queue for bad data\n- Per-field quality metrics\n- Watermarks and late data handling\n- Lightweight testing with synthetic data\n\n## Code Example\n```javascript\nfunction validateEvent(e) {\n  if (!e) return false;\n  const fields = ['user_id','event_type','ts','event_id'];\n  for (const f of fields) if (!(f in e)) return false;\n  const ts = Number(new Date(e.ts));\n  if (Number.isNaN(ts) || ts < 0) return false;\n  return true;\n}\n```\n\n## Follow-up Questions\n- How would you validate DLQ integrity under burst loads?\n- How would you extend to schema evolution without breaking dashboards?","diagram":null,"difficulty":"beginner","tags":["cca"],"channel":"cca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T11:41:37.640Z","createdAt":"2026-01-13T11:41:37.640Z"},{"id":"q-1344","question":"You're building a privacy-preserving, cross-tenant event ingestion and analytics service for a media analytics platform. Ingest 40k-120k events/sec from partner APIs and mobile SDKs, including PII fields. Design the end-to-end pipeline to enforce per-tenant isolation, field-level consent-based access, and auditability while preserving low-latency analytics. Include data model, masking, consent revocation handling, schema evolution, and testing strategy?","answer":"Adopt per-tenant data vaults in a centralized lake, tokenize PII with KMS-backed keys, and enforce field-level masks via a dynamic policy engine. Handle consent revocation by redacting affected fields","explanation":"## Why This Is Asked\nThis question probes multi-tenant privacy, data governance, and real-time compliance. It tests handling of PII, consent revocation, schema evolution, and auditability while maintaining latency.\n\n## Key Concepts\n- Multi-tenant isolation\n- PII masking and tokenization\n- Consent management and retroactive redaction\n- Streaming processing and schema evolution\n- Data lineage and end-to-end auditability\n\n## Code Example\n```python\n# Pseudocode for policy evaluation\ndef mask_fields(record, policy):\n    masked = record.copy()\n    for field in policy.mask_fields:\n        masked[field] = redact(masked[field])\n    return masked\n```\n\n## Follow-up Questions\n- How would you test consent revocation across partitions? \n- What metrics verify latency and correctness during backfills?","diagram":"flowchart TD\n  A[Ingest] --> B[PII Masking & Consent Check]\n  B --> C[Per-Tenant Store (Data Vault)]\n  C --> D[Analytics & Alerts]\n  D --> E[End-to-End Audit]\n  E --> F[Backfill & Validation]","difficulty":"intermediate","tags":["cca"],"channel":"cca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Meta","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T13:03:02.524Z","createdAt":"2026-01-13T13:03:02.526Z"},{"id":"q-1372","question":"Design a beginner-friendly, end-to-end pipeline to generate a daily per-user cca_score from 20-50k events/day across services. Each event has event_id, user_id, type (view, click, purchase), ts. Weights: view 0.2, click 0.5, purchase 2.0. Handle out-of-order data with a 4-hour watermark, deduplicate by event_id, and support day-level corrections (if a repair event arrives, recompute that day and upsert). Describe data schema, ETL steps, dedup strategy, and a minimal test plan?","answer":"Ingest into a staging table keyed by event_id to enforce dedup with a unique constraint. Compute daily per-user score by summing weights: view=0.2, click=0.5, purchase=2.0 within a 4-hour watermark wi","explanation":"## Why This Is Asked\nTests the ability to translate a basic scoring requirement into a concrete pipeline with dedup and late data handling.\n\n## Key Concepts\n- Data modeling for events\n- De-dup and idempotence\n- Windowing and watermarking\n- Reconciliation/corrections\n\n## Code Example\n```javascript\n// Pseudo dedup with Redis (per-day set)\nasync function isNewEvent(redis, dateKey, event_id) {\n  const added = await redis.sadd(dateKey, event_id);\n  return added === 1;\n}\n```\n\n## Follow-up Questions\n- How would you test with out-of-order events?\n- How would you monitor accuracy of daily scores?","diagram":"flowchart TD\n  Ingest[Ingest events from multiple services] --> Dedup[Deduplicate by event_id]\n  Dedup --> Window[Apply 4-hour watermark and daily window]\n  Window --> Compute[Compute per-user score with weights]\n  Compute --> Persist[Persist to cca_scores table]\n  Persist --> Repair[Corrections: recompute day and upsert]","difficulty":"beginner","tags":["cca"],"channel":"cca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["NVIDIA","Stripe","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T14:36:05.319Z","createdAt":"2026-01-13T14:36:05.319Z"},{"id":"q-1464","question":"You’re building a real-time cca scoring pipeline ingesting 30k–80k events/sec from multiple vendors and regions. A feature update changes the engagement formula and must be reproducible for historical backfills without mutating past results. Design end-to-end data lineage, feature versioning, and deterministic backfill workflows: how to version features, catalog definitions, replay with identical inputs, handle non-determinism, and test under clock skew and partitions?","answer":"Use an immutable, versioned feature store with a central catalog. Version keys as feature_name@vN; catalog stores versioned definitions, input schemas, and a deterministic hash of the calculation. Use","explanation":"## Why This Is Asked\nThis question probes the candidate’s ability to design reproducible, auditable ML/analytics pipelines in streaming systems, addressing data lineage, feature versioning, and backfills.\n\n## Key Concepts\n- Immutable feature store with versioned keys (feature@vN)\n- Central feature catalog with definitions and input schemas\n- Deterministic backfills using identical inputs and feature versions\n- Data lineage graphs for end-to-end provenance\n- Handling non-determinism (seeds, RNG)\n- Testing under clock skew and network partitions\n\n## Code Example\n```python\nclass FeatureDefinition:\n    def __init__(self, name, version, inputs, calc_fn):\n        self.name = name\n        self.version = version\n        self.inputs = inputs\n        self.calc_fn = calc_fn\n\ndef feature_key(name, version):\n    return f\"{name}@v{version}\"\n```\n\n## Follow-up Questions\n- How would you validate backfills against production results when feature definitions evolve?\n- What monitoring and alerting would you add to detect lineage gaps or replay divergences?","diagram":"flowchart TD\n  A[Ingest Event] --> B[Event Time Extraction]\n  B --> C[Feature Lookup: feature@vN]\n  C --> D[Compute Score]\n  D --> E[Store in Score Store]\n  E --> F[Delivery to downstream systems]","difficulty":"advanced","tags":["cca"],"channel":"cca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T18:01:08.766Z","createdAt":"2026-01-13T18:01:08.767Z"},{"id":"q-1489","question":"You're building an advanced real-time cca analytics platform for a multi-tenant product used by Zoom and Microsoft. The pipeline must respect per-tenant data residency, apply on-the-fly PII masking, and support per-user consent states while still delivering sub-second per-user scores. Explain the end-to-end architecture, privacy controls, and test strategy for backfill and audit trails?","answer":"Adopt a multi-tenant streaming path: per-tenant data residency via regional sinks; on-the-fly masking with deterministic tokens for PII fields; per-user consent gating; encryption at rest with KMS key","explanation":"## Why This Is Asked\nThis question probes privacy-preserving real-time analytics at scale, tenancy, and auditability.\n\n## Key Concepts\n- Privacy by design: PII masking, consent-based feature access, regional residency.\n- Exactly-once streaming and idempotent writes.\n- Backfill and data deletion in presence of masking.\n\n## Code Example\n```javascript\n// PII masking utility (deterministic per-tenant tokenization)\nfunction maskPII(value, tenantKey) {\n  const h = crypto.createHmac('sha256', tenantKey).update(value).digest('hex');\n  return h.substring(0, 16);\n}\n```\n\n## Follow-up Questions\n- How would you test for clock skew and data deletion across regions?\n- What monitoring would you put in place for masking failures and consent mismatches?","diagram":"flowchart TD\n  Ingest[Ingest Events] --> Mask[Mask PII & Gate by Consent]\n  Mask --> Compute[Compute Per-User Score]\n  Compute --> Persist[Persist with Idempotent Upsert]\n  Persist --> Audit[Audit Trails & Residency Enforcement]","difficulty":"advanced","tags":["cca"],"channel":"cca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Microsoft","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T19:01:00.549Z","createdAt":"2026-01-13T19:01:00.549Z"},{"id":"q-1585","question":"You're running a real-time cca analytics pipeline ingesting 20k-50k events/sec from multiple partners. Beyond latency and dedup, design a GDPR/CCPA-compliant data erasure and retention mechanism: when a user requests deletion, purge all derived scores and raw events across online stores, backfills, and audit logs within sub-second latency. Describe architecture, data model, and guarantees, plus testing under partitions and clock skew?","answer":"Implement a tombstone-based purge mechanism: mark user data as deleted in the event store with a tombstone record, propagate purge events through a changelog stream, and apply idempotent deletes across all downstream sinks. Maintain an immutable audit trail of deletion requests while ensuring sub-second purge latency across online stores, backfills, and derived analytics.","explanation":"## Why This Is Asked\n\nTests real-world privacy compliance in streaming analytics, requiring end-to-end data erasure with strict latency and strong consistency guarantees.\n\n## Key Concepts\n\n- Data erasure in streaming pipelines\n- Tombstone events and changelog propagation\n- Immutable auditability and retention policies\n- Testing under partitions, clock skew, and cross-region replication\n\n## Code Example\n\n```javascript\n// Pseudo purge handler\nfunction emitPurgeEvent(userId){\n  // create tombstone and emit to event bus\n}\n```\n\n## Follow-up Questions\n\n- How would you ensure purge operations don't impact system performance?\n- What strategies would you use for cross-region consistency?\n- How do you handle partial failures during the purge process?","diagram":"flowchart TD\n  A[SOURCES] --> B[Ingest]\n  B --> C[Event Store]\n  C --> D[Processing]\n  D --> E[Score Store]\n  E --> F[Downstream Consumers]\n  G[Deletion Request] --> C\n  G --> H[Audit Log]","difficulty":"advanced","tags":["cca"],"channel":"cca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Netflix","Oracle","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T05:41:42.802Z","createdAt":"2026-01-13T22:51:57.343Z"},{"id":"q-1607","question":"You're operating an advanced streaming cca scoring pipeline where a new feature-weighting model must be rolled out with minimal disruption. Design a canary rollout strategy that guarantees deterministic routing, comparability of scores, and safe rollback under drift or latency spikes. What data-plane changes, testing plans, and rollback criteria would you implement?","answer":"Route a fixed, hash-based slice of users to the new model while the remainder use the baseline. Maintain identical feature extraction to ensure score comparability, and implement deterministic routing per user to prevent data skew. Begin with shadow mode deployment, then gradually increase canary traffic percentage while monitoring automated drift detection and latency metrics. Establish rollback triggers for score distribution shifts exceeding statistical thresholds or latency increases beyond SLA targets.","explanation":"## Why This Is Asked\n\nTests ability to design safe, scalable canary rollout strategies in streaming CCA pipelines, addressing data skew, drift detection, latency spikes, and rollback safety with minimal production disruption.\n\n## Key Concepts\n\n- Canary rollout in streaming pipelines\n- Deterministic routing and model compatibility\n- Drift detection and rollback criteria\n- Observability: metrics, logs, audits\n- Idempotent writes and exactly-once semantics\n\n## Code Example\n\n```javascript\n// Example: deterministic routing using simple hash (illustrative)\nfunction isInCanary(userId, pct) {\n  let su","diagram":"flowchart TD\n  Ingest[Ingest] --> Extract[Feature Extractor]\n  Extract --> Baseline[Model Baseline]\n  Extract --> Canary[Model Canary]\n  Baseline --> ScoreStore[Score Store]\n  Canary --> ScoreStore\n  ScoreStore --> Metrics[Observability & Alerts]","difficulty":"advanced","tags":["cca"],"channel":"cca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T05:30:58.823Z","createdAt":"2026-01-14T02:34:16.631Z"},{"id":"q-1662","question":"You're operating a privacy-preserving, real-time cca scoring service for a multi-region delivery platform with strict tenant isolation. Design an architecture that guarantees per-tenant data isolation, deterministic routing for canary vs production, and safe rollback under drift or latency spikes. Include data partitioning, encryption, feature gating, testing plan, and rollback criteria?","answer":"Design a privacy-preserving real-time cca scoring pipeline for a multi-region platform with strict tenant isolation. Use tenant-scoped feature gating, per-tenant partitions, and envelope encryption so","explanation":"## Why This Is Asked\nTests ability to design privacy-aware, multi-tenant real-time scoring at scale with deterministic routing and rollback.\n\n## Key Concepts\n- Tenant isolation in streaming\n- Data encryption at rest/in transit\n- Canary rollout with per-tenant routing\n- Observability and rollback criteria\n\n## Code Example\n```javascript\n// Pseudo: per-tenant partitioning hash\nfunction tenantPartition(tenantId, partitions) {\n  const h = crypto.createHash('sha256').update(tenantId).digest('hex');\n  return parseInt(h.slice(0, 8), 16) % partitions;\n}\n```\n\n## Follow-up Questions\n- How would you test rollback under sudden latency spikes?\n- How do you verify that no tenant data leaks across partitions?","diagram":"flowchart TD\n  Client[Client Request] --> Ingest[Partitioned Ingestion]\n  Ingest --> Sec[Security/Isolation Layer]\n  Sec --> Score[Real-time cca Score]\n  Score --> Audit[Audit & Output]\n  Audit --> Sink[Bottom Sinks]","difficulty":"advanced","tags":["cca"],"channel":"cca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T05:44:53.391Z","createdAt":"2026-01-14T05:44:53.391Z"},{"id":"q-1684","question":"You're building a production-grade per-user cca_score service that spans multiple regions and partner integrations. Design a hybrid real-time/batch pipeline to compute and refresh cca_score with model versioning, ensuring region-local data processing, strict tenant isolation, dedup and exactly-once semantics, and safe backfill testing under clock skew and partitions. Include data model, streaming windowing, incremental scoring, canary rollout, rollback criteria, and test plan?","answer":"Hybrid real-time/batch pipeline with region-local streams, a versioned feature store, and incremental scoring. Deduplicate by event_id, enforce exactly-once writes, and apply windowed aggregations wit","explanation":"## Why This Is Asked\n\nThis question probes the ability to architect a scalable, privacy-aware CCA system spanning regions, with model versioning, latency budgets, and robust backfill handling under clock skew and partitions.\n\n## Key Concepts\n\n- Hybrid real-time/batch design\n- Exactly-once processing and deduplication\n- Region-local data locality and tenant isolation\n- Model versioning, drift detection, canary rollout, rollback\n- Backfill testing and clock-skew simulations\n\n## Diagram\n\nflowchart TD\n  A[Ingest Events] --> B[Region-local Stream] \n  B --> C[Incremental Scoring]\n  C --> D[Store Scores]\n  D --> E[Canary vs Prod]\n  E --> F[Prod Rollout / Rollback]\n\n## Follow-up Questions\n\n- How would you test backfills with late-arriving data?\n- What metrics signal a safe rollback during canaries?","diagram":null,"difficulty":"intermediate","tags":["cca"],"channel":"cca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Twitter","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T06:55:53.819Z","createdAt":"2026-01-14T06:55:53.819Z"},{"id":"q-1717","question":"You're building a production-grade real-time cca scoring service with tenant data residency rules. Some tenants require EU-only storage and compute, others are global. Design an architecture that (a) deterministically routes requests by tenant and user, (b) version-controls per-tenant models, and (c) supports zero-downtime hot-swapping with safe rollback under drift. Include data model, isolation, testing, and rollback criteria?","answer":"Design a per-tenant model registry with region-aware data planes and a deterministic router based on hash(tenant_id||user_id). Route to EU-only or global shards per tenant, with versioned models and i","explanation":"## Why This Is Asked\n\nEvaluates ability to design multi-tenant, privacy-conscious, low-downtime scoring systems with clear rollback guarantees.\n\n## Key Concepts\n\n- Tenant isolation\n- Data residency (EU vs global)\n- Deterministic routing\n- Versioned model registry\n- Hot-swapping and rollback\n- Drift detection\n\n## Code Example\n\n```javascript\nfunction route(tenantId, userId, numShards) {\n  const key = `${tenantId}:${userId}`;\n  let h = 0;\n  for (let i = 0; i < key.length; i++) h = (h * 31 + key.charCodeAt(i)) >>> 0;\n  return h % numShards;\n}\n```\n\n## Follow-up Questions\n\n- How would you verify EU-only data residency under audit?\n- How would you instrument drift metrics and rollback criteria?\n","diagram":null,"difficulty":"advanced","tags":["cca"],"channel":"cca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","Lyft","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T07:51:15.474Z","createdAt":"2026-01-14T07:51:15.475Z"},{"id":"q-1769","question":"You're building a region-scoped, multi-tenant cca_score streaming system with regional data residency guarantees for each tenant. The pipeline ingests 15k–60k events/sec from partner feeds, computes per-tenant cca_scores with model_versioning, and must support cross-region dedup, exactly-once semantics, and safe backfill during partitions. Describe end-to-end architecture, data model, and deployment strategy to meet residency, isolation, and rollback guarantees?","answer":"Architect a region-scoped, multi-tenant flow: ingest events into regional topics; write per-tenant cca_scores to regional stores with keys (tenant_id, region, model_version). Enforce dedup by (tenant_","explanation":"## Why This Is Asked\nTests ability to design a scalable, compliant, multi-tenant streaming architecture with data residency constraints and robust rollback. It also probes model versioning discipline, cross-region data flows, and backfill safety.\n\n## Key Concepts\n- Region-scoped tenancy and data residency\n- Exactly-once, dedup, transactional sinks\n- Watermarking, late data handling\n- Model/version governance and feature stores\n\n## Code Example\n```javascript\n// pseudocode illustrating region-scoped write with idempotent sink\nfunction writeEvent(event){\n  const key = `${event.tenant_id}:${event.region}:${event.model_version}`;\n  return sink.writeTxn({key, value: event});\n}\n```\n\n## Follow-up Questions\n- How would you test residency enforcement under partitions and clock skew?\n- How would you coordinate model_version across regions with backward compatibility?\n- How would you handle deletion requests and privacy constraints across tenants?","diagram":"flowchart TD\n  Ingest[(Regional Ingest)] --> Kafka[(Regional Kafka)]\n  Kafka --> Store[(Regional Store)]\n  Store --> Registry[(Model Registry)]\n  Store --> Backfill[(Backfill Window)]","difficulty":"advanced","tags":["cca"],"channel":"cca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T09:52:30.467Z","createdAt":"2026-01-14T09:52:30.467Z"},{"id":"q-1991","question":"Design a privacy- and compliance-focused real-time cca_score pipeline for a multi-tenant enterprise. The system must enforce per-tenant data isolation, immutable audit logs, and an on-demand privacy mode that halts ingestion and model updates for regulatory checks. Describe architecture, data lineage, deletion policies (Right-to-Deletion), drift detection, and rollback criteria under partitions and clock skew. How would you implement this?","answer":"Architect a multi-tenant streaming cca_score pipeline with per-tenant isolation, immutable audit logs, and a privacy mode that pauses data ingestion and model updates on demand. Use compartmentalized ","explanation":"## Why This Is Asked\n\n- Assesses ability to design privacy- and compliance-conscious real-time pipelines under multi-tenant constraints.\n- Evaluates end-to-end thinking: data isolation, lineage, deletion rights, and rollback under failures.\n\n## Key Concepts\n\n- Data isolation and tenant scoping in streams\n- Immutable audit logs and tamper-evidence\n- Right-to-Deletion and tombstoning\n- Drift detection, rollback, and partition/clock-skew testing\n\n## Code Example\n\n```javascript\n// Pseudo tombstone entry for Right-to-Deletion in audit log\nconst tombstone = { tenantId, userId, field, deletedAt: Date.now(), type: 'tombstone' };\n```\n\n## Follow-up Questions\n\n- How would you verify regulatory audit readiness across regions without impacting latency?\n- Which data-store choices best support per-tenant isolation and rapid deletion?","diagram":null,"difficulty":"advanced","tags":["cca"],"channel":"cca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Instacart","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T19:45:38.627Z","createdAt":"2026-01-14T19:45:38.627Z"},{"id":"q-2062","question":"You're building a beginner-friendly per-user cca_score API for a mobile app used worldwide. Design a minimal, region-aware scoring service with a 2-feature linear model, cache results in Redis (TTL 15m), and deduplicate requests via a request_id. Use Postgres for user features, gate features by region to meet privacy rules, and keep writes idempotent. Provide endpoints, data schemas, and a simple test plan with canary rollout, clock skew, and backfill considerations?","answer":"Implement GET /cca_score that loads features f1 and, if region != EU, f2 from Postgres, computes score = 0.6*f1 + 0.4*f2 (bias 0). Gate f2 by region for privacy. Cache in Redis with key cca_score:{region}:{user_id} (TTL 15m). Deduplicate via request_id with idempotent reads. Use Postgres for user features with region-aware access controls.","explanation":"## Why This Is Asked\n\nTests a practical, beginner-friendly design that combines latency goals, caching, deduplication, and privacy-aware feature gating in a single API.\n\n## Key Concepts\n\n- Latency-aware API design for per-user scores\n- Caching with TTL and region-scoped keys\n- Deduplication via request_id and idempotent reads\n- Region-based feature gating for privacy\n\n## Code Example\n\n```javascript\nfunction computeScore({f1, f2}, region) {\n  const f2Allowed = region !== 'EU'\n  return (f1 * 0.6) + (f2Allowed ? (f2 * 0.4) : 0)\n}\n```\n\n## Follow-up Questions\n\n- How would you extend to additional regions and features?\n- What monitoring would you add for cache hit rates and latency?\n- How would you handle schema migrations for new features?","diagram":null,"difficulty":"beginner","tags":["cca"],"channel":"cca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","IBM","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T05:39:35.344Z","createdAt":"2026-01-14T22:49:02.177Z"},{"id":"q-2267","question":"You're adding beginner-friendly observability to a cca_score microservice used by multiple regions. Design a minimal OpenTelemetry tracing plan that captures per-request latency, propagates trace context across API gateway, auth, feature-store fetch, scoring, and response. Show sample spans and how you'd link traces to centralized logs and metrics. Propose Grafana dashboards and alert thresholds for regional latency spikes. Include a reproducible test to verify trace propagation and a region-failover scenario?","answer":"Implement OpenTelemetry with a single global trace propagated via traceparent. Add spans: API Gateway, Auth, FeatureStoreFetch, Scoring, Response. Export to Jaeger for traces and Prometheus for metric","explanation":"## Why This Is Asked\nThis checks practical observability for a distributed surface area in a beginner-friendly way, focusing on real-world needs rather than abstract concepts.\n\n## Key Concepts\n- OpenTelemetry basics and trace propagation\n- Span naming and hierarchical causality\n- Logs-metrics-traces correlation\n- Region-aware labeling for dashboards\n\n## Code Example\n```javascript\n// Pseudo-code for initializing tracer\nconst tracer = opentelemetry.trace.getTracer('cca-score');\n```\n\n## Follow-up Questions\n- How would you tune sampling for prod without losing visibility?\n- How do you redact sensitive data from traces and logs?","diagram":"flowchart TD\n  A[API Gateway] --> B[Auth Service]\n  B --> C[FeatureStoreFetch]\n  C --> D[Scoring Service]\n  D --> E[Response]","difficulty":"beginner","tags":["cca"],"channel":"cca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google","Hashicorp"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T09:50:03.009Z","createdAt":"2026-01-15T09:50:03.009Z"},{"id":"q-2390","question":"You're designing a multi-tenant cca_score service with SLA-aware routing. Some tenants require sub-200 ms online scoring; others tolerate batch refresh. Design a dual-path architecture: hot path with per-tenant in-memory cache for latency-critical tenants, and a cold path for others. Include routing rules, data model, cache key schema, backpressure, degraded scoring modes, and a testing plan?","answer":"Propose a dual-path, SLA-aware routing: hot path with per-tenant in-memory cache for sub-200 ms online scoring, and a cold path that recomputes periodically for others. Route by tenant SLA, use circui","explanation":"## Why This Is Asked\nTests ability to design SLA-aware routing, latency budgets, and resilient scoring under load with clear trade-offs and validation.\n\n## Key Concepts\n- SLA-driven routing\n- Hot vs Cold paths\n- Per-tenant cache design\n- Backpressure and degradation strategies\n- Testing plan: canaries, synthetic load, chaos testing\n\n## Code Example\n```javascript\nfunction selectPath(tenant) {\n  // tenant.slaMs <= 200 implies hot path\n  return tenant.slaMs <= 200 ? \"hot\" : \"cold\";\n}\n```\n\n## Follow-up Questions\n- How would you handle cache invalidation when scores are updated?\n- How would you monitor SLA violations and auto-scale?\n","diagram":"flowchart TD\n  T[Tenant] --> R[Router]\n  R --> H[HotPath: In-Memory Cache]\n  R --> C[ColdPath: Recompute & Persist]\n  H --> S[CCA Score Service]\n  C --> S","difficulty":"intermediate","tags":["cca"],"channel":"cca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Hugging Face","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T15:53:00.326Z","createdAt":"2026-01-15T15:53:00.326Z"},{"id":"q-2413","question":"You're building a privacy-preserving, multi-tenant cca_score service with partners and regulatory constraints. Detail a design that enforces strict tenant isolation, per-tenant key management for features and models, encrypted data in transit, and explainability traces that redact PII. Include data flow, threat model, testing (simulated breaches, leakage tests), and rollback criteria?","answer":"Isolate each tenant in separate feature-store partitions with per-tenant KMS keys and envelope encryption for all persisted data. Use RBAC, tokenized IDs, and redact PII in explanations. Maintain immu","explanation":"## Why This Is Asked\nThis tests designing a compliant, scalable, multi-tenant cca_score system with strong isolation and auditability.\n\n## Key Concepts\n- Tenant isolation via partitioned feature stores\n- Per-tenant key management and encryption\n- Explainability safety and PII redaction\n- Immutable auditing and breach-testing\n\n## Code Example\n```javascript\n// Pseudocode: tenant-scoped scoring\nfunction scoreForTenant(tenantId, payload, store, modelReg) {\n  const part = store.getPartition(tenantId);\n  const features = part.fetchFeatures(payload);\n  const model = modelReg.getModel(tenantId);\n  return model.predict(features);\n}\n```\n\n## Follow-up Questions\n- How would you test rollout under partial breach scenarios?\n- How would you prove regulatory compliance to auditors?\n- What metrics indicate isolation violation?\n","diagram":null,"difficulty":"advanced","tags":["cca"],"channel":"cca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T17:01:13.393Z","createdAt":"2026-01-15T17:01:13.393Z"},{"id":"q-2455","question":"You’re building a privacy-preserving, real-time cca scoring service that runs in TEEs across multiple regions. Tenant data is encrypted in transit and at rest, and tenants supply per-tenant feature pipelines and keys. Design deterministic tenant routing, per-tenant model/versioning, TEEs attestation and key management, and a testing strategy to prove no data leakage, handle drift, and rollback safely under partitions?","answer":"Implement TEEs (SGX/SEV) with attested enclaves for feature decoding and scoring; route tenants deterministically via signed tenant tokens; store per-tenant model versions; use envelope encryption wit","explanation":"## Why This Is Asked\n\nThis probes security-first real-time design under multi-tenancy, focusing on trusted execution, key management, and safe rollback under partitions.\n\n## Key Concepts\n\n- TEEs and attestation for secure compute\n- Deterministic tenant routing and per-tenant model/versioning\n- Envelope encryption with per-tenant KEKs and rotation\n- In-enclave streaming processing and leakage minimization\n- Canary rollout, drift testing, and backfill under partitions\n\n## Code Example\n\n```javascript\n// Pseudocode: fetch KEK, decrypt features in enclave, and score\nasync function scoreTenantEvent(event, tenantId){\n  const kek = await kms.getTenantKey(tenantId);\n  const plain = decryptInHost(event.encryptedFeatures, kek);\n  const score = enclave.computeScore(tenantId, plain, getModelVersion(tenantId));\n  return score;\n}\n```\n\n## Follow-up Questions\n\n- How would you detect and mitigate side-channel risks in this setup?\n- How would you validate model version rollbacks across regions during a canary release?","diagram":"flowchart TD\n  A[Tenant Token] --> B[Deterministic Router]\n  B --> C[TEEs Enclave]\n  C --> D[Model & Feature Store]\n  D --> E[cca_score Output]","difficulty":"intermediate","tags":["cca"],"channel":"cca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Microsoft","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T18:58:16.592Z","createdAt":"2026-01-15T18:58:16.592Z"},{"id":"q-2599","question":"You're rolling out a region-aware cca scoring model that weights features by user segment. Design a deterministic, hash-based routing canary with region-level rollout, ensuring score comparability and fast rollback on drift or latency spikes. Include versioning, testing plan, metrics, and rollback criteria?","answer":"Route users deterministically using hash(user_id) to distribute between canary and baseline cohorts, with region-specific gradual rollout for v2. Calibrate the new model to maintain score comparability, and capture per-segment metrics to compare distributions. Implement drift detection with automated rollback triggers for latency spikes or score divergence.","explanation":"## Why This Is Asked\nEvaluate ability to design safe multi-region rollout with deterministic routing and robust rollback mechanisms.\n\n## Key Concepts\n- Deterministic routing via consistent hashing\n- Region-aware feature weighting and versioning\n- Comprehensive observability and rollback criteria\n- Data-plane safety and auditability\n\n## Code Example\n```javascript\n// JavaScript example: deterministic bucketing by user_id\nfunction bucket(userId, buckets) {\n  let h = 0;\n  for (let i = 0; i < userId.length; i++) h = (h * 31 + userId.charCodeAt(i)) >>> 0;\n  return h % buckets;\n}\n```\n\n## Follow-up Questions\n- How would you test rollback scenarios?","diagram":"flowchart TD\n  Ingest[Ingest] --> Router[Deterministic Router]\n  Router --> V1[Score v1]\n  Router --> V2[Score v2]\n  V1 --> Output[Output]\n  V2 --> Output\n  Output --> Monitor[Monitoring]","difficulty":"advanced","tags":["cca"],"channel":"cca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Snap","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T05:03:52.494Z","createdAt":"2026-01-16T02:28:09.400Z"},{"id":"q-2650","question":"Design a privacy-preserving, multi-tenant cca scoring pipeline that evaluates per-user scores inside confidential compute enclaves. Features and raw data must remain encrypted at rest and in transit. Describe data flows, tenant isolation, key management, audit trails, and methods for drift detection and rollback if enclave compromise is suspected. Include concrete components (KMS, envelope encryption, SGX/TEE, HSM), testing plan, and deployment strategy?","answer":"Design a privacy-preserving, multi-tenant cca scoring pipeline that evaluates per-user scores inside confidential compute enclaves. Encrypt features at rest and in transit; use envelope encryption via","explanation":"## Why This Is Asked\nThis probes privacy, security, and multi-tenant isolation in a live scoring pipeline, plus operational testing.\n\n## Key Concepts\n- Confidential computing (TEE/SGX)\n- Envelope encryption and KMS/HSM\n- Tenant isolation and per-tenant keys\n- Auditability and drift detection\n- Performance trade-offs under enclave overhead\n\n## Code Example\n```javascript\n// Pseudo: encrypt, send to enclave, compute score, decrypt result\nconst payload = { userId, features };\nconst enc = kms.encryptEnvelope(payload, tenantId);\nconst score = enclave.computeScore(enc);\nconst result = kms.decryptEnvelope(score);\n```\n\n## Follow-up Questions\n- How would you test isolation and key rotation?\n- How do you detect compromised enclaves and trigger rollback?","diagram":"flowchart TD\n  A[User Data] --> B[Encrypted Storage]\n  B --> C[Confidential Enclave]\n  C --> D[Encrypted Result]\n  D --> E[Audit System]","difficulty":"advanced","tags":["cca"],"channel":"cca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","DoorDash"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T05:37:13.503Z","createdAt":"2026-01-16T05:37:13.503Z"},{"id":"q-2671","question":"You're operating a real-time cca scoring service with per-tenant model_versioning and drift-control. Design an automated drift-detection and canary rollout framework that (a) compares live feature distributions against a stable reference per tenant, (b) promotes per-tenant canaries when drift crosses a threshold, (c) enables zero-downtime rollback with defined criteria during latency spikes or model degradation. Include data path, metrics, and rollback plan?","answer":"Propose a streaming drift-detection and per-tenant canary rollout. Track live feature distributions vs a reference using sketching (tdigest) per tenant, compute drift scores, and gradually route traff","explanation":"## Why This Is Asked\nThis question probes design for drift-aware, low-latency multi-tenant CCAs with safe rollouts. It tests data path choices, metrics, and rollback guarantees under partitions. \n\n## Key Concepts\n- Drift detection: per-tenant, sketching-based distributions. \n- Canary rollout: traffic splitting, progressive rollout, canary killswitch. \n- Telemetry: latency, AUC/log-loss drift, SLA adherence. \n- Isolation: per-tenant feature stores and model registries. \n\n## Code Example\n```javascript\n// Example snippet illustrating per-tenant routing once canary flag is active\nfunction route(tenantId, traffic, canaryFlag){\n  if(canaryFlag[tenantId] && traffic % 100 < 10){\n    return 'canary';\n  }\n  return 'prod';\n}\n```\n\n## Follow-up Questions\n- How would you handle late-arriving data affecting drift metrics? \n- What rollback criteria would you set for model degradation vs drift? ","diagram":"flowchart TD\n  A[Ingest cca events] --> B[Per-tenant feature store]\n  B --> C[Reference dist + drift score]\n  C --> D{Drift > threshold?}\n  D -- Yes --> E[Activate per-tenant canary]\n  D -- No --> F[Route to prod]\n  E --> G[Model inference (canary)]\n  F --> H[Model inference (prod)]\n  G --> I[Publish metrics/audit logs]\n  H --> I","difficulty":"intermediate","tags":["cca"],"channel":"cca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["OpenAI","Oracle","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T06:45:41.985Z","createdAt":"2026-01-16T06:45:41.987Z"},{"id":"q-2772","question":"You're designing a privacy-preserving cca_score service under GDPR/CCPA with cross-border data flows. Propose an architecture that guarantees data residency, minimizes cross-jurisdiction data sharing, and enables deterministic scoring for A/B tests. Include encryption, key management, access controls, and compliance testing plans?","answer":"Architect a multi-region cca_score service with region-local compute; process only local attributes and store PII within the originating region. Use per-region KMS keys and envelope encryption, with a","explanation":"## Why This Is Asked\n\nAssess privacy-by-design skills, cross-border data governance, and deterministic experimentation in production systems. Requires concrete architecture choices and testing plans, not generic principles.\n\n## Key Concepts\n\n- Data residency and minimization across jurisdictions\n- Encryption at rest and in transit; envelope encryption; region-specific KMS\n- Deterministic A/B assignment via user_id hashing\n- Auditable access, consent management, and DPIA-aligned data retention\n\n## Code Example\n\n```javascript\n// deterministic A/B assignment\nfunction regionForUser(userId, regions){\n  const h = hash(userId);\n  return regions[h % regions.length];\n}\n```\n\n## Follow-up Questions\n\n- How would you verify data residency in CI/CD and runtime monitors?\n- What privacy risk scenarios would trigger a rollback or patch deployment?","diagram":null,"difficulty":"advanced","tags":["cca"],"channel":"cca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Google","Instacart"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T11:29:55.587Z","createdAt":"2026-01-16T11:29:55.588Z"},{"id":"q-2988","question":"You're integrating a privacy-preserving cca scoring system across multi-tenant apps (Discord-like and Lyft-like). Design an end-to-end pipeline that enforces per-tenant data access controls, differential privacy budgets for features, and auditable lineage. Include feature store versioning, drift detection with automated safe rollback, and a test plan for backfill and latency under partition/latency spikes. Provide concrete data models, metrics, and rollback criteria?","answer":"Per-tenant privacy budgets, immutable feature-store versions, and DP noise added at read time. Use granular access controls and data lineage. Drift detection triggers automated rollback if AUC delta e","explanation":"## Why This Is Asked\nTests ability to design multi-tenant, privacy-aware scoring pipelines with robust safety brakes.\n\n## Key Concepts\n- Multi-tenant isolation, data access controls, data lineage\n- Differential privacy budgets and noise injection\n- Feature store versioning and immutable snapshots\n- Drift detection, canary rollout, and rollback criteria\n\n## Code Example\n```javascript\n// Pseudo: drift detector hook\nif (aucDelta > 0.02 && privacyBudgetRemaining < threshold) rollback();\n```\n\n## Follow-up Questions\n- How would you model per-tenant privacy budgets? \n- What metrics define a safe rollback? \n- How would backfill interact with DP constraints?","diagram":"flowchart TD\n A[Ingest] --> B[Feature Store vX]\n B --> C[Scoring Service]\n C --> D[DP Noise Layer]\n D --> E[Serving Layer]\n E --> F[Audit Logs]","difficulty":"advanced","tags":["cca"],"channel":"cca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Lyft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T20:31:42.678Z","createdAt":"2026-01-16T20:31:42.678Z"},{"id":"q-3062","question":"You're adding a new optional feature weight in a per-user cca_score service (2-feature linear model). Design a beginner-friendly, region-aware rollout using a model_version flag and an API that accepts model_version. Outline data model changes, a zero-downtime migration, and a basic test plan with canary rollout and rollback criteria?","answer":"Add a model_version column and a small feature flag to enable v2. The API accepts an optional model_version parameter, and scoring uses weights corresponding to the specified version. Migration: add the column with a default value of 'v1', backfill v2 for canary users, then expand gradually by region.","explanation":"## Why This Is Asked\nTests the ability to plan safe schema evolution and staged feature rollouts in a simple cca_score service.\n\n## Key Concepts\n- Model versioning and feature flags\n- Zero-downtime migrations\n- Canary and region-aware rollout\n- Backfill strategy and rollback criteria\n\n## Code Example\n```sql\n-- Add version column without blocking writes\nALTER TABLE cca_scores ADD COLUMN model_version VARCHAR(16) NOT NULL DEFAULT 'v1';\n```\n\n```javascript\n// API sketch (pseudo)\nPOST /cca_score { user_id, region, features, model_version }\n```\n\n## Follow-up Questions\n- How would you verify no drift between model versions?\n- What metrics would you monitor during canary rollout?\n- How would you handle rollback if v2 shows regression?","diagram":null,"difficulty":"beginner","tags":["cca"],"channel":"cca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Hugging Face","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T05:18:56.775Z","createdAt":"2026-01-16T23:32:30.873Z"},{"id":"q-3257","question":"You're deploying a new edge-scored cca feature at a CDN edge (e.g., Cloudflare Workers) to cut mobile latency. Design a rollout plan that guarantees deterministic user routing across edge locations, preserves score consistency during model_version changes, and enforces strict tenant isolation with safe rollback. Include data-plane changes, cache invalidation strategy, testing plan, and rollback criteria?","answer":"Bind each user to one edge shard using a stable hash(user_id) mod N, tag models with immutable model_version, run a 5% canary and pre-warm edge caches, route via a deterministic header carrying shard ","explanation":"## Why This Is Asked\nThe topic tests edge compute, deterministic routing, cache coherence, and safe rollbacks for latency-sensitive cca scoring at scale, aligning with Cloudflare/Lyft needs.\n\n## Key Concepts\n- Edge compute and CDN routing\n- Deterministic sharding and cache coherence\n- Immutable model_version tags\n- Canary rollout and rollback criteria\n- Observability and drift detection\n\n## Code Example\n```javascript\nfunction shard(userId, shards){\n  return Math.abs(hashFn(userId)) % shards;\n}\n```\n\n## Follow-up Questions\n- How would you detect score drift across edges and trigger rollback?\n- How to model latency budgets and backpressure during rollout?","diagram":null,"difficulty":"advanced","tags":["cca"],"channel":"cca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Lyft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T08:50:13.873Z","createdAt":"2026-01-17T08:50:13.873Z"},{"id":"q-3366","question":"You're building a privacy-aware, multi-tenant cca_score platform where tenants enforce different data retention and PII-masking rules. Design an end-to-end pipeline that (1) applies per-tenant masking without leaking raw data, (2) preserves analytic utility under masking, (3) supports real-time scoring with strict isolation, (4) provides an auditable data lineage and rollback path, and (5) includes a robust backfill strategy. Describe architecture, data model changes, and testing approach?","answer":"Partition streams by tenant and apply per-tenant masking at ingest (PII redaction, tokenization) so raw data never leaves processing. Compute cca_score with region-local state and strict tenant isolat","explanation":"## Why This Is Asked\n\nTests privacy-conscious, multi-tenant design for cca_score, focusing on masking, lineage, and reliable backfill under partitions.\n\n## Key Concepts\n\n- Tenant isolation with per-tenant masking\n- Data lineage and auditability\n- Exactly-once semantics in streaming\n- Backfill under partitions and clock skew\n- Privacy controls and data retention rules\n\n## Code Example\n\n```javascript\n// Pseudocode: mask and upsert with idempotent key\nfunction maskPII(event){ return { userId: event.userId, score: event.score, masked: true }; }\nasync function upsertScore(db, tenantId, userId, score){ await db.upsert({tenantId, userId, score}); }\n```\n\n## Follow-up Questions\n\n- How would you validate masking rules during backfills in the presence of late data?\n- What telemetry would you instrument to verify lineage integrity end-to-end?","diagram":null,"difficulty":"intermediate","tags":["cca"],"channel":"cca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Scale Ai","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T13:41:58.727Z","createdAt":"2026-01-17T13:41:58.727Z"},{"id":"q-3384","question":"You're building a multi-tenant cca scoring service with strict data isolation and per-tenant privacy budgets. Propose an architecture that enables cross-tenant feature reuse without leakage, implementing per-tenant differential privacy controls, per-feature epsilon budgeting, and real-time scoring with exactly-once semantics. Describe data model, streaming windowing, testing under partitions, and canary rollout?","answer":"Deliver per-tenant isolation with separate state stores and a per-tenant DP budget. Compute features in real time, apply Gaussian noise to aggregates, and clip per-tenant contributions to enforce epsi","explanation":"## Why This Is Asked\nTests multi-tenant isolation with privacy budgets, cross-tenant feature reuse, and streaming DP integration. It probes leakage risk, per-feature governance, and rollout strategies.\n\n## Key Concepts\n- Multi-tenant isolation\n- Differential privacy budgeting\n- Real-time streaming with DP noise\n- Feature namespaces for reuse\n- Watermarks and exactly-once semantics\n- Testing under partitions/backfills\n\n## Code Example\n```javascript\n// Pseudocode: add DP noise to metric aggregates\nfunction dpAddNoise(value, epsilon) {\n  // laplace or gaussian noise based on DP mechanism\n  return value + sampleNoise(epsilon);\n}\n```\n\n## Follow-up Questions\n- How would you monitor epsilon budget exhaustion per tenant?\n- How would you adjust budgets during canary vs. full rollout?","diagram":null,"difficulty":"intermediate","tags":["cca"],"channel":"cca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Google","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T14:27:02.356Z","createdAt":"2026-01-17T14:27:02.356Z"},{"id":"q-3454","question":"You're adding a beginner-friendly cca_score API with strict tenant isolation and auditability. Design a security-first flow where each request includes tenant_id, user_id, and a signed token; describe how you enforce per-tenant data boundaries, implement an immutable audit log, and expose a minimal endpoint model_version. Include data schemas and a basic test plan?","answer":"Implement a per-tenant cca_score API guarded by signed tokens (JWT) containing tenant_id and user_id; enforce row-level data boundaries; store scores in a tenant-scoped table; audit log writes are app","explanation":"## Why This Is Asked\n\nThis question probes understanding of secure multi-tenant design, auditable operations, and basic access control in a beginner-friendly setting.\n\n## Key Concepts\n\n- JWT-based auth with tenant_id boundaries\n- Row-Level Security or tenant-scoped data stores\n- Immutable/append-only audit log with canonical fields\n- Model versioning exposure and validation\n- End-to-end test strategy for security and data isolation\n\n## Code Example\n\n```javascript\n// Pseudo middleware to verify JWT and enforce tenant boundary\nfunction authorize(req) {\n  const token = req.headers['authorization'];\n  const payload = verifyJwt(token);\n  if (!payload || payload.tenant_id !== req.params.tenant_id) throw new Error('Unauthorized');\n  return payload;\n}\n```\n\n## Follow-up Questions\n\n- How would you validate audit logs to detect tampering?\n- How would you rotate signing keys without downtime?\n","diagram":"flowchart TD\n  Client(Client) -->|signed JWT| Gateway[API Gateway]\n  Gateway --> Score[CCA Score Service]\n  Score --> Audit[Audit Log]\n  Score --> DB[(Scores DB)]\n  Score --> Tenant[Tenant Isolation]","difficulty":"beginner","tags":["cca"],"channel":"cca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Snowflake","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T16:49:54.046Z","createdAt":"2026-01-17T16:49:54.046Z"},{"id":"q-3528","question":"You're adding an A/B testing layer to a beginner-friendly per-user cca_score API that uses a 2-feature linear model. The API accepts user_id, region, and an optional experiment_id. Describe how to deterministically assign users to variants, store per-experiment feature deltas, ensure consistent variant assignment per user, and design a safe canary rollout with rollback criteria. Include data models and a minimal test plan?","answer":"Deterministic hash on user_id and experiment_id to assign a variant per region; store experiment config in a small table (experiment_id, region, variant, delta1, delta2, active). On each request compu","explanation":"## Why This Is Asked\nAssess understanding of simple A/B testing, deterministic variant assignment, and safe rollouts in a global, multi-tenant cca_score service.\n\n## Key Concepts\n- Deterministic assignment\n- Minimal experiment schema\n- Canary rollout and rollback criteria\n- Idempotent reads/writes and testing\n\n## Code Example\n```javascript\nfunction assign(userId, experimentId) {\n  const seed = `${userId}|${experimentId}`;\n  const h = hash32(seed);\n  return h % 2;\n}\n```\n\n## Follow-up Questions\n- How would you handle data privacy constraints across regions during AB tests?\n- How would you detect and respond to drift in variant performance?","diagram":"flowchart TD\n  A[Start] --> B[Compute variant]\n  B --> C{Canary?}\n  C -->|Yes| D[Route 1% to new variant]\n  C -->|No| E[Route 100% to preferred variant]\n  D --> F[Monitor; if ok, continue; else rollback]\n  F --> G[End]","difficulty":"beginner","tags":["cca"],"channel":"cca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T20:31:16.448Z","createdAt":"2026-01-17T20:31:16.449Z"},{"id":"q-3593","question":"You're releasing an offline-first mobile app that uses a per-user cca_score API with a simple 2-feature model. Design a beginner-friendly offline sync flow for intermittent connectivity: client caches locally, applies optimistic updates, and reconciles with the server on reconnect. Outline the server data model and a minimal endpoint (POST /cca_score/sync) with fields device_id, user_id, model_version, client_revision, local_changes, server_time, and a simple conflict rule (latest revision wins). Include a basic test plan?","answer":"Implement an offline-first synchronization architecture where each device maintains a local queue of cca_score changes. Upon reconnection, the client POSTs to /cca_score/sync with the payload {device_id, user_id, model_version, client_revision, local_changes, server_time}. The server applies a conflict resolution rule where the latest revision wins, processes changes idempotently using change_id for deduplication, and returns the updated server_revision for the next synchronization cycle.","explanation":"## Why This Is Asked\nTests practical offline-first data flow and basic conflict resolution skills, which are essential for mobile-centric applications in multi-tenant environments.\n\n## Key Concepts\n- Offline caching with local change queue\n- Model versioning and revision-based merging\n- Idempotent writes with change_id deduplication\n- Simple conflict resolution: latest revision wins\n\n## Code Example\n```javascript\n// Pseudo endpoint contract for /cca_score/sync\n{ device_id, user_id, model_version, client_revision, local_changes, server_time }\n```\n\n## Follow-up Questions\n- How would you extend this to handle concurrent edits from multiple devices?\n- What optimizations would you add for large datasets?\n- How would you implement incremental sync instead of full synchronization?","diagram":"flowchart TD\n A[Device Offline] --> B[Cache Local Changes]\n B --> C[Reconnect]\n C --> D[Sync Endpoint]\n D --> E{Conflict?}\n E -- Yes --> F[Apply Delta, Update Revision]\n E -- No --> G[Confirm & Update Server]\n G --> H[Client Receives Server State]","difficulty":"beginner","tags":["cca"],"channel":"cca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","MongoDB","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T05:31:37.526Z","createdAt":"2026-01-17T22:39:33.957Z"},{"id":"q-3615","question":"Design a privacy-preserving, multi-tenant cca_score pipeline with per-tenant isolation and configurable DP noise. Provide end-to-end flow, a data model sketch, and an API spec for POST /cca_score/compute with tenant_id, user_id, features, model_version, epsilon, timestamp. Include a canary rollout plan and testing strategy for privacy guarantees, accuracy, latency, and auditability?","answer":"The end-to-end pipeline implements per-tenant isolation through dedicated data partitions, applies input masking and feature normalization, then injects differentially private noise calibrated by epsilon during scoring. API: POST /cca_score/compute with payload {tenant_id, user_id, features, model_version, epsilon, timestamp}.","explanation":"## Why This Is Asked\nEvaluates ability to design privacy-conscious, multi-tenant systems with differential privacy guarantees.\n\n## Key Concepts\n- Multi-tenant data isolation\n- Differential privacy mechanisms\n- Data modeling and API design\n- Canary deployment strategies\n- Observability and audit trails\n\n## Code Example\n```javascript\nfunction addNoise(score, epsilon) {\n  // Laplace/Gaussian noise injection based on epsilon\n}\n```\n\n## Follow-up Questions\n- How to validate differential privacy guarantees in production?\n- How to adapt epsilon per tenant without causing model drift?","diagram":null,"difficulty":"intermediate","tags":["cca"],"channel":"cca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Square","Two Sigma","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T05:14:48.431Z","createdAt":"2026-01-17T23:38:10.699Z"},{"id":"q-3695","question":"You're designing a real-time per-user cca_score service for Snowflake, Uber, and MongoDB where tenants enforce data-retention policies and deletion requests must purge data across streaming and batch layers. Outline an end-to-end flow: tombstone propagation (tenant_id, user_id, before_ts, model_version), purge semantics in state stores, auditability, and rollback. Propose a minimal API for deletions, a data model sketch, and a test plan for late data and clock skew?","answer":"Describe tombstone-driven purge: emit deletion events with tenant_id, user_id, before_ts, model_version to the event log; purge immediately in streaming and batch stores; retain audit logs and immutab","explanation":"## Why This Is Asked\nTests data governance, multi-tenant isolation, and purge correctness across streaming and batch layers. DEMO: tombstone propagation, per-tenant retention, and auditable data lineage with rollback capability.\n\n## Key Concepts\n- Tombstone events for deletions across all paths\n- Per-tenant data retention and purge semantics\n- Immutable audit logs and data lineage\n- Canary rollback and rollback safety checks\n- Testing under late data and clock skew\n\n## Code Example\n```javascript\n// Pseudo-code: emit tombstone and purge\nfunction deleteTenantUserData(tenantId, userId, beforeTs, modelVersion) {\n  const tombstone = {type:'tombstone', tenantId, userId, beforeTs, modelVersion, ts: Date.now()};\n  eventBus.emit('cca.delete', tombstone);\n  // downstream will purge relevant streams and state stores\n}\n```\n\n## Follow-up Questions\n- How would you verify purge completeness across multiple regions and data stores?\n- How would you handle partial purges during network or store degradation and ensure audit integrity?","diagram":null,"difficulty":"intermediate","tags":["cca"],"channel":"cca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","Snowflake","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T05:43:18.297Z","createdAt":"2026-01-18T05:43:18.297Z"},{"id":"q-3709","question":"You're migrating a per-user cca_score service to a multi-tenant, privacy-preserving store with tenant-scoped masking and encryption. Design a concrete rollout plan: (1) data model changes (tenant_id, user_id, model_version, features, score, audit_id) with per-tenant masking, (2) API changes for model_version-aware scoring and privacy settings, (3) zero-downtime migration with canary rollout and rollback criteria, (4) testing plan including privacy validation and performance under high load. Provide concrete steps and milestones?","answer":"Phased rollout: 1) extend data model with tenant_id, user_id, model_version, features, score; add per-tenant masking and encryption. 2) implement a model_version-aware /cca_score API with privacy flag","explanation":"## Why This Is Asked\nTests practical multi-tenant privacy migrations, data modeling, and rollout discipline for cca_score.\n\n## Key Concepts\n- Multi-tenant isolation and per-tenant masking\n- Encryption at rest and data-access controls\n- Shadow writes, feature flags, and canary rollouts\n- Zero-downtime migration and rollback criteria\n- Privacy-focused testing and performance validation\n\n## Code Example\n```javascript\n// Example masking before write\nconst masked = maskFeature(tenantId, features);\ndb.insert('cca_score_masked', { tenant_id, user_id, model_version, masked, score });\n```\n\n## Follow-up Questions\n- How would you validate no PII leakage in dashboards?\n- How would you monitor drift and trigger rollback across tenants?\n","diagram":"flowchart TD\n  A(Migration Start) --> B(Encrypt per-tenant data)\n  B --> C(Shadow writes to masked table)\n  C --> D(Canary rollout by tenant flag)\n  D --> E(Cutover and monitor)\n  E --> F(Outcome: success or rollback)","difficulty":"intermediate","tags":["cca"],"channel":"cca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Plaid","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T06:47:32.566Z","createdAt":"2026-01-18T06:47:32.566Z"},{"id":"q-3785","question":"You’re building a privacy-preserving, explainable cca_score API for a multi-tenant app where some tenants require differential privacy and strict data residency. The API must return a per-user score plus a concise explanation that does not reveal training data or model internals, while enforcing tenant isolation and model_versioning. Describe the API surface (endpoints and payloads), the data model, privacy controls (masking, differential privacy), and a testing plan including explainability fidelity and privacy risk assessment. Include a simple flow diagram and a sample canary test scenario?","answer":"Design a multi-tenant cca_score service with endpoints POST /cca_score/generate (tenant_id, user_id, model_version, features), POST /cca_score/explain (tenant_id, user_id, model_version), GET /cca_sco","explanation":"## Why This Is Asked\n\nAssesses ability to design privacy-preserving, explainable, multi-tenant scoring systems with model versioning and data residency constraints.\n\n## Key Concepts\n\n- Differential privacy controls and masking in explanations\n- Tenant isolation and per-tenant policies\n- Model versioning, auditing, and explainability fidelity\n\n## Code Example\n\n```javascript\n// Pseudo-endpoint sketch for generate\napp.post('/cca_score/generate', (req, res) => {\n  const {tenant_id, user_id, model_version, features} = req.body;\n  // validate policy, fetch model, compute score, apply DP if enabled\n  res.json({score, explanation});\n});\n```\n\n## Follow-up Questions\n\n- How would you test explainability fidelity under DP constraints?\n- What edge cases would trigger rollback or audit alerts?","diagram":"flowchart TD\n  A(ClientRequest) --> B[ValidateTenantPolicy]\n  B --> C[ScoreComputation(model_version)]\n  C --> D[Masking/DPApply]\n  D --> E[ExplainGeneration]\n  E --> F[ReturnScoreAndExplain]","difficulty":"intermediate","tags":["cca"],"channel":"cca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Discord","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T09:36:12.638Z","createdAt":"2026-01-18T09:36:12.639Z"},{"id":"q-3922","question":"You're adding a beginner-friendly cca_score API variant where explanations are provided through a short-lived token instead of containing training data. Design the API flow for POST /cca_score/compute with user_id, model_version, locale, device_id, and explain=true. Server returns score and an explanation_token encoding model_version, user_id, and expiry. Describe the minimal data model, token format (JWT HS256), and a basic test plan validating token issuance, expiry, and revocation?","answer":"Return a short-lived JWT (explanation_token) that encodes model_version, user_id, and expiry. POST /cca_score/compute with user_id, model_version, locale, device_id, and explain. Response: score and a","explanation":"## Why This Is Asked\nThe tokenized explanation approach isolates training data exposure, supporting privacy and auditability for beginners.\n\n## Key Concepts\n- JWT-based explanation tokens, HS256 signing\n- model_versioning, token rotation, expiry\n- input validation and per-user audit\n\n## Code Example\n```javascript\n// Issue explain token (Node.js)\nconst jwt = require('jsonwebtoken');\nfunction issueExplainToken(user_id, model_version, expiryMs, secret) {\n  const payload = { sub: user_id, mv: model_version, exp: Math.floor(Date.now()/1000) + expiryMs/1000 };\n  return jwt.sign(payload, secret, { algorithm: 'HS256', audience: user_id });\n}\n```\n\n## Follow-up Questions\n- How would you rotate signing keys and invalidate tokens after a model version update?\n- How would you test for token forgery or replay attacks?","diagram":null,"difficulty":"beginner","tags":["cca"],"channel":"cca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Google","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T15:38:04.113Z","createdAt":"2026-01-18T15:38:04.113Z"},{"id":"q-3948","question":"You're building a privacy-conscious cca_score API for a multi-tenant platform with data residency and regulatory constraints. Design a policy-driven routing layer that, per request, deterministically selects region and model_version based on tenant SLA, current load, and compliance constraints, ensuring tenant isolation and reproducible scoring. Describe the routing logic, data paths, and rollback tests?","answer":"Design a policy-driven routing layer that deterministically selects region and model_version per request using a hash-based routing key built from tenant_id, SLA tier, and current load; enforce tenant","explanation":"## Why This Is Asked\nTests ability to design policy-driven routing under strict data residency and regulatory constraints, ensuring isolation and reproducibility in production.\n\n## Key Concepts\n- Deterministic routing that respects SLAs and regulatory constraints\n- Policy engine integrating tenant profiles, region rules, and load metrics\n- Data isolation via tenant-scoped stores and sandboxes\n- Drift detection, regulatory-change hooks, and rollback mechanisms\n- Canary toggles and latency budgets to limit blast radius\n\n## Code Example\n```javascript\n// Pseudo-code for routing decisions\nfunction route(req, policies, metrics){\n  const key = hash(req.tenantId + req.sla + metrics.loadByRegion);\n  const region = selectRegion(key, policies);\n  const modelVersion = selectModel(req.tenantId, region, policies);\n  return { region, modelVersion };\n}\n```\n\n## Follow-up Questions\n- How would you test deterministic routing under region failover?\n- How would regulatory changes mid-flight trigger rollback and re-routing?","diagram":null,"difficulty":"advanced","tags":["cca"],"channel":"cca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Lyft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T16:45:13.663Z","createdAt":"2026-01-18T16:45:13.663Z"},{"id":"q-4108","question":"You're integrating a per-user cca_score API used by partner apps. Design a beginner-friendly rate-limiting and auditing layer to prevent abuse while preserving latency. Outline (1) per-tenant quotas and a simple token-bucket or fixed-window policy, (2) an API contract for POST /cca_score including fields tenant_id, user_id, model_version, timestamp, and quota_token, and (3) a minimal observable audit log and testing approach?","answer":"Implement per-tenant quotas using a Redis-backed token bucket algorithm: allocate N tokens per tenant per minute, with each API call consuming one token. For POST /cca_score, validate the tenant's bucket before processing; if insufficient tokens remain, return HTTP 429. The API contract requires tenant_id, user_id, model_version, timestamp, and quota_token fields for proper tracking and auditability.","explanation":"## Why This Is Asked\nThis evaluates your ability to design simple, production-ready rate limiting that provides tenant isolation while maintaining low latency for partner integrations.\n\n## Key Concepts\n- Rate limiting algorithms: token bucket vs fixed window approaches\n- Per-tenant resource isolation and quota management\n- API observability and audit trail requirements\n- Request tracing and idempotency considerations\n\n## Code Example\n```javascript\n// Node.js implementation using Redis\nconst rateLimit = async (tenantId, maxPerMinute) => {\n  const key = `quota:${tenantId}`;\n  const window = Math.floor(Date.now() / 60000);\n  const count = await redis.incr(`${key}:${window}`);\n  \n  if (count === 1) await redis.expire(`${key}:${window}`, 60);\n  if (count > maxPerMinute) {\n    throw new Error('429 Too Many Requests');\n  }\n  return true;\n};\n```\n\n## Follow-up Considerations\n- How would you handle burst capacity vs sustained rates?\n- What monitoring metrics would you track for quota usage?\n- How would you implement graceful degradation during Redis outages?","diagram":"flowchart TD\n  Client[Partner App] --> API[POST /cca_score]\n  API --> RateLimiter[Rate Limiter (Redis)]\n  RateLimiter --> ScoreService[CCA Score Service]\n  ScoreService --> AuditLog[(Audit Log)]\n  ScoreService --> Cache[Response Cache]","difficulty":"beginner","tags":["cca"],"channel":"cca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Discord","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-19T05:30:30.987Z","createdAt":"2026-01-19T02:41:48.843Z"},{"id":"q-4156","question":"You're running a multi-tenant cca_score API with tenants having different data distributions and privacy budgets. Design a drift-detection and retraining workflow that updates models per-tenant without impacting others. Describe telemetry, metrics, data versioning, and a canary rollout plan for retraining; include endpoints and data schema changes?","answer":"Per-tenant drift-detection and retraining: monitor per-tenant feature distributions and score calibration with KS-test and KL divergence, using a tenant-scoped feature store version. Trigger canary re","explanation":"## Why This Is Asked\nThis question probes the ability to design per-tenant adaptive ML pipelines with isolation, privacy, and governance under drift.\n\n## Key Concepts\n- Drift detection (KS, KL) and score calibration\n- Per-tenant data versioning, feature store\n- Canary rollout and rollback, online/offline validation\n- Telemetry, dashboards, alerting, data residency considerations\n\n## Code Example\n```python\nfrom scipy.stats import ks_2samp\ndef drift_pvalue(train, prod):\n    return ks_2samp(train, prod).pvalue  # per-tenant drift score\n```\n\n## Follow-up Questions\n- How would you handle drift across tenants with different privacy budgets?\n- What metrics would you surface in a per-tenant dashboard?","diagram":"flowchart TD\n  Ingest[Telemetry Ingest] --> FeatureStore[Feature Store vX]\n  FeatureStore --> Scoring[cca_score Service]\n  Scoring --> Drift[Drift Monitor]\n  Drift --> Retrain[Retraining Pipeline]\n  Retrain --> Canary[Canary Rollout]\n  Canary --> Deploy[Regional Deployments]","difficulty":"intermediate","tags":["cca"],"channel":"cca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Google","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-19T05:46:29.746Z","createdAt":"2026-01-19T05:46:29.746Z"},{"id":"q-4306","question":"You're running a real-time per-user cca_score service across tenants with varied data residency rules. When a new model_version is deployed, design a drift-detection and observability workflow that can detect per-tenant degradation and trigger safe rollbacks. Specify: API surface to trigger drift checks and fetch per-tenant reports; data model for drift metrics; how to synthesize and test drift; and the canary rollout strategy with rollback criteria. Include a simple diagram and a sample canary test scenario?","answer":"Propose a per-tenant drift framework: trigger drift checks via POST /cca_score/drift_check with payload {tenant_id, model_version, window}, return per-tenant drift_score and top contributing features.","explanation":"## Why This Is Asked\nEnsures robust observability, tenant isolation, and safe release discipline for production ML services.\n\n## Key Concepts\n- Per-tenant drift detection across model_versions\n- Canary rollout and rollback criteria\n- Data residency and privacy considerations\n- Drift metrics schema and reporting\n- Synthetic drift testing and alerting\n\n## Code Example\n```javascript\n// Pseudo drift score computation between old and new distributions\nfunction computeDrift(oldDist, newDist){ \n  let drift = 0;\n  for (let i = 0; i < oldDist.length; i++){\n    let p = oldDist[i] || 1e-9;\n    let q = newDist[i] || 1e-9;\n    drift += p * Math.log(p / q);\n  }\n  return drift;\n}\n```\n\n## Follow-up Questions\n- How would you handle a tenant with drift but small user base?\n- How would you scale drift computation to thousands of tenants?","diagram":"flowchart TD\n  A[Deploy new model_version] --> B[Run per-tenant drift checks]\n  B --> C{Drift detected?}\n  C -->|Yes| D[Incremental canary + alerting]\n  C -->|No| E[Continue traffic]\n  D --> F[Rollback if thresholds exceeded]\n  F --> G[Full rollback or hotfix if needed]","difficulty":"intermediate","tags":["cca"],"channel":"cca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-19T13:09:59.359Z","createdAt":"2026-01-19T13:09:59.359Z"},{"id":"q-4358","question":"You're adding drift detection and per-tenant calibration to a multi-tenant cca_score service. Design a practical experiment framework to detect score drift across tenants, enforce per-tenant calibration, and provide explainable drift reports without exposing training data. Specify metrics (AUC, calibration, PSI), data lineage, and a dedicated API to fetch drift reports; outline a test plan for clock skew and late data?","answer":"Design a drift-detection framework for multi-tenant cca_score. For each tenant, track rolling feature distributions and calibration, compute PSI and KL divergence, monitor AUC/calibration drift, and a","explanation":"## Why This Is Asked\nWhy drift detection and robust, explainable drift reports matter for regulated tenants.\n\n## Key Concepts\n- Drift metrics: PSI, KL divergence, AUC calibration\n- Per-tenant calibration and data lineage\n- Canary rollout and rollback with safety nets\n\n## Code Example\n```python\ndef psi(expected, actual, bins=10):\n    # placeholder: compute Population Stability Index per feature\n    return 0.0\n```\n\n## Follow-up Questions\n- How would you handle late data in drift assessments?\n- How would you test explainability of drift reports without exposing training data? ","diagram":"flowchart TD\nA[Ingest events] --> B[Feature Store]\nB --> C[Scoring]\nC --> D[Drift Monitor]\nD --> E[Alerts]\nD --> F[Drift Report API]","difficulty":"intermediate","tags":["cca"],"channel":"cca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","OpenAI","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-19T15:43:08.321Z","createdAt":"2026-01-19T15:43:08.321Z"},{"id":"q-4423","question":"You're adding lightweight telemetry for a beginner-friendly cca_score API to monitor health while avoiding PII. Design the telemetry contract and a minimal ingest endpoint (POST /cca_score/telemetry) with fields: anonymous_id, model_version, region, latency_ms, status_code, timestamp, sample_rate, and an obfuscated user_id field. Propose 1% sampling, retention, masking rules, and a basic test plan including privacy checks and canary rollout?","answer":"Proposed telemetry: POST /cca_score/telemetry with payload: anonymous_id, model_version, region, latency_ms, status_code, timestamp, sample_rate, user_id_hash. Use 1% sampling at the client, redact or","explanation":"## Why This Is Asked\nTests ability to add lightweight observability while preserving privacy at a beginner level.\n\n## Key Concepts\n- Lightweight telemetry contracts\n- Privacy-first data collection (masking/hashing, no PII)\n- Sampling strategies and data retention\n- End-to-end validation and canary rollout\n\n## Code Example\n```javascript\n// Example payload for ingest\n{\n  \"anonymous_id\": \"a1b2c3\",\n  \"model_version\": \"v1\",\n  \"region\": \"eu\",\n  \"latency_ms\": 42,\n  \"status_code\": 200,\n  \"timestamp\": \"2026-01-19T12:34:56Z\",\n  \"sample_rate\": 0.01\n}\n```\n\n## Follow-up Questions\n- How would you scale the ingest layer for burst traffic?\n- How would you validate privacy compliance across regions?","diagram":"flowchart TD\n  A[Client CTA to /cca_score] --> B[Emit telemetry event]\n  B --> C[Ingest API /cca_score/telemetry]\n  C --> D[Time-series store by region/model_version]\n  D --> E[Monitoring dashboards]","difficulty":"beginner","tags":["cca"],"channel":"cca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Robinhood","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-19T18:43:23.335Z","createdAt":"2026-01-19T18:43:23.335Z"},{"id":"q-4473","question":"You're building a multi-tenant cca_score service where tenants cannot share user data but want a globally calibrated score. Design an API surface and data model for a federated calibration flow (e.g., POST /cca_score/calibrate) that returns a per-tenant calibration factor and a privacy-preserving explanation. Explain how to aggregate without leaking tenant data, version calibration, and a test plan including privacy audits, drift detection, canary rollout, and rollback?","answer":"Architect a federated calibration service: each tenant stores a local factor f_t; a central aggregator uses secure aggregation with differential privacy to emit a per-tenant factor and a masked explan","explanation":"## Why This Is Asked\nEvaluates ability to architect federated calibration across tenants with privacy constraints, versioning, and realistic API design.\n\n## Key Concepts\n- Federated calibration\n- Secure aggregation\n- Differential privacy\n- Model versioning\n- Canary rollout\n\n## Code Example\n```javascript\n// Pseudo-handler outline\nasync function calibrate(req, res) {\n  const { tenant_id, model_version, privacy_settings } = req.body;\n  // fetch local factor or compute via secure aggregation\n  // return { tenant_id, factor, explanation_masked, model_version }\n}\n```\n\n## Follow-up Questions\n- How ensure data residency per tenant while aggregating?\n- How would you monitor drift in calibration factors over time?\n","diagram":null,"difficulty":"advanced","tags":["cca"],"channel":"cca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Lyft","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-19T20:35:38.606Z","createdAt":"2026-01-19T20:35:38.607Z"},{"id":"q-4778","question":"You're deploying an edge-first cca_score pipeline: a 2-feature model runs on-device with on-device differential privacy and signed model manifests. Describe the end-to-end data flow, data retention, and how the device fetches a signed manifest, computes the score, and posts an attestation back for verification. Include endpoints, data formats, and a testing plan for offline and tamper-resistance scenarios?","answer":"On-device cca_score computes a 2-feature model locally with DP noise; no PII leaves the device. The server issues a signed manifest (model_version, weights_hash, dp_epsilon, expiry). The app fetches i","explanation":"## Why This Is Asked\nReveal skills in edge ML, security, and observability for production systems.\n\n## Key Concepts\n- Edge inference, differential privacy, secure manifests, attestation, auditability.\n- Data integrity, key management, token signing, tamper-evident logs.\n- Testing for offline operation, drift, and adversarial tampering.\n\n## Code Example\n```javascript\n// Minimal attestation signing flow (pseudocode)\nfunction attest(deviceId, modelVersion, score, privateKey) {\n  const payload = JSON.stringify({deviceId, modelVersion, score, ts: Date.now()});\n  return sign(payload, privateKey);\n}\n```\n\n## Follow-up Questions\n- How would you rotate keys and revoke devices without service downtime?\n- How do you monitor drift between local scores and server-side expectations?","diagram":"flowchart TD\n  App[Mobile App] --> Manifest[GET /cca_score/manifest]\n  Manifest --> App\n  App --> Score[Compute score on-device]\n  Score --> Attest[POST /cca_score/attest]\n  Attest --> Server[Validate & store audit logs]","difficulty":"intermediate","tags":["cca"],"channel":"cca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Oracle","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T11:49:38.443Z","createdAt":"2026-01-20T11:49:38.444Z"},{"id":"q-4810","question":"You're building a beginner-friendly cca_score API for a privacy-conscious fintech app that must comply with data residency: all tenant data stays in a single region. Design an endpoint POST /cca_score/explain that returns a per-user score, a concise explanation, and a data_hash for lineage. Explanations must not reveal training data or internals, and must be deterministic per user using a per-tenant salt. Include data models, routing strategy, and a minimal test plan?","answer":"Design POST /cca_score/explain returning {score: float, explanation: string, data_hash: string}. Use a per-tenant salt to make explanations deterministic and non-revealing about training data. Route t","explanation":"## Why This Is Asked\n\nTests ability to design privacy-conscious explainability with data residency constraints and per-tenant isolation. Introduces deterministic explanations and a lineage hash, plus a regional routing strategy and tests.\n\n## Key Concepts\n\n- Data residency: route to regional stores to keep data within tenant region\n- Deterministic explainability: salt-based seeding so identical inputs yield same explanation\n- Data lineage: data_hash for provenance without exposing training data\n- Caching and security: cache explain results; avoid PII in explanations\n- Test plan: privacy, residency, explainability fidelity, rollback capability\n\n## Code Example\n\n```javascript\nfunction explain(score, features, nonce, tenantSalt) {\n  const seed = hmacSHA256(tenantSalt, nonce + JSON.stringify(features)).slice(0,8);\n  const top = features.slice(0,2).map((f)=>f.name);\n  return `Score ${score.toFixed(2)} mainly driven by ${top[0]} and ${top[1]}`;\n}\n```\n\n## Follow-up Questions\n\n- How would you detect explainability drift across model_versions?\n- How would you verify residency rules across tenants when scaling regions?","diagram":null,"difficulty":"beginner","tags":["cca"],"channel":"cca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Coinbase","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T14:39:48.663Z","createdAt":"2026-01-20T14:39:48.666Z"},{"id":"q-4843","question":"You're migrating a high-traffic cca_score service to a multi-tenant SaaS with a shared feature store. Some tenants require strict data residency and complete cross-tenant isolation to prevent leakage via timing/side channels. Design a per-tenant feature namespace, API surface (endpoints and payloads), data model, privacy controls, and a testing plan that proves no leakage, includes differential privacy, auditing, and a rollback plan with deterministic canary checks?","answer":"Propose a per-tenant feature namespace in a shared feature store, with tenant_id-scoped partitions and cryptographic masking to prevent cross-tenant inferences. API surface: POST /cca_score (tenant_id","explanation":"## Why This Is Asked\n\nTests cross-tenant isolation, data residency, and practical DP/audit considerations in a shared store.\n\n## Key Concepts\n\n- Cross-tenant isolation\n- Per-tenant namespaces in a shared feature store\n- Data residency and access audits\n- DP masking and leakage testing\n- Deterministic API design for migrations\n\n## Code Example\n\n```javascript\n// Example payload\n{\n  tenant_id: 't123',\n  user_id: 'u456',\n  features: { f1: 0.3, f2: 0.8 },\n  model_version: 'v2'\n}\n```\n\n## Follow-up Questions\n\n- How would you enforce per-tenant residency across cloud regions?\n- How would you measure and bound cross-tenant leakage risk in production?","diagram":"flowchart TD\n  A[Per-tenant namespace] --> B[Isolated feature partitions]\n  B --> C[API surface design]\n  C --> D[Privacy masking & DP]\n  D --> E[Auditing & rollback]","difficulty":"advanced","tags":["cca"],"channel":"cca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Netflix","Snowflake","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T16:00:06.813Z","createdAt":"2026-01-20T16:00:06.813Z"},{"id":"q-840","question":"In a secure messaging service, ciphertexts are decrypted by a server with a decryption oracle exposed to clients, creating a potential CCA risk. Design a practical IND-CCA secure scheme for message confidentiality using existing primitives (e.g., OAEP, AES-GCM, MACs). Explain how you prevent chosen-ciphertext attacks, outline a concrete protocol, and discuss trade-offs?","answer":"Use Encrypt-then-MAC: ciphertext = AES-GCM(key, plaintext, nonce) plus a separate MAC over the ciphertext. Verify MAC before decryption and bound decryption attempts to prevent the oracle from learnin","explanation":"## Why This Is Asked\n\nThis checks understanding of IND-CCA security and how to build practical, protocol-level defenses in modern cryptographic systems.\n\n## Key Concepts\n\n- IND-CCA and chosen-ciphertext resistance\n- Encrypt-then-MAC vs MAC-then-Encrypt\n- Nonce management, associated data, and MAC verification order\n- Trade-offs: performance, ciphertext size, API surface\n\n## Code Example\n\n```javascript\nfunction encrypt(plaintext, key) {\n  const nonce = crypto.randomBytes(12);\n  const cipher = crypto.createCipheriv('aes-128-gcm', key, nonce);\n  const enc = Buffer.concat([cipher.update(plaintext, 'utf8'), cipher.final()]);\n  const tag = cipher.getAuthTag();\n  const payload = Buffer.concat([nonce, enc, tag]);\n  const mac = crypto.createHmac('sha256', key).update(payload).digest();\n  return { payload, mac };\n}\n\nfunction decrypt(payload, mac, key) {\n  if (!verifyMac(mac, payload, key)) throw new Error('MAC fail');\n  const nonce = payload.slice(0, 12);\n  const enc = payload.slice(12, -16);\n  const tag = payload.slice(-16);\n  const decipher = crypto.createDecipheriv('aes-128-gcm', key, nonce);\n  decipher.setAuthTag(tag);\n  return Buffer.concat([decipher.update(enc), decipher.final()]);\n}\n```\n\n## Follow-up Questions\n\n- How would you audit for decryption oracle leakage?\n- How would you enforce constant-time MAC checks and retry limits to prevent side channels?","diagram":null,"difficulty":"intermediate","tags":["cca"],"channel":"cca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Microsoft","Oracle","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:24:47.474Z","createdAt":"2026-01-12T13:24:47.474Z"},{"id":"q-879","question":"Describe a cross-region user preferences syncing protocol using MongoDB that tolerates regional partitions. Specify the data model (per-field version stamps), conflict resolution policy, and read/write configurations. Provide a concrete merge approach and an example conflict scenario?","answer":"Model a cross-region user preferences store in MongoDB where each user document carries per-field version stamps: {userId, prefs, version: {ts, clientId}}. Writes publish (ts, clientId). Conflict reso","explanation":"## Why This Is Asked\nThis question probes cross-region consistency, conflict resolution, and practical MongoDB usage.\n\n## Key Concepts\n- CAP trade-offs in distributed systems\n- Per-field versioning and conflict resolution\n- ReadConcern/WriteConcern in MongoDB\n\n## Code Example\n```javascript\nfunction mergePrefs(existing, incoming) {\n  const out = JSON.parse(JSON.stringify(existing || {}));\n  for (const key of Object.keys(incoming)) {\n    const inc = incoming[key];\n    const cur = out[key];\n    const tsInc = inc.__version?.ts ?? 0;\n    const tsCur = cur?.__version?.ts ?? -1;\n    if (tsInc > tsCur || (tsInc === tsCur && (inc.__version?.clientId ?? 0) < (cur?.__version?.clientId ?? 0))) {\n      out[key] = inc;\n    }\n  }\n  return out;\n}\n```\n\n## Follow-up Questions\n- How would you test this merge under simultaneous updates?\n- How would you adapt for field-level strong consistency needs?","diagram":null,"difficulty":"advanced","tags":["cca"],"channel":"cca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","MongoDB","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:58:27.264Z","createdAt":"2026-01-12T13:58:27.264Z"},{"id":"q-970","question":"You're shipping an E2E chat feature for an internal Meta–Microsoft product. An attacker may access a decryption oracle. Outline a concrete IND-CCA2 secure scheme for message exchange using public-key crypto, detailing padding (OAEP), a KEM/DEM split or AEAD wrapper, ephemeral keys, and how you bind metadata (timestamps, sender IDs) to prevent malleability. What are the failure modes and mitigations?","answer":"Propose ECIES-X25519 with AES-256-GCM as the DEM, using a fresh ephemeral key per message and OAEP-like padding on the KEK if using RSA. Bind metadata to the AEAD as AAD (sender, recipient, timestamp)","explanation":"## Why This Is Asked\nTests understanding of IND-CCA2 security in practical chat scenarios and how to harden PKE with AEAD wrappers.\n\n## Key Concepts\n- IND-CCA2 security, decryption-oracle threats\n- ECIES/KEM-DEM patterns, ephemeral keys\n- AEAD with AAD binding metadata\n- Padding and malleability considerations\n\n## Code Example\n```javascript\n// Pseudo: derive KEK with ephemeral ECDH, encrypt payload with AES-GCM using AAD\n```\n\n## Follow-up Questions\n- How would you audit for relicensing or side-channel risks in your scheme?\n- How do you rotate keys and handle forward secrecy at scale?","diagram":"flowchart TD\n  A[Sender] --> B[Encrypt with ephemeral KEK]\n  B --> C[Ciphertext with AAD]\n  C --> D[Transmit ciphertext]\n  D --> E[Receiver decrypts to plaintext]","difficulty":"advanced","tags":["cca"],"channel":"cca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T17:33:55.215Z","createdAt":"2026-01-12T17:33:55.215Z"}],"subChannels":["general"],"companies":["Adobe","Airbnb","Amazon","Anthropic","Apple","Bloomberg","Citadel","Cloudflare","Coinbase","Databricks","Discord","DoorDash","Goldman Sachs","Google","Hashicorp","Hugging Face","IBM","Instacart","LinkedIn","Lyft","Meta","Microsoft","MongoDB","NVIDIA","Netflix","OpenAI","Oracle","PayPal","Plaid","Robinhood","Salesforce","Scale Ai","Snap","Snowflake","Square","Stripe","Tesla","Twitter","Two Sigma","Uber","Zoom"],"stats":{"total":54,"beginner":12,"intermediate":21,"advanced":21,"newThisWeek":41}}