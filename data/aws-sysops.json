{"questions":[{"id":"q-1004","question":"Scenario: A serverless API stack (API Gateway, Lambda, DynamoDB, S3, CloudFront) runs in us-east-1 with a DR region eu-west-1. Propose an automated DR plan using AWS Global Accelerator and Route 53 health checks to failover within 15 minutes. Include data replication choices, Lambda versioning strategy, S3 replication mode, and a safe rollback/verification approach that avoids production impact during tests?","answer":"Implement with AWS Global Accelerator exposing endpoint groups in us-east-1 and eu-west-1, backed by Route 53 health checks to flip traffic to DR within 15 minutes. Use Lambda aliases with canary prom","explanation":"## Why This Is Asked\nTests practical DR planning for serverless stacks, emphasizing automation, data replication choices, and safe testing.\n\n## Key Concepts\n- AWS Global Accelerator and multi-region endpoints\n- Route 53 health checks and failover routing\n- Lambda versioning and canary deployments\n- DynamoDB Global Tables and S3 cross-region replication\n- Canary DR testing and rollback workflows\n\n## Code Example\n```javascript\n// Promote DR version by updating Route53/alias configuration (illustrative)\nimport { Route53Client, ChangeResourceRecordSetsCommand } from '@aws-sdk/client-route-53';\n\nconst client = new Route53Client({region: 'us-east-1'});\nasync function failover() {\n  // Upsert DR-records to route traffic to DR region\n  await client.send(new ChangeResourceRecordSetsCommand({/* ... */}));\n}\n```\n\n## Follow-up Questions\n- How would you validate read/write integrity after failover?\n- What metrics and alarms ensure timely detection and safe rollback?","diagram":null,"difficulty":"intermediate","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T18:49:55.875Z","createdAt":"2026-01-12T18:49:55.875Z"},{"id":"q-1291","question":"In a 12-account AWS Organization, you must implement near real-time central audit logging for VPC Flow Logs, Lambda logs, and RDS logs in a dedicated Logging account. Design the end-to-end mechanism to ship logs from all member accounts to the central account, ensuring secure cross-account access, encryption, and resilience across Regions. What is your approach?","answer":"Design a centralized Logging account for all CloudWatch Logs (VPC Flow Log, Lambda, RDS) from 12 member accounts using CloudWatch Logs subscriptions to a Kinesis Data Firehose in the central account. ","explanation":"## Why This Is Asked\nTests ability to design cross-account log centralization with real-time data flow, security controls, and disaster recovery. Demonstrates practical use of CloudWatch Logs subscriptions, cross-account roles, and Kinesis Firehose in a scalable multi-account setup.\n\n## Key Concepts\n- Cross-account IAM roles and trust policies for log producers\n- CloudWatch Logs subscriptions as a transport to a central sink\n- Kinesis Data Firehose as the delivery path to S3 (encrypted with a central KMS key)\n- Central data lake with proper retention, access controls, and lifecycle\n- Regional resilience via multi-region delivery or replication\n\n## Code Example\n```javascript\n// Pseudo-CDK-like snippet for cross-account log delivery to Firehose\nconst role = new iam.Role(this, 'CWLogsToFirehose', {\n  assumedBy: new iam.AccountPrincipal('CENTRAL_ACCOUNT_ID')\n});\nrole.addToPolicy(new iam.PolicyStatement({\n  actions: ['firehose:PutRecord', 'firehose:PutRecordBatch'],\n  resources: [firehose.attrArn]\n}));\n```\n\n## Follow-up Questions\n- How would you validate coverage and detect delivery failures across accounts?\n- What changes would you make to handle a new region or account being added?","diagram":null,"difficulty":"advanced","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","Salesforce","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T08:35:57.950Z","createdAt":"2026-01-13T08:35:57.950Z"},{"id":"q-1481","question":"In a multi-account AWS Organization for a global SaaS app, mandate data residency: S3 buckets and DynamoDB tables used by customer data must not store or replicate data outside the designated region per account. Design an automated governance solution using SCPs, AWS Config, IAM Roles, EventBridge, and Lambda to enforce this, with testing and alerting?","answer":"Implement per-account guardrails: SCPs deny any S3/DynamoDB write or replication to non-designated regions; AWS Config custom rules audit region compliance across services; EventBridge captures drift ","explanation":"## Why This Is Asked\n\nTests knowledge of cross-account governance, data residency constraints, and automation at scale. It probes how to translate policy into enforceable controls and how to validate effectiveness.\n\n## Key Concepts\n\n- AWS Organizations SCPs\n- AWS Config custom rules and remediation\n- IAM Roles and cross-account access\n- EventBridge for event-driven governance\n- Lambda remediation and data movement\n\n## Code Example\n\n```python\nimport boto3\n\ndef is_in_designated_region(bucket_arn, target_region):\n    s3 = boto3.client('s3')\n    bucket = bucket_arn.split(':')[-1]\n    loc = s3.get_bucket_location(Bucket=bucket).get('LocationConstraint') or 'us-east-1'\n    return loc == target_region\n```\n\n## Follow-up Questions\n\n- How would you test across all accounts without affecting production data?\n- What are potential race conditions with cross-region replication and eventual consistency?","diagram":null,"difficulty":"advanced","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","IBM","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T18:54:11.477Z","createdAt":"2026-01-13T18:54:11.477Z"},{"id":"q-1536","question":"Design a DR orchestration for a global app with primary S3 (with Object Lock) and DynamoDB in us-east-1 and a DR site in us-west-2. Require 15 min RTO and 5 min RPO, data residency, automated backups via AWS Backup, cross-region replication that respects residency, and Route 53 failover. Describe a Step Functions workflow to automate failover, validation, and rollback without production impact, with testing cadence and success criteria?","answer":"Design a Step Functions–driven DR orchestration with AWS Backup vaults in us-east-1 and us-west-2 for S3 and DynamoDB; implement S3 cross-region replication with Object Lock and bucket versioning; utilize DynamoDB Global Tables or PITR; configure Route 53 failover routing with health checks; establish automated failback and rollback procedures.","explanation":"## Why This Is Asked\nThis tests practical cross-region DR orchestration with data residency constraints and non-disruptive testing capabilities.\n\n## Key Concepts\n- AWS Backup vaults per region with restoration windows\n- S3 Object Lock, cross-region replication, and versioning\n- DynamoDB backups vs Global Tables and PITR\n- Route 53 failover routing and health checks\n- Step Functions for end-to-end DR workflow automation and rollback\n\n## Code Example\n```json\n{\n  \"Comment\": \"DR workflow\",\n  \"StartAt\": \"PreFlight\",\n  \"States\": {\n    \"PreFlight\": { \"Type\": \"Task\", \"Next\": \"Backup\" },\n    \"Backup\": { \"Type\": \"Task\", \"Next\": \"Validation\" },\n    \"Validation\": { \"Type\": \"Task\", \"Next\": \"Failover\" },\n    \"Failover\": { \"Type\": \"Task\", \"Next\": \"PostFailoverCheck\" },\n    \"PostFailoverCheck\": { \"Type\": \"Task\", \"Next\": \"Success\" },\n    \"Success\": { \"Type\": \"Succeed\" }\n  }\n}\n```\n\n## Testing Cadence\n- Monthly DR drills with non-disruptive testing\n- Quarterly full failover validation\n- Weekly health check verification\n\n## Success Criteria\n- RTO ≤ 15 minutes, RPO ≤ 5 minutes\n- Zero data loss with Object Lock compliance\n- Automated rollback within 10 minutes\n- Health check response time < 30 seconds","diagram":"flowchart TD\n  A[Primary region us-east-1] -->|data residency check| B{DR eligible?}\n  B -->|Yes| C[Trigger DR in us-west-2]\n  C --> D[Route 53 DNS switch]\n  B --> E[Continue in primary if not]\n","difficulty":"intermediate","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Hashicorp","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T06:33:50.831Z","createdAt":"2026-01-13T20:52:52.321Z"},{"id":"q-1542","question":"In a three-account AWS Organization (prod, staging, dev) for a global SaaS app, enforce cost governance by requiring that all new resources carry the tags Owner and Environment in prod, with automated remediation for noncompliant resources and separate budgets/alerts per account. Design an end-to-end process using SCPs, AWS Config rules, EventBridge, Lambda, and AWS Budgets to detect, remediate, and alert. Include testing steps?","answer":"Policy: Implement an SCP requiring Owner and Environment tags for all production resources. Deploy AWS Config rules to detect missing tags on EC2, S3, and RDS resources. Configure EventBridge to trigger Lambda functions that automatically apply missing tags—using Owner from a lookup table (defaulting to 'unassigned') and Environment derived from account context. Establish separate AWS Budgets per account with alerts at 80% and 100% thresholds. Noncompliant resources persisting beyond 24 hours are flagged for manual review.","explanation":"## Why This Is Asked\nTests practical governance implementation using cross-account controls, automated remediation, and comprehensive cost visibility.\n\n## Key Concepts\n- AWS Organizations SCPs for tag enforcement across production accounts\n- AWS Config custom rules for continuous tag compliance monitoring\n- EventBridge for routing noncompliance events to remediation workflows\n- Lambda functions for automated tag application and resource remediation\n- AWS Budgets with per-account alerts for proactive cost governance\n\n## Code Example\n```javascript\nconst AWS = require('aws-sdk');\nconst ec2 = new AWS.EC2();\n\nasync function tagResource(arn, tags) {\n  await ec2.createTags({\n    Resources: [arn],\n    Tags: Object.entries(tags).map(([k, v]) => ({ Key: k, Value: v }))\n  });\n}\n```","diagram":"flowchart TD\n  A[Policy (SCP)] --> B[AWS Config Rule]\n  B --> C[EventBridge]\n  C --> D[Lambda remediation]\n  D --> E[Budgets/Alerts]","difficulty":"beginner","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","DoorDash","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T06:32:03.298Z","createdAt":"2026-01-13T21:30:58.393Z"},{"id":"q-1591","question":"In a multi-account AWS Organization hosting a global service, a misconfigured VPC peering or transit gateway leaks a route to the internet, risking data exposure and egress costs. Design an automated, end-to-end remediation and validation workflow (detection, isolation, rollback) using AWS Config and Config Rules, EventBridge, Lambda, IAM roles, and CloudWatch alarms, plus a controlled traffic test before rollback. What steps and artifacts would you implement?","answer":"The remediation workflow employs a Config custom rule to detect unintended 0.0.0.0/0 routes on VPCs or leaking transit gateway attachments. Upon detection, a Lambda function with least-privilege access removes the leaking route, detaches problematic transit gateway attachments, and updates route tables to route traffic through private endpoints. The workflow includes validation via controlled traffic testing before rollback, with CloudWatch alarms monitoring for continued leaks.","explanation":"## Why This Is Asked\nTests automated containment and rollback capabilities for network leaks at scale across multiple AWS accounts and regions.\n\n## Key Concepts\n- AWS Config rules and automated remediation\n- VPC route tables, Internet Gateways, and Transit Gateways\n- EventBridge, Lambda, and IAM least privilege\n- Canary testing and safe rollback strategies\n- Service Control Policies (SCPs) to harden cross-account controls\n\n## Code Example\n```python\n# Pseudo remediation outline\ndef remediation(event, context):\n    vpc_id = extract_vpc(event)\n    remove_leaking_route(vpc_id)\n    detach_transit_gateway_attachments(vpc_id)\n    update_route_tables(vpc_id)\n    validate_remediation(vpc_id)\n```","diagram":"flowchart TD\n  A[Detect leak] --> B[Isolate VPC and remove leaking route]\n  B --> C[Route to private subnet/NAT]\n  C --> D[Canary test]\n  D -- success --> E[Keep changes]\n  D -- fail --> F[Rollback to last good config]","difficulty":"intermediate","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Instacart","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T05:35:11.138Z","createdAt":"2026-01-13T22:56:15.361Z"},{"id":"q-1634","question":"How would you implement a guardrail so every new EC2 instance in a single AWS account for a mobile backend launches only in Prod-Subnet, carries Owner and Environment tags, and uses IAM role MobileBackendRole, with automated remediation and alerts via AWS Config, Lambda, EventBridge, and SNS while providing a testing plan?","answer":"Use an AWS Config Custom Rule to verify that each new EC2 instance is launched in Prod-Subnet, has tags Owner and Environment, and assumes IAM role MobileBackendRole. A Lambda remediation tags missing","explanation":"## Why This Is Asked\nTests guardrail automation basics for beginners; uses Config rules, Lambda remediation, EventBridge, SNS; ensures remediation and alerting.\n\n## Key Concepts\n- AWS Config Custom Rule\n- Lambda remediation\n- IAM roles and resource tagging\n- Subnet scoping\n- EventBridge and SNS integration\n\n## Code Example\n```javascript\n// Lambda remediation skeleton\nexports.handler = async (event) => {\n  // parse Config rule evaluation result\n  // attach missing tags or stop instance\n  return;\n}\n```\n\n## Follow-up Questions\n- How would you extend to multiple subnets or regions?\n- How would you handle resources in flight?\n- How would you reduce false positives and test coverage?","diagram":"flowchart TD\n  A[Launch Event] --> B[AWS Config Rule]\n  B --> C{Compliant?}\n  C -- Yes --> D[No Action]\n  C -- No --> E[Lambda Remediation]\n  E --> F[SNS Alert]","difficulty":"beginner","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","DoorDash"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T04:20:04.577Z","createdAt":"2026-01-14T04:20:04.577Z"},{"id":"q-1672","question":"Across a three-account AWS Org (prod, staging, dev) hosting a Databricks-powered data lake on S3, design an automated end-to-end remediation to ensure all compute resources access S3 only via VPC Endpoints, enforce private DNS, and block public S3 access. Include SCPs, AWS Config rules, EventBridge, Lambda, and GuardDuty findings with testing steps and rollback?","answer":"Enforce S3 VPCE-only access across accounts. Use SCPs to require VPCE usage, Config rules to flag public buckets, and EventBridge/Lambda to apply SourceVpce conditions and BlockPublicAccess. Tie findi","explanation":"## Why This Is Asked\n\nTests governance and automation for cross-account data lakes. Requires practical use of SCPs, Config, EventBridge, Lambda, and GuardDuty, plus testing and rollback strategies.\n\n## Key Concepts\n\n- SCPs and least privilege\n- AWS Config rules (custom or managed)\n- VPC Endpoints and SourceVpce conditions\n- S3 Block Public Access & bucket policies\n- EventBridge-driven remediation\n- GuardDuty findings integration\n\n## Code Example\n\n```javascript\n// Example: not provided; governance policy appears in IAM and bucket policy snippets in docs\n```\n\n## Follow-up Questions\n\n- How would you test this in a multi-region Databricks deployment?\n- How do you handle exceptions for shared data buckets?","diagram":"flowchart TD\n  A[Compute Resources] --> B[S3 Access Route: VPCE]\n  B --> C[S3 Bucket Policy: SourceVpce]\n  C --> D[BlockPublicAccess Enabled]\n  E[AWS Config + SCP] --> F[Remediation via EventBridge]\n  F --> G[GuardDuty/CloudWatch Alerts]","difficulty":"intermediate","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Hashicorp"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T05:58:44.344Z","createdAt":"2026-01-14T05:58:44.345Z"},{"id":"q-1765","question":"Within a multi-account AWS environment across six regions hosting a critical financial service, implement automated detection and remediation of IAM role trust policy misconfigurations that could expose cross-account access. Use AWS Config, Lambda, EventBridge, IAM Access Analyzer, and SCPs in a central Governance account. Outline controls, testing, and rollback?","answer":"Detect and remediate cross-account trust misconfig via a central Config rule that flags IAM Role AssumeRole policies granting principals outside allowed accounts; feed IAM Access Analyzer findings; tr","explanation":"## Why This Is Asked\nTests ability to design cross-account IAM governance with automated remediation and testing.\n\n## Key Concepts\n- IAM trust policy analysis; Access Analyzer; Config rules\n- Event-driven remediation; Lambda automation\n- SCPs; centralized auditing; rollback planning\n\n## Code Example\n```javascript\n// Pseudo: list roles and check trust policy\nconst AWS = require('aws-sdk');\nconst iam = new AWS.IAM();\n\nasync function findCrossAccountRoles() {\n  const roles = await iam.listRoles().promise();\n  const risky = roles.Roles.filter(r => {\n     const policy = JSON.parse(r.AssumeRolePolicyDocument || '{}');\n     const statements = policy.Statement || [];\n     return statements.some(s => s.Principal && s.Principal.AWS && !s.Principal.AWS.startsWith('arn:aws:iam::' + YOUR_ACCOUNT_ID));\n  });\n  return risky;\n}\n```\n\n## Follow-up Questions\n- How would you avoid false positives for legitimate cross-account roles used by partners?\n- How would you test this in a canary or pilot account without affecting production?","diagram":null,"difficulty":"advanced","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Goldman Sachs","Lyft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T09:49:25.368Z","createdAt":"2026-01-14T09:49:25.368Z"},{"id":"q-1886","question":"In a 3-account AWS Organization (prod, staging, dev) for a global SaaS app, enforce a global IAM password policy across all accounts: min length 14, require uppercase, lowercase, number, symbol, password age 90 days, and forbid reuse of last 5. Design an end-to-end automation using SCPs, an AWS Config custom rule, EventBridge, Lambda, and AWS Budgets to detect drift, remediate, and alert. Include testing steps?","answer":"Implement a centralized policy in a master account. Use an SCP to prevent downgrades of password policy, a Config custom rule that calls GetAccountPasswordPolicy to validate all accounts, and a Lambda","explanation":"## Why This Is Asked\nTests ability to design automated governance for IAM password policies across a multi-account org with basic tooling.\n\n## Key Concepts\n- IAM password policy and enforcement across accounts\n- AWS Config custom rules for drift detection\n- Lambda remediation patterns\n- Service Control Policies (SCPs) for guardrails\n- AWS Budgets for cost awareness\n\n## Code Example\n```python\nimport boto3\ndef handler(event, context):\n    iam = boto3.client('iam')\n    iam.update_account_password_policy(\n        MinimumPasswordLength=14,\n        RequireSymbols=True,\n        RequireNumbers=True,\n        RequireUppercaseCharacters=True,\n        RequireLowercaseCharacters=True,\n        PasswordReusePrevention=5,\n        MaxPasswordAge=90\n    )\n```\n\n## Follow-up Questions\n- How would you handle exceptions for service accounts that cannot meet policy?\n- How would you test disaster recovery if policy drift cannot be remediated automatically?","diagram":null,"difficulty":"beginner","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T15:46:32.526Z","createdAt":"2026-01-14T15:46:32.526Z"},{"id":"q-1934","question":"In a 5-account, multi-region AWS setup (prod, prod-sec, staging, dev, shared) spanning 3 regions, design an automated disaster recovery plan for a critical app running on **EKS** and **RDS**. The DR must auto‑failover **RDS** to cross‑region replicas, switch **Route 53** DNS, re‑sync **EKS** state with **ArgoCD**, rotate encryption keys, and enforce **SCPs** for DR accounts. Include testing, rollback, and post‑drill validation?","answer":"Plan: Trigger a DR workflow via EventBridge and Step Functions. Promote cross-region RDS replicas to primary, switch Route 53 DNS failover to DR region, re-sync EKS state with ArgoCD to a DR cluster, ","explanation":"## Why This Is Asked\nTests multi-account, multi-region DR engineering under real-world constraints with automation, guardrails, and validation.\n\n## Key Concepts\n- Cross-region DR orchestration with EventBridge and Step Functions\n- RDS cross-region replica promotion and failover behavior\n- Route 53 DNS failover and health checks across regions\n- EKS state synchronization via ArgoCD in DR context\n- KMS key rotation and backup integrity under DR\n- IAM guardrails via SCPs and drift detection with AWS Config\n\n## Code Example\n```json\n{\n  \"Comment\": \"DR state machine skeleton\",\n  \"StartAt\": \"PromoteDRRDS\",\n  \"States\": {\n    \"PromoteDRRDS\": {\"Type\":\"Task\",\"Resource\":\"arn:aws:lambda:REGION:ACCOUNT:function:PromoteDRRDS\"},\n    \"FailoverDNS\": {\"Type\":\"Task\",\"Resource\":\"arn:aws:lambda:REGION:ACCOUNT:function:FailoverDNS\"}\n  }\n}\n```\n\n## Follow-up Questions\n- How would you verify DR readiness and measure RTO/RPO?\n- What drill failure modes would you simulate and why?","diagram":"flowchart TD\n  A[DR Trigger] --> B[DR State Machine]\n  B --> C[RDS Promote]\n  B --> D[DNS Failover]\n  B --> E[EKS ArgoCD Sync]\n  B --> F[Key Rotation]","difficulty":"advanced","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["OpenAI","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T17:47:58.636Z","createdAt":"2026-01-14T17:47:58.636Z"},{"id":"q-1949","question":"In a 3-account AWS Organization (prod, staging, dev) for a global SaaS app, configure automatic idle-instance remediation. If an EC2 instance in prod has the tag AutoStop=true and CPUUtilization < 5% for 24h, stop it at 02:00 local time. Use AWS Config to detect noncompliance, EventBridge/Lambda to stop, and a Config rule to enforce tag presence. Include testing steps?","answer":"Detect prod EC2s with AutoStop=true and avg CPU <5% over 24h via a Config rule; schedule a nightly EventBridge at 02:00 local to trigger a Lambda that stops those instances. Gate with an IAM role perm","explanation":"## Why This Is Asked\nThis tests practical automation of cost control via server idle detection and cross-account governance, using Config, EventBridge, and Lambda with policy constraints.\n\n## Key Concepts\n- AWS Config rules and compliance state\n- EventBridge scheduling and Lambda remediations\n- EC2 CPU metrics and idle-detection patterns\n- IAM, SCPs, cross-account governance\n- Testing and observability\n\n## Code Example\n```python\nimport boto3\n# Pseudo: identify instances with tag AutoStop and low CPU, then stop them\n```\n\n## Follow-up Questions\n- How would you avoid stopping workloads that occasionally spike?\n- How would you test rollback if a stopped instance regains load?","diagram":null,"difficulty":"beginner","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T18:46:15.080Z","createdAt":"2026-01-14T18:46:15.080Z"},{"id":"q-2009","question":"In a single-region AWS setup hosting a small SaaS app, design a weekly automated DR test that validates restoring an RDS snapshot and at least one EBS volume, then runs a basic end-to-end check against the app. Outline practical steps using AWS Config, EventBridge, Lambda, and a separate DR bucket/account for test artifacts, including rollback steps and how you verify success?","answer":"Propose a weekly DR test pipeline in one region: schedule with EventBridge rule to trigger a Lambda that (1) creates an RDS snapshot and EBS volume snapshot; (2) copies snapshots to a DR S3 bucket or ","explanation":"## Why This Is Asked\n\nTests DR automation in a realistic, beginner-friendly context, focusing on end-to-end orchestration, cross-account artifacts, and visible validation without heavy complexity.\n\n## Key Concepts\n\n- Event-driven orchestration with EventBridge and Lambda\n- Snapshot management for RDS and EBS\n- Cross-account or isolated DR artifact storage\n- Basic health checks and rollback planning\n\n## Code Example\n\n```javascript\n// Minimal Lambda skeleton for DR test orchestration\nexports.handler = async () => {\n  // 1) createRDSnapshot, 2) snapshotEBSVolume, 3) copyArtifactsToDR, 4) restoreInDR, 5) runHealthCheck\n  return {status: 'DR test started'}\n}\n```\n\n## Follow-up Questions\n\n- How would you verify idempotence of the DR test?\n- What metrics and alerts would you configure for PASS/FAIL?\n","diagram":null,"difficulty":"beginner","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Hashicorp","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T20:47:09.139Z","createdAt":"2026-01-14T20:47:09.139Z"},{"id":"q-2046","question":"In a multi-region AWS deployment (prod across 4 regions) with accounts prod, security, infra, and audit, design an automated containment workflow triggered by GuardDuty findings of UnauthorizedAccess or PrivilegeEscalation. Automatically quarantine affected EC2s by swapping in a deny-all security group and routing traffic to a quarantine subnet, while persisting original SGs for rollback. Restoration requires security approval. Include cross-region EventBridge routing, Lambda orchestration, IAM permissions, rollback, and testing?","answer":"GuardDuty findings publish to EventBridge in the security account. A cross-account Lambda quarantines the EC2 by swapping to a deny-all security group and moving it to a quarantine subnet, storing the original security group configuration in SSM Parameter Store for rollback.","explanation":"## Why This Is Asked\n\nAssesses practical, scalable containment automation across accounts and regions with governance controls.\n\n## Key Concepts\n\n- GuardDuty to EventBridge integration across accounts\n- Cross-account Lambda orchestration with least privilege\n- Rapid network isolation via deny-all security group and quarantine subnet routing\n- Rollback capability using SSM Parameter Store (original security groups)\n- Approved restoration workflow (SNS or Systems Manager Automation)\n\n## Code Example\n\n```javascript\n// Skeleton containment handler\nexports.handler = async (event) => {\n  // parse GuardDuty finding\n  const finding = event.detail;\n  if (finding.type !== 'UnauthorizedAccess' && \n      finding.type !== 'PrivilegeEscalation') {\n    return;\n  }\n  \n  // extract EC2 instance details\n  const instanceId = finding.resource.instanceDetails.instanceId;\n  const region = finding.resource.region;\n  \n  // store original security groups for rollback\n  const ec2 = new AWS.EC2({ region });\n  const originalSGs = await ec2.describeInstances({\n    InstanceIds: [instanceId]\n  }).promise();\n  \n  // persist to SSM Parameter Store\n  await new AWS.SSM().putParameter({\n    Name: `/quarantine/${instanceId}/original-sgs`,\n    Value: JSON.stringify(originalSGs),\n    Type: 'String',\n    Overwrite: true\n  }).promise();\n  \n  // apply deny-all security group and quarantine subnet\n  await ec2.modifyInstanceAttribute({\n    InstanceId: instanceId,\n    Groups: ['sg-deny-all']\n  }).promise();\n  \n  // notify security team\n  await new AWS.SNS().publish({\n    TopicArn: process.env.SECURITY_TOPIC_ARN,\n    Message: JSON.stringify({\n      instanceId,\n      findingType: finding.type,\n      action: 'quarantined',\n      timestamp: new Date().toISOString()\n    })\n  }).promise();\n};\n```","diagram":"flowchart TD\n  GD[GuardDuty Finding] --> EB[EventBridge Rule]\n  EB --> L[Containment Lambda]\n  L --> SG[Update SG to DenyAll]\n  L --> Q[Quarantine Subnet Route]\n  SG --> Rb[Rollback Data in SSM]\n  Q --> Rb\n  Rb --> A[Await Restore Approval]","difficulty":"advanced","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Meta","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T06:07:35.908Z","createdAt":"2026-01-14T22:31:13.452Z"},{"id":"q-2136","question":"In a 3-account AWS Organization (prod, staging, dev) for a global SaaS app, enforce a secure S3 baseline in prod: no public access, SSE-KMS with a central CMK, and versioning enabled on all buckets. Use AWS Config rules (e.g., s3-bucket-public-access-prohibited, s3-bucket-server-side-encryption-enabled, s3-bucket-versioning-enabled), EventBridge, Lambda, and SCPs to detect, remediate, and alert. Include testing steps and rollback plan?","answer":"Define a prod baseline with Config rules for public access, encryption, and versioning; trigger a Lambda remediation via EventBridge to block public access, attach CMK-based SSE, and enable versioning","explanation":"## Why This Is Asked\n\nTests practical guardrails, cross-account enforcement, and hands-on use of AWS Config, EventBridge, Lambda, and SCPs to maintain a secure baseline.\n\n## Key Concepts\n\n- AWS Config managed rules for S3 posture\n- EventBridge-Lambda remediation\n- S3 SSE-KMS using central CMK\n- Versioning and bucket policy impact\n- Service Control Policies for drift prevention\n\n## Code Example\n\n```python\n# Placeholder remediation logic sketch\ndef remediate_bucket(bucket_name):\n    enable_versioning(bucket_name)\n    apply_sse_kms(bucket_name, 'arn:aws:kms:...:cmk')\n    block_public_access(bucket_name)\n```\n\n## Follow-up Questions\n\n- How would you test cross-account drift and rollback scenarios?\n- How would you adapt for buckets owned by third-party accounts?","diagram":null,"difficulty":"beginner","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","Oracle","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T04:18:13.152Z","createdAt":"2026-01-15T04:18:13.152Z"},{"id":"q-2196","question":"In a 4-account Organization across 3 regions hosting a real-time analytics stack (Kinesis Data Streams, S3 data lake, DynamoDB Global Tables), design an automated DR test and remediation plan that validates cross-region replication, IAM role trust policies, and automated failover with minimal downtime. Include tooling (AWS Config rules, EventBridge, Lambda, SSM Automation), rollback strategy, and testing steps?","answer":"Design a DR runbook using AWS Config to validate cross-region replication (DynamoDB Global Tables, S3), with an EventBridge trigger invoking a Lambda that runs an SSM Automation document to switch end","explanation":"## Why This Is Asked\nTests practical, scalable DR planning across accounts/regions with native AWS tooling and automated rollback.\n\n## Key Concepts\n- Automated DR testing across accounts/regions\n- AWS Config, EventBridge, Lambda, SSM Automation\n- DynamoDB Global Tables, S3 replication, endpoints\n- Rollback strategies and DR metrics\n\n## Code Example\n```javascript\n// Pseudo-implementation sketch for cutover\nconst cutover = async () => {\n  // disable primary region ingress\n  // enable secondary region endpoints\n  // validate replication lag < threshold\n};\n```\n\n## Follow-up Questions\n- How would you test the rollback triggers and ensure no data loss? \n- What metrics would you collect to meet RPO/RTO targets?","diagram":"flowchart TD\n  A[DR Trigger] --> B[Validate Replication]\n  B --> C{Healthy?}\n  C -->|Yes| D[Switch Endpoints to DR Region]\n  C -->|No| E[Abort & Notify]\n  D --> F[Run Synthetic Checks]\n  F --> G[Rollback if Needed]","difficulty":"advanced","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Netflix","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T07:03:48.463Z","createdAt":"2026-01-15T07:03:48.463Z"},{"id":"q-2241","question":"In a multi-account, multi-region production setup hosting a real-time analytics microservice, design an automated DR failover/fallback that shifts traffic to a hot standby region within 5 minutes of regional outage while preserving data consistency. Specify the wiring of Route 53 failover, AWS Global Accelerator, DynamoDB global tables, S3 cross-region replication, and IAM boundaries, plus orchestration via Step Functions/Lambda, post-failover checks, testing, and rollback?","answer":"Propose a DR framework with 2 regions (primary and hot-standby) using Route 53 failover routing, AWS Global Accelerator, and two DynamoDB global tables; S3 cross-region replication for assets; per-acc","explanation":"## Why This Is Asked\n\nAssesses advanced DR design across multiple AWS services, cross-account governance, data consistency, testing rigor, and rollback plans under real-world constraints.\n\n## Key Concepts\n\n- Route 53 failover routing and health checks\n- AWS Global Accelerator for fast global routing\n- DynamoDB global tables for cross-region writes\n- S3 cross-region replication for assets\n- IAM boundary policies and per-region KMS keys\n- Step Functions / Lambda for orchestration and playbooks\n- DR testing, rollback, and cost controls\n\n## Code Example\n\n```javascript\n// Minimal Step Functions skeleton for DR orchestration (pseudo-structure)\n{\n  \"Comment\": \"DR failover workflow\",\n  \"StartAt\": \"HealthCheck\",\n  \"States\": {\n    \"HealthCheck\": {\n      \"Type\": \"Task\",\n      \"Resource\": \"arn:aws:lambda:REGION:ACCOUNT:function:HealthCheck\",\n      \"Next\": \"Evaluate\"\n    },\n    \"Evaluate\": {\n      \"Type\": \"Choice\",\n      \"Choices\": [\n        { \"Variable\": \"$.outage\", \"BooleanEquals\": true, \"Next\": \"InitiateFailover\" }\n      ],\n      \"Default\": \"Done\"\n    },\n    \"InitiateFailover\": {\n      \"Type\": \"Task\",\n      \"Resource\": \"arn:aws:states:::aws-sdk:route53:changeResourceRecordSets\",\n      \"End\": true\n    },\n    \"Done\": { \"Type\": \"Succeed\" }\n  }\n}\n```\n\n## Follow-up Questions\n\n- How would you validate RTO/RPO during a simulated outage without impacting customers?\n- What metrics and alarms would you attach to the DR workflow to ensure timely detection and rollback readiness?","diagram":null,"difficulty":"advanced","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Hashicorp","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T08:59:43.906Z","createdAt":"2026-01-15T08:59:43.907Z"},{"id":"q-2323","question":"Beginner scenario: A SaaS app stores per-tenant data in a single S3 bucket and serves tenants from a shared app tier. Design a cost- and security-friendly, auditable isolation model using per-tenant S3 Access Points and IAM roles, plus an Org SCP to prevent cross-account bucket listing. Outline required policies, Access Point config, testing, and rollback steps?","answer":"Implement per-tenant S3 Access Points with dedicated IAM roles and organizational SCP enforcement. Create Access Points for each tenant with network and policy controls, assign least-privilege IAM roles scoped to specific Access Points, and enforce cross-account boundaries with SCPs blocking ListBucket operations.","explanation":"## Why This Is Asked\nTests knowledge of tenant isolation mechanisms in S3 using Access Points and granular IAM roles, plus organizational governance via SCPs. It also checks practical testing and rollback planning.\n\n## Key Concepts\n- S3 Access Points and per-tenant isolation\n- IAM roles with least privilege\n- Organization SCPs and cross-account boundaries\n- Testing isolation and rollback strategies\n\n## Implementation Architecture\n\n**1. S3 Bucket Structure**\n```\ns3://multi-tenant-data/\n├── tenant-a/\n│   ├── documents/\n│   └── uploads/\n├── tenant-b/\n│   ├── documents/\n│   └── uploads/\n```\n\n**2. Per-Tenant Access Point Configuration**\n```bash\n# Create Access Point for tenant-a\naws s3control create-access-point \\\n  --account-id 123456789012 \\\n  --name tenant-a-ap \\\n  --bucket multi-tenant-data \\\n  --vpc-configuration VpcId=vpc-12345678 \\\n  --policy '{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Principal\": {\"AWS\": \"arn:aws:iam::123456789012:role/tenant-a-role\"},\n            \"Action\": [\"s3:GetObject\", \"s3:PutObject\"],\n            \"Resource\": \"arn:aws:s3:us-east-1:123456789012:accesspoint/tenant-a-ap/object/tenant-a/*\"\n        }\n    ]\n  }'\n```\n\n**3. IAM Role Policy (Least Privilege)**\n```json\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"s3:GetObject\",\n        \"s3:PutObject\",\n        \"s3:DeleteObject\"\n      ],\n      \"Resource\": \"arn:aws:s3:us-east-1:123456789012:accesspoint/tenant-a-ap/object/tenant-a/*\"\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": \"s3:ListAccessPoints\",\n      \"Resource\": \"arn:aws:s3control:us-east-1:123456789012:accesspoint\"\n    }\n  ]\n}\n```\n\n**4. Organizational SCP Policy**\n```json\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Deny\",\n      \"Action\": \"s3:ListBucket\",\n      \"Resource\": \"arn:aws:s3:::multi-tenant-data\",\n      \"Condition\": {\n        \"StringNotEquals\": {\n          \"aws:PrincipalAccount\": [\"123456789012\"]\n        }\n      }\n    }\n  ]\n}\n```\n\n**5. Testing Isolation Procedure**\n```bash\n#!/bin/bash\n# isolation_test.sh\n\n# Test 1: Verify tenant cannot access other tenant data\naws s3 cp s3://multi-tenant-data/tenant-b/test.txt ./test.txt \\\n  --profile tenant-a \\\n  2>/dev/null && echo \"FAIL: Cross-tenant access\" || echo \"PASS: Isolation working\"\n\n# Test 2: Verify tenant can access own data\naws s3 cp ./test.txt s3://multi-tenant-data/tenant-a/test.txt \\\n  --profile tenant-a \\\n  && echo \"PASS: Own access working\" || echo \"FAIL: Own access denied\"\n\n# Test 3: Verify SCP blocks cross-account listing\naws s3 ls s3://multi-tenant-data --profile external-account \\\n  2>/dev/null && echo \"FAIL: SCP not blocking\" || echo \"PASS: SCP blocking\"\n```\n\n**6. Rollback Strategy**\n```bash\n#!/bin/bash\n# rollback_procedure.sh\n\n# Step 1: Disable new Access Points\naws s3control delete-access-point --account-id 123456789012 --name tenant-a-ap\n\n# Step 2: Restore direct bucket access (temporary)\naws iam put-role-policy \\\n  --role-name tenant-a-role \\\n  --policy-name DirectBucketAccess \\\n  --policy-document '{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [{\n        \"Effect\": \"Allow\",\n        \"Action\": [\"s3:GetObject\", \"s3:PutObject\"],\n        \"Resource\": \"arn:aws:s3:::multi-tenant-data/tenant-a/*\"\n    }]\n  }'\n\n# Step 3: Validate rollback\n./isolation_test.sh\n\n# Step 4: Re-create Access Points with corrected config\naws s3control create-access-point --name tenant-a-ap-fixed --bucket multi-tenant-data\n```\n\n**7. Monitoring and Auditing**\n- CloudTrail logs for all S3 Access Point operations\n- S3 access logs filtered by Access Point ARN\n- Config rules to monitor Access Point policy changes\n- CloudWatch alarms for cross-tenant access attempts","diagram":null,"difficulty":"beginner","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","IBM","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":["s3 access points","per-tenant isolation","iam roles","least privilege","organizational scps","cross-account boundaries","testing isolation","rollback strategies"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2026-01-20T05:40:04.513Z","createdAt":"2026-01-15T11:53:55.515Z"},{"id":"q-2359","question":"Single-region web app on EC2 behind an Application Load Balancer experiences daily traffic spikes. Outline a beginner-friendly Auto Scaling setup using a Launch Template, an ASG with Target Tracking on CPU, a CloudWatch alarm, and ALB health checks. Include how you would test scaling in and out and rollback if metrics misbehave?","answer":"Create a Launch Template with the app AMI and a simple user-data startup; ASG with min 2, max 20, desired 2; Target Tracking policy on CPU 50% with 300s cooldown; ALB health checks enabled; CloudWatch","explanation":"## Why This Is Asked\n\nAssesses ability to translate beginner concepts into a concrete AWS setup, focusing on practical wiring of Launch Templates, ASGs, Target Tracking, CloudWatch alarms, and health checks, plus testing and rollback.\n\n## Key Concepts\n\n- Launch Templates vs Launch Configs\n- Auto Scaling Groups with Target Tracking\n- CloudWatch alarms and evaluation periods\n- Application Load Balancer health checks\n- Safe rollback strategies\n\n## Code Example\n\n```yaml\n# ASG/Launch Template snippet (illustrative)\nLaunchTemplate:\n  Name: MyAppLT\n  Data:\n    ImageId: ami-0abcdef123456\n    InstanceType: t3.medium\n    UserData: |\n      #!/bin/bash\n      yum update -y\n      systemctl start myapp\n```\n\n## Follow-up Questions\n\n- How would you handle longer startup times or flaky health checks?\n- What are limits and costs to consider when scaling rapidly?","diagram":null,"difficulty":"beginner","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Hugging Face","IBM"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T14:47:18.302Z","createdAt":"2026-01-15T14:47:18.303Z"},{"id":"q-2576","question":"In a two-region real-time trading data platform, design an automated security incident response workflow triggered by a GuardDuty finding that an EC2 is compromised. Outline the end-to-end flow: finding → EventBridge → Lambda to (a) attach a restrictive security group, (b) snapshot/quarantine EBS volumes, (c) pause real-time Kinesis streams, (d) copy forensic data to a dedicated S3 bucket, (e) alert on-call via SNS. Include rollback, cross-region considerations, and a test plan?","answer":"Design an automated security incident response workflow for a two-region real-time trading data platform triggered by a GuardDuty EC2 compromise finding. The end-to-end flow begins with GuardDuty generating a high-severity finding that routes through EventBridge to invoke a central Lambda orchestrator. The Lambda executes five critical containment actions: (1) immediately applies a restrictive security group blocking all traffic except necessary security operations, (2) creates EBS snapshots of all attached volumes and detaches them to a quarantine availability zone, (3) pauses real-time Kinesis data streams to prevent data exfiltration while preserving in-flight records, (4) copies forensic artifacts including memory dumps, logs, and network captures to a dedicated S3 bucket with cross-region replication, and (5) alerts on-call security teams via SNS with detailed incident context. The workflow includes comprehensive rollback procedures to restore normal operations after investigation, with cross-region considerations encompassing active-active failover, data consistency validation, and regional coordination to ensure comprehensive containment while maintaining trading platform availability.","explanation":"## Why This Is Asked\nTests hands-on incident response automation across AWS services, emphasizing idempotence, cross-region containment, and auditability under time pressure.\n\n## Key Concepts\n- GuardDuty findings, EventBridge routing, Lambda orchestration\n- IAM least privilege and cross-account roles\n- EBS snapshots, volume quarantine, and forensics data collection\n- Real-time data streams (Kinesis) pause/resume mechanics\n- S3 data retention policies and cross-region replication\n- SNS alerting and rollback procedures\n\n## Code Example\n```python\n# Lambda handler sketch (high-level)\nimport json\nimport boto3\nfrom datetime import datetime\n\ndef lambda_handler(event, context):\n    # Parse GuardDuty finding\n    finding = event['detail']\n    instance_id = finding['resource']['instanceDetails']['instanceId']\n    \n    # Initialize AWS clients\n    ec2 = boto3.client('ec2')\n    kinesis = boto3.client('kinesis')\n    s3 = boto3.client('s3')\n    sns = boto3.client('sns')\n    \n    # Execute containment actions\n    apply_restrictive_sg(instance_id)\n    quarantine_ebs_volumes(instance_id)\n    pause_kinesis_streams()\n    copy_forensics_data(instance_id)\n    alert_security_team(finding)\n    \n    return {'status': 'containment initiated'}\n```","diagram":"flowchart TD\n  A[GuardDuty Finding] --> B[EventBridge Rule]\n  B --> C[Lambda: Isolate EC2]\n  C --> D[Attach restrictive SG]\n  C --> E[Snapshot EBS volumes]\n  C --> F[Pause Kinesis streams]\n  C --> G[Copy forensics to S3]\n  C --> H[SNS alert to on-call]\n  D --> I[Rollback path]\n  E --> J[Cross-region replication]\n  F --> K[Resume after containment]\n  G --> L[Audit log + retention]\n  H --> M[Post-incident review]","difficulty":"intermediate","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","IBM","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T05:16:09.850Z","createdAt":"2026-01-15T23:37:18.175Z"},{"id":"q-2597","question":"In a three-account, multi-region AWS setup (prod, security, shared) spanning two regions, design an automated S3 governance workflow: require production buckets to enable default encryption with a CMK, block public access, and enforce versioning, with centralized cross-account access controls. Outline an end-to-end solution using AWS Config custom rules, Lambda remediation, EventBridge alerts, and Organizations SCPs; include testing steps and rollback?","answer":"I would implement a Config aggregator across accounts to detect buckets missing KMS encryption, public access blocks, or versioning. A Lambda remediation function will apply encryption with a CMK, block public access, and enable versioning automatically. EventBridge will centralize alerts, while Organizations SCPs enforce preventive controls across all accounts. The solution includes comprehensive testing and rollback procedures.","explanation":"## Why This Is Asked\nThis tests cross-account governance automation, real-world policy enforcement, and robust testing/rollback capabilities in complex AWS environments.\n\n## Key Concepts\n- AWS Config custom rules across multiple accounts\n- Lambda-based remediation for S3 bucket properties\n- EventBridge for centralized alerts\n- Organizations SCPs for policy enforcement\n- Multi-region, multi-account orchestration\n\n## Code Example\n```python\nimport boto3\nfrom botocore.exceptions import ClientError\n\ndef remediate(bucket, region):\n    s3 = boto3.client('s3', region_name=region)\n    kms = boto3.client('kms')\n    # Enable encryption with CMK```","diagram":null,"difficulty":"intermediate","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Google","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T05:04:18.776Z","createdAt":"2026-01-16T02:26:39.982Z"},{"id":"q-2645","question":"Across a 6-account AWS Organization (prod, staging, dev) spanning three regions, design an automated cross-account S3 security posture that blocks public access and enforces encryption at rest for all buckets. Include automated remediation, drift detection, and rollback using AWS Config, Lambda, EventBridge, IAM Access Analyzer, and SCPs. How would you test and verify?","answer":"I would implement a centralized governance fabric to enforce S3 security across accounts. Use Config rules to require PublicAccessBlock and SSE-KMS on all buckets, plus Access Analyzer to flag risky p","explanation":"## Why This Is Asked\n\nAssessment of practical security posture automation across multiple accounts/regions.\n\n## Key Concepts\n\n- AWS Config rules for S3 public access and encryption\n- AWS Access Analyzer for policy risk\n- EventBridge/Lambda remediation\n- Service Control Policies\n- Drift testing and rollback strategies\n\n## Code Example\n\n```javascript\n// Pseudo remediation snippet for S3 bucket\nfunction remediateBucket(bucket) {\n  // enforce PublicAccessBlock and SSE-KMS\n}\n```\n\n## Follow-up Questions\n\n- How would you scale this to additional regions or accounts?\n- How would you handle legitimate exemptions without weakening posture?","diagram":null,"difficulty":"advanced","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","LinkedIn","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T04:26:48.457Z","createdAt":"2026-01-16T04:26:48.457Z"},{"id":"q-2652","question":"Across six AWS accounts in three regions, design a centralized, immutable audit-log pipeline: have all accounts deliver CloudTrail, VPC Flow Logs, and Config logs to a single S3 bucket in an Audit account with Object Lock (Compliance) for 7 years; enforce encryption via a shared KMS CMK and SCPs; use EventBridge and Lambda for delivery health checks and auto-remediation; outline validation, testing, and rollback?","answer":"Implement a centralized audit-log pipeline: every account delivers CloudTrail, VPC Flow Logs, and Config logs to a single S3 bucket in an Audit account via cross-account delivery; enable S3 Object Loc","explanation":"## Why This Is Asked\nAssesss ability to design cross-account governance, immutable logs, and automated remediation in large AWS footprints. The candidate should justify cross-account delivery, object locking, encryption, and monitoring.\n\n## Key Concepts\n- Centralized log sink, cross-account delivery, S3 Object Lock; AWS KMS CMK; SCPs; CloudTrail/VPC Flow Logs/Config; EventBridge/Lambda for health checks.\n- Testing strategy: staging accounts, dry-run delivery, drift detection, rollback.\n\n## Code Example\n```json\n{\n  \"BucketPolicy\": {\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n      {\"Effect\": \"Allow\", \"Principal\": {\"Service\": \"cloudtrail.amazonaws.com\"},\"Action\": \"s3:PutObject\",\"Resource\": \"arn:aws:s3:::audit-logs-bucket/*\"}\n    ]\n  }\n}\n```\n\n## Follow-up Questions\n- How would you validate cross-region delivery latency? \n- How would you handle CMK rotation and SCP exceptions during outages?","diagram":null,"difficulty":"advanced","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T05:39:12.539Z","createdAt":"2026-01-16T05:39:12.539Z"},{"id":"q-2676","question":"In a single AWS account hosting a regional web app, require that every new resource carries Owner and Environment tags; propose an end-to-end remediation flow using AWS Config Custom Rules, Lambda remediation, EventBridge, and SNS alerts. How would you test it, and what rollback steps would you include? Provide a minimal CloudFormation snippet for the Config rule and remediation Lambda, please?","answer":"Use an AWS Config Custom Rule named require-owner-env-tags backed by a remediation Lambda that adds missing Owner/Environment tags without overwriting existing values. Trigger alerts via SNS and surfa","explanation":"## Why This Is Asked\n\nTests ability to design automated governance with Config and Lambda, ensuring tag hygiene with safe remediation and observable alerts.\n\n## Key Concepts\n\n- AWS Config Custom Rules for evaluating tag presence\n- Lambda remediation that is idempotent and non-destructive\n- EventBridge for event routing and automation triggers\n- SNS for real-time alerts; CloudFormation for repeatable deployment\n\n## Code Example\n\n```yaml\nAWSTemplateFormatVersion: '2010-09-09'\nResources:\n  RequireTagsConfigRule:\n    Type: AWS::Config::ConfigRule\n    Properties:\n      Source:\n        Owner: CUSTOM_LAMBDA\n        SourceIdentifier: !GetAtt RemediateTagsLambda.Arn\n      InputParameters: '{\"RequiredTagKeys\":[\"Owner\",\"Environment\"]}'\n      ConfigRuleState: ACTIVE\n\n  RemediateTagsLambda:\n    Type: AWS::Lambda::Function\n    Properties:\n      Runtime: nodejs18.x\n      Handler: index.handler\n      Role: arn:aws:iam::123456789012:role/ConfigRemediationRole\n      Code:\n        ZipFile: |\n          exports.handler = async (event) => {\n            // remediation logic to tag missing keys\n          }\n```\n\n## Follow-up Questions\n\n- How would you handle new resource types with different tagging semantics?\n- How to ensure permissions least privilege for the remediation Lambda?","diagram":null,"difficulty":"beginner","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Hugging Face","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T06:48:58.546Z","createdAt":"2026-01-16T06:48:58.546Z"},{"id":"q-2718","question":"In a 3-region, multi-account SaaS deployment, enforce customer data locality so S3 data stays in the region defined by each customer and is not replicated cross-region unless explicitly approved. Design an automated policy engine using AWS Config custom rules, EventBridge, Lambda, and SCPs to prevent unauthorized replication, enforce region-bound KMS keys, and route alerts to a centralized security sink. Include testing and rollback steps with a simulated cross-region copy attempt?","answer":"Propose per-customer region tagging and a policy store; Config custom rules validate bucket replication against policy; EventBridge triggers Lambda remediation to remove unauthorized replication and a","explanation":"## Why This Is Asked\nTests ability to implement data locality governance in a real multi-region, multi-account context, with policy enforcement and automated remediation.\n\n## Key Concepts\n- AWS Config custom rules, EventBridge, Lambda, SCPs, KMS, CloudWatch/SNS\n- Data locality, cross-region replication controls, policy store\n- Testing/rollback strategies and auditability\n\n## Code Example\n```python\n# Pseudo AWS Config custom rule handler\ndef evaluate_config(config_item, policy_store):\n    bucket = config_item['resourceId']\n    region = config_item['awsRegion']\n    allowed = policy_store.is_allowed(bucket, region)\n    return {'compliance_type': 'COMPLIANT' if allowed else 'NON_COMPLIANT'}\n```\n\n## Follow-up Questions\n- How would you scale the policy store for thousands of customers?\n- How to test failure modes and ensure no data leakage during rollout?","diagram":"flowchart TD\n  A[Customer Policy] --> B[AWS Config Rule]\n  B --> C[EventBridge]\n  C --> D[Lambda Remediation]\n  D --> E[SCP Enforcement]\n  D --> F[Alerts & Audit]","difficulty":"intermediate","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Cloudflare","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T07:50:44.753Z","createdAt":"2026-01-16T07:50:44.753Z"},{"id":"q-2905","question":"Design a lightweight log aggregation pipeline for a single-region AWS deployment (EC2 behind ALB, RDS) that ships application logs and VPC flow logs to a centralized S3 bucket, with structured naming, encryption, and a 30-day retention policy. Include how CloudWatch Logs, Kinesis Data Firehose, and Lambda would be wired, how to monitor for unusual spikes, and basic testing steps?","answer":"Use CloudWatch Logs groups for app and VPC flow logs, route to a Kinesis Data Firehose delivery stream that writes to a central SSE-S3 encrypted S3 bucket. Apply a 30-day lifecycle to move data to Gla","explanation":"## Why This Is Asked\nTests knowledge of basic log ingestion, data protection, lifecycle, and alerting in a constrained environment. It validates practical wiring between CloudWatch, Firehose, and S3, plus basic testing steps.\n\n## Key Concepts\n- CloudWatch Logs\n- Kinesis Firehose\n- S3 lifecycle\n- IAM roles, SSE\n- Testing plan\n\n## Code Example\n```yaml\nResources:\n  FirehoseToS3:\n    Type: AWS::KinesisFirehose::DeliveryStream\n    Properties:\n      DeliveryStreamType: DirectPut\n      S3DestinationConfiguration:\n        BucketARN: arn:aws:s3:::central-logs\n        RoleARN: arn:aws:iam::123456789012:role/FirehoseDeliveryRole\n        BufferingHints:\n          SizeInMBs: 5\n          IntervalInSeconds: 300\n        CompressionFormat: UNCOMPRESSED\n```\n\n## Follow-up Questions\n- How would you scale for higher log throughput?\n- How would you implement log integrity checks and retries?","diagram":null,"difficulty":"beginner","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Meta","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T16:56:04.390Z","createdAt":"2026-01-16T16:56:04.390Z"},{"id":"q-2924","question":"Scenario: In a global SaaS with 3 AWS accounts across 2 regions, design an automated cost anomaly detection and remediation workflow that halts non-production resources when daily spend grows by more than 2x over a 7-day rolling window, while preserving prod SLAs. Specify integration of Cost Anomaly Detection, CloudWatch, Lambda, Step Functions, AWS Budgets, and guardrails and testing?","answer":"Implement a per-account cost anomaly workflow triggered by AWS Cost Anomaly Detection alerts. A Lambda listener channels events to a Step Functions state machine that tags non-prod resources, pauses i","explanation":"## Why This Is Asked\nTests ability to design automated cost governance with real-time remediation while safeguarding production workloads, and to articulate testing/rollback strategies.\n\n## Key Concepts\n- AWS Cost Anomaly Detection per account/region\n- Event-driven Lambda listeners\n- Step Functions for orchestration\n- Guardrails around prod resources\n- Tests: staging spikes, safe rollback\n\n## Code Example\n```json\n{\n  \"Comment\": \"Cost anomaly remediation state machine\",\n  \"StartAt\": \"EvaluateAnomaly\",\n  \"States\": {\n    \"EvaluateAnomaly\": {\"Type\": \"Choice\", \"Choices\": [{\"Variable\": \"$.detail.impact\", \"NumericGreaterThanEquals\": 2, \"Next\": \"Remediate\"}], \"Default\": \"NoOp\"},\n    \"Remediate\": {\"Type\": \"Task\", \"Resource\": \"arn:aws:lambda:region:acct:function:RemediateCosts\", \"End\": true}\n  }\n}\n```\n\n```python\nimport boto3\n\ndef lambda_handler(event, context):\n    # parse anomaly alert and trigger remediation workflow\n    pass\n```\n\n## Follow-up Questions\n- How would you test remediation in a non-prod account without impacting customers?\n- How would you handle false positives and ensure a safe rollback path?","diagram":"flowchart TD\n  A[Cost Anomaly Detected] --> B[Invoke Lambda]\n  B --> C[Start Step Functions]\n  C --> D[Tag Non-Prod]\n  D --> E[Pause Non-Prod Resources]\n  E --> F[Notify & Audit]","difficulty":"intermediate","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T17:46:44.528Z","createdAt":"2026-01-16T17:46:44.528Z"},{"id":"q-2981","question":"In a multi-account AWS environment hosting a global SaaS app, you must quickly detect and isolate a misconfigured OpenSearch domain that is publicly accessible. Design an end-to-end, beginner-friendly workflow using AWS Config rules, EventBridge, Lambda, and Organizations SCPs to detect, auto-remediate (restrict access and enable encryption), alert on-call, and provide a rollback plan. Include testing steps?","answer":"Detect with a Config rule for public access or missing encryption; a central EventBridge rule triggers a Lambda to (1) apply a deny-public-access domain policy and enable encryption at rest (KMS) if n","explanation":"## Why This Is Asked\nTests practical, beginner-friendly automation for common security misconfigurations across accounts, focusing on OpenSearch governance and automated remediation.\n\n## Key Concepts\n- AWS Config rules and remediation\n- EventBridge for orchestration\n- Lambda for automated changes\n- IAM SCPs for cross-account guardrails\n- OpenSearch domain security settings (public access, encryption)\n\n## Code Example\n```javascript\nexports.handler = async (event) => {\n  // parse OpenSearch domain from event\n  // if public access detected or encryption off:\n  //   call OpenSearch updateDomainConfig to set domainConfigOptions accordingly\n  //   apply a restricted access policy\n  //   emit alert payload to a central dashboard\n};\n```\n\n## Follow-up Questions\n- How would you test the rollback path if the remediation fails to apply? \n- What logging/metrics would you collect to validate ongoing policy compliance?","diagram":null,"difficulty":"beginner","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Stripe","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T19:45:00.470Z","createdAt":"2026-01-16T19:45:00.470Z"},{"id":"q-3005","question":"In a six-region, multi-account AWS setup hosting a critical data platform, design an automated secrets lifecycle that rotates database credentials and API keys stored in AWS Secrets Manager, revokes stale IAM principals, and enforces cross-account access controls via IAM and resource policies. Include rotation schedules, cross-region replication, audit trails, testing, and rollback?","answer":"Implement automatic rotation for Secrets Manager entries holding DB credentials and API keys, with region-scoped Lambda rotation functions; enable cross-region replication and per-secret KMS keys; enf","explanation":"## Why This Is Asked\n\nThis tests operational design for secrets lifecycle at scale, including rotation, cross-region replication, and access governance, with rollback readiness and auditing.\n\n## Key Concepts\n\n- AWS Secrets Manager rotation\n- Lambda-based rotation templates\n- cross-region secret replication\n- KMS per-secret keys\n- IAM/resource policy scoping\n- CloudTrail and EventBridge alerting\n\n## Code Example\n\n```javascript\n// Rotation function skeleton\nexports.handler = async (event) => {\n  // handle create/cancel/rotate\n};\n```\n\n## Follow-up Questions\n\n- How would you simulate a rotation failure and trigger automatic rollback?\n- What metrics and alarms would you monitor for rotation health and secret leakage risk?","diagram":"flowchart TD\n  S[Secret] --> R[Rotation]\n  R --> C[Lambda Rotation Function]\n  C --> P[Policy Enforcement]\n  P --> R2[Cross-Region Replication]\n  R2 --> A[Audit/Logging]","difficulty":"advanced","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Instacart"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T20:44:10.382Z","createdAt":"2026-01-16T20:44:10.382Z"},{"id":"q-3049","question":"Across six AWS accounts spanning four regions, design an automated failover strategy for SageMaker endpoints and associated data pipelines to achieve near-zero RTO during regional outages. Detail how Route 53 health checks, Global Accelerator, cross-region S3 replication, DynamoDB Global Tables, IAM trust models, and Step Functions/Lambda orchestration would work. Include testing, rollback, and observability?","answer":"Adopt an active‑active SageMaker deployment across two regions per model, with S3 artifacts replicated regionally, Route 53 health checks plus AWS Global Accelerator for traffic steering, DynamoDB Glo","explanation":"## Why This Is Asked\nTests ability to design cross‑region DR with low RTO for ML workloads, covering end‑to‑end automation, cross‑account security, and observability. It couples routing, data locality, and state synchronization in a production context.\n\n## Key Concepts\n- Multi‑region active‑active SageMaker deployments\n- Route 53 health checks + Global Accelerator routing\n- S3 cross‑region replication and data locality policies\n- DynamoDB Global Tables for shared state across accounts\n- Cross‑account IAM roles and trust policies\n- Step Functions/Lambda orchestration for failover and rollback\n- Observability: metrics, alarms, latency checks, data integrity tests\n\n## Code Example\n```javascript\n// Sample AWS Step Functions state machine (high level)\n{\n  \"Comment\": \"SageMaker failover workflow\",\n  \"StartAt\": \"HealthCheck\"\n  ,\"States\": {\n    \"HealthCheck\": {\"Type\": \"Task\", \"Resource\": \"arn:aws:lambda:region:acct:function:HealthCheck\", \"Next\": \"Decide\"},\n    \"Decide\": {\"Type\": \"Choice\", \"Choices\": [{\"Variable\": \"$.__state.finalError\", \"StringEquals\": \"true\", \"Next\": \"Failover\"}], \"Default\": \"Continue\"},\n    \"Continue\": {\"Type\": \"Task\", \"Resource\": \"arn:aws:lambda:region:acct:function:ContinueTraffic\", \"End\": true},\n    \"Failover\": {\"Type\": \"Task\", \"Resource\": \"arn:aws:lambda:region:acct:function:RouteFailover\", \"End\": true}\n  }\n}\n```\n\n## Follow-up Questions\n- How would you test failover without impacting production endpoints?\n- How would you validate data consistency after a failover and rollback if latency spikes occur?","diagram":"flowchart TD\n  A[User Request] --> B(Route 53 Health Check)\n  B --> C(Global Accelerator)\n  C --> D[SageMaker Endpoint Region A]\n  C --> E[SageMaker Endpoint Region B]\n  D --> F[Metrics & Health]\n  E --> F\n  F --> G{Healthy?}\n  G -- Yes --> H[Route to A or B]\n  G -- No --> I[Trigger Step Functions Failover]\n  I --> J[Data & Endpoint Sync]\n  J --> K[Rollback if needed]","difficulty":"advanced","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Databricks","Hugging Face"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T22:42:23.092Z","createdAt":"2026-01-16T22:42:23.092Z"},{"id":"q-3097","question":"In a 3-region SaaS deployment (prod us-east-1, prod eu-west-1, staging) implement automated containment for a newly detected public S3 bucket containing customer data. Use a Config rule to flag public buckets, EventBridge+Lambda to apply a DenyPublicAccess policy, and SCPs in prod to block exposure. Include testing steps and rollback plan?","answer":"Implement a Config rule that flags any S3 bucket with public access not blocked. On detection, a Lambda triggered by EventBridge applies a DenyPublicAccess policy to the bucket and enforces a prod SCP to prevent future public exposure.","explanation":"## Why This Is Asked\n\nTests ability to wire detection, containment, and rollback across accounts/regions using native AWS services.\n\n## Key Concepts\n\n- AWS Config custom rule for public bucket detection\n- EventBridge and Lambda for automated remediation\n- S3 public access policies and DenyPublicAccess patterns\n- Service Control Policies (SCPs) to enforce governance in prod\n- Testing and rollback planning across multi-region deployments\n\n## Code Example\n\n```python\n# Pseudo-Lambda: attach DenyPublicAccess to bucket policy\nimport json, boto3\ns3 = boto3.client('s3')\n\ndef handler(event, ctx):\n    bucket = event['detail']['requestParameters']['bucketName']\n    s3.put_public_access_block(\n        Bucket=bucket,\n        PublicAccessBlockConfiguration={\n            'BlockPublicAcls': True,\n            'IgnorePublicAcls': True,\n            'BlockPublicPolicy': True,\n            'RestrictPublicBuckets': True\n        }\n    )\n```\n\n## Testing Steps\n\n1. Create test bucket with public access in staging\n2. Verify Config rule detection within 5 minutes\n3. Confirm Lambda applies DenyPublicAccess policy\n4. Validate SCP blocks public access in prod regions\n5. Test rollback by removing Config rule and Lambda\n\n## Rollback Plan\n\n1. Disable Config rule to stop detection\n2. Remove EventBridge rule to prevent Lambda triggers\n3. Delete Lambda function\n4. Remove SCP from prod OUs\n5. Manually restore bucket policies if needed","diagram":"flowchart TD\n  A[Config Rule triggers] --> B[Lambda enforces policy]\n  B --> C[SCPs applied in prod]\n  C --> D[Alerts sent to security stack]\n  D --> E[Verification & rollback path]","difficulty":"intermediate","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","DoorDash","Hashicorp"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T04:59:48.350Z","createdAt":"2026-01-17T02:18:42.048Z"},{"id":"q-3160","question":"Scenario: In a single AWS region, a small app uses an RDS MySQL primary with one read replica to handle read traffic. Design a beginner-friendly auto-scaling approach that creates a second read replica during peak hours and tears it down after. Outline the AWS components, thresholds, cost guardrails, and testing steps?","answer":"Design a beginner-friendly auto-scaling strategy for an RDS MySQL primary with one read replica in a single region. Use CloudWatch to trigger a Lambda when CPUUtilization or replicaLag thresholds are ","explanation":"## Why This Is Asked\n\nTests practical workflow for on-demand read scaling and cost control using serverless automation.\n\n## Key Concepts\n\n- CloudWatch alarms and metrics\n- Lambda-driven lifecycle automation\n- RDS read replicas lifecycle\n- IAM least privilege permissions\n- AWS Budgets and SNS alerts\n\n## Code Example\n\n```bash\n# AWS CLI example to create a read replica\naws rds create-db-instance-read-replica \\\n  --db-instance-identifier mydb-ro-2 \\\n  --source-db-instance-identifier mydb-primary \\\n  --db-instance-class db.t3.medium \\\n  --region us-east-1\n```\n\n## Follow-up Questions\n\n- How would you handle a primary failure during peak with this setup?\n- How would you test the auto-scaling safely in production?","diagram":"flowchart TD\n  A[CloudWatch Alarm] --> B[Lambda Function]\n  B --> C{Replica Count < Max}\n  C -->|Yes| D[Create Read Replica]\n  C -->|No| E[No Action]","difficulty":"beginner","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Lyft","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T04:54:43.242Z","createdAt":"2026-01-17T04:54:43.242Z"},{"id":"q-3197","question":"In a single-region AWS setup for a small web app, design a lightweight log-collection and alerting pipeline. Use CloudWatch Logs as source, a Kinesis Data Firehose delivery stream to S3, and a Lambda function scheduled hourly to compute errors/requests. If hourly error rate > 5%, publish to SNS. Describe components, thresholds, IAM permissions, and a basic test plan?","answer":"Outline a lightweight log pipeline in one region: CloudWatch Logs as source, Firehose to S3, and a Lambda hourly job to compute errors per hour divided by total requests. If the hourly error rate exce","explanation":"## Why This Is Asked\nTests ability to design a simple, observable log pipeline using common AWS services with clear thresholds and automated alerting. Emphasizes practical data parsing and permissions best practices for a beginner-friendly scenario.\n\n## Key Concepts\n- CloudWatch Logs and metrics\n- Kinesis Data Firehose to S3\n- Lambda scheduling via EventBridge\n- IAM least privilege\n- SNS alerts\n\n## Code Example\n```javascript\nexports.handler = async (event) => {\n  // Parse S3 batch, compute errors vs total requests\n  return { status: 'ok' };\n};\n```\n\n## Follow-up Questions\n- How would you handle log lag or partial data?\n- How would you test without impacting production traffic?","diagram":null,"difficulty":"beginner","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Hashicorp","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T06:47:37.369Z","createdAt":"2026-01-17T06:47:37.369Z"},{"id":"q-3284","question":"In a globally distributed 4-region SaaS backend (us-east-1, eu-west-1, ap-southeast-1, ap-northeast-1) with Lambda, DynamoDB, and S3, you must guarantee an RPO of 15 minutes and an RTO of 60 minutes for customer data, with cross-account backups that survive region failure. Design a cross-region backup strategy using AWS Backup, DynamoDB backups, S3 replication, KMS with rotation, and cross-account restoration via IAM roles and SCPs. Include testing plan and cost considerations?","answer":"Design a cross-region backup strategy: use AWS Backup for EBS/AMI, enable DynamoDB on-demand backups with cross-region copies, and implement S3 cross-region replication to a centralized vault in a sep","explanation":"## Why This Is Asked\n\nTests practical multi-region backup strategy with cross-account controls and real DR validation, not theoretical talk.\n\n## Key Concepts\n\n- AWS Backup, DynamoDB backups (on-demand), S3 replication\n- Cross-account restore workflows, IAM roles, SCPs\n- KMS key management and rotation\n- DR testing, cost awareness\n\n## Code Example\n\n```javascript\n// Example: initiate a DynamoDB on-demand backup copy (pseudo)\nconst { DynamoDBClient, CreateBackupCommand } = require(\"@aws-sdk/client-dynamodb\");\nconst client = new DynamoDBClient({ region: \"us-east-1\" });\nawait client.send(new CreateBackupCommand({ TableName: \"Users\", BackupName: \"Users-backup-202601\" }));\n```\n\n## Follow-up Questions\n\n- How would you validate RPO/RTO after a drill?\n- What are the failure modes if the central vault is compromised?\n","diagram":"flowchart TD\n  A[Regions: us-east-1, eu-west-1, ap-southeast-1, ap-northeast-1] --> B[AWS Backup Plans (EBS/AMI)]\n  A --> C[DynamoDB On-Demand Backups + Cross-Region Copies]\n  A --> D[S3 Replication to Central Vault]\n  B --> E[Central Vault in Shared Account (KMS)]\n  E --> F[Cross-Account IAM Roles & SCPs for Restore]\n  F --> G[Validation: Drill, Restore Time]\n  G --> H[Cost & Compliance Review]","difficulty":"intermediate","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","IBM","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T09:46:28.364Z","createdAt":"2026-01-17T09:46:28.364Z"},{"id":"q-3406","question":"Scenario: A prod IAM key leaks across three AWS regions in a microservices app. Propose an automated, end-to-end incident response that detects, rotates credentials, revokes sessions, isolates the affected account with SCPs, and validates service health using EventBridge, Lambda, IAM, Secrets Manager, and Route 53 health checks. Include testing and rollback?","answer":"Automated cross‑account IR: detect via CloudTrail/GuardDuty a leaked prod IAM key across 3 regions; Lambda (central security account) rotates keys, disables old tokens, and revokes sessions; apply SCP","explanation":"## Why This Is Asked\nThis checks real-world incident response, cross‑account governance, and event‑driven automation across AWS services.\n\n## Key Concepts\n- Event-driven IR\n- Cross-account SCPs and IAM policies\n- Credential rotation and session control\n- Health checks and rollback\n\n## Code Example\n```javascript\nexports.handler = async (event) => {\n  // parse event, identify affected principals\n  // rotate credentials via IAM\n  // apply SCP to isolate account\n  // trigger health canaries\n};\n```\n\n## Follow-up Questions\n- How would you test the IR playbook dry-run?\n- How do you ensure idempotent retries and clean rollback?","diagram":"flowchart TD\n  A[Detect Incident] --> B[Trigger Remediation]\n  B --> C[Rotate IAM Keys]\n  C --> D[Apply SCP Isolation]\n  D --> E[Health Canary Checks]\n  E --> F{Healthy?}\n  F -->|Yes| G[Notify & Resume]\n  F -->|No| H[Rollback & Alert]","difficulty":"intermediate","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","LinkedIn","Lyft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T14:43:15.478Z","createdAt":"2026-01-17T14:43:15.478Z"},{"id":"q-3411","question":"In a single-region AWS setup, a data-processing pipeline uses an SQS standard queue triggering a Lambda function that writes results to DynamoDB. Design a beginner-friendly reliability improvement: (1) configure a Dead-Letter Queue for messages that fail processing after N receives; (2) tune Lambda retry behavior and visibility timeout; (3) add CloudWatch alarms and an SNS alert for DLQ growth and high processing latency; (4) provide a minimal runbook and testing steps?","answer":"Enable a DLQ on the SQS queue and set RedrivePolicy maxReceiveCount to 5; configure the Lambda event source mapping with MaximumRetryAttempts: 2 and a 60s visibility timeout; create CloudWatch alarms ","explanation":"## Why This Is Asked\n\nTests practical ops knowledge of reliable message processing: DLQ usage, Lambda retry behavior, visibility timeouts, and alerting. It also asks for a tangible test plan and runbook, not theory.\n\n## Key Concepts\n\n- SQS Dead-Letter Queue and redrive policy\n- Lambda Event Source Mapping: MaximumRetryAttempts, batch size\n- Visibility Timeout tuning\n- CloudWatch alarms and SNS-based alerts\n- Basic runbook and recovery testing\n\n## Code Example\n\n```yaml\nAWSTemplateFormatVersion: '2010-09-09'\nResources:\n  MainQueue:\n    Type: AWS::SQS::Queue\n    Properties:\n      RedrivePolicy:\n        deadLetterTargetArn: !GetAtt DLQ.Arn\n        maxReceiveCount: 5\n  DLQ:\n    Type: AWS::SQS::Queue\n  LambdaEventSourceMapping:\n    Type: AWS::Lambda::EventSourceMapping\n    Properties:\n      EventSourceArn: !GetAtt MainQueue.Arn\n      FunctionName: !GetAtt DataProcessor.Arn\n      MaximumRetryAttempts: 2\n      BatchSize: 10\n```\n\n## Follow-up Questions\n\n- How would you test DLQ backlog and recovery workflow?\n- How would you tune for burst traffic and minimize duplicate processing?","diagram":null,"difficulty":"beginner","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Meta","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T15:29:51.041Z","createdAt":"2026-01-17T15:29:51.041Z"},{"id":"q-3521","question":"In a three-region AWS multi-account data pipeline using S3 data lake, Glue ETL, and Redshift, design an automated incident response that detects data inconsistency or SLA breach, isolates the affected region, promotes a hot standby copy, and validates end-to-end data integrity before resuming writes. Outline the orchestration with EventBridge, Step Functions, Lambda, IAM roles, cross-region replication, and rollback procedures?","answer":"Detect via CloudWatch SLA metrics and data-quality anomalies. Trigger Step Functions via EventBridge to isolate the affected region, promote a hot standby (cross-region Redshift snapshot or S3 backup)","explanation":"## Why This Is Asked\nProbes real-world incident response in a multi-region data stack with automated remediation and rollback.\n\n## Key Concepts\n- Cross-region failover, data integrity checks, multi-account IAM\n- Event-driven orchestration: EventBridge, Step Functions, Lambda\n- Data-plane vs control-plane separation, traffic routing\n\n## Code Example\n```javascript\n// Pseudo-code: basic state machine flow\n```\n\n## Follow-up Questions\n- How would you simulate data corruption safely for tests?\n- Which metrics would you trust for rollback vs promotion?","diagram":null,"difficulty":"intermediate","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T19:34:57.698Z","createdAt":"2026-01-17T19:34:57.698Z"},{"id":"q-3597","question":"In a two-region deployment hosting a 24x7 financial service behind an ALB and an RDS primary with cross-region replication, design an automated DR workflow that fails over from us-east-1 to eu-west-1. Include data-sync approach (RDS cross-region replica vs DynamoDB Global Tables), Route 53 failover, a Lambda control plane for promoting failover, IAM rotation, ALB target switching, S3 replication, and a deterministic rollback. Outline RTO/RPO targets and a test plan?","answer":"Automate disaster recovery between us-east-1 and eu-west-1 using RDS cross-region replica promotion, Route 53 failover routing, and a Lambda control plane to promote the standby environment, rotate IAM credentials, and switch ALB target groups. Target RTO under 5 minutes and RPO under 1 minute with S3 cross-region replication for static assets. Include deterministic rollback using CloudFormation stacks and automated testing through chaos engineering.","explanation":"## Why This Is Asked\nAssessment of multi-region disaster recovery design capabilities with focus on automation, cost optimization, and testability.\n\n## Key Concepts\n- Cross-region disaster recovery architecture\n- Route 53 failover routing and health check configuration\n- RDS cross-region replication versus DynamoDB Global Tables comparison\n- Lambda control plane for environment promotion, credential rotation, and traffic routing\n- Application Load Balancer target group management and IAM credential rotation\n- S3 cross-region replication and cache invalidation strategies\n- Disaster recovery testing methodologies and rollback procedures","diagram":"flowchart TD\n  A[us-east-1 Active] --> B[Outage Detected]\n  B --> C[Promote replica in eu-west-1]\n  C --> D[Update Route53 failover]\n  D --> E[Switch ALB target group]\n  E --> F[Validate traffic and metrics]","difficulty":"advanced","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Meta","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T05:30:55.677Z","createdAt":"2026-01-17T22:41:37.198Z"},{"id":"q-3676","question":"In a multi-account AWS deployment across two regions, design an automated disaster recovery (DR) plan for a data processing pipeline (S3 data lake, AWS Glue, Lambda, DynamoDB). Primary region us-east-1, standby us-west-2. Achieve RPO <= 5 minutes and RTO <= 15 minutes with automated failover using Route 53, S3 cross-region replication, DynamoDB Global Tables, and a Lambda-driven reconfiguration. Include failover sequence, data synchronization strategy, testing plan, and rollback?","answer":"Use S3 Cross-Region Replication from us-east-1 to us-west-2, DynamoDB Global Tables in both regions, and Glue jobs with region-agnostic URIs. Route 53 failover with healthy checks directs traffic to t","explanation":"## Why This Is Asked\n\nEvaluates practical DR design across accounts and regions with concrete AWS services and automation, not abstract concepts.\n\n## Key Concepts\n\n- DR planning and RPO/RTO targets\n- Route 53 failover and health checks\n- S3 Cross-Region Replication and DynamoDB Global Tables\n- Glue workflow configuration and endpoint reconfiguration\n- Testing, validation, and rollback procedures\n\n## Code Example\n\n```bash\n# DR runbook placeholder for automation sequences\n```\n\n## Follow-up Questions\n\n- How would you validate DR readiness without impacting production data?\n- How would you handle eventual consistency in DynamoDB during a failover?","diagram":null,"difficulty":"advanced","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["NVIDIA","OpenAI","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T04:24:15.281Z","createdAt":"2026-01-18T04:24:15.281Z"},{"id":"q-3732","question":"In a three-region, two-account deployment hosting a real-time financial telemetry pipeline with tight latency, implement fully automated disaster recovery using Aurora Global Database, cross-region S3 replication, Route 53 failover, and Lambda-driven telemetry validation. Outline data integrity checks, failover criteria and rollback, automated DR tests, and cost considerations?","answer":"Leverage Aurora Global DB for <5s RPO, cross-region S3 replication with versioning, Route 53 failover using health checks, and Lambda checks for data latency/heartbeat. Use automated cutover with Clou","explanation":"## Why This Is Asked\nTests DR planning under strict financial SLAs and multi-region recovery, focusing on automation, data integrity, and cost control.\n\n## Key Concepts\n- Aurora Global Database cross-region replication\n- S3 cross-region replication and object versioning\n- Route 53 health checks and failover routing\n- Lambda-based telemetry validation and CloudWatch alarms\n- Rollback strategy and cost-aware DR testing\n\n## Code Example\n```bash\n# Conceptual: enable global db and ensure backups\naws rds create-global-database --db-cluster-identifier telemetry-global --engine aurora-mysql\naws s3 cp s3://telemetry-us-east-1/telemetry/ s3://telemetry-eu-west-1/telemetry/ --recursive\n```\n\n## Follow-up Questions\n- How would you validate DR readiness without impacting production data?\n- How would you handle RPO/RTO changes if latency spikes occur?","diagram":null,"difficulty":"advanced","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Coinbase","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T07:31:55.560Z","createdAt":"2026-01-18T07:31:55.560Z"},{"id":"q-3793","question":"In a single AWS region, a static React SPA is hosted on S3 and delivered through CloudFront; a Node API runs on EC2 behind an ALB. Design a beginner-friendly guardrail to improve security and cost control: (1) enable a rate-based AWS WAF rule to throttle abusive IPs, (2) enable CloudFront access logs to an S3 bucket with lifecycle, (3) set up a simple AWS Budgets alert for API costs, and (4) outline a practical testing plan to verify the controls?","answer":"Configure WAF with a rate-based rule that blocks IPs exceeding 100 requests per 5 minutes, and apply it to the CloudFront distribution; enable CloudFront access logs to an S3 bucket with a 30-day life","explanation":"## Why This Is Asked\nThis question tests practical, beginner-level ability to implement basic security guards and cost controls in a common web app deployment.\n\n## Key Concepts\n- AWS WAF rate-based rules\n- CloudFront access logs to S3\n- AWS Budgets alerts and SNS\n- End-to-end testing of security and cost controls\n\n## Code Example\n```javascript\n// AWS CLI example (illustrative)\naws wafv2 create-web-acl --scope CLOUDFRONT --name rate-limit --default-action Block --visibility-config ... \n```\n\n## Follow-up Questions\n- How would you adapt if the API backend is API Gateway instead of EC2?\n- What monitoring indicators would you add beyond logs and budgets to detect abuse early?\n","diagram":"flowchart TD\n  A[S3/CloudFront SPA] --> B[EC2/API/ALB]\n  A -.-> C[WAF] \n  B --> D[CloudFront Logs to S3]\n  E[Budget] --> F[SNS Alerts]","difficulty":"beginner","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Lyft","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T09:40:59.143Z","createdAt":"2026-01-18T09:40:59.143Z"},{"id":"q-3853","question":"In a 3-account, 2-region production setup (prod in us-east-1, replica in us-west-2) hosting a real-time analytics platform with S3 data lake, Glue Catalog, Athena queries, and a frontend API behind an ALB, design an automated DR plan achieving RPO ~5 minutes and RTO ~15 minutes. Include cross-region S3 replication, DynamoDB metadata, Glue catalog replication, Route 53 failover with health checks, and a Lambda-driven runbook to switch endpoints, update ALB target groups, and rollback steps. Provide testing plan?","answer":"Enable S3 Cross-Region Replication from us-east-1 to us-west-2 with versioning and SSE-KMS; mirror DynamoDB via Global Tables; replicate Glue Catalog and keep Athena queries in sync; Route 53 Failover","explanation":"## Why This Is Asked\n\nTests automation, cross-region DR, and runbook discipline in a multi-account, multi-region setting; checks understanding of AWS services integration, testing, and rollback.\n\n## Key Concepts\n\n- S3 Cross-Region Replication, versioning, SSE-KMS\n- DynamoDB Global Tables or cross-region replication\n- Glue Data Catalog replication and Athena cross-region queries\n- Route 53 Failover with health checks\n- Lambda-driven runbooks to switch ALB targets and rollback\n\n## Code Example\n\n```javascript\n// Example: switch Route53 failover to replica endpoint (simplified)\nconst { Route53Client, ChangeResourceRecordSetsCommand } = require(\"@aws-sdk/client-route53\");\nconst client = new Route53Client({ region: \"us-east-1\" });\nasync function failover() {\n  // placeholder: update DNS to point to us-west-2 endpoint\n}\n```\n\n## Follow-up Questions\n\n- How would you validate DR without exposing prod data?\n- What monitoring and cost controls accompany this setup?","diagram":"flowchart TD\n  US_East[Prod us-east-1] --> DR_Runbook[Runbook triggers DR]\n  S3_Primary[(S3 Data Lake us-east-1)] --> S3_Replica[(S3 Data Lake us-west-2)]\n  Glue_Primary[(Glue Catalog us-east-1)] --> Glue_Replica[(Glue Catalog us-west-2)]\n  Route53[Route53 Failover] --> ALB_Targets[(ALB us-east-1 targets)]\n  DR_Runbook --> Route53","difficulty":"advanced","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Robinhood","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T12:58:26.761Z","createdAt":"2026-01-18T12:58:26.761Z"},{"id":"q-3955","question":"In a multi-region, multi-account AWS environment hosting a global web app with a primary deployment in us-east-1 and a DR site in eu-west-1, design an automated, auditable failover procedure using Route 53 failover routing, S3 cross-region replication for assets, and RDS cross-region disaster recovery. Include promotion steps, timing targets, logging, and testing plan?","answer":"Route 53 failover with health checks directing traffic to eu-west-1 when us-east-1 is unhealthy; a DR-promo Lambda in the DR account flips DNS, updates TTLs, and switches assets; enable S3 cross-regio","explanation":"## Why This Is Asked\nThis question probes real-world DR orchestration across accounts and regions, including DNS-based failover, cross-region asset replication, and database DR, with auditable workflows.\n\n## Key Concepts\n- Route 53 health checks and failover\n- Cross-region S3 replication and lifecycle\n- RDS cross-region DR and read replicas\n- IAM roles, cross-account access, auditing via CloudTrail\n- Testing, rollback, and compliance\n\n## Code Example\n```javascript\n// Pseudo Lambda: promoteDRPipeline\nexports.handler = async () => {\n  // check Route53 health, promote records to DR when primary is down\n  // switch assets, promote RDS DR replica, log events\n}\n```\n\n## Follow-up Questions\n- How would you test failover without impacting customers?\n- What metrics indicate a successful failover and how would you validate rollback?","diagram":"flowchart TD\n A[Primary us-east-1] --> B[Route53 health checks]\n B --> C{Healthy?}\n C -- Yes --> D[Traffic stays primary]\n C -- No --> E[Promote DR eu-west-1]\n E --> F[Switch S3 assets]\n F --> G[Promote DR RDS replica]\n","difficulty":"intermediate","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Coinbase","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T16:53:40.834Z","createdAt":"2026-01-18T16:53:40.834Z"},{"id":"q-4082","question":"In a multi-region AWS deployment, a globally distributed app uses Lambda@Edge to personalize content at the edge and CloudFront to cache responses. When deploying a content update, how would you ensure deterministic cache invalidation across all regions, minimize user latency, and validate rollback strategies? Provide concrete steps and trade-offs?","answer":"Implement a coordinated staged rollout using CloudFront invalidations with per-cache-behavior TTLs and Lambda@Edge for version control. Deploy with a canary pattern by injecting version headers via Lambda@Edge, validate responses before full rollout, and use version-specific cache paths for deterministic invalidation. Establish weighted traffic distribution for gradual migration and configure automated rollback triggers based on error rates and response time thresholds.","explanation":"## Why This Is Asked\nTests ability to design edge cache invalidation and rollout strategies for multi-region deployments with Lambda@Edge.\n\n## Key Concepts\n- Lambda@Edge and CloudFront cache behaviors\n- Cache invalidation strategies and TTL control\n- Canary deployment patterns and rollback mechanisms\n\n## Code Example\n```javascript\n// Example: Lambda@Edge snippet to attach a version header\nexports.handler = async (event) => {\n  const request = event.Records[0].cf.request;\n  request.headers['x-version'] = [{ key: 'X-Version', value: 'v2' }];\n  return request;\n}\n```\n\n## Follow-up Questions\n- How would you handle cache invalidation for dynamic content versus static assets?\n- What monitoring metrics would you track during the rollout?\n- How do you ensure consistency across different edge locations?","diagram":null,"difficulty":"intermediate","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Discord","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T05:50:48.391Z","createdAt":"2026-01-18T22:52:27.713Z"},{"id":"q-4161","question":"In a global AWS deployment across three regions (us-east-1, eu-west-1, ap-southeast-1) hosting a critical API backed by EC2 behind ALB, DynamoDB, and RDS, design automated cross-region disaster recovery that keeps DynamoDB synchronized via Global Tables, uses RDS cross-region read replicas with automated promotion, and employs Route 53 failover to direct traffic to healthy regions. Include a Lambda-driven DR runbook, measurable RPO/RTO targets, failover criteria, testing plan, and rollback strategy?","answer":"Implement DynamoDB Global Tables across us-east-1, eu-west-1, ap-southeast-1; configure RDS cross-region read replicas with automatic promotion using a Lambda DR runner; Route 53 health checks with fa","explanation":"## Why This Is Asked\nTests hands-on DR design across regions, balancing data consistency, automation, and governance.\n\n## Key Concepts\n- DynamoDB Global Tables for active multi-region writes\n- RDS cross-region replicas and automated promotion\n- Route 53 failover routing with health checks\n- Lambda-based DR runbook and rollback\n\n## Code Example\n```javascript\n// Skeleton: Lambda that promotes replica and updates Route 53 failover\nconst AWS = require('aws-sdk');\nconst rds = new AWS.RDS();\nconst route53 = new AWS.Route53();\nexports.handler = async () => {\n  // Pseudo-logic: promote replica, switch Route 53 records, verify health, notify\n};\n```\n\n## Follow-up Questions\n- How would you simulate partial outages and validate RTO while preserving data consistency?\n- What IAM and cost controls would you implement to prevent drift during DR drills?","diagram":"flowchart TD\n  A[Healthy region us-east-1] --> B[Route 53 failover to eu-west-1/ap-southeast-1]\n  B --> C[DynamoDB Global Tables across 3 regions]\n  C --> D[RDS cross-region read replicas with automatic promotion]\n  D --> E[Lambda DR orchestrator & rollback]","difficulty":"advanced","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Oracle","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T05:49:52.623Z","createdAt":"2026-01-19T05:49:52.623Z"},{"id":"q-4195","question":"In a single AWS region hosting a web app on EC2 behind an ALB, SSH is accidentally exposed via a Security Group rule to 0.0.0.0/0. Propose a beginner-friendly automated guardrail using AWS Config and Lambda to detect and remediate within a few minutes: (1) detect SSH port 22 exposure, (2) auto-remediate to restrict SSH to a corporate CIDR or revoke 0.0.0.0/0, (3) enforce resource tagging for ownership, (4) send alerts to Slack/Email via SNS, and (5) outline testing steps?","answer":"Implement a Config rule that flags Security Groups with inbound TCP 22 open to 0.0.0.0/0; attach a Lambda remediation that revokes 0.0.0.0/0 or replaces it with a corporate CIDR. Enforce tags Owner an","explanation":"## Why This Is Asked\nTests practical guardrail automation for common misconfigurations using AWS Config and Lambda, in a beginner-friendly way.\n\n## Key Concepts\n- AWS Config custom rules for continuous compliance\n- Lambda remediation to modify Security Groups\n- IAM permissions for EC2, Config, and SNS\n- Tag governance and SNS notifications\n\n## Code Example\n```javascript\n// Node.js Lambda remediation sketch\nconst AWS = require('aws-sdk');\nconst ec2 = new AWS.EC2();\nexports.handler = async (event) => {\n  // extract sgId and rules from event; simplified\n  const sgId = event.detail.resourceId;\n  // revoke 0.0.0.0/0 for port 22\n  await ec2.revokeSecurityGroupIngress({GroupId: sgId, IpPermissions: [{IpProtocol: 'tcp', FromPort: 22, ToPort: 22, IpRanges: [{CidrIp: '0.0.0.0/0'}]}]}).promise().catch(()=>{});\n  // optionally enforce corporate CIDR\n  // publish alert\n};\n```\n\n## Follow-up Questions\n- How would you allow temporary SSH access without defeating the guardrail?\n- What metrics would you monitor to verify remediation completeness and avoidance of false positives? ","diagram":null,"difficulty":"beginner","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T07:08:50.381Z","createdAt":"2026-01-19T07:08:50.381Z"},{"id":"q-4305","question":"In a multi-account, multi-region environment, a centralized log pipeline ingests VPC flow logs, application logs, and security alerts. Compliance requires logs to be encrypted at rest with per-account CMKs, no PII stored, and an immutable audit trail. Design a centralized architecture using CloudWatch Logs, KMS, and S3 for long-term storage, with cross-account roles and a dedicated audit account. Outline validation and key rotation plan, and testing steps?","answer":"Create per-account CloudWatch Log Groups encrypted with distinct CMKs; implement a cross-account delivery role to forward logs to a central audit account; export logs to a centralized S3 data lake wit","explanation":"## Why This Is Asked\nTests ability to design cross-account, multi-region log governance with encryption, immutability, and data-loss prevention. It also checks how to enforce no PII and how to validate integrity through rotation and audits.\n\n## Key Concepts\n- Cross-account CloudWatch log delivery with IAM roles\n- Per-account CMKs and KMS key rotation\n- S3 Object Lock and retention for immutability\n- Logs export and data lake architecture\n- AWS Config checks for CMK usage and PII presence\n\n## Code Example\n```javascript\n// Pseudo JS to create a cross-account delivery role\nconst iam = new (require('aws-sdk').IAM)({region: 'us-east-1'});\n// trust policy and role creation would be here\n```\n\n## Follow-up Questions\n- How would you simulate a CMK rotation without downtime?\n- How would you validate no PII slips through the pipeline?","diagram":null,"difficulty":"intermediate","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Goldman Sachs","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T13:09:24.327Z","createdAt":"2026-01-19T13:09:24.327Z"},{"id":"q-4375","question":"In a three-region, multi-account deployment for a real-time vehicle telemetry platform (Kinesis, Firehose, S3, Lambda, DynamoDB, API Gateway) used by external partners, design an automated strategy to prevent data exfiltration via misconfigured VPC endpoints and S3 policies. Use SCPs, Config, Access Analyzer, EventBridge, and Lambda remediation with blue/green rollout; include testing and rollback?","answer":"In a three-region, multi-account deployment for a real-time vehicle telemetry platform, design an automated strategy to stop data exfiltration due to misconfigured VPC endpoints and S3 policies. Enfor","explanation":"## Why This Is Asked\n\nCross-account, multi-region telemetry data is high‑risk if VPC endpoints or S3 policies leak access. This question tests ability to architect centralized governance that detects misconfigurations, enforces policy, and remediates without downtime, reflecting real-world security needs.\n\n## Key Concepts\n\n- Cross-account governance with SCPs and AWS Organizations\n- S3 bucket policy hardening and VPC endpoint controls\n- AWS Config rules and IAM Access Analyzer for drift/risks\n- EventBridge orchestration and Lambda-driven remediation\n- Blue/green rollout and rollback testing\n\n## Code Example\n\n```javascript\n// Lambda remediation: prune public access in bucket policy\nconst AWS = require('aws-sdk');\nconst s3 = new AWS.S3();\n\nasync function remediateBucketPolicy(bucket){\n  const policyObj = await s3.getBucketPolicy({Bucket: bucket}).promise().catch(()=>null);\n  if(!policyObj) return;\n  const policy = JSON.parse(policyObj.Policy);\n  policy.Statement = policy.Statement.filter(s => !(s.Effect === 'Allow' && (!s.Principal || s.Principal === '*')));\n  await s3.putBucketPolicy({Bucket: bucket, Policy: JSON.stringify(policy)}).promise();\n}\n```\n\n## Follow-up Questions\n\n- How would you validate that SCPs do not block legitimate cross-account access during rollout?\n- What metrics and alarms would you deploy to detect policy drift in near real-time?","diagram":null,"difficulty":"advanced","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Salesforce","Tesla","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T16:43:15.883Z","createdAt":"2026-01-19T16:43:15.883Z"},{"id":"q-4645","question":"Scenario: A multinational SaaS app runs across four regions with separate accounts per region. Design a disaster recovery strategy that enables active-passive failover for the DB tier and stateless services with RTO < 5 minutes and RPO < 1 minute. Use Aurora Global Database, Route 53 failover, DynamoDB Global Tables, S3 backups, and Step Functions for orchestration. Include data consistency, testing plan, and rollback?","answer":"Use Aurora Global Database across two regions (writer in region A, hot secondary in region B) for sub-second RPO. Route53 failover health checks switch DNS to DR region when needed. DynamoDB Global Ta","explanation":"## Why This Is Asked\nTests multi-region DR design with low RPO/RTO, using managed DB cross-region capabilities and serverless orchestration.\n\n## Key Concepts\n- Aurora Global Database\n- Route 53 failover routing\n- DynamoDB Global Tables\n- S3 cross-region replication\n- Step Functions orchestration\n\n## Code Example\n```json\n{\n  \"Comment\": \"Sample Step Functions flow for DR failover\",\n  \"StartAt\": \"HealthCheck\",\n  \"States\": {\n    \"HealthCheck\": {\"Type\": \"Task\", \"Resource\": \"<lambda-arn>\", \"Next\": \"DecideFailover\"},\n    \"DecideFailover\": {\"Type\": \"Choice\", \"Choices\": [{\"Variable\": \"$.drNeeded\", \"BooleanEquals\": true, \"Next\": \"Failover\"}], \"Default\": \"NoOp\"},\n    \"Failover\": {\"Type\": \"Task\", \"Resource\": \"<lambda-arn-activate-dr>\", \"End\": true},\n    \"NoOp\": {\"Type\": \"Pass\", \"End\": true}\n  }\n}\n```\n\n## Follow-up Questions\n- How would you validate RTO/RPO with live failovers and no customer impact?\n- What metrics and alarms are critical to detect partial failures early?","diagram":"flowchart TD\n  A[Aurora Global DB: primary] --> B[Aurora Global DB: secondary]\n  C[Route53 health checks] --> D[Failover to DR region]\n  D --> E[Stateless services in DR region]\n  F[Step Functions] --> G[Orchestrate failover and rollback]\n","difficulty":"advanced","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Amazon","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T06:52:19.254Z","createdAt":"2026-01-20T06:52:19.255Z"},{"id":"q-4673","question":"Scenario: A global SaaS stores data in RDS, EFS, and S3 across regions and accounts. Design an automated backup integrity and retention strategy using AWS Backup, AWS Config, EventBridge, and Lambda that (1) validates cross-region backups daily, (2) detects backup drift against policy, and (3) auto-remediates drift with alerts. Include testing steps and rollback?","answer":"Leverage per-resource AWS Backup plans with cross-account vaults and S3 replication. Use AWS Config rules to ensure backups exist with defined retention; an EventBridge rule triggers a Lambda drift ch","explanation":"## Why This Is Asked\nOperational resilience and policy-compliant backup integrity across regions.\n\n## Key Concepts\n- AWS Backup plans, cross-account vaults\n- AWS Config backup rules\n- EventBridge workflow\n- Lambda remediation logic\n- Cross-region data protection and retention\n\n## Code Example\n```javascript\n// Pseudo-code: drift check\nasync function checkDrift(backupMetadata, policy) {\n  // verify retention, regions, coverage\n  // return true if drift detected\n  return driftDetected;\n}\n```\n\n## Follow-up Questions\n- How would you simulate drift and verify remediation in CI/CD?\n- What are failure modes if cross-account vaults are unreachable?","diagram":null,"difficulty":"intermediate","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","MongoDB","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T07:45:03.171Z","createdAt":"2026-01-20T07:45:03.171Z"},{"id":"q-4698","question":"In a two-account AWS setup (prod and shared-services) with ~200 Windows EC2s across multiple AZs, design an automated patch management workflow using AWS Systems Manager Patch Manager. Requirements: monthly maintenance windows, automatic approval for Critical patches, automated remediation for instances missing patches after 7 days, and audit-ready logs in CloudTrail/Audit bucket. Outline the end-to-end flow using SSM, EventBridge, Lambda, and Config; include testing steps and rollback plan?","answer":"Create a Patch Baseline in Patch Manager with auto-approval for Critical patches; schedule a monthly maintenance window; use AWS Config to enforce patch compliance and trigger an EventBridge rule that","explanation":"## Why This Is Asked\nThis assesses operational discipline for patch compliance across accounts, combining Patch Manager, Config, and EventBridge to automate detection, remediation, and auditing. It also tests testing and rollback planning in a realistic multi-account setup.\n\n## Key Concepts\n- AWS Systems Manager Patch Manager baselines and auto-approval\n- AWS Config rules for patch compliance\n- AWS EventBridge to orchestrate remediation\n- Lambda for idempotent remediation tasks\n- CloudTrail/S3 audit logs and governance\n- Testing and rollback in patch workflows\n\n## Code Example\n```javascript\n// Pseudo: create baseline with auto-approval for Critical patches via AWS SDK\n```\n\n## Follow-up Questions\n- How would you handle patching for different OS families (Windows vs Linux) in the same baseline?\n- How would you simulate a patch failure and verify rollback in a test account?","diagram":"flowchart TD\n  A[Patch Baseline] --> B[Maintenance Window]\n  B --> C[Config Compliance]\n  C --> D[EventBridge Rule]\n  D --> E[Lambda Remediation]\n  E --> F[Patch Manager Run]\n  F --> G[Audit Logs]","difficulty":"intermediate","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","DoorDash","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T08:51:05.108Z","createdAt":"2026-01-20T08:51:05.109Z"},{"id":"q-4772","question":"In a single AWS region, a serverless REST API uses API Gateway + Lambda + DynamoDB. Design beginner-friendly cost-controls: (1) enforce API rate quotas via usage plans for each stage, (2) cap Lambda concurrency with per-function reserved concurrency and a shared pool, (3) enable DynamoDB provisioned throughput max with autoscale, (4) set CloudWatch alarms and a daily AWS Budgets alert. Outline the components, thresholds, and a simple test plan?","answer":"To implement cost controls: API Gateway usage plans per stage with quotas (e.g., 1000 req/sec and 100k/day); Lambda per-function reserved concurrency (e.g., 200) plus a shared pool; DynamoDB max RCU/W","explanation":"## Why This Is Asked\n\nTests practical understanding of enforcing cost boundaries in serverless stacks and how to implement guardrails that survive traffic spikes.\n\n## Key Concepts\n\n- API Gateway usage plans and quotas\n- Lambda concurrency and reserved concurrency\n- DynamoDB throughput and autoscaling\n- CloudWatch alarms and AWS Budgets\n- Basic load testing of quotas\n\n## Code Example\n\n```javascript\n// Example: CDK-ish snippet to set usage plan and reserved concurrency\n```\n\n## Follow-up Questions\n\n- How would you adapt for multi-region deployment?\n- How would you handle bursty traffic that legitimately exceeds quotas?","diagram":"flowchart TD\n  A[Define quotas] --> B[Apply API Gateway usage plans]\n  B --> C[Set Lambda reserved concurrency]\n  C --> D[Configure DynamoDB throughput]\n  D --> E[Create CloudWatch alarms & Budget]\n  E --> F[Test plan]","difficulty":"beginner","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Databricks","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T11:45:17.857Z","createdAt":"2026-01-20T11:45:17.857Z"},{"id":"q-4900","question":"In a three-region AWS setup across multiple accounts hosting a real-time analytics pipeline (Kinesis Data Streams -> Firehose -> Redshift/S3), design an automated DR strategy to meet RTO 15 minutes and RPO 5 minutes. Include cross-region data replication, credential management, cutover sequencing, testing and rollback, and cost controls?","answer":"Active-passive DR with warm standby in two regions. Producers publish to a primary Kinesis stream and a DR mirror; Firehose writes to S3 and a standby Redshift cluster in each region. Route 53 health ","explanation":"## Why This Is Asked\nAddresses DR in multi-region, cross-account, and cost controls; tests ability to design automation and rollback.\n\n## Key Concepts\n- DR strategy across regions (active-passive)\n- Data replication patterns (Kinesis, S3)\n- IAM/SCP and KMS key management\n- Cutover orchestration with Step Functions\n- Testing and rollback processes\n\n## Code Example\n```javascript\n// Pseudocode: trigger DR failover\nconst sf = new AWS.StepFunctions();\nsf.startExecution({ stateMachineArn: 'arn:aws:states:...' , input: '{}' }, callback)\n```\n\n## Follow-up Questions\n- How would you measure RTO/RPO compliance and automate improvements?\n- What changes if data streams have strictly regulated retention windows?","diagram":null,"difficulty":"advanced","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Databricks","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T17:57:55.963Z","createdAt":"2026-01-20T17:57:55.963Z"},{"id":"q-4962","question":"In a multi-account AWS deployment where ECS Fargate services run behind an ALB in a hub-and-spoke VPC, a misconfigured security group allows outbound traffic to a known malicious IP range, triggering unexpected egress charges and potential data exfiltration. Design an end-to-end, automated detection, containment, and audit workflow that completes within 30 minutes using VPC Flow Logs, GuardDuty, AWS Config, EventBridge, Lambda, and AWS Budgets. Include detection logic, remediation steps, rollback plan, and testing procedure?","answer":"Detect outbound traffic to restricted IPs using a Config rule integrated with GuardDuty findings; when triggered, a Lambda function invoked via EventBridge revokes the offending security group rule and migrates affected tasks to a quarantine subnet.","explanation":"## Why This Is Asked\n\nThis question evaluates the ability to design comprehensive detection, containment, and audit workflows across multiple AWS accounts with real-time automation while balancing security and cost signals.\n\n## Key Concepts\n\n- Config + GuardDuty integration for instant security findings\n- EventBridge + Lambda-driven automated remediation\n- Security group isolation for ECS components\n- Cross-account auditing via CloudTrail, Config, and Budgets\n- Safe rollback procedures and controlled testing methodologies\n\n## Code Example\n\n```javascript\n// Lambda remediation skeleton\nexports.h","diagram":null,"difficulty":"intermediate","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","MongoDB","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T06:12:24.126Z","createdAt":"2026-01-20T21:41:53.156Z"},{"id":"q-5097","question":"In a two-region AWS deployment with Kinesis, S3, and Lambda for real-time analytics, a data surge throttles Kinesis shards and Lambda. Design an automated ops playbook to proactively scale shards and Lambda concurrency, enable cross-region failover, optimize costs, and validate rollback. What specific controls, processes, and tests would you implement?","answer":"Propose auto-scaling rules: enable Kinesis shard autoscaling (with Enhanced Fan-Out) and Lambda reserved concurrency per shard group; use SQS dead-letter with backoff for bursts. Implement cross-regio","explanation":"## Why This Is Asked\nTests practical ops automation for real-time analytics under burst pressure, focusing on scaling, cross-region resiliency, costs, and rollback.\n\n## Key Concepts\n- Kinesis shard autoscaling and Enhanced Fan-Out\n- Lambda reserved concurrency and per-shard scaling\n- Cross-region failover with Route 53 health checks\n- Step Functions-based remediation workflows\n- Observability and cost governance through CloudWatch and Budgets\n\n## Code Example\n```json\n{\n  \"Comment\": \"Example remediation workflow\",\n  \"StartAt\": \"Assess Surge\",\n  \"States\": {\n    \"Assess Surge\": {\n      \"Type\": \"Choice\",\n      \"Choices\": [\n        { \"Variable\": \"$.surge\", \"NumericGreaterThan\": 1000, \"Next\": \"Scale Resources\" }\n      ],\n      \"Default\": \"NoOp\"\n    },\n    \"Scale Resources\": {\n      \"Type\": \"Task\",\n      \"Resource\": \"arn:aws:states:::aws-sdk:kinesis:updateShardCount\",\n      \"Parameters\": {\n        \"StreamName.$\": \"$.stream\",\n        \"TargetShardCount\": 20\n      },\n      \"End\": true\n    }\n  }\n}\n```\n\n## Follow-up Questions\n- How would you validate the failover path and what tests would you run first?\n- What common pitfalls could break autoscaling or consistency during bursts?","diagram":"flowchart TD\n  A[Data surge detected] --> B[Scale Kinesis shards]\n  B --> C[Adjust Lambda concurrency]\n  C --> D[Enable cross-region failover]\n  D --> E[Remediation via Step Functions]\n  E --> F[Run tests & rollback]","difficulty":"intermediate","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","Oracle","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T05:55:45.017Z","createdAt":"2026-01-21T05:55:45.017Z"},{"id":"q-5138","question":"In a production stack: SPA on S3+CloudFront, API on ECS Fargate behind ALB, RDS primary in us-east-1 with cross-region replica in us-west-2. Us-east-1 outage occurs. Design an automated DR to keep services online within 15 minutes: (1) cutover data and API endpoints to us-west-2, (2) which AWS services and event flows to automate (EventBridge, Lambda, Step Functions), (3) how you verify end‑to‑end health, and (4) a testing plan and rollback?","answer":"Promote the us-west-2 RDS replica to primary, switch API traffic via Route53 DNS failover to the us-west-2 endpoint, and swap ALB target groups. Use EventBridge + Step Functions to orchestrate DB prom","explanation":"## Why This Is Asked\nTests DR readiness, cross-region orchestration, and automation under pressure.\n\n## Key Concepts\n- Cross-region failover for RDS; Route53 health checks; ALB target group swaps; Step Functions workflows; CloudFront invalidations; parameter store configs.\n\n## Code Example\n```bash\n# pseudo-commands illustrating a Step Functions task chain\naws rds promote-readReplica --db-instance-identifier us-west-2-replica\naws route53 change-resource-record-sets --change-batch file://weights.json\n```\n\n## Follow-up Questions\n- How would you ensure data durability with potential write lag?  \n- How would you test this DR workflow without impacting customers?","diagram":null,"difficulty":"intermediate","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Google","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T07:51:01.925Z","createdAt":"2026-01-21T07:51:01.925Z"},{"id":"q-5172","question":"Scenario: A payment service runs primary in us-east-1 with ECS Fargate, API Gateway, Lambda, DynamoDB, RDS and S3. To meet zero-downtime DR, implement an active/active DR in eu-west-1 with near-zero RTO/RPO. Provide concrete architecture and automation: cross-region replication, data consistency choices, networking, IAM, encryption, monitoring, automated DR drills, and rollback steps. Include failover workflow and testing plan?","answer":"Implement active/active DR between us-east-1 and eu-west-1 for a payments stack using DynamoDB Global Tables and Aurora Global Database, with Route 53 failover and latency-based routing. Use cross-reg","explanation":"## Why This Is Asked\nAssesses hands-on ability to design robust, automated disaster recovery for a real-world payment workload, balancing data consistency, latency, and regulatory requirements.\n\n## Key Concepts\n- DynamoDB Global Tables and Aurora Global Database for cross-region replication\n- Route 53 failover with health checks and latency routing\n- Cross-region S3 replication, KMS CMKs, IAM least privilege\n- Infrastructure as Code (CDK), automated DR drills, rollback playbooks\n- Observability: CloudWatch, GuardDuty, X-Ray, CloudTrail\n\n## Code Example\n```typescript\nimport * as dynamodb from 'aws-cdk-lib/aws-dynamodb';\n// minimal illustration of a global table across two regions\nconst globalTable = new dynamodb.Table(this, 'Orders', {\n  partitionKey: { name: 'PK', type: dynamodb.AttributeType.STRING },\n  replicationRegions: ['us-east-1', 'eu-west-1'],\n});\n```\n\n## Follow-up Questions\n- How would you adjust if PCI-DSS constraints limit cross-region data movement?\n- How would you validate RTO/RPO during drills without impacting customers?\n- What metrics and alarms would you surface to operators during a failover?","diagram":null,"difficulty":"advanced","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T09:42:00.342Z","createdAt":"2026-01-21T09:42:00.344Z"},{"id":"q-5272","question":"In a multi-region setup for a fintech-like app, design a DR strategy for a PostgreSQL RDS primary in us-east-1 with a cross-region read replica in eu-west-1. Requirements: 1) ensure max RPO 5 minutes, RTO 15 minutes for production; 2) enable automated cross-region backups and snapshot sharing to eu-west-1; 3) configure Route 53 health checks and failover routing from prod to secondary region; 4) implement automated failover testing in a separate staging account every month; 5) address IAM permissions, encryption, and least privilege. Describe the architecture, trade-offs, and testing steps?","answer":"Design a cross-region DR plan for a PostgreSQL RDS primary in us-east-1 with a cross-region read replica in eu-west-1. Target RPO ≤5 min, RTO ≤15 min. Enable continuous WAL shipping, cross-region auto","explanation":"## Why This Is Asked\nTests practical DR planning across regions, including data protection, failover automation, and governance.\n\n## Key Concepts\n- Cross-region RDS replication and RPO/RTO goals\n- Route 53 failover and health checks\n- Automated regional backups and snapshot sharing\n- IAM least privilege, encryption, and cross-account access\n- DR testing in a staging account\n\n## Code Example\n```bash\n# Example: enable snapshot sharing (pseudo)\naws rds modify-db-instance --db-instance-identifier mydb --allocated-storage 100\n```\n\n## Follow-up Questions\n- How would you monitor both regions for DR readiness and alert on drift?\n- What changes if the replica is in Aurora Global Database instead of standard RDS?","diagram":null,"difficulty":"intermediate","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Coinbase","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T14:43:13.735Z","createdAt":"2026-01-21T14:43:13.735Z"},{"id":"q-5379","question":"In a single AWS region, a small web app runs on EC2 behind an ALB and uses a small Redis cache on ElastiCache. Design a beginner-friendly auto-heal and cost-guard strategy: (1) configure an Auto Scaling Group with min 2, max 6 instances and a simple CPU-based scaling threshold, (2) add a monthly AWS Budgets alert with a sensible cap, (3) implement a basic health-check remediation that recycles unhealthy instances, (4) set up basic logging and (optional) scheduled non-prod shutdown. Outline components, thresholds, and testing steps?","answer":"Configure an Auto Scaling Group with min 2 and max 6, a target CPUUtilization of 60% for scale-out and 40% for scale-in. Add a Budgets alert at $50/month with SNS. Implement a basic health check that ","explanation":"## Why This Is Asked\nAssesses basic Ops skills in autoscaling, cost controls, remediation, and observability for a common web app.\n\n## Key Concepts\n- Auto Scaling Group min/max, simple CPU-based scaling\n- CloudWatch alarms and health checks\n- AWS Budgets and SNS alerts\n- Basic remediation (recycle unhealthy instances)\n- Logging/monitoring basics\n\n## Code Example\n```javascript\n// Example: create a CloudWatch alarm for CPUUtilization using AWS SDK v3\nconst { CloudWatchClient, PutMetricAlarmCommand } = require(\"@aws-sdk/client-cloudwatch\");\n\n// ... instantiate client and call PutMetricAlarmCommand with threshold 60\n```\n\n## Follow-up Questions\n- How would you validate the cost guard during a simulated spike?\n- What changes if the workload is memory-bound rather than CPU-bound?","diagram":"flowchart TD\n  A[EC2] --> B[ALB]\n  B --> C[ASG]\n  C --> D[Health Checks/Remediation]\n  C --> E[Budgets SNS]","difficulty":"beginner","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","IBM","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T19:53:20.124Z","createdAt":"2026-01-21T19:53:20.124Z"},{"id":"q-5514","question":"In a six-region AWS deployment for real-time analytics, implement automated cross-account regional DR with a warm standby region and 10-minute RTO using Route 53 health checks, Lambda orchestration, SSM Automation, EventBridge, and Step Functions. How would you handle stateful components (DynamoDB, Kinesis, EMR), testing, and rollback?","answer":"Propose a primary region with a warm standby in a separate account and Route 53 health checks with latency-based routing to switch traffic within 10 minutes. Mirror DynamoDB via Global Tables, replica","explanation":"## Why This Is Asked\n\nReal-world DR planning across multiple regions with cross-account controls, automation, and data/state considerations.\n\n## Key Concepts\n\n- Cross-region disaster recovery orchestration\n- Stateful data replication (DynamoDB Global Tables, Kinesis)\n- Cross-account automation (SSM, IAM roles)\n- Event-driven workflows (EventBridge, Step Functions)\n- Testing, validation, rollback and compliance\n\n## Code Example\n\n```javascript\n// Example: trigger DR workflow via Step Functions during outage\nconst { SFClient, StartExecutionCommand } = require('@aws-sdk/client-sfn');\nconst sf = new SFClient({ region: 'us-east-1' });\nasync function triggerDR(input) {\n  const cmd = new StartExecutionCommand({\n    stateMachineArn: 'arn:aws:states:us-east-1:123456789012:stateMachine:DRWorkflow',\n    input: JSON.stringify(input)\n  });\n  return sf.send(cmd);\n}\n```\n\n## Follow-up Questions\n\n- How would you verify RTO/RPO targets with automated runbooks and tests?\n- What idle-time and cost controls would you apply to keep standby region ready without breaking budgets?","diagram":null,"difficulty":"advanced","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","MongoDB","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T04:13:53.424Z","createdAt":"2026-01-22T04:13:53.427Z"},{"id":"q-5600","question":"Scenario: A single-region deployment runs a web app on EC2 with a 100 GB EBS volume. How would you implement a beginner-friendly automated backup policy using Data Lifecycle Manager (hourly snapshots, 7-day retention), tag snapshots with App=Web and Env=Prod, include a Lambda-based integrity test that mounts the latest snapshot to a temporary volume, add an AWS Budgets alert to cap costs, and provide a clear testing and rollback plan?","answer":"Use Data Lifecycle Manager to automate hourly EBS snapshots on the 100 GB volume with 7-day retention. Tag snapshots App=Web,Env=Prod. Add a Lambda integrity test that mounts the latest snapshot on a ","explanation":"## Why This Is Asked\nTests practical backup automation in a common AWS SysOps scenario, including tagging, cost control, and validation.\n\n## Key Concepts\n- Data Lifecycle Manager (DLM) for snapshots\n- EBS snapshot retention and tagging policies\n- Lambda-based integrity validation of snapshots\n- AWS Budgets alerts for backup costs\n- Testing and rollback procedures\n\n## Code Example\n```bash\n#!/bin/bash\n# DLM policy creation steps (illustrative)\n```\n\n## Follow-up Questions\n- How would you adjust this for cross-region backups?\n- What metrics would you monitor to detect failed or slow restores?","diagram":"flowchart TD\n  A[EC2: Web App] --> B[DLM Policy]\n  B --> C[Hourly Snapshots]\n  C --> D[7-day Retention]\n  E[Lambda Test] --> F[Mount & Verify]\n  F --> G[Budgets Alert]\n  G --> H[Test & Rollback]","difficulty":"beginner","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","IBM","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T07:53:25.956Z","createdAt":"2026-01-22T07:53:25.956Z"},{"id":"q-5673","question":"In a dual-region deployment (Region us-east-1 and us-west-2) serving with DynamoDB Global Tables, S3, and EC2 workers behind ALB, design an automated disaster-recovery workflow that meets RPO 5 minutes and RTO 15 minutes. Specify your cross-region failover strategy using Route 53, DynamoDB, and S3 replication; outline how a centralized OPS account triggers failover via EventBridge and Step Functions; and provide testing and rollback steps?","answer":"Implement active-passive failover with Route53 health-check-based routing, DynamoDB Global Tables multi-region replication, and S3 cross-region replication. Use a centralized Ops account to trigger fa","explanation":"## Why This Is Asked\n\nTests DR planning across regions, automation, and cross-account permissions; expects use of standard AWS DR tooling (Route 53 failover, DynamoDB Global Tables, S3 replication) and a controlled failover flow via EventBridge/Step Functions.\n\n## Key Concepts\n\n- Route53 health check failover\n- DynamoDB Global Tables cross-region replication\n- S3 cross-region replication and bucket policies\n- EC2 Auto Scaling Groups and ALB failover\n- EventBridge, Step Functions cross-account triggers\n- Testing and rollback procedures\n\n## Code Example\n\n```yaml\n# Step Functions state machine skeleton for region failover\nComment: FailoverWorkflow\n```\n\n## Follow-up Questions\n\n- How would you validate RPO/RTO during a simulated failover without impacting customers?\n- What IAM and SCP controls would you enforce to prevent accidental or malicious failover executions?","diagram":null,"difficulty":"advanced","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T10:57:52.674Z","createdAt":"2026-01-22T10:57:52.674Z"},{"id":"q-5780","question":"In a four-account, multi-region production setup hosting critical data and compute, design an automated DR failover for S3 data and metadata. Use AWS Backup, S3 Cross-Region Replication, IAM roles, EventBridge, Lambda, and Route 53 failover. Define RPO/RTO targets, failover triggers, and a testing plan including rollback?","answer":"Propose DR using per-account S3 buckets with versioning and cross-region replication to a partner region, AWS Backup for lifecycle, and IAM roles for cross-account access. Use HealthAlerts to trigger ","explanation":"## Why This Is Asked\nThis tests multi-region DR design, cross-account IAM, event-driven automation, routing failover, and validation.\n\n## Key Concepts\n- Cross-account roles and trust\n- S3 replication and backups\n- Route 53 health checks and failover\n- EventBridge/Lambda orchestration\n- DR testing and rollback\n\n## Code Example\n```yaml\n# CloudFormation stub illustrating a Route53 record with failover (simplified)\nResources:\n  DRRecord:\n    Type: AWS::Route53::RecordSet\n    Properties:\n      Name: dr.example.com.\n      Type: A\n      SetIdentifier: primary\n      FailoverRegion: us-east-1\n      ResourceRecords:\n        - 1.2.3.4\n      TTL: 60\n      HealthCheckId: !Ref HealthCheck\n```\n\n## Follow-up Questions\n- How would you handle credential rotation for cross-account access?\n- What changes if the primary region experiences latency spikes rather than full outage?","diagram":null,"difficulty":"advanced","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T16:13:22.651Z","createdAt":"2026-01-22T16:13:22.651Z"},{"id":"q-5915","question":"In a four-region prod deployment across two AWS accounts (prod and DR), design an automated DR for a stateless API fronted by ALB and backed by a multi-region database. Use Route 53 failover, cross-region replicas or DynamoDB global tables, S3 backups with replication, and cross-account IAM roles. Provide RTO/RPO, failover/failback choreography, testing plan, and rollback steps?","answer":"Route 53 active-passive failover configuration with health checks monitoring primary region ALB endpoints; cross-region database replication using RDS cross-region read replicas or DynamoDB Global Tables for automatic data synchronization; S3 cross-region replication with versioning enabled for backup durability; cross-account IAM roles implementing least privilege principles for secure resource access; Infrastructure as Code-driven DR promotion using GitOps workflows with automated rollback capabilities. RTO target: <5 minutes for DNS failover, <15 minutes for full service restoration; RPO target: <1 minute for database replication, <5 minutes for S3 replication. Failover choreography: health check failure triggers Route 53 failover, promotes DR region infrastructure, updates DNS records; Failback: manual verification followed by automated promotion of primary region. Testing plan: monthly DR drills with automated chaos engineering, quarterly full-scale failover testing. Rollback steps: GitOps workflow reverts to previous infrastructure state, Route 53 health checks restored to primary endpoints.","explanation":"## Why This Is Asked\n\nThis question evaluates multi-region DR architecture design, cross-account IAM implementation, and automated failover orchestration under realistic SRE constraints.\n\n## Key Concepts\n\n- Route 53 failover routing and health checks\n- Cross-region DB replication (RDS read replicas) or DynamoDB Global Tables\n- S3 cross-region replication and versioning\n- Cross-account IAM roles and least privilege principles\n- IaC-driven, GitOps DR promotion workflows\n\n## Code Example\n\n```javascript\n// Pseudo-IaC snippet sketch (not executable)\nconst drPlan = { \n  regions: ['us-east-1','eu-west-1'],\n  accounts: ['prod','dr'],\n  failover: {\n    dns: 'route53',\n    healthChecks: 'alb-endpoints',\n    rto: '<5min',\n    rpo: '<1min'\n  }\n};\n```","diagram":"flowchart TD\n  A[Global DNS] --> B[Prod region health checks]\n  B --> C{Healthy?}\n  C -- Yes --> D[Route to prod ALB]\n  C -- No --> E[Failover to DR region and DR ALB]","difficulty":"advanced","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Anthropic","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T05:53:06.966Z","createdAt":"2026-01-22T21:52:37.592Z"},{"id":"q-5961","question":"In a multi-account AWS data lake with cross-region replication (S3 CRR) to support Snowflake data sharing and Google Cloud consumers, CRR for a critical bucket pair started failing after a role rotation. Provide a practical remediation plan, including IAM trust/permissions verification, CRR reinitialization steps, automated alerts, and a rollback/test strategy to confirm data consistency?","answer":"1) Verify the cross-region replication role's trust policy and permissions after rotation; reattach the CRR role if needed. 2) Examine S3 CRR error logs in CloudWatch, fix bucket ownership and replica configuration settings. 3) Reinitialize CRR by updating the replication configuration and triggering a sync of existing objects. 4) Set up automated CloudWatch alerts for CRR failures and IAM role changes. 5) Implement a rollback strategy with data consistency validation using S3 inventory and checksum comparisons.","explanation":"## Why This Is Asked\nThis tests practical crisis remediation for CRR failures, IAM drift, and automated recovery in a multi-account data lake.\n\n## Key Concepts\n- S3 Cross-Region Replication and troubleshooting\n- IAM role trust policies and least privilege\n- CloudWatch metrics/logs and alarms\n- Safe rollback and data-consistency validation\n\n## Code Example\n```javascript\n// Node.js: fetch and inspect CRR configuration for a bucket\nconst { S3Client, GetBucketReplicationConfigurationCommand } = require('@aws-sdk/client-s3');\nconst c = new S3Client({region:'us-east-1'});\nasync function inspectCRR(bucketName) {\n  const cmd = new GetBucketReplicationConfigurationCommand({Bucket: bucketName});\n  const config = await c.send(cmd);\n  console.log('CRR Role:', config.ReplicationConfiguration.Role);\n  console.log('Rules:', config.ReplicationConfiguration.Rules);\n  return config;\n}\n```","diagram":null,"difficulty":"intermediate","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T04:03:44.740Z","createdAt":"2026-01-22T23:47:54.569Z"},{"id":"q-6034","question":"Design a beginner-friendly cross-region disaster recovery plan for a web app with a primary in us-east-1 (EC2 behind ALB and RDS MySQL) and a warm standby in us-west-2 via an RDS cross-region read replica. Describe failover criteria, Route 53 health checks and DNS switch, replication considerations for near 5-minute RPO, cost guardrails, and a practical DR test plan?","answer":"Create an RDS cross-region read replica in us-west-2 from us-east-1. On DR, promote to primary; update ALB target group to the new endpoint; Route 53 active-passive failover with a health check on the","explanation":"## Why This Is Asked\nTests practical DR thinking for beginner level, not just theory.\n\n## Key Concepts\n- Cross-region RDS replicas and promotion\n- Route 53 DNS failover with health checks\n- ALB target management during failover\n- Cost control for warm standby\n- DR testing plan and RPO targets\n\n## Code Example\n```javascript\n// Lambda skeleton to switch DNS to standby\nexports.handler = async () => {\n  // use AWS.Route53 to update A record to standby endpoint\n};\n```\n\n## Follow-up Questions\n- How would you test failover without impacting production data?\n- What changes if RPO target is 1 minute?","diagram":"flowchart TD\n  A[User requests page] --> B[ALB us-east-1]\n  B --> C[RDS us-east-1 primary]\n  D[Replica us-west-2 standby]\n  E[Route53 health checks]\n  F[Failover to us-west-2]","difficulty":"beginner","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Discord","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T05:50:41.715Z","createdAt":"2026-01-23T05:50:41.715Z"},{"id":"q-6070","question":"In a cross-region AWS setup spanning three accounts (prod, shared-services, dr), design an automated DR workflow for a real-time data ingestion pipeline (MSK to S3 analytics) that preserves data integrity and achieves RPO < 5s and RTO < 15m. Include MSK cross-region replication, S3 replication/archiving, Route 53 failover, EventBridge/Lambda orchestration, and SCP-based IAM controls; outline a testing plan?","answer":"Architect a DR that auto-fails over to DR within 15 minutes, achieving RPO under 5 seconds. Use MSK cross-region replication to a standby cluster, mirror S3 data with bucket replication and Glacier ar","explanation":"## Why This Is Asked\n\nTests ability to design cross-account, cross-region DR for streaming pipelines with automated failover and data integrity.\n\n## Key Concepts\n- Cross-region replication for streaming\n- IAM SCPs and least privilege in multi-account setups\n- Route 53 failover and EventBridge/Lambda orchestration\n- DR testing, rollback automation, RPO/RTO goals\n\n## Code Example\n```bash\naws route53 change-resource-record-sets --hosted-zone-id Z123... --change-batch file://dr-failover.json\n```\n\n## Follow-up Questions\n- How would you monitor lag between MSK clusters during DR?\n- How would you extend to multiple DR regions and data validation checks?","diagram":"flowchart TD\n  A[Prod MSK] --> B[DR MSK]\n  C[S3 Prod] --> D[S3 DR]\n  E[Route53 Health Check] --> F[Failover to DR]\n  G[EventBridge/Lambda] --> H[Repoint Producers to DR]","difficulty":"advanced","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T07:39:44.750Z","createdAt":"2026-01-23T07:39:44.751Z"},{"id":"q-6264","question":"Scenario: A two-region deployment (us-east-1, eu-west-1) runs a multi-tier app: front-end SPA on S3+CloudFront, API on ECS Fargate behind ALB, and a managed Postgres (Aurora) with cross-region read replica in eu-west-1. You must meet RTO of 15 minutes and RPO of 5 minutes for critical data, while keeping monthly costs under control. Design an automated DR strategy: replication topology, failover decision logic via Route 53 health checks, a Step Functions/ Lambda workflow for failover, and a quarterly DR test plus rollback plan. Include monitoring and IAM controls?","answer":"Implement Aurora Global Database with cross-region writes; target RPO 5m, RTO 15m. Route 53 failover to eu-west-1 on us-east-1 outage; ECS Fargate in both regions; CloudFront origin failover. Use Step","explanation":"## Why This Is Asked\n\nThis tests real-world DR planning across regions, linking RPO/RTO to concrete AWS primitives and automation.\n\n## Key Concepts\n\n- Aurora Global Database cross-region replication\n- Route 53 health checks and failover routing\n- ECS Fargate, ALB, CloudFront origin switching\n- Step Functions and Lambda for automation\n- DR testing and rollback planning\n\n## Code Example\n\n```javascript\n// Lambda to trigger DR workflow upon health alert\nexports.handler = async (event) => {\n  const regionHealthy = event.detail?.regionStatus === 'Healthy';\n  if (!regionHealthy) return {action: 'start-dr'};\n  return {action: 'ok'};\n}\n```\n\n## Follow-up Questions\n\n- How would you simulate RPO violations in DR tests?\n- What IAM boundaries ensure least privilege during failover?","diagram":null,"difficulty":"intermediate","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","Instacart","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T16:51:21.466Z","createdAt":"2026-01-23T16:51:21.466Z"},{"id":"q-6304","question":"In a single AWS region, a small API app runs behind API Gateway + Lambda and uses DynamoDB. Design a beginner-friendly disaster-recovery guardrail to automatically redirect traffic to a warm standby in a second region during regional outages. Outline data replication method, Route 53 failover configuration, health-check thresholds, target RTO/RPO, IaC approach, and testing steps?","answer":"Use DynamoDB Global Tables for cross-region replication; Route 53 failover with health checks (Region A healthy if API Gateway 200s and DynamoDB latency under threshold). Primary targets Region A; sta","explanation":"## Why This Is Asked\n\nTests ability to design basic DR for serverless components with multi-region replication using AWS services.\n\n## Key Concepts\n\n- DynamoDB Global Tables for multi-region replication\n- Route 53 health checks and failover routing\n- CloudFormation as IaC for repeatable deployments\n- RTO/RPO targets and testing strategies\n- Data consistency and latency considerations\n\n## Code Example\n\n```yaml\nAWSTemplateFormatVersion: '2010-09-09'\nResources:\n  DynamoTable:\n    Type: AWS::DynamoDB::Table\n    Properties:\n      AttributeDefinitions:\n        - AttributeName: PK\n          AttributeType: S\n      KeySchema:\n        - AttributeName: PK\n          KeyType: HASH\n      BillingMode: PAY_PER_REQUEST\n      StreamSpecification:\n        StreamViewType: NEW_IMAGE\n      TableName: MyTable\n```\n\n## Follow-up Questions\n\n- How would you test failover in a non-production environment?\n- What changes would you make if data writes occur during failover to prevent conflicts?","diagram":"flowchart TD\n  A[User Request] --> B[Route 53 Health Check]\n  B --> C{Region A Healthy?}\n  C -- Yes --> D[Route 53 to Region A API]\n  C -- No --> E[Route 53 to Region B API]","difficulty":"beginner","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Lyft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T18:49:28.381Z","createdAt":"2026-01-23T18:49:28.382Z"},{"id":"q-6347","question":"Context: Beginner AWS SysOps, a small web app deployed in one region with EC2 Auto Scaling behind an ALB and a PostgreSQL RDS. Design a beginner-friendly disaster recovery plan to recover in a second AWS region within 2 hours during a regional outage. Provide concrete steps using cross-region RDS backups or read replicas, cross-region S3 replication for logs, Route 53 failover with health checks, and an IaC template (CloudFormation or CDK) to redeploy quickly. Include a simple runbook, testing steps, and cost guardrails?","answer":"DR plan uses cross-region RDS backups (snapshots) copied to the target region, stand up an identical EC2/ASG behind an ALB there, enable cross-region S3 replication for logs, and configure Route 53 fa","explanation":"## Why This Is Asked\n\nTests practical DR readiness for a real workload using core AWS services (RDS, EC2/ASG, S3, Route 53) with an IaC-first mindset. Focuses on beginners learning backups, replication, DNS failover, and cost governance.\n\n## Key Concepts\n\n- Cross-region backups and replication\n- Disaster recovery runbooks and rehearsals\n- DNS failover with Route 53 health checks\n- IaC for rapid redeployment and repeatability\n\n## Code Example\n\n```yaml\nAWSTemplateFormatVersion: '2010-09-09'\nResources:\n  DRRecordSet:\n    Type: 'AWS::Route53::RecordSet'\n    Properties:\n      Name: \"app.example.com.\"\n      Type: \"A\"\n      Failover: \"PRIMARY\"\n      TTL: 300\n      ResourceRecords:\n        - \"1.2.3.4\"  # placeholder for primary region\n```\n\n## Follow-up Questions\n\n- How would you reduce RTO to under 60 minutes?  \n- What monitoring would you add to detect DR readiness issues early?","diagram":null,"difficulty":"beginner","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","Oracle","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T20:34:47.220Z","createdAt":"2026-01-23T20:34:47.223Z"},{"id":"q-6452","question":"Design an automated containment and forensics workflow for a suspected compromised EC2 in prod within a three-account AWS Organization spanning two regions. The solution must detect via GuardDuty/Security Hub, isolate using Security Groups and SSM, rotate IAM credentials and keys, preserve forensics data (EBS snapshots, CloudTrail, VPC flow logs) to encrypted S3 with Object Lock, and implement a rollback/restore plan with runbooks and testing?","answer":"Implement a cross-region containment workflow orchestrated via Step Functions, triggered by GuardDuty/Security Hub findings. Isolate the compromised EC2 by updating Security Groups to deny traffic, use SSM Run Command to quarantine the host, rotate IAM credentials and access keys, preserve forensics data through EBS snapshots, CloudTrail exports, and VPC flow logs to encrypted S3 with Object Lock, and establish a comprehensive rollback/restore plan with documented runbooks and regular testing.","explanation":"## Why This Is Asked\nTests automated incident response capabilities across multi-account, multi-region AWS environments with emphasis on containment automation and forensics preservation.\n\n## Key Concepts\n- GuardDuty/Security Hub automation and cross-account integration\n- Cross-region IAM credential rotation workflows\n- SSM-based quarantine and Run Command execution\n- EBS snapshots, CloudTrail, and VPC flow logs preservation\n- S3 with KMS encryption, Object Lock, and immutable vault storage\n- Rollback procedures, runbooks, and testing frameworks\n\n## Code Example\n```javascript\n{\n  \"Comment\": \"Step Functions workflow for EC2 containment\",\n  \"StartAt\": \"DetectCompromise\",\n  \"States\": {\n    \"DetectCompromise\": {\n      \"Type\": \"Task\",\n      \"Resource\": \"arn:aws:states:::lambda:invoke\",\n      \"Parameters\": {\n        \"FunctionName\": \"process-guardduty-alert\"\n      },\n      \"Next\": \"IsolateInstance\"\n    },\n    \"IsolateInstance\": {\n      \"Type\": \"Parallel\",\n      \"Branches\": [\n        {\n          \"StartAt\": \"UpdateSecurityGroups\",\n          \"States\": {\n            \"UpdateSecurityGroups\": {\n              \"Type\": \"Task\",\n              \"Resource\": \"arn:aws:states:::lambda:invoke\",\n              \"Parameters\": {\n                \"FunctionName\": \"deny-all-traffic-sg\"\n              }\n            }\n          }\n        },\n        {\n          \"StartAt\": \"QuarantineHost\",\n          \"States\": {\n            \"QuarantineHost\": {\n              \"Type\": \"Task\",\n              \"Resource\": \"arn:aws:states:::ssm:sendCommand\",\n              \"Parameters\": {\n                \"InstanceIds\": [\"$.instanceId\"],\n                \"DocumentName\": \"AWS-RunShellScript\",\n                \"Parameters\": {\n                  \"commands\": [\"iptables -A INPUT -j DROP\"]\n                }\n              }\n            }\n          }\n        }\n      ],\n      \"Next\": \"PreserveForensics\"\n    },\n    \"PreserveForensics\": {\n      \"Type\": \"Parallel\",\n      \"Branches\": [\n        {\n          \"StartAt\": \"CreateEBSSnapshots\",\n          \"States\": {\n            \"CreateEBSSnapshots\": {\n              \"Type\": \"Task\",\n              \"Resource\": \"arn:aws:states:::lambda:invoke\",\n              \"Parameters\": {\n                \"FunctionName\": \"create-encrypted-snapshots\"\n              }\n            }\n          }\n        },\n        {\n          \"StartAt\": \"ExportCloudTrail\",\n          \"States\": {\n            \"ExportCloudTrail\": {\n              \"Type\": \"Task\",\n              \"Resource\": \"arn:aws:states:::lambda:invoke\",\n              \"Parameters\": {\n                \"FunctionName\": \"export-cloudtrail-to-s3\"\n              }\n            }\n          }\n        }\n      ],\n      \"Next\": \"RotateCredentials\"\n    },\n    \"RotateCredentials\": {\n      \"Type\": \"Task\",\n      \"Resource\": \"arn:aws:states:::lambda:invoke\",\n      \"Parameters\": {\n        \"FunctionName\": \"rotate-iam-credentials\"\n      },\n      \"Next\": \"CreateRollbackPlan\"\n    },\n    \"CreateRollbackPlan\": {\n      \"Type\": \"Task\",\n      \"Resource\": \"arn:aws:states:::lambda:invoke\",\n      \"Parameters\": {\n        \"FunctionName\": \"generate-rollback-runbook\"\n      },\n      \"End\": true\n    }\n  }\n}\n```\n\n## Implementation Notes\n- **Cross-Account Integration**: Use AWS Organizations SCPs and IAM roles for centralized incident response\n- **Immutable Storage**: S3 Object Lock with WORM compliance for forensics data preservation\n- **Automated Testing**: Regular chaos engineering drills to validate containment procedures\n- **Rollback Strategy**: Documented restoration procedures with tested recovery time objectives (RTO/RPO)","diagram":null,"difficulty":"advanced","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","DoorDash","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T04:55:26.810Z","createdAt":"2026-01-24T02:23:26.454Z"},{"id":"q-6718","question":"In an AWS environment spanning two regions and three accounts, design an automated disaster-recovery workflow for an Aurora PostgreSQL global cluster handling production workloads. Requirements: (1) RPO ≤ 60s, (2) RTO ≤ 10 minutes; (3) cross-region replication topology; (4) failover orchestration using Route 53, Lambda, and Step Functions; (5) cross-region backups to S3 with KMS; (6) IAM permissions and SCPs; (7) testing and rollback?","answer":"Design an Aurora PostgreSQL Global Database with the writer in Region A and a replica in Region B. Route53 fails over the DNS alias; a Step Functions workflow via Lambda promotes the replica to writer","explanation":"## Why This Is Asked\n\nTest ability to design automated, cross-region DR for a stateful workload with strict RPO/RTO in a multi-account setup. Evaluates orchestration, permissions, and testability under real-world constraints.\n\n## Key Concepts\n\n- Aurora Global Database topology and failover\n- Route53 failover routing and health checks\n- Step Functions with Lambda orchestration\n- Cross-region backups to S3 with KMS\n- IAM roles, SCPs, and least-privilege controls\n- Disaster-recovery testing and rollback procedures\n\n## Code Example\n\n```javascript\nconst drState = {\n  Comment: \"Aurora DR\",\n  StartAt: \"CheckHealth\",\n  States: {\n    CheckHealth: {\n      Type: \"Task\",\n      Resource: \"arn:aws:lambda:region:acct:function:CheckHealth\",\n      Next: \"Decide\"\n    },\n    Decide: {\n      Type: \"Choice\",\n      Choices: [\n        { Variable: \"$.healthy\", Boolean: true, Next: \"Failover\"}\n      ],\n      Default: \"NoFailover\"\n    },\n    Failover: { Type: \"Task\", Resource: \"arn:aws:lambda:region:acct:function:Failover\", End: true}\n  }\n}\n```\n\n## Follow-up Questions\n\n- How would you validate RPO/RTO during drills?\n- How handle data inconsistencies during failover?\n- What changes would you make for a blue/green restore?","diagram":null,"difficulty":"advanced","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Plaid","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T14:36:54.128Z","createdAt":"2026-01-24T14:36:54.128Z"},{"id":"q-675","question":"You manage a two-region AWS deployment (us-east-1, us-west-2) behind an ALB with private subnets, NAT gateway, and RDS in us-east-1. During business hours, us-west-2 exhibits spike in 5xx errors and higher latency. Outline immediate incident triage steps, AWS CLI commands to run, how you’d identify root causes (NAT saturation, DNS routing, cross-region replication lag), and both short- and long-term mitigations with verification steps?","answer":"Start by verifying scope in CloudWatch and X-Ray, then check ALB target health, NAT gateway quotas, and RDS latency across regions. Collect logs, compare regional error rates, and run a quick failover","explanation":"## Why This Is Asked\nThis question evaluates incident triage skills in a real multi-region AWS setup, focusing on identifying root causes under pressure, deciding rapid mitigations, and confirming with telemetry. It tests familiarity with ALB, NAT, RDS, Route53, and CloudWatch/X-Ray.\n\n## Key Concepts\n- Multi-region deployment\n- Incident triage\n- ALB, NAT gateway, RDS latency\n- CloudWatch, VPC Flow Logs, X-Ray\n- DR planning\n\n## Code Example\n```javascript\n// Pseudo-logic for triage steps in a real incident\nfunction triage(incident) {\n  // fetch metrics from CloudWatch and X-Ray\n  // check ALB target health and NAT quotas\n  // identify root cause and apply mitigation\n}\n```\n\n## Follow-up Questions\n- Which metrics would you alert on for cross-region failures?\n- How would you automate the triage workflow?","diagram":"flowchart TD\n  A[Incident] --> B[Triage]\n  B --> C{Root Cause?}\n  C --> D[NAT Saturation]\n  C --> E[DNS/Routing]\n  C --> F[RDS/Replication Lag]\n  D --> G[Mitigate: scale NAT]\n  E --> H[Mitigate: Route53 routing]\n  F --> I[Mitigate: DR drill]","difficulty":"intermediate","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T14:46:56.102Z","createdAt":"2026-01-11T14:46:56.102Z"},{"id":"q-6867","question":"In a single AWS region, a small web app runs on an EC2 Auto Scaling group behind an Application Load Balancer with a single RDS MySQL primary. Design a beginner-friendly auto-scaling strategy that uses both scheduled scaling for predictable load and dynamic scaling for bursts, while enforcing a monthly cost guardrail. Specify: min/max instances, Launch Template, scaling policies with thresholds, metrics to monitor (CPU, networkIn, networkOut, requestRate), CloudWatch alarms, a Budget alert, and a concise testing plan to verify both scaling behavior and cost control?","answer":"ASG: min 1, max 3; Launch Template: t3.medium; Scheduled scale-out to 2 instances 09:00-19:00, back to 1; Dynamic scaling: target tracking on CPU 60% scale-out, 40% scale-in; Metrics: CPU, networkIn, ","explanation":"## Why This Is Asked\nThe candidate demonstrates hands-on understanding of combining scheduled and dynamic autoscaling with cost controls in a realistic web-app scenario.\n\n## Key Concepts\n- EC2 Auto Scaling Groups, Launch Templates\n- Scheduled vs dynamic scaling\n- CloudWatch metrics and alarms\n- AWS Budgets and cost controls\n\n## Code Example\n```javascript\n// Example ASG policy shorthand\n{\n  \"MinSize\": 1,\n  \"MaxSize\": 3,\n  \"TargetTrackingConfiguration\": {\"TargetValue\": 60}\n}\n```\n\n## Follow-up Questions\n- How would you test scheduled scaling in a staging environment?\n- How would you handle sudden aggressive traffic spikes beyond max capacity?","diagram":"flowchart TD\n  A[User Request] --> B[ALB]\n  B --> C[ASG]\n  C --> D[EC2 instances]\n  C --> E[RDS]\n  B --> F[CloudWatch Alarms]\n  G[Budget] --> H[Cost Alerts]","difficulty":"beginner","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T20:50:29.186Z","createdAt":"2026-01-24T20:50:29.186Z"},{"id":"q-6963","question":"In a single-region production stack running 3 EC2 instances behind an ALB with RDS in the same region and S3 logging, latency spikes during business hours. Design a practical incident-response playbook for intermediate SysOps that (1) detects bottlenecks via CloudWatch and X-Ray; (2) drains the unhealthy AZ/instance safely; (3) auto-scales or replaces instances; (4) preserves data integrity; (5) verifies post-fix health and implements rollback. Include concrete AWS services, metrics, and rollback steps?","answer":"Candidates should outline: detect latency spikes using CloudWatch p95 latency alarms and X-Ray distributed traces to identify bottlenecks; safely drain the affected availability zone or instance using Auto Scaling Group lifecycle hooks with connection draining; perform canary or blue-green validation before full traffic cutover; maintain data integrity through RDS automated backups and transaction validation; verify post-remediation health using synthetic monitoring and implement rollback procedures via previous AMI versions and weighted ALB target group routing.","explanation":"## Why This Is Asked\nThis evaluates practical incident response capabilities for handling latency issues in a production EC2/ALB environment, testing systematic approach to detection, isolation, remediation, and rollback procedures.\n\n## Key Concepts\n- CloudWatch alarm configuration and p95 latency monitoring\n- X-Ray distributed tracing for bottleneck identification\n- Auto Scaling Group lifecycle hooks and connection draining\n- Canary and blue-green deployment strategies\n- RDS backup and data integrity validation\n- Application Load Balancer routing controls and weighted targets\n\n## Follow-up Questions\nCandidates should be prepared to discuss specific CloudWatch metrics, X-Ray trace analysis techniques, and detailed rollback scenarios.","diagram":null,"difficulty":"intermediate","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T05:26:33.601Z","createdAt":"2026-01-25T02:42:30.502Z"},{"id":"q-7016","question":"In a single AWS region, a fleet of Linux EC2 instances behind an ALB hosts a critical web app. Design a beginner-friendly patching workflow using AWS Systems Manager Patch Manager: weekly maintenance window, a baseline that auto-approves Security patches, tag-based targeting, after-hours execution, and a post-patch health check. Include rollback steps and a simple test plan to verify uptime and service health after patching?","answer":"Use AWS Systems Manager Patch Manager: weekly maintenance window, a Linux baseline that auto-approves Security patches, tag-based targeting (App:Web), after-hours execution, and a post-patch health ch","explanation":"## Why This Is Asked\nTests patch management automation, safe rollback, and a practical validation plan in a real-world, low-risk scenario.\n\n## Key Concepts\n- AWS Systems Manager Patch Manager\n- Maintenance windows and baselines\n- Tag-based targeting\n- Post-patch validation and rollback\n\n## Code Example\n```json\n{\n  \"Name\": \"Baseline-Security\",\n  \"OperatingSystem\": \"Linux\",\n  \"ApprovalRules\": {\n    \"PatchRules\": [\n      {\"PatchClassification\": \"Security\", \"ApproveAfterDays\": 0}\n    ]\n  }\n}\n```\n\n## Follow-up Questions\n- How would you handle patches requiring reboot during maintenance?\n- How would you test patching in a non-prod environment?","diagram":"flowchart TD\n  A[Weekly Patch Window] --> B[Linux Baseline: Security patches]\n  B --> C[Target by Tag App:Web]\n  C --> D[Run Patch Manager]\n  D --> E[Post-patch Health Check]\n  E --> F{Healthy?}\n  F --> G[Continue Monitoring]\n  F --> H[Rollback: AMI snapshot & undo Run Command]","difficulty":"beginner","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T05:49:15.957Z","createdAt":"2026-01-25T05:49:15.957Z"},{"id":"q-7061","question":"In a single AWS region, a serverless web app uses API Gateway fronting Lambda functions that access a DynamoDB table and emit logs to S3. Design a beginner-friendly disaster-recovery and cost-control guardrail: enable DynamoDB point-in-time recovery, set TTL on time-series items, add S3 lifecycle rules to move logs to Glacier after 90 days, and configure a Budgets alert. Include practical thresholds and a basic testing plan?","answer":"Enable DynamoDB PITR for a 30-day window, and TTL on time-series items (TTL=30 days). Add S3 Lifecycle to move logs older than 90 days to Glacier. Create a Budgets alert at 70% of the monthly cap and ","explanation":"## Why This Is Asked\n\nChecks ability to design practical DR and cost controls for a serverless stack using DynamoDB, S3, and Lambda in a single region.\n\n## Key Concepts\n\n- DynamoDB Point-In-Time Recovery (PITR)\n- DynamoDB Time To Live (TTL)\n- S3 Lifecycle policies (Standard to Glacier)\n- AWS Budgets and alarms\n- Basic testing of DR and cost controls\n\n## Code Example\n\n```javascript\n// Pseudo-code: enable PITR and TTL\nconst ddb = new DynamoDBClient({region: 'us-east-1'});\nawait ddb.send(new UpdateContinuousBackupsCommand({TableName: 'MyTable', PointInTimeRecoverySpecification: {PointInTimeRecoveryEnabled: true}}));\nawait ddb.send(new UpdateTimeToLiveCommand({TableName: 'MyTable', TimeToLiveSpecification: {Enabled: true, AttributeName: 'TTL'}}));\n```\n\n## Follow-up Questions\n\n- How would you adjust TTL windows to balance cost and data visibility?\n- What steps validate PITR restores and Lifecycle transitions end-to-end?","diagram":null,"difficulty":"beginner","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Citadel","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T07:43:15.325Z","createdAt":"2026-01-25T07:43:15.326Z"},{"id":"q-7110","question":"In a production app, the frontend is static on S3/CloudFront and the API runs behind an ALB with an RDS primary in us-east-1. A warm standby exists in us-west-2 via a cross-region RDS read replica and cross-region S3 replication. Design an automated failover plan to switch traffic to the standby within 15 minutes, including Route 53 health checks, DNS failover, RDS promotion, ALB target reconfiguration, CloudFront invalidation, and rollback steps. Include testing steps?","answer":"Plan a failover to a us-west-2 warm standby: Route 53 health checks trigger DNS failover, promote the us-west-2 RDS read replica to primary, switch ALB targets and update DB connection strings (via SS","explanation":"Why This Is Asked\n\nTests practical cross-region DR planning with concrete steps, timing, and rollback.\n\nKey Concepts\n- Route 53 failover\n- RDS cross-region replica promotion\n- ALB target switching\n- CloudFront invalidation and S3 replication state\n- Rollback and verification testing\n\nCode Example\n\n```javascript\n// Example placeholder not required for DR plan\n```\n\nFollow-up Questions\n- What telemetry confirms MTTR targets?\n- How would you handle ongoing writes during failover to avoid data loss?","diagram":"flowchart TD\n  A[Route53 health check] --> B[DNS failover to us-west-2]\n  B --> C[RDS read replica promoted to primary]\n  C --> D[ALB target switch and DB connection update]\n  D --> E[CloudFront invalidation & S3 replication update]\n  E --> F[Traffic verified and SLA drift checked]","difficulty":"intermediate","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Bloomberg","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T09:39:32.615Z","createdAt":"2026-01-25T09:39:32.615Z"},{"id":"q-7178","question":"In a single AWS region, a small web app runs on EC2 behind an ALB with an RDS MySQL primary and one read replica. Design a beginner-friendly automated backup and disaster-recovery test that weekly creates a fresh RDS snapshot, copies it to a separate test VPC, launches a temporary EC2 instance to validate DB connectivity, and records pass/fail metrics via CloudWatch Logs. Outline required resources, IAM permissions, schedule, and rollback steps?","answer":"Trigger weekly via EventBridge. Lambda: 1) Create a manual RDS snapshot of the primary; 2) wait for completion; 3) Restore DB instance from snapshot into a dedicated test VPC; 4) boot a small EC2 in t","explanation":"## Why This Is Asked\nAssesses understanding of automated DR testing using low-cost, isolated test runs that don’t impact production.\n\n## Key Concepts\n- RDS manual snapshots and restore patterns; cross-resource IAM permissions\n- EventBridge scheduling and Lambda orchestration\n- SSM-based validation in a separate test VPC\n- CloudWatch Logs and SNS for pass/fail reporting; resource cleanup\n\n## Code Example\n```python\n# pseudo\nimport boto3\nrds=boto3.client('rds')\n# 1) create snapshot\nresp = rds.create_db_snapshot(DBSnapshotIdentifier='prod-snap-001')\n# 2) wait for completion\n# 3) restore test DB in test VPC\n```\n\n## Follow-up Questions\n- How would you protect against failed snapshots or long restore times?\n- How would you scale this to multiple regions or trigger on demand for outages?","diagram":"flowchart TD\n  A[EventBridge weekly] --> B[Lambda: create snapshot]\n  B --> C[Wait for snapshot]\n  C --> D[Restore test DB]\n  D --> E[Launch test EC2]\n  E --> F[Run validation via SSM]\n  F --> G[Report & cleanup]","difficulty":"beginner","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Google","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T13:04:28.938Z","createdAt":"2026-01-25T13:04:28.938Z"},{"id":"q-7253","question":"In a cross-region AWS deployment hosting a real-time analytics pipeline (Kinesis, Lambda, Glue, S3) with a DR region, design an automated DR strategy that achieves an RPO of 5 minutes and an RTO of 15 minutes across two regions. Include data replication, cross-region KMS access, IAM safeguards, and an orchestration using EventBridge, Step Functions, and Route 53. Describe architecture, automation, testing, and rollback?","answer":"Implement warm-standby DR in a second region. Replicate data via S3 cross-region replication and keep a DR Glue catalog and KMS CMK material replicated with cross-region grants. Pre-provision Lambda, ","explanation":"## Why This Is Asked\nTests DR design with concrete RPO/RTO targets, multi-region data protection, and automated orchestration using native AWS services.\n\n## Key Concepts\n- Cross-region data replication (S3, Glue catalog)\n- KMS key material replication and cross-region grants\n- Pre-warmed DR resources and IAM safeguards\n- Orchestration with EventBridge, Step Functions, Route 53\n- Testing and rollback procedures\n\n## Code Example\n```bash\n#!/usr/bin/env bash\n# DR test scaffold\necho DR_TEST_STARTED\n```\n\n## Follow-up Questions\n- How would you validate DR readiness during drills without impacting production latency?\n- What metrics and alarms would you rely on to determine a successful failover versus partial degradation?","diagram":"flowchart TD\n  Primary[Primary Region] --> DR[DR Region]\n  Primary --> DNS1[Route53: Primary Health]\n  DR --> DNS2[Route53: DR Health]\n  DNS1 --> FailoverPolicy[Failover policy triggers]\n  FailoverPolicy --> DRResources[Activate DR resources]\n  DRResources --> Route53Update[Update DNS to DR endpoints]","difficulty":"advanced","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","DoorDash","Instacart"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T15:33:37.716Z","createdAt":"2026-01-25T15:33:37.716Z"},{"id":"q-7514","question":"In a multi-account AWS environment spanning three regions, you run a real-time analytics pipeline (Kinesis Data Streams, Firehose to S3, and Lambda processing) with separate prod, staging, and data-ops accounts. Design an automated cross-region failover so that if Region-1 becomes unavailable the pipeline automatically shifts to Region-2 within 15 minutes with at most 2 minutes data loss. Describe data replication, cross-account IAM roles, KMS key usage, event ordering guarantees, failover orchestration (Route 53, EventBridge, Lambda), testing plan, and rollback?","answer":"Adopt an active-passive DR where Region-1 data is continuously replicated to Region-2 for S3 via CRR, and a parallel Kinesis + Lambda stack in Region-2 is kept in a hot standby. Route53 health checks ","explanation":"## Why This Is Asked\n\nTests cross-region DR planning, cross-account IAM, and automated failover for real-time data pipelines under outages.\n\n## Key Concepts\n\n- Cross-region replication (S3 CRR) and data gravity\n- Cross-account IAM roles and least privilege\n- KMS CMK usage across regions and key policy design\n- Route 53 failover routing and health checks\n- EventBridge orchestration and Lambda handoffs\n\n## Code Example\n\n```javascript\n// Example: publish failover event to EventBridge for regional switch\nconst eb = new AWS.EventBridge({ region: 'us-east-1' });\neb.putEvents({ Entries: [{ EventBusName: 'region-failover', Source: 'dr failover', Detail: JSON.stringify({ targetRegion: 'us-west-2' }) }] }).promise();\n```\n\n## Follow-up Questions\n\n- How would you validate RPO/RTO during drills and what metrics would you alert on?\n- What rollback script would you run to re-synchronize Regions after failover ends?","diagram":null,"difficulty":"advanced","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","NVIDIA","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T05:40:55.426Z","createdAt":"2026-01-26T05:40:55.426Z"},{"id":"q-7701","question":"Scenario: A data pipeline runs Kinesis Data Streams -> Lambda -> S3 in Account A; Analytics Redshift in Account B. Partners push logs to Account A via cross-account IAM roles. Design a secure, auditable cross-account data sharing model with least privilege, KMS CMKs, and SCPs; implement automated drift checks with Config/State Manager; include testing and rollback steps?","answer":"Use a cross-account role in Account A that partners assume via sts:AssumeRole, limited to a specific S3 prefix and Redshift external schema. Enforce least privilege with resource ARNs, enable CMK encr","explanation":"## Why This Is Asked\nTests ability to design cross‑account access, least privilege, encryption, and drift detection, plus practical tests and rollback.\n\n## Key Concepts\n- Cross-account IAM roles with STS AssumeRole\n- SCPs and least privilege\n- CMK encryption in KMS\n- AWS Config drift checks and automation\n- Testing and rollback strategies\n\n## Code Example\n```json\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\"Effect\": \"Allow\",\"Action\": [\"s3:GetObject\",\"s3:PutObject\"],\"Resource\": \"arn:aws:s3:::partner-logs/*\"},\n    {\"Effect\": \"Allow\",\"Action\": [\"redshift:CopyFrom\",\"redshift:CreateTable\"],\"Resource\": \"arn:aws:iam::*:role/Redshift*\"}\n  ]\n}\n```\n\n## Follow-up Questions\n- How would you monitor for drift in role trust policies?\n- How would you simulate a breach to verify responses?","diagram":null,"difficulty":"intermediate","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Databricks","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T13:41:22.780Z","createdAt":"2026-01-26T13:41:22.780Z"},{"id":"q-7708","question":"In a multi-account AWS setup across three regions (primary us-east-1, secondary us-west-2, DR eu-west-1), design an automated DR for a data lake (S3, Glue Catalog, Athena) that achieves an RTO of 15 minutes and an RPO of 5 minutes. Include cross-region S3 replication, Glue Catalog replication, Lambda/Step Functions orchestration, Route 53 failover, and a test/rollback plan. Provide architecture, steps, and acceptance criteria?","answer":"Implement cross-region replication for S3 data buckets to the DR region with KMS-encrypted objects and strict bucket policies; mirror Glue Data Catalog in DR and auto-sync metadata via a Lambda/Step F","explanation":"## Why This Is Asked\n\nEvaluates DR planning at scale, cross-region data and metadata replication, and production-grade failover orchestration.\n\n## Key Concepts\n\n- Cross-region replication for S3\n- Glue Data Catalog mirroring\n- Step Functions orchestration\n- Route 53 failover routing\n- Cross-account IAM role design\n\n## Code Example\n\n```yaml\n# High-level DR failover workflow (pseudo)\nStateMachine:\n  Comment: DR failover workflow\n  StartAt: CheckHealth\n  States:\n    CheckHealth:\n      Type: Choice\n      Choices:\n        - Variable: $.status\n          StringEquals: healthy\n          Next: PromotePrimary\n```\n\n## Follow-up Questions\n\n- How would you measure and verify RPO during drills?  \n- How would you handle data mutations during failover and ensure idempotent re-synchronization after failback?","diagram":null,"difficulty":"advanced","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Instacart","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T14:44:06.682Z","createdAt":"2026-01-26T14:44:06.682Z"},{"id":"q-7772","question":"In a multi-account, multi-region data ingestion pipeline (Kinesis Data Streams -> Lambda -> S3 Data Lake, with a dedicated analytics account), design an automated incident response to detect a production data producer compromise (e.g., sudden 10x data spike, unusual source IPs, or unexpected record formats) and automatically isolate the producer's resources across accounts, preserve evidence, maintain data integrity, and notify the on-call. Outline detection signals, required IAM changes (least privilege for cross-account actions), cross-account Lambda/Step Functions logic, and testing plan including rollback?","answer":"Detect ingest anomalies using CloudWatch throughput, odd IPs in VPC logs, and record-format checks. On trigger, a cross-account Step Functions workflow revokes the producer's IAM roles, pauses Kinesis","explanation":"## Why This Is Asked\nAssesses IR automation across accounts, IAM least privilege, cross-account perms, and testing discipline for data pipelines.\n\n## Key Concepts\n- Cross-account IAM, Step Functions, Kinesis isolation\n- Data integrity, evidence preservation, forensics\n- Chaos testing, rollback planning\n\n## Code Example\n```javascript\n{\n  \"Comment\": \"Incident response state machine\",\n  \"StartAt\": \"IsolateProducer\",\n  \"States\": {\n    \"IsolateProducer\": {\"Type\": \"Task\", \"Resource\": \"arn:aws:lambda:...:function:IsolateProducer\", \"Next\": \"Notify\"},\n    \"Notify\": {\"Type\": \"Task\", \"Resource\": \"arn:aws:lambda:...:function:NotifyOnCall\", \"End\": true}\n  }\n}\n```\n\n## Follow-up Questions\n- How would you verify the rollback path without impacting production data?\n- What metrics would you ship to Security/IR dashboards to detect re-attack patterns?","diagram":null,"difficulty":"intermediate","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Plaid","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T17:11:28.927Z","createdAt":"2026-01-26T17:11:28.927Z"},{"id":"q-7896","question":"In a globally distributed SaaS app with data residency constraints, design an automated disaster-recovery plan across two AWS regions to meet RPO 15 minutes and RTO 1 hour for a stateful service (PostgreSQL RDS, Redis, and S3-backed object store). Include cross-region replication (RDS cross-region replicas, DynamoDB Global Tables), DNS failover via Route 53 and Global Accelerator, backup strategies, IAM least privileges, and a tested runbook with rollback?","answer":"Design a two-region DR for a stateful SaaS app: RDS PostgreSQL primary in Region A with cross-region replica, DynamoDB Global Tables, S3 Cross-Region Replication, and ElastiCache in Region B. Orchestr","explanation":"## Why This Is Asked\nTests cross-region DR design for stateful workloads, focusing on data residency, RPO/RTO targets, and automated cutover.\n\n## Key Concepts\n- Cross-region replication for RDS, DynamoDB Global Tables, S3 replication\n- DNS failover and global acceleration for fast cutover\n- Automated runbooks (Step Functions) and PITR\n- Validation, rollback plans, and testing cadence\n\n## Code Example\n```javascript\n// Pseudo Step Functions invocation for failover\nconst startFailoverWorkflow = async () => {\n  // trigger DR workflow\n};\n```\n\n## Follow-up Questions\n- How would you validate RPO/RTO during drills and what metrics would you surface?\n- How do you handle replication lag and ensure idempotent failover across regions?","diagram":null,"difficulty":"advanced","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Netflix","Scale Ai","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T21:53:23.063Z","createdAt":"2026-01-26T21:53:23.063Z"},{"id":"q-7909","question":"In a single AWS region, a small app runs on EC2 behind an ALB with an RDS MySQL database. Design a beginner-friendly backup and recovery guardrail: (1) automate daily RDS snapshots with 7-day retention, (2) implement a cross-region restore drill in a separate test VPC, (3) add CloudWatch alarms for snapshot success/failure and a Budgets alert for RDS costs, (4) outline a practical testing plan and rollback criteria?","answer":"Configure RDS automated backups with 7-day retention and enable a weekly manual snapshot. Implement a Lambda restore workflow to recreate the latest snapshot into a staging RDS instance in a separate test VPC, set up CloudWatch alarms for snapshot success/failure and AWS Budgets for RDS cost monitoring, and establish a testing plan with clear rollback criteria.","explanation":"## Why This Is Asked\nTests knowledge of basic RDS backups, cross-region drills, and cost controls in a practical disaster recovery scenario.\n\n## Key Concepts\n- RDS automated backups, retention policies, and manual snapshots\n- Cross-region restore procedures in a staging environment\n- CloudWatch alarms and AWS Budgets for operational guardrails\n- Testing methodologies and rollback criteria\n\n## Code Example\n```javascript\n// AWS CLI: create a daily snapshot (example placeholder)\nconst cmd = `aws rds create-db-snapshot --db-instance-identifier mydb --db-snapshot-identifier mydb-snap-${Date.now()}`\n```\n\n## Follow-up Questions\n- How would you adjust retention periods based on compliance requirements?\n- What additional monitoring would you implement for production readiness?","diagram":null,"difficulty":"beginner","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Robinhood","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T04:00:13.703Z","createdAt":"2026-01-26T22:42:09.783Z"},{"id":"q-982","question":"Design an automated cross-region disaster recovery plan for a globally distributed web app currently active in us-east-1 with a DR site in us-west-2, covering RDS, DynamoDB, S3, and ALB-backed frontend. Specify data synchronization, failover steps, testing, and rollback?","answer":"Cross-region DR plan with a warm DR in us-west-2. DynamoDB Global Tables for multi-region writes; RDS cross-region read replicas with automatic failover; S3 Cross-Region Replication for objects; Route","explanation":"## Why This Is Asked\nTests practical DR thinking across AWS services, including data consistency, failover automation, and testing.\n\n## Key Concepts\n- Cross-region DR strategies and RTO/RPO\n- Data synchronization for RDS, DynamoDB, and S3\n- Automation with Step Functions/SSM for failover\n\n## Code Example\n```yaml\nAWSTemplateFormatVersion: '2010-09-09'\nResources:\n  DRStateMachine:\n    Type: AWS::StepFunctions::StateMachine\n    Properties:\n      DefinitionString: |\n        {\n          \"Comment\": \"DR failover\",\n          \"StartAt\": \"PromoteDRRDS\",\n          \"States\": {\n            \"PromoteDRRDS\": {\"Type\": \"Task\", \"Resource\": \"arn:aws:states:::rds:promoteReadReplica\"}\n          }\n        }\n```\n\n## Follow-up Questions\n- How would you test RPO guarantees for DynamoDB Global Tables?\n- What are potential pitfalls with Route 53 failover in multi-Region deployments?","diagram":"flowchart TD\nA[Primary Region us-east-1] --> B{Healthy?}\nB -- Yes --> C[Normal traffic]\nB -- No --> D[Failover to us-west-2]\nD --> E[Promote DR RDS; re-point ALB; update DynamoDB endpoints]\nE --> F[Run DR tests and verify]\nF --> G[Record results and rollback if needed]","difficulty":"advanced","tags":["aws-sysops"],"channel":"aws-sysops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T17:45:18.021Z","createdAt":"2026-01-12T17:45:18.021Z"}],"subChannels":["general"],"companies":["Adobe","Airbnb","Amazon","Anthropic","Apple","Bloomberg","Citadel","Cloudflare","Coinbase","Databricks","Discord","DoorDash","Goldman Sachs","Google","Hashicorp","Hugging Face","IBM","Instacart","LinkedIn","Lyft","Meta","Microsoft","MongoDB","NVIDIA","Netflix","OpenAI","Oracle","PayPal","Plaid","Robinhood","Salesforce","Scale Ai","Snap","Snowflake","Square","Stripe","Tesla","Twitter","Two Sigma","Uber","Zoom"],"stats":{"total":87,"beginner":27,"intermediate":28,"advanced":32,"newThisWeek":37}}