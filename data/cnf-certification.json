{"questions":[{"id":"q-1019","question":"You're operating a CNF-based API gateway cluster that terminates TLS for thousands of tenants across 3 regions. A mandate requires migrating all TLS to post-quantum algorithms with per-tenant keys sourced from an HSM, while delivering zero-downtime upgrades, per-tenant key isolation, and a rollback plan. Outline an end-to-end rollout including (a) inventory and compatibility checks, (b) PQC algorithm and certificate strategy, (c) HSM PKCS#11 integration and key rotation, (d) canary/traffic-mirror rollout and drift detection, (e) observability and rollback criteria?","answer":"Audit TLS usage per tenant; adopt a hybrid post-quantum TLS approach (Kyber/Dilithium) for session keys and per-tenant certificates; load per-tenant keys from an HSM via PKCS#11 with rotation primitiv","explanation":"## Why This Is Asked\nThis question probes practical expertise in upgrading cryptography without downtime, with tenancy isolation and HSM-backed keys. It tests rollout discipline, drift handling, and observability under real-world regulatory pressure.\n\n## Key Concepts\n- Post-quantum cryptography deployment in live CNFs\n- Per-tenant key material management with HSM (PKCS#11)\n- Zero-downtime, region-wise canary rollouts and drift detection\n- Observability: handshake metrics, cipher adoption, rollback criteria\n\n## Code Example\n```javascript\n// Pseudo-code for config wiring\nconst tlsConfig = {\n  pqc: {\n    enabled: true,\n    suites: ['Kyber512', 'Dilithium3']\n  },\n  hsm: {\n    pkcs11Module: '/usr/lib/softhsm/libsofthsm2.so',\n    slot: 1,\n    pin: 'REDACTED'\n  }\n}\n```\n\n## Follow-up Questions\n- How would you verify compatibility with legacy TLS clients during the migration?\n- How would you measure and bound the impact on tenants during the rollout?","diagram":"flowchart TD\n  A[Inventory TLS usage] --> B[Choose PQC suites]\n  B --> C[HSM PKCS11 integration]\n  C --> D[Canary rollout with traffic mirroring]\n  D --> E[Observability metrics]\n  E --> F[Rollback criteria]","difficulty":"advanced","tags":["cnf-certification"],"channel":"cnf-certification","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","LinkedIn","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T19:32:58.382Z","createdAt":"2026-01-12T19:32:58.382Z"},{"id":"q-1040","question":"You're deploying a CNF-based UDP gateway across four data centers. A CVE requires hardware-backed attestation before image execution. Outline a concrete end-to-end rollout plan that (a) signs CNF images with Cosign and SBOMs, (b) enforces attestation via TPM 2.0-based attestation bundles and ImagePolicyWebhook, (c) rolls out region-by-region with traffic mirroring and per-tenant quotas, (d) implements drift detection and automated rollback on attestation failure, and (e) provides observability for attestation metrics and rollback triggers?","answer":"Sign CNF images with Cosign and SBOMs, publish to Rekor. Enforce using TPM 2.0 attestation bundles and an ImagePolicyWebhook policy so only attested images execute. Roll out region-by-region with traf","explanation":"## Why This Is Asked\nTests hardware-backed attestation, signing, and policy enforcement in CNF rollouts, plus region-by-region rollout, drift detection, and observability.\n\n## Key Concepts\n- Cosign and SBOMs with Rekor\n- TPM 2.0 attestation\n- Sigstore attestation bundles\n- ImagePolicyWebhook enforcement\n- Traffic mirroring and per-tenant quotas\n- Drift detection and automated rollback\n- Observability: attestation latency, success rate, per-tenant metrics\n\n## Code Example\n```yaml\napiVersion: policy.k8s.io/v1beta1\nkind: ImageReview\nspec:\n  image: <attested-image>\n  attestation: true\n```\n\n## Follow-up Questions\n- How would you test rollback behavior under attestation failure? \n- What metrics would you surface to detect persistent drift?\n","diagram":"flowchart TD\n  A[CNF Image] --> B[Cosign SBOM Sign]\n  B --> C[Publish to Rekor]\n  C --> D[TPM 2.0 Attestation Bundle]\n  D --> E[ImagePolicyWebhook Gate]\n  E --> F[Region 1 Canary with Traffic Mirroring]\n  F --> G[Drift Reconciliation]\n  G --> H[Observability & Rollback Triggers]","difficulty":"intermediate","tags":["cnf-certification"],"channel":"cnf-certification","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T20:28:03.844Z","createdAt":"2026-01-12T20:28:03.844Z"},{"id":"q-1071","question":"You're deploying a CNF-based API gateway across 2 Kubernetes clusters. A recent upgrade causes a cross-tenant data bleed under load due to a shared in-memory cache. Outline a beginner-friendly, end-to-end plan to fix and roll out safely: (a) reproduce in staging with two tenants and isolated traffic, (b) implement tenant-scoped cache keys and per-tenant isolation checks, (c) add unit/integration tests for isolation, (d) perform blue/green canary rollout with tenant-based traffic splits, (e) observability: per-tenant SLA metrics and automatic rollback if bleed is detected. Include a minimal code snippet for tenant-scoped cache key?","answer":"Reproduce with two tenants in separate namespaces, enable per-tenant cache keys (tenant_id + resource_id), add a small unit test asserting no cross-tenant data is returned, roll out a blue/green canar","explanation":"## Why This Is Asked\nThis question probes practical skills in tenant isolation, cache safety, and controlled rollouts, plus test design and observability.\n\n## Key Concepts\n- Multi-tenant isolation\n- Tenant-scoped caching\n- Unit/integration tests for isolation\n- Blue/green canary rollouts with traffic splitting\n- Observability and automatic rollback\n\n## Code Example\n```javascript\nfunction cacheKey(tenantId, resourceId) {\n  return `${tenantId}:${resourceId}`;\n}\n```\n\n## Follow-up Questions\n- How would you monitor cross-tenant bleed in production?\n- How would you adapt the canary ratio if bleed indicators appear?","diagram":"flowchart TD\n  A[Start] --> B[Reproduce in staging]\n  B --> C[Implement tenant cache keys]\n  C --> D[Add tests]\n  D --> E[Blue/Green rollout]\n  E --> F[Observe -> Rollback if needed]\n","difficulty":"beginner","tags":["cnf-certification"],"channel":"cnf-certification","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Stripe","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T21:31:42.231Z","createdAt":"2026-01-12T21:31:42.231Z"},{"id":"q-1081","question":"You're deploying a CNF-based API gateway that uses workload identities for tenants. Describe an end-to-end plan to enforce identity attestation and per-tenant isolation using SPIRE for SPIFFE IDs, OPA Gatekeeper for policy decisions, and a local Kind testbed. Include (a) issuing and rotating SVIDs, (b) encoding tenant permissions in policies, (c) testing isolation with two tenants, and (d) observability hooks for attestation and policy evaluations?","answer":"SPIRE issues per-tenant SVIDs with 24h rotation; gateway validates SVIDs; policy via OPA Gatekeeper requires SPIFFE ID prefix spiffe://example.org/tenant-<tenant> and tenant-scoped access; test in Kin","explanation":"## Why This Is Asked\nTests knowledge of workload identity and policy enforcement in CNF deployments.\n\n## Key Concepts\n- SPIRE/SPIFFE identities for CNFs\n- OPA Gatekeeper policy enforcement\n- Per-tenant network isolation\n- Observability hooks (attestation metrics, policy latency)\n\n## Code Example\n```rego\npackage cnf.auth\n\ndefault allow = false\n\nallow {\n  input.spiffe_id.startsWith(\"spiffe://example.org/tenant-\")\n  input.tenant == data.tenants[input.spiffe_id].name\n}\n```\n\n## Follow-up Questions\n- How would you rotate SVIDs with zero downtime?\n- How would you validate policy drift and trigger rollback?","diagram":"flowchart TD\n  A[Start] --> B[SPIRE setup]\n  B --> C[OPA policy]\n  C --> D[Kind tests]\n  D --> E[Observability]","difficulty":"beginner","tags":["cnf-certification"],"channel":"cnf-certification","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T22:17:18.359Z","createdAt":"2026-01-12T22:17:18.359Z"},{"id":"q-1107","question":"You're deploying a CNF-based API gateway serving two tenants with strict data-retention and per-tenant log-redaction requirements. Outline an end-to-end plan using SPIRE for workload identities and OPA Gatekeeper for policy decisions to enforce data handling rules while enabling zero-downtime upgrades in a local Kind testbed. Include (a) SVID issuance/rotation tied to tenant IDs, (b) tenant-scoped policies for retention windows and redaction, (c) runtime redaction checks on logs/traces, (d) cross-tenant isolation testing under load, and (e) observability hooks for attestation, policy decisions, and redaction misses?","answer":"Propose a two-tenant end-to-end CNF gateway rollout enforcing per-tenant data-retention and log-redaction policies via SPIRE for workload identities and OPA Gatekeeper for policy decisions. Include SV","explanation":"## Why This Is Asked\n\nThis question tests practical integration of workload identity with policy enforcement in a CNF gateway, emphasizing data governance, per-tenant isolation, and non-disruptive upgrades. It requires concrete steps for credential rotation, policy encoding, runtime checks, and observability.\n\n## Key Concepts\n\n- SPIRE SVIDs and workload attestation for multitenant isolation\n- OPA Gatekeeper policy modeling per tenant (retention windows, redaction rules)\n- Runtime log redaction and per-tenant observability\n- Kind-based local testbed for end-to-end verification\n- Zero-downtime rollout with traffic shaping and drift checks\n\n## Code Example\n\n```javascript\nfunction redactLog(entry, policy) {\n  const redacted = { ...entry };\n  (policy.redactFields || []).forEach(field => {\n    if (redacted.hasOwnProperty(field)) redacted[field] = \"***REDACTED***\";\n  });\n  return redacted;\n}\n```\n\n## Follow-up Questions\n\n- How would you rotate SVIDs across tenants with revocation semantics in SPIRE?\n- How would you validate redaction coverage across logs in CI/CD and production?","diagram":null,"difficulty":"intermediate","tags":["cnf-certification"],"channel":"cnf-certification","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Meta","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T23:16:16.754Z","createdAt":"2026-01-12T23:16:16.754Z"},{"id":"q-1187","question":"A beginner CNF certification scenario: you implement a gated CI/CD pipeline for a CNF gateway image. Outline an end-to-end workflow to ensure image provenance before deployment: (a) sign CNF images with Cosign using a KMS-backed key, (b) generate and publish SBOMs, (c) enforce signatures via ImagePolicyWebhook, and (d) surface observability in the monitoring stack for signing success/failure and rollback signals. Provide concrete steps and minimal config snippets?","answer":"GitHub Actions CI: build CNF image, sign with Cosign using a KMS-backed key, generate and publish SBOMs, update ImagePolicyWebhook to require signed images, gate deployment on attestation success, and","explanation":"## Why This Is Asked\nTests practical understanding of image provenance gates in CNF pipelines, focusing on Cosign, SBOM, ImagePolicyWebhook, and basic observability.\n\n## Key Concepts\n- Cosign signing with a KMS-backed key\n- SBOM generation and publishing\n- ImagePolicyWebhook enforcement\n- Observability signals and rollback triggers\n- CI/CD gating (e.g., GitHub Actions)\n\n## Code Example\n```javascript\n// CI pseudo steps (pseudocode)\nconst signCmd = \"cosign sign --key kms://my/key ghcr.io/org/cnf-image:${process.env.GITHUB_SHA}\";\n```\n\n## Follow-up Questions\n- How would you test the gate without a real deployment?\n- How would you handle Cosign key rotation with zero downtime?","diagram":"flowchart TD\n  CIBuild[CI Build] --> Sign[Cosign Sign]\n  Sign --> SBOM[SBOM Gen & Publish]\n  SBOM --> Policy[ImagePolicyWebhook]\n  Policy --> Deploy[Deploy to Registry]\n  Deploy --> Observability[Observability Signals]\n","difficulty":"beginner","tags":["cnf-certification"],"channel":"cnf-certification","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","LinkedIn","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T04:39:07.086Z","createdAt":"2026-01-13T04:39:07.086Z"},{"id":"q-1221","question":"You're operating a CNF-based API gateway deployed across three regions behind a service mesh. Propose a practical upgrade workflow that enforces runtime integrity along with image attestations: (a) sign images with Cosign using a KMS-backed key and publish SBOMs, (b) require TPM/measured-boot attestation plus runtime integrity checks for CNFs, (c) roll out region-by-region with per-tenant canaries and live connection migration, (d) implement drift detection and automatic rollback on attestation/runtime mismatch, (e) surface observability for sign-off, attestation, and rollback triggers. Provide minimal config references?","answer":"Cosign-sign CNF images with a KMS-backed key and publish SBOMs; gate deploys via ImagePolicyWebhook. Enforce TPM/measured-boot attestation plus runtime integrity checks for CNFs. Roll out region-by-re","explanation":"## Why This Is Asked\nTests a concrete upgrade workflow combining image attestation, hardware-backed integrity, and multi-region rollout with per-tenant considerations.\n\n## Key Concepts\n- Image attestation with Cosign and SBOMs\n- TPM/measured boot and runtime integrity checks\n- ImagePolicyWebhook enforcement\n- Canary-based region rollout with live migration\n- Drift detection and automatic rollback\n- Observability for signatures, attestations, rollbacks\n\n## Code Example\n```javascript\n// Minimal policy concept (illustrative)\nconst policy = { requireAttestation: true, requireSBOM: true, tenantIsolation: true }\n```\n\n## Follow-up Questions\n- How would you validate rollback triggers under peak load?\n- What metrics validate attestation health in dashboards?","diagram":"flowchart TD\n  A[Sign image with Cosign] --> B[Publish SBOM]\n  B --> C[ImagePolicyWebhook]\n  C --> D[TPM/measured boot attestation]\n  D --> E[Runtime integrity checks]\n  E --> F[Region-by-region rollout]\n  F --> G[Drift detection]\n  G --> H[Auto rollback]","difficulty":"advanced","tags":["cnf-certification"],"channel":"cnf-certification","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Netflix","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T05:33:18.061Z","createdAt":"2026-01-13T05:33:18.061Z"},{"id":"q-1347","question":"Advanced CNF certification scenario: deploy a CNF gateway across three air‑gapped data centers. Design an end‑to‑end plan to ensure image provenance and runtime integrity for updates, covering (a) offline Cosign signing with a TPM‑backed key, (b) SBOM generation and private catalog, (c) per‑tenant policy enforcement via OPA/Gatekeeper, (d) edge‑site offline attestation broker, (e) drift detection with automated rollback, and (f) observability hooks. Include concrete config examples?","answer":"Use an offline attestation flow: sign CNF images with Cosign using a TPM-backed key; publish SBOMs to a private catalog; enforce attestation via ImagePolicyWebhook with per-tenant OPA policies; deploy","explanation":"## Why This Is Asked\n\nThis question probes offline attestation, edge CNF deployment, and policy-driven updates in air‑gapped environments, ensuring provenance, integrity, and rollback safety.\n\n## Key Concepts\n\n- Offline attestation and TPM keys\n- SBOM generation and private catalogs\n- ImagePolicyWebhook + OPA/Gatekeeper policies\n- Edge/offline attestation brokers\n- Drift detection and automated rollback\n- Observability/telemetry for attestation and rollbacks\n\n## Code Example\n\n```javascript\n// Pseudo: Cosign signing with TPM key and SBOM publication\ncosign sign --key tpm-key.pem gcr.io/org/cnf-gateway:latest\ncosign generate sbom -o sbom.json gcr.io/org/cnf-gateway:latest\n```\n\n## Follow-up Questions\n\n- How would you handle clock skew between air‑gapped sites and the signing CA?\n- What metrics would you surface to detect attestation drift at scale?","diagram":"flowchart TD\n  A[CNF image] --> B[Cosign signing (TPM)]\n  B --> C[SBOM published]\n  C --> D[ImagePolicyWebhook]\n  D --> E[GitOps rollout]\n  E --> F[Attestation broker (edge)]\n  F --> G[Drift detection]\n  G --> H[Auto rollback]\n  H --> I[Observability]","difficulty":"advanced","tags":["cnf-certification"],"channel":"cnf-certification","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Citadel","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T13:05:53.114Z","createdAt":"2026-01-13T13:05:53.115Z"},{"id":"q-847","question":"You're deploying a CNF gateway (e.g., NGINX CNF) on a 50-node Kubernetes cluster handling streaming traffic. A node eviction hits during peak load. Outline a concrete plan to maintain streaming availability, focusing on (1) graceful drain with preStop, (2) health checks/readiness/liveness, and (3) traffic affinity and pod topology, and (4) observability and rollout strategy?","answer":"Use a Deployment of 4 replicas with PodDisruptionBudget minAvailable=3 and RollingUpdate maxUnavailable=25%. Implement a preStop hook to run nginx -s quit and wait 30s for active requests. Readiness o","explanation":"## Why This Is Asked\n\nTests practical CNF operations under eviction, not just theory.\n\n## Key Concepts\n\n- PodDisruptionBudget\n- Graceful draining\n- Readiness/Liveness probes\n- Traffic affinity\n\n## Code Example\n\n```javascript\n// Drain hook example\n```\n\n## Follow-up Questions\n\n- How would you adapt this for stateful CNFs?\n- How would you observe and rollback failed rollouts?","diagram":"flowchart TD\n  A[Node Eviction] --> B[Drain Initiated]\n  B --> C[Pod Stop/Drain]\n  C --> D[Traffic Shifts]\n  D --> E[New Pod Ready]","difficulty":"intermediate","tags":["cnf-certification"],"channel":"cnf-certification","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Netflix","Plaid","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:29:51.594Z","createdAt":"2026-01-12T13:29:51.594Z"},{"id":"q-918","question":"You're rolling out a CNF-based NAT gateway across a 3-region multi-cluster Kubernetes setup. A policy change must be applied without disrupting live traffic. Outline a concrete, end-to-end rollout plan emphasizing (a) shadow canaries with traffic mirroring, (b) region-by-region rollout with per-region SLI targets, (c) policy-state reconciliation and drift detection, (d) rollback conditions and observability instrumentation?","answer":"Plan a canary rollout of CNF NAT across 3 regions. Validate in a shadow region with traffic mirroring; rollout region-by-region with explicit latency, error-rate, and drop targets; run a continuous po","explanation":"## Why This Is Asked\nTests practical rollout discipline for CNFs, multi-region consistency, and robust rollback.\n\n## Key Concepts\n- Canary rollouts in CNFs across regions\n- Policy drift detection and reconciliation\n- Observability with eBPF, traces, and metrics\n- Safe rollback criteria and traffic mirroring\n- Per-region SLI targets and controlled rollout\n\n## Code Example\n```javascript\n// Pseudocode: reconcilePolicy(desired, runtime, driftThreshold) \nasync function reconcilePolicy(desired, runtime){\n  const drift = diff(desired, runtime);\n  if(drift > driftThreshold) throw new Error('Policy drift');\n  // apply reconciliation\n}\n```\n\n## Follow-up Questions\n- How would you define drift metrics and thresholds for CNFs?\n- How would you test rollback reliability under sudden latency spikes?","diagram":"flowchart TD\n  A[Policy Change Initiated] --> B{Canary Shadow?}\n  B -- Yes --> C[Shadow Region Mirroring]\n  B -- No --> D[Region-by-Region Rollout]\n  C --> E[Monitor Shadows]\n  E --> F{Drift Detected?}\n  F -- Yes --> G[Rollback and Alert]\n  F -- No --> H[Proceed Rollout]\n  D --> H\n  H --> I[Centralize Telemetry]","difficulty":"intermediate","tags":["cnf-certification"],"channel":"cnf-certification","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Coinbase"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T15:30:48.188Z","createdAt":"2026-01-12T15:30:48.188Z"},{"id":"q-935","question":"You manage CNF-based gateways across 4 regions. A suspected supply-chain compromise requires enforcing in-cluster image attestations before rollout without downtime. Outline an end-to-end plan to (a) sign CNF images with Cosign and SBOMs, (b) enforce signatures via ImagePolicyWebhook, (c) roll out region-by-region with traffic mirroring, (d) implement drift detection and automatic rollback on attestation failure. Include observability?","answer":"Sign CNF images with Cosign and SBOMs, publish to Rekor; enforce with ImagePolicyWebhook requiring valid signature and SBOM; perform region-by-region canaries with traffic mirroring; implement drift c","explanation":"Why This Is Asked\n\nTests ability to design secure, zero-downtime CNF rollouts with supply chain attestations.\n\nKey Concepts\n\n- Image signing (Cosign), SBOMs, Rekor\n- Kubernetes policy enforcement (ImagePolicyWebhook)\n- Canary rollout, traffic mirroring, drift detection\n- Rollback criteria and observability\n\nCode Example\n\n```javascript\n// Pseudocode: drift detection between desired and deployed SBOMs\nfunction isDrifted(deployedSBOM, desiredSBOM) {\n  return deployedSBOM.hash !== desiredSBOM.hash;\n}\n```\n\nFollow-up Questions\n\n- How would you revoke a compromised image in Rekor without blocking new deployments?\n- How do you keep SBOMs up-to-date across regions during rapid changes?","diagram":"flowchart TD\n  A[Sign Image] --> B[Publish to Rekor]\n  B --> C[ImagePolicyWebhook]\n  C --> D[Canary Rollout]\n  D --> E[Drift Check]\n  E --> F[Rollback if needed]","difficulty":"intermediate","tags":["cnf-certification"],"channel":"cnf-certification","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","Tesla","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T16:25:14.304Z","createdAt":"2026-01-12T16:25:14.304Z"},{"id":"q-985","question":"Design a zero-downtime, CNF-based API gateway rollout where per-tenant routing rules update live without dropping connections. Outline end-to-end steps: (a) safe rule distribution, (b) canary-ingress slicing with weighted traffic, (c) drift detection between desired and active routes, (d) observability and rollback?","answer":"Zero-downtime CNF gateway update for per-tenant routes: use a pull-based config bus for safe rule distribution, run canary shards with weighted traffic, and progressively shift tenants. Implement drif","explanation":"## Why This Is Asked\nTests mastery of live CNF config delivery under scale, using dynamic route management, traffic shaping, and robust rollback. It probes understanding of drift detection, observability integration, and safe rollout in multi-tenant environments.\n\n## Key Concepts\n- Zero-downtime rolling updates with canary deployments\n- Dynamic configuration distribution and consistency\n- Drift detection and automated rollback with observability\n\n## Code Example\n```javascript\n// Pseudo-implementation snippet to illustrate traffic weight update\nasync function rollWithCanary(nextWeight) {\n  await distributeConfig({ tenants: 'all', weight: nextWeight });\n  await monitorErrors(60_000);\n  if (riskTooHigh()) rollback();\n}\n```\n\n## Follow-up Questions\n- How would you detect hidden slippage in latency during canary?\n- How would you handle tenants with non-uniform traffic patterns during ramp?","diagram":"flowchart TD\n  A[Config Bus] --> B[Rule Distribution]\n  B --> C[Canary Shard]\n  C --> D[Metrics/Observability]\n  B --> E[Active Routes]\n  D --> F[Rollback on Regression]","difficulty":"advanced","tags":["cnf-certification"],"channel":"cnf-certification","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Meta","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T18:31:16.146Z","createdAt":"2026-01-12T18:31:16.146Z"}],"subChannels":["general"],"companies":["Adobe","Apple","Citadel","Coinbase","Discord","Goldman Sachs","Google","IBM","Instacart","LinkedIn","Lyft","Meta","Netflix","OpenAI","Oracle","PayPal","Plaid","Stripe","Tesla","Twitter","Two Sigma"],"stats":{"total":12,"beginner":3,"intermediate":5,"advanced":4,"newThisWeek":12}}