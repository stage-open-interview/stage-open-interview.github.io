{"questions":[{"id":"gcp-cloud-engineer-access-security-1768195883281-0","question":"A web app running on Google Kubernetes Engine (GKE) in your GCP project needs to read objects from a Cloud Storage bucket and publish messages to a Pub/Sub topic. To follow the principle of least privilege and avoid long‑lived credentials, which approach should you implement?","answer":"[{\"id\":\"a\",\"text\":\"Bind the project’s default service account to the bucket and the Pub/Sub topic at the project level, granting broad access.\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Create a dedicated service account for the app and grant it only storage.objectViewer on the bucket and pubsub.publisher on the topic, with IAM bindings at the resource level; consider using Workload Identity Federation if the app runs on Kubernetes.\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Store a service account key for the app in Cloud Storage and rotate it monthly.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use the allUsers principal to allow anyone to read objects from the bucket.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct because it applies the principle of least privilege by granting the app only the permissions it needs on the specific resources, and not at the project level. Using a dedicated service account prevents over-broad access and avoids embedding long‑lived credentials; when the app runs on Kubernetes, Workload Identity Federation further eliminates the need to manage short‑lived keys.\n\n## Why Other Options Are Wrong\n- Option A grants broad access at the project level, increasing blast radius and violating least privilege.\n- Option C relies on long‑lived credentials (a key), which is discouraged; rotating keys does not mitigate the risk of exposure.\n- Option D makes the bucket public to all users, which is insecure and inappropriate for private data.\n\n## Key Concepts\n- Least privilege principle\n- Service accounts and resource-level IAM bindings\n- Workload Identity Federation (WIF)\n- Avoiding long‑lived credentials\n\n## Real-World Application\nThis pattern is commonly used when deploying apps to GKE that need selective access to Cloud Storage and Pub/Sub, reducing risk while maintaining operational practicality.\n","diagram":null,"difficulty":"intermediate","tags":["GCP","IAM","Kubernetes","Workload Identity Federation","Cloud Storage","Pub/Sub","AWS","Terraform","certification-mcq","domain-weight-22"],"channel":"gcp-cloud-engineer","subChannel":"access-security","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T05:31:23.282Z","createdAt":"2026-01-12 05:31:23"},{"id":"gcp-cloud-engineer-access-security-1768195883281-1","question":"Your organization wants to ensure that data egress from your Google Cloud environment to external networks is restricted and that access to Google APIs is allowed only within a defined security perimeter. Which construct best enforces this boundary?","answer":"[{\"id\":\"a\",\"text\":\"Configure a firewall rule to block all egress traffic from resources except to Google API endpoints.\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Create a VPC Service Controls perimeter and use Access Context Manager to define trusted sources and restrict Google API access to within the perimeter.\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Use Cloud Identity-Aware Proxy to restrict access to the workload.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use Cloud NAT to control outbound internet access.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct because VPC Service Controls perimeters, together with Access Context Manager, define a security boundary that limits data access and API calls to resources inside the perimeter, reducing data exfiltration risk.\n\n## Why Other Options Are Wrong\n- Option A is insufficient in practice and hard to maintain for all Google API endpoints; perimeters provide stronger, centralized control.\n- Option C focuses on user access to workloads rather than restricting perimeters for API access and data exfiltration.\n- Option D controls outbound NAT behavior but does not enforce API access boundaries or data exfiltration controls across services.\n\n## Key Concepts\n- VPC Service Controls perimeters\n- Access Context Manager\n- Data exfiltration risk mitigation\n\n## Real-World Application\nThis approach is standard for regulated environments (e.g., financial, healthcare) to prevent data leakage when workloads access Google APIs or store data in Google services.\n","diagram":null,"difficulty":"intermediate","tags":["GCP","VPC Service Controls","Access Context Manager","Data Security","GKE","AWS","Terraform","certification-mcq","domain-weight-22"],"channel":"gcp-cloud-engineer","subChannel":"access-security","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T05:31:23.647Z","createdAt":"2026-01-12 05:31:23"},{"id":"gcp-cloud-engineer-access-security-1768195883281-2","question":"An organization uses an AWS-hosted CI/CD pipeline that must deploy to GCP resources. To avoid managing long‑lived credentials, which option provides temporary credentials for the pipeline to access GCP resources when running on AWS?","answer":"[{\"id\":\"a\",\"text\":\"Create a Google Cloud service account and store its key in AWS Secrets Manager; use the key directly from Jenkins.\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Configure Workload Identity Federation with an AWS OpenID Connect provider; grant the associated Google service account minimal permissions; Jenkins obtains short‑lived credentials via STS.\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Create a Google user account and share login credentials with the AWS team.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use a Google Cloud API key for the pipeline.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct because Workload Identity Federation allows the external AWS identity to obtain short‑lived credentials from GCP without creating or storing long‑lived keys, enabling secure cross‑cloud access with least privilege.\n\n## Why Other Options Are Wrong\n- Option A relies on long‑lived keys stored in AWS Secrets Manager, which increases risk if secrets are compromised.\n- Option C uses a human user account credentials, which is inappropriate for automated pipelines and violates best practices.\n- Option D API keys are not suitable for server‑to‑server interactions requiring identity and fine‑grained IAM controls.\n\n## Key Concepts\n- Workload Identity Federation (WIF)\n- OIDC and external identities\n- Short‑lived credentials\n- Least privilege cross‑cloud access\n\n## Real-World Application\nThis pattern is widely used to safely connect CI/CD pipelines across cloud providers without managing long‑lived credentials, improving security posture for multi‑cloud deployments.\n","diagram":null,"difficulty":"intermediate","tags":["GCP","Workload Identity Federation","AWS","OIDC","STS","IAM","Terraform","certification-mcq","domain-weight-22"],"channel":"gcp-cloud-engineer","subChannel":"access-security","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T05:31:24.014Z","createdAt":"2026-01-12 05:31:24"},{"id":"gcp-cloud-engineer-deploying-implementing-1768156084677-0","question":"You manage a web application that currently runs on a single Compute Engine VM. You expect traffic spikes and want autoscaling, rolling updates with zero downtime, and minimal operational overhead. Which approach best meets these requirements?","answer":"[{\"id\":\"a\",\"text\":\"Use a managed instance group with an HTTP(S) load balancer and a rolling update configuration.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Migrate to App Engine Standard and deploy the app as a single service version.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Containerize the app and run it on Cloud Run with a traffic split across revisions.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Keep using the single VM with manual scaling and manual deployments.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA\n\nUsing a managed instance group with an HTTP(S) load balancer and rolling update configuration provides autoscaling, rolling deployments with minimal downtime, and low operational overhead for VM-based workloads.\n\n## Why Other Options Are Wrong\n- B: Migrating to App Engine Standard could offer autoscaling and zero-downtime deployments, but it implies refactoring a VM-based workload and may not guarantee zero-downtime for the existing setup without significant changes.\n- C: Cloud Run requires containerization; while it supports traffic splitting, it adds containerization overhead and may not be ideal for a monolithic VM-based app without refactoring.\n- D: A single VM with manual scaling offers no autoscaling and higher risk of downtime during deployments.\n\n## Key Concepts\n- Managed Instance Groups (MIG)\n- HTTP(S) Load Balancer\n- Rolling updates\n\n## Real-World Application\nIn production, many teams consolidate to MIG + HTTP(S) Load Balancer to achieve predictable autoscaling and safe, zero-downtime deployments for VM-based workloads.\n","diagram":null,"difficulty":"intermediate","tags":["GCP","Compute Engine","Cloud Load Balancing","Kubernetes","Terraform","AWS EC2","EKS","certification-mcq","domain-weight-22"],"channel":"gcp-cloud-engineer","subChannel":"deploying-implementing","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T18:28:04.678Z","createdAt":"2026-01-11 18:28:04"},{"id":"gcp-cloud-engineer-deploying-implementing-1768156084677-1","question":"Your organization has two GCP projects: dev and prod. You want centralized firewall policies and shared subnets across both projects, while maintaining isolation of resources. Which networking pattern should you choose?","answer":"[{\"id\":\"a\",\"text\":\"Shared VPC where a host project contains the shared subnets and firewall rules, with service projects attaching to it.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"VPC peering between dev and prod projects.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Two independent VPCs with identical firewall rules replicated manually.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Project-level firewall rules with no network sharing.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA\n\nShared VPC allows central management of subnets and firewall policies in a host project while each environment (service project) maintains resource isolation and security boundaries.\n\n## Why Other Options Are Wrong\n- B: VPC peering connects networks but does not centralize firewall rules or shared subnets, making policy management harder across projects.\n- C: Manually replicating rules increases drift risk and defeats centralization and consistency.\n- D: Project-level firewall rules alone do not enable shared subnets or centralized policy management across projects.\n\n## Key Concepts\n- Shared VPC\n- Host project vs. service projects\n- Centralized firewall rules\n\n## Real-World Application\nOrganizations use Shared VPC to enforce a common security posture across multiple environments (dev/prod) while preserving project isolation and resource boundaries.\n","diagram":null,"difficulty":"intermediate","tags":["GCP","VPC","Shared VPC","Kubernetes","Terraform","AWS EC2","EKS","certification-mcq","domain-weight-22"],"channel":"gcp-cloud-engineer","subChannel":"deploying-implementing","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T18:28:05.057Z","createdAt":"2026-01-11 18:28:05"},{"id":"gcp-cloud-engineer-deploying-implementing-1768156084677-2","question":"You are deploying a multi-region web service with strict low-latency requirements. You need global load balancing with regional backends and automatic failover; which approach best achieves this?","answer":"[{\"id\":\"a\",\"text\":\"Configure Google Cloud Load Balancing with backend services spanning multiple regions and backends in Compute Engine or GKE.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Deploy a single regional load balancer and rely on DNS for latency optimization.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Place a Cloud CDN in front of a single regional endpoint.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Set up VPN tunnels to connect regional networks and route traffic manually.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA\n\nGlobal HTTP(S) Load Balancing with multi-region backends provides low-latency routing, automatic failover, and seamless autoscaling across regions for a worldwide user base.\n\n## Why Other Options Are Wrong\n- B: A single regional load balancer cannot optimally serve users across regions and lacks true global failover.\n- C: Cloud CDN improves cacheability and edge performance but does not provide regional compute backends or automatic failover across regions.\n- D: VPN tunnels are complex to manage for user traffic routing and do not provide built-in global load balancing or automatic failover.\n\n## Key Concepts\n- Global HTTP(S) Load Balancing\n- Multi-region backends\n- Regional Compute Engine / GKE integration\n\n## Real-World Application\nGlobal SaaS providers deploy backend services across multiple regions and front them with a global load balancer to minimize latency and ensure resilience during regional outages.\n","diagram":null,"difficulty":"intermediate","tags":["GCP","Cloud Load Balancing","GKE","Compute Engine","Terraform","AWS EC2","EKS","certification-mcq","domain-weight-22"],"channel":"gcp-cloud-engineer","subChannel":"deploying-implementing","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T18:28:05.434Z","createdAt":"2026-01-11 18:28:05"},{"id":"gcp-cloud-engineer-operations-1768224379071-0","question":"You are running a managed instance group on Compute Engine behind a regional HTTP(S) load balancer. Your traffic varies daily and you want to keep cost low while maintaining performance. Which autoscaling setup is the most appropriate?","answer":"[{\"id\":\"a\",\"text\":\"Enable autoscaler with targetUtilization 0.6, min 2, max 10\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Set fixed 5 instances\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Disable autoscaling and rely on manual scaling\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use per-region manual start/stop schedules\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nThe correct option is **A** because autoscaling with a target utilization balances cost and performance by scaling between the defined min and max instances as traffic changes.\n\n## Why Other Options Are Wrong\n\n- B: Fixed-size instances waste resources when traffic drops and can under-provision when demand spikes.\n- C: Disabling autoscaling prevents automatic adjustment to real-time load.\n- D: Manual schedules don't respond to real-time traffic patterns and complicate regional orchestration.\n\n## Key Concepts\n\n- Managed Instance Groups auto-scaling\n- Target utilization policies and min/max bounds\n- Regional HTTP(S) Load Balancing and health checks\n\n## Real-World Application\n\n- Example: A web app with unpredictable daily demand uses autoscaling to maintain latency targets while controlling costs; operators adjust min/max as traffic patterns evolve.","diagram":null,"difficulty":"intermediate","tags":["GCP","Kubernetes","Terraform","AWS-EC2","AWS-EKS","certification-mcq","domain-weight-21"],"channel":"gcp-cloud-engineer","subChannel":"operations","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:26:19.073Z","createdAt":"2026-01-12 13:26:19"},{"id":"gcp-cloud-engineer-operations-1768224379071-1","question":"For globally distributed, highly available relational data with strong consistency requirements, which GCP service is best?","answer":"[{\"id\":\"a\",\"text\":\"Cloud SQL with cross-region read replicas\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Cloud Spanner\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Firestore in Datastore mode\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"BigQuery\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nThe correct option is **B**. Cloud Spanner provides globally distributed, strongly consistent transactions and horizontal scaling for relational workloads, which is ideal for globally distributed systems.\n\n## Why Other Options Are Wrong\n\n- A: Cloud SQL supports read replicas but does not provide global distributed strong consistency or cross-region transactional guarantees.\n- C: Firestore in Datastore mode is non-relational and not suited for traditional SQL workloads.\n- D: BigQuery is an analytics data warehouse, not a transactional relational database.\n\n## Key Concepts\n\n- Cloud Spanner global distribution with external consistency\n- Horizontal scaling and SQL semantics\n- Strong consistency across regions\n\n## Real-World Application\n\n- Example: A global ecommerce platform requires ACID transactions across regions; Cloud Spanner enables writes in any region with consistent reads globally.","diagram":null,"difficulty":"intermediate","tags":["GCP","Kubernetes","Terraform","AWS-EKS","Cloud Spanner","Cloud SQL","certification-mcq","domain-weight-21"],"channel":"gcp-cloud-engineer","subChannel":"operations","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:26:19.902Z","createdAt":"2026-01-12 13:26:20"},{"id":"gcp-cloud-engineer-operations-1768224379071-2","question":"Which option represents best practice for securely storing and providing access to API secrets in a Google Cloud CI/CD pipeline?","answer":"[{\"id\":\"a\",\"text\":\"Store secrets directly in source code\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use Google Secret Manager with IAM-based access control\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Fetch secrets from the Compute Engine metadata server at runtime\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Store secrets in a Cloud Storage bucket with bucket-level encryption only\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nThe correct option is **B**. Google Secret Manager provides centralized secret storage with fine-grained IAM permissions and audit logging, enabling secure, auditable access for CI/CD pipelines.\n\n## Why Other Options Are Wrong\n\n- A: Embedding secrets in source code risks exposure through repo access and backups.\n- C: Metadata server is not a secure or scalable secret management mechanism for pipelines; it may expose credentials and complicate rotation.\n- D: Cloud Storage encryption at rest does not provide fine-grained access control or secret rotation guarantees for API keys.\n\n## Key Concepts\n\n- Google Secret Manager\n- IAM-based access control and audit logging\n- Secret rotation and least-privilege practices\n\n## Real-World Application\n\n- Example: A CI/CD pipeline pulls API keys from Secret Manager during build and deployment, with strict service accounts and rotation schedules.","diagram":null,"difficulty":"intermediate","tags":["GCP","Kubernetes","Terraform","AWS-EC2","Secret Manager","certification-mcq","domain-weight-21"],"channel":"gcp-cloud-engineer","subChannel":"operations","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:26:20.423Z","createdAt":"2026-01-12 13:26:20"},{"id":"gcp-cloud-engineer-operations-1768224379071-3","question":"You want to prevent data exfiltration while allowing required services to communicate with restricted public internet egress. Which architectural pattern best achieves this?","answer":"[{\"id\":\"a\",\"text\":\"Create restrictive firewall rules and rely on deny-all egress\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Implement VPC Service Controls perimeters with Private Service Connect\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Move all sensitive data to on-premises network\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Enable Cloud Armor on all traffic\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nThe correct option is **B**. VPC Service Controls perimeters, combined with Private Service Connect, prevent data exfiltration from GCP services while allowing controlled access to required Google APIs and services.\n\n## Why Other Options Are Wrong\n\n- A: Deny-all egress blocks legitimate service-to-service communication and is not flexible.\n- C: Moving data on-premises defeats cloud benefits and adds latency and maintenance cost.\n- D: Cloud Armor protects against malicious traffic at the edge but does not inherently prevent data exfiltration.\n\n## Key Concepts\n\n- VPC Service Controls perimeters\n- Private Service Connect for private service access\n- Data exfiltration prevention patterns\n\n## Real-World Application\n\n- Example: A regulated data workload uses perimeters to isolate data and ensures outbound calls only to approved Google APIs via private paths.","diagram":null,"difficulty":"intermediate","tags":["GCP","Kubernetes","Terraform","AWS-EC2","VPC Service Controls","certification-mcq","domain-weight-21"],"channel":"gcp-cloud-engineer","subChannel":"operations","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:26:20.604Z","createdAt":"2026-01-12 13:26:20"},{"id":"gcp-cloud-engineer-operations-1768224379071-4","question":"To deploy a microservice with low latency worldwide and automatic scaling, which approach is most suitable on Google Cloud?","answer":"[{\"id\":\"a\",\"text\":\"Deploy to Cloud Run (fully managed) in multiple regions and front with a global HTTP(S) load balancer\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Deploy to App Engine Standard in a single region\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Deploy to a single GKE cluster and expose via a regional load balancer\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use Cloud Functions in a single region with a regional endpoint\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nThe correct option is **A**. Cloud Run (fully managed) supports multi-region deployment and automatic scaling, and when fronted by a global HTTP(S) load balancer, users worldwide are served from the nearest region with low latency.\n\n## Why Other Options Are Wrong\n\n- B: App Engine Standard in a single region cannot provide true global latency optimization.\n- C: A single GKE cluster limits regional latency benefits and resilience.\n- D: Cloud Functions in a single region does not cover multi-region latency optimization and may complicate global routing.\n\n## Key Concepts\n\n- Cloud Run multi-region deployment\n- Global HTTP(S) Load Balancer\n- Serverless scalability and low-latency routing\n\n## Real-World Application\n\n- Example: A global API needs to serve users with sub-100ms latency; deployment across three regions with a global LB ensures fast responses and easy scaling.","diagram":null,"difficulty":"intermediate","tags":["GCP","Kubernetes","Terraform","AWS-EKS","Cloud Run","Global Load Balancing","certification-mcq","domain-weight-21"],"channel":"gcp-cloud-engineer","subChannel":"operations","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:26:20.786Z","createdAt":"2026-01-12 13:26:20"}],"subChannels":["access-security","deploying-implementing","operations"],"companies":[],"stats":{"total":11,"beginner":0,"intermediate":11,"advanced":0,"newThisWeek":11}}