{"questions":[{"id":"q-1016","question":"Design a regional streaming pipeline for a fintech app: on‑prem and GKE emit events to region Pub/Sub topics; a Dataflow streaming job enforces exactly-once, writes partitioned regional BigQuery tables, and triggers Vertex AI scoring in near real-time. How would you achieve low latency, data residency, schema evolution, and reliable failure recovery?","answer":"Design a regional streaming pipeline: on‑prem and GKE emit to region Pub/Sub topics; Dataflow streaming enforces exactly-once, writes partitioned regional BigQuery tables, and triggers Vertex AI scori","explanation":"## Why This Is Asked\nThis question probes cross-region streaming, data residency, and production-ready fault tolerance in a real-time fraud pipeline.\n\n## Key Concepts\n- Regional Pub/Sub topics and fanout\n- Dataflow streaming with exactly-once\n- BigQuery regional storage and partitioning\n- Vertex AI scoring integration\n- Data residency, VPC Service Controls, IAM least privilege\n\n## Code Example\n```javascript\n// Pseudo-configuration for regional Pub/Sub and Dataflow wiring\nconst config = {\n  regions: ['us-east1','europe-west1'],\n  pubsubTopics: {\n    'us-east1': 'fraud-events-us-east1',\n    'europe-west1': 'fraud-events-eu-west1'\n  },\n  sinks: {\n    'BigQuery': 'project.dataset.fraud_events_$YYYYMM'\n  }\n};\n```\n\n## Follow-up Questions\n- How would you handle schema evolution for the events?\n- How do you ensure end-to-end latency remains under 150 ms during traffic spikes?","diagram":null,"difficulty":"advanced","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","PayPal","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T19:30:47.603Z","createdAt":"2026-01-12T19:30:47.603Z"},{"id":"q-1055","question":"Design an audit-logging pipeline for a payments platform on GCP with sub-100ms end-to-end write latency at multi-region scale (millions of events/sec). Ingest via Pub/Sub, process with Dataflow (Beam), and sink to BigQuery. Explain how you ensure idempotent writes (insertId), handle schema evolution, and provide auditor access across projects without exposing sensitive data. Also outline retries, backoffs, and monitoring?","answer":"Pub/Sub regional topic feeds a Dataflow (Beam) streaming job. Deduplicate by event_id in a stateful DoFn and write to BigQuery with insertId to achieve idempotent, replay-safe sinks. Run Dataflow work","explanation":"## Why This Is Asked\nEvaluates end-to-end streaming architecture, cross-project access, and data governance for payments.\n\n## Key Concepts\n- Pub/Sub streaming ingestion\n- Dataflow/Beam stateful dedup\n- BigQuery streaming inserts with insertId\n- Cross-project IAM for auditors\n- Private Google access and VPC Service Controls\n- Schema evolution with backward-compatible fields\n\n## Code Example\n```javascript\nconst bigQuery = require('@google-cloud/bigquery')();\nasync function sink(rows) {\n  await bigQuery.dataset('audit').table('events').insert(rows, {\n    insertId: rows.map(r => r.event_id)\n  });\n}\n```\n\n## Follow-up Questions\n- How would you test the dedup logic under bursty load?\n- How would you migrate schema without delaying streaming?\n","diagram":null,"difficulty":"advanced","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T20:38:50.148Z","createdAt":"2026-01-12T20:38:50.148Z"},{"id":"q-1105","question":"You're building a beginner-friendly ingestion pipeline: external partners upload daily CSVs to a Cloud Storage bucket; design a minimal flow using a Cloud Function triggered on object finalize to parse the CSV and load a daily summary as a single row into BigQuery. Include IAM permissions, how to trigger on new files, and how to ensure idempotent writes?","answer":"Configure a Cloud Function in Python or Node triggered by Cloud Storage object.finalize on the bucket. It reads the CSV, computes a file-level summary (rows, bytes, elapsed time) and writes one row to","explanation":"## Why This Is Asked\n\nAssesses practical ability to wire GCS events to a function and BigQuery with idempotence, using least-privilege IAM and bucket settings.\n\n## Key Concepts\n\n- GCS object.finalize event\n- Cloud Functions permissions\n- BigQuery insertId for idempotent writes\n- Uniform Bucket Access and IAM roles\n- Testing with sample file and re-uploads\n\n## Code Example\n\n```javascript\n// Node.js Cloud Function skeleton\nconst {BigQuery} = require('@google-cloud/bigquery');\nexports.ingestCSV = async (event) => {\n  const bucket = event.bucket;\n  const name = event.name;\n  // read file, compute summary, insert one row with insertId = name\n};\n```\n\n## Follow-up Questions\n\n- How handle large files and memory limits?\n- How to add retries and failure notifications?","diagram":null,"difficulty":"beginner","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Microsoft","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T22:35:43.917Z","createdAt":"2026-01-12T22:35:43.917Z"},{"id":"q-1269","question":"Design a real-time data pipeline on GCP for a multi-tenant SaaS where each tenant's data must reside in a specified region, is encrypted with CMEK, and access is strictly controlled per-tenant using IAM Conditions and VPC Service Controls; use Pub/Sub, Dataflow, and BigQuery, ensure idempotent writes and exactly-once semantics, and include monitoring and incident response steps?","answer":"Architect a region-scoped, CMEK-protected pipeline: publish tenant events to per-tenant Pub/Sub topics, Dataflow streaming to region-specific BigQuery datasets, enable CMEK on Cloud Storage and BigQue","explanation":"## Why This Is Asked\n\nTests ability to design a cross-tenant pipeline with data residency and security controls, spanning Pub/Sub, Dataflow, and BigQuery, while addressing reliability (idempotency) and observability.\n\n## Key Concepts\n\n- Data residency by tenant/region\n- CMEK on Storage and BigQuery\n- IAM Conditions and VPC Service Controls\n- Exactly-once vs at-least-once semantics\n- Idempotent writes and deduplication\n- Observability and incident response\n\n## Code Example\n\n```python\n# Example Beam pipeline skeleton (Python)\nimport apache_beam as beam\nimport json\nfrom apache_beam.options.pipeline_options import PipelineOptions\n\ndef to_row(msg):\n    obj = json.loads(msg.decode('utf-8'))\n    obj['row_id'] = f\"{obj['tenant']}_{obj['event_id']}\"\n    return obj\n\nopts = PipelineOptions(streaming=True, project='PROJECT')\nwith beam.Pipeline(options=opts) as p:\n    (p\n     | 'ReadFromPubSub' >> beam.io.ReadFromPubSub(topic='projects/PROJECT/topics/tenant-events')\n     | 'Parse' >> beam.Map(to_row)\n     | 'ToBQ' >> beam.io.WriteToBigQuery(\n           table='PROJECT:DATASET.TENANT_EVENTS',\n           schema='tenant STRING, event_id STRING, payload STRING, row_id STRING',\n           write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND,\n           create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED)\n    )\n```\n\n## Follow-up Questions\n\n- How would you validate data residency policy compliance across regions in production?\n- What strategies would you use to evolve the schema per-tenant without downtime?","diagram":null,"difficulty":"advanced","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","PayPal","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T07:34:14.972Z","createdAt":"2026-01-13T07:34:14.972Z"},{"id":"q-1331","question":"Design a beginner-friendly nightly backup workflow on GCP: a Cloud SQL MySQL export writes a dump to a Cloud Storage bucket each night; a Cloud Function triggers on the new object to gzip, append a date stamp, and move it to backups/archived. Include IAM bindings, trigger method on new files, and how to ensure exactly-one backup per day (idempotency)?","answer":"Use Cloud Scheduler to trigger a nightly Cloud SQL export to Cloud Storage; grant Cloud SQL SA write access to the bucket; create a Cloud Function triggered on objectFinalize to gzip and rename to bac","explanation":"## Why This Is Asked\nTests practical usage of scheduled exports, storage event triggers, and idempotent processing in a basic backup workflow. It also evaluates IAM scoping and lifecycle management.\n\n## Key Concepts\n- Cloud Scheduler for periodic tasks\n- Cloud SQL export to Cloud Storage\n- Cloud Functions triggered by storage events (objectFinalize)\n- Idempotent file naming and conditional processing\n- Storage Lifecycle rules for retention\n\n## Code Example\n```javascript\n// Cloud Function (Node.js) skeleton\nconst {Storage} = require('@google-cloud/storage');\nconst storage = new Storage();\n\nexports.backupPostProcess = async (event, context) => {\n  const bucketName = event.bucket;\n  const srcName = event.name; // e.g., dumps/db-20260113.sql.gz\n  const destName = srcName\n    .replace('dumps/', 'backups/archived/')\n    .replace(/\\.gz$/, '.sql.gz');\n  const bucket = storage.bucket(bucketName);\n  const dest = bucket.file(destName);\n  const [exists] = await dest.exists();\n  if (exists) return;\n  await bucket.file(srcName).copy(dest);\n  await bucket.file(srcName).delete();\n};\n```\n\n## Follow-up Questions\n- How would you verify idempotency across multiple runs?\n- How would you handle a failed export and implement retries with alerting?","diagram":null,"difficulty":"beginner","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","NVIDIA","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T11:40:25.703Z","createdAt":"2026-01-13T11:40:25.704Z"},{"id":"q-1370","question":"Design a private, streaming ingestion from on-prem to GCP for sensitive financial logs using Pub/Sub, Dataflow, and BigQuery. Include CMEK, IAM, VPC Service Controls, idempotent writes, exactly-once semantics, failure recovery, and cost considerations. Explain choices and trade-offs?","answer":"Design a private streaming ingestion from on-prem to GCP using Pub/Sub, Dataflow, and BigQuery. Enable CMEK on Pub/Sub and BigQuery, apply VPC Service Controls, and grant least-privilege IAM roles. En","explanation":"## Why This Is Asked\nThis probes end-to-end streaming design under regulatory constraints, balancing latency, security, and reliability across on-prem and GCP.\n\n## Key Concepts\n- Exactly-once streaming with Pub/Sub and Dataflow\n- CMEK on Pub/Sub and BigQuery\n- Private connectivity and VPC Service Controls\n- IAM least privilege and service accounts\n- Idempotent sinks and dedupe strategies\n- Dead-lettering, replay protection, and failure handling\n- Cost considerations (throughput, windowing, autoscaling)\n\n## Code Example\n```java\n// Pseudo-beam snippet: read from Pub/Sub, map to BigQuery rows, write with schema\nPipeline p = Pipeline.create(options);\nPCollection<TableRow> rows = p.apply(\"ReadPubSub\", PubsubIO.readMessagesWithAttributes().fromTopic(\"projects/PROJECT/topics/TOPIC\"))\n  .apply(\"ToBQRow\", ParDo.of(new DoFn<PubsubMessage, TableRow>() {\n     @ProcessElement\n     public void process(@Element PubsubMessage m, OutputReceiver<TableRow> out) {\n        TableRow row = parseToTableRow(m);\n        out.output(row);\n     }\n  }));\nrows.apply(BigQueryIO.writeTableRows().to(\"PROJECT:DATASET.TABLE\").withSchema(schema)\n  .withCreateDisposition(CreateDisposition.CREATE_IF_NEEDED)\n  .withWriteDisposition(WriteDisposition.WRITE_APPEND);\np.run();\n```\n\n## Follow-up Questions\n- How would you monitor Dataflow backpressure and skew?\n- How would you rotate CMEK without downtime and audit key usage?","diagram":"flowchart TD\n  A[On-Prem / Partner Source] -->|Private Connectivity| B[Pub/Sub Topic]\n  B --> C[Dataflow Streaming] \n  C --> D[BigQuery]\n  D --> E[BI Dashboards]","difficulty":"intermediate","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","IBM","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T14:34:40.359Z","createdAt":"2026-01-13T14:34:40.360Z"},{"id":"q-1413","question":"Design a real-time image ingestion pipeline on GCP for a multi-tenant SaaS app. Customers upload images to per-tenant Cloud Storage buckets with CMEK; a Pub/Sub topic notifies on new uploads; a Dataflow streaming job processes images to extract features and writes results to BigQuery, while archived copies are moved to per-tenant archive buckets. Outline architecture, IAM bindings, service account isolation, exactly-once guarantees (e.g., row_id dedup), and cost/latency trade-offs. Include monitoring and failure-response plan?","answer":"Describe end-to-end architecture for a multi-tenant real-time image pipeline on GCP with per-tenant CMEK buckets, Pub/Sub notifications, Dataflow streaming processing, BigQuery storage, and per-tenant","explanation":"## Why This Is Asked\nTests multi-tenant data isolation, customer-managed encryption keys, streaming dataflow, and BigQuery deduplication. Also covers IAM scoping, service account separation, and operational resilience.\n\n## Key Concepts\n- Tenant isolation with CMEK and per-tenant storage\n- Pub/Sub as the ingestion trigger\n- Dataflow streaming pipeline and BigQuery write with deduplication via row_id\n- Archival strategy and cost-aware design\n- Monitoring, alerting, and failure handling\n\n## Code Example\n```javascript\nimport apache_beam as beam\nfrom apache_beam.options.pipeline_options import PipelineOptions\nclass ParseEvent(beam.DoFn):\n  def process(self, element):\n    import json\n    msg = json.loads(element.decode('utf-8'))\n    yield {\n      'tenant_id': msg['tenant_id'],\n      'image_id': msg['image_id'],\n      'row_id': f\"{msg['image_id']}_{msg['timestamp']}\",\n      'embeddings': msg.get('embeddings', None)\n    }\n\n# skeleton; actual IOs would be configured with real project details\nwith beam.Pipeline(options=PipelineOptions()) as p:\n  (p\n   | beam.io.ReadFromPubSub(topic='projects/PROJECT/topics/images')\n   | beam.ParDo(ParseEvent())\n   | beam.io.WriteToBigQuery(\n       table='PROJECT:DATASET.TABLE',\n       schema='SCHEMA_AUTODETECT',\n       write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND,\n       create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED\n     )\n  )\n```\n\n## Follow-up Questions\n- How would you implement row-level security for tenants in BigQuery?\n- What would you monitor to detect data skew or latency regressions?","diagram":"flowchart TD\n  A[User Upload] --> B[Per-tenant GCS (CMEK)]\n  B --> C[Pub/Sub Notification]\n  C --> D[Dataflow Streaming]\n  D --> E[BigQuery (Features)]\n  D --> F[Archive Bucket]\n  E --> G[Dashboards/Alerts]\n","difficulty":"intermediate","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Apple","IBM"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T15:52:25.734Z","createdAt":"2026-01-13T15:52:25.734Z"},{"id":"q-1479","question":"Design a beginner-friendly pipeline on GCP where a daily vendor JSON health report is uploaded to Cloud Storage; create a Cloud Function triggered by finalize to validate JSON against a schema, write valid rows to BigQuery, and move invalid files to a quarantine bucket with a Pub/Sub notice to the on-call channel. Include IAM roles and idempotent processing?","answer":"Outline a pipeline using Cloud Storage, Cloud Functions (Python) triggered on object finalize, a BigQuery table for daily health rows, and a quarantine bucket for invalid files. Emphasize idempotent w","explanation":"## Why This Is Asked\nTests practical data intake, serverless glue, and basic data quality handling on GCP.\n\n## Key Concepts\n- Cloud Storage triggers and object finalize events\n- Schema validation and idempotent BigQuery writes\n- Service accounts with least privilege IAM roles\n- Quarantine workflow and Pub/Sub notifications\n\n## Code Example\n```python\n# skeleton Cloud Function (Python) to validate and route\nfrom google.cloud import bigquery\nimport json\n\nSCHEMA = {...}\nBQ_CLIENT = bigquery.Client()\n\ndef handler(event, context):\n    bucket = event['bucket']\n    name = event['name']\n    data = fetch_json(bucket, name)\n    if validate(data, SCHEMA):\n        write_to_bq(data)\n    else:\n        move_to_quarantine(bucket, name)\n        notify_pubsub(name)\n```\n\n## Follow-up Questions\n- How would you implement idempotent writes in BigQuery?\n- What logging and monitoring would you add for reliability?","diagram":"flowchart TD\nA[Vendor reports: gs://vendor-bucket/daily/report.json] --> B[Cloud Function: onFinalize]\nB --> C{Schema Valid?}\nC -->|Yes| D[Write to BigQuery]\nC -->|No| E[Move to Quarantine bucket]\nE --> F[Pub/Sub alert to on-call]","difficulty":"beginner","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Hashicorp","Hugging Face"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T18:52:30.655Z","createdAt":"2026-01-13T18:52:30.656Z"},{"id":"q-1661","question":"You're architecting a multi-tenant data lake on GCP. Ingest partner feeds via Pub/Sub, land raw into Cloud Storage, process with Dataflow streaming, and emit per-tenant results to dedicated BigQuery datasets with IAM controls. How would you enforce isolation, meet <60s latency, support schema evolution, and implement per-tenant cost governance and quotas? Include IAM bindings, Dataflow templates, and error handling?","answer":"Implement a per-tenant, Pub/Sub-based Dataflow pipeline: publish messages with tenant_id key; Dataflow routes to per-tenant BigQuery datasets with streaming inserts; enforce isolation via separate dat","explanation":"## Why This Is Asked\nThis question probes advanced design for multi-tenant data lakes on GCP, covering data isolation, latency, schema evolution, and cost governance, plus practical implementation details like Pub/Sub routing and per-tenant BigQuery datasets.\n\n## Key Concepts\n- Multi-tenant data routing in Pub/Sub and Dataflow\n- Per-tenant BigQuery datasets and IAM\n- Schema evolution in BigQuery, partitioning\n- Cost governance: quotas, budgets, alerts\n- Dead-letter, idempotent writes, monitoring\n\n## Code Example\n```python\n# Pseudocode: extract tenant_id and route to per-tenant table\nfrom apache_beam.options.pipeline_options import PipelineOptions\nimport apache_beam as beam\n```\n\n## Follow-up Questions\n- How would you handle late-arriving messages?\n- How would you automate tenant onboarding/offboarding?","diagram":null,"difficulty":"advanced","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Two Sigma","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T05:44:21.092Z","createdAt":"2026-01-14T05:44:21.092Z"},{"id":"q-1677","question":"In a high-value fintech setting, design a globally distributed, PCI-compliant fraud-detection pipeline on GCP that ingests events from on-prem via Private Service Connect, publishes to Pub/Sub, processes with Dataflow, stores raw and processed data in Cloud Storage and BigQuery, and uses Vertex AI for real-time scoring; describe IAM, CMEK, VPC, and failure handling, with idempotency?","answer":"Use a multi-region Pub/Sub + Dataflow pipeline with Private Service Connect from on‑prem to Google Cloud, enforce PCI controls, store raw data in Cloud Storage with CMEK, load into BigQuery for analys","explanation":"## Why This Is Asked\nPresents end-to-end architecture for cross-domain connectivity, data governance, and real-time inference, testing latency, throughput, security controls, and failure handling.\n\n## Key Concepts\n- Private Service Connect\n- Pub/Sub, Dataflow\n- BigQuery, Vertex AI\n- CMEK, IAM, VPC\n- Multi-region replication\n- Idempotent sinks, replay protection\n\n## Code Example\n```javascript\n// Example idempotent sink using upsert\nasync function upsertBigQuery(table, key, payload) {\n  // MERGE or INSERT ... ON DUPLICATE KEY UPDATE logic here\n}\n```\n\n## Follow-up Questions\n- How would you validate PCI DSS mapping for data in BigQuery and Cloud Storage?\n- How would you implement message replay protection and exactly-once semantics across Dataflow?\n- What testing strategy ensures latency targets under peak load?","diagram":null,"difficulty":"advanced","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T06:48:52.981Z","createdAt":"2026-01-14T06:48:52.982Z"},{"id":"q-1708","question":"Daily vendor CSVs arrive in Cloud Storage; design a beginner-friendly ingestion pipeline that first scans each file with Cloud DLP to detect PII, and if PII is found moves the file to a quarantine bucket and publishes an alert; if not, parse the CSV and load daily rows into a partitioned BigQuery table. Include IAM bindings, object-finalize trigger, and idempotent processing?","answer":"Trigger a Cloud Function on object finalize. Run a DLP inspect; if PII detected, copy to `quarantine/` and publish a Pub/Sub alert, skipping ingestion. If clean, parse the CSV and append to a daily-pa","explanation":"## Why This Is Asked\nTests ability to stitch together Cloud Storage events, DLP screening, and safe ingestion into BigQuery with proper access control and idempotency.\n\n## Key Concepts\n- Event-driven ingestion\n- Cloud DLP, Cloud Functions, Pub/Sub\n- BigQuery partitioning and deduplication\n- IAM least privilege\n\n## Code Example\n```javascript\n// Example Cloud Function skeleton\nexports.processCsv = async (event) => {\n  // read object, run DLP, route accordingly, write to BigQuery or quarantine\n};\n```\n\n## Follow-up Questions\n- How would you implement retries and exactly-once semantics across the flow?\n- How would you monitor and alert on failures in any step?","diagram":"flowchart TD\n  A[Vendor Upload] --> B[Cloud Storage finalize]\n  B --> C{PII detected?}\n  C -->|Yes| D[Quarantine + Pub/Sub alert]\n  C -->|No| E[Parse CSV]\n  E --> F[BigQuery insert (daily partition)]","difficulty":"beginner","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Discord","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T07:43:23.941Z","createdAt":"2026-01-14T07:43:23.942Z"},{"id":"q-1729","question":"Design a beginner-friendly thumbnail pipeline for user-uploaded images in GCP. Images uploaded to Cloud Storage bucket incoming-images should trigger a Cloud Run container to generate two thumbnails (200px and 400px wide) and store them in image-thumbs. Use a Cloud Function to trigger on finalization and invoke the Cloud Run HTTP endpoint. Output names: <orig>_200_<hash>.jpg and <orig>_400_<hash>.jpg to ensure idempotency. IAM: restrict access so only the Cloud Run service account can write to image-thumbs. Include failure handling and a basic test plan?","answer":"Cloud Function on finalize calls Cloud Run endpoint with bucket/object, computes a hash, and Cloud Run resizes to 200px and 400px, saving as orig_200_hash.jpg and orig_400_hash.jpg in image-thumbs. Id","explanation":"## Why This Is Asked\nTests event-driven design, Cloud Run vs Cloud Functions, and idempotent outputs in GCS pipelines. It also covers IAM least privilege and basic testing strategies.\n\n## Key Concepts\n- Cloud Storage finalize events\n- Cloud Functions and Cloud Run integration\n- Idempotent object writes in GCS\n- IAM bindings for service accounts\n- Containerized image processing with ImageMagick or similar\n\n## Code Example\n```javascript\n// Cloud Function (node) skeleton\nexports.thumbsTrigger = async (data) => {\n  const bucket = data.bucket;\n  const name = data.name;\n  // call Cloud Run with metadata\n  await fetch(RUN_ENDPOINT, { method: 'POST', body: JSON.stringify({ bucket, name })});\n};\n```\n\n## Follow-up Questions\n- How would you test locally and with CI for this flow?\n- How would you handle transient failures and retries without duplicating work?","diagram":null,"difficulty":"beginner","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","NVIDIA","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T08:48:23.676Z","createdAt":"2026-01-14T08:48:23.677Z"},{"id":"q-1864","question":"Given a fintech app ingesting market ticks from multiple regions via Pub/Sub, design an intermediate streaming pipeline to load data into BigQuery with deduplication, schema evolution, and per-region partitioning. Use Dataflow for ETL, ensure exactly-once processing, handle late data up to 2 minutes, implement TTL retention on raw data, and outline monitoring and testing strategy?","answer":"Design a streaming pipeline: Pub/Sub per region -> Dataflow -> BigQuery. Use Pub/Sub message IDs for exactly-once semantics with dedup. Partition BigQuery tables by region and ticker; allow schema evo","explanation":"## Why This Is Asked\nAssess real-world streaming data pipeline design with data correctness, scalability, and ops.\n\n## Key Concepts\n- Streaming ingestion, exactly-once, dedup\n- BigQuery partitioning and TTL\n- Dataflow autoscaling and schema evolution\n- Monitoring, testing (replay, chaos)\n\n## Code Example\n```javascript\n// Skeleton Dataflow-like pipeline\nfunction run(){\n  // Read Pub/Sub\n  // Parse JSON\n  // Window and deduplicate by message_id\n  // Write to BigQuery with region partitioning\n}\n```\n\n## Follow-up Questions\n- How would you handle out-of-order data across regions?\n- What are failure modes and how would you test rollback behavior?","diagram":null,"difficulty":"intermediate","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Google","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T14:49:53.093Z","createdAt":"2026-01-14T14:49:53.093Z"},{"id":"q-1912","question":"Design a beginner-friendly end-to-end GCP pipeline for user avatars stored in Cloud Storage. On object finalize, a Cloud Function should invoke a Cloud Run service to produce two thumbnails (100x100 and 256x256), store them in avatars-resized with deterministic naming, and publish a log entry. If processing fails, publish a Pub/Sub alert and quarantine the original file. Keep IAM minimal and describe a basic test plan and idempotency strategy?","answer":"Use a Cloud Storage finalize trigger to invoke a Cloud Function, which calls a Cloud Run thumbnail service to generate 100x100 and 256x256 images. Output to avatars-resized/<orig>_100_<hash>.jpg and <","explanation":"## Why This Is Asked\nThis tests practical event-driven design with Cloud Storage, Cloud Run, and Cloud Functions, plus idempotent output naming and failure handling.\n\n## Key Concepts\n- Event-driven primitives: storage finalize, function invocation, HTTP calls to Cloud Run\n- Idempotency via deterministic filenames and pre-write checks\n- Observability and alerting via Pub/Sub and Cloud Logging\n- IAM least privilege for service accounts\n\n## Code Example\n```\n// Node.js Cloud Function (handleAvatarUpload)\nexports.handleAvatarUpload = async (event) => {\n  // parse file, compute hash, call Cloud Run with image, await result\n};\n```\n\n## Follow-up Questions\n- How would you scale to more sizes without many Run invocations?\n- How would you handle large file uploads or retries without duplicate work?","diagram":"flowchart TD\n  A[Cloud Storage: finalize] --> B[Cloud Function: trigger]\n  B --> C[Cloud Run: thumbnail service]\n  C --> D[avatars-resized bucket]\n  B --> E[Pub/Sub: alerts]","difficulty":"beginner","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","Twitter","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T16:57:24.361Z","createdAt":"2026-01-14T16:57:24.361Z"},{"id":"q-2061","question":"Design an end-to-end cross-region streaming data pipeline on GCP to ingest high-volume telemetry from Pub/Sub into BigQuery with real-time dashboards. Use Dataflow (Beam) for streaming, enable exactly-once processing, implement automatic primary/DR region failover, use multi-region Pub/Sub topics and cross-region BigQuery datasets, handle schema evolution, and provide monitoring and rollback strategies?","answer":"Design a two-region streaming pipeline with primary deployment in us-central1 and disaster recovery in europe-west1. Utilize Pub/Sub multi-region topics for cross-region message distribution, implement Dataflow jobs with exactly-once processing semantics, and establish deduplication using event_id through stateful DoFn operations. Write transformed data to cross-region BigQuery datasets with automatic schema evolution support, enable real-time dashboards via Looker or Data Studio connections, and implement comprehensive monitoring through Cloud Monitoring alerts and automated rollback capabilities using deployment templates.","explanation":"## Why This Is Asked\nAssesses real-world cross-region streaming architecture design, exactly-once processing semantics, disaster recovery failover mechanisms, and operational governance practices.\n\n## Key Concepts\n- Cross-region Pub/Sub topics and BigQuery datasets for geographic redundancy\n- Apache Beam Dataflow exactly-once processing guarantees\n- Schema evolution governance and backward compatibility\n- Comprehensive observability and automated rollback strategies\n\n## Code Example\n\n```python\nimport apache_beam as beam\nfrom apache_beam.options.pipeline_options import PipelineOptions\n\nclass Deduplicate(beam.DoFn):\n    def __init__(self):\n        self.seen = set()\n    \n    def process(self, event):\n        if event['event_id'] in self.seen:\n            return\n        self.seen.add(event['event_id'])\n        yield event\n\noptions = PipelineOptions(\n    streaming=True,\n    project='your-project',\n    region='us-central1',\n    job_name='cross-region-streaming'\n)\n```","diagram":"flowchart TD\n  A[Pub/Sub multi-region] --> B[Dataflow in primary]\n  B --> C[BigQuery (region A)]\n  A --> D[Dataflow in DR]\n  D --> E[BigQuery (region B)]","difficulty":"intermediate","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","IBM"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T05:39:55.295Z","createdAt":"2026-01-14T22:48:34.134Z"},{"id":"q-2100","question":"Design a beginner-friendly, event-driven data validation pipeline on GCP for daily event JSONs uploaded to Cloud Storage: on file finalize, a Cloud Function validates each record against a simple JSON Schema, quarantines invalid files, and writes valid records to a BigQuery table with partitioning by date and a deterministic write-ID to ensure idempotency; explain IAM bindings and a basic test plan?","answer":"Use a Cloud Storage onFinalize trigger. Validate each record with a lightweight JSON Schema in the Cloud Function; if valid, write to a partitioned BigQuery table using a deterministic write-id (date + file hash) for idempotency, and move invalid files to a quarantine bucket with Pub/Sub notifications.","explanation":"## Why This Is Asked\n\nTests the ability to design a practical, beginner-friendly data validation flow on GCP that handles real-world issues like schema drift, idempotent writes, and fault isolation.\n\n## Key Concepts\n\n- Event-driven workflows and Cloud Functions\n- Cloud Storage triggers and onFinalize semantics\n- JSON Schema validation and lightweight data validation\n- BigQuery partitioning and idempotent writes with deterministic IDs\n- Quarantine handling and Pub/Sub alerts\n- IAM least privilege and secure service-to-service access\n- Testing strategies: unit + end-to-end\n\n## Code Example\n\n```javascript\n// Cloud Function: validate-events\nconst { Storage } = require('@google-cloud/storage');\nconst { BigQuery } = require('@google-cloud/bigquery');\nconst crypto = require('crypto');\n\nconst storage = new Storage();\nconst bigquery = new BigQuery();\n\nexports.validateEvents = async (event, context) => {\n  const fileName = context.eventId;\n  const bucketName = event.bucket;\n  \n  // Download and validate file\n  const file = storage.bucket(bucketName).file(fileName);\n  const contents = await file.download();\n  const records = JSON.parse(contents.toString());\n  \n  const validRecords = [];\n  const invalidRecords = [];\n  \n  // Validate each record against JSON Schema\n  for (const record of records) {\n    if (validateRecord(record)) {\n      validRecords.push(record);\n    } else {\n      invalidRecords.push(record);\n    }\n  }\n  \n  // Write valid records to BigQuery\n  if (validRecords.length > 0) {\n    const writeId = generateDeterministicId(fileName, new Date());\n    await writeToBigQuery(validRecords, writeId);\n  }\n  \n  // Move invalid files to quarantine\n  if (invalidRecords.length > 0) {\n    await moveToQuarantine(file, invalidRecords);\n  }\n};\n\nfunction generateDeterministicId(fileName, date) {\n  const hash = crypto.createHash('md5').update(fileName).digest('hex');\n  return `${date.toISOString().split('T')[0]}_${hash}`;\n}\n\nfunction validateRecord(record) {\n  // Simple JSON Schema validation\n  const schema = {\n    type: 'object',\n    required: ['id', 'timestamp', 'event_type'],\n    properties: {\n      id: { type: 'string' },\n      timestamp: { type: 'string', format: 'date-time' },\n      event_type: { type: 'string' }\n    }\n  };\n  \n  return validateAgainstSchema(record, schema);\n}\n\nasync function writeToBigQuery(records, writeId) {\n  const dataset = bigquery.dataset('events_dataset');\n  const table = dataset.table('events_table');\n  \n  const rows = records.map(record => ({\n    ...record,\n    write_id: writeId,\n    partition_date: new Date().toISOString().split('T')[0]\n  }));\n  \n  await table.insert(rows);\n}\n\nasync function moveToQuarantine(file, invalidRecords) {\n  const quarantineBucket = storage.bucket('events-quarantine');\n  await file.move(quarantineBucket.file(file.name));\n  \n  // Send Pub/Sub notification\n  const pubsub = new PubSub();\n  await pubsub.topic('validation-failures').publish(Buffer.from(JSON.stringify({\n    file: file.name,\n    invalidCount: invalidRecords.length,\n    timestamp: new Date().toISOString()\n  })));\n}\n```\n\n## IAM Bindings\n\n```yaml\n# Cloud Function Service Account\n- role: roles/cloudfunctions.serviceAgent\n  member: serviceAccount:validate-events@project.iam.gserviceaccount.com\n\n# Storage Access\n- role: roles/storage.objectViewer\n  member: serviceAccount:validate-events@project.iam.gserviceaccount.com\n  resource: projects/_/buckets/events-input\n\n- role: roles/storage.objectAdmin\n  member: serviceAccount:validate-events@project.iam.gserviceaccount.com\n  resource: projects/_/buckets/events-quarantine\n\n# BigQuery Access\n- role: roles/bigquery.dataEditor\n  member: serviceAccount:validate-events@project.iam.gserviceaccount.com\n  resource: projects/project/datasets/events_dataset\n\n# Pub/Sub Access\n- role: roles/pubsub.publisher\n  member: serviceAccount:validate-events@project.iam.gserviceaccount.com\n  resource: projects/project/topics/validation-failures\n```\n\n## Test Plan\n\n### Unit Tests\n- Test JSON Schema validation with valid/invalid records\n- Test deterministic ID generation\n- Test BigQuery row formatting\n\n### Integration Tests\n- Test end-to-end flow with sample files\n- Test quarantine bucket movement\n- Test Pub/Sub notification publishing\n\n### Manual Tests\n- Upload valid JSON file → verify BigQuery insertion\n- Upload invalid JSON file → verify quarantine movement\n- Upload mixed file → verify partial processing\n\n```bash\n# Deploy and test\ngcloud functions deploy validate-events --runtime nodejs14 --trigger-bucket events-input\n\n# Test with valid file\necho '[{\"id\":\"1\",\"timestamp\":\"2023-01-01T00:00:00Z\",\"event_type\":\"click\"}]' > valid.json\ngsutil cp valid.json gs://events-input/\n\n# Test with invalid file\necho '[{\"id\":\"1\",\"event_type\":\"click\"}]' > invalid.json\ngsutil cp invalid.json gs://events-input/\n```","diagram":"flowchart TD\n  A[Cloud Storage: onFinalize] --> B[Cloud Function: validate]\n  B --> C{Valid?}\n  C -->|Yes| D[BigQuery: insert with id]\n  C -->|No| E[Quarantine bucket & Pub/Sub alert]","difficulty":"beginner","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Databricks","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T05:01:21.623Z","createdAt":"2026-01-15T02:15:17.066Z"},{"id":"q-904","question":"How would you configure a Cloud Run (fully managed) service to securely connect to a Cloud SQL PostgreSQL instance using a private connection, including IAM bindings and deployment steps to ensure the app talks via the Cloud SQL socket and never uses the instance's public IP?","answer":"Grant the Cloud Run service account the roles/cloudsql.client, enable the sqladmin API, deploy with --add-cloudsql-instances PROJECT:REGION:INSTANCE, and connect using the Unix socket /cloudsql/INSTAN","explanation":"## Why This Is Asked\nTests private connectivity setup between Cloud Run and Cloud SQL and correct IAM binding.\n\n## Key Concepts\n- Cloud Run (Managed)\n- Cloud SQL (PostgreSQL)\n- IAM roles and private connections\n- gcloud deployment flags\n\n## Code Example\n```javascript\n// Node.js example connection string (pseudo)\nconst client = new Client({ connectionString: 'postgres://user:pass@localhost/db?host=/cloudsql/PROJECT:REGION:INSTANCE' });\n```\n\n## Follow-up Questions\n- How do you test the connection in a CI environment?\n- How would you rotate credentials without downtime?","diagram":null,"difficulty":"beginner","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["OpenAI","Slack","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T14:43:17.098Z","createdAt":"2026-01-12T14:43:17.098Z"}],"subChannels":["general"],"companies":["Adobe","Airbnb","Amazon","Apple","Bloomberg","Citadel","Databricks","Discord","Google","Hashicorp","Hugging Face","IBM","Instacart","Microsoft","MongoDB","NVIDIA","OpenAI","PayPal","Robinhood","Salesforce","Slack","Snap","Snowflake","Stripe","Tesla","Twitter","Two Sigma","Uber","Zoom"],"stats":{"total":17,"beginner":8,"intermediate":4,"advanced":5,"newThisWeek":17}}