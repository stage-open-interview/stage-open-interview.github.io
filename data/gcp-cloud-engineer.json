{"questions":[{"id":"q-1016","question":"Design a regional streaming pipeline for a fintech app: on‑prem and GKE emit events to region Pub/Sub topics; a Dataflow streaming job enforces exactly-once, writes partitioned regional BigQuery tables, and triggers Vertex AI scoring in near real-time. How would you achieve low latency, data residency, schema evolution, and reliable failure recovery?","answer":"Design a regional streaming pipeline: on‑prem and GKE emit to region Pub/Sub topics; Dataflow streaming enforces exactly-once, writes partitioned regional BigQuery tables, and triggers Vertex AI scori","explanation":"## Why This Is Asked\nThis question probes cross-region streaming, data residency, and production-ready fault tolerance in a real-time fraud pipeline.\n\n## Key Concepts\n- Regional Pub/Sub topics and fanout\n- Dataflow streaming with exactly-once\n- BigQuery regional storage and partitioning\n- Vertex AI scoring integration\n- Data residency, VPC Service Controls, IAM least privilege\n\n## Code Example\n```javascript\n// Pseudo-configuration for regional Pub/Sub and Dataflow wiring\nconst config = {\n  regions: ['us-east1','europe-west1'],\n  pubsubTopics: {\n    'us-east1': 'fraud-events-us-east1',\n    'europe-west1': 'fraud-events-eu-west1'\n  },\n  sinks: {\n    'BigQuery': 'project.dataset.fraud_events_$YYYYMM'\n  }\n};\n```\n\n## Follow-up Questions\n- How would you handle schema evolution for the events?\n- How do you ensure end-to-end latency remains under 150 ms during traffic spikes?","diagram":null,"difficulty":"advanced","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","PayPal","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T19:30:47.603Z","createdAt":"2026-01-12T19:30:47.603Z"},{"id":"q-1055","question":"Design an audit-logging pipeline for a payments platform on GCP with sub-100ms end-to-end write latency at multi-region scale (millions of events/sec). Ingest via Pub/Sub, process with Dataflow (Beam), and sink to BigQuery. Explain how you ensure idempotent writes (insertId), handle schema evolution, and provide auditor access across projects without exposing sensitive data. Also outline retries, backoffs, and monitoring?","answer":"Pub/Sub regional topic feeds a Dataflow (Beam) streaming job. Deduplicate by event_id in a stateful DoFn and write to BigQuery with insertId to achieve idempotent, replay-safe sinks. Run Dataflow work","explanation":"## Why This Is Asked\nEvaluates end-to-end streaming architecture, cross-project access, and data governance for payments.\n\n## Key Concepts\n- Pub/Sub streaming ingestion\n- Dataflow/Beam stateful dedup\n- BigQuery streaming inserts with insertId\n- Cross-project IAM for auditors\n- Private Google access and VPC Service Controls\n- Schema evolution with backward-compatible fields\n\n## Code Example\n```javascript\nconst bigQuery = require('@google-cloud/bigquery')();\nasync function sink(rows) {\n  await bigQuery.dataset('audit').table('events').insert(rows, {\n    insertId: rows.map(r => r.event_id)\n  });\n}\n```\n\n## Follow-up Questions\n- How would you test the dedup logic under bursty load?\n- How would you migrate schema without delaying streaming?\n","diagram":null,"difficulty":"advanced","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T20:38:50.148Z","createdAt":"2026-01-12T20:38:50.148Z"},{"id":"q-1105","question":"You're building a beginner-friendly ingestion pipeline: external partners upload daily CSVs to a Cloud Storage bucket; design a minimal flow using a Cloud Function triggered on object finalize to parse the CSV and load a daily summary as a single row into BigQuery. Include IAM permissions, how to trigger on new files, and how to ensure idempotent writes?","answer":"Configure a Cloud Function in Python or Node triggered by Cloud Storage object.finalize on the bucket. It reads the CSV, computes a file-level summary (rows, bytes, elapsed time) and writes one row to","explanation":"## Why This Is Asked\n\nAssesses practical ability to wire GCS events to a function and BigQuery with idempotence, using least-privilege IAM and bucket settings.\n\n## Key Concepts\n\n- GCS object.finalize event\n- Cloud Functions permissions\n- BigQuery insertId for idempotent writes\n- Uniform Bucket Access and IAM roles\n- Testing with sample file and re-uploads\n\n## Code Example\n\n```javascript\n// Node.js Cloud Function skeleton\nconst {BigQuery} = require('@google-cloud/bigquery');\nexports.ingestCSV = async (event) => {\n  const bucket = event.bucket;\n  const name = event.name;\n  // read file, compute summary, insert one row with insertId = name\n};\n```\n\n## Follow-up Questions\n\n- How handle large files and memory limits?\n- How to add retries and failure notifications?","diagram":null,"difficulty":"beginner","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Microsoft","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T22:35:43.917Z","createdAt":"2026-01-12T22:35:43.917Z"},{"id":"q-1269","question":"Design a real-time data pipeline on GCP for a multi-tenant SaaS where each tenant's data must reside in a specified region, is encrypted with CMEK, and access is strictly controlled per-tenant using IAM Conditions and VPC Service Controls; use Pub/Sub, Dataflow, and BigQuery, ensure idempotent writes and exactly-once semantics, and include monitoring and incident response steps?","answer":"Architect a region-scoped, CMEK-protected pipeline: publish tenant events to per-tenant Pub/Sub topics, Dataflow streaming to region-specific BigQuery datasets, enable CMEK on Cloud Storage and BigQue","explanation":"## Why This Is Asked\n\nTests ability to design a cross-tenant pipeline with data residency and security controls, spanning Pub/Sub, Dataflow, and BigQuery, while addressing reliability (idempotency) and observability.\n\n## Key Concepts\n\n- Data residency by tenant/region\n- CMEK on Storage and BigQuery\n- IAM Conditions and VPC Service Controls\n- Exactly-once vs at-least-once semantics\n- Idempotent writes and deduplication\n- Observability and incident response\n\n## Code Example\n\n```python\n# Example Beam pipeline skeleton (Python)\nimport apache_beam as beam\nimport json\nfrom apache_beam.options.pipeline_options import PipelineOptions\n\ndef to_row(msg):\n    obj = json.loads(msg.decode('utf-8'))\n    obj['row_id'] = f\"{obj['tenant']}_{obj['event_id']}\"\n    return obj\n\nopts = PipelineOptions(streaming=True, project='PROJECT')\nwith beam.Pipeline(options=opts) as p:\n    (p\n     | 'ReadFromPubSub' >> beam.io.ReadFromPubSub(topic='projects/PROJECT/topics/tenant-events')\n     | 'Parse' >> beam.Map(to_row)\n     | 'ToBQ' >> beam.io.WriteToBigQuery(\n           table='PROJECT:DATASET.TENANT_EVENTS',\n           schema='tenant STRING, event_id STRING, payload STRING, row_id STRING',\n           write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND,\n           create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED)\n    )\n```\n\n## Follow-up Questions\n\n- How would you validate data residency policy compliance across regions in production?\n- What strategies would you use to evolve the schema per-tenant without downtime?","diagram":null,"difficulty":"advanced","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","PayPal","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T07:34:14.972Z","createdAt":"2026-01-13T07:34:14.972Z"},{"id":"q-1331","question":"Design a beginner-friendly nightly backup workflow on GCP: a Cloud SQL MySQL export writes a dump to a Cloud Storage bucket each night; a Cloud Function triggers on the new object to gzip, append a date stamp, and move it to backups/archived. Include IAM bindings, trigger method on new files, and how to ensure exactly-one backup per day (idempotency)?","answer":"Use Cloud Scheduler to trigger a nightly Cloud SQL export to Cloud Storage; grant Cloud SQL SA write access to the bucket; create a Cloud Function triggered on objectFinalize to gzip and rename to bac","explanation":"## Why This Is Asked\nTests practical usage of scheduled exports, storage event triggers, and idempotent processing in a basic backup workflow. It also evaluates IAM scoping and lifecycle management.\n\n## Key Concepts\n- Cloud Scheduler for periodic tasks\n- Cloud SQL export to Cloud Storage\n- Cloud Functions triggered by storage events (objectFinalize)\n- Idempotent file naming and conditional processing\n- Storage Lifecycle rules for retention\n\n## Code Example\n```javascript\n// Cloud Function (Node.js) skeleton\nconst {Storage} = require('@google-cloud/storage');\nconst storage = new Storage();\n\nexports.backupPostProcess = async (event, context) => {\n  const bucketName = event.bucket;\n  const srcName = event.name; // e.g., dumps/db-20260113.sql.gz\n  const destName = srcName\n    .replace('dumps/', 'backups/archived/')\n    .replace(/\\.gz$/, '.sql.gz');\n  const bucket = storage.bucket(bucketName);\n  const dest = bucket.file(destName);\n  const [exists] = await dest.exists();\n  if (exists) return;\n  await bucket.file(srcName).copy(dest);\n  await bucket.file(srcName).delete();\n};\n```\n\n## Follow-up Questions\n- How would you verify idempotency across multiple runs?\n- How would you handle a failed export and implement retries with alerting?","diagram":null,"difficulty":"beginner","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","NVIDIA","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T11:40:25.703Z","createdAt":"2026-01-13T11:40:25.704Z"},{"id":"q-1370","question":"Design a private, streaming ingestion from on-prem to GCP for sensitive financial logs using Pub/Sub, Dataflow, and BigQuery. Include CMEK, IAM, VPC Service Controls, idempotent writes, exactly-once semantics, failure recovery, and cost considerations. Explain choices and trade-offs?","answer":"Design a private streaming ingestion from on-prem to GCP using Pub/Sub, Dataflow, and BigQuery. Enable CMEK on Pub/Sub and BigQuery, apply VPC Service Controls, and grant least-privilege IAM roles. En","explanation":"## Why This Is Asked\nThis probes end-to-end streaming design under regulatory constraints, balancing latency, security, and reliability across on-prem and GCP.\n\n## Key Concepts\n- Exactly-once streaming with Pub/Sub and Dataflow\n- CMEK on Pub/Sub and BigQuery\n- Private connectivity and VPC Service Controls\n- IAM least privilege and service accounts\n- Idempotent sinks and dedupe strategies\n- Dead-lettering, replay protection, and failure handling\n- Cost considerations (throughput, windowing, autoscaling)\n\n## Code Example\n```java\n// Pseudo-beam snippet: read from Pub/Sub, map to BigQuery rows, write with schema\nPipeline p = Pipeline.create(options);\nPCollection<TableRow> rows = p.apply(\"ReadPubSub\", PubsubIO.readMessagesWithAttributes().fromTopic(\"projects/PROJECT/topics/TOPIC\"))\n  .apply(\"ToBQRow\", ParDo.of(new DoFn<PubsubMessage, TableRow>() {\n     @ProcessElement\n     public void process(@Element PubsubMessage m, OutputReceiver<TableRow> out) {\n        TableRow row = parseToTableRow(m);\n        out.output(row);\n     }\n  }));\nrows.apply(BigQueryIO.writeTableRows().to(\"PROJECT:DATASET.TABLE\").withSchema(schema)\n  .withCreateDisposition(CreateDisposition.CREATE_IF_NEEDED)\n  .withWriteDisposition(WriteDisposition.WRITE_APPEND);\np.run();\n```\n\n## Follow-up Questions\n- How would you monitor Dataflow backpressure and skew?\n- How would you rotate CMEK without downtime and audit key usage?","diagram":"flowchart TD\n  A[On-Prem / Partner Source] -->|Private Connectivity| B[Pub/Sub Topic]\n  B --> C[Dataflow Streaming] \n  C --> D[BigQuery]\n  D --> E[BI Dashboards]","difficulty":"intermediate","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","IBM","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T14:34:40.359Z","createdAt":"2026-01-13T14:34:40.360Z"},{"id":"q-1413","question":"Design a real-time image ingestion pipeline on GCP for a multi-tenant SaaS app. Customers upload images to per-tenant Cloud Storage buckets with CMEK; a Pub/Sub topic notifies on new uploads; a Dataflow streaming job processes images to extract features and writes results to BigQuery, while archived copies are moved to per-tenant archive buckets. Outline architecture, IAM bindings, service account isolation, exactly-once guarantees (e.g., row_id dedup), and cost/latency trade-offs. Include monitoring and failure-response plan?","answer":"Describe end-to-end architecture for a multi-tenant real-time image pipeline on GCP with per-tenant CMEK buckets, Pub/Sub notifications, Dataflow streaming processing, BigQuery storage, and per-tenant","explanation":"## Why This Is Asked\nTests multi-tenant data isolation, customer-managed encryption keys, streaming dataflow, and BigQuery deduplication. Also covers IAM scoping, service account separation, and operational resilience.\n\n## Key Concepts\n- Tenant isolation with CMEK and per-tenant storage\n- Pub/Sub as the ingestion trigger\n- Dataflow streaming pipeline and BigQuery write with deduplication via row_id\n- Archival strategy and cost-aware design\n- Monitoring, alerting, and failure handling\n\n## Code Example\n```javascript\nimport apache_beam as beam\nfrom apache_beam.options.pipeline_options import PipelineOptions\nclass ParseEvent(beam.DoFn):\n  def process(self, element):\n    import json\n    msg = json.loads(element.decode('utf-8'))\n    yield {\n      'tenant_id': msg['tenant_id'],\n      'image_id': msg['image_id'],\n      'row_id': f\"{msg['image_id']}_{msg['timestamp']}\",\n      'embeddings': msg.get('embeddings', None)\n    }\n\n# skeleton; actual IOs would be configured with real project details\nwith beam.Pipeline(options=PipelineOptions()) as p:\n  (p\n   | beam.io.ReadFromPubSub(topic='projects/PROJECT/topics/images')\n   | beam.ParDo(ParseEvent())\n   | beam.io.WriteToBigQuery(\n       table='PROJECT:DATASET.TABLE',\n       schema='SCHEMA_AUTODETECT',\n       write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND,\n       create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED\n     )\n  )\n```\n\n## Follow-up Questions\n- How would you implement row-level security for tenants in BigQuery?\n- What would you monitor to detect data skew or latency regressions?","diagram":"flowchart TD\n  A[User Upload] --> B[Per-tenant GCS (CMEK)]\n  B --> C[Pub/Sub Notification]\n  C --> D[Dataflow Streaming]\n  D --> E[BigQuery (Features)]\n  D --> F[Archive Bucket]\n  E --> G[Dashboards/Alerts]\n","difficulty":"intermediate","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Apple","IBM"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T15:52:25.734Z","createdAt":"2026-01-13T15:52:25.734Z"},{"id":"q-1479","question":"Design a beginner-friendly pipeline on GCP where a daily vendor JSON health report is uploaded to Cloud Storage; create a Cloud Function triggered by finalize to validate JSON against a schema, write valid rows to BigQuery, and move invalid files to a quarantine bucket with a Pub/Sub notice to the on-call channel. Include IAM roles and idempotent processing?","answer":"Outline a pipeline using Cloud Storage, Cloud Functions (Python) triggered on object finalize, a BigQuery table for daily health rows, and a quarantine bucket for invalid files. Emphasize idempotent w","explanation":"## Why This Is Asked\nTests practical data intake, serverless glue, and basic data quality handling on GCP.\n\n## Key Concepts\n- Cloud Storage triggers and object finalize events\n- Schema validation and idempotent BigQuery writes\n- Service accounts with least privilege IAM roles\n- Quarantine workflow and Pub/Sub notifications\n\n## Code Example\n```python\n# skeleton Cloud Function (Python) to validate and route\nfrom google.cloud import bigquery\nimport json\n\nSCHEMA = {...}\nBQ_CLIENT = bigquery.Client()\n\ndef handler(event, context):\n    bucket = event['bucket']\n    name = event['name']\n    data = fetch_json(bucket, name)\n    if validate(data, SCHEMA):\n        write_to_bq(data)\n    else:\n        move_to_quarantine(bucket, name)\n        notify_pubsub(name)\n```\n\n## Follow-up Questions\n- How would you implement idempotent writes in BigQuery?\n- What logging and monitoring would you add for reliability?","diagram":"flowchart TD\nA[Vendor reports: gs://vendor-bucket/daily/report.json] --> B[Cloud Function: onFinalize]\nB --> C{Schema Valid?}\nC -->|Yes| D[Write to BigQuery]\nC -->|No| E[Move to Quarantine bucket]\nE --> F[Pub/Sub alert to on-call]","difficulty":"beginner","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Hashicorp","Hugging Face"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T18:52:30.655Z","createdAt":"2026-01-13T18:52:30.656Z"},{"id":"q-1661","question":"You're architecting a multi-tenant data lake on GCP. Ingest partner feeds via Pub/Sub, land raw into Cloud Storage, process with Dataflow streaming, and emit per-tenant results to dedicated BigQuery datasets with IAM controls. How would you enforce isolation, meet <60s latency, support schema evolution, and implement per-tenant cost governance and quotas? Include IAM bindings, Dataflow templates, and error handling?","answer":"Implement a per-tenant, Pub/Sub-based Dataflow pipeline: publish messages with tenant_id key; Dataflow routes to per-tenant BigQuery datasets with streaming inserts; enforce isolation via separate dat","explanation":"## Why This Is Asked\nThis question probes advanced design for multi-tenant data lakes on GCP, covering data isolation, latency, schema evolution, and cost governance, plus practical implementation details like Pub/Sub routing and per-tenant BigQuery datasets.\n\n## Key Concepts\n- Multi-tenant data routing in Pub/Sub and Dataflow\n- Per-tenant BigQuery datasets and IAM\n- Schema evolution in BigQuery, partitioning\n- Cost governance: quotas, budgets, alerts\n- Dead-letter, idempotent writes, monitoring\n\n## Code Example\n```python\n# Pseudocode: extract tenant_id and route to per-tenant table\nfrom apache_beam.options.pipeline_options import PipelineOptions\nimport apache_beam as beam\n```\n\n## Follow-up Questions\n- How would you handle late-arriving messages?\n- How would you automate tenant onboarding/offboarding?","diagram":null,"difficulty":"advanced","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Two Sigma","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T05:44:21.092Z","createdAt":"2026-01-14T05:44:21.092Z"},{"id":"q-1677","question":"In a high-value fintech setting, design a globally distributed, PCI-compliant fraud-detection pipeline on GCP that ingests events from on-prem via Private Service Connect, publishes to Pub/Sub, processes with Dataflow, stores raw and processed data in Cloud Storage and BigQuery, and uses Vertex AI for real-time scoring; describe IAM, CMEK, VPC, and failure handling, with idempotency?","answer":"Use a multi-region Pub/Sub + Dataflow pipeline with Private Service Connect from on‑prem to Google Cloud, enforce PCI controls, store raw data in Cloud Storage with CMEK, load into BigQuery for analys","explanation":"## Why This Is Asked\nPresents end-to-end architecture for cross-domain connectivity, data governance, and real-time inference, testing latency, throughput, security controls, and failure handling.\n\n## Key Concepts\n- Private Service Connect\n- Pub/Sub, Dataflow\n- BigQuery, Vertex AI\n- CMEK, IAM, VPC\n- Multi-region replication\n- Idempotent sinks, replay protection\n\n## Code Example\n```javascript\n// Example idempotent sink using upsert\nasync function upsertBigQuery(table, key, payload) {\n  // MERGE or INSERT ... ON DUPLICATE KEY UPDATE logic here\n}\n```\n\n## Follow-up Questions\n- How would you validate PCI DSS mapping for data in BigQuery and Cloud Storage?\n- How would you implement message replay protection and exactly-once semantics across Dataflow?\n- What testing strategy ensures latency targets under peak load?","diagram":null,"difficulty":"advanced","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T06:48:52.981Z","createdAt":"2026-01-14T06:48:52.982Z"},{"id":"q-1708","question":"Daily vendor CSVs arrive in Cloud Storage; design a beginner-friendly ingestion pipeline that first scans each file with Cloud DLP to detect PII, and if PII is found moves the file to a quarantine bucket and publishes an alert; if not, parse the CSV and load daily rows into a partitioned BigQuery table. Include IAM bindings, object-finalize trigger, and idempotent processing?","answer":"Trigger a Cloud Function on object finalize. Run a DLP inspect; if PII detected, copy to `quarantine/` and publish a Pub/Sub alert, skipping ingestion. If clean, parse the CSV and append to a daily-pa","explanation":"## Why This Is Asked\nTests ability to stitch together Cloud Storage events, DLP screening, and safe ingestion into BigQuery with proper access control and idempotency.\n\n## Key Concepts\n- Event-driven ingestion\n- Cloud DLP, Cloud Functions, Pub/Sub\n- BigQuery partitioning and deduplication\n- IAM least privilege\n\n## Code Example\n```javascript\n// Example Cloud Function skeleton\nexports.processCsv = async (event) => {\n  // read object, run DLP, route accordingly, write to BigQuery or quarantine\n};\n```\n\n## Follow-up Questions\n- How would you implement retries and exactly-once semantics across the flow?\n- How would you monitor and alert on failures in any step?","diagram":"flowchart TD\n  A[Vendor Upload] --> B[Cloud Storage finalize]\n  B --> C{PII detected?}\n  C -->|Yes| D[Quarantine + Pub/Sub alert]\n  C -->|No| E[Parse CSV]\n  E --> F[BigQuery insert (daily partition)]","difficulty":"beginner","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Discord","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T07:43:23.941Z","createdAt":"2026-01-14T07:43:23.942Z"},{"id":"q-1729","question":"Design a beginner-friendly thumbnail pipeline for user-uploaded images in GCP. Images uploaded to Cloud Storage bucket incoming-images should trigger a Cloud Run container to generate two thumbnails (200px and 400px wide) and store them in image-thumbs. Use a Cloud Function to trigger on finalization and invoke the Cloud Run HTTP endpoint. Output names: <orig>_200_<hash>.jpg and <orig>_400_<hash>.jpg to ensure idempotency. IAM: restrict access so only the Cloud Run service account can write to image-thumbs. Include failure handling and a basic test plan?","answer":"Cloud Function on finalize calls Cloud Run endpoint with bucket/object, computes a hash, and Cloud Run resizes to 200px and 400px, saving as orig_200_hash.jpg and orig_400_hash.jpg in image-thumbs. Id","explanation":"## Why This Is Asked\nTests event-driven design, Cloud Run vs Cloud Functions, and idempotent outputs in GCS pipelines. It also covers IAM least privilege and basic testing strategies.\n\n## Key Concepts\n- Cloud Storage finalize events\n- Cloud Functions and Cloud Run integration\n- Idempotent object writes in GCS\n- IAM bindings for service accounts\n- Containerized image processing with ImageMagick or similar\n\n## Code Example\n```javascript\n// Cloud Function (node) skeleton\nexports.thumbsTrigger = async (data) => {\n  const bucket = data.bucket;\n  const name = data.name;\n  // call Cloud Run with metadata\n  await fetch(RUN_ENDPOINT, { method: 'POST', body: JSON.stringify({ bucket, name })});\n};\n```\n\n## Follow-up Questions\n- How would you test locally and with CI for this flow?\n- How would you handle transient failures and retries without duplicating work?","diagram":null,"difficulty":"beginner","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","NVIDIA","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T08:48:23.676Z","createdAt":"2026-01-14T08:48:23.677Z"},{"id":"q-1864","question":"Given a fintech app ingesting market ticks from multiple regions via Pub/Sub, design an intermediate streaming pipeline to load data into BigQuery with deduplication, schema evolution, and per-region partitioning. Use Dataflow for ETL, ensure exactly-once processing, handle late data up to 2 minutes, implement TTL retention on raw data, and outline monitoring and testing strategy?","answer":"Design a streaming pipeline: Pub/Sub per region -> Dataflow -> BigQuery. Use Pub/Sub message IDs for exactly-once semantics with dedup. Partition BigQuery tables by region and ticker; allow schema evo","explanation":"## Why This Is Asked\nAssess real-world streaming data pipeline design with data correctness, scalability, and ops.\n\n## Key Concepts\n- Streaming ingestion, exactly-once, dedup\n- BigQuery partitioning and TTL\n- Dataflow autoscaling and schema evolution\n- Monitoring, testing (replay, chaos)\n\n## Code Example\n```javascript\n// Skeleton Dataflow-like pipeline\nfunction run(){\n  // Read Pub/Sub\n  // Parse JSON\n  // Window and deduplicate by message_id\n  // Write to BigQuery with region partitioning\n}\n```\n\n## Follow-up Questions\n- How would you handle out-of-order data across regions?\n- What are failure modes and how would you test rollback behavior?","diagram":null,"difficulty":"intermediate","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Google","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T14:49:53.093Z","createdAt":"2026-01-14T14:49:53.093Z"},{"id":"q-1912","question":"Design a beginner-friendly end-to-end GCP pipeline for user avatars stored in Cloud Storage. On object finalize, a Cloud Function should invoke a Cloud Run service to produce two thumbnails (100x100 and 256x256), store them in avatars-resized with deterministic naming, and publish a log entry. If processing fails, publish a Pub/Sub alert and quarantine the original file. Keep IAM minimal and describe a basic test plan and idempotency strategy?","answer":"Use a Cloud Storage finalize trigger to invoke a Cloud Function, which calls a Cloud Run thumbnail service to generate 100x100 and 256x256 images. Output to avatars-resized/<orig>_100_<hash>.jpg and <","explanation":"## Why This Is Asked\nThis tests practical event-driven design with Cloud Storage, Cloud Run, and Cloud Functions, plus idempotent output naming and failure handling.\n\n## Key Concepts\n- Event-driven primitives: storage finalize, function invocation, HTTP calls to Cloud Run\n- Idempotency via deterministic filenames and pre-write checks\n- Observability and alerting via Pub/Sub and Cloud Logging\n- IAM least privilege for service accounts\n\n## Code Example\n```\n// Node.js Cloud Function (handleAvatarUpload)\nexports.handleAvatarUpload = async (event) => {\n  // parse file, compute hash, call Cloud Run with image, await result\n};\n```\n\n## Follow-up Questions\n- How would you scale to more sizes without many Run invocations?\n- How would you handle large file uploads or retries without duplicate work?","diagram":"flowchart TD\n  A[Cloud Storage: finalize] --> B[Cloud Function: trigger]\n  B --> C[Cloud Run: thumbnail service]\n  C --> D[avatars-resized bucket]\n  B --> E[Pub/Sub: alerts]","difficulty":"beginner","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","Twitter","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T16:57:24.361Z","createdAt":"2026-01-14T16:57:24.361Z"},{"id":"q-2061","question":"Design an end-to-end cross-region streaming data pipeline on GCP to ingest high-volume telemetry from Pub/Sub into BigQuery with real-time dashboards. Use Dataflow (Beam) for streaming, enable exactly-once processing, implement automatic primary/DR region failover, use multi-region Pub/Sub topics and cross-region BigQuery datasets, handle schema evolution, and provide monitoring and rollback strategies?","answer":"Design a two-region streaming pipeline with primary deployment in us-central1 and disaster recovery in europe-west1. Utilize Pub/Sub multi-region topics for cross-region message distribution, implement Dataflow jobs with exactly-once processing semantics, and establish deduplication using event_id through stateful DoFn operations. Write transformed data to cross-region BigQuery datasets with automatic schema evolution support, enable real-time dashboards via Looker or Data Studio connections, and implement comprehensive monitoring through Cloud Monitoring alerts and automated rollback capabilities using deployment templates.","explanation":"## Why This Is Asked\nAssesses real-world cross-region streaming architecture design, exactly-once processing semantics, disaster recovery failover mechanisms, and operational governance practices.\n\n## Key Concepts\n- Cross-region Pub/Sub topics and BigQuery datasets for geographic redundancy\n- Apache Beam Dataflow exactly-once processing guarantees\n- Schema evolution governance and backward compatibility\n- Comprehensive observability and automated rollback strategies\n\n## Code Example\n\n```python\nimport apache_beam as beam\nfrom apache_beam.options.pipeline_options import PipelineOptions\n\nclass Deduplicate(beam.DoFn):\n    def __init__(self):\n        self.seen = set()\n    \n    def process(self, event):\n        if event['event_id'] in self.seen:\n            return\n        self.seen.add(event['event_id'])\n        yield event\n\noptions = PipelineOptions(\n    streaming=True,\n    project='your-project',\n    region='us-central1',\n    job_name='cross-region-streaming'\n)\n```","diagram":"flowchart TD\n  A[Pub/Sub multi-region] --> B[Dataflow in primary]\n  B --> C[BigQuery (region A)]\n  A --> D[Dataflow in DR]\n  D --> E[BigQuery (region B)]","difficulty":"intermediate","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","IBM"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T05:39:55.295Z","createdAt":"2026-01-14T22:48:34.134Z"},{"id":"q-2100","question":"Design a beginner-friendly, event-driven data validation pipeline on GCP for daily event JSONs uploaded to Cloud Storage: on file finalize, a Cloud Function validates each record against a simple JSON Schema, quarantines invalid files, and writes valid records to a BigQuery table with partitioning by date and a deterministic write-ID to ensure idempotency; explain IAM bindings and a basic test plan?","answer":"Use a Cloud Storage onFinalize trigger. Validate each record with a lightweight JSON Schema in the Cloud Function; if valid, write to a partitioned BigQuery table using a deterministic write-id (date + file hash) for idempotency, and move invalid files to a quarantine bucket with Pub/Sub notifications.","explanation":"## Why This Is Asked\n\nTests the ability to design a practical, beginner-friendly data validation flow on GCP that handles real-world issues like schema drift, idempotent writes, and fault isolation.\n\n## Key Concepts\n\n- Event-driven workflows and Cloud Functions\n- Cloud Storage triggers and onFinalize semantics\n- JSON Schema validation and lightweight data validation\n- BigQuery partitioning and idempotent writes with deterministic IDs\n- Quarantine handling and Pub/Sub alerts\n- IAM least privilege and secure service-to-service access\n- Testing strategies: unit + end-to-end\n\n## Code Example\n\n```javascript\n// Cloud Function: validate-events\nconst { Storage } = require('@google-cloud/storage');\nconst { BigQuery } = require('@google-cloud/bigquery');\nconst crypto = require('crypto');\n\nconst storage = new Storage();\nconst bigquery = new BigQuery();\n\nexports.validateEvents = async (event, context) => {\n  const fileName = context.eventId;\n  const bucketName = event.bucket;\n  \n  // Download and validate file\n  const file = storage.bucket(bucketName).file(fileName);\n  const contents = await file.download();\n  const records = JSON.parse(contents.toString());\n  \n  const validRecords = [];\n  const invalidRecords = [];\n  \n  // Validate each record against JSON Schema\n  for (const record of records) {\n    if (validateRecord(record)) {\n      validRecords.push(record);\n    } else {\n      invalidRecords.push(record);\n    }\n  }\n  \n  // Write valid records to BigQuery\n  if (validRecords.length > 0) {\n    const writeId = generateDeterministicId(fileName, new Date());\n    await writeToBigQuery(validRecords, writeId);\n  }\n  \n  // Move invalid files to quarantine\n  if (invalidRecords.length > 0) {\n    await moveToQuarantine(file, invalidRecords);\n  }\n};\n\nfunction generateDeterministicId(fileName, date) {\n  const hash = crypto.createHash('md5').update(fileName).digest('hex');\n  return `${date.toISOString().split('T')[0]}_${hash}`;\n}\n\nfunction validateRecord(record) {\n  // Simple JSON Schema validation\n  const schema = {\n    type: 'object',\n    required: ['id', 'timestamp', 'event_type'],\n    properties: {\n      id: { type: 'string' },\n      timestamp: { type: 'string', format: 'date-time' },\n      event_type: { type: 'string' }\n    }\n  };\n  \n  return validateAgainstSchema(record, schema);\n}\n\nasync function writeToBigQuery(records, writeId) {\n  const dataset = bigquery.dataset('events_dataset');\n  const table = dataset.table('events_table');\n  \n  const rows = records.map(record => ({\n    ...record,\n    write_id: writeId,\n    partition_date: new Date().toISOString().split('T')[0]\n  }));\n  \n  await table.insert(rows);\n}\n\nasync function moveToQuarantine(file, invalidRecords) {\n  const quarantineBucket = storage.bucket('events-quarantine');\n  await file.move(quarantineBucket.file(file.name));\n  \n  // Send Pub/Sub notification\n  const pubsub = new PubSub();\n  await pubsub.topic('validation-failures').publish(Buffer.from(JSON.stringify({\n    file: file.name,\n    invalidCount: invalidRecords.length,\n    timestamp: new Date().toISOString()\n  })));\n}\n```\n\n## IAM Bindings\n\n```yaml\n# Cloud Function Service Account\n- role: roles/cloudfunctions.serviceAgent\n  member: serviceAccount:validate-events@project.iam.gserviceaccount.com\n\n# Storage Access\n- role: roles/storage.objectViewer\n  member: serviceAccount:validate-events@project.iam.gserviceaccount.com\n  resource: projects/_/buckets/events-input\n\n- role: roles/storage.objectAdmin\n  member: serviceAccount:validate-events@project.iam.gserviceaccount.com\n  resource: projects/_/buckets/events-quarantine\n\n# BigQuery Access\n- role: roles/bigquery.dataEditor\n  member: serviceAccount:validate-events@project.iam.gserviceaccount.com\n  resource: projects/project/datasets/events_dataset\n\n# Pub/Sub Access\n- role: roles/pubsub.publisher\n  member: serviceAccount:validate-events@project.iam.gserviceaccount.com\n  resource: projects/project/topics/validation-failures\n```\n\n## Test Plan\n\n### Unit Tests\n- Test JSON Schema validation with valid/invalid records\n- Test deterministic ID generation\n- Test BigQuery row formatting\n\n### Integration Tests\n- Test end-to-end flow with sample files\n- Test quarantine bucket movement\n- Test Pub/Sub notification publishing\n\n### Manual Tests\n- Upload valid JSON file → verify BigQuery insertion\n- Upload invalid JSON file → verify quarantine movement\n- Upload mixed file → verify partial processing\n\n```bash\n# Deploy and test\ngcloud functions deploy validate-events --runtime nodejs14 --trigger-bucket events-input\n\n# Test with valid file\necho '[{\"id\":\"1\",\"timestamp\":\"2023-01-01T00:00:00Z\",\"event_type\":\"click\"}]' > valid.json\ngsutil cp valid.json gs://events-input/\n\n# Test with invalid file\necho '[{\"id\":\"1\",\"event_type\":\"click\"}]' > invalid.json\ngsutil cp invalid.json gs://events-input/\n```","diagram":"flowchart TD\n  A[Cloud Storage: onFinalize] --> B[Cloud Function: validate]\n  B --> C{Valid?}\n  C -->|Yes| D[BigQuery: insert with id]\n  C -->|No| E[Quarantine bucket & Pub/Sub alert]","difficulty":"beginner","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Databricks","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T05:01:21.623Z","createdAt":"2026-01-15T02:15:17.066Z"},{"id":"q-2214","question":"You run a SaaS platform on GCP and collect telemetry per tenant via Pub/Sub. Design an end-to-end, multi-region ingestion and analytics pipeline that ensures tenant isolation, exactly-once processing, and scalable cost control. Requirements: (a) topic/subscription layout with a central ingress project and per-tenant downstream sinks; (b) Dataflow streaming job that deduplicates using event_id, enriches with metadata from a centralized store, and writes to daily-partitioned BigQuery tables per tenant; (c) reliable dead-letter handling; (d) security (IAM, CMEK, and VPC Service Controls); (e) cross-region DR and testing plan. Provide a diagram?","answer":"Implement a streaming ingest: per-tenant Pub/Sub topics in a central ingress project feed into a Dataflow streaming job that deduplicates on event_id, enriches with tenant metadata, and writes to dail","explanation":"## Why This Is Asked\nDesigning a scalable, secure, multi-tenant pipeline with strong guarantees is critical for production SaaS on GCP.\n\n## Key Concepts\n- Pub/Sub topic/subscription topology for multi-tenancy and isolation\n- Dataflow streaming enforces dedupe and exactly-once writes\n- BigQuery per-tenant, daily-partitioned datasets\n- Dead-letter handling, CMEK, IAM scoping, and VPC Service Controls\n- Cross-region replication and DR strategies\n\n## Code Example\n```python\n# Pseudocode: Dataflow DoFn dedups by event_id and writes with insertId\nfrom apache_beam import DoFn\nclass DedupEnrich(DoFn):\n    def process(self, element, *args, **kwargs):\n        event_id = element['event_id']\n        # check and write to BigQuery using insertId=event_id\n        yield element\n```\n\n## Follow-up Questions\n- How would you validate idempotent behavior and monitor late or out-of-order events?\n- How would you evolve tenant schemas without breaking existing pipelines?","diagram":"flowchart TD\n  Ingest[Pub/Sub Ingress] --> Dataflow[Dataflow Streaming]\n  Dataflow --> BigQuery[BigQuery per-tenant, daily partitions]\n  Ingest --> DeadLetter[DLQ (Cloud Storage)]\n  BigQuery --> DR[Cross-region replication/DR]","difficulty":"advanced","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Discord","IBM"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T07:42:10.589Z","createdAt":"2026-01-15T07:42:10.589Z"},{"id":"q-2245","question":"Design a real-time, multi-tenant clickstream pipeline on Google Cloud Platform. Ingest events per-tenant from Pub/Sub, enrich with product catalog data stored in BigQuery, compute 5-minute per-tenant engagement metrics, and persist both raw enriched events and per-tenant aggregates to BigQuery. Address exactly-once semantics, late data, per-tenant IAM, data cataloging, and failure handling with a dead-letter queue?","answer":"Explain a streaming Dataflow (Beam) pipeline for per-tenant clickstreams: Pub/Sub input per-tenant, enrichment with catalog data from BigQuery, fixed 5-minute windows with 2-minute lateness, raw enric","explanation":"## Why This Is Asked\nReal-world multi-tenant pipelines demand strict isolation, predictable timing semantics, and robust failure handling. This question probes dataflow design choices, stateful enrichment, and secure data ownership.\n\n## Key Concepts\n- Apache Beam / Dataflow for streaming pipelines\n- Pub/Sub as a per-tenant event ingress\n- BigQuery as both enrichment source and sinks\n- Exactly-once semantics and idempotent writes (insertId) \n- Windowing and late data handling (5-min fixed windows, 2-min lateness)\n- Dead-letter queue for enrichment failures\n- CMEK and per-tenant IAM\n\n## Code Example\n```python\nimport apache_beam as beam\nfrom apache_beam.options.pipeline_options import PipelineOptions\nimport json\n\nclass EnrichDoFn(beam.DoFn):\n  def __init__(self, catalog_table):\n    self.catalog_table = catalog_table\n  def process(self, element, catalog):\n    e = json.loads(element)\n    prod = catalog.get(e[\"product_id\"], {})\n    e[\"product_info\"] = prod\n    yield e\n\n# Placeholder for pipeline construction; details omitted for brevity\n```\n\n## Follow-up Questions\n- How would you test idempotency and replay safety across restarts?\n- What monitoring and alerting would you implement for late data and failed enrichments?","diagram":null,"difficulty":"intermediate","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Plaid","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T09:02:07.711Z","createdAt":"2026-01-15T09:02:07.711Z"},{"id":"q-2365","question":"Design a multi-tenant data ingestion and analytics pipeline on GCP for three customers (Snap, Nvidia, Zoom). Each tenant streams events to Pub/Sub, which Dataflow consumes and writes to a central BigQuery data lake with per-tenant isolation using CMEK-encrypted Cloud Storage staging and BigQuery Authorized Views/Row-Level Security. Include idempotent processing, audit logging, cost controls, and a test plan?","answer":"Ingest via Pub/Sub, process with Dataflow to a central BigQuery lake, enforce tenant isolation with CMEK-encrypted Cloud Storage staging and per-tenant Authorized Views/row-level security in BigQuery.","explanation":"## Why This Is Asked\nThis question probes ability to design scalable, secure, multi-tenant data pipelines on GCP, balancing isolation, cost control, and reliability in real-world enterprises.\n\n## Key Concepts\n- Pub/Sub and Dataflow streaming, exactly-once or dedup strategies\n- BigQuery partitioning, per-tenant isolation with Authorized Views and row-level security\n- CMEK for Cloud Storage staging, IAM least privilege\n- Audit logging, monitoring, cost controls, testing strategies\n\n## Code Example\n```javascript\n// Deduplication helper (conceptual)\nfunction isDuplicate(event, seen) {\n  const key = event.event_id + '|' + event.event_time;\n  if (seen.has(key)) return true;\n  seen.add(key);\n  return false;\n}\n```\n\n## Follow-up Questions\n- How would you validate idempotency in production?\n- What rollback strategy for failed batches might you implement?","diagram":"flowchart TD\n  PubSub[Pub/Sub] --> Dataflow[Dataflow]\n  Dataflow --> BigQuery[BigQuery]\n  BigQuery --> Views[Authorized Views]\n  BigQuery --> CMEK[Per-tenant CMEK]\n  CMEK --> Storage[Cloud Storage staging]","difficulty":"advanced","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["NVIDIA","Snap","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T14:55:02.883Z","createdAt":"2026-01-15T14:55:02.883Z"},{"id":"q-2438","question":"Design a cross-region, multi-tenant analytics pipeline on GCP that ingests streaming user events from Cloud Pub/Sub, processes with Dataflow, and writes per-tenant BigQuery datasets in the region of arrival. Include strict data isolation via IAM and VPC Service Controls, encryption (CMEK), backup, failover, and a canary rollout plan for tenants?","answer":"Use a streaming Dataflow template that subscribes to region-local Pub/Sub topics, processes per-tenant events with fixed windows, and writes to per-tenant BigQuery datasets in the same region using CM","explanation":"## Why This Is Asked\nTests ability to design a scalable, secure, multi-region analytics pipeline with tenant isolation and DR readiness, a common real-world requirement for SaaS platforms.\n\n## Key Concepts\n- Dataflow templates and per-tenant partitioning\n- Pub/Sub regionalization and data localization\n- BigQuery datasets per tenant with CMEK\n- IAM, VPC Service Controls, and cross-region DR\n- Canary rollout, DLQ, and observability\n\n## Code Example\n```python\nimport apache_beam as beam\nimport json\n\ndef parse(msg):\n    x = json.loads(msg.decode('utf-8'))\n    return x['tenant_id'], x\n\nwith beam.Pipeline(options=...) as p:\n    events = (\n        p\n        | 'Read' >> beam.io.ReadFromPubSub(topic='projects/..../topics/region-events')\n        | 'Parse' >> beam.Map(lambda b: parse(b))\n        | 'Window' >> beam.WindowInto(beam.window.FixedWindows(60))\n        | 'WriteBQ' >> beam.io.WriteToBigQuery(\n            table=lambda elem: f\"myproj:tenant_{elem[0]}.events\",\n            schema='SCHEMA_AUTODETECT',\n            write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND)\n    )\n```\n\n## Follow-up Questions\n- How would you verify DR failover consistency and data freshness across regions?\n- What are cost implications of per-tenant datasets and cross-region replication?\n- How would you enforce tenant onboarding/offboarding and access revocation without downtime?","diagram":null,"difficulty":"advanced","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","DoorDash","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T17:55:48.397Z","createdAt":"2026-01-15T17:55:48.397Z"},{"id":"q-2549","question":"You receive telemetry events from a mobile app into Pub/Sub. Design a beginner-friendly end-to-end ingestion on GCP: a Pub/Sub topic telemetry, a Cloud Run HTTP push endpoint processes messages, validates JSON schema, and writes to BigQuery telemetry.events using streaming inserts with insertId = event_id to guarantee idempotency. Include IAM bindings, retry strategy, and a basic test plan?","answer":"Publish telemetry events to a Pub/Sub topic, configure a push subscription to deliver messages to a Cloud Run HTTP endpoint, validate the JSON schema, and perform streaming inserts into BigQuery's telemetry.events table using insertId = event_id to guarantee idempotency. IAM: grant Pub/Sub Publisher role on the topic and Cloud Run Invoker role on the service, plus BigQuery Data Editor on the dataset. Implement exponential backoff retry strategy and create a test plan covering schema validation, idempotency verification, and error handling.","explanation":"## Why This Is Asked\nTests understanding of Pub/Sub push subscriptions to Cloud Run, idempotent BigQuery writes, and principle of least privilege IAM.\n\n## Key Concepts\n- Pub/Sub push subscriptions with Cloud Run endpoints\n- BigQuery streaming inserts with insertId for idempotency\n- IAM least privilege bindings\n- Retry strategies and error handling\n- Testing schema validation and idempotency\n\n## Code Example\n```javascript\nconst {BigQuery} = require('@google-cloud/bigquery');\nconst bigquery = new BigQuery();\n\nexports.ingestTelemetry = async (req, res) => {\n  const msg = JSON.parse(Buffer.from(req.body.message.data, 'base64').toString());\n  \n  // Validate schema\n  if (!msg.event_id || !msg.timestamp || !msg.payload) {\n    return res.status(400).json({error: 'Invalid schema'});\n  }\n  \n  // Insert to BigQuery with idempotency\n  await bigquery.dataset('telemetry').table('events').insert({\n    rows: [{json: msg}],\n    insertId: msg.event_id\n  });\n  \n  res.status(200).send();\n};\n```\n\n## Test Plan\n1. **Schema Validation**: Send malformed JSON and verify 400 response\n2. **Idempotency**: Send duplicate events with same event_id and verify single row\n3. **Error Handling**: Test retry behavior with temporary failures\n4. **End-to-End**: Verify data appears correctly in BigQuery","diagram":null,"difficulty":"beginner","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Amazon","Databricks"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T05:32:35.819Z","createdAt":"2026-01-15T22:38:56.559Z"},{"id":"q-2643","question":"Design a beginner-friendly ingestion pipeline on GCP for mobile app analytics. When a JSONL file lands in Cloud Storage (analytics-logs), a Cloud Function validates each event, de-duplicates by event_id, and appends to a date-partitioned BigQuery table analytics.events. Use insertId for idempotency. Minimal IAM, exponential backoff retries, and a basic test plan with sample files and duplicates?","answer":"Event-driven ingestion: Cloud Storage finalization triggers a Cloud Function that validates event schema, upserts with insertId=event_id to BigQuery analytics.events partitioned by date, and logs fail","explanation":"## Why This Is Asked\nTests practical, beginner-friendly edge cases: event validation, idempotent writes, and simple error handling in GCP.\n\n## Key Concepts\n- Cloud Storage triggers, Cloud Functions\n- BigQuery partitioned tables and insertId deduping\n- IAM least privilege and service accounts\n- Retries, backoff, and failure monitoring\n- Test strategy with sample data\n\n## Code Example\n```javascript\nconst {Storage} = require('@google-cloud/storage');\nconst {BigQuery} = require('@google-cloud/bigquery');\nexports.ingestAnalytics = async (data, ctx) => {\n  // simplified: read lines, validate, upsert with insertId\n};\n```\n\n## Follow-up Questions\n- How would you validate schema changes?\n- How would you test idempotency with duplicates?","diagram":null,"difficulty":"beginner","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","DoorDash"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T04:25:27.430Z","createdAt":"2026-01-16T04:25:27.431Z"},{"id":"q-2667","question":"Design a real-time telemetry pipeline on Google Cloud Platform for a multi-region ride-hailing fleet that ingests vehicle events from Pub/Sub, deduplicates by event_id, performs streaming enrichment and 1-minute window aggregations in Dataflow, and writes to a region-partitioned BigQuery table with CMEK. Include raw archival in Cloud Storage, cross-region failover, IAM, and a test plan?","answer":"In this pipeline, ingest telemetry from Pub/Sub with per-event dedup by event_id; a Dataflow streaming job enriches events and computes 1-min windows; write to a region-partitioned BigQuery table tele","explanation":"## Why This Is Asked\nThis question tests end-to-end real-time pipeline design with data governance, multi-region resiliency, and secure encryption, common at top-tier firms.\n\n## Key Concepts\n- Pub/Sub ingestion with dedup by event_id\n- Dataflow streaming with enrichment and windowing\n- BigQuery partitioning by region/date and CMEK\n- Cloud Storage archival and audit trails\n- Cross-region failover and least-privilege IAM\n\n## Code Example\n```yaml\n# Dataflow + BigQuery config (conceptual)\ndataflow:\n  streaming: true\n  dedup_by_event_id: true\n  sink:\n    bigquery:\n      table: project.dataset.telemetry_events\n      partition_by: region,date\n      encryption: CMEK\ngcs:\n  audit_bucket: gs://telemetry-audit/raw\niam:\n  bindings:\n    - role: roles/bigquery.jobUser\n      members:\n        - serviceAccount:dataflow-sa@project.iam.gserviceaccount.com\n```\n\n## Follow-up Questions\n- How would you test idempotency and data drift in this pipeline?\n- What monitoring dashboards and SLAs would you implement?","diagram":"flowchart TD\n  PS[Pub/Sub: telemetry.events] --> DF[Dataflow: streaming]\n  DF --> BQ[BigQuery: telemetry.events partitioned by region/date]\n  DF --> GS[Cloud Storage: raw events for audit]\n  BQ --> CMEK[Encrypted with CMEK]\n  GS --> AUD[Audit/search logs]\n  DF --> MON[Monitoring & Alerts]","difficulty":"advanced","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Two Sigma","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T05:50:33.852Z","createdAt":"2026-01-16T05:50:33.852Z"},{"id":"q-2728","question":"Design a real-time feature store pipeline on GCP for a fraud-detection model. Ingest 120k events/sec from multiple services into Pub/Sub, stream to Vertex AI Feature Store online and BigQuery for offline features, with cross-region replication, CMEK, and IAM least privilege. Include an idempotent write strategy, testing plan, and a drift-monitoring approach with alerting within 5 minutes?","answer":"An end-to-end real-time feature store for fraud detection: ingest via Pub/Sub at 120k/s, stream with Dataflow to Vertex AI Feature Store online and BigQuery offline; enable cross-region replication an","explanation":"## Why This Is Asked\n\nThe scenario probes real-time ML data pipelines, the separation of online/offline features, data governance, and cross-region resiliency, with encryption and IAM controls in play.\n\n## Key Concepts\n\n- Vertex AI Feature Store\n- Streaming ingestion: Pub/Sub, Dataflow\n- Online vs Offline stores\n- Cross-region replication and CMEK\n- IAM least privilege and service accounts\n- Data drift monitoring and alerting\n\n## Code Example\n\n```python\n# Pseudo setup for Feature Store with online store and entity type\nfrom google.cloud import aiplatform\naiplatform.init(project=\"my-proj\", location=\"us-central1\")\nfs = aiplatform.Featurestore(name=\"projects/.../locations/.../featurestores/ff\")\n# Define an entity type and features (illustrative)\n```\n\n## Follow-up Questions\n\n- How would you ensure idempotent writes across retries and duplicate events?\n- How would you test drift detection and schema evolution in production?","diagram":"flowchart TD\n  A[Clients] --> B[Pub/Sub]\n  B --> C[Dataflow]\n  C --> D(Vertex AI Feature Store Online)\n  C --> E(BigQuery Offline)\n  D --> F[Cross-Region Replication]\n  F --> G[Monitoring & Alerts]","difficulty":"advanced","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","Oracle","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T09:40:50.697Z","createdAt":"2026-01-16T09:40:50.697Z"},{"id":"q-2807","question":"Describe a beginner-friendly GCP ingestion workflow: CSV logs arrive in Cloud Storage at logs-origin; a Cloud Function validates the header schema, parses rows, and deduplicates by a per-row key; inserts into a date-partitioned BigQuery table logs.events using insertId for idempotency. Include minimal IAM bindings, exponential backoff retries, and a test plan with sample files and duplicates?","answer":"Describe a Cloud Function triggered on finalize of a CSV in logs-origin that validates headers, filters invalid rows, and deduplicates by a per-row key. It should insert into a date-partitioned BigQue","explanation":"## Why This Is Asked\n\nAssess practical understanding of GCP ingestion patterns, idempotent writes, and minimal-privilege design.\n\n## Key Concepts\n\n- Cloud Functions triggers on Cloud Storage finalize\n- CSV header/schema validation\n- Per-row deduplication\n- BigQuery partitioned tables and streaming inserts\n- insertId for idempotency\n- IAM least privilege and retries\n\n## Code Example\n\n```python\n# skeleton Cloud Function (Python)\nfrom google.cloud import bigquery\n\ndef ingest_csv(event, context):\n    # parse bucket/object from event, validate headers\n    # stream insert rows with insertId\n    pass\n```\n\n## Follow-up Questions\n\n- How would you test idempotency with duplicate rows?\n- What changes for large CSVs to avoid cold starts?","diagram":"flowchart TD\n  A[Cloud Storage: logs-origin] --> B[Cloud Function: Validate CSV]\n  B --> C{Valid}\n  C -->|Yes| D[BigQuery: logs.events partitioned by date]\n  C -->|No| E[Pub/Sub: errors]","difficulty":"beginner","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Apple","Instacart"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T13:11:28.597Z","createdAt":"2026-01-16T13:11:28.597Z"},{"id":"q-2826","question":"Design an intermediate-level, end-to-end GCP streaming ingestion for telemetry data from 10k devices. Devices push JSON messages to a Cloud Pub/Sub topic. Build an idempotent pipeline using Dataflow (Beam) that deduplicates by event_id, aggregates per minute per device_type, and writes results to a partitioned BigQuery table telemetry.events (date partition). Include exactly-once considerations, dead-letter handling, and IAM least privilege. Provide testing plan?","answer":"Design a streaming telemetry pipeline: devices send JSON to Pub/Sub with exactly-once delivery; a Dataflow (Beam) job deduplicates on event_id, windows by 1 minute, computes per-device_type sums, and ","explanation":"## Why This Is Asked\n\nThis question probes practical streaming architecture in GCP, focusing on exactly-once semantics, deduplication, windowed aggregations, and error handling, all with real-world IAM constraints.\n\n## Key Concepts\n\n- Pub/Sub exactly-once delivery\n- Dataflow/Beam windowing and dedup by event_id\n- BigQuery partitioned tables for time-partitioned analytics\n- Dead-letter queues and message retries\n- IAM least privilege and service accounts\n\n## Code Example\n\n```python\n# PyBeam skeleton for ingestion, dedupe, and windowed aggregation\nimport apache_beam as beam\nfrom apache_beam.options.pipeline_options import PipelineOptions\n\ndef run():\n    p = beam.Pipeline(options=PipelineOptions(streaming=True))\n    (p\n     | 'ReadFromPubSub' >> beam.io.ReadFromPubSub(subscription='projects/PROJECT/subscriptions/SUB')\n     | 'ParseJson' >> beam.Map(lambda x: json.loads(x))\n     | 'Deduplicate' >> beam.Filter(lambda m: m['event_id'] not in seen)\n     | 'Window' >> beam.WindowInto(beam.window.SlidingWindows(60, 60))\n     | 'KeyByDevice' >> beam.Map(lambda m: (m['device_type'], m['value']))\n     | 'Sum' >> beam.CombinePerKey(sum)\n     | 'ToRow' >> beam.Map(lambda kv: {'device_type': kv[0], 'sum': kv[1], 'dt': ...})\n     | 'WriteToBQ' >> beam.io.WriteToBigQuery('project:dataset.telemetry_events', schema='...', write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND)\n    )\n    p.run()\n```\n\n## Follow-up Questions\n\n- How would you handle late-arriving data and watermarking? \n- How would you test idempotency and failure scenarios? \n- What monitoring metrics would you set up in Cloud Monitoring for SLAs?","diagram":"flowchart TD\n  A[Devices] --> B[Pub/Sub] \n  B --> C[Dataflow (Beam)] \n  C --> D[BigQuery: telemetry_events] \n  D --> E[Monitoring & Alerts]","difficulty":"intermediate","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T14:00:51.738Z","createdAt":"2026-01-16T14:00:51.738Z"},{"id":"q-3014","question":"Design a beginner-friendly privacy-conscious ingestion workflow on GCP for CSV analytics logs. When a CSV file lands in gs://analytics-logs/csv/, a Cloud Function validates the header and redacts PII fields (emails and phone numbers) in every row, writing a sanitized file to gs://redacted-logs/csv/ with the same name. A second Cloud Function loads the sanitized file into a date-partitioned BigQuery table analytics.logs, using a deterministic loadId for idempotency. Outline IAM least privilege, a basic test plan with sample input including PII, and how you would verify redaction and idempotent loads?","answer":"Two Cloud Functions: the first validates the CSV header, redacts PII (emails and phone numbers) using regex patterns on every row, then writes the sanitized file to gs://redacted-logs/csv/ with the same filename. The second Cloud Function loads the sanitized file into a date-partitioned BigQuery table analytics.logs, using a deterministic loadId for idempotency. IAM follows least privilege principles: the first function has Storage Object Viewer on the source bucket and Storage Object Creator on the destination bucket; the second function has Storage Object Viewer on the redacted bucket and BigQuery Data Editor on the target table. The test plan includes sample CSV files containing PII data, header validation tests, redaction verification, and idempotent load verification by reprocessing the same file multiple times.","explanation":"## Why This Is Asked\nThis tests practical data sanitization, event-driven processing, and BigQuery loading under real-world constraints.\n\n## Key Concepts\n- Cloud Functions triggered by Cloud Storage events\n- PII redaction with regex patterns and data privacy fundamentals\n- Idempotent loads using deterministic IDs\n- Least-privilege IAM and minimal security surface area\n- End-to-end test planning with sample inputs\n\n## Code Example\n```javascript\nfunction redactRow(row) {\n  row.email = '[REDACTED_EMAIL]';\n  row.phone = '[REDACTED_PHONE]';\n  return row;\n}\n```\n\n## Follow-up Questions\n- How would you test idempotency for reprocessing the same file?\n- What monitoring and alerting would you implement for this workflow?\n- How would you handle schema evolution in the CSV files?","diagram":"flowchart TD\n  A[CSV arrives in gs://analytics-logs/csv/] --> B[Cloud Function: validate headers]\n  B --> C[Cloud Function: redact PII in rows]\n  C --> D[Write sanitized to gs://redacted-logs/csv/]\n  D --> E[Cloud Function: load to BigQuery analytics.logs]\n  E --> F[Partition by date; loadId ensures idempotency]","difficulty":"beginner","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","NVIDIA","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T06:04:00.945Z","createdAt":"2026-01-16T21:33:14.167Z"},{"id":"q-3058","question":"Design a beginner-friendly nightly cleanup pipeline on GCP. In bucket gs://user-uploads/temp, files older than 7 days should be moved to gs://archive/YYYY/MM/DD/ preserving original name, and the original should be deleted. Triggered by Cloud Scheduler; Cloud Function performs listing, copy, delete, and logs a row in BigQuery with insertId = filePath+timestamp. Include minimal IAM, retries, and a test plan with sample files and duplicates?","answer":"Nightly cleanup: Cloud Scheduler triggers a Cloud Function that scans gs://user-uploads/temp for files older than 7 days, copies each to gs://archive/YYYY/MM/DD/ with the original name, deletes the source file, and logs the operation to BigQuery using insertId = filePath+timestamp for idempotency.","explanation":"## Why This Is Asked\nTests practical familiarity with serverless scheduling and object lifecycle in GCP, plus deduplication and idempotency basics. It covers Cloud Scheduler, Cloud Functions, Cloud Storage, and BigQuery integration in a small, repeatable workflow.\n\n## Key Concepts\n- Cloud Scheduler and Cloud Functions integration\n- GCS object lifecycle with time-based filtering\n- Idempotent design with insertId and existence checks\n- Minimal IAM and retry strategies\n\n## Code Example\n```javascript\n// Pseudo Cloud Function outline (Node.js)\nexports.cleanupOldUploads = async (event, context) => {","diagram":"flowchart TD\n  A[Scheduler triggers] --> B[Function lists old files]\n  B --> C{Exists in archive?}\n  C -- Yes --> D[Skip]\n  C -- No --> E[Copy+Delete to archive]\n  E --> F[BigQuery log with insertId]","difficulty":"beginner","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Apple","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T05:19:16.794Z","createdAt":"2026-01-16T22:48:49.749Z"},{"id":"q-3090","question":"Design a real-time, cross-region streaming pipeline on GCP to replicate high-volume financial transactions from a primary OLTP DB to an analytics warehouse. Explain ingestion (CDC from Cloud SQL using Debezium on GKE), Pub/Sub topics, Dataflow streaming with deduplication and exactly-once semantics, enrichment, and error handling; store into regional BigQuery datasets, with cross-region DR and automatic failover; outline IAM, DLQ, cost controls, and a practical test plan including outage simulations?","answer":"Use Debezium CDC on GKE to publish Cloud SQL transactions to Pub/Sub; Dataflow streaming deduplicates by transaction_id and uses Pub/Sub IDs and BigQuery insertId for exactly-once semantics; enrich with reference data from Cloud Spanner, write to regional BigQuery datasets with cross-region replication; implement DLQ for failed messages, IAM least-privilege service accounts, and cost controls via BigQuery slot reservations and Pub/Sub quotas; test with chaos engineering including simulated outages and failover validation.","explanation":"## Why This Is Asked\nThis question tests real-time CDC pipelines, cross-region disaster recovery, idempotency, and cost control in GCP, aligning with security and operational concerns of enterprise organizations.\n\n## Key Concepts\n- CDC with Debezium and GKE\n- Pub/Sub semantics and dead-letter queues\n- Dataflow streaming, exactly-once processing, and deduplication\n- BigQuery regional datasets and disaster recovery strategy\n- IAM least privilege, service accounts, and monitoring\n\n## Code Example\n```python\nimport apache_beam as beam\nfrom apache_beam.options.pipeline_options import PipelineOptions\n\nclass AddInsertId(beam.DoFn):\n    def process(self, element, timestamp=beam.DoFn.TimeParam):\n        element['insertId'] = element.get('transaction_id')\n        yield element\n\nwith beam.Pipeline(options=PipelineOptions()) as p:\n    (p | 'ReadFromPubSub' >> beam.io.ReadFromPubSub()\n       | 'AddInsertId' >> beam.ParDo(AddInsertId())\n       | 'WriteToBigQuery' >> beam.io.WriteToBigQuery(\n           table='project:dataset.transactions',\n           schema='transaction_id:STRING, amount:FLOAT, timestamp:TIMESTAMP',\n           write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND,\n           create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED))\n```","diagram":"flowchart TD\n  A[CDC Source: Cloud SQL] --> B[Debezium on GKE]\n  B --> C[Pub/Sub: transactions]\n  C --> D[Dataflow: streaming with dedupe]\n  D --> E[BigQuery: regional datasets]\n  E --> F[Cross-region DR & failover]\n  C --> G[Pub/Sub Dead Letter Queue]","difficulty":"advanced","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Snap","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T05:01:21.851Z","createdAt":"2026-01-17T02:14:53.192Z"},{"id":"q-3143","question":"Advanced: design a production-ready IoT telemetry ingestion pipeline on GCP with strict data isolation and exactly-once guarantees. Devices upload JSONL to Cloud Storage at gs://telemetry-origin; a Cloud Function validates schema and forwards records to Pub/Sub; a Dataflow streaming job consumes Pub/Sub, deduplicates by event_id across tenants, and writes to a per-tenant partitioned BigQuery table telemetry.events using insertId for idempotency. Include IAM bindings, retries,-VPC Service Controls considerations, and a test plan with representative data including duplicates?","answer":"Deploy a Cloud Function to validate incoming JSONL schema and publish records to Pub/Sub, then a Dataflow streaming job reads Pub/Sub, deduplicates on event_id across tenants, and writes to a per-tena","explanation":"## Why This Is Asked\n\nTests ability to design end-to-end data pipelines with strict data isolation, exactly-once semantics, and multi-tenant concerns. Evaluates schema validation, event deduplication, and proper boundary controls.\n\n## Key Concepts\n\n- End-to-end data flow: Cloud Storage -> Cloud Function -> Pub/Sub -> Dataflow -> BigQuery\n- Exactly-once ingestion using insertId and Pub/Sub/Dataflow guarantees\n- Multi-tenant isolation: per-tenant datasets or partitioning, IAM boundaries\n- Operational concerns: retries, backoff, error handling, testing with duplicates\n- Security: least-privilege IAM, VPC Service Controls\n\n## Code Example\n\n```python\n# Apache Beam-like pseudocode for deduplication concept\nimport apache_beam as beam\n\nclass DeduplicateEvents(beam.DoFn):\n  def process(self, element, event_id=beam.DoFn.StateParam('event_id')):\n    if not event_id.read():\n      event_id.write(True)\n      yield element\n```\n\n## Follow-up Questions\n\n- How would you implement schema evolution handling in the Cloud Function and Dataflow\n- What monitoring or alerting would you add for data skew or late arrivals","diagram":"flowchart TD\n  A[Telemetry-origin] --> B[Cloud Function]\n  B --> C[Pub/Sub]\n  C --> D[Dataflow]\n  D --> E[BigQuery telemetry.events]","difficulty":"advanced","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","NVIDIA","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T04:43:03.104Z","createdAt":"2026-01-17T04:43:03.105Z"},{"id":"q-3307","question":"Design a beginner-friendly ingestion pipeline on GCP for server logs. When a file lands in gs://srv-logs/raw/server-logs-*.jsonl, a Cloud Function validates each JSON line (host, ts, level, msg); invalid lines go to gs://srv-logs/dlq/, valid lines are written to BigQuery via streaming insert into analytics.logs with insertId = host|ts|hash(msg). Include IAM, exponential backoff retries, and a basic test plan with duplicates?","answer":"Validate schema and idempotency at ingestion: a Cloud Function triggered by finalization of gs://srv-logs/raw/server-logs-*.jsonl parses each line, drops invalid ones to gs://srv-logs/dlq/, and stream","explanation":"## Why This Is Asked\nTests practical understanding of building a simple, reliable ingestion path with validation, DLQ handling, and idempotent writes using familiar GCP services.\n\n## Key Concepts\n- Cloud Storage event-driven triggers\n- NDJSON parsing and schema validation\n- Dead-letter queue design\n- BigQuery streaming inserts with insertId for idempotency\n- IAM least privilege and retry strategies\n\n## Code Example\n```javascript\nconst {BigQuery} = require('@google-cloud/bigquery');\nconst crypto = require('crypto');\n\nexports.ingestLogs = async (data, context) => {\n  // Pseudo-implementation: read file from GCS, line-split, validate fields\n  // For each valid line:\n  //   const insertId = host + '|' + ts + '|' + crypto.createHash('md5').update(msg).digest('hex');\n  //   await bigQueryClient.insert({ insertId, ...row }, { raw: true });\n  // Invalid lines go to DLQ bucket\n};\n```\n\n## Follow-up Questions\n- How would you test idempotency and failure scenarios?\n- How would you monitor DLQ growth and alert on repeated failures?","diagram":"flowchart TD\n  A[gs://srv-logs/raw/server-logs-*.jsonl] --> B[Cloud Function]\n  B --> C{Line valid?}\n  C -- Yes --> D[BigQuery analytics.logs with insertId]\n  C -- No --> E[gs://srv-logs/dlq/]\n","difficulty":"beginner","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Meta","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T10:42:50.818Z","createdAt":"2026-01-17T10:42:50.818Z"},{"id":"q-3344","question":"Design a real-time ingestion pipeline to process 1M user activity events per second from mobile clients. Events arrive as JSON in Pub/Sub and must be enriched (e.g., profile lookup), de-duplicated by event_id, and written to a date-partitioned BigQuery table analytics.events with insertId semantics. Propose architecture (Pub/Sub topics, Dataflow/Beam, BigQuery), late data handling, schema evolution strategy, error handling, and a minimal IAM model. Include a test plan with synthetic events and duplicates; justify trade-offs?","answer":"Use a real-time pipeline with Pub/Sub -> Dataflow (Beam) -> BigQuery. Ingest JSON events into Pub/Sub, Dataflow enriches using a cached profile lookup, deduplicates by event_id, and writes to a date-p","explanation":"## Why This Is Asked\nTests ability to design end-to-end real-time data pipelines at scale, covering ingestion, enrichment, deduplication, and storage with proper failure handling.\n\n## Key Concepts\n- Streaming ingestion with Pub/Sub and Dataflow (Beam)\n- Idempotent writes via BigQuery insertId\n- Deduplication strategy and late-arriving data handling\n- Schema evolution and minimal IAM controls\n\n## Code Example\n```javascript\n// Pseudo Beam transforms: read from Pub/Sub, enrich, dedupe by event_id, write to BigQuery with insertId\n```\n\n## Follow-up Questions\n- How would you monitor data skew and backpressure?\n- What changes for multi-region deployment and failover?","diagram":"flowchart TD\n  A[Mobile Client] --> B[Pub/Sub:raw]\n  B --> C[Dataflow: enrich & dedupe]\n  C --> D[BigQuery: analytics.events]\n  D --> E[Partitioned by date]","difficulty":"advanced","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Meta","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T13:02:10.346Z","createdAt":"2026-01-17T13:02:10.346Z"},{"id":"q-3466","question":"In GCP, you operate a real-time clickstream pipeline: Pub/Sub -> Dataflow streaming job computes session-level metrics (visit duration, pages per session) and writes to BigQuery. Data can arrive late and out-of-order. Design an end-to-end strategy to ensure timely yet accurate metrics: include watermarking, windowing, late data handling, dedup, idempotent writes to BigQuery, and testing with synthetic data. Provide concrete DoFn ideas and config choices?","answer":"Design a Dataflow streaming job consuming Pub/Sub click events to compute session metrics (duration, pages/session). Use event-time windows with allowed lateness, proper watermarking, and backlogs han","explanation":"## Why This Is Asked\nTests practical mastery of real-time data pipelines on GCP, focusing on correctness under late data and out-of-order events, not just theory.\n\n## Key Concepts\n- Dataflow streaming and event-time processing\n- Watermarks and allowed lateness\n- Stateful DoFn for deduplication\n- Idempotent BigQuery writes via insertId\n- Backfilling and late data strategy\n- Observability and testing with synthetic data\n\n## Code Example\n\n```python\n# Pseudo-code: dedup by event_id in a stateful DoFn\nimport apache_beam as beam\n\nclass DedupByEventId(beam.DoFn):\n  def process(self, element, timestamp=beam.DoFn.TimestampParam, state=beam.DoFn.StateParam(...)):\n    event_id = element['event_id']\n    if not state.read().contains(event_id):\n      state.add(event_id)\n      yield element\n```\n\n## Follow-up Questions\n- How would you monitor downstream latency and backlogs in Dataflow?\n- How would you handle schema evolution in BigQuery for the streamed metrics?","diagram":null,"difficulty":"intermediate","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Robinhood","Stripe","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T17:31:27.310Z","createdAt":"2026-01-17T17:31:27.310Z"},{"id":"q-3539","question":"Design a beginner-friendly GCP ingestion flow for daily activity dumps stored in gs://raw-activity/YYYY/MM/DD/logs.jsonl. Implement a Cloud Function (Python) triggered on finalization to validate lines, deduplicate by (user_id, event_id), and write to a date-partitioned BigQuery table activity.events using insertId. Add a Pub/Sub replay path and DLQ for failures; keep IAM minimal and provide a basic test plan with sample data?","answer":"I’d implement a Cloud Function (Python) triggered on finalization of gs://raw-activity/YYYY/MM/DD/logs.jsonl. It validates lines, deduplicates by (user_id,event_id), and writes to a date‑partitioned B","explanation":"Why This Is Asked\n- Tests Cloud Functions, BigQuery partitioning, idempotency, and end-to-end dataflow basics.\n- Covers error handling with DLQ and basic monitoring prerequisites.\n- Checks ability to design minimal IAM bindings for a real pipeline.\n\nKey Concepts\n- JSONL validation, dedup by composite key, insertId idempotency\n- Date-partitioned BigQuery ingestion\n- Pub/Sub as replay mechanism and dead-letter handling\n- Minimal IAM and basic test plan\n\nCode Example\n```python\n# skeleton Cloud Function (Python)\nimport json\n\ndef process(event, context):\n    lines = read_lines(event)\n    for line in lines:\n        rec = json.loads(line)\n        if not valid(rec):\n            publish_dlq(rec)\n            continue\n        if is_duplicate(rec):\n            continue\n        write_bigquery(rec)\n```\n\nFollow-up Questions\n- How would you scale this to multi-tenant data with per-tenant quotas?\n- How would you test schema evolution and backward compatibility?","diagram":null,"difficulty":"beginner","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Snap","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T20:38:57.031Z","createdAt":"2026-01-17T20:38:57.031Z"},{"id":"q-3609","question":"Design a fault-tolerant, multi-region ingestion pipeline for real-time clickstream analytics on GCP: events arrive as JSON in regional Pub/Sub topics; a Dataflow streaming job deduplicates by event_id and writes to per-region BigQuery datasets with insertId, while a centralized BigQuery dataset aggregates near-real-time metrics. Explain exactly-once semantics, cross-region replication, IAM boundaries, and a test plan including regional failover scenarios?","answer":"Implement a fault-tolerant, multi-region ingestion pipeline using regional Pub/Sub topics for event ingestion, Dataflow streaming jobs with stateful deduplication by event_id and BigQuery insertId for exactly-once writes to per-region datasets, and a centralized BigQuery dataset for near-real-time metric aggregation. Exactly-once semantics are achieved through Pub/Sub message deduplication, Dataflow checkpointing, and BigQuery's insertId mechanism. Cross-region replication leverages BigQuery dataset replication with IAM boundaries enforced through region-scoped service accounts following the principle of least privilege.","explanation":"## Why This Is Asked\nThis question evaluates expertise in building production-grade, cross-region data pipelines with strict consistency guarantees using GCP managed services (Pub/Sub, Dataflow, BigQuery) and proper IAM governance.\n\n## Key Concepts\n- Exactly-once processing semantics across distributed regions\n- Dual-layer deduplication: event_id in Dataflow + insertId in BigQuery\n- Stateful stream processing with proper windowing and checkpointing\n- Multi-region BigQuery architecture: regional datasets + centralized aggregation\n- IAM boundaries with region-scoped service accounts and least-privilege access controls","diagram":"flowchart TD\n  IngestRegional[Regional Pub/Sub Ingest] --> PubSub[Pub/Sub Topic]\n  PubSub --> Dataflow[Dataflow Streaming Job]\n  Dataflow --> RegionalBQ[Regional BigQuery]\n  RegionalBQ --> CentralBQ[Central BigQuery Dataset]\n  CentralBQ --> Dash[Global Dashboard]","difficulty":"advanced","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T05:15:53.649Z","createdAt":"2026-01-17T23:34:28.517Z"},{"id":"q-3628","question":"Design a real-time content moderation pipeline on GCP for a globally distributed social app. Messages arrive via regional Pub/Sub topics; implement a Dataflow streaming job that calls a Vertex AI deployed moderation endpoint to score each post. If score >= threshold, publish to a human-review Pub/Sub and write a row to BigQuery with insertId=event_id. Enforce regional processing for data residency; discuss latency, scaling, idempotency, and failure handling?","answer":"Regional Pub/Sub → Dataflow streaming → Vertex AI moderation endpoint (regional) → results to BigQuery (insertId=event_id) and flagged items to moderation-queue Pub/Sub. Process in source region to sa","explanation":"## Why This Is Asked\nTests ability to design a real-time ML-assisted moderation pipeline with data locality, idempotency, and robust failure handling. It also probes how to connect Pub/Sub, Dataflow, Vertex AI, and BigQuery in a low-latency chain while maintaining governance.\n\n## Key Concepts\n- Streaming Dataflow pipelines with DoFn calls to Vertex AI endpoints\n- Regional data residency and regional endpoints\n- Idempotent writes via insertId and exactly-once semantics\n- Alerting and human-review handoff for flagged content\n\n## Code Example\n```python\nimport apache_beam as beam\n\nclass ModerateFn(beam.DoFn):\n    def process(self, element, endpoint_client):\n        content = element['text']\n        score = endpoint_client.moderate(content)\n        yield {'event_id': element['event_id'], 'score': score}\n```\n\n## Follow-up Questions\n- How would you handle model drift and retraining cadence without impacting latency?\n- How would you test end-to-end latency and correctness at scale (e.g., 100k msgs/s) with backpressure scenarios?","diagram":"flowchart TD\n  A[Regional Pub/Sub] --> B[Dataflow Streaming]\n  B --> C[Vertex AI Endpoint]\n  C --> D[BigQuery: moderation_results]\n  C --> E[Moderation-Queue Pub/Sub]","difficulty":"intermediate","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T02:30:57.842Z","createdAt":"2026-01-18T02:30:57.842Z"},{"id":"q-3658","question":"Design a real-world GCP pipeline that automatically redacts PII from new CSV uploads in gs://incoming-csvs using the DLP API, writes redacted copies to gs://redacted-csvs with a _redacted suffix, triggers on Finalize, processes large files in parallel with Dataflow, logs to BigQuery with insertId=filePath+timestamp, ensures idempotency, uses minimal IAM, and provides a robust test plan including duplicates and failure alerts. How would you implement and validate this?","answer":"Trigger: Cloud Storage Finalize on gs://incoming-csvs. A Cloud Function starts a Dataflow batch job that reads the file, applies DLP to redact emails, SSNs, and sensitive numbers, and writes to gs://r","explanation":"## Why This Is Asked\nExplores data governance, DLP integration, and scalable batch processing.\n\n## Key Concepts\n- DLP integration with Dataflow\n- Idempotent file naming and insertId auditing\n- Least-privilege IAM for Storage/Dataflow/DLP\n- Failure handling with Pub/Sub and quarantine\n\n## Code Example\n```python\n# Example: Cloud Function triggering Dataflow template\nfrom google.cloud import dlp\n# ... setup and launch Dataflow...\n```\n\n## Follow-up Questions\n- How would you test redaction correctness at scale?\n- How would you handle schema evolution for CSVs?\n","diagram":null,"difficulty":"intermediate","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Databricks","Discord"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T04:12:45.918Z","createdAt":"2026-01-18T04:12:45.918Z"},{"id":"q-3783","question":"Design a cross-project, multi-tenant event bus on GCP: producers in project A publish JSON events to a shared Pub/Sub topic; a sink in project B routes events to per-tenant BigQuery datasets and to per-tenant GCS archives, enforcing per-tenant IAM conditions and VPC Service Controls. Explain isolation, exactly-once processing, and failure handling; include a minimal test plan?","answer":"Core approach: a streaming Dataflow job reads Pub/Sub events, deduplicates by event_id, and writes to per-tenant BigQuery datasets using insertId for idempotency; raw events are archived to per-tenant","explanation":"## Why This Is Asked\n\nGauges ability to design cross-project isolation, tenant-specific data planes, and secure data egress. Tests understanding of idempotent processing, per-tenant IAM, and network controls.\n\n## Key Concepts\n\n- Multi-tenant data separation\n- Dataflow (Beam) streaming with deduplication\n- Pub/Sub as ingestion backbone\n- BigQuery insertId for idempotency\n- IAM Conditions and VPC Service Controls for isolation\n\n## Code Example\n\n```javascript\n// Pseudo-code: Dataflow-like skeleton (not runnable)\nclass TenantRouter {\n  process(event) {\n    const tenant = event.tenantId;\n    writeToBQ(tenant, event); // with insertId\n    archiveToGCS(tenant, event);\n  }\n}\n```\n\n## Follow-up Questions\n\n- How would you test for out-of-order events and partial failures?\n- What monitoring and alerting would you add for tenant-specific QoS and cost control?","diagram":"flowchart TD\n  P[Producers in P-A] --> S[Pub/Sub Topic]\n  S --> DF[Dataflow Job in P-B]\n  DF --> BQ[Per-tenant BigQuery datasets]\n  DF --> GS[Per-tenant GCS archives]","difficulty":"intermediate","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T09:35:11.944Z","createdAt":"2026-01-18T09:35:11.944Z"},{"id":"q-3840","question":"Design a beginner-friendly CSV ingestion pipeline on GCP: when a CSV file lands in gs://data-exports/csv/YYYY/MM/DD/, a Cloud Function triggered on finalize validates headers, checks numeric columns, and emits a message to Pub/Sub. A second Cloud Function (or Dataflow) consumes the Pub/Sub message and appends to a date-partitioned BigQuery table analytics.sales_YYYYMMDD, using insertId=fileName_rowNumber to ensure idempotency. Include minimal IAM bindings and a simple test plan?","answer":"The pipeline uses two Cloud Functions and Pub/Sub. Function A triggers on csv finalize, validates header and numeric columns, rejects malformed files, and publishes a message with path and date to Pub","explanation":"## Why This Is Asked\nTests event-driven ingestion, data validation, and idempotent loading with minimal IAM.\n\n## Key Concepts\n- GCS finalize triggers in Cloud Functions\n- Pub/Sub as decoupled ingest buffer\n- BigQuery date-partitioned tables and streaming inserts\n- insertId-based idempotency\n- Least-privilege IAM and a straightforward test plan\n\n## Code Example\n```javascript\n// Function A: validate headers and numeric columns, publish to Pub/Sub\nexports.validateAndPublish = (event, context) => {\n  // pseudo: read CSV, validate header, numeric checks, publish {path,date}\n};\n```\n\n## Follow-up Questions\n- How would you handle very large CSV files or streaming vs batch loads?\n- How would you implement retries, backoff, and a dead-letter path for failed messages?","diagram":"flowchart TD\n  A[CSV in gs://data-exports/csv/YYYY/MM/DD/] --> B[Cloud Function: Validate]\n  B --> C[Pub/Sub: ingestion-tasks]\n  C --> D[Cloud Function/Dataflow: Load to BigQuery]\n  D --> E[BigQuery: analytics.sales_YYYYMMDD]","difficulty":"beginner","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Oracle","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T11:34:09.556Z","createdAt":"2026-01-18T11:34:09.556Z"},{"id":"q-3894","question":"Design a cross-region telemetry ingest pipeline on GCP for Nvidia edge devices. Ingest JSON events via Pub/Sub (per region), normalize with Dataflow Streaming, deduplicate using device_id+sequence, and write to a partitioned BigQuery table telemetry.events with insertId. Include idempotency guarantees, disaster recovery to a secondary region, testing plan, and cost/monitoring strategies?","answer":"Design a cross-region telemetry ingest pipeline: Pub/Sub regional topics → Dataflow streaming with per-event insertId dedup by device_id+sequence → BigQuery telemetry.events (partitioned by day). Add ","explanation":"## Why This Is Asked\nTests ability to design cross-region data pipelines with strong at-most-once vs exactly-once semantics, idempotent sinks, and DR strategies.\n\n## Key Concepts\n- Pub/Sub regional topics, Dataflow streaming, BigQuery partitioning\n- insertId-based deduplication and watermarking\n- Cross-region disaster recovery and storage replication\n- Cloud Monitoring, alerting, cost controls\n\n## Code Example\n```python\n# Pseudo-Beam sketch for dedup and sink\nimport apache_beam as beam\nfrom apache_beam.options.pipeline_options import PipelineOptions\noptions = PipelineOptions(streaming=True)\nwith beam.Pipeline(options=options) as p:\n  (p | 'Read' >> beam.io.ReadFromPubSub(topic='projects/.../topics/region')\n     | 'Parse' >> beam.Map(lambda b: json.loads(b))\n     | 'Key' >> beam.WithKeys(lambda e: (e['device_id'], e['sequence']))\n     | 'Dedup' >> beam.Distinct()\n     | 'ToBQ' >> beam.io.WriteToBigQuery('project:dataset.telemetry.events', insert_retry=True))\n```\n\n## Follow-up Questions\n- How would you test idempotency with replayed Pub/Sub messages?\n- What deltas would trigger a failover and how would you verify DR integrity?","diagram":null,"difficulty":"advanced","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T14:26:16.932Z","createdAt":"2026-01-18T14:26:16.934Z"},{"id":"q-4006","question":"Design a beginner-friendly, privacy-aware data routing pipeline on GCP: new user event files land as JSON in gs://company-logs/tenant-*/incoming/. Build a Cloud Function (triggered on object finalize) that validates schema, redacts PII according to a policy file in gs://company-logs/policies.json, and writes sanitized rows to a date- and tenant-partitioned BigQuery table analytics.events. Use insertId for idempotency, minimal IAM, and a basic test plan with sample tenants and edge cases?","answer":"Trigger a Cloud Function on file finalize for gs://company-logs/tenant-*/incoming/*.json. Validate each record, redact PII using policies.json, and emit sanitized rows to BigQuery analytics.events par","explanation":"## Why This Is Asked\n\nTests a candidate's ability to design an end-to-end, event-driven ingestion with privacy controls, multi-tenant isolation, and idempotent writes in a beginner-friendly setting.\n\n## Key Concepts\n\n- Cloud Functions triggers on Cloud Storage object finalize\n- Schema validation and per-tenant data routing\n- PII redaction driven by a policies.json file\n- BigQuery partitioning by tenant and date\n- Idempotent writes using insertId\n- Minimal IAM bindings and simple error handling\n\n## Code Example\n\n```javascript\n// skeleton: Cloud Function triggered on new object\nexports.handleFile = async (data, context) => {\n  // 1) read file from data.name in data.bucket\n  // 2) parse JSON, validate required fields\n  // 3) redact according to policies.json\n  // 4) write to BigQuery with insertId = tenant|event_id|ts\n  // 5) publish to DLQ on error\n};\n```\n\n## Follow-up Questions\n\n- How would you test redaction policy updates without reprocessing data?\n- How handle mixed valid/invalid records within a single file and ensure idempotency?","diagram":"flowchart TD\n  A[gs://company-logs/tenant-*/incoming/] --> B[Cloud Function: finalize trigger]\n  B --> C[Validate & redact using policies.json]\n  C --> D[BigQuery analytics.events (tenant/date partition)]\n  B --> E[Error/alerts via Pub/Sub]","difficulty":"beginner","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Goldman Sachs","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T19:27:03.009Z","createdAt":"2026-01-18T19:27:03.010Z"},{"id":"q-4090","question":"Design an end-to-end real-time telemetry pipeline on GCP for a vehicle fleet. Vehicles publish JSON lines to Pub/Sub (fleet.telemetry). Implement a Dataflow streaming job to parse, deduplicate by message_id, anomaly-detect (speed > 120, abnormal geo drift), and write to BigQuery analytics.fleet_events with daily partitions. Publish alerts to Cloud Tasks for remediation. Include minimal IAM, dead-letter Pub/Sub, test plan, and monitoring strategy. How would you implement?","answer":"Create a Pub/Sub topic fleet.telemetry; a Dataflow streaming job (Apache Beam) parses JSON lines, deduplicates by message_id, detects anomalies (speed > 120, abnormal geo drift), and writes to BigQuer","explanation":"## Why This Is Asked\n\nThis question stresses real‑world streaming data pipelines, dedup vs idempotency, cross-service integration, and robust monitoring.\n\n## Key Concepts\n\n- Dataflow streaming with Apache Beam\n- Pub/Sub ingestion and dead-lettering\n- BigQuery partitioned tables and insertId\n- Cloud Tasks for remediation workflows\n- IAM least privilege and service accounts\n- Observability and monitoring\n\n## Code Example\n\n```python\nimport apache_beam as beam\nimport json\nfrom apache_beam.options.pipeline_options import PipelineOptions\n\nclass ParseAndDedup(beam.DoFn):\n    def process(self, element):\n        rec = json.loads(element)\n        rec['insertId'] = rec['message_id'] + '_' + rec.get('ts','0')\n        yield rec\n\ndef run():\n    opts = PipelineOptions()\n    with beam.Pipeline(options=opts) as p:\n        (p\n         | \"Read\" >> beam.io.ReadFromPubSub(topic='projects/PROJECT/topics/fleet.telemetry')\n         | \"Parse\" >> beam.Map(lambda m: json.loads(m.decode('utf-8')))\n         | \"Prepare\" >> beam.Map(lambda r: {'message_id': r['message_id'], 'payload': r['payload'], 'ts': r['ts'], 'insertId': r['message_id'] + '_' + r['ts']})\n         | \"ToBigQuery\" >> beam.io.WriteToBigQuery(\n              table='PROJECT:dataset.analytics.fleet_events',\n              insert_retry_limit=3,\n              time_partitioning=beam.io.BigQueryTimePartitioning(field='date')\n         ))\n\nif __name__ == '__main__':\n    run()\n```\n\n## Follow-up Questions\n\n- How would you implement idempotency in case of late messages?\n- How do you monitor Dataflow backpressure and scale?","diagram":"flowchart TD\n  A[Pub/Sub: fleet.telemetry] --> B[Dataflow streaming]\n  B --> C(BigQuery: analytics.fleet_events)\n  B --> D[Cloud Tasks: remediation]\n  A --> E[Dead-letter Pub/Sub]","difficulty":"intermediate","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Tesla","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T23:35:51.739Z","createdAt":"2026-01-18T23:35:51.739Z"},{"id":"q-4111","question":"Design a beginner-friendly data redaction pipeline on GCP: when JSONL files land in gs://customer-logs, a Cloud Function triggers on finalize, reads each line, redacts PII fields (email, phone, SSN) per a schema, writes redacted lines to gs://redacted-logs/YYYY/MM/DD/<orig>.jsonl, and inserts a daily summary into BigQuery (date, file, redacted counts). Use minimal IAM, exponential backoff retries, and a test plan with sample lines including missing fields and nulls?","answer":"When JSONL files are uploaded to gs://customer-logs, a Cloud Function automatically triggers on the finalize event, processes each line to redact PII fields (email, phone, SSN) based on a predefined schema, writes the sanitized output to gs://redacted-logs/YYYY/MM/DD/<original_filename>.jsonl, and inserts a daily summary record into BigQuery with the date, file path, and redaction counts. The implementation follows the principle of least privilege for IAM permissions, incorporates exponential backoff for retry logic, and includes comprehensive testing with sample data covering edge cases like missing fields and null values.","explanation":"## Why This Is Asked\nThis practical, beginner-friendly data redaction workflow demonstrates hands-on experience with Cloud Storage events, Cloud Functions, and BigQuery integration, while emphasizing idempotency and proper error handling.\n\n## Key Concepts\n- Cloud Functions triggered by Cloud Storage finalize events\n- JSONL streaming processing and per-line transformation\n- Redaction policy enforcement and schema validation\n- Idempotent writes and time-partitioned destination paths\n\n## Code Example\n```javascript\n// Pseudo: Cloud Function handler outline for redacting PII fields\n```\n\n## Follow-up","diagram":"flowchart TD\n  A[JSONL arrives in gs://customer-logs] --> B[Cloud Function on finalize]\n  B --> C{Line redaction}\n  C --> D[Write redacted lines to gs://redacted-logs/YYYY/MM/DD/]\n  B --> E[BigQuery daily summary]","difficulty":"beginner","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Databricks","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-19T05:29:53.098Z","createdAt":"2026-01-19T02:43:34.603Z"},{"id":"q-4146","question":"Design a beginner-friendly file-audit pipeline on GCP: when a CSV lands in gs://payments/ingest/, a Cloud Function validates headers (sid, amount, currency, ts), normalizes currency to USD, and writes cleaned rows to gs://payments/clean/YYYY/MM/DD/filename.cleaned.csv. It should also append an audit row to BigQuery (insertId = filePath+ts). If any row is invalid, publish a message to Pub/Sub and quarantine the file. How would you implement this end-to-end, including idempotency and test plan?","answer":"Trigger a Cloud Function on object_finalize for gs://payments/ingest/. Validate headers (sid, amount, currency, ts) and normalize currency to USD using a tiny map. Write cleaned rows to gs://payments/","explanation":"## Why This Is Asked\n\nTests end-to-end event-driven thinking with basic data validation, simple transformation, and auditing on GCP. It combines Cloud Functions, Cloud Storage, BigQuery, and Pub/Sub with a focus on idempotency and minimal IAM.\n\n## Key Concepts\n\n- Event-driven processing on object finalize\n- Lightweight data validation and field normalization\n- Idempotent writes via insertId and deterministic output paths\n- Audit logging in BigQuery and error routing via Pub/Sub\n\n## Code Example\n\n```javascript\n// Node.js Cloud Function skeleton for object finalize\nexports.handleIngest = async (event, context) => {\n  const bucket = event.bucket;\n  const name = event.name;\n  // read object from Storage, parse CSV, validate headers\n  // for each row: validate, normalize currency, write to cleaned path\n  // on success: insert audit row to BigQuery with insertId\n  // on invalid: publish message to Pub/Sub and quarantine\n};\n```\n\n## Follow-up Questions\n\n- How would you implement idempotency for reprocessing the same file?\n- What minimal IAM bindings would you apply for each service involved?","diagram":"flowchart TD\n  A[CSV lands in gs://payments/ingest/] --> B[Object Finalize Trigger]\n  B --> C[Validate Headers & Normalize USD]\n  C --> D[Write Cleaned CSV to gs://payments/clean/YYYY/MM/DD/filename.cleaned.csv]\n  C --> E[Append Audit Row to BigQuery (insertId=filePath+ts)]\n  D & E --> F[Success]\n  B --> G[Invalid Rows -> Pub/Sub]\n  G --> H[Quarantine Original File]","difficulty":"beginner","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Lyft","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-19T04:50:27.121Z","createdAt":"2026-01-19T04:50:27.121Z"},{"id":"q-4157","question":"Design a beginner-friendly GCP pipeline for per-customer data isolation when new data arrives in Cloud Storage: each customer has a bucket gs://customer-data/<customerId> that receives JSON lines files. Create a Cloud Function triggered on object finalize that copies the file to an archive bucket gs://archive-<customerId>/YYYY/MM/DD/<orig>, updates the IAM so only the customer’s service account can read the archived data, and appends a manifest row to BigQuery with fields customerId, filePath, timestamp. Include idempotency, minimal IAM bindings, and a simple test plan with sample files?","answer":"Trigger on object finalize; derive customerId from the source path; copy to gs://archive-<customerId>/YYYY/MM/DD/<orig>; update bucket IAM to grant read only to that customer’s service account; append","explanation":"## Why This Is Asked\nTests practical skills in GCS triggers, IAM, and BigQuery integration; focuses on data isolation per customer and minimal permissions.\n\n## Key Concepts\n- Cloud Functions event handling\n- GCS IAM and per-bucket policies\n- idempotent writes to BigQuery\n- simple test plan with sample files\n\n## Code Example\n```javascript\n// Pseudo: Cloud Function handler that copies file and writes manifest\nexports.onFinalize = (evt, ctx) => { /* … */ }\n```\n\n## Follow-up Questions\n- How would you adapt this for high churn of customers or very large archives?\n- How ensure replay safety if a file is re-uploaded?","diagram":"flowchart TD\n  A[Cloud Storage finalize event] --> B[Cloud Function: parse path & extract customerId]\n  B --> C[Copy to archive bucket gs://archive-<customerId>/YYYY/MM/DD/]\n  C --> D[Update IAM: restrict read access to customer SA]\n  D --> E[BigQuery: insert manifest (insertId = filePath+timestamp)]","difficulty":"beginner","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-19T05:47:05.232Z","createdAt":"2026-01-19T05:47:05.232Z"},{"id":"q-4202","question":"Design a real-time, multi-tenant data ingestion and analytics pipeline on GCP for a SaaS app. Ingest per-tenant events into Pub/Sub, process with Dataflow to apply per-tenant isolation, write to a partitioned BigQuery dataset, and enforce per-tenant retention and audit trails. Include VPC Service Controls/Private Service Connect, Cloud DLP masking, IAM bindings, cross-region replication, and a practical test plan for burst traffic?","answer":"Ingest per-tenant events into Pub/Sub, route through a Dataflow streaming job keyed by tenant_id, write to a BigQuery table partitioned by day and clustered by tenant_id, apply DLP masking in-flight, ","explanation":"## Why This Is Asked\nThis tests ability to design cross-project, multi-tenant pipelines with security, compliance, and DR.\n\n## Key Concepts\n- Real-time streaming\n- Per-tenant isolation\n- Pub/Sub + Dataflow\n- BigQuery partitioning/ clustering\n- Data Loss Prevention masking\n- IAM least privilege\n- DR with cross-region replication\n\n## Code Example\n```java\n// Beam streaming skeleton to read from per-tenant Pub/Sub, group by tenant_id, write to BigQuery\n```\n\n## Follow-up Questions\n- How would you enforce per-tenant quotas to prevent bursts?\n- How would you validate retention and cross-region DR?\n","diagram":null,"difficulty":"advanced","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Salesforce","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-19T08:20:43.754Z","createdAt":"2026-01-19T08:20:43.754Z"},{"id":"q-4267","question":"Design a secure, cost-aware telemetry ingestion pipeline for multi-tenant data on GCP. Inbound JSON logs arrive in gs://telemetry-raw and include tenant_id and event_id. Build an end-to-end flow with Cloud Functions (finalize), Pub/Sub, Dataflow, and BigQuery that isolates tenants, ensures idempotent processing (insertId = tenant_id|event_id|ts), supports schema evolution, and quarantines invalid events. Include a minimal IAM policy and a test plan?","answer":"Use a Cloud Function triggered on gs://telemetry-raw finalize to publish each event to Pub/Sub telemetry.events. Dataflow streaming reads Pub/Sub, validates against a registry-defined schema, and writ","explanation":"## Why This Is Asked\nThis tests designing end-to-end data ingestion with tenant isolation, idempotency, schema evolution, and operational guards in GCP.\n\n## Key Concepts\n- Cloud Functions triggers on GCS\n- Pub/Sub and dead-letter topics\n- Dataflow streaming and BigQuery writes\n- insertId-driven idempotency and MERGE\n- Schema evolution and registry\n- ACLs and least privilege\n\n## Code Example\n```javascript\n// pseudo-code for insertId and MERGE\n```\n\n## Follow-up Questions\n- How would you handle schema drift across tenants?\n- How would you verify idempotency in production?","diagram":"flowchart TD\n  A[gs://telemetry-raw upload] --> B[Cloud Function publishes to Pub/Sub telemetry.events]\n  B --> C[Dataflow streaming to BigQuery (insertId = tenant_id|event_id|ts)]\n  C --> D[BigQuery telemetry.events table (partitioned by date)]\n  B --> E[telemetry-errors quarantined bucket (DLQ)]","difficulty":"intermediate","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","MongoDB","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-19T10:57:25.326Z","createdAt":"2026-01-19T10:57:25.326Z"},{"id":"q-4385","question":"Design a real-time anomaly detection pipeline on GCP for authentication events. Source: Pub/Sub topic projects/ORG/auth.events; stream to Dataflow (Python) with dedup by event_id, 30s per-user windows, and per-user fail_rate = failures/attempts. Trigger alert if fail_rate > 0.3; store enriched events in BigQuery analytics.auth_events partitioned by date. Ensure exactly-once processing, idempotent BigQuery writes, and cost controls. Include a test plan and trade-offs?","answer":"Implement with Pub/Sub to Dataflow (Python Beam). Deduplicate by event_id, window 30s per user, compute per-user fail_rate (fails/attempts) in window, emit alert if fail_rate > 0.3; write enriched eve","explanation":"Why This Is Asked\n\nTests real-time streaming skills across Pub/Sub, Dataflow, and BigQuery with production reliability patterns: dedup, windowing, exactly-once, and alerting.\n\nKey Concepts\n\n- Dataflow (Beam) streaming pipelines on GCP\n- Pub/Sub as the source with at-least-once delivery\n- Windowing and watermarking for per-user aggregates\n- Stateful deduplication by event_id\n- BigQuery insertId for idempotent writes\n- Alerting via Pub/Sub and Cloud Monitoring\n- Cost controls through autoscaling and resource tagging\n\nCode Example\n\n```python\nimport apache_beam as beam\nfrom apache_beam.options.pipeline_options import PipelineOptions\n\nclass ParseEvent(beam.DoFn):\n    def process(self, element):\n        # parse to dict with fields: event_id, user_id, outcome, ts\n        return [parsed]\n\noptions = PipelineOptions(streaming=True, project='my-project')\nwith beam.Pipeline(options=options) as p:\n    events = (p\n        | 'Read' >> beam.io.ReadFromPubSub(topic='projects/my-project/topics/auth.events')\n        | 'Parse' >> beam.Map(ParseEvent().process)\n        | 'Dedup' >> beam.Distinct(key=lambda e: e['event_id'])\n        | 'Window' >> beam.WindowInto(beam.window.FixedWindows(30))\n        | 'Enrich' >> beam.Map(lambda e: enrich(e))\n        | 'ToBigQuery' >> beam.io.WriteToBigQuery(\n            table='my-project:dataset.analytics.auth_events',\n            insert_retry_limit=3,\n            insert_id_fn=lambda el: el['event_id']\n        )\n    )\n```\n\nFollow-up Questions\n\n- How would you handle late events and allowed lateness?\n- How would you evolve the schema without breaking writes?\n- What alternatives to Dataflow could you consider for this workload and why?","diagram":null,"difficulty":"intermediate","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Goldman Sachs","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-19T16:50:08.750Z","createdAt":"2026-01-19T16:50:08.751Z"},{"id":"q-4504","question":"Design a beginner-friendly end-to-end GCP pipeline to process user-uploaded videos. When a video is uploaded to gs://user-videos/raw/, a Cloud Function should enqueue a Cloud Task that calls a Cloud Run service to transcode to 720p and 360p MP4 outputs stored in gs://user-videos/processed/720p/ and gs://user-videos/processed/360p/ with names <orig>_<hash>.mp4. Ensure idempotency via the hash, handle Cloud Tasks retries, and keep IAM minimal. Provide a basic test plan?","answer":"A Cloud Function triggered by uploads to gs://user-videos/raw/ enqueues a Cloud Task that invokes a Cloud Run service. The Cloud Run service transcodes videos to 720p and 360p MP4 formats, storing outputs in gs://user-videos/processed/720p/ and gs://user-videos/processed/360p/ with filenames formatted as <orig>_<hash>.mp4. The hash ensures idempotency by preventing duplicate processing, while Cloud Tasks manages retries with exponential backoff. IAM follows the principle of least privilege, granting the Cloud Function only tasks.enqueuer permissions and Cloud Run only storage.objects.write permissions.","explanation":"## Why This Is Asked\nThis question tests practical, end-to-end workflow design using cost-effective and operationally efficient patterns. It evaluates understanding of event-driven architectures, Cloud Tasks for throttling, Cloud Run for scalable compute, and proper implementation of idempotency and minimal IAM principles.\n\n## Key Concepts\n- Event-driven pipeline architecture with GCS triggers\n- Cloud Tasks for reliable job queuing and retry management\n- Cloud Run for serverless video processing\n- Idempotent file naming using content hashing\n- IAM least privilege and permission scoping\n- GCS partitioning for organized output storage","diagram":"flowchart TD\n  A[Raw video uploaded] --> B[Cloud Function]\n  B --> C[Cloud Tasks queue]\n  C --> D[Cloud Run transcode service]\n  D --> E[gs://user-videos/processed/720p/]\n  D --> F[gs://user-videos/processed/360p/]","difficulty":"beginner","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Google","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T07:47:36.632Z","createdAt":"2026-01-19T21:45:16.833Z"},{"id":"q-4541","question":"Design a real-time data masking and routing pipeline on GCP for Pub/Sub telemetry-raw. Build a Dataflow streaming pipeline that reads from Pub/Sub, sources masking rules from Firestore (collection policy.telemetry) and applies dynamic masking to PII fields (e.g., email, phone_number, device_id). Support rule updates without redeploy, write masked events to BigQuery (telemetry.region tables partitioned by event_date) and archive a copy to Cloud Storage. Late data handling, dead-letter Pub/Sub, idempotent writes, and minimal IAM must be considered. Include a concise test plan and policy-update scenario?","answer":"Use a Beam Dataflow streaming job reading telemetry-raw from Pub/Sub, with Firestore as a dynamic side-input for masking rules. Mask fields like email, phone_number, and device_id; refresh rules perio","explanation":"## Why This Is Asked\nTests real-time data privacy engineering, dynamic policy management, and multi-sink pipelines on GCP. Evaluates handling of late data, dead-letter routing, and IAM least privilege in a realistic telemetry scenario.\n\n## Key Concepts\n- Apache Beam (Dataflow) streaming with Pub/Sub\n- Firestore as dynamic policy store and live updates\n- Field-level masking for PII (email, phone, device_id)\n- Exactly-once-like semantics with idempotent writes to BigQuery\n- Multi-sink: BigQuery partitioned tables and Cloud Storage archives\n- Dead-letter topics and observability/alerting\n\n## Code Example\n```python\nimport apache_beam as beam\nfrom apache_beam.options.pipeline_options import PipelineOptions\nfrom apache_beam.options.pipeline_options import PipelineOptions\n\nclass MaskPII(beam.DoFn):\n    def __init__(self, policy_ref):\n        self.policy_ref = policy_ref\n    def process(self, element, policy):\n        # element is JSON dict; apply masking per policy\n        # mask email, phone_number, device_id accordingly\n        yield masked_element\n\ndef run():\n    options = PipelineOptions(streaming=True, project='my-gcp-project')\n    with beam.Pipeline(options=options) as p:\n        raw = (p\n               | 'ReadPubSub' >> beam.io.ReadFromPubSub(topic='projects/myproj/topics/telemetry-raw')\n               | 'ParseJSON' >> beam.Map(lambda b: json.loads(b))\n               | 'Mask' >> beam.ParDo(MaskPII(policy_ref='policies/telemetry'))\n               | 'WriteBigQuery' >> beam.io.WriteToBigQuery(table='mydataset.telemetry_${region}', write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND)\n               | 'Archive' >> beam.io.WriteToFiles('gs://telemetry-archive')\n        )\n\nif __name__ == '__main__':\n    run()\n```\n\n## Follow-up Questions\n- How would you test dynamic policy updates without redeploying a pipeline?\n- How do you ensure idempotency across restarts with event_id as a key?\n- How would you monitor masking latency and policy-refresh failures?\n","diagram":"flowchart TD\n  A[Telemetry Pub/Sub: telemetry-raw] --> B[Dataflow Streaming Job]\n  B --> C[Firestore: dynamic masking policy]\n  B --> D[BigQuery: telemetry.region (partitioned)]\n  B --> E[Cloud Storage: telemetry-archive]\n  B --> F[Pub/Sub: dead-letter]\n  G[Monitoring & IAM] --> B","difficulty":"intermediate","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Microsoft","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-19T22:52:38.454Z","createdAt":"2026-01-19T22:52:38.454Z"},{"id":"q-4553","question":"Design a real-time anomaly detection pipeline for user login events across multiple regions on GCP. Ingest via Pub/Sub, run a streaming Dataflow (Beam) pipeline computing a rolling per-user risk score using features like IP, device, region, and time of day. Persist features and scores to BigQuery, publish anomalies to a separate Pub/Sub topic, and trigger a Cloud Run remediation service when a threshold is crossed. Consider idempotency, privacy, tracing, and a practical test plan?","answer":"Ingest login events via Pub/Sub, execute a Dataflow (Apache Beam) streaming pipeline with 5-minute fixed windows to compute per-user risk scores from features including IP address, device fingerprint, geographic region, and time-of-day patterns. Persist event features and calculated scores to BigQuery using MERGE statements for idempotent upserts. Publish anomalous events to a dedicated Pub/Sub topic that triggers a Cloud Run remediation service when risk thresholds are exceeded. Implement PII data masking, end-to-end distributed tracing with Cloud Trace, and ensure exactly-once processing semantics throughout the pipeline.","explanation":"## Why This Is Asked\nThis question evaluates your ability to design real-time data pipelines that operate across multiple geographic regions while addressing production concerns including latency requirements, fault tolerance, and enterprise-grade operational practices.\n\n## Key Concepts\n- **Pub/Sub**: Decoupled ingestion mechanism for login events and downstream alerting\n- **Dataflow/Apache Beam**: Streaming processing with windowed aggregations and per-key state management\n- **BigQuery**: Feature storage with idempotent write patterns using MERGE operations\n- **Cloud Run Integration**: Event-driven remediation via Pub/Sub triggers\n- **Observability**: End-to-end tracing implementation using Cloud Trace/OpenTelemetry\n- **Data Privacy**: PII masking and anonymization techniques\n- **Testing Strategy**: Synthetic traffic generation, historical data replay, backfill validation, and cost optimization analysis\n\n## Code Example\n```python\nimport apache_beam as beam\nfrom apache_beam.options.pipeline_options import PipelineOptions\nfrom apache_beam.transforms.window import FixedWindows\nimport json\n\nclass ComputeRiskScore(beam.DoFn):\n    def process(self, element, window=beam.DoFn.WindowParam):\n        # Risk scoring logic using IP, device, region, time features\n        risk_score = self._calculate_risk(element)\n        yield {\n            'user_id': element['user_id'],\n            'risk_score': risk_score,\n            'window_end': window.end.to_utc_datetime()\n        }\n\ndef run_pipeline():\n    options = PipelineOptions(streaming=True)\n    \n    with beam.Pipeline(options=options) as p:\n        (p\n         | 'ReadFromPubSub' >> beam.io.ReadFromPubSub(\n             subscription='projects/project/subscriptions/login-events')\n         | 'ParseJSON' >> beam.Map(lambda x: json.loads(x.decode('utf-8')))\n         | 'Window' >> beam.WindowInto(FixedWindows(300))  # 5-minute windows\n         | 'ComputeRisk' >> beam.ParDo(ComputeRiskScore())\n         | 'WriteToBigQuery' >> beam.io.WriteToBigQuery(\n             'project:dataset.user_login_features',\n             schema='user_id:STRING, risk_score:FLOAT64, window_end:TIMESTAMP',\n             write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND,\n             create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED)\n         | 'FilterAnomalies' >> beam.Filter(\n             lambda x: x['risk_score'] > ANOMALY_THRESHOLD)\n         | 'PublishAlerts' >> beam.io.WriteToPubSub(\n             'projects/project/topics/login-anomalies'))\n```","diagram":null,"difficulty":"intermediate","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T06:12:27.696Z","createdAt":"2026-01-19T23:42:29.791Z"},{"id":"q-4789","question":"Design a real-time analytics pipeline on Google Cloud for IoT telemetry: devices publish JSON to Pub/Sub; a Dataflow streaming job parses and window-aggregates per device to compute anomaly scores using a lightweight model, writes anomalies to BigQuery, and triggers alerts via a Cloud Function to Slack; also emit enrichment artifacts to Cloud Storage. How would you ensure exactly-once semantics, late data handling, and minimal IAM, with a testing plan?","answer":"Ingest IoT data via Pub/Sub, run a Dataflow streaming job that computes per-device anomaly scores in event-time windows, write anomalies to BigQuery (using insertId for idempotence), and trigger a Sla","explanation":"## Why This Is Asked\nTests real-time streaming design, fault tolerance, and alerting in GCP at intermediate complexity.\n\n## Key Concepts\n- Pub/Sub ingestion; Dataflow streaming; event-time windowing; idempotent sinks; Cloud Functions for alerts; Cloud Storage for enrichment; IAM least privilege.\n\n## Code Example\n```javascript\n// placeholder: Dataflow DoFn skeleton for per-device anomaly scoring\n```\n\n## Follow-up Questions\n- How would you test late-arriving data and streams with backfill?\n- What monitoring and cost controls would you implement for this pipeline?","diagram":"flowchart TD\n  P[Pub/Sub: telemetry] --> D[Dataflow: streaming]\n  D --> B[BigQuery: anomalies]\n  D --> F[Cloud Function: Slack alerts]\n  D --> S[Cloud Storage: enrichment artifacts]\n  S --> B","difficulty":"intermediate","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T13:11:07.518Z","createdAt":"2026-01-20T13:11:07.518Z"},{"id":"q-4871","question":"Design a cross-region log ingestion and enrichment pipeline on GCP to satisfy data residency and auditability. Logs arrive in us-central1 Cloud Storage at gs://logs-origin/us/*.log; automatically replicate to europe-west1 with a region tag and store a processed stream in BigQuery analytics.logs. Build the pipeline with Pub/Sub, Dataflow, and BigQuery, ensuring exactly-once processing, idempotent writes, CMEK-protected buckets, and IAM least privilege. Include failure modes, DR testing, and a concrete test plan?","answer":"Use Pub/Sub for regional events, Dataflow streaming job with guaranteed exactly-once via watermarking and insertId for BigQuery, and cross-region replication with CMEK. Use region metadata in analytic","explanation":"## Why This Is Asked\nProbes cross-region streaming, residency, security, and DR planning.\n\n## Key Concepts\n- Cross-region data residency\n- Pub/Sub + Dataflow streaming\n- Exactly-once and idempotent BigQuery writes (insertId)\n- CMEK and IAM least privilege\n- DR testing and canary rollouts\n\n## Code Example\n```python\n# Dataflow streaming skeleton (pseudo)\nimport apache_beam as beam\nwith beam.Pipeline(...) as p:\n  events = (\n    p | 'Read' >> beam.io.ReadFromPubSub(topic=topic)\n      | 'Parse' >> beam.Map(parse)\n      | 'Dedup' >> beam.GroupByKey().distinct())\n  events | 'WriteBQ' >> beam.io.WriteToBigQuery(\n      table='dataset.analytics.logs', insert_data=True,\n      insert_id_fn=lambda e: e.event_id)\n```\n\n## Follow-up Questions\n- How would you test residency and DR capabilities with replayable logs?\n- How handle schema evolution and late data without violating exactly-once guarantees?","diagram":"flowchart TD\n  A[Raw Logs US] --> B[Cross-Region Replication to EU]\n  B --> C[Pub/Sub US Topic]\n  B --> D[Pub/Sub EU Topic]\n  C --> E[Dataflow US Stream]\n  D --> F[Dataflow EU Stream]\n  E --> G[BigQuery analytics.logs]\n  F --> G","difficulty":"advanced","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T17:00:02.886Z","createdAt":"2026-01-20T17:00:02.886Z"},{"id":"q-4913","question":"Design a beginner-friendly event ingestion pipeline on GCP for mobile game events arriving as JSONL in gs://mobile-events/raw. On object_finalize, a Cloud Function should validate a simple schema (user_id:string, event:string, ts:int). Valid lines go to BigQuery analytics.events with insertId = user_id+\"_\"+ts+\"_\"+line_num and a date partition by ts. Invalid lines go to gs://mobile-events/dead with metadata. Include minimal IAM bindings, retries, and a test plan with good/bad samples?","answer":"Design a Cloud Function triggered on Finalize for gs://mobile-events/raw. It reads JSONL lines, validates a schema: user_id (string), event (string), ts (int). Writes valid lines to BigQuery analytics","explanation":"## Why This Is Asked\nTests practical design of event ingestion with schema validation, idempotent writes, and dead-letter handling.\n\n## Key Concepts\n- Cloud Functions, Cloud Storage triggers\n- JSONL parsing and per-line validation\n- BigQuery insert with insertId, date partitioning\n- Dead-letter handling and log metadata\n- IAM least privilege and retries\n\n## Code Example\n```python\ndef handler(event, context):\n    bucket = event['bucket']\n    name = event['name']\n    if not name.endswith('.jsonl'):\n        return\n    # pseudo-reading logic\n    for line in read_lines(bucket, name):\n        data = json.loads(line)\n        if is_valid(data):\n            write_to_bq(data)\n        else:\n            write_dead_letter(data)\n```\n\n## Follow-up Questions\n- How would you test idempotency when a function retries after partial success?\n- How would you monitor failures and alert on dead-letter growth?","diagram":"flowchart TD\nA[Finalize event blob] --> B[Cloud Function]\nB --> C(BigQuery insert to analytics.events)\nB --> D[Dead-letter gs://mobile-events/dead]\nC --> E[partition by date(ts)]","difficulty":"beginner","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T19:03:09.470Z","createdAt":"2026-01-20T19:03:09.470Z"},{"id":"q-5117","question":"Design a real-world GCP data ingestion pipeline for a high-traffic mobile analytics stream (1-2M events/sec): events arrive via Pub/Sub; build an end-to-end pipeline that ensures exactly-once delivery, dedup by event_id, and time-based aggregation into BigQuery. Use Dataflow (Beam) for streaming, with a DLQ for failed events, and separate raw and summary tables. Include IAM, testing, and cost considerations?","answer":"Adopt Pub/Sub with event_id as the dedupe key; implement a Dataflow (Beam) streaming job using a stateful DoFn to deduplicate, applying 5-minute fixed windows; write to analytics.events_raw with inser","explanation":"## Why This Is Asked\n\nAssesses ability to design end-to-end streaming pipelines in GCP at scale with correctness guarantees, operational resilience, and proper IAM controls.\n\n## Key Concepts\n\n- Pub/Sub delivery semantics and DLQ\n- Apache Beam stateful processing and windowing\n- BigQuery streaming inserts and insertId for idempotency\n- Dataflow autoscaling, backpressure, and cost awareness\n- IAM least privilege and monitoring\n\n## Code Example\n\n```python\n# Pseudo Beam DoFn for dedup by event_id\nclass DedupDoFn(DoFn):\n  def process(self, element, timestamp=DoFn.TimestampParam, state=DoFn.StateParam):\n    event_id = element['event_id']\n    if not state.read(event_id):\n      state.write(event_id, True)\n      yield element\n```\n\n## Follow-up Questions\n\n- How would you validate exactly-once during chaos testing?\n- How would you evolve schema without breaking readers?\n","diagram":null,"difficulty":"advanced","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","MongoDB","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T07:05:03.243Z","createdAt":"2026-01-21T07:05:03.243Z"},{"id":"q-5207","question":"You operate a real-time analytics pipeline on GCP that ingests mobile events at ~50k events/sec from two regions. Design an end-to-end path using Pub/Sub, Dataflow streaming, and BigQuery with exactly-once semantics, cross-region failover, and DR. Specify data formats, dedup by event_id, using insertId, handling schema evolution, and IAM least privilege. Include testing, observability, and cost considerations?","answer":"Propose a streaming pipeline: two Pub/Sub topics (region A and B) feeding a Dataflow (Apache Beam) streaming job that ingests from both, deduplicates by event_id, and writes to a single BigQuery table","explanation":"## Why This Is Asked\nReal-time analytics across regions with strict at-least-once guarantees requires careful dedup, schema evolution, and DR.\n\n## Key Concepts\n- Exactly-once semantics via insertId and idempotent sinks\n- Multi-region Pub/Sub and Dataflow orchestration\n- Schema evolution, cost, and observability\n\n## Code Example\n\n```python\nimport apache_beam as beam\nfrom apache_beam.options.pipeline_options import PipelineOptions\n\nclass AddInsertId(beam.DoFn):\n  def process(self, row):\n    row['insertId'] = row['event_id']\n    yield row\n\nwith beam.Pipeline(options=PipelineOptions()) as p:\n  events = (p | 'Read' >> beam.io.ReadFromPubSub(...)\n              | 'Parse' >> beam.Map(lambda m: json.loads(m))\n              | 'AddId' >> beam.ParDo(AddInsertId()))\n\n  events | 'WriteBQ' >> beam.io.WriteToBigQuery(\n      table='project.dataset.table',\n      schema='SCHEMA',\n      insert_id_fn=lambda row: row['event_id'],\n      write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND,\n      create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED)\n```\n\n## Follow-up Questions\n- How would you test idempotency under replay, and how would you simulate cross-region failover?\n- What are the trade-offs of using BigQuery Streaming Inserts vs. BQ bulk loads for this workload?","diagram":"flowchart TD\n  A[Mobile SDK] --> B[Pub/Sub Region A]\n  A --> C[Pub/Sub Region B]\n  B --> D[Dataflow Region A]\n  C --> E[Dataflow Region B]\n  D --> F[BigQuery ( analytics.events )]\n  E --> F\n  F --> G[BI dashboards / Alerts]","difficulty":"advanced","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Citadel","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T10:53:04.302Z","createdAt":"2026-01-21T10:53:04.302Z"},{"id":"q-5243","question":"Design an advanced, multi-region telemetry ingestion pipeline on GCP for a SaaS product. Telemetry JSON lines arrive into region-specific Cloud Storage buckets under gs://telemetry-origin/{region}/YYYY/MM/DD. Build a streaming pipeline using Pub/Sub, Dataflow, and BigQuery that delivers idempotent writes (upsert by event_id) to a region-partitioned BigQuery dataset. Enforce EU residency so data from EU regions never leaves the EU; replicate only metadata to US for DR, with private endpoints (Private Service Connect), VPC Service Controls, and least-privilege IAM. Include error handling, data replay safety, testing plan, and a plan to meet RPO of 5 seconds and RTO of 60 seconds with cross-region failover?","answer":"Design an advanced, multi-region telemetry ingest using Pub/Sub per region, a streaming Dataflow pipeline (BigQueryIO) writing to regionally partitioned BigQuery tables with event_id as insertId to en","explanation":"## Why This Is Asked\nTests ability to design reliable, compliant, scalable pipelines across regions with strict residency and security requirements.\n\n## Key Concepts\n- Real-time ingestion, idempotent writes, cross-region DR, data residency, IAM least privilege, Private Service Connect.\n- Dataflow templates, BigQuery streaming writes, insertId semantics, replay-safe design.\n\n## Code Example\n```python\n# Dataflow skeleton showing event_id propagation for idempotent BigQuery inserts\nimport apache_beam as beam\nfrom apache_beam.options.pipeline_options import PipelineOptions\n\nclass AttachEventId(beam.DoFn):\n    def process(self, element):\n        yield {**element, 'insertId': element['event_id']}\n\nwith beam.Pipeline(options=PipelineOptions()) as p:\n    (p | 'Read' >> beam.io.ReadFromText('gs://telemetry-origin/...')\n       | 'Parse' >> beam.Map(lambda l: parse_json(l))\n       | 'Attach' >> beam.ParDo(AttachEventId())\n       | 'WriteBQ' >> beam.io.gcp.bigquery.BigQueryWrite(\n            table='project:dataset.telemetry',\n            write_disposition=BigQueryDisposition.WRITE_APPEND,\n            insert_retry=True)\n    )\n```\n\n## Follow-up Questions\n- How would you test idempotency and replay safety at scale?\n- How would you validate RPO/RTO during DR failover?","diagram":"flowchart TD\n  S[Telemetry land: gs://telemetry-origin/{region}/YYYY/MM/DD] --> P[Pub/Sub: telemetry-{region}]\n  P --> DF[Dataflow streaming job]\n  DF --> BQ[BigQuery: analytics.telemetry_{region}]\n  EU_Fence[EU residency guard] --> DR[Metadata replicated to US DR]\n  PSC[Private Service Connect] --> BQ\n  IAM[Least-privilege IAM] --> PSC\n  Tests[Test plan: replay, dedupe, latency, cross-region failover]","difficulty":"advanced","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Oracle","Salesforce","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T11:56:48.595Z","createdAt":"2026-01-21T11:56:48.595Z"},{"id":"q-5276","question":"Design an end-to-end GCP streaming pipeline for mobile analytics events with strict data residency. Ingest via Pub/Sub to Dataflow (Beam) and write to a regional BigQuery dataset as analytics.events in real-time. Ensure exactly-once semantics, support schema evolution, and cost controls. Include tracing, error handling, and a test plan with sample events and replays?","answer":"Use Pub/Sub in a single region, Dataflow streaming with stateful dedup and event-time windows; write to BigQuery using insertId from event_id+ingest_time; enforce regional residency; implement schema ","explanation":"## Why This Is Asked\nThis tests end-to-end streaming, exactly-once guarantees, and data residency across GCP services.\n\n## Key Concepts\n- Pub/Sub regionalization\n- Dataflow Beam stateful processing for dedup\n- BigQuery insertId semantics for idempotent writes\n- Schema evolution and monitoring\n\n## Code Example\n```python\nimport apache_beam as beam\nclass DedupFn(beam.DoFn):\n    def process(self, element, timestamp=beam.DoFn.TimestampParam, state=beam.DoFn.StateParam):\n        event_id = element['event_id']\n        # deduplicate by event_id using per-key state\n        yield element\n```\n\n## Follow-up Questions\n- How would you implement schema evolution in the pipeline without breaking on-going jobs?\n- How would you test exactly-once behavior under retries and network partitions?","diagram":"flowchart TD\n  A[Ingest] --> B[Pub/Sub]\n  B --> C[Dataflow]\n  C --> D[BigQuery regional]\n  D --> E[Monitoring & Logging]","difficulty":"advanced","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Slack","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T14:51:17.288Z","createdAt":"2026-01-21T14:51:17.288Z"},{"id":"q-904","question":"How would you configure a Cloud Run (fully managed) service to securely connect to a Cloud SQL PostgreSQL instance using a private connection, including IAM bindings and deployment steps to ensure the app talks via the Cloud SQL socket and never uses the instance's public IP?","answer":"Grant the Cloud Run service account the roles/cloudsql.client, enable the sqladmin API, deploy with --add-cloudsql-instances PROJECT:REGION:INSTANCE, and connect using the Unix socket /cloudsql/INSTAN","explanation":"## Why This Is Asked\nTests private connectivity setup between Cloud Run and Cloud SQL and correct IAM binding.\n\n## Key Concepts\n- Cloud Run (Managed)\n- Cloud SQL (PostgreSQL)\n- IAM roles and private connections\n- gcloud deployment flags\n\n## Code Example\n```javascript\n// Node.js example connection string (pseudo)\nconst client = new Client({ connectionString: 'postgres://user:pass@localhost/db?host=/cloudsql/PROJECT:REGION:INSTANCE' });\n```\n\n## Follow-up Questions\n- How do you test the connection in a CI environment?\n- How would you rotate credentials without downtime?","diagram":null,"difficulty":"beginner","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["OpenAI","Slack","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T14:43:17.098Z","createdAt":"2026-01-12T14:43:17.098Z"}],"subChannels":["general"],"companies":["Adobe","Airbnb","Amazon","Anthropic","Apple","Bloomberg","Citadel","Cloudflare","Coinbase","Databricks","Discord","DoorDash","Goldman Sachs","Google","Hashicorp","Hugging Face","IBM","Instacart","LinkedIn","Lyft","Meta","Microsoft","MongoDB","NVIDIA","Netflix","OpenAI","Oracle","PayPal","Plaid","Robinhood","Salesforce","Scale Ai","Slack","Snap","Snowflake","Square","Stripe","Tesla","Twitter","Two Sigma","Uber","Zoom"],"stats":{"total":59,"beginner":22,"intermediate":16,"advanced":21,"newThisWeek":45}}