{"questions":[{"id":"q-1016","question":"Design a regional streaming pipeline for a fintech app: on‑prem and GKE emit events to region Pub/Sub topics; a Dataflow streaming job enforces exactly-once, writes partitioned regional BigQuery tables, and triggers Vertex AI scoring in near real-time. How would you achieve low latency, data residency, schema evolution, and reliable failure recovery?","answer":"Design a regional streaming pipeline: on‑prem and GKE emit to region Pub/Sub topics; Dataflow streaming enforces exactly-once, writes partitioned regional BigQuery tables, and triggers Vertex AI scori","explanation":"## Why This Is Asked\nThis question probes cross-region streaming, data residency, and production-ready fault tolerance in a real-time fraud pipeline.\n\n## Key Concepts\n- Regional Pub/Sub topics and fanout\n- Dataflow streaming with exactly-once\n- BigQuery regional storage and partitioning\n- Vertex AI scoring integration\n- Data residency, VPC Service Controls, IAM least privilege\n\n## Code Example\n```javascript\n// Pseudo-configuration for regional Pub/Sub and Dataflow wiring\nconst config = {\n  regions: ['us-east1','europe-west1'],\n  pubsubTopics: {\n    'us-east1': 'fraud-events-us-east1',\n    'europe-west1': 'fraud-events-eu-west1'\n  },\n  sinks: {\n    'BigQuery': 'project.dataset.fraud_events_$YYYYMM'\n  }\n};\n```\n\n## Follow-up Questions\n- How would you handle schema evolution for the events?\n- How do you ensure end-to-end latency remains under 150 ms during traffic spikes?","diagram":null,"difficulty":"advanced","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","PayPal","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T19:30:47.603Z","createdAt":"2026-01-12T19:30:47.603Z"},{"id":"q-1055","question":"Design an audit-logging pipeline for a payments platform on GCP with sub-100ms end-to-end write latency at multi-region scale (millions of events/sec). Ingest via Pub/Sub, process with Dataflow (Beam), and sink to BigQuery. Explain how you ensure idempotent writes (insertId), handle schema evolution, and provide auditor access across projects without exposing sensitive data. Also outline retries, backoffs, and monitoring?","answer":"Pub/Sub regional topic feeds a Dataflow (Beam) streaming job. Deduplicate by event_id in a stateful DoFn and write to BigQuery with insertId to achieve idempotent, replay-safe sinks. Run Dataflow work","explanation":"## Why This Is Asked\nEvaluates end-to-end streaming architecture, cross-project access, and data governance for payments.\n\n## Key Concepts\n- Pub/Sub streaming ingestion\n- Dataflow/Beam stateful dedup\n- BigQuery streaming inserts with insertId\n- Cross-project IAM for auditors\n- Private Google access and VPC Service Controls\n- Schema evolution with backward-compatible fields\n\n## Code Example\n```javascript\nconst bigQuery = require('@google-cloud/bigquery')();\nasync function sink(rows) {\n  await bigQuery.dataset('audit').table('events').insert(rows, {\n    insertId: rows.map(r => r.event_id)\n  });\n}\n```\n\n## Follow-up Questions\n- How would you test the dedup logic under bursty load?\n- How would you migrate schema without delaying streaming?\n","diagram":null,"difficulty":"advanced","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T20:38:50.148Z","createdAt":"2026-01-12T20:38:50.148Z"},{"id":"q-1105","question":"You're building a beginner-friendly ingestion pipeline: external partners upload daily CSVs to a Cloud Storage bucket; design a minimal flow using a Cloud Function triggered on object finalize to parse the CSV and load a daily summary as a single row into BigQuery. Include IAM permissions, how to trigger on new files, and how to ensure idempotent writes?","answer":"Configure a Cloud Function in Python or Node triggered by Cloud Storage object.finalize on the bucket. It reads the CSV, computes a file-level summary (rows, bytes, elapsed time) and writes one row to","explanation":"## Why This Is Asked\n\nAssesses practical ability to wire GCS events to a function and BigQuery with idempotence, using least-privilege IAM and bucket settings.\n\n## Key Concepts\n\n- GCS object.finalize event\n- Cloud Functions permissions\n- BigQuery insertId for idempotent writes\n- Uniform Bucket Access and IAM roles\n- Testing with sample file and re-uploads\n\n## Code Example\n\n```javascript\n// Node.js Cloud Function skeleton\nconst {BigQuery} = require('@google-cloud/bigquery');\nexports.ingestCSV = async (event) => {\n  const bucket = event.bucket;\n  const name = event.name;\n  // read file, compute summary, insert one row with insertId = name\n};\n```\n\n## Follow-up Questions\n\n- How handle large files and memory limits?\n- How to add retries and failure notifications?","diagram":null,"difficulty":"beginner","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Microsoft","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T22:35:43.917Z","createdAt":"2026-01-12T22:35:43.917Z"},{"id":"q-1269","question":"Design a real-time data pipeline on GCP for a multi-tenant SaaS where each tenant's data must reside in a specified region, is encrypted with CMEK, and access is strictly controlled per-tenant using IAM Conditions and VPC Service Controls; use Pub/Sub, Dataflow, and BigQuery, ensure idempotent writes and exactly-once semantics, and include monitoring and incident response steps?","answer":"Architect a region-scoped, CMEK-protected pipeline: publish tenant events to per-tenant Pub/Sub topics, Dataflow streaming to region-specific BigQuery datasets, enable CMEK on Cloud Storage and BigQue","explanation":"## Why This Is Asked\n\nTests ability to design a cross-tenant pipeline with data residency and security controls, spanning Pub/Sub, Dataflow, and BigQuery, while addressing reliability (idempotency) and observability.\n\n## Key Concepts\n\n- Data residency by tenant/region\n- CMEK on Storage and BigQuery\n- IAM Conditions and VPC Service Controls\n- Exactly-once vs at-least-once semantics\n- Idempotent writes and deduplication\n- Observability and incident response\n\n## Code Example\n\n```python\n# Example Beam pipeline skeleton (Python)\nimport apache_beam as beam\nimport json\nfrom apache_beam.options.pipeline_options import PipelineOptions\n\ndef to_row(msg):\n    obj = json.loads(msg.decode('utf-8'))\n    obj['row_id'] = f\"{obj['tenant']}_{obj['event_id']}\"\n    return obj\n\nopts = PipelineOptions(streaming=True, project='PROJECT')\nwith beam.Pipeline(options=opts) as p:\n    (p\n     | 'ReadFromPubSub' >> beam.io.ReadFromPubSub(topic='projects/PROJECT/topics/tenant-events')\n     | 'Parse' >> beam.Map(to_row)\n     | 'ToBQ' >> beam.io.WriteToBigQuery(\n           table='PROJECT:DATASET.TENANT_EVENTS',\n           schema='tenant STRING, event_id STRING, payload STRING, row_id STRING',\n           write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND,\n           create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED)\n    )\n```\n\n## Follow-up Questions\n\n- How would you validate data residency policy compliance across regions in production?\n- What strategies would you use to evolve the schema per-tenant without downtime?","diagram":null,"difficulty":"advanced","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","PayPal","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T07:34:14.972Z","createdAt":"2026-01-13T07:34:14.972Z"},{"id":"q-1331","question":"Design a beginner-friendly nightly backup workflow on GCP: a Cloud SQL MySQL export writes a dump to a Cloud Storage bucket each night; a Cloud Function triggers on the new object to gzip, append a date stamp, and move it to backups/archived. Include IAM bindings, trigger method on new files, and how to ensure exactly-one backup per day (idempotency)?","answer":"Use Cloud Scheduler to trigger a nightly Cloud SQL export to Cloud Storage; grant Cloud SQL SA write access to the bucket; create a Cloud Function triggered on objectFinalize to gzip and rename to bac","explanation":"## Why This Is Asked\nTests practical usage of scheduled exports, storage event triggers, and idempotent processing in a basic backup workflow. It also evaluates IAM scoping and lifecycle management.\n\n## Key Concepts\n- Cloud Scheduler for periodic tasks\n- Cloud SQL export to Cloud Storage\n- Cloud Functions triggered by storage events (objectFinalize)\n- Idempotent file naming and conditional processing\n- Storage Lifecycle rules for retention\n\n## Code Example\n```javascript\n// Cloud Function (Node.js) skeleton\nconst {Storage} = require('@google-cloud/storage');\nconst storage = new Storage();\n\nexports.backupPostProcess = async (event, context) => {\n  const bucketName = event.bucket;\n  const srcName = event.name; // e.g., dumps/db-20260113.sql.gz\n  const destName = srcName\n    .replace('dumps/', 'backups/archived/')\n    .replace(/\\.gz$/, '.sql.gz');\n  const bucket = storage.bucket(bucketName);\n  const dest = bucket.file(destName);\n  const [exists] = await dest.exists();\n  if (exists) return;\n  await bucket.file(srcName).copy(dest);\n  await bucket.file(srcName).delete();\n};\n```\n\n## Follow-up Questions\n- How would you verify idempotency across multiple runs?\n- How would you handle a failed export and implement retries with alerting?","diagram":null,"difficulty":"beginner","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","NVIDIA","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T11:40:25.703Z","createdAt":"2026-01-13T11:40:25.704Z"},{"id":"q-904","question":"How would you configure a Cloud Run (fully managed) service to securely connect to a Cloud SQL PostgreSQL instance using a private connection, including IAM bindings and deployment steps to ensure the app talks via the Cloud SQL socket and never uses the instance's public IP?","answer":"Grant the Cloud Run service account the roles/cloudsql.client, enable the sqladmin API, deploy with --add-cloudsql-instances PROJECT:REGION:INSTANCE, and connect using the Unix socket /cloudsql/INSTAN","explanation":"## Why This Is Asked\nTests private connectivity setup between Cloud Run and Cloud SQL and correct IAM binding.\n\n## Key Concepts\n- Cloud Run (Managed)\n- Cloud SQL (PostgreSQL)\n- IAM roles and private connections\n- gcloud deployment flags\n\n## Code Example\n```javascript\n// Node.js example connection string (pseudo)\nconst client = new Client({ connectionString: 'postgres://user:pass@localhost/db?host=/cloudsql/PROJECT:REGION:INSTANCE' });\n```\n\n## Follow-up Questions\n- How do you test the connection in a CI environment?\n- How would you rotate credentials without downtime?","diagram":null,"difficulty":"beginner","tags":["gcp-cloud-engineer"],"channel":"gcp-cloud-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["OpenAI","Slack","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T14:43:17.098Z","createdAt":"2026-01-12T14:43:17.098Z"}],"subChannels":["general"],"companies":["Adobe","Amazon","Discord","Google","Instacart","Microsoft","NVIDIA","OpenAI","PayPal","Salesforce","Slack","Snap","Stripe","Tesla"],"stats":{"total":6,"beginner":3,"intermediate":0,"advanced":3,"newThisWeek":6}}