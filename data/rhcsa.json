{"questions":[{"id":"rhcsa-configure-storage-1768225806334-0","question":"You have two unused disks, /dev/sdb and /dev/sdc. You need a 200 GiB data volume that survives reboot, can be grown later by adding space to the volume group, and is mounted at /data. Which sequence of steps is correct?","answer":"[{\"id\":\"a\",\"text\":\"pvcreate /dev/sdb /dev/sdc; vgcreate vg_data /dev/sdb /dev/sdc; lvcreate -L200G -n lv_data vg_data; mkfs.ext4 /dev/vg_data/lv_data; mkdir /data; mount /dev/vg_data/lv_data /data; Add a /etc/fstab entry mounting the LV at /data by its UUID.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"mdadm --create /dev/md0 --level=1 --raid-devices=2 /dev/sdb /dev/sdc; mkfs.ext4 /dev/md0; mkdir /data; mount /dev/md0 /data; Add an /etc/fstab entry using /dev/md0.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"pvcreate /dev/sdb; vgcreate vg_data /dev/sdb; lvcreate -L200G -n lv_data vg_data; mkfs.ext4 /dev/vg_data/lv_data; mkdir /data; mount /dev/vg_data/lv_data /data; This uses only one disk and ignores the second disk.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Create a 200G ext4 partition on /dev/sdb, format it, and mount at /data without using LVM; This does not allow later growth via the volume group.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA\n\n## Why Other Options Are Wrong\n- B is incorrect because it uses a RAID array (mdadm) without LVM, which does not satisfy the requirement of a data volume that can be grown later via the volume group.\n- C is incorrect because it uses only /dev/sdb, not the two disks, and fails to meet the two-disk configuration requirement.\n- D is incorrect because it bypasses LVM entirely, preventing seamless future growth through the VG/LV hierarchy.\n\n## Key Concepts\n- LVM basics: PV, VG, LV creation enables flexible growth.\n- Use UUIDs in /etc/fstab for reliable mounting across reboots.\n- Data volumes can be resized by expanding the LV and then resizing the filesystem.\n\n## Real-World Application\n- This pattern supports scalable storage for services that require growing data capacity without downtime, common in database or log-intensive workloads on enterprise servers.","diagram":null,"difficulty":"intermediate","tags":["RHCSA","LVM","RAID","AWS","Kubernetes","Terraform","certification-mcq","domain-weight-15"],"channel":"rhcsa","subChannel":"configure-storage","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:50:06.336Z","createdAt":"2026-01-12 13:50:06"},{"id":"rhcsa-configure-storage-1768225806334-1","question":"The system uses an LVM-based root filesystem mounted at /, with the LV /dev/mapper/rhclv-root extended by 50G. Which sequence correctly grows the filesystem to use the new space without reboot?","answer":"[{\"id\":\"a\",\"text\":\"lvextend -l +50G /dev/mapper/rhclv-root; resize2fs /dev/mapper/rhclv-root\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"lvextend -L +50G /dev/mapper/rhclv-root; xfs_growfs /\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"pvresize /dev/sda3; lvextend -L +50G /dev/mapper/rhclv-root; resize2fs /dev/mapper/rhclv-root\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"resize2fs /dev/mapper/rhclv-root; lvextend -l +50G /dev/mapper/rhclv-root\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA\n\n## Why Other Options Are Wrong\n- B is incorrect because it suggests using an XFS-specific grow operation on an ext4 filesystem.\n- C is incorrect because pvresize is used on physical volumes, not the root LV, and the sequence is not correct for online growth.\n- D is incorrect because resizing the filesystem before resizing the LV can fail if the LV has not been enlarged yet.\n\n## Key Concepts\n- LV extension (lvextend) followed by filesystem resize (resize2fs) expands capacity for ext4.\n- Online resizing is supported for ext4 on many distributions when the LV is enlarged first.\n\n## Real-World Application\n- This pattern is common when expanding the root filesystem to accommodate growing data without downtime, especially on servers with LVM-backed volumes.","diagram":null,"difficulty":"intermediate","tags":["RHCSA","LVM","Ext4","AWS","Kubernetes","certification-mcq","domain-weight-15"],"channel":"rhcsa","subChannel":"configure-storage","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:50:06.909Z","createdAt":"2026-01-12 13:50:07"},{"id":"rhcsa-configure-storage-1768225806334-2","question":"You want to add 4 GiB of swap on a system that currently has no swap partition. Which sequence correctly creates a persistent swap file and enables it at boot?","answer":"[{\"id\":\"a\",\"text\":\"fallocate -l 4G /swapfile; chmod 600 /swapfile; mkswap /swapfile; swapon /swapfile; echo '/swapfile none swap defaults 0 0' >> /etc/fstab\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Create a 4G swap partition on /dev/sdb1 and enable it; add an entry for /dev/sdb1 in /etc/fstab as swap\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"mkswap /swapfile; swapon /swapfile; echo '/swapfile swap defaults 0 0' >> /etc/fstab\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use a 4G swap file named /swapfile2 and enable it without updating /etc/fstab\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA\n\n## Why Other Options Are Wrong\n- B describes creating a swap partition, which contradicts the scenario that there is no swap partition and also diverges from a swap file approach.\n- C omits making the swap file eligible for swap with mkswap on the intended path, which is necessary before swapon.\n- D relies on a non-standard path and omits persistence in /etc/fstab, so the swap won’t survive reboots.\n\n## Key Concepts\n- Swap files are a valid modern alternative to swap partitions.\n- Persistence requires an /etc/fstab entry or equivalent configuration.\n\n## Real-World Application\n- Swap files are commonly used on systems that lack a separate swap partition or when the disk layout must stay dynamic without repartitioning.","diagram":null,"difficulty":"intermediate","tags":["RHCSA","swap","Ext4","AWS","Kubernetes","certification-mcq","domain-weight-15"],"channel":"rhcsa","subChannel":"configure-storage","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:50:07.427Z","createdAt":"2026-01-12 13:50:07"},{"id":"rhcsa-configure-storage-1768225806334-3","question":"You need to set up a software RAID1 across /dev/sdb1 and /dev/sdc1, then use the resulting array as a filesystem /raid. Which sequence is correct to ensure the system boots with the RAID available after reboot?","answer":"[{\"id\":\"a\",\"text\":\"mdadm --create /dev/md0 --level=1 --raid-devices=2 /dev/sdb1 /dev/sdc1; mkfs.ext4 /dev/md0; mkdir /raid; mount /dev/md0 /raid; mdadm --detail --scan >> /etc/mdadm/mdadm.conf; update-initramfs -u\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"mdadm --assemble /dev/md0 --scan; mkfs.ext4 /dev/md0; mkdir /raid; mount /dev/md0 /raid\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"mdadm --create /dev/md0 --level=0 --raid-devices=2 /dev/sdb1 /dev/sdc1; mkfs.ext4 /dev/md0; mount /dev/md0 /raid\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Convert existing /dev/sdb1 and /dev/sdc1 to a RAID1 without using mdadm; then format /dev/md0\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA\n\n## Why Other Options Are Wrong\n- B uses --assemble without ensuring a proper initial create, which may fail if no array exists yet.\n- C uses a RAID0 level, which provides no redundancy and does not meet the requirement of RAID1.\n- D describes an unsupported approach of converting existing partitions into RAID without mdadm management.\n\n## Key Concepts\n- mdadm is used to create and manage software RAID arrays.\n- After creating RAID, format and mount, and ensure mdadm.conf is updated for boot-time assembly.\n\n## Real-World Application\n- Software RAID1 is a common pattern for protecting data against disk failure in servers; ensuring the array is assembled at boot guarantees availability after reboot.","diagram":null,"difficulty":"intermediate","tags":["RHCSA","RAID","mdadm","AWS","Kubernetes","certification-mcq","domain-weight-15"],"channel":"rhcsa","subChannel":"configure-storage","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:50:07.608Z","createdAt":"2026-01-12 13:50:07"},{"id":"rhcsa-configure-storage-1768225806334-4","question":"You want to enable user and group quotas on /home (ext4). Which steps are correct to enable quotas and enforce them?","answer":"[{\"id\":\"a\",\"text\":\"Add usrquota and grpquota to /etc/fstab for /home; remount /home with quotas enabled; quotacheck -cug /home; quotaon /home; edquota -u username\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Only run quotaon without enabling quotas in fstab; quotas will be active automatically\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use tune2fs to enable quotas on the filesystem and do not run quotacheck\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Set quotas by using setquota without enabling quota infrastructure or fstab changes\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA\n\n## Why Other Options Are Wrong\n- B is incorrect because quota must be enabled via fstab remount or quotaon after the quota flags are set; simply running quotaon without enabling quotas in fstab is insufficient.\n- C is incorrect because tune2fs can enable quota flags, but quotas also require quotacheck and quotaon to be active; relying solely on tune2fs is incomplete.\n- D is incorrect because setquota requires quotas infrastructure to be active and quotas to be enabled via fstab/remount; without enabling, quotas won’t enforce.\n\n## Key Concepts\n- Quotas require filesystem support (usrquota/grpquota) and quotacheck/quotaon to execute.\n- fstab persistence ensures quotas survive reboots.\n\n## Real-World Application\n- Enforcing user and group quotas helps manage shared storage on multi-tenant systems and prevent any single user from exhausting /home.","diagram":null,"difficulty":"intermediate","tags":["RHCSA","Quotas","Ext4","AWS","Kubernetes","certification-mcq","domain-weight-15"],"channel":"rhcsa","subChannel":"configure-storage","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:50:07.789Z","createdAt":"2026-01-12 13:50:07"},{"id":"rhcsa-essential-tools-1768166429187-0","question":"During configuration maintenance, you need to replace the string 'listen 8080' with 'listen 80' in all nginx configuration files under /etc/nginx, and you want to keep a backup file for each original. Which command sequence achieves this safely?","answer":"[{\"id\":\"a\",\"text\":\"find /etc/nginx -type f -name '*.conf' -exec sed -i.bak 's/listen 8080/listen 80/' {} +\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"find /etc/nginx -type f -name '*.conf' -exec sed -i 's/listen 8080/listen 80/' {} +\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"sed -i 's/listen 8080/listen 80/' /etc/nginx/*.conf\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"tar -czf /backup/nginx.tar.gz -C /etc/nginx .\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA. Uses find to locate all .conf files under /etc/nginx, and applies sed -i.bak to edit in place while creating a backup for each file. This is safer for bulk changes.\n\n## Why Other Options Are Wrong\n- B: Edits files in place but does not create backups, risking data loss on mistakes.\n- C: Only affects a single directory level and may miss files in subdirectories.\n- D: Creates an archive instead of applying in-file edits.\n\n## Key Concepts\n- find -type f to locate files\n- -exec ... {} + to apply commands to multiple files\n- sed -i.bak to edit in place and create a backup with .bak extension\n\n## Real-World Application\nUsed when performing bulk script-driven config changes across multiple config files while preserving original copies for rollback.","diagram":null,"difficulty":"intermediate","tags":["RHCSA","Linux","CLI","Sed","Find","Nginx","AWS","Kubernetes","Terraform","certification-mcq","domain-weight-20"],"channel":"rhcsa","subChannel":"essential-tools","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T21:20:29.188Z","createdAt":"2026-01-11 21:20:29"},{"id":"rhcsa-essential-tools-1768166429187-1","question":"During maintenance, you want to verify the contents of a gzip-compressed tarball site.tar.gz without extracting it. Which command would you use?","answer":"[{\"id\":\"a\",\"text\":\"tar -tzf site.tar.gz\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"tar -tf site.tar.gz\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"tar -xzf site.tar.gz\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"tar -czf site.tar.gz\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA. tar -tzf site.tar.gz lists the contents of a gzip-compressed tarball without extracting.\n\n## Why Other Options Are Wrong\n- B: tar -tf may not correctly handle gzip compression on some tar implementations unless combined with -z; in many environments, -z is required.\n- C: tar -xzf extracts the archive, not just listing contents.\n- D: tar -czf creates a new archive, not listing contents.\n\n## Key Concepts\n- -t lists contents, -z for gzip, -f specifies archive file\n\n## Real-World Application\nUsed during maintenance to audit archive contents before deployment or extraction.","diagram":null,"difficulty":"intermediate","tags":["RHCSA","Tar","Gzip","Linux","Nginx","AWS","Kubernetes","Terraform","certification-mcq","domain-weight-20"],"channel":"rhcsa","subChannel":"essential-tools","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T21:20:29.967Z","createdAt":"2026-01-11 21:20:30"},{"id":"rhcsa-essential-tools-1768166429187-2","question":"From /etc/passwd, print usernames for accounts with UID >= 1000 and login shells that do not contain the string 'nologin'. Which command achieves this?","answer":"[{\"id\":\"a\",\"text\":\"awk -F: '$3>=1000 && $7 !~ /nologin/ {print $1}' /etc/passwd\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"awk -F: '$3>=1000 && $7 != \\\"/sbin/nologin\\\" {print $1}' /etc/passwd\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"awk -F: '$3>=1000 {print $1}' /etc/passwd\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"awk -F: '$3<1000 {print $1}' /etc/passwd\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA. Uses the UID field (third field) to filter >=1000 and checks that the login shell (seventh field) does not contain the string nologin, printing the username (first field).\n\n## Why Other Options Are Wrong\n- B: Excludes only the exact path /sbin/nologin; many systems use other paths such as /usr/sbin/nologin, which would still be non-login shells.\n- C: Does not filter out non-login shells.\n- D: Filters by UID < 1000, which selects system accounts, not human users.\n\n## Key Concepts\n- Field delimiter -F: for /etc/passwd format (user:pass:uid:gname:home:shell)\n- UID threshold for human users (>=1000)\n- Pattern matching to exclude login shells containing nologin\n\n## Real-World Application\nHelps reduce audit scope to real human users for access reviews and onboarding/offboarding tasks.","diagram":null,"difficulty":"intermediate","tags":["RHCSA","Awk","Passwd","UserManagement","Linux","AWS","Kubernetes","Terraform","certification-mcq","domain-weight-20"],"channel":"rhcsa","subChannel":"essential-tools","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T21:20:30.513Z","createdAt":"2026-01-11 21:20:30"},{"id":"rhcsa-operate-systems-1768206719121-0","question":"A server uses LVM with a dedicated logical volume lv_home mounted on /home. The volume group is vg00. You need to extend /home by 10G without rebooting. Which sequence of commands achieves this correctly on a Red Hat system with XFS on /home?","answer":"[{\"id\":\"a\",\"text\":\"lvextend -L +10G /dev/vg00/lv_home; xfs_growfs /home\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"lvextend -L +12G /dev/vg00/lv_home; resize2fs /home\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"lvresize -L +10G /dev/vg00/lv_home; xfs_growfs /home\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"lvextend -L +10G /dev/vg00/lv_home; xfs_growfs /\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA. lvextend -L +10G /dev/vg00/lv_home; xfs_growfs /home\n\nThis extends the LV by 10G and expands the XFS filesystem live. The other options are incorrect: B uses resize2fs on XFS (not supported); C uses lvresize (works, but not the canonical command in this scenario); D grows the wrong filesystem, targeting the root rather than /home.\n\n## Why Other Options Are Wrong\n- B: resize2fs does not support XFS and cannot resize /home in this scenario.\n- C: lvresize is a valid LV operation but uses a different tool; not the standard approach here.\n- D: Attempts to grow the wrong filesystem mountpoint (/ instead of /home).\n\n## Key Concepts\n- LVM lvextend vs lvresize\n- Online filesystem resizing for XFS\n- Mountpoints and filesystem types\n\n## Real-World Application\nEngineers often need to quickly scale storage for user data without rebooting or downtime, relying on LVM and XFS capabilities.\n","diagram":null,"difficulty":"intermediate","tags":["RHCSA","Operate Running Systems","LVM","SELinux","firewalld","systemd","AWS","EC2","Kubernetes","Terraform","certification-mcq","domain-weight-20"],"channel":"rhcsa","subChannel":"operate-systems","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T08:31:59.122Z","createdAt":"2026-01-12 08:31:59"},{"id":"rhcsa-operate-systems-1768206719121-1","question":"An administrator moves the content directory for a web site from /var/www/html to a new mount at /mnt/storage/www. After mounting, the web server cannot serve the content due to SELinux context mismatch. Which sequence ensures the new location has proper persistent SELinux context?","answer":"[{\"id\":\"a\",\"text\":\"semanage fcontext -a -t httpd_sys_content_t '/mnt/storage(/.*)?'; restorecon -R -v /mnt/storage\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"chcon -R -t httpd_sys_content_t /mnt/storage\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"restorecon -R -v /mnt/storage/www\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"setenforce 0\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA. semanage fcontext -a -t httpd_sys_content_t '/mnt/storage(/.*)?'; restorecon -R -v /mnt/storage\n\nThis adds a persistent SELinux file context mapping for the new storage location and applies it recursively. B applies a temporary context change that won’t persist across relabels. C only runs restorecon without a proper persistent mapping, and D disables SELinux entirely, which is insecure.\n\n## Why Other Options Are Wrong\n- B: chcon changes context only temporarily and won’t survive relabels or reboots.\n- C: restorecon without proper fcontext mapping has no lasting effect for the new path.\n- D: Disabling SELinux is insecure and bypasses policy enforcement.\n\n## Key Concepts\n- Persistent SELinux contexts with semanage fcontext\n- httpd_sys_content_t for web content\n- restorecon usage\n\n## Real-World Application\nWeb admins relocate web content and must preserve secure, policy-driven access without disabling security controls.\n","diagram":null,"difficulty":"intermediate","tags":["RHCSA","Operate Running Systems","SELinux","httpd","systemd","AWS","EC2","Kubernetes","Terraform","certification-mcq","domain-weight-20"],"channel":"rhcsa","subChannel":"operate-systems","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T08:31:59.533Z","createdAt":"2026-01-12 08:31:59"},{"id":"rhcsa-operate-systems-1768206719121-2","question":"You need to expose a service on port 8080 to external clients. The server uses firewalld and is in the public zone. Which command sequence opens the port permanently and makes it survive reboot?","answer":"[{\"id\":\"a\",\"text\":\"firewall-cmd --zone=public --add-port=8080/tcp --permanent; firewall-cmd --reload\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"firewall-cmd --zone=public --add-service=http --permanent; firewall-cmd --reload\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"firewall-cmd --zone=public --remove-port=8080/tcp; firewall-cmd --reload\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"firewall-cmd --zone=internal --add-port=8080/tcp --permanent; systemctl restart firewalld\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA. firewall-cmd --zone=public --add-port=8080/tcp --permanent; firewall-cmd --reload\n\nThis permanently opens port 8080 in the public zone and reloads the firewall rules. B opens a different port (HTTP default 80) rather than 8080. C removes the port, defeating the requirement. D uses the wrong zone and an unnecessary restart.\n\n## Why Other Options Are Wrong\n- B: Opens port 80 (HTTP) instead of 8080, not meeting the requirement.\n- C: Removes the port, opposite of the goal.\n- D: Uses the wrong zone and relies on a restart rather than a reload; the zone consistency is wrong.\n\n## Key Concepts\n- firewalld permanent rules\n- Correct zone alignment\n- Persistence across reboots\n\n## Real-World Application\nExposing services securely requires precise firewall configuration that survives system restarts.\n","diagram":null,"difficulty":"intermediate","tags":["RHCSA","Operate Running Systems","firewalld","systemd","AWS","EC2","Kubernetes","Terraform","certification-mcq","domain-weight-20"],"channel":"rhcsa","subChannel":"operate-systems","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T08:31:59.944Z","createdAt":"2026-01-12 08:32:00"}],"subChannels":["configure-storage","essential-tools","operate-systems"],"companies":[],"stats":{"total":11,"beginner":0,"intermediate":11,"advanced":0,"newThisWeek":11}}