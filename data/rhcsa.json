{"questions":[{"id":"rhcsa-configure-storage-1768225806334-0","question":"You have two unused disks, /dev/sdb and /dev/sdc. You need a 200 GiB data volume that survives reboot, can be grown later by adding space to the volume group, and is mounted at /data. Which sequence of steps is correct?","answer":"[{\"id\":\"a\",\"text\":\"pvcreate /dev/sdb /dev/sdc; vgcreate vg_data /dev/sdb /dev/sdc; lvcreate -L200G -n lv_data vg_data; mkfs.ext4 /dev/vg_data/lv_data; mkdir /data; mount /dev/vg_data/lv_data /data; Add a /etc/fstab entry mounting the LV at /data by its UUID.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"mdadm --create /dev/md0 --level=1 --raid-devices=2 /dev/sdb /dev/sdc; mkfs.ext4 /dev/md0; mkdir /data; mount /dev/md0 /data; Add an /etc/fstab entry using /dev/md0.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"pvcreate /dev/sdb; vgcreate vg_data /dev/sdb; lvcreate -L200G -n lv_data vg_data; mkfs.ext4 /dev/vg_data/lv_data; mkdir /data; mount /dev/vg_data/lv_data /data; This uses only one disk and ignores the second disk.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Create a 200G ext4 partition on /dev/sdb, format it, and mount at /data without using LVM; This does not allow later growth via the volume group.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA\n\n## Why Other Options Are Wrong\n- B is incorrect because it uses a RAID array (mdadm) without LVM, which does not satisfy the requirement of a data volume that can be grown later via the volume group.\n- C is incorrect because it uses only /dev/sdb, not the two disks, and fails to meet the two-disk configuration requirement.\n- D is incorrect because it bypasses LVM entirely, preventing seamless future growth through the VG/LV hierarchy.\n\n## Key Concepts\n- LVM basics: PV, VG, LV creation enables flexible growth.\n- Use UUIDs in /etc/fstab for reliable mounting across reboots.\n- Data volumes can be resized by expanding the LV and then resizing the filesystem.\n\n## Real-World Application\n- This pattern supports scalable storage for services that require growing data capacity without downtime, common in database or log-intensive workloads on enterprise servers.","diagram":null,"difficulty":"intermediate","tags":["RHCSA","LVM","RAID","AWS","Kubernetes","Terraform","certification-mcq","domain-weight-15"],"channel":"rhcsa","subChannel":"configure-storage","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:50:06.336Z","createdAt":"2026-01-12 13:50:06"},{"id":"rhcsa-configure-storage-1768225806334-1","question":"The system uses an LVM-based root filesystem mounted at /, with the LV /dev/mapper/rhclv-root extended by 50G. Which sequence correctly grows the filesystem to use the new space without reboot?","answer":"[{\"id\":\"a\",\"text\":\"lvextend -l +50G /dev/mapper/rhclv-root; resize2fs /dev/mapper/rhclv-root\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"lvextend -L +50G /dev/mapper/rhclv-root; xfs_growfs /\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"pvresize /dev/sda3; lvextend -L +50G /dev/mapper/rhclv-root; resize2fs /dev/mapper/rhclv-root\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"resize2fs /dev/mapper/rhclv-root; lvextend -l +50G /dev/mapper/rhclv-root\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA\n\n## Why Other Options Are Wrong\n- B is incorrect because it suggests using an XFS-specific grow operation on an ext4 filesystem.\n- C is incorrect because pvresize is used on physical volumes, not the root LV, and the sequence is not correct for online growth.\n- D is incorrect because resizing the filesystem before resizing the LV can fail if the LV has not been enlarged yet.\n\n## Key Concepts\n- LV extension (lvextend) followed by filesystem resize (resize2fs) expands capacity for ext4.\n- Online resizing is supported for ext4 on many distributions when the LV is enlarged first.\n\n## Real-World Application\n- This pattern is common when expanding the root filesystem to accommodate growing data without downtime, especially on servers with LVM-backed volumes.","diagram":null,"difficulty":"intermediate","tags":["RHCSA","LVM","Ext4","AWS","Kubernetes","certification-mcq","domain-weight-15"],"channel":"rhcsa","subChannel":"configure-storage","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:50:06.909Z","createdAt":"2026-01-12 13:50:07"},{"id":"rhcsa-configure-storage-1768225806334-2","question":"You want to add 4 GiB of swap on a system that currently has no swap partition. Which sequence correctly creates a persistent swap file and enables it at boot?","answer":"[{\"id\":\"a\",\"text\":\"fallocate -l 4G /swapfile; chmod 600 /swapfile; mkswap /swapfile; swapon /swapfile; echo '/swapfile none swap defaults 0 0' >> /etc/fstab\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Create a 4G swap partition on /dev/sdb1 and enable it; add an entry for /dev/sdb1 in /etc/fstab as swap\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"mkswap /swapfile; swapon /swapfile; echo '/swapfile swap defaults 0 0' >> /etc/fstab\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use a 4G swap file named /swapfile2 and enable it without updating /etc/fstab\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA\n\n## Why Other Options Are Wrong\n- B describes creating a swap partition, which contradicts the scenario that there is no swap partition and also diverges from a swap file approach.\n- C omits making the swap file eligible for swap with mkswap on the intended path, which is necessary before swapon.\n- D relies on a non-standard path and omits persistence in /etc/fstab, so the swap won’t survive reboots.\n\n## Key Concepts\n- Swap files are a valid modern alternative to swap partitions.\n- Persistence requires an /etc/fstab entry or equivalent configuration.\n\n## Real-World Application\n- Swap files are commonly used on systems that lack a separate swap partition or when the disk layout must stay dynamic without repartitioning.","diagram":null,"difficulty":"intermediate","tags":["RHCSA","swap","Ext4","AWS","Kubernetes","certification-mcq","domain-weight-15"],"channel":"rhcsa","subChannel":"configure-storage","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:50:07.427Z","createdAt":"2026-01-12 13:50:07"},{"id":"rhcsa-configure-storage-1768225806334-3","question":"You need to set up a software RAID1 across /dev/sdb1 and /dev/sdc1, then use the resulting array as a filesystem /raid. Which sequence is correct to ensure the system boots with the RAID available after reboot?","answer":"[{\"id\":\"a\",\"text\":\"mdadm --create /dev/md0 --level=1 --raid-devices=2 /dev/sdb1 /dev/sdc1; mkfs.ext4 /dev/md0; mkdir /raid; mount /dev/md0 /raid; mdadm --detail --scan >> /etc/mdadm/mdadm.conf; update-initramfs -u\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"mdadm --assemble /dev/md0 --scan; mkfs.ext4 /dev/md0; mkdir /raid; mount /dev/md0 /raid\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"mdadm --create /dev/md0 --level=0 --raid-devices=2 /dev/sdb1 /dev/sdc1; mkfs.ext4 /dev/md0; mount /dev/md0 /raid\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Convert existing /dev/sdb1 and /dev/sdc1 to a RAID1 without using mdadm; then format /dev/md0\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA\n\n## Why Other Options Are Wrong\n- B uses --assemble without ensuring a proper initial create, which may fail if no array exists yet.\n- C uses a RAID0 level, which provides no redundancy and does not meet the requirement of RAID1.\n- D describes an unsupported approach of converting existing partitions into RAID without mdadm management.\n\n## Key Concepts\n- mdadm is used to create and manage software RAID arrays.\n- After creating RAID, format and mount, and ensure mdadm.conf is updated for boot-time assembly.\n\n## Real-World Application\n- Software RAID1 is a common pattern for protecting data against disk failure in servers; ensuring the array is assembled at boot guarantees availability after reboot.","diagram":null,"difficulty":"intermediate","tags":["RHCSA","RAID","mdadm","AWS","Kubernetes","certification-mcq","domain-weight-15"],"channel":"rhcsa","subChannel":"configure-storage","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:50:07.608Z","createdAt":"2026-01-12 13:50:07"},{"id":"rhcsa-configure-storage-1768225806334-4","question":"You want to enable user and group quotas on /home (ext4). Which steps are correct to enable quotas and enforce them?","answer":"[{\"id\":\"a\",\"text\":\"Add usrquota and grpquota to /etc/fstab for /home; remount /home with quotas enabled; quotacheck -cug /home; quotaon /home; edquota -u username\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Only run quotaon without enabling quotas in fstab; quotas will be active automatically\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use tune2fs to enable quotas on the filesystem and do not run quotacheck\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Set quotas by using setquota without enabling quota infrastructure or fstab changes\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA\n\n## Why Other Options Are Wrong\n- B is incorrect because quota must be enabled via fstab remount or quotaon after the quota flags are set; simply running quotaon without enabling quotas in fstab is insufficient.\n- C is incorrect because tune2fs can enable quota flags, but quotas also require quotacheck and quotaon to be active; relying solely on tune2fs is incomplete.\n- D is incorrect because setquota requires quotas infrastructure to be active and quotas to be enabled via fstab/remount; without enabling, quotas won’t enforce.\n\n## Key Concepts\n- Quotas require filesystem support (usrquota/grpquota) and quotacheck/quotaon to execute.\n- fstab persistence ensures quotas survive reboots.\n\n## Real-World Application\n- Enforcing user and group quotas helps manage shared storage on multi-tenant systems and prevent any single user from exhausting /home.","diagram":null,"difficulty":"intermediate","tags":["RHCSA","Quotas","Ext4","AWS","Kubernetes","certification-mcq","domain-weight-15"],"channel":"rhcsa","subChannel":"configure-storage","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:50:07.789Z","createdAt":"2026-01-12 13:50:07"},{"id":"rhcsa-deploy-configure-1768253031047-0","question":"To allow httpd to serve content located under /srv/myapp when SELinux is enforcing, which file context should you apply to those files?","answer":"[{\"id\":\"a\",\"text\":\"httpd_sys_content_t\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"var_www_t\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"public_content_t\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"usr_t\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption a) httpd_sys_content_t is the correct file context for content served by httpd.\n\n## Why Other Options Are Wrong\n- Option b: var_www_t is used for content under /var/www, not /srv/myapp.\n- Option c: public_content_t is a generic type not specific to httpd serving.\n- Option d: usr_t is for user binaries, not web content.\n\n## Key Concepts\n- SELinux file contexts\n- httpd_sys_content_t for web content\n- Contexts must match the service accessing the files\n\n## Real-World Application\nWhen moving web content to a non-default directory, labeling with the correct SELinux type ensures the web server can read files without relaxing security.\n","diagram":null,"difficulty":"intermediate","tags":["RHCSA","SELinux","Kubernetes","certification-mcq","domain-weight-15"],"channel":"rhcsa","subChannel":"deploy-configure","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T21:23:51.048Z","createdAt":"2026-01-12 21:23:51"},{"id":"rhcsa-deploy-configure-1768253031047-1","question":"A custom service unit file myapp.service on a RHCSA system should automatically restart after a failure, with a 30-second delay, but must not restart if it exits cleanly. Which set of systemd directives in the unit file implements this behavior?","answer":"[{\"id\":\"a\",\"text\":\"Restart=on-failure; RestartSec=30s\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Restart=always; RestartSec=30s\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Restart=on-failure; RestartSec=0s\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Restart=never; RestartSec=30s\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption a) Restart=on-failure and RestartSec=30s ensures the service restarts after failures only, with a 30-second delay. It does not restart if the process exits cleanly.\n\n## Why Other Options Are Wrong\n- Option b: Restart=always restarts regardless of exit status, which includes successful exits.\n- Option c: RestartSec=0s would cause immediate restarts on failure, potentially causing thrashing.\n- Option d: Restart=never prevents restarts entirely.\n\n## Key Concepts\n- systemd Restart settings control when a unit is restarted\n- Restart=on-failure and RestartSec define delayed, conditional restarts\n\n## Real-World Application\nThis pattern is common for long-running services where transient failures should be retried, but clean exits should not cause restarts.\n","diagram":null,"difficulty":"intermediate","tags":["RHCSA","Kubernetes","AWS","certification-mcq","domain-weight-15"],"channel":"rhcsa","subChannel":"deploy-configure","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T21:23:51.804Z","createdAt":"2026-01-12 21:23:52"},{"id":"rhcsa-deploy-configure-1768253031047-2","question":"A database service on port 3306 should be accessible only from the app server 192.168.1.10. Which firewalld rich-rule correctly implements this?","answer":"[{\"id\":\"a\",\"text\":\"firewall-cmd --permanent --zone=public --add-rich-rule 'rule family=\\\"ipv4\\\" source address=\\\"192.168.1.10/32\\\" port protocol=\\\"tcp\\\" port=\\\"3306\\\" accept'\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"firewall-cmd --permanent --zone=public --add-rich-rule 'rule family=\\\"ipv4\\\" source address=\\\"0.0.0.0/0\\\" port protocol=\\\"tcp\\\" port=\\\"3306\\\" accept'\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"firewall-cmd --permanent --zone=public --add-service=mysql\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"firewall-cmd --permanent --zone=public --add-rich-rule 'rule family=\\\"ipv4\\\" destination address=\\\"192.168.1.10\\\" port protocol=\\\"tcp\\\" port=\\\"3306\\\" accept'\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption a) correctly binds access to 192.168.1.10/32 for TCP port 3306.\n\n## Why Other Options Are Wrong\n- Option b: Allows access from any source, defeating the restriction.\n- Option c: Uses a service name; may not map strictly to port 3306 and can be broader.\n- Option d: Uses destination address instead of restricting by source, which is not the intended restriction.\n\n## Key Concepts\n- Firewalld rich rules for source-based access control\n- Port-level restrictions for database services\n\n## Real-World Application\nRestricting DB access to a known app server reduces exposure and enforces network separation in production environments.\n","diagram":null,"difficulty":"intermediate","tags":["RHCSA","Kubernetes","AWS","certification-mcq","domain-weight-15"],"channel":"rhcsa","subChannel":"deploy-configure","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T21:23:52.344Z","createdAt":"2026-01-12 21:23:52"},{"id":"rhcsa-deploy-configure-1768253031047-3","question":"You have an LV mounted at /data using XFS. You need to extend the LV by 20G and then grow the filesystem to use the new space. Which command sequence will perform this correctly?","answer":"[{\"id\":\"a\",\"text\":\"lvextend -L +20G /dev/vg0/data; xfs_growfs /data\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"lvextend -L +20G /dev/vg0/data; resize2fs /data\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"lvresize -L +20G /dev/vg0/data; xfs_growfs /data\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"vgextend vg0 /dev/sdb; xfs_growfs /data\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption a) extends the logical volume and then grows the XFS filesystem on the mounted path.\n\n## Why Other Options Are Wrong\n- Option b: resize2fs is for ext filesystem families, not XFS.\n- Option c: lvresize is a valid alternative to lvextend, but here the exact sequence prefers lvextend per standard practice.\n- Option d: vgextend is used to extend a volume group with a new physical volume, not to grow an existing LV, and the syntax here is incorrect for this task.\n\n## Key Concepts\n- lvextend and xfs_growfs for XFS volumes\n- Proper sequence: extend LV, then grow filesystem\n\n## Real-World Application\nAllocating additional storage to an active XFS filesystem without downtime is a common maintenance task on production servers.\n","diagram":null,"difficulty":"intermediate","tags":["RHCSA","LVM","Kubernetes","certification-mcq","domain-weight-15"],"channel":"rhcsa","subChannel":"deploy-configure","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T21:23:52.526Z","createdAt":"2026-01-12 21:23:52"},{"id":"rhcsa-deploy-configure-1768253031047-4","question":"You want to back up /etc and /var/www to an NFS mount at /mnt/backups, preserving permissions, ownership, and links, and deleting files that no longer exist in the source. Which rsync command achieves this?","answer":"[{\"id\":\"a\",\"text\":\"rsync -a --delete /etc /var/www /mnt/backups\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"rsync -a --delete --ignore-errors /etc /var/www /mnt/backups\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"rsync -r --delete /etc /var/www /mnt/backups\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"rsync -a --delete --exclude='**/*.tmp' /etc /var/www /mnt/backups\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption a) uses -a to preserve permissions, ownership, and links, and --delete to synchronize deletions.\n\n## Why Other Options Are Wrong\n- Option b: --ignore-errors ignores permission-denied errors, which can hide real issues during backup.\n- Option c: -r alone does not preserve all attributes covered by -a (permissions, ownership, timestamps, etc.).\n- Option d: Excludes certain files; this defeats the goal of a full backup of /etc and /var/www.\n\n## Key Concepts\n- Rsync archive mode (-a) preserves metadata\n- --delete mirrors deletions from source to backup\n\n## Real-World Application\nRegular, exact backups of key system configuration and web content to an NFS share are essential for rapid recovery after failures.\n","diagram":null,"difficulty":"intermediate","tags":["RHCSA","NFS","Kubernetes","certification-mcq","domain-weight-15"],"channel":"rhcsa","subChannel":"deploy-configure","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T21:23:52.705Z","createdAt":"2026-01-12 21:23:52"},{"id":"rhcsa-essential-tools-1768166429187-0","question":"During configuration maintenance, you need to replace the string 'listen 8080' with 'listen 80' in all nginx configuration files under /etc/nginx, and you want to keep a backup file for each original. Which command sequence achieves this safely?","answer":"[{\"id\":\"a\",\"text\":\"find /etc/nginx -type f -name '*.conf' -exec sed -i.bak 's/listen 8080/listen 80/' {} +\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"find /etc/nginx -type f -name '*.conf' -exec sed -i 's/listen 8080/listen 80/' {} +\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"sed -i 's/listen 8080/listen 80/' /etc/nginx/*.conf\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"tar -czf /backup/nginx.tar.gz -C /etc/nginx .\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA. Uses find to locate all .conf files under /etc/nginx, and applies sed -i.bak to edit in place while creating a backup for each file. This is safer for bulk changes.\n\n## Why Other Options Are Wrong\n- B: Edits files in place but does not create backups, risking data loss on mistakes.\n- C: Only affects a single directory level and may miss files in subdirectories.\n- D: Creates an archive instead of applying in-file edits.\n\n## Key Concepts\n- find -type f to locate files\n- -exec ... {} + to apply commands to multiple files\n- sed -i.bak to edit in place and create a backup with .bak extension\n\n## Real-World Application\nUsed when performing bulk script-driven config changes across multiple config files while preserving original copies for rollback.","diagram":null,"difficulty":"intermediate","tags":["RHCSA","Linux","CLI","Sed","Find","Nginx","AWS","Kubernetes","Terraform","certification-mcq","domain-weight-20"],"channel":"rhcsa","subChannel":"essential-tools","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T21:20:29.188Z","createdAt":"2026-01-11 21:20:29"},{"id":"rhcsa-essential-tools-1768166429187-1","question":"During maintenance, you want to verify the contents of a gzip-compressed tarball site.tar.gz without extracting it. Which command would you use?","answer":"[{\"id\":\"a\",\"text\":\"tar -tzf site.tar.gz\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"tar -tf site.tar.gz\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"tar -xzf site.tar.gz\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"tar -czf site.tar.gz\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA. tar -tzf site.tar.gz lists the contents of a gzip-compressed tarball without extracting.\n\n## Why Other Options Are Wrong\n- B: tar -tf may not correctly handle gzip compression on some tar implementations unless combined with -z; in many environments, -z is required.\n- C: tar -xzf extracts the archive, not just listing contents.\n- D: tar -czf creates a new archive, not listing contents.\n\n## Key Concepts\n- -t lists contents, -z for gzip, -f specifies archive file\n\n## Real-World Application\nUsed during maintenance to audit archive contents before deployment or extraction.","diagram":null,"difficulty":"intermediate","tags":["RHCSA","Tar","Gzip","Linux","Nginx","AWS","Kubernetes","Terraform","certification-mcq","domain-weight-20"],"channel":"rhcsa","subChannel":"essential-tools","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T21:20:29.967Z","createdAt":"2026-01-11 21:20:30"},{"id":"rhcsa-essential-tools-1768166429187-2","question":"From /etc/passwd, print usernames for accounts with UID >= 1000 and login shells that do not contain the string 'nologin'. Which command achieves this?","answer":"[{\"id\":\"a\",\"text\":\"awk -F: '$3>=1000 && $7 !~ /nologin/ {print $1}' /etc/passwd\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"awk -F: '$3>=1000 && $7 != \\\"/sbin/nologin\\\" {print $1}' /etc/passwd\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"awk -F: '$3>=1000 {print $1}' /etc/passwd\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"awk -F: '$3<1000 {print $1}' /etc/passwd\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA. Uses the UID field (third field) to filter >=1000 and checks that the login shell (seventh field) does not contain the string nologin, printing the username (first field).\n\n## Why Other Options Are Wrong\n- B: Excludes only the exact path /sbin/nologin; many systems use other paths such as /usr/sbin/nologin, which would still be non-login shells.\n- C: Does not filter out non-login shells.\n- D: Filters by UID < 1000, which selects system accounts, not human users.\n\n## Key Concepts\n- Field delimiter -F: for /etc/passwd format (user:pass:uid:gname:home:shell)\n- UID threshold for human users (>=1000)\n- Pattern matching to exclude login shells containing nologin\n\n## Real-World Application\nHelps reduce audit scope to real human users for access reviews and onboarding/offboarding tasks.","diagram":null,"difficulty":"intermediate","tags":["RHCSA","Awk","Passwd","UserManagement","Linux","AWS","Kubernetes","Terraform","certification-mcq","domain-weight-20"],"channel":"rhcsa","subChannel":"essential-tools","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T21:20:30.513Z","createdAt":"2026-01-11 21:20:30"},{"id":"rhcsa-file-systems-1768238927339-0","question":"You have a 100 GB logical volume mounted at /data with ext4. You want to enforce per-user disk quotas so individual users cannot exceed 2 GB. Which sequence of steps is correct to configure quotas on that filesystem?","answer":"[{\"id\":\"a\",\"text\":\"Add usrquota to /etc/fstab for /data, remount the filesystem, run quotacheck -avugm, run quotaon /data, and use edquota to set per-user quotas.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Add grpquota to /etc/fstab and enable quotas on /data.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Mount the filesystem with noquota and use setquota to assign quotas.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Create a quota database file at /data/quota.db and configure quota on.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption a is correct because enabling user quotas requires usrquota in /etc/fstab, remounting the filesystem, running quotacheck to initialize quota structures, enabling quotas with quotaon, and then using edquota to set per-user limits.\n\n## Why Other Options Are Wrong\n- Option b is incorrect because grpquota alone enables group quotas; the scenario asks for per-user quotas, not per-group.\n- Option c is incorrect because noquota disables quotas; quotas must be enabled and quota files initialized.\n- Option d is incorrect because there is no quota database file at /data/quota.db; quotas are stored in aquota.user/aquota.group files on the filesystem, manipulated via edquota/setquota.\n\n## Key Concepts\n- Quotas: usrquota and grpquota options, quotacheck, quotaon, edquota.\n- /etc/fstab must include quota-related options for persistence.\n\n## Real-World Application\n- This pattern is used to prevent any single user from consuming excessive disk space on shared servers, preserving storage for others.\n","diagram":null,"difficulty":"intermediate","tags":["RHCSA","Linux","Quotas","File Systems","LVM","certification-mcq","domain-weight-15"],"channel":"rhcsa","subChannel":"file-systems","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T17:28:47.341Z","createdAt":"2026-01-12 17:28:47"},{"id":"rhcsa-file-systems-1768238927339-1","question":"Which /etc/fstab entry will mount the filesystem at /data with noexec and nosuid by default and persist across reboots?","answer":"[{\"id\":\"a\",\"text\":\"/dev/mapper/data on /data ext4 defaults,noexec,nosuid 0 0\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"/dev/mapper/data on /data ext4 defaults 0 0\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"/dev/mapper/data on /data ext4 rw,noatime 0 0\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"/dev/mapper/data on /data ext4 noexec 0 0\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption a is correct because it combines noexec and nosuid with the default mount options, ensuring these restrictions persist after reboot.\n\n## Why Other Options Are Wrong\n- Option b omits the noexec and nosuid restrictions.\n- Option c uses noatime instead of the required noexec/nosuid and does not set the needed protections.\n- Option d sets only noexec, missing nosuid.\n\n## Key Concepts\n- Mount options in /etc/fstab control persistence across reboots.\n- noexec and nosuid mitigate specific security and execution risks on mounted filesystems.\n\n## Real-World Application\n- Critical for securing shared data partitions where executables should not run from the data mount and privilege bits should be restricted.\n","diagram":null,"difficulty":"intermediate","tags":["RHCSA","Linux","Mounts","Fstab","Security","certification-mcq","domain-weight-15"],"channel":"rhcsa","subChannel":"file-systems","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T17:28:47.987Z","createdAt":"2026-01-12 17:28:48"},{"id":"rhcsa-file-systems-1768238927339-2","question":"You need to take a consistent backup of a running PostgreSQL data directory located on a logical volume. What is the correct approach to perform the backup with minimal downtime?","answer":"[{\"id\":\"a\",\"text\":\"Run pg_start_backup, perform the LVM snapshot of the LV, then run pg_stop_backup.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Stop the database, create an LVM snapshot, then start the database.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Create the LVM snapshot while the database is online without backup mode.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use rsync to copy the data directory while the database is running.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption a is correct because PostgreSQL backups tied to filesystem snapshots must be initiated with pg_start_backup, then an LVM snapshot captures a consistent state, followed by pg_stop_backup to finalize the backup.\n\n## Why Other Options Are Wrong\n- Option b introduces downtime to stop/start the DB, which contradicts the minimal-downtime goal.\n- Option c does not ensure WAL-consistent state for PostgreSQL backups when using snapshots.\n- Option d copies live files but does not guarantee logical consistency of the database.\n\n## Key Concepts\n- pg_start_backup/pg_stop_backup for logical consistency during filesystem snapshots.\n- LVM snapshots capture a point-in-time image of the LV.\n\n## Real-World Application\n- Essential technique for cold-restore or disaster recovery workflows in production PostgreSQL environments.\n","diagram":null,"difficulty":"intermediate","tags":["RHCSA","Linux","PostgreSQL","LVM","Backups","certification-mcq","domain-weight-15"],"channel":"rhcsa","subChannel":"file-systems","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T17:28:48.494Z","createdAt":"2026-01-12 17:28:48"},{"id":"rhcsa-file-systems-1768238927339-3","question":"For a database workload with high IOPS requirements, which filesystem is generally recommended on Linux for storing database files and why?","answer":"[{\"id\":\"a\",\"text\":\"XFS, because it scales well with large files and parallel I/O and has robust metadata handling\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"ext4, because it is the default and fastest for all workloads\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"btrfs, because of copy-on-write and snapshots\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"vfat, because cross-compatibility\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption a is correct because XFS is well-suited for large files and high parallel I/O workloads, common in database data directories.\n\n## Why Other Options Are Wrong\n- Option b is not universally fastest for all workloads; ext4 is common but not always the best for high-IO databases.\n- Option c votre; while btrfs offers snapshots, it is typically not the preferred default for high-IO database storage due to stability considerations in some environments.\n- Option d is inappropriate for databases due to limited feature support and performance.\n\n## Key Concepts\n- XFS scalability and metadata performance for large, concurrent I/O.\n- Filesystem choice impacts database throughput and reliability.\n\n## Real-World Application\n- In production, DB storage often uses XFS on LVM or hardware-backed devices to maximize IOPS and throughput.\n","diagram":null,"difficulty":"intermediate","tags":["RHCSA","Linux","XFS","Ext4","Performance","Databases","certification-mcq","domain-weight-15"],"channel":"rhcsa","subChannel":"file-systems","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T17:28:48.687Z","createdAt":"2026-01-12 17:28:48"},{"id":"rhcsa-file-systems-1768238927339-4","question":"You want to ensure a data directory mounted at /var/lib/app uses noatime to reduce metadata updates and improve performance. Which action will achieve this and persist across reboots?","answer":"[{\"id\":\"a\",\"text\":\"Add noatime to the /etc/fstab entry for /var/lib/app and remount the filesystem.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Run mount -o noatime for the current session only.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use tune2fs -o noatime to modify the filesystem.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Enable relatime as a permanent default instead of noatime.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption a is correct because adding noatime to the /etc/fstab entry ensures noatime is applied on boot and persists across reboots.\n\n## Why Other Options Are Wrong\n- Option b applies noatime only to the current session; it is not persistent.\n- Option c is not a valid persistent method for enabling noatime; tune2fs does not support that flag for most FS types in this context.\n- Option d relocates to relatime, which is a different behavior and not the requested noatime setting.\n\n## Key Concepts\n- noatime disables updating access time on reads, reducing write I/O.\n- Persistence requires editing /etc/fstab and remounting or rebooting.\n\n## Real-World Application\n- Useful for read-heavy data directories (e.g., content repositories) where access-time does not impact application logic.\n","diagram":null,"difficulty":"intermediate","tags":["RHCSA","Linux","Mounts","Performance","noatime","certification-mcq","domain-weight-15"],"channel":"rhcsa","subChannel":"file-systems","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T17:28:48.867Z","createdAt":"2026-01-12 17:28:48"},{"id":"q-1023","question":"Design a centralized, tamper-evident logging pipeline for 6 RHEL hosts. Include: enable persistent journald, forward logs over TLS to a central collector, configure rotation/retention, protect in transit with certificate-based auth, and a rollback/validation plan that proves delivery during outages. Outline testing steps and failure scenarios?","answer":"Enable persistent journald on all six hosts; configure rsyslog to forward logs via TLS to a central collector with certificate-based authentication; enforce rotation/retention (90 days) and archive to","explanation":"## Why This Is Asked\nConcrete, scalable, secure logging is critical in production. This question tests practical setup, TLS, journald/rsyslog integration, retention, and failover planning.\n\n## Key Concepts\n- systemd-journald persistent storage\n- TLS-encrypted log forwarding (rsyslog/omfwd)\n- log rotation and retention policies\n- log integrity and rollback strategies\n- outage/failover validation\n\n## Code Example\n```javascript\n// Simple hash of a log line to simulate tamper-detection\nconst crypto = require('crypto');\nfunction hashLine(line){ return crypto.createHash('sha256').update(line).digest('hex'); }\n```\n\n## Follow-up Questions\n- How would you verify end-to-end delivery during a network outage?\n- How would you rotate encryption certificates without dropping logs?","diagram":"flowchart TD\n  A[Hosts] --> B[Local journald]\n  B --> C[rsyslog forward]\n  C --> D[Central TLS collector]\n  D --> E[Archive/Retention]","difficulty":"advanced","tags":["rhcsa"],"channel":"rhcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Hashicorp"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T19:35:30.051Z","createdAt":"2026-01-12T19:35:30.051Z"},{"id":"q-1118","question":"On a RHEL 9 host, a service named app writes to /srv/app/data. After deployment, SELinux denials prevent writes. Without disabling SELinux, outline exact, implementable steps to restore functionality, including identifying the AVC, creating a targeted policy module with audit2allow, loading it, labeling the data directory, and validating the fix with a controlled write and audit checks?","answer":"Identify AVCs with ausearch -m avc -ts today | tail. Build a policy with: ausearch -m avc -ts today | audit2allow -M app_fix; load: semodule -i app_fix.pp. Label: semanage fcontext -a -t app_data_t '/","explanation":"## Why This Is Asked\nTests SELinux troubleshooting, policy generation, and safe remediation without disabling security controls.\n\n## Key Concepts\n- AVCs and audit2allow\n- Custom SELinux policy modules\n- File context labeling and restorecon\n- Validation via audits and controlled writes\n\n## Code Example\n```bash\n# 1) Identify AVCs from today\nausearch -m avc -ts today | tail\n# 2) Create a policy module\nausearch -m avc -ts today | audit2allow -M app_fix\n# 3) Load the policy\nsemodule -i app_fix.pp\n# 4) Label directory and content\nsemanage fcontext -a -t app_data_t '/srv/app/data(/.*)?'\nrestorecon -R /srv/app/data\n```\n\n## Follow-up Questions\n- How would you adjust this if the data directory is on NFS?\n- How can you troubleshoot if the policy still blocks legitimate writes after labeling?","diagram":"flowchart TD\n  A[App writes to /srv/app/data] --> B{AVC denied}\n  B --> C[Audit log shows AVC]\n  C --> D[Create policy with audit2allow]\n  D --> E[Load policy with semodule]\n  E --> F[Label dir with semanage/restorecon]\n  F --> G[Test write succeeds]","difficulty":"advanced","tags":["rhcsa"],"channel":"rhcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Hugging Face","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T23:27:33.371Z","createdAt":"2026-01-12T23:27:33.371Z"},{"id":"q-1148","question":"On a RHEL 8 server, implement a daily backup of /home/userdata to /backup/userdata-YYYYMMDD.tgz, exclude caches and temp dirs, preserve permissions, and generate a sha256 checksum. Schedule at 02:30 via cron and rotate backups to keep last 7 days. Provide commands and a script outline?","answer":"Use a cron-driven backup with tar -p and excludes, plus a sha256 checksum and a 7-day rotation. Example approach: create /backup if needed, tar czpf /backup/userdata-YYYY-MM-DD.tgz -p --exclude patter","explanation":"## Why This Is Asked\nTests practical backup scripting skills: tar with permissions, exclude patterns, integrity checks, cron scheduling, and simple rotation.\n\n## Key Concepts\n- tar with permission preservation\n- --exclude patterns\n- sha256sum checksums\n- cron scheduling\n- rotation by date\n\n## Code Example\n```bash\n#!/bin/bash\nDATE=$(date +%F)\nSRC=\"/home/userdata\"\nDEST=\"/backup\"\nTAR=\"$DEST/userdata-$DATE.tgz\"\nEXCLUDES=\"--exclude='**/.cache' --exclude='**/tmp'\"\ntar czpf \"$TAR\" -p $EXCLUDES \"$SRC\"\nsha256sum \"$TAR\" > \"$TAR.sha256\"\nfind \"$DEST\" -name 'userdata-*.tgz' -mtime +7 -delete\n```\n\n## Follow-up Questions\n- How would you implement incremental backups?\n- How would you verify restoration from a random backup file?","diagram":null,"difficulty":"beginner","tags":["rhcsa"],"channel":"rhcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Instacart","Lyft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T01:33:46.097Z","createdAt":"2026-01-13T01:33:46.097Z"},{"id":"q-1169","question":"Design and implement an encrypted root on LVM for a production RH host. Boot partition remains unencrypted; the root filesystem sits on a LUKS2 container inside an LVM PV, with a keyfile for unattended boot. Provide a concrete, command-level plan including crypttab, fstab, initramfs (dracut) generation, and grub configuration to ensure the system boots automatically after rotation?","answer":"Two-stage layout: /boot unencrypted; root on LUKS2 inside an LVM PV. Create 1G /boot partition and encrypt the rest. Steps: format and open LUKS, set up PV/VG/LV, create filesystem, copy data, update ","explanation":"## Why This Is Asked\nThis question probes practical, production-grade encryption setup, including boot-time unlocking, crypttab/fstab integration, and initramfs/grub changes — skills RHCSA expects for secure deployments.\n\n## Key Concepts\n- LUKS2 on a dedicated partition\n- LVM PV/VG/LV layering on top of a mapped device\n- crypttab and initramfs integration\n- Grub configuration and unattended boot considerations\n- Keyfile management and security\n\n## Code Example\n```javascript\n// Shell commands illustrating the flow\ncryptsetup luksFormat /dev/sdb2 --type luks2\ncryptsetup luksOpen /dev/sdb2 cryptroot --key-file /root/keyfile\npvcreate /dev/mapper/cryptroot\nvgcreate vg0 /dev/mapper/cryptroot\nlvcreate -L 100G -n root vg0\nmkfs.xfs /dev/vg0/root\n```\n\n## Follow-up Questions\n- How would you rotate the LUKS keyfile safely without reboot?\n- How do you verify the encrypted volume opens during boot with a degraded initramfs?","diagram":null,"difficulty":"advanced","tags":["rhcsa"],"channel":"rhcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Plaid","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T03:35:27.852Z","createdAt":"2026-01-13T03:35:27.852Z"},{"id":"q-1192","question":"On a RHEL8 host, set up a minimal Python HTTP server listening on port 8080, accessible only from 192.168.100.0/24. Use a non-root user, a systemd service, SELinux port labeling, and firewalld rules. Provide exact commands to create the service, configure semanage for port 8080, apply firewall rules, and test from a client. Address potential SELinux and port conflict caveats?","answer":"Create a non-root user webu; place app.py with a minimal HTTP server listening on 0.0.0.0:8080; a systemd service webserver.service (User=webu, WorkingDirectory=/home/webu, ExecStart=/usr/bin/python3 ","explanation":"## Why This Is Asked\nTests practical sysadmin tasks across users, services, SELinux, and firewall rules.\n\n## Key Concepts\n- systemd service creation\n- semanage port labeling\n- firewalld rich rules for IP ranges\n- binding to 0.0.0.0 and access control\n- testing with curl\n\n## Code Example\n```javascript\n# /etc/systemd/system/webserver.service\n[Unit]\nDescription=Minimal Python HTTP server\nAfter=network-online.target\n[Service]\nUser=webu\nWorkingDirectory=/home/webu\nExecStart=/usr/bin/python3 /home/webu/app.py\nRestart=on-failure\n[Install]\nWantedBy=multi-user.target\n```\n\n```javascript\n# app.py\nimport http.server, socketserver\nPORT=8080\nHandler=http.server.SimpleHTTPRequestHandler\nwith socketserver.TCPServer((\"0.0.0.0\", PORT), Handler) as httpd:\n    httpd.serve_forever()\n```","diagram":"flowchart TD\n  A[Create user and app] --> B[Create systemd service]\n  B --> C[Label port with SELinux]\n  C --> D[Configure firewall]\n  D --> E[Test access]","difficulty":"beginner","tags":["rhcsa"],"channel":"rhcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","LinkedIn"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T04:42:25.489Z","createdAt":"2026-01-13T04:42:25.489Z"},{"id":"q-1247","question":"On a RHEL 8 server, a web service listens on 127.0.0.1:9090. Configure firewalld to expose port 9090 only to the 10.1.0.0/24 management network, log drops, and persist across reboots. Provide exact commands, and describe test steps using curl from an allowed host and from a non-allowed host?","answer":"firewall-cmd --get-default-zone\nfirewall-cmd --permanent --zone=public --add-rich-rule='rule family=\"ipv4\" source address=\"10.1.0.0/24\" port port=\"9090\" protocol=\"tcp\" accept'\nfirewall-cmd --permanent","explanation":"## Why This Is Asked\nTests practical firewall configuration with persistent rules and live-testing.\n\n## Key Concepts\n- firewalld zones and rich rules\n- persistence with --permanent and firewall-cmd reload\n- testing network access with curl\n\n## Code Example\n```javascript\n// Commands illustrating the approach\nfirewall-cmd --get-default-zone\nfirewall-cmd --permanent --zone=public --add-rich-rule='rule family=\"ipv4\" source address=\"10.1.0.0/24\" port port=\"9090\" protocol=\"tcp\" accept'\nfirewall-cmd --permanent --zone=public --add-rich-rule='rule family=\"ipv4\" port port=\"9090\" protocol=\"tcp\" drop'\nfirewall-cmd --reload\n```\n\n## Follow-up Questions\n- How would you audit if the rule is being bypassed by another chain? \n- How would you revert the changes quickly? ","diagram":"flowchart TD\n  A[Define allowed subnet] --> B[Add accept rich rule]\n  B --> C[Add drop rule for others]\n  C --> D[Reload firewalld]\n  D --> E[Test with curl]","difficulty":"beginner","tags":["rhcsa"],"channel":"rhcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T06:42:21.017Z","createdAt":"2026-01-13T06:42:21.017Z"},{"id":"q-933","question":"On a fresh RHEL 9 installation with a 120 GB disk, implement an LVM layout: ROOT 40G, HOME 40G, VAR 40G, all using XFS. Create PV, VG, and LVs, format, and mount at /, /home, /var with fstab. Enable and configure firewalld to allow http and https. Ensure SELinux is enforcing. Create a non-root user 'dev' and add to the wheel group with sudo privileges. Show commands and rationale?","answer":"I would create a single PV on /dev/sda, a VG named rhel, and three LVs: root, home, var at 40G each. After mkfs.xfs on each, mount them to /, /home, /var and persist in /etc/fstab. Enable firewalld an","explanation":"## Why This Is Asked\nTests practical, real-world sysadmin tasks: LVM planning, filesystem layout, boot/root considerations, firewall and SELinux configuration, and user privilege management.\n\n## Key Concepts\n- LVM: pvcreate, vgcreate, lvcreate\n- Filesystems: XFS and fstab\n- Security: SELinux enforcing and wheel-based sudo\n- Networking: firewalld http/https rules\n\n## Code Example\n```bash\npvcreate /dev/sda\nvgcreate rhel /dev/sda\nlvcreate -L40G -n root rhel\nlvcreate -L40G -n home rhel\nlvcreate -L40G -n var rhel\nmkfs.xfs /dev/rhel/root\nmkfs.xfs /dev/rhel/home\nmkfs.xfs /dev/rhel/var\nmount /dev/rhel/root /\nmkdir -p /home /var\nmount /dev/rhel/home /home\nmount /dev/rhel/var /var\n# /etc/fstab entries would reference /dev/rhel/... set here\nsystemctl enable --now firewalld\nfirewall-cmd --permanent --add-service=http\nfirewall-cmd --permanent --add-service=https\nfirewall-cmd --reload\nsetenforce 1\nsed -i 's/^SELINUX=.*/SELINUX=enforcing/' /etc/selinux/config\nuseradd -m dev\nusermod -aG wheel dev\n```\n\n## Follow-up Questions\n- How would you resize LVs while preserving data?\n- How would you verify SELinux contexts after mounting new filesystems?","diagram":"flowchart TD\n  A[Disk] --> B[PV] \n  B --> C[VG] \n  C --> D[LVs: root, home, var] \n  D --> E[Mounts: /, /home, /var] \n  E --> F[Services: firewalld, SELinux, sudo]","difficulty":"intermediate","tags":["rhcsa"],"channel":"rhcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","IBM","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T15:42:25.882Z","createdAt":"2026-01-12T15:42:25.882Z"},{"id":"rhcsa-manage-users-1768285851090-0","question":"You want to apply password aging settings to all users in the group 'devs' so they expire after 90 days with a 7-day warning? Which approach would correctly enforce this across the group?","answer":"[{\"id\":\"a\",\"text\":\"chage -M 90 -W 7 username\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"for u in $(getent group devs | cut -d: -f4 | tr ',' ' '); do chage -M 90 -W 7 \\\"$u\\\"; done\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Edit /etc/shadow to set ageing fields for users in the group\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"authconfig --setpassword-aging 90 --warn 7\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct choice is **b** because it loops over all current members of the group and applies the aging settings to each user. Options a, c, and d do not scale to multiple users in the group.\n\n## Why Other Options Are Wrong\n- A only updates a single user, not all group members.\n- C involves manual editing of /etc/shadow and is dangerous and not scalable.\n- D is not a valid approach on RH-based systems for applying group-wide policy.\n\n## Key Concepts\n- Password aging is configured per-user via chage or via /etc/login.defs defaults.\n- To apply policies to a group, you typically script over the group's members.\n- getent can be used to enumerate group members; parsing output yields user names.\n\n## Real-World Application\n- Used when onboarding a new development team and ensuring all members rotate passwords within 90 days without manual per-user edits.\n","diagram":null,"difficulty":"intermediate","tags":["RHCSA","Linux","Users","Groups","Sudo","Kubernetes","AWS","Terraform","certification-mcq","domain-weight-10"],"channel":"rhcsa","subChannel":"manage-users","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T06:30:51.093Z","createdAt":"2026-01-13 06:30:51"},{"id":"rhcsa-manage-users-1768285851090-1","question":"To grant a specific sudo command to user 'jdoe' without a password prompt, which is best practice?","answer":"[{\"id\":\"a\",\"text\":\"Create a dedicated file in /etc/sudoers.d/ with a rule for jdoe using NOPASSWD\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Edit /etc/sudoers directly and add the rule\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Add user to the wheel group\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Disable password prompts for all commands in /etc/sudoers\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct choice is **a** because placing a precise rule in /etc/sudoers.d creates a minimal, auditable configuration for a single user without risking the global sudoers file. \n\n## Why Other Options Are Wrong\n- **b** Editing /etc/sudoers directly is supported but riskier and more error-prone; best practice is to use a dedicated file under /etc/sudoers.d with visudo validation.\n- **c** Adding the user to the wheel group grants broad sudo access unless the wheel group is explicitly configured; this is not a per-command restriction.\n- **d** Disabling password prompts for all commands weakens security and is not correct for a single command.\n\n## Key Concepts\n- Use /etc/sudoers.d for per-user or per-command rules.\n- Always validate with visudo to avoid syntax errors.\n- NOPASSWD is only appropriate for specific commands.\n\n## Real-World Application\n- This pattern is common when delegating limited admin tasks to junior admins without broad sudo privileges.\n","diagram":null,"difficulty":"intermediate","tags":["RHCSA","Linux","Sudo","Kubernetes","AWS","Terraform","certification-mcq","domain-weight-10"],"channel":"rhcsa","subChannel":"manage-users","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T06:30:51.465Z","createdAt":"2026-01-13 06:30:51"},{"id":"rhcsa-manage-users-1768285851090-2","question":"Which command sequence correctly creates user 'jdoe' with primary group 'devs' (gid 1500) and supplementary groups 'developers' and 'docker', while ensuring /home/jdoe is created and login shell is /bin/bash?","answer":"[{\"id\":\"a\",\"text\":\"groupadd -g 1500 devs; useradd -m -d /home/jdoe -g devs -G developers,docker -s /bin/bash jdoe\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"useradd -m -g devs -G developers docker jdoe\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"useradd -m -g 1501 devs; useradd -m -G developers,docker jdoe\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"groupadd devs; useradd -m -d /home/jdoe -g 1500 -G developers,docker -s /bin/bash jdoe\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct option is **a** because it creates the group with the specified GID and then creates the user with that primary group and the required supplementary groups, home directory, and shell.\n\n## Why Other Options Are Wrong\n- **b** Omits setting the proper primary group and GID and may fail on systems requiring explicit gid.\n- **c** Assigns a non-existent primary group or mixes up required attributes.\n- **d** Creates the group but uses a different GID for the user or lacks explicit shell/home settings.\n\n## Key Concepts\n- useradd -g sets the primary group; -G sets supplementary groups.\n- -d sets home directory; -s sets login shell; -m creates home.\n\n## Real-World Application\n- Ensures new hires start with correct group memberships and environment.\n","diagram":null,"difficulty":"intermediate","tags":["RHCSA","Linux","Users","Groups","Sudo","Kubernetes","AWS","Terraform","certification-mcq","domain-weight-10"],"channel":"rhcsa","subChannel":"manage-users","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T06:30:51.871Z","createdAt":"2026-01-13 06:30:51"},{"id":"rhcsa-manage-users-1768285851090-3","question":"Which command will force password reset for all users in the group 'contractors' efficiently and correctly?","answer":"[{\"id\":\"a\",\"text\":\"passwd -e $(getent group contractors | cut -d: -f4 | tr ',' ' ')\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"chage -d 0 $(getent group contractors | cut -d: -f4 | tr ',' ' ')\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"usermod -L all\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"for u in $(getent group contractors | cut -d: -f4 | tr ',' ' '); do chage -d 0 \\\"$u\\\"; done\",\"isCorrect\":true}]","explanation":"## Correct Answer\nOption **d** demonstrates a scalable approach: loop through all users in the contractors group and run chage -d 0 for each, forcing a password change at next login.\n\n## Why Other Options Are Wrong\n- **a** attempts to expire passwords for multiple users in a single command, which is not supported by passwd.\n- **b** would fail when more than one username is supplied to chage in many environments.\n- **c** locks accounts instead of forcing a password change.\n\n## Key Concepts\n- chage -d 0 expires the current password, triggering change at next login.\n- getent group lists the group members; careful parsing is required for multiple users.\n\n## Real-World Application\n- Useful when contractors leave or rejoin and you need to enforce password changes en masse.\n","diagram":null,"difficulty":"intermediate","tags":["RHCSA","Linux","Users","Groups","Sudo","Kubernetes","AWS","Terraform","certification-mcq","domain-weight-10"],"channel":"rhcsa","subChannel":"manage-users","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T06:30:52.001Z","createdAt":"2026-01-13 06:30:52"},{"id":"rhcsa-manage-users-1768285851090-4","question":"To customize the initial files copied to every new user by pointing useradd to a custom skeleton directory located at /opt/skel, which file do you modify to set this path?","answer":"[{\"id\":\"a\",\"text\":\"/etc/skel\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"/etc/default/useradd\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"/etc/passwd\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"/etc/login.defs\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct choice is **b** because /etc/default/useradd defines the SKEL value that points useradd to the skeleton directory; /etc/skel is the actual skeleton contents that get copied. \n\n## Why Other Options Are Wrong\n- **a** /etc/skel is the default skeleton contents, not the path setting.\n- **c** /etc/passwd is the user database and not related to skeletons.\n- **d** /etc/login.defs configures defaults for password policies, not skeleton directory.\n\n## Key Concepts\n- SKEL in /etc/default/useradd controls the skeleton directory path.\n- By default, /etc/skel is used for the skeleton contents.\n\n## Real-World Application\n- Enables centralized customization of new-user environments across hosts.\n","diagram":null,"difficulty":"intermediate","tags":["RHCSA","Linux","Users","Groups","Kubernetes","AWS","Terraform","certification-mcq","domain-weight-10"],"channel":"rhcsa","subChannel":"manage-users","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T06:30:52.126Z","createdAt":"2026-01-13 06:30:52"},{"id":"rhcsa-operate-systems-1768206719121-0","question":"A server uses LVM with a dedicated logical volume lv_home mounted on /home. The volume group is vg00. You need to extend /home by 10G without rebooting. Which sequence of commands achieves this correctly on a Red Hat system with XFS on /home?","answer":"[{\"id\":\"a\",\"text\":\"lvextend -L +10G /dev/vg00/lv_home; xfs_growfs /home\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"lvextend -L +12G /dev/vg00/lv_home; resize2fs /home\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"lvresize -L +10G /dev/vg00/lv_home; xfs_growfs /home\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"lvextend -L +10G /dev/vg00/lv_home; xfs_growfs /\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA. lvextend -L +10G /dev/vg00/lv_home; xfs_growfs /home\n\nThis extends the LV by 10G and expands the XFS filesystem live. The other options are incorrect: B uses resize2fs on XFS (not supported); C uses lvresize (works, but not the canonical command in this scenario); D grows the wrong filesystem, targeting the root rather than /home.\n\n## Why Other Options Are Wrong\n- B: resize2fs does not support XFS and cannot resize /home in this scenario.\n- C: lvresize is a valid LV operation but uses a different tool; not the standard approach here.\n- D: Attempts to grow the wrong filesystem mountpoint (/ instead of /home).\n\n## Key Concepts\n- LVM lvextend vs lvresize\n- Online filesystem resizing for XFS\n- Mountpoints and filesystem types\n\n## Real-World Application\nEngineers often need to quickly scale storage for user data without rebooting or downtime, relying on LVM and XFS capabilities.\n","diagram":null,"difficulty":"intermediate","tags":["RHCSA","Operate Running Systems","LVM","SELinux","firewalld","systemd","AWS","EC2","Kubernetes","Terraform","certification-mcq","domain-weight-20"],"channel":"rhcsa","subChannel":"operate-systems","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T08:31:59.122Z","createdAt":"2026-01-12 08:31:59"},{"id":"rhcsa-operate-systems-1768206719121-1","question":"An administrator moves the content directory for a web site from /var/www/html to a new mount at /mnt/storage/www. After mounting, the web server cannot serve the content due to SELinux context mismatch. Which sequence ensures the new location has proper persistent SELinux context?","answer":"[{\"id\":\"a\",\"text\":\"semanage fcontext -a -t httpd_sys_content_t '/mnt/storage(/.*)?'; restorecon -R -v /mnt/storage\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"chcon -R -t httpd_sys_content_t /mnt/storage\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"restorecon -R -v /mnt/storage/www\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"setenforce 0\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA. semanage fcontext -a -t httpd_sys_content_t '/mnt/storage(/.*)?'; restorecon -R -v /mnt/storage\n\nThis adds a persistent SELinux file context mapping for the new storage location and applies it recursively. B applies a temporary context change that won’t persist across relabels. C only runs restorecon without a proper persistent mapping, and D disables SELinux entirely, which is insecure.\n\n## Why Other Options Are Wrong\n- B: chcon changes context only temporarily and won’t survive relabels or reboots.\n- C: restorecon without proper fcontext mapping has no lasting effect for the new path.\n- D: Disabling SELinux is insecure and bypasses policy enforcement.\n\n## Key Concepts\n- Persistent SELinux contexts with semanage fcontext\n- httpd_sys_content_t for web content\n- restorecon usage\n\n## Real-World Application\nWeb admins relocate web content and must preserve secure, policy-driven access without disabling security controls.\n","diagram":null,"difficulty":"intermediate","tags":["RHCSA","Operate Running Systems","SELinux","httpd","systemd","AWS","EC2","Kubernetes","Terraform","certification-mcq","domain-weight-20"],"channel":"rhcsa","subChannel":"operate-systems","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T08:31:59.533Z","createdAt":"2026-01-12 08:31:59"},{"id":"rhcsa-operate-systems-1768206719121-2","question":"You need to expose a service on port 8080 to external clients. The server uses firewalld and is in the public zone. Which command sequence opens the port permanently and makes it survive reboot?","answer":"[{\"id\":\"a\",\"text\":\"firewall-cmd --zone=public --add-port=8080/tcp --permanent; firewall-cmd --reload\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"firewall-cmd --zone=public --add-service=http --permanent; firewall-cmd --reload\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"firewall-cmd --zone=public --remove-port=8080/tcp; firewall-cmd --reload\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"firewall-cmd --zone=internal --add-port=8080/tcp --permanent; systemctl restart firewalld\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA. firewall-cmd --zone=public --add-port=8080/tcp --permanent; firewall-cmd --reload\n\nThis permanently opens port 8080 in the public zone and reloads the firewall rules. B opens a different port (HTTP default 80) rather than 8080. C removes the port, defeating the requirement. D uses the wrong zone and an unnecessary restart.\n\n## Why Other Options Are Wrong\n- B: Opens port 80 (HTTP) instead of 8080, not meeting the requirement.\n- C: Removes the port, opposite of the goal.\n- D: Uses the wrong zone and relies on a restart rather than a reload; the zone consistency is wrong.\n\n## Key Concepts\n- firewalld permanent rules\n- Correct zone alignment\n- Persistence across reboots\n\n## Real-World Application\nExposing services securely requires precise firewall configuration that survives system restarts.\n","diagram":null,"difficulty":"intermediate","tags":["RHCSA","Operate Running Systems","firewalld","systemd","AWS","EC2","Kubernetes","Terraform","certification-mcq","domain-weight-20"],"channel":"rhcsa","subChannel":"operate-systems","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T08:31:59.944Z","createdAt":"2026-01-12 08:32:00"}],"subChannels":["configure-storage","deploy-configure","essential-tools","file-systems","general","manage-users","operate-systems"],"companies":["Adobe","Amazon","Apple","Bloomberg","Goldman Sachs","Hashicorp","Hugging Face","IBM","Instacart","LinkedIn","Lyft","Microsoft","Plaid","Snap","Twitter"],"stats":{"total":33,"beginner":3,"intermediate":27,"advanced":3,"newThisWeek":33}}