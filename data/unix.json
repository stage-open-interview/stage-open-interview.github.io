{"questions":[{"id":"q-1002","question":"In a Unix environment logs are stored in /var/log/app/*.log with lines formatted as timestamp|user|action|resource. Write a practical one-liner using standard UNIX tools to output the top 5 users by total actions in the last 24 hours. Explain how you would handle log rotation and malformed lines?","answer":"One practical approach is to filter the last 24h files, extract the user, and count actions. Example: find /var/log/app/*.log -type f -mtime -1 -print0 | xargs -0 cat | awk -F'|' 'NF>=4{cnt[$2]++} END","explanation":"## Why This Is Asked\nTests practical log-scan skills: building a robust one-liner that handles log rotation, malformed lines, and spaces in fields. It also gauges comfort with standard tools and edge-case thinking.\n\n## Key Concepts\n- Log rotation: using -mtime to limit scope to last day.\n- Field delimiting: using a stable delimiter (|) for reliable parsing.\n- Robust counting: associative arrays in awk for tallying per user.\n- Safe I/O: null-delimited input with -print0/xargs -0 to support spaces.\n\n## Code Example\n```javascript\nfind /var/log/app/*.log -type f -mtime -1 -print0 | xargs -0 cat | awk -F'|' 'NF>=4{cnt[$2]++} END{for(u in cnt) print cnt[u], u}' | sort -nr | head -5\n```\n\n## Follow-up Questions\n- How would you handle logs where the user field is sometimes missing or empty?\n- How would you validate results across multiple days with varying time zones?\n- How would you adapt this for extremely large log pools to minimize I/O impact?","diagram":"flowchart TD\n  Start --> ReadLogs[Read log files in /var/log/app]\n  ReadLogs --> Filter[Filter last 24 hours with -mtime]\n  Filter --> Extract[Extract user field (split on |)]\n  Extract --> Count[Count per user (awk)]\n  Count --> Sort[Sort counts descending]\n  Sort --> Top5[Output top 5 users]\n  Top5 --> End[Done]","difficulty":"beginner","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T18:47:17.203Z","createdAt":"2026-01-12T18:47:17.203Z"},{"id":"q-1032","question":"How would you capture stdout and stderr of a simple shell command into separate log files while still displaying live output in the terminal? Provide a concrete Bash command and brief justification?","answer":"Use Bash process substitution to split streams: command > >(tee -a stdout.log) 2> >(tee -a stderr.log >&2). This sends stdout to both terminal and stdout.log, and stderr to both terminal (via 2>&) and","explanation":"## Why This Is Asked\n\nAssess understanding of Unix IO redirection, common tooling (tee), and Bash-specific features (process substitution) that enable real-time logging without losing output.\n\n## Key Concepts\n\n- stdout/stderr redirection\n- process substitution\n- tee for file logging\n\n## Code Example\n\n```bash\ncommand > >(tee -a stdout.log) 2> >(tee -a stderr.log >&2)\n```\n\n## Follow-up Questions\n\n- How would you implement a portable alternative for POSIX sh?\n- What if the command writes to stdout after its own buffering ends; how could you ensure timely logs?","diagram":null,"difficulty":"beginner","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","LinkedIn","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T20:22:14.784Z","createdAt":"2026-01-12T20:22:14.785Z"},{"id":"q-1069","question":"In a Unix environment, logs live under /var/log/metrics/*.log and are hourly rotated to .log and .log.gz. Each line is like [YYYY-MM-DD HH:MM:SS] LEVEL: message. Propose a robust, portable approach (one-liner preferred) to output the number of ERROR events per hour for the last 6 hours, handling missing files and rotations without external dependencies?","answer":"I’d implement a robust, portable pipeline reading both compressed and plain logs, extracting the hour from each timestamp, filtering for ERROR, and aggregating counts for the last 6 hours. Use globbin","explanation":"## Why This Is Asked\nTests practical mastery of Unix tooling, log rotation handling, and resilient scripting under real-world constraints.\n\n## Key Concepts\n- Robust log ingestion across rotated files\n- Mixed gzip/uncompressed handling with zcat\n- Time bucketing and boundary handling\n- Idempotent, side-effect-free pipelines\n\n## Code Example\n```javascript\n// Bash sketch\nstart=$(date -u -d '-6 hours' '+%Y-%m-%d %H')\nzcat /var/log/metrics/metrics-*.log.gz /var/log/metrics/metrics-*.log 2>/dev/null \\\n| awk -v cutoff=\"$start\" '/ERROR/ { if ($0 ~ /\\[/) { hour=substr($1,1,10)\" \"substr($2,1,2); if (hour>=cutoff) c[hour]++ } } END{for (h in c) print h\":00\", c[h]}'\n```\n\n## Follow-up Questions\n- How would you test this under log rotation? \n- How would you adapt for multiple timezones?","diagram":"flowchart TD\n  A[Start] --> B[Collect log files (gz and plain)]\n  B --> C[Parse timestamps to hour bucket]\n  C --> D[Filter ERROR events]\n  D --> E[Aggregate last 6 hours]\n  E --> F[Output results]","difficulty":"advanced","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Meta","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T21:30:28.843Z","createdAt":"2026-01-12T21:30:28.843Z"},{"id":"q-1123","question":"In a Unix environment, multiple services write JSON logs under /var/log/diag/*.log and rotated hourly to *.log.gz. Each line is a JSON object like {\"ts\":\"YYYY-MM-DDTHH:MM:SSZ\",\"svc\":\"name\",\"lvl\":\"ERROR\",\"msg\":\"...\"}; Propose a robust one-liner (no external dependencies beyond standard UNIX tools) to output the number of ERROR events per hour for the last 4 hours, aggregated across all services, and tolerant of missing files and rotated archives?","answer":"I’d implement a robust portable one-liner that streams both .log and .log.gz files, extracts ts and lvl from each JSON line, converts ts to epoch, filters for the last 4 hours, buckets per hour (YYYY-","explanation":"## Why This Is Asked\n\nTests ability to write compact, production-grade log queries that tolerate rotations, mixed compression, and JSON-like lines using only core Unix tools.\n\n## Key Concepts\n\n- Robust file discovery with find and null-termination\n- Decompression with zcat for .gz files\n- Inline JSON-like parsing with simple regex in awk\n- Time-window filtering via epoch comparison using date\n- Hour bucketing and aggregation with associative arrays\n\n## Code Example\n\n```javascript\n#!/bin/sh\ncutoff=$(date -u -d '4 hours ago' +%s)\nfind /var/log/diag -type f \\( -name '*.log' -o -name '*.log.gz' \\) -print0 | \\\n  xargs -0 -I{} sh -c ' [ -f \"$1\" ] && { if [ \"${1##*.}\" = \"gz\" ]; then zcat \"$1\"; else cat \"$1\"; fi; }' _ {} | \\\nawk -v cut=\"$cutoff\" ' /\"lvl\":\"ERROR\"/ { if (match($0, /\"ts\":\"([^\\\\\"]+)\"/, a)) { cmd=\"date -u -d \\\"\" a[1] \"\\\" +%s\"; cmd | getline t; close(cmd); if (t >= cut) { hour = substr(a[1],1,13) \"Z\"; count[hour]++ } } } END { for (h in count) print h, count[h] } '\n```\n\n## Follow-up Questions\n\n- How would you adjust for millisecond timestamps?\n- How would this scale across many servers and years of logs?","diagram":"flowchart TD\n  A[Start] --> B[Find logs (*.log, *.log.gz)]\n  B --> C[Decompress and read lines]\n  C --> D[Parse JSON ts and lvl]\n  D --> E[Filter last 4 hours]\n  E --> F[Bucket by hour]\n  F --> G[Count ERRORs]\n  G --> H[Output]","difficulty":"intermediate","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","OpenAI","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T23:30:41.563Z","createdAt":"2026-01-12T23:30:41.563Z"},{"id":"q-1139","question":"Scenario: JSON logs at /var/log/diag/*.log, rotated hourly to *.log.gz. Each line: {\"ts\":\"YYYY-MM-DDTHH:MM:SSZ\",\"svc\":\"name\",\"lvl\":\"ERROR\",\"msg\":\"...\"}. Propose a robust one-liner (no external deps beyond standard UNIX tools) that outputs the per-hour 99th percentile of message length for the last 4 hours, across all services, deduplicating identical messages per hour, and tolerant of missing files and gz archives?","answer":"Use a two-pass approach: 1) gzip -cd /var/log/diag/*.log.gz 2>/dev/null | awk to emit hour bucket and msg length, deduplicating by hour+msg with an associative array; filter to last 4 hours by ts; 2) ","explanation":"## Why This Is Asked\nTests real-world log ingestion under rotation; requires careful handling of gz files, missing files, and cross-service aggregation. The candidate should justify a robust percentile computation and dedup logic.\n\n## Key Concepts\n- Standard UNIX toolchain for parsing JSON-like lines without jq\n- Handling gzip-rotated logs and missing files\n- Per-hour bucketing and deduplication by message\n- Computation of a percentile from a 1-D length distribution\n\n## Code Example\n```bash\n# skeleton command outline (not complete here)\n```\n\n## Follow-up Questions\n- How would you test with synthetic logs at scale?\n- How would you adapt for different rotations or non-UTC timestamps?","diagram":"flowchart TD\n  A[Read logs (gz and plain)] --> B[Parse ts and msg length]\n  B --> C[Bucket by hour]\n  C --> D[Deduplicate by hour+msg]\n  D --> E[Compute 99th percentile per hour]\n  E --> F[Output results]","difficulty":"advanced","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["OpenAI","Plaid","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T01:26:59.322Z","createdAt":"2026-01-13T01:26:59.322Z"},{"id":"q-1224","question":"Scenario: In a Unix cluster, logs are emitted as JSON lines under /var/log/cluster/*/*.log and rotated hourly to *.log.gz. Each line contains {\"ts\":\"YYYY-MM-DDTHH:MM:SSZ\",\"tenant\":\"tenant-id\",\"svc\":\"service\",\"lvl\":\"LEVEL\",\"msg\":\"...\"}; Propose a robust one-liner (no external deps beyond standard UNIX tools) to identify the top 3 tenants by error rate (ERROR / total events) for the last 6 hours, aggregated across all services, tolerant of missing files and gz archives?","answer":"Stream all logs (including .log.gz), extract ts and tenant via awk regex, filter lines within the last 6 hours using START/END timestamps from date -u, accumulate per-tenant total and ERROR counts, th","explanation":"## Why This Is Asked\n- Assesses ability to build robust, single-line data pipelines over multi-file logs, including compressed files, without external dependencies. \n- Tests time-window handling, cross-service aggregation, and per-tenant bucketing under rotation. \n\n## Key Concepts\n- Stream processing with standard UNIX tools (find, xargs, zcat, awk). \n- JSON field extraction via regex in awk; resilient to line variations. \n- Time-window filtering using START/END derived from date; cross-file aggregation. \n- Sorting by computed metric (error rate) without extra tools.\n\n## Code Example\n```javascript\nSTART=$(date -u -d \"-6 hours\" +\"%Y-%m-%dT%H\")\nEND=$(date -u +\"%Y-%m-%dT%H\")\nfind /var/log/cluster -type f \\( -name '*.log' -o -name '*.log.gz' \\) -print0 | \\\n  xargs -0 -I{} sh -lc 'case \"{}\" in *.gz) zcat \"{}\" ;; *) cat \"{}\" ;; esac' | \\\n  awk -v s=\"$START\" -v e=\"$END\" '{\n    if (match($0, /\"ts\":\"([^\"]+)\"/, a)) ts=a[1]; hour=substr(ts,1,13); gsub(/T/,\" \",hour); hour_key=hour;\n    if (match($0, /\"tenant\":\"([^\"]+)\"/, b)) tenant=b[1];\n    if (match($0, /\"lvl\":\"([^\"]+)\"/, c)) lvl=c[1];\n    if (hour_key && tenant && hour_key>=s && hour_key<=e) {\n      total[tenant] += 1; if (lvl==\"ERROR\") errors[tenant] += 1;\n    }\n  }\n  END { for (t in total) { rate = total[t] ? errors[t] / total[t] : 0; printf \"%s\\t%.6f\\n\", t, rate } }\n``` \n\n## Follow-up Questions\n- How would you adapt this to include per-hour breakdowns in addition to the overall top 3? \n- How would you handle non-UTC timestamps or malformed JSON lines? ","diagram":null,"difficulty":"advanced","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Coinbase","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T05:34:59.109Z","createdAt":"2026-01-13T05:34:59.109Z"},{"id":"q-1241","question":"Scenario: In a multi-user Unix workspace, /home contains subdirectories per user with various files. Some are large, some are temporary. Provide a robust one-liner (no external dependencies) that lists the five largest regular files under /home (recursively), excluding hidden files and symlinks, printing each as 'size<TAB>path' with sizes in human-readable form. Explain how you ensure safety against spaces and newlines in filenames?","answer":"One robust option is a single pipeline that uses find with a safe delimiter, sorts by size, and formats to human units. For example: find /home -type f -not -path '*/.*' -print0 | xargs -0 stat -c '%s","explanation":"Why this is asked: tests understanding of safe file discovery, handling of spaces, and large-file detection. Key points: exclude hidden paths, only regular files, robust delimiting with null separators, and portable human-readable formatting via an awk function that scales from B up to GB. Edge considerations include spaces in names and potential binary data in paths.","diagram":"flowchart TD\n  A[Find largest files] --> B[Filter /home recursively]\n  B --> C[Exclude hidden/symlinks]\n  C --> D[Emit size+path]\n  D --> E[Sort desc & take top 5]\n  E --> F[Format human units]","difficulty":"beginner","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Lyft","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T06:36:11.300Z","createdAt":"2026-01-13T06:36:11.300Z"},{"id":"q-1492","question":"Scenario: multiple services log JSON lines to /var/log/app/*.log, rotated hourly to *.log.gz. Each line looks like {\"ts\":\"YYYY-MM-DDTHH:MM:SSZ\",\"svc\":\"name\",\"lat_ms\":123}. Propose a robust one-liner (no external deps beyond standard UNIX tools) to compute the per-service average lat_ms in the last 2 hours, across all logs, tolerant of missing and gzipped files, ignoring malformed lines. Output: 'service avg_ms'?","answer":"cutoff=$(date -u -d '-2 hours' '+%Y-%m-%dT%H:%M:%SZ'); find /var/log/app -type f \\( -name '*.log' -o -name '*.log.gz' \\) -print0 | while IFS= read -r -d '' f; do if [[ \"$f\" == *.gz ]]; then zcat \"$f\";","explanation":"Why This Is Asked: tests robust log ingestion across gzipped rotations. Key Concepts: shell pipelines, handling gzip, JSON-like parsing with regex in awk, per-service aggregation, time-window filtering via ISO timestamps. Code Example: the command reads both .log and .log.gz, extracts ts/svc/lat_ms, filters to last 2 hours, accumulates sums and counts per service, then prints averages. Follow-ups explore error handling and performance tweaks.","diagram":null,"difficulty":"intermediate","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["PayPal","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T19:28:41.147Z","createdAt":"2026-01-13T19:28:41.149Z"},{"id":"q-1809","question":"Scenario: A Linux host logs auth events into /var/log/auth/{service}/*.log and rotates hourly to *.log.gz. Each line is JSON like {\"ts\":\"YYYY-MM-DDTHH:MM:SSZ\",\"user\":\"alice\",\"action\":\"LOGIN\",\"result\":\"FAIL\"}. Propose a robust one-liner (no external deps beyond standard UNIX tools) to print the top 5 users by failed login rate in the last 2 hours, aggregated across all services, tolerant of missing files and gz archives, and resilient to malformed lines?","answer":"Stream both gz and plain logs via zcat, filter for status FAIL, extract ts and user with awk, convert ts to epoch and compare to 2-hour window, accumulate per-user totals, then emit in descending orde","explanation":"## Why This Is Asked\nThis tests real-world log aggregation with mixed rotation and JSON parsing using canonical UNIX tools.\n\n## Key Concepts\n- zcat + globbing for .log.gz and .log\n- awk-based JSON field extraction without external deps\n- date for UTC epoch bucketing; last N hours window\n- robust handling of missing/malformed lines\n\n## Code Example\n```javascript\n// Pseudocode sketch of the approach\nconst cutoff = Date.now() - 2*3600*1000\nfor each line in streams:\n  parse json\n  if (record.status == 'FAIL' && new Date(record.ts) >= cutoff) {\n    counts[record.user]++\n  }\n}\nprint top 5 by counts\n```\n\n## Follow-up Questions\n- How would you adapt this to count per-minute windows?\n- How would you test with synthetic rotated logs?","diagram":"flowchart TD\n  A[Parse logs] --> B[Extract fields]\n  B --> C[Apply time window]\n  C --> D[Aggregate per user]\n  D --> E[Sort & select top5]","difficulty":"intermediate","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Robinhood","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T11:44:32.982Z","createdAt":"2026-01-14T11:44:32.982Z"},{"id":"q-1903","question":"Propose a robust one-liner to compute the number of ERROR lines per hour for the previous 12 hours, aggregating across all logs in /var/log/app and across both uncompressed (*.log) and rotated/compressed files (*.log.*, *.log.gz). The one-liner must tolerate missing files and gz archives and rely only on standard UNIX tools?","answer":"One-liner approach: stream all logs (/*.log, *.log.*, *.log.gz) and pipe to awk. Parse timestamps [YYYY-MM-DD HH:MM:SS], convert to epoch with mktime, bucket by hour with strftime, and count ERROR lin","explanation":"## Why This Is Asked\n\nAssesses ability to craft a robust log-analysis one-liner that handles mixed compression formats, time-window filtering, and cross-file aggregation using only standard UNIX tools.\n\n## Key Concepts\n\n- Shell file globbing and safe looping over missing files\n- Decompression with zcat and streaming with cat\n- GNU awk time parsing (mktime, systime, strftime)\n- Time-window filtering and hour bucketing\n- Sorting deterministic output\n\n## Code Example\n\n```javascript\n// Conceptual core approach in a single pipeline\nfor f in /var/log/app/*.log /var/log/app/*.log.* /var/log/app/*.log.gz; do\n  case \"$f\" in\n    *.gz) zcat \"$f\" ;; *) cat \"$f\" ;; esac;\ndone | awk 'BEGIN{now=systime()} /\\[[0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2}\\]/ { if (match($0, /\\[([0-9]{4})-([0-9]{2})-([0-9]{2}) ([0-9]{2}):([0-9]{2}):([0-9]{2})\\]/, d)) { ts = mktime(d[1]\" \"d[2]\" \"d[3]\" \"d[4]\" \"d[5]\" \"d[6]); if (ts>=now-12*3600 && /ERROR/) { h=strftime(\"%Y-%m-%d %H:00\", ts); c[h]++ } } } END{ for (k in c) print k, c[k] }' | sort\n```\n\n## Follow-up Questions\n\n- How would you adapt this to handle a different timestamp format or time zone?\n- How would you modify to also count WARN and INFO separately while preserving performance?","diagram":null,"difficulty":"beginner","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Slack","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T16:49:53.401Z","createdAt":"2026-01-14T16:49:53.401Z"},{"id":"q-1975","question":"In a Unix host, logs live in /logs/web/*.log and are rotated hourly to *.log.gz. Each line is a JSON object like {\"ts\":\"YYYY-MM-DDTHH:MM:SSZ\",\"ip\":\"1.2.3.4\",\"method\":\"GET\",\"path\":\"/api\"}. Propose a robust one-liner (no external deps beyond standard UNIX tools) to output the top 5 IPs by request count in the last 4 hours, aggregating across all files and rotations, tolerant of missing files and compressed archives?","answer":"Stream all logs from /logs/web/*.log and *.log.gz using a single, portable pipeline, extract ip and ts from each JSON line, filter lines within the last 4 hours by converting ts to epoch, then count p","explanation":"## Why This Is Asked\nAssesses ability to reason about log rotation, gzipped files, and stream processing with minimal tools.\n\n## Key Concepts\n- Globbing and rotated logs\n- Stream processing with zcat and cat\n- Text extraction with sed/awk\n- Data aggregation with awk\n\n## Code Example\n```bash\n# Candidate approach (illustrative)\nzcat /logs/web/*.log.gz /logs/web/*.log 2>/dev/null | \\\nawk -v cutoff=\"$(date -u -d '4 hours ago' +%s)\" 'BEGIN{RS=\"\"; FS=\"\"} /\"ts\":\"/ { if(match($0, /\"ts\":\"([^\"]+)\"/, m)) {\n  cmd=\"date -u -d \\\"\" m[1] \"\\\" +%s\"; cmd | getline t; close(cmd);\n  if(t>=cutoff && match($0, /\"ip\":\"([0-9.]+)\"/, ip)) { c[ip[1]]++ }\n}} END{ for(i in c) print c[i]\" \"\"i }' | \\\nsort -nr | head -n5\n```\n\n## Follow-up Questions\n- How would you adapt for non-GNU awk?\n- What changes if timestamps are in a different timezone?","diagram":null,"difficulty":"beginner","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Snap","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T19:30:05.919Z","createdAt":"2026-01-14T19:30:05.919Z"},{"id":"q-2094","question":"Scenario: In a Unix CI environment, logs are written by build agents under /build/logs/*/*.log and rotated hourly to *.log.gz. Each line is BUILDID|TIMESTAMP|STEP|STATUS|MESSAGE. Propose a robust one-liner (no external deps beyond standard UNIX tools) to report, for the last 24 hours, per BUILDID and per hour, the count of failed steps (STATUS != 'SUCCESS'), aggregating across all agents and rotations and tolerating missing files and compressed archives?","answer":"Approach: Stream all logs (.log and .log.gz), decompress as needed, then use awk to parse, convert TIMESTAMP to epoch with date -d, keep only entries from the last 24h, and increment counts[BUILDID][hour] where STATUS != 'SUCCESS'.","explanation":"## Why This Is Asked\nTests practical CLI scripting and edge-case handling in production-like log workflows. Candidates justify a compact, robust solution that tolerates missing files and compressed archives.\n\n## Key Concepts\n- POSIX vs GNU utilities compatibility\n- Efficient streaming of mixed plain/compressed logs\n- Time-window calculations and per-key aggregation\n\n## Code Example\n```bash\n# Example: outline of the one-liner approach (exact command may vary by shell)\nnow=$(date +%s); for f in /build/logs/*/*.{log,log.gz}; do [ -f \"$f\" ] && case \"$f\" in *.gz) gzip -dc \"$f\" ;; *) cat \"$f\" ;; esac; done | awk -F'|' -v now=\"$now\" 'function epoch(ts) { cmd=\"date -d \\\"\" ts \"\\\" +%s\"; cmd | getline e; close(cmd); return e } { ts=epoch($2); if(now-ts <= 86400 && $4 != \"SUCCESS\") { build=$1; hour=sprintf(\"%02d\", strftime(\"%H\", ts)); counts[build][hour]++ } } END { for(b in counts) for(h in counts[b]) print b\"|\"h\"|\"counts[b][h] }'\n```\n\n## Edge Cases Handled\n- Missing files gracefully skipped\n- Mixed compressed/uncompressed logs\n- Timestamp parsing across formats\n- Memory-efficient streaming","diagram":null,"difficulty":"beginner","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","IBM"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T05:02:34.376Z","createdAt":"2026-01-14T23:45:04.503Z"},{"id":"q-2253","question":"Scenario: /var/run contains *.pid per daemon (service name == filename). Some daemons crash leaving stale PIDs. Propose a robust one-liner (no external deps) that lists service:pid pairs for all PID files whose PID is not running, tolerant of missing files and spaces in names?","answer":"for f in /var/run/*.pid; do [ -f \\\"$f\\\" ] || continue; pid=$(cat \\\"$f\\\" 2>/dev/null); [ -n \\\"$pid\\\" ] || continue; if [ ! -e \\\"/proc/$pid\\\" ]; then echo \\\"$(basename \\\"$f\\\" .pid):$pid\\\"; fi; done","explanation":"Why This Is Asked: Tests ability to detect stale daemon state using native tools, robust against missing pid files and whitespace in names. Key Concepts: PID file semantics, /proc existence checks, shell safety with globbing, handling spaces in filenames. Follow-up: adapt for systemd pidfiles and atomic cleanup.","diagram":null,"difficulty":"intermediate","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T09:39:00.686Z","createdAt":"2026-01-15T09:39:00.686Z"},{"id":"q-2293","question":"In a Unix environment, logs live under /var/log/app-logs/*.log and rotations *.log.gz. Each line is a JSON object like {\"ts\":\"YYYY-MM-DDTHH:MM:SSZ\",\"svc\":\"name\",\"dur_ms\":1234,\"ev\":\"RUN\"}. Propose a robust one-liner (no external deps beyond standard UNIX tools) to compute, per service, the mean and 95th percentile of dur_ms for the last 2 hours, aggregating across all files and rotations, tolerating missing files and archives?","answer":"Use a single Bash pipeline that reads both *.log and *.log.gz, extracts ts, svc, and dur_ms, converts ts to epoch with TZ=UTC, filters to now-2h, then aggregates per service in awk. Store durations pe","explanation":"## Why This Is Asked\nTests practical UNIX log analytics with JSON lines, mixed rotations, and no external deps.\n\n## Key Concepts\n- Handling mixed log formats and gzipping\n- Time-window filtering with epoch comparisons\n- Per-service aggregation and percentile computation (GNU awk)\n\n## Code Example\n```\n# Pseudocode: read logs (log/*.log and *.log.gz), extract ts,srv,dur_ms, filter 2h window, per-service aggregate to compute mean and 95th percentile\n```\n\n## Follow-up Questions\n- How would you adapt this to compute per-minute percentiles?\n- How would you validate correctness with synthetic logs?","diagram":"flowchart TD\nA[Log Files] --> B{Uncompressed vs.gz}\nB --> C[Parse JSON fields ts, svc, dur_ms]\nC --> D{Time-window check (2h)}\nD --> E[Aggregate by svc]\nE --> F[Compute mean and 95th percentile]\nF --> G[Emit svc, mean, p95]","difficulty":"intermediate","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","IBM","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T10:54:51.056Z","createdAt":"2026-01-15T10:54:51.056Z"},{"id":"q-2396","question":"In a Unix host, multiple services write JSON log lines to /logs/app/*/*.log, rotated hourly to *.log.gz. Each line is {\"ts\":\"YYYY-MM-DDTHH:MM:SSZ\",\"svc\":\"name\",\"lvl\":\"LEVEL\",\"msg\":\"...\"}. Propose a robust one-liner (no external deps beyond standard UNIX tools) that outputs a per-minute histogram of WARN or ERROR events for the last 60 minutes, aggregated across all files and rotations, tolerant of missing files and gz archives, and resilient to out-of-order timestamps within the minute bucket?","answer":"Loop over /logs/app/*/*.log, decompress gz with zcat when needed, stream lines to awk. In awk, extract ts via regex, convert to minute bucket with date -u +%Y-%m-%dT%H:%M -d \"$ts\"; skip invalid; if lv","explanation":"## Why This Is Asked\nTests the ability to design robust log aggregation across rotation boundaries, missing files, and clock quirks in a real-world Unix setup.\n\n## Key Concepts\n- Globbing and rotated logs across .log and .log.gz\n- On-the-fly decompression of gz files with standard tools\n- Parsing JSON-like lines with basic UNIX utilities\n- Per-minute bucketing and aggregation with streaming tools\n- Robust handling of invalid lines and out-of-order timestamps\n\n## Code Example\n```bash\nfor f in /logs/app/*/*.log; do \n  [ -f \"$f\" ] && case \"$f\" in \n    *.gz) zcat \"$f\" ;; \n    *)   cat \"$f\" ;; \n  esac; \ndone | awk -F'\"' '/\"ts\":/ { if (match($0, /\"ts\":\"([^\"]+)\"/, m)) { t=m[1]; if (t ~ /^[0-9]{4}-[0-9]{2}-[0-9]{2}T[0-9]{2}:[0-9]{2}:[0-9]{2}Z$/) { bucket = strftime(\"%Y-%m-%dT%H:%M\", mktime(substr(t,1,4)\" \"substr(t,6,2)\" \"substr(t,9,2)\" \"substr(t,12,2)\" \"substr(t,15,2)\" \"substr(t,18,2)))); if ($0 ~ /\"lvl\":\"(WARN|ERROR)\"/) cnt[bucket]++ } } } END { for (b in cnt) print b, cnt[b] }' | sort\n``` \n\n## Follow-up Questions\n- How would you extend to 24 hours and a fixed windowing strategy? \n- How would timezone and clock skew be accounted for in production dashboards?","diagram":null,"difficulty":"intermediate","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Databricks","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T16:45:47.287Z","createdAt":"2026-01-15T16:45:47.287Z"},{"id":"q-2420","question":"Scenario: In a Unix host, logs live under /var/log/app-logs with daily rotation and filenames like app-<service>-YYYYMMDD.log. Write a robust one-liner (no external deps beyond standard UNIX tools) that prints a total count of lines containing the word 'ERROR' across all log files modified in the last 3 days. The command must gracefully skip missing/unreadable files and work across many services?","answer":"find /var/log/app-logs -type f -name 'app-*-*.log' -mtime -3 -print0 2>/dev/null | xargs -0 cat 2>/dev/null | awk '/ERROR/ {c++} END {print c+0}'","explanation":"## Why This Is Asked\nTests practical ability to craft a robust, portable one-liner using only standard UNIX tools for cross-service log aggregation. It handles file rotation by using mtime, streams via cat, and counts with awk. It tolerates missing/unreadable files and scales with many files.\n\n## Key Concepts\n- find with -mtime, -type, -name, -print0\n- xargs -0 for safe multi-file input\n- cat to concatenate streams\n- awk for counting matches across input\n- redirection to ignore errors for robustness\n\n## Code Example\n```bash\nfind /var/log/app-logs -type f -name 'app-*-*.log' -mtime -3 -print0 2>/dev/null | xargs -0 cat 2>/dev/null | awk '/ERROR/ {c++} END {print c+0}'\n```\n\n## Follow-up Questions\n- How would you get per-service breakdown?\n- How would you extend to include gzipped logs (gz) without external deps?","diagram":"flowchart TD\n  A[Start] --> B[Find last 3 days logs]\n  B --> C[Stream with cat]\n  C --> D[Count with awk]\n  D --> E[Output total]","difficulty":"beginner","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","MongoDB","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T17:42:36.538Z","createdAt":"2026-01-15T17:42:36.539Z"},{"id":"q-2520","question":"In a fleet of Linux hosts, log files sit under /var/log/relay/*/*.log and rotated to *.log.gz. Each line is a JSON object with fields ts (YYYY-MM-DDTHH:MM:SSZ), svc, lat_ms. Write a robust one-liner (no external deps beyond standard UNIX tools) that outputs each service's 95th percentile latency over the last 15 minutes, aggregating across all files and rotations, tolerant of missing/unreadable files and out-of-order lines?","answer":"Stream all logs through a single awk script that reads plain and gzipped files, parses ts with date -d, keeps only lines within the last 15 minutes, collects lat_ms by svc, and on END sorts and emits ","explanation":"## Why This Is Asked\n\nThis question probes the ability to perform robust, cross-file, cross-rotation log analysis using only standard UNIX tools. It requires handling both plain and gzipped inputs, time-window filtering, and percentile aggregation per service, all while tolerating malformed data.\n\n## Key Concepts\n\n- Streaming log processing across rotated files\n- Reading gzipped and plain logs without external deps\n- Time-window filtering with date parsing\n- Percentile computation in awk\n\n## Code Example\n\n```javascript\n// illustrative skeleton: the actual one-liner would embed in a script or be a longer one-liner\nconst sample = 'Parse and aggregate lat_ms per svc within 15m window';\n```\n\n## Follow-up Questions\n\n- How would you extend to multiple percentile targets (e.g., 95th and 99th)?\n- How would you test correctness with synthetic logs and randomized rotations?","diagram":null,"difficulty":"advanced","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Lyft","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T21:31:22.635Z","createdAt":"2026-01-15T21:31:22.636Z"},{"id":"q-2624","question":"In a Linux host running dozens of worker processes inside containers, build a POSIX-friendly watchdog using only standard UNIX tools to detect any worker that has spent uninterruptible sleep (D state) for more than 60 seconds within a 10-minute window and restart such workers with exponential backoff. Use /proc to derive each worker's startup command from /proc/$pid/cmdline and /proc/cgroups to avoid system processes?","answer":"Use a time-window watchdog: every 2s for 10m, scan /proc to map PID to State and accumulate seconds in D state. After the window, restart offenders with backoff (5s, 15s, 30s) using the process cmdlin","explanation":"## Why This Is Asked\nThis tests OS internals, container boundaries, and tool discipline.\n\n## Key Concepts\n- /proc parsing with POSIX tools\n- D-state dwell time accumulation\n- Windowed aggregation over time\n- Restart with backoff and startup command reconstruction\n- Excluding system processes via /proc/[pid]/cgroup\n\n## Code Example\n\n```bash\n# Outline of approach (not full implementation)\n```\n\n## Follow-up Questions\n- How would you unit-test this watcher in a multi-tenant cluster?\n- How would you handle PIDs recycled within the window?\n","diagram":null,"difficulty":"advanced","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T04:08:56.983Z","createdAt":"2026-01-16T04:08:56.984Z"},{"id":"q-2738","question":"In a Unix host, per-job logs are under /srv/ci/jobs/<job_id>/logs, rotated daily with filenames log-YYYYMMDD.log. A symlink log-current.log points to the current day’s log file for each job. Write a robust one-liner (no external deps beyond standard UNIX tools) that prints the count of lines containing 'STATUS=FAILED' across all current-day log files for all jobs, gracefully handling missing files and broken links?","answer":"One robust approach is: find -L /srv/ci/jobs -name log-current.log -type f -print0 | xargs -0 -r grep -h 'STATUS=FAILED' | wc -l. This follows symlinks, ignores missing files, and counts only lines wi","explanation":"The candidate should propose a pipeline that safely traverses job dirs, follows symlinks where needed, and uses null-separated streams to handle spaces in paths. They should justify using -L with find to follow log-current.log, and using -print0/xargs -0 to prevent tokenization issues, culminating in wc -l for a total.","diagram":null,"difficulty":"beginner","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T09:47:40.582Z","createdAt":"2026-01-16T09:47:40.582Z"},{"id":"q-2781","question":"In a Unix host, logs live under /var/log/services/*/*.log and are rotated hourly; some files are .log, others .log.gz. Each line is of the form [YYYY-MM-DD HH:MM:SS] <service>: <message>. Write a robust one-liner (no external deps beyond standard UNIX tools) that prints a per-service histogram of the number of lines containing the word ERROR in the last 24 hours, aggregating across all files and rotations. Must handle missing or unreadable files and gzipped archives, and work with spaces in paths?","answer":"One viable solution: find /var/log/services -type f -mtime -1 -print0 | while IFS= read -r -d '' f; do case \"$f\" in *.gz) zcat \"$f\" ;; *) cat \"$f\" ;; esac; done | awk '/ERROR/ && match($0, /^\\[[0-9]{4","explanation":"Why this is asked: tests ability to craft robust shell one-liners that span plain and compressed logs, handle spaces, and aggregate by service. Key concepts: find -mtime, print0, safe looping, zcat for gz, awk for parsing and aggregation. Code Example: (see block). Follow-up: how would you adapt to count ERROR codes with 5xx prefixes, or test locally with mock logs?","diagram":null,"difficulty":"beginner","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Robinhood","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T11:39:00.928Z","createdAt":"2026-01-16T11:39:00.928Z"},{"id":"q-3086","question":"In a Unix host, a policy requires that any file within data/ under /shared/projects must be group-writable and have the group set to the project's primary group, but only for files changed in the last 24 hours. Write a robust one-liner (no external deps beyond standard UNIX tools) to identify files that violate this policy, ignoring unreadable files and handling spaces in names?","answer":"find /shared/projects -type f -path '*/data/*' -mtime -1 ! -perm -g+w -print0 2>/dev/null | while IFS= read -r -d '' f; do printf 'VIOLATION: %s\n' \"$f\"; stat -c '%A %G %n' \"$f\"; done","explanation":"## Why This Is Asked\n\nTests ability to enforce security policies using standard Unix tools, including robust file discovery, path handling, and error management.\n\n## Key Concepts\n\n- `find` with `-mtime` for filtering by modification time\n- `-path` for directory pattern matching\n- `-perm` to check group write permissions\n- `-print0` and `read -d ''` for handling filenames with spaces\n- `stat` for displaying file permissions and group information\n- Error redirection to ignore unreadable directories\n\n## Code Example\n\n```bash\nfind /shared/projects -type f -path '*/data/*' -mtime -1 ! -perm -g+w -print0 2>/dev/null | while IFS= read -r -d '' f; do printf 'VIOLATION: %s\\n' \"$f\"; stat -c '%A %G %n' \"$f\"; done\n```\n\n## How It Works\n\n1. `find /shared/projects` searches the target directory recursively\n2. `-type f` restricts to regular files only\n3. `-path '*/data/*'` matches files within any data subdirectory\n4. `-mtime -1` finds files modified in the last 24 hours\n5. `! -perm -g+w` identifies files without group write permissions\n6. `-print0` uses null bytes to handle spaces in filenames\n7. `2>/dev/null` suppresses permission errors for unreadable files\n8. The while loop reads each file and displays violation details with current permissions and group","diagram":"flowchart TD\n  A[Start] --> B[Find files in /shared/projects/*/data/*]\n  B --> C{Changed in last 24h?}\n  C -->|Yes| D{Has group-w write?}\n  D -->|No| E[Violation detected]\n  D -->|Yes| F[OK]\n  E --> G[Print details]\n  F --> H[End]","difficulty":"beginner","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","MongoDB","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T05:09:57.945Z","createdAt":"2026-01-17T02:10:43.765Z"},{"id":"q-3134","question":"In a Unix host, backups live under /srv/backups; each backup is named backup-YYYYMMDD.tar.gz and contains a data/ directory. Write a robust one-liner (no external deps beyond standard UNIX tools) that prints the total number of unique .log files across all backups modified in the last 7 days. The command should gracefully skip unreadable archives and ignore non-log files?","answer":"One-liner: find /srv/backups -name 'backup-*.tar.gz' -mtime -7 -print0 2>/dev/null | xargs -0 -I{} sh -c 'tar -tzf \"{}\" --wildcards \"*.log\" 2>/dev/null' | sort -u | wc -l. This uses tar to enumerate l","explanation":"## Why This Is Asked\n\nTests ability to orchestrate standard UNIX tools across many archives, handling spaces and errors.\n\n## Key Concepts\n\n- tar -tzf and --wildcards\n- find -mtime\n- xargs -0 with placeholder\n- sort -u for deduping\n- 2>/dev/null for resilience\n\n## Code Example\n\n```bash\nfind /srv/backups -name 'backup-*.tar.gz' -mtime -7 -print0 2>/dev/null | xargs -0 -I{} sh -c 'tar -tzf \"{}\" --wildcards \"*.log\" 2>/dev/null' | sort -u | wc -l\n```\n\n## Follow-up Questions\n\n- How would you adapt if backups could be .tar (uncompressed) as well?\n- How would you exclude logs under a certain path like data/tmp/ ?","diagram":null,"difficulty":"beginner","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Google","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T04:14:49.463Z","createdAt":"2026-01-17T04:14:49.463Z"},{"id":"q-3206","question":"In a Unix host, logs live under /var/log/ops/* with hourly rotation to *.log and sometimes *.log.gz. Lines are either plain text LEVEL: message or JSON like {ts: ISO8601, svc: name, lvl: ERROR}, and multi-line messages may follow a line that starts with a timestamp. Propose a robust one-liner (no external deps beyond standard UNIX tools) that prints a per-minute histogram of ERROR events for the last 90 minutes, aggregating across all files and rotations, tolerant of missing files and gz archives, and robust to mixed formats?","answer":"Plan: a streaming command reading all logs (*.log and *.log.gz) via zcat/cat, piped to awk. The awk normalizes timestamps from JSON ts or ISO header, filters ERROR lines, buckets by minute, and mainta","explanation":"## Why This Is Asked\nTests ability to design robust log aggregation across mixed formats, gzipped, and rotated logs under load.\n\n## Key Concepts\n- Pipeline robustness and fault tolerance\n- Timestamp normalization for JSON and plain text\n- Per-minute bucketing and sliding windows\n- Handling multi-line records and unreadable files\n\n## Code Example\n\n## Follow-up Questions\n- How would you adapt to 5-minute buckets? \n- How would you unit-test with synthetic logs?","diagram":null,"difficulty":"intermediate","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Microsoft","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T06:56:52.176Z","createdAt":"2026-01-17T06:56:52.176Z"},{"id":"q-3443","question":"In a Unix host, logs live under /var/log/trace/* with hourly rotation; some files end in .log, others .log.gz. Each line is either [YYYY-MM-DD HH:MM:SS] svc: duration=NNNms or JSON {\"ts\":\"YYYY-MM-DDTHH:MM:SSZ\",\"svc\":\"name\",\"dur_ms\":NNN}. Propose a robust one-liner (no external deps beyond standard UNIX tools) to produce a per-service histogram of total request duration binned into 2-minute intervals for the last 60 minutes, aggregating across all files and rotations, tolerant of missing/unreadable files and out-of-order timestamps?","answer":"Loop over /var/log/trace/*.{log,log.gz}, decompress with zcat when needed, parse either [YYYY-MM-DD HH:MM:SS] or JSON ts, extract dur_ms, compute bucket = int(epoch(ts)/120), accumulate totals per svc","explanation":"## Why This Is Asked\nTests ability to design a robust, real-world log analysis task across mixed formats and rotated archives.\n\n## Key Concepts\n- Parsing plain and JSON logs\n- Handling gzipped rotated files without extra deps\n- Time bucketing and out-of-order events\n- Resilience to missing/unreadable files\n\n## Code Example\n```javascript\n// Implementation outline (not the full solution)\n```\n\n## Follow-up Questions\n- How would you scale this for millions of logs?\n- How would you test with synthetic data and verify correctness?","diagram":"flowchart TD\n  A[Start] --> B[Scan files]\n  B --> C[Parse lines]\n  C --> D[Bucket by 2-min]\n  D --> E[Sum per svc]\n  E --> F[Output histogram]\n  F --> G[End]","difficulty":"intermediate","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["NVIDIA","Netflix","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T16:40:40.287Z","createdAt":"2026-01-17T16:40:40.287Z"},{"id":"q-3499","question":"In a Unix host, multiple services write JSON log lines under /logs/app/*/*.log and are rotated hourly to *.log.gz. Each line is {\"ts\":\"YYYY-MM-DDTHH:MM:SSZ\",\"svc\":\"<name>\",\"lvl\":\"LEVEL\",\"msg\":\"...\"}. Write a robust one-liner (no external deps beyond standard UNIX tools) that prints a per-service idle time metric: time since last log line for each service within the last hour, flagging services idle for more than 2 minutes. Handle gz archives, unreadable files, and out-of-order timestamps?","answer":"Approach: read all files (uncompressed and gzipped) with a single pass, parse ts and svc using lightweight regex in awk (no jq), convert ts to epoch, track per-service last_seen, compute age = now - l","explanation":"## Why This Is Asked\n\nTests ability to design a minimal, robust monitoring query using basic UNIX tools without dependencies, and to reason about time-windowed data across rotated logs.\n\n## Key Concepts\n\n- Robust globbing and gzip handling\n- Lightweight JSON parsing with awk\n- Per-service state tracking and time-window logic\n- Handling out-of-order/timestamp skew\n\n## Code Example\n\n```javascript\n# Idea for one-liner (not runnable here):\n# find /logs/app -type f \\( -name '*.log' -o -name '*.log.gz' \\) -print0 | \\\n#   xargs -0 -I{} sh -c '...'\n```\n\n## Follow-up Questions\n\n- How would you adapt this for a rolling window of 5 minutes with a trailing watermark?\n- What are edge cases if clocks drift or NTP is off?","diagram":"flowchart TD\n  A[Collect files] --> B[Decompress/Read]\n  B --> C[Parse ts & svc]\n  C --> D[Update last_seen per svc]\n  D --> E[Compute age; idle flag]\n  E --> F[Output results]","difficulty":"intermediate","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Netflix","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T18:54:04.134Z","createdAt":"2026-01-17T18:54:04.134Z"},{"id":"q-3573","question":"You are deploying a per-host log-tailer for thousands of services across a fleet. Logs live under /var/log/services/<svc>/*.log and rotate daily; older files may be *.log.gz. The on-host agent must stream new lines to a central sink with at-least-once delivery, tolerate missing/unreadable files, and stay correct during concurrent rotations. Describe design, data flow, and failure modes; outline an implementation strategy using only standard UNIX tools?","answer":"Design a per-host log-tailer using inotify/fanotify to monitor new and rotated files, track per-file offsets (inode+offset) in a local manifest, decompress *.log.gz files on-the-fly with gzip -d or zcat, and implement at-least-once delivery with retry logic and backpressure management to a central sink.","explanation":"## Why This Is Asked\n\nEvaluates practical OS knowledge: file watching, log rotation, compression, and reliable delivery at scale.\n\n## Key Concepts\n- inotify and fanotify for file system monitoring\n- Per-file inode and offset tracking for state persistence\n- Handling gzipped rotations (zcat/gzip)\n- Buffering and backpressure to sink\n- Idempotent replay and deduplication on restart\n\n## Code Example\n```bash\n# Minimal sketch of watch loop (not a full solution)\ninotifywait -m -e create,close_write,move /var/log/services\n```\n\n## Follow-up Questions\n- How to test rotation edge cases?\n- How to scale the agent across thousands of services?\n- What monitoring and alerting would you implement?","diagram":null,"difficulty":"advanced","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","NVIDIA","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T05:42:58.410Z","createdAt":"2026-01-17T21:38:25.742Z"},{"id":"q-3583","question":"Scenario: A Unix host stores executables under /opt/apps with nested folders. Some files have setuid/setgid bits. Write a robust one-liner (no external deps beyond standard UNIX tools) that lists all files modified in the last 24 hours with setuid or setgid bits, printing per file: permissions, owner:group, and full path. The command must skip unreadable files and handle spaces in paths?","answer":"find /opt/apps -type f -mtime -1 -perm /6000 -print0 2>/dev/null | while IFS= read -r -d '' f; do stat -c '%A %U:%G %n' \"$f\"; done","explanation":"## Why This Is Asked\nThis question tests advanced shell scripting skills, requiring candidates to create robust, portable one-liners that handle edge cases like spaces in filenames and unreadable files using standard Unix tools.\n\n## Key Concepts\n- **find command**: Uses `-mtime -1` to filter files modified within the last 24 hours\n- **Permission checking**: Employs `-perm /6000` to detect files with setuid (4000) or setgid (2000) bits\n- **Safe filename handling**: Uses `-print0` with `read -d ''` to properly handle spaces and special characters in paths\n- **Error handling**: Redirects stderr to `/dev/null` to skip unreadable files\n- **Formatted output**: Leverages `stat -c` to display permissions, owner:group, and full path\n\n## Code Example\n```bash\nfind /opt/apps -type f -mtime -1 -perm /6000 -print0 2>/dev/null | while IFS= read -r -d '' f; do\n  stat -c '%A %U:%G %n' \"$f\"\ndone\n```\n\n## Follow-up Questions\n- How would you modify this command to search multiple directories?\n- What changes would be needed for BSD systems where `stat` syntax differs?\n- How could you extend this to also show file sizes?","diagram":null,"difficulty":"beginner","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Bloomberg","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T05:35:05.500Z","createdAt":"2026-01-17T22:34:06.936Z"},{"id":"q-3617","question":"On a Linux host with many long-running services, write a robust one-liner (no external deps beyond standard UNIX tools) that lists the top 5 processes by number of open file descriptors. Only include processes that started at least 24 hours ago. Output: PID, UID, user, FD count, executable path, and a human-friendly age (e.g., 3d4h)? Must handle unreadable pids and spaces in cmdlines. Use /proc and standard tools?","answer":"Enumerate /proc/[0-9]*; for each pid with readable /proc/$pid/fd, read starttime from /proc/$pid/stat and convert to age = starttime / CLK_TCK; skip age < 86400. Count fd in /proc/$pid/fd; map UID to username with getent; output formatted results sorted by FD count.","explanation":"## Why This Is Asked\n\nTests ability to reason about /proc, process metadata, and resource attribution in large systems; probes edge cases (spaces in paths, unreadable pids) and performance implications of scanning /proc.\n\n## Key Concepts\n\n- /proc filesystem and permissions\n- Counting file descriptors per process via /proc/[pid]/fd\n- starttime in /proc/[pid]/stat and CLK_TCK conversion\n- Mapping UID to username\n- Robust error handling and portability\n\n## Code Example\n\n```bash\n#!/bin/bash\nh=$(getconf CLK_TCK)\nfor p in /proc/[0-9]*; do\n  [ -r \"$p/fd\" ] || continue\n  s=$(awk '{print $22}' \"$p/stat\" 2>/dev/null) || continue\n  age=$((($(date +%s) - $(awk '{print $21}' \"$p/stat\" 2>/dev/null)/$h - $(cat \"$p/uptime\" 2>/dev/null | awk '{print $1}'))))\n  [ $age -ge 86400 ] || continue\n  fd=$(ls \"$p/fd\" 2>/dev/null | wc -l)\n  uid=$(stat -c %u \"$p\" 2>/dev/null)\n  user=$(getent passwd \"$uid\" 2>/dev/null | cut -d: -f1)\n  cmd=$(tr '\\0' ' ' < \"$p/cmdline\" 2>/dev/null | sed 's/ *$//')\n  printf \"%d %d %s %d %s %s\\n\" \"${p##*/}\" \"$uid\" \"$user\" \"$fd\" \"$cmd\" \"$(awk -v a=$age 'BEGIN{d=int(a/86400); h=int((a%86400)/3600); printf \"%dd%dh\", d, h}')\"\ndone | sort -k4 -nr | head -5 | column -t\n```\n\nThis solution handles unreadable PIDs gracefully, properly counts file descriptors, converts process start time to human-readable age, and outputs the top 5 processes by FD count with all requested fields formatted cleanly.","diagram":"flowchart TD\n  A[Start] --> B[Enumerate /proc]\n  B --> C{PID readable?}\n  C -- Yes --> D[Read stat starttime]\n  D --> E[Compute age]\n  E -- age>=24h --> F[Count /proc/$pid/fd]\n  F --> G[Resolve uid/user/exe]\n  G --> H[Emit line]\n  H --> I[Sort, head -5]\n  C -- No --> J[Skip]","difficulty":"advanced","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","Oracle","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T05:14:33.957Z","createdAt":"2026-01-17T23:40:09.633Z"},{"id":"q-3765","question":"In a Unix host, there is a directory /var/experiments containing subdirectories for each experiment. Each subdir may contain a file metrics.txt with lines that are either PASS or FAIL; some subdirs may contain metrics.txt.gz instead. Write a robust one-liner (no external dependencies beyond standard UNIX tools) that prints per-experiment fail rate as: <experiment>: <fails>/<total> (<percent>%), handling missing files and gzipped files, and across many experiments (spaces in names)?","answer":"for d in /var/experiments/*; do f=\"$d/metrics.txt\"; gz=\"$d/metrics.txt.gz\"; if [ -f \"$f\" ]; then t=$(wc -l < \"$f\"); e=$(grep -c '^FAIL' \"$f\"); elif [ -f \"$gz\" ]; then t=$(gzip -cd \"$gz\" | wc -l); e=$(","explanation":"## Why This Is Asked\nTests ability to compose robust shell pipelines handling gzipped files and missing inputs. \n\n## Key Concepts\n- Directory traversal with globbing; file presence checks; gzip decompression with standard tools; integer arithmetic in shell.\n\n## Code Example\n```bash\nfor d in /var/experiments/*; do\n  f=\"$d/metrics.txt\"; gz=\"$d/metrics.txt.gz\"\n  if [ -f \"$f\" ]; then t=$(wc -l < \"$f\"); e=$(grep -c '^FAIL' \"$f\");\n  elif [ -f \"$gz\" ]; then t=$(gzip -cd \"$gz\" | wc -l); e=$(gzip -cd \"$gz\" | grep -c '^FAIL');\n  else t=0; e=0; fi\n  if [ \"$t\" -gt 0 ]; then p=$(( e * 100 / t )); echo \"$(basename \"$d\"): $e/$t ($p%)\"; fi\ndone\n```\n\n## Follow-up Questions\n- How would you adapt for mixed case-sensitive values like FAIL and FAILURES?\n- How would you parallelize this for many experiments while preserving stable output order?","diagram":null,"difficulty":"beginner","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google","Hugging Face"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T08:46:04.010Z","createdAt":"2026-01-18T08:46:04.010Z"},{"id":"q-3836","question":"Scenario: A Unix host runs dozens of services that log to daily rotated plain-text logs under /var/log/app-logs/app-<service>-YYYYMMDD.log, plus a real-time stream via a named pipe at /var/log/app-logs/pipe.log which receives JSON entries like {ts: ISO8601, svc: name, lvl: ERROR, msg: ...}. Some services also compress old logs to .gz. Write a robust one-liner (no external deps beyond standard UNIX tools) that outputs a per-minute histogram of ERROR events for the last 60 minutes, aggregating across both file logs and the live pipe, tolerant of missing/unreadable files and gzipped archives, and resilient to mixed formats (plain text and JSON)?","answer":"One-liner approach: read both plain logs and gzips, parse lines for either JSON {'ts':'...','svc':'...','lvl':'ERROR',...} or text containing 'ERROR', normalize timestamp to epoch, filter last 60 minu","explanation":"## Why This Is Asked\n\nInterviews test ability to craft resilient one-liners that mix formats, rotations, and streaming input. It emphasizes time-windowing, gzip handling, and incremental aggregation with standard tools.\n\n## Key Concepts\n- Log format normalization across JSON and plain text\n- Time-window filtering with date/epoch\n- Combining multiple sources (files + pipe)\n- Robustness to missing/unreadable files\n- Streaming compression (gzip) and rotations\n\n## Code Example\n```javascript\n// Implementation example not shown here as this is the candidate's task\n```\n\n## Follow-up Questions\n- How would you adapt this for a multi-node cluster?\n- How would you test edge cases like clock skew?","diagram":"flowchart TD\n  A[Start] --> B[Collect sources]\n  B --> C[Parse lines (JSON/text)]\n  C --> D[Normalize time to epoch]\n  D --> E[Filter last 60m]\n  E --> F[Bucket by service + minute]\n  F --> G[Output histogram]","difficulty":"advanced","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Meta","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T11:30:55.314Z","createdAt":"2026-01-18T11:30:55.314Z"},{"id":"q-3904","question":"On a Unix host, multiple services write logs under /var/log/app-logs/service-*/ and a central JSON log at /var/log/central/combined.log.gz. Each plain-text line in app-logs uses [YYYY-MM-DD HH:MM:SS] <service>: <msg>, while the central log has JSON lines like {\"ts\":\"ISO\",\"svc\":\"name\",\"lvl\":\"LEVEL\",\"msg\":\"...\"}. Write a robust one-liner (no external deps beyond standard UNIX tools) that prints a per-service count of ERROR events in the last 2 hours, aggregating across all files and rotations, tolerating missing/unreadable files and gz archives, and handling mixed formats?","answer":"Proposed one-liner approach: scan both plain and gz logs, extract timestamps from text lines or JSON ts, filter to last 2 hours, and accumulate per-service ERROR counts with an awk associative array; ","explanation":"## Why This Is Asked\nTests ability to handle mixed log formats, gzipped archives, and resilience to missing files with a simple, scalable one-liner.\n\n## Key Concepts\n- POSIX tools: find/grep/awk/zgrep\n- Parsing multiple formats (text and JSON)\n- Time window filtering and per-service aggregation\n\n## Code Example\n```javascript\n// Conceptual example: build per-service counts from mixed sources\nconst counts = {};\n// parse lines from both formats, update counts['service'] for ERROR\n```\n\n## Follow-up Questions\n- How would you adapt this to scale across thousands of log files?\n- How would you test this for time zones and clock skew issues?","diagram":null,"difficulty":"beginner","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","MongoDB","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T14:35:41.069Z","createdAt":"2026-01-18T14:35:41.070Z"},{"id":"q-3976","question":"Scenario: On a Unix host, logs sit in /var/log/app-logs with two formats: app-<service>-YYYYMMDD.log and rotated .log.gz. Write a robust one-liner (no external deps beyond standard UNIX tools) that prints the total number of non-empty lines across all files modified today. The command should gracefully skip unreadable files and work across many services?","answer":"Use a single Bash one-liner: collect today-modified files (log and gz), read with zcat for gz, filter non-empty lines, and count. For example:\nbash -lc 'set -o pipefail; find /var/log/app-logs -daysta","explanation":"## Why This Is Asked\nTests robust file handling, gz support, and today-filtering in a single pipeline.\n\n## Key Concepts\n- find with -daystart -mtime 0\n- -print0 and while read -r -d '' for spaces\n- zcat for gz, cat for uncompressed\n- skip unreadable with redirections\n\n## Code Example\n```bash\n# see answer for the one-liner\n```\n\n## Follow-up Questions\n- How would you adapt to per-service totals?\n- How to safely run in a cron job?","diagram":null,"difficulty":"beginner","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Google","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T18:38:02.888Z","createdAt":"2026-01-18T18:38:02.890Z"},{"id":"q-4011","question":"In a Unix host, logs live under /logs/platform/*/*.log and are rotated hourly into *.log.gz. Each line is a JSON object: {\"ts\":\"YYYY-MM-DDTHH:MM:SSZ\",\"svc\":\"name\",\"lvl\":\"LEVEL\",\"msg\":\"...\"}. Provide a robust one-liner (no external deps beyond standard UNIX tools) that outputs a per-service, per-minute histogram of ERROR events for the last 60 minutes, aggregating across all files and rotations, tolerant of missing/unreadable files and gz archives, and deduplicating lines that may appear in multiple rotated files via inode tracking?","answer":"I would build a single streaming pipeline: enumerate /logs/platform/*/*.{log,log.gz}, skip seen inodes to dedupe across rotations, decompress with zcat, filter lines with ERROR, extract ts and svc fro","explanation":"Why This Is Asked: tests practical Unix skills. Key Concepts: log rotation, gz handling, JSON parsing with awk, time bucketing, inode-based dedup. Code Example: Sketch: for f in /logs/platform/*/*.{log,log.gz}; do ...; done | awk '...'. Follow-up: BSD/macOS date differences; scaling to many hosts.","diagram":"flowchart TD\n  A[Collect files] --> B[Decompress if gz]\n  B --> C[Parse JSON lines]\n  C --> D[Extract ts and svc]\n  D --> E[Bucket by minute]\n  E --> F[Aggregate per svc]\n  F --> G[Output histogram]","difficulty":"intermediate","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Anthropic","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T19:30:51.551Z","createdAt":"2026-01-18T19:30:51.551Z"},{"id":"q-4028","question":"In a Unix host, logs live under /logs/** and rotate hourly into *.log and *.log.gz. Each line is either 'YYYY-MM-DD HH:MM:SS <LEVEL>: <text>' or JSON with fields ts, lvl, msg. Write a robust one-liner (no external deps beyond standard UNIX tools) that prints the number of unique ERROR messages observed in the last 24 hours across all files, across both formats, tolerant of missing/unreadable files and gz archives?","answer":"find /logs -type f \\( -name \"*.log\" -o -name \"*.log.gz\" \\) -mtime -1 -print0 | xargs -0 -I{} sh -lc 'gzip -dc \"{}\" 2>/dev/null || cat \"{}\" 2>/dev/null' | grep -i 'error' | sort -u | wc -l","explanation":"## Why This Is Asked\nThis question tests the ability to handle mixed log formats, file rotation, and compressed archives in a concise, robust manner using standard UNIX tools.\n\n## Key Concepts\n- **File Discovery**: Locate log files modified within the last 24 hours using `find` with `-mtime -1`\n- **Content Streaming**: Handle both uncompressed and compressed logs using conditional decompression\n- **Error Filtering**: Apply case-insensitive filtering across different log formats\n- **Deduplication**: Remove identical error messages and count unique occurrences\n- **Error Tolerance**: Gracefully handle missing files, unreadable archives, and decompression failures\n\n## Command Breakdown\n1. `find /logs -type f \\( -name \"*.log\" -o -name \"*.log.gz\" \\) -mtime -1` - Discover log files modified in the last 24 hours\n2. `-print0 | xargs -0` - Safely handle filenames with spaces or special characters\n3. `sh -lc 'gzip -dc \"{}\" 2>/dev/null || cat \"{}\" 2>/dev/null'` - Decompress gz files or read plain files, suppressing errors\n4. `grep -i 'error'` - Filter for error messages case-insensitively\n5. `sort -u | wc -l` - Remove duplicates and count unique error messages","diagram":null,"difficulty":"beginner","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","LinkedIn","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-19T07:41:44.543Z","createdAt":"2026-01-18T20:40:17.491Z"},{"id":"q-4175","question":"Scenario: A Unix host aggregates logs for dozens of services under /var/log/services/<service>/. Logs rotate hourly; some are plain .log and others .log.gz. Lines are either a plain timestamped message like [YYYY-MM-DD HH:MM:SS] LEVEL: text or a JSON object {\"ts\": \"YYYY-MM-DDTHH:MM:SSZ\", \"svc\": \"service\", \"lvl\": \"ERROR\", \"msg\": \"...\"}. A per-service live feed is written to /var/log/services/<service>/live.log as JSON lines. Write a robust pipeline (standard UNIX tools only) that outputs a per-service, per-minute histogram of ERROR events for the last 60 minutes, aggregating across files, archives, and the live feed, tolerating unreadable files and spaces in paths?","answer":"Stream all relevant logs in one pass: read both plain and JSON lines from /var/log/services and gzipped archives, plus per-service live streams; normalize timestamps (text: [YYYY-MM-DD HH:MM:SS], JSON","explanation":"## Why This Is Asked\nTests ability to design robust log pipelines at scale with mixed formats, rotation, and live data.\n\n## Key Concepts\n- Cross-format parsing (text and JSON)\n- Time-window bucketing per minute\n- Rotated and gzipped files\n- Inode-based dedup across rotations\n- Live data integration\n\n## Code Example\n\n```bash\n# skeleton Example\nfind /var/log/services -type f -name '*.log*' -print0 | xargs -0 -I{} sh -c 'zcat \"$1\" 2>/dev/null || cat \"$1\" 2>/dev/null' -- {}\n```\n\n## Follow-up Questions\n- How would you scale to 10k services?\n- How would you unit-test the windowing logic?","diagram":"flowchart TD\n  Ingest[Ingest logs & live streams] --> Parse[Parse timestamps]\n  Parse --> Bucket[Bucket by service & minute]\n  Bucket --> Count[Count ERROR events]\n  Count --> Output[Output histogram]","difficulty":"advanced","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-19T06:54:31.062Z","createdAt":"2026-01-19T06:54:31.063Z"},{"id":"q-4213","question":"In a Unix host, /data/projects contains thousands of files across nested dirs; some filenames contain spaces. Write a robust one-liner (no external deps beyond standard UNIX tools) that prints the five most common file extensions among files modified in the last 7 days, printing extension and count, ignoring directories and unreadable files?","answer":"One-liner approach: enumerate files modified in the last 7 days with find, handle spaces via -print0, extract the extension from the basename, skip files with no extension, and tally counts with sort ","explanation":"Why This Is Asked\n- Tests ability to craft reliable one-liners around file discovery and parsing. \n\nKey Concepts\n- find usage: -type f, -mtime, -print0\n- safe parsing with -print0 and read -d ''\n- basename and extension extraction\n- handle no-extension cases gracefully\n- sort/uniq for top counts\n\nCode Example\n```bash\n# Example approach (not required in answer field)\nfind /data/projects -type f -mtime -7 -print0 2>/dev/null | while IFS= read -r -d '' f; do base=\"${f##*/}\"; ext=\"${base##*.}\"; [ \"$base\" = \"$ext\" ] && continue; echo \"$ext\"; done | sort | uniq -c | sort -nr | head -n 5\n```\n\nFollow-up Questions\n- How would you adapt for archives or hidden files?\n- How would you handle files with multiple dots in their names?","diagram":"flowchart TD\n  Start --> FindFiles\n  FindFiles --> ProcessBasename\n  ProcessBasename --> ExtractExt\n  ExtractExt --> CountExts\n  CountExts --> OutputTop5","difficulty":"beginner","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","LinkedIn","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-19T08:55:18.973Z","createdAt":"2026-01-19T08:55:18.973Z"},{"id":"q-4271","question":"In a Unix host, logs live under /var/logs/services/*/*.log and *.log.gz, rotated hourly. Two formats exist: (a) plain lines 'YYYY-MM-DD HH:MM:SS LEVEL: message', (b) JSON lines '{\"ts\":\"YYYY-MM-DDTHH:MM:SSZ\",\"svc\":\"name\",\"lvl\":\"LEVEL\",\"msg\":\"...\"}'. Write a robust one-liner (no external deps beyond standard UNIX tools) that prints the top 3 services by total count of ERROR messages in the last 12 hours, aggregating across both formats and all rotated files, and gracefully skipping unreadable files?","answer":"Strong answer outline: a single pipeline that uses find to gather *.log and *.log.gz, zcat for gz, and awk to parse both formats, convert timestamps to epoch, filter to the last 12 hours, tally ERROR ","explanation":"## Why This Is Asked\nTests ability to unify multi-format logs and operate across rotated files with standard tools, a common on-call need.\n\n## Key Concepts\n- Cross-format parsing (plain and JSON)\n- Timestamp normalization to epoch\n- Robust file discovery and error handling\n- Per-service aggregation and top-k output\n\n## Code Example\n```bash\n# sketch of approach (full one-liner too long to fit here)\nfind /var/logs/services -type f \\( -name '*.log' -o -name '*.log.gz' \\) -print0 | \\\n  xargs -0 -I{} sh -lc 'case \"$1\" in *.gz) zcat \"$1\" ;; *) cat \"$1\" ;; esac' | \\\n  awk -v thr=$(date -d '12 hours ago' +%s) '{...} END{...}'\n```\n\n## Follow-up Questions\n- How would you adapt for concurrent decompression?\n- How would you test with synthetic logs to validate correctness?","diagram":"flowchart TD\n  A[Input Logs] --> B[Handle Formats]\n  B --> C[Parse Timestamps]\n  C --> D[Aggregate by Service]\n  D --> E[Output Top 3]","difficulty":"beginner","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-19T11:01:25.004Z","createdAt":"2026-01-19T11:01:25.004Z"},{"id":"q-4344","question":"On a Linux host serving many services, dangling (deleted) file handles can cause resource leaks. Create a robust one-liner (no external deps beyond standard UNIX tools) that prints, per process, any open file descriptor that points to a path that no longer exists on disk. Output: PID, process name, FD number, and the target path as read by readlink (including ' (deleted)' if present). Handle spaces in names and permissions gracefully?","answer":"for fd in /proc/*/fd/*; do pid=$(basename $(dirname \"$fd\")); if [ -L \"$fd\" ]; then tgt=$(readlink \"$fd\" 2>/dev/null); if echo \"$tgt\" | grep -q ' (deleted)'; then comm=$(ps -p \"$pid\" -o comm= 2>/dev/nu","explanation":"## Why This Is Asked\nTriage and reason about deleted-but-open files on a live host.\n\n## Key Concepts\n- /proc fd namespace, readlink behavior for deleted files, permission-safe enumeration, robust printing.\n- Handling spaces in PIDs/comm names; forgiving on transient processes.\n\n## Code Example\n```bash\nfor fd in /proc/*/fd/*; do\n  pid=$(basename \"$(dirname \"$fd\")\")\n  if [ -L \"$fd\" ]; then\n    tgt=$(readlink \"$fd\" 2>/dev/null)\n    if echo \"$tgt\" | grep -q ' (deleted)'; then\n      comm=$(ps -p \"$pid\" -o comm= 2>/dev/null)\n      printf '%s\\t%s\\t%s\\t%s\\n' \"$pid\" \"$comm\" \"${fd##*/}\" \"$tgt\"\n    fi\n  fi\ndone 2>/dev/null\n```\n\n## Follow-up Questions\n- How would you adapt this to handle deleted files that are not yet reported in readlink?\n- How would you integrate this into a periodic health check with minimal overhead?","diagram":"flowchart TD\n  A[Enumerate /proc/*/fd/*] --> B[Filter symlinks]\n  B --> C[readlink; detect ' (deleted)']\n  C --> D[Query process name]\n  D --> E[Print PID, name, fd, path]","difficulty":"advanced","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Hashicorp","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-19T14:53:09.806Z","createdAt":"2026-01-19T14:53:09.806Z"},{"id":"q-4605","question":"In a Unix host, logs live under /srv/logs/app/ with files app-YYYY-MM-DD.log and nightly rotation to app-YYYY-MM-DD.log.gz. Each line is either a plain log line like YYYY-MM-DD HH:MM:SS - svc: message or a JSON object {\"ts\":\"YYYY-MM-DD HH:MM:SS\",\"svc\":\"name\",\"lvl\":\"LEVEL\",\"msg\":\"...\"}. Write a robust one-liner (no external deps beyond standard UNIX tools) that prints, for every service, the number of unique error messages seen in the last 60 minutes, across all files and rotations, deduplicating identical messages per service, and tolerant of unreadable files?","answer":"Collect both plain and JSON log lines from app-*.log and app-*.log.gz, filter to the last 60 minutes, parse both formats to extract svc and msg, then for each service count distinct messages and outpu","explanation":"## Why This Is Asked\nTests ability to merge heterogeneous log formats, handle compressed files, enforce a time window, and deduplicate per service—common in production observability tasks.\n\n## Key Concepts\n- Robust multi-format parsing (plain and JSON)\n- Reading gzipped logs transparently\n- Time-window filtering via epoch comparison\n- Per-service deduplication and aggregation\n\n## Code Example\n\n```bash\n# (Conceptual outline; exact one-liner will combine find, zcat/cat, and awk to parse both formats, filter by last 60m, deduplicate per svc/msg, and print top-5)\nfind /srv/logs/app -type f \\( -name 'app-*.log' -o -name 'app-*.log.gz' \\) -print0 | \\\n  xargs -0 -I{} sh -c 'if [ -f \"{}\" ]; then if [[ \"{}\" == *.gz ]]; then zcat \"{}\"; else cat \"{}\"; fi; fi' | \\\n  awk -v since=\"$(date -d \"60 minutes ago\" +%s)\" '\n    # parse both formats, emit svc|msg when ts>=since\n  ' | \\\n  sort | uniq -c | sort -nr | head -n5\n```\n\n## Follow-up Questions\n- How would you extend to handle a configurable time window? \n- How would you port this to a distributed logging system (eg, Kafka)?","diagram":"flowchart TD\n  A[Start] --> B[Read plain lines] \n  A --> C[Read JSON lines]\n  B --> D[Parse ts svc msg from text]\n  C --> D\n  D --> E[Filter by last 60m]\n  E --> F[Deduplicate per svc/msg]\n  F --> G[Aggregate counts]\n  G --> H[Output top-5 per svc]\n","difficulty":"beginner","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Hashicorp","Hugging Face"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T04:17:04.667Z","createdAt":"2026-01-20T04:17:04.668Z"},{"id":"q-4649","question":"Context: A multi-tenant Unix host under heavy load shows sporadic 'Too many open files' errors. Without external tooling, design a robust approach (one or two small scripts) using only standard UNIX utilities to identify the top 5 processes by open file descriptors in the last 60 minutes. Output must include: PID, user, CMD, fd count, and a sample FD target path (including deleted files). Address unreadable /proc entries and long paths?","answer":"Approach: snapshot /proc every minute for 60 minutes, count /proc/<pid>/fd entries, map PID to user from /proc/<pid>/status and CMD from /proc/<pid>/cmdline, resolve fd targets with readlink -f, and f","explanation":"## Why This Is Asked\nTests deep Unix familiarity with /proc, process accounting, and resilient scripting. Candidates must propose a robust, low-overhead approach that remains accurate under unreadable entries and deleted file targets.\n\n## Key Concepts\n- /proc fs navigation and permissions\n- Open file descriptor counting per pid\n- Handling deleted/inaccessible targets\n- Producing CSV-friendly, production-safe output\n\n## Code Example\n```bash\n# Snapshot loop (conceptual)\nfor pid in /proc/[0-9]*; do\n  [ -d \"$pid/fd\" ] || continue\n  count=$(ls -1 \"$pid/fd\" 2>/dev/null | wc -l)\n  user=$(ps -o user= -p \"${pid##*/}\" 2>/dev/null)\n  cmd=$(tr '\\0' ' ' < \"$pid/cmdline\" 2>/dev/null)\n  echo \"$pid,$user,$cmd,$count\"\ndone\n```\n\n## Follow-up Questions\n- How would you centralize results across many hosts?\n- How would you adapt for containers with PID namespaces?","diagram":null,"difficulty":"advanced","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Coinbase","Databricks"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T06:57:00.384Z","createdAt":"2026-01-20T06:57:00.384Z"},{"id":"q-4697","question":"On a Unix host, logs are under /logs/app-logs with hourly rotation into app-<service>-YYYYMMDD.log and gzipped as *.log.gz. A real-time stream provides JSON lines appended to /logs/app-logs/stream.log: {\"ts\":\"YYYY-MM-DDTHH:MM:SSZ\",\"svc\":\"name\",\"lvl\":\"ERROR\",\"msg\":\"...\"}. Write a robust one-liner (no external deps beyond standard UNIX tools) that outputs the per-service moving average of ERROR events per minute over the last 15 minutes, across both file-based logs and the live stream, tolerant of missing/unreadable files and gz archives, and avoiding double-counting lines that appear in both current and rotated copies?","answer":"Enumerate all files under /logs/app-logs including *.log and *.log.gz, plus the live stream. Use zcat for gz, parse either plaintext or JSON via regex, extract ts and svc, bucket by minute, and accumu","explanation":"## Why This Is Asked\nTests capability to federate multiple log sources, handle rotations and compression, and derive a moving statistic without external deps.\n\n## Key Concepts\n- Cross-format parsing (plain log vs JSON)\n- Decompression of gz files without external tools\n- Time bucketing by minute and sliding window maintenance\n- Per-service aggregation in a single one-liner\n\n## Code Example\n```bash\n#!/usr/bin/env bash\n# Conceptual outline: one-liner approach using awk and standard tools\n# Not a drop-in; illustrates the technique described in the answer.\nfiles=$(printf '%s\n' /logs/app-logs/*.log /logs/app-logs/*.log.gz 2>/dev/null)\nstream=/logs/app-logs/stream.log\nzcat \"$f\" 2>/dev/null | awk -v W=15 'BEGIN{FS=\"\"} {if(match($0,/^\\[([0-9]{4}-[0-9]{2}-[0-9]{2} [0-9:]{8})\\]/)){t=$1\" \"$2}else if(match($0,/{\"ts\":\"([^\"]+)\"/)){t=$1}}; svc=($0~\"svc\"?gensub(/.*\\\"svc\\\":\\\"([^\\\"]+)\\\".*/,\"\\\\1\",\"g\",$0):\"unknown\"); if(t && $0 ~ /ERROR/){minute=substr(t,1,16); count[svc,minute]++}}\nEND{for(k in count){split(k,a,\",\"); svc=a[1]; minute=a[2]; print svc\":\"minute\" \"count[k]}}' < /dev/null\n```\n\n## Follow-up Questions\n- How would you adapt the solution to streaming dashboards with real-time updates?\n- How to test correctness with synthetic logs and rotations without downtime?","diagram":"flowchart TD\n  A[Start] --> B[Discover files and stream]\n  B --> C[Decompress gz files]\n  C --> D[Parse text and JSON formats]\n  D --> E[Bucket by minute per service]\n  E --> F[Maintain 15-minute sliding window]\n  F --> G[Output per-service moving averages]\n  G --> H[End]","difficulty":"advanced","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","Lyft","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T08:50:25.550Z","createdAt":"2026-01-20T08:50:25.550Z"},{"id":"q-4777","question":"In a Unix host, directory tree under /projects contains files some world-writable and some recently modified. Write a robust one-liner (no external deps beyond standard UNIX tools) that lists each world-writable file modified in the last 24 hours, printing path, owner, and permission bits. Ensure it handles spaces in filenames and large trees, skipping unreadable dirs?","answer":"find /projects -type f -mtime -1 -perm -0002 -printf \"%p %U %m\\n\"","explanation":"## Why This Is Asked\nThis question tests basic file discovery, permission checks, and robustness of a shell one-liner in real-world directory trees.\n\n## Key Concepts\n- find, -type f, -mtime, -perm, -printf\n- Handling spaces and large trees\n- Portability GNU vs BSD\n\n## Code Example\n```javascript\nfind /projects -type f -mtime -1 -perm -0002 -printf \"%p %U %m\\\\n\"\n```\n\n## Follow-up Questions\n- How would you adjust for 7 days or a specific user?\n- How would you safely handle filenames with newlines?","diagram":null,"difficulty":"beginner","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Google","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T11:49:02.854Z","createdAt":"2026-01-20T11:49:02.854Z"},{"id":"q-481","question":"You're debugging a production system where processes are hanging. Using only Unix tools, how would you identify which processes are blocked on I/O, what they're waiting for, and safely terminate them without causing data corruption?","answer":"Use `lsof -p <PID>` to see open files and `strace -p <PID>` to identify blocked system calls. Check `/proc/<PID>/fd` for file descriptors. For safe termination, send SIGTERM first: `kill -15 <PID>`, wait for graceful shutdown, then use SIGKILL if necessary.","explanation":"## Process Identification\n- `ps aux | grep D` shows processes in uninterruptible sleep\n- `top` with 'H' shows thread-level status\n- `iostat -x 1` identifies I/O bottlenecks\n\n## Root Cause Analysis\n- `strace -p <PID>` reveals blocked system calls\n- `lsof -p <PID>` shows open files and network connections\n- `/proc/<PID>/status` provides process state details\n\n## Safe Termination\n- SIGTERM allows graceful shutdown\n- Check for child processes before killing\n- Verify no critical writes in progress","diagram":"flowchart TD\n  A[Detect hanging process] --> B[ps aux | grep D]\n  B --> C[strace -p PID]\n  C --> D[lsof -p PID]\n  D --> E{Safe to terminate?}\n  E -->|Yes| F[kill -15 PID]\n  E -->|No| G[Wait for I/O completion]\n  F --> H[Monitor with ps]\n  G --> F","difficulty":"advanced","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Snap","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-10T03:29:09.162Z","createdAt":"2025-12-26 12:51:05"},{"id":"q-4824","question":"Write a robust one-liner (no external deps beyond standard UNIX tools) to print the five most frequent ERROR messages observed in the last 2 hours across logs under /logs/app/** and /var/log, where lines are either 'YYYY-MM-DD HH:MM:SS <LEVEL>: <text>' or JSON lines {'ts':'YYYY-MM-DDTHH:MM:SSZ','lvl':'LEVEL','msg':'...'}, and support gzip-compressed files (*.log.gz)?","answer":"Approach: scan /logs/app and /var/log, handle *.log and *.log.gz, stream lines (zcat or cat), extract timestamps from either 'YYYY-MM-DD HH:MM:SS' or ts in JSON, keep only last 2 hours, count occurren","explanation":"Demonstrates multi-source log collection, mixed formats, gzip support, time-window filtering, and top-N aggregation with a portable toolchain.","diagram":"flowchart TD\n  A[Start] --> B[Collect files]\n  B --> C[Stream lines]\n  C --> D[Parse timestamps]\n  D --> E[Filter last 2h and ERROR]\n  E --> F[Count messages]\n  F --> G[Output top 5]","difficulty":"beginner","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","Slack","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T15:05:10.995Z","createdAt":"2026-01-20T15:05:10.995Z"},{"id":"q-5062","question":"In a Unix host, audit the /etc tree for dangerous permissions: print all files under /etc that are world-readable and world-writable and were modified in the last 24 hours. Provide a robust one-liner (no external deps beyond standard UNIX tools) that outputs the path and current permissions, gracefully handles unreadable files, ignores .gz files, and works with spaces in filenames?","answer":"I would propose the one-liner: find /etc -type f -mtime -1 -readable ! -name '*.gz' -perm /002 -a -perm /004 -printf '%p %m\\n' 2>/dev/null. It finds recently modified, world-readable and world-writabl","explanation":"## Why This Is Asked\nTests ability to compose a portable one-liner using standard UNIX tools. It covers discovery by modification time, precise permission checks, handling unreadable paths, and spaces in filenames, all in a robust, edge-case-friendly way.\n\n## Key Concepts\n- find with -mtime, -readable\n- -perm /002 and -perm /004 for world-writable/readable\n- -not -name '*.gz' to skip compressed archives\n- -printf for compact output; 2>/dev/null to suppress noise\n\n## Code Example\n```bash\nfind /etc -type f -mtime -1 -readable ! -name '*.gz' -perm /002 -a -perm /004 -printf '%p %m\\n' 2>/dev/null\n```\n\n## Follow-up Questions\n- How would you include the user owner and octal permissions in the output?\n- How would you integrate this into a daily audit script with structured logging?","diagram":"flowchart TD\n  A[Start] --> B[Search /etc for files modified 24h ago]\n  B --> C[Filter readable & not .gz]\n  C --> D[Require world-readable + world-writable]\n  D --> E[Output path and permissions]","difficulty":"beginner","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Instacart"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T04:19:11.703Z","createdAt":"2026-01-21T04:19:11.703Z"},{"id":"q-510","question":"You're debugging a production issue where a process is stuck in uninterruptible sleep (D state). How would you identify and handle this situation?","answer":"Use `ps aux | awk '$8 ~ /D/ {print $2, $11}'` to find D-state processes. Check `dmesg | grep -i oom` for OOM killer activity. For I/O issues, use `lsof -p <PID>` to identify blocked files. If it's NFS, verify mount status and network connectivity.","explanation":"## Identifying D-State Processes\n\n- Use `ps` with state filtering to find uninterruptible processes\n- Check system logs for hardware or filesystem errors\n- Examine I/O queues and block device status\n\n## Common Causes\n\n- NFS mount issues or network storage problems\n- Faulty hardware devices (disk, controller)\n- Kernel bugs or driver issues\n- Memory pressure causing I/O blocking\n\n## Resolution Strategies\n\n- Wait for hardware timeout (usually 30-120 seconds)\n- Check and fix underlying storage issues\n- Reboot as last resort if process won't recover\n- Monitor `/proc/<PID>/stack` for kernel call traces","diagram":"flowchart TD\n  A[Process enters D state] --> B{Identify cause}\n  B --> C[Hardware issue]\n  B --> D[Network storage]\n  B --> E[Kernel/driver]\n  C --> F[Check dmesg/logs]\n  D --> G[Verify mount status]\n  E --> H[Examine stack trace]\n  F --> I[Wait or fix hardware]\n  G --> J[Resolve network/storage]\n  H --> K[Update/reboot if needed]","difficulty":"intermediate","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["OpenAI","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-09T03:44:39.931Z","createdAt":"2025-12-26 12:51:06"},{"id":"q-5199","question":"On a Unix host, logs for services live under /var/log/services/** with mixed rotation: some files are daily rotated by logrotate (copytruncate) and others are hourly into .gz archives. Write a robust one-liner (no external deps beyond standard UNIX tools) that outputs a per-service, per-minute event rate for the last 15 minutes, counting each unique line only once even if it appears in multiple rotated copies, and tolerating unreadable files. The log lines come in either plain text like [YYYY-MM-DD HH:MM:SS] service: msg or JSON like {\\\"ts\\\":\\\"YYYY-MM-DDTHH:MM:SSZ\\\",\\\"svc\\\":\\\"name\\\",\\\"lvl\\\":\\\"ERROR\\\",\\\"msg\\\":\\\"...\\\"}?","answer":"I would deliver a compact Bash script: enumerate files with find, feed gzipped and plain logs to awk, parse both formats, convert ts to epoch, bucket by minute for the last 15 minutes, deduplicate per","explanation":"## Why This Is Asked\nTests practical Unix skills: multi-format parsing, rotation quirks, and dedup in data streams.\n\n## Key Concepts\n- Time parsing across formats\n- Handling gz archives without external tools\n- Deduplication of log lines\n- Incremental per-minute aggregation at scale\n\n## Code Example\n```bash\n# candidate would provide a script here\n```\n\n## Follow-up Questions\n- How would you scale this to terabytes of logs?\n- How would you adapt for non-UTC timestamps?\n","diagram":null,"difficulty":"advanced","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","Tesla","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T10:41:13.213Z","createdAt":"2026-01-21T10:41:13.213Z"},{"id":"q-5225","question":"On a Unix host, central configs live under /etc/configs, organized by environment as immediate subdirectories (e.g., /etc/configs/dev, /etc/configs/prod). Some filenames contain spaces. Write a robust one-liner (no external deps beyond standard UNIX tools) that prints a per-environment count of YAML files (*.yaml) modified in the last 24 hours. It should gracefully skip unreadable files and work with a large tree?","answer":"One-liner solution: find /etc/configs -type f -name '*.yaml' -mtime -1 -printf '%h\\n' 2>/dev/null | awk -F'/' '{env=$4; c[env]++} END{for (e in c) print e\":\", c[e]}'","explanation":"## Why This Is Asked\nTests practical skills in robust file querying, path parsing, and per-group aggregation under real-world constraints (spaces in names, unreadable files).","diagram":"flowchart TD\n  A[Start] --> B[Find YAML files modified last 24h]\n  B --> C[Print directory with env name]\n  C --> D[Aggregate counts per env]\n  D --> E[Output results]","difficulty":"beginner","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","Meta","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T11:35:37.081Z","createdAt":"2026-01-21T11:35:37.081Z"},{"id":"q-5258","question":"Scenario: A Unix host stores logs under /logs/cluster/*/app-logs with hourly rotation into .log and .log.gz. Lines are either 'YYYY-MM-DD HH:MM:SS <LEVEL> <text>' or JSON {\"ts\":\"...\",\"lvl\":\"ERROR\",\"msg\":\"...\"}. Write a robust one-liner (no external deps beyond standard UNIX tools) that prints the top 5 most frequent ERROR messages observed in the last 6 hours across all files, across formats and archives, skipping unreadable files?","answer":"Use a single pipeline that normalizes both formats to an error-message stream, filters for ERROR, and tallies by content. One robust one-liner: find /logs/cluster -type f \\( -name '*.log' -o -name '*.","explanation":"## Why This Is Asked\nTests ability to integrate text and JSON logs, handle rotation and compression, and produce actionable, concise output under time pressure. \n\n## Key Concepts\n- Normalizing heterogeneous log formats\n- Safe traversal of rotated and gzipped files\n- Shell pipelines, associative counting, and sorting for top results\n\n## Code Example\n```bash\nfind /logs/cluster -type f \\( -name '*.log' -o -name '*.log.gz' \\) -newermt '-6 hours' -print0 | \\\n  xargs -0 -I{} sh -c 'case {} in *.gz) zcat {} ;; *) cat {} ;; esac' | \\\n  awk 'BEGIN{IGNORECASE=1} /ERROR/ { if(match($0,/\"msg\":\"([^\"]+)\"/,m)) msg=m[1]; else { if(match($0,/ERROR/)){ msg=$0 } } if(msg) c[msg]++ } END{for(k in c) print c[k],\"|\",k}' | \\\n  sort -nr | head -n5\n```\n\n## Follow-up Questions\n- How would you adapt this to deduplicate identical messages across multiple hosts? \n- How would you verify correctness when logs are partially missing or severely skewed in time coverage?","diagram":"flowchart TD\n  A[Start] --> B[Discover logs]\n  B --> C[Decompress gzipped files]\n  C --> D[Normalize to message field]\n  D --> E[Filter ERROR rows]\n  E --> F[Count by message]\n  F --> G[Output top 5]","difficulty":"advanced","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T13:24:55.701Z","createdAt":"2026-01-21T13:24:55.701Z"},{"id":"q-538","question":"You notice a process is consuming excessive CPU on a production server. How would you diagnose and troubleshoot this issue using Unix commands?","answer":"I would start by using `top` or `htop` to identify the process ID (PID) consuming excessive CPU, then run `ps aux | grep PID` to get detailed process information and command line arguments. For deeper analysis, I'd use `strace -p PID` to monitor system calls in real-time, `lsof -p PID` to examine open files and network connections, and `perf top` for CPU performance profiling.","explanation":"## Diagnosis Steps\n- Use `top` or `htop` to identify the high CPU process and its PID\n- Run `ps aux` to view process details and command line arguments\n- Monitor system calls with `strace -p PID` to understand process behavior\n\n## Investigation Tools\n- `lsof -p PID` reveals open files and network connections\n- `perf top` provides CPU performance profiling and bottleneck identification\n- `/proc/PID/status` contains comprehensive memory and CPU statistics\n\n## Resolution\n- Send SIGTERM (`kill -15`) for graceful process shutdown\n- Use SIGKILL (`kill -9`) only when the process is unresponsive\n- Analyze logs in `/var/log/` to identify and address the root cause","diagram":"flowchart TD\n  A[High CPU Alert] --> B[top/htop - Identify PID]\n  B --> C[ps aux - Process Details]\n  C --> D[strace - System Calls]\n  D --> E[lsof - Open Files]\n  E --> F[perf top - Performance Profile]\n  F --> G{Process Responsive?}\n  G -->|Yes| H[kill -15 PID]\n  G -->|No| I[kill -9 PID]\n  H --> J[Monitor Resolution]\n  I --> J","difficulty":"intermediate","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Goldman Sachs","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":["troubleshooting","cpu profiling","strace","lsof","performance","diagnosis"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2026-01-08T11:43:07.374Z","createdAt":"2025-12-26 12:51:06"},{"id":"q-564","question":"You're debugging a production system where processes are hanging. Using only Unix tools, how would you identify which processes are stuck in uninterruptible sleep (D state) and what could be causing this?","answer":"Use `ps aux | awk '$8 ~ /^D/ {print $2, $11}'` to identify processes in uninterruptible sleep state. Examine `/proc/<pid>/stack` for kernel stack traces to understand what system calls are blocking. Common causes include NFS server issues, faulty storage drivers, hardware I/O problems, or disk bottlenecks. Use `iostat -x 1` to monitor I/O activity and `dmesg | grep -i error` to check for hardware or driver errors.","explanation":"## Identifying D-State Processes\n- Use `ps aux | awk '$8 ~ /^D/ {print $2, $11}'` to filter processes in uninterruptible sleep\n- Check `/proc/<pid>/status` for detailed process state information\n- Monitor system-wide D-state processes with `top` or `htop` filtered by state\n\n## Root Cause Analysis\n- Examine `/proc/<pid>/stack` to identify the specific kernel functions blocking the process\n- Use `dmesg | grep -i error` to detect hardware or driver-related issues\n- Monitor I/O statistics with `iostat -x 1` to identify storage bottlenecks\n- Check `lsblk` and `smartctl` for disk health and controller issues\n\n## Common Causes\n- NFS server unavailability or network connectivity issues\n- Faulty disk controllers, RAID arrays, or storage drivers\n- Hardware failures in storage subsystem (bad sectors, failing drives)\n- Memory pressure causing excessive swap activity\n- Kernel bugs or incompatible device drivers\n- Storage system saturation or filesystem corruption","diagram":"flowchart TD\n  A[ps aux | awk] --> B[Identify D-state PIDs]\n  B --> C[/proc/<pid>/stack]\n  C --> D[Analyze kernel stack]\n  D --> E[lsof -p <pid>]\n  E --> F[Check open files]\n  F --> G[strace -p <pid>]\n  G --> H[Trace syscalls]","difficulty":"advanced","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","OpenAI","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":["uninterruptible sleep","d state","ps command","proc filesystem","stack traces","nfs","i/o bottlenecks","kernel debugging"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2026-01-08T11:57:22.806Z","createdAt":"2025-12-26 12:51:07"},{"id":"q-894","question":"On a Linux host, /var/log/myapp.log is written by multiple processes. Implement a robust log rotation that triggers when the file reaches 100MB, keeps 7 rotated files, compresses older logs, and ensures no log loss while writers continue. Describe the approach, commands, and failure modes for concurrent writers?","answer":"Configure logrotate for /var/log/myapp.log with size 100M, rotate 7, compress and delaycompress, using sharedscripts. Use a postrotate block to signal the processes to reopen the log (e.g., systemctl ","explanation":"## Why This Is Asked\\n\\nThis question probes practical log management under concurrency, emphasizing safe rotation with multi-process writers and minimizing data loss. Expect discussion of reopen signals vs copytruncate, service integration, and failure modes.\\n\\n## Key Concepts\\n- logrotate configuration: size-based rotation, retention, compression\\n- handling concurrent writers: reopen vs copytruncate, signals (SIGHUP)\\n- service integration: systemd or pid signaling; permissions and timing\\n\\n## Code Example\\n```javascript\\n/var/log/myapp.log {\\n  size 100M\\n  rotate 7\\n  compress\\n  delaycompress\\n  missingok\\n  notifempty\\n  create 0644 root root\\n  sharedscripts\\n  postrotate\\n    systemctl is-active myapp >/dev/null && systemctl kill -s HUP myapp || kill -HUP `cat /var/run/myapp.pid` 2>/dev/null || true\\n  endscript\\n}\\n```\\n\\n## Follow-up Questions\\n- What if the app cannot reopen log files on HUP?\\n- How would you test concurrent writers during rotation?","diagram":null,"difficulty":"intermediate","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Discord","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T14:35:24.296Z","createdAt":"2026-01-12T14:35:24.296Z"},{"id":"q-264","question":"How do Unix pipes enable inter-process communication and what are their performance implications?","answer":"Pipes provide unidirectional byte streams between processes, utilizing kernel buffers for efficient inter-process communication with blocking I/O semantics.","explanation":"## Why Asked\nTests understanding of IPC mechanisms and system design principles for scalable applications.\n\n## Key Concepts\nUnidirectional communication, kernel buffering, blocking I/O, file descriptor abstraction, pipe capacity limits.\n\n## Code Example\n```bash\n# Create pipe and connect processes\nls -l | grep \".txt\" | wc -l\n# Kernel manages 64KB buffer between processes\n```\n\n## Follow-up Questions\nWhat's the difference between named and anonymous pipes? How do pipes handle backpressure? What are alternatives to pipes?","diagram":"flowchart TD\n  A[Process A] -->|writes| B[Pipe Buffer]\n  B -->|reads| C[Process B]\n  D[Kernel] -->|manages| B","difficulty":"beginner","tags":["posix","signals","pipes","sockets"],"channel":"unix","subChannel":"system-programming","sourceUrl":"https://man7.org/linux/man-pages/pipe.2","videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Apple","Google","Meta","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-03T06:38:32.248Z","createdAt":"2025-12-26 12:51:07"}],"subChannels":["general","system-programming"],"companies":["Adobe","Airbnb","Amazon","Anthropic","Apple","Bloomberg","Citadel","Cloudflare","Coinbase","Databricks","Discord","DoorDash","Goldman Sachs","Google","Hashicorp","Hugging Face","IBM","Instacart","LinkedIn","Lyft","Meta","Microsoft","MongoDB","NVIDIA","Netflix","OpenAI","Oracle","PayPal","Plaid","Robinhood","Salesforce","Scale Ai","Slack","Snap","Snowflake","Square","Stripe","Tesla","Twitter","Two Sigma","Uber","Zoom"],"stats":{"total":53,"beginner":24,"intermediate":13,"advanced":16,"newThisWeek":38}}