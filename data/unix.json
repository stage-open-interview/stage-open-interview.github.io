{"questions":[{"id":"q-1002","question":"In a Unix environment logs are stored in /var/log/app/*.log with lines formatted as timestamp|user|action|resource. Write a practical one-liner using standard UNIX tools to output the top 5 users by total actions in the last 24 hours. Explain how you would handle log rotation and malformed lines?","answer":"One practical approach is to filter the last 24h files, extract the user, and count actions. Example: find /var/log/app/*.log -type f -mtime -1 -print0 | xargs -0 cat | awk -F'|' 'NF>=4{cnt[$2]++} END","explanation":"## Why This Is Asked\nTests practical log-scan skills: building a robust one-liner that handles log rotation, malformed lines, and spaces in fields. It also gauges comfort with standard tools and edge-case thinking.\n\n## Key Concepts\n- Log rotation: using -mtime to limit scope to last day.\n- Field delimiting: using a stable delimiter (|) for reliable parsing.\n- Robust counting: associative arrays in awk for tallying per user.\n- Safe I/O: null-delimited input with -print0/xargs -0 to support spaces.\n\n## Code Example\n```javascript\nfind /var/log/app/*.log -type f -mtime -1 -print0 | xargs -0 cat | awk -F'|' 'NF>=4{cnt[$2]++} END{for(u in cnt) print cnt[u], u}' | sort -nr | head -5\n```\n\n## Follow-up Questions\n- How would you handle logs where the user field is sometimes missing or empty?\n- How would you validate results across multiple days with varying time zones?\n- How would you adapt this for extremely large log pools to minimize I/O impact?","diagram":"flowchart TD\n  Start --> ReadLogs[Read log files in /var/log/app]\n  ReadLogs --> Filter[Filter last 24 hours with -mtime]\n  Filter --> Extract[Extract user field (split on |)]\n  Extract --> Count[Count per user (awk)]\n  Count --> Sort[Sort counts descending]\n  Sort --> Top5[Output top 5 users]\n  Top5 --> End[Done]","difficulty":"beginner","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T18:47:17.203Z","createdAt":"2026-01-12T18:47:17.203Z"},{"id":"q-1032","question":"How would you capture stdout and stderr of a simple shell command into separate log files while still displaying live output in the terminal? Provide a concrete Bash command and brief justification?","answer":"Use Bash process substitution to split streams: command > >(tee -a stdout.log) 2> >(tee -a stderr.log >&2). This sends stdout to both terminal and stdout.log, and stderr to both terminal (via 2>&) and","explanation":"## Why This Is Asked\n\nAssess understanding of Unix IO redirection, common tooling (tee), and Bash-specific features (process substitution) that enable real-time logging without losing output.\n\n## Key Concepts\n\n- stdout/stderr redirection\n- process substitution\n- tee for file logging\n\n## Code Example\n\n```bash\ncommand > >(tee -a stdout.log) 2> >(tee -a stderr.log >&2)\n```\n\n## Follow-up Questions\n\n- How would you implement a portable alternative for POSIX sh?\n- What if the command writes to stdout after its own buffering ends; how could you ensure timely logs?","diagram":null,"difficulty":"beginner","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","LinkedIn","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T20:22:14.784Z","createdAt":"2026-01-12T20:22:14.785Z"},{"id":"q-1069","question":"In a Unix environment, logs live under /var/log/metrics/*.log and are hourly rotated to .log and .log.gz. Each line is like [YYYY-MM-DD HH:MM:SS] LEVEL: message. Propose a robust, portable approach (one-liner preferred) to output the number of ERROR events per hour for the last 6 hours, handling missing files and rotations without external dependencies?","answer":"I’d implement a robust, portable pipeline reading both compressed and plain logs, extracting the hour from each timestamp, filtering for ERROR, and aggregating counts for the last 6 hours. Use globbin","explanation":"## Why This Is Asked\nTests practical mastery of Unix tooling, log rotation handling, and resilient scripting under real-world constraints.\n\n## Key Concepts\n- Robust log ingestion across rotated files\n- Mixed gzip/uncompressed handling with zcat\n- Time bucketing and boundary handling\n- Idempotent, side-effect-free pipelines\n\n## Code Example\n```javascript\n// Bash sketch\nstart=$(date -u -d '-6 hours' '+%Y-%m-%d %H')\nzcat /var/log/metrics/metrics-*.log.gz /var/log/metrics/metrics-*.log 2>/dev/null \\\n| awk -v cutoff=\"$start\" '/ERROR/ { if ($0 ~ /\\[/) { hour=substr($1,1,10)\" \"substr($2,1,2); if (hour>=cutoff) c[hour]++ } } END{for (h in c) print h\":00\", c[h]}'\n```\n\n## Follow-up Questions\n- How would you test this under log rotation? \n- How would you adapt for multiple timezones?","diagram":"flowchart TD\n  A[Start] --> B[Collect log files (gz and plain)]\n  B --> C[Parse timestamps to hour bucket]\n  C --> D[Filter ERROR events]\n  D --> E[Aggregate last 6 hours]\n  E --> F[Output results]","difficulty":"advanced","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Meta","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T21:30:28.843Z","createdAt":"2026-01-12T21:30:28.843Z"},{"id":"q-1123","question":"In a Unix environment, multiple services write JSON logs under /var/log/diag/*.log and rotated hourly to *.log.gz. Each line is a JSON object like {\"ts\":\"YYYY-MM-DDTHH:MM:SSZ\",\"svc\":\"name\",\"lvl\":\"ERROR\",\"msg\":\"...\"}; Propose a robust one-liner (no external dependencies beyond standard UNIX tools) to output the number of ERROR events per hour for the last 4 hours, aggregated across all services, and tolerant of missing files and rotated archives?","answer":"I’d implement a robust portable one-liner that streams both .log and .log.gz files, extracts ts and lvl from each JSON line, converts ts to epoch, filters for the last 4 hours, buckets per hour (YYYY-","explanation":"## Why This Is Asked\n\nTests ability to write compact, production-grade log queries that tolerate rotations, mixed compression, and JSON-like lines using only core Unix tools.\n\n## Key Concepts\n\n- Robust file discovery with find and null-termination\n- Decompression with zcat for .gz files\n- Inline JSON-like parsing with simple regex in awk\n- Time-window filtering via epoch comparison using date\n- Hour bucketing and aggregation with associative arrays\n\n## Code Example\n\n```javascript\n#!/bin/sh\ncutoff=$(date -u -d '4 hours ago' +%s)\nfind /var/log/diag -type f \\( -name '*.log' -o -name '*.log.gz' \\) -print0 | \\\n  xargs -0 -I{} sh -c ' [ -f \"$1\" ] && { if [ \"${1##*.}\" = \"gz\" ]; then zcat \"$1\"; else cat \"$1\"; fi; }' _ {} | \\\nawk -v cut=\"$cutoff\" ' /\"lvl\":\"ERROR\"/ { if (match($0, /\"ts\":\"([^\\\\\"]+)\"/, a)) { cmd=\"date -u -d \\\"\" a[1] \"\\\" +%s\"; cmd | getline t; close(cmd); if (t >= cut) { hour = substr(a[1],1,13) \"Z\"; count[hour]++ } } } END { for (h in count) print h, count[h] } '\n```\n\n## Follow-up Questions\n\n- How would you adjust for millisecond timestamps?\n- How would this scale across many servers and years of logs?","diagram":"flowchart TD\n  A[Start] --> B[Find logs (*.log, *.log.gz)]\n  B --> C[Decompress and read lines]\n  C --> D[Parse JSON ts and lvl]\n  D --> E[Filter last 4 hours]\n  E --> F[Bucket by hour]\n  F --> G[Count ERRORs]\n  G --> H[Output]","difficulty":"intermediate","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","OpenAI","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T23:30:41.563Z","createdAt":"2026-01-12T23:30:41.563Z"},{"id":"q-1139","question":"Scenario: JSON logs at /var/log/diag/*.log, rotated hourly to *.log.gz. Each line: {\"ts\":\"YYYY-MM-DDTHH:MM:SSZ\",\"svc\":\"name\",\"lvl\":\"ERROR\",\"msg\":\"...\"}. Propose a robust one-liner (no external deps beyond standard UNIX tools) that outputs the per-hour 99th percentile of message length for the last 4 hours, across all services, deduplicating identical messages per hour, and tolerant of missing files and gz archives?","answer":"Use a two-pass approach: 1) gzip -cd /var/log/diag/*.log.gz 2>/dev/null | awk to emit hour bucket and msg length, deduplicating by hour+msg with an associative array; filter to last 4 hours by ts; 2) ","explanation":"## Why This Is Asked\nTests real-world log ingestion under rotation; requires careful handling of gz files, missing files, and cross-service aggregation. The candidate should justify a robust percentile computation and dedup logic.\n\n## Key Concepts\n- Standard UNIX toolchain for parsing JSON-like lines without jq\n- Handling gzip-rotated logs and missing files\n- Per-hour bucketing and deduplication by message\n- Computation of a percentile from a 1-D length distribution\n\n## Code Example\n```bash\n# skeleton command outline (not complete here)\n```\n\n## Follow-up Questions\n- How would you test with synthetic logs at scale?\n- How would you adapt for different rotations or non-UTC timestamps?","diagram":"flowchart TD\n  A[Read logs (gz and plain)] --> B[Parse ts and msg length]\n  B --> C[Bucket by hour]\n  C --> D[Deduplicate by hour+msg]\n  D --> E[Compute 99th percentile per hour]\n  E --> F[Output results]","difficulty":"advanced","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["OpenAI","Plaid","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T01:26:59.322Z","createdAt":"2026-01-13T01:26:59.322Z"},{"id":"q-1224","question":"Scenario: In a Unix cluster, logs are emitted as JSON lines under /var/log/cluster/*/*.log and rotated hourly to *.log.gz. Each line contains {\"ts\":\"YYYY-MM-DDTHH:MM:SSZ\",\"tenant\":\"tenant-id\",\"svc\":\"service\",\"lvl\":\"LEVEL\",\"msg\":\"...\"}; Propose a robust one-liner (no external deps beyond standard UNIX tools) to identify the top 3 tenants by error rate (ERROR / total events) for the last 6 hours, aggregated across all services, tolerant of missing files and gz archives?","answer":"Stream all logs (including .log.gz), extract ts and tenant via awk regex, filter lines within the last 6 hours using START/END timestamps from date -u, accumulate per-tenant total and ERROR counts, th","explanation":"## Why This Is Asked\n- Assesses ability to build robust, single-line data pipelines over multi-file logs, including compressed files, without external dependencies. \n- Tests time-window handling, cross-service aggregation, and per-tenant bucketing under rotation. \n\n## Key Concepts\n- Stream processing with standard UNIX tools (find, xargs, zcat, awk). \n- JSON field extraction via regex in awk; resilient to line variations. \n- Time-window filtering using START/END derived from date; cross-file aggregation. \n- Sorting by computed metric (error rate) without extra tools.\n\n## Code Example\n```javascript\nSTART=$(date -u -d \"-6 hours\" +\"%Y-%m-%dT%H\")\nEND=$(date -u +\"%Y-%m-%dT%H\")\nfind /var/log/cluster -type f \\( -name '*.log' -o -name '*.log.gz' \\) -print0 | \\\n  xargs -0 -I{} sh -lc 'case \"{}\" in *.gz) zcat \"{}\" ;; *) cat \"{}\" ;; esac' | \\\n  awk -v s=\"$START\" -v e=\"$END\" '{\n    if (match($0, /\"ts\":\"([^\"]+)\"/, a)) ts=a[1]; hour=substr(ts,1,13); gsub(/T/,\" \",hour); hour_key=hour;\n    if (match($0, /\"tenant\":\"([^\"]+)\"/, b)) tenant=b[1];\n    if (match($0, /\"lvl\":\"([^\"]+)\"/, c)) lvl=c[1];\n    if (hour_key && tenant && hour_key>=s && hour_key<=e) {\n      total[tenant] += 1; if (lvl==\"ERROR\") errors[tenant] += 1;\n    }\n  }\n  END { for (t in total) { rate = total[t] ? errors[t] / total[t] : 0; printf \"%s\\t%.6f\\n\", t, rate } }\n``` \n\n## Follow-up Questions\n- How would you adapt this to include per-hour breakdowns in addition to the overall top 3? \n- How would you handle non-UTC timestamps or malformed JSON lines? ","diagram":null,"difficulty":"advanced","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Coinbase","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T05:34:59.109Z","createdAt":"2026-01-13T05:34:59.109Z"},{"id":"q-1241","question":"Scenario: In a multi-user Unix workspace, /home contains subdirectories per user with various files. Some are large, some are temporary. Provide a robust one-liner (no external dependencies) that lists the five largest regular files under /home (recursively), excluding hidden files and symlinks, printing each as 'size<TAB>path' with sizes in human-readable form. Explain how you ensure safety against spaces and newlines in filenames?","answer":"One robust option is a single pipeline that uses find with a safe delimiter, sorts by size, and formats to human units. For example: find /home -type f -not -path '*/.*' -print0 | xargs -0 stat -c '%s","explanation":"Why this is asked: tests understanding of safe file discovery, handling of spaces, and large-file detection. Key points: exclude hidden paths, only regular files, robust delimiting with null separators, and portable human-readable formatting via an awk function that scales from B up to GB. Edge considerations include spaces in names and potential binary data in paths.","diagram":"flowchart TD\n  A[Find largest files] --> B[Filter /home recursively]\n  B --> C[Exclude hidden/symlinks]\n  C --> D[Emit size+path]\n  D --> E[Sort desc & take top 5]\n  E --> F[Format human units]","difficulty":"beginner","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Lyft","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T06:36:11.300Z","createdAt":"2026-01-13T06:36:11.300Z"},{"id":"q-1492","question":"Scenario: multiple services log JSON lines to /var/log/app/*.log, rotated hourly to *.log.gz. Each line looks like {\"ts\":\"YYYY-MM-DDTHH:MM:SSZ\",\"svc\":\"name\",\"lat_ms\":123}. Propose a robust one-liner (no external deps beyond standard UNIX tools) to compute the per-service average lat_ms in the last 2 hours, across all logs, tolerant of missing and gzipped files, ignoring malformed lines. Output: 'service avg_ms'?","answer":"cutoff=$(date -u -d '-2 hours' '+%Y-%m-%dT%H:%M:%SZ'); find /var/log/app -type f \\( -name '*.log' -o -name '*.log.gz' \\) -print0 | while IFS= read -r -d '' f; do if [[ \"$f\" == *.gz ]]; then zcat \"$f\";","explanation":"Why This Is Asked: tests robust log ingestion across gzipped rotations. Key Concepts: shell pipelines, handling gzip, JSON-like parsing with regex in awk, per-service aggregation, time-window filtering via ISO timestamps. Code Example: the command reads both .log and .log.gz, extracts ts/svc/lat_ms, filters to last 2 hours, accumulates sums and counts per service, then prints averages. Follow-ups explore error handling and performance tweaks.","diagram":null,"difficulty":"intermediate","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["PayPal","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T19:28:41.147Z","createdAt":"2026-01-13T19:28:41.149Z"},{"id":"q-1809","question":"Scenario: A Linux host logs auth events into /var/log/auth/{service}/*.log and rotates hourly to *.log.gz. Each line is JSON like {\"ts\":\"YYYY-MM-DDTHH:MM:SSZ\",\"user\":\"alice\",\"action\":\"LOGIN\",\"result\":\"FAIL\"}. Propose a robust one-liner (no external deps beyond standard UNIX tools) to print the top 5 users by failed login rate in the last 2 hours, aggregated across all services, tolerant of missing files and gz archives, and resilient to malformed lines?","answer":"Stream both gz and plain logs via zcat, filter for status FAIL, extract ts and user with awk, convert ts to epoch and compare to 2-hour window, accumulate per-user totals, then emit in descending orde","explanation":"## Why This Is Asked\nThis tests real-world log aggregation with mixed rotation and JSON parsing using canonical UNIX tools.\n\n## Key Concepts\n- zcat + globbing for .log.gz and .log\n- awk-based JSON field extraction without external deps\n- date for UTC epoch bucketing; last N hours window\n- robust handling of missing/malformed lines\n\n## Code Example\n```javascript\n// Pseudocode sketch of the approach\nconst cutoff = Date.now() - 2*3600*1000\nfor each line in streams:\n  parse json\n  if (record.status == 'FAIL' && new Date(record.ts) >= cutoff) {\n    counts[record.user]++\n  }\n}\nprint top 5 by counts\n```\n\n## Follow-up Questions\n- How would you adapt this to count per-minute windows?\n- How would you test with synthetic rotated logs?","diagram":"flowchart TD\n  A[Parse logs] --> B[Extract fields]\n  B --> C[Apply time window]\n  C --> D[Aggregate per user]\n  D --> E[Sort & select top5]","difficulty":"intermediate","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Robinhood","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T11:44:32.982Z","createdAt":"2026-01-14T11:44:32.982Z"},{"id":"q-1903","question":"Propose a robust one-liner to compute the number of ERROR lines per hour for the previous 12 hours, aggregating across all logs in /var/log/app and across both uncompressed (*.log) and rotated/compressed files (*.log.*, *.log.gz). The one-liner must tolerate missing files and gz archives and rely only on standard UNIX tools?","answer":"One-liner approach: stream all logs (/*.log, *.log.*, *.log.gz) and pipe to awk. Parse timestamps [YYYY-MM-DD HH:MM:SS], convert to epoch with mktime, bucket by hour with strftime, and count ERROR lin","explanation":"## Why This Is Asked\n\nAssesses ability to craft a robust log-analysis one-liner that handles mixed compression formats, time-window filtering, and cross-file aggregation using only standard UNIX tools.\n\n## Key Concepts\n\n- Shell file globbing and safe looping over missing files\n- Decompression with zcat and streaming with cat\n- GNU awk time parsing (mktime, systime, strftime)\n- Time-window filtering and hour bucketing\n- Sorting deterministic output\n\n## Code Example\n\n```javascript\n// Conceptual core approach in a single pipeline\nfor f in /var/log/app/*.log /var/log/app/*.log.* /var/log/app/*.log.gz; do\n  case \"$f\" in\n    *.gz) zcat \"$f\" ;; *) cat \"$f\" ;; esac;\ndone | awk 'BEGIN{now=systime()} /\\[[0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2}\\]/ { if (match($0, /\\[([0-9]{4})-([0-9]{2})-([0-9]{2}) ([0-9]{2}):([0-9]{2}):([0-9]{2})\\]/, d)) { ts = mktime(d[1]\" \"d[2]\" \"d[3]\" \"d[4]\" \"d[5]\" \"d[6]); if (ts>=now-12*3600 && /ERROR/) { h=strftime(\"%Y-%m-%d %H:00\", ts); c[h]++ } } } END{ for (k in c) print k, c[k] }' | sort\n```\n\n## Follow-up Questions\n\n- How would you adapt this to handle a different timestamp format or time zone?\n- How would you modify to also count WARN and INFO separately while preserving performance?","diagram":null,"difficulty":"beginner","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Slack","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T16:49:53.401Z","createdAt":"2026-01-14T16:49:53.401Z"},{"id":"q-1975","question":"In a Unix host, logs live in /logs/web/*.log and are rotated hourly to *.log.gz. Each line is a JSON object like {\"ts\":\"YYYY-MM-DDTHH:MM:SSZ\",\"ip\":\"1.2.3.4\",\"method\":\"GET\",\"path\":\"/api\"}. Propose a robust one-liner (no external deps beyond standard UNIX tools) to output the top 5 IPs by request count in the last 4 hours, aggregating across all files and rotations, tolerant of missing files and compressed archives?","answer":"Stream all logs from /logs/web/*.log and *.log.gz using a single, portable pipeline, extract ip and ts from each JSON line, filter lines within the last 4 hours by converting ts to epoch, then count p","explanation":"## Why This Is Asked\nAssesses ability to reason about log rotation, gzipped files, and stream processing with minimal tools.\n\n## Key Concepts\n- Globbing and rotated logs\n- Stream processing with zcat and cat\n- Text extraction with sed/awk\n- Data aggregation with awk\n\n## Code Example\n```bash\n# Candidate approach (illustrative)\nzcat /logs/web/*.log.gz /logs/web/*.log 2>/dev/null | \\\nawk -v cutoff=\"$(date -u -d '4 hours ago' +%s)\" 'BEGIN{RS=\"\"; FS=\"\"} /\"ts\":\"/ { if(match($0, /\"ts\":\"([^\"]+)\"/, m)) {\n  cmd=\"date -u -d \\\"\" m[1] \"\\\" +%s\"; cmd | getline t; close(cmd);\n  if(t>=cutoff && match($0, /\"ip\":\"([0-9.]+)\"/, ip)) { c[ip[1]]++ }\n}} END{ for(i in c) print c[i]\" \"\"i }' | \\\nsort -nr | head -n5\n```\n\n## Follow-up Questions\n- How would you adapt for non-GNU awk?\n- What changes if timestamps are in a different timezone?","diagram":null,"difficulty":"beginner","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Snap","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T19:30:05.919Z","createdAt":"2026-01-14T19:30:05.919Z"},{"id":"q-2094","question":"Scenario: In a Unix CI environment, logs are written by build agents under /build/logs/*/*.log and rotated hourly to *.log.gz. Each line is BUILDID|TIMESTAMP|STEP|STATUS|MESSAGE. Propose a robust one-liner (no external deps beyond standard UNIX tools) to report, for the last 24 hours, per BUILDID and per hour, the count of failed steps (STATUS != 'SUCCESS'), aggregating across all agents and rotations and tolerating missing files and compressed archives?","answer":"Approach: Stream all logs (.log and .log.gz), decompress as needed, then use awk to parse, convert TIMESTAMP to epoch with date -d, keep only entries from the last 24h, and increment counts[BUILDID][hour] where STATUS != 'SUCCESS'.","explanation":"## Why This Is Asked\nTests practical CLI scripting and edge-case handling in production-like log workflows. Candidates justify a compact, robust solution that tolerates missing files and compressed archives.\n\n## Key Concepts\n- POSIX vs GNU utilities compatibility\n- Efficient streaming of mixed plain/compressed logs\n- Time-window calculations and per-key aggregation\n\n## Code Example\n```bash\n# Example: outline of the one-liner approach (exact command may vary by shell)\nnow=$(date +%s); for f in /build/logs/*/*.{log,log.gz}; do [ -f \"$f\" ] && case \"$f\" in *.gz) gzip -dc \"$f\" ;; *) cat \"$f\" ;; esac; done | awk -F'|' -v now=\"$now\" 'function epoch(ts) { cmd=\"date -d \\\"\" ts \"\\\" +%s\"; cmd | getline e; close(cmd); return e } { ts=epoch($2); if(now-ts <= 86400 && $4 != \"SUCCESS\") { build=$1; hour=sprintf(\"%02d\", strftime(\"%H\", ts)); counts[build][hour]++ } } END { for(b in counts) for(h in counts[b]) print b\"|\"h\"|\"counts[b][h] }'\n```\n\n## Edge Cases Handled\n- Missing files gracefully skipped\n- Mixed compressed/uncompressed logs\n- Timestamp parsing across formats\n- Memory-efficient streaming","diagram":null,"difficulty":"beginner","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","IBM"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T05:02:34.376Z","createdAt":"2026-01-14T23:45:04.503Z"},{"id":"q-2253","question":"Scenario: /var/run contains *.pid per daemon (service name == filename). Some daemons crash leaving stale PIDs. Propose a robust one-liner (no external deps) that lists service:pid pairs for all PID files whose PID is not running, tolerant of missing files and spaces in names?","answer":"for f in /var/run/*.pid; do [ -f \\\"$f\\\" ] || continue; pid=$(cat \\\"$f\\\" 2>/dev/null); [ -n \\\"$pid\\\" ] || continue; if [ ! -e \\\"/proc/$pid\\\" ]; then echo \\\"$(basename \\\"$f\\\" .pid):$pid\\\"; fi; done","explanation":"Why This Is Asked: Tests ability to detect stale daemon state using native tools, robust against missing pid files and whitespace in names. Key Concepts: PID file semantics, /proc existence checks, shell safety with globbing, handling spaces in filenames. Follow-up: adapt for systemd pidfiles and atomic cleanup.","diagram":null,"difficulty":"intermediate","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T09:39:00.686Z","createdAt":"2026-01-15T09:39:00.686Z"},{"id":"q-2293","question":"In a Unix environment, logs live under /var/log/app-logs/*.log and rotations *.log.gz. Each line is a JSON object like {\"ts\":\"YYYY-MM-DDTHH:MM:SSZ\",\"svc\":\"name\",\"dur_ms\":1234,\"ev\":\"RUN\"}. Propose a robust one-liner (no external deps beyond standard UNIX tools) to compute, per service, the mean and 95th percentile of dur_ms for the last 2 hours, aggregating across all files and rotations, tolerating missing files and archives?","answer":"Use a single Bash pipeline that reads both *.log and *.log.gz, extracts ts, svc, and dur_ms, converts ts to epoch with TZ=UTC, filters to now-2h, then aggregates per service in awk. Store durations pe","explanation":"## Why This Is Asked\nTests practical UNIX log analytics with JSON lines, mixed rotations, and no external deps.\n\n## Key Concepts\n- Handling mixed log formats and gzipping\n- Time-window filtering with epoch comparisons\n- Per-service aggregation and percentile computation (GNU awk)\n\n## Code Example\n```\n# Pseudocode: read logs (log/*.log and *.log.gz), extract ts,srv,dur_ms, filter 2h window, per-service aggregate to compute mean and 95th percentile\n```\n\n## Follow-up Questions\n- How would you adapt this to compute per-minute percentiles?\n- How would you validate correctness with synthetic logs?","diagram":"flowchart TD\nA[Log Files] --> B{Uncompressed vs.gz}\nB --> C[Parse JSON fields ts, svc, dur_ms]\nC --> D{Time-window check (2h)}\nD --> E[Aggregate by svc]\nE --> F[Compute mean and 95th percentile]\nF --> G[Emit svc, mean, p95]","difficulty":"intermediate","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","IBM","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T10:54:51.056Z","createdAt":"2026-01-15T10:54:51.056Z"},{"id":"q-2396","question":"In a Unix host, multiple services write JSON log lines to /logs/app/*/*.log, rotated hourly to *.log.gz. Each line is {\"ts\":\"YYYY-MM-DDTHH:MM:SSZ\",\"svc\":\"name\",\"lvl\":\"LEVEL\",\"msg\":\"...\"}. Propose a robust one-liner (no external deps beyond standard UNIX tools) that outputs a per-minute histogram of WARN or ERROR events for the last 60 minutes, aggregated across all files and rotations, tolerant of missing files and gz archives, and resilient to out-of-order timestamps within the minute bucket?","answer":"Loop over /logs/app/*/*.log, decompress gz with zcat when needed, stream lines to awk. In awk, extract ts via regex, convert to minute bucket with date -u +%Y-%m-%dT%H:%M -d \"$ts\"; skip invalid; if lv","explanation":"## Why This Is Asked\nTests the ability to design robust log aggregation across rotation boundaries, missing files, and clock quirks in a real-world Unix setup.\n\n## Key Concepts\n- Globbing and rotated logs across .log and .log.gz\n- On-the-fly decompression of gz files with standard tools\n- Parsing JSON-like lines with basic UNIX utilities\n- Per-minute bucketing and aggregation with streaming tools\n- Robust handling of invalid lines and out-of-order timestamps\n\n## Code Example\n```bash\nfor f in /logs/app/*/*.log; do \n  [ -f \"$f\" ] && case \"$f\" in \n    *.gz) zcat \"$f\" ;; \n    *)   cat \"$f\" ;; \n  esac; \ndone | awk -F'\"' '/\"ts\":/ { if (match($0, /\"ts\":\"([^\"]+)\"/, m)) { t=m[1]; if (t ~ /^[0-9]{4}-[0-9]{2}-[0-9]{2}T[0-9]{2}:[0-9]{2}:[0-9]{2}Z$/) { bucket = strftime(\"%Y-%m-%dT%H:%M\", mktime(substr(t,1,4)\" \"substr(t,6,2)\" \"substr(t,9,2)\" \"substr(t,12,2)\" \"substr(t,15,2)\" \"substr(t,18,2)))); if ($0 ~ /\"lvl\":\"(WARN|ERROR)\"/) cnt[bucket]++ } } } END { for (b in cnt) print b, cnt[b] }' | sort\n``` \n\n## Follow-up Questions\n- How would you extend to 24 hours and a fixed windowing strategy? \n- How would timezone and clock skew be accounted for in production dashboards?","diagram":null,"difficulty":"intermediate","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Databricks","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T16:45:47.287Z","createdAt":"2026-01-15T16:45:47.287Z"},{"id":"q-2420","question":"Scenario: In a Unix host, logs live under /var/log/app-logs with daily rotation and filenames like app-<service>-YYYYMMDD.log. Write a robust one-liner (no external deps beyond standard UNIX tools) that prints a total count of lines containing the word 'ERROR' across all log files modified in the last 3 days. The command must gracefully skip missing/unreadable files and work across many services?","answer":"find /var/log/app-logs -type f -name 'app-*-*.log' -mtime -3 -print0 2>/dev/null | xargs -0 cat 2>/dev/null | awk '/ERROR/ {c++} END {print c+0}'","explanation":"## Why This Is Asked\nTests practical ability to craft a robust, portable one-liner using only standard UNIX tools for cross-service log aggregation. It handles file rotation by using mtime, streams via cat, and counts with awk. It tolerates missing/unreadable files and scales with many files.\n\n## Key Concepts\n- find with -mtime, -type, -name, -print0\n- xargs -0 for safe multi-file input\n- cat to concatenate streams\n- awk for counting matches across input\n- redirection to ignore errors for robustness\n\n## Code Example\n```bash\nfind /var/log/app-logs -type f -name 'app-*-*.log' -mtime -3 -print0 2>/dev/null | xargs -0 cat 2>/dev/null | awk '/ERROR/ {c++} END {print c+0}'\n```\n\n## Follow-up Questions\n- How would you get per-service breakdown?\n- How would you extend to include gzipped logs (gz) without external deps?","diagram":"flowchart TD\n  A[Start] --> B[Find last 3 days logs]\n  B --> C[Stream with cat]\n  C --> D[Count with awk]\n  D --> E[Output total]","difficulty":"beginner","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","MongoDB","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T17:42:36.538Z","createdAt":"2026-01-15T17:42:36.539Z"},{"id":"q-2520","question":"In a fleet of Linux hosts, log files sit under /var/log/relay/*/*.log and rotated to *.log.gz. Each line is a JSON object with fields ts (YYYY-MM-DDTHH:MM:SSZ), svc, lat_ms. Write a robust one-liner (no external deps beyond standard UNIX tools) that outputs each service's 95th percentile latency over the last 15 minutes, aggregating across all files and rotations, tolerant of missing/unreadable files and out-of-order lines?","answer":"Stream all logs through a single awk script that reads plain and gzipped files, parses ts with date -d, keeps only lines within the last 15 minutes, collects lat_ms by svc, and on END sorts and emits ","explanation":"## Why This Is Asked\n\nThis question probes the ability to perform robust, cross-file, cross-rotation log analysis using only standard UNIX tools. It requires handling both plain and gzipped inputs, time-window filtering, and percentile aggregation per service, all while tolerating malformed data.\n\n## Key Concepts\n\n- Streaming log processing across rotated files\n- Reading gzipped and plain logs without external deps\n- Time-window filtering with date parsing\n- Percentile computation in awk\n\n## Code Example\n\n```javascript\n// illustrative skeleton: the actual one-liner would embed in a script or be a longer one-liner\nconst sample = 'Parse and aggregate lat_ms per svc within 15m window';\n```\n\n## Follow-up Questions\n\n- How would you extend to multiple percentile targets (e.g., 95th and 99th)?\n- How would you test correctness with synthetic logs and randomized rotations?","diagram":null,"difficulty":"advanced","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Lyft","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T21:31:22.635Z","createdAt":"2026-01-15T21:31:22.636Z"},{"id":"q-2624","question":"In a Linux host running dozens of worker processes inside containers, build a POSIX-friendly watchdog using only standard UNIX tools to detect any worker that has spent uninterruptible sleep (D state) for more than 60 seconds within a 10-minute window and restart such workers with exponential backoff. Use /proc to derive each worker's startup command from /proc/$pid/cmdline and /proc/cgroups to avoid system processes?","answer":"Use a time-window watchdog: every 2s for 10m, scan /proc to map PID to State and accumulate seconds in D state. After the window, restart offenders with backoff (5s, 15s, 30s) using the process cmdlin","explanation":"## Why This Is Asked\nThis tests OS internals, container boundaries, and tool discipline.\n\n## Key Concepts\n- /proc parsing with POSIX tools\n- D-state dwell time accumulation\n- Windowed aggregation over time\n- Restart with backoff and startup command reconstruction\n- Excluding system processes via /proc/[pid]/cgroup\n\n## Code Example\n\n```bash\n# Outline of approach (not full implementation)\n```\n\n## Follow-up Questions\n- How would you unit-test this watcher in a multi-tenant cluster?\n- How would you handle PIDs recycled within the window?\n","diagram":null,"difficulty":"advanced","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T04:08:56.983Z","createdAt":"2026-01-16T04:08:56.984Z"},{"id":"q-2738","question":"In a Unix host, per-job logs are under /srv/ci/jobs/<job_id>/logs, rotated daily with filenames log-YYYYMMDD.log. A symlink log-current.log points to the current day’s log file for each job. Write a robust one-liner (no external deps beyond standard UNIX tools) that prints the count of lines containing 'STATUS=FAILED' across all current-day log files for all jobs, gracefully handling missing files and broken links?","answer":"One robust approach is: find -L /srv/ci/jobs -name log-current.log -type f -print0 | xargs -0 -r grep -h 'STATUS=FAILED' | wc -l. This follows symlinks, ignores missing files, and counts only lines wi","explanation":"The candidate should propose a pipeline that safely traverses job dirs, follows symlinks where needed, and uses null-separated streams to handle spaces in paths. They should justify using -L with find to follow log-current.log, and using -print0/xargs -0 to prevent tokenization issues, culminating in wc -l for a total.","diagram":null,"difficulty":"beginner","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T09:47:40.582Z","createdAt":"2026-01-16T09:47:40.582Z"},{"id":"q-2781","question":"In a Unix host, logs live under /var/log/services/*/*.log and are rotated hourly; some files are .log, others .log.gz. Each line is of the form [YYYY-MM-DD HH:MM:SS] <service>: <message>. Write a robust one-liner (no external deps beyond standard UNIX tools) that prints a per-service histogram of the number of lines containing the word ERROR in the last 24 hours, aggregating across all files and rotations. Must handle missing or unreadable files and gzipped archives, and work with spaces in paths?","answer":"One viable solution: find /var/log/services -type f -mtime -1 -print0 | while IFS= read -r -d '' f; do case \"$f\" in *.gz) zcat \"$f\" ;; *) cat \"$f\" ;; esac; done | awk '/ERROR/ && match($0, /^\\[[0-9]{4","explanation":"Why this is asked: tests ability to craft robust shell one-liners that span plain and compressed logs, handle spaces, and aggregate by service. Key concepts: find -mtime, print0, safe looping, zcat for gz, awk for parsing and aggregation. Code Example: (see block). Follow-up: how would you adapt to count ERROR codes with 5xx prefixes, or test locally with mock logs?","diagram":null,"difficulty":"beginner","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Robinhood","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T11:39:00.928Z","createdAt":"2026-01-16T11:39:00.928Z"},{"id":"q-3086","question":"In a Unix host, a policy requires that any file within data/ under /shared/projects must be group-writable and have the group set to the project's primary group, but only for files changed in the last 24 hours. Write a robust one-liner (no external deps beyond standard UNIX tools) to identify files that violate this policy, ignoring unreadable files and handling spaces in names?","answer":"find /shared/projects -type f -path '*/data/*' -mtime -1 ! -perm -g+w -print0 2>/dev/null | while IFS= read -r -d '' f; do printf 'VIOLATION: %s\n' \"$f\"; stat -c '%A %G %n' \"$f\"; done","explanation":"## Why This Is Asked\n\nTests ability to enforce security policies using standard Unix tools, including robust file discovery, path handling, and error management.\n\n## Key Concepts\n\n- `find` with `-mtime` for filtering by modification time\n- `-path` for directory pattern matching\n- `-perm` to check group write permissions\n- `-print0` and `read -d ''` for handling filenames with spaces\n- `stat` for displaying file permissions and group information\n- Error redirection to ignore unreadable directories\n\n## Code Example\n\n```bash\nfind /shared/projects -type f -path '*/data/*' -mtime -1 ! -perm -g+w -print0 2>/dev/null | while IFS= read -r -d '' f; do printf 'VIOLATION: %s\\n' \"$f\"; stat -c '%A %G %n' \"$f\"; done\n```\n\n## How It Works\n\n1. `find /shared/projects` searches the target directory recursively\n2. `-type f` restricts to regular files only\n3. `-path '*/data/*'` matches files within any data subdirectory\n4. `-mtime -1` finds files modified in the last 24 hours\n5. `! -perm -g+w` identifies files without group write permissions\n6. `-print0` uses null bytes to handle spaces in filenames\n7. `2>/dev/null` suppresses permission errors for unreadable files\n8. The while loop reads each file and displays violation details with current permissions and group","diagram":"flowchart TD\n  A[Start] --> B[Find files in /shared/projects/*/data/*]\n  B --> C{Changed in last 24h?}\n  C -->|Yes| D{Has group-w write?}\n  D -->|No| E[Violation detected]\n  D -->|Yes| F[OK]\n  E --> G[Print details]\n  F --> H[End]","difficulty":"beginner","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","MongoDB","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T05:09:57.945Z","createdAt":"2026-01-17T02:10:43.765Z"},{"id":"q-3134","question":"In a Unix host, backups live under /srv/backups; each backup is named backup-YYYYMMDD.tar.gz and contains a data/ directory. Write a robust one-liner (no external deps beyond standard UNIX tools) that prints the total number of unique .log files across all backups modified in the last 7 days. The command should gracefully skip unreadable archives and ignore non-log files?","answer":"One-liner: find /srv/backups -name 'backup-*.tar.gz' -mtime -7 -print0 2>/dev/null | xargs -0 -I{} sh -c 'tar -tzf \"{}\" --wildcards \"*.log\" 2>/dev/null' | sort -u | wc -l. This uses tar to enumerate l","explanation":"## Why This Is Asked\n\nTests ability to orchestrate standard UNIX tools across many archives, handling spaces and errors.\n\n## Key Concepts\n\n- tar -tzf and --wildcards\n- find -mtime\n- xargs -0 with placeholder\n- sort -u for deduping\n- 2>/dev/null for resilience\n\n## Code Example\n\n```bash\nfind /srv/backups -name 'backup-*.tar.gz' -mtime -7 -print0 2>/dev/null | xargs -0 -I{} sh -c 'tar -tzf \"{}\" --wildcards \"*.log\" 2>/dev/null' | sort -u | wc -l\n```\n\n## Follow-up Questions\n\n- How would you adapt if backups could be .tar (uncompressed) as well?\n- How would you exclude logs under a certain path like data/tmp/ ?","diagram":null,"difficulty":"beginner","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Google","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T04:14:49.463Z","createdAt":"2026-01-17T04:14:49.463Z"},{"id":"q-3206","question":"In a Unix host, logs live under /var/log/ops/* with hourly rotation to *.log and sometimes *.log.gz. Lines are either plain text LEVEL: message or JSON like {ts: ISO8601, svc: name, lvl: ERROR}, and multi-line messages may follow a line that starts with a timestamp. Propose a robust one-liner (no external deps beyond standard UNIX tools) that prints a per-minute histogram of ERROR events for the last 90 minutes, aggregating across all files and rotations, tolerant of missing files and gz archives, and robust to mixed formats?","answer":"Plan: a streaming command reading all logs (*.log and *.log.gz) via zcat/cat, piped to awk. The awk normalizes timestamps from JSON ts or ISO header, filters ERROR lines, buckets by minute, and mainta","explanation":"## Why This Is Asked\nTests ability to design robust log aggregation across mixed formats, gzipped, and rotated logs under load.\n\n## Key Concepts\n- Pipeline robustness and fault tolerance\n- Timestamp normalization for JSON and plain text\n- Per-minute bucketing and sliding windows\n- Handling multi-line records and unreadable files\n\n## Code Example\n\n## Follow-up Questions\n- How would you adapt to 5-minute buckets? \n- How would you unit-test with synthetic logs?","diagram":null,"difficulty":"intermediate","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Microsoft","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T06:56:52.176Z","createdAt":"2026-01-17T06:56:52.176Z"},{"id":"q-3443","question":"In a Unix host, logs live under /var/log/trace/* with hourly rotation; some files end in .log, others .log.gz. Each line is either [YYYY-MM-DD HH:MM:SS] svc: duration=NNNms or JSON {\"ts\":\"YYYY-MM-DDTHH:MM:SSZ\",\"svc\":\"name\",\"dur_ms\":NNN}. Propose a robust one-liner (no external deps beyond standard UNIX tools) to produce a per-service histogram of total request duration binned into 2-minute intervals for the last 60 minutes, aggregating across all files and rotations, tolerant of missing/unreadable files and out-of-order timestamps?","answer":"Loop over /var/log/trace/*.{log,log.gz}, decompress with zcat when needed, parse either [YYYY-MM-DD HH:MM:SS] or JSON ts, extract dur_ms, compute bucket = int(epoch(ts)/120), accumulate totals per svc","explanation":"## Why This Is Asked\nTests ability to design a robust, real-world log analysis task across mixed formats and rotated archives.\n\n## Key Concepts\n- Parsing plain and JSON logs\n- Handling gzipped rotated files without extra deps\n- Time bucketing and out-of-order events\n- Resilience to missing/unreadable files\n\n## Code Example\n```javascript\n// Implementation outline (not the full solution)\n```\n\n## Follow-up Questions\n- How would you scale this for millions of logs?\n- How would you test with synthetic data and verify correctness?","diagram":"flowchart TD\n  A[Start] --> B[Scan files]\n  B --> C[Parse lines]\n  C --> D[Bucket by 2-min]\n  D --> E[Sum per svc]\n  E --> F[Output histogram]\n  F --> G[End]","difficulty":"intermediate","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["NVIDIA","Netflix","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T16:40:40.287Z","createdAt":"2026-01-17T16:40:40.287Z"},{"id":"q-3499","question":"In a Unix host, multiple services write JSON log lines under /logs/app/*/*.log and are rotated hourly to *.log.gz. Each line is {\"ts\":\"YYYY-MM-DDTHH:MM:SSZ\",\"svc\":\"<name>\",\"lvl\":\"LEVEL\",\"msg\":\"...\"}. Write a robust one-liner (no external deps beyond standard UNIX tools) that prints a per-service idle time metric: time since last log line for each service within the last hour, flagging services idle for more than 2 minutes. Handle gz archives, unreadable files, and out-of-order timestamps?","answer":"Approach: read all files (uncompressed and gzipped) with a single pass, parse ts and svc using lightweight regex in awk (no jq), convert ts to epoch, track per-service last_seen, compute age = now - l","explanation":"## Why This Is Asked\n\nTests ability to design a minimal, robust monitoring query using basic UNIX tools without dependencies, and to reason about time-windowed data across rotated logs.\n\n## Key Concepts\n\n- Robust globbing and gzip handling\n- Lightweight JSON parsing with awk\n- Per-service state tracking and time-window logic\n- Handling out-of-order/timestamp skew\n\n## Code Example\n\n```javascript\n# Idea for one-liner (not runnable here):\n# find /logs/app -type f \\( -name '*.log' -o -name '*.log.gz' \\) -print0 | \\\n#   xargs -0 -I{} sh -c '...'\n```\n\n## Follow-up Questions\n\n- How would you adapt this for a rolling window of 5 minutes with a trailing watermark?\n- What are edge cases if clocks drift or NTP is off?","diagram":"flowchart TD\n  A[Collect files] --> B[Decompress/Read]\n  B --> C[Parse ts & svc]\n  C --> D[Update last_seen per svc]\n  D --> E[Compute age; idle flag]\n  E --> F[Output results]","difficulty":"intermediate","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Netflix","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T18:54:04.134Z","createdAt":"2026-01-17T18:54:04.134Z"},{"id":"q-3573","question":"You are deploying a per-host log-tailer for thousands of services across a fleet. Logs live under /var/log/services/<svc>/*.log and rotate daily; older files may be *.log.gz. The on-host agent must stream new lines to a central sink with at-least-once delivery, tolerate missing/unreadable files, and stay correct during concurrent rotations. Describe design, data flow, and failure modes; outline an implementation strategy using only standard UNIX tools?","answer":"Design a per-host log-tailer using inotify/fanotify to monitor new and rotated files, track per-file offsets (inode+offset) in a local manifest, decompress *.log.gz files on-the-fly with gzip -d or zcat, and implement at-least-once delivery with retry logic and backpressure management to a central sink.","explanation":"## Why This Is Asked\n\nEvaluates practical OS knowledge: file watching, log rotation, compression, and reliable delivery at scale.\n\n## Key Concepts\n- inotify and fanotify for file system monitoring\n- Per-file inode and offset tracking for state persistence\n- Handling gzipped rotations (zcat/gzip)\n- Buffering and backpressure to sink\n- Idempotent replay and deduplication on restart\n\n## Code Example\n```bash\n# Minimal sketch of watch loop (not a full solution)\ninotifywait -m -e create,close_write,move /var/log/services\n```\n\n## Follow-up Questions\n- How to test rotation edge cases?\n- How to scale the agent across thousands of services?\n- What monitoring and alerting would you implement?","diagram":null,"difficulty":"advanced","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","NVIDIA","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T05:42:58.410Z","createdAt":"2026-01-17T21:38:25.742Z"},{"id":"q-3583","question":"Scenario: A Unix host stores executables under /opt/apps with nested folders. Some files have setuid/setgid bits. Write a robust one-liner (no external deps beyond standard UNIX tools) that lists all files modified in the last 24 hours with setuid or setgid bits, printing per file: permissions, owner:group, and full path. The command must skip unreadable files and handle spaces in paths?","answer":"find /opt/apps -type f -mtime -1 -perm /6000 -print0 2>/dev/null | while IFS= read -r -d '' f; do stat -c '%A %U:%G %n' \"$f\"; done","explanation":"## Why This Is Asked\nThis question tests advanced shell scripting skills, requiring candidates to create robust, portable one-liners that handle edge cases like spaces in filenames and unreadable files using standard Unix tools.\n\n## Key Concepts\n- **find command**: Uses `-mtime -1` to filter files modified within the last 24 hours\n- **Permission checking**: Employs `-perm /6000` to detect files with setuid (4000) or setgid (2000) bits\n- **Safe filename handling**: Uses `-print0` with `read -d ''` to properly handle spaces and special characters in paths\n- **Error handling**: Redirects stderr to `/dev/null` to skip unreadable files\n- **Formatted output**: Leverages `stat -c` to display permissions, owner:group, and full path\n\n## Code Example\n```bash\nfind /opt/apps -type f -mtime -1 -perm /6000 -print0 2>/dev/null | while IFS= read -r -d '' f; do\n  stat -c '%A %U:%G %n' \"$f\"\ndone\n```\n\n## Follow-up Questions\n- How would you modify this command to search multiple directories?\n- What changes would be needed for BSD systems where `stat` syntax differs?\n- How could you extend this to also show file sizes?","diagram":null,"difficulty":"beginner","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Bloomberg","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T05:35:05.500Z","createdAt":"2026-01-17T22:34:06.936Z"},{"id":"q-3617","question":"On a Linux host with many long-running services, write a robust one-liner (no external deps beyond standard UNIX tools) that lists the top 5 processes by number of open file descriptors. Only include processes that started at least 24 hours ago. Output: PID, UID, user, FD count, executable path, and a human-friendly age (e.g., 3d4h)? Must handle unreadable pids and spaces in cmdlines. Use /proc and standard tools?","answer":"Enumerate /proc/[0-9]*; for each pid with readable /proc/$pid/fd, read starttime from /proc/$pid/stat and convert to age = starttime / CLK_TCK; skip age < 86400. Count fd in /proc/$pid/fd; map UID to username with getent; output formatted results sorted by FD count.","explanation":"## Why This Is Asked\n\nTests ability to reason about /proc, process metadata, and resource attribution in large systems; probes edge cases (spaces in paths, unreadable pids) and performance implications of scanning /proc.\n\n## Key Concepts\n\n- /proc filesystem and permissions\n- Counting file descriptors per process via /proc/[pid]/fd\n- starttime in /proc/[pid]/stat and CLK_TCK conversion\n- Mapping UID to username\n- Robust error handling and portability\n\n## Code Example\n\n```bash\n#!/bin/bash\nh=$(getconf CLK_TCK)\nfor p in /proc/[0-9]*; do\n  [ -r \"$p/fd\" ] || continue\n  s=$(awk '{print $22}' \"$p/stat\" 2>/dev/null) || continue\n  age=$((($(date +%s) - $(awk '{print $21}' \"$p/stat\" 2>/dev/null)/$h - $(cat \"$p/uptime\" 2>/dev/null | awk '{print $1}'))))\n  [ $age -ge 86400 ] || continue\n  fd=$(ls \"$p/fd\" 2>/dev/null | wc -l)\n  uid=$(stat -c %u \"$p\" 2>/dev/null)\n  user=$(getent passwd \"$uid\" 2>/dev/null | cut -d: -f1)\n  cmd=$(tr '\\0' ' ' < \"$p/cmdline\" 2>/dev/null | sed 's/ *$//')\n  printf \"%d %d %s %d %s %s\\n\" \"${p##*/}\" \"$uid\" \"$user\" \"$fd\" \"$cmd\" \"$(awk -v a=$age 'BEGIN{d=int(a/86400); h=int((a%86400)/3600); printf \"%dd%dh\", d, h}')\"\ndone | sort -k4 -nr | head -5 | column -t\n```\n\nThis solution handles unreadable PIDs gracefully, properly counts file descriptors, converts process start time to human-readable age, and outputs the top 5 processes by FD count with all requested fields formatted cleanly.","diagram":"flowchart TD\n  A[Start] --> B[Enumerate /proc]\n  B --> C{PID readable?}\n  C -- Yes --> D[Read stat starttime]\n  D --> E[Compute age]\n  E -- age>=24h --> F[Count /proc/$pid/fd]\n  F --> G[Resolve uid/user/exe]\n  G --> H[Emit line]\n  H --> I[Sort, head -5]\n  C -- No --> J[Skip]","difficulty":"advanced","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","Oracle","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T05:14:33.957Z","createdAt":"2026-01-17T23:40:09.633Z"},{"id":"q-3765","question":"In a Unix host, there is a directory /var/experiments containing subdirectories for each experiment. Each subdir may contain a file metrics.txt with lines that are either PASS or FAIL; some subdirs may contain metrics.txt.gz instead. Write a robust one-liner (no external dependencies beyond standard UNIX tools) that prints per-experiment fail rate as: <experiment>: <fails>/<total> (<percent>%), handling missing files and gzipped files, and across many experiments (spaces in names)?","answer":"for d in /var/experiments/*; do f=\"$d/metrics.txt\"; gz=\"$d/metrics.txt.gz\"; if [ -f \"$f\" ]; then t=$(wc -l < \"$f\"); e=$(grep -c '^FAIL' \"$f\"); elif [ -f \"$gz\" ]; then t=$(gzip -cd \"$gz\" | wc -l); e=$(","explanation":"## Why This Is Asked\nTests ability to compose robust shell pipelines handling gzipped files and missing inputs. \n\n## Key Concepts\n- Directory traversal with globbing; file presence checks; gzip decompression with standard tools; integer arithmetic in shell.\n\n## Code Example\n```bash\nfor d in /var/experiments/*; do\n  f=\"$d/metrics.txt\"; gz=\"$d/metrics.txt.gz\"\n  if [ -f \"$f\" ]; then t=$(wc -l < \"$f\"); e=$(grep -c '^FAIL' \"$f\");\n  elif [ -f \"$gz\" ]; then t=$(gzip -cd \"$gz\" | wc -l); e=$(gzip -cd \"$gz\" | grep -c '^FAIL');\n  else t=0; e=0; fi\n  if [ \"$t\" -gt 0 ]; then p=$(( e * 100 / t )); echo \"$(basename \"$d\"): $e/$t ($p%)\"; fi\ndone\n```\n\n## Follow-up Questions\n- How would you adapt for mixed case-sensitive values like FAIL and FAILURES?\n- How would you parallelize this for many experiments while preserving stable output order?","diagram":null,"difficulty":"beginner","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google","Hugging Face"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T08:46:04.010Z","createdAt":"2026-01-18T08:46:04.010Z"},{"id":"q-3836","question":"Scenario: A Unix host runs dozens of services that log to daily rotated plain-text logs under /var/log/app-logs/app-<service>-YYYYMMDD.log, plus a real-time stream via a named pipe at /var/log/app-logs/pipe.log which receives JSON entries like {ts: ISO8601, svc: name, lvl: ERROR, msg: ...}. Some services also compress old logs to .gz. Write a robust one-liner (no external deps beyond standard UNIX tools) that outputs a per-minute histogram of ERROR events for the last 60 minutes, aggregating across both file logs and the live pipe, tolerant of missing/unreadable files and gzipped archives, and resilient to mixed formats (plain text and JSON)?","answer":"One-liner approach: read both plain logs and gzips, parse lines for either JSON {'ts':'...','svc':'...','lvl':'ERROR',...} or text containing 'ERROR', normalize timestamp to epoch, filter last 60 minu","explanation":"## Why This Is Asked\n\nInterviews test ability to craft resilient one-liners that mix formats, rotations, and streaming input. It emphasizes time-windowing, gzip handling, and incremental aggregation with standard tools.\n\n## Key Concepts\n- Log format normalization across JSON and plain text\n- Time-window filtering with date/epoch\n- Combining multiple sources (files + pipe)\n- Robustness to missing/unreadable files\n- Streaming compression (gzip) and rotations\n\n## Code Example\n```javascript\n// Implementation example not shown here as this is the candidate's task\n```\n\n## Follow-up Questions\n- How would you adapt this for a multi-node cluster?\n- How would you test edge cases like clock skew?","diagram":"flowchart TD\n  A[Start] --> B[Collect sources]\n  B --> C[Parse lines (JSON/text)]\n  C --> D[Normalize time to epoch]\n  D --> E[Filter last 60m]\n  E --> F[Bucket by service + minute]\n  F --> G[Output histogram]","difficulty":"advanced","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Meta","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T11:30:55.314Z","createdAt":"2026-01-18T11:30:55.314Z"},{"id":"q-481","question":"You're debugging a production system where processes are hanging. Using only Unix tools, how would you identify which processes are blocked on I/O, what they're waiting for, and safely terminate them without causing data corruption?","answer":"Use `lsof -p <PID>` to see open files and `strace -p <PID>` to identify blocked system calls. Check `/proc/<PID>/fd` for file descriptors. For safe termination, send SIGTERM first: `kill -15 <PID>`, wait for graceful shutdown, then use SIGKILL if necessary.","explanation":"## Process Identification\n- `ps aux | grep D` shows processes in uninterruptible sleep\n- `top` with 'H' shows thread-level status\n- `iostat -x 1` identifies I/O bottlenecks\n\n## Root Cause Analysis\n- `strace -p <PID>` reveals blocked system calls\n- `lsof -p <PID>` shows open files and network connections\n- `/proc/<PID>/status` provides process state details\n\n## Safe Termination\n- SIGTERM allows graceful shutdown\n- Check for child processes before killing\n- Verify no critical writes in progress","diagram":"flowchart TD\n  A[Detect hanging process] --> B[ps aux | grep D]\n  B --> C[strace -p PID]\n  C --> D[lsof -p PID]\n  D --> E{Safe to terminate?}\n  E -->|Yes| F[kill -15 PID]\n  E -->|No| G[Wait for I/O completion]\n  F --> H[Monitor with ps]\n  G --> F","difficulty":"advanced","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Snap","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-10T03:29:09.162Z","createdAt":"2025-12-26 12:51:05"},{"id":"q-510","question":"You're debugging a production issue where a process is stuck in uninterruptible sleep (D state). How would you identify and handle this situation?","answer":"Use `ps aux | awk '$8 ~ /D/ {print $2, $11}'` to find D-state processes. Check `dmesg | grep -i oom` for OOM killer activity. For I/O issues, use `lsof -p <PID>` to identify blocked files. If it's NFS, verify mount status and network connectivity.","explanation":"## Identifying D-State Processes\n\n- Use `ps` with state filtering to find uninterruptible processes\n- Check system logs for hardware or filesystem errors\n- Examine I/O queues and block device status\n\n## Common Causes\n\n- NFS mount issues or network storage problems\n- Faulty hardware devices (disk, controller)\n- Kernel bugs or driver issues\n- Memory pressure causing I/O blocking\n\n## Resolution Strategies\n\n- Wait for hardware timeout (usually 30-120 seconds)\n- Check and fix underlying storage issues\n- Reboot as last resort if process won't recover\n- Monitor `/proc/<PID>/stack` for kernel call traces","diagram":"flowchart TD\n  A[Process enters D state] --> B{Identify cause}\n  B --> C[Hardware issue]\n  B --> D[Network storage]\n  B --> E[Kernel/driver]\n  C --> F[Check dmesg/logs]\n  D --> G[Verify mount status]\n  E --> H[Examine stack trace]\n  F --> I[Wait or fix hardware]\n  G --> J[Resolve network/storage]\n  H --> K[Update/reboot if needed]","difficulty":"intermediate","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["OpenAI","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-09T03:44:39.931Z","createdAt":"2025-12-26 12:51:06"},{"id":"q-538","question":"You notice a process is consuming excessive CPU on a production server. How would you diagnose and troubleshoot this issue using Unix commands?","answer":"I would start by using `top` or `htop` to identify the process ID (PID) consuming excessive CPU, then run `ps aux | grep PID` to get detailed process information and command line arguments. For deeper analysis, I'd use `strace -p PID` to monitor system calls in real-time, `lsof -p PID` to examine open files and network connections, and `perf top` for CPU performance profiling.","explanation":"## Diagnosis Steps\n- Use `top` or `htop` to identify the high CPU process and its PID\n- Run `ps aux` to view process details and command line arguments\n- Monitor system calls with `strace -p PID` to understand process behavior\n\n## Investigation Tools\n- `lsof -p PID` reveals open files and network connections\n- `perf top` provides CPU performance profiling and bottleneck identification\n- `/proc/PID/status` contains comprehensive memory and CPU statistics\n\n## Resolution\n- Send SIGTERM (`kill -15`) for graceful process shutdown\n- Use SIGKILL (`kill -9`) only when the process is unresponsive\n- Analyze logs in `/var/log/` to identify and address the root cause","diagram":"flowchart TD\n  A[High CPU Alert] --> B[top/htop - Identify PID]\n  B --> C[ps aux - Process Details]\n  C --> D[strace - System Calls]\n  D --> E[lsof - Open Files]\n  E --> F[perf top - Performance Profile]\n  F --> G{Process Responsive?}\n  G -->|Yes| H[kill -15 PID]\n  G -->|No| I[kill -9 PID]\n  H --> J[Monitor Resolution]\n  I --> J","difficulty":"intermediate","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Goldman Sachs","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":["troubleshooting","cpu profiling","strace","lsof","performance","diagnosis"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2026-01-08T11:43:07.374Z","createdAt":"2025-12-26 12:51:06"},{"id":"q-564","question":"You're debugging a production system where processes are hanging. Using only Unix tools, how would you identify which processes are stuck in uninterruptible sleep (D state) and what could be causing this?","answer":"Use `ps aux | awk '$8 ~ /^D/ {print $2, $11}'` to identify processes in uninterruptible sleep state. Examine `/proc/<pid>/stack` for kernel stack traces to understand what system calls are blocking. Common causes include NFS server issues, faulty storage drivers, hardware I/O problems, or disk bottlenecks. Use `iostat -x 1` to monitor I/O activity and `dmesg | grep -i error` to check for hardware or driver errors.","explanation":"## Identifying D-State Processes\n- Use `ps aux | awk '$8 ~ /^D/ {print $2, $11}'` to filter processes in uninterruptible sleep\n- Check `/proc/<pid>/status` for detailed process state information\n- Monitor system-wide D-state processes with `top` or `htop` filtered by state\n\n## Root Cause Analysis\n- Examine `/proc/<pid>/stack` to identify the specific kernel functions blocking the process\n- Use `dmesg | grep -i error` to detect hardware or driver-related issues\n- Monitor I/O statistics with `iostat -x 1` to identify storage bottlenecks\n- Check `lsblk` and `smartctl` for disk health and controller issues\n\n## Common Causes\n- NFS server unavailability or network connectivity issues\n- Faulty disk controllers, RAID arrays, or storage drivers\n- Hardware failures in storage subsystem (bad sectors, failing drives)\n- Memory pressure causing excessive swap activity\n- Kernel bugs or incompatible device drivers\n- Storage system saturation or filesystem corruption","diagram":"flowchart TD\n  A[ps aux | awk] --> B[Identify D-state PIDs]\n  B --> C[/proc/<pid>/stack]\n  C --> D[Analyze kernel stack]\n  D --> E[lsof -p <pid>]\n  E --> F[Check open files]\n  F --> G[strace -p <pid>]\n  G --> H[Trace syscalls]","difficulty":"advanced","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","OpenAI","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":["uninterruptible sleep","d state","ps command","proc filesystem","stack traces","nfs","i/o bottlenecks","kernel debugging"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2026-01-08T11:57:22.806Z","createdAt":"2025-12-26 12:51:07"},{"id":"q-894","question":"On a Linux host, /var/log/myapp.log is written by multiple processes. Implement a robust log rotation that triggers when the file reaches 100MB, keeps 7 rotated files, compresses older logs, and ensures no log loss while writers continue. Describe the approach, commands, and failure modes for concurrent writers?","answer":"Configure logrotate for /var/log/myapp.log with size 100M, rotate 7, compress and delaycompress, using sharedscripts. Use a postrotate block to signal the processes to reopen the log (e.g., systemctl ","explanation":"## Why This Is Asked\\n\\nThis question probes practical log management under concurrency, emphasizing safe rotation with multi-process writers and minimizing data loss. Expect discussion of reopen signals vs copytruncate, service integration, and failure modes.\\n\\n## Key Concepts\\n- logrotate configuration: size-based rotation, retention, compression\\n- handling concurrent writers: reopen vs copytruncate, signals (SIGHUP)\\n- service integration: systemd or pid signaling; permissions and timing\\n\\n## Code Example\\n```javascript\\n/var/log/myapp.log {\\n  size 100M\\n  rotate 7\\n  compress\\n  delaycompress\\n  missingok\\n  notifempty\\n  create 0644 root root\\n  sharedscripts\\n  postrotate\\n    systemctl is-active myapp >/dev/null && systemctl kill -s HUP myapp || kill -HUP `cat /var/run/myapp.pid` 2>/dev/null || true\\n  endscript\\n}\\n```\\n\\n## Follow-up Questions\\n- What if the app cannot reopen log files on HUP?\\n- How would you test concurrent writers during rotation?","diagram":null,"difficulty":"intermediate","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Discord","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T14:35:24.296Z","createdAt":"2026-01-12T14:35:24.296Z"},{"id":"q-264","question":"How do Unix pipes enable inter-process communication and what are their performance implications?","answer":"Pipes provide unidirectional byte streams between processes, utilizing kernel buffers for efficient inter-process communication with blocking I/O semantics.","explanation":"## Why Asked\nTests understanding of IPC mechanisms and system design principles for scalable applications.\n\n## Key Concepts\nUnidirectional communication, kernel buffering, blocking I/O, file descriptor abstraction, pipe capacity limits.\n\n## Code Example\n```bash\n# Create pipe and connect processes\nls -l | grep \".txt\" | wc -l\n# Kernel manages 64KB buffer between processes\n```\n\n## Follow-up Questions\nWhat's the difference between named and anonymous pipes? How do pipes handle backpressure? What are alternatives to pipes?","diagram":"flowchart TD\n  A[Process A] -->|writes| B[Pipe Buffer]\n  B -->|reads| C[Process B]\n  D[Kernel] -->|manages| B","difficulty":"beginner","tags":["posix","signals","pipes","sockets"],"channel":"unix","subChannel":"system-programming","sourceUrl":"https://man7.org/linux/man-pages/pipe.2","videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Apple","Google","Meta","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-03T06:38:32.248Z","createdAt":"2025-12-26 12:51:07"}],"subChannels":["general","system-programming"],"companies":["Adobe","Airbnb","Amazon","Anthropic","Apple","Bloomberg","Citadel","Cloudflare","Coinbase","Databricks","Discord","Goldman Sachs","Google","Hugging Face","IBM","LinkedIn","Lyft","Meta","Microsoft","MongoDB","NVIDIA","Netflix","OpenAI","Oracle","PayPal","Plaid","Robinhood","Salesforce","Slack","Snap","Snowflake","Square","Stripe","Tesla","Twitter","Two Sigma","Uber","Zoom"],"stats":{"total":36,"beginner":14,"intermediate":12,"advanced":10,"newThisWeek":31}}