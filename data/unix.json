{"questions":[{"id":"q-1002","question":"In a Unix environment logs are stored in /var/log/app/*.log with lines formatted as timestamp|user|action|resource. Write a practical one-liner using standard UNIX tools to output the top 5 users by total actions in the last 24 hours. Explain how you would handle log rotation and malformed lines?","answer":"One practical approach is to filter the last 24h files, extract the user, and count actions. Example: find /var/log/app/*.log -type f -mtime -1 -print0 | xargs -0 cat | awk -F'|' 'NF>=4{cnt[$2]++} END","explanation":"## Why This Is Asked\nTests practical log-scan skills: building a robust one-liner that handles log rotation, malformed lines, and spaces in fields. It also gauges comfort with standard tools and edge-case thinking.\n\n## Key Concepts\n- Log rotation: using -mtime to limit scope to last day.\n- Field delimiting: using a stable delimiter (|) for reliable parsing.\n- Robust counting: associative arrays in awk for tallying per user.\n- Safe I/O: null-delimited input with -print0/xargs -0 to support spaces.\n\n## Code Example\n```javascript\nfind /var/log/app/*.log -type f -mtime -1 -print0 | xargs -0 cat | awk -F'|' 'NF>=4{cnt[$2]++} END{for(u in cnt) print cnt[u], u}' | sort -nr | head -5\n```\n\n## Follow-up Questions\n- How would you handle logs where the user field is sometimes missing or empty?\n- How would you validate results across multiple days with varying time zones?\n- How would you adapt this for extremely large log pools to minimize I/O impact?","diagram":"flowchart TD\n  Start --> ReadLogs[Read log files in /var/log/app]\n  ReadLogs --> Filter[Filter last 24 hours with -mtime]\n  Filter --> Extract[Extract user field (split on |)]\n  Extract --> Count[Count per user (awk)]\n  Count --> Sort[Sort counts descending]\n  Sort --> Top5[Output top 5 users]\n  Top5 --> End[Done]","difficulty":"beginner","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T18:47:17.203Z","createdAt":"2026-01-12T18:47:17.203Z"},{"id":"q-1032","question":"How would you capture stdout and stderr of a simple shell command into separate log files while still displaying live output in the terminal? Provide a concrete Bash command and brief justification?","answer":"Use Bash process substitution to split streams: command > >(tee -a stdout.log) 2> >(tee -a stderr.log >&2). This sends stdout to both terminal and stdout.log, and stderr to both terminal (via 2>&) and","explanation":"## Why This Is Asked\n\nAssess understanding of Unix IO redirection, common tooling (tee), and Bash-specific features (process substitution) that enable real-time logging without losing output.\n\n## Key Concepts\n\n- stdout/stderr redirection\n- process substitution\n- tee for file logging\n\n## Code Example\n\n```bash\ncommand > >(tee -a stdout.log) 2> >(tee -a stderr.log >&2)\n```\n\n## Follow-up Questions\n\n- How would you implement a portable alternative for POSIX sh?\n- What if the command writes to stdout after its own buffering ends; how could you ensure timely logs?","diagram":null,"difficulty":"beginner","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","LinkedIn","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T20:22:14.784Z","createdAt":"2026-01-12T20:22:14.785Z"},{"id":"q-1069","question":"In a Unix environment, logs live under /var/log/metrics/*.log and are hourly rotated to .log and .log.gz. Each line is like [YYYY-MM-DD HH:MM:SS] LEVEL: message. Propose a robust, portable approach (one-liner preferred) to output the number of ERROR events per hour for the last 6 hours, handling missing files and rotations without external dependencies?","answer":"I’d implement a robust, portable pipeline reading both compressed and plain logs, extracting the hour from each timestamp, filtering for ERROR, and aggregating counts for the last 6 hours. Use globbin","explanation":"## Why This Is Asked\nTests practical mastery of Unix tooling, log rotation handling, and resilient scripting under real-world constraints.\n\n## Key Concepts\n- Robust log ingestion across rotated files\n- Mixed gzip/uncompressed handling with zcat\n- Time bucketing and boundary handling\n- Idempotent, side-effect-free pipelines\n\n## Code Example\n```javascript\n// Bash sketch\nstart=$(date -u -d '-6 hours' '+%Y-%m-%d %H')\nzcat /var/log/metrics/metrics-*.log.gz /var/log/metrics/metrics-*.log 2>/dev/null \\\n| awk -v cutoff=\"$start\" '/ERROR/ { if ($0 ~ /\\[/) { hour=substr($1,1,10)\" \"substr($2,1,2); if (hour>=cutoff) c[hour]++ } } END{for (h in c) print h\":00\", c[h]}'\n```\n\n## Follow-up Questions\n- How would you test this under log rotation? \n- How would you adapt for multiple timezones?","diagram":"flowchart TD\n  A[Start] --> B[Collect log files (gz and plain)]\n  B --> C[Parse timestamps to hour bucket]\n  C --> D[Filter ERROR events]\n  D --> E[Aggregate last 6 hours]\n  E --> F[Output results]","difficulty":"advanced","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Meta","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T21:30:28.843Z","createdAt":"2026-01-12T21:30:28.843Z"},{"id":"q-1123","question":"In a Unix environment, multiple services write JSON logs under /var/log/diag/*.log and rotated hourly to *.log.gz. Each line is a JSON object like {\"ts\":\"YYYY-MM-DDTHH:MM:SSZ\",\"svc\":\"name\",\"lvl\":\"ERROR\",\"msg\":\"...\"}; Propose a robust one-liner (no external dependencies beyond standard UNIX tools) to output the number of ERROR events per hour for the last 4 hours, aggregated across all services, and tolerant of missing files and rotated archives?","answer":"I’d implement a robust portable one-liner that streams both .log and .log.gz files, extracts ts and lvl from each JSON line, converts ts to epoch, filters for the last 4 hours, buckets per hour (YYYY-","explanation":"## Why This Is Asked\n\nTests ability to write compact, production-grade log queries that tolerate rotations, mixed compression, and JSON-like lines using only core Unix tools.\n\n## Key Concepts\n\n- Robust file discovery with find and null-termination\n- Decompression with zcat for .gz files\n- Inline JSON-like parsing with simple regex in awk\n- Time-window filtering via epoch comparison using date\n- Hour bucketing and aggregation with associative arrays\n\n## Code Example\n\n```javascript\n#!/bin/sh\ncutoff=$(date -u -d '4 hours ago' +%s)\nfind /var/log/diag -type f \\( -name '*.log' -o -name '*.log.gz' \\) -print0 | \\\n  xargs -0 -I{} sh -c ' [ -f \"$1\" ] && { if [ \"${1##*.}\" = \"gz\" ]; then zcat \"$1\"; else cat \"$1\"; fi; }' _ {} | \\\nawk -v cut=\"$cutoff\" ' /\"lvl\":\"ERROR\"/ { if (match($0, /\"ts\":\"([^\\\\\"]+)\"/, a)) { cmd=\"date -u -d \\\"\" a[1] \"\\\" +%s\"; cmd | getline t; close(cmd); if (t >= cut) { hour = substr(a[1],1,13) \"Z\"; count[hour]++ } } } END { for (h in count) print h, count[h] } '\n```\n\n## Follow-up Questions\n\n- How would you adjust for millisecond timestamps?\n- How would this scale across many servers and years of logs?","diagram":"flowchart TD\n  A[Start] --> B[Find logs (*.log, *.log.gz)]\n  B --> C[Decompress and read lines]\n  C --> D[Parse JSON ts and lvl]\n  D --> E[Filter last 4 hours]\n  E --> F[Bucket by hour]\n  F --> G[Count ERRORs]\n  G --> H[Output]","difficulty":"intermediate","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","OpenAI","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T23:30:41.563Z","createdAt":"2026-01-12T23:30:41.563Z"},{"id":"q-1139","question":"Scenario: JSON logs at /var/log/diag/*.log, rotated hourly to *.log.gz. Each line: {\"ts\":\"YYYY-MM-DDTHH:MM:SSZ\",\"svc\":\"name\",\"lvl\":\"ERROR\",\"msg\":\"...\"}. Propose a robust one-liner (no external deps beyond standard UNIX tools) that outputs the per-hour 99th percentile of message length for the last 4 hours, across all services, deduplicating identical messages per hour, and tolerant of missing files and gz archives?","answer":"Use a two-pass approach: 1) gzip -cd /var/log/diag/*.log.gz 2>/dev/null | awk to emit hour bucket and msg length, deduplicating by hour+msg with an associative array; filter to last 4 hours by ts; 2) ","explanation":"## Why This Is Asked\nTests real-world log ingestion under rotation; requires careful handling of gz files, missing files, and cross-service aggregation. The candidate should justify a robust percentile computation and dedup logic.\n\n## Key Concepts\n- Standard UNIX toolchain for parsing JSON-like lines without jq\n- Handling gzip-rotated logs and missing files\n- Per-hour bucketing and deduplication by message\n- Computation of a percentile from a 1-D length distribution\n\n## Code Example\n```bash\n# skeleton command outline (not complete here)\n```\n\n## Follow-up Questions\n- How would you test with synthetic logs at scale?\n- How would you adapt for different rotations or non-UTC timestamps?","diagram":"flowchart TD\n  A[Read logs (gz and plain)] --> B[Parse ts and msg length]\n  B --> C[Bucket by hour]\n  C --> D[Deduplicate by hour+msg]\n  D --> E[Compute 99th percentile per hour]\n  E --> F[Output results]","difficulty":"advanced","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["OpenAI","Plaid","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T01:26:59.322Z","createdAt":"2026-01-13T01:26:59.322Z"},{"id":"q-1224","question":"Scenario: In a Unix cluster, logs are emitted as JSON lines under /var/log/cluster/*/*.log and rotated hourly to *.log.gz. Each line contains {\"ts\":\"YYYY-MM-DDTHH:MM:SSZ\",\"tenant\":\"tenant-id\",\"svc\":\"service\",\"lvl\":\"LEVEL\",\"msg\":\"...\"}; Propose a robust one-liner (no external deps beyond standard UNIX tools) to identify the top 3 tenants by error rate (ERROR / total events) for the last 6 hours, aggregated across all services, tolerant of missing files and gz archives?","answer":"Stream all logs (including .log.gz), extract ts and tenant via awk regex, filter lines within the last 6 hours using START/END timestamps from date -u, accumulate per-tenant total and ERROR counts, th","explanation":"## Why This Is Asked\n- Assesses ability to build robust, single-line data pipelines over multi-file logs, including compressed files, without external dependencies. \n- Tests time-window handling, cross-service aggregation, and per-tenant bucketing under rotation. \n\n## Key Concepts\n- Stream processing with standard UNIX tools (find, xargs, zcat, awk). \n- JSON field extraction via regex in awk; resilient to line variations. \n- Time-window filtering using START/END derived from date; cross-file aggregation. \n- Sorting by computed metric (error rate) without extra tools.\n\n## Code Example\n```javascript\nSTART=$(date -u -d \"-6 hours\" +\"%Y-%m-%dT%H\")\nEND=$(date -u +\"%Y-%m-%dT%H\")\nfind /var/log/cluster -type f \\( -name '*.log' -o -name '*.log.gz' \\) -print0 | \\\n  xargs -0 -I{} sh -lc 'case \"{}\" in *.gz) zcat \"{}\" ;; *) cat \"{}\" ;; esac' | \\\n  awk -v s=\"$START\" -v e=\"$END\" '{\n    if (match($0, /\"ts\":\"([^\"]+)\"/, a)) ts=a[1]; hour=substr(ts,1,13); gsub(/T/,\" \",hour); hour_key=hour;\n    if (match($0, /\"tenant\":\"([^\"]+)\"/, b)) tenant=b[1];\n    if (match($0, /\"lvl\":\"([^\"]+)\"/, c)) lvl=c[1];\n    if (hour_key && tenant && hour_key>=s && hour_key<=e) {\n      total[tenant] += 1; if (lvl==\"ERROR\") errors[tenant] += 1;\n    }\n  }\n  END { for (t in total) { rate = total[t] ? errors[t] / total[t] : 0; printf \"%s\\t%.6f\\n\", t, rate } }\n``` \n\n## Follow-up Questions\n- How would you adapt this to include per-hour breakdowns in addition to the overall top 3? \n- How would you handle non-UTC timestamps or malformed JSON lines? ","diagram":null,"difficulty":"advanced","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Coinbase","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T05:34:59.109Z","createdAt":"2026-01-13T05:34:59.109Z"},{"id":"q-1241","question":"Scenario: In a multi-user Unix workspace, /home contains subdirectories per user with various files. Some are large, some are temporary. Provide a robust one-liner (no external dependencies) that lists the five largest regular files under /home (recursively), excluding hidden files and symlinks, printing each as 'size<TAB>path' with sizes in human-readable form. Explain how you ensure safety against spaces and newlines in filenames?","answer":"One robust option is a single pipeline that uses find with a safe delimiter, sorts by size, and formats to human units. For example: find /home -type f -not -path '*/.*' -print0 | xargs -0 stat -c '%s","explanation":"Why this is asked: tests understanding of safe file discovery, handling of spaces, and large-file detection. Key points: exclude hidden paths, only regular files, robust delimiting with null separators, and portable human-readable formatting via an awk function that scales from B up to GB. Edge considerations include spaces in names and potential binary data in paths.","diagram":"flowchart TD\n  A[Find largest files] --> B[Filter /home recursively]\n  B --> C[Exclude hidden/symlinks]\n  C --> D[Emit size+path]\n  D --> E[Sort desc & take top 5]\n  E --> F[Format human units]","difficulty":"beginner","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Lyft","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T06:36:11.300Z","createdAt":"2026-01-13T06:36:11.300Z"},{"id":"q-1492","question":"Scenario: multiple services log JSON lines to /var/log/app/*.log, rotated hourly to *.log.gz. Each line looks like {\"ts\":\"YYYY-MM-DDTHH:MM:SSZ\",\"svc\":\"name\",\"lat_ms\":123}. Propose a robust one-liner (no external deps beyond standard UNIX tools) to compute the per-service average lat_ms in the last 2 hours, across all logs, tolerant of missing and gzipped files, ignoring malformed lines. Output: 'service avg_ms'?","answer":"cutoff=$(date -u -d '-2 hours' '+%Y-%m-%dT%H:%M:%SZ'); find /var/log/app -type f \\( -name '*.log' -o -name '*.log.gz' \\) -print0 | while IFS= read -r -d '' f; do if [[ \"$f\" == *.gz ]]; then zcat \"$f\";","explanation":"Why This Is Asked: tests robust log ingestion across gzipped rotations. Key Concepts: shell pipelines, handling gzip, JSON-like parsing with regex in awk, per-service aggregation, time-window filtering via ISO timestamps. Code Example: the command reads both .log and .log.gz, extracts ts/svc/lat_ms, filters to last 2 hours, accumulates sums and counts per service, then prints averages. Follow-ups explore error handling and performance tweaks.","diagram":null,"difficulty":"intermediate","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["PayPal","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T19:28:41.147Z","createdAt":"2026-01-13T19:28:41.149Z"},{"id":"q-1809","question":"Scenario: A Linux host logs auth events into /var/log/auth/{service}/*.log and rotates hourly to *.log.gz. Each line is JSON like {\"ts\":\"YYYY-MM-DDTHH:MM:SSZ\",\"user\":\"alice\",\"action\":\"LOGIN\",\"result\":\"FAIL\"}. Propose a robust one-liner (no external deps beyond standard UNIX tools) to print the top 5 users by failed login rate in the last 2 hours, aggregated across all services, tolerant of missing files and gz archives, and resilient to malformed lines?","answer":"Stream both gz and plain logs via zcat, filter for status FAIL, extract ts and user with awk, convert ts to epoch and compare to 2-hour window, accumulate per-user totals, then emit in descending orde","explanation":"## Why This Is Asked\nThis tests real-world log aggregation with mixed rotation and JSON parsing using canonical UNIX tools.\n\n## Key Concepts\n- zcat + globbing for .log.gz and .log\n- awk-based JSON field extraction without external deps\n- date for UTC epoch bucketing; last N hours window\n- robust handling of missing/malformed lines\n\n## Code Example\n```javascript\n// Pseudocode sketch of the approach\nconst cutoff = Date.now() - 2*3600*1000\nfor each line in streams:\n  parse json\n  if (record.status == 'FAIL' && new Date(record.ts) >= cutoff) {\n    counts[record.user]++\n  }\n}\nprint top 5 by counts\n```\n\n## Follow-up Questions\n- How would you adapt this to count per-minute windows?\n- How would you test with synthetic rotated logs?","diagram":"flowchart TD\n  A[Parse logs] --> B[Extract fields]\n  B --> C[Apply time window]\n  C --> D[Aggregate per user]\n  D --> E[Sort & select top5]","difficulty":"intermediate","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Robinhood","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T11:44:32.982Z","createdAt":"2026-01-14T11:44:32.982Z"},{"id":"q-1903","question":"Propose a robust one-liner to compute the number of ERROR lines per hour for the previous 12 hours, aggregating across all logs in /var/log/app and across both uncompressed (*.log) and rotated/compressed files (*.log.*, *.log.gz). The one-liner must tolerate missing files and gz archives and rely only on standard UNIX tools?","answer":"One-liner approach: stream all logs (/*.log, *.log.*, *.log.gz) and pipe to awk. Parse timestamps [YYYY-MM-DD HH:MM:SS], convert to epoch with mktime, bucket by hour with strftime, and count ERROR lin","explanation":"## Why This Is Asked\n\nAssesses ability to craft a robust log-analysis one-liner that handles mixed compression formats, time-window filtering, and cross-file aggregation using only standard UNIX tools.\n\n## Key Concepts\n\n- Shell file globbing and safe looping over missing files\n- Decompression with zcat and streaming with cat\n- GNU awk time parsing (mktime, systime, strftime)\n- Time-window filtering and hour bucketing\n- Sorting deterministic output\n\n## Code Example\n\n```javascript\n// Conceptual core approach in a single pipeline\nfor f in /var/log/app/*.log /var/log/app/*.log.* /var/log/app/*.log.gz; do\n  case \"$f\" in\n    *.gz) zcat \"$f\" ;; *) cat \"$f\" ;; esac;\ndone | awk 'BEGIN{now=systime()} /\\[[0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2}\\]/ { if (match($0, /\\[([0-9]{4})-([0-9]{2})-([0-9]{2}) ([0-9]{2}):([0-9]{2}):([0-9]{2})\\]/, d)) { ts = mktime(d[1]\" \"d[2]\" \"d[3]\" \"d[4]\" \"d[5]\" \"d[6]); if (ts>=now-12*3600 && /ERROR/) { h=strftime(\"%Y-%m-%d %H:00\", ts); c[h]++ } } } END{ for (k in c) print k, c[k] }' | sort\n```\n\n## Follow-up Questions\n\n- How would you adapt this to handle a different timestamp format or time zone?\n- How would you modify to also count WARN and INFO separately while preserving performance?","diagram":null,"difficulty":"beginner","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Slack","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T16:49:53.401Z","createdAt":"2026-01-14T16:49:53.401Z"},{"id":"q-1975","question":"In a Unix host, logs live in /logs/web/*.log and are rotated hourly to *.log.gz. Each line is a JSON object like {\"ts\":\"YYYY-MM-DDTHH:MM:SSZ\",\"ip\":\"1.2.3.4\",\"method\":\"GET\",\"path\":\"/api\"}. Propose a robust one-liner (no external deps beyond standard UNIX tools) to output the top 5 IPs by request count in the last 4 hours, aggregating across all files and rotations, tolerant of missing files and compressed archives?","answer":"Stream all logs from /logs/web/*.log and *.log.gz using a single, portable pipeline, extract ip and ts from each JSON line, filter lines within the last 4 hours by converting ts to epoch, then count p","explanation":"## Why This Is Asked\nAssesses ability to reason about log rotation, gzipped files, and stream processing with minimal tools.\n\n## Key Concepts\n- Globbing and rotated logs\n- Stream processing with zcat and cat\n- Text extraction with sed/awk\n- Data aggregation with awk\n\n## Code Example\n```bash\n# Candidate approach (illustrative)\nzcat /logs/web/*.log.gz /logs/web/*.log 2>/dev/null | \\\nawk -v cutoff=\"$(date -u -d '4 hours ago' +%s)\" 'BEGIN{RS=\"\"; FS=\"\"} /\"ts\":\"/ { if(match($0, /\"ts\":\"([^\"]+)\"/, m)) {\n  cmd=\"date -u -d \\\"\" m[1] \"\\\" +%s\"; cmd | getline t; close(cmd);\n  if(t>=cutoff && match($0, /\"ip\":\"([0-9.]+)\"/, ip)) { c[ip[1]]++ }\n}} END{ for(i in c) print c[i]\" \"\"i }' | \\\nsort -nr | head -n5\n```\n\n## Follow-up Questions\n- How would you adapt for non-GNU awk?\n- What changes if timestamps are in a different timezone?","diagram":null,"difficulty":"beginner","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Snap","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T19:30:05.919Z","createdAt":"2026-01-14T19:30:05.919Z"},{"id":"q-2094","question":"Scenario: In a Unix CI environment, logs are written by build agents under /build/logs/*/*.log and rotated hourly to *.log.gz. Each line is BUILDID|TIMESTAMP|STEP|STATUS|MESSAGE. Propose a robust one-liner (no external deps beyond standard UNIX tools) to report, for the last 24 hours, per BUILDID and per hour, the count of failed steps (STATUS != 'SUCCESS'), aggregating across all agents and rotations and tolerating missing files and compressed archives?","answer":"Approach: Stream all logs (.log and .log.gz), decompress as needed, then use awk to parse, convert TIMESTAMP to epoch with date -d, keep only entries from the last 24h, and increment counts[BUILDID][hour] where STATUS != 'SUCCESS'.","explanation":"## Why This Is Asked\nTests practical CLI scripting and edge-case handling in production-like log workflows. Candidates justify a compact, robust solution that tolerates missing files and compressed archives.\n\n## Key Concepts\n- POSIX vs GNU utilities compatibility\n- Efficient streaming of mixed plain/compressed logs\n- Time-window calculations and per-key aggregation\n\n## Code Example\n```bash\n# Example: outline of the one-liner approach (exact command may vary by shell)\nnow=$(date +%s); for f in /build/logs/*/*.{log,log.gz}; do [ -f \"$f\" ] && case \"$f\" in *.gz) gzip -dc \"$f\" ;; *) cat \"$f\" ;; esac; done | awk -F'|' -v now=\"$now\" 'function epoch(ts) { cmd=\"date -d \\\"\" ts \"\\\" +%s\"; cmd | getline e; close(cmd); return e } { ts=epoch($2); if(now-ts <= 86400 && $4 != \"SUCCESS\") { build=$1; hour=sprintf(\"%02d\", strftime(\"%H\", ts)); counts[build][hour]++ } } END { for(b in counts) for(h in counts[b]) print b\"|\"h\"|\"counts[b][h] }'\n```\n\n## Edge Cases Handled\n- Missing files gracefully skipped\n- Mixed compressed/uncompressed logs\n- Timestamp parsing across formats\n- Memory-efficient streaming","diagram":null,"difficulty":"beginner","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","IBM"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T05:02:34.376Z","createdAt":"2026-01-14T23:45:04.503Z"},{"id":"q-2253","question":"Scenario: /var/run contains *.pid per daemon (service name == filename). Some daemons crash leaving stale PIDs. Propose a robust one-liner (no external deps) that lists service:pid pairs for all PID files whose PID is not running, tolerant of missing files and spaces in names?","answer":"for f in /var/run/*.pid; do [ -f \\\"$f\\\" ] || continue; pid=$(cat \\\"$f\\\" 2>/dev/null); [ -n \\\"$pid\\\" ] || continue; if [ ! -e \\\"/proc/$pid\\\" ]; then echo \\\"$(basename \\\"$f\\\" .pid):$pid\\\"; fi; done","explanation":"Why This Is Asked: Tests ability to detect stale daemon state using native tools, robust against missing pid files and whitespace in names. Key Concepts: PID file semantics, /proc existence checks, shell safety with globbing, handling spaces in filenames. Follow-up: adapt for systemd pidfiles and atomic cleanup.","diagram":null,"difficulty":"intermediate","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T09:39:00.686Z","createdAt":"2026-01-15T09:39:00.686Z"},{"id":"q-2293","question":"In a Unix environment, logs live under /var/log/app-logs/*.log and rotations *.log.gz. Each line is a JSON object like {\"ts\":\"YYYY-MM-DDTHH:MM:SSZ\",\"svc\":\"name\",\"dur_ms\":1234,\"ev\":\"RUN\"}. Propose a robust one-liner (no external deps beyond standard UNIX tools) to compute, per service, the mean and 95th percentile of dur_ms for the last 2 hours, aggregating across all files and rotations, tolerating missing files and archives?","answer":"Use a single Bash pipeline that reads both *.log and *.log.gz, extracts ts, svc, and dur_ms, converts ts to epoch with TZ=UTC, filters to now-2h, then aggregates per service in awk. Store durations pe","explanation":"## Why This Is Asked\nTests practical UNIX log analytics with JSON lines, mixed rotations, and no external deps.\n\n## Key Concepts\n- Handling mixed log formats and gzipping\n- Time-window filtering with epoch comparisons\n- Per-service aggregation and percentile computation (GNU awk)\n\n## Code Example\n```\n# Pseudocode: read logs (log/*.log and *.log.gz), extract ts,srv,dur_ms, filter 2h window, per-service aggregate to compute mean and 95th percentile\n```\n\n## Follow-up Questions\n- How would you adapt this to compute per-minute percentiles?\n- How would you validate correctness with synthetic logs?","diagram":"flowchart TD\nA[Log Files] --> B{Uncompressed vs.gz}\nB --> C[Parse JSON fields ts, svc, dur_ms]\nC --> D{Time-window check (2h)}\nD --> E[Aggregate by svc]\nE --> F[Compute mean and 95th percentile]\nF --> G[Emit svc, mean, p95]","difficulty":"intermediate","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","IBM","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T10:54:51.056Z","createdAt":"2026-01-15T10:54:51.056Z"},{"id":"q-2396","question":"In a Unix host, multiple services write JSON log lines to /logs/app/*/*.log, rotated hourly to *.log.gz. Each line is {\"ts\":\"YYYY-MM-DDTHH:MM:SSZ\",\"svc\":\"name\",\"lvl\":\"LEVEL\",\"msg\":\"...\"}. Propose a robust one-liner (no external deps beyond standard UNIX tools) that outputs a per-minute histogram of WARN or ERROR events for the last 60 minutes, aggregated across all files and rotations, tolerant of missing files and gz archives, and resilient to out-of-order timestamps within the minute bucket?","answer":"Loop over /logs/app/*/*.log, decompress gz with zcat when needed, stream lines to awk. In awk, extract ts via regex, convert to minute bucket with date -u +%Y-%m-%dT%H:%M -d \"$ts\"; skip invalid; if lv","explanation":"## Why This Is Asked\nTests the ability to design robust log aggregation across rotation boundaries, missing files, and clock quirks in a real-world Unix setup.\n\n## Key Concepts\n- Globbing and rotated logs across .log and .log.gz\n- On-the-fly decompression of gz files with standard tools\n- Parsing JSON-like lines with basic UNIX utilities\n- Per-minute bucketing and aggregation with streaming tools\n- Robust handling of invalid lines and out-of-order timestamps\n\n## Code Example\n```bash\nfor f in /logs/app/*/*.log; do \n  [ -f \"$f\" ] && case \"$f\" in \n    *.gz) zcat \"$f\" ;; \n    *)   cat \"$f\" ;; \n  esac; \ndone | awk -F'\"' '/\"ts\":/ { if (match($0, /\"ts\":\"([^\"]+)\"/, m)) { t=m[1]; if (t ~ /^[0-9]{4}-[0-9]{2}-[0-9]{2}T[0-9]{2}:[0-9]{2}:[0-9]{2}Z$/) { bucket = strftime(\"%Y-%m-%dT%H:%M\", mktime(substr(t,1,4)\" \"substr(t,6,2)\" \"substr(t,9,2)\" \"substr(t,12,2)\" \"substr(t,15,2)\" \"substr(t,18,2)))); if ($0 ~ /\"lvl\":\"(WARN|ERROR)\"/) cnt[bucket]++ } } } END { for (b in cnt) print b, cnt[b] }' | sort\n``` \n\n## Follow-up Questions\n- How would you extend to 24 hours and a fixed windowing strategy? \n- How would timezone and clock skew be accounted for in production dashboards?","diagram":null,"difficulty":"intermediate","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Databricks","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T16:45:47.287Z","createdAt":"2026-01-15T16:45:47.287Z"},{"id":"q-2420","question":"Scenario: In a Unix host, logs live under /var/log/app-logs with daily rotation and filenames like app-<service>-YYYYMMDD.log. Write a robust one-liner (no external deps beyond standard UNIX tools) that prints a total count of lines containing the word 'ERROR' across all log files modified in the last 3 days. The command must gracefully skip missing/unreadable files and work across many services?","answer":"find /var/log/app-logs -type f -name 'app-*-*.log' -mtime -3 -print0 2>/dev/null | xargs -0 cat 2>/dev/null | awk '/ERROR/ {c++} END {print c+0}'","explanation":"## Why This Is Asked\nTests practical ability to craft a robust, portable one-liner using only standard UNIX tools for cross-service log aggregation. It handles file rotation by using mtime, streams via cat, and counts with awk. It tolerates missing/unreadable files and scales with many files.\n\n## Key Concepts\n- find with -mtime, -type, -name, -print0\n- xargs -0 for safe multi-file input\n- cat to concatenate streams\n- awk for counting matches across input\n- redirection to ignore errors for robustness\n\n## Code Example\n```bash\nfind /var/log/app-logs -type f -name 'app-*-*.log' -mtime -3 -print0 2>/dev/null | xargs -0 cat 2>/dev/null | awk '/ERROR/ {c++} END {print c+0}'\n```\n\n## Follow-up Questions\n- How would you get per-service breakdown?\n- How would you extend to include gzipped logs (gz) without external deps?","diagram":"flowchart TD\n  A[Start] --> B[Find last 3 days logs]\n  B --> C[Stream with cat]\n  C --> D[Count with awk]\n  D --> E[Output total]","difficulty":"beginner","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","MongoDB","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T17:42:36.538Z","createdAt":"2026-01-15T17:42:36.539Z"},{"id":"q-2520","question":"In a fleet of Linux hosts, log files sit under /var/log/relay/*/*.log and rotated to *.log.gz. Each line is a JSON object with fields ts (YYYY-MM-DDTHH:MM:SSZ), svc, lat_ms. Write a robust one-liner (no external deps beyond standard UNIX tools) that outputs each service's 95th percentile latency over the last 15 minutes, aggregating across all files and rotations, tolerant of missing/unreadable files and out-of-order lines?","answer":"Stream all logs through a single awk script that reads plain and gzipped files, parses ts with date -d, keeps only lines within the last 15 minutes, collects lat_ms by svc, and on END sorts and emits ","explanation":"## Why This Is Asked\n\nThis question probes the ability to perform robust, cross-file, cross-rotation log analysis using only standard UNIX tools. It requires handling both plain and gzipped inputs, time-window filtering, and percentile aggregation per service, all while tolerating malformed data.\n\n## Key Concepts\n\n- Streaming log processing across rotated files\n- Reading gzipped and plain logs without external deps\n- Time-window filtering with date parsing\n- Percentile computation in awk\n\n## Code Example\n\n```javascript\n// illustrative skeleton: the actual one-liner would embed in a script or be a longer one-liner\nconst sample = 'Parse and aggregate lat_ms per svc within 15m window';\n```\n\n## Follow-up Questions\n\n- How would you extend to multiple percentile targets (e.g., 95th and 99th)?\n- How would you test correctness with synthetic logs and randomized rotations?","diagram":null,"difficulty":"advanced","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Lyft","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T21:31:22.635Z","createdAt":"2026-01-15T21:31:22.636Z"},{"id":"q-2624","question":"In a Linux host running dozens of worker processes inside containers, build a POSIX-friendly watchdog using only standard UNIX tools to detect any worker that has spent uninterruptible sleep (D state) for more than 60 seconds within a 10-minute window and restart such workers with exponential backoff. Use /proc to derive each worker's startup command from /proc/$pid/cmdline and /proc/cgroups to avoid system processes?","answer":"Use a time-window watchdog: every 2s for 10m, scan /proc to map PID to State and accumulate seconds in D state. After the window, restart offenders with backoff (5s, 15s, 30s) using the process cmdlin","explanation":"## Why This Is Asked\nThis tests OS internals, container boundaries, and tool discipline.\n\n## Key Concepts\n- /proc parsing with POSIX tools\n- D-state dwell time accumulation\n- Windowed aggregation over time\n- Restart with backoff and startup command reconstruction\n- Excluding system processes via /proc/[pid]/cgroup\n\n## Code Example\n\n```bash\n# Outline of approach (not full implementation)\n```\n\n## Follow-up Questions\n- How would you unit-test this watcher in a multi-tenant cluster?\n- How would you handle PIDs recycled within the window?\n","diagram":null,"difficulty":"advanced","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T04:08:56.983Z","createdAt":"2026-01-16T04:08:56.984Z"},{"id":"q-2738","question":"In a Unix host, per-job logs are under /srv/ci/jobs/<job_id>/logs, rotated daily with filenames log-YYYYMMDD.log. A symlink log-current.log points to the current day’s log file for each job. Write a robust one-liner (no external deps beyond standard UNIX tools) that prints the count of lines containing 'STATUS=FAILED' across all current-day log files for all jobs, gracefully handling missing files and broken links?","answer":"One robust approach is: find -L /srv/ci/jobs -name log-current.log -type f -print0 | xargs -0 -r grep -h 'STATUS=FAILED' | wc -l. This follows symlinks, ignores missing files, and counts only lines wi","explanation":"The candidate should propose a pipeline that safely traverses job dirs, follows symlinks where needed, and uses null-separated streams to handle spaces in paths. They should justify using -L with find to follow log-current.log, and using -print0/xargs -0 to prevent tokenization issues, culminating in wc -l for a total.","diagram":null,"difficulty":"beginner","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T09:47:40.582Z","createdAt":"2026-01-16T09:47:40.582Z"},{"id":"q-2781","question":"In a Unix host, logs live under /var/log/services/*/*.log and are rotated hourly; some files are .log, others .log.gz. Each line is of the form [YYYY-MM-DD HH:MM:SS] <service>: <message>. Write a robust one-liner (no external deps beyond standard UNIX tools) that prints a per-service histogram of the number of lines containing the word ERROR in the last 24 hours, aggregating across all files and rotations. Must handle missing or unreadable files and gzipped archives, and work with spaces in paths?","answer":"One viable solution: find /var/log/services -type f -mtime -1 -print0 | while IFS= read -r -d '' f; do case \"$f\" in *.gz) zcat \"$f\" ;; *) cat \"$f\" ;; esac; done | awk '/ERROR/ && match($0, /^\\[[0-9]{4","explanation":"Why this is asked: tests ability to craft robust shell one-liners that span plain and compressed logs, handle spaces, and aggregate by service. Key concepts: find -mtime, print0, safe looping, zcat for gz, awk for parsing and aggregation. Code Example: (see block). Follow-up: how would you adapt to count ERROR codes with 5xx prefixes, or test locally with mock logs?","diagram":null,"difficulty":"beginner","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Robinhood","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T11:39:00.928Z","createdAt":"2026-01-16T11:39:00.928Z"},{"id":"q-3086","question":"In a Unix host, a policy requires that any file within data/ under /shared/projects must be group-writable and have the group set to the project's primary group, but only for files changed in the last 24 hours. Write a robust one-liner (no external deps beyond standard UNIX tools) to identify files that violate this policy, ignoring unreadable files and handling spaces in names?","answer":"find /shared/projects -type f -path '*/data/*' -mtime -1 ! -perm -g+w -print0 2>/dev/null | while IFS= read -r -d '' f; do printf 'VIOLATION: %s\n' \"$f\"; stat -c '%A %G %n' \"$f\"; done","explanation":"## Why This Is Asked\n\nTests ability to enforce security policies using standard Unix tools, including robust file discovery, path handling, and error management.\n\n## Key Concepts\n\n- `find` with `-mtime` for filtering by modification time\n- `-path` for directory pattern matching\n- `-perm` to check group write permissions\n- `-print0` and `read -d ''` for handling filenames with spaces\n- `stat` for displaying file permissions and group information\n- Error redirection to ignore unreadable directories\n\n## Code Example\n\n```bash\nfind /shared/projects -type f -path '*/data/*' -mtime -1 ! -perm -g+w -print0 2>/dev/null | while IFS= read -r -d '' f; do printf 'VIOLATION: %s\\n' \"$f\"; stat -c '%A %G %n' \"$f\"; done\n```\n\n## How It Works\n\n1. `find /shared/projects` searches the target directory recursively\n2. `-type f` restricts to regular files only\n3. `-path '*/data/*'` matches files within any data subdirectory\n4. `-mtime -1` finds files modified in the last 24 hours\n5. `! -perm -g+w` identifies files without group write permissions\n6. `-print0` uses null bytes to handle spaces in filenames\n7. `2>/dev/null` suppresses permission errors for unreadable files\n8. The while loop reads each file and displays violation details with current permissions and group","diagram":"flowchart TD\n  A[Start] --> B[Find files in /shared/projects/*/data/*]\n  B --> C{Changed in last 24h?}\n  C -->|Yes| D{Has group-w write?}\n  D -->|No| E[Violation detected]\n  D -->|Yes| F[OK]\n  E --> G[Print details]\n  F --> H[End]","difficulty":"beginner","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","MongoDB","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T05:09:57.945Z","createdAt":"2026-01-17T02:10:43.765Z"},{"id":"q-3134","question":"In a Unix host, backups live under /srv/backups; each backup is named backup-YYYYMMDD.tar.gz and contains a data/ directory. Write a robust one-liner (no external deps beyond standard UNIX tools) that prints the total number of unique .log files across all backups modified in the last 7 days. The command should gracefully skip unreadable archives and ignore non-log files?","answer":"One-liner: find /srv/backups -name 'backup-*.tar.gz' -mtime -7 -print0 2>/dev/null | xargs -0 -I{} sh -c 'tar -tzf \"{}\" --wildcards \"*.log\" 2>/dev/null' | sort -u | wc -l. This uses tar to enumerate l","explanation":"## Why This Is Asked\n\nTests ability to orchestrate standard UNIX tools across many archives, handling spaces and errors.\n\n## Key Concepts\n\n- tar -tzf and --wildcards\n- find -mtime\n- xargs -0 with placeholder\n- sort -u for deduping\n- 2>/dev/null for resilience\n\n## Code Example\n\n```bash\nfind /srv/backups -name 'backup-*.tar.gz' -mtime -7 -print0 2>/dev/null | xargs -0 -I{} sh -c 'tar -tzf \"{}\" --wildcards \"*.log\" 2>/dev/null' | sort -u | wc -l\n```\n\n## Follow-up Questions\n\n- How would you adapt if backups could be .tar (uncompressed) as well?\n- How would you exclude logs under a certain path like data/tmp/ ?","diagram":null,"difficulty":"beginner","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Google","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T04:14:49.463Z","createdAt":"2026-01-17T04:14:49.463Z"},{"id":"q-3206","question":"In a Unix host, logs live under /var/log/ops/* with hourly rotation to *.log and sometimes *.log.gz. Lines are either plain text LEVEL: message or JSON like {ts: ISO8601, svc: name, lvl: ERROR}, and multi-line messages may follow a line that starts with a timestamp. Propose a robust one-liner (no external deps beyond standard UNIX tools) that prints a per-minute histogram of ERROR events for the last 90 minutes, aggregating across all files and rotations, tolerant of missing files and gz archives, and robust to mixed formats?","answer":"Plan: a streaming command reading all logs (*.log and *.log.gz) via zcat/cat, piped to awk. The awk normalizes timestamps from JSON ts or ISO header, filters ERROR lines, buckets by minute, and mainta","explanation":"## Why This Is Asked\nTests ability to design robust log aggregation across mixed formats, gzipped, and rotated logs under load.\n\n## Key Concepts\n- Pipeline robustness and fault tolerance\n- Timestamp normalization for JSON and plain text\n- Per-minute bucketing and sliding windows\n- Handling multi-line records and unreadable files\n\n## Code Example\n\n## Follow-up Questions\n- How would you adapt to 5-minute buckets? \n- How would you unit-test with synthetic logs?","diagram":null,"difficulty":"intermediate","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Microsoft","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T06:56:52.176Z","createdAt":"2026-01-17T06:56:52.176Z"},{"id":"q-3443","question":"In a Unix host, logs live under /var/log/trace/* with hourly rotation; some files end in .log, others .log.gz. Each line is either [YYYY-MM-DD HH:MM:SS] svc: duration=NNNms or JSON {\"ts\":\"YYYY-MM-DDTHH:MM:SSZ\",\"svc\":\"name\",\"dur_ms\":NNN}. Propose a robust one-liner (no external deps beyond standard UNIX tools) to produce a per-service histogram of total request duration binned into 2-minute intervals for the last 60 minutes, aggregating across all files and rotations, tolerant of missing/unreadable files and out-of-order timestamps?","answer":"Loop over /var/log/trace/*.{log,log.gz}, decompress with zcat when needed, parse either [YYYY-MM-DD HH:MM:SS] or JSON ts, extract dur_ms, compute bucket = int(epoch(ts)/120), accumulate totals per svc","explanation":"## Why This Is Asked\nTests ability to design a robust, real-world log analysis task across mixed formats and rotated archives.\n\n## Key Concepts\n- Parsing plain and JSON logs\n- Handling gzipped rotated files without extra deps\n- Time bucketing and out-of-order events\n- Resilience to missing/unreadable files\n\n## Code Example\n```javascript\n// Implementation outline (not the full solution)\n```\n\n## Follow-up Questions\n- How would you scale this for millions of logs?\n- How would you test with synthetic data and verify correctness?","diagram":"flowchart TD\n  A[Start] --> B[Scan files]\n  B --> C[Parse lines]\n  C --> D[Bucket by 2-min]\n  D --> E[Sum per svc]\n  E --> F[Output histogram]\n  F --> G[End]","difficulty":"intermediate","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["NVIDIA","Netflix","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T16:40:40.287Z","createdAt":"2026-01-17T16:40:40.287Z"},{"id":"q-3499","question":"In a Unix host, multiple services write JSON log lines under /logs/app/*/*.log and are rotated hourly to *.log.gz. Each line is {\"ts\":\"YYYY-MM-DDTHH:MM:SSZ\",\"svc\":\"<name>\",\"lvl\":\"LEVEL\",\"msg\":\"...\"}. Write a robust one-liner (no external deps beyond standard UNIX tools) that prints a per-service idle time metric: time since last log line for each service within the last hour, flagging services idle for more than 2 minutes. Handle gz archives, unreadable files, and out-of-order timestamps?","answer":"Approach: read all files (uncompressed and gzipped) with a single pass, parse ts and svc using lightweight regex in awk (no jq), convert ts to epoch, track per-service last_seen, compute age = now - l","explanation":"## Why This Is Asked\n\nTests ability to design a minimal, robust monitoring query using basic UNIX tools without dependencies, and to reason about time-windowed data across rotated logs.\n\n## Key Concepts\n\n- Robust globbing and gzip handling\n- Lightweight JSON parsing with awk\n- Per-service state tracking and time-window logic\n- Handling out-of-order/timestamp skew\n\n## Code Example\n\n```javascript\n# Idea for one-liner (not runnable here):\n# find /logs/app -type f \\( -name '*.log' -o -name '*.log.gz' \\) -print0 | \\\n#   xargs -0 -I{} sh -c '...'\n```\n\n## Follow-up Questions\n\n- How would you adapt this for a rolling window of 5 minutes with a trailing watermark?\n- What are edge cases if clocks drift or NTP is off?","diagram":"flowchart TD\n  A[Collect files] --> B[Decompress/Read]\n  B --> C[Parse ts & svc]\n  C --> D[Update last_seen per svc]\n  D --> E[Compute age; idle flag]\n  E --> F[Output results]","difficulty":"intermediate","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Netflix","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T18:54:04.134Z","createdAt":"2026-01-17T18:54:04.134Z"},{"id":"q-3573","question":"You are deploying a per-host log-tailer for thousands of services across a fleet. Logs live under /var/log/services/<svc>/*.log and rotate daily; older files may be *.log.gz. The on-host agent must stream new lines to a central sink with at-least-once delivery, tolerate missing/unreadable files, and stay correct during concurrent rotations. Describe design, data flow, and failure modes; outline an implementation strategy using only standard UNIX tools?","answer":"Design a per-host log-tailer using inotify/fanotify to monitor new and rotated files, track per-file offsets (inode+offset) in a local manifest, decompress *.log.gz files on-the-fly with gzip -d or zcat, and implement at-least-once delivery with retry logic and backpressure management to a central sink.","explanation":"## Why This Is Asked\n\nEvaluates practical OS knowledge: file watching, log rotation, compression, and reliable delivery at scale.\n\n## Key Concepts\n- inotify and fanotify for file system monitoring\n- Per-file inode and offset tracking for state persistence\n- Handling gzipped rotations (zcat/gzip)\n- Buffering and backpressure to sink\n- Idempotent replay and deduplication on restart\n\n## Code Example\n```bash\n# Minimal sketch of watch loop (not a full solution)\ninotifywait -m -e create,close_write,move /var/log/services\n```\n\n## Follow-up Questions\n- How to test rotation edge cases?\n- How to scale the agent across thousands of services?\n- What monitoring and alerting would you implement?","diagram":null,"difficulty":"advanced","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","NVIDIA","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T05:42:58.410Z","createdAt":"2026-01-17T21:38:25.742Z"},{"id":"q-3583","question":"Scenario: A Unix host stores executables under /opt/apps with nested folders. Some files have setuid/setgid bits. Write a robust one-liner (no external deps beyond standard UNIX tools) that lists all files modified in the last 24 hours with setuid or setgid bits, printing per file: permissions, owner:group, and full path. The command must skip unreadable files and handle spaces in paths?","answer":"find /opt/apps -type f -mtime -1 -perm /6000 -print0 2>/dev/null | while IFS= read -r -d '' f; do stat -c '%A %U:%G %n' \"$f\"; done","explanation":"## Why This Is Asked\nThis question tests advanced shell scripting skills, requiring candidates to create robust, portable one-liners that handle edge cases like spaces in filenames and unreadable files using standard Unix tools.\n\n## Key Concepts\n- **find command**: Uses `-mtime -1` to filter files modified within the last 24 hours\n- **Permission checking**: Employs `-perm /6000` to detect files with setuid (4000) or setgid (2000) bits\n- **Safe filename handling**: Uses `-print0` with `read -d ''` to properly handle spaces and special characters in paths\n- **Error handling**: Redirects stderr to `/dev/null` to skip unreadable files\n- **Formatted output**: Leverages `stat -c` to display permissions, owner:group, and full path\n\n## Code Example\n```bash\nfind /opt/apps -type f -mtime -1 -perm /6000 -print0 2>/dev/null | while IFS= read -r -d '' f; do\n  stat -c '%A %U:%G %n' \"$f\"\ndone\n```\n\n## Follow-up Questions\n- How would you modify this command to search multiple directories?\n- What changes would be needed for BSD systems where `stat` syntax differs?\n- How could you extend this to also show file sizes?","diagram":null,"difficulty":"beginner","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Bloomberg","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T05:35:05.500Z","createdAt":"2026-01-17T22:34:06.936Z"},{"id":"q-3617","question":"On a Linux host with many long-running services, write a robust one-liner (no external deps beyond standard UNIX tools) that lists the top 5 processes by number of open file descriptors. Only include processes that started at least 24 hours ago. Output: PID, UID, user, FD count, executable path, and a human-friendly age (e.g., 3d4h)? Must handle unreadable pids and spaces in cmdlines. Use /proc and standard tools?","answer":"Enumerate /proc/[0-9]*; for each pid with readable /proc/$pid/fd, read starttime from /proc/$pid/stat and convert to age = starttime / CLK_TCK; skip age < 86400. Count fd in /proc/$pid/fd; map UID to username with getent; output formatted results sorted by FD count.","explanation":"## Why This Is Asked\n\nTests ability to reason about /proc, process metadata, and resource attribution in large systems; probes edge cases (spaces in paths, unreadable pids) and performance implications of scanning /proc.\n\n## Key Concepts\n\n- /proc filesystem and permissions\n- Counting file descriptors per process via /proc/[pid]/fd\n- starttime in /proc/[pid]/stat and CLK_TCK conversion\n- Mapping UID to username\n- Robust error handling and portability\n\n## Code Example\n\n```bash\n#!/bin/bash\nh=$(getconf CLK_TCK)\nfor p in /proc/[0-9]*; do\n  [ -r \"$p/fd\" ] || continue\n  s=$(awk '{print $22}' \"$p/stat\" 2>/dev/null) || continue\n  age=$((($(date +%s) - $(awk '{print $21}' \"$p/stat\" 2>/dev/null)/$h - $(cat \"$p/uptime\" 2>/dev/null | awk '{print $1}'))))\n  [ $age -ge 86400 ] || continue\n  fd=$(ls \"$p/fd\" 2>/dev/null | wc -l)\n  uid=$(stat -c %u \"$p\" 2>/dev/null)\n  user=$(getent passwd \"$uid\" 2>/dev/null | cut -d: -f1)\n  cmd=$(tr '\\0' ' ' < \"$p/cmdline\" 2>/dev/null | sed 's/ *$//')\n  printf \"%d %d %s %d %s %s\\n\" \"${p##*/}\" \"$uid\" \"$user\" \"$fd\" \"$cmd\" \"$(awk -v a=$age 'BEGIN{d=int(a/86400); h=int((a%86400)/3600); printf \"%dd%dh\", d, h}')\"\ndone | sort -k4 -nr | head -5 | column -t\n```\n\nThis solution handles unreadable PIDs gracefully, properly counts file descriptors, converts process start time to human-readable age, and outputs the top 5 processes by FD count with all requested fields formatted cleanly.","diagram":"flowchart TD\n  A[Start] --> B[Enumerate /proc]\n  B --> C{PID readable?}\n  C -- Yes --> D[Read stat starttime]\n  D --> E[Compute age]\n  E -- age>=24h --> F[Count /proc/$pid/fd]\n  F --> G[Resolve uid/user/exe]\n  G --> H[Emit line]\n  H --> I[Sort, head -5]\n  C -- No --> J[Skip]","difficulty":"advanced","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","Oracle","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T05:14:33.957Z","createdAt":"2026-01-17T23:40:09.633Z"},{"id":"q-3765","question":"In a Unix host, there is a directory /var/experiments containing subdirectories for each experiment. Each subdir may contain a file metrics.txt with lines that are either PASS or FAIL; some subdirs may contain metrics.txt.gz instead. Write a robust one-liner (no external dependencies beyond standard UNIX tools) that prints per-experiment fail rate as: <experiment>: <fails>/<total> (<percent>%), handling missing files and gzipped files, and across many experiments (spaces in names)?","answer":"for d in /var/experiments/*; do f=\"$d/metrics.txt\"; gz=\"$d/metrics.txt.gz\"; if [ -f \"$f\" ]; then t=$(wc -l < \"$f\"); e=$(grep -c '^FAIL' \"$f\"); elif [ -f \"$gz\" ]; then t=$(gzip -cd \"$gz\" | wc -l); e=$(","explanation":"## Why This Is Asked\nTests ability to compose robust shell pipelines handling gzipped files and missing inputs. \n\n## Key Concepts\n- Directory traversal with globbing; file presence checks; gzip decompression with standard tools; integer arithmetic in shell.\n\n## Code Example\n```bash\nfor d in /var/experiments/*; do\n  f=\"$d/metrics.txt\"; gz=\"$d/metrics.txt.gz\"\n  if [ -f \"$f\" ]; then t=$(wc -l < \"$f\"); e=$(grep -c '^FAIL' \"$f\");\n  elif [ -f \"$gz\" ]; then t=$(gzip -cd \"$gz\" | wc -l); e=$(gzip -cd \"$gz\" | grep -c '^FAIL');\n  else t=0; e=0; fi\n  if [ \"$t\" -gt 0 ]; then p=$(( e * 100 / t )); echo \"$(basename \"$d\"): $e/$t ($p%)\"; fi\ndone\n```\n\n## Follow-up Questions\n- How would you adapt for mixed case-sensitive values like FAIL and FAILURES?\n- How would you parallelize this for many experiments while preserving stable output order?","diagram":null,"difficulty":"beginner","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google","Hugging Face"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T08:46:04.010Z","createdAt":"2026-01-18T08:46:04.010Z"},{"id":"q-3836","question":"Scenario: A Unix host runs dozens of services that log to daily rotated plain-text logs under /var/log/app-logs/app-<service>-YYYYMMDD.log, plus a real-time stream via a named pipe at /var/log/app-logs/pipe.log which receives JSON entries like {ts: ISO8601, svc: name, lvl: ERROR, msg: ...}. Some services also compress old logs to .gz. Write a robust one-liner (no external deps beyond standard UNIX tools) that outputs a per-minute histogram of ERROR events for the last 60 minutes, aggregating across both file logs and the live pipe, tolerant of missing/unreadable files and gzipped archives, and resilient to mixed formats (plain text and JSON)?","answer":"One-liner approach: read both plain logs and gzips, parse lines for either JSON {'ts':'...','svc':'...','lvl':'ERROR',...} or text containing 'ERROR', normalize timestamp to epoch, filter last 60 minu","explanation":"## Why This Is Asked\n\nInterviews test ability to craft resilient one-liners that mix formats, rotations, and streaming input. It emphasizes time-windowing, gzip handling, and incremental aggregation with standard tools.\n\n## Key Concepts\n- Log format normalization across JSON and plain text\n- Time-window filtering with date/epoch\n- Combining multiple sources (files + pipe)\n- Robustness to missing/unreadable files\n- Streaming compression (gzip) and rotations\n\n## Code Example\n```javascript\n// Implementation example not shown here as this is the candidate's task\n```\n\n## Follow-up Questions\n- How would you adapt this for a multi-node cluster?\n- How would you test edge cases like clock skew?","diagram":"flowchart TD\n  A[Start] --> B[Collect sources]\n  B --> C[Parse lines (JSON/text)]\n  C --> D[Normalize time to epoch]\n  D --> E[Filter last 60m]\n  E --> F[Bucket by service + minute]\n  F --> G[Output histogram]","difficulty":"advanced","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Meta","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T11:30:55.314Z","createdAt":"2026-01-18T11:30:55.314Z"},{"id":"q-3904","question":"On a Unix host, multiple services write logs under /var/log/app-logs/service-*/ and a central JSON log at /var/log/central/combined.log.gz. Each plain-text line in app-logs uses [YYYY-MM-DD HH:MM:SS] <service>: <msg>, while the central log has JSON lines like {\"ts\":\"ISO\",\"svc\":\"name\",\"lvl\":\"LEVEL\",\"msg\":\"...\"}. Write a robust one-liner (no external deps beyond standard UNIX tools) that prints a per-service count of ERROR events in the last 2 hours, aggregating across all files and rotations, tolerating missing/unreadable files and gz archives, and handling mixed formats?","answer":"Proposed one-liner approach: scan both plain and gz logs, extract timestamps from text lines or JSON ts, filter to last 2 hours, and accumulate per-service ERROR counts with an awk associative array; ","explanation":"## Why This Is Asked\nTests ability to handle mixed log formats, gzipped archives, and resilience to missing files with a simple, scalable one-liner.\n\n## Key Concepts\n- POSIX tools: find/grep/awk/zgrep\n- Parsing multiple formats (text and JSON)\n- Time window filtering and per-service aggregation\n\n## Code Example\n```javascript\n// Conceptual example: build per-service counts from mixed sources\nconst counts = {};\n// parse lines from both formats, update counts['service'] for ERROR\n```\n\n## Follow-up Questions\n- How would you adapt this to scale across thousands of log files?\n- How would you test this for time zones and clock skew issues?","diagram":null,"difficulty":"beginner","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","MongoDB","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T14:35:41.069Z","createdAt":"2026-01-18T14:35:41.070Z"},{"id":"q-3976","question":"Scenario: On a Unix host, logs sit in /var/log/app-logs with two formats: app-<service>-YYYYMMDD.log and rotated .log.gz. Write a robust one-liner (no external deps beyond standard UNIX tools) that prints the total number of non-empty lines across all files modified today. The command should gracefully skip unreadable files and work across many services?","answer":"Use a single Bash one-liner: collect today-modified files (log and gz), read with zcat for gz, filter non-empty lines, and count. For example:\nbash -lc 'set -o pipefail; find /var/log/app-logs -daysta","explanation":"## Why This Is Asked\nTests robust file handling, gz support, and today-filtering in a single pipeline.\n\n## Key Concepts\n- find with -daystart -mtime 0\n- -print0 and while read -r -d '' for spaces\n- zcat for gz, cat for uncompressed\n- skip unreadable with redirections\n\n## Code Example\n```bash\n# see answer for the one-liner\n```\n\n## Follow-up Questions\n- How would you adapt to per-service totals?\n- How to safely run in a cron job?","diagram":null,"difficulty":"beginner","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Google","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T18:38:02.888Z","createdAt":"2026-01-18T18:38:02.890Z"},{"id":"q-4011","question":"In a Unix host, logs live under /logs/platform/*/*.log and are rotated hourly into *.log.gz. Each line is a JSON object: {\"ts\":\"YYYY-MM-DDTHH:MM:SSZ\",\"svc\":\"name\",\"lvl\":\"LEVEL\",\"msg\":\"...\"}. Provide a robust one-liner (no external deps beyond standard UNIX tools) that outputs a per-service, per-minute histogram of ERROR events for the last 60 minutes, aggregating across all files and rotations, tolerant of missing/unreadable files and gz archives, and deduplicating lines that may appear in multiple rotated files via inode tracking?","answer":"I would build a single streaming pipeline: enumerate /logs/platform/*/*.{log,log.gz}, skip seen inodes to dedupe across rotations, decompress with zcat, filter lines with ERROR, extract ts and svc fro","explanation":"Why This Is Asked: tests practical Unix skills. Key Concepts: log rotation, gz handling, JSON parsing with awk, time bucketing, inode-based dedup. Code Example: Sketch: for f in /logs/platform/*/*.{log,log.gz}; do ...; done | awk '...'. Follow-up: BSD/macOS date differences; scaling to many hosts.","diagram":"flowchart TD\n  A[Collect files] --> B[Decompress if gz]\n  B --> C[Parse JSON lines]\n  C --> D[Extract ts and svc]\n  D --> E[Bucket by minute]\n  E --> F[Aggregate per svc]\n  F --> G[Output histogram]","difficulty":"intermediate","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Anthropic","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T19:30:51.551Z","createdAt":"2026-01-18T19:30:51.551Z"},{"id":"q-4028","question":"In a Unix host, logs live under /logs/** and rotate hourly into *.log and *.log.gz. Each line is either 'YYYY-MM-DD HH:MM:SS <LEVEL>: <text>' or JSON with fields ts, lvl, msg. Write a robust one-liner (no external deps beyond standard UNIX tools) that prints the number of unique ERROR messages observed in the last 24 hours across all files, across both formats, tolerant of missing/unreadable files and gz archives?","answer":"find /logs -type f \\( -name \"*.log\" -o -name \"*.log.gz\" \\) -mtime -1 -print0 | xargs -0 -I{} sh -lc 'gzip -dc \"{}\" 2>/dev/null || cat \"{}\" 2>/dev/null' | grep -i 'error' | sort -u | wc -l","explanation":"## Why This Is Asked\nThis question tests the ability to handle mixed log formats, file rotation, and compressed archives in a concise, robust manner using standard UNIX tools.\n\n## Key Concepts\n- **File Discovery**: Locate log files modified within the last 24 hours using `find` with `-mtime -1`\n- **Content Streaming**: Handle both uncompressed and compressed logs using conditional decompression\n- **Error Filtering**: Apply case-insensitive filtering across different log formats\n- **Deduplication**: Remove identical error messages and count unique occurrences\n- **Error Tolerance**: Gracefully handle missing files, unreadable archives, and decompression failures\n\n## Command Breakdown\n1. `find /logs -type f \\( -name \"*.log\" -o -name \"*.log.gz\" \\) -mtime -1` - Discover log files modified in the last 24 hours\n2. `-print0 | xargs -0` - Safely handle filenames with spaces or special characters\n3. `sh -lc 'gzip -dc \"{}\" 2>/dev/null || cat \"{}\" 2>/dev/null'` - Decompress gz files or read plain files, suppressing errors\n4. `grep -i 'error'` - Filter for error messages case-insensitively\n5. `sort -u | wc -l` - Remove duplicates and count unique error messages","diagram":null,"difficulty":"beginner","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","LinkedIn","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T07:41:44.543Z","createdAt":"2026-01-18T20:40:17.491Z"},{"id":"q-4175","question":"Scenario: A Unix host aggregates logs for dozens of services under /var/log/services/<service>/. Logs rotate hourly; some are plain .log and others .log.gz. Lines are either a plain timestamped message like [YYYY-MM-DD HH:MM:SS] LEVEL: text or a JSON object {\"ts\": \"YYYY-MM-DDTHH:MM:SSZ\", \"svc\": \"service\", \"lvl\": \"ERROR\", \"msg\": \"...\"}. A per-service live feed is written to /var/log/services/<service>/live.log as JSON lines. Write a robust pipeline (standard UNIX tools only) that outputs a per-service, per-minute histogram of ERROR events for the last 60 minutes, aggregating across files, archives, and the live feed, tolerating unreadable files and spaces in paths?","answer":"Stream all relevant logs in one pass: read both plain and JSON lines from /var/log/services and gzipped archives, plus per-service live streams; normalize timestamps (text: [YYYY-MM-DD HH:MM:SS], JSON","explanation":"## Why This Is Asked\nTests ability to design robust log pipelines at scale with mixed formats, rotation, and live data.\n\n## Key Concepts\n- Cross-format parsing (text and JSON)\n- Time-window bucketing per minute\n- Rotated and gzipped files\n- Inode-based dedup across rotations\n- Live data integration\n\n## Code Example\n\n```bash\n# skeleton Example\nfind /var/log/services -type f -name '*.log*' -print0 | xargs -0 -I{} sh -c 'zcat \"$1\" 2>/dev/null || cat \"$1\" 2>/dev/null' -- {}\n```\n\n## Follow-up Questions\n- How would you scale to 10k services?\n- How would you unit-test the windowing logic?","diagram":"flowchart TD\n  Ingest[Ingest logs & live streams] --> Parse[Parse timestamps]\n  Parse --> Bucket[Bucket by service & minute]\n  Bucket --> Count[Count ERROR events]\n  Count --> Output[Output histogram]","difficulty":"advanced","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T06:54:31.062Z","createdAt":"2026-01-19T06:54:31.063Z"},{"id":"q-4213","question":"In a Unix host, /data/projects contains thousands of files across nested dirs; some filenames contain spaces. Write a robust one-liner (no external deps beyond standard UNIX tools) that prints the five most common file extensions among files modified in the last 7 days, printing extension and count, ignoring directories and unreadable files?","answer":"One-liner approach: enumerate files modified in the last 7 days with find, handle spaces via -print0, extract the extension from the basename, skip files with no extension, and tally counts with sort ","explanation":"Why This Is Asked\n- Tests ability to craft reliable one-liners around file discovery and parsing. \n\nKey Concepts\n- find usage: -type f, -mtime, -print0\n- safe parsing with -print0 and read -d ''\n- basename and extension extraction\n- handle no-extension cases gracefully\n- sort/uniq for top counts\n\nCode Example\n```bash\n# Example approach (not required in answer field)\nfind /data/projects -type f -mtime -7 -print0 2>/dev/null | while IFS= read -r -d '' f; do base=\"${f##*/}\"; ext=\"${base##*.}\"; [ \"$base\" = \"$ext\" ] && continue; echo \"$ext\"; done | sort | uniq -c | sort -nr | head -n 5\n```\n\nFollow-up Questions\n- How would you adapt for archives or hidden files?\n- How would you handle files with multiple dots in their names?","diagram":"flowchart TD\n  Start --> FindFiles\n  FindFiles --> ProcessBasename\n  ProcessBasename --> ExtractExt\n  ExtractExt --> CountExts\n  CountExts --> OutputTop5","difficulty":"beginner","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","LinkedIn","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T08:55:18.973Z","createdAt":"2026-01-19T08:55:18.973Z"},{"id":"q-4271","question":"In a Unix host, logs live under /var/logs/services/*/*.log and *.log.gz, rotated hourly. Two formats exist: (a) plain lines 'YYYY-MM-DD HH:MM:SS LEVEL: message', (b) JSON lines '{\"ts\":\"YYYY-MM-DDTHH:MM:SSZ\",\"svc\":\"name\",\"lvl\":\"LEVEL\",\"msg\":\"...\"}'. Write a robust one-liner (no external deps beyond standard UNIX tools) that prints the top 3 services by total count of ERROR messages in the last 12 hours, aggregating across both formats and all rotated files, and gracefully skipping unreadable files?","answer":"Strong answer outline: a single pipeline that uses find to gather *.log and *.log.gz, zcat for gz, and awk to parse both formats, convert timestamps to epoch, filter to the last 12 hours, tally ERROR ","explanation":"## Why This Is Asked\nTests ability to unify multi-format logs and operate across rotated files with standard tools, a common on-call need.\n\n## Key Concepts\n- Cross-format parsing (plain and JSON)\n- Timestamp normalization to epoch\n- Robust file discovery and error handling\n- Per-service aggregation and top-k output\n\n## Code Example\n```bash\n# sketch of approach (full one-liner too long to fit here)\nfind /var/logs/services -type f \\( -name '*.log' -o -name '*.log.gz' \\) -print0 | \\\n  xargs -0 -I{} sh -lc 'case \"$1\" in *.gz) zcat \"$1\" ;; *) cat \"$1\" ;; esac' | \\\n  awk -v thr=$(date -d '12 hours ago' +%s) '{...} END{...}'\n```\n\n## Follow-up Questions\n- How would you adapt for concurrent decompression?\n- How would you test with synthetic logs to validate correctness?","diagram":"flowchart TD\n  A[Input Logs] --> B[Handle Formats]\n  B --> C[Parse Timestamps]\n  C --> D[Aggregate by Service]\n  D --> E[Output Top 3]","difficulty":"beginner","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T11:01:25.004Z","createdAt":"2026-01-19T11:01:25.004Z"},{"id":"q-4344","question":"On a Linux host serving many services, dangling (deleted) file handles can cause resource leaks. Create a robust one-liner (no external deps beyond standard UNIX tools) that prints, per process, any open file descriptor that points to a path that no longer exists on disk. Output: PID, process name, FD number, and the target path as read by readlink (including ' (deleted)' if present). Handle spaces in names and permissions gracefully?","answer":"for fd in /proc/*/fd/*; do pid=$(basename $(dirname \"$fd\")); if [ -L \"$fd\" ]; then tgt=$(readlink \"$fd\" 2>/dev/null); if echo \"$tgt\" | grep -q ' (deleted)'; then comm=$(ps -p \"$pid\" -o comm= 2>/dev/nu","explanation":"## Why This Is Asked\nTriage and reason about deleted-but-open files on a live host.\n\n## Key Concepts\n- /proc fd namespace, readlink behavior for deleted files, permission-safe enumeration, robust printing.\n- Handling spaces in PIDs/comm names; forgiving on transient processes.\n\n## Code Example\n```bash\nfor fd in /proc/*/fd/*; do\n  pid=$(basename \"$(dirname \"$fd\")\")\n  if [ -L \"$fd\" ]; then\n    tgt=$(readlink \"$fd\" 2>/dev/null)\n    if echo \"$tgt\" | grep -q ' (deleted)'; then\n      comm=$(ps -p \"$pid\" -o comm= 2>/dev/null)\n      printf '%s\\t%s\\t%s\\t%s\\n' \"$pid\" \"$comm\" \"${fd##*/}\" \"$tgt\"\n    fi\n  fi\ndone 2>/dev/null\n```\n\n## Follow-up Questions\n- How would you adapt this to handle deleted files that are not yet reported in readlink?\n- How would you integrate this into a periodic health check with minimal overhead?","diagram":"flowchart TD\n  A[Enumerate /proc/*/fd/*] --> B[Filter symlinks]\n  B --> C[readlink; detect ' (deleted)']\n  C --> D[Query process name]\n  D --> E[Print PID, name, fd, path]","difficulty":"advanced","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Hashicorp","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T14:53:09.806Z","createdAt":"2026-01-19T14:53:09.806Z"},{"id":"q-4605","question":"In a Unix host, logs live under /srv/logs/app/ with files app-YYYY-MM-DD.log and nightly rotation to app-YYYY-MM-DD.log.gz. Each line is either a plain log line like YYYY-MM-DD HH:MM:SS - svc: message or a JSON object {\"ts\":\"YYYY-MM-DD HH:MM:SS\",\"svc\":\"name\",\"lvl\":\"LEVEL\",\"msg\":\"...\"}. Write a robust one-liner (no external deps beyond standard UNIX tools) that prints, for every service, the number of unique error messages seen in the last 60 minutes, across all files and rotations, deduplicating identical messages per service, and tolerant of unreadable files?","answer":"Collect both plain and JSON log lines from app-*.log and app-*.log.gz, filter to the last 60 minutes, parse both formats to extract svc and msg, then for each service count distinct messages and outpu","explanation":"## Why This Is Asked\nTests ability to merge heterogeneous log formats, handle compressed files, enforce a time window, and deduplicate per service—common in production observability tasks.\n\n## Key Concepts\n- Robust multi-format parsing (plain and JSON)\n- Reading gzipped logs transparently\n- Time-window filtering via epoch comparison\n- Per-service deduplication and aggregation\n\n## Code Example\n\n```bash\n# (Conceptual outline; exact one-liner will combine find, zcat/cat, and awk to parse both formats, filter by last 60m, deduplicate per svc/msg, and print top-5)\nfind /srv/logs/app -type f \\( -name 'app-*.log' -o -name 'app-*.log.gz' \\) -print0 | \\\n  xargs -0 -I{} sh -c 'if [ -f \"{}\" ]; then if [[ \"{}\" == *.gz ]]; then zcat \"{}\"; else cat \"{}\"; fi; fi' | \\\n  awk -v since=\"$(date -d \"60 minutes ago\" +%s)\" '\n    # parse both formats, emit svc|msg when ts>=since\n  ' | \\\n  sort | uniq -c | sort -nr | head -n5\n```\n\n## Follow-up Questions\n- How would you extend to handle a configurable time window? \n- How would you port this to a distributed logging system (eg, Kafka)?","diagram":"flowchart TD\n  A[Start] --> B[Read plain lines] \n  A --> C[Read JSON lines]\n  B --> D[Parse ts svc msg from text]\n  C --> D\n  D --> E[Filter by last 60m]\n  E --> F[Deduplicate per svc/msg]\n  F --> G[Aggregate counts]\n  G --> H[Output top-5 per svc]\n","difficulty":"beginner","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Hashicorp","Hugging Face"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T04:17:04.667Z","createdAt":"2026-01-20T04:17:04.668Z"},{"id":"q-4649","question":"Context: A multi-tenant Unix host under heavy load shows sporadic 'Too many open files' errors. Without external tooling, design a robust approach (one or two small scripts) using only standard UNIX utilities to identify the top 5 processes by open file descriptors in the last 60 minutes. Output must include: PID, user, CMD, fd count, and a sample FD target path (including deleted files). Address unreadable /proc entries and long paths?","answer":"Approach: snapshot /proc every minute for 60 minutes, count /proc/<pid>/fd entries, map PID to user from /proc/<pid>/status and CMD from /proc/<pid>/cmdline, resolve fd targets with readlink -f, and f","explanation":"## Why This Is Asked\nTests deep Unix familiarity with /proc, process accounting, and resilient scripting. Candidates must propose a robust, low-overhead approach that remains accurate under unreadable entries and deleted file targets.\n\n## Key Concepts\n- /proc fs navigation and permissions\n- Open file descriptor counting per pid\n- Handling deleted/inaccessible targets\n- Producing CSV-friendly, production-safe output\n\n## Code Example\n```bash\n# Snapshot loop (conceptual)\nfor pid in /proc/[0-9]*; do\n  [ -d \"$pid/fd\" ] || continue\n  count=$(ls -1 \"$pid/fd\" 2>/dev/null | wc -l)\n  user=$(ps -o user= -p \"${pid##*/}\" 2>/dev/null)\n  cmd=$(tr '\\0' ' ' < \"$pid/cmdline\" 2>/dev/null)\n  echo \"$pid,$user,$cmd,$count\"\ndone\n```\n\n## Follow-up Questions\n- How would you centralize results across many hosts?\n- How would you adapt for containers with PID namespaces?","diagram":null,"difficulty":"advanced","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Coinbase","Databricks"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T06:57:00.384Z","createdAt":"2026-01-20T06:57:00.384Z"},{"id":"q-4697","question":"On a Unix host, logs are under /logs/app-logs with hourly rotation into app-<service>-YYYYMMDD.log and gzipped as *.log.gz. A real-time stream provides JSON lines appended to /logs/app-logs/stream.log: {\"ts\":\"YYYY-MM-DDTHH:MM:SSZ\",\"svc\":\"name\",\"lvl\":\"ERROR\",\"msg\":\"...\"}. Write a robust one-liner (no external deps beyond standard UNIX tools) that outputs the per-service moving average of ERROR events per minute over the last 15 minutes, across both file-based logs and the live stream, tolerant of missing/unreadable files and gz archives, and avoiding double-counting lines that appear in both current and rotated copies?","answer":"Enumerate all files under /logs/app-logs including *.log and *.log.gz, plus the live stream. Use zcat for gz, parse either plaintext or JSON via regex, extract ts and svc, bucket by minute, and accumu","explanation":"## Why This Is Asked\nTests capability to federate multiple log sources, handle rotations and compression, and derive a moving statistic without external deps.\n\n## Key Concepts\n- Cross-format parsing (plain log vs JSON)\n- Decompression of gz files without external tools\n- Time bucketing by minute and sliding window maintenance\n- Per-service aggregation in a single one-liner\n\n## Code Example\n```bash\n#!/usr/bin/env bash\n# Conceptual outline: one-liner approach using awk and standard tools\n# Not a drop-in; illustrates the technique described in the answer.\nfiles=$(printf '%s\n' /logs/app-logs/*.log /logs/app-logs/*.log.gz 2>/dev/null)\nstream=/logs/app-logs/stream.log\nzcat \"$f\" 2>/dev/null | awk -v W=15 'BEGIN{FS=\"\"} {if(match($0,/^\\[([0-9]{4}-[0-9]{2}-[0-9]{2} [0-9:]{8})\\]/)){t=$1\" \"$2}else if(match($0,/{\"ts\":\"([^\"]+)\"/)){t=$1}}; svc=($0~\"svc\"?gensub(/.*\\\"svc\\\":\\\"([^\\\"]+)\\\".*/,\"\\\\1\",\"g\",$0):\"unknown\"); if(t && $0 ~ /ERROR/){minute=substr(t,1,16); count[svc,minute]++}}\nEND{for(k in count){split(k,a,\",\"); svc=a[1]; minute=a[2]; print svc\":\"minute\" \"count[k]}}' < /dev/null\n```\n\n## Follow-up Questions\n- How would you adapt the solution to streaming dashboards with real-time updates?\n- How to test correctness with synthetic logs and rotations without downtime?","diagram":"flowchart TD\n  A[Start] --> B[Discover files and stream]\n  B --> C[Decompress gz files]\n  C --> D[Parse text and JSON formats]\n  D --> E[Bucket by minute per service]\n  E --> F[Maintain 15-minute sliding window]\n  F --> G[Output per-service moving averages]\n  G --> H[End]","difficulty":"advanced","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","Lyft","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T08:50:25.550Z","createdAt":"2026-01-20T08:50:25.550Z"},{"id":"q-4777","question":"In a Unix host, directory tree under /projects contains files some world-writable and some recently modified. Write a robust one-liner (no external deps beyond standard UNIX tools) that lists each world-writable file modified in the last 24 hours, printing path, owner, and permission bits. Ensure it handles spaces in filenames and large trees, skipping unreadable dirs?","answer":"find /projects -type f -mtime -1 -perm -0002 -printf \"%p %U %m\\n\"","explanation":"## Why This Is Asked\nThis question tests basic file discovery, permission checks, and robustness of a shell one-liner in real-world directory trees.\n\n## Key Concepts\n- find, -type f, -mtime, -perm, -printf\n- Handling spaces and large trees\n- Portability GNU vs BSD\n\n## Code Example\n```javascript\nfind /projects -type f -mtime -1 -perm -0002 -printf \"%p %U %m\\\\n\"\n```\n\n## Follow-up Questions\n- How would you adjust for 7 days or a specific user?\n- How would you safely handle filenames with newlines?","diagram":null,"difficulty":"beginner","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Google","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T11:49:02.854Z","createdAt":"2026-01-20T11:49:02.854Z"},{"id":"q-481","question":"You're debugging a production system where processes are hanging. Using only Unix tools, how would you identify which processes are blocked on I/O, what they're waiting for, and safely terminate them without causing data corruption?","answer":"Use `lsof -p <PID>` to see open files and `strace -p <PID>` to identify blocked system calls. Check `/proc/<PID>/fd` for file descriptors. For safe termination, send SIGTERM first: `kill -15 <PID>`, wait for graceful shutdown, then use SIGKILL if necessary.","explanation":"## Process Identification\n- `ps aux | grep D` shows processes in uninterruptible sleep\n- `top` with 'H' shows thread-level status\n- `iostat -x 1` identifies I/O bottlenecks\n\n## Root Cause Analysis\n- `strace -p <PID>` reveals blocked system calls\n- `lsof -p <PID>` shows open files and network connections\n- `/proc/<PID>/status` provides process state details\n\n## Safe Termination\n- SIGTERM allows graceful shutdown\n- Check for child processes before killing\n- Verify no critical writes in progress","diagram":"flowchart TD\n  A[Detect hanging process] --> B[ps aux | grep D]\n  B --> C[strace -p PID]\n  C --> D[lsof -p PID]\n  D --> E{Safe to terminate?}\n  E -->|Yes| F[kill -15 PID]\n  E -->|No| G[Wait for I/O completion]\n  F --> H[Monitor with ps]\n  G --> F","difficulty":"advanced","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Snap","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-10T03:29:09.162Z","createdAt":"2025-12-26 12:51:05"},{"id":"q-4824","question":"Write a robust one-liner (no external deps beyond standard UNIX tools) to print the five most frequent ERROR messages observed in the last 2 hours across logs under /logs/app/** and /var/log, where lines are either 'YYYY-MM-DD HH:MM:SS <LEVEL>: <text>' or JSON lines {'ts':'YYYY-MM-DDTHH:MM:SSZ','lvl':'LEVEL','msg':'...'}, and support gzip-compressed files (*.log.gz)?","answer":"Approach: scan /logs/app and /var/log, handle *.log and *.log.gz, stream lines (zcat or cat), extract timestamps from either 'YYYY-MM-DD HH:MM:SS' or ts in JSON, keep only last 2 hours, count occurren","explanation":"Demonstrates multi-source log collection, mixed formats, gzip support, time-window filtering, and top-N aggregation with a portable toolchain.","diagram":"flowchart TD\n  A[Start] --> B[Collect files]\n  B --> C[Stream lines]\n  C --> D[Parse timestamps]\n  D --> E[Filter last 2h and ERROR]\n  E --> F[Count messages]\n  F --> G[Output top 5]","difficulty":"beginner","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","Slack","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T15:05:10.995Z","createdAt":"2026-01-20T15:05:10.995Z"},{"id":"q-5062","question":"In a Unix host, audit the /etc tree for dangerous permissions: print all files under /etc that are world-readable and world-writable and were modified in the last 24 hours. Provide a robust one-liner (no external deps beyond standard UNIX tools) that outputs the path and current permissions, gracefully handles unreadable files, ignores .gz files, and works with spaces in filenames?","answer":"I would propose the one-liner: find /etc -type f -mtime -1 -readable ! -name '*.gz' -perm /002 -a -perm /004 -printf '%p %m\\n' 2>/dev/null. It finds recently modified, world-readable and world-writabl","explanation":"## Why This Is Asked\nTests ability to compose a portable one-liner using standard UNIX tools. It covers discovery by modification time, precise permission checks, handling unreadable paths, and spaces in filenames, all in a robust, edge-case-friendly way.\n\n## Key Concepts\n- find with -mtime, -readable\n- -perm /002 and -perm /004 for world-writable/readable\n- -not -name '*.gz' to skip compressed archives\n- -printf for compact output; 2>/dev/null to suppress noise\n\n## Code Example\n```bash\nfind /etc -type f -mtime -1 -readable ! -name '*.gz' -perm /002 -a -perm /004 -printf '%p %m\\n' 2>/dev/null\n```\n\n## Follow-up Questions\n- How would you include the user owner and octal permissions in the output?\n- How would you integrate this into a daily audit script with structured logging?","diagram":"flowchart TD\n  A[Start] --> B[Search /etc for files modified 24h ago]\n  B --> C[Filter readable & not .gz]\n  C --> D[Require world-readable + world-writable]\n  D --> E[Output path and permissions]","difficulty":"beginner","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Instacart"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T04:19:11.703Z","createdAt":"2026-01-21T04:19:11.703Z"},{"id":"q-510","question":"You're debugging a production issue where a process is stuck in uninterruptible sleep (D state). How would you identify and handle this situation?","answer":"Use `ps aux | awk '$8 ~ /D/ {print $2, $11}'` to find D-state processes. Check `dmesg | grep -i oom` for OOM killer activity. For I/O issues, use `lsof -p <PID>` to identify blocked files. If it's NFS, verify mount status and network connectivity.","explanation":"## Identifying D-State Processes\n\n- Use `ps` with state filtering to find uninterruptible processes\n- Check system logs for hardware or filesystem errors\n- Examine I/O queues and block device status\n\n## Common Causes\n\n- NFS mount issues or network storage problems\n- Faulty hardware devices (disk, controller)\n- Kernel bugs or driver issues\n- Memory pressure causing I/O blocking\n\n## Resolution Strategies\n\n- Wait for hardware timeout (usually 30-120 seconds)\n- Check and fix underlying storage issues\n- Reboot as last resort if process won't recover\n- Monitor `/proc/<PID>/stack` for kernel call traces","diagram":"flowchart TD\n  A[Process enters D state] --> B{Identify cause}\n  B --> C[Hardware issue]\n  B --> D[Network storage]\n  B --> E[Kernel/driver]\n  C --> F[Check dmesg/logs]\n  D --> G[Verify mount status]\n  E --> H[Examine stack trace]\n  F --> I[Wait or fix hardware]\n  G --> J[Resolve network/storage]\n  H --> K[Update/reboot if needed]","difficulty":"intermediate","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["OpenAI","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-09T03:44:39.931Z","createdAt":"2025-12-26 12:51:06"},{"id":"q-5199","question":"On a Unix host, logs for services live under /var/log/services/** with mixed rotation: some files are daily rotated by logrotate (copytruncate) and others are hourly into .gz archives. Write a robust one-liner (no external deps beyond standard UNIX tools) that outputs a per-service, per-minute event rate for the last 15 minutes, counting each unique line only once even if it appears in multiple rotated copies, and tolerating unreadable files. The log lines come in either plain text like [YYYY-MM-DD HH:MM:SS] service: msg or JSON like {\\\"ts\\\":\\\"YYYY-MM-DDTHH:MM:SSZ\\\",\\\"svc\\\":\\\"name\\\",\\\"lvl\\\":\\\"ERROR\\\",\\\"msg\\\":\\\"...\\\"}?","answer":"I would deliver a compact Bash script: enumerate files with find, feed gzipped and plain logs to awk, parse both formats, convert ts to epoch, bucket by minute for the last 15 minutes, deduplicate per","explanation":"## Why This Is Asked\nTests practical Unix skills: multi-format parsing, rotation quirks, and dedup in data streams.\n\n## Key Concepts\n- Time parsing across formats\n- Handling gz archives without external tools\n- Deduplication of log lines\n- Incremental per-minute aggregation at scale\n\n## Code Example\n```bash\n# candidate would provide a script here\n```\n\n## Follow-up Questions\n- How would you scale this to terabytes of logs?\n- How would you adapt for non-UTC timestamps?\n","diagram":null,"difficulty":"advanced","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","Tesla","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T10:41:13.213Z","createdAt":"2026-01-21T10:41:13.213Z"},{"id":"q-5225","question":"On a Unix host, central configs live under /etc/configs, organized by environment as immediate subdirectories (e.g., /etc/configs/dev, /etc/configs/prod). Some filenames contain spaces. Write a robust one-liner (no external deps beyond standard UNIX tools) that prints a per-environment count of YAML files (*.yaml) modified in the last 24 hours. It should gracefully skip unreadable files and work with a large tree?","answer":"One-liner solution: find /etc/configs -type f -name '*.yaml' -mtime -1 -printf '%h\\n' 2>/dev/null | awk -F'/' '{env=$4; c[env]++} END{for (e in c) print e\":\", c[e]}'","explanation":"## Why This Is Asked\nTests practical skills in robust file querying, path parsing, and per-group aggregation under real-world constraints (spaces in names, unreadable files).","diagram":"flowchart TD\n  A[Start] --> B[Find YAML files modified last 24h]\n  B --> C[Print directory with env name]\n  C --> D[Aggregate counts per env]\n  D --> E[Output results]","difficulty":"beginner","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","Meta","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T11:35:37.081Z","createdAt":"2026-01-21T11:35:37.081Z"},{"id":"q-5258","question":"Scenario: A Unix host stores logs under /logs/cluster/*/app-logs with hourly rotation into .log and .log.gz. Lines are either 'YYYY-MM-DD HH:MM:SS <LEVEL> <text>' or JSON {\"ts\":\"...\",\"lvl\":\"ERROR\",\"msg\":\"...\"}. Write a robust one-liner (no external deps beyond standard UNIX tools) that prints the top 5 most frequent ERROR messages observed in the last 6 hours across all files, across formats and archives, skipping unreadable files?","answer":"Use a single pipeline that normalizes both formats to an error-message stream, filters for ERROR, and tallies by content. One robust one-liner: find /logs/cluster -type f \\( -name '*.log' -o -name '*.","explanation":"## Why This Is Asked\nTests ability to integrate text and JSON logs, handle rotation and compression, and produce actionable, concise output under time pressure. \n\n## Key Concepts\n- Normalizing heterogeneous log formats\n- Safe traversal of rotated and gzipped files\n- Shell pipelines, associative counting, and sorting for top results\n\n## Code Example\n```bash\nfind /logs/cluster -type f \\( -name '*.log' -o -name '*.log.gz' \\) -newermt '-6 hours' -print0 | \\\n  xargs -0 -I{} sh -c 'case {} in *.gz) zcat {} ;; *) cat {} ;; esac' | \\\n  awk 'BEGIN{IGNORECASE=1} /ERROR/ { if(match($0,/\"msg\":\"([^\"]+)\"/,m)) msg=m[1]; else { if(match($0,/ERROR/)){ msg=$0 } } if(msg) c[msg]++ } END{for(k in c) print c[k],\"|\",k}' | \\\n  sort -nr | head -n5\n```\n\n## Follow-up Questions\n- How would you adapt this to deduplicate identical messages across multiple hosts? \n- How would you verify correctness when logs are partially missing or severely skewed in time coverage?","diagram":"flowchart TD\n  A[Start] --> B[Discover logs]\n  B --> C[Decompress gzipped files]\n  C --> D[Normalize to message field]\n  D --> E[Filter ERROR rows]\n  E --> F[Count by message]\n  F --> G[Output top 5]","difficulty":"advanced","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T13:24:55.701Z","createdAt":"2026-01-21T13:24:55.701Z"},{"id":"q-538","question":"You notice a process is consuming excessive CPU on a production server. How would you diagnose and troubleshoot this issue using Unix commands?","answer":"I would start by using `top` or `htop` to identify the process ID (PID) consuming excessive CPU, then run `ps aux | grep PID` to get detailed process information and command line arguments. For deeper analysis, I'd use `strace -p PID` to monitor system calls in real-time, `lsof -p PID` to examine open files and network connections, and `perf top` for CPU performance profiling.","explanation":"## Diagnosis Steps\n- Use `top` or `htop` to identify the high CPU process and its PID\n- Run `ps aux` to view process details and command line arguments\n- Monitor system calls with `strace -p PID` to understand process behavior\n\n## Investigation Tools\n- `lsof -p PID` reveals open files and network connections\n- `perf top` provides CPU performance profiling and bottleneck identification\n- `/proc/PID/status` contains comprehensive memory and CPU statistics\n\n## Resolution\n- Send SIGTERM (`kill -15`) for graceful process shutdown\n- Use SIGKILL (`kill -9`) only when the process is unresponsive\n- Analyze logs in `/var/log/` to identify and address the root cause","diagram":"flowchart TD\n  A[High CPU Alert] --> B[top/htop - Identify PID]\n  B --> C[ps aux - Process Details]\n  C --> D[strace - System Calls]\n  D --> E[lsof - Open Files]\n  E --> F[perf top - Performance Profile]\n  F --> G{Process Responsive?}\n  G -->|Yes| H[kill -15 PID]\n  G -->|No| I[kill -9 PID]\n  H --> J[Monitor Resolution]\n  I --> J","difficulty":"intermediate","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Goldman Sachs","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":["troubleshooting","cpu profiling","strace","lsof","performance","diagnosis"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2026-01-08T11:43:07.374Z","createdAt":"2025-12-26 12:51:06"},{"id":"q-5533","question":"Scenario: A Unix server stores per-user exports under /data/exports; a daily cron archives each user into /archives as tarballs named user-<name>-YYYYMMDD.tar.gz. Some users have no data; some archives may be corrupted. Write a robust one-liner (no external deps beyond standard UNIX tools) that lists all users with a tarball created in the last 7 days and prints the tarball filename and its size, skipping unreadable/corrupted archives?","answer":"find /archives -type f -name 'user-*-*.tar.gz' -mtime -7 -print0 | while IFS= read -r -d '' f; do tar tf \"$f\" >/dev/null 2>&1 && echo \"$f\" $(stat -c%s \"$f\"); done","explanation":"## Why This Is Asked\nTests comfort with robust shell pipelines, safe file handling, and cross-file formats. The task focuses on minimal dependencies and resilient parsing.\n\n## Key Concepts\n- find -mtime for dating, -print0 for spaces in names\n- while read with IFS to handle null-terminated input\n- tar tf to validate archive integrity without extracting\n- stat -c%s to report file size\n\n## Code Example\n```bash\nfind /archives -type f -name 'user-*-*.tar.gz' -mtime -7 -print0 | while IFS= read -r -d '' f; do tar tf \"$f\" >/dev/null 2>&1 && echo \"$f\" $(stat -c%s \"$f\"); done\n```\n\n## Follow-up Questions\n- How would you modify to also verify tar hash integrity when archives are large?\n- How would you adapt to multiple archive naming schemes or non-standard time stamps?","diagram":null,"difficulty":"beginner","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T04:34:06.769Z","createdAt":"2026-01-22T04:34:06.769Z"},{"id":"q-5559","question":"You manage Linux hosts with logs in /logs/service-*/logs, rotated hourly to .log and .log.gz. Lines are either: [YYYY-MM-DD HH:MM:SS] LEVEL: text or JSON {\"ts\":\"...\",\"level\":\"LEVEL\",\"msg\":\"...\"}; some files unreadable. Write a robust one-liner (no external deps beyond standard UNIX tools) that prints the number of JSON lines with level=ERROR whose ts is within the last 24 hours, across all files and archives?","answer":"Compute a 24h epoch cutoff with date, enumerate both .log and .log.gz via find, stream through zcat or cat, filter for JSON lines with ts and level, convert ts to epoch with date, compare to cutoff, a","explanation":"## Why This Is Asked\nTests cross-format parsing, gz handling, and timestamp filtering without extra tools. It also checks resilience to unreadable files.\n\n## Key Concepts\n- cross-format parsing\n- gzipped logs with zcat\n- timestamp parsing with date\n- robust file iteration\n\n## Code Example\n```bash\n# Outline: compute cutoff, find files, stream, filter JSON lines, parse ts, compare to cutoff, count ERROR\n```\n\n## Follow-up Questions\n- How would you adapt if date does not support -d?\n- How would you validate timezone consistency across sources?","diagram":"flowchart TD\n  A[Start] --> B[Discover files]\n  B --> C[Read files (gzipped & plain)]\n  C --> D[Parse JSON ts & level]\n  D --> E[Compare to cutoff]\n  E --> F[Count true cases]\n  F --> G[Output]","difficulty":"intermediate","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","Meta","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T06:04:32.383Z","createdAt":"2026-01-22T06:04:32.384Z"},{"id":"q-5599","question":"On a Unix host, logs reside in /data/ingest, with subdirs per dataset. Weekly audit logs named audit-YYYY-WW.log and nightly archives *.log.gz. Each line is either ts=YYYY-MM-DDTHH:MM:SS user=NAME action=ACTION or a JSON object {\"ts\":\"YYYY-MM-DDTHH:MM:SS\",\"user\":\"NAME\",\"action\":\"ACTION\"}. Write a robust one-liner (no external deps beyond standard UNIX tools) that prints the top 3 most frequent actions performed in the last 7 days across all files, across formats and archives, skipping unreadable files?","answer":"Compute a 7-day threshold, stream both .log and .log.gz, and tally actions in a single pass. Example skeleton: threshold=$(date -u -d '7 days ago' +%s); zcat /data/ingest/**/*.log*.gz /data/ingest/**/","explanation":"## Why This Is Asked\\nTests ability to combine multiple log formats, gzip archives, and shell tools to extract and aggregate data from rotating logs with no dependencies beyond core UNIX utilities.\\n\\n## Key Concepts\\n- Robust multi-format parsing (action from action=... or \\\"action\\\":\\\"...\\\")\\n- Reading gzipped and plain logs with zcat\\n- Time-window filtering using epoch with date\\n- Text processing with awk and sorting\\n\\n## Code Example\\n```javascript\\n// Implementation sketch\\n```\\n\\n## Follow-up Questions\\n- How would you extend to produce per-dataset stats?\\n- How would you parallelize for very large log sets?","diagram":null,"difficulty":"beginner","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Snowflake","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T07:52:34.062Z","createdAt":"2026-01-22T07:52:34.062Z"},{"id":"q-5633","question":"Scenario: Do a log normalization and streaming pipeline across /logs/app-logs/service-*/log-*.log and rotated .log.gz; produce a continuous JSONL stream of normalized records with ts, svc, lvl, msg for the last 60 minutes; handle both plain text and JSON lines; decompress gz on the fly; deduplicate duplicates across rotations; ensure ordering. Provide a robust one-liner or small script (no external deps) that outputs to stdout?","answer":"Use a single Bash pipeline: glob /logs/app-logs/service-*/log-*.log{,.gz}, pipe through a decompressing step (zcat/ gzcat), and feed lines to awk that detects JSON vs plain lines, extracts ts, svc, lv","explanation":"## Why This Is Asked\n\nTests ability to normalize heterogeneous log formats into a single, streamable standard while handling compression and rotations. It also probes deduplication, timestamp normalization, and ordering under real-world constraints.\n\n## Key Concepts\n\n- Robust format detection (plain vs JSON)\n- On-the-fly decompressing of gzip logs\n- Timestamp normalization to a common ISO8601\n- Time-window filtering (last 60 minutes)\n- Deduplication across rotated files\n\n## Code Example\n\n```bash\n# Example approach (not executable as-is):\nfor f in /logs/app-logs/*/log-*.log /logs/app-logs/*/log-*.log.gz; do\n  [ -f \"$f\" ] || continue\n  case \"$f\" in\n    *.gz) zcat \"$f\" ;; *) cat \"$f\" ;; esac\n  fi\ndone | \\\nawk 'BEGIN{now=systime(); win=3600}# ...\n''\n```\n\n## Follow-up Questions\n\n- How would you scale this across multiple hosts?\n- How to integrate with a central sink (e.g., Kafka) while preserving order?","diagram":"flowchart TD\n  A[Collect logs] --> B[Decompress/Parse]\n  B --> C[Normalize fields]\n  C --> D[Filter by last 60m]\n  D --> E[Deduplicate]\n  E --> F[Emit JSONL to stdout]","difficulty":"advanced","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","MongoDB","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T09:06:50.947Z","createdAt":"2026-01-22T09:06:50.948Z"},{"id":"q-564","question":"You're debugging a production system where processes are hanging. Using only Unix tools, how would you identify which processes are stuck in uninterruptible sleep (D state) and what could be causing this?","answer":"Use `ps aux | awk '$8 ~ /^D/ {print $2, $11}'` to identify processes in uninterruptible sleep state. Examine `/proc/<pid>/stack` for kernel stack traces to understand what system calls are blocking. Common causes include NFS server issues, faulty storage drivers, hardware I/O problems, or disk bottlenecks. Use `iostat -x 1` to monitor I/O activity and `dmesg | grep -i error` to check for hardware or driver errors.","explanation":"## Identifying D-State Processes\n- Use `ps aux | awk '$8 ~ /^D/ {print $2, $11}'` to filter processes in uninterruptible sleep\n- Check `/proc/<pid>/status` for detailed process state information\n- Monitor system-wide D-state processes with `top` or `htop` filtered by state\n\n## Root Cause Analysis\n- Examine `/proc/<pid>/stack` to identify the specific kernel functions blocking the process\n- Use `dmesg | grep -i error` to detect hardware or driver-related issues\n- Monitor I/O statistics with `iostat -x 1` to identify storage bottlenecks\n- Check `lsblk` and `smartctl` for disk health and controller issues\n\n## Common Causes\n- NFS server unavailability or network connectivity issues\n- Faulty disk controllers, RAID arrays, or storage drivers\n- Hardware failures in storage subsystem (bad sectors, failing drives)\n- Memory pressure causing excessive swap activity\n- Kernel bugs or incompatible device drivers\n- Storage system saturation or filesystem corruption","diagram":"flowchart TD\n  A[ps aux | awk] --> B[Identify D-state PIDs]\n  B --> C[/proc/<pid>/stack]\n  C --> D[Analyze kernel stack]\n  D --> E[lsof -p <pid>]\n  E --> F[Check open files]\n  F --> G[strace -p <pid>]\n  G --> H[Trace syscalls]","difficulty":"advanced","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","OpenAI","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":["uninterruptible sleep","d state","ps command","proc filesystem","stack traces","nfs","i/o bottlenecks","kernel debugging"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2026-01-08T11:57:22.806Z","createdAt":"2025-12-26 12:51:07"},{"id":"q-5717","question":"Scenario: A Unix host maintains multi-tenant logs under /logs/tenant-*/service-*.log, rotated hourly into .log and .log.gz. Lines may be plain text 'YYYY-MM-DD HH:MM:SS LEVEL: message' or JSON {\"ts\":\"...\",\"lvl\":\"ERROR\",\"msg\":\"...\"}. Provide a robust, self-contained UNIX pipeline (no external deps beyond standard tools) that emits a per-tenant, per-service, per-minute COUNT of ERROR events for the last 15 minutes, across both formats and file types, tolerating missing/unreadable files and gz archives, in a streaming fashion?","answer":"Design a compact POSIX shell script that streams per-tenant/service-minute ERROR counts for the last 15 minutes. Iterate /logs/tenant-*/service-*.log(.gz); skip unreadable files. Decompress with zcat,","explanation":"## Why This Is Asked\nTests ability to design a robust streaming log pipeline on UNIX, handling multi-tenant namespaces, mixed line formats, and rotated/compressed files, using standard tools only.\n\n## Key Concepts\n- Log normalization across text and JSON formats\n- Time-windowing and per-bucket aggregation\n- Handling gzip archives without external deps\n- Fault tolerance for unreadable files and missing data\n- Streaming output with incremental updates\n\n## Code Example\n```bash\n#!/usr/bin/env bash\n# Skeleton: collect, decompress, normalize timestamps, bucket by tenant/service/minute, count ERRORs in last 15m\n```\n\n## Follow-up Questions\n- How would you adapt this to a distributed collector with fault tolerance and backpressure?\n- What tests would you add to verify correctness with mixed formats and rotations?","diagram":"flowchart TD\n  A[Collect files: /logs/tenant-*/service-*.log(.gz)] --> B[Skip unreadable files]\n  B --> C[Decompress gz with zcat / cat]\n  C --> D[Extract timestamp (text or JSON ts)]\n  D --> E[Convert to epoch and floor to minute]\n  E --> F[Key by tenant|service|minute and count errors]\n  F --> G[Emit rolling counts every minute]","difficulty":"advanced","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","Netflix","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T13:12:57.297Z","createdAt":"2026-01-22T13:12:57.297Z"},{"id":"q-5999","question":"On a Linux host, logs live under /var/log/containers/<app>/app.log.YYYYMMDD(.gz). Each line is either 'YYYY-MM-DD HH:MM:SS LEVEL: text' or a JSON object {\"ts\":\"YYYY-MM-DDTHH:MM:SSZ\",\"lvl\":\"ERROR\",\"msg\":\"...\"} and files rotate hourly; write a robust one-liner (no external deps beyond standard UNIX tools) that prints per-app, per-hour error counts for the last 2 hours, aggregating across plain and JSON formats and across compressed files, tolerating unreadable files?","answer":"Construct a single shell one-liner that globs /var/log/containers/*/app.log.* (including .gz), uses zcat for gz files, extracts timestamps from either 'YYYY-MM-DD HH:MM:SS' lines or JSON ts, converts ","explanation":"## Why This Is Asked\nTests ability to parse mixed log formats, handle gzipped rotations, and compute time-windowed aggregates across multiple apps. The candidate must design a robust pipeline with correct bucketing and resilient file handling.\n\n## Key Concepts\n- Glob patterns across rotated logs\n- Parsing plain and JSON timestamps\n- Time-window filtering with date/mktime\n- Handling gzipped files (zcat) without deps\n- Per-app, per-hour bucketing and stable output\n\n## Code Example\n```bash\n# high-level sketch of the approach (not a production script)\nnow=$(date +%s); start=$((now-7200))\nfor f in /var/log/containers/*/app.log.*; do\n  app=$(basename \"$(dirname \"$f\")\")\n  if [ -f \"$f\" ]; then\n    data=$( [[ \"$f\" == *.gz ]] && zcat \"$f\" || cat \"$f\" ) 2>/dev/null || true )\n    echo \"$data\" | awk -v s=$start -v e=$now -v app=$app '\n      /ERROR/ { t=0; if (match($0,/^[0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:/,m)) { t=m[0] } else if (match($0, /\"ts\":\"([^\"]+)\"/,m2)) { t=mktime(substr(m2[1],1,4\" \"substr(m2[1],6,2)\" \"substr(m2[1],9,2)\" \"substr(m2[1],12,2)\" \"substr(m2[1],15,2)\" \"substr(m2[1],18,2))) }\n        if (t>=s && t<=e) cnt[app\":\"strftime(\"%Y-%m-%d %H\", t)]+=1\n      } \n      END{ for (k in cnt) print k, cnt[k] }\n    '\n  fi\n done | sort\n```","diagram":"flowchart TD\n  A[Logs discovered] --> B[Parse lines (plain/JSON)]\n  B --> C[Normalize to epoch hours]\n  C --> D[Accumulate per-app/hour]\n  D --> E[Output sorted counts]","difficulty":"intermediate","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Hashicorp","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T04:15:26.103Z","createdAt":"2026-01-23T04:15:26.103Z"},{"id":"q-6075","question":"Scenario: A Unix host stores logs under /logs/global/** and /logs/app-logs/** with hourly rotation into .log and .log.gz. Lines are either 'YYYY-MM-DD HH:MM:SS <LEVEL>: <text>' or JSON {ts, svc, msg, lvl}. A live stream via a named pipe at /var/log/live.err.log also feeds JSON. Provide a robust one-liner (no external deps beyond standard UNIX tools) that prints a per-service count of unique ERROR messages observed in the last 24 hours, across files and live input, tolerant of unreadable files and gz archives, deduplicating by (svc, msg)?","answer":"find /logs/global /logs/app-logs -name '*.log' -o -name '*.log.gz' 2>/dev/null | while read f; do if [[ $f == *.gz ]]; then gunzip -c \"$f\" 2>/dev/null || continue; else cat \"$f\" 2>/dev/null || continue; fi; done & tail -F /var/log/live.err.log 2>/dev/null | awk -v cutoff=$(date -d '24 hours ago' +%s) '/^{/{gsub(/\"/, \"\"); if(match($0, /ts\": *([0-9]+)/, ts)) {if(ts[1] >= cutoff && match($0, /lvl\": *\"ERROR\"/) && match($0, /svc\": *\"([^\"]+)/, svc) && match($0, /msg\": *\"([^\"]+)/, msg)) print svc[1] \"\\t\" msg[1]}} else if(match($0, /^([0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2}) +ERROR: +(.+)/, m)) {cmd=\"date -d \\\"\" m[1] \"\\\" +%s\"; cmd | getline ts; close(cmd); if(ts >= cutoff) print \"unknown\\t\" m[2]}}' | sort -u | awk -F'\\t' '{svc[$1]++} END{for(s in svc) print s \": \" svc[s]}'","explanation":"## Why This Is Asked\nTests ability to reason about mixed log formats, streaming input, and dedup logic for production telemetry. It gauges robustness to unreadable files and gz archives, and requires a compact pipeline.\n\n## Key Concepts\n- Unified parsing of two log formats without external parsers\n- Streaming input from files and a named pipe\n- Sliding window timestamps, 24h filter\n- De-duplication of (svc,msg) across streams\n- Performance implications with many files\n\n## Code Example\n```bash\nfind /logs/global /logs/app-logs -name '*.log' -o -name '*.log.gz' 2>/dev/null | while read f; do if [[ $f == *.gz ]]; then gunzip -c \"$f\" 2>/dev/null || continue; else cat \"$f\" 2>/dev/null || continue; fi; done & tail -F /var/log/live.err.log 2>/dev/null | awk -v cutoff=$(date -d '24 hours ago' +%s) '/^{/{gsub(/\"/, \"\"); if(match($0, /ts\": *([0-9]+)/, ts)) {if(ts[1] >= cutoff && match($0, /lvl\": *\"ERROR\"/) && match($0, /svc\": *\"([^\"]+)/, svc) && match($0, /msg\": *\"([^\"]+)/, msg)) print svc[1] \"\\t\" msg[1]}} else if(match($0, /^([0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2}) +ERROR: +(.+)/, m)) {cmd=\"date -d \\\"\" m[1] \"\\\" +%s\"; cmd | getline ts; close(cmd); if(ts >= cutoff) print \"unknown\\t\" m[2]}}' | sort -u | awk -F'\\t' '{svc[$1]++} END{for(s in svc) print s \": \" svc[s]}'\n```\n\n## Follow-up Questions\n- How would you test against clock skew?\n- How would you adapt for per-minute granularity?","diagram":"flowchart TD\n  A[Start] --> B[Enumerate logs]\n  B --> C{Format: text vs JSON}\n  C --> D[Parse timestamps]\n  D --> E[Filter 24h window]\n  E --> F[Deduplicate (svc,msg)]\n  F --> G[Print per-service counts]","difficulty":"advanced","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":["log file parsing","json log format","timestamp filtering","named pipe streaming","gzip archive handling","error message deduplication","service count aggregation","24 hour window"],"voiceSuitable":true,"isNew":true,"lastUpdated":"2026-01-27T05:28:54.880Z","createdAt":"2026-01-23T07:47:30.777Z"},{"id":"q-6154","question":"In a Unix host, /data/exports contains nested date-based folders with CSV files (*.csv) and gzipped CSVs (*.csv.gz). Each row: customer_id,timestamp,amount. Timestamps are YYYY-MM-DD. Write a robust one-liner (no external deps beyond standard UNIX tools) that prints the top 3 customers by total revenue in the last 30 days, aggregating across all files, skipping unreadable files?","answer":"Approach: stream both CSV and CSV.gz, skip unreadable files, extract customer_id and amount, convert ISO timestamps to epoch, keep totals for last 30 days, then sort and output top 3. Uses find -print","explanation":"## Why This Is Asked\nTests practical data aggregation across mixed text and gz files; date filtering and per-customer top results. Handles unreadable files gracefully. \n\n## Key Concepts\n- find -print0 for safe filenames\n- zcat vs cat for compression\n- awk + date -d for date to epoch conversion\n- defensive checks for readability and headers\n\n## Code Example\n```javascript\nstart=$(date -d '30 days ago' +%s)\nfind /data/exports -type f \\( -iname '*.csv' -o -iname '*.csv.gz' \\) -print0 |\n  while IFS= read -r -d '' f; do\n    [ -r \"$f\" ] && (gzip -t \"$f\" >/dev/null 2>&1 && zcat \"$f\" || cat \"$f\");\ndone |\nawk -F',' -v s=\"$start\" 'NR>1{ if ($2 ~ /^[0-9]{4}-[0-9]{2}-[0-9]{2}$/){ cmd=\"date -d \\\"\"$2\"\\\" +%s\"; cmd | getline ts; close(cmd); if (ts+0 >= s) sums[$1]+=$3 } } END { for(id in sums) print id, sums[id] } ' |\nsort -k2,2nr | head -n3\n``` \n\n## Follow-up Questions\n- How would you adapt this to handle negative amounts or different column orders?\n- How could you optimize for very large datasets or add a memory-safe streaming approach?","diagram":"flowchart TD\n  A[Collect files] --> B[Decompress/Read]\n  B --> C[Parse lines and filter by date]\n  C --> D[Accumulate per-customer totals]\n  D --> E[Sort and output top 3]","difficulty":"beginner","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Cloudflare","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T11:06:13.210Z","createdAt":"2026-01-23T11:06:13.210Z"},{"id":"q-6276","question":"Scenario: A Unix host stores user uploads under /data/uploads with subdirectories per user. Some files may be unreadable or symlinks. Write a robust one-liner (no external deps beyond standard UNIX tools) that prints the top 3 users by total size of regular files created in the last 24 hours, excluding symlinks, reporting lines as 'user: total_bytes'?","answer":"find /data/uploads -type f -mtime -1 -print0 2>/dev/null | while IFS= read -r -d '' f; do u=$(stat -c%U \"$f\" 2>/dev/null); s=$(stat -c%s \"$f\" 2>/dev/null); [ -n \"$u\" ] && echo \"$u|$s\"; done | awk -F'|","explanation":"## Why This Is Asked\n\nTests ability to combine basic Unix tools to derive per-user totals under unreadable files and symlinks. Emphasizes robustness with -print0, per-file stat, and defensive 2>/dev/null.\n\n## Key Concepts\n\n- Robust file traversal with find -type f and -mtime\n- Handling spaces in paths via -print0 and read -d ''\n- Per-file metadata via stat; aggregation with awk\n\n## Code Example\n\n```javascript\n// Implementation code here\n```\n\n## Follow-up Questions\n\n- How would you modify for 7-day window or incorporate file sizes in a human-friendly format?\n- How would you adapt for a shallow vs deep directory structure?","diagram":null,"difficulty":"beginner","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T17:32:27.372Z","createdAt":"2026-01-23T17:32:27.374Z"},{"id":"q-6326","question":"Scenario: A Unix host aggregates logs from dozens of tenants under /logs/tenants/*/logs, rotated hourly into .log and .log.gz. A live JSON stream writes to /var/log/tenants/ingest.log. Each log line is either a timestamped plain text line or JSON lines with ts, lvl, msg, tenant. Write a robust one-liner (no external deps beyond standard UNIX tools) that prints, for every tenant, the number of unique ERROR messages observed in the last 24 hours across both file logs and the live stream, deduplicating identical messages across sources, and skipping unreadable files?","answer":"Stream all sources (gz via zcat, plain logs via cat, plus tail -F ingest.log) into a single awk script. The awk detects text or JSON, extracts ts, tenant, and msg, converts ts to epoch, discards older","explanation":"## Why This Is Asked\nTests ability to fuse multiple log sources (files, gz, live pipe), handle mixed formats, and compute per-tenant unique-error counts under a tight window.\n\n## Key Concepts\n- Shared-nothing streaming, gz streams, JSON-lite parsing with awk, time windowing, dedup via composite key, fault tolerance.\n\n## Code Example\n```bash\n# Pseudo one-liner sketch\n(now=$(date +%s); cutoff=$((now-86400)); awk '...'\n```\n\n## Follow-up Questions\n- How would you extend to different time zones?\n- How would you scale to millions of lines per hour?","diagram":"flowchart TD\n  A[Collect logs] --> B[Stream gz, text, live ingest]\n  B --> C[Normalize fields: ts, tenant, msg]\n  C --> D[Filter 24h]\n  D --> E[Deduplicate by tenant+msg]\n  E --> F[Aggregate per tenant]\n","difficulty":"advanced","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T19:34:11.858Z","createdAt":"2026-01-23T19:34:11.858Z"},{"id":"q-6353","question":"In a Unix host, logs live under /logs/app-*/logs and rotate hourly into .log and .log.gz. Each line is either a timestamped plain text format or a JSON object with ts and msg fields. For the current day, write a robust one-liner (no external deps beyond standard UNIX tools) that outputs a per hour count of unique ERROR messages whose text begins with AUTH or DB, aggregating across all files and archives, and tolerating unreadable files. Ensure uniqueness is scoped per hour and handle gzipped files transparently?","answer":"A robust one-liner would pipe rotated logs (plain and gz) into a single awk that detects both formats, extracts the hour from the timestamp, and counts unique ERROR messages that start with AUTH or DB","explanation":"## Why This Is Asked\nAssesses practical Unix log processing: handling multiple formats, gz archives, unreadable files, and per-hour aggregation with dedup.\n\n## Key Concepts\n- Robust multi-format parsing (text and JSON)\n- Per-hour aggregation of unique messages\n- Transparent gz handling with zcat\n- Resilience to unreadable files\n- Memory-conscious streaming pipeline\n\n## Code Example\n```javascript\n// Pseudocode sketch showing approach\n```\n\n## Follow-up Questions\n- How would you extend to multiple time windows (e.g., today and last 7 days) without duplicating work?\n- How would you test with synthetic logs including corrupted lines?","diagram":"flowchart TD\n  A[Logs: /logs/app-*/logs] --> B[Parse: text or JSON]\n  B --> C[Extract hour for current day]\n  C --> D[Filter: ERROR and AUTH/DB]\n  D --> E[Per-hour unique messages]\n  E --> F[Output: hour:count]","difficulty":"intermediate","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Hugging Face","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T20:42:37.271Z","createdAt":"2026-01-23T20:42:37.271Z"},{"id":"q-6371","question":"Scenario: A Unix host stores traces under /logs/traces/** with hourly rotation into .log and .log.gz. Each line is either JSON {\"ts\":\"YYYY-MM-DDTHH:MM:SSZ\",\"latency_ms\":<n>,\"svc\":\"name\"} or plain text 'YYYY-MM-DD HH:MM:SS <trace_id> latency=<n>ms svc=<name>'. Write a robust one-liner (no external deps beyond standard UNIX tools) that prints a per-minute histogram of maximum latency per service for the last 60 minutes, across all files and rotations, tolerant of unreadable files and gz archives?","answer":"Use a single awk invocation that reads both formats (zcat for .gz, cat for .log), parses JSON or text, converts timestamps to epoch with mktime, filters the last 60 minutes, and tracks maximum latency per service per minute.","explanation":"## Why This Is Asked\nTests ability to design robust, compact log analytics across mixed formats and rotated archives using only standard UNIX tools.\n\n## Key Concepts\n- Parsing JSON and text with awk\n- Time window filtering with epoch comparisons\n- Per-service per-minute histogram generation\n- Handling gzipped logs without external dependencies\n\n## Code Example\n```awk\n# Implementation sketch in awk (simplified for explanation)\n```\n\n## Follow-up Questions\n- How would you extend to compute the 95th percentile latency?\n- How would you adapt for configurable time window sizes?","diagram":"flowchart TD\n  A[Input Files] --> B[Parse Lines]\n  B --> C[Normalize Timestamps]\n  C --> D[Compute Per-Minute Max]\n  D --> E[Aggregate & Output Histogram]","difficulty":"advanced","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Oracle","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T05:36:03.475Z","createdAt":"2026-01-23T21:30:22.810Z"},{"id":"q-6401","question":"In a Unix host, logs live under /logs/app-*/events.log[.gz], with hourly rotation. Each line is either a plain text 'YYYY-MM-DD HH:MM:SS LEVEL: msg' or a JSON {\"ts\":..., \"lvl\":..., \"msg\":...}. The system handles both formats across gzip archives. Build a robust, self-contained pipeline using only standard UNIX tools that outputs a rolling 5-minute histogram of unique ERROR messages by exact message text across all files and formats, deduplicating identical messages across rotations and tolerating unreadable files. The pipeline should be memory-efficient?","answer":"Maintain a 5-minute rolling histogram of unique ERROR messages by exact text across /logs/app-*/events.log[.gz]. Pipe all files through zcat (ignoring unreadable files). Use a compact awk script that recognizes both timestamp formats, filters for ERROR level, buckets messages by minute, tracks unique messages per minute using associative arrays, and outputs the rolling 5-minute count of distinct messages.","explanation":"## Why This Is Asked\nTests streaming parsing, mixed formats, and rolling window logic on large, real-world logs. Also checks deduplication across rotated files and fault tolerance.\n\n## Key Concepts\n- Dual-format parsing (text and JSON)\n- On-the-fly gzip handling with zcat\n- Rolling window with per-minute deduplication sets\n- Robustness to unreadable files and clock skew\n\n## Code Example\n```awk\n# simplified sketch\nBEGIN{FS=\"\"}\n# parse both formats, extract ts and msg, bucket by minute, dedup msgs per minute\n```\n\n## Follow-up Questions\n- How would you scale this across many hosts?\n- How would you handle timezone differences?\n- What optimizations would you add for very high-volume logs?","diagram":"flowchart TD\n  A[Read logs] --> B[Decompress gz]\n  B --> C[Parse formats]\n  C --> D[Build per-minute sets]\n  D --> E[Emit oldest minute]\n","difficulty":"intermediate","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T05:20:14.502Z","createdAt":"2026-01-23T22:31:50.617Z"},{"id":"q-6551","question":"Scenario: A Unix host stores logs under /logs/teams/*/app-logs with hourly rotation into .log and .log.gz. Lines are either JSON {ts, svc, lvl, msg} or plain 'YYYY-MM-DD HH:MM:SS <LEVEL>: <text>'. Write a robust one-liner (no external deps) that outputs per-service, per-minute ERROR event rates for the last 5 minutes, across formats and archives, deduplicating identical lines across rotations and skipping unreadable files?","answer":"Use a single shell one-liner that finds all /logs/teams/*/app-logs/*.log{,.gz}, streams with zcat or cat, extracts JSON ts/svc/lvl or text ts, filters ERROR, converts ts to epoch, buckets by svc and m","explanation":"Why This Is Asked\n- Tests robust log parsing across mixed formats and rotated archives.\n- Evaluates handling of gzipped files, missing files, and dedup across rotations.\n- Probes ability to implement time-windowed, per-service aggregation with minimal tooling.\n\nKey Concepts\n- Text vs JSON parsing, timestamps to epoch, time-window bucketing\n- Deduplication across rotated files, inodes vs content\n- Safe handling of unreadable files and large data streams\n\nCode Example\n```bash\n# Pseudo-structure (not a full one-liner):\nfind /logs/teams -path '*/app-logs/*' -type f \\( -name '*.log' -o -name '*.log.gz' \\) -print0 | \\\n  while IFS= read -r -d '' f; do\n    [ -f \"$f\" ] || continue\n    if [ -n \"${f##*.gz}\" ]; then\n      zcat \"$f\" || continue\n    else\n      cat \"$f\" || continue\n    fi\n  done | awk -v W=300 '...extract ts, svc; if lvl==ERROR, epoch=toEpoch(ts); if epoch>=now-W; bucket[svc,minute(epoch)]++ ; dedup by line';\n```\n\nFollow-up Questions\n- How would you adapt this to handle multi-line JSON events and nested fields?\n- How would you unit-test the command against synthetic rotated logs?","diagram":null,"difficulty":"advanced","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","DoorDash","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T07:54:15.970Z","createdAt":"2026-01-24T07:54:15.970Z"},{"id":"q-6573","question":"Scenario: A Unix host stores logs under /logs/ops/** with hourly rotation into .log and .log.gz. Each line is either 'YYYY-MM-DD HH:MM:SS <LEVEL>: <text>' or JSON like {\"ts\":\"YYYY-MM-DDTHH:MM:SSZ\",\"level\":\"ERROR\",\"svc\":\"service-name\",\"msg\":\"...\"} . Write a robust one-liner (no external deps beyond standard UNIX tools) that prints, for each hour in the last 8 hours, the number of distinct services that emitted an ERROR message, across both formats and without failing on unreadable files or gz archives?","answer":"Proposed one-liner approach: gather all log files from /logs/ops modified in the last 8 hours (including .log and .log.gz); stream through with zcat or cat; parse both formats with a small awk to extr","explanation":"## Why This Is Asked\nTests parsing of dual formats, gz handling, and aggregation by hour with dedup on service.\n\n## Key Concepts\n- Robust file collection across rotations\n- Mixed-format parsing without external tools\n- De-dup by (hour,service) and per-hour tally\n\n## Code Example\n```bash\n# Example approach (conceptual)\nfind /logs/ops -type f \\( -name \"*.log\" -o -name \"*.log.gz\" \\) -mmin -480 -print0 | \\\nxargs -0 -I{} bash -lc 'case \"{}\" in *.gz) zcat \"{}\" ;; *) cat \"{}\" ;; esac' | \\\nawk '\\n' \n```\n\n## Follow-up Questions\n- How would you modify for last 24 hours?\n- How would you add a count by level or add tenant support?\n","diagram":"flowchart TD\n  A[Start] --> B[Find files in last 8h]\n  B --> C[Stream lines (gz and plain)]\n  C --> D[Normalize to hour and service]\n  D --> E[Deduplicate by (hour,service)]\n  E --> F[Count per hour] \n  F --> G[Output]","difficulty":"beginner","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","LinkedIn"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T08:43:57.433Z","createdAt":"2026-01-24T08:43:57.433Z"},{"id":"q-6600","question":"Scenario: A Unix host runs multi-tenant apps logging to /logs/tenant-*/app-*.log with hourly rotations into .log and .log.gz. Each line is either 'YYYY-MM-DD HH:MM:SS LEVEL: text' or JSON {\"ts\",\"lvl\",\"msg\"}. Write a robust one-liner (no external deps beyond standard UNIX tools) that prints a per-tenant, per-minute, per-level count of WARN/ERROR events for the last 45 minutes, across all files and archives, tolerating unreadable files and deduplicating identical lines that may appear in multiple rotated copies?","answer":"Stream all files (including gz), normalize to minute buckets, and accumulate per-tenant, per-minute, per-level counts for WARN/ERROR in the last 45 minutes. Deduplicate lines by a lightweight key (ino","explanation":"## Why This Is Asked\n\nTests practical UNIX data-sourcing skills: multi-format logs, rotation, and per-tenant aggregation. It also probes streaming correctness, fault tolerance, and dedup logic in a single pipeline.\n\n## Key Concepts\n- Unified log parsing across formats without external tools\n- Streaming aggregation with a time window\n- De-duplication across rotated copies per inode\n- Shell portability and fault tolerance\n\n## Code Example\n```javascript\n# Pseudocode example of a streaming pipeline (not run as-is)\n# 1. Collect files across tenants\n# 2. Decompress .gz on the fly\n# 3. Parse both formats and emit tenant, ts, lvl\n# 4. Bucket by minute and filter last 45 minutes\n# 5. Emit counts by (tenant,minute,lvl)\n```\n\n## Follow-up Questions\n- How would you adapt for missing timestamps?\n- How would you test with synthetic rotated logs?\n","diagram":"flowchart TD\nA[Start] --> B[Gather files across tenants]\nB --> C[Decompress .gz where present]\nC --> D[Parse lines => tenant, minute, level]\nD --> E[Subtract 45 minutes and filter]\nE --> F[Accumulate counts by (tenant,minute,level)]\nF --> G[Output histogram]","difficulty":"intermediate","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T09:44:26.384Z","createdAt":"2026-01-24T09:44:26.384Z"},{"id":"q-6683","question":"Scenario: A Unix host serves a frontend service; access logs live under /logs/frontend/*/access.log and are rotated hourly into .log.gz. Each line is either Common Log Format: 1.2.3.4 - - [01/Jan/2026:12:34:56 +0000] ... or JSON: {\"ts\":\"2026-01-24T12:34:56Z\",\"ip\":\"1.2.3.4\",\"req\":\"GET /\"}. Write a robust one-liner (no external deps beyond standard UNIX tools) that prints the top 5 client IPs by total requests in the last 60 minutes, aggregating across both formats and archives, tolerant of unreadable files?","answer":"Use a single pipeline: decompress both formats with zcat, extract IPs from CLF or JSON, convert timestamps to seconds with date, filter for last 3600 seconds, then count per IP with awk, and finally s","explanation":"## Why This Is Asked\nTests handling of mixed log formats, time-window filtering, and gzipped archives in a compact unix pipeline.\n\n## Key Concepts\n- zcat for mixed plain/gz logs\n- Parsing CLF vs JSON to extract IPs\n- date -d for epoch conversion across formats\n- awk for per-IP aggregation\n- Robustness with 2>/dev/null\n\n## Code Example\n```javascript\n// Conceptual Bash snippet (not runnable JS)\nconst cmd = \"zcat /logs/frontend/*/*.log.gz /logs/frontend/*/*.log 2>/dev/null | awk '...'\";\nconsole.log(cmd);\n```\n\n## Follow-up Questions\n- How would you adapt to a different window (e.g., last 2 hours)?\n- How would you test with synthetic mixed-format logs?","diagram":null,"difficulty":"beginner","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Cloudflare"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T13:22:38.647Z","createdAt":"2026-01-24T13:22:38.647Z"},{"id":"q-6801","question":"You manage a Unix host where logs are stored under /logs/host-*/service-*/log-YYYYMMDD-HH.log and rotated hourly into *.log.gz. Each line is either 'YYYY-MM-DD HH:MM:SS LEVEL: message' or a JSON line with fields ts, msg, and possibly lvl. Write a robust one-liner (no external deps beyond standard UNIX tools) that reports, for the last 24 hours, the set of host/service pairs that had at least one ERROR, along with the count of distinct ERROR messages observed across both formats. Ensure it tolerates unreadable files and gz archives, and works when some hours are missing?","answer":"Propose a one-liner that normalizes both formats to (host,service,message) with an epoch timestamp; deduplicate messages per host/service; aggregate counts for the last 24 hours; print host/service an","explanation":"## Why This Is Asked\nTests ability to blend text and JSON parsing, compression handling, and multi-tenant aggregation in tight CLI form. It also probes robustness to partial data and missing rotations.\n\n## Key Concepts\n- cross-format log parsing\n- gzip and plain text handling with standard tools\n- per-host/per-service aggregation\n- time-window filtering (24h)\n- fault tolerance and file skippage\n\n## Code Example\n```bash\n# illustrative approach (not a complete one-liner)\n# 1) collect files\n# 2) parse lines to extract host,service,message when lvl=ERROR\n# 3) dedupe per host/service/message and count distinct msgs\n```\n\n## Follow-up Questions\n- How would you extend for 1-minute windows?  - How do you test with synthetic data? ","diagram":"flowchart TD\n  A[Ingest Logs] --> B[Parse Formats]\n  B --> C[Dedupe Messages]\n  C --> D[Aggregate by Host/Service]\n  D --> E[Output 24h Counts]","difficulty":"intermediate","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","DoorDash","IBM"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T17:54:05.222Z","createdAt":"2026-01-24T17:54:05.222Z"},{"id":"q-6834","question":"In a host with multiple microservices writing logs to /var/log/microservices/<svc>/*.log and rotated daily into .log and .log.gz, lines can be either 'YYYY-MM-DD HH:MM:SS LEVEL: message' or JSON {\"ts\":\"...\",\"level\":\"ERROR\",\"msg\":\"...\"}. Provide a robust one-liner Bash pipeline (no external deps) that prints the top 3 services by number of ERROR events observed in the last 2 hours, across both formats and archives, skipping unreadable files and deduplicating across rotations?","answer":"Use a single pipeline that collects both log types, decompresses gz when needed, and streams into a compact awk that normalizes timestamps and derives the service name from the path. It should count o","explanation":"## Why This Is Asked\nTests parsing of heterogeneous log formats, rotation handling, and cross-service aggregation under time constraints, all with standard UNIX tools. It also probes robustness against unreadable files and dedup across rotations.\n\n## Key Concepts\n- Cross-format log parsing (plain text and JSON) without extra deps\n- Time-window filtering using epoch timestamps\n- Handling gzipped rotations and unreadable files\n- Service extraction from file paths and per-service aggregation\n- Deduplication across rotated files\n\n## Code Example\n```javascript\n// Pseudo: demonstrates approach, not production-ready\n``` \n\n## Follow-up Questions\n- How would you scale this to thousands of services and multiple hosts?\n- How would you adapt for multi-line log messages and partial JSON lines?","diagram":"flowchart TD\n  A[Collect /var/log/microservices/*/*.log and *.log.gz] --> B[Decompress/read formats]\n  B --> C[Normalize timestamps]\n  C --> D[Extract service from path]\n  D --> E[Count ERROR events per service in last 2h]\n  E --> F[Sort & output top 3]","difficulty":"intermediate","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Microsoft","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T19:26:11.941Z","createdAt":"2026-01-24T19:26:11.941Z"},{"id":"q-7026","question":"On a Unix host, logs live under /logs/trace/*/*.log and rotate hourly into .log.gz. Each line is either 'YYYY-MM-DD HH:MM:SS <LEVEL> <text>' or JSON {ts, svc, lat_ms, latency_ms, lat}. Some lines also use lat. Write a robust one-liner (no external deps beyond standard UNIX tools) that prints a per-minute maximum latency for every service in the last 60 minutes, aggregating across both formats and archives, tolerant of unreadable files. Output format: YYYYMMDDHHMM, <svc>, <max_latency_ms>?","answer":"Stream all logs (gzipped and plain) with gzip -cd, normalize to minute, extract latency from lat_ms, latency_ms, or lat, and use awk to track per-minute per-service max latency; print minute, svc, max","explanation":"## Why This Is Asked\nTests ability to parse multiple log formats, handle gzipped rotations, and perform time-windowed per-service aggregation without external tooling.\n\n## Key Concepts\n- Normalize logs from text and JSON formats\n- Handle gzipped rotations with gzip/gzcat\n- Bucket by minute and per-service max latency\n- Robustly skip unreadable files\n- Work within a single one-liner\n\n## Code Example\n```bash\n# Conceptual approach (not a complete one-liner)\n# zcat/gzip streams, extract ts, svc, latency fields, bucket by minute, compute max\ngzcat /logs/trace/**/*.log.gz /logs/trace/**/*.log 2>/dev/null | \\\nawk 'BEGIN{MIN=60} { /* parse and aggregate by minute and svc */ } END { /* print results */ }'\n```\n\n## Follow-up Questions\n- How would you extend to compute p95/p99 latencies per minute?\n- How would you test correctness at scale (sampling, synthetic data, monitoring)?","diagram":"flowchart TD\n  A[Ingest Logs] --> B[Parse and Normalize]\n  B --> C[Bucket by Minute and Service]\n  C --> D[Compute Max Latency]\n  D --> E[Output Results]","difficulty":"advanced","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T06:42:42.726Z","createdAt":"2026-01-25T06:42:42.728Z"},{"id":"q-7067","question":"In a Unix host, log files live under /logs/app*/{service1,service2}/*.log and *.log.gz. Each line is either [YYYY-MM-DD HH:MM:SS] <LEVEL>: <text> or JSON {\"ts\":\"YYYY-MM-DDTHH:MM:SSZ\",\"svc\":\"name\",\"lvl\":\"ERROR\",\"msg\":\"...\"}. Provide a robust one-liner (no external deps beyond core UNIX tools) that prints, for every service, a per-minute COUNT of ERROR events in the last 15 minutes, aggregating across both formats and archives, while tolerating unreadable files and gz archives, and streaming as new lines arrive?","answer":"Streaming approach: use zcat -f to read both .log and .log.gz files, lower-level lines go through an AWK parser that normalizes to fields ts, svc, lvl. If lvl is ERROR and ts is within the last 15 min","explanation":"## Why This Is Asked\nTests ability to build a robust streaming analysis over mixed log formats, compressed archives, and rotating files without extra deps.\n\n## Key Concepts\n- Streaming pipelines using zcat, awk, date\n- Parsing both text and JSON lines\n- Time-windowed, per-key aggregation\n- Resilience to unreadable files and mixed rotations\n\n## Code Example\n```javascript\n// Conceptual sketch (not runnable as-is)\n```\n\n## Follow-up Questions\n- How would you adapt for dynamic window sizes? 5/30/60 minutes?\n- How would you detect and handle clock skew across hosts?\n","diagram":"flowchart TD\n  A[Parse line] --> B[Normalize ts/svc/msg]\n  B --> C{lvl==ERROR}\n  C --> D[Update window counts]\n  D --> E[Emit per-minute histogram]\n","difficulty":"intermediate","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Oracle","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T07:47:50.450Z","createdAt":"2026-01-25T07:47:50.450Z"},{"id":"q-7115","question":"Scenario: On a Unix host, logs live under /logs/** with hourly rotation into .log and .log.gz. Lines are either 'YYYY-MM-DD HH:MM:SS <LEVEL>: <text>' or JSON {\"ts\":...,\"lvl\":...,\"msg\":...}. Write a robust one-liner (no external deps beyond standard UNIX tools) that prints, for each log file touched in the last 24 hours, the number of lines containing ERROR. Output format: <path>:<count> per line. Skip unreadable files; work across nested directories and archives?","answer":"find /logs -type f \\( -name '*.log' -o -name '*.log.gz' \\) -mtime -1 -print0 | while IFS= read -r -d '' f; do if [[ \"$f\" == *.gz ]]; then count=$(zcat \"$f\" 2>/dev/null | grep -a -E 'ERROR|\"lvl\":\"ERROR","explanation":"Why this works: (1) find locates both .log and .log.gz files modified in the last day, safely handling spaces. (2) A while read loop processes each file; for .gz it uses zcat, else grep. (3) Grep patterns match both plain-text ERROR lines and JSON-encoded ERROR levels. (4) Reads are redirected to skip unreadable files; outputs per-file counts in a simple path:count format.","diagram":"flowchart TD\n  A[Start] --> B[Discover files in /logs/** with -mtime -1]\n  B --> C{Is gz?}\n  C -- yes --> D[zcat and search for ERROR]\n  C -- no --> E[grep search for ERROR]\n  D & E --> F[Emit file path and count]\n  F --> G[Finish]","difficulty":"beginner","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Cloudflare"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T09:44:19.561Z","createdAt":"2026-01-25T09:44:19.561Z"},{"id":"q-7261","question":"On a Unix host, logs live under /logs/service/* with hourly rotation into *.log and *.log.gz. Some lines are plain text 'YYYY-MM-DD HH:MM:SS LEVEL: message' and others are JSON {\"ts\":...,\"lvl\":...,\"msg\":...}. Write a robust one-liner (no external deps beyond standard UNIX tools) that computes the 95th percentile of the byte-length of lines that contain ERROR for all files touched in the last 24 hours, aggregating across plaintext and JSON lines and across both compressed and uncompressed files. Output a single number?","answer":"Proposed approach: gather all files under /logs/service/* modified in the last 24h (including *.log and *.log.gz). For each file, decompress on the fly with zcat when .gz, otherwise cat. Filter lines ","explanation":"## Why This Is Asked\nTests ability to design robust, real-world log analytics on Unix without dependencies, handling mixed formats and rotated archives.\n\n## Key Concepts\n- Shell pipelines with find, zcat, and cat\n- Regex for both text and JSON patterns\n- Immutable, non-destructive processing across rotations\n- Percentile computation with standard tools\n\n## Code Example\n```javascript\n// Implementation idea only; actual one-liner can vary\n```\n\n## Follow-up Questions\n- How would you adapt this for streaming logs without listing all files?\n- How would you parallelize safely on a multi-core host?","diagram":"flowchart TD\n  A[Identify recent files] --> B[Decompress if needed]\n  B --> C[Filter ERROR lines]\n  C --> D[Measure length]\n  D --> E[Compute 95th percentile]\n","difficulty":"advanced","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Cloudflare","Hugging Face"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T15:43:55.718Z","createdAt":"2026-01-25T15:43:55.718Z"},{"id":"q-7393","question":"Scenario: A Unix host aggregates multi-tenant logs under /logs/tenant-*/service-*/log-*.log, rotated hourly into .log/.log.gz. Each line is either 'YYYY-MM-DD HH:MM:SS <LEVEL>: <text>' or JSON {\"ts\":...,\"lvl\":...,\"msg\":...}. Build a robust, self-contained pipeline (no external deps) that, for each tenant/service, reports the number of ERROR events in the most recent complete minute, streaming with time, tolerating unreadable files and gz archives, across nested dirs?","answer":"Implement a robust, self-contained streaming pipeline that traverses the nested directory structure /logs/tenant-*/service-*/log-*/, processes both uncompressed (.log) and compressed (.log.gz) files, normalizes mixed log formats (plain text and JSON) to extract timestamps and error levels, tracks ERROR events within sliding one-minute windows, aggregates counts by tenant and service, and continuously streams results with fault tolerance for unreadable files and archives.","explanation":"This technical scenario assesses the ability to design a resilient log processing pipeline using standard UNIX utilities. The evaluation focuses on several core competencies: navigating complex nested directory structures with wildcard patterns, handling mixed file formats including compressed archives, parsing and normalizing heterogeneous log data formats, extracting contextual metadata from file paths, implementing time-based aggregation logic with sliding windows, and building fault-tolerant systems that gracefully handle file access errors and malformed data.","diagram":"flowchart TD\n  A[Discover files] --> B[Parse lines]\n  B --> C[Extract ts or JSON ts]\n  C --> D[Bucket by tenant/service/minute]\n  D --> E[Accumulate ERROR counts]\n  E --> F[Emit latest minute counts]","difficulty":"intermediate","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Snap","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T06:53:18.057Z","createdAt":"2026-01-25T21:30:22.159Z"},{"id":"q-7521","question":"On a Unix host, container logs live under /var/log/containers/*.log and rotate hourly into .log and .log.gz. Each line is either 'YYYY-MM-DD HH:MM:SS <LEVEL>: <svc>: <text>' or JSON {\"ts\":\"...\",\"svc\":\"name\",\"lvl\":\"ERROR\",\"msg\":\"...\"}. Write a robust one-liner (no external deps beyond standard UNIX tools) that outputs the top 3 services by ERROR count in the last 15 minutes, across all files and compressed archives, skipping unreadable files, and deduplicating identical lines across rotated copies?","answer":"Proposed approach: filter last 15 minutes with find -mmin, read both formats (plain: 'YYYY-MM-DD HH:MM:SS LEVEL: svc: msg', JSON: {\"ts\":\"...\",\"svc\":\"name\",\"lvl\":\"ERROR\",\"msg\":\"...\"}). Use a two-path p","explanation":"## Why This Is Asked\n\nTests ability to design robust cross-format parsing, time-bound selection, de-duplication, and aggregated reporting with standard tools.\n\n## Key Concepts\n\n- Time filtering with find -mmin\n- Handling .log and .log.gz uniformly\n- Parsing plain and JSON lines without external deps\n- Deduplication and per-service aggregation\n\n## Code Example\n\n```javascript\n// Placeholder: example demonstrating parsing strategy, not executable in shell\nfunction parseLine(line){/* ... */}\n```\n\n## Follow-up Questions\n\n- How would you adapt this to stream processing for 10k+ files?\n- How would you validate correctness with synthetic logs?\n","diagram":"flowchart TD\n  A[Discover files] --> B[Decompress if gz]\n  B --> C[Parse lines (plain/JSON)]\n  C --> D[Deduplicate lines]\n  D --> E[Aggregate by service]\n  E --> F[Emit top3]","difficulty":"advanced","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","MongoDB","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T05:45:50.166Z","createdAt":"2026-01-26T05:45:50.166Z"},{"id":"q-7626","question":"On a Unix host, backups are stored under /backups/<service>/ with files named backup-YYYYMMDD-HHMMSS(.gz). Some services have many files; write a robust one-liner (no external deps beyond standard UNIX tools) that prints, for every service, the path to its most recently modified backup file that is not older than 7 days. Ignore unreadable files?","answer":"Iterate per service directory, use a find-based filter for files modified within 7 days, select the newest, and print service -> path. Handles spaces and unreadable files via proper quoting and 2>/dev","explanation":"## Why This Is Asked\nTests comfort with shell one-liners, per-directory aggregation, and robust error handling in a realistic backup-check task.\n\n## Key Concepts\n- Per-directory data aggregation\n- find -mtime and -printf usage\n- Quoting for spaces and 2>/dev/null for unreadable files\n\n## Code Example\n```bash\nfor d in /backups/*/; do [ -d \"$d\" ] || continue; f=$(find \"$d\" -maxdepth 1 -type f -mtime -7 -printf '%T@ %p\\n' 2>/dev/null | sort -n | tail -n1 | cut -d' ' -f2-); if [ -n \"$f\" ]; then echo \"$d: $f\"; fi; done\n```\n\n## Follow-up Questions\n- How would you modify to include file size and human-readable date?\n- How would you integrate this into a cron job with logging?","diagram":null,"difficulty":"beginner","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T10:11:23.079Z","createdAt":"2026-01-26T10:11:23.079Z"},{"id":"q-7675","question":"Scenario: Logs are stored under /logs/tenants/*/services/*/*.log and rotated hourly into .log and .log.gz. Each line is either \"[YYYY-MM-DD HH:MM:SS] LEVEL: message\" or a JSON object {\"ts\": \"2026-01-26T14:23:05Z\", \"lvl\": \"ERROR\", \"host\": \"host-01\", \"msg\": ...}. Write a robust one-liner (no external deps beyond standard UNIX tools) that prints a per-host, per-minute histogram of ERROR events for the last 30 minutes, across all files and compressed archives, skipping unreadable files. Output: host:YYYY-MM-DDTHH:MM:SSZ count per line, sorted by time. How would you implement this?","answer":"Use a streaming pipeline: collect all .log and .log.gz under the path, read gz with zcat -f, parse both formats in awk, extract timestamp (text or ts), derive host from JSON host or path, count ERROR ","explanation":"## Why This Is Asked\nTests ability to unify mixed log formats, streaming aggregation, and robust file handling.\n\n## Key Concepts\n- Mixed-format parsing (text timestamps vs JSON ts)\n- Minute-level bucketing and 30-minute sliding window\n- On-the-fly decompression (zcat -f) and handling unreadable files\n- Deriving host from path or JSON field, multi-tenant logs\n\n## Code Example\n```bash\n# Sketch pipeline (high-level, not a complete one-liner)\nzcat -f /logs/tenants/*/services/*/*.log.gz /logs/tenants/*/services/*/*.log 2>/dev/null |\nawk '/* simple placeholder for parsing timestamps and JSON fields to extract host, ts and status */' \n```\n\n## Follow-up Questions\n- How would you test correctness with clock skew or missing fields?\n- How would you scale this for petabytes of logs across many hosts?","diagram":"flowchart TD\n  A[Start] --> B[Discover log files (.log/.log.gz)]\n  B --> C[Read files with zcat -f or cat]\n  C --> D[Parse lines: text timestamp or JSON ts]\n  D --> E[Extract host from path or JSON]\n  E --> F[Bucket by minute for ERROR events]\n  F --> G[Maintain 30-minute sliding window]\n  G --> H[Output host:minute count, sorted by time]","difficulty":"intermediate","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Google","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T12:07:27.667Z","createdAt":"2026-01-26T12:07:27.667Z"},{"id":"q-7877","question":"New angle: On a Unix host, logs under /logs/** include .log and .log.gz, with lines in either 'YYYY-MM-DD HH:MM:SS <LEVEL>: <text>' or JSON {\"ts\":...,\"service\":...,\"lvl\":...,\"msg\":...}. For a cross-dir service dashboard, write a robust one-liner that prints a per-service, per-minute histogram of WARN events for the last 45 minutes across all files, including gzipped rotations, deduplicating identical lines across rotations by a content hash (sha256 of the line). Skip unreadable files?","answer":"I would construct a robust one-liner that recursively traverses /logs for *.log and *.log.gz files, processes each readable line, extracts timestamps and service names from either plain text or JSON format, converts timestamps to minute buckets for the last 45 minutes, filters for WARN events, and aggregates counts per service per minute while deduplicating identical lines across rotations using SHA256 content hashing.","explanation":"## Why This Is Asked\nTests practical UNIX log processing across mixed formats, compression, and rotations. It also assesses deduplication strategy and time bucketing under real-world constraints.\n\n## Key Concepts\n- Mixed log formats (text and JSON)\n- Transparent gz handling\n- Time bucketing and 45-minute windowing\n- Content-hash based deduplication (sha256sum)\n- Streaming aggregation (awk/sed) per service\n\n## Code Example\n```bash\n# Skeleton outline (not a full one-liner)\nfind /logs -type f \\( -name \"*.log\" -o -name \"*.log.gz\" \\) -print0 | \\\n  while IFS= read -r -d '' f; do\n    if [[ \"$f\" == *.gz ]]; then\n      gunzip -c \"$f\" 2>/dev/null || continue\n    else\n      cat \"$f\" 2>/dev/null || continue\n    fi\n  done | \\\n  # Process lines, extract timestamps, filter WARN, deduplicate, aggregate\n  awk '...process and aggregate...'\n```\n\n## Implementation Notes\n- Use `find -print0` and `read -d ''` for safe filename handling\n- `gunzip -c` for transparent gz decompression\n- `2>/dev/null` to skip unreadable files gracefully\n- SHA256 hashing for content deduplication across rotations\n- Minute-level time bucketing with 45-minute window filter","diagram":null,"difficulty":"advanced","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T06:48:21.795Z","createdAt":"2026-01-26T21:35:04.013Z"},{"id":"q-7988","question":"On a Unix host, logs live under /logs/tenant-*/service-*/log-*.log and log.gz rotated hourly. Each line is either [YYYY-MM-DD HH:MM:SS] LEVEL: message or a JSON line like {\"ts\":\"2026-01-27T12:34:56Z\",\"lvl\":\"ERROR\",\"msg\":\"...\"}. For a streaming analytics task, write a practical but robust one-liner that, every minute, prints per-tenant-per-service per-minute counts of ERROR events in the last minute, across both formats and gzip archives, skipping unreadable files. Output format: tenant/service <YYYY-MM-DD HH:MM> <count> per line?","answer":"Approach: stream all logs via find -type f -name '*.log*' and zcat for gz; feed to awk that uses mktime to parse [YYYY-MM-DD HH:MM:SS] or a JSON ts; derive tenant and service from the path, bucket by ","explanation":"## Why This Is Asked\nTests ability to build a real-time, cross-format log analytics pipeline across multi-tenant paths with gzip handling.\n\n## Key Concepts\n- Streaming pipelines with find, zcat, and awk\n- Time parsing across two log formats with minute bucketing\n- Multi-tenant path extraction and robust error handling\n\n## Code Example\n```bash\n# conceptual outline; not a full solution\nfind /logs -type f -name '*.log*' 2>/dev/null | while read f; do zcat \"$f\" 2>/dev/null || cat \"$f\" 2>/dev/null; done | \\\nawk '...' \n```\n\n## Follow-up Questions\n- How would you adapt for a 5-minute window and higher cardinality tenants?\n- How would you scale to tens of thousands of tenants and services without hot spots?","diagram":null,"difficulty":"intermediate","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Netflix","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T04:23:56.466Z","createdAt":"2026-01-27T04:23:56.466Z"},{"id":"q-8047","question":"Scenario: On a Unix host, logs live under /logs/service-*/ with hourly rotation into .log and .log.gz. Each line is either 'YYYY-MM-DD HH:MM:SS LEVEL: text' or JSON {\"ts\":...,\"lvl\":...,\"msg\":...}. Write a robust one-liner (no external deps beyond standard UNIX tools) that detects if any line in the last 24 hours is out of order with respect to the previous line in the same file (timestamps must be non-decreasing). Handle gzipped files and unreadable files across nested dirs; output <path>:1 if out of order found, else <path>:0?","answer":"I would implement a single shell loop: enumerate files under /logs/service-*/ modified in the last 24 hours, decompress .log.gz with zcat, stream into awk that extracts timestamps from both formats an","explanation":"## Why This Is Asked\nTests ability to fuse text and JSON log formats, gz handling, and per-file monotonicity checks in production-like depth.\n\n## Key Concepts\n- Text/JSON parsing in awk\n- on-the-fly gz handling with zcat\n- per-file state across a stream\n- robust error handling in pipelines\n\n## Code Example\n```bash\nfind /logs -type f \\( -name '*.log' -o -name '*.log.gz' \\) -mtime -1 -print0 | \\\n  while IFS= read -r -d '' f; do\n    if [[ \"$f\" == *.gz ]]; then\n      data=$(zcat \"$f\" 2>/dev/null)\n    else\n      data=$(cat \"$f\" 2>/dev/null)\n    fi\n    echo \"$f|$data\" | awk 'BEGIN{out=0; prev=\"\"} { if ($0 ~ /^([0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2})/){ ts=$1\" \"$2 } else if ($0 ~ /\\{.*\"ts\".*/){ gsub(/[^0-9T:-]/, \"\", $0); ts=$0 } if (ts && prev && ts<prev) out=1; if (ts) prev=ts } } END{ print FILENAME\":\"out }'\n  done\n```\n\n## Follow-up Questions\n- How would you adapt for out-of-order across file rotations?\n- How would you extend to multi-tenant setups with reporting delays?","diagram":"flowchart TD\n  A[Enumerate log files] --> B[Decompress if gz]\n  B --> C[Extract timestamps]\n  C --> D[Detect out-of-order per file]\n  D --> E[Output file:flag]","difficulty":"advanced","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T07:14:17.146Z","createdAt":"2026-01-27T07:14:17.146Z"},{"id":"q-8366","question":"Scenario: On a Unix host, logs live under /var/log/app-*/events/** and rotate daily into .log and .log.gz. Each line is either a CSV: ts,sev,msg (ts in ISO or 'YYYY-MM-DD HH:MM:SS') or JSON {\"ts\":...,\"sev\":...,\"msg\":...}. The path includes the service name as the directory directly under events, e.g., /var/log/app-12/events/auth-service/*.log(.gz). Write a robust one-liner (no external deps beyond standard UNIX tools) that emits, for each service, the per-minute rate of ERROR events over the last 30 minutes, across formats and archives, skipping unreadable files?","answer":"Proposed answer: A single streaming shell pipeline that finds all relevant logs, streams both .log and .log.gz via zcat/cat, and uses awk to bucket by service and minute. It should parse timestamps from CSV or JSON formats, filter for ERROR severity, calculate rates over the last 30 minutes, and output per-service statistics.","explanation":"## Why This Is Asked\n\nThis question tests the ability to build a real-world, single-line log analytics pipeline that handles multiple formats, gzipped archives, per-service aggregation, and a 30-minute sliding window.\n\n## Key Concepts\n\n- Streaming text processing\n- Mixed log formats (CSV and JSON)\n- Time bucketing with epoch seconds\n- Robust file handling (gzipped and potentially unreadable files)\n- Service-based aggregation\n\n## Code Example\n\n```bash\nfind /var/log/app-*/events -type f \\( -name \"*.log\" -o -name \"*.log.gz\" \\) -print0 | \\\nwhile IFS= read -r -d '' f; do\n  if [[ \"$f\" == *.gz ]]; then zcat \"$f\" || true; else cat \"$f\" || true; fi\ndone | \\\nawk -v window=1800 -v now=\"$(date +%s)\" '\n{\n  # Extract service from path\n  match($0, /\\/events\\/([^\\/]+)\\//, arr); service = arr[1]\n  if (!service) next\n  \n  # Parse CSV or JSON timestamp\n  if ($0 ~ /^{/) {\n    # JSON format\n    gsub(/[{}\",]/, \"\")\n    split($0, parts, /[=:]/)\n    for (i in parts) {\n      if (parts[i] == \"ts\") ts = parts[i+1]\n      if (parts[i] == \"sev\") sev = parts[i+1]\n    }\n  } else {\n    # CSV format\n    split($0, parts, \",\")\n    ts = parts[1]; sev = parts[2]\n  }\n  \n  # Filter for ERROR severity within time window\n  if (sev == \"ERROR\") {\n    # Convert timestamp to epoch seconds\n    if (ts ~ \"-\") {\n      cmd = \"date -d \\\"\" ts \"\\\" +%s\"\n      cmd | getline epoch; close(cmd)\n    } else epoch = ts\n    \n    if (epoch >= now - window) {\n      bucket = int(epoch / 60) * 60\n      errors[service, bucket]++\n    }\n  }\n}\nEND {\n  for (key in errors) {\n    split(key, arr, SUBSEP)\n    printf \"%s: %.2f errors/min\\n\", arr[1], errors[key] / 1\n  }\n}'\n```\n\nThis pipeline handles mixed formats, gzipped archives, unreadable files gracefully, and provides per-service ERROR event rates over the specified time window.","diagram":"flowchart TD\n  A[Find logs] --> B[Stream lines]\n  B --> C[Parse ts and service]\n  C --> D[Bucket by minute and service]\n  D --> E[Emit per-minute ERROR rates]","difficulty":"intermediate","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","Netflix","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-28T05:33:16.832Z","createdAt":"2026-01-27T21:43:11.496Z"},{"id":"q-8494","question":"On a Unix host, logs reside under /logs and rotate hourly into .log and .log.gz. Each line is either 'YYYY-MM-DD HH:MM:SS LEVEL: text' or a JSON {\"ts\":...,\"svc\":...,\"lvl\":...,\"msg\":...}. Write a single robust one-liner (no external deps beyond standard UNIX tools) that, for the last 48 hours, prints per-service the count of unique ERROR messages observed across both formats and rotations, skipping unreadable files and deduplicating identical messages across files. Output: <svc>:<count> per line?","answer":"Single pipeline traverses the last 48 hours with find, streams both .log and .log.gz via zcat (or cat for plain logs), then uses awk to parse text or JSON, extract svc and the message, filter for ERRO","explanation":"## Why This Is Asked\nTests the ability to design a compact, robust Unix one-liner that handles mixed log formats, compressed files, per-service aggregation, and a strict time window.\n\n## Key Concepts\n- find with -mtime and -print0 for safe traversal\n- zcat vs cat for gzipped input\n- awk parsing for text and JSON lines\n- associative arrays for per-service dedup\n- robust error handling with 2>/dev/null\n\n## Code Example\n```bash\n# Outline (not runnable)\nfind /logs -type f \\( -name '*.log' -o -name '*.log.gz' \\) -mtime -2 -print0 | \\\nxargs -0 -I{} bash -lc 'zcat \"$1\" 2>/dev/null || cat \"$1\" 2>/dev/null' -- {} | \\\nawk -v since=\"$(date -u --date='48 hours ago' +%s)\" '\n  # determine service and message from text or json\n  /ERROR/ { svc=$svc; msg=$msg; count[svc\"|\"msg]++ }\n  END { for (k in count) { split(k, a, \"|\"); print a[1]\":\"count[k] } }'\n```\n\n## Follow-up Questions\n- How would you adapt this to run in a multi-tenant environment with quota controls?\n- How would you verify correctness under log rotation edge cases?","diagram":"flowchart TD\n  A[Start] --> B[Traverse last 48h logs]\n  B --> C[Stream text and json lines]\n  C --> D[Extract svc and message]\n  D --> E[Deduplicate and count per service]\n  E --> F[Output svc:count]","difficulty":"advanced","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Google","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-28T05:50:11.566Z","createdAt":"2026-01-28T05:50:11.566Z"},{"id":"q-894","question":"On a Linux host, /var/log/myapp.log is written by multiple processes. Implement a robust log rotation that triggers when the file reaches 100MB, keeps 7 rotated files, compresses older logs, and ensures no log loss while writers continue. Describe the approach, commands, and failure modes for concurrent writers?","answer":"Configure logrotate for /var/log/myapp.log with size 100M, rotate 7, compress and delaycompress, using sharedscripts. Use a postrotate block to signal the processes to reopen the log (e.g., systemctl ","explanation":"## Why This Is Asked\\n\\nThis question probes practical log management under concurrency, emphasizing safe rotation with multi-process writers and minimizing data loss. Expect discussion of reopen signals vs copytruncate, service integration, and failure modes.\\n\\n## Key Concepts\\n- logrotate configuration: size-based rotation, retention, compression\\n- handling concurrent writers: reopen vs copytruncate, signals (SIGHUP)\\n- service integration: systemd or pid signaling; permissions and timing\\n\\n## Code Example\\n```javascript\\n/var/log/myapp.log {\\n  size 100M\\n  rotate 7\\n  compress\\n  delaycompress\\n  missingok\\n  notifempty\\n  create 0644 root root\\n  sharedscripts\\n  postrotate\\n    systemctl is-active myapp >/dev/null && systemctl kill -s HUP myapp || kill -HUP `cat /var/run/myapp.pid` 2>/dev/null || true\\n  endscript\\n}\\n```\\n\\n## Follow-up Questions\\n- What if the app cannot reopen log files on HUP?\\n- How would you test concurrent writers during rotation?","diagram":null,"difficulty":"intermediate","tags":["unix"],"channel":"unix","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Discord","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T14:35:24.296Z","createdAt":"2026-01-12T14:35:24.296Z"},{"id":"q-264","question":"How do Unix pipes enable inter-process communication and what are their performance implications?","answer":"Pipes provide unidirectional byte streams between processes, utilizing kernel buffers for efficient inter-process communication with blocking I/O semantics.","explanation":"## Why Asked\nTests understanding of IPC mechanisms and system design principles for scalable applications.\n\n## Key Concepts\nUnidirectional communication, kernel buffering, blocking I/O, file descriptor abstraction, pipe capacity limits.\n\n## Code Example\n```bash\n# Create pipe and connect processes\nls -l | grep \".txt\" | wc -l\n# Kernel manages 64KB buffer between processes\n```\n\n## Follow-up Questions\nWhat's the difference between named and anonymous pipes? How do pipes handle backpressure? What are alternatives to pipes?","diagram":"flowchart TD\n  A[Process A] -->|writes| B[Pipe Buffer]\n  B -->|reads| C[Process B]\n  D[Kernel] -->|manages| B","difficulty":"beginner","tags":["posix","signals","pipes","sockets"],"channel":"unix","subChannel":"system-programming","sourceUrl":"https://man7.org/linux/man-pages/pipe.2","videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Apple","Google","Meta","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-03T06:38:32.248Z","createdAt":"2025-12-26 12:51:07"}],"subChannels":["general","system-programming"],"companies":["Adobe","Airbnb","Amazon","Anthropic","Apple","Bloomberg","Citadel","Cloudflare","Coinbase","Databricks","Discord","DoorDash","Goldman Sachs","Google","Hashicorp","Hugging Face","IBM","Instacart","LinkedIn","Lyft","Meta","Microsoft","MongoDB","NVIDIA","Netflix","OpenAI","Oracle","PayPal","Plaid","Robinhood","Salesforce","Scale Ai","Slack","Snap","Snowflake","Square","Stripe","Tesla","Twitter","Two Sigma","Uber","Zoom"],"stats":{"total":85,"beginner":32,"intermediate":25,"advanced":28,"newThisWeek":35}}