{"questions":[{"id":"cka-cluster-arch-1768193310463-0","question":"During a kubeadm-based HA cluster setup with three control-plane nodes, which design choice ensures etcd quorum remains available if any one control-plane node fails?","answer":"[{\"id\":\"a\",\"text\":\"Run a single etcd member on one control-plane node and rely on API server replication\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Run three etcd members, one on each control-plane node\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Run five etcd members spanning two control-plane nodes and one worker node\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Run etcd as a cluster on all nodes with data replicated in memory only\",\"isCorrect\":false}]","explanation":"## Correct Answer\nB. Run three etcd members, one on each control-plane node\n\nEtcd quorum requires a majority of members to be healthy. With three members, the majority is 2, so if one control-plane node fails, the remaining two etcd members still form a quorum and the cluster continues to operate. \n\n## Why Other Options Are Wrong\n- A: A single etcd member cannot maintain quorum if it fails, breaking HA.\n- C: Five members across two nodes is not feasible and would still risk quorum loss if nodes fail; etcd should be spread across distinct nodes for HA.\n- D: Etcd data must be persisted on disk; in-memory only would lose state on restart and is not HA.\n\n## Key Concepts\n- Etcd quorum and HA require an odd number of members\n- Distributing etcd across control-plane nodes supports resilience\n- Persistent storage for etcd is essential\n- Use a load-balanced API endpoint to keep API access stable\n\n## Real-World Application\n- For a production-grade small cluster, deploy a 3-member etcd cluster across the 3 control-plane nodes and front the API with a load balancer to maintain availability during node failures.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","kubeadm","etcd","HA","AWS","Terraform","certification-mcq","domain-weight-25"],"channel":"cka","subChannel":"cluster-arch","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T04:48:30.465Z","createdAt":"2026-01-12 04:48:31"},{"id":"cka-cluster-arch-1768193310463-1","question":"In a 3-control-plane HA cluster behind a DNS load balancer, the API server certificate SANs currently include only control-plane IPs. You cannot connect to the API server via the DNS name k8s-api.example.com. What is the recommended fix?","answer":"[{\"id\":\"a\",\"text\":\"Recreate the cluster using --apiserver-cert-extra-sans to include the DNS name\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Add an /etc/hosts entry on every client to map k8s-api.example.com to one control-plane IP\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use the IP-based endpoint instead of the DNS name\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Disable TLS on the API server to bypass SAN checks\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA. Recreate the cluster using --apiserver-cert-extra-sans to include the DNS name\n\nThe API server certificate must include SAN entries for any DNS names used to access the cluster. If the DNS name k8s-api.example.com is not present in the certificate, TLS handshakes fail when clients connect via that name. Regenerate or rotate the API server certificate to include the DNS SAN and apply the changes.\n\n## Why Other Options Are Wrong\n- B: Modifying hosts files does not fix TLS SAN validation and is not scalable for multi-client access.\n- C: Using IPs avoids DNS name usage but defeats the HA access pattern via DNS name and may still fail if IPs change.\n- D: Disabling TLS completely is insecure and not acceptable in production.\n\n## Key Concepts\n- TLS SAN and API server certificates\n- Control-plane endpoint DNS vs IPs\n- Certificate rotation for kubeadm-managed clusters\n\n## Real-World Application\n- When exposing the API via a DNS name behind a load balancer, ensure the certificate SAN includes that DNS name to prevent client TLS errors.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","kubeadm","etcd","HA","AWS","Terraform","certification-mcq","domain-weight-25"],"channel":"cka","subChannel":"cluster-arch","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T04:48:31.277Z","createdAt":"2026-01-12 04:48:31"},{"id":"cka-cluster-arch-1768193310463-2","question":"You want to enable encryption at rest for Kubernetes Secrets in a kubeadm-managed cluster. Which action is required?","answer":"[{\"id\":\"a\",\"text\":\"Provide an encryption configuration file and pass it to the API server via --encryption-provider-config, then rotate keys to re-encrypt existing data\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Enable encryption at rest by turning on encryption on etcd\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Store encryption keys in a Kubernetes Secret so they are encrypted by etcd\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Encrypt all pods' data at rest using a runtime encryption feature\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA. Provide an encryption configuration file and pass it to the API server via --encryption-provider-config, then rotate keys to re-encrypt existing data\n\nTo enable encryption at rest for Secrets, you must configure the API server with an EncryptionConfig that specifies a provider (e.g., AES-CBC or a KMS provider) and restart the API server. After enabling, you typically rotate the encryption keys to re-encrypt existing data in etcd. This ensures secrets are stored encrypted at rest. \n\n## Why Other Options Are Wrong\n- B: Encryption at rest is configured via the API server, not by enabling encryption on etcd itself. \n- C: Keys should be managed by the encryption provider; storing keys in a Secret does not secure the data at rest. \n- D: Pod data encryption is not the standard method for Secrets; it’s about secret storage in etcd. \n\n## Key Concepts\n- Encryption provider configuration\n- API server certificate and TLS considerations not applicable here\n- Data at rest for secrets in etcd\n- Key rotation and re-encryption process\n\n## Real-World Application\n- In production, enable encryption at rest for Secrets to meet compliance requirements; implement a proper KMS-backed provider and plan key rotation.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","kubeadm","etcd","Encryption","AWS","Terraform","certification-mcq","domain-weight-25"],"channel":"cka","subChannel":"cluster-arch","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T04:48:31.677Z","createdAt":"2026-01-12 04:48:31"},{"id":"cka-cluster-arch-1768267653287-0","question":"You are designing a five-node on-prem Kubernetes cluster and want high availability for etcd without risking data loss. Which approach is recommended to ensure etcd HA and cluster resilience?","answer":"[{\"id\":\"a\",\"text\":\"Run etcd as a static pod on all control-plane nodes\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use an external etcd cluster with at least three nodes separate from the control-plane\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Run etcd on every worker node\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use a single etcd instance on the primary control-plane\",\"isCorrect\":false}]","explanation":"## Correct Answer\n**Option B** is correct because an external etcd cluster provides quorum replication across failure domains, separating etcd from control-plane to improve resilience. A 3-node etcd cluster is the minimum to maintain quorum in the event of a node failure. Running etcd as a static pod on control-plane nodes (A) ties etcd life to those nodes and is not HA across separate failures. Running etcd on workers (C) expands the surface area and risks scheduling conflicts. A single etcd instance on the primary control-plane (D) is a single point of failure.\n\n## Why Other Options Are Wrong\n- A: Static pods on control-plane nodes tie etcd to those nodes; if a control-plane node fails, etcd may lose quorum.\n- C: etcd on workers is not a recommended HA design.\n- D: Single instance cannot tolerate node failures.\n\n## Key Concepts\n- etcd quorum and HA\n- External etcd vs embedded etcd\n- Odd node counts for quorum\n\n## Real-World Application\n- In production, deploy etcd as an independent cluster (3 or 5 nodes) and point the API server to it to improve resilience during outages.","diagram":null,"difficulty":"intermediate","tags":["kubernetes","kubeadm","etcd","on-prem","aws","eks","terraform","HA","certification-mcq","domain-weight-25"],"channel":"cka","subChannel":"cluster-arch","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T01:27:33.288Z","createdAt":"2026-01-13 01:27:33"},{"id":"cka-cluster-arch-1768267653287-1","question":"After migrating from PodSecurityPolicy to Pod Security Standards, which approach enforces baseline policies by namespace labeling?","answer":"[{\"id\":\"a\",\"text\":\"Enable PodSecurityPolicy in the API server\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use Pod Security Admission with namespace labels to enforce baseline policies\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Use NetworkPolicy to enforce security boundaries\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use OPA Gatekeeper to enforce policies\",\"isCorrect\":false}]","explanation":"## Correct Answer\n**Option B** is correct because Pod Security Standards are enforced via Pod Security Admission, operating with namespace labels to set baseline policies. PSP has been deprecated and removed in newer Kubernetes versions; A is incorrect as PSP is obsolete; C relates to network security rather than pod security level baseline; D is an optional policy engine, not the built-in baseline replacement.\n\n## Why Other Options Are Wrong\n- A: PodSecurityPolicy is deprecated/removed and not available by default.\n- C: NetworkPolicy enforces network rules, not pod security level baseline.\n- D: OPA Gatekeeper is a policy engine, not the built-in baseline enforcement.\n\n## Key Concepts\n- Pod Security Standards\n- Pod Security Admission\n- PSP deprecation timeline\n\n## Real-World Application\n- Plan migrating clusters to Pod Security Standards to maintain consistent security baselines across namespaces.","diagram":null,"difficulty":"intermediate","tags":["kubernetes","pod-security","psp","policy","opa","gatekeeper","aws","eks","certification-mcq","domain-weight-25"],"channel":"cka","subChannel":"cluster-arch","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T01:27:33.761Z","createdAt":"2026-01-13 01:27:34"},{"id":"cka-cluster-arch-1768267653287-2","question":"During installation, you notice kubelet and containerd use different cgroup drivers, causing instability. What is the recommended fix to ensure stable resource accounting and avoid crashes?","answer":"[{\"id\":\"a\",\"text\":\"Configure containerd to use cgroupfs and set kubelet to cgroupfs\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Disable cgroups in kubelet\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Configure containerd to use systemd and set kubelet to systemd\",\"isCorrect\":true},{\"id\":\"d\",\"text\":\"Switch the cluster to Docker runtime\",\"isCorrect\":false}]","explanation":"## Correct Answer\n**Option C** is correct because aligning the container runtime cgroup driver with the kubelet (both to systemd) ensures consistent resource accounting and avoids crashes caused by driver mismatches. A is plausible but incorrect because mixed drivers can still fail; B is invalid; D is not recommended as Docker is deprecated in recent Kubernetes versions.\n\n## Why Other Options Are Wrong\n- A: If you switch to cgroupfs for both, you may still get issues; modern guidance prefers systemd.\n- B: Disabling cgroups is not feasible or safe.\n- D: Docker runtime is deprecated/unsupported in recent Kubernetes versions.\n\n## Key Concepts\n- cgroup drivers: systemd vs cgroupfs\n- kubelet configuration\n- containerd runtime integration\n\n## Real-World Application\n- After installation, verify /sys/fs/cgroup and kubelet logs to ensure both use systemd and restart services if needed.","diagram":null,"difficulty":"intermediate","tags":["kubernetes","containerd","systemd","cgroup","kubelet","aws","eks","terraform","certification-mcq","domain-weight-25"],"channel":"cka","subChannel":"cluster-arch","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T01:27:34.235Z","createdAt":"2026-01-13 01:27:34"},{"id":"cka-cluster-arch-1768267653287-3","question":"You are upgrading a single-control-plane Kubernetes cluster managed with kubeadm. What is the first step you should run to verify available upgrade options?","answer":"[{\"id\":\"a\",\"text\":\"kubeadm upgrade plan\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"kubeadm upgrade apply --version\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"kubectl drain control-plane --ignore-daemonsets\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"apt-get upgrade kubeadm\",\"isCorrect\":false}]","explanation":"## Correct Answer\n**Option A** is correct because kubeadm upgrade plan shows available upgrade versions and informs the administrator of compatibility and constraints. B is not valid as there is no separate kubectl upgrade command for cluster upgrades. C is a drain operation, not an upgrade planning step. D upgrades the package without evaluating upgrade feasibility.\n\n## Why Other Options Are Wrong\n- B: There is no kubectl upgrade command for cluster version upgrades.\n- C: Draining is part of a rolling upgrade workflow but not the first step to plan the upgrade.\n- D: Upgrading kubeadm without planning can lead to compatibility issues.\n\n## Key Concepts\n- kubeadm upgrade plan\n- Upgrade workflow for control plane\n- Downtime considerations\n\n## Real-World Application\n- Always run kubeadm upgrade plan before applying the upgrade to understand version compatibility and required steps.","diagram":null,"difficulty":"intermediate","tags":["kubernetes","kubeadm","upgrade","controls-plane","aws","eks","terraform","certification-mcq","domain-weight-25"],"channel":"cka","subChannel":"cluster-arch","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T01:27:34.389Z","createdAt":"2026-01-13 01:27:34"},{"id":"cka-cluster-arch-1768267653287-4","question":"A cluster uses a CSI driver for dynamic PV provisioning. You want to avoid data loss when a PVC is deleted and retain the underlying PV data for manual reclamation. Which PersistentVolumeReclaimPolicy should you configure on the PV?","answer":"[{\"id\":\"a\",\"text\":\"Delete\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Retain\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Recycle\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Bind\",\"isCorrect\":false}]","explanation":"## Correct Answer\n**Option B** is correct because Retain keeps the PV and its data when the PVC is deleted, allowing manual reclamation or reuse after cleanup. Delete would remove the PV and the backing storage, and Recycle is deprecated and no longer recommended. Bind is not a valid reclaim policy.\n\n## Why Other Options Are Wrong\n- A: Delete would delete the PV and the underlying storage.\n- C: Recycle is deprecated and not recommended.\n- D: Bind is not a reclaim policy.\n\n## Key Concepts\n- CSI dynamic provisioning\n- PV reclaim policies: Delete, Retain, Recycle\n- Data preservation vs. automatic cleanup\n\n## Real-World Application\n- Use Retain for volumes containing sensitive or critical data that require careful manual handling before reuse.","diagram":null,"difficulty":"intermediate","tags":["kubernetes","storage","persistent-volumes","csi","aws","eks","terraform","certification-mcq","domain-weight-25"],"channel":"cka","subChannel":"cluster-arch","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T01:27:34.553Z","createdAt":"2026-01-13 01:27:34"},{"id":"q-1087","question":"You're running a 5-node HA Kubernetes control plane (3 in AZ-a, 2 in AZ-b) with a 3-member etcd cluster. After a regional outage, etcd loses quorum. Describe exact, command-level steps to restore quorum, rejoin the third member, and validate API availability across both AZs, including risk notes and DR readiness checks?","answer":"Identify the two healthy etcd members and verify quorum with etcdctl member list; check cluster-health. Stop the out-of-quorum member, restore the missing member from the latest snapshot using etcdctl","explanation":"## Why This Is Asked\nTests practical DR/HA recovery for etcd and API availability in multi-AZ. Requires exact commands and sequencing.\n\n## Key Concepts\n- etcd quorum and member lifecycle\n- Snapshot restore with initial-cluster state\n- HA API server restart order across AZs\n- DR readiness, RPO/RTO implications\n\n## Code Example\n```javascript\n// Illustrative DR restore flow (non-production, for understanding)\nconst {execSync} = require('child_process')\nexecSync(\"etcdctl snapshot restore snapshot.db --name etcd2 --initial-cluster 'etcd0=https://A:2380,etcd1=https://A2:2380,etcd2=https://B:2380' --initial-cluster-state=new\", {stdio:'inherit'})\n```\n\n## Follow-up Questions\n- How would you test this upgrade procedure to minimize downtime?\n- What are risks of multi-AZ etcd topologies and how would you mitigate them?","diagram":null,"difficulty":"intermediate","tags":["cka"],"channel":"cka","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Scale Ai","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T22:20:40.941Z","createdAt":"2026-01-12T22:20:40.941Z"},{"id":"q-874","question":"In a Kubernetes cluster used by Salesforce/Cloudflare/Snap engineers, a Deployment's startup latency rose from 1–2s to 6–8s after introducing an initContainer that runs a health check before the application starts. Describe how you would diagnose, what metrics/logs to collect, and concrete fixes (e.g., moving checks to readiness, caching, parallel init, or canary rollout). Include rollback and validation steps?","answer":"Capture startup timings and initContainer logs, kubectl describe pod, and events; monitor readiness transitions and image pulls. If init work is heavy, move health checks to readiness, cache results, ","explanation":"## Why This Is Asked\nThis question probes practical troubleshooting of startup latency caused by init containers in a real Kubernetes setup, emphasizing observability, risk-aware fixes, and rollback discipline.\n\n## Key Concepts\n- Kubernetes readiness vs startup probes\n- InitContainers vs parallel init\n- Canary rollouts and rollback\n- Observability: pod events, container logs, metrics\n\n## Code Example\n```yaml\nreadinessProbe:\n  exec:\n    command: [\"bash\",\"-lc\",\"echo ok\"]\n  initialDelaySeconds: 5\n  periodSeconds: 10\n```\n\n## Follow-up Questions\n- How would you gate a rollout to avoid user impact during a fix?\n- What would you monitor post-rollout to ensure latency doesn't regress?","diagram":"flowchart TD\n  A[Baseline Timings] --> B{InitContainer}\n  B --> C[Measure Startup Latency]\n  C --> D[Diagnosis & Fix Plan]\n  D --> E[Rollout Canary]\n  E --> F[Validate Metrics]","difficulty":"intermediate","tags":["cka"],"channel":"cka","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Salesforce","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:55:35.532Z","createdAt":"2026-01-12T13:55:35.532Z"},{"id":"q-893","question":"You manage a 3-node Kubernetes control plane backed by an etcd cluster. After a power outage, one etcd member reports corruption. Describe the exact steps to detect the corrupted member, restore from a known-good snapshot, rejoin the cluster, and validate API availability. Include concrete commands, risk notes, and how you would verify DR readiness?","answer":"Verify health with etcdctl endpoint health for all endpoints, then identify the corrupted member via etcdctl member list/status. Stop the faulty node, restore a known-good snapshot using etcdctl snaps","explanation":"## Why This Is Asked\nInterview context explanation.\n\n## Key Concepts\n- etcd health checks\n- Snapshot restore and initial-cluster config\n- Member lifecycle and data-dir safety\n- Validation of API and cluster state\n\n## Code Example\n```bash\netcdctl endpoint health --endpoints=https://node1:2379,https://node2:2379,https://node3:2379\n```\n\n## Follow-up Questions\n- How would you automate this DR runbook?\n","diagram":null,"difficulty":"intermediate","tags":["cka"],"channel":"cka","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","IBM","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T14:34:21.100Z","createdAt":"2026-01-12T14:34:21.100Z"},{"id":"q-936","question":"A 3-control-plane Kubernetes cluster on AWS experiences API server latency spikes after a webhook deployment. The admission webhook is malfunctioning and causing slow requests; outline precise steps to identify the failing webhook, safely disable it to restore API responsiveness, validate cluster availability, and prepare a rollback plan with minimal downtime?","answer":"Capture API server latency from metrics, then list webhook configurations: kubectl get mutatingwebhookconfiguration, validatingwebhookconfiguration. Identify culprit by error rate and latency. Patch t","explanation":"## Why This Is Asked\n\nInterviews gauge practical debugging of admission webhooks and API responsiveness under failure, plus rollback discipline in a live cluster.\n\n## Key Concepts\n\n- apiserver metrics and profiling\n- MutatingWebhookConfiguration and ValidatingWebhookConfiguration\n- safe-disable/rollback patterns\n- impact on cluster availability and security\n\n## Code Example\n\n```bash\n# Disable a specific webhook by removing it from the MutatingWebhookConfiguration\nkubectl get mutatingwebhookconfiguration <name> -o json | \\\n  jq 'del(.webhooks[] | select(.name == \"<target-webhook-name>\"))' | \\\n  kubectl apply -f -\n```\n```\n\n## Follow-up Questions\n\n- How would you test disablement in a non-prod cluster with minimal risk?\n- How would you ensure a controlled rollback if the webhook changes cause issues?\n","diagram":"flowchart TD\n  A[Start] --> B[Check apiserver metrics]\n  B --> C{Culprit found?}\n  C -->|Yes| D[Patch MutatingWebhookConfiguration to remove culprit]\n  C -->|No| E[Check ValidatingWebhookConfiguration]\n  D --> F[Validate API responsiveness with kubectl get ns]\n  F --> G{Healthy?}\n  G -->|Yes| H[Document rollback plan and monitor]\n  G -->|No| I[Escalate and revert changes]\n","difficulty":"intermediate","tags":["cka"],"channel":"cka","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Databricks","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T16:25:48.349Z","createdAt":"2026-01-12T16:25:48.349Z"},{"id":"cka-services-1768221805634-0","question":"Scenario: A Kubernetes cluster has a frontend service in namespace frontend and a backend service in namespace backend. You want to ensure that only pods from the frontend namespace can reach the backend service on port 8080, and that traffic from all other sources is denied. Which NetworkPolicy best achieves this?","answer":"[{\"id\":\"a\",\"text\":\"A NetworkPolicy in the backend namespace selecting pods app=backend and allowing ingress only from pods in the frontend namespace labeled app=frontend on port 8080.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"A NetworkPolicy in the backend namespace selecting pods app=backend and allowing ingress from any source on port 8080.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"A NetworkPolicy in the frontend namespace selecting pods app=frontend and allowing ingress from the backend namespace on port 8080.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"A NetworkPolicy in the backend namespace selecting pods app=backend and allowing ingress from the frontend namespace labeled name=frontend but port 80.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption a is correct because it explicitly binds the policy to backend pods and restricts ingress to only frontend pods from the frontend namespace on port 8080, implementing a least-privilege policy across namespaces.\n\n## Why Other Options Are Wrong\n- Option b would allow traffic from any source, defeating the cross-namespace restriction.\n- Option c applies the policy in the frontend namespace and would not protect the backend service.\n- Option d references a cross-namespace source but specifies port 80, not the required 8080, and its policy scope is incorrect.\n\n## Key Concepts\n- NetworkPolicy: namespace scoping, podSelector, namespaceSelector, ingress rules, ports\n- Default-deny behavior when proper NetworkPolicies exist\n- Cross-namespace traffic control in Kubernetes\n\n## Real-World Application\n- Ensures service segmentation between teams and environments in multi-namespace clusters, reducing blast radius if a frontend pod is compromised.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","Kubernetes-Networking","NetworkPolicy","EKS","certification-mcq","domain-weight-20"],"channel":"cka","subChannel":"services","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T12:43:25.636Z","createdAt":"2026-01-12 12:43:26"},{"id":"cka-services-1768221805634-1","question":"Scenario: You deploy a Kubernetes Service of type LoadBalancer in AWS EKS. To preserve the original client IP in your backend pods for request logging, which service configuration should you rely on?","answer":"[{\"id\":\"a\",\"text\":\"externalTrafficPolicy: Local\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"externalTrafficPolicy: Cluster\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"service.beta.kubernetes.io/aws-load-balancer-proxy-protocol: \\\"*\\\"\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"loadBalancerIP: 1.2.3.4\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption a is correct because externalTrafficPolicy Local preserves the original client IP by not SNATing traffic to the node IP, allowing backend pods to see the client's IP.\n\n## Why Other Options Are Wrong\n- Option b would SNAT client IP to the node IP, so the backend sees the node IP instead of the client IP.\n- Option c enables the PROXY protocol, which can pass client IP in some setups but is not the standard or universally reliable method for all AWS Load Balancers and configurations.\n- Option d fixes the external IP of the load balancer and does not address client IP preservation.\n\n## Key Concepts\n- externalTrafficPolicy (Local vs Cluster)\n- Client IP preservation in Services\n- AWS Load Balancer integration with Kubernetes\n\n## Real-World Application\n- Enables accurate logging for security and auditing when services are exposed via a cloud load balancer.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","Kubernetes-Networking","LoadBalancer","EKS","certification-mcq","domain-weight-20"],"channel":"cka","subChannel":"services","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T12:43:26.121Z","createdAt":"2026-01-12 12:43:26"},{"id":"cka-services-1768221805634-2","question":"Scenario: A pod in namespace frontend tries to reach a service named inventory in namespace backend using the in-cluster DNS. The DNS name inventory is not resolving. What is the correct way to reference the cross-namespace service?","answer":"[{\"id\":\"a\",\"text\":\"inventory.backend.svc.cluster.local\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"inventory.svc.cluster.local\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"inventory.default.svc.cluster.local\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"inventory.backend.svc.cluster.local.local\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption a is correct because cross-namespace service discovery in Kubernetes uses the fully qualified domain name in the form serviceName.namespace.svc.cluster.local.\n\n## Why Other Options Are Wrong\n- Option b refers to the service using only the current namespace, which fails across namespaces.\n- Option c points to the default namespace, which is incorrect for a backend named inventory in the backend namespace.\n- Option d adds an extra .local segment, making it an invalid DNS name for Kubernetes in-cluster service discovery.\n\n## Key Concepts\n- In-cluster DNS and service discovery\n- FQDN format: serviceName.namespace.svc.cluster.local\n- Cross-namespace service access patterns\n\n## Real-World Application\n- Enables microservices to reliably discover and communicate with cross-namespace services in a multi-team cluster.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","Kubernetes-Networking","DNS","EKS","certification-mcq","domain-weight-20"],"channel":"cka","subChannel":"services","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T12:43:26.602Z","createdAt":"2026-01-12 12:43:26"},{"id":"cka-services-1768286110309-0","question":"In a bare-metal Kubernetes cluster using MetalLB, you want to expose an app on port 80 with a fixed external IP 203.0.113.10. Which approach should you take?","answer":"[{\"id\":\"a\",\"text\":\"Create a NodePort service and configure MetalLB to advertise 203.0.113.10\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Create a LoadBalancer service and configure MetalLB to advertise 203.0.113.10\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Create a ClusterIP service and expose via kubectl port-forward\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use an Ingress resource with TLS termination\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct. In a bare-metal cluster, MetalLB provides the Kubernetes LoadBalancer implementation. You expose the app by creating a Service of type LoadBalancer and ensure MetalLB's address pool includes the requested external IP (e.g., 203.0.113.10). The service will then advertise that IP and route traffic to the endpoints.\n\n## Why Other Options Are Wrong\n- A: NodePort exposes a port on each node, but MetalLB won't reserve a single fixed external IP unless backed by a LoadBalancer service; this approach is less predictable and not how MetalLB is intended to be used for fixed external IPs.\n- C: ClusterIP with port-forward does not provide a stable external IP or load-balancing, making it unsuitable for external clients.\n- D: Ingress requires an Ingress controller and typically handles L7 routing, but it does not provide a fixed external IP in the same way a LoadBalancer service does.\n\n## Key Concepts\n- MetalLB provides LoadBalancer support on bare-metal clusters.\n- Service type LoadBalancer integrates with MetalLB to expose a stable external IP.\n- loadBalancerIP (optional) can be specified to reserve a fixed IP.\n\n## Real-World Application\nUsed when deploying on bare-metal or private clouds where cloud LB isn’t available, ensuring external clients can reliably reach services via a stable IP.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","Networking","LoadBalancer","MetalLB","Bare Metal","certification-mcq","domain-weight-20"],"channel":"cka","subChannel":"services","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T06:35:10.310Z","createdAt":"2026-01-13 06:35:10"},{"id":"cka-services-1768286110309-1","question":"After a CoreDNS upgrade, pod names inside the cluster stop resolving DNS; CoreDNS pods show status running but the Corefile lacks the Kubernetes plugin. Which action fixes this?","answer":"[{\"id\":\"a\",\"text\":\"Update CoreDNS Corefile to include the kubernetes plugin and reload CoreDNS\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Add an A record in /etc/hosts on each node\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Switch DNS policy to Default\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Delete CoreDNS and use kube-dns\",\"isCorrect\":false}]","explanation":"## Correct Answer\nAdding the kubernetes plugin to CoreDNS Corefile ensures in-cluster DNS for service names works again. After updating Corefile, reload or roll the CoreDNS pods so changes take effect.\n\n## Why Other Options Are Wrong\n- A: Modifying hosts files only affects local resolution and does not restore cluster DNS provisioning.\n- C: DNS policy affects pod DNS behavior but does not fix missing CoreDNS plugin configuration.\n- D: kube-dns is deprecated in newer clusters; replacing components is unnecessary and risks compatibility issues.\n\n## Key Concepts\n- CoreDNS uses plugins like kubernetes to resolve in-cluster service names.\n- CoreDNS must be properly configured and reloaded after Corefile changes.\n\n## Real-World Application\nCrucial after cluster upgrades that modify DNS components or CoreDNS deployments; ensures internal service discovery remains functional.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","CoreDNS","DNS","Networking","certification-mcq","domain-weight-20"],"channel":"cka","subChannel":"services","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T06:35:10.715Z","createdAt":"2026-01-13 06:35:10"},{"id":"cka-services-1768286110309-2","question":"You want to block all traffic to the backend pods except traffic from frontend pods within the same namespace, allowing only port 80. Which NetworkPolicy would achieve this?","answer":"[{\"id\":\"a\",\"text\":\"apiVersion: networking.k8s.io/v1\\nkind: NetworkPolicy\\nmetadata:\\n  name: allow-from-frontend\\nspec:\\n  podSelector:\\n    matchLabels:\\n      app: backend\\n  policyTypes:\\n  - Ingress\\n  ingress:\\n  - from:\\n    - podSelector:\\n        matchLabels:\\n          app: frontend\\n    ports:\\n    - protocol: TCP\\n      port: 80\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"apiVersion: networking.k8s.io/v1\\nkind: NetworkPolicy\\nmetadata:\\n  name: allow-from-frontend-ns\\nspec:\\n  podSelector:\\n    matchLabels:\\n      app: backend\\n  policyTypes:\\n  - Ingress\\n  ingress:\\n  - from:\\n    - namespaceSelector:\\n        matchLabels:\\n          name: frontend\\n    ports:\\n    - protocol: TCP\\n      port: 80\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"apiVersion: networking.k8s.io/v1\\nkind: NetworkPolicy\\nmetadata:\\n  name: allow-all\\nspec:\\n  podSelector:\\n    matchLabels:\\n      app: backend\\n  policyTypes:\\n  - Ingress\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"apiVersion: networking.k8s.io/v1\\nkind: NetworkPolicy\\nmetadata:\\n  name: deny-all-egress\\nspec:\\n  podSelector:\\n    matchLabels:\\n      app: backend\\n  policyTypes:\\n  - Egress\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption a is correct. The NetworkPolicy selects backend pods and permits Ingress only from pods labeled app=frontend within the same namespace, on port 80. This enforces the desired isolation.\n\n## Why Other Options Are Wrong\n- b uses a namespaceSelector; it would allow from a frontend namespace if labeled accordingly, which might not be the same namespace, defeating the scope.\n- c allows all ingress traffic because there is no from restriction, defeating the intention.\n- d controls egress instead of ingress and would not restrict inbound traffic as required.\n\n## Key Concepts\n- NetworkPolicy can restrict ingress to specific sources via podSelector and namespaceSelector.\n- Ingress rules need explicit ports to limit traffic to a specific port.\n\n## Real-World Application\nEssential for multi-tier deployments where frontend services must be the only allowed clients for backend services, reducing blast radius.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","Networking","NetworkPolicy","Security","certification-mcq","domain-weight-20"],"channel":"cka","subChannel":"services","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T06:35:11.065Z","createdAt":"2026-01-13 06:35:11"},{"id":"cka-services-1768286110309-3","question":"You need TLS termination at L7 and host-based routing for multiple services in Kubernetes. Which resource would you configure?","answer":"[{\"id\":\"a\",\"text\":\"Service of type LoadBalancer with TLS certificates\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Ingress resource with TLS and a secret that contains the certificate\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Headless service with TLS\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"NetworkPolicy to enforce TLS\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct. An Ingress resource with TLS configured via a secret enables L7 routing based on hostnames, making it ideal for multiple services behind a single entry point with TLS termination.\n\n## Why Other Options Are Wrong\n- a: While LoadBalancer can terminate TLS with a controller, Ingress is the standard approach for host-based routing and TLS with multiple services.\n- c: A Headless service does not provide TLS termination or host-based routing.\n- d: NetworkPolicy controls traffic, not TLS termination or routing rules.\n\n## Key Concepts\n- Ingress + TLS secret enables host-based routing and TLS termination.\n- Ingress controllers (e.g., Nginx, Traefik) implement the TLS termination and routing rules.\n\n## Real-World Application\nCommon in multi-service applications where a single external URL pattern maps to many internal services with per-host TLS certificates.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","Ingress","TLS","Networking","certification-mcq","domain-weight-20"],"channel":"cka","subChannel":"services","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T06:35:11.190Z","createdAt":"2026-01-13 06:35:11"},{"id":"cka-services-1768286110309-4","question":"In a cluster using Calico as the CNI, you want to ensure that pods receive IPs from a dedicated block to reduce IP collisions when migrating workloads across namespaces. Which approach best aligns with Calico's IPAM capabilities?","answer":"[{\"id\":\"a\",\"text\":\"Create a separate IPPool with a unique CIDR per tenant and assign workloads to that pool\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Use a single global IP pool and rely on namespace quotas to prevent collisions\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Assign IPs statically to pods during deployment\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Disable IPAM and use hostNetwork for all pods\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption a is correct. Calico IPPools allow defining distinct CIDRs for IP allocation. By creating a separate IPPool per tenant or namespace, you can reduce cross-namespace IP collisions and enforce clearer IP management.\n\n## Why Other Options Are Wrong\n- b: A single global pool increases collision risk across tenants; namespaces do not automatically isolate IP ranges.\n- c: Static IP assignment is brittle and conflicts with dynamic pod creation.\n- d: Disabling IPAM and using hostNetwork defeats the purpose of IP-based inter-pod communication and isolation.\n\n## Key Concepts\n- Calico IPPool defines CIDR blocks for IP assignment.\n- IP allocations are dynamic and per-pool; multiple pools enable tenant isolation.\n\n## Real-World Application\nUseful in multi-tenant environments where predictable pod IP ranges simplify firewalling and monitoring.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","Networking","Calico","IPPool","CNI","certification-mcq","domain-weight-20"],"channel":"cka","subChannel":"services","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T06:35:11.315Z","createdAt":"2026-01-13 06:35:11"},{"id":"cka-storage-1768249705753-0","question":"When a PVC is dynamically provisioned and its StorageClass has reclaimPolicy: Delete, what happens to the backing storage and PV object after you delete the PVC?","answer":"[{\"id\":\"a\",\"text\":\"The PV becomes Released and is retained with its data on the backing storage\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"The external provisioner deletes the backing volume and the PV object is removed\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"The PV is released and rebinds to another PVC automatically\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"A snapshot is created and the PV is kept\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct option is B. When reclaimPolicy is Delete, deleting the PVC prompts the external provisioner to remove the backing storage and the PV object itself is removed from the API server.\n\n## Why Other Options Are Wrong\n- A: Retain behavior occurs with reclaimPolicy: Retain, not Delete, so data is not automatically preserved in the PV object.\n- C: Automatic rebinding does not occur; a new PVC must be created and bound to a new PV if needed.\n- D: Snapshots are not automatically created as part of PVC deletion.\n\n## Key Concepts\n- Reclaim policy semantics\n- Dynamic provisioning lifecycle\n- External provisioner responsibilities\n\n## Real-World Application\n- Align deletion policy with data retention and backup strategies; ensure backups exist before deleting volumes in production environments.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","CSI","Storage","DynamicProvisioning","certification-mcq","domain-weight-10"],"channel":"cka","subChannel":"storage","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T20:28:25.753Z","createdAt":"2026-01-12 20:28:26"},{"id":"cka-storage-1768249705753-1","question":"Which resource defines the reclaim policy for a dynamically provisioned PV?","answer":"[{\"id\":\"a\",\"text\":\"StorageClass\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"PersistentVolume\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"PersistentVolumeClaim\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"CSI Driver\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct because reclaimPolicy is a field on the PersistentVolume (PV) resource, not on StorageClass or PVC.\n\n## Why Other Options Are Wrong\n- A: StorageClass defines provisioning behavior and parameters, not the reclaim policy.\n- C: PVC defines the claim request, not the reclaim policy.\n- D: The CSI driver provides volume operations, but reclaim behavior is governed by PV.\n\n## Key Concepts\n- PV reclaimPolicy placement\n- Separation of provisioning and lifecycle policies\n\n## Real-World Application\n- Review PV specifications before deletion to avoid unintended data loss.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","CSI","Storage","PV-Management","certification-mcq","domain-weight-10"],"channel":"cka","subChannel":"storage","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T20:28:26.115Z","createdAt":"2026-01-12 20:28:26"},{"id":"cka-storage-1768249705753-2","question":"WaitForFirstConsumer with dynamic provisioning defers volume provisioning until Pod using the PVC is created and scheduled; which statement is true?","answer":"[{\"id\":\"a\",\"text\":\"PV is created at cluster startup before any Pod is scheduled\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\" PVC is bound immediately upon creation, regardless of Pod placement\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"The volume is provisioned only when a Pod using the PVC is scheduled, ensuring node locality\",\"isCorrect\":true},{\"id\":\"d\",\"text\":\"Provisioning occurs only after at least two Pods use the PVC\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct option is C. WaitForFirstConsumer delays provisioning until a Pod that will use the PVC is scheduled, ensuring the volume is created on the appropriate node for locality.\n\n## Why Other Options Are Wrong\n- A: PV provisioning is not guaranteed at cluster startup; it is deferred until the consumer is scheduled.\n- B: PVC binding can be delayed by WaitForFirstConsumer depending on binding mode and scheduling.\n- D: Provisioning does not wait for multiple Pods; it waits for the consumer Pod scheduling.\n\n## Key Concepts\n- WaitForFirstConsumer behavior\n- Node locality in provisioning\n\n## Real-World Application\n- Helps optimize performance for local storage and reduces unnecessary cross-node data movement.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","CSI","Storage","Provisioning","certification-mcq","domain-weight-10"],"channel":"cka","subChannel":"storage","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T20:28:26.513Z","createdAt":"2026-01-12 20:28:26"},{"id":"cka-storage-1768249705753-3","question":"To resize a PVC online using a CSI driver, which conditions must be satisfied?","answer":"[{\"id\":\"a\",\"text\":\"The StorageClass has allowVolumeExpansion: true and the CSI driver supports online expansion\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"The PV must be deleted and recreated with the new size\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"The Pod must be restarted for the expansion to take effect\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"The PVC must be recreated with a higher size after a restart\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct option is A. Online resizing requires a StorageClass that enables volume expansion (allowVolumeExpansion: true) and a CSI driver that supports expansion.\n\n## Why Other Options Are Wrong\n- B: PV deletion/recreation is not required for online expansion if the driver supports it.\n- C: A restart is not typically required for online resizing when supported.\n- D: Recreating the PVC is unnecessary if online expansion is supported.\n\n## Key Concepts\n- Volume expansion support in StorageClass and CSI drivers\n- Online vs offline resizing\n\n## Real-World Application\n- Verify driver and StorageClass capabilities before enabling resize in production; test end-to-end resizing and filesystem growth inside running pods.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","CSI","Storage","VolumeExpansion","certification-mcq","domain-weight-10"],"channel":"cka","subChannel":"storage","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T20:28:26.647Z","createdAt":"2026-01-12 20:28:26"},{"id":"cka-storage-1768249705753-4","question":"To allow multiple Pods on different nodes to mount the same volume concurrently, which combination is required?","answer":"[{\"id\":\"a\",\"text\":\"ReadWriteOnce with a CSI driver that supports multi-node mounts\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"ReadWriteMany with a CSI driver that supports multi-node mounts\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"ReadOnlyMany with any driver\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use a StatefulSet with a single replica\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct option is B. ReadWriteMany allows multiple Pods across multiple nodes to mount the same volume simultaneously, provided the CSI driver and storage backend support this mode.\n\n## Why Other Options Are Wrong\n- A: ReadWriteOnce restricts access to a single node at a time, so multi-node mounting is not possible.\n- C: ReadOnlyMany only allows read access, not writable access for multiple Pods.\n- D: A StatefulSet with a single replica does not demonstrate multi-node concurrent access.\n\n## Key Concepts\n- Access modes in PVCs\n- CSI driver capabilities for multi-node mounts\n\n## Real-World Application\n- When designing shared storage for microservices spanning nodes, choose a driver and access mode that supports multi-node writable access to meet application requirements.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","CSI","Storage","AccessModes","certification-mcq","domain-weight-10"],"channel":"cka","subChannel":"storage","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T20:28:26.777Z","createdAt":"2026-01-12 20:28:26"},{"id":"cka-troubleshoot-1768151801683-0","question":"A Deployment's pods crash with exit code 137 after the node experiences memory pressure, and the kubelet reports OOMKilled for the containers. Which action most reliably resolves this issue?","answer":"[{\"id\":\"a\",\"text\":\"Increase the container memory requests and limits\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Increase the container CPU requests and limits\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Decrease the number of replicas to reduce memory usage\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Disable the OOM killer on the node\",\"isCorrect\":false}]","explanation":"## Correct Answer\nIncrease the container memory requests and limits\n\nThe exit code 137 under OOMKilled indicates the container was terminated due to memory pressure. The remedy is to provide more memory headroom by increasing the container's memory requests and/or limits so the pod can be scheduled with sufficient RAM and avoid being killed under memory pressure.\n\n## Why Other Options Are Wrong\n- Option B: Increasing CPU alone does not address memory pressure causing OOMKilled.\n- Option C: Reducing replicas increases per-pod load but does not reduce overall memory pressure on the node.\n- Option D: The node's OOM killer is a mechanism the cluster should not be disabled; disabling it is unsafe and ineffective in this context.\n\n## Key Concepts\n- OOMKilled and memory pressure\n- Resource requests and limits\n- Kubernetes QoS classes and scheduling under memory pressure\n\n## Real-World Application\nWhen pods crash during memory pressure, review per-container memory requirements, monitor node memory usage, and adjust requests/limits or consider scaling out to add more nodes or optimize memory usage in the workload.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","Troubleshooting","OOM","Memory","certification-mcq","domain-weight-30"],"channel":"cka","subChannel":"troubleshoot","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T17:16:41.685Z","createdAt":"2026-01-11 17:16:42"},{"id":"cka-troubleshoot-1768151801683-1","question":"A Pod cannot reach a target service within the cluster, and you discover a NetworkPolicy that denies all ingress to the destination namespace. What is the recommended action to restore legitimate traffic from the Pod to that service?","answer":"[{\"id\":\"a\",\"text\":\"Create or update a NetworkPolicy to explicitly allow ingress to the destination service from the Pod's namespace\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Change the service type to LoadBalancer to bypass NetworkPolicy\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Disable NetworkPolicy feature gate cluster-wide\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Annotate the Pod with a bypass label to skip policy evaluation\",\"isCorrect\":false}]","explanation":"## Correct Answer\nCreate or update a NetworkPolicy to explicitly allow ingress to the destination service from the Pod's namespace\n\nNetworkPolicy can restrict traffic between pods. If a policy denies ingress, you must add a policy that permits the necessary traffic (e.g., from the Pod’s namespace to the service’s namespace and port). This preserves security while enabling the required communication.\n\n## Why Other Options Are Wrong\n- Option B: Changing service type to LoadBalancer does not affect NetworkPolicy restrictions, and is not a correct mechanism to allow intra-cluster traffic.\n- Option C: Disabling NetworkPolicy cluster-wide undermines security posture and is not advisable.\n- Option D: There is no standard annotation to bypass NetworkPolicy evaluation for a single pod.\n\n## Key Concepts\n- Kubernetes NetworkPolicy basics\n- Ingress permission scoping by namespace and pod selectors\n- NetworkPolicy-by-default behavior when policies exist\n\n## Real-World Application\nIn multi-tenant or segmented environments, use well-scoped NetworkPolicies to allow only the required cross-pod communications, validating with tests like wget or curl from the pod to the service before promoting changes to production.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","Networking","NetworkPolicy","Troubleshooting","certification-mcq","domain-weight-30"],"channel":"cka","subChannel":"troubleshoot","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T17:16:42.115Z","createdAt":"2026-01-11 17:16:42"},{"id":"cka-troubleshoot-1768151801683-2","question":"A Deployment cannot pull its images from a private container registry and the pod status shows imagePullBackOff. Which action best resolves this issue in a typical Kubernetes cluster?","answer":"[{\"id\":\"a\",\"text\":\"Create an ImagePullSecret with the registry credentials and reference it in the Pod/Deployment or in the namespace's default service account\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Set imagePullPolicy to Always on all images\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Switch the image registry to Docker Hub\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Disable image pulling and rely on local cache\",\"isCorrect\":false}]","explanation":"## Correct Answer\nCreate an ImagePullSecret with the registry credentials and reference it in the Pod/Deployment or in the namespace's default service account\n\nIf a private registry requires authentication, Kubernetes needs an ImagePullSecret containing the credentials. Attaching it to the Deployment (via spec.imagePullSecrets) or to the namespace's default service account resolves imagePullBackOff caused by authentication failure.\n\n## Why Other Options Are Wrong\n- Option B: imagePullPolicy alone does not fix authentication failures and may cause other caching issues.\n- Option C: Swapping registries may not be feasible and does not address the authentication problem.\n- Option D: Relying on a local cache is unreliable and not scalable in clusters that pull images from registries.\n\n## Key Concepts\n- ImagePullSecrets and secret management\n- Namespace-scoped vs pod-scoped secrets\n- Private registry authentication in Kubernetes\n\n## Real-World Application\nIn production clusters, store registry credentials as Kubernetes Secrets (type=kubernetes.io/dockerconfigjson) and ensure pods or service accounts reference them, then verify via kubectl describe pod to confirm image pull success.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","ImagePull","SecretManagement","Troubleshooting","certification-mcq","domain-weight-30"],"channel":"cka","subChannel":"troubleshoot","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T17:16:42.527Z","createdAt":"2026-01-11 17:16:42"},{"id":"cka-workloads-1768235409098-0","question":"A Pod manifest includes nodeSelector: { disktype: ssd }. The cluster has three nodes with labels: node1 disktype=ssd, node2 disktype=hdd, node3 disktype=ssd. The Pod requests CPU and memory. Which statement is true?","answer":"[{\"id\":\"a\",\"text\":\"It will be scheduled to any node with available resources regardless of the label.\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"It will only schedule to nodes labeled disktype=ssd; if none have enough resources, the Pod remains Pending.\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"It will fail the deployment because there are multiple SSD nodes.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"It will automatically use a different label if the node with disktype=ssd is full.\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\n**B** is correct because NodeSelector restricts scheduling to nodes with the label disktype=ssd, and if none of those SSD-labeled nodes have sufficient resources, the Pod remains in Pending until resources become available.\n\n## Why Other Options Are Wrong\n\n- **A**: Scheduling respects NodeSelector labels; it cannot land on a non-matching node.\n- **C**: The presence of multiple matching nodes does not cause a failure; the scheduler simply chooses among matching nodes.\n- **D**: Kubernetes does not dynamically re-label nodes to satisfy a Pod's request.\n\n## Key Concepts\n\n- NodeSelector constraints\n- Pod scheduling and Pending state\n- Resource availability considerations\n\n## Real-World Application\n\nIf you want to ensure a workload only runs on high-performance SSD-backed nodes, use NodeSelector with careful capacity planning and labeling of nodes to avoid unexpected bottlenecks.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","Scheduling","NodeSelector","certification-mcq","domain-weight-15"],"channel":"cka","subChannel":"workloads","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T16:30:09.099Z","createdAt":"2026-01-12 16:30:09"},{"id":"cka-workloads-1768235409098-1","question":"A Node is tainted with key nvidiagpu, value present, effect NoSchedule. A Pod without a matching toleration is created. What happens?","answer":"[{\"id\":\"a\",\"text\":\"The Pod will be scheduled on the GPU node regardless of the taint.\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"The Pod will be scheduled on the GPU node only if it has a toleration for the taint key and effect.\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"If the Pod's namespace has a global toleration, it will automatically tolerate the taint.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Kubernetes will automatically remove the taint when a Pod is scheduled on the node.\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\n**B** is correct because a NoSchedule taint prevents scheduling on that node unless a pod presents a matching toleration for the taint key and effect.\n\n## Why Other Options Are Wrong\n\n- **A**: The NoSchedule taint blocks scheduling without a matching toleration.\n- **C**: Tolerations are applied per Pod, not per namespace, and a namespace toleration does not bypass node taints.\n- **D**: Taints are not automatically removed by scheduling; they persist until manually changed.\n\n## Key Concepts\n\n- Taints and tolerations mechanics\n- Impact on pod scheduling\n- Per-Pod tolerations scope\n\n## Real-World Application\n\nUse tolerations to allow scheduled workloads to run on specialized nodes (like GPU or preemptible hardware) while keeping the rest of the cluster unaffected.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","Scheduling","TaintsAndTolerations","certification-mcq","domain-weight-15"],"channel":"cka","subChannel":"workloads","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T16:30:09.649Z","createdAt":"2026-01-12 16:30:10"},{"id":"cka-workloads-1768235409098-2","question":"You have a high-priority Pod and the cluster is CPU-constrained with several lower-priority pods running. If preemption is enabled, what will the scheduler typically do to place the high-priority Pod?","answer":"[{\"id\":\"a\",\"text\":\"The scheduler may preempt lower-priority pods to make room for the high-priority pod.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"It will always schedule the high-priority pod on the node with the most free CPU, with no preemption.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Preemption is blocked if any PodDisruptionBudget exists for any workload.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"The scheduler will create a new node automatically for the high-priority pod.\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\n**A** is correct because Kubernetes preemption can evict lower-priority pods to free resources for a higher-priority pod, subject to constraints like PDB and QoS.\n\n## Why Other Options Are Wrong\n\n- **B**: Preemption can occur; it is not guaranteed that no preemption happens.\n- **C**: PodDisruptionBudget can influence preemption decisions; it does not completely disable preemption.\n- **D**: The scheduler cannot autonomously add nodes; cluster scaling is a separate control plane action.\n\n## Key Concepts\n\n- Pod priority and preemption\n- QoS and resource availability\n- Interaction with PDB\n\n## Real-World Application\n\nIn a congested cluster, assign higher priority to critical services (e.g., control plane components) to ensure they run by allowing preemption of non-critical workloads.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","Scheduling","PriorityClasses","certification-mcq","domain-weight-15"],"channel":"cka","subChannel":"workloads","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T16:30:10.210Z","createdAt":"2026-01-12 16:30:10"},{"id":"cka-workloads-1768235409098-3","question":"A Deployment of 4 replicas has a TopologySpreadConstraint with topologyKey: kube-systemZone (simulated as topology.kubernetes.io/zone) and maxSkew: 1 across zones zone-a and zone-b. Which statement best describes the scheduler's behavior when 4 pods are created?","answer":"[{\"id\":\"a\",\"text\":\"It will place all four pods in the same zone to maximize packing.\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"It will spread pods across zones to achieve even distribution (no more than one pod difference per zone).\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"It will ignore topology constraints and schedule purely by node capacity.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"It will create new zones to satisfy the constraint.\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\n**B** is correct because TopologySpreadConstraints with maxSkew=1 across zones aims to distribute pods evenly across the specified topology (zones), preventing large imbalances.\n\n## Why Other Options Are Wrong\n\n- **A**: This would violate the intended spread policy.\n- **C**: Topology constraints are considered in scheduling decisions.\n- **D**: Kubernetes does not create new topology zones automatically.\n\n## Key Concepts\n\n- Pod Topology Spread constraints\n- Zone-based distribution\n- maxSkew parameter\n\n## Real-World Application\n\nUse TopologySpreadConstraints to improve availability by avoiding single-zone bottlenecks and distributing load across zones.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","Scheduling","PodTopologySpread","certification-mcq","domain-weight-15"],"channel":"cka","subChannel":"workloads","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T16:30:10.397Z","createdAt":"2026-01-12 16:30:10"},{"id":"cka-workloads-1768235409098-4","question":"A Deployment has 3 replicas and a PodDisruptionBudget minAvailable: 2. A high-priority pod needs to be scheduled, and preemption would evict one of the 3 existing pods. Will preemption proceed?","answer":"[{\"id\":\"a\",\"text\":\"Preemption will proceed even if it would reduce available replicas below minAvailable.\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Preemption will proceed only if it would not violate minAvailable.\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Preemption is blocked whenever a PodDisruptionBudget exists, regardless of minAvailable.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"The scheduler will create a new pod to satisfy the high-priority requirement.\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\n**B** is correct because PodDisruptionBudget minAvailable defines the minimum number of pods that must remain available; preemption will not proceed if it would violate that minimum.\n\n## Why Other Options Are Wrong\n\n- **A**: Preemption respects PDB and won’t reduce availability below minAvailable.\n- **C**: PDB does not universally block preemption; it constrains it based on minAvailable.\n- **D**: The scheduler cannot create new pods on its own to satisfy a single high-priority demand.\n\n## Key Concepts\n\n- PodDisruptionBudget semantics\n- Preemption vs availability guarantees\n- Deployment replica management\n\n## Real-World Application\n\nWhen using PDBs, ensure they reflect essential service availability; avoid single-pod disruption in critical services.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","Scheduling","PodDisruptionBudget","Preemption","certification-mcq","domain-weight-15"],"channel":"cka","subChannel":"workloads","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T16:30:10.587Z","createdAt":"2026-01-12 16:30:10"}],"subChannels":["cluster-arch","general","services","storage","troubleshoot","workloads"],"companies":["Airbnb","Cloudflare","Databricks","Google","IBM","Netflix","Salesforce","Scale Ai","Snap"],"stats":{"total":33,"beginner":0,"intermediate":33,"advanced":0,"newThisWeek":33}}