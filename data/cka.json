{"questions":[{"id":"cka-cluster-arch-1768193310463-0","question":"During a kubeadm-based HA cluster setup with three control-plane nodes, which design choice ensures etcd quorum remains available if any one control-plane node fails?","answer":"[{\"id\":\"a\",\"text\":\"Run a single etcd member on one control-plane node and rely on API server replication\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Run three etcd members, one on each control-plane node\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Run five etcd members spanning two control-plane nodes and one worker node\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Run etcd as a cluster on all nodes with data replicated in memory only\",\"isCorrect\":false}]","explanation":"## Correct Answer\nB. Run three etcd members, one on each control-plane node\n\nEtcd quorum requires a majority of members to be healthy. With three members, the majority is 2, so if one control-plane node fails, the remaining two etcd members still form a quorum and the cluster continues to operate. \n\n## Why Other Options Are Wrong\n- A: A single etcd member cannot maintain quorum if it fails, breaking HA.\n- C: Five members across two nodes is not feasible and would still risk quorum loss if nodes fail; etcd should be spread across distinct nodes for HA.\n- D: Etcd data must be persisted on disk; in-memory only would lose state on restart and is not HA.\n\n## Key Concepts\n- Etcd quorum and HA require an odd number of members\n- Distributing etcd across control-plane nodes supports resilience\n- Persistent storage for etcd is essential\n- Use a load-balanced API endpoint to keep API access stable\n\n## Real-World Application\n- For a production-grade small cluster, deploy a 3-member etcd cluster across the 3 control-plane nodes and front the API with a load balancer to maintain availability during node failures.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","kubeadm","etcd","HA","AWS","Terraform","certification-mcq","domain-weight-25"],"channel":"cka","subChannel":"cluster-arch","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T04:48:30.465Z","createdAt":"2026-01-12 04:48:31"},{"id":"cka-cluster-arch-1768193310463-1","question":"In a 3-control-plane HA cluster behind a DNS load balancer, the API server certificate SANs currently include only control-plane IPs. You cannot connect to the API server via the DNS name k8s-api.example.com. What is the recommended fix?","answer":"[{\"id\":\"a\",\"text\":\"Recreate the cluster using --apiserver-cert-extra-sans to include the DNS name\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Add an /etc/hosts entry on every client to map k8s-api.example.com to one control-plane IP\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use the IP-based endpoint instead of the DNS name\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Disable TLS on the API server to bypass SAN checks\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA. Recreate the cluster using --apiserver-cert-extra-sans to include the DNS name\n\nThe API server certificate must include SAN entries for any DNS names used to access the cluster. If the DNS name k8s-api.example.com is not present in the certificate, TLS handshakes fail when clients connect via that name. Regenerate or rotate the API server certificate to include the DNS SAN and apply the changes.\n\n## Why Other Options Are Wrong\n- B: Modifying hosts files does not fix TLS SAN validation and is not scalable for multi-client access.\n- C: Using IPs avoids DNS name usage but defeats the HA access pattern via DNS name and may still fail if IPs change.\n- D: Disabling TLS completely is insecure and not acceptable in production.\n\n## Key Concepts\n- TLS SAN and API server certificates\n- Control-plane endpoint DNS vs IPs\n- Certificate rotation for kubeadm-managed clusters\n\n## Real-World Application\n- When exposing the API via a DNS name behind a load balancer, ensure the certificate SAN includes that DNS name to prevent client TLS errors.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","kubeadm","etcd","HA","AWS","Terraform","certification-mcq","domain-weight-25"],"channel":"cka","subChannel":"cluster-arch","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T04:48:31.277Z","createdAt":"2026-01-12 04:48:31"},{"id":"cka-cluster-arch-1768193310463-2","question":"You want to enable encryption at rest for Kubernetes Secrets in a kubeadm-managed cluster. Which action is required?","answer":"[{\"id\":\"a\",\"text\":\"Provide an encryption configuration file and pass it to the API server via --encryption-provider-config, then rotate keys to re-encrypt existing data\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Enable encryption at rest by turning on encryption on etcd\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Store encryption keys in a Kubernetes Secret so they are encrypted by etcd\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Encrypt all pods' data at rest using a runtime encryption feature\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA. Provide an encryption configuration file and pass it to the API server via --encryption-provider-config, then rotate keys to re-encrypt existing data\n\nTo enable encryption at rest for Secrets, you must configure the API server with an EncryptionConfig that specifies a provider (e.g., AES-CBC or a KMS provider) and restart the API server. After enabling, you typically rotate the encryption keys to re-encrypt existing data in etcd. This ensures secrets are stored encrypted at rest. \n\n## Why Other Options Are Wrong\n- B: Encryption at rest is configured via the API server, not by enabling encryption on etcd itself. \n- C: Keys should be managed by the encryption provider; storing keys in a Secret does not secure the data at rest. \n- D: Pod data encryption is not the standard method for Secrets; it’s about secret storage in etcd. \n\n## Key Concepts\n- Encryption provider configuration\n- API server certificate and TLS considerations not applicable here\n- Data at rest for secrets in etcd\n- Key rotation and re-encryption process\n\n## Real-World Application\n- In production, enable encryption at rest for Secrets to meet compliance requirements; implement a proper KMS-backed provider and plan key rotation.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","kubeadm","etcd","Encryption","AWS","Terraform","certification-mcq","domain-weight-25"],"channel":"cka","subChannel":"cluster-arch","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T04:48:31.677Z","createdAt":"2026-01-12 04:48:31"},{"id":"cka-services-1768221805634-0","question":"Scenario: A Kubernetes cluster has a frontend service in namespace frontend and a backend service in namespace backend. You want to ensure that only pods from the frontend namespace can reach the backend service on port 8080, and that traffic from all other sources is denied. Which NetworkPolicy best achieves this?","answer":"[{\"id\":\"a\",\"text\":\"A NetworkPolicy in the backend namespace selecting pods app=backend and allowing ingress only from pods in the frontend namespace labeled app=frontend on port 8080.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"A NetworkPolicy in the backend namespace selecting pods app=backend and allowing ingress from any source on port 8080.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"A NetworkPolicy in the frontend namespace selecting pods app=frontend and allowing ingress from the backend namespace on port 8080.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"A NetworkPolicy in the backend namespace selecting pods app=backend and allowing ingress from the frontend namespace labeled name=frontend but port 80.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption a is correct because it explicitly binds the policy to backend pods and restricts ingress to only frontend pods from the frontend namespace on port 8080, implementing a least-privilege policy across namespaces.\n\n## Why Other Options Are Wrong\n- Option b would allow traffic from any source, defeating the cross-namespace restriction.\n- Option c applies the policy in the frontend namespace and would not protect the backend service.\n- Option d references a cross-namespace source but specifies port 80, not the required 8080, and its policy scope is incorrect.\n\n## Key Concepts\n- NetworkPolicy: namespace scoping, podSelector, namespaceSelector, ingress rules, ports\n- Default-deny behavior when proper NetworkPolicies exist\n- Cross-namespace traffic control in Kubernetes\n\n## Real-World Application\n- Ensures service segmentation between teams and environments in multi-namespace clusters, reducing blast radius if a frontend pod is compromised.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","Kubernetes-Networking","NetworkPolicy","EKS","certification-mcq","domain-weight-20"],"channel":"cka","subChannel":"services","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T12:43:25.636Z","createdAt":"2026-01-12 12:43:26"},{"id":"cka-services-1768221805634-1","question":"Scenario: You deploy a Kubernetes Service of type LoadBalancer in AWS EKS. To preserve the original client IP in your backend pods for request logging, which service configuration should you rely on?","answer":"[{\"id\":\"a\",\"text\":\"externalTrafficPolicy: Local\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"externalTrafficPolicy: Cluster\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"service.beta.kubernetes.io/aws-load-balancer-proxy-protocol: \\\"*\\\"\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"loadBalancerIP: 1.2.3.4\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption a is correct because externalTrafficPolicy Local preserves the original client IP by not SNATing traffic to the node IP, allowing backend pods to see the client's IP.\n\n## Why Other Options Are Wrong\n- Option b would SNAT client IP to the node IP, so the backend sees the node IP instead of the client IP.\n- Option c enables the PROXY protocol, which can pass client IP in some setups but is not the standard or universally reliable method for all AWS Load Balancers and configurations.\n- Option d fixes the external IP of the load balancer and does not address client IP preservation.\n\n## Key Concepts\n- externalTrafficPolicy (Local vs Cluster)\n- Client IP preservation in Services\n- AWS Load Balancer integration with Kubernetes\n\n## Real-World Application\n- Enables accurate logging for security and auditing when services are exposed via a cloud load balancer.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","Kubernetes-Networking","LoadBalancer","EKS","certification-mcq","domain-weight-20"],"channel":"cka","subChannel":"services","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T12:43:26.121Z","createdAt":"2026-01-12 12:43:26"},{"id":"cka-services-1768221805634-2","question":"Scenario: A pod in namespace frontend tries to reach a service named inventory in namespace backend using the in-cluster DNS. The DNS name inventory is not resolving. What is the correct way to reference the cross-namespace service?","answer":"[{\"id\":\"a\",\"text\":\"inventory.backend.svc.cluster.local\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"inventory.svc.cluster.local\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"inventory.default.svc.cluster.local\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"inventory.backend.svc.cluster.local.local\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption a is correct because cross-namespace service discovery in Kubernetes uses the fully qualified domain name in the form serviceName.namespace.svc.cluster.local.\n\n## Why Other Options Are Wrong\n- Option b refers to the service using only the current namespace, which fails across namespaces.\n- Option c points to the default namespace, which is incorrect for a backend named inventory in the backend namespace.\n- Option d adds an extra .local segment, making it an invalid DNS name for Kubernetes in-cluster service discovery.\n\n## Key Concepts\n- In-cluster DNS and service discovery\n- FQDN format: serviceName.namespace.svc.cluster.local\n- Cross-namespace service access patterns\n\n## Real-World Application\n- Enables microservices to reliably discover and communicate with cross-namespace services in a multi-team cluster.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","Kubernetes-Networking","DNS","EKS","certification-mcq","domain-weight-20"],"channel":"cka","subChannel":"services","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T12:43:26.602Z","createdAt":"2026-01-12 12:43:26"},{"id":"cka-troubleshoot-1768151801683-0","question":"A Deployment's pods crash with exit code 137 after the node experiences memory pressure, and the kubelet reports OOMKilled for the containers. Which action most reliably resolves this issue?","answer":"[{\"id\":\"a\",\"text\":\"Increase the container memory requests and limits\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Increase the container CPU requests and limits\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Decrease the number of replicas to reduce memory usage\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Disable the OOM killer on the node\",\"isCorrect\":false}]","explanation":"## Correct Answer\nIncrease the container memory requests and limits\n\nThe exit code 137 under OOMKilled indicates the container was terminated due to memory pressure. The remedy is to provide more memory headroom by increasing the container's memory requests and/or limits so the pod can be scheduled with sufficient RAM and avoid being killed under memory pressure.\n\n## Why Other Options Are Wrong\n- Option B: Increasing CPU alone does not address memory pressure causing OOMKilled.\n- Option C: Reducing replicas increases per-pod load but does not reduce overall memory pressure on the node.\n- Option D: The node's OOM killer is a mechanism the cluster should not be disabled; disabling it is unsafe and ineffective in this context.\n\n## Key Concepts\n- OOMKilled and memory pressure\n- Resource requests and limits\n- Kubernetes QoS classes and scheduling under memory pressure\n\n## Real-World Application\nWhen pods crash during memory pressure, review per-container memory requirements, monitor node memory usage, and adjust requests/limits or consider scaling out to add more nodes or optimize memory usage in the workload.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","Troubleshooting","OOM","Memory","certification-mcq","domain-weight-30"],"channel":"cka","subChannel":"troubleshoot","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T17:16:41.685Z","createdAt":"2026-01-11 17:16:42"},{"id":"cka-troubleshoot-1768151801683-1","question":"A Pod cannot reach a target service within the cluster, and you discover a NetworkPolicy that denies all ingress to the destination namespace. What is the recommended action to restore legitimate traffic from the Pod to that service?","answer":"[{\"id\":\"a\",\"text\":\"Create or update a NetworkPolicy to explicitly allow ingress to the destination service from the Pod's namespace\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Change the service type to LoadBalancer to bypass NetworkPolicy\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Disable NetworkPolicy feature gate cluster-wide\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Annotate the Pod with a bypass label to skip policy evaluation\",\"isCorrect\":false}]","explanation":"## Correct Answer\nCreate or update a NetworkPolicy to explicitly allow ingress to the destination service from the Pod's namespace\n\nNetworkPolicy can restrict traffic between pods. If a policy denies ingress, you must add a policy that permits the necessary traffic (e.g., from the Pod’s namespace to the service’s namespace and port). This preserves security while enabling the required communication.\n\n## Why Other Options Are Wrong\n- Option B: Changing service type to LoadBalancer does not affect NetworkPolicy restrictions, and is not a correct mechanism to allow intra-cluster traffic.\n- Option C: Disabling NetworkPolicy cluster-wide undermines security posture and is not advisable.\n- Option D: There is no standard annotation to bypass NetworkPolicy evaluation for a single pod.\n\n## Key Concepts\n- Kubernetes NetworkPolicy basics\n- Ingress permission scoping by namespace and pod selectors\n- NetworkPolicy-by-default behavior when policies exist\n\n## Real-World Application\nIn multi-tenant or segmented environments, use well-scoped NetworkPolicies to allow only the required cross-pod communications, validating with tests like wget or curl from the pod to the service before promoting changes to production.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","Networking","NetworkPolicy","Troubleshooting","certification-mcq","domain-weight-30"],"channel":"cka","subChannel":"troubleshoot","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T17:16:42.115Z","createdAt":"2026-01-11 17:16:42"},{"id":"cka-troubleshoot-1768151801683-2","question":"A Deployment cannot pull its images from a private container registry and the pod status shows imagePullBackOff. Which action best resolves this issue in a typical Kubernetes cluster?","answer":"[{\"id\":\"a\",\"text\":\"Create an ImagePullSecret with the registry credentials and reference it in the Pod/Deployment or in the namespace's default service account\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Set imagePullPolicy to Always on all images\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Switch the image registry to Docker Hub\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Disable image pulling and rely on local cache\",\"isCorrect\":false}]","explanation":"## Correct Answer\nCreate an ImagePullSecret with the registry credentials and reference it in the Pod/Deployment or in the namespace's default service account\n\nIf a private registry requires authentication, Kubernetes needs an ImagePullSecret containing the credentials. Attaching it to the Deployment (via spec.imagePullSecrets) or to the namespace's default service account resolves imagePullBackOff caused by authentication failure.\n\n## Why Other Options Are Wrong\n- Option B: imagePullPolicy alone does not fix authentication failures and may cause other caching issues.\n- Option C: Swapping registries may not be feasible and does not address the authentication problem.\n- Option D: Relying on a local cache is unreliable and not scalable in clusters that pull images from registries.\n\n## Key Concepts\n- ImagePullSecrets and secret management\n- Namespace-scoped vs pod-scoped secrets\n- Private registry authentication in Kubernetes\n\n## Real-World Application\nIn production clusters, store registry credentials as Kubernetes Secrets (type=kubernetes.io/dockerconfigjson) and ensure pods or service accounts reference them, then verify via kubectl describe pod to confirm image pull success.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","ImagePull","SecretManagement","Troubleshooting","certification-mcq","domain-weight-30"],"channel":"cka","subChannel":"troubleshoot","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T17:16:42.527Z","createdAt":"2026-01-11 17:16:42"}],"subChannels":["cluster-arch","services","troubleshoot"],"companies":[],"stats":{"total":9,"beginner":0,"intermediate":9,"advanced":0,"newThisWeek":9}}