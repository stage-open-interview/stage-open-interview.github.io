{"questions":[{"id":"q-1021","question":"Design a globally distributed feature-flag engine for per-user rollout. Provide data model, consistency guarantees, and deployment strategy for scale across multiple regions. Include choice of storage (DynamoDB vs Redis), how regionOverrides and segments are merged, and hot-flip safety. Provide evaluation protocol and a concise pseudocode snippet for evaluateFeature(user, flag, context)?","answer":"Flag model: name, rolloutPct, segments, regionOverride, version. Source of truth: DynamoDB multi-region; Redis as near-cache with TTL. Evaluate by hashing userId+flag to get 0–99; enabled if value < r","explanation":"## Why This Is Asked\nAssesses ability to design scalable feature flags with per-user rollout, regional controls, and safe hot flips.\n\n## Key Concepts\n- Global distribution and eventual consistency\n- Data modeling for flags and segments\n- Cache invalidation and TTL\n- Rollout semantics and hot-flip safety\n\n## Code Example\n```javascript\nfunction evaluateFeature(user, flag, context){\n  const bucket = hash(user.id + flag.name) % 100;\n  const inSegment = (flag.segments && flag.segments.length) ? flag.segments.includes(user.segment) : true;\n  const regionOK = flag.regionOverride ? user.region === flag.regionOverride : true;\n  return (bucket < flag.rolloutPct) && inSegment && regionOK;\n}\n```\n\n## Follow-up Questions\n- How would you validate rollout accuracy under traffic spikes?\n- How would you rollback a faulty flag without service interruption?","diagram":"flowchart TD\n  A[Client Request] --> B[Eval Service]\n  B --> C[Flag Store]\n  B --> D[Cache]\n  C --> E[Flag Data (DB)]\n  D --> F[Return Result]","difficulty":"advanced","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Meta","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T19:34:13.849Z","createdAt":"2026-01-12T19:34:13.849Z"},{"id":"q-1042","question":"Design a high-throughput, fault-tolerant event ingestion pipeline for a fraud-detection service. It must guarantee exactly-once processing, deduplicate events across shards, support backpressure, and provide end-to-end observability. Explain the data model for dedup IDs, queue choice (Kafka vs Kinesis), idempotent sinks, and how you'd test failure scenarios?","answer":"Explain how you would implement exactly-once semantics for a high-throughput ingestion pipeline. Include dedup IDs, an atomic sink, idempotent writes, and at-least-once transport handling. Justify que","explanation":"## Why This Is Asked\n\nTests distributed-systems thinking: exactly-once, dedup, backpressure, observability.\n\n## Key Concepts\n\n- Exactly-once processing\n- Deduplication with dedup_id\n- Backpressure strategies (batching, pause vs drop)\n- Observability (tracing, metrics, logs)\n- Kafka vs Kinesis trade-offs\n- Idempotent sinks and compensating actions\n\n## Code Example\n\n```javascript\n// Pseudocode for idempotent sink write\nfunction writeEvent(evt, sink, store) {\n  const id = evt.dedup_id;\n  if (store.seen(id)) return;\n  sink.write(evt);\n  store.markSeen(id);\n}\n```\n\n## Follow-up Questions\n\n- How would you test guarantees for partial failures?\n- How would you ensure exactly-once with at-least-once queues?\n","diagram":null,"difficulty":"advanced","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Plaid","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T20:29:46.149Z","createdAt":"2026-01-12T20:29:46.149Z"},{"id":"q-1067","question":"In a Zoom-like multi-tenant Kubernetes cluster, you discover a pod running as root with a hostPath mounted. What concrete remediation plan would you implement using OPA Gatekeeper, Pod Security Standards, RBAC, NetworkPolicies, and image scanning? Include how you would verify tenant isolation with a minimal test that should fail if misconfigured?","answer":"Disable privileged mode and hostPath; require non-root and readOnlyRootFilesystem. Enforce OPA Gatekeeper Pod Security Standards and tighten RBAC. Add namespace NetworkPolicies and image scanning. Ver","explanation":"## Why This Is Asked\n\nThis question probes practical security hardening steps in a multi-tenant Kubernetes cluster, including policy enforcement, RBAC least privilege, network isolation, and image security.\n\n## Key Concepts\n\n- Pod Security Standards (PSP/OPA Gatekeeper)\n- Least-privilege RBAC and non-root containers\n- NetworkPolicies for tenant isolation\n- Image scanning and SBOM controls\n- Policy auditing and explainability\n\n## Code Example\n\n```javascript\n// Pseudo-test scaffold for policy denial\nfunction testPolicyDenial() {\n  // deploy non-root pod that tries hostPath; expect rejection\n}\n```\n\n## Follow-up Questions\n\n- How would you monitor and alert on policy violations in prod?\n- How would you safely roll back a policy that blocks legitimate workloads?","diagram":null,"difficulty":"intermediate","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Scale Ai","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T21:27:19.489Z","createdAt":"2026-01-12T21:27:19.489Z"},{"id":"q-1096","question":"In a simple analytics microservice, implement a Python function that reads a CSV file with headers user_id, action, timestamp (timestamp optional) and returns a dict mapping each user_id to the list of unique actions performed in chronological order; if timestamp is missing, preserve input order. Explain your approach and trade-offs?","answer":"Read a CSV with headers user_id, action, timestamp (optional). Build a dict: user_id -> list of unique actions in chronological order. Maintain a per-user seen set to deduplicate; when timestamp exist","explanation":"## Why This Is Asked\n\nThis question tests practical data processing: grouping, deduplication, and ordering in a realistic CSV workflow.\n\n## Key Concepts\n\n- CSV parsing with csv.DictReader\n\n- Dicts and sets for grouping and dedup\n\n- Optional timestamp handling and stable ordering\n\n- Edge cases: missing fields, non-integer user_id\n\n## Code Example\n\n```python\nimport csv\nfrom collections import defaultdict\n\ndef aggregate_actions(csv_path: str):\n    per_user = defaultdict(list)\n    seen = defaultdict(set)\n    with open(csv_path, newline='') as f:\n        for row in csv.DictReader(f):\n            uid = row.get('user_id')\n            action = row.get('action')\n            ts = row.get('timestamp')\n            if uid is None or action is None:\n                continue\n            try:\n                user = int(uid)\n            except ValueError:\n                continue\n            if action in seen[user]:\n                continue\n            seen[user].add(action)\n            per_user[user].append((action, ts))\n    result = {}\n    for user, items in per_user.items():\n        items_sorted = sorted(items, key=lambda x: (x[1] is None, x[1]))\n        result[user] = [act for act, _ in items_sorted]\n    return result\n```\n\n## Follow-up Questions\n\n- How would you scale this for millions of rows with streaming input?\n- How would you test robustness against malformed rows?","diagram":null,"difficulty":"beginner","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["OpenAI","Oracle","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T22:28:04.410Z","createdAt":"2026-01-12T22:28:04.410Z"},{"id":"q-1129","question":"Given a Kubernetes-deployed event-processing service that suddenly drops throughput from 10k to 6k after a feature release, outline a concrete, testable plan to diagnose and restore throughput. Include: **metrics** to monitor, **bottlenecks** to consider, and concrete, incremental changes like **rate limiting**, **backpressure**, and **idempotency**. Provide a safe rollback strategy and canary plan?","answer":"Instrument p95/p99 latency, error rate, queue depth, and throughput; enable OpenTelemetry traces and compare CPU/GC pauses, DB latency, and network. If bottleneck is consumer, introduce bounded queue,","explanation":"## Why This Is Asked\n\nAssess real-world troubleshooting, instrumentation, and change-management skills under production-like pressure. It probes how a candidate reasons about observability, bottlenecks, and safe rollbacks in a live system.\n\n## Key Concepts\n\n- Observability: p95/p99 latency, error rate, queue depth, throughput\n- Backpressure and rate limiting: bounded queues, controlled retries\n- Idempotency and safety: ensure duplicates don’t break state\n- Canary and rollback strategies\n\n## Code Example\n\n```javascript\n// Simple rate limiter example (token bucket)\nfunction createRateLimiter(tokensPerWindow, windowMs) {\n  let tokens = tokensPerWindow; let t0 = Date.now();\n  return function allow() {\n    const now = Date.now();\n    const elapsed = now - t0;\n    if (elapsed > windowMs) { t0 = now; tokens = tokensPerWindow; }\n    if (tokens > 0) { tokens--; return true; }\n    return false;\n  };\n}\n```\n\n## Follow-up Questions\n\n- How would you quantify a rollback safety threshold for the canary?\n- Which traces, metrics, and tests would you add to prevent recurrence?","diagram":null,"difficulty":"intermediate","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Scale Ai","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T23:33:22.477Z","createdAt":"2026-01-12T23:33:22.477Z"},{"id":"q-1184","question":"You maintain a public API for ride estimates and expect bursts. Implement a per-API-key rate limiter that allows 60 requests per minute using an in-memory structure. Provide the core logic in JavaScript (Node.js) and briefly justify your data structure, test approach, and how you’d handle restart scenarios?","answer":"Implement a per-API-key sliding window limiter. Use a Map<key, number[]> of timestamps. On each request: drop timestamps older than 60s; if the remaining count < 60, push now and allow; else return 42","explanation":"## Why This Is Asked\n\nThis question probes practical rate-limiting basics, a common reliability concern in public APIs. It tests per-key quotas, in-memory state, and how small designs behave under restart and burst scenarios.\n\n## Key Concepts\n\n- Sliding window algorithm\n- Map-based per-key state\n- Time-based pruning\n- Testing strategy and limitations\n\n## Code Example\n\n```javascript\n// Core limiter (simplified)\nconst limiter = new Map();\n\nfunction allowRequest(key, now = Date.now()) {\n  const windowMs = 60000;\n  const limit = 60;\n  const timestamps = limiter.get(key) || [];\n  while (timestamps.length && timestamps[0] <= now - windowMs) timestamps.shift();\n  if (timestamps.length < limit) {\n    timestamps.push(now);\n    limiter.set(key, timestamps);\n    return true;\n  }\n  return false;\n}\n```\n\n## Follow-up Questions\n\n- How would you adapt for multiple instances?\n- How would you test time-dependent behavior without slow sleeps?","diagram":"flowchart TD\n  A[Request] --> B[Prune stale timestamps]\n  B --> C{Allowed?}\n  C -->|Yes| D[Accept]\n  C -->|No| E[Reject]","difficulty":"beginner","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Lyft","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T03:44:27.012Z","createdAt":"2026-01-13T03:44:27.012Z"},{"id":"q-1227","question":"In a tiny model-usage service, write a Python function that validates a JSON payload with user_id, model_id, action, and timestamp, then inserts a document into MongoDB with a created_at field. Ensure an index exists on (model_id, timestamp) and handle duplicate submissions gracefully (idempotent using a unique composite index on (user_id, model_id, timestamp)). Include basic error handling and a minimal test snippet?","answer":"Use PyMongo; ensure a unique index on (user_id, model_id, timestamp) for idempotency. Validate payload: user_id and model_id non-empty strings, action in {'load','infer','monitor'}, timestamp parseabl","explanation":"## Why This Is Asked\nThis tests practical data validation, idempotent writes, and MongoDB indexing in a real-world microservice.\n\n## Key Concepts\n- Input validation\n- MongoDB upserts and unique indexes\n- Error handling and idempotency\n\n## Code Example\n\n```javascript\n// Pseudo: show intent without full implementation\n```\n\n## Follow-up Questions\n- How would you test this with bulk logs?\n- What are potential race conditions and mitigations?","diagram":null,"difficulty":"beginner","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Hugging Face","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T05:38:32.718Z","createdAt":"2026-01-13T05:38:32.718Z"},{"id":"q-1246","question":"Design a policy for a shared Kubernetes cluster serving multiple teams, ensuring namespace isolation, least privilege RBAC, image provenance, Pod Security Standards, and auditable policies. Propose concrete constraints using RBAC, PSP/PSA, NetworkPolicy, and OPA Gatekeeper; include a sample ConstraintTemplate and a test that rejects nonconforming pods. Explain trade-offs and monitoring strategy?","answer":"Implement a strict multi-tenant policy: namespace-per-team, RBAC restricted to its namespace, Pod Security Standards (restricted), image provenance via a signature-verified registry, and network segme","explanation":"## Why This Is Asked\n\nTest candidate's ability to design scalable security controls for multi-tenant K8s, covering RBAC, PodSecurity, image provenance, network isolation, and policy enforcement with Gatekeeper. Expect explanation of trade-offs (complexity vs. security) and how to validate in CI.\n\n## Key Concepts\n\n- Least privilege RBAC per-namespace\n- Pod Security Standards\n- OPA Gatekeeper constraint templates\n- Image provenance and signed images\n- NetworkPolicy segmentation\n- Audit logging and policy testing\n\n## Code Example\n\n```javascript\n// Test helper: ensures pods use allowed registries and non-root users\nfunction isPodCompliant(pod){\n  const okNamespace = pod.metadata.namespace.startsWith('team-');\n  const okImage = pod.spec.containers.every(c => c.image.startsWith('registry.internal/'));\n  const okUser = pod.spec.containers.every(c => c.securityContext?.runAsNonRoot === true);\n  return okNamespace && okImage && okUser;\n}\n```\n\n## Follow-up Questions\n\n- How would you test Gatekeeper in CI/CD for new policies?\n- How would you handle exception paths during incidents without breaking policy?","diagram":"flowchart TD\n  A[Multi-tenant cluster] --> B[Namespace isolation]\n  B --> C[RBAC scoped toNS]\n  B --> D[Pod Security Standards]\n  D --> E[Gatekeeper constraints]\n  E --> F[NetworkPolicy]\n  F --> G[Audit + monitoring]","difficulty":"advanced","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T06:41:29.284Z","createdAt":"2026-01-13T06:41:29.284Z"},{"id":"q-1298","question":"Design a globally-distributed rate limiter for a high-traffic service with per-tenant limits and cross-region fairness. Compare token-bucket and sliding-window approaches, specify data structures and caching, describe hot-path optimization, failure modes, and testing strategy. Assume latency budgets and real-time enforcement?","answer":"Hot path: cache per-tenant counters in memory; Redis as backing store for cross-node sync. Use a sliding-window with N buckets (e.g., 200) to bound latency; update counters atomically via Lua script; ","explanation":"## Why This Is Asked\nTests the ability to design scalable, low-latency controls with cross-region consistency and per-tenant quotas. It probes trade-offs between token-bucket and sliding-window, data modeling in Redis, and hot-path optimizations.\n\n## Key Concepts\n- Global rate limiting\n- Sliding-window vs token bucket\n- Atomic updates and Redis Lua scripts\n- Cross-region replication and timing\n- Observability and failure handling\n\n## Code Example\n```javascript\n// Pseudo-implementation sketch of a hot-path check\nfunction allow(tenantId, limit, windowMs){/*...*/}\n```\n\n## Follow-up Questions\n- How would you test backpressure under burst traffic?\n- How would you monitor for drift between regions and recover from clock skew?","diagram":"flowchart TD\n  A[Client Request] --> B[Edge Cache]\n  B --> C[Rate Limiter]\n  C --> D[Backend Service]\n  D --> E[Metrics & Alerts]","difficulty":"advanced","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","LinkedIn","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T08:44:35.661Z","createdAt":"2026-01-13T08:44:35.661Z"},{"id":"q-1351","question":"In a Node.js Express API backed by PostgreSQL, POST /search accepts { term } and currently builds a SQL with string concatenation. Describe a secure rewrite using parameterized queries and input validation, including error handling and ensuring the DB user has minimal privileges. How would you structure this to prevent SQL injection?","answer":"Use parameterized queries and input validation. Build pattern in code and pass as a parameter: 'SELECT * FROM products WHERE name ILIKE $1' with values ['%term%']. Validate term length and allowed cha","explanation":"## Why This Is Asked\nTests practical security habits for handling user input and avoiding SQL injection in a real code path.\n\n## Key Concepts\n- Parameterized queries\n- Input validation\n- Least privilege\n- Error handling\n\n## Code Example\n```javascript\n// Example in Node.js using pg\nconst pattern = '%' + term + '%';\nconst res = await client.query('SELECT * FROM products WHERE name ILIKE $1', [pattern]);\n```\n\n## Follow-up Questions\n- How would you test SQL injection resilience?\n- What edge cases on input do you consider?","diagram":null,"difficulty":"beginner","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Salesforce","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T13:08:15.280Z","createdAt":"2026-01-13T13:08:15.280Z"},{"id":"q-1680","question":"In a Kubernetes-deployed microservice that ingests up to 200 events/sec from a queue, each event has event_id and payload. The handler must be idempotent so a duplicate delivery does not write to Postgres. Propose a concrete Redis-based dedup strategy (SETNX with EXPIRE TTL) and outline how you'd implement the dedup path in code, including how you'd handle retries, restarts, and cleanup?","answer":"Use Redis as a dedup cache keyed by event_id with a TTL (e.g., 1 hour). On receipt, SETNX event_id 1; if created, EXPIRE 3600, process and persist, return 200. If exists, skip work and return 200. Opt","explanation":"## Why This Is Asked\nThis question probes practical idempotency design for high-throughput services, a common real-world requirement in large-scale systems.\n\n## Key Concepts\n- Idempotency and deduplication\n- Redis SETNX with EXPIRE for lockless dedup\n- Atomicity and race condition handling\n- Retries, crash recovery, and cleanup strategies\n\n## Code Example\n```javascript\n// Node.js with ioredis\nconst key = `dedup:${event.event_id}`;\nconst created = await redis.set(key, '1', 'NX', 'EX', 3600);\nif (!created) return; // duplicate, skip\n// process and persist payload\n```\n\n## Follow-up Questions\n- How would you handle dedup across multiple replicas?\n- What TTL would you choose for varying workloads, and why?\n","diagram":"flowchart TD\n  A[Event arrives] --> B{NX created?}\n  B -- Yes --> C[Process & Persist]\n  B -- No --> D[Skip]\n  C --> E[Done]\n  D --> E","difficulty":"beginner","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","NVIDIA","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T06:52:21.824Z","createdAt":"2026-01-14T06:52:21.824Z"},{"id":"q-1788","question":"You're designing a payment authorization path for a fintech platform (think Plaid) where a mobile tap triggers multiple services: Auth, Fraud/Risk, Ledger, and Notification. How would you ensure idempotent processing, at-least-once retries, and eventual consistency across services? Describe data models, the flow (outbox or saga), and fault tolerance (backoffs, DLQ)?","answer":"Use a per-transaction idempotency key generated on user tap and stored centrally. Route flow via an outbox-saga: Auth → Fraud → Ledger, with compensating actions if a step fails. Deduplicate retries b","explanation":"## Why This Is Asked\n\nThis question probes understanding of idempotency, distributed transactions, and fault tolerance in real-time payment systems. It mirrors fintech patterns at Plaid and Lyft, where user actions must not duplicate or lose funds.\n\n## Key Concepts\n\n- Idempotency keys\n- Outbox pattern and sagas\n- Exactly-once vs at-least-once\n- Dead-letter queues and backoff strategies\n\n## Code Example\n\n```javascript\n// Sketch: idempotency check and outbox write\nfunction process(tx, store) {\n  const key = tx.id;\n  if (store.exists(key)) return;\n  store.saveOutbox(tx);\n  // publish events ...\n}\n```\n\n## Follow-up Questions\n\n- How would you test idempotency guarantees under partial failures?\n- How do you monitor for orphaned or duplicate transactions in production?\n","diagram":"flowchart TD\n  Tap[User taps Pay] --> Auth[Auth Service]\n  Auth --> Fraud[Fraud/Risk]\n  Fraud --> Ledger[Ledger]\n  Ledger --> Notify[Notification]\n  Ledger --> Outbox[Outbox]","difficulty":"intermediate","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T10:49:53.252Z","createdAt":"2026-01-14T10:49:53.252Z"},{"id":"q-1828","question":"You operate a polyglot data stack with MongoDB and Oracle. A global e-commerce app experiences 300–500ms latency on order placement during peak hours, despite modest CPU usage. Propose an end-to-end plan to diagnose and fix, covering data model, indexing, shard/cluster topology, connection pooling, caching, and cross-database consistency. Include concrete knobs you would adjust and how you'd validate impact?","answer":"Investigate end-to-end latency during peak: collect per-query latency, queueing, and stall times. MongoDB: validate shard keys and data distribution, verify index usage with explain, tune writeConcern","explanation":"## Why This Is Asked\n\nTests the ability to design end-to-end diagnostics and multi-database optimization under load, not just theory.\n\n## Key Concepts\n\n- Shard keys and data distribution\n- Explain plans and index selection\n- WriteConcern, journaling, balancer health\n- SGA/PGA tuning, RAC, partitioning\n- Caching strategies and end-to-end validation\n\n## Code Example\n\n```javascript\n// Example: inspect MongoDB index usage\ndb.orders.find({userId: \"X\"}).explain(\"executionStats\")\n```\n\n## Follow-up Questions\n\n- How would you validate a canary rollout across both databases?\n- What metrics define a successful latency reduction and how would you monitor it long-term?","diagram":"flowchart TD\n  A[User Action] --> B[MongoDB]\n  B --> C[Oracle]\n  C --> D[Cache/APP]\n  D --> E[Metrics]","difficulty":"advanced","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T13:10:08.577Z","createdAt":"2026-01-14T13:10:08.578Z"},{"id":"q-1877","question":"In a multi-tenant data pipeline on Kubernetes serving a PayPal-like payments gateway, events flow from a single Kafka topic to a Spark streaming job that writes to a data lake. How would you enforce per-tenant data isolation, encryption, auditing, and masking while maintaining a 95th percentile latency under peak load?","answer":"Per-tenant isolation with SPIFFE IDs and mTLS; namespace RBAC; OPA Gatekeeper policies; envelope encryption with KMS at rest; TLS in transit; per-tenant masking service for PII; immutable audit logs; ","explanation":"## Why This Is Asked\nTests practical security, observability, and performance trade-offs in a real streaming, multi-tenant data stack on Kubernetes; requires knowledge of security primitives, streaming best practices, and performance validation.\n\n## Key Concepts\n- Multi-tenant isolation in Kubernetes\n- SPIFFE/SVIDs and mutual TLS\n- Per-tenant access in Kafka and Spark (RBAC/ABAC)\n- Data masking and envelope encryption with KMS\n- Auditing and immutable logs\n- Latency targets (95th percentile) under burst load\n- Observability and tracing across components\n\n## Code Example\n```javascript\n// Masking example for streaming records\nfunction maskPII(record) {\n  if (record.cardNumber) record.cardNumber = \"****-****-****-\" + record.cardNumber.slice(-4);\n  if (record.email) record.email = \"***@***\";\n  return record;\n}\n```\n\n## Follow-up Questions\n- How would you validate masking does not leak data under failure modes?\n- How would you simulate tenant-level bursts to prove 95th percentile latency remains within SLA?","diagram":"flowchart TD\n  Tenant[Tenant] --> Kafka[Kafka Topic]\n  Kafka --> Processor[Spark Streaming Job]\n  Processor --> Lake[Data Lake / Warehouse]\n  Lake --> Audit[Audit & Masking Service]","difficulty":"intermediate","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Google","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T15:37:19.288Z","createdAt":"2026-01-14T15:37:19.288Z"},{"id":"q-1950","question":"You’re asked to implement a simple in-memory rate limiter for a Node.js/Express API: cap each API key at 100 requests per 10 minutes. Provide a minimal in-process solution using a per-key timestamp array, how you prune old entries, and how you handle bursts. Explain trade-offs and a plan to scale to multiple processes?","answer":"On each API key bucket, store an array of request timestamps. For a request, purge timestamps older than 10 minutes, then permit if length < 100; otherwise return 429. Schedule periodic cleanup to bou","explanation":"## Why This Is Asked\n\nTests practical understanding of simple rate limiting, memory bounds, and concurrency in a real service.\n\n## Key Concepts\n\n- In-memory data structures for per-key state\n- Time-window calculations and pruning\n- Concurrency and process-safety; scalability trade-offs\n\n## Code Example\n\n```javascript\nconst buckets = new Map();\n\nfunction limit(req, res, next){\n  const key = req.headers['x-api-key'];\n  if(!key) return res.status(400).send('API key required');\n  const now = Date.now();\n  const arr = buckets.get(key) || [];\n  // prune\n  while (arr.length && now - arr[0] > 10 * 60 * 1000) arr.shift();\n  if (arr.length >= 100) return res.status(429).send('Too many requests');\n  arr.push(now);\n  buckets.set(key, arr);\n  next();\n}\n```\n\n## Follow-up Questions\n\n- How would you adapt this for a multi-instance deployment?\n- What are the limitations of in-memory rate limiting?","diagram":"flowchart TD\n  A[API Request] --> B{Has Key}\n  B --> C[Prune Window]\n  C --> D{Under Limit?}\n  D -->|Yes| E[Allow & Record]\n  D -->|No| F[Reject 429]","difficulty":"beginner","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Instacart","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T18:46:55.888Z","createdAt":"2026-01-14T18:46:55.888Z"},{"id":"q-2008","question":"In a cloud-native data platform delivering streaming telemetry to a data lake, how would you implement end-to-end data quality and schema drift control for Kafka → Flink → Parquet, when tenants frequently emit extra fields? Describe schema versioning, compatibility, drift detection, and automated remediation to downstream dashboards, with concrete knobs and testing steps?","answer":"Leverage a schema registry (AVRO) with backward/forward compatibility; require optional tenant fields and default values. Tag schemas with tenant/version; use a per-tenant topic namespace. Add a drift","explanation":"## Why This Is Asked\nAssesses practical data quality controls, schema evolution discipline, and safe deployment practices in multi-tenant streams.\n\n## Key Concepts\n- Schema Registry with backward/forward compatibility\n- Schema versioning and tenant scoping\n- Drift detection and automated remediation\n- Canary validation and rollback\n\n## Code Example\n```javascript\n// Pseudo: drift check trigger\nif (driftDetected(messages)) { failDeployment(); }\n```\n\n## Follow-up Questions\n- How would you handle non-breaking field removals? \n- How would you scale drift checks for high-throughput topics?","diagram":"flowchart TD\n  A[Kafka topics] --> B[Flink job]\n  B --> C[Parquet in S3]\n  D[Schema Registry] --> E[Drift Detector]\n  E --> F[Audit/Alerts]","difficulty":"intermediate","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T20:46:37.547Z","createdAt":"2026-01-14T20:46:37.547Z"},{"id":"q-2085","question":"How would you implement compliant per-tenant data erasure in a streaming analytics pipeline that ingests tenant events from Kafka, writes to a data lake as Parquet, and serves BI queries, ensuring immutable data, auditability, and zero-downtime erasure while preserving peak-load latency? Include data-modeling, catalog updates, and operational steps?","answer":"Implement a tombstone-based erasure strategy using per-tenant partitioning in Kafka topics and immutable Parquet files. Upon receiving a data erasure request, queue a purge job that identifies all blocks containing the tenant's data, rewrites only the affected blocks to redact tenant-specific rows while preserving other data, updates the data catalog metadata to reflect the changes, and schedules block deletion after retention policies allow. This approach maintains auditability through immutable original blocks until safe deletion, ensures zero-downtime operations through asynchronous background processing, and preserves peak-load latency by minimizing rewrite scope and leveraging partition pruning for efficient query performance.","explanation":"## Why This Is Asked\nTests ability to design compliant data erasure in streaming pipelines, covering governance, data modeling, and operational safety while balancing performance requirements.\n\n## Key Concepts\n- Immutable data modeling with per-tenant partitions\n- Tombstone-based erasure and block-level rewriting\n- Data catalog updates and auditable erase events\n- Latency and backpressure considerations in streaming systems\n- Partition pruning for query performance optimization\n\n## Code Example\n```javascript\n// Pseudo purge orchestrator (high level)\nasync function purgeTenant(tenantId) {\n  con\n","diagram":"flowchart TD\n  A[Erasure Request] --> B{Tenant partition exists?}\n  B -- Yes --> C[Enqueue purge job]\n  C --> D[Scan Parquet blocks by tenant]\n  D --> E[Write tombstones/masked rows]\n  E --> F[DROP blocks after retention]\n  F --> G[Audit log entry]\n  G --> H[Catalog update]","difficulty":"intermediate","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Robinhood","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T05:04:58.536Z","createdAt":"2026-01-14T23:36:31.648Z"},{"id":"q-2119","question":"In a frontend data pipeline, you receive two sorted arrays of strings representing tags. Write a function mergeUnique(a,b) that returns a new array with all unique elements in sorted order, without mutating inputs. Assume inputs are sorted. Provide a minimal, robust JS implementation and explain its time/space complexity?","answer":"Use a two-pointer merge algorithm on the sorted inputs. Compare the current elements from both arrays, push the smaller value to the result array and advance its pointer. Skip duplicates by comparing with the last added element. Handle empty arrays gracefully. Time complexity: O(n+m); Space complexity: O(k) where k is the number of unique elements.","explanation":"## Why This Is Asked\nThis question tests practical array manipulation skills while maintaining sorted order, a common frontend data processing task. It evaluates understanding of efficient algorithms and the ability to work with constraints (no mutation, sorted inputs).\n\n## Key Concepts\n- Two-pointer technique for merging sorted arrays\n- Deduplication by comparing with the last added element\n- Time and space complexity analysis\n- Non-mutating operations\n\n## Code Example\n```javascript\nfunction mergeUnique(a, b) {\n  let i = 0, j = 0, result = [];\n  \n  while (i < a.length || j < b.length) {\n    let current;\n    \n    if (j >= b.length || (i < a.length && a[i] <= b[j])) {\n      current = a[i++];\n    } else {\n      current = b[j++];\n    }\n    \n    if (result.length === 0 || result[result.length - 1] !== current) {\n      result.push(current);\n    }\n  }\n  \n  return result;\n}\n```\n\n## Follow-up Questions\n- How would you adapt this if inputs might not be sorted?\n- What if we need to preserve the original order of first occurrence?\n- How would this change for very large datasets that don't fit in memory?\n- Could we optimize further if we know the range of possible tags?","diagram":null,"difficulty":"beginner","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Instacart"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T04:54:23.859Z","createdAt":"2026-01-15T02:27:01.426Z"},{"id":"q-836","question":"You are analyzing a web app's login events, captured as an array of objects: { userId: string, ts: string (ISO 8601) }. Write a JavaScript function that returns the top 3 users by login count in the past 24 hours. Tie-break with alphabetical userId. Provide a concise implementation and explain its time complexity?","answer":"Use a rolling 24h window: parse ts to ms, keep per-user counts in a Map, filter to now - t <= 24h, sort by count desc then userId asc, return top 3. Time complexity O(n log m) where m is distinct user","explanation":"## Why This Is Asked\nTests practical data processing in JavaScript: time-window filtering, per-user aggregation, and deterministic tie-breaking.\n\n## Key Concepts\n- Time-window filtering\n- Hash map for counts\n- Stable sorting with tie-breakers\n- Input validation and edge cases\n\n## Code Example\n\n```javascript\nfunction topUsers(events) {\n  const now = Date.now();\n  const window = 24 * 60 * 60 * 1000;\n  const counts = new Map();\n  for (const e of events) {\n    const t = Date.parse(e.ts);\n    if (Number.isNaN(t)) continue;\n    if (now - t <= window) {\n      counts.set(e.userId, (counts.get(e.userId) || 0) + 1);\n    }\n  }\n  const arr = Array.from(counts.entries());\n  arr.sort((a, b) => b[1] - a[1] || a[0].localeCompare(b[0]));\n  return arr.slice(0, 3).map(([id]) => id);\n}\n```\n\n## Follow-up Questions\n- How would you adapt this for streaming data?\n- How would you test with synthetic data to cover edge cases?","diagram":"flowchart TD\n  A[Events] --> B{Filter 24h}\n  B --> C[Count per User]\n  C --> D[Sort by Count, then ID]\n  D --> E[Top 3 Results]","difficulty":"beginner","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T13:21:30.077Z","createdAt":"2026-01-12T13:21:30.077Z"},{"id":"q-869","question":"In a high-traffic search suggestions feature, implement a Node.js module that debounces input by 300ms and caches per-query results in memory. Show cache invalidation and handle concurrent requests for the same key without duplicating work. Provide a compact, testable example with usage?","answer":"Implement an in-memory Map cache keyed by query with fields ts, data, and inFlight. Debounce input by 300ms; if cache exists and not expired (TTL 60s), return data; if inFlight exists for the key, awa","explanation":"This tests practical skills in debouncing, TTL caching, and in-flight deduplication under concurrent access.","diagram":null,"difficulty":"beginner","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Lyft","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T13:50:03.410Z","createdAt":"2026-01-12T13:50:03.410Z"},{"id":"q-912","question":"**KCSA Beginner Question: Inventory Pagination** Scenario: In a warehouse app, design a minimal endpoint GET /items?limit=&offset= that returns items ordered by id, supports pagination, and scales with 10k+ rows. Explain data model, indexing, and error handling. How would you implement a function to validate and apply LIMIT/OFFSET in code, ensuring correctness?","answer":"I’d implement GET /items?limit=&offset= with items ordered by id. Use an index on id and query: SELECT id, name, qty FROM items ORDER BY id ASC LIMIT $limit OFFSET $offset; return {total, items}. Vali","explanation":"## Why This Is Asked\nThe goal is to assess practical pagination design and data modeling skills on a tiny API layer, including correctness and testability.\n\n## Key Concepts\n- Pagination parameters validation\n- Indexed queries and ORDER BY performance\n- Edge-case handling and test coverage\n\n## Code Example\n\n```javascript\nfunction paginate(items, limit, offset) {\n  if (limit <= 0 || offset < 0) throw new Error(\"Invalid pagination params\");\n  const total = items.length;\n  const paged = items.slice(offset, offset + limit);\n  return { total, items: paged };\n}\n```\n\n## Follow-up Questions\n- How would you test pagination with 10k+ records and caching?\n- How would you adapt logic for distributed databases or replicas?","diagram":null,"difficulty":"beginner","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Apple"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T15:24:39.634Z","createdAt":"2026-01-12T15:24:39.634Z"},{"id":"q-947","question":"You're building a fintech API used by wallets and partner services to transfer funds; describe a secure, scalable auth and data-protection design for REST endpoints, including per-partner API keys, short-lived access tokens with rotating signing keys, mTLS between services, replay protection, and observability to detect abuse, plus failure modes and trade-offs?","answer":"Adopt OAuth2-like flows with PKCE for public clients, issue short-lived JWTs signed by rotating keys in an HSM-backed keystore, enforce mutual TLS between services, bind tokens to a client certificate","explanation":"## Why This Is Asked\n\nEvaluates practical fintech security decisions: key management, rotating keys, PKCE, mTLS, replay protection, and observability trade-offs.\n\n## Key Concepts\n\n- OAuth2-like flows with PKCE\n- JWT rotation and key management (HSM backing)\n- mTLS and token binding\n- Replay protection (nonce, jti)\n- Observability, rate limits, and revocation\n\n## Code Example\n\n```javascript\n// Token verification sketch\nfunction verifyToken(token, keystore) {\n  const { header } = parseJwt(token);\n  const key = keystore.getKey(header.kid);\n  const payload = verifyJwt(token, key);\n  if (!payload) return false;\n  if (payload.exp * 1000 < Date.now()) return false;\n  return payload.aud.includes(\"api\");\n}\n```\n\n## Follow-up Questions\n\n- How would you test key rotation without downtime?\n- How would you detect token abuse and adapt rate limits?","diagram":"flowchart TD\n  A[Client Request] --> B[Validate API Key]\n  B --> C[Authenticate & Authorize]\n  C --> D[Issue JWT with kid]\n  D --> E[Mutual TLS between services]\n  E --> F[Audit & Telemetry]\n  F --> G[Key Rotation Schedule]","difficulty":"intermediate","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","PayPal","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T16:36:34.150Z","createdAt":"2026-01-12T16:36:34.150Z"}],"subChannels":["general"],"companies":["Adobe","Amazon","Anthropic","Apple","Cloudflare","Coinbase","Databricks","Discord","DoorDash","Google","Hugging Face","Instacart","LinkedIn","Lyft","Meta","MongoDB","NVIDIA","Netflix","OpenAI","Oracle","PayPal","Plaid","Robinhood","Salesforce","Scale Ai","Slack","Square","Tesla","Twitter","Zoom"],"stats":{"total":22,"beginner":10,"intermediate":7,"advanced":5,"newThisWeek":22}}