{"questions":[{"id":"q-1021","question":"Design a globally distributed feature-flag engine for per-user rollout. Provide data model, consistency guarantees, and deployment strategy for scale across multiple regions. Include choice of storage (DynamoDB vs Redis), how regionOverrides and segments are merged, and hot-flip safety. Provide evaluation protocol and a concise pseudocode snippet for evaluateFeature(user, flag, context)?","answer":"Flag model: name, rolloutPct, segments, regionOverride, version. Source of truth: DynamoDB multi-region; Redis as near-cache with TTL. Evaluate by hashing userId+flag to get 0–99; enabled if value < r","explanation":"## Why This Is Asked\nAssesses ability to design scalable feature flags with per-user rollout, regional controls, and safe hot flips.\n\n## Key Concepts\n- Global distribution and eventual consistency\n- Data modeling for flags and segments\n- Cache invalidation and TTL\n- Rollout semantics and hot-flip safety\n\n## Code Example\n```javascript\nfunction evaluateFeature(user, flag, context){\n  const bucket = hash(user.id + flag.name) % 100;\n  const inSegment = (flag.segments && flag.segments.length) ? flag.segments.includes(user.segment) : true;\n  const regionOK = flag.regionOverride ? user.region === flag.regionOverride : true;\n  return (bucket < flag.rolloutPct) && inSegment && regionOK;\n}\n```\n\n## Follow-up Questions\n- How would you validate rollout accuracy under traffic spikes?\n- How would you rollback a faulty flag without service interruption?","diagram":"flowchart TD\n  A[Client Request] --> B[Eval Service]\n  B --> C[Flag Store]\n  B --> D[Cache]\n  C --> E[Flag Data (DB)]\n  D --> F[Return Result]","difficulty":"advanced","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Meta","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T19:34:13.849Z","createdAt":"2026-01-12T19:34:13.849Z"},{"id":"q-1042","question":"Design a high-throughput, fault-tolerant event ingestion pipeline for a fraud-detection service. It must guarantee exactly-once processing, deduplicate events across shards, support backpressure, and provide end-to-end observability. Explain the data model for dedup IDs, queue choice (Kafka vs Kinesis), idempotent sinks, and how you'd test failure scenarios?","answer":"Explain how you would implement exactly-once semantics for a high-throughput ingestion pipeline. Include dedup IDs, an atomic sink, idempotent writes, and at-least-once transport handling. Justify que","explanation":"## Why This Is Asked\n\nTests distributed-systems thinking: exactly-once, dedup, backpressure, observability.\n\n## Key Concepts\n\n- Exactly-once processing\n- Deduplication with dedup_id\n- Backpressure strategies (batching, pause vs drop)\n- Observability (tracing, metrics, logs)\n- Kafka vs Kinesis trade-offs\n- Idempotent sinks and compensating actions\n\n## Code Example\n\n```javascript\n// Pseudocode for idempotent sink write\nfunction writeEvent(evt, sink, store) {\n  const id = evt.dedup_id;\n  if (store.seen(id)) return;\n  sink.write(evt);\n  store.markSeen(id);\n}\n```\n\n## Follow-up Questions\n\n- How would you test guarantees for partial failures?\n- How would you ensure exactly-once with at-least-once queues?\n","diagram":null,"difficulty":"advanced","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Plaid","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T20:29:46.149Z","createdAt":"2026-01-12T20:29:46.149Z"},{"id":"q-1067","question":"In a Zoom-like multi-tenant Kubernetes cluster, you discover a pod running as root with a hostPath mounted. What concrete remediation plan would you implement using OPA Gatekeeper, Pod Security Standards, RBAC, NetworkPolicies, and image scanning? Include how you would verify tenant isolation with a minimal test that should fail if misconfigured?","answer":"Disable privileged mode and hostPath; require non-root and readOnlyRootFilesystem. Enforce OPA Gatekeeper Pod Security Standards and tighten RBAC. Add namespace NetworkPolicies and image scanning. Ver","explanation":"## Why This Is Asked\n\nThis question probes practical security hardening steps in a multi-tenant Kubernetes cluster, including policy enforcement, RBAC least privilege, network isolation, and image security.\n\n## Key Concepts\n\n- Pod Security Standards (PSP/OPA Gatekeeper)\n- Least-privilege RBAC and non-root containers\n- NetworkPolicies for tenant isolation\n- Image scanning and SBOM controls\n- Policy auditing and explainability\n\n## Code Example\n\n```javascript\n// Pseudo-test scaffold for policy denial\nfunction testPolicyDenial() {\n  // deploy non-root pod that tries hostPath; expect rejection\n}\n```\n\n## Follow-up Questions\n\n- How would you monitor and alert on policy violations in prod?\n- How would you safely roll back a policy that blocks legitimate workloads?","diagram":null,"difficulty":"intermediate","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Scale Ai","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T21:27:19.489Z","createdAt":"2026-01-12T21:27:19.489Z"},{"id":"q-1096","question":"In a simple analytics microservice, implement a Python function that reads a CSV file with headers user_id, action, timestamp (timestamp optional) and returns a dict mapping each user_id to the list of unique actions performed in chronological order; if timestamp is missing, preserve input order. Explain your approach and trade-offs?","answer":"Read a CSV with headers user_id, action, timestamp (optional). Build a dict: user_id -> list of unique actions in chronological order. Maintain a per-user seen set to deduplicate; when timestamp exist","explanation":"## Why This Is Asked\n\nThis question tests practical data processing: grouping, deduplication, and ordering in a realistic CSV workflow.\n\n## Key Concepts\n\n- CSV parsing with csv.DictReader\n\n- Dicts and sets for grouping and dedup\n\n- Optional timestamp handling and stable ordering\n\n- Edge cases: missing fields, non-integer user_id\n\n## Code Example\n\n```python\nimport csv\nfrom collections import defaultdict\n\ndef aggregate_actions(csv_path: str):\n    per_user = defaultdict(list)\n    seen = defaultdict(set)\n    with open(csv_path, newline='') as f:\n        for row in csv.DictReader(f):\n            uid = row.get('user_id')\n            action = row.get('action')\n            ts = row.get('timestamp')\n            if uid is None or action is None:\n                continue\n            try:\n                user = int(uid)\n            except ValueError:\n                continue\n            if action in seen[user]:\n                continue\n            seen[user].add(action)\n            per_user[user].append((action, ts))\n    result = {}\n    for user, items in per_user.items():\n        items_sorted = sorted(items, key=lambda x: (x[1] is None, x[1]))\n        result[user] = [act for act, _ in items_sorted]\n    return result\n```\n\n## Follow-up Questions\n\n- How would you scale this for millions of rows with streaming input?\n- How would you test robustness against malformed rows?","diagram":null,"difficulty":"beginner","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["OpenAI","Oracle","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T22:28:04.410Z","createdAt":"2026-01-12T22:28:04.410Z"},{"id":"q-1129","question":"Given a Kubernetes-deployed event-processing service that suddenly drops throughput from 10k to 6k after a feature release, outline a concrete, testable plan to diagnose and restore throughput. Include: **metrics** to monitor, **bottlenecks** to consider, and concrete, incremental changes like **rate limiting**, **backpressure**, and **idempotency**. Provide a safe rollback strategy and canary plan?","answer":"Instrument p95/p99 latency, error rate, queue depth, and throughput; enable OpenTelemetry traces and compare CPU/GC pauses, DB latency, and network. If bottleneck is consumer, introduce bounded queue,","explanation":"## Why This Is Asked\n\nAssess real-world troubleshooting, instrumentation, and change-management skills under production-like pressure. It probes how a candidate reasons about observability, bottlenecks, and safe rollbacks in a live system.\n\n## Key Concepts\n\n- Observability: p95/p99 latency, error rate, queue depth, throughput\n- Backpressure and rate limiting: bounded queues, controlled retries\n- Idempotency and safety: ensure duplicates don’t break state\n- Canary and rollback strategies\n\n## Code Example\n\n```javascript\n// Simple rate limiter example (token bucket)\nfunction createRateLimiter(tokensPerWindow, windowMs) {\n  let tokens = tokensPerWindow; let t0 = Date.now();\n  return function allow() {\n    const now = Date.now();\n    const elapsed = now - t0;\n    if (elapsed > windowMs) { t0 = now; tokens = tokensPerWindow; }\n    if (tokens > 0) { tokens--; return true; }\n    return false;\n  };\n}\n```\n\n## Follow-up Questions\n\n- How would you quantify a rollback safety threshold for the canary?\n- Which traces, metrics, and tests would you add to prevent recurrence?","diagram":null,"difficulty":"intermediate","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Scale Ai","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T23:33:22.477Z","createdAt":"2026-01-12T23:33:22.477Z"},{"id":"q-1184","question":"You maintain a public API for ride estimates and expect bursts. Implement a per-API-key rate limiter that allows 60 requests per minute using an in-memory structure. Provide the core logic in JavaScript (Node.js) and briefly justify your data structure, test approach, and how you’d handle restart scenarios?","answer":"Implement a per-API-key sliding window limiter. Use a Map<key, number[]> of timestamps. On each request: drop timestamps older than 60s; if the remaining count < 60, push now and allow; else return 42","explanation":"## Why This Is Asked\n\nThis question probes practical rate-limiting basics, a common reliability concern in public APIs. It tests per-key quotas, in-memory state, and how small designs behave under restart and burst scenarios.\n\n## Key Concepts\n\n- Sliding window algorithm\n- Map-based per-key state\n- Time-based pruning\n- Testing strategy and limitations\n\n## Code Example\n\n```javascript\n// Core limiter (simplified)\nconst limiter = new Map();\n\nfunction allowRequest(key, now = Date.now()) {\n  const windowMs = 60000;\n  const limit = 60;\n  const timestamps = limiter.get(key) || [];\n  while (timestamps.length && timestamps[0] <= now - windowMs) timestamps.shift();\n  if (timestamps.length < limit) {\n    timestamps.push(now);\n    limiter.set(key, timestamps);\n    return true;\n  }\n  return false;\n}\n```\n\n## Follow-up Questions\n\n- How would you adapt for multiple instances?\n- How would you test time-dependent behavior without slow sleeps?","diagram":"flowchart TD\n  A[Request] --> B[Prune stale timestamps]\n  B --> C{Allowed?}\n  C -->|Yes| D[Accept]\n  C -->|No| E[Reject]","difficulty":"beginner","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Lyft","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T03:44:27.012Z","createdAt":"2026-01-13T03:44:27.012Z"},{"id":"q-1227","question":"In a tiny model-usage service, write a Python function that validates a JSON payload with user_id, model_id, action, and timestamp, then inserts a document into MongoDB with a created_at field. Ensure an index exists on (model_id, timestamp) and handle duplicate submissions gracefully (idempotent using a unique composite index on (user_id, model_id, timestamp)). Include basic error handling and a minimal test snippet?","answer":"Use PyMongo; ensure a unique index on (user_id, model_id, timestamp) for idempotency. Validate payload: user_id and model_id non-empty strings, action in {'load','infer','monitor'}, timestamp parseabl","explanation":"## Why This Is Asked\nThis tests practical data validation, idempotent writes, and MongoDB indexing in a real-world microservice.\n\n## Key Concepts\n- Input validation\n- MongoDB upserts and unique indexes\n- Error handling and idempotency\n\n## Code Example\n\n```javascript\n// Pseudo: show intent without full implementation\n```\n\n## Follow-up Questions\n- How would you test this with bulk logs?\n- What are potential race conditions and mitigations?","diagram":null,"difficulty":"beginner","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Hugging Face","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T05:38:32.718Z","createdAt":"2026-01-13T05:38:32.718Z"},{"id":"q-1246","question":"Design a policy for a shared Kubernetes cluster serving multiple teams, ensuring namespace isolation, least privilege RBAC, image provenance, Pod Security Standards, and auditable policies. Propose concrete constraints using RBAC, PSP/PSA, NetworkPolicy, and OPA Gatekeeper; include a sample ConstraintTemplate and a test that rejects nonconforming pods. Explain trade-offs and monitoring strategy?","answer":"Implement a strict multi-tenant policy: namespace-per-team, RBAC restricted to its namespace, Pod Security Standards (restricted), image provenance via a signature-verified registry, and network segme","explanation":"## Why This Is Asked\n\nTest candidate's ability to design scalable security controls for multi-tenant K8s, covering RBAC, PodSecurity, image provenance, network isolation, and policy enforcement with Gatekeeper. Expect explanation of trade-offs (complexity vs. security) and how to validate in CI.\n\n## Key Concepts\n\n- Least privilege RBAC per-namespace\n- Pod Security Standards\n- OPA Gatekeeper constraint templates\n- Image provenance and signed images\n- NetworkPolicy segmentation\n- Audit logging and policy testing\n\n## Code Example\n\n```javascript\n// Test helper: ensures pods use allowed registries and non-root users\nfunction isPodCompliant(pod){\n  const okNamespace = pod.metadata.namespace.startsWith('team-');\n  const okImage = pod.spec.containers.every(c => c.image.startsWith('registry.internal/'));\n  const okUser = pod.spec.containers.every(c => c.securityContext?.runAsNonRoot === true);\n  return okNamespace && okImage && okUser;\n}\n```\n\n## Follow-up Questions\n\n- How would you test Gatekeeper in CI/CD for new policies?\n- How would you handle exception paths during incidents without breaking policy?","diagram":"flowchart TD\n  A[Multi-tenant cluster] --> B[Namespace isolation]\n  B --> C[RBAC scoped toNS]\n  B --> D[Pod Security Standards]\n  D --> E[Gatekeeper constraints]\n  E --> F[NetworkPolicy]\n  F --> G[Audit + monitoring]","difficulty":"advanced","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T06:41:29.284Z","createdAt":"2026-01-13T06:41:29.284Z"},{"id":"q-1298","question":"Design a globally-distributed rate limiter for a high-traffic service with per-tenant limits and cross-region fairness. Compare token-bucket and sliding-window approaches, specify data structures and caching, describe hot-path optimization, failure modes, and testing strategy. Assume latency budgets and real-time enforcement?","answer":"Hot path: cache per-tenant counters in memory; Redis as backing store for cross-node sync. Use a sliding-window with N buckets (e.g., 200) to bound latency; update counters atomically via Lua script; ","explanation":"## Why This Is Asked\nTests the ability to design scalable, low-latency controls with cross-region consistency and per-tenant quotas. It probes trade-offs between token-bucket and sliding-window, data modeling in Redis, and hot-path optimizations.\n\n## Key Concepts\n- Global rate limiting\n- Sliding-window vs token bucket\n- Atomic updates and Redis Lua scripts\n- Cross-region replication and timing\n- Observability and failure handling\n\n## Code Example\n```javascript\n// Pseudo-implementation sketch of a hot-path check\nfunction allow(tenantId, limit, windowMs){/*...*/}\n```\n\n## Follow-up Questions\n- How would you test backpressure under burst traffic?\n- How would you monitor for drift between regions and recover from clock skew?","diagram":"flowchart TD\n  A[Client Request] --> B[Edge Cache]\n  B --> C[Rate Limiter]\n  C --> D[Backend Service]\n  D --> E[Metrics & Alerts]","difficulty":"advanced","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","LinkedIn","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T08:44:35.661Z","createdAt":"2026-01-13T08:44:35.661Z"},{"id":"q-1351","question":"In a Node.js Express API backed by PostgreSQL, POST /search accepts { term } and currently builds a SQL with string concatenation. Describe a secure rewrite using parameterized queries and input validation, including error handling and ensuring the DB user has minimal privileges. How would you structure this to prevent SQL injection?","answer":"Use parameterized queries and input validation. Build pattern in code and pass as a parameter: 'SELECT * FROM products WHERE name ILIKE $1' with values ['%term%']. Validate term length and allowed cha","explanation":"## Why This Is Asked\nTests practical security habits for handling user input and avoiding SQL injection in a real code path.\n\n## Key Concepts\n- Parameterized queries\n- Input validation\n- Least privilege\n- Error handling\n\n## Code Example\n```javascript\n// Example in Node.js using pg\nconst pattern = '%' + term + '%';\nconst res = await client.query('SELECT * FROM products WHERE name ILIKE $1', [pattern]);\n```\n\n## Follow-up Questions\n- How would you test SQL injection resilience?\n- What edge cases on input do you consider?","diagram":null,"difficulty":"beginner","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Salesforce","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T13:08:15.280Z","createdAt":"2026-01-13T13:08:15.280Z"},{"id":"q-836","question":"You are analyzing a web app's login events, captured as an array of objects: { userId: string, ts: string (ISO 8601) }. Write a JavaScript function that returns the top 3 users by login count in the past 24 hours. Tie-break with alphabetical userId. Provide a concise implementation and explain its time complexity?","answer":"Use a rolling 24h window: parse ts to ms, keep per-user counts in a Map, filter to now - t <= 24h, sort by count desc then userId asc, return top 3. Time complexity O(n log m) where m is distinct user","explanation":"## Why This Is Asked\nTests practical data processing in JavaScript: time-window filtering, per-user aggregation, and deterministic tie-breaking.\n\n## Key Concepts\n- Time-window filtering\n- Hash map for counts\n- Stable sorting with tie-breakers\n- Input validation and edge cases\n\n## Code Example\n\n```javascript\nfunction topUsers(events) {\n  const now = Date.now();\n  const window = 24 * 60 * 60 * 1000;\n  const counts = new Map();\n  for (const e of events) {\n    const t = Date.parse(e.ts);\n    if (Number.isNaN(t)) continue;\n    if (now - t <= window) {\n      counts.set(e.userId, (counts.get(e.userId) || 0) + 1);\n    }\n  }\n  const arr = Array.from(counts.entries());\n  arr.sort((a, b) => b[1] - a[1] || a[0].localeCompare(b[0]));\n  return arr.slice(0, 3).map(([id]) => id);\n}\n```\n\n## Follow-up Questions\n- How would you adapt this for streaming data?\n- How would you test with synthetic data to cover edge cases?","diagram":"flowchart TD\n  A[Events] --> B{Filter 24h}\n  B --> C[Count per User]\n  C --> D[Sort by Count, then ID]\n  D --> E[Top 3 Results]","difficulty":"beginner","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:21:30.077Z","createdAt":"2026-01-12T13:21:30.077Z"},{"id":"q-869","question":"In a high-traffic search suggestions feature, implement a Node.js module that debounces input by 300ms and caches per-query results in memory. Show cache invalidation and handle concurrent requests for the same key without duplicating work. Provide a compact, testable example with usage?","answer":"Implement an in-memory Map cache keyed by query with fields ts, data, and inFlight. Debounce input by 300ms; if cache exists and not expired (TTL 60s), return data; if inFlight exists for the key, awa","explanation":"This tests practical skills in debouncing, TTL caching, and in-flight deduplication under concurrent access.","diagram":null,"difficulty":"beginner","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Lyft","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:50:03.410Z","createdAt":"2026-01-12T13:50:03.410Z"},{"id":"q-912","question":"**KCSA Beginner Question: Inventory Pagination** Scenario: In a warehouse app, design a minimal endpoint GET /items?limit=&offset= that returns items ordered by id, supports pagination, and scales with 10k+ rows. Explain data model, indexing, and error handling. How would you implement a function to validate and apply LIMIT/OFFSET in code, ensuring correctness?","answer":"I’d implement GET /items?limit=&offset= with items ordered by id. Use an index on id and query: SELECT id, name, qty FROM items ORDER BY id ASC LIMIT $limit OFFSET $offset; return {total, items}. Vali","explanation":"## Why This Is Asked\nThe goal is to assess practical pagination design and data modeling skills on a tiny API layer, including correctness and testability.\n\n## Key Concepts\n- Pagination parameters validation\n- Indexed queries and ORDER BY performance\n- Edge-case handling and test coverage\n\n## Code Example\n\n```javascript\nfunction paginate(items, limit, offset) {\n  if (limit <= 0 || offset < 0) throw new Error(\"Invalid pagination params\");\n  const total = items.length;\n  const paged = items.slice(offset, offset + limit);\n  return { total, items: paged };\n}\n```\n\n## Follow-up Questions\n- How would you test pagination with 10k+ records and caching?\n- How would you adapt logic for distributed databases or replicas?","diagram":null,"difficulty":"beginner","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Apple"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T15:24:39.634Z","createdAt":"2026-01-12T15:24:39.634Z"},{"id":"q-947","question":"You're building a fintech API used by wallets and partner services to transfer funds; describe a secure, scalable auth and data-protection design for REST endpoints, including per-partner API keys, short-lived access tokens with rotating signing keys, mTLS between services, replay protection, and observability to detect abuse, plus failure modes and trade-offs?","answer":"Adopt OAuth2-like flows with PKCE for public clients, issue short-lived JWTs signed by rotating keys in an HSM-backed keystore, enforce mutual TLS between services, bind tokens to a client certificate","explanation":"## Why This Is Asked\n\nEvaluates practical fintech security decisions: key management, rotating keys, PKCE, mTLS, replay protection, and observability trade-offs.\n\n## Key Concepts\n\n- OAuth2-like flows with PKCE\n- JWT rotation and key management (HSM backing)\n- mTLS and token binding\n- Replay protection (nonce, jti)\n- Observability, rate limits, and revocation\n\n## Code Example\n\n```javascript\n// Token verification sketch\nfunction verifyToken(token, keystore) {\n  const { header } = parseJwt(token);\n  const key = keystore.getKey(header.kid);\n  const payload = verifyJwt(token, key);\n  if (!payload) return false;\n  if (payload.exp * 1000 < Date.now()) return false;\n  return payload.aud.includes(\"api\");\n}\n```\n\n## Follow-up Questions\n\n- How would you test key rotation without downtime?\n- How would you detect token abuse and adapt rate limits?","diagram":"flowchart TD\n  A[Client Request] --> B[Validate API Key]\n  B --> C[Authenticate & Authorize]\n  C --> D[Issue JWT with kid]\n  D --> E[Mutual TLS between services]\n  E --> F[Audit & Telemetry]\n  F --> G[Key Rotation Schedule]","difficulty":"intermediate","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","PayPal","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T16:36:34.150Z","createdAt":"2026-01-12T16:36:34.150Z"}],"subChannels":["general"],"companies":["Adobe","Amazon","Apple","Cloudflare","Coinbase","Discord","Google","Hugging Face","LinkedIn","Lyft","Meta","MongoDB","NVIDIA","OpenAI","Oracle","PayPal","Plaid","Salesforce","Scale Ai","Slack","Tesla","Twitter","Zoom"],"stats":{"total":14,"beginner":7,"intermediate":3,"advanced":4,"newThisWeek":14}}