{"questions":[{"id":"q-1021","question":"Design a globally distributed feature-flag engine for per-user rollout. Provide data model, consistency guarantees, and deployment strategy for scale across multiple regions. Include choice of storage (DynamoDB vs Redis), how regionOverrides and segments are merged, and hot-flip safety. Provide evaluation protocol and a concise pseudocode snippet for evaluateFeature(user, flag, context)?","answer":"Flag model: name, rolloutPct, segments, regionOverride, version. Source of truth: DynamoDB multi-region; Redis as near-cache with TTL. Evaluate by hashing userId+flag to get 0–99; enabled if value < r","explanation":"## Why This Is Asked\nAssesses ability to design scalable feature flags with per-user rollout, regional controls, and safe hot flips.\n\n## Key Concepts\n- Global distribution and eventual consistency\n- Data modeling for flags and segments\n- Cache invalidation and TTL\n- Rollout semantics and hot-flip safety\n\n## Code Example\n```javascript\nfunction evaluateFeature(user, flag, context){\n  const bucket = hash(user.id + flag.name) % 100;\n  const inSegment = (flag.segments && flag.segments.length) ? flag.segments.includes(user.segment) : true;\n  const regionOK = flag.regionOverride ? user.region === flag.regionOverride : true;\n  return (bucket < flag.rolloutPct) && inSegment && regionOK;\n}\n```\n\n## Follow-up Questions\n- How would you validate rollout accuracy under traffic spikes?\n- How would you rollback a faulty flag without service interruption?","diagram":"flowchart TD\n  A[Client Request] --> B[Eval Service]\n  B --> C[Flag Store]\n  B --> D[Cache]\n  C --> E[Flag Data (DB)]\n  D --> F[Return Result]","difficulty":"advanced","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Meta","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T19:34:13.849Z","createdAt":"2026-01-12T19:34:13.849Z"},{"id":"q-1042","question":"Design a high-throughput, fault-tolerant event ingestion pipeline for a fraud-detection service. It must guarantee exactly-once processing, deduplicate events across shards, support backpressure, and provide end-to-end observability. Explain the data model for dedup IDs, queue choice (Kafka vs Kinesis), idempotent sinks, and how you'd test failure scenarios?","answer":"Explain how you would implement exactly-once semantics for a high-throughput ingestion pipeline. Include dedup IDs, an atomic sink, idempotent writes, and at-least-once transport handling. Justify que","explanation":"## Why This Is Asked\n\nTests distributed-systems thinking: exactly-once, dedup, backpressure, observability.\n\n## Key Concepts\n\n- Exactly-once processing\n- Deduplication with dedup_id\n- Backpressure strategies (batching, pause vs drop)\n- Observability (tracing, metrics, logs)\n- Kafka vs Kinesis trade-offs\n- Idempotent sinks and compensating actions\n\n## Code Example\n\n```javascript\n// Pseudocode for idempotent sink write\nfunction writeEvent(evt, sink, store) {\n  const id = evt.dedup_id;\n  if (store.seen(id)) return;\n  sink.write(evt);\n  store.markSeen(id);\n}\n```\n\n## Follow-up Questions\n\n- How would you test guarantees for partial failures?\n- How would you ensure exactly-once with at-least-once queues?\n","diagram":null,"difficulty":"advanced","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Plaid","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T20:29:46.149Z","createdAt":"2026-01-12T20:29:46.149Z"},{"id":"q-1067","question":"In a Zoom-like multi-tenant Kubernetes cluster, you discover a pod running as root with a hostPath mounted. What concrete remediation plan would you implement using OPA Gatekeeper, Pod Security Standards, RBAC, NetworkPolicies, and image scanning? Include how you would verify tenant isolation with a minimal test that should fail if misconfigured?","answer":"Disable privileged mode and hostPath; require non-root and readOnlyRootFilesystem. Enforce OPA Gatekeeper Pod Security Standards and tighten RBAC. Add namespace NetworkPolicies and image scanning. Ver","explanation":"## Why This Is Asked\n\nThis question probes practical security hardening steps in a multi-tenant Kubernetes cluster, including policy enforcement, RBAC least privilege, network isolation, and image security.\n\n## Key Concepts\n\n- Pod Security Standards (PSP/OPA Gatekeeper)\n- Least-privilege RBAC and non-root containers\n- NetworkPolicies for tenant isolation\n- Image scanning and SBOM controls\n- Policy auditing and explainability\n\n## Code Example\n\n```javascript\n// Pseudo-test scaffold for policy denial\nfunction testPolicyDenial() {\n  // deploy non-root pod that tries hostPath; expect rejection\n}\n```\n\n## Follow-up Questions\n\n- How would you monitor and alert on policy violations in prod?\n- How would you safely roll back a policy that blocks legitimate workloads?","diagram":null,"difficulty":"intermediate","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Scale Ai","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T21:27:19.489Z","createdAt":"2026-01-12T21:27:19.489Z"},{"id":"q-1096","question":"In a simple analytics microservice, implement a Python function that reads a CSV file with headers user_id, action, timestamp (timestamp optional) and returns a dict mapping each user_id to the list of unique actions performed in chronological order; if timestamp is missing, preserve input order. Explain your approach and trade-offs?","answer":"Read a CSV with headers user_id, action, timestamp (optional). Build a dict: user_id -> list of unique actions in chronological order. Maintain a per-user seen set to deduplicate; when timestamp exist","explanation":"## Why This Is Asked\n\nThis question tests practical data processing: grouping, deduplication, and ordering in a realistic CSV workflow.\n\n## Key Concepts\n\n- CSV parsing with csv.DictReader\n\n- Dicts and sets for grouping and dedup\n\n- Optional timestamp handling and stable ordering\n\n- Edge cases: missing fields, non-integer user_id\n\n## Code Example\n\n```python\nimport csv\nfrom collections import defaultdict\n\ndef aggregate_actions(csv_path: str):\n    per_user = defaultdict(list)\n    seen = defaultdict(set)\n    with open(csv_path, newline='') as f:\n        for row in csv.DictReader(f):\n            uid = row.get('user_id')\n            action = row.get('action')\n            ts = row.get('timestamp')\n            if uid is None or action is None:\n                continue\n            try:\n                user = int(uid)\n            except ValueError:\n                continue\n            if action in seen[user]:\n                continue\n            seen[user].add(action)\n            per_user[user].append((action, ts))\n    result = {}\n    for user, items in per_user.items():\n        items_sorted = sorted(items, key=lambda x: (x[1] is None, x[1]))\n        result[user] = [act for act, _ in items_sorted]\n    return result\n```\n\n## Follow-up Questions\n\n- How would you scale this for millions of rows with streaming input?\n- How would you test robustness against malformed rows?","diagram":null,"difficulty":"beginner","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["OpenAI","Oracle","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":["csv parsing","csv.dictreader","data aggregation","unique actions","chronological order","timestamp handling","per-user grouping","deduplication logic"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2026-01-17T04:44:36.052Z","createdAt":"2026-01-12T22:28:04.410Z"},{"id":"q-1129","question":"Given a Kubernetes-deployed event-processing service that suddenly drops throughput from 10k to 6k after a feature release, outline a concrete, testable plan to diagnose and restore throughput. Include: **metrics** to monitor, **bottlenecks** to consider, and concrete, incremental changes like **rate limiting**, **backpressure**, and **idempotency**. Provide a safe rollback strategy and canary plan?","answer":"Instrument p95/p99 latency, error rate, queue depth, and throughput; enable OpenTelemetry traces and compare CPU/GC pauses, DB latency, and network. If bottleneck is consumer, introduce bounded queue,","explanation":"## Why This Is Asked\n\nAssess real-world troubleshooting, instrumentation, and change-management skills under production-like pressure. It probes how a candidate reasons about observability, bottlenecks, and safe rollbacks in a live system.\n\n## Key Concepts\n\n- Observability: p95/p99 latency, error rate, queue depth, throughput\n- Backpressure and rate limiting: bounded queues, controlled retries\n- Idempotency and safety: ensure duplicates don’t break state\n- Canary and rollback strategies\n\n## Code Example\n\n```javascript\n// Simple rate limiter example (token bucket)\nfunction createRateLimiter(tokensPerWindow, windowMs) {\n  let tokens = tokensPerWindow; let t0 = Date.now();\n  return function allow() {\n    const now = Date.now();\n    const elapsed = now - t0;\n    if (elapsed > windowMs) { t0 = now; tokens = tokensPerWindow; }\n    if (tokens > 0) { tokens--; return true; }\n    return false;\n  };\n}\n```\n\n## Follow-up Questions\n\n- How would you quantify a rollback safety threshold for the canary?\n- Which traces, metrics, and tests would you add to prevent recurrence?","diagram":null,"difficulty":"intermediate","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Scale Ai","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T23:33:22.477Z","createdAt":"2026-01-12T23:33:22.477Z"},{"id":"q-1184","question":"You maintain a public API for ride estimates and expect bursts. Implement a per-API-key rate limiter that allows 60 requests per minute using an in-memory structure. Provide the core logic in JavaScript (Node.js) and briefly justify your data structure, test approach, and how you’d handle restart scenarios?","answer":"Implement a per-API-key sliding window limiter. Use a Map<key, number[]> of timestamps. On each request: drop timestamps older than 60s; if the remaining count < 60, push now and allow; else return 42","explanation":"## Why This Is Asked\n\nThis question probes practical rate-limiting basics, a common reliability concern in public APIs. It tests per-key quotas, in-memory state, and how small designs behave under restart and burst scenarios.\n\n## Key Concepts\n\n- Sliding window algorithm\n- Map-based per-key state\n- Time-based pruning\n- Testing strategy and limitations\n\n## Code Example\n\n```javascript\n// Core limiter (simplified)\nconst limiter = new Map();\n\nfunction allowRequest(key, now = Date.now()) {\n  const windowMs = 60000;\n  const limit = 60;\n  const timestamps = limiter.get(key) || [];\n  while (timestamps.length && timestamps[0] <= now - windowMs) timestamps.shift();\n  if (timestamps.length < limit) {\n    timestamps.push(now);\n    limiter.set(key, timestamps);\n    return true;\n  }\n  return false;\n}\n```\n\n## Follow-up Questions\n\n- How would you adapt for multiple instances?\n- How would you test time-dependent behavior without slow sleeps?","diagram":"flowchart TD\n  A[Request] --> B[Prune stale timestamps]\n  B --> C{Allowed?}\n  C -->|Yes| D[Accept]\n  C -->|No| E[Reject]","difficulty":"beginner","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Lyft","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T03:44:27.012Z","createdAt":"2026-01-13T03:44:27.012Z"},{"id":"q-1227","question":"In a tiny model-usage service, write a Python function that validates a JSON payload with user_id, model_id, action, and timestamp, then inserts a document into MongoDB with a created_at field. Ensure an index exists on (model_id, timestamp) and handle duplicate submissions gracefully (idempotent using a unique composite index on (user_id, model_id, timestamp)). Include basic error handling and a minimal test snippet?","answer":"Use PyMongo; ensure a unique index on (user_id, model_id, timestamp) for idempotency. Validate payload: user_id and model_id non-empty strings, action in {'load','infer','monitor'}, timestamp parseabl","explanation":"## Why This Is Asked\nThis tests practical data validation, idempotent writes, and MongoDB indexing in a real-world microservice.\n\n## Key Concepts\n- Input validation\n- MongoDB upserts and unique indexes\n- Error handling and idempotency\n\n## Code Example\n\n```javascript\n// Pseudo: show intent without full implementation\n```\n\n## Follow-up Questions\n- How would you test this with bulk logs?\n- What are potential race conditions and mitigations?","diagram":null,"difficulty":"beginner","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Hugging Face","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T05:38:32.718Z","createdAt":"2026-01-13T05:38:32.718Z"},{"id":"q-1246","question":"Design a policy for a shared Kubernetes cluster serving multiple teams, ensuring namespace isolation, least privilege RBAC, image provenance, Pod Security Standards, and auditable policies. Propose concrete constraints using RBAC, PSP/PSA, NetworkPolicy, and OPA Gatekeeper; include a sample ConstraintTemplate and a test that rejects nonconforming pods. Explain trade-offs and monitoring strategy?","answer":"Implement a strict multi-tenant policy: namespace-per-team, RBAC restricted to its namespace, Pod Security Standards (restricted), image provenance via a signature-verified registry, and network segme","explanation":"## Why This Is Asked\n\nTest candidate's ability to design scalable security controls for multi-tenant K8s, covering RBAC, PodSecurity, image provenance, network isolation, and policy enforcement with Gatekeeper. Expect explanation of trade-offs (complexity vs. security) and how to validate in CI.\n\n## Key Concepts\n\n- Least privilege RBAC per-namespace\n- Pod Security Standards\n- OPA Gatekeeper constraint templates\n- Image provenance and signed images\n- NetworkPolicy segmentation\n- Audit logging and policy testing\n\n## Code Example\n\n```javascript\n// Test helper: ensures pods use allowed registries and non-root users\nfunction isPodCompliant(pod){\n  const okNamespace = pod.metadata.namespace.startsWith('team-');\n  const okImage = pod.spec.containers.every(c => c.image.startsWith('registry.internal/'));\n  const okUser = pod.spec.containers.every(c => c.securityContext?.runAsNonRoot === true);\n  return okNamespace && okImage && okUser;\n}\n```\n\n## Follow-up Questions\n\n- How would you test Gatekeeper in CI/CD for new policies?\n- How would you handle exception paths during incidents without breaking policy?","diagram":"flowchart TD\n  A[Multi-tenant cluster] --> B[Namespace isolation]\n  B --> C[RBAC scoped toNS]\n  B --> D[Pod Security Standards]\n  D --> E[Gatekeeper constraints]\n  E --> F[NetworkPolicy]\n  F --> G[Audit + monitoring]","difficulty":"advanced","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T06:41:29.284Z","createdAt":"2026-01-13T06:41:29.284Z"},{"id":"q-1298","question":"Design a globally-distributed rate limiter for a high-traffic service with per-tenant limits and cross-region fairness. Compare token-bucket and sliding-window approaches, specify data structures and caching, describe hot-path optimization, failure modes, and testing strategy. Assume latency budgets and real-time enforcement?","answer":"Hot path: cache per-tenant counters in memory; Redis as backing store for cross-node sync. Use a sliding-window with N buckets (e.g., 200) to bound latency; update counters atomically via Lua script; ","explanation":"## Why This Is Asked\nTests the ability to design scalable, low-latency controls with cross-region consistency and per-tenant quotas. It probes trade-offs between token-bucket and sliding-window, data modeling in Redis, and hot-path optimizations.\n\n## Key Concepts\n- Global rate limiting\n- Sliding-window vs token bucket\n- Atomic updates and Redis Lua scripts\n- Cross-region replication and timing\n- Observability and failure handling\n\n## Code Example\n```javascript\n// Pseudo-implementation sketch of a hot-path check\nfunction allow(tenantId, limit, windowMs){/*...*/}\n```\n\n## Follow-up Questions\n- How would you test backpressure under burst traffic?\n- How would you monitor for drift between regions and recover from clock skew?","diagram":"flowchart TD\n  A[Client Request] --> B[Edge Cache]\n  B --> C[Rate Limiter]\n  C --> D[Backend Service]\n  D --> E[Metrics & Alerts]","difficulty":"advanced","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","LinkedIn","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T08:44:35.661Z","createdAt":"2026-01-13T08:44:35.661Z"},{"id":"q-1351","question":"In a Node.js Express API backed by PostgreSQL, POST /search accepts { term } and currently builds a SQL with string concatenation. Describe a secure rewrite using parameterized queries and input validation, including error handling and ensuring the DB user has minimal privileges. How would you structure this to prevent SQL injection?","answer":"Use parameterized queries and input validation. Build pattern in code and pass as a parameter: 'SELECT * FROM products WHERE name ILIKE $1' with values ['%term%']. Validate term length and allowed cha","explanation":"## Why This Is Asked\nTests practical security habits for handling user input and avoiding SQL injection in a real code path.\n\n## Key Concepts\n- Parameterized queries\n- Input validation\n- Least privilege\n- Error handling\n\n## Code Example\n```javascript\n// Example in Node.js using pg\nconst pattern = '%' + term + '%';\nconst res = await client.query('SELECT * FROM products WHERE name ILIKE $1', [pattern]);\n```\n\n## Follow-up Questions\n- How would you test SQL injection resilience?\n- What edge cases on input do you consider?","diagram":null,"difficulty":"beginner","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Salesforce","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T13:08:15.280Z","createdAt":"2026-01-13T13:08:15.280Z"},{"id":"q-1680","question":"In a Kubernetes-deployed microservice that ingests up to 200 events/sec from a queue, each event has event_id and payload. The handler must be idempotent so a duplicate delivery does not write to Postgres. Propose a concrete Redis-based dedup strategy (SETNX with EXPIRE TTL) and outline how you'd implement the dedup path in code, including how you'd handle retries, restarts, and cleanup?","answer":"Use Redis as a dedup cache keyed by event_id with a TTL (e.g., 1 hour). On receipt, SETNX event_id 1; if created, EXPIRE 3600, process and persist, return 200. If exists, skip work and return 200. Opt","explanation":"## Why This Is Asked\nThis question probes practical idempotency design for high-throughput services, a common real-world requirement in large-scale systems.\n\n## Key Concepts\n- Idempotency and deduplication\n- Redis SETNX with EXPIRE for lockless dedup\n- Atomicity and race condition handling\n- Retries, crash recovery, and cleanup strategies\n\n## Code Example\n```javascript\n// Node.js with ioredis\nconst key = `dedup:${event.event_id}`;\nconst created = await redis.set(key, '1', 'NX', 'EX', 3600);\nif (!created) return; // duplicate, skip\n// process and persist payload\n```\n\n## Follow-up Questions\n- How would you handle dedup across multiple replicas?\n- What TTL would you choose for varying workloads, and why?\n","diagram":"flowchart TD\n  A[Event arrives] --> B{NX created?}\n  B -- Yes --> C[Process & Persist]\n  B -- No --> D[Skip]\n  C --> E[Done]\n  D --> E","difficulty":"beginner","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","NVIDIA","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T06:52:21.824Z","createdAt":"2026-01-14T06:52:21.824Z"},{"id":"q-1788","question":"You're designing a payment authorization path for a fintech platform (think Plaid) where a mobile tap triggers multiple services: Auth, Fraud/Risk, Ledger, and Notification. How would you ensure idempotent processing, at-least-once retries, and eventual consistency across services? Describe data models, the flow (outbox or saga), and fault tolerance (backoffs, DLQ)?","answer":"Use a per-transaction idempotency key generated on user tap and stored centrally. Route flow via an outbox-saga: Auth → Fraud → Ledger, with compensating actions if a step fails. Deduplicate retries b","explanation":"## Why This Is Asked\n\nThis question probes understanding of idempotency, distributed transactions, and fault tolerance in real-time payment systems. It mirrors fintech patterns at Plaid and Lyft, where user actions must not duplicate or lose funds.\n\n## Key Concepts\n\n- Idempotency keys\n- Outbox pattern and sagas\n- Exactly-once vs at-least-once\n- Dead-letter queues and backoff strategies\n\n## Code Example\n\n```javascript\n// Sketch: idempotency check and outbox write\nfunction process(tx, store) {\n  const key = tx.id;\n  if (store.exists(key)) return;\n  store.saveOutbox(tx);\n  // publish events ...\n}\n```\n\n## Follow-up Questions\n\n- How would you test idempotency guarantees under partial failures?\n- How do you monitor for orphaned or duplicate transactions in production?\n","diagram":"flowchart TD\n  Tap[User taps Pay] --> Auth[Auth Service]\n  Auth --> Fraud[Fraud/Risk]\n  Fraud --> Ledger[Ledger]\n  Ledger --> Notify[Notification]\n  Ledger --> Outbox[Outbox]","difficulty":"intermediate","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T10:49:53.252Z","createdAt":"2026-01-14T10:49:53.252Z"},{"id":"q-1828","question":"You operate a polyglot data stack with MongoDB and Oracle. A global e-commerce app experiences 300–500ms latency on order placement during peak hours, despite modest CPU usage. Propose an end-to-end plan to diagnose and fix, covering data model, indexing, shard/cluster topology, connection pooling, caching, and cross-database consistency. Include concrete knobs you would adjust and how you'd validate impact?","answer":"Investigate end-to-end latency during peak: collect per-query latency, queueing, and stall times. MongoDB: validate shard keys and data distribution, verify index usage with explain, tune writeConcern","explanation":"## Why This Is Asked\n\nTests the ability to design end-to-end diagnostics and multi-database optimization under load, not just theory.\n\n## Key Concepts\n\n- Shard keys and data distribution\n- Explain plans and index selection\n- WriteConcern, journaling, balancer health\n- SGA/PGA tuning, RAC, partitioning\n- Caching strategies and end-to-end validation\n\n## Code Example\n\n```javascript\n// Example: inspect MongoDB index usage\ndb.orders.find({userId: \"X\"}).explain(\"executionStats\")\n```\n\n## Follow-up Questions\n\n- How would you validate a canary rollout across both databases?\n- What metrics define a successful latency reduction and how would you monitor it long-term?","diagram":"flowchart TD\n  A[User Action] --> B[MongoDB]\n  B --> C[Oracle]\n  C --> D[Cache/APP]\n  D --> E[Metrics]","difficulty":"advanced","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T13:10:08.577Z","createdAt":"2026-01-14T13:10:08.578Z"},{"id":"q-1877","question":"In a multi-tenant data pipeline on Kubernetes serving a PayPal-like payments gateway, events flow from a single Kafka topic to a Spark streaming job that writes to a data lake. How would you enforce per-tenant data isolation, encryption, auditing, and masking while maintaining a 95th percentile latency under peak load?","answer":"Per-tenant isolation with SPIFFE IDs and mTLS; namespace RBAC; OPA Gatekeeper policies; envelope encryption with KMS at rest; TLS in transit; per-tenant masking service for PII; immutable audit logs; ","explanation":"## Why This Is Asked\nTests practical security, observability, and performance trade-offs in a real streaming, multi-tenant data stack on Kubernetes; requires knowledge of security primitives, streaming best practices, and performance validation.\n\n## Key Concepts\n- Multi-tenant isolation in Kubernetes\n- SPIFFE/SVIDs and mutual TLS\n- Per-tenant access in Kafka and Spark (RBAC/ABAC)\n- Data masking and envelope encryption with KMS\n- Auditing and immutable logs\n- Latency targets (95th percentile) under burst load\n- Observability and tracing across components\n\n## Code Example\n```javascript\n// Masking example for streaming records\nfunction maskPII(record) {\n  if (record.cardNumber) record.cardNumber = \"****-****-****-\" + record.cardNumber.slice(-4);\n  if (record.email) record.email = \"***@***\";\n  return record;\n}\n```\n\n## Follow-up Questions\n- How would you validate masking does not leak data under failure modes?\n- How would you simulate tenant-level bursts to prove 95th percentile latency remains within SLA?","diagram":"flowchart TD\n  Tenant[Tenant] --> Kafka[Kafka Topic]\n  Kafka --> Processor[Spark Streaming Job]\n  Processor --> Lake[Data Lake / Warehouse]\n  Lake --> Audit[Audit & Masking Service]","difficulty":"intermediate","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Google","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T15:37:19.288Z","createdAt":"2026-01-14T15:37:19.288Z"},{"id":"q-1950","question":"You’re asked to implement a simple in-memory rate limiter for a Node.js/Express API: cap each API key at 100 requests per 10 minutes. Provide a minimal in-process solution using a per-key timestamp array, how you prune old entries, and how you handle bursts. Explain trade-offs and a plan to scale to multiple processes?","answer":"On each API key bucket, store an array of request timestamps. For a request, purge timestamps older than 10 minutes, then permit if length < 100; otherwise return 429. Schedule periodic cleanup to bou","explanation":"## Why This Is Asked\n\nTests practical understanding of simple rate limiting, memory bounds, and concurrency in a real service.\n\n## Key Concepts\n\n- In-memory data structures for per-key state\n- Time-window calculations and pruning\n- Concurrency and process-safety; scalability trade-offs\n\n## Code Example\n\n```javascript\nconst buckets = new Map();\n\nfunction limit(req, res, next){\n  const key = req.headers['x-api-key'];\n  if(!key) return res.status(400).send('API key required');\n  const now = Date.now();\n  const arr = buckets.get(key) || [];\n  // prune\n  while (arr.length && now - arr[0] > 10 * 60 * 1000) arr.shift();\n  if (arr.length >= 100) return res.status(429).send('Too many requests');\n  arr.push(now);\n  buckets.set(key, arr);\n  next();\n}\n```\n\n## Follow-up Questions\n\n- How would you adapt this for a multi-instance deployment?\n- What are the limitations of in-memory rate limiting?","diagram":"flowchart TD\n  A[API Request] --> B{Has Key}\n  B --> C[Prune Window]\n  C --> D{Under Limit?}\n  D -->|Yes| E[Allow & Record]\n  D -->|No| F[Reject 429]","difficulty":"beginner","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Instacart","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T18:46:55.888Z","createdAt":"2026-01-14T18:46:55.888Z"},{"id":"q-2008","question":"In a cloud-native data platform delivering streaming telemetry to a data lake, how would you implement end-to-end data quality and schema drift control for Kafka → Flink → Parquet, when tenants frequently emit extra fields? Describe schema versioning, compatibility, drift detection, and automated remediation to downstream dashboards, with concrete knobs and testing steps?","answer":"Implement a comprehensive schema governance framework using Confluent Schema Registry with AVRO, featuring per-tenant schema namespaces and automated drift detection. For schema versioning, use semantic versioning (1.0.0 → 1.1.0) with backward/forward compatibility rules. Configure tenant fields as optional with default values using AVRO's union types (e.g., `['null', 'string']`). Deploy schema validation middleware in Flink that checks incoming messages against registered schemas, flagging drift when new fields appear without proper schema evolution. For automated remediation, implement a circuit breaker pattern that routes anomalous data to a quarantine topic while triggering alerting to downstream dashboards via Prometheus/Grafana. Key configuration knobs include: `schema.compatibility.level=BACKWARD`, `schema.enable.cache=true`, and `topic.name.strategy=TopicNameStrategy` with tenant prefixes. Testing involves: 1) Schema evolution unit tests using Schema Registry's compatibility API, 2) Canary deployments with synthetic data containing extra fields, 3) Load testing drift detection at 10K msg/sec, and 4) Chaos engineering by injecting schema-breaking changes to validate rollback procedures.","explanation":"## Why This Is Asked\nAssesses practical data quality controls, schema evolution discipline, and safe deployment practices in multi-tenant streaming environments.\n\n## Key Concepts\n- Schema Registry with AVRO and compatibility enforcement\n- Per-tenant schema namespaces and semantic versioning\n- Real-time drift detection in Flink processing pipeline\n- Automated quarantine and alerting mechanisms\n- Circuit breaker patterns for data quality protection\n\n## Code Example\n```java\n// Flink schema validation with drift detection\npublic class SchemaValidator extends ProcessFunction<GenericRecord, GenericRecord> {\n    private final SchemaRegistryClient registry;\n    \n    @Override\n    public void processElement(GenericRecord record, Context ctx) {\n        try {\n            Schema schema = registry.getLatestSchema(tenantTopic);\n            if (!isValidAgainstSchema(record, schema)) {\n                ctx.output(quarantineTag, record);\n                alertDashboard(\"SCHEMA_DRIFT\", tenantTopic);\n            } else {\n                ctx.output(validTag, record);\n            }\n        } catch (Exception e) {\n            ctx.output(errorTag, record);\n        }\n    }\n}\n```\n\n## Follow-up Questions\n- How would you handle non-breaking field removals while maintaining data lake query compatibility?\n- How would you scale drift checks for high-throughput topics (>100K msg/sec)?\n- What's your approach for schema evolution across multiple downstream consumers?","diagram":"flowchart TD\n  A[Kafka topics] --> B[Flink job]\n  B --> C[Parquet in S3]\n  D[Schema Registry] --> E[Drift Detector]\n  E --> F[Audit/Alerts]","difficulty":"intermediate","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":["schema registry","schema drift","schema versioning","backward compatibility","forward compatibility","semantic versioning","circuit breaker pattern","quarantine topic","automated remediation","per-tenant namespaces","real-time validation","chaos engineering"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2026-01-16T04:58:32.347Z","createdAt":"2026-01-14T20:46:37.547Z"},{"id":"q-2085","question":"How would you implement compliant per-tenant data erasure in a streaming analytics pipeline that ingests tenant events from Kafka, writes to a data lake as Parquet, and serves BI queries, ensuring immutable data, auditability, and zero-downtime erasure while preserving peak-load latency? Include data-modeling, catalog updates, and operational steps?","answer":"Implement a tombstone-based erasure strategy using per-tenant partitioning in Kafka topics and immutable Parquet files. Upon receiving a data erasure request, queue a purge job that identifies all blocks containing the tenant's data, rewrites only the affected blocks to redact tenant-specific rows while preserving other data, updates the data catalog metadata to reflect the changes, and schedules block deletion after retention policies allow. This approach maintains auditability through immutable original blocks until safe deletion, ensures zero-downtime operations through asynchronous background processing, and preserves peak-load latency by minimizing rewrite scope and leveraging partition pruning for efficient query performance.","explanation":"## Why This Is Asked\nTests ability to design compliant data erasure in streaming pipelines, covering governance, data modeling, and operational safety while balancing performance requirements.\n\n## Key Concepts\n- Immutable data modeling with per-tenant partitions\n- Tombstone-based erasure and block-level rewriting\n- Data catalog updates and auditable erase events\n- Latency and backpressure considerations in streaming systems\n- Partition pruning for query performance optimization\n\n## Code Example\n```javascript\n// Pseudo purge orchestrator (high level)\nasync function purgeTenant(tenantId) {\n  con\n","diagram":"flowchart TD\n  A[Erasure Request] --> B{Tenant partition exists?}\n  B -- Yes --> C[Enqueue purge job]\n  C --> D[Scan Parquet blocks by tenant]\n  D --> E[Write tombstones/masked rows]\n  E --> F[DROP blocks after retention]\n  F --> G[Audit log entry]\n  G --> H[Catalog update]","difficulty":"intermediate","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Robinhood","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T05:04:58.536Z","createdAt":"2026-01-14T23:36:31.648Z"},{"id":"q-2119","question":"In a frontend data pipeline, you receive two sorted arrays of strings representing tags. Write a function mergeUnique(a,b) that returns a new array with all unique elements in sorted order, without mutating inputs. Assume inputs are sorted. Provide a minimal, robust JS implementation and explain its time/space complexity?","answer":"Use a two-pointer merge algorithm on the sorted inputs. Compare the current elements from both arrays, push the smaller value to the result array and advance its pointer. Skip duplicates by comparing with the last added element. Handle empty arrays gracefully. Time complexity: O(n+m); Space complexity: O(k) where k is the number of unique elements.","explanation":"## Why This Is Asked\nThis question tests practical array manipulation skills while maintaining sorted order, a common frontend data processing task. It evaluates understanding of efficient algorithms and the ability to work with constraints (no mutation, sorted inputs).\n\n## Key Concepts\n- Two-pointer technique for merging sorted arrays\n- Deduplication by comparing with the last added element\n- Time and space complexity analysis\n- Non-mutating operations\n\n## Code Example\n```javascript\nfunction mergeUnique(a, b) {\n  let i = 0, j = 0, result = [];\n  \n  while (i < a.length || j < b.length) {\n    let current;\n    \n    if (j >= b.length || (i < a.length && a[i] <= b[j])) {\n      current = a[i++];\n    } else {\n      current = b[j++];\n    }\n    \n    if (result.length === 0 || result[result.length - 1] !== current) {\n      result.push(current);\n    }\n  }\n  \n  return result;\n}\n```\n\n## Follow-up Questions\n- How would you adapt this if inputs might not be sorted?\n- What if we need to preserve the original order of first occurrence?\n- How would this change for very large datasets that don't fit in memory?\n- Could we optimize further if we know the range of possible tags?","diagram":null,"difficulty":"beginner","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Instacart"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T04:54:23.859Z","createdAt":"2026-01-15T02:27:01.426Z"},{"id":"q-2193","question":"In a real-time analytics pipeline processing user activity events from a mobile app, you must support dynamic event schemas, provide backward compatibility, and enforce per-user data retention, while keeping end-to-end latency under 300 ms at 99th percentile. Describe the architecture, data model, schema evolution strategy, validation in-flight, and how you would monitor and test latency?","answer":"Design a streaming pipeline with Kafka, a Schema Registry, and Avro, enabling backward/forward compatible evolution (optional fields, default values). Validate schemas in-flight (null checks, type gua","explanation":"## Why This Is Asked\n\nExplores dynamic schema handling in streaming pipelines with low latency, data retention, and masking.\n\n## Key Concepts\n\n- Schema evolution (backward/forward compatibility)\n- In-flight validation (type checks, nullability, ranges)\n- Exactly-once processing and idempotent sinks\n- Retention, masking, and data lake design\n\n## Code Example\n\n```javascript\n// Pseudo-configuration sketch: Avro with optional fields, default values\nconst schema = {\n  type: 'record',\n  name: 'UserEvent',\n  fields: [\n    {name: 'userId', type: 'string'},\n    {name: 'event', type: 'string'},\n    {name: 'timestamp', type: 'long'},\n    {name: 'extra', type: ['null','string'], default: null}\n  ]\n};\n```\n\n## Follow-up Questions\n\n- How would you test compatibility across 3 schema versions?\n- What metrics define p99 latency and SLA adherence?\n","diagram":null,"difficulty":"intermediate","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","MongoDB","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T07:01:50.579Z","createdAt":"2026-01-15T07:01:50.579Z"},{"id":"q-2251","question":"In a Kubernetes-based microservices stack processing per-tenant user events, design a per-tenant dynamic tracing sampling policy to keep OTLP ingestion overhead under 5% while preserving 95th percentile latency visibility for the most latency-sensitive tenants. Discuss config storage, crash-safe fallback, and validation under peak load?","answer":"Use per-tenant adaptive sampling in OpenTelemetry. Store tenant-specific rates in a fast config store (Redis) and apply a lightweight in-process sampler using TraceID and a tenant label. Set tighter r","explanation":"## Why This Is Asked\n\nDesigning per-tenant adaptive sampling addresses cost, observability, and latency trade-offs in multi-tenant, data-heavy Kubernetes stacks. It tests practical use of OpenTelemetry, sampling strategies, and operational validation under load.\n\n## Key Concepts\n\n- Per-tenant sampling rates\n- Adaptive / tail-based sampling\n- OpenTelemetry and OTLP exporters\n- Config storage (Redis) and crash-safe fallbacks\n- Validation under peak load and latency budgets\n\n## Code Example\n\n```yaml\ntenants:\n  tenantA:\n    sampling_rate: 0.05\n  tenantB:\n    sampling_rate: 0.2\n```\n\n## Follow-up Questions\n\n- How would you handle tenants with rapidly changing latency budgets?\n- What metrics confirm the approach doesn’t undercut critical tracing visibility?","diagram":"flowchart TD\n  A[Request] --> B{Tenant}\n  B --> C[Sampling decision]\n  C --> D[Export trace via OTLP]\n  C --> E[Drop trace]","difficulty":"intermediate","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","Plaid","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T09:37:02.043Z","createdAt":"2026-01-15T09:37:02.043Z"},{"id":"q-2368","question":"You’re building a real-time fraud-detection scoring service for a payments platform that must process 100k transactions per second with sub-20ms latency and strict privacy constraints. Describe an architecture to ingest events, compute per-transaction risk scores, handle backpressure, ensure idempotent processing, and observe the system. Include data model, latency budget, and fault-tolerance trade-offs?","answer":"Ingest 100k tps of transactions via Kafka; compute per-transaction risk with a streaming processor (Kafka Streams or Flink) and keep latency under 20ms for the 99th percentile. Persist idempotent resu","explanation":"## Why This Is Asked\n\nAssesses real-world streaming design, latency tuning, and fault tolerance for high-stakes payments workloads.\n\n## Key Concepts\n\n- Streaming ingestion and backpressure\n- Exactly-once processing and idempotence\n- Windowed risk scoring and latency budgets\n- Observability and tracing\n\n## Code Example\n\n```javascript\n// Pseudocode: idempotent upsert in Redis\nfunction upsertRisk(userId, score, ts) {\n  const key = `risk:${userId}`;\n  const script = `\n  local oldTs = redis.call('HGET', KEYS[1], 'ts')\n  if not oldTs or tonumber(ARGV[2]) > tonumber(oldTs) then\n    redis.call('HSET', KEYS[1], 'score', ARGV[1])\n    redis.call('HSET', KEYS[1], 'ts', ARGV[2])\n  end\n  `;\n  redis.eval(script, 1, key, score, ts);\n}\n```\n\n## Follow-up Questions\n\n- How would you test latency distribution under traffic spikes?\n- How would you validate correctness with replayed events?","diagram":"flowchart TD\n  A[Vendor update] --> B[Kafka]\n  B --> C[Stream processor]\n  C --> D[Store]\n  D --> E[Dashboard]","difficulty":"advanced","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","OpenAI","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T15:35:09.482Z","createdAt":"2026-01-15T15:35:09.484Z"},{"id":"q-2408","question":"In a 3-namespace Kubernetes cluster (dev, stage, prod), design a least-privilege model: per-namespace Role/RoleBinding (no ClusterRole bindings), enforce Pod Security Standards:restricted via OPA Gatekeeper, and apply default-deny NetworkPolicies with explicit allows. Explain testing with kubectl can-i and provide minimal YAML samples for Role, RoleBinding, NetworkPolicy, and a Gatekeeper constraint template?","answer":"Per-namespace RBAC with Role/RoleBinding in dev, stage, prod; no cluster-wide roles. Enforce PS Standards 'restricted' via Gatekeeper. Use a default-deny NetworkPolicy per namespace and explicit allow","explanation":"## Why This Is Asked\n Assesses ability to design isolation via RBAC, admission controls, and network policies in real clusters.\n\n## Key Concepts\n - RBAC scoping\n - Pod Security Standards / Gatekeeper\n - NetworkPolicy default-deny\n - Cross-namespace isolation\n - Testing and auditability\n\n## Code Example\n```yaml\n# Role example\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  namespace: dev\n  name: pod-creator\nrules:\n- apiGroups: [\"\"]\n  resources: [\"pods\"]\n  verbs: [\"create\",\"get\",\"list\",\"watch\"]\n```\n\n```yaml\n# NetworkPolicy example (default-deny)\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: default-deny\n  namespace: dev\nspec:\n  podSelector: {}\n  policyTypes: [\"Ingress\",\"Egress\"]\n  ingress: []\n  egress: []\n```\n\n```yaml\n# Gatekeeper constraint template placeholder (illustrative)\napiVersion: constraints.gatekeeper.sh/v1beta1\nkind: K8sPSS\nmetadata:\n  name: ps-restricted\nspec:\n  match:\n    kinds:\n    - apiGroups: [\"\"]\n      kinds: [\"Pod\"]\n  parameters:\n    restricted: true\n```\n\n## Follow-up Questions\n - How would you test breach scenarios and failure modes?\n - How would you extend this for multi-tenant data separation and monitoring?","diagram":null,"difficulty":"advanced","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","LinkedIn"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T16:57:55.409Z","createdAt":"2026-01-15T16:57:55.409Z"},{"id":"q-2462","question":"Scenario: You’re maintaining a Kubernetes cluster used by Plaid and Oracle developers. Provide a minimal RBAC setup to grant a ServiceAccount named 'dev-read' in namespace 'dev' read-only access to pods in that namespace. Include the ServiceAccount, a Role with get, list, watch on pods, and a RoleBinding. Explain how you would test it and why this is least privilege?","answer":"Create ServiceAccount dev-read in namespace dev. Define Role pod-reader with get, list, watch on pods in dev. Create RoleBinding dev-read-binding to attach dev-read to pod-reader. Test: kubectl --as=s","explanation":"## Why This Is Asked\n\nTests practical RBAC knowledge: namespace-scoped Roles, RoleBinding usage, and least-privilege design. It checks ability to translate a policy into concrete Kubernetes objects and validate permissions.\n\n## Key Concepts\n\n- RBAC basics: Role, RoleBinding, service accounts\n- Namespace scoping vs ClusterRoles\n- least privilege: read-only access for visibility, no write rights\n- testing permissions with kubectl can-i\n\n## Code Example\n\n```yaml\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: dev-read\n  namespace: dev\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: pod-reader\n  namespace: dev\nrules:\n- apiGroups: [\"\"]\n  resources: [\"pods\"]\n  verbs: [\"get\", \"list\", \"watch\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: dev-read-binding\n  namespace: dev\nsubjects:\n- kind: ServiceAccount\n  name: dev-read\n  namespace: dev\nroleRef:\n  kind: Role\n  name: pod-reader\n  apiGroup: rbac.authorization.k8s.io\n```\n\n## Follow-up Questions\n\n- How would you extend this to cover multiple namespaces while keeping separation?\n- How would you audit and rotate these permissions across clusters?","diagram":null,"difficulty":"beginner","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Oracle","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T19:03:50.685Z","createdAt":"2026-01-15T19:03:50.685Z"},{"id":"q-2518","question":"Design a zero-trust, multi-region deployment for a real-time chat service on Kubernetes using Istio. The system must support mTLS, service-to-service authentication, PII audit logging, and RBAC for on-call engineers. Describe namespace layout, policy enforcement, DR strategy, and how you’d validate SLOs during a regional outage?","answer":"Design a zero-trust, multi-region chat service on Kubernetes with Istio. Enforce STRICT mTLS and per-service AuthorizationPolicies; isolate PII access via RBAC; centralize redacted audit logs; use geo","explanation":"## Why This Is Asked\n\nThis question probes practical depth in scalable Kubernetes security, Istio policy design, RBAC, audit/compliance for PII, DR strategy, and testing.\n\n## Key Concepts\n\n- Zero trust\n- Istio mTLS and authorization\n- RBAC and namespaces\n- Audit logging and PII\n- DR and multi-region\n- SLO validation and chaos testing\n\n## Code Example\n\n```yaml\napiVersion: security.istio.io/v1beta1\nkind: PeerAuthentication\nmetadata:\n  name: default\n  namespace: chat\nspec:\n  mtls:\n    mode: STRICT\n```\n\n## Follow-up Questions\n\n- How would you test DR failover under latency constraints?\n- How would you ensure audit-log integrity and retention?","diagram":"flowchart TD\n  A[User Request] --> B[Ingress Gateway]\n  B --> C[Auth Service]\n  C --> D[Service Mesh (Envoy)]\n  D --> E[Chat Service]\n  E --> F[Geo-Replicated DB]","difficulty":"advanced","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Square","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T21:29:52.536Z","createdAt":"2026-01-15T21:29:52.537Z"},{"id":"q-2612","question":"In a Kubernetes cluster, a microservice named 'orders' communicates with a MongoDB instance. Describe a beginner-friendly RBAC and Secret strategy to grant only this pod access to MongoDB credentials stored in a Kubernetes Secret. Include minimal YAML references (namespace, ServiceAccount, Role/RoleBinding, Secret mount), explain rotation, and how you would verify a pod can connect without leaking credentials?","answer":"Create a dedicated namespace, a ServiceAccount for the orders service, and a RoleBinding that grants Get/List access on a specific Secret named db-credentials. Mount the Secret as either an environment variable (MONGO_URI) or as a file, ensuring only the orders pod can access the MongoDB credentials while maintaining least privilege.","explanation":"## Why This Is Asked\nThis question tests fundamental Kubernetes RBAC, Secrets management, and credential rotation in a realistic MongoDB microservice scenario.\n\n## Key Concepts\n- Kubernetes RBAC (Role-Based Access Control)\n- Secrets management and secure mounting\n- ServiceAccounts, Roles, and RoleBindings\n- Secret rotation strategies\n- Verification without credential exposure\n\n## Code Example\n```yaml\n# Namespace and ServiceAccount\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: shop\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: orders-sa\n  namespace: shop\n---\n# Secret (illustrative)\napiVersion: v1\nkind: Secret\nmetadata:\n  name: db-credentials\n  namespace: shop\ntype: Opaque\ndata:\n  # Base64 encoded MongoDB connection string\n  MONGO_URI: bW9uZ29kYjovL3VzZXI6cGFzc0Btb25nbzoyNzAxNw==\n---\n# Role for Secret access\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: secret-reader\n  namespace: shop\nrules:\n- apiGroups: [\"\"]\n  resources: [\"secrets\"]\n  resourceNames: [\"db-credentials\"]\n  verbs: [\"get\", \"list\"]\n---\n# RoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: orders-secret-access\n  namespace: shop\nsubjects:\n- kind: ServiceAccount\n  name: orders-sa\n  namespace: shop\nroleRef:\n  kind: Role\n  name: secret-reader\n  apiGroup: rbac.authorization.k8s.io\n---\n# Pod configuration\napiVersion: v1\nkind: Pod\nmetadata:\n  name: orders\n  namespace: shop\nspec:\n  serviceAccountName: orders-sa\n  containers:\n  - name: orders\n    image: orders:latest\n    env:\n    - name: MONGO_URI\n      valueFrom:\n        secretKeyRef:\n          name: db-credentials\n          key: MONGO_URI\n```\n\n## Rotation Strategy\n1. Create new Secret with updated credentials\n2. Update Secret and restart pod\n3. Use automated tools like External Secrets Operator or HashiCorp Vault\n4. Implement rolling updates for zero-downtime rotation\n\n## Verification Approach\n1. Check RBAC permissions: `kubectl auth can-i get secret/db-credentials --as=system:serviceaccount:shop:orders-sa -n shop`\n2. Verify secret mounting: `kubectl exec orders -n shop -- env | grep MONGO_URI`\n3. Test connection: `kubectl exec orders -n shop -- mongo --eval 'db.adminCommand(\"ping\")'`\n4. Audit logs: `kubectl get events -n shop --field-selector involvedObject.name=orders`","diagram":null,"difficulty":"beginner","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","MongoDB","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T05:01:20.132Z","createdAt":"2026-01-16T02:43:14.252Z"},{"id":"q-2674","question":"In a **KCSA** beginner scenario, a namespace hosts a small web app on **Kubernetes**. Under load, pods restart and 5xx errors appear. Describe a concrete, step‑by‑step debugging plan using real commands (kubectl, logs, metrics) and show how you would fix resource limits, readiness/liveness probes, and a minimal **NetworkPolicy** to improve security?","answer":"Diagnose with concrete kubectl steps under load: show pod restarts and status, fetch recent logs, and compare current CPU/memory to requests. Commands: `kubectl get pods -n ns`, `kubectl describe pod ","explanation":"## Why This Is Asked\nThis question tests practical debugging in a real Kubernetes workflow and security-minded thinking under pressure.\n\n## Key Concepts\n- kubectl core commands\n- resource requests/limits and HPA\n- readiness/liveness probes\n- basic NetworkPolicy\n- debugging under load\n\n## Code Example\n```javascript\n// no code required for answer\n```\n\n## Follow-up Questions\n- How would you reproduce the issue in a staging cluster?\n- What metrics would you monitor long-term to prevent recurrence?\n","diagram":"flowchart TD\nA[Pod issue] -->|Inspect status| B[Kubectl describe pod]\nB -->|Show restarts| C[Logs]\nC -->|Fetch events| D[Events]\nD -->|Check CPU/Memory| E[Metrics]\nE -->|Check spec| F[Resources]\nF -->|Adjust limits| G[Probes]\nG -->|Add HPA if needed| H[Autoscale]\nH -->|Secure with policy| I[NetworkPolicy]","difficulty":"beginner","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","LinkedIn","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T06:47:15.269Z","createdAt":"2026-01-16T06:47:15.269Z"},{"id":"q-2766","question":"In a kcsa-beginner namespace, a small web app reads its Postgres credentials from a Kubernetes Secret mounted as environment variables. After a secret rotation, pods crash with authentication errors. Describe a concrete, step-by-step plan to diagnose and fix, including kubectl commands to inspect secrets, decode values, confirm they are mounted, and how to roll the deployment without downtime; also show minimal secret rotation best practices?","answer":"Begin by verifying the secret exists and the value is correct, then confirm the deployment maps the exact secret name to the env var. Decode the secret with base64 and compare to what the app expects,","explanation":"## Why This Is Asked\nTests practical Kubernetes secret management, mapping to apps, and safe rotation with minimal downtime.\n\n## Key Concepts\n- Kubernetes Secrets and env/envFrom mounting\n- Deployment rolling updates and downtime control\n- Secret rotation processes and auditing\n- Validating external DB connectivity in-cluster\n- Basic security considerations for sensitive data\n\n## Code Example\n```bash\nkubectl get secret kcsa-postgres-secret -n kcsa-beginner -o jsonpath='{.data.PASSWORD}' | base64 -d\n```\n```\n\n## Follow-up Questions\n- How would you automate secret rotation to avoid manual steps?\n- What security considerations arise when mounting secrets as environment variables?\n","diagram":"flowchart TD\n  A[Verify secret exists] --> B[Check env mapping in Deployment]\n  B --> C[Decode and compare secret value]\n  C --> D[Rollout restart deployment]\n  D --> E[DB connectivity test in transient pod]","difficulty":"beginner","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T10:55:56.082Z","createdAt":"2026-01-16T10:55:56.082Z"},{"id":"q-2897","question":"In a kcsa-prod namespace hosting a microservices app with Istio mTLS, a CA rotation disrupted service-to-service trust, causing cascading 5xx errors. Provide a concrete, step-by-step diagnostic plan with exact kubectl/istioctl commands to verify cert validity, trust anchors, and sidecar config; outline a zero-downtime rollout with canaries and a rollback path; include future rotation guidance?","answer":"Begin by confirming CA rotation completed and trust anchors distributed: kubectl -n istio-system get secret cacerts -o jsonpath='{.data[\"certs.pem\"]}' | base64 -d; kubectl get pods -n kcsa-prod -l app","explanation":"## Why This Is Asked\nTests ability to diagnose service-mesh TLS issues during CA rotations, mirroring real outages.\n\n## Key Concepts\n- Service mesh mTLS trust propagation\n- CA rotation and secret distribution\n- Canary rollouts and safe rollbacks\n- Observability: proxy logs, TLS checks, metrics\n- Rollout automation for rotation\n\n## Code Example\n```javascript\n// example: check TLS status via istioctl\n// not actual code; illustrates commands used in workflow\n```\n\n## Follow-up Questions\n- How would you automate rotation checks and rollbacks in GitOps?\n- How do you test mTLS reachability in canary vs baseline before full rollout?","diagram":null,"difficulty":"advanced","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Microsoft","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T16:49:42.440Z","createdAt":"2026-01-16T16:49:42.440Z"},{"id":"q-3074","question":"In a kcsa-prod namespace hosting a GPU-accelerated ML inference service on Kubernetes with Nvidia device plugins and Istio mTLS, a recent config change triggers sporadic 5xx errors during model warmup under peak load. Provide a concrete, step-by-step diagnostic plan with exact kubectl/istioctl commands to verify GPU provisioning (nvidia-smi on nodes), device-plugin DaemonSet health, CSI devices, and Istio sidecar behavior; outline a zero-downtime canary rollout and rollback path; include validation that P95 latency stays under 60ms during rollout?","answer":"Begin with GPU health and device plugin diagnostics, then trace traffic flow through Istio to isolate GPU versus sidecar issues. Commands: kubectl get nodes -o wide; kubectl get ds nvidia-device-plugin-daemonset -n kube-system; kubectl get pods -n kcsa-prod -l app=ml-inference; kubectl exec -it <pod-name> -- nvidia-smi; istioctl proxy-status; kubectl logs -n kcsa-prod -l app=ml-inference --tail=100; kubectl top nodes; kubectl top pods -n kcsa-prod; istioctl proxy-config routes <pod-name>.kcsa-prod; kubectl get virtualservice -n kcsa-prod; kubectl get destinationrule -n kcsa-prod","explanation":"## Why This Is Asked\n\nEvaluates depth in diagnosing GPU-accelerated workloads, device plugin health, and Istio-driven traffic behavior under real-world pressure.\n\n## Key Concepts\n\n- Nvidia device plugin health and DaemonSet status\n- GPU provisioning and CSI device wiring\n- Istio mTLS and sidecar visibility\n- Canary rollouts and safe rollback strategies\n- Observability: latency budgets and failure signals\n\n## Code Example\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ml-inference-canary\nspec:\n  replicas: 1\n  template:\n    spec:\n      containers:\n      - name: ml-inference\n        image: ml-inference:v1.2.0\n        resources:\n          requests:\n            nvidia.com/gpu: 1\n          limits:\n            nvidia.com/gpu: 1\n```","diagram":"flowchart TD\n  A[Client] --> B[Ingress]\n  B --> C[Gateway/VirtualService]\n  C --> D[ML Inference Pod (GPU)]\n  D --> E[Metrics & Traces]\n  E --> F[Rollback Path]","difficulty":"intermediate","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T05:13:10.299Z","createdAt":"2026-01-16T23:43:35.562Z"},{"id":"q-3089","question":"In kcsa-prod namespace, a payments API behind an Istio service mesh with mTLS shows 401s for a subset of tenants after a policy update that changes JWT claims. Provide a concrete diagnostic plan with exact kubectl/istioctl commands to verify the JWT filter config, inspect Envoy sidecars, check JWKS rotation, and per-tenant routing; propose a zero-downtime canary rollout path and rollback, and how to validate P95 latency stays under 120 ms during rollout?","answer":"Identify affected tenants from gateway logs, then verify the JWT flow and JWKS rotation:\n- kubectl -n kcsa-prod logs deploy/payments-gw -c istio-proxy | grep Tenant\n- kubectl get requestauthentication jwt-auth -n kcsa-prod -o yaml\n- istioctl proxy-config routes deploy/payments-api -n kcsa-prod -o json | jq '.virtualHosts[].routes[] | select(.match.headers[\"authorization\"] != null)'\n- kubectl get jwks payments-jwks -n kcsa-prod -o yaml && kubectl describe secret payments-jwks -n kcsa-prod\n- istioctl proxy-config clusters deploy/payments-api -n kcsa-prod | grep jwt\n\nFor canary rollout:\n- Create weighted VirtualService: 10% traffic to canary, 90% to stable\n- Monitor: istioctl proxy-config metrics deploy/payments-api --type filter | grep jwt && kubectl top pods -n kcsa-prod -l app=payments\n- Validate latency: kubectl exec -n kcsa-prod deploy/payments-api -- curl -w '%{time_total}' localhost/metrics | awk '{if ($1 > 0.12) print \"SLA breach\"}'\n- Rollback if needed: kubectl patch virtualservice payments-vs -n kcsa-prod --type='json' -p='[{\"op\": \"replace\", \"path\": \"/spec/http/0/route/0/weight\", \"value\": 100}]'","explanation":"## Why This Is Asked\n\nAssesses practical skills in diagnosing JWT-based authentication issues within a real Istio+mTLS setup, including CRD inspection, secret rotation checks, and traffic engineering via canaries. Emphasizes observability, per-tenant routing, and safe rollback procedures.\n\n## Key Concepts\n\n- JWT validation with RequestAuthentication/AuthorizationPolicy CRDs\n- JWKS rotation and secret management in Kubernetes\n- Envoy proxy configuration inspection and debugging\n- Canary rollouts with weighted traffic splitting\n- Rollback strategies and SLO validation\n\n## Code Example\n\n```yaml\n#","diagram":"flowchart TD\n  A[JWT Policy Update] --> B[JWT Validation at Envoy]\n  B --> C{Tenant Affected?}\n  C -->|Yes| D[Investigate Logs]\n  C -->|No| E[Proceed to Canary]\n  D --> F[JWKS Check]\n  F --> G[Canary Rollout]","difficulty":"intermediate","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Airbnb","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T05:06:41.852Z","createdAt":"2026-01-17T02:14:14.967Z"},{"id":"q-3161","question":"In kcsa-prod, a multi-tenant cluster uses OPA Gatekeeper to enforce per-tenant API access. A policy update yields intermittent 403s when tenants read ConfigMaps in their namespaces. Outline a concrete diagnostic plan with exact kubectl/opa commands to validate ConstraintTemplates, inspect violations, and verify Gatekeeper behavior; propose a zero-downtime rollback path and tests to confirm access for all tenants remains intact during rollback?","answer":"Run: kubectl get violations -A; kubectl describe <constraint> -n <tenant-ns> to see mismatch; kubectl get constrainttemplates -A; kubectl logs -n gatekeeper-system -l app=gatekeeper -c manager <pod>; ","explanation":"## Why This Is Asked\nThis tests practical debugging of policy-driven security in a multi-tenant cluster and the ability to reconcile Gatekeeper violations with policy templates.\n\n## Key Concepts\n- OPA Gatekeeper, ConstraintTemplates, Constraints, Violations, Audit logs\n- Canary rollout, safe rollback, per-tenant testing\n- kubectl, gatekeeper logs, and testing data access\n\n## Code Example\n```javascript\nkubectl get violations -A\nkubectl describe constraint <name> -n <tenant-ns>\nkubectl logs -n gatekeeper-system -l app=gatekeeper -c manager <pod>\n```\n\n## Follow-up Questions\n- How would you isolate the issue to a single tenant?\n- How would you design a canary rollout that minimizes blast radius?","diagram":"flowchart TD\n  A[Policy Update] --> B[Gatekeeper Violations]\n  B --> C[Audit Logs]\n  C --> D[Rollback Plan]\n  D --> E[Validation]\n","difficulty":"intermediate","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Google","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T04:55:50.044Z","createdAt":"2026-01-17T04:55:50.044Z"},{"id":"q-3187","question":"In kcsa-prod namespace, a multi-tenant streaming API behind Istio mTLS intermittently returns 5xx during a Chaos Mesh CPU-throttle experiment targeting the sink service. Provide a concrete diagnostic plan with exact kubectl/istioctl/chaosctl commands to verify the experiment, inspect per-tenant routing, check Envoy stats, CPU quotas, and rollback safely while ensuring a P95 latency under 120 ms during the rollout?","answer":"Start by listing active Chaos Mesh experiments in kcsa-prod: kubectl get chaos -n kcsa-prod. Inspect CPU throttling on affected pods: kubectl top pod -n kcsa-prod; node; and cat /sys/fs/cgroup/cpu/...","explanation":"## Why This Is Asked\nThis asks practical debugging of a Chaos Mesh + Istio issue with multi-tenant routing.\n\n## Key Concepts\n- Chaos Mesh experiment lifecycle\n- CPU throttling and cgroups\n- Envoy stats and Istio TrafficSplit\n- Safe rollback and latency validation\n\n## Code Example\n```bash\nkubectl get chaos -n kcsa-prod\nkubectl top pod -n kcsa-prod\n```\n\n## Follow-up Questions\n- How would you automate rollbacks and postmortems?\n- How to isolate tenants with per-tenant routing guards?\n","diagram":null,"difficulty":"intermediate","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Microsoft","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T06:40:33.804Z","createdAt":"2026-01-17T06:40:33.806Z"},{"id":"q-3327","question":"In kcsa-general, a multi-tenant web app runs in per-tenant namespaces. A recent RBAC policy update restricted read access to a shared ConfigMap containing per-tenant feature flags. Some namespaces report UI features not toggled (403 errors when reading the ConfigMap). Provide a concrete diagnostic plan with exact kubectl commands to verify RBAC across affected namespaces, inspect RoleBindings/ClusterRoleBindings, test access with kubectl auth can-i, and outline a safe rollback strategy that restores per-tenant config access with minimal downtime?","answer":"Identify affected namespaces and the SA used by the web pods; inspect RoleBindings/ClusterRoleBindings for the shared ConfigMap; verify access with kubectl auth can-i get configmap feature-flags -n <n","explanation":"## Why This Is Asked\n\nRBAC misconfig in multi-tenant Kubernetes is common and tricky; tests knowledge of RBAC, serviceaccounts, and can-i.\n\n## Key Concepts\n\n- RBAC: Roles, RoleBindings, and ClusterRoles\n- ServiceAccounts and per-namespace scope\n- Least privilege and safe rollback\n\n- kubectl can-i for quick checks\n\n## Code Example\n\n```javascript\n// RBAC manifest example (conceptual)\n```\n\n```javascript\n// RoleBinding manifest example (conceptual)\n```\n\n## Follow-up Questions\n\n- How would you verify can-i across multiple tenants?\n- How would you design a safe rollback with minimal downtime?","diagram":"flowchart TD\n  A[Affected Namespace] --> B[RBAC baseline checked]\n  B --> C{Access denied?}\n  C -->|Yes| D[Fix RBAC and re-test]\n  C -->|No| E[Proceed with validation]","difficulty":"beginner","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","PayPal","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T11:35:27.546Z","createdAt":"2026-01-17T11:35:27.546Z"},{"id":"q-3567","question":"In a kcsa namespace, a small web service behind an API Gateway intermittently returns 503s after a ConfigMap-driven feature flag update. Provide a concrete diagnostic plan with exact kubectl/describe/logs commands to verify the flag is loaded by pods, inspect in-pod config reload behavior, confirm rollout status, check readiness probes, and outline a safe, zero-downtime rollback path with latency validation that P95 stays under 200 ms?","answer":"Run: kubectl get cm app-flags; kubectl describe cm app-flags; kubectl rollout status deployment/web; kubectl get pods -l app=web -o wide; kubectl logs deploy/web | grep -i flag; kubectl exec deploy/web -- cat /etc/config/flags.json; kubectl top pods -l app=web; kubectl get events --field-selector involvedObject.name=web","explanation":"## Why This Is Asked\nTests ability to diagnose ConfigMap-driven feature flags, config propagation, and safe rollout without downtime.\n\n## Key Concepts\n- Kubernetes ConfigMaps and in-pod reload\n- Rolling updates and canary strategies\n- Readiness and latency metrics\n\n## Code Example\n```javascript\n// Simple feature flag check\nconst enabled = process.env.FEATURE_FLAG === 'true';\nif (enabled) startNewPath();\n```\n\n## Follow-up Questions\n- How would you validate multi-tenant isolation during rollouts?\n- What metrics would you surface to detect regressions during rollout?","diagram":null,"difficulty":"beginner","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Lyft","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T05:44:04.806Z","createdAt":"2026-01-17T21:35:20.011Z"},{"id":"q-3655","question":"In kcsa-prod, a multi-tenant REST API fronted by Istio mTLS experiences sporadic 403s and delayed responses for a subset of tenants after a policy change enabling tenant-scoped routing to a Redis-backed cache layer. Provide a concrete diagnostic plan with exact kubectl/istioctl commands to verify per-tenant header propagation, VirtualService/DestinationRule routing, and Redis ACLs; propose a safe 5% canary rollout and rollback path; how to confirm P95 latency stays under 120 ms during rollout?","answer":"Plan: verify per-tenant header propagation, review VirtualService and DestinationRule routing, confirm Redis ACLs, and stage a 5% canary. Commands: kubectl -n kcsa-prod get virtualservice; istioctl pc","explanation":"## Why This Is Asked\nTests ability to diagnose multi-tenant routing issues with Istio, header propagation, and fast rollback.\n\n## Key Concepts\n- Istio routing: VirtualService, DestinationRule, mTLS\n- Per-tenant headers, header-based routing\n- Redis ACLs and per-tenant keys\n- Canary rollouts and latency budgets\n\n## Code Example\n```javascript\n// Pseudo: per-tenant routing rule sketch (illustrative)\nconst vs = {\n  http: [{\n    match: [{ headers: { 'x-tenant-id': { exact: 'tenantA' } } }],\n    route: [{ destination: { host: 'api-tenantA' } }]\n  }]\n};\n```\n\n## Follow-up Questions\n- How would you automate rollback with metrics thresholds?\n- How would you extend to support dynamic tenant onboarding without restarts?","diagram":"flowchart TD\n  A[Tenant request] --> B[IngressGateway]\n  B --> C[VirtualService per-tenant routing]\n  C --> D[Redis-backed cache]\n  D --> E[Backend service]","difficulty":"intermediate","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T04:11:15.944Z","createdAt":"2026-01-18T04:11:15.944Z"},{"id":"q-3900","question":"In kcsa-prod, a multi-tenant real-time analytics API using gRPC over Istio mTLS experiences sporadic 502s on streaming calls after a per-tenant header normalization EnvoyFilter was introduced. Provide a concrete diagnostic plan with exact kubectl/istioctl commands to inspect EnvoyFilter rules, per-tenant route/listener config, and stream state; propose a zero-downtime disable/rollback path and describe how to validate P95 latency stays below 150 ms during rollout?","answer":"POD=$(kubectl get pod -l app=analytics -n kcsa-prod -o jsonpath='{.items[0].metadata.name}'); kubectl get envoyfilters -n kcsa-prod -o yaml; istioctl proxy-config routes -n kcsa-prod $POD -o json; ist","explanation":"## Why This Is Asked\nTests knowledge of EnvoyFilter impact on streaming gRPC in a multi-tenant mesh and requires concrete tooling steps.\n\n## Key Concepts\n- EnvoyFilter scope and per-tenant routing impact\n- gRPC streaming behavior under Istio mTLS\n- Canary rollout and rollback in production\n\n## Code Example\n```javascript\n// sample helper to fetch a pod name for scripting\nconst pod = require('child_process').execSync(\"kubectl get pod -l app=analytics -n kcsa-prod -o jsonpath='{.items[0].metadata.name}'\").toString().trim()\n```\n\n## Follow-up Questions\n- How would you validate no tenant data leakage during rollback?\n- Which metrics confirm P95 latency stays under target during rollout?","diagram":"flowchart TD\n  A[EnvoyFilter applied] --> B{Tenant traffic}\n  B --> C[Healthy]\n  B --> D[502s on streams]\n  D --> E[Diagnosis steps]\n","difficulty":"advanced","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T14:31:37.279Z","createdAt":"2026-01-18T14:31:37.279Z"},{"id":"q-3919","question":"In kcsa-prod, a multi-tenant data-ingestion API uses a Redis-backed rate limiter behind Istio mTLS and a per-tenant quota policy. After a policy update, a subset of tenants observe 429s during bursts. Provide a concrete diagnostic plan with exact kubectl/istioctl commands to verify quota rules, inspect Envoy stats, check rate-limit service health, and per-tenant counters; propose a zero-downtime canary rollout path and rollback, and how to validate P95 latency stays under 200 ms during rollout?","answer":"Diagnose per-tenant rate limiting in kcsa-prod behind Istio. Verify quota rules with istioctl and Envoy stats, inspect the Redis-backed token counters per tenant, and check the rate-limit service conf","explanation":"## Why This Is Asked\nThis question probes ability to reason about multi-tenant rate limiting, SLOs, and safe rollout strategies under Istio, with concrete tooling paths.\n\n## Key Concepts\n- Istio quotas and rate limiting\n- Envoy stats and metrics\n- Redis-backed token buckets\n- Canary rollout strategies\n- Observability and latency targets\n\n## Code Example\n```bash\n# Debug commands\nkubectl get svc rate-limit -n kcsa-prod\nistioctl pc rsh <envoy-pod> -n kcsa-prod curl localhost:42422/stats\n```\n\n## Follow-up Questions\n- How would you rollback if latency worsens after the rollout?\n- How do you isolate offending tenants without impacting others?","diagram":"flowchart TD\n  A[Issue detected] --> B[Inspect quota rules]\n  B --> C{Per-tenant counters OK?}\n  C -->|Yes| D[Analyze latency impact]\n  C -->|No| E[Update quota service config]\n  D --> F[Canary rollout]\n  F --> G[Monitor latency targets]","difficulty":"advanced","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Netflix","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T15:35:25.271Z","createdAt":"2026-01-18T15:35:25.271Z"},{"id":"q-4016","question":"In kcsa-prod, a multi-tenant API gateway behind Istio mTLS shows intermittent TLS handshake failures for a subset of tenants after a policy update that changed gateway TLS origination mode from SIMPLE to MUTUAL. Provide a concrete diagnostic plan with exact kubectl/istioctl commands to verify Gateway, VirtualService, and DestinationRule TLS configs, inspect per-tenant SNI routing and Envoy listeners, check certificate secrets rotation, and propose a zero-downtime canary rollout path and rollback; show how to validate P95 latency stays under 180 ms during rollout?","answer":"Identify TLS origination drift and per-tenant SNI misrouting causing handshake failures after the TLS policy change. Verify TLS modes in Gateway/VirtualService/DestinationRule, confirm per-tenant SNI ","explanation":"## Why This Is Asked\n\nTests ability to diagnose Istio TLS origination issues in a multi-tenant environment, including per-tenant SNI routing, secret rotation, and safe rollout strategies.\n\n## Key Concepts\n\n- Istio TLS origination modes (SIMPLE, MUTUAL)\n- Gateway, VirtualService, DestinationRule TLS config\n- Per-tenant SNI routing and Envoy listener/state\n- Kubernetes Secrets rotation and certificate management\n- Canary rollout with gateway subset labels and safe rollback\n\n## Code Example\n\n```yaml\n# illustrative TLS origination config\napiVersion: networking.istio.io/v1beta1\nkind: Gateway\nmetadata:\n  name: kcsa-gw\nspec:\n  selector:\n    istio: ingressgateway\n  servers:\n  - port:\n      number: 443\n      name: https\n      protocol: TLS\n    hosts: [\"tenant-a.example.com\"]\n    tls:\n      mode: MUTUAL\n      credentialName: tenant-a-cred\n```\n\n## Follow-up Questions\n\n- How would you automate per-tenant rollouts and rollbacks safely?\n- What metrics and logs confirm TLS handshakes and latency targets during rollout?","diagram":null,"difficulty":"intermediate","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","IBM"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T19:34:06.392Z","createdAt":"2026-01-18T19:34:06.392Z"},{"id":"q-4056","question":"In kcsa-prod, a multi-tenant telemetry backend using an OpenTelemetry Collector and Istio mTLS shows tail latency spikes and intermittent 5xx in peak ingest as the per-tenant sampling policy changes. Provide a concrete diagnostic plan with exact kubectl/istioctl commands to inspect the otel-collector config (per-tenant sampling, exporters), pod resource usage, and per-tenant traces; propose a zero-downtime canary rollout to adjust sampling and verify P95 latency stays under 150 ms during rollout?","answer":"Limit investigation to otel-collector backpressure from per-tenant sampling saturation. Check the collector configuration, per-tenant sampling rates, exporter queues, and pod resource usage. Commands: kubectl get configmap otel-collector-config -n observability -o yaml, kubectl top pods -l app=otel-collector -n observability, istioctl proxy-config routes deployment/otel-collector -n observability, kubectl logs -l app=otel-collector -n observability --tail=1000 | grep -E '(backpressure|queue|sampling)'","explanation":"## Why This Is Asked\nThis probes real-world observability issues in a high-throughput mesh, focusing on per-tenant sampling and collector backpressure.\n\n## Key Concepts\n- OpenTelemetry collector configuration and per-tenant sampling\n- Kubernetes ConfigMaps and resource metrics\n- Canary rollouts and latency validation\n- Istio service mesh observability\n\n## Code Example\n```javascript\nfunction canaryRollout(current, target) {\n  // Simple ramp\n  return Math.min(target, current + 10);\n}\n```\n\n## Follow-up Questions\n- How would you automatically detect sampling-induced backpressure?\n- What other exporters or backpressure indicators should be monitored?","diagram":null,"difficulty":"advanced","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","Netflix","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T05:58:03.292Z","createdAt":"2026-01-18T21:39:57.399Z"},{"id":"q-4107","question":"kcsa-prod in a multi-region Istio mesh hosts a payments API behind an Istio mTLS ingress. A new WASM-based field-redaction filter is applied per-tenant, but intermittent leakage occurs under cache warmup. Provide a concrete, practical diagnostic plan with exact kubectl/istioctl commands to verify the WASM module version loaded by sidecars, inspect per-tenant redaction rules in ConfigMaps, check Envoy config_dump and wasm stats, and design a zero-downtime rollback path with validation that P95 latency stays under 180 ms?","answer":"Start by validating the WASM module version and per-tenant redaction rules loaded into each payments sidecar, then inspect config_dump, envoy logs, and wasm_stats to confirm the filter is active and properly configured across all regions.","explanation":"## Why This Is Asked\nTests the ability to diagnose WASM-based filters in Istio, tenant-scoped configuration, and safe rollouts in production-like traffic scenarios.\n\n## Key Concepts\n- Istio mTLS and WASM filters\n- config_dump and envoy WASM state inspection\n- Canary rollouts via VirtualService weights\n- Per-tenant configuration management and rollback strategies\n\n## Code Example\n```javascript\n// Placeholder: no code required for this answer\n```\n\n## Follow-up Questions\n- How would you extend this approach to 1% traffic and monitor memory usage for the WASM module?\n- What changes to observability would you implement to detect cache warmup issues earlier?","diagram":null,"difficulty":"advanced","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T05:30:50.602Z","createdAt":"2026-01-19T02:39:56.653Z"},{"id":"q-4150","question":"In kcsa-prod, a multi-tenant data ingestion API behind Istio mTLS shows intermittent 5xx under peak load after introducing a per-tenant rate-limiting EnvoyFilter backed by Redis. Provide a concrete diagnostic plan with exact kubectl/istioctl commands to verify the EnvoyFilter config, inspect per-tenant quotas, check the ratelimit service and Redis metrics, and validate per-tenant routing; propose a zero-downtime canary rollout path and rollback, and how to ensure P95 latency stays under 150 ms during rollout?","answer":"Verify per-tenant rate-limiting via Istio: 1) verify EnvoyFilter exists and matches tenant keys; 2) inspect ratelimit service and Redis quotas; 3) fetch per-tenant routes/clusters via istioctl proxy-c","explanation":"## Why This Is Asked\nTests practical debugging of per-tenant rate limits in Istio with Redis, including troubleshooting xDS config, and safe rollout.\n\n## Key Concepts\n- Istio EnvoyFilter and per-tenant configuration\n- Rate limiting with Redis-backed quotas\n- istioctl proxy-config introspection\n- Canary rollouts and safe rollback\n- Observability: Envoy stats and access logs\n\n## Code Example\n```bash\n# Inspect EnvoyFilter\nkubectl get envoyfilters -n kcsa-prod\n# Show ratelimit filter configuration for a pod\nistioctl proxy-config filters <pod> -n kcsa-prod\n# Check Redis quotas\nredis-cli INFO keyspace\n# View per-tenant routes\nistioctl proxy-config routes <pod> -n kcsa-prod | grep -i tenant\n```\n\n## Follow-up Questions\n- How would you automate drift detection between per-tenant quotas and actual usage?\n- What metrics would you collect to decide when to expand the canary?","diagram":null,"difficulty":"advanced","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Goldman Sachs","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T05:41:21.430Z","createdAt":"2026-01-19T05:41:21.430Z"},{"id":"q-4191","question":"In kcsa-prod, a small web API deployed as a Kubernetes Deployment experiences intermittent 5xx during a recent image update. Provide a concrete diagnostic plan with exact kubectl commands to inspect the Deployment, ReplicaSets, Pods, readiness probes, and the image rollout; outline a zero-downtime canary rollout (including patch/scale steps) and rollback path, plus how to validate that P95 latency stays under 200 ms during rollout?","answer":"Start by inspecting the rollout: kubectl get deploy orders-api -o wide; kubectl rollout history deploy orders-api; kubectl describe deploy orders-api; kubectl get rs -a; kubectl logs deploy/orders-api","explanation":"## Why This Is Asked\n\nTests ability to diagnose and execute a safe, low-downtime rollout when a new image introduces instability. Focuses on rollout metadata, readiness, and quick rollback.\n\n## Key Concepts\n\n- Kubernetes Deployments and ReplicaSets\n- Rollout history and remediation\n- Readiness/Liveness probes and startup checks\n- Canary/blue-green deployment patterns for safety\n- Rollback with kubectl rollout undo\n- Latency validation under load\n\n## Code Example\n\n```javascript\n// Node.js pseudo-code using Kubernetes client to check rollout and query metrics\nconst k8s = require('@kubernetes/client-node');\nconst kc = new k8s.KubeConfig();\nkc.loadFromDefault();\nconst apps = kc.makeApiClient(k8s.AppsV1Api);\nasync function checkRollout(name, ns='default') {\n  const r = await apps.readNamespacedDeployment(name, ns);\n  console.log(r.body.status);\n}\ncheckRollout('orders-api', 'kcsa-prod');\n```\n\n## Follow-up Questions\n\n- How would you explicitly structure a canary deployment to guarantee no more than 10% traffic to the new version while monitoring SLI/SLOs?\n- What metrics and alerts would you rely on to detect regressions during rollout and trigger rollback automatically?","diagram":"flowchart TD\n  A[Check rollout] --> B[Identify issues]\n  B --> C[Canary rollout]\n  C --> D[Monitor metrics]\n  D --> E{Issue?}\n  E --> |Yes| F[Rollback]\n  E --> |No| G[Complete rollout]","difficulty":"beginner","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["PayPal","Scale Ai","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T07:05:50.648Z","createdAt":"2026-01-19T07:05:50.648Z"},{"id":"q-4207","question":"KCsa-prod: A multi-tenant payments API behind Istio mTLS uses a per-tenant Envoy WASM rate-limiter injected via an EnvoyFilter. After releasing a new WASM module, certain tenants see 429s during peak payment bursts. Provide a concrete diagnostic plan with exact kubectl/istioctl commands to verify WASM module loading and version, inspect per-tenant rate-limit config in the module, check Envoy stats, inspect rate-limit service health, and implement a zero-downtime canary rollout with per-tenant gating and rollback; ensure P95 latency stays under 120 ms?","answer":"Check loaded WASM and config per tenant with Istio. Commands:\n- kubectl -n kcsa-prod get pods -l app=payments-api -o name\n- istioctl proxy-status\n- for a pod: istioctl proxy-config listeners <pod> -n ","explanation":"## Why This Is Asked\nTests practical debugging of advanced Istio/WASM scenarios, focusing on per-tenant config, module loading, and safe rollouts.\n\n## Key Concepts\n- Istio Envoy WASM module loading\n- Per-tenant rate-limit configuration\n- Canary rollouts and safe rollback\n\n## Code Example\n```javascript\n// Example: extract wasm module names from config_dump\nconst wasm = configDump.filter(m => m.type === 'wasm_module');\n```\n\n## Follow-up Questions\n- How would you automate per-tenant gating for future wasm upgrades?\n- What metrics indicate a successful canary rollout?","diagram":"flowchart TD\n  A[Tenant] --> B[Envoy WASM Module]\n  B --> C[Rate limit]\n  C --> D[Canary rollout]\n  D --> E[Roll back if latency spikes]","difficulty":"advanced","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Lyft","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T08:46:36.548Z","createdAt":"2026-01-19T08:46:36.548Z"},{"id":"q-4284","question":"In kcsa-prod, a multi-tenant event-ingestion API behind Istio mTLS uses a custom Envoy Lua script to enforce per-tenant rate limiting. After a change to the script, some tenants hit 429s during bursts while others are fine. Provide a concrete diagnostic plan with exact kubectl/istioctl commands to inspect the Lua filter, verify per-tenant keys in Redis, confirm rate-limit service health, and examine Envoy stats; propose a zero-downtime canary rollout path and rollback, and how to validate P95 latency stays under 180 ms during rollout?","answer":"Probe the Lua rate-limit filter and per-tenant keys: verify Redis tenant counters; inspect the EnvoyFilter and Lua script; read Envoy stats; check rate-limit service health. Then perform a safe canary","explanation":"## Why This Is Asked\nThis assesses diagnosing complex mesh policy regressions with per-tenant controls and safe rollout.\n\n## Key Concepts\n- Istio EnvoyFilter Lua scripts\n- Per-tenant rate limiting and Redis keys\n- Envoy stats and service health, canary rollout\n\n## Code Example\n```javascript\n# Commands to run (illustrative)\nkubectl -n kcsa-prod get envoyfilters\nkubectl -n kcsa-prod get configmap rate_limit_lua -o yaml\nredis-cli -h <host> KEYS tenant:*\nkubectl -n kcsa-prod exec -it <proxy-pod> -- curl 127.0.0.1/stats\n```\n\n## Follow-up Questions\n- How would you instrument alerting for burst-induced 429s?\n- How would you test rollback safety under peak load?","diagram":"flowchart TD\n  A[Start] --> B[Inspect Lua filter]\n  B --> C[Check Redis keys]\n  C --> D[Verify rate-limit service health]\n  D --> E[Plan canary rollback]\n  E --> F[Validate P95 latency]","difficulty":"intermediate","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["OpenAI","Robinhood","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T11:33:29.673Z","createdAt":"2026-01-19T11:33:29.673Z"},{"id":"q-4380","question":"In kcsa-prod, a Deployment payments-collector behind a Service intermittently serves 503s as readiness probes fail during a config change. Provide a concrete diagnostic plan with exact kubectl commands to inspect pod readiness, probe definitions, events, and health endpoints; propose a zero-downtime rollback and a safe rollout path, and explain how you would verify latency stays under 200 ms during rollout?","answer":"Run: kubectl get deploy payments-collector -n kcsa-prod; kubectl get pods -n kcsa-prod -l app=payments-collector; kubectl describe pod <pod> -n kcsa-prod; kubectl logs <pod> -c payments-collector -n k","explanation":"## Why This Is Asked\n\nTests practical debugging of a common Kubernetes issue: readiness misconfiguration causing intermittent 503s. It evaluates command fluency, observability skills, and safe rollback/rollout discipline.\n\n## Key Concepts\n\n- Readiness vs Liveness probes\n- Pod events and container logs\n- Rollout undo/restart for zero-downtime recovery\n- Canary rollout and latency monitoring\n\n## Code Example\n\n```javascript\nreadinessProbe: {\n  httpGet: { path: \"/health\", port: 8080 },\n  initialDelaySeconds: 5,\n  periodSeconds: 5\n}\n```\n\n## Follow-up Questions\n\n- How would you adjust probes to tolerate cold starts?\n- How would you validate latency impact during a canary rollout?","diagram":null,"difficulty":"beginner","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Lyft","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T16:46:06.286Z","createdAt":"2026-01-19T16:46:06.286Z"},{"id":"q-4417","question":"In kcsa-prod namespace, a multi-tenant event ingestion API behind Istio mTLS experiences 403s for a subset of tenants after an update to an External Authorization (OPA) policy that scopes access by a tenant header. Provide a concrete diagnostic plan with exact kubectl/istioctl commands to verify the ExternalAuthorization filter config, OPA reachability, per-tenant policy evaluation, and per-tenant routing; propose a zero-downtime canary rollout and rollback; and explain how to validate P95 latency stays under 180 ms during rollout?","answer":"Plan: verify ExternalAuthorization (OPA) policy is reachable and headers map per tenant; inspect Envoy ext_authz filter config via istioctl pc listener; confirm OPA URL, TLS, and policy IDs; tail logs","explanation":"## Why This Is Asked\nTests practical troubleshooting of Istio ExternalAuthorization with OPA, plus safe rollout mechanics.\n\n## Key Concepts\n- Istio ext_authz integration with OPA\n- Tenant-scoped headers and policy evaluation\n- Canary rollouts and rollback strategies in Istio\n\n## Code Example\n```bash\n# Inspect OPA deployment\nkubectl get pods -n kcsa-prod -l app=opa\n# Check Envoy ext_authz filter via a gateway listener\nistioctl pc listener <gateway-pod> -n kcsa-prod\n```\n\n## Follow-up Questions\n- How to limit blast radius if policy failures persist?\n- How would you verify policy changes across tenants during rollback?","diagram":null,"difficulty":"intermediate","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Meta","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T17:57:53.556Z","createdAt":"2026-01-19T17:57:53.556Z"},{"id":"q-4561","question":"In kcsa-prod, a multi-tenant data ingestion API behind Istio mTLS intermittently returns 429s for a subset of tenants after enabling a Redis-backed per-tenant quota service. Provide a concrete diagnostic plan with exact kubectl/istioctl commands to verify quota service reachability, inspect per-tenant quota keys in Redis, validate the Envoy rate-limit filter config and per-tenant routing, and design a zero-downtime canary rollout with rollback; include how to validate P95 latency stays under 150 ms during rollout?","answer":"Plan: verify quota service health, Redis keys, and Envoy rate-limit config; verify per-tenant routes and canary rollout safety. Commands:\n- kubectl get pods -n kcsa-prod\n- kubectl exec -n kcsa-prod <quota-pod> -- curl -f http://quota-service.kcsa-prod.svc.cluster.local:8080/healthz\n- kubectl exec -n kcsa-prod <redis-pod> -- redis-cli --scan --pattern \"quota:*\" | head -10\n- istioctl proxy-config routes <gateway-pod> --name http.80 -n istio-system | grep -A5 -B5 \"quota-service\"\n- kubectl get envoyfilter rate-limit-filter -n istio-system -o yaml\n- kubectl patch deployment quota-service -p '{\"spec\":{\"template\":{\"metadata\":{\"labels\":{\"canary\":\"true\"}}}}}'\n- kubectl patch virtualservice quota-service -p '{\"spec\":{\"http\":[{\"match\":{\"headers\":{\"canary\":{\"exact\":\"true\"}}},\"route\":{\"destination\":{\"subset\":\"canary\"}}},{\"route\":{\"destination\":{\"subset\":\"stable\"}}}]}}'\n- kubectl top pods -n kcsa-prod --sort-by=cpu\n- istioctl proxy-config metrics <gateway-pod> --name http.80 -n istio-system | grep \"envoy_cluster_upstream_rq_time_bucket\"","explanation":"## Why This Is Asked\n\nTests ability to reason about tenant-aware quotas, service reachability, and rollout safety.\n\n## Key Concepts\n\n- Per-tenant quotas, Redis, Envoy rate-limiter, Istio routes, canary rollouts\n\n## Code Example\n\n```javascript\n// Minimal example of quota call\nfetch('http://quota-service.kcsa-prod.svc.cluster.local:8080/limits', {headers: {'Tenant':'t1'}})\n```\n\n## Follow-up Questions\n\n- How would you simulate tenant traffic bursts?\n- What metrics would you monitor during the rollout and rollback?","diagram":null,"difficulty":"beginner","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T06:04:43.453Z","createdAt":"2026-01-19T23:52:56.718Z"},{"id":"q-4606","question":"In kcsa-prod, a multi-tenant API gateway behind Istio mTLS experiences intermittent TLS handshake failures for tenants after automated workload certificate rotation. Provide a concrete diagnostic plan with exact kubectl/istioctl commands to verify certificate issuance, Envoy TLS context, per-tenant SPIFFE IDs, and next-hop TLS settings; propose a zero-downtime rotation path and rollback, and how to validate P95 latency stays under 120 ms during rollout?","answer":"Verify rotation status on both workloads and gateway; inspect Envoy TLS contexts and SPIFFE IDs; confirm cert validity and issuer trust; test with openssl s_client for sample tenants; review gateway l","explanation":"## Why This Is Asked\n\nTests practical TLS rotation troubleshooting in Istio with multi-tenant routing, focusing on handshake failures, SPIFFE identities, and safe rollout strategies.\n\n## Key Concepts\n\n- Istio mTLS and certificate rotation\n- Envoy TLS context inspection and SPIFFE IDs per tenant\n- Certificate validity, issuer trust, and cross-cluster issuance\n- Canary rollout with latency gating and safe rollback\n\n## Code Example\n\n```bash\n# Example handshake test for a tenant\nopenssl s_client -connect tenant1.example.com:443 -servername tenant1.example.com\n```\n\n## Follow-up Questions\n\n- How would you mitigate future handshake drift during rotation?\n- Which metrics signal a rollout should pause and rollback?","diagram":null,"difficulty":"intermediate","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Slack","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T04:18:25.867Z","createdAt":"2026-01-20T04:18:25.867Z"},{"id":"q-4688","question":"In kcsa-prod, a multi-tenant data-ingest API behind Istio mTLS intermittently returns 503s when a new per-tenant quota sidecar was deployed. Quotas are stored in Redis and enforced via a custom EnvoyFilter. Provide a concrete diagnostic plan with exact kubectl/istioctl commands to verify the EnvoyFilter, per-tenant route config, Redis keys/TTL, and quota filter stats; propose a zero-downtime canary rollout to adjust TTL or move to in-memory caching, and outline how to validate P95 latency stays under 150 ms during rollout?","answer":"Run: kubectl get envoyfilters -n kcsa-prod -o yaml; istioctl pc listener -n kcsa-prod <gateway>; kubectl get authorizationpolicy -n kcsa-prod -o yaml; redis-cli -h redis-prod KEYS tenant:*quota*; redi","explanation":"## Why This Is Asked\nTests practical debugging across Istio, Envoy, Redis, and canary rollouts with per-tenant state.\n\n## Key Concepts\n- Istio authorization, EnvoyFilter, per-tenant routing\n- Redis-backed quotas and TTLs\n- Canary rollouts and metrics validation\n\n## Code Example\n```javascript\n// No code\n```\n\n## Follow-up Questions\n- How would you automate this diagnostic with a one-click runbook?\n- How would you protect Redis TTL misconfigurations in production?","diagram":"flowchart TD\n  A[Ingress] --> B[Envoy]\n  B --> C[Quota service]\n  C --> D[Redis]\n  D --> E[Backend]","difficulty":"intermediate","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["NVIDIA","Snap","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T07:55:15.231Z","createdAt":"2026-01-20T07:55:15.232Z"},{"id":"q-4776","question":"In kcsa-prod, a multi-tenant data-processing service on Kubernetes uses Gatekeeper for security policies. A policy update enforces non-root execution and removal of hostPath across all namespaces; a subset of tenants experiences deployment delays and intermittent 4xx during upgrades. Provide a concrete diagnostic plan with exact kubectl and gatekeeper commands to identify violating policies, inspect ConstraintTemplates and Constraints, review namespace exemptions, and trace the admission pipeline; propose a zero-downtime rollout with per-tenant allowlists and rollback, and explain how you would validate latency and throughput during rollout?","answer":"Query Gatekeeper: kubectl get ConstraintTemplates; kubectl get Constraints; kubectl describe <constraint> to see violations. Inspect admission webhook logs: kubectl logs -n gatekeeper deploy/gatekeepe","explanation":"## Why This Is Asked\nTests ability to diagnose policy-driven enforcement issues in a multi-tenant Kubernetes cluster, focusing on admission control, policy scope, and safe rollouts.\n\n## Key Concepts\n- Gatekeeper: policy enforcement at admission time\n- ConstraintTemplates and Constraints: define and apply rules\n- Violations and admission path: identify which policy blocks deployments\n- Namespace exemptions: per-tenant grace rules\n- Canary rollout: safe policy change across tenants\n- SLA validation: ensure latency/throughput targets during rollout\n\n## Code Example\n```yaml\n# ConstraintTemplate (example)\napiVersion: templates.gatekeeper.sh/v1beta1\nkind: ConstraintTemplate\nmetadata:\n  name: k8snonroot\nspec:\n  crd:\n    spec:\n      names:\n        kind: K8sNonRoot\n      validation:\n        openAPIV3Schema:\n          type: object\n  targets:\n  - target: admission.k8s.gatekeeper.sh\n    services:\n    - name: rego\n```\n```\n# Constraint (example)\napiVersion: constraints.gatekeeper.sh/v1beta1\nkind: K8sNonRoot\nmetadata:\n  name: nonroot-pods\nspec:\n  match:\n    kinds:\n      - apiGroups: [\"\"]\n        kinds: [\"Pod\"]\n```\n\n## Follow-up Questions\n- How would you surface per-tenant exemptions and verify they’re honored?\n- What’s your rollback plan if the policy causes widespread disruption, and how would you validate SLA during rollout?","diagram":null,"difficulty":"advanced","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","DoorDash","Lyft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T11:48:17.063Z","createdAt":"2026-01-20T11:48:17.064Z"},{"id":"q-4933","question":"In kcsa-prod, a multi-tenant payments API behind Istio mTLS intermittently returns 403s for new tenants after a network policy update restricted outbound calls. Provide a concrete diagnostic plan with exact kubectl/istioctl commands to verify policy selectors, inspect Envoy sidecars, per-tenant RBAC, and egress logs; propose a zero-downtime canary rollback and explain how to verify P95 latency stays under 120 ms during rollout?","answer":"Investigate a policy drift causing outbound calls to be blocked post-policy update. Validate exact NetPolicy selectors and Istio RBAC; inspect per-tenant egress rules and Envoy sidecars for access con","explanation":"## Why This Is Asked\nTests ability to diagnose multi-tenant policy drift in a service mesh with strict egress controls and RBAC.\n\n## Key Concepts\n- Kubernetes NetworkPolicy and Istio RBAC interactions\n- Envoy sidecar inspection and egress gateways\n- Canary rollouts with zero downtime and latency validation\n\n## Code Example\n```javascript\n// Example pseudo-code to fetch policies and print mismatches\nconst k8s = require('@kubernetes/client-node');\nconst kc = new k8s.KubeConfig();\nkc.loadFromDefault();\nconst k8sApi = kc.makeApiClient(k8s.NetworkPolicyApis); // pseudo\n// Fetch policies for kcsa-prod and compare selectors\n```\n\n## Follow-up Questions\n- How would you automate drift detection across tenants?\n- What metrics would you surface to confirm latency remains within targets during rollout?\n","diagram":"flowchart TD\n  A[Tenant] --> B[NetPolicy & RBAC]\n  B --> C[Envoy sidecar]\n  C --> D[Egress gateway]\n  D --> E[Backend]","difficulty":"advanced","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Microsoft","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T20:23:28.813Z","createdAt":"2026-01-20T20:23:28.813Z"},{"id":"q-4992","question":"kcsa-prod namespace hosts a payments-api Deployment behind a Service. A custom-metric HPA scales on requests_per_sec, but during a spike it fails to scale and latency rises. Provide a concrete diagnostic plan with exact kubectl commands to verify the HPA spec, inspect the custom metrics API, confirm metrics-server health, check per-pod latency, and propose a zero-downtime canary rollout path to restore performance while keeping P95 latency under 150 ms?","answer":"Run: kubectl get hpa payments-api -n kcsa-prod; kubectl describe hpa payments-api -n kcsa-prod; kubectl top pods -n kcsa-prod; kubectl get --raw '/apis/custom.metrics.k8s.io/v1beta1/namespaces/kcsa-prod/*/requests_per_sec'; kubectl get pods -n kcsa-prod -l app=payments-api -o wide; kubectl logs -n kcsa-prod -l app=payments-api --tail=100; kubectl get events -n kcsa-prod --field-selector involvedObject.kind=HPA","explanation":"## Why This Is Asked\nTests practical Kubernetes operations skills: diagnosing HPA behavior, custom metrics integration, and safe rollout strategies in a live production system.\n\n## Key Concepts\n- Horizontal Pod Autoscaler with custom metrics configuration\n- metrics-server health and custom.metrics.k8s.io API availability\n- Per-pod latency observability and performance monitoring\n- Zero-downtime canary rollouts with rollback capabilities\n\n## Code Example\n```javascript\n// PromQL example for P95 latency evaluation\nhistogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))\n```\n\n## Follow-up Questions\n- How would you verify the custom metrics adapter is properly registered?\n- What steps would you take to implement a progressive canary rollout?\n- How would you configure HPA to prevent thrashing during metric fluctuations?","diagram":"flowchart TD\n  A[Payments API] --> B[HPA]\n  B --> C[Pods]\n  A --> D[Metrics API]\n  C --> E[Latency Observability]\n  F[Canary Rollout] --> G[Service]","difficulty":"beginner","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T05:48:22.585Z","createdAt":"2026-01-20T22:44:38.304Z"},{"id":"q-5110","question":"kcsa-prod: A payments API Deployment behind a Service reports ImagePullBackOff after a secret rotation for the imagePullSecret. Provide concrete steps with exact kubectl commands to verify the secret, serviceAccount, image reference, and to apply a safe, zero-downtime rollout or rollback?","answer":"kubectl get secret regcred -n kcsa-prod\nkubectl get sa default -n kcsa-prod -o yaml\nkubectl get deploy payments-api -n kcsa-prod -o jsonpath='{.spec.template.spec.imagePullSecrets[*].name}'\n# If missi","explanation":"## Why This Is Asked\nTests practical Kubernetes troubleshooting: handling a secret rotation that breaks image pulls, validating secrets and SA bindings, and executing a safe rollout/rollback.\n\n## Key Concepts\n- imagePullSecrets, Secrets, ServiceAccounts, Deployments\n- kubectl patch, rollout status, rollout undo\n\n## Code Example\n```javascript\n// Patch for imagePullSecrets (example)\nkubectl patch deployment payments-api -n kcsa-prod -p '{\"spec\":{\"template\":{\"spec\":{\"imagePullSecrets\":[{\"name\":\"regcred\"}]}}}}'\n```\n\n## Follow-up Questions\n- How would you verify secret rotation across namespaces?\n- What metrics would you monitor to ensure no downtime during rollback?","diagram":"flowchart TD\n  A[Start] --> B[Check secret exists]\n  B --> C[Verify imagePullSecrets]\n  C --> D[Patch deployment if needed]\n  D --> E[Rollout status]\n  E --> F[Rollback if needed]","difficulty":"beginner","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Goldman Sachs","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T06:56:58.083Z","createdAt":"2026-01-21T06:56:58.083Z"},{"id":"q-5145","question":"In kcsa-prod, a multi-tenant search API behind Istio mTLS intermittently returns 502s when OpenTelemetry tracing is enabled and the collector scales. Provide a concrete diagnostic plan with exact kubectl/istioctl commands to verify the collector deployment and config, inspect Envoy trace proxies, confirm per-tenant sampling, and craft a zero-downtime canary rollout with rollback; include how to validate P95 latency stays under 150 ms during rollout?","answer":"Run: kubectl -n kcsa-prod get deploy opentelemetry-collector; kubectl -n kcsa-prod get configmap otel-config -o yaml; kubectl logs -n kcsa-prod -l app=otel-collector --tail=200; kubectl get pods -n kc","explanation":"Why This Is Asked\n\nTests ability to diagnose tracing-induced instability in a real multi-tenant service mesh with OpenTelemetry, plus safe rollout practices.\n\nKey Concepts\n\n- OpenTelemetry collector health and config drift\n- Envoy sidecar proxy stats for tracing\n- Per-tenant sampling configuration\n- Canary rollouts in Istio and rollback strategies\n- P95 latency monitoring during rollout\n\nCode Example\n\n```bash\n# Collector status\nkubectl -n kcsa-prod get deploy opentelemetry-collector\n\n# Current tracing config\nkubectl -n kcsa-prod get configmap otel-config -o yaml\n\n# Collector logs\nkubectl logs -n kcsa-prod -l app=otel-collector --tail=200\n\n# Service pods\nkubectl get pods -n kcsa-prod -l app=search-api\n\n# Envoy tracing stats (per pod)\nkubectl exec -n kcsa-prod -i <search-pod> -c istio-proxy -- curl -s http://127.0.0.1:15000/stats | grep tracing\n```\n\nFollow-up Questions\n\n- How would you automate the canary rollout ramp and automatic rollback?\n- What metrics and thresholds would you codify for promotion/demotion across environments?\n","diagram":"flowchart TD\n  A[User Request] --> B[Envoy (istio-proxy)]\n  B --> C[search-api]\n  C --> D[OpenTelemetry Collector]\n  D --> E[Backend]\n","difficulty":"intermediate","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Hugging Face"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T08:06:30.291Z","createdAt":"2026-01-21T08:06:30.291Z"},{"id":"q-5331","question":"In kcsa-prod, a GPU-accelerated inference service 'infer-api' runs in namespace kcsa-prod with NVIDIA device plugin. Multiple tenants share GPUs under per-tenant quotas; during mixed load, one tenant's spike causes QoS degradation with increased P95 latency for that tenant while others stay healthy. Provide a concrete diagnostic plan with exact kubectl and nvidia-smi commands to verify per-tenant GPU allocation, inspect device-plugin logs, assess GPU memory pressure, and review scheduler decisions; propose a zero-downtime canary rollout to restore performance while keeping P95 latency under 200 ms, and how to rollback?","answer":"Analyze per-tenant GPU allocation and contention with kubectl and nvidia-smi. Steps: verify NVIDIA device-plugin DS; inspect per-tenant quotas and limits; review pod events; exec into representative p","explanation":"## Why This Is Asked\nTests practical troubleshooting of multi-tenant GPU workloads in KCSA, focusing on resource isolation, device plugin behavior, and safe rollout practices under strict latency targets.\n\n## Key Concepts\n- GPU device plugin behavior and per-tenant isolation\n- Resource quotas, limits, and scheduling decisions\n- Real-time GPU memory pressure detection\n- Canary rollouts with zero-downtime rollback\n\n## Code Example\n```javascript\n// Example: simple P95 calculator for latency logs\nfunction p95(arr) {\n  if (!arr.length) return 0\n  arr.sort((a,b)=>a-b)\n  const idx = Math.floor(0.95 * (arr.length - 1))\n  return arr[idx]\n}\n```\n\n## Follow-up Questions\n- How would you bind a per-tenant GPU pool to prevent cross-tenant memory pressure?\n- What metrics and alerting would you add to catch GPU memory pressure before latency degrades?","diagram":null,"difficulty":"advanced","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["NVIDIA","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T17:25:43.757Z","createdAt":"2026-01-21T17:25:43.757Z"},{"id":"q-5368","question":"kcsa-prod namespace hosts a multi-tenant API behind Istio mTLS. A new gateway-level feature flag toggles per-tenant RBAC, but a subset of tenants intermittently receive 403s during peak traffic. Provide a concrete diagnostic plan with exact kubectl/istioctl commands to verify the gateway policy changes, inspect Envoy RBAC filters, per-tenant token claims, and rollout a zero-downtime canary to restore access while keeping P95 latency under 180 ms?","answer":"Plan: snapshot policies and gateways, identify failing tenants, verify per-tenant RBAC rules and Envoy RBAC filters, inspect token claims, test with curl respecting tenant scoping, and perform a stage","explanation":"## Why This Is Asked\nThis tests practical debugging of per-tenant RBAC gating and Istio policy propagation under load.\n\n## Key Concepts\n- Istio AuthorizationPolicy, Gateway, RBAC\n- Envoy RBAC filters, per-tenant JWT claims\n- Canary rollout with VirtualService weights\n- Observability: Prometheus-based latency targets\n\n## Code Example\n```javascript\n// Example commands (illustrative)\nkubectl get authorizationpolicy -n kcsa-prod\nkubectl get gateways -n kcsa-prod\nistioctl proxy-config listeners <pod> -n kcsa-prod\n```\n\n## Follow-up Questions\n- How would you roll back if 403s persist past the canary window?\n- How would you extend tests to ensure P95 latency stays below 180 ms during ramp?","diagram":"flowchart TD\n  A(Client) --> B(Gateway)\n  B --> C(Envoy RBAC)\n  C --> D(Tenant Services)\n  D --> E(Telemetry/Alerts)","difficulty":"beginner","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T19:42:32.776Z","createdAt":"2026-01-21T19:42:32.777Z"},{"id":"q-5396","question":"In kcsa-prod namespace, inventory-service pods call the Kubernetes API to fetch per-tenant feature flags from a CRD named TenantFlags. After a gatekeeper policy change, the service returns 403s for some tenants and 5xx under load. Provide a concrete diagnostic plan with exact kubectl commands to verify the ServiceAccount and RoleBindings, verify the CRD permissions, inspect API server audit logs, and plan a zero-downtime canary rollout with rollback; show how to validate latency targets during rollout?","answer":"Plan: verify inventory-service's ServiceAccount and relevant RoleBindings/ClusterRoleBindings; inspect TenantFlags CRD permissions and webhook policies; search API server audit logs for denied request","explanation":"## Why This Is Asked\nTests RBAC, CRD permissions, and rollout safety in a beginner-friendly Kubernetes scenario.\n\n## Key Concepts\n- Kubernetes RBAC and service accounts\n- CustomResourceDefinitions and API permissions\n- Gatekeeper policy implications\n- Canary deployments and rollbacks\n\n## Code Example\n```javascript\nkubectl get sa inventory-service -n kcsa-prod -o yaml\nkubectl describe rolebinding -n kcsa-prod | grep inventory-service\nkubectl get crd TenantFlags -o yaml\nkubectl auth can-i list tenantflags --as=system:serviceaccount:kcsa-prod:inventory-service\n```\n\n## Follow-up Questions\n- How would you roll back after a failed canary?\n- How would you measure p95 latency during rollout?","diagram":null,"difficulty":"beginner","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","DoorDash","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T20:47:52.088Z","createdAt":"2026-01-21T20:47:52.088Z"},{"id":"q-5556","question":"In kcsa-prod, a multi-tenant API namespace uses OPA Gatekeeper for admission control. After a policy update, a subset of tenants fail to create resources with 403s due to missing constraints. Provide a concrete diagnostic plan with exact kubectl commands to: inspect gatekeeper-system pods; ConstraintTemplates; Constraints; ValidatingWebhookConfiguration; reproduce failing requests; check ConstraintStatus; and outline a zero-downtime rollback path to remove or relax the new constraint while validating no regressions?","answer":"Diagnose Gatekeeper policy breach with concrete steps. Collect state: kubectl get pods -n gatekeeper-system; kubectl get ConstraintTemplates -A; kubectl get Constraints -A; kubectl get ValidatingWebho","explanation":"## Why This Is Asked\nTests real-world policy-eviction incidents and rollback safety in a multi-tenant Kubernetes cluster using OPA Gatekeeper. It evaluates practical debugging skills, command fluency, and risk-aware rollback planning.\n\n## Key Concepts\n- OPA Gatekeeper policy flow\n- ConstraintTemplates/Constraints\n- ValidatingWebhookConfiguration\n- Rollback strategies and safety\n\n## Code Example\n```javascript\n// Not applicable; diagnostic steps are shell commands above\n```\n\n## Follow-up Questions\n- How would you monitor policy violation trends post-rollout?\n- How would you test a safe canary for policy changes?","diagram":null,"difficulty":"intermediate","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Goldman Sachs","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T06:00:22.362Z","createdAt":"2026-01-22T06:00:22.362Z"},{"id":"q-5642","question":"In kcsa-prod, a multi-tenant analytics API behind Istio mTLS starts returning 429s for a subset of tenants during peak after enabling a policy that enforces per-tenant data residency via an admission webhook injecting an region header. Provide concrete diagnostic steps with exact kubectl/istioctl commands to verify the policy decision, inspect Gatekeeper/OPA logs, confirm header propagation across Envoy, review per-tenant routing, and implement a zero-downtime canary rollout with rollback; ensure P95 latency stays under 180 ms during rollout?","answer":"Inspect policy layer: kubectl get ConstraintTemplate -A; kubectl get constraints -A; Gatekeeper audit/logs: kubectl logs -n istio-system deploy/gatekeeper; Inspect Envoy and routing: istioctl pc route","explanation":"## Why This Is Asked\nTests ability to diagnose intersection of policy enforcement, data-plane routing, and performance in a real-world multi-tenant setup. Focuses on admission-webhook policy effects, Envoy header propagation, and safe canary rollouts under strict latency SLOs.\n\n## Key Concepts\n- Kubernetes policy engines (Gatekeeper/OPA)\n- Istio traffic management (VirtualService, DestinationRule, canary routing)\n- Envoy header propagation and per-tenant routing\n- Canary rollout, rollback, and latency SLO validation\n- Observability: logs, metrics, and tracing for rate-limiting disruptions\n\n## Code Example\n```yaml\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: payments-api\nspec:\n  http:\n  - route:\n    - destination:\n        host: payments-api\n        subset: v1\n      weight: 90\n    - destination:\n        host: payments-api\n        subset: v2\n      weight: 10\n```\n\n## Follow-up Questions\n- How would you validate admission webhook latency contributes to 429s?\n- How would you extend the canary to a metric-driven rollback on P95 threshold breaches?","diagram":null,"difficulty":"intermediate","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T09:45:00.110Z","createdAt":"2026-01-22T09:45:00.110Z"},{"id":"q-5787","question":"kcsa-prod namespace hosts a simple API behind Istio mTLS using a Redis-backed per-tenant rate limiter. After a feature-flag rollout, several tenants experience 429s under moderate load while others do not. Provide a concrete diagnostic plan with exact kubectl/istioctl commands to verify the rate limiter deployment/config, inspect Redis keys per tenant, check Envoy local_rate_limit stats, and confirm per-tenant routing; propose a zero-downtime canary rollout with rollback and how to verify latency targets during rollout?","answer":"Outline a concrete diagnostic plan with exact kubectl/istioctl commands to verify the rate limiter deployment/config, inspect Redis keys per tenant, check Envoy local_rate_limit stats, and confirm per","explanation":"## Why This Is Asked\nThis question tests practical debugging of per-tenant rate limiting in a service mesh, including cross-service coordination with Redis, Envoy stats, and safe rollouts.\n\n## Key Concepts\n- Istio mTLS, per-tenant rate limiting, Redis-backed quotas, Envoy stats, canary rollouts\n- Observability checks and rollback procedures\n- Safe rollout strategies under feature flags\n\n## Code Example\n```javascript\n// placeholder for illustration\n```\n\n## Follow-up Questions\n- How would you adapt the plan for multi-region deployments?\n- What metrics would you track to validate performance during rollout?","diagram":"flowchart TD\n  A[Tenant Request] --> B[IngressGateway]\n  B --> C[RateLimiter]\n  C --> D[Backend Service]\n  D --> E[Telemetry/Logs]","difficulty":"beginner","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Meta","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T16:50:58.228Z","createdAt":"2026-01-22T16:50:58.228Z"},{"id":"q-5930","question":"kcsa-prod namespace hosts a multi-tenant data API behind Istio mTLS. A recent AuthorizationPolicy update gates access using tenantId claims and causes sporadic 403s for tenants with odd IDs at scale. Provide a concrete diagnostic plan with exact kubectl/istioctl commands to inspect AuthorizationPolicy precedence, per-tenant rules, Envoy RBAC filters, and sidecar logs; outline a zero-downtime canary rollout to revert the policy for affected tenants and validate latency targets during rollout?","answer":"An AuthorizationPolicy update in the kcsa-prod namespace is causing intermittent 403 errors for tenants with odd IDs when accessing the multi-tenant data API behind Istio mTLS. To diagnose this issue systematically: execute `kubectl get authorizationpolicy -n kcsa-prod` to list all policies and their precedence; run `istioctl experimental authz check <service> -f policy.yaml` to validate policy execution against specific tenant requests; use `kubectl logs deployment/<deployment-name> -c istio-proxy --tail=100` to inspect Envoy sidecar RBAC filter logs for denied requests; and analyze per-tenant rule evaluation with `istioctl proxy-config routes <pod-name> -n kcsa-prod -o json`. For remediation, implement a zero-downtime canary rollout by creating a new AuthorizationPolicy with relaxed rules for odd-tenant IDs, gradually shifting traffic using VirtualService weighted routing (5%, 25%, 50%, 100%), monitoring latency metrics with `istioctl proxy-status` and Prometheus queries, and validating that 99th percentile latency stays below 100ms during each phase before proceeding to the next.","explanation":"## Why This Is Asked\nThis scenario evaluates practical troubleshooting skills for complex service mesh authorization issues in a production multi-tenant environment. It tests the ability to systematically diagnose policy conflicts, understand Istio's authorization precedence model, and execute safe rollback procedures without service disruption.\n\n## Key Concepts\n- Istio AuthorizationPolicy precedence and rule evaluation order\n- JWT claim extraction for tenant-based access control\n- Envoy sidecar RBAC filter configuration and logging\n- Service mesh observability and policy debugging tools\n- Canary deployment strategies for policy changes\n- Zero-downtime rollout methodologies\n- Latency monitoring and SLO validation\n\n## What It Demonstrates\n- Technical proficiency with Istio debugging tools\n- Understanding of service mesh security policies\n- Production-safe operational procedures\n- Performance validation during infrastructure changes","diagram":"flowchart TD\nA[Policy Update] --> B[AuthorizationPolicy in kcsa-prod]\nB --> C{Tenant access outcome}\nC -- 403 for odd IDs --> D[Investigate precedence]\nC -- OK --> E[Normal traffic]\nD --> F[Canary rollback plan]\nF --> G[Verify latency P95 <200ms]","difficulty":"intermediate","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Anthropic","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T05:43:54.810Z","createdAt":"2026-01-22T22:37:14.788Z"},{"id":"q-5962","question":"In kcsa-prod, a data-ingestion API behind Istio mTLS starts returning 429s for large payload tenants after a gateway rate-limiter policy change. Provide a concrete diagnostic plan with exact kubectl/istioctl commands to inspect the ratelimit filter config, per-tenant quotas in Redis, and Redis state; outline a zero-downtime canary rollout and rollback, ensuring P95 latency stays under 180 ms during rollout?","answer":"Plan: confirm rate-limit policy drift and 429s, inspect ratelimit filter on ingressgateway and per-tenant quotas in Redis; verify Redis state. Commands: kubectl -n kcsa-prod get gateway; istioctl proxy-config routes -n kcsa-prod deployment/ingressgateway -o json | jq '.virtualHosts[].routes[].match.headers[] | select(.name==\"x-tenant-id\")'; kubectl -n kcsa-prod exec -it deployment/redis-cli -- redis-cli --scan --pattern 'ratelimit:*' | head -10; kubectl -n kcsa-prod top pod -l app=redis; istioctl proxy-config listeners -n kcsa-prod deployment/ingressgateway -o json | jq '.dynamicListeners[].activeListener.filters[] | select(.name==\"envoy.filters.http.ratelimit\")'. Canary: kubectl patch deployment api-service -p '{\"spec\":{\"template\":{\"spec\":{\"containers\":[{\"name\":\"api\",\"image\":\"api-service:v2\"}]}}}}' --dry-run=client; kubectl rollout status deployment/api-service --timeout=300s; monitor P95 latency via Prometheus: rate(istio_request_duration_seconds_bucket{destination_service=\"api-service.kcsa-prod.svc.cluster.io\",le=\"0.18\"}[5m]). Rollback: kubectl rollout undo deployment/api-service.","explanation":"## Why This Is Asked\nThis question probes practical debugging of rate-limiter policies in a real cluster, bridging Istio config, Redis-backed quotas, and safe rollout.\n\n## Key Concepts\n- Istio ratelimit filters and Redis-backed quotas\n- Per-tenant routing and canary deployments\n- Observability: Envoy stats, gateway config, latency metrics\n\n## Code Example\n```javascript\n// Canary rollout helper (pseudo)\nfunction nextStep(current, target){ return Math.min(current+5, target); }\n```\n\n## Follow-up Questions\n- How would you test negative scenarios (burst traffic, Redis outage)?\n- How would you parameterize rate limits per tenant dynamically?","diagram":null,"difficulty":"intermediate","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","NVIDIA","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T05:10:10.104Z","createdAt":"2026-01-22T23:48:35.484Z"},{"id":"q-5989","question":"kcsa-prod namespace hosts a multi-tenant API gateway that reads per-tenant feature flags from a ConfigMap named tenant-flags. During a patch rollout, tenants with betaRoute enabled intermittently get 502 responses. Provide a concrete diagnostic plan with exact kubectl commands to verify the ConfigMap contents, confirm how pods receive updates (env vars vs file mounts), inspect gateway and Envoy logs, and implement a zero-downtime canary rollout that validates per-tenant routing changes while keeping P95 latency under 150 ms. How would you approach this?","answer":"During a patch rollout, tenants with betaRoute enabled intermittently experience 502 errors. Diagnostic approach: (1) inspect tenant-flags ConfigMap contents and pod consumption method; (2) verify gateway reflects updated flag configurations; (3) examine Envoy configuration and logs for routing issues; (4) implement canary rollout with validation. Commands: `kubectl get configmap tenant-flags -n kcsa-prod -o yaml`, `kubectl describe pod -l app=gateway -n kcsa-prod`, `kubectl logs -l app=gateway -n kcsa-prod --tail=100`, `kubectl exec -l app=gateway -n kcsa-prod -- cat /etc/config/tenant-flags`. For zero-downtime deployment: utilize progressive canary with 5%/25%/50%/100% traffic splitting, validate P95 latency remains under 150ms at each stage, and implement automatic rollback on 502 error spikes.","explanation":"## Why This Is Asked\nTests practical debugging of per-tenant feature flags and zero-downtime rollout strategies.\n\n## Key Concepts\n- Kubernetes ConfigMaps and pod configuration propagation\n- Environment variables vs mounted files for configuration delivery\n- Canary deployments and rollback safety mechanisms\n- Observability practices and latency budget management\n\n## Code Example\n```javascript\n// Reads a per-tenant flag from environment variable with fallback to false\nfunction isBetaEnabled(tenantId){\n  const cfg = JSON.parse(process.env.TENANT_FLAGS || '{}');\n  return !!cfg[tenantId]?.betaRoute;\n}\n```","diagram":"flowchart TD\n  A[tenant-flags ConfigMap] --> B[Pod receives flags]\n  B --> C[Gateway routing decision]\n  C --> D[Client requests]\n  D --> E[Latency targets OK]","difficulty":"beginner","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Microsoft","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T04:53:06.213Z","createdAt":"2026-01-23T02:46:06.387Z"},{"id":"q-6050","question":"kcsa-prod namespace hosts a multi-tenant API gateway behind Istio mTLS. A recent AuthorizationPolicy rollout inadvertently grants cross-tenant read/write permissions, risking data leakage. Provide a concrete diagnostic plan with exact kubectl/istioctl commands to verify active policies, inspect per-tenant Envoy config, confirm RBAC boundaries and audit trails, and implement a zero-downtime rollback with rollout validation that P95 latency stays under 150 ms?","answer":"Inspect policy state, envoy config, and audit trails; rollback safely. Commands: kubectl get authorizationpolicy -n kcsa-prod; istioctl analyze -n kcsa-prod; kubectl get envoyfilters -n kcsa-prod; kub","explanation":"Why this is asked: tests ability to reason about multi-tenant authorization boundaries, rapid detection of policy drift, and safe rollback in production. Key concepts: AuthorizationPolicy, Envoy per-tenant config, RBAC boundaries, audit trails, and canary rollout with latency validation. Code example shows reverting to a known-good policy. Follow-ups explore verification and monitoring strategies.","diagram":null,"difficulty":"advanced","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T06:55:25.725Z","createdAt":"2026-01-23T06:55:25.725Z"},{"id":"q-6089","question":"In kcsa-prod, a multi-tenant API gateway fronts the billing API. A quota policy update (per-tenant token bucket) causes a subset of tenants to see sporadic 429s and elevated latency under moderate traffic. Provide a concrete diagnostic plan with exact kubectl commands to verify the quota policy, inspect Redis-backed counters, review Envoy per-tenant routes, and implement a zero-downtime canary rollout of the quota changes with rollback; include how to validate P95 latency stays under 180 ms during rollout?","answer":"Check quota policy source in kcsa-prod ConfigMap, confirm gateway rollout status, inspect Redis counters for a sample tenant, and review per-tenant Envoy routes. Then apply a canary quota change and m","explanation":"## Why This Is Asked\n\nTests the ability to reason about per-tenant quotas, policy propagation, and safe rollouts in a realistic kcsa setup with Redis-backed counters, Istio/Envoy routing, and Kubernetes ConfigMaps.\n\n## Key Concepts\n\n- Per-tenant quotas backed by Redis counters\n- ConfigMap-driven policy management in Kubernetes\n- Canary rollout and rollback for zero-downtime changes\n- Envoy/Istio route introspection per tenant\n- Latency validation via Prometheus or metrics endpoints\n\n## Code Example\n\n```javascript\n# Diagnostic steps (example commands)\nkubectl get configmap quota-policy -n kcsa-prod -o yaml\n kubectl get pods -n kcsa-prod -l app=gateway\n kubectl rollout status deploy/gateway -n kcsa-prod\n kubectl get pods -n kcsa-prod -l app=quota-redis\n kubectl exec -n kcsa-prod <redis-pod> -- redis-cli GET quota:tenant:<id>:tokens\n istioctl pc routes <gateway-pod> -n kcsa-prod\n kubectl apply -f quota-canary.yaml\n kubectl rollout status deploy/gateway-canary -n kcsa-prod\n kubectl rollout undo deploy/gateway-canary -n kcsa-prod\n```\n\n## Follow-up Questions\n\n- How would you test a single tenant in isolation without affecting others?\n- Which metrics and thresholds would confirm a safe promotion of the canary rollout?","diagram":"flowchart TD\n  A[Quota Policy ConfigMap] --> B[Gateway Deployment]\n  B --> C[Redis Counters]\n  B --> D[Envoy Per-tenant Routes]\n  D --> E[Canary Rollout]\n  E --> F[Rollback]","difficulty":"beginner","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Salesforce","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T08:02:18.840Z","createdAt":"2026-01-23T08:02:18.840Z"},{"id":"q-6173","question":"**Scenario**: In kcsa-prod, a multi-tenant API gateway relies on a per-tenant feature flag stored in a ConfigMap. Toggling a flag causes a subset of tenants to misbehave. Provide a concrete diagnostic plan with exact kubectl commands to verify flag propagation to pods, inspect the ConfigMap data and volumes, trigger a zero-downtime canary rollout of the flag change with rollback, and validate that only intended tenants are affected?","answer":"Plan: verify the per-tenant flag changes propagate to pods and roll out safely. Steps: 1) kubectl -n kcsa-prod get cm tenant-flags -o yaml; 2) kubectl rollout restart deployment gateway -n kcsa-prod; ","explanation":"## Why This Is Asked\nTests a practical, low-risk diagnostic workflow for per-tenant config changes in a multi-tenant gateway. It emphasizes ConfigMap propagation, rollout controls, and targeted canary strategies.\n\n## Key Concepts\n- ConfigMap propagation to pods\n- Rolling updates and rollbacks\n- Canary deployments in multi-tenant contexts\n- Per-tenant feature flag behavior validation\n\n## Code Example\n```javascript\n// Example: fetch and print a ConfigMap value\nconst {execSync}=require('child_process');\nconst cm = JSON.parse(execSync(\"kubectl -n kcsa-prod get cm tenant-flags -o json\").toString());\nconsole.log(cm.data);\n```\n\n## Follow-up Questions\n- How would you automate flag health checks across tenants post-rollout?\n- What metrics would you watch during canary to detect regressions?","diagram":"flowchart TD\n  A[ConfigMap updated] --> B[Restart rollout]\n  B --> C[Canary validated]\n  C --> D[Full rollout]\n  D --> E[Monitor metrics]","difficulty":"beginner","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Instacart","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T11:42:42.055Z","createdAt":"2026-01-23T11:42:42.055Z"},{"id":"q-6299","question":"kcsa-prod: A global product-search API behind an Istio service mesh experiences intermittent incorrect results and latency spikes for a subset of tenants after a feature-flag refresh loaded from Redis. Provide a concrete diagnostic plan with exact kubectl/istioctl commands to verify the feature-flag delivery path, inspect Envoy route cache per-tenant, verify Redis key TTLs, and test a zero-downtime canary rollout of the feature flag seed; include how to validate P95 latency stays under 180 ms during rollout?","answer":"End-to-end flag flow check: Redis -> gateway -> Envoy per-tenant route -> service. Commands:\n- kubectl -n kcsa-prod get cm feature-flags -o yaml\n- kubectl -n kcsa-prod exec deploy/redis -- redis-cli G","explanation":"## Why This Is Asked\n\nTests diagnosing a real-world, multi-tenant feature-flag issue in an Istio/KCSA stack, including data path, per-tenant routing, and safe canary rollout under latency SLAs.\n\n## Key Concepts\n\n- Redis-backed feature flags, per-tenant routing, Envoy cache, canary rollouts, Prometheus-based latency checks\n\n## Code Example\n\n```javascript\n// Example per-tenant fetch with header\nfetch('/v1/search?q=foo', {headers: {'X-Tenant':'tenant123'}})\n```\n\n## Follow-up Questions\n\n- How would you detect stale flags across tenants?\n- How would you handle rollback if canary shows SLA violations?","diagram":"flowchart TD\n  A[Client] --> B[Gateway / Istio Ingress]\n  B --> C[Envoy Sidecar (per-tenant route cache)]\n  C --> D[Product-Search Service]\n  D --> E[Redis: feature flags]\n  E --> F[Metrics & Tracing]","difficulty":"intermediate","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Meta","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T18:04:26.046Z","createdAt":"2026-01-23T18:04:26.047Z"},{"id":"q-6399","question":"In kcsa-prod namespace, a real-time analytics API uses Redis streams for per-tenant event ingestion and a Node.js microservice. After a rolling redeploy, data lag and occasional missing events appear under moderate load, with P95 latency above 180 ms. Provide a concrete diagnostic plan with exact kubectl and redis-cli commands to verify the per-tenant pipeline, inspect Redis stream counters and consumer group lag, review the deployment rollout, and implement a safe canary rollout with rollback; include validation that P95 latency stays under 180 ms during rollout?","answer":"kubectl rollout status deploy/analytics-api -n kcsa-prod && kubectl get pods -n kcsa-prod -l app=analytics-api && redis-cli -h REDIS_HOST -p REDIS_PORT XINFO STREAM analytics:tenant:* && redis-cli -h REDIS_HOST -p REDIS_PORT XRANGE analytics:tenant:* - + LIMIT 0 10 && redis-cli -h REDIS_HOST -p REDIS_PORT XPENDING analytics:tenant:* analytics-group","explanation":"## Why This Is Asked\n\nTests practical diagnostic thinking for a real-time multi-tenant pipeline using Redis streams and Kubernetes rollouts, emphasizing per-tenant lag visibility, safe canary rollout, and rollback procedures.\n\n## Key Concepts\n\n- Redis Streams per-tenant counters and consumer group lag monitoring\n- Kubernetes deployment rollouts and canary deployment patterns\n- XINFO, XRANGE, XPENDING commands for stream health diagnostics\n- Latency targets and per-tenant QoS validation during rollouts\n\n## Code Example\n\n```javascript\n// Node.js pseudo-consumer outline for per-tenant Redis stream processing\nconst redis = require('redis');\nconst client = redis.createClient({ host: REDIS_HOST, port: REDIS_PORT });\n\nasync function processTenantEvents(tenantId) {\n  const streamKey = `analytics:tenant:${tenantId}`;\n  const groupName = 'analytics-group';\n  \n  try {\n    // Read pending messages for this tenant\n    const pending = await client.xPending(streamKey, groupName);\n    if (pending > 0) {\n      console.log(`Tenant ${tenantId} has ${pending} pending events`);\n    }\n    \n    // Process new events\n    const messages = await client.xReadGroup(groupName, 'consumer-1', \n      [{ key: streamKey, id: '>' }], { COUNT: 10, BLOCK: 1000 });\n      \n    for (const message of messages) {\n      await processEvent(message);\n      await client.xAck(streamKey, groupName, message.id);\n    }\n  } catch (error) {\n    console.error(`Error processing tenant ${tenantId}:`, error);\n  }\n}\n```","diagram":"flowchart TD\n  A[Tenant Event] --> B[Redis Stream] \n  B --> C[Consumer Group Lag] \n  C --> D[Analytics API] \n  D --> E[P95 Latency Target]","difficulty":"beginner","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Hashicorp"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T05:31:14.312Z","createdAt":"2026-01-23T22:30:18.620Z"},{"id":"q-6567","question":"kcsa-prod exposes a Redis-backed, per-tenant cache layer for pricing. A config change purges the cache for all tenants, but due to a race in the purge logic, several tenants observe stale prices for several minutes under light traffic. Provide a concrete diagnostic plan with exact kubectl commands to verify the cache deployment, inspect Redis keys and TTLs, review the per-tenant cache policy, and implement a zero-downtime rollback of the purge change; include how to validate P95 latency stays under 200 ms and data freshness within 2 minutes during rollout?","answer":"Plan: verify cache deployment, inspect Redis TTL and keys, review purge policy config, and roll out canary changes with rollback. Commands: kubectl rollout status deploy/pricing-cache -n kcsa-prod; ku","explanation":"## Why This Is Asked\nTests knowledge of multi-tenant cache correctness, safe rollout patterns, and practical debugging of Redis-backed caches in Kubernetes.\n\n## Key Concepts\n- Cache invalidation race conditions in multi-tenant systems\n- Redis key TTLs, per-tenant keys, and purge flows\n- Canary rollouts and safe rollback in Kubernetes\n\n## Code Example\n```bash\n# Quick Redis TTL sanity check\nkubectl -n kcsa-prod exec $(kubectl -n kcsa-prod get pod -l app=pricing-redis -o jsonpath='{.items[0].metadata.name}') -- redis-cli TTL pricing:tenant123\n```\n\n## Follow-up Questions\n- How would you verify that the canary rollout doesn’t regress data freshness?\n- How would you instrument a long-running rollback if latency spikes recur?","diagram":"flowchart TD\n  A[Client request] --> B[Pricing API]\n  B --> C{Cache hit?}\n  C -- yes --> D[Return cache]\n  C -- no --> E[Fetch from origin]\n  E --> F[Update cache]\n  F --> C","difficulty":"beginner","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Oracle","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T08:38:12.231Z","createdAt":"2026-01-24T08:38:12.232Z"},{"id":"q-6657","question":"In kcsa-prod, a payments API behind an Istio service mesh with mTLS intermittently returns 502s for a subset of tenants after a policy update adding per-tenant circuit-breaker thresholds. Provide a concrete diagnostic plan with exact kubectl/istioctl commands to inspect EnvoyFilters, per-tenant circuit-breaker settings, destination rules, and cluster load; propose a zero-downtime canary rollout of the circuit-breaker policy and rollback, and how to validate P95 latency stays under 200 ms during rollout?","answer":"Begin by listing Istio policy objects and EnvoyFilters, then inspect per-tenant circuit-breaker thresholds in Envoy (maxConnections, httpMaxPendingRequests) via kubectl -n kcsa-prod get envoyfilters a","explanation":"## Why This Is Asked\nTests real-world debugging of per-tenant circuit-breaker policies in Istio, requiring precise commands to inspect EnvoyFilters, clusters, and routing, plus a safe canary rollout plan.\n\n## Key Concepts\n- Istio EnvoyFilters and per-tenant circuit-breakers\n- DestinationRule and load balancing strategies\n- Canary rollouts with tenant-scoped routing\n- Telemetry signals (P95 latency, error rate)\n\n## Code Example\n```javascript\n// Pseudo-check script outline for per-tenant circuit-breakers\nasync function diagTenantCB(tenantId){ /* query EnvoyFilters, clusters, and RBAC matching */ }\n```\n\n## Follow-up Questions\n- How would you automate rollback in case of a failed canary without data loss?\n- Which metrics and dashboards would you surface to verify stability during rollout?","diagram":"flowchart TD\n  A[Identify problem] --> B[Inspect Istio config]\n  B --> C[Validate per-tenant CBs]\n  C --> D[Canary rollout plan]\n  D --> E[Monitor & rollback]","difficulty":"intermediate","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","LinkedIn","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T11:49:31.197Z","createdAt":"2026-01-24T11:49:31.197Z"},{"id":"q-6698","question":"kcsa-prod hosts a payments API behind Istio with OPA Gatekeeper enforcing ABAC. A policy update adds tenant-specific scopes; under load, a subset of tenants gets 403s despite valid tokens. Provide a concrete diagnostic plan with exact kubectl/istioctl commands to verify Gatekeeper constraints, per-tenant scopes, and Envoy config, plus a zero-downtime canary rollout path to revert the policy while keeping P95 latency under 150 ms?","answer":"Plan: 1) kubectl get constraints, ConstraintTemplates; 2) kubectl get policy kcsa-authz -n kcsa-prod -o yaml; 3) istioctl proxy-config log payment-api-pod -n kcsa-prod; 4) curl -H 'Authorization: Bear","explanation":"## Why This Is Asked\nTests concrete ABAC/OPA Gatekeeper debugging in a real multi-tenant setup with precise commands and rollout discipline.\n\n## Key Concepts\n- OPA Gatekeeper constraints\n- ABAC policy evaluation and per-tenant scopes\n- Envoy/sidecar observability in Istio\n- Canary rollouts with safe rollback and latency monitoring\n\n## Code Example\n```rego\npackage kcsa.authz\ndefault allow = false\nallow {\n  input.tenant == t\n  some t\n  input.scopes[_] == \"payments.write\"\n}\n```\n\n## Follow-up Questions\n- How would you distinguish tenant misconfig from a global policy error?\n- Which metrics are critical to monitor during rollback to ensure P95 stays below target?","diagram":null,"difficulty":"intermediate","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","OpenAI","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T13:53:00.739Z","createdAt":"2026-01-24T13:53:00.739Z"},{"id":"q-6752","question":"kcsa-prod: A payments API behind an Istio service mesh uses a new weighted routing policy to roll out backend version v2 for a subset of tenants. During rollout, tenants outside the target set experience 502s and elevated latency. Provide a concrete diagnostic plan with exact kubectl/istioctl commands to verify VirtualService weights, inspect Envoy routes and per-tenant headers, validate DestinationRule subsets, and implement a zero-downtime canary rollout with rollback; ensure P95 latency stays under 150 ms during rollout?","answer":"Check VirtualService weights and tenant-specific routes, DestinationRule subsets, and per-tenant headers; inspect envoy routes and ingress gateway logs; verify that tenants hit the intended subset usi","explanation":"## Why This Is Asked\nTests real-world Istio routing and multi-tenant rollout troubleshooting under canary conditions.\n\n## Key Concepts\n- Istio VirtualService weighting and tenant scoping\n- DestinationRule subsets and subset routing\n- Envoy admin/router config inspection\n- Canary rollout patterns with zero-downtime rollback\n- Latency/throughput observability during rollout\n\n## Code Example\n```bash\n# Inspect current routing weights\nkubectl -n kcsa-prod get virtualservice payments -o yaml\n\n# Show envoy routes for ingress gateway proxy\nistioctl proxy-config routes deploy/istio-ingressgateway-<pod> -n kcsa-prod -o json\n\n# Confirm destination rules and subsets\nkubectl -n kcsa-prod get destinationrule payments -o yaml\n```\n\n## Follow-up Questions\n- How would you adjust the rollout if a single tenant deviates from the observed pattern?\n- What metrics and alerts would you use to trigger rollback and how would you validate post-rollback stability?","diagram":"flowchart TD\n  A[Client Request] --> B[Ingress Gateway]\n  B --> C{VirtualService routing}\n  C -->|V2| D[V2 backend]\n  C -->|V1| E[V1 backend]\n  D & E --> F[Upstream service]\n  F --> G[Response to Client]","difficulty":"intermediate","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T15:45:55.474Z","createdAt":"2026-01-24T15:45:55.474Z"},{"id":"q-6909","question":"kcsa-prod namespace hosts a checkout-api Deployment behind a basic Ingress. A feature flag enableDiscount lives in a ConfigMap and should be picked up by pods at startup. After a deploy, some pods enable it and some don't, creating inconsistent checkout totals under load. Outline a concrete diagnostic plan with exact kubectl commands to verify ConfigMap versions mounted in pods, read the flag inside containers, and perform a zero-downtime rollout with rollback; show how to verify P95 latency stays under 180 ms during rollout?","answer":"Inspect ConfigMap versions and pod state; verify mapping between mounted data and in-container flag. Steps: kubectl get cm checkout-flags -n kcsa-prod -o yaml; kubectl get pods -n kcsa-prod -l app=checkout-api -o wide; kubectl describe pod <pod-name> -n kcsa-prod | grep -A 10 'Mounts'; kubectl exec <pod-name> -n kcsa-prod -- cat /etc/config/flags.json | jq '.enableDiscount'; kubectl rollout status deployment/checkout-api -n kcsa-prod; kubectl rollout undo deployment/checkout-api -n kcsa-prod; kubectl top pods -n kcsa-prod -l app=checkout-api --use-protocol-buffers; kubectl get --raw \"/apis/metrics.k8s.io/v1beta1/namespaces/kcsa-prod/pods\" | jq '.items[] | select(.metadata.labels.app==\"checkout-api\") | .containers[0].usage | {cpu, memory}'","explanation":"## Why This Is Asked\nDiagnoses ConfigMap-driven feature flag drift across pods, a common operational issue that demonstrates understanding of Kubernetes configuration propagation and rollout strategies.\n\n## Key Concepts\n- ConfigMaps as runtime configuration sources\n- Rolling deployment strategies with zero downtime\n- Pod lifecycle and configuration mounting\n- Observability and performance validation during rollouts\n- Rollback procedures for configuration issues\n\n## Code Example\n```bash\n# Diagnostic commands provided in answer\n# No additional code required\n```\n\n## Follow-up Questions\n- How would","diagram":"flowchart TD\n  A[ConfigMap: checkout-flags] --> B[Mounted in pods]\n  B --> C{Flag value loaded}\n  C --> D[Discount path on/off per pod]\n  D --> E[Potential inconsistency]\n","difficulty":"beginner","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","PayPal","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T05:56:17.252Z","createdAt":"2026-01-24T22:32:56.024Z"},{"id":"q-7080","question":"kcsa-prod: A service mesh behind Istio mTLS uses SDS-based TLS certs rotated from a central CA. During rotation, TLS handshakes intermittently fail for a subset of pods, causing 503s under load. Provide a concrete diagnostic plan with exact kubectl and istioctl commands to verify SDS propagation, inspect Envoy TLS contexts, confirm certificate chain validity across pods, and implement a zero-downtime canary rollout of the rotation with rollback; include how to validate P95 latency stays under 200 ms during rollout?","answer":"Investigate SDS propagation and TLS handshakes. Check that all pods receive the new CA cert and that Envoy TLS contexts reflect it. Commands:\n- kubectl get pods -n kcsa-prod\n- kubectl exec -n kcsa-pro","explanation":"## Why This Is Asked\nTests ability to diagnose TLS/CA rotation in a service mesh, reason about SDS propagation, and execute a controlled canary rollout without downtime. Emphasizes correctness, observability, and rollback discipline.\n\n## Key Concepts\n- SDS-based TLS and CA rotation in Istio\n- Envoy TLS context inspection\n- Canary rollouts and rollback in Kubernetes\n- Latency targets and failure-mode detection\n\n## Code Example\n```javascript\n// Example CLI checks used during diagnosis\n``` \n\n## Follow-up Questions\n- How would you automate this diagnostic plan as a runbook?\n- What metrics indicate rotation success beyond P95 latency?","diagram":null,"difficulty":"advanced","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T08:42:14.049Z","createdAt":"2026-01-25T08:42:14.049Z"},{"id":"q-7207","question":"kcsa-prod: A multi-tenant API gateway fronts the billing service. A new per-tenant rate-limit policy is deployed via Istio's EnvoyFilter using Redis counters. In burst traffic, a subset of tenants see sporadic 429s while others are fine. Provide a concrete diagnostic plan with exact kubectl/istioctl commands to verify per-tenant Redis keys, inspect EnvoyFilter hashes and per-tenant quotas, and implement a zero-downtime canary rollout of the policy changes with rollback; include how to validate P95 latency stays under 180 ms during rollout?","answer":"Diagnose per-tenant quota drift due to TTL changes and Redis counters. Verify Redis keys per tenant, TTLs, and consumption counts; inspect EnvoyFilter hashes and per-tenant quotas; compare baseline vs","explanation":"## Why This Is Asked\n\nTests the ability to diagnose complex, multi-system production issues involving per-tenant rate limiting, cross-service config propagation, and safe rollouts. It requires precise kubectl/istioctl usage, Redis introspection, and a solid rollback plan under real-time traffic.\n\n## Key Concepts\n\n- Per-tenant quotas and Redis-backed token buckets\n- EnvoyFilter configuration and hash stability\n- Canary rollout with zero downtime\n- Latency surveillance (P95) during changes\n\n## Code Example\n\n```javascript\n// Pseudo check of quota TTLs per tenant\nconst tenants = ['t1','t2'];\nfor (const t of tenants) {\n  console.log(await redis.ttl(`quota:${t}`), await redis.get(`quota:${t}:count`))\n}\n```\n\n## Follow-up Questions\n\n- How would you scale this to multiple Redis clusters with TTL drift boundaries?\n- What metrics would you collect to prove the rollout is non-regressive for all tenants?","diagram":null,"difficulty":"advanced","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Snowflake","Tesla","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T13:47:08.567Z","createdAt":"2026-01-25T13:47:08.567Z"},{"id":"q-7244","question":"kcsa-prod: An advanced multi-tenant API gateway behind Istio is rolling out a per-tenant canary using header-based routing to a shadow deployment with an extra rate limiter. Under load, intermittent 502s appear. Outline a concrete diagnostic plan with exact kubectl/istioctl commands to inspect DestinationRule and VirtualService, verify per-tenant routing and weights, check the rate limiter state in Redis, inspect Envoy proxies, and craft a zero-downtime rollout with rollback; include how to validate P95 latency stays under 150 ms during rollout?","answer":"Validate Istio routing and per-tenant state first. Use istioctl analyze -n kcsa-prod and kubectl get virtualservice -n kcsa-prod; kubectl get destinationrule -n kcsa-prod. Check Envoy proxies with kub","explanation":"## Why This Is Asked\nTests ability to reason about per-tenant routing under canary, plus real-world debugging of Istio configs, rate limiter state, and zero-downtime rollout.\n\n## Key Concepts\n- Istio routing, VirtualService, DestinationRule\n- Canary rollout safety, rollback\n- Per-tenant state in Redis rate limiter\n- Observability: logs, metrics, tracing\n\n## Code Example\n```javascript\n// Placeholder: not required for the question\n```\n\n## Follow-up Questions\n- How would you automate this diagnostic plan as a runbook?\n- What metrics would you alert on during rollout?","diagram":null,"difficulty":"advanced","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","PayPal","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T14:59:15.942Z","createdAt":"2026-01-25T14:59:15.942Z"},{"id":"q-7350","question":"kcsa-prod: A payments-api Deployment behind Istio uses a three-layer cache (in-process, Redis, CDN). Under sudden traffic, Redis-backed latency spikes and some tenants time out. Provide a concrete diagnostic plan with exact kubectl/redis-cli commands to inspect Redis memory usage, eviction policy, slowlog, per-tenant keyspace, and cache hit rate; verify Redis cluster health; and implement a zero-downtime canary rollout that bypasses Redis for 20% of tenants via a feature flag, validating P95 latency stays under 180 ms during rollout?","answer":"Plan targets Redis saturation and cache-hit fairness. Verify memory pressure, eviction policy, slowlog, and per-tenant hit rate. Commands: kubectl -n kcsa-prod get deploy payments-api; kubectl -n kcsa","explanation":"## Why This Is Asked\nTests practical debugging of a multi-layer cache under load and safe rollout strategies.\n\n## Key Concepts\n- Cache hierarchy and eviction impact\n- Per-tenant isolation and hit-rate analysis\n- Redis observability: memory, slowlog, stats\n- Canary rollouts and rollback safety\n\n## Code Example\n```bash\n# Example commands used in debugging (illustrative)\nkubectl -n kcsa-prod get deploy payments-api\nkubectl -n kcsa-prod get pods\nkubectl -n kcsa-prod exec -it redis-0 -- redis-cli INFO memory\n```\n\n## Follow-up Questions\n- How would you measure per-tenant cache-hit rate in Prometheus?\n- How would you handle data freshness when bypassing Redis for a subset of tenants?","diagram":null,"difficulty":"intermediate","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Meta","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T19:40:41.553Z","createdAt":"2026-01-25T19:40:41.554Z"},{"id":"q-7438","question":"kcsa-prod: A gateway-api fronts a multi-tenant data-processor. A new per-tenant feature flag 'enableBatch' is read from a ConfigMap and propagated to pods via an InitContainer. Under peak load, some tenants experience delayed batch processing and higher P95 latency. Design a concrete diagnostic plan with exact kubectl/istioctl commands to verify InitContainer completion, check the ConfigMap version mounted, read the flag at runtime, and implement a zero-downtime canary rollout with rollback; include how to validate P95 latency stays under 200 ms during rollout?","answer":"Plan: Verify InitContainer completion across all gateway-api pods, confirm ConfigMap version consistency, validate ENABLE_BATCH flag runtime values, execute a 10% canary rollout with incremental traffic routing, monitor P95 latency adherence to 200ms SLA, and trigger automated rollback if thresholds are exceeded.","explanation":"## Why This Is Asked\nTests startup orchestration and configuration propagation in multi-tenant environments, requiring canary deployment strategies with SLA-driven rollback capabilities.\n\n## Key Concepts\n- InitContainer execution sequencing\n- ConfigMap mounting and versioning strategies\n- Per-pod environment variable propagation\n- Canary deployment and automated rollback procedures\n- P95 latency monitoring and SLA enforcement\n\n## Code Example\n```bash\nkubectl get pods -n kcsa-prod -l app=gateway-api\n```\n\n## Follow-up Questions\n- How would you implement automated per-tenant flag validation?\n- What monitoring tools would you use for real-time P95 latency tracking?\n- How would you handle ConfigMap updates during active canary deployments?","diagram":"flowchart TD\n  A[InitContainer completes] --> B[ConfigMap mounted]\n  B --> C[Runtime flag set]\n  C --> D[Traffic spikes]\n  D --> E[P95 latency < 200 ms during rollout]","difficulty":"advanced","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Hashicorp","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T05:57:58.279Z","createdAt":"2026-01-25T23:35:02.747Z"},{"id":"q-7520","question":"kcsa-prod runs an ingest-processor StatefulSet behind Istio. A rollout upgrades the image and adds an OpenTelemetry sidecar; under peak load, events lag and occasional duplicates emerge due to non-idempotent writes. Design a zero-downtime canary rollout with rollback, and include exact kubectl/istioctl steps to verify revision, sidecar status, Kafka lag, and idempotency; ensure P95 latency stays under 200 ms during rollout?","answer":"Design a zero-downtime canary rollout that upgrades ingest-processor image and adds the OpenTelemetry sidecar without duplicates. Use Istio to split 10-20% traffic to the new revision; verify Stateful","explanation":"## Why This Is Asked\nTests ability to orchestrate safe rollouts in a streaming data path while preserving data correctness, observability, and SLA. Requires practical use of Kubernetes, Istio, and streaming semantics.\n\n## Key Concepts\n- Canary rollouts for StatefulSets with traffic shifting\n- Sidecar injection status and its observability impact\n- Exactly-once processing and idempotency in streaming ingests\n- End-to-end latency monitoring and rollback triggers\n\n## Code Example\n```bash\n# verify canary rollout status\nkubectl rollout status statefulset/ingest-processor\n\n# inspect Istio routing for canary weights\nkubectl get virtualservice ingest-processor -o yaml\n\n# Kafka lag check for ingest-processor group\nkafka-consumer-groups.sh --bootstrap-server kafka:9092 --describe --group ingest-processor\n```\n\n## Follow-up Questions\n- How would you validate idempotency under replay scenarios? \n- What metrics and alerting would you add to detect rollout regressions early?","diagram":null,"difficulty":"advanced","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","Tesla","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T05:45:06.059Z","createdAt":"2026-01-26T05:45:06.059Z"},{"id":"q-7799","question":"kcsa-prod: A per-tenant gateway uses an Istio WASM-based rate limiter with per-tenant Redis buckets. After a deployment, a subset of tenants sees sporadic 429s under moderate load, while others are fine. Provide a concrete diagnostic plan with exact kubectl/istioctl commands to inspect the WASM module and its version, verify per-tenant Redis counters, dump Envoy/WASM-driven route config, and implement a zero-downtime canary rollout of the WASM extension with rollback; include how to validate P95 latency stays under 160 ms during rollout?","answer":"Reproduce under load; verify WASM module version/sha on the gateway pod; dump Envoy/WASM config (istioctl proxy-config clusters/routes <pod> -n kcsa-prod); inspect per-tenant counters in Redis (redis-","explanation":"## Why This Is Asked\nTests ability to diagnose production issues involving WASM-based rate limiting, per-tenant isolation, and safe rollout.\n\n## Key Concepts\n- Istio WASM extension management\n- Envoy proxy config dumps and per-tenant routing\n- Redis per-tenant state\n- Canary and rollback strategies\n- Observability and latency targets (P95)\n\n## Code Example\n```bash\n# Example commands for exploration\nkubectl -n kcsa-prod get pods -l app=gateway -o wide\nistioctl proxy-config clusters <gateway-pod> -n kcsa-prod\n```\n\n## Follow-up Questions\n- How would you mitigate impact during the canary rollout and ensure rollback is instantaneous?\n- Which metrics would you add to Prometheus to detect drift in per-tenant rate limits during rollout?\n","diagram":null,"difficulty":"advanced","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","NVIDIA","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T18:07:17.121Z","createdAt":"2026-01-26T18:07:17.121Z"},{"id":"q-7939","question":"kcsa-prod hosts a frontend API behind Nginx Ingress. After rotating the TLS secret used by the Ingress, a subset of pods returns 502/503 under moderate traffic. Provide a concrete diagnostic plan with exact kubectl commands to verify the Ingress TLS secret binding, inspect the Ingress resource, confirm the certificate matches the hostname, trigger a zero-downtime rollout to reload the secret, and validate P95 latency stays under 180 ms during rollout?","answer":"Steps: 1) kubectl get secret tls-prod -n kcsa-prod -o json; 2) kubectl describe secret tls-prod -n kcsa-prod; 3) kubectl get ingress frontend-ingress -n kcsa-prod -o yaml; 4) kubectl rollout restart d","explanation":"## Why This Is Asked\n\nTests ability to diagnose TLS/Ingreess secret impacts and perform zero-downtime rollouts. It checks kubectl proficiency, ingress/secret linkage, and basic latency verification under load.\n\n## Key Concepts\n\n- Kubernetes secrets and Ingress TLS bindings\n- Rollout strategies and rollback\n- Latency verification with Prometheus/metrics\n\n## Code Example\n\n```javascript\n// no code\n```\n\n## Follow-up Questions\n\n- How would you automate secret rotation with a healthcheck gate?\n- How would you verify cross-namespace secret exposure is safe?","diagram":null,"difficulty":"beginner","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T23:52:15.930Z","createdAt":"2026-01-26T23:52:15.930Z"},{"id":"q-7964","question":"kcsa-prod: After a per-tenant NetworkPolicy update restricting egress to external payment-processor API endpoints, a subset of tenants experiences intermittent 502s. Propose a concrete diagnostic plan using kubectl/istioctl to verify netpols, inspect CNI logs, test per-tenant egress reachability from sidecars, and implement a zero-downtime canary rollout of the policy with rollback; include how to validate P95 latency remains under 200 ms during rollout?","answer":"Identify affected tenants; audit NetPolicies across namespaces with `kubectl get netpol -A` and `kubectl describe netpol -n TENANT`. Test per-tenant egress from sidecars using `kubectl run test-tenant1 -n TENANT --image=curlimages/curl --command -- curl -v https://payment-processor.example.com`. Inspect CNI logs via `kubectl logs -n kube-system -l k8s-app=calico-node -c calico-node --tail=1000`. Validate Istio sidecar routing with `istioctl proxy-config routes deployment/app -n TENANT`. Implement zero-downtime canary rollout by applying NetworkPolicy with `matchLabels` to target 10% of pods, monitor with `kubectl get pods -n TENANT -l canary=true`, and scale up gradually. Validate P95 latency during rollout using `istioctl proxy-config clusters deployment/app -n TENANT | jq '.clusters[] | select(.name | contains(\"payment-processor\")) | .stats'` and Prometheus queries: `histogram_quantile(0.95, sum(rate(istio_request_duration_seconds_bucket{destination_service=\"payment-processor\"}[5m])) by (pod, tenant)) < 0.2`.","explanation":"## Why This Is Asked\n\nDiagnosing egress policy issues in a multi-tenant, Istio-enabled environment requires concrete steps to verify network policies, CNI behavior, and per-tenant routing. The question tests practical debugging fluency across Kubernetes networking, service mesh, and rollout safety.\n\n## Key Concepts\n\n- Kubernetes NetworkPolicy and per-tenant scoping\n- CNI logs and egress handling\n- Istio sidecar routing and proxy-config inspection\n- Canary rollouts with safe rollback and latency SLO validation\n\n## Code Example\n\n```bash\n# Representative commands for diagnostic workflow\nkubectl get netpol -A | grep payment-processor\nkubectl describe netpol -n tenant-1 restrict-egress-payment\nkubectl exec -n tenant-1 deployment/app -- curl -v https://payment-processor.example.com\nistioctl proxy-config routes deployment/app -n tenant-1\n```\n\nThe answer demonstrates systematic troubleshooting from policy verification through production-safe rollout with performance validation.","diagram":null,"difficulty":"intermediate","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T04:09:00.592Z","createdAt":"2026-01-27T02:46:35.087Z"},{"id":"q-836","question":"You are analyzing a web app's login events, captured as an array of objects: { userId: string, ts: string (ISO 8601) }. Write a JavaScript function that returns the top 3 users by login count in the past 24 hours. Tie-break with alphabetical userId. Provide a concise implementation and explain its time complexity?","answer":"Use a rolling 24h window: parse ts to ms, keep per-user counts in a Map, filter to now - t <= 24h, sort by count desc then userId asc, return top 3. Time complexity O(n log m) where m is distinct user","explanation":"## Why This Is Asked\nTests practical data processing in JavaScript: time-window filtering, per-user aggregation, and deterministic tie-breaking.\n\n## Key Concepts\n- Time-window filtering\n- Hash map for counts\n- Stable sorting with tie-breakers\n- Input validation and edge cases\n\n## Code Example\n\n```javascript\nfunction topUsers(events) {\n  const now = Date.now();\n  const window = 24 * 60 * 60 * 1000;\n  const counts = new Map();\n  for (const e of events) {\n    const t = Date.parse(e.ts);\n    if (Number.isNaN(t)) continue;\n    if (now - t <= window) {\n      counts.set(e.userId, (counts.get(e.userId) || 0) + 1);\n    }\n  }\n  const arr = Array.from(counts.entries());\n  arr.sort((a, b) => b[1] - a[1] || a[0].localeCompare(b[0]));\n  return arr.slice(0, 3).map(([id]) => id);\n}\n```\n\n## Follow-up Questions\n- How would you adapt this for streaming data?\n- How would you test with synthetic data to cover edge cases?","diagram":"flowchart TD\n  A[Events] --> B{Filter 24h}\n  B --> C[Count per User]\n  C --> D[Sort by Count, then ID]\n  D --> E[Top 3 Results]","difficulty":"beginner","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:21:30.077Z","createdAt":"2026-01-12T13:21:30.077Z"},{"id":"q-869","question":"In a high-traffic search suggestions feature, implement a Node.js module that debounces input by 300ms and caches per-query results in memory. Show cache invalidation and handle concurrent requests for the same key without duplicating work. Provide a compact, testable example with usage?","answer":"Implement an in-memory Map cache keyed by query with fields ts, data, and inFlight. Debounce input by 300ms; if cache exists and not expired (TTL 60s), return data; if inFlight exists for the key, awa","explanation":"This tests practical skills in debouncing, TTL caching, and in-flight deduplication under concurrent access.","diagram":null,"difficulty":"beginner","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Lyft","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:50:03.410Z","createdAt":"2026-01-12T13:50:03.410Z"},{"id":"q-912","question":"**KCSA Beginner Question: Inventory Pagination** Scenario: In a warehouse app, design a minimal endpoint GET /items?limit=&offset= that returns items ordered by id, supports pagination, and scales with 10k+ rows. Explain data model, indexing, and error handling. How would you implement a function to validate and apply LIMIT/OFFSET in code, ensuring correctness?","answer":"I’d implement GET /items?limit=&offset= with items ordered by id. Use an index on id and query: SELECT id, name, qty FROM items ORDER BY id ASC LIMIT $limit OFFSET $offset; return {total, items}. Vali","explanation":"## Why This Is Asked\nThe goal is to assess practical pagination design and data modeling skills on a tiny API layer, including correctness and testability.\n\n## Key Concepts\n- Pagination parameters validation\n- Indexed queries and ORDER BY performance\n- Edge-case handling and test coverage\n\n## Code Example\n\n```javascript\nfunction paginate(items, limit, offset) {\n  if (limit <= 0 || offset < 0) throw new Error(\"Invalid pagination params\");\n  const total = items.length;\n  const paged = items.slice(offset, offset + limit);\n  return { total, items: paged };\n}\n```\n\n## Follow-up Questions\n- How would you test pagination with 10k+ records and caching?\n- How would you adapt logic for distributed databases or replicas?","diagram":null,"difficulty":"beginner","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Apple"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T15:24:39.634Z","createdAt":"2026-01-12T15:24:39.634Z"},{"id":"q-947","question":"You're building a fintech API used by wallets and partner services to transfer funds; describe a secure, scalable auth and data-protection design for REST endpoints, including per-partner API keys, short-lived access tokens with rotating signing keys, mTLS between services, replay protection, and observability to detect abuse, plus failure modes and trade-offs?","answer":"Adopt OAuth2-like flows with PKCE for public clients, issue short-lived JWTs signed by rotating keys in an HSM-backed keystore, enforce mutual TLS between services, bind tokens to a client certificate","explanation":"## Why This Is Asked\n\nEvaluates practical fintech security decisions: key management, rotating keys, PKCE, mTLS, replay protection, and observability trade-offs.\n\n## Key Concepts\n\n- OAuth2-like flows with PKCE\n- JWT rotation and key management (HSM backing)\n- mTLS and token binding\n- Replay protection (nonce, jti)\n- Observability, rate limits, and revocation\n\n## Code Example\n\n```javascript\n// Token verification sketch\nfunction verifyToken(token, keystore) {\n  const { header } = parseJwt(token);\n  const key = keystore.getKey(header.kid);\n  const payload = verifyJwt(token, key);\n  if (!payload) return false;\n  if (payload.exp * 1000 < Date.now()) return false;\n  return payload.aud.includes(\"api\");\n}\n```\n\n## Follow-up Questions\n\n- How would you test key rotation without downtime?\n- How would you detect token abuse and adapt rate limits?","diagram":"flowchart TD\n  A[Client Request] --> B[Validate API Key]\n  B --> C[Authenticate & Authorize]\n  C --> D[Issue JWT with kid]\n  D --> E[Mutual TLS between services]\n  E --> F[Audit & Telemetry]\n  F --> G[Key Rotation Schedule]","difficulty":"intermediate","tags":["kcsa"],"channel":"kcsa","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","PayPal","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T16:36:34.150Z","createdAt":"2026-01-12T16:36:34.150Z"}],"subChannels":["general"],"companies":["Adobe","Airbnb","Amazon","Anthropic","Apple","Bloomberg","Citadel","Cloudflare","Coinbase","Databricks","Discord","DoorDash","Goldman Sachs","Google","Hashicorp","Hugging Face","IBM","Instacart","LinkedIn","Lyft","Meta","Microsoft","MongoDB","NVIDIA","Netflix","OpenAI","Oracle","PayPal","Plaid","Robinhood","Salesforce","Scale Ai","Slack","Snap","Snowflake","Square","Stripe","Tesla","Twitter","Two Sigma","Uber","Zoom"],"stats":{"total":86,"beginner":31,"intermediate":30,"advanced":25,"newThisWeek":35}}