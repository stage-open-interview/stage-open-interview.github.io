{"questions":[{"id":"q-1086","question":"Design a Snowflake CDC pattern for an SCD2 customer_dim using a stream on the source table. Explain how to implement an idempotent upsert via MERGE, how deletes are represented, and how to handle late-arriving changes to preserve history?","answer":"Use Snowflake Streams on the source table and a MERGE into the SCD2 target to achieve idempotent CDC. Use src.metadata$action to distinguish INSERT, UPDATE, DELETE. For deletes, set end_date on the ex","explanation":"## Why This Is Asked\n\nDemonstrates pragmatic CDC design in Snowflake, including stream-driven MERGE, and SCD2 history guarantees.\n\n## Key Concepts\n\n- Snowflake Streams and metadata$action to detect DML\n- MERGE pattern for upserts and tombstone deletes\n- SCD Type 2 history with start_date and end_date\n- Handling late-arriving data and idempotence\n\n## Code Example\n\n```javascript\n// Snowflake MERGE with STREAM snippet\nconst mergeSql = `\nMERGE INTO customer_dim AS target\nUSING customers_stream AS src\nON target.customer_id = src.customer_id\nWHEN MATCHED AND src.metadata$action = 'DELETE' THEN DELETE\nWHEN MATCHED THEN UPDATE SET\n  name = src.name, email = src.email, ...\nWHEN NOT MATCHED THEN INSERT (customer_id, name, email, start_date)\nVALUES (src.customer_id, src.name, src.email, src.metadata$start_time);\n`;\n```\n\n## Follow-up Questions\n\n- How would you test this CDC pipeline end-to-end? \n- How do you handle conflicts if key updates arrive out of order?","diagram":null,"difficulty":"intermediate","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T22:20:06.609Z","createdAt":"2026-01-12T22:20:06.609Z"},{"id":"q-1152","question":"Design a least-privilege access layer in Snowflake for a multi-tenant data lake spanning five domains (marketing, sales, finance, analytics, operations). Describe how you would implement ROW ACCESS POLICIES and MASKING POLICIES on a payments table (columns: id, customer_id, amount, card_number, region) to restrict data by region and user role, and how you would audit access?","answer":"Define domain roles and a service role; apply a masking policy on card_number to reveal full value only to ADMIN_ROLE and mask others; implement a row access policy using a user-context REGION value t","explanation":"## Why This Is Asked\nThis question verifies practical application of Snowflake security primitives (masking, RLS, views) in a multi-tenant context, including auditing and governance considerations.\n\n## Key Concepts\n- ROW ACCESS POLICY and MASKING POLICY\n- USER_CONTEXT/SESSION_CONTEXT for dynamic access decisions\n- SECURE_VIEW vs direct table access for enforcement\n- Auditing via QUERY_TAGs and ACCESS_HISTORY\n\n## Code Example\n```sql\n-- Masking policy: full card only for admins\nCREATE MASKING POLICY mask_card_number AS (val STRING) RETURNS STRING ->\n  CASE WHEN CURRENT_ROLE() = 'ADMIN_ROLE' THEN val\n       ELSE CONCAT('XXXX-XXXX-XXXX-', RIGHT(val, 4))\n  END;\n\nALTER TABLE payments MODIFY COLUMN card_number SET MASKING POLICY mask_card_number;\n\n-- Row access policy by region from user-context (conceptual)\nCREATE OR REPLACE ROW ACCESS POLICY region_rls AS (region STRING) RETURNS BOOLEAN ->\n  region = CURRENT_SESSION_REGION();\n\n-- Secure view to surface allowed columns\nCREATE VIEW payments_secure AS\nSELECT id, customer_id, amount, region\nFROM payments;\n``` \n\n## Follow-up Questions\n- How would you test masking and region filtering for multiple roles? \n- What are the performance implications of masking and RLS at scale, and how would you mitigate them?","diagram":null,"difficulty":"advanced","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Databricks","Lyft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T01:36:23.332Z","createdAt":"2026-01-13T01:36:23.332Z"},{"id":"q-1239","question":"You’re building a beginner Snowflake task: a large SALES table is routinely queried with date-range filters. Propose a minimal clustering solution to improve pruning. Include exact commands to add a single clustering key on SALE_DATE, how to monitor its effectiveness, and how to decide if reclustering is needed. Keep changes focused and explain validation steps with a simple test?","answer":"ALTER TABLE SALES CLUSTER BY (SALE_DATE);\n-- Inspect clustering depth\nSELECT SYSTEM$CLUSTERING_INFORMATION('MY_DB.MY_SCHEMA.SALES');\n-- If depth indicates suboptimal pruning, recluster\nALTER TABLE SAL","explanation":"## Why This Is Asked\nTests practical use of manual clustering for query pruning, plus pragmatic validation and cost awareness for beginners.\n\n## Key Concepts\n- Clustering keys: SALE_DATE to aid pruning\n- SYSTEM$CLUSTERING_INFORMATION: evaluate clustering depth\n- Reclustering threshold: avoid over-clustering; measure impact with a test query\n\n## Code Example\n```javascript\nALTER TABLE SALES CLUSTER BY (SALE_DATE);\nSELECT SYSTEM$CLUSTERING_INFORMATION('MY_DB.MY_SCHEMA.SALES');\nALTER TABLE SALES RECLUSTER;\n-- Simple validation query (date range) to compare performance pre/post\n```\n\n## Follow-up Questions\n- How would you adjust clustering if the query pattern includes filters on region in addition to date?\n- What metrics would you monitor over time to assess clustering maintenance needs?","diagram":"flowchart TD\n  A[SALES Table] --> B[Add clustering key: SALE_DATE]\n  B --> C[Monitor: SYSTEM$CLUSTERING_INFORMATION]\n  C --> D[Evaluate clustering depth]\n  D --> E[Option: RECLUSTER]","difficulty":"beginner","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T06:34:44.338Z","createdAt":"2026-01-13T06:34:44.338Z"},{"id":"q-868","question":"Design a cross-account data-sharing solution in Snowflake for a multinational fintech requiring regional affiliates to access a shared dataset containing PII. How would you implement Secure Data Sharing, dynamic data masking, region-specific RBAC, and auditable access, while enabling updates to masking policies without breaking consumer queries?","answer":"Use a provider-to-consumer Secure Data Share, attach region-specific masking policies to PII columns, and grant region roles in each consumer account. Admins with higher roles see full data; regular r","explanation":"## Why This Is Asked\nThis question tests practical data-sharing governance at scale—cross-account sharing, dynamic masking, RBAC, and auditable access, plus policy evolution without breaking consumers.\n\n## Key Concepts\n- Secure Data Sharing architecture across provider and consumer accounts\n- Dynamic Data Masking policies with role-based access\n- Cross-account RBAC mapping and masking policy scoping\n- Auditing using Snowflake ACCOUNT_USAGE and ACCESS_HISTORY\n- Policy versioning and non-breaking updates\n\n## Code Example\n```sql\n-- Masking policy\nCREATE MASKING POLICY pii_mask AS (VAL STRING) RETURNS STRING ->\nCASE WHEN CURRENT_ROLE() IN ('REGION_A_ANALYST','REGION_B_ADMIN') THEN VAL ELSE 'REDACTED' END;\n\n-- Provider shares\nCREATE SHARE fintech_share;\nGRANT USAGE ON DATABASE fintech_db TO SHARE fintech_share;\nGRANT SELECT ON ALL TABLES IN SCHEMA fintech_db.public TO SHARE fintech_share;\n\n-- Consumer startup\nCREATE DATABASE region_a_db FROM SHARE provider.fintech_share;\n```\n\n## Follow-up Questions\n- How would you test policy updates in a greenfield environment?\n- How would you monitor leakage risk and performance impact of masking at scale?","diagram":"flowchart TD\n  A[Provider] --> B[Secure Data Share]\n  B --> C[Consumer Region A]\n  B --> D[Consumer Region B]\n  C --> E[Dynamic Masking Applied]\n  D --> F[Admins See Full Data]","difficulty":"advanced","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:48:59.902Z","createdAt":"2026-01-12T13:48:59.902Z"},{"id":"q-981","question":"How would you configure Snowflake so regional analysts can run ad-hoc queries on a shared dataset while ensuring isolation, predictable performance, and cost control? Include concrete settings for: (a) warehouse topology (min/max clusters, auto-suspend/resume), (b) RBAC (roles, grant scopes on databases/schemas/tables), (c) cost governance (resource monitors and credit caps), (d) a sample GRANT script giving REGIONAL_ANALYST access to only SALES and EVENTS schemas, and (e) auditing and reproducibility considerations?","answer":"Configure a dedicated ANALYST_WAREHOUSE as a multi-cluster warehouse with MIN 1, MAX 4, AUTO_SUSPEND 300, AUTO_RESUME ON. Create a REGIONAL_ANALYST role with USAGE on the DB and SELECT on SALES and EV","explanation":"## Why This Is Asked\nTests practical Snowflake fundamentals: per-region access, isolation, cost governance, and auditing in a beginner-friendly way.\n\n## Key Concepts\n- Multi-cluster warehouses and auto-suspend/resume\n- RBAC with granular schema/table permissions\n- Resource monitors and credit caps for cost control\n- Auditing via query tags and reproducibility\n- Safe read-only ad-hoc access to shared data\n\n## Code Example\n```javascript\n-- Grants (SQL flavored in a JS code block due to formatting)\nCREATE ROLE REGIONAL_ANALYST;\nGRANT USAGE ON DATABASE FINTECH_DEMO TO ROLE REGIONAL_ANALYST;\nGRANT USAGE ON SCHEMA FINTECH_DEMO.SALES TO ROLE REGIONAL_ANALYST;\nGRANT SELECT ON ALL TABLES IN SCHEMA FINTECH_DEMO.SALES TO ROLE REGIONAL_ANALYST;\nGRANT USAGE ON SCHEMA FINTECH_DEMO.EVENTS TO ROLE REGIONAL_ANALYST;\nGRANT SELECT ON ALL TABLES IN SCHEMA FINTECH_DEMO.EVENTS TO ROLE REGIONAL_ANALYST;\n```\n\n## Follow-up Questions\n- How would you extend permissions if analysts need aggregated results across both schemas?\n- How would you monitor per-user query latency and credit usage, and alert on anomalies?","diagram":"flowchart TD\n  A[Analyst] --> B[ANALYST_WAREHOUSE]\n  B --> C[SALES, EVENTS schemas]\n  C --> D[READ-ONLY access]\n  E[Resource Monitor] --> B\n  F[Auditing] --> A","difficulty":"beginner","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Cloudflare","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T17:44:36.792Z","createdAt":"2026-01-12T17:44:36.792Z"}],"subChannels":["general"],"companies":["Adobe","Anthropic","Cloudflare","Coinbase","Databricks","Google","Hugging Face","Lyft","Microsoft","NVIDIA","PayPal","Robinhood"],"stats":{"total":5,"beginner":2,"intermediate":1,"advanced":2,"newThisWeek":5}}