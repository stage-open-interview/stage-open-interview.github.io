{"questions":[{"id":"q-1086","question":"Design a Snowflake CDC pattern for an SCD2 customer_dim using a stream on the source table. Explain how to implement an idempotent upsert via MERGE, how deletes are represented, and how to handle late-arriving changes to preserve history?","answer":"Use Snowflake Streams on the source table and a MERGE into the SCD2 target to achieve idempotent CDC. Use src.metadata$action to distinguish INSERT, UPDATE, DELETE. For deletes, set end_date on the ex","explanation":"## Why This Is Asked\n\nDemonstrates pragmatic CDC design in Snowflake, including stream-driven MERGE, and SCD2 history guarantees.\n\n## Key Concepts\n\n- Snowflake Streams and metadata$action to detect DML\n- MERGE pattern for upserts and tombstone deletes\n- SCD Type 2 history with start_date and end_date\n- Handling late-arriving data and idempotence\n\n## Code Example\n\n```javascript\n// Snowflake MERGE with STREAM snippet\nconst mergeSql = `\nMERGE INTO customer_dim AS target\nUSING customers_stream AS src\nON target.customer_id = src.customer_id\nWHEN MATCHED AND src.metadata$action = 'DELETE' THEN DELETE\nWHEN MATCHED THEN UPDATE SET\n  name = src.name, email = src.email, ...\nWHEN NOT MATCHED THEN INSERT (customer_id, name, email, start_date)\nVALUES (src.customer_id, src.name, src.email, src.metadata$start_time);\n`;\n```\n\n## Follow-up Questions\n\n- How would you test this CDC pipeline end-to-end? \n- How do you handle conflicts if key updates arrive out of order?","diagram":null,"difficulty":"intermediate","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T22:20:06.609Z","createdAt":"2026-01-12T22:20:06.609Z"},{"id":"q-1152","question":"Design a least-privilege access layer in Snowflake for a multi-tenant data lake spanning five domains (marketing, sales, finance, analytics, operations). Describe how you would implement ROW ACCESS POLICIES and MASKING POLICIES on a payments table (columns: id, customer_id, amount, card_number, region) to restrict data by region and user role, and how you would audit access?","answer":"Define domain roles and a service role; apply a masking policy on card_number to reveal full value only to ADMIN_ROLE and mask others; implement a row access policy using a user-context REGION value t","explanation":"## Why This Is Asked\nThis question verifies practical application of Snowflake security primitives (masking, RLS, views) in a multi-tenant context, including auditing and governance considerations.\n\n## Key Concepts\n- ROW ACCESS POLICY and MASKING POLICY\n- USER_CONTEXT/SESSION_CONTEXT for dynamic access decisions\n- SECURE_VIEW vs direct table access for enforcement\n- Auditing via QUERY_TAGs and ACCESS_HISTORY\n\n## Code Example\n```sql\n-- Masking policy: full card only for admins\nCREATE MASKING POLICY mask_card_number AS (val STRING) RETURNS STRING ->\n  CASE WHEN CURRENT_ROLE() = 'ADMIN_ROLE' THEN val\n       ELSE CONCAT('XXXX-XXXX-XXXX-', RIGHT(val, 4))\n  END;\n\nALTER TABLE payments MODIFY COLUMN card_number SET MASKING POLICY mask_card_number;\n\n-- Row access policy by region from user-context (conceptual)\nCREATE OR REPLACE ROW ACCESS POLICY region_rls AS (region STRING) RETURNS BOOLEAN ->\n  region = CURRENT_SESSION_REGION();\n\n-- Secure view to surface allowed columns\nCREATE VIEW payments_secure AS\nSELECT id, customer_id, amount, region\nFROM payments;\n``` \n\n## Follow-up Questions\n- How would you test masking and region filtering for multiple roles? \n- What are the performance implications of masking and RLS at scale, and how would you mitigate them?","diagram":null,"difficulty":"advanced","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Databricks","Lyft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T01:36:23.332Z","createdAt":"2026-01-13T01:36:23.332Z"},{"id":"q-1239","question":"You’re building a beginner Snowflake task: a large SALES table is routinely queried with date-range filters. Propose a minimal clustering solution to improve pruning. Include exact commands to add a single clustering key on SALE_DATE, how to monitor its effectiveness, and how to decide if reclustering is needed. Keep changes focused and explain validation steps with a simple test?","answer":"ALTER TABLE SALES CLUSTER BY (SALE_DATE);\n-- Inspect clustering depth\nSELECT SYSTEM$CLUSTERING_INFORMATION('MY_DB.MY_SCHEMA.SALES');\n-- If depth indicates suboptimal pruning, recluster\nALTER TABLE SAL","explanation":"## Why This Is Asked\nTests practical use of manual clustering for query pruning, plus pragmatic validation and cost awareness for beginners.\n\n## Key Concepts\n- Clustering keys: SALE_DATE to aid pruning\n- SYSTEM$CLUSTERING_INFORMATION: evaluate clustering depth\n- Reclustering threshold: avoid over-clustering; measure impact with a test query\n\n## Code Example\n```javascript\nALTER TABLE SALES CLUSTER BY (SALE_DATE);\nSELECT SYSTEM$CLUSTERING_INFORMATION('MY_DB.MY_SCHEMA.SALES');\nALTER TABLE SALES RECLUSTER;\n-- Simple validation query (date range) to compare performance pre/post\n```\n\n## Follow-up Questions\n- How would you adjust clustering if the query pattern includes filters on region in addition to date?\n- What metrics would you monitor over time to assess clustering maintenance needs?","diagram":"flowchart TD\n  A[SALES Table] --> B[Add clustering key: SALE_DATE]\n  B --> C[Monitor: SYSTEM$CLUSTERING_INFORMATION]\n  C --> D[Evaluate clustering depth]\n  D --> E[Option: RECLUSTER]","difficulty":"beginner","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T06:34:44.338Z","createdAt":"2026-01-13T06:34:44.338Z"},{"id":"q-1337","question":"You're ingesting data daily into table sensor_readings(device_id STRING, ts TIMESTAMP_NTZ, reading FLOAT). You frequently query last 24 hours per device. Propose a beginner-friendly optimization path, choosing clustering key, automatic clustering, or a materialized view, and include an exact query to compute per-device count and average reading for the last 24 hours?","answer":"Best path: add a clustering key on (ts, device_id) to improve pruning for the last-24h window and rely on Snowflake's automatic clustering to maintain it. Avoid premature materialized views in a begin","explanation":"## Why This Is Asked\nTests understanding of Snowflake data pruning, clustering strategies, and practical trade-offs for lightweight optimization.\n\n## Key Concepts\n- Clustering keys and partition pruning\n- Automatic vs manual clustering maintenance\n- Trade-offs of materialized views for beginners\n- 24-hour window query patterns\n\n## Code Example\n```sql\nSELECT device_id, COUNT(*) AS n, AVG(reading) AS avg_reading\nFROM sensor_readings\nWHERE ts >= DATEADD(hour, -24, CURRENT_TIMESTAMP())\nGROUP BY device_id;\n```\n\n## Follow-up Questions\n- How would you monitor clustering impact and costs?\n- When would a materialized view be preferable to clustering in this scenario?","diagram":null,"difficulty":"beginner","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["PayPal","Plaid","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T11:43:36.716Z","createdAt":"2026-01-13T11:43:36.716Z"},{"id":"q-1475","question":"You have a Snowflake table raw_events with columns file_name STRING, payload VARIANT containing an array at payload:'events'. Each event is an object { event, timestamp, user: { id }, region, amount }. Design a beginner-friendly approach to compute daily total_amount and per-user purchases for the last 7 days using a temporary VIEW and LATERAL FLATTEN. Provide the exact query you would run?","answer":"Create a lightweight view that normalizes the JSON events and use LATERAL FLATTEN on payload:'events' to explode each event. Filter where event='purchase', cast timestamp to date, and group by day, us","explanation":"## Why This Is Asked\n\nTests ability to work with semi-structured data, practice using VIEW for readability, and apply LATERAL FLATTEN to explode arrays, all at a beginner level.\n\n## Key Concepts\n\n- VARIANT and semi-structured data access\n- LATERAL FLATTEN for arrays\n- VIEWs to avoid repeated computation\n- Date/time conversion and basic aggregation\n\n## Code Example\n\n```sql\nCREATE OR REPLACE VIEW v_purchases_last7 AS\nSELECT\n  DATE_TRUNC('day', TO_TIMESTAMP_NTZ(evt.value:\"timestamp\"))::DATE AS day,\n  evt.value:\"user\":\"id\" AS user_id,\n  evt.value:\"region\" AS region,\n  evt.value:\"amount\"::FLOAT AS amount\nFROM raw_events r,\nLATERAL FLATTEN(input => r.payload:\"events\") AS evt\nWHERE evt.value:\"event\" = 'purchase'\n  AND TO_TIMESTAMP_NTZ(evt.value:\"timestamp\") >= DATEADD(day, -7, CURRENT_DATE());\n\nSELECT day, user_id, region, SUM(amount) AS total_amount, COUNT(*) AS purchases\nFROM v_purchases_last7\nGROUP BY day, user_id, region\nORDER BY day, region, user_id;\n```\n\n## Follow-up Questions\n\n- How would you handle missing fields in some events?\n- How would you index or cluster this data for faster queries on the last 7 days?","diagram":null,"difficulty":"beginner","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Cloudflare","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T18:49:36.671Z","createdAt":"2026-01-13T18:49:36.671Z"},{"id":"q-1499","question":"You're building a Snowflake-based telemetry store: table `raw.events` (device_id STRING, ts TIMESTAMP_NTZ, payload VARIANT) ingests 50M+ rows daily. BI dashboards query per-device event_count and median reading for last 7 days and last 24 hours. Propose a pragmatic optimization, selecting between clustering, automatic clustering, and a rolling aggregate MV/table. Include exact commands to implement your approach and how you would validate improvements?","answer":"Cluster by (device_id, DATE(ts)); enable AUTOMATIC_CLUSTERING. Create 7-day rolling MV mv_last7: SELECT device_id, COUNT(*) AS event_count, APPROX_PERCENTILE(CAST(payload:reading AS FLOAT), 0.5) AS me","explanation":"## Why This Is Asked\nTests understanding of Snowflake data-pruning strategies and trade-offs between manual clustering, auto clustering, and pre-aggregation for rolling windows.\n\n## Key Concepts\n- Clustering keys and pruning effectiveness\n- Automatic clustering maintenance\n- Rolling aggregates via materialized views\n- Validation via cost/latency experiments\n\n## Code Example\n```sql\nALTER TABLE raw.events CLUSTER BY (device_id, DATE(ts));\nALTER TABLE raw.events SET AUTOMATIC_CLUSTERING = TRUE;\nCREATE MATERIALIZED VIEW mv_last7 AS\nSELECT device_id, COUNT(*) AS event_count,\n       APPROX_PERCENTILE(CAST(payload:reading AS FLOAT), 0.5) AS median_reading\nFROM raw.events\nWHERE ts >= DATEADD(DAY, -7, CURRENT_TIMESTAMP())\nGROUP BY device_id;\n```\n\n## Follow-up Questions\n- How would you adapt this for the 24-hour window?\n- What are the trade-offs of MV vs a rolling summary table in Snowflake?","diagram":"flowchart TD\n  A[Ingest] --> B[raw.events]\n  B --> C[Cluster by device_id, DATE(ts)]\n  C --> D[Automatic Clustering]\n  B --> E[MV mv_last7]\n  E --> F[BI queries]","difficulty":"intermediate","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Tesla","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T19:35:08.508Z","createdAt":"2026-01-13T19:35:08.508Z"},{"id":"q-1567","question":"You're operating a Snowflake telemetry store with table raw.events(device_id STRING, ts TIMESTAMP_NTZ, payload VARIANT) ingesting 100M+ rows daily. BI requires per-device metrics: (a) last hour event_count, (b) last 24h median of payload.metrics.reading. Propose a production-ready strategy: data model, clustering choices vs automatic clustering vs materialized views, and how to validate performance. Include exact commands to implement and how to verify improvements?","answer":"Implement a two-layer architecture: maintain raw.events for ingestion while creating per-device rolling aggregates. Use a STREAM on raw.events with a TASK to populate agg_hour(device_id, hour_ts, cnt) for the last hour, and a materialized view for the 24-hour median. Cluster by device_id to optimize query pruning.","explanation":"## Why This Is Asked\nThis question evaluates end-to-end design capabilities for streaming analytics at scale, encompassing data modeling, pruning strategies, and performance validation.\n\n## Key Concepts\n- Streams and Tasks for incremental processing\n- Materialized views versus rolling aggregates\n- Clustering versus automatic clustering for hot queries\n- MEDIAN calculations over VARIANT payloads with per-device rollups\n\n## Code Example\n```sql\nCREATE STREAM s_raw_events ON TABLE raw.events;\nCREATE TASK t_last_hour WAREHOUSE = WH AS\n  INSERT INTO agg_hour\n  SELECT device_id, DATE_TRUNC('hour', ts) AS hour_ts, COUNT(*)\n  FROM s_raw_events\n  GROUP BY device_id, DATE_TRUNC('hour', ts);\n```","diagram":null,"difficulty":"advanced","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T06:18:13.265Z","createdAt":"2026-01-13T22:30:34.610Z"},{"id":"q-1605","question":"Design a Snowflake-based near real-time anomaly-detection pipeline for telemetry events. Ingested table: raw_events(device_id STRING, ts TIMESTAMP_NTZ, payload VARIANT). Ingests 200M+ rows/day. You need to detect spikes in a metric inside payload (e.g., latency) on a per-device basis with a 5-minute window and surface alerts to an alerts table when a spike exceeds a dynamic threshold. Describe the architecture using Streams, Tasks, and possibly a Snowflake procedure; include DDLs to create the stream, a task schedule, sample SQL for the rolling compute, and how you would validate the pipeline end-to-end?","answer":"Design a Snowflake-based near real-time anomaly-detection pipeline using Streams and Tasks. Ingest: raw_events(device_id STRING, ts TIMESTAMP_NTZ, payload VARIANT). Build a stream on raw_events, a staging table, and an alerts table. Create a scheduled task that processes new stream data every minute, computes rolling 5-minute windows per device, extracts metrics from the payload (e.g., payload.latency_ms), compares against dynamic thresholds, and inserts alerts using MERGE for idempotency. Include error handling and monitoring.","explanation":"## Why This Is Asked\n\nTests practical use of Streams and Tasks for near real-time analytics, including per-device rolling metrics, dynamic thresholds, and idempotent alerts. Evaluates fault handling, scheduling, and test strategies under high-volume ingestion.\n\n## Key Concepts\n- Snowflake Streams & Tasks\n- Rolling window analytics per device\n- Variant payload parsing (e.g., payload.latency_ms)\n- Idempotent alerting with merge/upsert\n- Validation with synthetic data and backfill handling\n\n## Code Example\n```sql\nCREATE OR REPLACE TABLE raw_events (\n  device_id STRING,\n  ts TIMESTAMP_NTZ,\n  payload VARIANT\n);\n\nCREATE OR REPLACE STREAM raw_events_stream ON TABLE raw_events;\n\nCREATE OR REPLACE TABLE device_metrics (\n  device_id STRING,\n  window_start TIMESTAMP_NTZ,\n  metric_name STRING,\n  avg_value FLOAT,\n  threshold FLOAT,\n  processed_at TIMESTAMP_NTZ\n);\n\nCREATE OR REPLACE TABLE alerts (\n  alert_id STRING DEFAULT UUID_STRING(),\n  device_id STRING,\n  metric_name STRING,\n  detected_at TIMESTAMP_NTZ,\n  actual_value FLOAT,\n  threshold FLOAT,\n  created_at TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP(),\n  PRIMARY KEY (alert_id)\n);\n\nCREATE OR REPLACE PROCEDURE detect_anomalies()\nRETURNS STRING\nLANGUAGE JAVASCRIPT\nAS\n$$\n  var sql = `\n    MERGE INTO alerts a\n    USING (\n      SELECT \n        e.device_id,\n        'latency_ms' as metric_name,\n        AVG(e.payload:latency_ms::FLOAT) as avg_latency,\n        3 * STDDEV(e.payload:latency_ms::FLOAT) as threshold,\n        DATEADD('minute', 5, MIN(e.ts)) as window_end\n      FROM raw_events_stream e\n      WHERE e.ts >= DATEADD('minute', -5, CURRENT_TIMESTAMP())\n      GROUP BY e.device_id, DATE_TRUNC('minute', e.ts, 5)\n      HAVING AVG(e.payload:latency_ms::FLOAT) > 3 * STDDEV(e.payload:latency_ms::FLOAT)\n    ) src\n    ON (a.device_id = src.device_id AND a.detected_at = src.window_end)\n    WHEN NOT MATCHED THEN INSERT (\n      device_id, metric_name, detected_at, actual_value, threshold\n    ) VALUES (\n      src.device_id, src.metric_name, src.window_end, src.avg_latency, src.threshold\n    );\n    \n    ALTER STREAM raw_events_stream SET OFFSET = CURRENT_TIMESTAMP();\n  `;\n  \n  snowflake.execute({sqlText: sql});\n  return 'Anomaly detection completed';\n$$;\n\nCREATE OR REPLACE TASK anomaly_detection_task\n  WAREHOUSE = COMPUTE_WH\n  SCHEDULE = '1 MINUTE'\n  AS\n  CALL detect_anomalies();\n\nALTER TASK anomaly_detection_task RESUME;\n```\n\n## Validation Strategy\n1. **Synthetic Data Generation**: Insert test events with known anomaly patterns\n2. **Backfill Testing**: Process historical data to verify threshold calculations\n3. **End-to-End Monitoring**: Track stream lag, task execution times, and alert volumes\n4. **Threshold Calibration**: Validate dynamic thresholds against historical baselines","diagram":"flowchart TD\n  A[raw_events] --> B[raw_events_stream]\n  B --> C[staging_events]\n  C --> D[rolling_metrics_per_device]\n  D --> E[alerts_table]\n  E --> F[downstream_consumer]","difficulty":"intermediate","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Scale Ai","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T04:20:20.721Z","createdAt":"2026-01-14T02:32:59.488Z"},{"id":"q-1678","question":"You're provisioning a Snowflake warehouse for a multi-tenant SaaS app. All tenant data sits in one table: raw_events (tenant_id STRING, event_ts TIMESTAMP_NTZ, event_type STRING, payload VARIANT) ingesting 40M+ rows/day. Propose a hybrid approach: (1) add a composite clustering key (tenant_id, event_ts) for pruning, and (2) a rolling 7-day materialized view with per-tenant metrics. Include exact SQL commands to implement clustering, MV, and a validation query showing the improvement?","answer":"Add clustering on (tenant_id, event_ts) to prune by tenant and date, and create a rolling 7-day MV mv_events_7d with per-tenant day counts. Implement with: create table ... CLUSTER BY (tenant_id, even","explanation":"## Why This Is Asked\n\nTests ability to design scalable, tenant-aware analytics in Snowflake, balancing manual clustering with a rolling materialized view for time-bound queries. It also probes validation strategies and drift monitoring.\n\n## Key Concepts\n\n- Composite clustering keys for multi-tenant pruning\n- Rolling 7-day materialized views for per-tenant aggregates\n- Validation via query plans (EXPLAIN) and performance comparisons\n- MV refresh considerations and clustering drift monitoring\n\n## Code Example\n\n```sql\nCREATE TABLE raw_events (\n  tenant_id STRING,\n  event_ts TIMESTAMP_NTZ,\n  event_type STRING,\n  payload VARIANT\n)\nCLUSTER BY (tenant_id, event_ts);\n\nCREATE MATERIALIZED VIEW mv_events_7d AS\nSELECT tenant_id,\n       DATE_TRUNC('day', event_ts) AS day,\n       COUNT(*) AS event_cnt\nFROM analytics.raw_events\nWHERE event_ts >= DATEADD('day', -7, CURRENT_TIMESTAMP())\nGROUP BY tenant_id, day;\n\nEXPLAIN SELECT tenant_id, day, event_cnt\nFROM mv_events_7d\nWHERE day >= DATE_TRUNC('day', CURRENT_TIMESTAMP()) - 1;\n```\n\n```sql\n-- Basic validation: inspect MV contents for recent days\nSELECT tenant_id, day, event_cnt\nFROM mv_events_7d\nORDER BY tenant_id, day;\n```\n\n## Follow-up Questions\n\n- How would you adapt this approach for hot vs cold data and potential tiering?\n- What metrics and alerts would you set to detect MV staleness or clustering drift?","diagram":null,"difficulty":"intermediate","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Goldman Sachs","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T06:50:55.919Z","createdAt":"2026-01-14T06:50:55.919Z"},{"id":"q-1711","question":"In a Snowflake telemetry store with table raw.events(device_id STRING, ts TIMESTAMP_NTZ, payload VARIANT) ingesting 50M+ rows daily, implement a production-grade data retention pipeline that archives data older than 365 days to an external stage and keeps only the last 365 days in the main table, while preserving query performance on recent data. Describe the architecture and provide exact SQL commands to create the stage, stream, task, and archive procedure, plus a validation plan?","answer":"Architect a retention pipeline using a STREAM on raw.events, a TASK to purge data older than 365 days, and an external archive via COPY INTO to the S3 stage. The plan preserves recent data in Snowflak","explanation":"## Why This Is Asked\nAssess lifecycle design, automation, and validation for production Snowflake workloads.\n\n## Key Concepts\n- Data retention windows, Streams, Tasks\n- External stages and COPY INTO\n- Validation strategies and data correctness\n\n## Code Example\n```javascript\n-- Archive older data to external stage\nCREATE OR REPLACE STAGE archive_stage URL='s3://bucket/archive/';\nCREATE OR REPLACE STREAM raw.events_stream ON TABLE raw.events (APPEND_ONLY = FALSE);\nCREATE OR REPLACE TASK archive_old_events\n  WAREHOUSE = compute_wh\n  SCHEDULE = 'USING CRON 0 2 * * *'\nAS\n  COPY INTO @archive_stage/events FROM (\n    SELECT * FROM raw.events WHERE ts < DATEADD(DAY, -365, CURRENT_TIMESTAMP())\n  ) FILE_FORMAT=(TYPE=CSV);\nDELETE FROM raw.events WHERE ts < DATEADD(DAY, -365, CURRENT_TIMESTAMP());\n```\n\n## Follow-up Questions\n- How would you test idempotency and failure recovery for the archive task?\n- What metrics would you monitor to ensure retention does not impact recent-data queries?","diagram":null,"difficulty":"advanced","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Microsoft","Oracle","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T07:46:52.119Z","createdAt":"2026-01-14T07:46:52.119Z"},{"id":"q-2024","question":"Implement row-level security for regional data access in Snowflake. Table `customer.sales` has `customer_id`, `region`, `amount`, `last_purchase_ts`. Roles `SA_US` and `SA_EU` should only see rows for their region. Describe and implement a Snowflake ROW ACCESS POLICY using a role-context and attach it to the table, then provide a sample query that would be allowed for role `SA_US` and a test plan to verify enforcement?","answer":"Create a ROW ACCESS POLICY that maps roles to regions and apply it to the region column. For example: region_rlp (region STRING) RETURNS BOOLEAN AS CASE WHEN CURRENT_ROLE() = 'SA_US' THEN region = 'US","explanation":"## Why This Is Asked\nTests practical use of Snowflake Row Access Policies, role-based access, and testing.\n\n## Key Concepts\n- Row Access Policies\n- CURRENT_ROLE()\n- ALTER TABLE ADD ROW ACCESS POLICY\n- Validation via role switching and selective querying\n\n## Code Example\n```javascript\nCREATE OR REPLACE ROW ACCESS POLICY region_rlp AS (region STRING) RETURNS BOOLEAN ->\n  CASE\n    WHEN CURRENT_ROLE() = 'SA_US' THEN region = 'US'\n    WHEN CURRENT_ROLE() = 'SA_EU' THEN region = 'EU'\n    ELSE FALSE\n  END;\nALTER TABLE customer.sales ADD ROW ACCESS POLICY region_rlp ON (region);\n```\n\n## Follow-up Questions\n- How would you test policy coverage with automated tests?\n- How would you extend to additional regions or dynamic role mappings?","diagram":"flowchart TD\n  A[Role SA_US] --> B[Query on region column]\n  B --> C{Policy allows?}\n  C -->|Yes| D[Rows returned]\n  C -->|No| E[Zero rows]","difficulty":"beginner","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Hugging Face","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T21:33:15.741Z","createdAt":"2026-01-14T21:33:15.741Z"},{"id":"q-2048","question":"You're building a multi-tenant analytics warehouse in Snowflake. Ingested events go into raw.events(tenant_id STRING, event_ts TIMESTAMP_NTZ, payload VARIANT). BI needs per-tenant metrics: (a) count of events in the last 15 minutes, (b) 95th percentile of latency from payload.metrics.latency in the last 24 hours. Propose a production-ready architecture: data model (partitioning, clustering), streaming vs batch paths (streams/tasks vs MV), decision between automatic clustering and materialized views, and a concrete validation plan with exact SQL commands?","answer":"Implement a streaming architecture using Snowflake's STREAM and TASK constructs to incrementally process raw events into per-tenant metrics. Create a STREAM on raw.events to capture new records, then schedule a TASK that continuously updates tenant_metrics with 15-minute event counts and 24-hour 95th percentile latency calculations extracted from payload.metrics.latency. This approach delivers real-time analytics while optimizing compute costs through incremental processing.","explanation":"## Why This Is Asked\nTests ability to design multi-tenant analytics architectures with streaming ingestion, cost-efficient aggregation strategies, and proper utilization of Snowflake's native constructs (streams, tasks, clustering, materialized views).\n\n## Key Concepts\n- Multi-tenant data modeling and isolation strategies\n- Streams and tasks for incremental ETL pipelines\n- Per-tenant windowed aggregates and percentile calculations\n- Clustering strategies versus materialized views versus automatic clustering\n- Real-time analytics with cost optimization\n\n## Code Example\n```sql\n-- Core tables\nCREATE TABLE raw.events (\n  tenant_id STRING, \n  event_ts TIMESTAMP_NTZ, \n  payload VARIANT\n);\n\n-- Streaming infrastructure\nCREATE STREAM s_raw AS SELECT * FROM raw.events;\n\n-- Target metrics table\nCREATE TABLE tenant_metrics (\n  tenant_id STRING,\n  event_count_15min NUMBER,\n  p95_latency_24h NUMBER,\n  updated_at TIMESTAMP_NTZ\n);\n\n-- Incremental processing task\nCREATE OR REPLACE TASK process_metrics\n  WAREHOUSE = analytics_wh\n  SCHEDULE = '1 MINUTE'\nAS\nINSERT INTO tenant_metrics\nSELECT \n  tenant_id,\n  COUNT(*) as event_count_15min,\n  PERCENTILE_CONT(0.95) WITHIN GROUP (\n    ORDER BY payload.metrics.latency\n  ) as p95_latency_24h,\n  CURRENT_TIMESTAMP()\nFROM s_raw \nWHERE event_ts >= DATEADD(minute, -15, CURRENT_TIMESTAMP())\nGROUP BY tenant_id;\n```","diagram":"flowchart TD\nA[Ingest] --> B[RAW.EVENTS]\nB --> C[STREAM S_RAW]\nC --> D[TENANT_METRICS]\nD --> E[BI/Reports]","difficulty":"advanced","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Scale Ai","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T05:53:18.231Z","createdAt":"2026-01-14T22:32:57.077Z"},{"id":"q-2229","question":"You're operating a Snowflake event store table raw.events(device_id STRING, ts TIMESTAMP_NTZ, payload VARIANT). Implement GDPR-like retention: keep 90 days of live data, purge older rows by archiving to raw.events_archive and deleting from live daily via a Snowflake Task, while preserving time travel history and audits. How would you design the architecture, what SQL would you use, and how would you validate?","answer":"Architect a 90-day live retention with archival: create raw.events_archive LIKE raw.events; daily TASK moves rows with ts < dateadd(day,-90,current_timestamp()) to archive and deletes from live; enabl","explanation":"## Why This Is Asked\nTests practical data lifecycle design, governance, and cost trade-offs in Snowflake.\n\n## Key Concepts\n- Time Travel, Fail-Safe, archival workflow, idempotent tasks, cost-aware data movement.\n- Data-archival pipelines and rollback strategies.\n\n## Code Example\n```javascript\n-- Archive old data\nCREATE TABLE raw.events_archive LIKE raw.events;\n\nCREATE OR REPLACE TASK archive_old_events\n  WAREHOUSE = 'COMPUTE_WH'\n  SCHEDULE = '1 DAY'\nAS\n  INSERT INTO raw.events_archive\n  SELECT * FROM raw.events WHERE ts < DATEADD(day, -90, CURRENT_TIMESTAMP());\n\n-- Purge from live\nDELETE FROM raw.events WHERE ts < DATEADD(day, -90, CURRENT_TIMESTAMP());\n```\n\n## Follow-up Questions\n- How would you handle schema changes in the payload VARIANT over time?\n- How would you validate the completeness of archival after a rollback event?","diagram":"flowchart TD\n  A[Ingest] --> B[Live: raw.events]\n  B --> C[Archive: raw.events_archive]\n  B --> D[Daily purge task]\n  B --> E[Time Travel: 1d]\n  C --> F[Audit via archive TZ]","difficulty":"intermediate","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Apple","DoorDash"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T08:45:10.570Z","createdAt":"2026-01-15T08:45:10.570Z"},{"id":"q-2277","question":"In Snowflake, ingest telemetry into RAW.events (device_id STRING, ts TIMESTAMP_NTZ, payload VARIANT) from two sources: real-time stream and nightly batch. You must deliver per-device 15-minute latency metrics for the last hour (avg_reading, cnt). Propose an end-to-end architecture using a Stream, a Materialized View (MV) or incremental aggregation, and a TASK scheduled every 15 minutes to refresh. Include exact SQL to create the stream, MV, task, and a validation query to prove latency targets?","answer":"Adopt a stream-based MV with a 15-minute refresh. Create stream RAW.events_strm on RAW.events; create MV mv_device_hour as SELECT device_id, DATE_TRUNC('hour', ts) AS hh, AVG(payload:reading::FLOAT) A","explanation":"## Why This Is Asked\nTests design of real-time-then-batch pipelines using Snowflake features.\n\n## Key Concepts\n- Streams, Materialized Views, Tasks, VARIANT payloads, per-device windowing, latency validation\n\n## Code Example\n```sql\n-- Create stream\nCREATE OR REPLACE STREAM RAW.events_strm ON TABLE RAW.events;\n\n-- Materialized View\nCREATE MATERIALIZED VIEW mv_device_hour AS\nSELECT device_id,\n       DATE_TRUNC('hour', ts) AS hh,\n       AVG(payload:reading::FLOAT) AS avg_reading,\n       COUNT(*) AS cnt\nFROM RAW.events_strm\nGROUP BY device_id, hh;\n\n-- Task\nCREATE OR REPLACE TASK t_refresh_mv\n  WAREHOUSE = WH\n  SCHEDULE = '15 MINUTE'\nAS\nREFRESH MATERIALIZED VIEW mv_device_hour;\n\n-- Validation\nWITH lag AS (\n  SELECT device_id,\n         DATEDIFF(second, MAX(ts), CURRENT_TIMESTAMP()) AS lag_s\n  FROM RAW.events\n  GROUP BY device_id\n)\nSELECT APPROX_PERCENTILE(lag_s, 0.95) AS p95_lag_sec\nFROM lag;\n```\n\n## Follow-up Questions\n- How would you handle late-arriving data affecting MV freshness?\n- What are cost implications of MV refresh vs. manual reshaping?","diagram":null,"difficulty":"advanced","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Slack","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T10:38:45.778Z","createdAt":"2026-01-15T10:38:45.778Z"},{"id":"q-2303","question":"Scenario: a Snowflake account holds prod_sales.customers with PII. External partners at Oracle, Microsoft, and Nvidia need a sanitized subset for analytics. Propose a beginner-friendly data-share plan: masking policies for PII, a region-limited secure view, and an account-based share. Include commands to implement?","answer":"Share a sanitized dataset via Snowflake data sharing. Create a masking policy for PII, apply it to emails/phones, and expose an EU-only secure view. Add that view to an account-based share granted to ","explanation":"## Why This Is Asked\nInterview context explanation.\n\n## Key Concepts\n- Data sharing basics; account-based vs reader accounts\n- Secure views for row-level access; masking policies\n- Grant hierarchy and testing access\n\n## Code Example\n```sql\n-- mask policy\nCREATE MASKING POLICY pii_mask AS (VAL STRING) RETURNS STRING\n  CASE WHEN CURRENT_ROLE() IN ('EU_ANALYST') THEN 'REDACTED' ELSE VAL END;\n\n-- apply mask\nALTER TABLE prod_sales.customers MODIFY COLUMN email SET MASKING POLICY pii_mask;\n```\n\n## Follow-up Questions\n- How would you audit share usage?\n- How would you revoke access?","diagram":"flowchart TD\n  Internal[Internal Snowflake] --> Share[Data Share eu_sales]\n  Share --> Oracle[Oracle Partner]\n  Share --> Microsoft[Microsoft Partner]\n  Share --> Nvidia[Nvidia Partner]","difficulty":"beginner","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Microsoft","NVIDIA","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T11:31:03.043Z","createdAt":"2026-01-15T11:31:03.043Z"},{"id":"q-2371","question":"You're building a Snowflake-based telemetry store loaded into a shared table raw.events (tenant_id STRING, ts TIMESTAMP_NTZ, device_id STRING, payload VARIANT). Design a production-ready, multi-tenant design ensuring strict data isolation and fast dashboards for last 30 days across hundreds of tenants. Outline RAP rules, masking, secure views or materialized aggregates, and a scalable data-sharing approach. Include concrete SQL patterns and validation steps?","answer":"Use a single shared table raw.events with a TENANT_ID column. Enforce row-level access via a ROW ACCESS POLICY that binds tenant_id to a user context (CURRENT_USER) via a central mapping table. Apply ","explanation":"## Why This Is Asked\n\nTests ability to design a scalable, isolation-first multi-tenant architecture in Snowflake using RAP, masking, and secure views, plus performance strategies.\n\n## Key Concepts\n\n- Row Access Policies and tenant-context binding\n- Secure views and dynamic data masking\n- Time-series clustering and auto-clustering\n- Data-sharing controls and audits\n\n## Code Example\n\n```javascript\n-- Pseudo-SQL illustrating the approach (not real Snowflake syntax)\nALTER TABLE raw.events ADD COLUMN tenant_id STRING;\nCREATE ROW ACCESS POLICY tenant_rap AS\n  (tenant_id STRING) RETURNS BOOLEAN ->\n  tenant_id = get_tenant_for_user(CURRENT_USER());\n\nCREATE VIEW v_last_30d AS\nSELECT * FROM raw.events\nWHERE tenant_id = get_tenant_for_user(CURRENT_USER())\n  AND ts >= CURRENT_DATE() - INTERVAL '30' DAY;\n```\n\n## Follow-up Questions\n\n- How to test for cross-tenant leakage at scale?\n- What monitoring would you add to ensure sustained performance?","diagram":null,"difficulty":"advanced","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","PayPal","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T15:37:21.200Z","createdAt":"2026-01-15T15:37:21.200Z"},{"id":"q-2483","question":"Intermediate Snowflake-core: Ingest 200M rows daily into raw.sessions(user_id STRING, session_id STRING, started_at TIMESTAMP_NTZ, ended_at TIMESTAMP_NTZ, events VARIANT). Need per-user risk scores updated every 5 minutes and dashboards for (a) count of high-risk sessions in last 15 minutes, (b) average duration of high-risk sessions, (c) top 10 user cohorts by risk. Propose a production plan covering data model (raw vs curated), clustering, streaming vs materialized views, schema evolution, validation, and rollback strategy with exact SQL commands?","answer":"Build a curated per-user risk table curate.user_risk updated by a STREAM over raw.sessions and a TASK every 5 minutes. Use a compact SQL expression over the last 5–10 minutes of sessions to compute ri","explanation":"## Why This Is Asked\nTests end-to-end productionizing of streaming + batch in Snowflake, including data model separation, clustering, Streams/Tasks vs MV, schema evolution, validation, and rollback.\n\n## Key Concepts\n- Streams and tasks for near-real-time processing\n- Raw vs curated data separation\n- Composite clustering and pruning\n- Time Travel and rollback\n\n## Code Example\n```sql\n-- example: create stream, task, and insert into curated table (pseudo)\n```\n\n## Follow-up Questions\n- How to handle late-arriving events that modify risk scores?\n- What operational metrics ensure per-user score freshness and data skew detection?","diagram":"flowchart TD\n  A[raw.sessions insert] --> B[streams]\n  B --> C[tasks: compute risk]\n  C --> D[curate.user_risk insert]\n  D --> E[dashboards]","difficulty":"intermediate","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Amazon"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T19:44:46.336Z","createdAt":"2026-01-15T19:44:46.337Z"},{"id":"q-2522","question":"Snowflake-core intermediate: You're ingesting telemetry into Snowflake with raw.events(tenant_id STRING, user_id STRING, ts TIMESTAMP_NTZ, payload VARIANT). In a multi-tenant SaaS, implement per-tenant latency metrics (p95 over last 30 min), dynamic masking of sensitive payload fields for dashboards, and fast per-user drilldown without scanning raw payloads. Propose a production plan: curated schema, partitioning, streaming vs materialized path, VARIANT schema evolution, validation, and rollback with exact commands?","answer":"Implement a curated analytics path with incremental processing: create `curated.events_by_tenant(tenant_id, ts, user_id, latency_ms, payload_masked)` with a STREAM on `raw.events` and a TASK that computes per-tenant p95 latency over rolling 30-minute windows. Apply dynamic masking policies to sensitive payload fields and create materialized views for fast per-user drilldown without scanning raw payloads.","explanation":"## Why This Is Asked\nTests ability to design a production-ready, multi-tenant analytics pipeline with governance, performance optimization, and operational safety in Snowflake.\n\n## Key Concepts\n- Streams and Tasks for incremental ETL processing\n- Per-tenant rolling window metrics (p95 calculations)\n- VARIANT payload masking and secure dashboard views\n- Data governance with dynamic masking policies\n- Performance optimization through materialized views\n- Time Travel and rollback strategies for production safety\n\n## Code Example\n```javascript\n-- Create curated table\nCREATE TABLE curated.events_by_tenant (\n  tenant_id STRING,\n  ts TIMESTAMP_NTZ,\n  user_id STRING,\n  latency_ms NUMBER,\n  payload_masked VARIANT\n);\n\n-- Create stream over raw\nCREATE OR REPLACE STREAM raw_events_stream\n  ON TABLE raw.events;\n\n-- Create task for incremental processing\nCREATE OR REPLACE TASK process_tenant_metrics\n  WAREHOUSE = analytics_wh\n  SCHEDULE = '1 minute'\nAS\nBEGIN\n  INSERT INTO curated.events_by_tenant\n  SELECT \n    tenant_id,\n    ts,\n    user_id,\n    latency_ms,\n    mask_sensitive_fields(payload) as payload_masked\n  FROM raw_events_stream\n  WHERE METADATA$ACTION = 'INSERT';\n  \n  -- Update per-tenant p95 metrics\n  MERGE INTO tenant_latency_metrics t\n  USING (\n    SELECT tenant_id, \n           PERCENTILE_CONT(latency_ms, 0.95) OVER (\n             PARTITION BY tenant_id \n             ORDER BY ts \n             RANGE BETWEEN INTERVAL '30 minutes' PRECEDING AND CURRENT ROW\n           ) as p95_latency\n    FROM curated.events_by_tenant\n  ) s ON t.tenant_id = s.tenant_id\n  WHEN MATCHED THEN UPDATE SET t.p95_latency = s.p95_latency;\nEND;\n```","diagram":null,"difficulty":"intermediate","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Oracle","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T05:46:29.573Z","createdAt":"2026-01-15T21:32:29.212Z"},{"id":"q-2591","question":"You're designing a multi-tenant analytics layer in Snowflake. A single table raw.events stores events for all tenants (tenant_id, ts, payload). BI dashboards must be isolated per tenant and support sub-second queries for the last 7 days. Propose and implement a scalable access-control strategy that avoids per-tenant copies, considering growth to hundreds of tenants. Include how you'd implement Row Access Policies, a Secure View, and validation steps with concrete SQL commands?","answer":"Implement a single shared raw.events table with tenant_id and enforce access through a Row Access Policy bound to session tenant_id, complemented by a Secure View that projects only authorized rows for BI dashboards. Map tenants to Snowflake roles and establish session context to enable dynamic tenant isolation without data duplication.","explanation":"## Why This Is Asked\nAssesses ability to design scalable, auditable tenant isolation using Snowflake features like Row Access Policies and Secure Views while maintaining performance.\n\n## Key Concepts\n- Row Access Policies\n- Secure Views\n- Role mapping vs dynamic tenant mapping\n- Validation: cross-tenant isolation and performance\n\n## Code Example\n\n```sql\n-- Skeleton policy (illustrative)\nCREATE OR REPLACE ROW ACCESS POLICY ra_events AS\n  (tenant_id STRING) RETURNS BOOLEAN ->\n  CASE WHEN CURRENT_ROLE() = 'ADMIN' THEN TRUE\n       ELSE tenant_id = CURRENT_TENANT()\n  END;\n```\n\n## Follow-up Question","diagram":null,"difficulty":"advanced","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Discord","DoorDash"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T05:05:41.403Z","createdAt":"2026-01-15T23:49:04.445Z"},{"id":"q-2592","question":"You operate a multi-tenant Snowflake data lake: table raw.orders(tenant_id STRING, order_id STRING, amount NUMBER, credit_card VARIANT). Implement per-tenant data isolation using row access policies and a masking policy for credit_card so tenants see only last 4 digits. Provide exact SQL to create masking policy, row access policy, a secure view that enforces both policies, and grants. Include how you'd test with two tenants?","answer":"Implement comprehensive per-tenant data isolation using Snowflake's governance features: a masking policy to obscure credit card numbers showing only the last 4 digits, a row access policy to filter data by tenant_id, and a secure view that enforces both policies while granting appropriate privileges to tenant-specific roles.","explanation":"## Why This Is Asked\nThis question tests practical implementation of Snowflake's multi-tenant security model, evaluating understanding of governance features including masking policies, row access policies, and secure views. It assesses ability to implement proper data isolation while maintaining functional access patterns.\n\n## Key Concepts\n- Dynamic data masking using policy-based approach\n- Row-level security through session context validation\n- Secure views for centralized policy enforcement\n- Role-based access control with least privilege principles\n- Cross-tenant isolation and testing methodologies\n\n## Code Example\n```sql\n-- Create masking policy for credit card data\nCREATE MASKING POLICY credit_card_mask AS (val VARIANT) RETURNS VARIANT ->\n  CASE\n    WHEN CURRENT_ROLE() IN ('ACCOUNTADMIN', 'SECURITYADMIN') THEN val\n    ELSE SUBSTR(TO_STRING(val), -4)::VARIANT\n  END;\n\n-- Create row access policy for tenant isolation\nCREATE ROW ACCESS POLICY tenant_isolation AS (tenant_id_col STRING) ->\n  CASE\n    WHEN CURRENT_ROLE() = 'ACCOUNTADMIN' THEN TRUE\n    ELSE tenant_id_col = CURRENT_USER()\n  END;\n\n-- Apply policies to base table\nALTER TABLE raw.orders \n  MODIFY COLUMN credit_card SET MASKING POLICY credit_card_mask;\nALTER TABLE raw.orders \n  ADD ROW ACCESS POLICY tenant_isolation ON (tenant_id);\n\n-- Create secure view with policy enforcement\nCREATE SECURE VIEW orders_secure AS\nSELECT \n  tenant_id,\n  order_id,\n  amount,\n  credit_card\nFROM raw.orders;\n\n-- Create tenant roles and grants\nCREATE ROLE tenant_a_role;\nCREATE ROLE tenant_b_role;\nGRANT SELECT ON orders_secure TO ROLE tenant_a_role;\nGRANT SELECT ON orders_secure TO ROLE tenant_b_role;\n```\n\n## Testing Strategy\n```sql\n-- Test tenant isolation\nUSE ROLE tenant_a_role;\nSET user_tenant = 'tenant_a';\nSELECT COUNT(*) FROM orders_secure; -- Should return only tenant_a rows\n\nUSE ROLE tenant_b_role;\nSET user_tenant = 'tenant_b';\nSELECT COUNT(*) FROM orders_secure; -- Should return only tenant_b rows\n\n-- Verify masking\nSELECT credit_card FROM orders_secure LIMIT 1; -- Should show only last 4 digits\n```","diagram":null,"difficulty":"advanced","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T05:05:23.988Z","createdAt":"2026-01-16T02:21:35.370Z"},{"id":"q-2657","question":"Beginner Snowflake-core: daily load from staged.orders_stage into production.orders, dedup by order_id. Columns: order_id STRING, customer_id STRING, amount NUMBER, load_date DATE. Write a single MERGE UPSERT that keeps only the latest load_date per order_id. Include exact MERGE SQL and a quick verify query?","answer":"MERGE INTO production.orders AS t USING ( SELECT order_id, customer_id, amount, load_date FROM ( SELECT *, ROW_NUMBER() OVER (PARTITION BY order_id ORDER BY load_date DESC) AS rn FROM staging.orders_s","explanation":"## Why This Is Asked\nTests ability to perform a safe upsert with de-dup logic using ROW_NUMBER and a single MERGE, a common beginner data-ops task.\n\n## Key Concepts\n- MERGE UPSERT\n- ROW_NUMBER windowing for de-dup\n- verification queries for data integrity\n\n## Code Example\n```sql\nMERGE INTO production.orders AS t\nUSING (\n  SELECT order_id, customer_id, amount, load_date\n  FROM (\n    SELECT *, ROW_NUMBER() OVER (PARTITION BY order_id ORDER BY load_date DESC) AS rn\n    FROM staging.orders_stage\n  ) s WHERE rn = 1\n) AS s\nON t.order_id = s.order_id\nWHEN MATCHED THEN UPDATE SET customer_id = s.customer_id, amount = s.amount, load_date = s.load_date\nWHEN NOT MATCHED THEN INSERT (order_id, customer_id, amount, load_date) VALUES (s.order_id, s.customer_id, s.amount, s.load_date);\n```\n\n## Follow-up Questions\n- How would you adjust for late-arriving data with different load_date semantics?\n- How would you test this in a non-prod environment?","diagram":null,"difficulty":"beginner","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","Oracle","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T05:42:34.776Z","createdAt":"2026-01-16T05:42:34.776Z"},{"id":"q-2692","question":"You're designing a beginner Snowflake data-curation task: given a raw table RAW.events(device_id string, event_ts timestamp_ntz, payload variant) ingesting 50M rows daily with a JSON payload containing fields event_type, region, and email (PII). Propose a concrete end-to-end approach to surface a curated table ANALYTICS.curated.events_daily with columns device_id, event_type, region, user_email where user_email is masked for external queries via a masking policy. Include exact SQL statements to (1) create the curated table, (2) extract fields, (3) define and apply masking policy on user_email, and (4) create a daily REFRESH TASK at 01:00 UTC that recomputes the curated table from the last 24 hours?","answer":"Create curated table ANALYTICS.curated.events_daily (device_id STRING, event_type STRING, region STRING, user_email STRING); load last 24h from RAW.events extracting payload fields: SELECT device_id, ","explanation":"## Why This Is Asked\nTests practical use of Snowflake’s data-curation primitives: using VARIANT payloads, risk-free exposure for external audiences, and automation with Tasks.\n\n## Key Concepts\n- Extracting fields from VARIANT payloads\n- Masking policies on table columns\n- ALTER TABLE to bind masking policies\n- Snowflake TASKs for scheduled refreshes\n- Lightweight, deterministic daily refresh pattern\n\n## Code Example\n```sql\n-- Create curated table\nCREATE OR REPLACE TABLE ANALYTICS.curated.events_daily (\n  device_id STRING,\n  event_type STRING,\n  region STRING,\n  user_email STRING\n);\n\n-- Load last 24h from RAW.events\nINSERT INTO ANALYTICS.curated.events_daily\nSELECT\n  device_id,\n  payload:event_type::STRING,\n  payload:region::STRING,\n  payload:email::STRING\nFROM RAW.events\nWHERE event_ts >= DATEADD(day,-1,CURRENT_TIMESTAMP());\n\n-- Masking policy\nCREATE OR REPLACE MASKING POLICY analytics.mask_email AS\n(VAL STRING) RETURNS STRING ->\nCASE\n  WHEN current_role() IN ('EXTERNAL_ANALYST') THEN\n    CONCAT(SUBSTR(VAL,1,1), '***', SUBSTR(VAL, -1))\n  ELSE VAL\nEND;\n\nALTER TABLE ANALYTICS.curated.events_daily\n  MODIFY COLUMN user_email SET MASKING POLICY analytics.mask_email;\n\n-- Daily refresh task\nCREATE OR REPLACE TASK ANALYTICS.daily_refresh.refresh_events\n  WAREHOUSE = 'COMPUTE_WH'\n  SCHEDULE = 'CRON 0 1 * * *'\nAS\n  TRUNCATE TABLE ANALYTICS.curated.events_daily;\n  INSERT INTO ANALYTICS.curated.events_daily\n  SELECT\n    device_id,\n    payload:event_type::STRING,\n    payload:region::STRING,\n    payload:email::STRING\n  FROM RAW.events\n  WHERE event_ts >= DATEADD(day,-1,CURRENT_TIMESTAMP());\n```\n\n## Follow-up Questions\n- How would you test masking behavior for different roles?\n- How would you handle schema evolution if payload fields change?","diagram":"flowchart TD\n  A[Raw events] --> B[Extract fields]\n  B --> C[Curated table]\n  C --> D[Masking policy]\n  D --> E[Daily refresh TASK]","difficulty":"beginner","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Discord","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T07:03:15.412Z","createdAt":"2026-01-16T07:03:15.412Z"},{"id":"q-2729","question":"Mid-level Snowflake-core: In a multi-tenant telemetry store, raw.events(tenant_id STRING, device_id STRING, ts TIMESTAMP_NTZ, payload VARIANT) feeds dashboards showing per-tenant events last 24h. Implement a Slowly Changing Dimension for tenants (tenants_dim) as Type 2 using Streams and Tasks, and a curated per-tenant 24h rollup without scanning payload. Provide exact SQL to (1) create SCD Type 2, (2) propagate changes, (3) build the 24h rollup, and (4) validation steps. Discuss trade-offs?","answer":"Implement an SCD Type 2 for tenants with a surrogate key (tenant_sk), current flag via end_date, and historical rows. Use a STREAM on the source tenant table and a TASK to upsert into tenants_dim, clo","explanation":"## Why This Is Asked\nTests ability to design SCD Type 2 in Snowflake, orchestrate via streams and tasks, and derive per-tenant rolling metrics without inspecting VARIANT payloads.\n\n## Key Concepts\n- Slowly Changing Dimension Type 2\n- Streams and Tasks orchestration\n- Surrogate keys and end_date semantics\n- Curated rollups without payload scans\n- Data validation and governance\n\n## Code Example\n```javascript\n-- SQL-like schema for SCD Type 2\nCREATE TABLE tenants_dim (\n  tenant_sk NUMBER PRIMARY KEY,\n  tenant_id STRING,\n  name STRING,\n  plan STRING,\n  status STRING,\n  start_date TIMESTAMP_NTZ,\n  end_date TIMESTAMP_NTZ\n);\n\n-- Source and stream setup (simplified)\nCREATE TABLE tenants_source (\n  tenant_id STRING,\n  name STRING,\n  plan STRING,\n  status STRING,\n  last_modified TIMESTAMP_NTZ\n);\nCREATE STREAM tenants_stream ON TABLE tenants_source;\n\n-- Task to apply changes would upsert into tenants_dim and close old rows when needed\n```\n\n## Follow-up Questions\n- How would you handle a tenant re-brand where tenant_id remains but name changes?\n- How would you test the SCD pipeline under high-velocity changes?","diagram":"flowchart TD\n  A[raw.events] --> B[stream: tenants_stream]\n  B --> C[staging: upsert logic]\n  C --> D[tenants_dim: SCD Type 2]\n  D --> E[rollup_24h: per-tenant events]\n  E --> F[dashboards]\n","difficulty":"intermediate","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T09:41:25.721Z","createdAt":"2026-01-16T09:41:25.721Z"},{"id":"q-2752","question":"Snowflake-core intermediate: You manage a multi-tenant analytics layer. Raw data lives in `raw.transactions` and `raw.users`. Design an end-to-end plan to expose BI-ready data per-tenant with: (a) a curated layer, (b) masking policies for PII (email, phone, card), (c) a ROW ACCESS POLICY tied to a per-tenant session context, (d) a SECURE VIEW for BI access, (e) incremental refresh via STREAMS/TASKS, and (f) validation and rollback. Include exact SQL commands for the masking policy, the row access policy, and a sample curated secure view?","answer":"Create a curated layer with tenant_id; mask PII (email, phone, card) using pii_mask; attach a row-access policy tenant_rap that allows rows where tenant_id equals the session tenant; expose via a SECU","explanation":"## Why This Is Asked\nTests practical governance: multi-tenant access, PI I masking, and incremental data delivery.\n\n## Key Concepts\n- Masking policies for PII columns\n- Row access policy driven by session context per tenant\n- Secure views for BI isolation\n- Streams and Tasks for incremental refresh\n- Validation and rollback strategies\n\n## Code Example\n```javascript\n-- Masking policy on PII\nCREATE MASKING POLICY pii_mask AS (VAL STRING) RETURNS STRING\n  -> CASE WHEN CURRENT_ROLE() IN ('FULL_ACCESS') THEN VAL ELSE '*****' END;\n\n-- Row access policy example (conceptual)\nCREATE ROW ACCESS POLICY tenant_rap AS (TENANT_ID STRING) RETURNS BOOLEAN\n  -> CASE WHEN CURRENT_ROLE() = 'TENANT_A' THEN TENANT_ID = 'TENANT_A' \n           WHEN CURRENT_ROLE() = 'TENANT_B' THEN TENANT_ID = 'TENANT_B' \n           ELSE FALSE END;\n\n-- Secure curated view example (conceptual)\nCREATE SECURE VIEW curated.transactions_view AS\nSELECT * FROM curated.transactions;\n```\n\n## Follow-up Questions\n- How would you audit tenant access to curated data?\n- What are failure modes for STREAM+TASK refresh and how would you mitigate them?","diagram":null,"difficulty":"intermediate","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Cloudflare","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T10:39:40.483Z","createdAt":"2026-01-16T10:39:40.483Z"},{"id":"q-2790","question":"Beginner Snowflake-core: daily ingest loads 1M rows into `staging.image_events` (image_id STRING, ts TIMESTAMP_NTZ, status STRING, size_bytes NUMBER). Propose a minimal ETL to populate a curated table `analytics.image_events` with columns `image_id STRING, day DATE, status STRING, size_kb NUMBER`, where day = DATE(ts) and size_kb = size_bytes/1024. Provide exact SQL for table creation and a single MERGE to upsert the batch. Include how you'd validate row counts and data types?","answer":"Use a daily MERGE from staging.image_events into analytics.image_events on (image_id, day). Transform day = DATE(ts), size_kb = size_bytes/1024. Create analytics.image_events(image_id STRING, day DATE","explanation":"## Why This Is Asked\nTests ability to design a simple, reliable ETL path with upserts, basic data typing, and data validation for a common Snowflake pattern.\n\n## Key Concepts\n- MERGE for incremental upserts\n- Derived columns: day from TIMESTAMP_NTZ and size_kb from bytes\n- Simple validation: row counts and sample value checks\n\n## Code Example\n```sql\nCREATE TABLE IF NOT EXISTS analytics.image_events (\n  image_id STRING,\n  day DATE,\n  status STRING,\n  size_kb NUMBER\n);\n\nMERGE INTO analytics.image_events AS t\nUSING (\n  SELECT image_id, DATE(ts) AS day, status, size_bytes/1024.0 AS size_kb\n  FROM staging.image_events\n) AS s\nON t.image_id = s.image_id AND t.day = s.day\nWHEN MATCHED THEN UPDATE SET t.status = s.status, t.size_kb = s.size_kb\nWHEN NOT MATCHED THEN INSERT (image_id, day, status, size_kb) VALUES (s.image_id, s.day, s.status, s.size_kb);\n```\n\n## Follow-up Questions\n- How would you handle late-arriving data or corrections to past days?\n- How would you automate this daily run and validate no regressions?","diagram":"flowchart TD\n  A[Daily Ingest] --> B[Staging: staging.image_events]\n  B --> C[Transform: day, size_kb]\n  C --> D[MERGE into analytics.image_events]\n  D --> E[Validation & Monitoring]","difficulty":"beginner","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Cloudflare","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T11:46:50.495Z","createdAt":"2026-01-16T11:46:50.495Z"},{"id":"q-2801","question":"Intermediate Snowflake-core: You’re building a per-tenant feature-flag telemetry store. Ingest ~5M events per day into raw.flags(tenant_id STRING, feature_id STRING, user_id STRING, eval_ts TIMESTAMP_NTZ, value VARIANT). Implement strict tenant isolation, per-tenant latency metrics for flag evaluation (p95 over last 5 minutes), and avoid scanning payloads for dashboards. Propose a production plan detailing schema, streams, tasks, RBAC + masking, auditing, validation, and rollback?","answer":"Proposed plan: isolate tenants in dedicated schemas; stream raw.flags (tenant_id, feature_id, user_id, eval_ts, value) into curated.flags_cache via a 30s TASK, backed by a flags_view. enforce row-leve","explanation":"## Why This Is Asked\nThis test checks ability to design tenant-aware analytics, combine streaming ingestion, per-tenant SLAs, and secure data access.\n\n## Key Concepts\n- STREAMS and TASKS\n- Row Access Policies and masking\n- Per-tenant dashboards with curated views\n- Time Travel and cloning for rollback\n\n## Code Example\n```javascript\n-- SQL-like example (labeled as javascript per spec)\nCREATE TABLE raw.flags (\n  tenant_id STRING,\n  feature_id STRING,\n  user_id STRING,\n  eval_ts TIMESTAMP_NTZ,\n  value VARIANT\n);\nCREATE STREAM s_flags ON TABLE raw.flags;\nCREATE TABLE curated.flags_cache (...);\nCREATE OR REPLACE VIEW flags_view AS SELECT * FROM curated.flags_cache;\n```\n\n## Follow-up Questions\n- How would you test correctness and latency under late-arriving data?\n- How would you monitor cost and adjust warehouses?","diagram":null,"difficulty":"intermediate","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","NVIDIA","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T13:07:10.851Z","createdAt":"2026-01-16T13:07:10.851Z"},{"id":"q-2836","question":"Design a Snowflake data-sharing solution for partner analytics. In RAW.partner_logs(partner_id STRING, ts TIMESTAMP_NTZ, payload VARIANT), expose per-partner curated analytics while preventing data leakage. Outline architecture using Streams, Tasks, and a curated view; implement masking for PII, per-partner access controls, and quotas. Provide concrete SQL for creating the share, roles, masking policy, and a test plan to verify isolation?","answer":"Leverage a per-partner share with a curated view filtered by the partner's role. Use a STREAM on RAW and a daily TASK to populate CURATED.partner_analytics; apply a masking policy to PII fields in the","explanation":"## Why This Is Asked\n\nDemonstrates architecture for secure, isolated data sharing at scale with governance.\n\n## Key Concepts\n\n- Data sharing governance and isolation\n- Streams and Tasks for incremental pipelines\n- Masking policies and role-based access\n- Per-partner quotas with resource monitors\n\n## Code Example\n\n```javascript\n-- Masking policy example\nCREATE MASKING POLICY pii_redact AS (val STRING) RETURNS STRING ->\n  CASE WHEN CURRENT_ROLE() IN ('PARTNER_A','PARTNER_B') THEN val ELSE 'REDACTED' END;\n\nALTER TABLE CURATED.partner_analytics MODIFY COLUMN payload.email SET MASKING POLICY pii_redact;\n```\n\n```javascript\n-- Share setup example\nCREATE SHARE partner_analytics_share;\nGRANT USAGE ON DATABASE analytics_db TO SHARE partner_analytics_share;\nALTER SHARE partner_analytics_share ADD SCHEMA analytics_db CURATED;\n\n-- Note: account-level IMPORT grants are handled on partner accounts\n```\n\n## Follow-up Questions\n\n- How would you test cross-partner isolation?\n- How would you update schemas without breaking shares?","diagram":null,"difficulty":"advanced","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Microsoft","NVIDIA","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T14:11:24.613Z","createdAt":"2026-01-16T14:11:24.613Z"},{"id":"q-2870","question":"You have a table raw.clicks(user_id STRING, click_ts TIMESTAMP_NTZ, price NUMBER) ingesting 20M rows/day. Build a daily aggregation stored in dashboard.daily_user_engagement(user_id STRING, day DATE, total_spend NUMBER, clicks INT) and refresh it every day at 02:00 UTC using a Snowflake Stream and Task. What are the exact steps and SQL commands you would run to implement this end-to-end?","answer":"Create a stream on raw.clicks to capture inserts, then a daily Task at 02:00 UTC that reads yesterday's data, aggregates by user_id and day, and MERGE-UPSERTs into dashboard.daily_user_engagement (tot","explanation":"## Why This Is Asked\nTests understanding of incremental ETL using Snowflake Streams and Tasks, and idempotent daily rollups.\n\n## Key Concepts\n- Streams for CDC on raw.clicks\n- Scheduled Tasks to drive daily refresh\n- MERGE upsert pattern for idempotent writes\n- Date arithmetic to anchor yesterday's window\n- Performance: avoid full reprocesses and minimize locking\n\n## Code Example\n```sql\n-- Create a stream to capture changes\nCREATE OR REPLACE STREAM s_clicks ON TABLE raw.clicks;\n\n-- Create a daily task to refresh yesterday's aggregation\nCREATE OR REPLACE TASK t_daily_agg\n  WAREHOUSE = compute_wh\n  SCHEDULE = 'USING CRON 0 2 * * * UTC'\nAS\nMERGE INTO dashboard.daily_user_engagement AS d\nUSING (\n  SELECT user_id, click_ts::date AS day, SUM(price) AS total_spend, COUNT(*) AS clicks\n  FROM raw.clicks\n  WHERE click_ts >= DATEADD(day, -1, CURRENT_DATE())\n  GROUP BY user_id, day\n) AS s\nON (d.user_id = s.user_id AND d.day = s.day)\nWHEN MATCHED THEN UPDATE SET d.total_spend = s.total_spend, d.clicks = s.clicks\nWHEN NOT MATCHED THEN INSERT (user_id, day, total_spend, clicks) VALUES (s.user_id, s.day, s.total_spend, s.clicks);\n```\n\n## Follow-up Questions\n- How would you handle days with no data to avoid stale rows?\n- How would you test the Task in a dev environment?","diagram":null,"difficulty":"beginner","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T15:40:45.344Z","createdAt":"2026-01-16T15:40:45.344Z"},{"id":"q-2998","question":"You operate a Snowflake data warehouse for a multi-tenant SaaS product ingesting telemetry in a table raw.events(tenant_id STRING, device_id STRING, ts TIMESTAMP_NTZ, payload VARIANT). Design a production plan to enforce per-tenant isolation in dashboards while still enabling cross-tenant analytics like top tenants by event count in the last 24 hours. Specify concrete SQL commands for row access policies, secure views, masking, and testing?","answer":"Implement a per-tenant row access policy on raw.events using tenant_id, and expose curated.events via a secure view that enforces the policy automatically. Add a masking policy for sensitive fields in","explanation":"## Why This Is Asked\nTests the ability to design multi-tenant governance with Snowflake features and production-grade security.\n\n## Key Concepts\n- Row access policies\n- Secure views\n- Masking policies\n- Snowflake Tasks and Streams\n- Testing isolation\n\n## Code Example\n```javascript\n-- Snowflake SQL: per-tenant isolation\nCREATE OR REPLACE ROW ACCESS POLICY rap_tenant ON raw.events\n  AS (tenant_id STRING) RETURNS BOOLEAN -> (CURRENT_ROLE() = 'ADMIN' OR raw.events.tenant_id = CURRENT_TENANT());\n```\n\n## Follow-up Questions\n- How would you monitor policy breaches and audit trails?\n- How would you handle onboarding new tenants with minimal downtime?","diagram":"flowchart TD\n  A[Ingest raw.events] --> B{Tenant filter}\n  B --> C[Secure view curated.events]\n  C --> D[Dashboards]\n  D --> E[(Top tenants metric)]","difficulty":"advanced","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Goldman Sachs","Instacart"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T20:39:22.960Z","createdAt":"2026-01-16T20:39:22.961Z"},{"id":"q-3022","question":"Design an end-to-end Snowflake architecture to securely share a telemetry store across regional accounts while enabling automated per-device anomaly scoring every 5 minutes. Specify data modeling (raw vs curated), clustering strategy, use of streams and tasks, cross-region data sharing with a reader account, row-level access controls, and a practical validation plan for correctness, latency, and cost?","answer":"Use a per-device clustering key on (DEVICE_ID, TS) in the curated layer. Ingest via a STREAM on raw.events and run a periodic TASK to populate curated.device_scores with 5-minute windows. Expose curated.device_scores through a secure DATA SHARE with a reader account, implement ROW ACCESS POLICY for device-level governance, and validate with end-to-end latency testing and cost monitoring.","explanation":"## Why This Is Asked\n\nTests understanding of end-to-end Snowflake architectures: data modeling between raw and curated layers, efficient querying via clustering, and production-grade data sharing with governance across regions.\n\n## Key Concepts\n\n- Secure Data Sharing with reader accounts across regions\n- Streams and Tasks for incremental processing\n- Per-device windowed scoring for anomaly detection\n- ROW ACCESS POLICY and Dynamic Data Masking for governance\n- Cost-aware validation (latency, query counts, compute credits)\n\n## Code Example\n\n```sql\n-- Create a stream on the raw telemetry to feed\nCREATE OR REPLACE STREAM raw_events_stream\nON TABLE raw.events;\n\n-- Set up periodic task for 5-minute scoring\nCREATE OR REPLACE TASK score_devices_task\nWAREHOUSE = SCORING_WH\nSCHEDULE = '5 MINUTE'\nAS\nBEGIN\n  INSERT INTO curated.device_scores\n  SELECT \n    device_id,\n    window_start,\n    anomaly_score,\n    confidence_level\n  FROM raw_events_stream\n  MATCH_RECOGNIZE(\n    PARTITION BY device_id\n    ORDER BY ts\n    MEASURES\n      FIRST(ts) AS window_start,\n      anomaly_detection(metrics) AS anomaly_score,\n      confidence_calc(metrics) AS confidence_level\n    PATTERN (STRT FUTURE*)\n    DEFINE\n      FUTURE AS metrics > baseline * 1.5\n  );\nEND;\n\n-- Enable secure data sharing\nCREATE DATA SHARE telemetry_share\nACCOUNTS = ('reader_account_1', 'reader_account_2');\n\nGRANT USAGE ON DATABASE curated TO SHARE telemetry_share;\nGRANT SELECT ON TABLE curated.device_scores TO SHARE telemetry_share;\n\n-- Implement row-level security\nCREATE ROW ACCESS POLICY device_policy\nAS (device_id VARCHAR) RETURNS BOOLEAN ->\n  CURRENT_ROLE() = 'ADMIN' OR \n  CURRENT_ROLE() = 'REGION_MANAGER' OR\n  device_id IN (SELECT device_id FROM user_device_mapping WHERE user_id = CURRENT_USER());\n\nAPPLY ROW ACCESS POLICY device_policy ON TABLE curated.device_scores;\n```\n\n## Validation Plan\n\n1. **Correctness**: Compare stream-based results with batch processing on sample datasets\n2. **Latency**: Measure end-to-end processing time from ingestion to scoring completion\n3. **Cost**: Monitor compute credits used by the scoring task and optimize warehouse sizing\n4. **Governance**: Verify row access policies enforce proper device-level restrictions","diagram":null,"difficulty":"advanced","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Slack","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T04:32:25.323Z","createdAt":"2026-01-16T21:40:31.702Z"},{"id":"q-3091","question":"You're running a Snowflake-based analytics platform used by multiple squads. Design a cost-governance system that enforces per-squad monthly credit budgets, auto-suspends idle warehouses, and dynamically prioritizes critical ETL workloads during business hours using RESOURCE_MONITORS, ACCOUNT_USAGE, and a quotas table. Provide concrete SQL for monitors, a JavaScript stored procedure to estimate remaining credits, a daily TASK to refresh quotas, and Slack alert examples?","answer":"Design per-squad monthly budgets using a dedicated RESOURCE_MONITOR per squad, linked to a squad tag. Track usage via ACCOUNT_USAGE.CREDIT_USAGE and a quotas table. Enforce auto_suspend on idle wareho","explanation":"## Why This Is Asked\nTests practical cost governance in Snowflake, not just performance. Requires knowledge of MONITORS, usage views, scheduling, and automation, plus cross-team coordination.\n\n## Key Concepts\n- RESOURCE_MONITOR, CREDIT_USAGE, ACCOUNT_USAGE\n- Storage of per-squad quotas, and dynamic reallocations\n- TASKS, SCHEDULED LOGIC, and alerting (Slack/Email)\n- Safety: rollback, testing dashboards, and rollback paths\n\n## Code Example\n```sql\n-- Create a per-squad resource monitor (example)\nCREATE RESOURCE MONITOR squadA_mon\n  WITH CREDIT_QUOTA = 10000\n  TRIGGERS ON 80 PERCENT DO SUSPEND;\n```\n```javascript\n// Skeleton: remaining credits calculator\nCREATE OR REPLACE PROCEDURE remaining_credits(squad_id STRING)\nRETURNS FLOAT\nLANGUAGE JAVASCRIPT\nAS $$\n  // QUERY ACCOUNT_USAGE and quotas table to compute remaining credits\n  return 0.0;\n$$;\n```\n## Follow-up Questions\n\n- How would you test the monitor thresholds and rollback plan?\n- How would you handle quota resets across fiscal months?\n","diagram":"flowchart TD\n  A[Squad Budget] --> B[ squad_mon monitors ]\n  B --> C[Alerts]\n  C --> D[Slack]\n  B --> E[Auto-suspend idle warehouses]\n  E --> F[Cost savings]","difficulty":"intermediate","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T02:15:33.371Z","createdAt":"2026-01-17T02:15:33.371Z"},{"id":"q-3170","question":"In a Snowflake data lake for an online retailer, raw.orders(order_id STRING, customer_id STRING, order_ts TIMESTAMP_NTZ, items VARIANT, total DECIMAL) ingested 80M rows daily via Snowpipe. Propose a production plan for end-to-end data quality: non-null and unique checks, item totals reconcile to total, automated quarantine for failures, streaming + batch QA, and exact SQL snippets plus rollback/alert procedures?","answer":"Use a streaming QA pipeline: stream on raw.orders, a curated.orders_qc view, and a nightly task that flags anomalies and quarantines bad rows. Enforce not-null and unique order_id, reconcile item sums","explanation":"## Why This Is Asked\n\nAssesses practical data quality design in Snowflake, including streams, tasks, quarantine workflows, and how QA results feed dashboards.\n\n## Key Concepts\n\n- Snowflake Streams and Tasks\n- Data quality checks and quarantine tables\n- Ingestion patterns (Snowpipe) and batch vs streaming QA\n- Alerts and dashboards for QA health\n\n## Code Example\n\n```sql\n-- Create a change-tracking stream\nCREATE OR REPLACE STREAM s_orders ON TABLE raw.orders APPEND_ONLY = FALSE;\n\n-- QC view aggregating basic checks\nCREATE OR REPLACE VIEW curated.orders_qc AS\nSELECT\n  (SELECT COUNT(*) FROM raw.orders) AS total_rows,\n  (SELECT COUNT(*) FROM raw.orders WHERE order_id IS NULL) AS null_order_id,\n  (SELECT COUNT(*) FROM (SELECT order_id, COUNT(*) AS c FROM raw.orders GROUP BY order_id HAVING COUNT(*) > 1)) AS dup_order_ids;\n\n-- Quarantine table for bad records\nCREATE OR REPLACE TABLE quarantine.orders_bad LIKE raw.orders;\n\n-- Move bad records to quarantine\nINSERT INTO quarantine.orders_bad\nSELECT * FROM raw.orders\nWHERE order_id IS NULL\n   OR order_id IN (\n     SELECT order_id FROM raw.orders GROUP BY order_id HAVING COUNT(*) > 1\n   );\n```\n\n```sql\n-- Nightly QA job example (pseudo)\nINSERT INTO curated.orders_qc\nSELECT total_rows, null_order_id, dup_order_ids FROM s_orders;\n```\n\n## Follow-up Questions\n\n- How would you handle late-arriving orders in QC? \n- How would you scale checks during peak ingestion?","diagram":"flowchart TD\n  A[Snowpipe Ingest] --> B[Raw.Orders]\n  B --> C[Stream s_orders]\n  C --> D[QC Task]\n  D -- Pass --> E[Curated.Orders]\n  D -- Fail --> F[Quarantine.Orders_Bad]\n  E --> G[Dashboards]\n  F --> H[Alerts]","difficulty":"intermediate","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","Oracle","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T05:33:43.491Z","createdAt":"2026-01-17T05:33:43.491Z"},{"id":"q-3236","question":"Intermediate Snowflake-core: You run a ML feature store for a fintech app. Ingest 1B feature rows daily into RAW.FEATURES(feature_id STRING, feature_value VARIANT, feature_ts TIMESTAMP_NTZ, customer_id STRING, model_id STRING). Build CURATED.FEATURES with versioning and a SERVING.FEATURES path for online inference with sub-50 ms latency. Provide concrete SQL for ingestion, version upserts (MERGE), MV/secure views, per-model RBAC, and data quality tests, plus a rollback plan?","answer":"Design a two-tier store: RAW for ingestion; CURATED with per-key versioning; SERVING for online inference with <50 ms latency. Use MERGE from RAW to CURATED to upsert latest per (feature_id,customer_i","explanation":"## Why This Is Asked\nTests end-to-end Snowflake feature store design: ingestion, versioned curated data, fast serving, governance, and rollback strategies that real fintechs require.\n\n## Key Concepts\n- Feature versioning and time travel\n- MERGE upserts for incremental updates\n- Serving layer with MV or clustered results\n- Row-access policies and per-model RBAC\n- Data quality checks and rollback planning\n\n## Code Example\n```javascript\n-- Ingest: RAW.FEATURES\nCREATE TABLE RAW.FEATURES (\n  feature_id STRING, feature_value VARIANT, feature_ts TIMESTAMP_NTZ,\n  customer_id STRING, model_id STRING\n);\n\n-- Curated: CURATED.FEATURES\nCREATE TABLE CURATED.FEATURES (\n  feature_id STRING, customer_id STRING, model_id STRING,\n  feature_ts TIMESTAMP_NTZ, feature_value VARIANT, version NUMBER\n);\n\n-- Merge to upsert latest per key\nMERGE INTO CURATED.FEATURES AS c\nUSING (\n  SELECT feature_id, customer_id, model_id, feature_ts, feature_value,\n         ROW_NUMBER() OVER (PARTITION BY feature_id, customer_id, model_id\n                            ORDER BY feature_ts DESC) AS rn\n  FROM RAW.FEATURES\n) AS s\nON (c.feature_id = s.feature_id AND c.customer_id = s.customer_id\n    AND c.model_id = s.model_id AND s.rn = 1)\nWHEN MATCHED THEN UPDATE SET\n  feature_ts = s.feature_ts, feature_value = s.feature_value,\n  version = c.version + 1\nWHEN NOT MATCHED THEN INSERT (feature_id, customer_id, model_id, feature_ts, feature_value, version)\n  VALUES (s.feature_id, s.customer_id, s.model_id, s.feature_ts, s.feature_value, 1);\n```\n\n## Follow-ups\n- How would you monitor cost and latency under load?\n- How would you test schema evolution and rollback?","diagram":"flowchart TD\n  RAW[RAW.FEATURES ingest] --> CURATED[CURATED.FEATURES upsert]\n  CURATED --> SERVING[SERVING.FEATURES latest per key]\n  RAW --> QA[Ingest QA checks]\n  CURATED --> QA\n  RBAC[RBAC Policies] --> SERVING","difficulty":"intermediate","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","Square","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T08:36:01.455Z","createdAt":"2026-01-17T08:36:01.455Z"},{"id":"q-3370","question":"Global Snowflake analytics pipeline: Ingest 100M events/day into raw.events(account_id STRING, user_id STRING, event_type STRING, ts TIMESTAMP_NTZ, metadata VARIANT). Design for (a) last-15-minute per-segment user counts from metadata, (b) cohort LV by region, (c) share a governed subset with a partner via secure data sharing while masking PII. Include data model, ingestion, MV vs table, access controls, validation, and rollback. End with a practical SQL snippet?","answer":"Two-layer model: RAW.events and CURATED.analytics. Ingest via Snowpipe into RAW.events, then use STREAM+TASK to populate CURATED.analytics.segment and CURATED.analytics.last15m_users. Build COHORT MV ","explanation":"## Why This Is Asked\nTests end-to-end Snowflake workflow: ingestion, transformation, security, and data sharing under real-world constraints.\n\n## Key Concepts\n- Snowpipe, Streams, Tasks for nearreal-time pipelines\n- Raw vs curated data models and MV vs table trade-offs\n- Row access policies and dynamic masking for PII\n- Secure Data Sharing / Data Exchange with governance\n- Validation, anomaly checks, and Time Travel rollback\n\n## Code Example\n```sql\n-- example: create a stream on raw.events\nCREATE OR REPLACE STREAM raw.events_stream ON TABLE raw.events (APPEND_ONLY = FALSE);\n```\n\n## Follow-up Questions\n- How would you handle evolving metadata schemas without breaking downstreams?\n- What tests verify freshness and accuracy of the 15-minute dashboards?","diagram":null,"difficulty":"intermediate","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Microsoft","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T13:45:00.344Z","createdAt":"2026-01-17T13:45:00.344Z"},{"id":"q-3386","question":"Design an idempotent ingest pipeline for Snowflake where JSON events land in S3 and are loaded via Snowpipe into RAW.STAGING_EVENTS(event_id, device_id, ts, payload); ensure duplicates are prevented and late/out-of-order data is handled; provide schema, streams, MERGE logic, a TASK, and validation steps with exact commands?","answer":"Use a unique event_id; ingest JSON into RAW.STAGING_EVENTS; create a STREAM on RAW.STAGING_EVENTS and MERGE into RAW.EVENTS using event_id as the dedupe key; configure Snowpipe with ON_ERROR=CONTINUE;","explanation":"## Why This Is Asked\n\nThis question probes idempotent ingestion design in Snowflake, combining Snowpipe, Streams, and scheduled Tasks to tolerate retries and late data while avoiding duplicates.\n\n## Key Concepts\n\n- Snowpipe ingestion and staged data\n- Streams for CDC and deduplication\n- MERGE upserts with event_id as key\n- Scheduling and validation for retries\n\n## Code Example\n\n```sql\n-- Create staging\nCREATE OR REPLACE TABLE RAW.STAGING_EVENTS (\n  event_id STRING,\n  device_id STRING,\n  ts TIMESTAMP_NTZ,\n  payload VARIANT\n);\n\n-- Create final\nCREATE OR REPLACE TABLE RAW.EVENTS (\n  event_id STRING PRIMARY KEY,\n  device_id STRING,\n  ts TIMESTAMP_NTZ,\n  payload VARIANT\n);\n\n-- Create stream\nCREATE OR REPLACE STREAM RAW.STAGING_EVENTS_STREAM ON TABLE RAW.STAGING_EVENTS (APPEND_ONLY => FALSE);\n\n-- Merge into final\nMERGE INTO RAW.EVENTS AS F\nUSING RAW.STAGING_EVENTS_STREAM AS S\n  ON F.event_id = S.event_id\nWHEN NOT MATCHED THEN\n  INSERT (event_id, device_id, ts, payload)\n  VALUES (S.event_id, S.device_id, S.ts, S.payload);\n```\n\n## Follow-up Questions\n\n- How would you adapt this for late-arriving events that arrive with ts earlier than current window?\n- How would you test idempotence under simulated retries and network errors?","diagram":null,"difficulty":"advanced","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T14:28:16.545Z","createdAt":"2026-01-17T14:28:16.545Z"},{"id":"q-3424","question":"In a Snowflake analytics layer ingesting raw.events(tenant_id STRING, device_id STRING, ts TIMESTAMP_NTZ, payload VARIANT), implement per-tenant data isolation and PII masking at query time. Outline a scalable approach using ROW ACCESS POLICY and MASKING POLICY, surface a tenant-scoped view, and provide exact SQL to bootstrap the policies, create the view, and validate separation for two roles?","answer":"Use a session-scoped tenant_id with a ROW ACCESS POLICY to filter rows, and masking policies to redact PII unless the user has an admin role. Surface per-tenant data via a curated view that extracts a","explanation":"## Why This Is Asked\nTests practical use of Snowflake's row access and masking policies, plus secure per-tenant data exposure at scale.\n\n## Key Concepts\n- ROW ACCESS POLICY and SESSION_CONTEXT usage\n- MASKING POLICY for PII within VARIANT payloads\n- Surface data through a tenant-scoped view, not direct table access\n- Role-based access testing across tenant_user vs admin\n\n## Code Example\n```sql\n-- Mask email/phone fields in the payload via a view\nCREATE OR REPLACE MASKING POLICY mask_email AS (val STRING) RETURNS STRING ->\n  CASE WHEN CURRENT_ROLE() IN ('TENANT_ADMIN','ADMIN') THEN val ELSE 'REDACTED' END;\n\nCREATE OR REPLACE MASKING POLICY mask_phone AS (val STRING) RETURNS STRING ->\n  CASE WHEN CURRENT_ROLE() IN ('TENANT_ADMIN','ADMIN') THEN val ELSE 'XXX-XXX-XXXX' END;\n\n-- Row access policy using a session-scoped tenant_id\nCREATE OR REPLACE ROW ACCESS POLICY ra_tenant AS (t STRING) RETURNS BOOLEAN ->\n  CURRENT_ROLE() IN ('SYSADMIN','ADMIN') OR t = CURRENT_SESSION('TENANT_ID');\n\n-- Apply: (1) create a curated view that extracts fields, (2) apply masking on extracted columns\nCREATE OR REPLACE VIEW curated.events_per_tenant AS\nSELECT tenant_id,\n       device_id,\n       ts,\n       payload,\n       mask_email(payload:'email') AS email_masked,\n       mask_phone(payload:'phone') AS phone_masked\nFROM raw.events\nWHERE ra_tenant(tenant_id);\n```\n\n## Follow-up Questions\n- How would you validate performance and cost for the view under high concurrency?\n- How would you evolve this approach if privacy regs require per-tenant data retention controls?","diagram":"flowchart TD\n  A[Tenant User] --> B[ROW ACCESS POLICY ra_tenant]\n  B --> C[raw.events]\n  D[Admin Role] --> B\n  E[Masking Policy mask_email] --> F[email_masked]\n  G[Masking Policy mask_phone] --> H[phone_masked]\n  I[Tenant-scoped View curated.events_per_tenant] --> J[BI Dashboards]","difficulty":"advanced","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Microsoft","Netflix","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T15:38:40.106Z","createdAt":"2026-01-17T15:38:40.106Z"},{"id":"q-3496","question":"You're adding two new fields to a Snowflake telemetry table raw.events(device_id STRING, ts TIMESTAMP_NTZ, payload VARIANT): source STRING and ingested_by STRING. Propose a zero-downtime schema evolution strategy using zero-copy cloning, time travel, and a view-based interface. Provide exact SQL to (1) evolve the schema safely, (2) create a curated view for BI that remains backward-compatible, (3) perform a safe backfill and validate with test queries, and (4) outline rollback steps?","answer":"Keep raw.events immutable; create raw.events_v2 with source and ingested_by; backfill in batches from raw.events. Expose curated_events as a view over v2. Switch BI to curated_events and monitor. Vali","explanation":"## Why This Is Asked\nThis tests zero-downtime schema evolution in Snowflake with large tables, ensuring data safety while keeping dashboards running.\n\n## Key Concepts\n- Zero-downtime schema evolution\n- Zero-copy cloning\n- Time travel\n- Views vs tables\n- Backfill and validation\n\n## Code Example\n```sql\n-- Step 1: clone and alter\nCREATE TABLE raw.events_v2 LIKE raw.events;\nALTER TABLE raw.events_v2 ADD COLUMN source STRING;\nALTER TABLE raw.events_v2 ADD COLUMN ingested_by STRING;\n\n-- Step 2: backfill\nINSERT INTO raw.events_v2 (device_id, ts, payload, source, ingested_by)\nSELECT device_id, ts, payload, NULL, NULL FROM raw.events;\n\n-- Step 3: curated view\nCREATE OR REPLACE VIEW curated_events AS\nSELECT device_id, ts, payload, source, ingested_by FROM raw.events_v2;\n\n-- Step 4: swap consumption (example)\n-- BI queries should reference curated_events\n```\n\n## Follow-up Questions\n- How would you automate backfill monitoring and alerts?\n- How would you validate no regression in dashboards after swap?\n","diagram":null,"difficulty":"advanced","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Coinbase","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T18:51:57.729Z","createdAt":"2026-01-17T18:51:57.729Z"},{"id":"q-3559","question":"Beginner Snowflake-core: You have prod_db.sales with tables customers and orders. A dev sandbox is needed without copying data. Using zero-copy cloning, outline exact steps and provide commands to (1) clone prod_db.sales into dev_db.sales_clone, (2) restrict writes on prod, (3) grant only read access to the clone for role DEV, (4) set data retention on the clone to 7 days, and (5) verify isolation so changes in the clone do not affect prod. What commands would you run and why?","answer":"Clone production to development: CREATE DATABASE dev_db.sales_clone CLONE prod_db.sales; Restrict writes on production: REVOKE INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA prod_db.sales FROM ROLE DEV; Grant read-only access to clone: GRANT USAGE ON DATABASE dev_db.sales_clone TO ROLE DEV; GRANT SELECT ON ALL TABLES IN SCHEMA dev_db.sales_clone TO ROLE DEV; Set data retention: ALTER DATABASE dev_db.sales_clone SET DATA_RETENTION_TIME_IN_DAYS = 7; Verify isolation: Test by inserting into clone tables and confirming production tables remain unchanged.","explanation":"## Why This Is Asked\n\nTests knowledge of Snowflake's zero-copy cloning and sandbox governance for beginners.\n\n## Key Concepts\n\n- Zero-copy cloning\n- Access control (USAGE, SELECT, REVOKE)\n- Data retention settings\n- Isolation verification\n\n## Code Example\n\n```sql\n-- 1. Clone production database\nCREATE DATABASE dev_db.sales_clone CLONE prod_db.sales;\n\n-- 2. Restrict writes on production\nREVOKE INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA prod_db.sales FROM ROLE DEV;\n\n-- 3. Grant read-only access to clone\nGRANT USAGE ON DATABASE dev_db.sales_clone TO ROLE DEV;\nGRANT SELECT ON ALL TABLES IN SCHEMA dev_db.sales_clone TO ROLE DEV;\n\n-- 4. Set data retention on clone\nALTER DATABASE dev_db.sales_clone SET DATA_RETENTION_TIME_IN_DAYS = 7;\n\n-- 5. Verify isolation\n-- Test by inserting into clone tables and confirming production tables remain unchanged\n```","diagram":"flowchart TD\n  A[Clone prod to dev] --> B[Restrict prod writes]\n  B --> C[Grant read on clone to DEV]\n  C --> D[Set retention on clone]\n  D --> E[Verify isolation]","difficulty":"beginner","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","LinkedIn","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T05:45:54.724Z","createdAt":"2026-01-17T21:29:17.695Z"},{"id":"q-3657","question":"Scenario: In Snowflake you ingest 100M events/day into raw.telemetry(tenant_id STRING, device_id STRING, ts TIMESTAMP_NTZ, payload VARIANT). You must generate near real-time alerts for anomalous temperature spikes per device with <= 2 minutes latency, while preserving per-tenant isolation for dashboards. Design a pipeline using Streams and Tasks (and decide on Snowpipe vs auto-ingest), specify idempotent processing, dead-letter handling, monitoring, and provide concrete SQL patterns for alert generation and backfill?","answer":"Two-tier pipeline: use a stream on raw.telemetry, a TASK every 2 minutes to compute per-device 5-min rolling max temperature, flag spikes beyond a threshold, deduplicate with a sequence and METADATA$S","explanation":"## Why This Is Asked\nTests practical use of Snowflake streams, tasks, and ingestion strategies to meet strict latency while maintaining tenant isolation. It also probes idempotent processing, dead-letter handling, and observability.\n\n## Key Concepts\n- Snowflake Streams and Tasks for near-real-time work\n- Raw vs curated data layers and per-tenant isolation\n- Idempotent MERGE patterns and stream offsets\n- Latency monitoring and backfill handling\n\n## Code Example\n```sql\n-- Example: create stream and task (illustrative)\nCREATE OR REPLACE STREAM raw.telemetry_stream ON TABLE raw.telemetry (APPEND_ONLY = FALSE);\nCREATE OR REPLACE TASK alert_task\nWAREHOUSE = WH\nSCHEDULE = '2 MINUTE'\nAS\nMERGE INTO curated.alerts AS a\nUSING (\n  SELECT device_id, tenant_id, MAX(payload:\"temperature\")::FLOAT AS max_temp, MAX(ts) AS ts\n  FROM raw.telemetry_stream\n  GROUP BY device_id, tenant_id\n) AS s\nON a.device_id = s.device_id AND a.tenant_id = s.tenant_id\nWHEN MATCHED THEN UPDATE SET max_temp = s.max_temp, ts = s.ts\nWHEN NOT MATCHED THEN INSERT (device_id, tenant_id, max_temp, ts) VALUES (s.device_id, s.tenant_id, s.max_temp, s.ts);\n```\n\n## Follow-up Questions\n- How would you handle late data and backfills?\n- How would you test accuracy and latency per tenant in production?","diagram":"flowchart TD\n  A[Raw Telemetry] --> B[Stream: raw.telemetry_stream]\n  B --> C[Consolidation: alert_task]\n  C --> D[Curated: curated.alerts]\n  D --> E[Dashboards per tenant]","difficulty":"intermediate","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T04:12:06.507Z","createdAt":"2026-01-18T04:12:06.507Z"},{"id":"q-3718","question":"Design a near-real-time anomaly detection pipeline in Snowflake for telemetry data: raw.events(tenant_id, device_id, ts, value). Propose data model, a streaming ingestion, and a scheduled task that recomputes per-tenant z-scores over the last 5 minutes every minute, flags |z|>3, and writes to curated.anomalies; expose a secure view for dashboards with per-tenant filtering and discuss latency, cost, and failure handling?","answer":"Use a STREAM on raw.events and a TASK every minute to compute per-tenant/device z-scores over the last 5 minutes; write anomalies to curated.anomalies(tenant_id, ts, device_id, value, zscore, is_anoma","explanation":"## Why This Is Asked\nTests ability to design near-real-time per-tenant anomaly pipeline using Snowflake streams, tasks, and windowed analytics; evaluates architectural choices, latency, and cost control.\n\n## Key Concepts\n- Snowflake STREAMS and TASKS for incremental processing\n- Windowed statistics: mean/stddev per (tenant_id, device_id)\n- Per-tenant security: secure views and access controls\n- Clustering strategy and cost considerations\n\n## Code Example\n```sql\n-- simplified per-tenant z-score for last 5 minutes\nSELECT tenant_id, device_id, ts, value,\n       (value - AVG(value) OVER (PARTITION BY tenant_id, device_id\n                                 ORDER BY ts\n                                 RANGE BETWEEN INTERVAL '5' MINUTE PRECEDING AND CURRENT ROW)) /\n       NULLIF(STDDEV_POP(value) OVER (PARTITION BY tenant_id, device_id\n                                 ORDER BY ts\n                                 RANGE BETWEEN INTERVAL '5' MINUTE PRECEDING AND CURRENT ROW), 0)\n       AS zscore\nFROM raw.events;\n```\n\n## Follow-up Questions\n- How would you handle late-arriving data or out-of-order events?\n- What monitoring would you set up to detect pipeline drift or skew?","diagram":"flowchart TD\n  A[Ingest Raw Events] --> B[Create STREAM on raw.events]\n  B --> C[Scheduled TASK (1 min)]\n  C --> D[Compute rolling stats per tenant/device]\n  D --> E[Write to curated.anomalies]\n  E --> F[Secure view anomalies_by_tenant for dashboards]","difficulty":"intermediate","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Discord","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T06:53:07.228Z","createdAt":"2026-01-18T06:53:07.228Z"},{"id":"q-3791","question":"You operate a Snowflake data warehouse for a multi-tenant platform ingesting 100M events/day into raw.events(tenant_id STRING, device_id STRING, ts TIMESTAMP_NTZ, payload VARIANT). Partners require a secure data share of a masked subset. Design a production plan that covers: (a) handling payload schema drift with semi-structured data, (b) per-tenant isolation for dashboards, (c) a rolling 24h MV for event type counts, (d) secure, revocable data sharing with masking, (e) validation, rollback, and alerting. Include concrete SQL for masking rules and a sample secure view?","answer":"Leverage VARIANT for payload drift; ingest 100M daily via Snowpipe into raw.events(tenant_id, device_id, ts, payload). Build curated.events with MV rolling 24h by tenant and event_type, plus a per-ten","explanation":"## Why This Is Asked\nTests the ability to design end-to-end ingestion and governance for a multi-tenant, high-volume Snowflake warehouse: schema drift, MV strategies, and secure data sharing.\n\n## Key Concepts\n- VARIANT for semi-structured data\n- Snowpipe ingestion and Streams/Tasks for upserts\n- MV vs table for rolling dashboards\n- Row-level and masking policies for partner shares\n- Time travel and failover validation\n\n## Code Example\n```sql\n-- Example DDL illustrating masking and secure sharing\nCREATE MASKING POLICY pii_email_mask AS (val STRING) RETURNS STRING ->\n  CASE WHEN current_role() IN ('PARTNER_READ') THEN 'REDACTED' ELSE val END;\n\nCREATE OR REPLACE VIEW partner_share_vw AS\nSELECT tenant_id,\n       payload:device_id::STRING AS device_id,\n       pii_email_mask(payload:email::STRING) AS user_email_masked,\n       payload:event_type::STRING AS event_type,\n       ts\nFROM curated.events;\n```\n\n## Follow-up Questions\n- How would you monitor data freshness and mask policy compliance across tenants?\n- What testing strategy ensures rollback safety for the secure share?","diagram":"flowchart TD\nA[Ingest 100M events/day] --> B[Raw->Curated: MV by tenant/event_type]\nB --> C[Rolling 24h metrics for dashboards]\nC --> D[Secure, revocable partner data share]\nD --> E[Tenant-scoped dashboards and alerts]","difficulty":"intermediate","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","Instacart","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T09:39:58.170Z","createdAt":"2026-01-18T09:39:58.170Z"},{"id":"q-3911","question":"Beginner Snowflake-core: You need a dev sandbox for prod_sales that mirrors production but excludes PII and minimizes cost. Outline a practical plan using (1) cloning the schema, (2) masking policies for SSN and email, (3) a read-only data share to the sandbox, and (4) a nightly refresh. Include the exact SQL commands you would run for each step?","answer":"Clone prod_db.prod_sales to sandbox_db via CREATE DATABASE sandbox_db CLONE prod_db; mask pii with masking policies: CREATE MASKING POLICY mask_ssn AS (v STRING) RETURNS STRING -> substr(v,1,3)||'***-","explanation":"## Why This Is Asked\n\nTests practical workflow for creating safe dev sandboxes in Snowflake: cloning, masking policies, secure sharing, and scheduled refresh. Encourages thinking about cost control and data governance at beginner level.\n\n## Key Concepts\n\n- Cloning and zero-copy databases\n- Masking policies on sensitive columns\n- Data sharing and access controls\n- Scheduling and automation with TASK\n\n## Code Example\n\n```javascript\n-- masking policy example\nCREATE MASKING POLICY mask_ssn AS (v STRING) RETURNS STRING -> substr(v,1,3) || '***-****';\n```\n\n## Follow-up Questions\n\n- How would you test masking effectiveness?\n- How would you handle schema changes in prod and reflect them in sandbox?","diagram":"flowchart TD\n  A[Clone prod_db] --> B[Sandbox DB]\n  B --> C[Masking policies]\n  C --> D[Shared sandbox]\n  D --> E[Nightly refresh]","difficulty":"beginner","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Databricks","Instacart"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T14:45:02.411Z","createdAt":"2026-01-18T14:45:02.411Z"},{"id":"q-3938","question":"Beginner Snowflake-core: Onboard a new partner to run analytics on sanitized sales data without touching PII or prod tables. Design a minimal reproducible setup using (1) zero-copy cloning for a dev sandbox, (2) a masking policy on email and phone in a curated view, (3) a secure view enforcing partner_role access, (4) a read-only data share to the partner’s account, and (5) a daily refresh pipeline with Snowflake Tasks. Provide exact SQL for each step?","answer":"Clone: CREATE DATABASE sandbox_curated CLONE prod_curated; Masking: CREATE MASKING POLICY mp_email AS (e STRING) RETURNS STRING -> CASE WHEN CURRENT_ROLE() IN ('PARTNER') THEN e ELSE NULL END; Secure ","explanation":"## Why This Is Asked\nThis probes practical Snowflake-core skills: sandboxing with zero-copy cloning, data masking decisions, secure views for role-based access, external sharing, and scheduled refreshes. It tests end-to-end governance with minimal risk.\n\n## Key Concepts\n- Zero-copy cloning for safe sandboxes\n- Dynamic/masked data handling for PII\n- Secure views with access control\n- Data shares across accounts\n- Snowflake Tasks for automation\n\n## Code Example\n```sql\n-- Masking policy example (SQL)\nCREATE MASKING POLICY mp_email AS (email STRING) RETURNS STRING ->\n  CASE WHEN CURRENT_ROLE() IN ('PARTNER') THEN email ELSE NULL END;\n```\n\n## Follow-up Questions\n- How would you audit data access across the shared sandbox?\n- What are the considerations for revoking access if the partner no longer should see data?","diagram":"flowchart TD\n  A[Clone prod_curated] --> B[Masking policy] \n  B --> C[Secure view] \n  C --> D[Data share to partner] \n  D --> E[Daily refresh task]","difficulty":"beginner","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Hugging Face","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T16:34:24.753Z","createdAt":"2026-01-18T16:34:24.754Z"},{"id":"q-4079","question":"Beginner Snowflake-core: daily ingestion of 1M rows into RAW_TRANSACTIONS (id STRING, user_id STRING, amount NUMBER, status STRING, txn_date DATE). Implement a data quality guard using a STREAM on RAW_TRANSACTIONS and a TASK every 6 hours to check (1) non-null critical fields, (2) amount >= 0, (3) user_id exists in DIM_USERS. On failures, log into QA_LOG (txn_id, issue, ts). Provide exact SQL for: a) create stream on RAW_TRANSACTIONS, b) create QA_LOG, c) create a stored procedure or SQL block that runs the checks, d) create the task?","answer":"Create a STREAM on RAW_TRANSACTIONS to capture new/updated rows, a QA_LOG table to record issues, and a TASK every 6 hours that invokes a stored procedure performing three checks: non-null critical fields, amount >= 0, and user_id existence in DIM_USERS, logging failures to QA_LOG.","explanation":"## Why This Is Asked\nTests practical use of Streams and Tasks to guard a daily ingestion pipeline with minimal disruption, plus a simple logging mechanism for failures.\n\n## Key Concepts\n- Snowflake Streams and Tasks\n- Data quality checks (nulls, valid ranges, referential integrity)\n- Lightweight auditing table (QA_LOG)\n- Scheduling with a cron-like cadence (every 6 hours)\n- Basic stored procedures or SQL blocks for checks\n\n## Code Example\n```sql\n-- Create a stream to track changes\nCREATE STREAM s_raw_transactions ON TABLE RAW_TRANSACTIONS;\n\n-- Create QA_LOG for issues\nCREATE TABLE QA_LOG (\n  txn_id STRING,\n  issue STRING,\n  ts TIMESTAMP DEFAULT CURRENT_TIMESTAMP()\n);\n\n-- Create stored procedure for data quality checks\nCREATE OR REPLACE PROCEDURE check_raw_transactions_quality()\nRETURNS STRING\nLANGUAGE JAVASCRIPT\nAS\n$$\n  var sql = `\n    INSERT INTO QA_LOG (txn_id, issue)\n    SELECT \n      s.id,\n      CASE \n        WHEN s.id IS NULL OR s.user_id IS NULL OR s.amount IS NULL OR s.status IS NULL OR s.txn_date IS NULL THEN 'NULL_CRITICAL_FIELD'\n        WHEN s.amount < 0 THEN 'NEGATIVE_AMOUNT'\n        WHEN NOT EXISTS (SELECT 1 FROM DIM_USERS WHERE user_id = s.user_id) THEN 'INVALID_USER_ID'\n      END as issue\n    FROM \n      TABLE(s_raw_transactions) s\n    WHERE \n      (s.id IS NULL OR s.user_id IS NULL OR s.amount IS NULL OR s.status IS NULL OR s.txn_date IS NULL)\n      OR s.amount < 0\n      OR NOT EXISTS (SELECT 1 FROM DIM_USERS WHERE user_id = s.user_id)\n  `;\n  \n  try {\n    snowflake.execute({sqlText: sql});\n    return 'Data quality check completed';\n  } catch (err) {\n    return 'Error: ' + err;\n  }\n$$;\n\n-- Create task to run every 6 hours\nCREATE OR REPLACE TASK qa_raw_transactions_task\n  WAREHOUSE = COMPUTE_WH\n  SCHEDULE = '0 */6 * * *'\n  AS\n  CALL check_raw_transactions_quality();\n\n-- Resume the task\nALTER TASK qa_raw_transactions_task RESUME;\n```","diagram":null,"difficulty":"beginner","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Oracle","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T04:43:05.723Z","createdAt":"2026-01-18T22:50:29.637Z"},{"id":"q-4182","question":"Intermediate Snowflake-core: Build a multi-tenant analytics pipeline for a SaaS platform where each customer publishes events into raw.events (tenant_id STRING, user_id STRING, event_type STRING, ts TIMESTAMP_NTZ, payload VARIANT). Implement per-tenant isolation with Row Access Policies, create curated views per tenant, enable masked sharing to a partner via secure data sharing, and set up a 5-minute refresh using Snowflake Tasks. Provide exact SQL for: (a) a tenancy-aware Row Access Policy, (b) a curated per-tenant aggregate view, (c) provisioning a secure data share to a partner, and (d) a 5-minute refresh Task?","answer":"SET TENANT_ID = 'tenantA'; CREATE ROW ACCESS POLICY rap_tenant USING (tenant_id = $TENANT_ID); CREATE VIEW curated.tenant_agg AS SELECT tenant_id, COUNT(*) AS events FROM raw.events GROUP BY tenant_id","explanation":"## Why This Is Asked\nTests ability to encode real Snowflake patterns for multi-tenant isolation, per-tenant aggregates, and partner sharing. It also probes automation and rollback considerations.\n\n## Key Concepts\n- Row Access Policies, Secure Data Sharing, Snowflake Tasks\n- Per-tenant aggregates, governance, masking considerations\n- Production-ready, minimal, repeatable SQL\n\n## Code Example\n```sql\nSET TENANT_ID = 'tenantA';\nCREATE ROW ACCESS POLICY rap_tenant USING (tenant_id = $TENANT_ID);\n```\n\n## Follow-up Questions\n- How would you version curated schema for drift?\n- How would you monitor failures and rollback?","diagram":null,"difficulty":"intermediate","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Cloudflare","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T06:59:43.170Z","createdAt":"2026-01-19T06:59:43.170Z"},{"id":"q-4218","question":"Beginner Snowflake-core: you need a nightly data-quality run for RAW.SALES (sale_id STRING, amount NUMBER, sale_ts TIMESTAMP_NTZ, customer_email STRING). Validate: sale_id IS NOT NULL, amount > 0, sale_ts within last 24h, and customer_email looks like an email. Outline a practical plan using (a) a lightweight QC query, (b) a TASK to run nightly, (c) a QA.SALES_QC results table, and (d) a simple alert when any check fails. Include exact SQL commands for each step?","answer":"Plan: create a QA sink and a nightly QC Task that emits four checks. Commands (condensed):\nCREATE SCHEMA IF NOT EXISTS QA;\nCREATE OR REPLACE TABLE QA.SALES_QC (check_name STRING, passed BOOLEAN, detai","explanation":"## Why This Is Asked\nTests practical data-quality automation in Snowflake: defining a QA sink, scheduling with Tasks, and multi-check validation. <br>## Key Concepts\n- Snowflake Tasks for nightly runs\n- Lightweight per-check validation in SQL\n- Idempotent QA logging table and simple alert hooks\n- Handling edge cases to avoid false positives\n<br>## Code Example\n```sql\n-- see question for full commands\n```\n## Follow-up Questions\n- How would you extend checks for duplicates?\n- How would you surface alerts to a Slack channel?","diagram":null,"difficulty":"beginner","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T09:09:10.160Z","createdAt":"2026-01-19T09:09:10.160Z"},{"id":"q-4337","question":"Snowflake-core: Ingest 1B events/day into RAW.events(sensor_id STRING, metric STRING, value FLOAT, ts TIMESTAMP_NTZ). Build an hourly rollup DEVICE_HOURLY(sensor_id STRING, metric STRING, hour_ts TIMESTAMP_NTZ, avg_value FLOAT, max_value FLOAT, min_value FLOAT) using streams and tasks. Address late data (up to 2 hours), out-of-order events, and schema evolution (add battery_level STRING) without downtime. Provide exact SQL for the stream, the hourly task, the rollup (MV or table), a rollback strategy using TIME_TRAVEL, validation checks, and a simple alert. End with a concrete health-check query?","answer":"Use RAW.events_STREAM to capture inserts, and an hourly TASK that inserts aggregated results into DEVICE_HOURLY filtered by DATE_TRUNC('hour', ts). Late data is allowed within a 2-hour window and repl","explanation":"## Why This Is Asked\nAssess ability to design fault-tolerant Snowflake pipelines with streams, tasks, time travel, and schema evolution; evaluate handling of late data and out-of-order events in a large-scale ingestion path.\n\n## Key Concepts\n- Streams and tasks\n- Time travel rollback\n- Late data windowing\n- Schema evolution without downtime\n- Validation and alerting\n\n## Code Example\n```javascript\n// Example SQL skeletons for stream and task creation\nCREATE OR REPLACE STREAM RAW.events_stream ON TABLE RAW.events;\nCREATE OR REPLACE TASK hourly_rollup\n  WAREHOUSE = my_wh\n  SCHEDULE = 'USING CRON 0 * * * *'\nAS\nMERGE INTO DEVICE_HOURLY AS d\nUSING (\n  SELECT sensor_id, metric, DATE_TRUNC('hour', ts) AS hour_ts,\n         AVG(value) AS avg_value, MAX(value) AS max_value, MIN(value) AS min_value\n  FROM RAW.events_stream\n  GROUP BY sensor_id, metric, DATE_TRUNC('hour', ts)\n) AS s\nON (d.sensor_id = s.sensor_id AND d.metric = s.metric AND d.hour_ts = s.hour_ts)\nWHEN MATCHED THEN UPDATE SET d.avg_value = s.avg_value, d.max_value = s.max_value, d.min_value = s.min_value\nWHEN NOT MATCHED THEN INSERT (sensor_id, metric, hour_ts, avg_value, max_value, min_value)\nVALUES (s.sensor_id, s.metric, s.hour_ts, s.avg_value, s.max_value, s.min_value);\n```\n\n## Follow-up Questions\n- How would you monitor data quality and alert when a rollup hour is incomplete?","diagram":"flowchart TD\n  A[Ingest RAW.events] --> B[RAW.events_stream]\n  B --> C[Hourly TASK -> DEVICE_HOURLY]\n  D[Late data window] --> C\n  E[Schema evolution] --> F[ALTER TABLE RAW.events]\n  G[ROLLBACK via TIME_TRAVEL] --> C","difficulty":"intermediate","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","Meta","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T14:47:17.771Z","createdAt":"2026-01-19T14:47:17.771Z"},{"id":"q-4392","question":"Design and implement a multi-tenant access pattern in Snowflake for analytics on RAW.events(tenant_id STRING, user_id STRING, event_time TIMESTAMP_NTZ, payload VARIANT). Each tenant must only see their own data in BI dashboards. Provide concrete SQL for: (a) a ROW ACCESS POLICY on RAW.events using a session-context tenant_id_ctx, (b) a SECURE VIEW CURATED.events that enforces the policy, (c) a provisioning script to grant access when a new tenant is added, (d) a validation test showing TenantA cannot read TenantB data. Include exact commands and sample outputs?","answer":"Implement multi-tenant access with a ROW ACCESS POLICY tied to a session context tenant_id_ctx. Create a SECURE VIEW CURATED.events over RAW.events with a filter tenant_id = current_session_context('t","explanation":"## Why This Is Asked\n\nTests practical use of Snowflake row access policies and secure views for data isolation in a scalable, multi-tenant BI environment.\n\n## Key Concepts\n\n- Row Access Policies\n- Secure Views\n- Session Contexts and role grants\n- Tenant provisioning automation\n\n## Code Example\n\n```sql\n-- Example snippet illustrating the approach\nCREATE OR REPLACE ROW ACCESS POLICY RAW.events_tenant_policy\nAS (tenant_id STRING) RETURNS BOOLEAN -> CURRENT_SESSION_CONTEXT('TENANT_ID') = tenant_id;\n```\n\n## Follow-up Questions\n\n- How would you validate policy enforcement in CI?\n- How to handle schema evolution with policies?\n","diagram":null,"difficulty":"advanced","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Slack","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T16:56:34.198Z","createdAt":"2026-01-19T16:56:34.198Z"},{"id":"q-4474","question":"Beginner Snowflake-core: You have RAW.SALES(sale_id STRING, amount NUMBER, sale_ts TIMESTAMP_NTZ, customer_email STRING, region STRING) shared for two tenants. Outline exact SQL to (1) create a masking policy on customer_email that returns NULL for TENANT_B_ROLE, (2) create a row access policy on region that filters REGION by role (TENANT_A_ROLE sees US, TENANT_B_ROLE sees EU), (3) create a secure view SANDBOX.TENANT_A_SAFE_SALES exposing only non-PII fields, and (4) grant access to TENANT_A_ROLE with SELECT on the view. Include the exact SQL for each step?","answer":"Use a masking policy on customer_email that returns NULL for TENANT_B_ROLE, a row access policy on region that restricts rows by role (TENANT_A_ROLE sees US, TENANT_B_ROLE sees EU), and a secure view ","explanation":"## Why This Is Asked\n\nTests end-to-end tenant isolation using Snowflake security features: masking policies for PII, row access policies for row-level filtering, and secure views for controlled exposure. Beginner level focuses on applying these features with concrete role names and object paths.\n\n## Key Concepts\n\n- Masking policies: dynamic data masking based on role\n- Row access policies: per-row filtering using role context\n- Secure views: prevent direct table access and expose only intended columns\n- Grants: least-privilege access to specific roles\n\n## Code Example\n\n```sql\nCREATE OR REPLACE MASKING POLICY mask_email\nAS (EMAIL STRING) RETURNS STRING ->\nCASE WHEN CURRENT_ROLE() = 'TENANT_B_ROLE' THEN NULL ELSE EMAIL END;\n```\n\n```sql\nALTER TABLE RAW.SALES MODIFY COLUMN customer_email SET MASKING POLICY mask_email;\n```\n\n```sql\nCREATE OR REPLACE ROW ACCESS POLICY region_filter\nAS (REGION STRING) RETURNS BOOLEAN ->\nCASE WHEN CURRENT_ROLE() = 'TENANT_A_ROLE' THEN REGION = 'US'\n     WHEN CURRENT_ROLE() = 'TENANT_B_ROLE' THEN REGION = 'EU'\n     ELSE TRUE\nEND;\n```\n\n```sql\nALTER TABLE RAW.SALES ADD ROW ACCESS POLICY region_filter ON (REGION);\n```\n\n```sql\nCREATE OR REPLACE SECURE VIEW SANDBOX.TENANT_A_SAFE_SALES AS\nSELECT sale_id, amount, sale_ts, region\nFROM RAW.SALES;\n```\n\n```sql\nGRANT SELECT ON SECURE VIEW SANDBOX.TENANT_A_SAFE_SALES TO ROLE TENANT_A_ROLE;\n```\n\n## Follow-up Questions\n\n- How would you test that TENANT_B_ROLE sees NULL emails and only US data?\n- What changes would you make to allow TENANT_A_ROLE to see emails in a controlled, auditable way?","diagram":null,"difficulty":"beginner","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Databricks","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T20:36:41.304Z","createdAt":"2026-01-19T20:36:41.304Z"},{"id":"q-4727","question":"Intermediate Snowflake-core: Ingest transactions from three sources into RAW.SALES_TXN (txn_id STRING, amount NUMBER, currency STRING, ts TIMESTAMP_NTZ, customer_id STRING). Build a CURATED.SALES_TXN that (a) converts amount to USD using a daily rates table RATES(currency, rate_to_usd, effective_date), (b) flags anomalies (negative or null amount, unknown currency, ts older than 30 days, missing txn_id), (c) provides a secure, read-only view for a partner via a secure data share, and (d) includes a nightly rollback plan for any QC failure. Include exact SQL snippets for ingestion, conversion, QC checks, view creation, and rollback?","answer":"Ingest uses streams per source on RAW.SALES_TXN, then a nightly TASK MERGEs into CURATED.SALES_TXN applying currency conversion from RATES (JOIN on currency where effective_date <= ts, latest rate). Q","explanation":"## Why This Is Asked\nTests end-to-end Snowflake core: multi-source ingestion, currency normalization, robust QC, secure data sharing, and a rollback strategy.\n\n## Key Concepts\n- Streams and Tasks for incremental ingestion\n- MERGE for upserts and transformations\n- Time Travel and CLONE for rollback\n- Access controls and secure views for data sharing\n- Reference data handling with daily rates\n\n## Code Example\n```sql\n-- 1) Ingest: create streams on RAW.SALES_TXN for each source\nCREATE OR REPLACE STREAM RAW.SALES_TXN_S1 (...);\nCREATE OR REPLACE STREAM RAW.SALES_TXN_S2 (...);\nCREATE OR REPLACE STREAM RAW.SALES_TXN_S3 (...);\n\n-- 2) Currency conversion (example for MERGE)\nMERGE INTO CURATED.SALES_TXN AS t\nUSING (\n  SELECT s.txn_id, s.amount, s.currency, s.ts, s.customer_id,\n         r.rate_to_usd\n  FROM RAW.SALES_TXN_S1 s\n  JOIN LATERAL (\n    SELECT rate_to_usd\n    FROM RATES r\n    WHERE r.currency = s.currency\n      AND r.effective_date <= s.ts\n    ORDER BY r.effective_date DESC\n    LIMIT 1\n  ) r ON TRUE\n) AS src\nON t.txn_id = src.txn_id\nWHEN MATCHED THEN UPDATE SET amount_usd = src.amount * src.rate_to_usd, ...\nWHEN NOT MATCHED THEN INSERT (..., amount_usd) VALUES (...);\n\n-- 3) QC checks\nINSERT INTO QC_SALES\nSELECT * FROM CURATED.SALES_TXN WHERE amount_usd <= 0 OR currency NOT IN ('USD','EUR',...);\n\n-- 4) Partner view and share\nCREATE OR REPLACE SECURE VIEW CURATED.SALES_TXN_PARTNER_VIEW AS\nSELECT txn_id, amount_usd, ts, customer_id_masked\nFROM CURATED.SALES_TXN\nWHERE current_role() = 'PARTNER_ROLE';\n\n-- 5) Rollback (pre-merge state)\nCREATE OR REPLACE CLONE CURATED.SALES_TXN_PRE_MERGE AT (TIMESTAMP => BEFORE MERGE);\n```\n\n## Follow-up Questions\n- How would you test currency rate drift edge cases?\n- How would you handle rate table outages without halting pipelines?","diagram":"flowchart TD\n  A[Ingest Sources] --> B[RAW.SALES_TXN]\n  B --> C[STREAMS & TASKS]\n  C --> D[CURATED.SALES_TXN]\n  D --> E[Partner Secure View via Share]","difficulty":"intermediate","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T10:02:45.666Z","createdAt":"2026-01-20T10:02:45.666Z"},{"id":"q-4888","question":"Ingest 100B telemetry events daily into raw.events (device_id STRING, ts TIMESTAMP_NTZ, value NUMBER). Design a real-time anomaly-detection pipeline that runs every 5 minutes, scoring each device by a 5-minute rolling mean and stddev of value. Flag values outside mean±3*stddev. Implement with Streams and Tasks; append results to curated.anomalies; cover late data handling, idempotency, testing, and scaling?","answer":"Use a Stream on raw.events, a 5-minute Task to compute per-device rolling mean and stddev, and a MERGE into curated.anomalies with a score and anomaly flag. Handle late data via a 15-minute lag window","explanation":"## Why This Is Asked\nTests ability to design a streaming analytics pattern in Snowflake, handle late data, and ensure idempotent results at scale.\n\n## Key Concepts\n- Snowflake Streams and Tasks for incremental processing\n- Per-device rolling window aggregations and anomaly scoring\n- Idempotent upserts with MERGE; late-data strategies\n\n## Code Example\n```sql\n-- Stream on raw.events\nCREATE OR REPLACE STREAM raw.events_stream ON TABLE raw.events (device_id STRING, ts TIMESTAMP_NTZ, value NUMBER);\n\n-- Task skeleton to run every 5 minutes\nCREATE OR REPLACE TASK anomalies_calc\nWAREHOUSE = my_wh\nSCHEDULE = '5 MINUTE'\nAS\nMERGE INTO curated.anomalies AS a\nUSING (\n  SELECT device_id,\n         MIN(ts) AS window_start,\n         AVG(value) AS mean_v,\n         STDDEV_SAMP(value) AS std_v\n  FROM raw.events\n  WHERE ts >= DATEADD(minute, -5, CURRENT_TIMESTAMP())\n  GROUP BY device_id\n) AS b\nON a.device_id = b.device_id AND a.window_start = b.window_start\nWHEN NOT MATCHED THEN INSERT (device_id, window_start, mean_v, std_v, is_anomaly)\nVALUES (b.device_id, b.window_start, b.mean_v, b.std_v, (ABS(value - b.mean_v) > 3 * b.std_v));\n```\n\n## Follow-up Questions\n- How would you test end-to-end with realistic data? \n- What monitoring and alerting would you add for pipeline health and anomalies?","diagram":null,"difficulty":"intermediate","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["NVIDIA","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T17:46:01.729Z","createdAt":"2026-01-20T17:46:01.729Z"},{"id":"q-4936","question":"Beginner Snowflake-core: Design a nightly incremental pipeline to move new rows from RAW.SALES to CURATED.SALES using a STREAM and a TASK. Transform: mask customer_email local part, compute weekday from sale_ts, and drop invalid rows (sale_id NULL or amount <= 0). Log invalids to ALERT.SALES_MOVE (last_run, error_desc). Include exact SQL commands for: CURATED.SALES, RAW.SALES_STREAM, ALERT.SALES_MOVE, and the NIGHTLY TASK MERGE?","answer":"Nightly incremental pipeline using a STREAM on RAW.SALES and a TASK that MERGEs into CURATED.SALES. Mask email with REGEXP_REPLACE, compute weekday from sale_ts, and filter valid rows (sale_id IS NOT ","explanation":"## Why This Is Asked\nThis tests practical Snowflake pipeline skills: streams, tasks, simple transformations, and error logging in a reusable pattern.\n\n## Key Concepts\n- Snowflake streams and tasks\n- MERGE for upserts\n- Data masking with REGEXP_REPLACE\n- DATE_PART for weekday calculation\n- Simple auditing via an alert table\n\n## Code Example\n```sql\n-- CURATED.SALES\nCREATE TABLE curated.sales (\n  sale_id STRING,\n  amount NUMBER,\n  sale_ts TIMESTAMP_NTZ,\n  customer_email STRING,\n  weekday STRING\n);\n\n-- RAW.SALES_STREAM\nCREATE OR REPLACE STREAM raw.sales_stream ON TABLE raw.sales;\n\n-- ALERT.SALES_MOVE\nCREATE TABLE IF NOT EXISTS alert.sales_move (\n  last_run TIMESTAMP_NTZ,\n  error_desc STRING\n);\n\n-- NIGHTLY TASK (MERGE)\nCREATE OR REPLACE TASK nightly_sales_pipeline\n  WAREHOUSE = WH\n  SCHEDULE = 'USING CRON 0 2 * * *'\nAS\nMERGE INTO curated.sales AS c\nUSING (\n  SELECT sale_id, amount, sale_ts,\n         REGEXP_REPLACE(customer_email, '^[^@]+', '***') AS customer_email_mask,\n         CASE DATE_PART('DOW', sale_ts)\n           WHEN 0 THEN 'Sun' WHEN 1 THEN 'Mon' WHEN 2 THEN 'Tue'\n           WHEN 3 THEN 'Wed' WHEN 4 THEN 'Thu' WHEN 5 THEN 'Fri' WHEN 6 THEN 'Sat'\n         END AS weekday\n  FROM raw.sales_stream\n  WHERE sale_id IS NOT NULL AND amount > 0\n) s\nON c.sale_id = s.sale_id\nWHEN MATCHED THEN UPDATE SET amount = s.amount, sale_ts = s.sale_ts,\n  customer_email = s.customer_email_mask, weekday = s.weekday\nWHEN NOT MATCHED THEN INSERT (sale_id, amount, sale_ts, customer_email, weekday)\nVALUES (s.sale_id, s.amount, s.sale_ts, s.customer_email_mask, s.weekday);\n```\n\n## Follow-up Questions\n- How would you handle duplicates in RAW.SALES_STREAM?\n- How would you test the pipeline in a dev environment?","diagram":"flowchart TD\n  RAW_SALES[RAW.SALES] --> RAW_STREAM[RAW.SALES_STREAM]\n  RAW_STREAM --> NIGHTLY_TASK[NIGHTLY_TASK MERGE]\n  NIGHTLY_TASK --> CURATED[CURATED.SALES]\n  CURATED --> ALERT[ALERT.SALES_MOVE]","difficulty":"beginner","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Meta","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T20:27:34.237Z","createdAt":"2026-01-20T20:27:34.237Z"},{"id":"q-5044","question":"Snowflake core: Build a secure, scalable multi-tenant data mesh with RAW and CURATED layers. Implement per-tenant isolation via ROW ACCESS POLICIES and dynamic masking on PII fields; expose per-tenant views with restricted columns. Provide concrete objects (roles, masking policies, row policies, views, grants) and exact Snowflake commands to implement, test, and rollback?","answer":"Define per-tenant roles (TENANT_A_VIEWER, TENANT_B_VIEWER); create masking policies on PII fields (EMAIL, SSN) that return REDACTED for non-admin roles; attach ROW ACCESS POLICIES to CURATED.SALES to enforce tenant isolation; expose per-tenant views with restricted columns; implement comprehensive grants and audit trails.","explanation":"## Why This Is Asked\nTests practical data governance in Snowflake at scale, combining masking, row-level security, and secure views in a multi-tenant setup.\n\n## Key Concepts\n- Dynamic data masking on PII columns\n- Row access policies for tenant isolation\n- Secure views per-tenant access\n- Role-based grants and auditability\n- Rollback and testability\n\n## Code Example\n```sql\n-- Create masking policies for email and ssn\nCREATE MASKING POLICY pii_email_mask AS (email STRING) RETURNS STRING ->\n  CASE WHEN CURRENT_ROLE() IN ('ADMIN','TENANT_A_VIEWER') THEN email ELSE 'REDACTED' END;\n\nCREATE MASKING POLICY pii_ssn_mask AS (ssn STRING) RETURNS STRING ->\n  CASE WHEN CURRENT_ROLE() IN ('ADMIN','TENANT_A_VIEWER') THEN ssn ELSE 'REDACTED' END;\n\n-- Apply masking policies\nALTER TABLE CURATED.CUSTOMERS MODIFY COLUMN EMAIL SET MASKING POLICY pii_email_mask;\nALTER TABLE CURATED.CUSTOMERS MODIFY COLUMN SSN SET MASKING POLICY pii_ssn_mask;\n\n-- Create row access policy for tenant isolation\nCREATE ROW ACCESS POLICY tenant_isolation AS (tenant_id STRING) RETURNS BOOLEAN ->\n  CASE WHEN CURRENT_ROLE() IN ('ADMIN') THEN TRUE\n       WHEN CURRENT_ROLE() = 'TENANT_A_VIEWER' THEN tenant_id = 'TENANT_A'\n       WHEN CURRENT_ROLE() = 'TENANT_B_VIEWER' THEN tenant_id = 'TENANT_B'\n       ELSE FALSE END;\n\n-- Apply row access policy\nALTER TABLE CURATED.SALES ADD ROW ACCESS POLICY tenant_isolation ON (TENANT_ID);\n\n-- Create per-tenant secure views\nCREATE SECURE VIEW CURATED.V_SALES_TENANT_A AS\nSELECT SALE_ID, CUSTOMER_ID, AMOUNT, SALE_DATE\nFROM CURATED.SALES\nWHERE TENANT_ID = 'TENANT_A';\n\nCREATE SECURE VIEW CURATED.V_SALES_TENANT_B AS\nSELECT SALE_ID, CUSTOMER_ID, AMOUNT, SALE_DATE\nFROM CURATED.SALES\nWHERE TENANT_ID = 'TENANT_B';\n\n-- Grant permissions\nGRANT ROLE TENANT_A_VIEWER TO USER TENANT_A_USER;\nGRANT ROLE TENANT_B_VIEWER TO USER TENANT_B_USER;\nGRANT SELECT ON VIEW CURATED.V_SALES_TENANT_A TO ROLE TENANT_A_VIEWER;\nGRANT SELECT ON VIEW CURATED.V_SALES_TENANT_B TO ROLE TENANT_B_VIEWER;\n\n-- Test queries\nUSE ROLE TENANT_A_VIEWER;\nSELECT * FROM CURATED.V_SALES_TENANT_A; -- Should return only tenant A data\nSELECT EMAIL FROM CURATED.CUSTOMERS; -- Should show actual emails for tenant A\n\n-- Rollback commands\nALTER TABLE CURATED.SALES DROP ROW ACCESS POLICY tenant_isolation;\nALTER TABLE CURATED.CUSTOMERS MODIFY COLUMN EMAIL UNSET MASKING POLICY;\nDROP MASKING POLICY IF EXISTS pii_email_mask;\nDROP MASKING POLICY IF EXISTS pii_ssn_mask;\nDROP VIEW IF EXISTS CURATED.V_SALES_TENANT_A;\nDROP VIEW IF EXISTS CURATED.V_SALES_TENANT_B;\n```\n\n## Implementation Notes\n- Test masking and row access policies separately\n- Use CURRENT_ROLE() for dynamic access control\n- Secure views prevent view definition exposure\n- Always test rollback procedures in development\n- Monitor access through ACCESS_HISTORY view","diagram":"flowchart TD\n  A[Ingest to RAW] --> B[Masking Policies Applied]\n  B --> C[Row Access Enforced]\n  C --> D[Per-Tenant Secure Views]\n  D --> E[Auditing & Tests]","difficulty":"advanced","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T05:09:13.746Z","createdAt":"2026-01-21T02:52:11.935Z"},{"id":"q-5071","question":"Beginner Snowflake-core: You ingest daily JSON events into RAW_EVENTS (event_id STRING, user_id STRING, event_ts TIMESTAMP_NTZ, category STRING, payload VARIANT). Propose a minimal plan to speed analytics without changing ETL: (a) clustering strategy, (b) enable auto clustering, (c) cost-conscious warehouse sizing, (d) a quick verification query. Include exact SQL to implement each step?","answer":"ALTER TABLE RAW_EVENTS CLUSTER BY (TO_DATE(event_ts), category); ALTER TABLE RAW_EVENTS SET AUTO_CLUSTERING = TRUE; CREATE WAREHOUSE WAREHOUSE_COSTCTRL WITH WAREHOUSE_SIZE = 'XSMALL'; CREATE RESOURCE ","explanation":"## Why This Is Asked\nTests understanding of practical performance tuning (clustering vs auto clustering) and cost control in Snowflake, plus how to validate impact with a simple query.\n\n## Key Concepts\n- Clustering vs AUTO_CLUSTERING\n- Warehouse sizing and resource monitors\n- Lightweight verification queries to measure pruning and performance\n\n## Code Example\n```javascript\nALTER TABLE RAW_EVENTS CLUSTER BY (TO_DATE(event_ts), category);\nALTER TABLE RAW_EVENTS SET AUTO_CLUSTERING = TRUE;\nCREATE WAREHOUSE WAREHOUSE_COSTCTRL WITH WAREHOUSE_SIZE = 'XSMALL';\nCREATE RESOURCE MONITOR RM_MONTHLY WITH CREDIT_QUOTA = 1000 TRIGGERS ON 90 PERCENT DO SUSPEND;\n```\n\n## Follow-up Questions\n- How would you measure actual pruning gains after enabling clustering?\n- What trade-offs exist between auto clustering cost and query latency?","diagram":"flowchart TD\n  A[Ingest RAW_EVENTS] --> B[Cluster by (TO_DATE(event_ts), category)]\n  B --> C[AUTO_CLUSTERING ON]\n  C --> D[Budget: WAREHOUSE_SIZE='XSMALL', RM with 1000 credits]\n  D --> E[Verify with sample date-filter query]","difficulty":"beginner","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Netflix","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T04:30:09.216Z","createdAt":"2026-01-21T04:30:09.217Z"},{"id":"q-5086","question":"In Snowflake, ingest 200M events/day into RAW_EVENTS(event_id STRING, user_id STRING, event_type STRING, amount NUMBER, ts TIMESTAMP_NTZ, metadata VARIANT). Build a deduplicated CURATED_EVENTS and a 7-day rolling fraud_score per user. Outline the pipeline: (a) data model & ingestion, (b) idempotent dedupe with a STREAM + MERGE on event_id, (c) incremental fraud_score storage, (d) performance/cost trade-offs (clustering, warehouse sizing), (e) validation and rollback. Include concrete SQL for key steps?","answer":"Use a STREAM on RAW_EVENTS and MERGE into CURATED_EVENTS by event_id for idempotent dedupe. Run a 15-minute TASK to MERGE new stream rows into CURATED_EVENTS. Compute a 7-day rolling fraud_score per u","explanation":"## Why This Is Asked\nRationalizes a real-world pattern: dedupe, streaming, and rolling analytics in Snowflake.\n\n## Key Concepts\n- STREAM + MERGE for idempotent ingestion\n- TASK scheduling and windowed analytics\n- Clustering, cost-tradeoffs, and rollback\n\n## Code Example\n```sql\n-- Example: create stream and merge into curated\nCREATE OR REPLACE STREAM RAW_EVENTS_STREAM ON TABLE RAW_EVENTS; \nMERGE INTO CURATED_EVENTS AS c\nUSING RAW_EVENTS_STREAM AS r\nON c.event_id = r.event_id\nWHEN NOT MATCHED THEN INSERT (...);\n```\n\n## Follow-up Questions\n- How would you test late-arriving data and ensure idempotency?\n- How would you monitor sliding-window fraud_score freshness and guard against skew?","diagram":"flowchart TD\n  A[RAW_EVENTS] --> B[RAW_EVENTS_STREAM]\n  B --> C[CURATED_EVENTS (MERGE on event_id)]\n  C --> D[USER_FRAUD_SCORE (7d window)]\n  D --> E[DASHBOARD]\n  F[QA & Rollback] --> E","difficulty":"intermediate","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Google","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T05:48:01.184Z","createdAt":"2026-01-21T05:48:01.185Z"},{"id":"q-5164","question":"Beginner Snowflake-core: Build a nightly incremental load from RAW.STAGING.ORDERS (order_id STRING, customer_id STRING, order_ts TIMESTAMP_NTZ, amount NUMBER) into ANALYTICS.ORDERS using a STREAM and a TASK. Outline the approach and provide the exact SQL to (a) create a STREAM on RAW.STAGING.ORDERS, (b) implement a JS STORED PROCEDURE MERGE_ORDERS() that MERGEs into ANALYTICS.ORDERS using the stream, and (c) create a NIGHTLY_LOAD TASK scheduled at 02:00 that CALLS MERGE_ORDERS(). Include basic error handling and a simple audit log?","answer":"Create a Snowflake STREAM on RAW.STAGING.ORDERS to capture new/updated rows. Implement a JavaScript stored procedure MERGE_ORDERS() that MERGEs ANALYTICS.ORDERS using RAW.STAGING.ORDERS_STREAM on ORDE","explanation":"## Why This Is Asked\nTests practical use of Streams and Tasks for incremental loads, including simple auditing and error handling with a stored procedure.\n\n## Key Concepts\n- Snowflake Streams\n- Snowflake Tasks\n- MERGE semantics for idempotent upserts\n- JavaScript stored procedures for orchestration\n- Basic auditing\n\n## Code Example\n```sql\n-- STREAM creation\nCREATE OR REPLACE STREAM RAW.STAGING.ORDERS_STREAM ON TABLE RAW.STAGING.ORDERS;\n```\n```javascript\n// Stored procedure skeleton\nCREATE OR REPLACE PROCEDURE ANALYTICS.MERGE_ORDERS()\nRETURNS STRING\nLANGUAGE JAVASCRIPT\nAS\n$$\ntry {\n  // MERGE logic here\n  return 'success';\n} catch (e) {\n  return 'fail ' + e;\n}\n$$;\n```\n```sql\n-- TASK\nCREATE OR REPLACE TASK NIGHTLY_LOAD\nWAREHOUSE = 'COMPUTE_WH'\nSCHEDULE = 'USING CRON 0 2 * * *'\nWHEN SYSTEM$STREAM_HAS_DATA('RAW.STAGING.ORDERS_STREAM')\nAS CALL ANALYTICS.MERGE_ORDERS();\n```\n\n## Follow-up Questions\n- How would you handle late-arriving data in this pipeline?\n- How would you extend this for multi-region replication or add monitoring alerts?","diagram":null,"difficulty":"beginner","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Hashicorp","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T08:55:52.026Z","createdAt":"2026-01-21T08:55:52.026Z"},{"id":"q-5176","question":"You're architecting a cross-region Snowflake data lake for a high-velocity ride-hailing platform. You must implement a governance layer that supports per-organization access, data lineage, and schema evolution while retaining data older than 365 days. Design the data model and automation using Snowflake features (tags, dynamic data masking, row access policies, Streams, Tasks, Time Travel, and replication). Provide exact SQL commands to (1) tag tables with lineage info, (2) create a masking policy for PII, (3) apply policy to an example view, (4) set up a Stream on RAW.ORDERS and a Task to push lineage entries into GOVERNANCE.LINEAGE. Finally discuss how you'd validate correctness and performance?","answer":"Use a lineage TAG on RAW tables, a masking policy for PII, a ROW ACCESS POLICY to gate data by org, a STREAM on RAW.ORDERS, and a TASK to insert lineage rows into GOVERNANCE.LINEAGE. This supports cro","explanation":"## Why This Is Asked\n\nThis question probes practical mastery of Snowflake governance patterns at scale: tagging for lineage, masking for PII, and row-level access across multiple orgs, plus automation to keep lineage records up to date in a cross-region setup.\n\n## Key Concepts\n\n- Data lineage using TAGs and STREAMS to capture changes\n- Dynamic data masking for sensitive fields tied to roles\n- ROW ACCESS POLICIES for per-organization data access\n- Streams + Tasks to populate a centralized GOVERNANCE.LINEAGE table\n- Time Travel and cross-region replication considerations for schema evolution and audits\n\n## Code Example\n\n```sql\n-- (1) Tag tables with lineage info\nCREATE TAG lineage_source STRING;\nCREATE TAG lineage_owner STRING;\n\nALTER TABLE RAW.ORDERS SET TAG lineage_source = 'RAW';\nALTER TABLE RAW.ORDERS SET TAG lineage_owner = 'data-team';\n```\n\n```sql\n-- (2) Create a masking policy for PII\nCREATE MASKING POLICY pii_masking AS (pii STRING) RETURNS STRING ->\n  CASE WHEN CURRENT_ROLE() IN ('ORG_A_ANALYST','ORG_B_ANALYST') THEN pii ELSE 'REDACTED' END;\n```\n\n```sql\n-- (3) Apply policy to an example view (underlying column is masked)\nALTER TABLE RAW.ORDERS ALTER COLUMN customer_email SET MASKING POLICY pii_masking(customer_email);\nCREATE OR REPLACE VIEW CURATED.ORDERS_V AS\nSELECT order_id, amount, ts, customer_email\nFROM RAW.ORDERS;\n```\n\n```sql\n-- (4) Stream and lineage task setup\nCREATE STREAM RAW.ORDERS_STRM ON TABLE RAW.ORDERS (APPEND_ONLY = FALSE);\n\nCREATE TABLE GOVERNANCE.LINEAGE (\n  source_table STRING,\n  target_table STRING,\n  action STRING,\n  changed_at TIMESTAMP_NTZ\n);\n\nCREATE OR REPLACE TASK governance.lineage_refresh\n  WAREHOUSE = 'GOV_WH'\n  SCHEDULE = 'USING CRON 5 * * * *'\n  AS\n  INSERT INTO GOVERNANCE.LINEAGE (source_table, target_table, action, changed_at)\n  SELECT 'RAW.ORDERS', 'CURATED.ORDERS_V', METADATA$ACTION, CURRENT_TIMESTAMP()\n  FROM RAW.ORDERS_STRM;\n```\n\n> Note: Cross-region replication setup is environment-specific and involves account-level configuration outside these DDLs (replication for databases, objects, and policies).\n\n## Follow-up Questions\n\n- How would you test masking and lineage in a prod-like sandbox without affecting live data?\n- What metrics would you monitor to ensure lineage accuracy and replication latency stays within SLOs?","diagram":"flowchart TD\n  A[RAW.ORDERS] --> B[RAW.ORDERS_STRM]\n  B --> C[GOVERNANCE.LINEAGE_INSERT]\n  C --> D[GOVERNANCE.LINEAGE]\n  E[REGIONAL_REPLICATION] --> D","difficulty":"advanced","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Lyft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T09:46:28.539Z","createdAt":"2026-01-21T09:46:28.539Z"},{"id":"q-5263","question":"Snowflake-core intermediate: You run a global e-commerce analytics pipeline. In RAW_ORDERS(order_id STRING, customer_id STRING, amount NUMBER, currency STRING, placed_at TIMESTAMP_NTZ), you need near real-time dashboards by country and currency conversion. Outline (a) ingestion to a curated table with 5-minute latency, (b) a daily currency_rate table and a view that converts to USD, (c) a secure partner share with masked customer emails, (d) cross-region replication strategy for DR with minimal data loss, and (e) exact SQL for each step?","answer":"Use Snowpipe to ingest RAW_ORDERS into CURATED_ORDERS with a 5-minute TASK driven by a STREAM on RAW_ORDERS. Currency conversion via a daily CURRENCY_RATE table; USD_REVENUE view joins CURATED_ORDERS ","explanation":"## Why This Is Asked\nAssesses ability to design near‑real‑time Snowflake pipelines with data sharing and DR.\n\n## Key Concepts\n- Snowpipe, Tasks, Streams for low-latency ingestion\n- CURATED_ORDERS and USD_REVENUE view for on‑the‑fly currency\n- Masking policies and secure views for partner data sharing\n- ACCOUNT REPLICATION and Failover Groups for DR\n\n## Code Example\n```sql\n-- Ingestion pipe and task (illustrative)\nCREATE OR REPLACE PIPE p_raw_orders AS COPY INTO CURATED_ORDERS FROM @staging\n  FILE_FORMAT=(TYPE=CSV);\nCREATE TASK t_ingest_5m WAREHOUSE = w_load SCHEDULE = '5 MINUTE' AS\n  ALTER PIPE p_raw_orders REFRESH;\n\n-- USD conversion view\nCREATE OR REPLACE VIEW USD_REVENUE AS\nSELECT o.order_id, o.customer_id, o.amount, o.currency, o.placed_at,\n       o.amount * r.rate AS usd_amount\nFROM CURATED_ORDERS o\nJOIN CURRENCY_RATE r ON o.currency = r.currency;\n```\n\n## Follow-up Questions\n- How would you validate currency conversions across currencies with holiday/weekend rate gaps?\n- What failure scenarios would trigger DR failover, and how would you test them?","diagram":"flowchart TD\n  RAW[RAW_ORDERS] --> CURATED[CURATED_ORDERS]\n  CURATED --> USD[USD_REVENUE_VIEW]\n  CURATED --> PARTNER[PARTNER_SHARE_VIEW]\n  USD --> DASH[DASHBOARDS]\n  PARTNER --> SHARE[DATA SHARE]\n  DR[DR] --> CURATED\n  DR --> SHARE","difficulty":"intermediate","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Microsoft","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T13:29:23.782Z","createdAt":"2026-01-21T13:29:23.783Z"},{"id":"q-5442","question":"Advanced Snowflake-core: You run a multi-tenant analytics service ingesting 50M+ events/hour into a shared raw.events table (tenant_id, user_id, ts, event_type, payload). Propose a production design that guarantees per-tenant isolation, cost control, and governance across SLA tiers. Include data modeling choices, access controls, compute strategy (multi-cluster vs per-tenant warehouses), data lifecycle, DR, and observability. Provide concrete SQL examples to implement a row-access policy, a masking policy, and to configure two multi-cluster warehouses with MIN/MAX clusters and a retention plan?","answer":"Use a shared raw.events table with tenant_id, secured by a ROW ACCESS POLICY that reads CURRENT_CONTEXT('TENANT_ID') and enforces tenant_id = TENANT. Apply a MASKING POLICY on customer_email. Compute with two multi-cluster warehouses: ENTERPRISE_WAREHOUSE (MIN=2, MAX=8) for premium tenants and STANDARD_WAREHOUSE (MIN=1, MAX=4) for standard tenants. Implement tiered data lifecycle with 30-day raw retention, 90-day aggregate retention, and automatic archival to S3. Configure cross-region replication for DR with failover testing. Monitor via Snowflake's ACCOUNT_USAGE views and custom dashboards.","explanation":"## Why This Is Asked\nTests practical multi-tenant Snowflake design, focusing on isolation, governance, and cost control across varying SLA requirements.\n\n## Key Concepts\n- Row-level security for tenant isolation\n- Dynamic masking for PII protection\n- Multi-cluster warehouses for split latency/throughput tiers\n- Data lifecycle: retention and Time Travel\n- DR and observability via Snowflake metadata\n\n## Code Example\n```sql\n-- Row access policy\nCREATE OR REPLACE ROW ACCESS POLICY rap_tenant\n  AS (tenant_id STRING) RETURNS BOOLEAN ->\n  (tenant_id = CURRENT_CONTEXT('TENANT_ID'));\n\nALTER TABLE raw.events\n  ADD ROW ACCESS POLICY rap_tenant ON (tenant_id);\n\n-- Masking policy\nCREATE OR REPLACE MASKING POLICY mp_email\n  AS (val STRING) RETURNS STRING ->\n  CASE\n    WHEN CURRENT_ROLE() IN ('ADMIN', 'SUPPORT') THEN val\n    ELSE '***@***.***'\n  END;\n\nALTER TABLE raw.events\n  MODIFY COLUMN customer_email SET MASKING POLICY mp_email;\n\n-- Multi-cluster warehouses\nCREATE WAREHOUSE ENTERPRISE_WAREHOUSE\n  WAREHOUSE_SIZE = 'LARGE'\n  MIN_CLUSTER_COUNT = 2\n  MAX_CLUSTER_COUNT = 8\n  SCALING_POLICY = 'STANDARD';\n\nCREATE WAREHOUSE STANDARD_WAREHOUSE\n  WAREHOUSE_SIZE = 'MEDIUM'\n  MIN_CLUSTER_COUNT = 1\n  MAX_CLUSTER_COUNT = 4\n  SCALING_POLICY = 'STANDARD';\n\n-- Retention plan\nALTER TABLE raw.events SET DATA_RETENTION_TIME_IN_DAYS = 30;\n```","diagram":"flowchart TD\n  A[Raw.events shared storage] --> B[Tenant isolation policy]\n  A --> C[Warehouses WH_REALTIME / WH_BURST]\n  B --> D[Row Access Policy]\n  D --> E[Isolated views per tenant]\n  F[Masking Policy] --> G[PII masking]","difficulty":"advanced","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Microsoft","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T06:00:24.691Z","createdAt":"2026-01-21T22:40:41.612Z"},{"id":"q-5554","question":"You're building a multi-tenant SaaS analytics layer on Snowflake that must isolate 1,000 tenants within a single account. Propose an end-to-end architecture and SQL you would implement to enforce per-tenant isolation, dynamic data masking, and cost governance. Include (a) data model and access controls, (b) a session-based Row Access Policy, (c) a masking policy for PII fields, (d) per-tenant quotas with automated warehouse suspension, and (e) a validation plan with concrete checks and queries?","answer":"Architect a shared data lake with a tenant_id column and Row Access Policies driven by a session tenant_id; apply masking policies to PII fields; implement per-tenant quotas via a Resource Monitor tha","explanation":"## Why This Is Asked\n\nTests ability to design multi-tenant isolation in Snowflake with practical controls.\n\n## Key Concepts\n\n- Row Access Policies using session settings\n- Masking Policies for PII\n- Resource Monitors for per-tenant cost governance\n- Secure Views for data exposure\n- Validation: synthetic tenants, log audits, cost checks\n\n## Code Example\n\n```sql\n-- Row Access Policy\nCREATE OR REPLACE ROW ACCESS POLICY ra_tenant USING (tenant_id = CURRENT_SETTING('tenant_id')::STRING);\nALTER TABLE raw.events ADD ROW ACCESS POLICY ra_tenant ON (tenant_id);\n\n-- Masking Policy\nCREATE MASKING POLICY mp_email (val STRING) RETURNS STRING ->\n  CASE WHEN CURRENT_ROLE() IN ('TENANT_ADMIN') THEN val\n       ELSE REGEXP_REPLACE(val, '@.*', '@*****')\n  END;\nALTER TABLE raw.users MODIFY COLUMN email SET MASKING POLICY mp_email;\n\n-- Resource Monitor (example values)\nCREATE RESOURCE MONITOR rm_tenant_quota WITH CREDIT_QUOTA = 1000 NOTIFY_USERS = ('admin@example.com');\n```\n\n## Follow-up Questions\n\n- How would you test cross-tenant isolation at scale?\n- How would you handle schema evolution and tenant onboarding?","diagram":"flowchart TD\n  A[Tenant Context] --> B[Row Access Policy]\n  B --> C[Masking Policies]\n  C --> D[Secure Views]\n  D --> E[Per-Tenant Resource Monitor]\n  E --> F[Validation]","difficulty":"advanced","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","Square","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T05:56:29.879Z","createdAt":"2026-01-22T05:56:29.879Z"},{"id":"q-5635","question":"Beginner Snowflake-core: You have RAW.SALES with sale_id, amount, sale_ts, region. Propose a simple nightly regional digest pipeline that (a) computes daily totals per region, (b) stores results in a digest_sales table, (c) records runs in an audit table, and (d) triggers a basic alert if any region's total is NULL or negative. Include high-level SQL and the commands to create the TASK and the tables?","answer":"Implement a nightly pipeline: create digest_sales(region STRING, day DATE, total_amount NUMBER); create digest_audit(region, day, run_at, status). Schedule a TASK at 02:00 to populate digest_sales wit","explanation":"## Why This Is Asked\nTests practical understanding of simple Snowflake ETL: TASKs, basic aggregation, and simple alerting. It avoids heavy automation and focuses on safe, repeatable nightly jobs.\n\n## Key Concepts\n- Snowflake TASKs and Schedules\n- Basic aggregation per region\n- Audit/log tables for runs\n- Lightweight alerting via data checks or webhook\n\n## Code Example\n```sql\n-- create tables\nCREATE TABLE digest_sales(region STRING, day DATE, total_amount NUMBER);\nCREATE TABLE digest_audit(region STRING, day DATE, run_at TIMESTAMP_NTZ, status STRING);\n```\n\n```sql\n-- nightly job (concept)\nINSERT INTO digest_sales\nSELECT region, CAST(sale_ts AS DATE), SUM(amount)\nFROM RAW.SALES\nGROUP BY region;\n```\n\n## Follow-up Questions\n- How would you test this with varying data volumes?\n- How would you handle data gaps or DST changes?","diagram":null,"difficulty":"beginner","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Google","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T09:09:25.855Z","createdAt":"2026-01-22T09:09:25.856Z"},{"id":"q-5702","question":"Ingest 5TB/day of JSON clickstream into Snowflake. Build a compliant pipeline that: (1) ingests into RAW.clickstream, (2) deduplicates by event_id, (3) masks user_id via a masking policy for downstream views, (4) computes hourly active users in the last 24h and stores in hourly_clicks, (5) publishes a governed subset to a partner via secure data sharing with a read-only view and access controls, plus tests and alerting. How would you implement it?","answer":"Design uses Snowpipe to load RAW.clickstream(event_id, user_id, ts, payload) into RAW. A STREAM/MERGE deduplicates on event_id. A masking policy hides user_id in downstream VIEWs. A TASK runs hourly t","explanation":"## Why This Is Asked\nThe scenario tests end‑to‑end pipeline design with Snowpipe, Streams, Tasks, masking, and data sharing, plus testing and governance.\n\n## Key Concepts\n- Snowpipe ingestion; Streams for dedupe; Tasks for schedules; MASKING POLICY; Secure VIEW; Secure Data Sharing; Tests/Alerts.\n\n## Code Example\n```sql\n-- Masking policy example\nCREATE MASKING POLICY mask_user_id AS (val STRING) RETURNS STRING -> CASE WHEN CURRENT_ROLE() IN ('PARTNER','READER') THEN 'REDACTED' ELSE val END;\n```\n\n## Follow-up Questions\n- How would you test dedupe under clock skew? \n- How to handle schema drift in RAW vs curated zones?","diagram":"flowchart TD\n A[Ingest with Snowpipe] --> B[Dedupe via STREAM/MERGE]\n B --> C[Masking in downstream VIEW]\n C --> D[Hourly aggregates with TASK]\n D --> E[Secure share with partner]\n E --> F[Governance & Tests]","difficulty":"intermediate","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T11:51:34.528Z","createdAt":"2026-01-22T11:51:34.528Z"},{"id":"q-5730","question":"Design a near-real-time digest pipeline for RAW.SALES (sale_id STRING, amount NUMBER, sale_ts TIMESTAMP_NTZ, region STRING, partner_id STRING). Data arrives with duplicates and up to 5 minutes late. Build a Snowflake pipeline using STREAM + TASKs to (a) upsert new/updated rows into CURATED.SALES_DIGEST keyed by sale_id, (b) produce a daily region/partner summary table, and (c) log each run in an audit table. Provide exact SQL for STREAM, TASKs, MERGE upserts, and the summary table creation?","answer":"Use a STREAM on RAW.SALES to capture inserts, and a TASK that runs every 2 minutes to MERGE into CURATED.SALES_DIGEST keyed by sale_id (upsert). Then generate a daily region/partner summary from CURAT","explanation":"## Why This Is Asked\nTests ability to design a robust near-real-time pipeline with change data capture, idempotent upserts, and proper handling of late data. \n\n## Key Concepts\n- Snowflake STREAM + TASK for incremental processing\n- MERGE upserts using sale_id as primary key\n- Watermarking to tolerate late arrivals\n- Daily summaries and audit trails\n\n## Code Example\n```sql\n-- create stream\nCREATE OR REPLACE STREAM RAW.SALES_STREAM ON TABLE RAW.SALES (APPEND_ONLY = FALSE);\n\n-- upsert target\nCREATE TABLE CURATED.SALES_DIGEST (\n  sale_id STRING PRIMARY KEY,\n  amount NUMBER,\n  sale_ts TIMESTAMP_NTZ,\n  region STRING,\n  partner_id STRING\n);\n\n-- upsert task (schedu: 2 minutes)\nCREATE OR REPLACE TASK t_upsert_sales\nWAREHOUSE = WH\nSCHEDULE = '2 MINUTE'\nAS\nMERGE INTO CURATED.SALES_DIGEST AS d\nUSING RAW.SALES_STREAM AS s\n  ON d.sale_id = s.sale_id\nWHEN MATCHED THEN UPDATE SET amount = s.amount, sale_ts = s.sale_ts, region = s.region, partner_id = s.partner_id\nWHEN NOT MATCHED THEN INSERT (sale_id, amount, sale_ts, region, partner_id)\nVALUES (s.sale_id, s.amount, s.sale_ts, s.region, s.partner_id);\n```\n\n## Follow-up Questions\n- How would you monitor data freshness and detect stream lags?\n- How would you adjust for high cardinality regions/partners in the daily summary?","diagram":"flowchart TD\n  A[Ingest RAW.SALES] --> B[STREAM RAW.SALES_STREAM]\n  B --> C[MERGE into CURATED.SALES_DIGEST]\n  C --> D|Daily| E[DAILY_SUMMARY_REGION_PARTNER]\n  D --> F[AUDIT_LOG]","difficulty":"intermediate","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","MongoDB","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T13:24:50.389Z","createdAt":"2026-01-22T13:24:50.389Z"},{"id":"q-5837","question":"Scenario: RAW.SALES(account_id, sale_id, customer_email, amount, ts, region) feeds a partner data share. Deliver a sanitized daily dataset with no plain emails. Implement: (1) SHA256 hashing producing hashed_email, (2) a secure view exposing hashed_email, region, and date(ts), (3) a daily TASK loading CURATED.SALES_SECURE from RAW.SALES, (4) a secure share to the partner account, (5) a validation query ensuring no plain emails exist in the share. Provide exact SQL?","answer":"Hash emails with SHA2(256) and present hashed_email in a secure view; include region, date(ts) and amount_bucket; do not expose customer_email. Use a daily TASK to populate CURATED.SALES_SECURE from R","explanation":"## Why This Is Asked\nTests data sanitization, secure sharing, and automation in Snowflake.\n\n## Key Concepts\n- Data sharing and shares\n- Secure views and masking/hash policies\n- TASKs for scheduled ingestion\n- Validation via information_schema\n\n## Code Example\n```sql\n-- 1) Build secure view with hashed_email\nCREATE OR REPLACE VIEW CURATED.SALES_SECURE AS\nSELECT account_id, sale_id, region,\n       DATE(ts) AS sale_date,\n       amount,\n       SHA2(customer_email, 256) AS hashed_email\nFROM RAW.SALES;\n```\n```sql\n-- 4) Create share and grant access\nCREATE SHARE partner_share;\nALTER SHARE partner_share ADD DATABASE mydb;\nALTER SHARE partner_share ADD SCHEMAS mydb.CURATED;\nALTER SHARE partner_share ADD ACCOUNTS = ('<PARTNER_ACCOUNT_ID>');\n```\n```sql\n-- 5) Validation: no plain email column in secure view\nSELECT COUNT(*) AS plain_email_column\nFROM INFORMATION_SCHEMA.COLUMNS\nWHERE TABLE_SCHEMA = 'CURATED' AND TABLE_NAME = 'SALES_SECURE' AND COLUMN_NAME = 'CUSTOMER_EMAIL';\n```\n","diagram":"flowchart TD\n  A[RAW.SALES] --> B[SHA2 on customer_email -> hashed_email]\n  B --> C[CURATED.SALES_SECURE VIEW]\n  C --> D[Secure SHARE: partner_share]\n  D --> E[Partner Account]","difficulty":"intermediate","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Hashicorp","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T18:57:02.326Z","createdAt":"2026-01-22T18:57:02.326Z"},{"id":"q-6037","question":"In Snowflake, given RAW.TRANSLOG ingesting 200M rows/day for a fintech app, design a near-real-time per-customer risk pipeline that updates CURATED.CUSTOMER_RISK every 2 minutes. Use Streams and Tasks, explain data model (RAW vs CURATED), whether to use a materialized view or a rolling aggregate table, and provide exact SQL commands to: 1) create a stream on RAW.TRANSLOG, 2) create a 2-minute TASK to compute 99th percentile of amount in the last 5 minutes and a 2-minute failed transaction count per customer, 3) MERGE results into CURATED.CUSTOMER_RISK, 4) validate with sample queries?","answer":"Proposed approach: use RAW.TRANSLOG_STREAM; two tasks run every 2 minutes: compute risk_score = APPROX_PERCENTILE(amount, 0.99) over last 5 minutes; failed_count = COUNT(*) FILTER(WHERE status='FAILED","explanation":"## Why This Is Asked\nTests ability to design near-real-time pipelines in Snowflake using streams, tasks, and upserts across RAW and CURATED zones. It also touches late-arriving data, windowed aggregates, and cost considerations.\n\n## Key Concepts\n- Snowflake Streams and Tasks for CDC pipelines\n- Windowed aggregations on streams\n- MERGE upserts into curated tables\n- Decision between MV/rolling table\n- Data governance and cost controls\n\n## Code Example\n```sql\nCREATE STREAM RAW.TRANSLOG_STREAM ON TABLE RAW.TRANSLOG;\nCREATE OR REPLACE TASK risk_update\nWAREHOUSE = 'WH_RISK'\nSCHEDULE = '2 MINUTES'\nWHEN SYSTEM$STREAM_HAS_DATA('RAW.TRANSLOG_STREAM')\nAS\nMERGE INTO CURATED.CUSTOMER_RISK AS TARGET\nUSING (\n  SELECT customer_id,\n         APPROX_PERCENTILE(amount, 0.99) AS risk_score,\n         MAX(ts) AS last_updated\n  FROM RAW.TRANSLOG_STREAM\n  WHERE METADATA$ACTION = 'INSERT'\n    AND ts >= DATEADD(MINUTE, -5, CURRENT_TIMESTAMP())\n  GROUP BY customer_id\n) AS SOURCE\nON TARGET.customer_id = SOURCE.customer_id\nWHEN MATCHED THEN UPDATE SET risk_score = SOURCE.risk_score, last_updated = SOURCE.last_updated\nWHEN NOT MATCHED THEN INSERT (customer_id, risk_score, last_updated) VALUES (SOURCE.customer_id, SOURCE.risk_score, SOURCE.last_updated);\n```\n\n## Follow-up Questions\n- How would you test with synthetic data?\n- How handle schema evolution and rollback?","diagram":null,"difficulty":"advanced","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Plaid","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T05:54:02.898Z","createdAt":"2026-01-23T05:54:02.899Z"},{"id":"q-6061","question":"In a Snowflake data lake serving 10 tenants with strict data isolation and cost controls, design a scalable tenancy model using per-tenant schemas, roles, masking policies, and resource monitors, plus secure shares for common datasets. Explain how you enforce per-tenant quotas, prevent cross-tenant leakage, and validate isolation under peak load. Include concrete SQL snippets for: onboarding a tenant, masking policy, resource monitor, and isolation validation?","answer":"Propose a tenancy model with per-tenant schemas and roles, dynamic masking, and per-tenant resource monitors plus per-tenant secure shares for common datasets. Explain quota enforcement, leakage preve","explanation":"## Why This Is Asked\n\nTests ability to design a scalable multi-tenant Snowflake architecture focusing on strict isolation and cost controls, leveraging schemas, roles, masking policies, resource monitors, and secure shares.\n\n## Key Concepts\n\n- Per-tenant isolation via separate schemas and roles\n- Cost controls with per-tenant RESOURCE MONITORs\n- Dynamic data masking and row-level access patterns\n- Secure data sharing for common datasets without leakage\n- Validation under peak load and automated tests\n\n## Code Example\n\n```javascript\n-- Onboard tenant\nCREATE SCHEMA TENANT_TEN01;\nCREATE ROLE TENANT_TEN01_ROLE;\nGRANT USAGE ON SCHEMA TENANT_TEN01 TO ROLE TENANT_TEN01_ROLE;\nGRANT USAGE ON DATABASE DB TO ROLE TENANT_TEN01_ROLE;\n\n-- Masking policy example\nCREATE MASKING POLICY tenant_email_mask AS (val STRING) RETURNS STRING\n  -> CASE WHEN CURRENT_ROLE() = 'TENANT_TEN01_ROLE' THEN 'REDACTED' ELSE val END;\n\n-- Resource monitor per tenant\nCREATE RESOURCE MONITOR RM_TEN01 WITH CREDIT_LIMIT = 1000;\nALTER WAREHOUSE WH_TEN01 SET RESOURCE_MONITOR = RM_TEN01;\n\n-- Isolation validation\n-- Ensure cross-tenant access is denied\nSELECT * FROM TENANT_TEN02.SALES LIMIT 1;\n```\n\n## Follow-up Questions\n\n- How would you automate tenant onboarding and deprovisioning at scale?\n- How would you evolve schemas without breaking active tenants?","diagram":"flowchart TD\n  A[Tenants] --> B[Per-tenant Schemas]\n  B --> C[Per-tenant Roles]\n  C --> D[Masking & Row-Level Policies]\n  A --> E[Resource Monitors per Tenant]\n  A --> F[Secure Shared Datasets]\n  F --> G[Audit & Validation]","difficulty":"advanced","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","Meta","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T07:10:47.600Z","createdAt":"2026-01-23T07:10:47.600Z"},{"id":"q-6227","question":"Beginner Snowflake-core: A daily data load lands RAW.EVENTS(event_id STRING, user_id STRING, etype STRING, payload VARIANT, created_at TIMESTAMP_NTZ). You need a cost-conscious nightly data-quality gate that: (1) validates event_id NOT NULL and created_at >= current_timestamp()-24h, (2) etype is one of ('click','view','purchase'), (3) payload contains a 'version' field, (4) quarantines any failing rows into RAW.EVENTS_QC_QUARANTINE for review, while updating QA.EVENTS_QC with summary stats. Outline the plan with (a) a lightweight QC query, (b) a NIGHTLY TASK, (c) the QA table, and (d) a basic ALERT when any check fails. Include exact SQL commands for each step?","answer":"QC checks: validate event_id not null, created_at within 24h, etype in allowed set, payload.version exists; quarantine failing rows to RAW.EVENTS_QC_QUARANTINE; log run summary to QA.EVENTS_QC; trigge","explanation":"## Why This Is Asked\n\nThis question tests a beginner's ability to design a low-cost, end-to-end data-quality gate for Snowflake that balances correctness with compute cost. It requires connecting lightweight QC logic to a small nightly job and a simple alerting mechanism, plus practical handling for failing data.\n\n## Key Concepts\n\n- Lightweight QC queries over RAW datasets\n- Snowflake TASKs for scheduled, cost-conscious runs\n- Quarantine/audit tables to isolate and review bad data\n- QA tables for run summaries and simple alert triggering\n- Idempotent, restart-friendly data quality patterns\n\n## Code Example\n\n```sql\n-- Lightweight QC example (conceptual)\nSELECT\n  SUM(CASE WHEN event_id IS NULL THEN 1 ELSE 0 END) AS missing_id,\n  SUM(CASE WHEN created_at < DATEADD(hour,-24,CURRENT_TIMESTAMP()) THEN 1 ELSE 0 END) AS stale_rows,\n  SUM(CASE WHEN etype NOT IN ('click','view','purchase') THEN 1 ELSE 0 END) AS bad_type,\n  SUM(CASE WHEN payload:'version' IS NULL THEN 1 ELSE 0 END) AS missing_version\nFROM RAW.EVENTS;\n```\n\n```sql\n-- Quarantine failing rows (conceptual)\nINSERT INTO RAW.EVENTS_QC_QUARANTINE\nSELECT * FROM RAW.EVENTS\nWHERE event_id IS NULL\n   OR created_at < DATEADD(hour,-24,CURRENT_TIMESTAMP())\n   OR etype NOT IN ('click','view','purchase')\n   OR payload:'version' IS NULL;\n```\n\n```sql\n-- Simple QA table (conceptual)\nCREATE TABLE QA.EVENTS_QC(run_ts TIMESTAMP_NTZ, total_rows NUMBER, failed_rows NUMBER, status STRING);\n```\n\n```sql\n-- Nightly TASK (conceptual)\nCREATE TASK QA.EVENTS_QC_NIGHTLY\nWAREHOUSE = 'XSMALL_WH'\nSCHEDULE = 'USING CRON 0 1 * * *'\nAS\nINSERT INTO QA.EVENTS_QC(run_ts, total_rows, failed_rows, status)\nSELECT CURRENT_TIMESTAMP(),\n       (SELECT COUNT(*) FROM RAW.EVENTS),\n       (SELECT COUNT(*) FROM RAW.EVENTS_QC_QUARANTINE),\n       CASE WHEN (SELECT COUNT(*) FROM RAW.EVENTS_QC_QUARANTINE) > 0 THEN 'FAIL' ELSE 'PASS' END;\n```\n\n```sql\n-- Alerting (conceptual)\nINSERT INTO ALERTS (alert_time, dataset, severity, message)\nSELECT CURRENT_TIMESTAMP(), 'RAW.EVENTS', 'CRITICAL', 'QC failures detected; see QA.EVENTS_QC';\n```","diagram":null,"difficulty":"beginner","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Instacart","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T15:36:07.675Z","createdAt":"2026-01-23T15:36:07.678Z"},{"id":"q-6274","question":"You’re designing a global Snowflake fraud-detection pipeline. RAW.PAYMENTS(payment_id STRING, user_id STRING, amount NUMBER, currency STRING, region STRING, created_ts TIMESTAMP_NTZ, metadata VARIANT). A CURATED FRAUD_RISK(user_id, region, risk_score, updated_at) is refreshed every 5 minutes from an external model. How would you (a) enforce region-based access so analysts only see their region, (b) accelerate last-24h high-risk events by region and customer segment, and (c) minimize storage while preserving 7 days of time travel? Include data-model choices, clustering/materialized views/streams, security policies, a concrete 5-minute ETL snippet, validations, and rollback plan?","answer":"Plan: implement region-based ROW ACCESS POLICIES using a per-session REGION context; use RAW.PAYMENTS and CURATED.FRAUD_RISK with a region-scoped VIEW joining them; cluster by region and created_ts; e","explanation":"## Why This Is Asked\nTests ability to design multi-tenant access, real-time updates, and cost-aware Snowflake patterns at scale.\n\n## Key Concepts\n- Row Access Policies and session context for region isolation\n- RAW vs CURATED data modeling and join strategy\n- Streams, Tasks, and Materialized Views for near-real-time analytics\n- Time Travel retention and safe rollback using cloning or staged loads\n- Cross-region data sharing and cost/performance trade-offs\n\n## Code Example\n```javascript\n-- 5-minute ETL snippet to refresh FRAUD_RISK\nCREATE OR REPLACE TASK FRAUD_RISK_REFRESH\n  WAREHOUSE = WH_X\n  SCHEDULE = '5 MINUTES'\nAS\nMERGE INTO FRAUD_RISK AS t\nUSING (\n  SELECT user_id, region, AVG(pred_score) AS risk_score, CURRENT_TIMESTAMP() AS updated_at\n  FROM EXTERNAL_MODEL_OUTPUT\n  GROUP BY user_id, region\n) s\nON t.user_id = s.user_id\nWHEN MATCHED THEN UPDATE SET risk_score = s.risk_score, updated_at = s.updated_at\nWHEN NOT MATCHED THEN INSERT (user_id, region, risk_score, updated_at) VALUES (s.user_id, s.region, s.risk_score, s.updated_at);\n```\n\n## Follow-up Questions\n- How would you test regional access policies across roles?\n- How would you verify MV freshness and time-travel cost?\n","diagram":null,"difficulty":"advanced","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Discord","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T17:06:35.681Z","createdAt":"2026-01-23T17:06:35.681Z"},{"id":"q-6287","question":"Intermediate Snowflake-core: You operate a multi-tenant analytics platform. RAW.ENTRIES(tenant_id STRING, event_id STRING, payload VARIANT, ts TIMESTAMP_NTZ). DIGEST.TENANT_SUMMARY(tenant_id STRING, day DATE, total_events NUMBER, active_users NUMBER). Design an incremental per-tenant daily summary using Streams and Tasks that scales with thousands of tenants while enforcing tenant data isolation. Include (a) ingestion and schema, (b) a Streams-based delta merge into DIGEST.TENANT_SUMMARY, (c) a per-tenant SECURE_VIEW masking PII, (d) a clustering strategy to optimize cost, (e) late-arrival handling with Time Travel and reprocessing, and (f) monitoring/alerts for failed runs. Provide exact SQL for key steps, including the TASK definitions?","answer":"Leverage a per-tenant delta pipeline: create RAW.ENTRIES_STREAM on RAW.ENTRIES; MERGE into DIGEST.TENANT_SUMMARY by tenant_id and day using a windowed aggregation from payload to update total_events a","explanation":"## Why This Is Asked\nTests practical application of Snowflake Streams/Tasks in a multi-tenant setup, focusing on isolation, incremental processing, and late-arrival handling, plus governance via secure views and cost-conscious design.\n\n## Key Concepts\n- Snowflake Streams and Tasks for incremental pipelines\n- Time Travel and reprocessing in late-arrival scenarios\n- Row-level security and secure views for tenant isolation\n- Clustering and cost optimization in a multi-tenant digest\n\n## Code Example\n```sql\n-- Example: create stream and baseline table\nCREATE OR REPLACE TABLE RAW.ENTRIES (\n  tenant_id STRING,\n  event_id STRING,\n  payload VARIANT,\n  ts TIMESTAMP_NTZ\n);\nCREATE OR REPLACE STREAM RAW.ENTRIES_STREAM ON TABLE RAW.ENTRIES (APPEND_ONLY = FALSE);\n```\n\n## Follow-up Questions\n- How would you test tenant isolation guarantees?\n- How would you scale the secure view layer as tenants grow?\n","diagram":"flowchart TD\n  Ingest[Ingest RAW.ENTRIES] --> Stream[Create RAW.ENTRIES_STREAM]\n  Stream --> Merge[Merge to DIGEST.TENANT_SUMMARY]\n  Merge --> SecureView[Secure per-tenant view]\n  SecureView --> Monitor[Monitor/alerts]","difficulty":"intermediate","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Coinbase","Discord"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T17:51:39.364Z","createdAt":"2026-01-23T17:51:39.365Z"},{"id":"q-6511","question":"Beginner Snowflake-core: implement a nightly data‑quality plan for RAW.SALES that detects anomalies in daily volume and average amount using STREAMS and TASKS. Outline (a) a QC delta query against yesterday and the day before, (b) a STREAM on RAW.SALES to capture changes, (c) a NIGHTLY TASK to run the QC and insert results into QA.SALES_QC, (d) a simple ALERTS mechanism when anomalies are found. Include exact SQL for each step?","answer":"Use a STREAM RAW.SALES_STREAM ON RAW.SALES to capture new rows. Create a NIGHTLY TASK QC_SALES_NIGHTLY that computes yesterday vs day-before daily aggregates (rows, total_amount, avg_amount) and flags","explanation":"## Why This Is Asked\n\nTests practical use of Snowflake streams and tasks to implement incremental data quality checks, a common real‑world need for beginner/intermediate roles.\n\n## Key Concepts\n\n- Snowflake STREAMS for change data capture\n- TASKS for scheduled nightly jobs\n- Lightweight delta QC with daily aggregates\n- Simple alerting table to surface anomalies\n\n## Code Example\n\n```sql\n-- Create a stream to capture changes\nCREATE OR REPLACE STREAM RAW.SALES_STREAM ON TABLE RAW.SALES (APPEND_ONLY = FALSE);\n\n-- QC: nightly delta -- example (pseudo-structure for clarity)\nWITH daily_today AS (\n  SELECT DATE(sale_ts) AS day, COUNT(*) AS rows, SUM(amount) AS total_amount, AVG(amount) AS avg_amount\n  FROM RAW.SALES\n  WHERE sale_ts >= CURRENT_DATE() - 1\n  GROUP BY 1\n), daily_yesterday AS (\n  SELECT DATE(sale_ts) AS day, COUNT(*) AS rows, SUM(amount) AS total_amount, AVG(amount) AS avg_amount\n  FROM RAW.SALES\n  WHERE sale_ts >= CURRENT_DATE() - 2 AND sale_ts < CURRENT_DATE() - 1\n  GROUP BY 1\n)\nSELECT a.day AS day_today, b.day AS day_yesterday, a.rows AS rows_today, b.rows AS rows_yesterday,\n       a.total_amount AS total_today, b.total_amount AS total_yesterday,\n       a.avg_amount AS avg_today, b.avg_amount AS avg_yesterday,\n       CASE\n         WHEN b.avg_amount IS NULL THEN FALSE\n         WHEN ABS(a.avg_amount - b.avg_amount) > 0.5 * NULLIF(b.avg_amount,0) THEN TRUE\n         ELSE FALSE\n       END AS anomaly\nFROM daily_today a\nLEFT JOIN daily_yesterday b ON a.day_today = b.day;\n```\n\n```sql\n-- Create QA tables\nCREATE OR REPLACE TABLE QA.SALES_QC (\n  day_today DATE,\n  day_yesterday DATE,\n  rows_today NUMBER,\n  rows_yesterday NUMBER,\n  total_today NUMBER,\n  total_yesterday NUMBER,\n  avg_today NUMBER,\n  avg_yesterday NUMBER,\n  anomaly BOOLEAN,\n  checked_at TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP()\n);\n\nCREATE OR REPLACE TABLE QA.ALERTS (\n  alert_at TIMESTAMP_NTZ,\n  metric STRING,\n  value NUMBER,\n  note STRING\n);\n```\n\n```sql\n-- Create a nightly task (scheduling example)\nCREATE OR REPLACE TASK QC_SALES_NIGHTLY\n  WAREHOUSE = 'WAREHOUSE_QC'\n  SCHEDULE = 'USING CRON 0 2 * * *' -- 2:00 AM local\nAS\n  INSERT INTO QA.SALES_QC\n  SELECT * FROM (\n    -- the QC query above should be placed here as a CTE or a separate include\n  );\n```\n\n```sql\n-- Alert insertion logic (to be invoked when anomaly = TRUE)\nINSERT INTO QA.ALERTS (alert_at, metric, value, note)\nSELECT CURRENT_TIMESTAMP(), 'SALES_ANOMALY', AVG(avg_today), 'Anomaly detected in daily average amount';\n```\n\n## Follow-up Questions\n\n- How would you test this pipeline locally and in a sandbox?\n- How would you tweak anomaly thresholds for different production loads?","diagram":null,"difficulty":"beginner","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Hashicorp","LinkedIn"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T05:59:43.736Z","createdAt":"2026-01-24T05:59:43.736Z"},{"id":"q-6529","question":"You're building a global fraud-detection pipeline in Snowflake: raw.transactions (txn_id STRING, user_id STRING, amount NUMBER, currency STRING, country STRING, device_id STRING, ts TIMESTAMP_NTZ, metadata VARIANT). Ingest from 6 regions via Snowpipe; design end-to-end near real-time risk scoring that updates every minute, with dashboards showing (a) high-risk transactions in last 5 minutes and (b) average risk by region. Propose data model (RAW vs CURATED), streaming vs batch, retention, and rollback plan, with exact SQL commands to set up streams and tasks and a small rolling-risk query for validation?","answer":"Use a two-layer pipeline: RAW.transactions for ingestion and CURATED.fraud_scores for scoring. Create a STREAM on RAW.transactions and a TASK that runs every minute to compute rolling risk per user/re","explanation":"## Why This Is Asked\nTo assess practical mastery of Snowflake real-time patterns: streams, tasks, and layered data models, with cross-region ingestion, rolling window analytics, rollback/replay, and validation.\n\n## Key Concepts\n- Snowflake Streams and Tasks for near-real-time processing\n- RAW vs CURATED data modeling\n- Rolling window calculations (1-minute window, per-user/region)\n- Ingestion from multiple regions and data replay\n- Rollback and replay strategies (checkpointing, replay table, backfill)\n\n## Code Example\n```javascript\n-- SQL: set up stream and rolling risk (illustrative)\nCREATE OR REPLACE STREAM raw.transactions_strm ON TABLE raw.transactions;\n\nCREATE OR REPLACE TABLE curated.fraud_scores (\n  txn_id STRING,\n  user_id STRING,\n  region STRING,\n  rolling_risk NUMBER,\n  ts TIMESTAMP_NTZ\n);\n\nCREATE OR REPLACE TASK t_update_fraud_risk\nWAREHOUSE = compute_wh\nSCHEDULE = '1 minute'\nAS\nMERGE INTO curated.fraud_scores AS t\nUSING (\n  SELECT txn_id, user_id, region, risk_score, ts\n  FROM raw.transactions_strm\n) s\nON t.txn_id = s.txn_id\nWHEN NOT MATCHED THEN\n  INSERT (txn_id, user_id, region, rolling_risk, ts)\n  VALUES (s.txn_id, s.user_id, s.region, s.risk_score, s.ts);\n```\n\n## Follow-up Questions\n- How would you validate rolling_risk accuracy with late data?\n- How would you scale the compute and manage backfills?","diagram":null,"difficulty":"advanced","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T07:07:57.545Z","createdAt":"2026-01-24T07:07:57.545Z"},{"id":"q-6584","question":"Design a Snowflake-based multi-tenant telemetry store for RAW.telemetry(tenant_id STRING, user_id STRING, event VARIANT, ts TIMESTAMP_NTZ). Propose an architecture that enforces tenant isolation, per-tenant retention, and auditability using ROW ACCESS POLICIES, masking policies, secure views, and a streaming ETL with STREAMS/TASKS, plus time-travel support via cloning. Include concrete steps and trade-offs?","answer":"Isolate per-tenant by filtering RAW.telemetry into CURATED.telemetry using a ROW ACCESS POLICY on tenant_id. Apply MASKING POLICY to PII fields. Use STREAMS + TASKS to populate CURATED incrementally; ","explanation":"## Why This Is Asked\nTests multi-tenant isolation, policy-driven access control, data masking, and production ETL design at scale.\n\n## Key Concepts\n- ROW ACCESS POLICIES\n- MASKING POLICIES\n- Secure views\n- STREAMS and TASKS\n- Time travel and cloning\n- ACCOUNT_USAGE auditing\n\n## Code Example\n```sql\n-- Example row access policy constraining tenant_id\nCREATE OR REPLACE ROW ACCESS POLICY tenant_ra AS\n  (t_id STRING) RETURNS BOOLEAN -> CURRENT_ROLE() IN ('ROLE_TENANT_' || t_id || '_ACCESS');\n```\n\n## Follow-up Questions\n- How would you test and monitor policy performance?\n- How would you onboard a new tenant with minimal downtime?","diagram":"flowchart TD\nA[RAW.telemetry] --> B[STREAMS/TASKS ETL]\nB --> C[CURATED.telemetry]\nC --> D[ACCESS CONTROL: ROW POLICY]\nC --> E[MASKED FIELDS: POLICY]\nF[Forensic Time Travel] --> C","difficulty":"advanced","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Microsoft","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T08:55:24.760Z","createdAt":"2026-01-24T08:55:24.760Z"},{"id":"q-6643","question":"You're building a Snowflake based real-time product analytics pipeline ingesting RAW.PRODUCT_EVENTS (device_id STRING, event_ts TIMESTAMP_NTZ, payload VARIANT) at 100M+ per day. Derive oppty_id from payload and expose per oppty aggregates with sub minute latency while keeping writes non-blocking and cost efficient. Propose a production plan using Snowpipe plus STREAM on RAW.PRODUCT_EVENTS, a TASK to incrementally MERGE into CURATED.OPPTY_AGG, a MV on CURATED.OPPTY_AGG for dashboards, a clustering strategy, and a validation rollback plan. Include exact SQL commands for each step?","answer":"Use a streaming path: Snowpipe ingests RAW.PRODUCT_EVENTS, create a STREAM on RAW.PRODUCT_EVENTS, and a TASK to MERGE into CURATED.OPPTY_AGG (oppty_id, cnt, last_ts) from the stream; expose CURATED.OP","explanation":"## Why This Is Asked\nTests ability to design a real-time, scalable Snowflake data path with streaming, incremental aggregation, and fast dashboards under cost constraints.\n\n## Key Concepts\n- Snowpipe, STREAMS, TASKS\n- Materialized views for fast dashboards\n- Clustering and multi-cluster warehouses for concurrency\n- Validation, rollback, and fault tolerance\n\n## Code Example\n```sql\nCREATE STREAM RAW.PRODUCT_EVENTS_STRM ON TABLE RAW.PRODUCT_EVENTS;\nCREATE MATERIALIZED VIEW CURATED.OPPTY_AGG_MV AS\nSELECT payload:opportunity_id::STRING AS oppty_id, COUNT(*) AS cnt, MAX(event_ts) AS last_ts\nFROM RAW.PRODUCT_EVENTS_STRM\nGROUP BY 1;\n```\n\n## Follow-up Questions\n- How would you handle late-arriving data and backfill?\n- What monitoring would you put in place for stream lag and MV staleness?","diagram":null,"difficulty":"advanced","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Snap","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T11:30:06.379Z","createdAt":"2026-01-24T11:30:06.379Z"},{"id":"q-6885","question":"Ingest 50M+ events per day into RAW.telemetry(device_id STRING, ts TIMESTAMP_NTZ, payload VARIANT). Analysts require per-device 5-minute rolling event counts and median payload size over the last hour, with dashboards refreshing every minute. Design a production plan with data model (RAW vs CURATED), clustering, streams, and tasks for incremental aggregates, and a strategy for late data and cost control. Include exact commands for one incremental materialized view and a 5-minute refresh task?","answer":"Design a per-device 5-minute rolling view using a stream+task architecture. Ingest into RAW.telemetry(device_id, ts, payload). Create a STREAM on RAW.telemetry and a CURATED.device_stats table with the following incremental materialized view: SELECT device_id, DATE_TRUNC('MINUTE', ts) as minute_bucket, COUNT(*) as event_count, MEDIAN(OBJECT_SIZE(payload)) as median_payload_size FROM RAW.telemetry WHERE ts >= DATEADD(HOUR, -1, CURRENT_TIMESTAMP()) GROUP BY device_id, DATE_TRUNC('MINUTE', ts). Implement a 5-minute refresh task using CREATE OR REPLACE TASK device_stats_refresh_task WAREHOUSE = COMPUTE_WH SCHEDULE = '5 MINUTE' AS CALL refresh_device_stats_procedure();","explanation":"## Why This Is Asked\nTests ability to design real-time, scalable Snowflake pipelines with streams, tasks, and curated tables. Focus on late data handling, cost control, and per-device analytics.\n\n## Key Concepts\n- Streams and Tasks for incremental analytics\n- Raw vs Curated data modeling\n- Clustering strategy and windowed aggregations\n- Late-arrival tolerance and backfill validation\n\n## Code Example\n```sql\nCREATE STREAM raw_telemetry_strm ON TABLE RAW.telemetry;\nCREATE TABLE CURATED.device_stats (\n    device_id STRING,\n    minute_bucket TIMESTAMP_NTZ,\n    event_count NUMBER,\n    median_payload_size NUMBER\n) CLUSTER BY (device_id, minute_bucket);\n\nCREATE OR REPLACE PROCEDURE refresh_device_stats_procedure()\nAS $$\nBEGIN\n    INSERT INTO CURATED.device_stats\n    SELECT device_id, \n           DATE_TRUNC('MINUTE', ts) as minute_bucket,\n           COUNT(*) as event_count,\n           MEDIAN(OBJECT_SIZE(payload)) as median_payload_size\n    FROM raw_telemetry_strm\n    WHERE ts >= DATEADD(HOUR, -1, CURRENT_TIMESTAMP())\n    GROUP BY device_id, DATE_TRUNC('MINUTE', ts);\nEND;\n$$;\n\nCREATE OR REPLACE TASK device_stats_refresh_task\nWAREHOUSE = COMPUTE_WH\nSCHEDULE = '5 MINUTE'\nAS CALL refresh_device_stats_procedure();\n```","diagram":null,"difficulty":"advanced","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","NVIDIA","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T06:15:00.958Z","createdAt":"2026-01-24T21:37:24.448Z"},{"id":"q-6917","question":"You're running a multi-tenant analytics platform on Snowflake ingesting RAW.APP_EVENTS(tenant_id, user_id, event_type, event_ts, payload). Design a cost-conscious pipeline that supports thousands of tenants, fast dashboards, and strict isolation. Use (a) per-tenant clustering, (b) daily per-tenant Materialized Views, (c) Streams + Tasks for incremental refresh, (d) SECURE_VIEW masking, (e) partner data-share with masked data, (f) late-arrival handling and monitoring. Include exact SQL for the key steps?","answer":"I’d implement per-tenant isolation with clustering by tenant_id; expose a SECURE_VIEW masking PII; MV_APP_EVENTS_DAILY for daily aggregates refreshed by a STREAM on RAW.APP_EVENTS and a TASK; share ma","explanation":"## Why This Is Asked\nThis question probes cost-aware, multi-tenant Snowflake design balancing isolation, performance, and scale. It requires concrete patterns: clustering, materialized views vs tables, Streams/Tasks, secure views, and partner sharing, plus late-arrival handling.\n\n## Key Concepts\n- Tenant-level clustering and micro-partition pruning\n- Materialized views for fast per-tenant aggregates\n- Streams and Tasks for incremental refresh\n- SECURE_VIEW masking for PII and per-tenant security\n- Secure data sharing for partners with masked data\n- Time Travel and reprocessing for late arrivals\n- Observability: task alerts and query-history dashboards\n\n## Code Example\n```sql\n-- example snippet\nCREATE TABLE RAW.APP_EVENTS (\n  tenant_id STRING,\n  user_id STRING,\n  event_type STRING,\n  event_ts TIMESTAMP_NTZ,\n  payload VARIANT\n);\nCREATE MATERIALIZED VIEW MV_APP_EVENTS_DAILY AS\n  SELECT tenant_id, CAST(event_ts AS DATE) AS day, COUNT(*) AS events\n  FROM RAW.APP_EVENTS\n  GROUP BY 1,2;\n```\n\n## Follow-up Questions\n- How would you test MV refresh latency under spikes?\n- How would you audit per-tenant data access in a shared environment?","diagram":"flowchart TD\n  RAW[RAW.APP_EVENTS] --> MV[MV_APP_EVENTS_DAILY]\n  RAW --> SECURE[SECURE_VIEW]\n  SECURE --> SHARE[PARTNER_SHARE]\n  MV --> DASH[Dashboards]","difficulty":"intermediate","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T22:47:36.514Z","createdAt":"2026-01-24T22:47:36.514Z"},{"id":"q-6933","question":"Beginner Snowflake-core: nightly incremental load from RAW.SALES_NY (sale_id STRING, amount NUMBER, sale_ts TIMESTAMP_NTZ, customer_email STRING) into ANALYTICS.SALES_DIGEST, computing total_amount per customer_email per day. Use a STREAM on RAW.SALES_NY, a TASK at 02:00 UTC, and a MERGE UPSERT into SALES_DIGEST; plus a simple DIGEST_AUDIT entry for each run. Outline exact SQL for each step and explain idempotence?","answer":"Create a stream on RAW.SALES_NY, define a MERGE UPSERT to ANALYTICS.SALES_DIGEST using aggregated data from the stream, add a TASK scheduled at 02:00 UTC to execute the MERGE operation, and append a row to DIGEST_AUDIT for each run.","explanation":"## Why This Is Asked\nTests practical data orchestration in Snowflake using streams, tasks, and MERGE for an idempotent nightly pipeline.\n\n## Key Concepts\n- Snowflake Streams for change data capture\n- Tasks for scheduled ETL\n- MERGE UPSERT for incremental upserts\n- Basic audit logging for runs\n\n## Code Example\n```sql\n-- Create stream\nCREATE STREAM RAW_SALES_NY_STREAM ON TABLE RAW.SALES_NY;\n\n-- MERGE into digest\nMERGE INTO ANALYTICS.SALES_DIGEST AS d\nUSING (\n  SELECT DATE(sale_ts) AS day, customer_email, SUM(amount) AS total_amount\n  FROM RAW_SALES_NY_STREAM\n  GROUP BY 1,2\n) s\nON d.sales_day = s.day AND d.customer_email = s.customer_email\nWHEN MATCHED THEN UPDATE SET total_amount = s.total_amount\nWHEN NOT MATCHED THEN INSERT (sales_day, customer_email, total_amount) VALUES (s.day, s.customer_email, s.total_amount);\n\n-- Create task\nCREATE TASK NIGHTLY_SALES_DIGEST_TASK\nWAREHOUSE = COMPUTE_WH\nSCHEDULE = 'USING CRON 0 2 * * * UTC'\nAS\nBEGIN\n  -- Execute MERGE\n  -- (MERGE statement from above)\n  \n  -- Audit entry\n  INSERT INTO DIGEST_AUDIT (run_ts, records_processed, status)\n  VALUES (CURRENT_TIMESTAMP(), (SELECT COUNT(*) FROM RAW_SALES_NY_STREAM), 'SUCCESS');\nEND;\n\n-- Resume task\nALTER TASK NIGHTLY_SALES_DIGEST_TASK RESUME;\n```\n\n## Idempotence Explained\nThe pipeline is idempotent because:\n1. Streams only capture new/changed records since last consumption\n2. MERGE UPSERT ensures no duplicate rows - matching records are updated, non-matching are inserted\n3. Daily aggregation prevents double-counting across multiple runs\n4. Task scheduling prevents concurrent executions","diagram":null,"difficulty":"beginner","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T05:50:55.357Z","createdAt":"2026-01-24T23:32:07.700Z"},{"id":"q-7040","question":"Beginner Snowflake-core: You have RAW.EVENTS(event_id STRING, payload VARIANT, event_ts TIMESTAMP_NTZ). Design a nightly pipeline that populates CLEAN.EVENTS(event_id STRING, user_id STRING, action STRING, event_ts TIMESTAMP_NTZ) by extracting fields from payload. Ensure idempotent loads, implement a TASK to run nightly, a QA table to track run counts, and a simple alert when counts are off. Provide exact SQL for each step?","answer":"MERGE INTO CLEAN.EVENTS AS t USING (SELECT event_id, payload:'user_id'::STRING AS user_id, payload:'action'::STRING AS action, event_ts FROM RAW.EVENTS WHERE event_ts > (SELECT COALESCE(MAX(event_ts),","explanation":"## Why This Is Asked\nTest practical skill with VARIANT extraction, idempotent loads via MERGE, and scheduling with TASKs plus basic QA.\n\n## Key Concepts\n- VARIANT field extraction\n- MERGE upserts for idempotency\n- CREATE TASK with CRON scheduling\n- QA/audit tables and simple alerting\n\n## Code Example\n```javascript\n// MERGE from RAW to CLEAN\nMERGE INTO CLEAN.EVENTS AS t\nUSING (\n  SELECT event_id,\n         payload:'user_id'::STRING AS user_id,\n         payload:'action'::STRING AS action,\n         event_ts\n  FROM RAW.EVENTS\n  WHERE event_ts > (SELECT COALESCE(MAX(event_ts), TIMESTAMP_NTZ '1970-01-01') FROM CLEAN.EVENTS)\n) s ON t.event_id = s.event_id\nWHEN NOT MATCHED THEN INSERT (event_id, user_id, action, event_ts)\nVALUES (s.event_id, s.user_id, s.action, s.event_ts);\n```\n```javascript\n// Nightly TASK\nCREATE OR REPLACE TASK nightly_events_load\n  WAREHOUSE = WH_LOAD\n  SCHEDULE = 'USING CRON 0 2 * * *'\nAS\nMERGE INTO CLEAN.EVENTS AS t USING (...);\n```\n```javascript\n// QC table\nCREATE TABLE QC.EVENTS_NIGHTLY (\n  run_ts TIMESTAMP_NTZ,\n  loaded_rows NUMBER,\n  errors NUMBER\n);\n```\n## Follow-up Questions\n- How would you handle payload schema drift?\n- How would you extend alerts for partial failures?","diagram":null,"difficulty":"beginner","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Hugging Face","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T07:00:44.124Z","createdAt":"2026-01-25T07:00:44.124Z"},{"id":"q-7094","question":"Beginner Snowflake-core: In RAW.SALES (sale_id STRING, amount NUMBER, sale_ts TIMESTAMP_NTZ, customer_email STRING, customer_ssn STRING), build a compliant analytics path for marketing. Propose masking policy on email/SSN, a masked view for analysts, a nightly refresh of a materialized dataset, and a read-only data share for partners. Include exact SQL commands for each step?","answer":"Introduce masking policies email_mask and ssn_mask, apply to RAW.SALES columns; build ANALYTICS_DB.MARKETING.V_SALES; create MV_SALES; schedule nightly TASK marketing_nightly_refresh to refresh MV_SAL","explanation":"## Why This Is Asked\nThis tests basic Snowflake governance: masking, views, materialized views, tasks, and sharing in a marketing analytics context.\n\n## Key Concepts\n- Masking policies\n- Views vs materialized views\n- Tasks and scheduling\n- Secure data sharing and RBAC\n\n## Code Example\n```sql\nCREATE MASKING POLICY email_mask AS (val STRING) RETURNS STRING\n  -> CASE WHEN (CURRENT_ROLE() = 'ANALYST') THEN 'REDACTED' ELSE val END;\nCREATE MASKING POLICY ssn_mask AS (val STRING) RETURNS STRING\n  -> CASE WHEN (CURRENT_ROLE() = 'ANALYST') THEN 'REDACTED' ELSE val END;\nALTER TABLE RAW.SALES ALTER COLUMN CUSTOMER_EMAIL SET MASKING POLICY email_mask;\nALTER TABLE RAW.SALES ALTER COLUMN CUSTOMER_SSN SET MASKING POLICY ssn_mask;\n```\n\n## Follow-up Questions\n- How would you extend this for dynamic role-based access across multiple regions?\n- What are the performance implications of materialized views vs regular views here?","diagram":"flowchart TD\n  RAW_SALES[RAW.SALES] --> V_SALES[V_SALES_VIEW]\n  V_SALES --> MV_SALES[MV_SALES]\n  MV_SALES --> NIGHTLY[Task: marketing_nightly_refresh]\n  MV_SALES --> SHARE[Share: marketing_share]","difficulty":"beginner","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Tesla","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T08:56:39.958Z","createdAt":"2026-01-25T08:56:39.958Z"},{"id":"q-7243","question":"Beginner Snowflake-core: Implement a 90-day automated retention policy for PUBLIC.CUSTOMER_LOGS (customer_id STRING, action STRING, action_ts TIMESTAMP_NTZ). Outline a plan to (a) archive older rows to PUBLIC.CUSTOMER_LOGS_ARCHIVE, (b) purge main via a nightly TASK, (c) log counts in QA.RETENTION_AUDIT, and (d) issue an ALERT if no rows archived. Provide exact SQL for each step?","answer":"Create ARCHIVE and QA tables, then a nightly TASK in Snowflake Scripting that: (1) INSERT INTO PUBLIC.CUSTOMER_LOGS_ARCHIVE SELECT * FROM PUBLIC.CUSTOMER_LOGS WHERE action_ts < DATEADD('day', -90, CUR","explanation":"Why: demonstrates practical data lifecycle controls, uses ARCHIVE + DELETE pattern, a TASK for automation, and simple alerting. Key steps: schema setup, task definition, audit table, and alert logic. Consider testing with a dry-run in a sandbox and adjusting the retention window.","diagram":"flowchart TD\n  A[Older data detected] --> B[Archive to ARCHIVE]\n  B --> C[Purge from main]\n  C --> D[Update QA log]\n  D --> E[Alert if zero archived]","difficulty":"beginner","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Citadel","Discord"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T14:58:27.731Z","createdAt":"2026-01-25T14:58:27.731Z"},{"id":"q-7285","question":"Beginner Snowflake-core: A daily load brings data into RAW.ORDERS (order_id STRING, customer_id STRING, amount NUMBER, event_ts TIMESTAMP_NTZ). Duplicates may occur due to retries. Outline a nightly dedup plan to upsert into PROD.ORDERS, including: (a) stage ORDERS_STG from RAW.ORDERS with ROW_NUMBER() over (PARTITION BY order_id ORDER BY event_ts DESC) to keep latest; (b) MERGE into PROD.ORDERS to insert or update; (c) QA metrics table QA.ORDERS_DEDUPE_METRICS; (d) ALERT in QA.ALERTS if duplicates_found > 0. Provide exact SQL for each step and a NIGHTLY TASK to run?","answer":"I would stage duplicates in ORDERS_STG from RAW.ORDERS using ROW_NUMBER() to keep only the latest per order_id, then MERGE into PROD.ORDERS to upsert: update when newer, insert when new. Populate QA.O","explanation":"## Why This Is Asked\nTests ability to design a durable nightly dedup workflow with staging, a MERGE upsert, and observable QA metrics in Snowflake.\n\n## Key Concepts\n- Snowflake MERGE upsert semantics\n- ROW_NUMBER() window for dedupe\n- Nightly TASK orchestration\n- QA metrics and alerting via QA.* tables\n\n## Code Example\n```sql\n-- 1) Stage and dedupe\nCREATE OR REPLACE TABLE ORDERS_STG AS SELECT * FROM RAW.ORDERS;\n\nCREATE OR REPLACE VIEW ORDERS_STG_DEDUPE AS\nSELECT * FROM (\n  SELECT t.*, ROW_NUMBER() OVER (PARTITION BY order_id ORDER BY event_ts DESC) AS rn\n  FROM ORDERS_STG t\n) WHERE rn = 1;\n\n-- 2) Upsert\nMERGE INTO PROD.ORDERS AS t\nUSING ORDERS_STG_DEDUPE AS s\nON t.order_id = s.order_id\nWHEN MATCHED THEN UPDATE SET amount = s.amount, event_ts = s.event_ts\nWHEN NOT MATCHED THEN INSERT (order_id, customer_id, amount, event_ts)\nVALUES (s.order_id, s.customer_id, s.amount, s.event_ts);\n\n-- 3) QA\nINSERT INTO QA.ORDERS_DEDUPE_METRICS\nSELECT COUNT(*) AS total_rows, SUM(CASE WHEN ... THEN 1 ELSE 0 END) AS updated_rows\nFROM ORDERS_STG_DEDUPE;\n\n-- 4) Alert\nINSERT INTO QA.ALERTS SELECT CURRENT_TIMESTAMP(), 'Duplicates detected' WHERE (SELECT duplicates_found FROM QA.ORDERS_DEDUPE_METRICS) > 0;\n```","diagram":"flowchart TD\n  A[RAW.ORDERS] --> B[ORDERS_STG]\n  B --> C[ORDERS_STG_DEDUPE]\n  C --> D[PROD.ORDERS (MERGE)]\n  D --> E[QA.ORDERS_DEDUPE_METRICS]\n  E --> F[QA.ALERTS]","difficulty":"beginner","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T16:53:08.171Z","createdAt":"2026-01-25T16:53:08.171Z"},{"id":"q-7509","question":"Advanced Snowflake-core: A fintech app ingests 50M+ device events daily into RAW.SF_EVENTS(device_id STRING, ts TIMESTAMP_NTZ, payload VARIANT). You must compute per-device anomaly_score in CURATED.DEVICE_ANOMALY updated every 2 minutes, with dashboards showing (a) top 20 anomalous devices in last 5 minutes, (b) average anomaly by hour. Design a production plan using Streams, Tasks, and Materialized Views. Address late data, schema evolution, testing, and rollback with exact SQL commands?","answer":"Use a STREAM on RAW.SF_EVENTS to feed a TASK that computes per-device anomaly_score on a 2-minute sliding window, writing to CURATED.DEVICE_ANOMALY (device_id, ts, anomaly_score). Dashboards show last","explanation":"## Why This Is Asked\nTests ability to design a streaming enrichment pipeline at scale, including near‑real‑time anomaly scoring, data modeling (RAW vs CURATED), and robust late-data handling. It also probes testing, rollback, and schema evolution in a production Snowflake setup.\n\n## Key Concepts\n- Snowflake Streams and Tasks for continuous processing\n- Sliding window computations and per‑device aggregations\n- Watermarks, late data handling, and reprocessing paths\n- Data modeling: RAW vs CURATED, with MV considerations\n- Rollback, testing strategies, and schema evolution plans\n\n## Code Example\n```javascript\n-- Core objects\nCREATE OR REPLACE TABLE RAW.SF_EVENTS (\n  device_id STRING,\n  ts TIMESTAMP_NTZ,\n  payload VARIANT\n);\n\nCREATE OR REPLACE STREAM RAW.SF_EVENTS_STREAM ON TABLE RAW.SF_EVENTS;\n\nCREATE OR REPLACE TABLE CURATED.DEVICE_ANOMALY (\n  device_id STRING,\n  ts TIMESTAMP_NTZ,\n  anomaly_score FLOAT\n);\n\nCREATE OR REPLACE TASK t_anomaly_ingest\nWAREHOUSE = 'COMPUTE_WH'\nSCHEDULE = 'USING CRON 2 * * * *'\nAS\nMERGE INTO CURATED.DEVICE_ANOMALY AS d\nUSING (\n  SELECT device_id,\n         MAX(ts) AS ts,\n         AVG(anomaly_score) AS anomaly_score\n  FROM (\n    SELECT device_id, ts, payload, compute_anomaly_score(payload) AS anomaly_score\n    FROM RAW.SF_EVENTS_STREAM\n    WHERE ts >= DATEADD(minute, -2, CURRENT_TIMESTAMP())\n  )\n  GROUP BY device_id\n) s\nON d.device_id = s.device_id\nWHEN MATCHED THEN UPDATE SET ts = s.ts, anomaly_score = s.anomaly_score\nWHEN NOT MATCHED THEN INSERT (device_id, ts, anomaly_score) VALUES (s.device_id, s.ts, s.anomaly_score);\n```\n\n## Follow-up Questions\n- How would you validate latency and anomaly-score accuracy end-to-end?\n- How would you handle schema changes in RAW.SF_EVENTS without downtime?","diagram":"flowchart TD\n  RAW.SF_EVENTS --> RAW_SF_EVENTS_STREAM\n  RAW_SF_EVENTS_STREAM --> t_anomaly_ingest\n  t_anomaly_ingest --> CURATED.DEVICE_ANOMALY\n  CURATED.DEVICE_ANOMALY --> DASHBOARD[Dashboards: Last 5m Top20 & Hourly Avg]","difficulty":"advanced","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Robinhood","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T04:47:24.658Z","createdAt":"2026-01-26T04:47:24.659Z"},{"id":"q-7524","question":"Advanced Snowflake-core scenario: A two-region data lake ingests RAW.TRANSACTIONS (id STRING, amount NUMBER, ts TIMESTAMP_NTZ, customer_id STRING) at high volume. You need near-real-time per-customer aggregates in CURATED.CUSTOMER_AGG, enforce role-based masking, and a disaster-recovery plan with <5 min RPO across regions. Propose architecture using Streams, Tasks, MV/Materialized views, clustering, masking policies, and cross-region replication. Include exact SQL for: creating the stream on RAW.TRANSACTIONS, a TASK to upsert per-customer totals into CURATED.CUSTOMER_AGG, and a MV to support fast queries. Also discuss validation and rollback?","answer":"Use a stream on RAW.TRANSACTIONS and a scheduled TASK that MERGEs per-customer totals into CURATED.CUSTOMER_AGG (GROUP BY customer_id, SUM(amount)) every 5 minutes. Apply role-based masking policies o","explanation":"## Why This Is Asked\nThis question probes the candidate's ability to design a real-time aggregated pipeline in Snowflake across regions, including data governance and DR SLAs. It expects concrete patterns (streams, tasks, MV, masking, cross-region replication) and rationale for chosen trade-offs.\n\n## Key Concepts\n- Streams and Tasks for near-real-time ETL\n- Materialized Views / MV for fast queries\n- Row/Column level masking policies by role\n- Cross-region replication and DR planning\n- Time Travel and rollback strategies\n\n## Code Example\n```sql\n-- Create stream on raw transactions\nCREATE OR REPLACE STREAM RAW_TRANSACTIONS_STREAM ON TABLE RAW.TRANSACTIONS (id STRING, amount NUMBER, ts TIMESTAMP_NTZ, customer_id STRING) APPEND_ONLY = FALSE;\n\n-- Upsert per-customer aggregates (incremental) (simplified example)\nMERGE INTO CURATED.CUSTOMER_AGG AS tgt\nUSING (\n  SELECT customer_id, SUM(amount) AS total_amount, MAX(ts) AS ts\n  FROM RAW.TRANSACTIONS_STREAM\n  GROUP BY customer_id\n) AS src\nON tgt.customer_id = src.customer_id\nWHEN MATCHED THEN UPDATE SET total_amount = src.total_amount, ts = src.ts\nWHEN NOT MATCHED THEN INSERT (customer_id, total_amount, ts) VALUES (src.customer_id, src.total_amount, src.ts);\n```\n\n## Follow-up Questions\n- How would you validate DR failover meets the RPO target during drills?\n- How would you handle late-arriving transactions without corrupting aggregates?","diagram":"flowchart TD\n  RAW[RAW.TRANSACTIONS] --> STREAM[STREAM: RAW_TRANSACTIONS_STREAM]\n  STREAM --> UPSERT[Task: UPSERT to CURATED.CUSTOMER_AGG]\n  UPSERT --> MV[MV: CURATED.CUSTOMER_AGG_FAST]\n  DR[DR Plan] --> DR2[Region B Ready]","difficulty":"advanced","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Goldman Sachs","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T05:51:49.961Z","createdAt":"2026-01-26T05:51:49.961Z"},{"id":"q-7715","question":"Advanced Snowflake-core: Ingesting daily into RAW.transactions (tx_id STRING, user_id STRING, tenant_id STRING, amount NUMBER, tx_ts TIMESTAMP_NTZ). Implement per-tenant analytics window of 18 months; archive data older than 18 months to an external stage ARCHIVE_STAGE (e.g., s3://archive-bucket/tenant_archive) while keeping recent data in Snowflake for dashboards. Design end-to-end using Streams, Tasks, and zero-copy cloning, with MV for aggregated views. Provide exact SQL to: (1) create archival objects and external stage, (2) a daily TASK that moves older data to ARCHIVE, (3) validation checks counting archived vs source, (4) restricted access via views/masking to archival data, and (5) rollback plan. End with ?","answer":"Leverage a STREAM on RAW.TRANSACTIONS, a separate ARCHIVE schema and ARCHIVE_TRANSACTIONS, and a daily TASK to COPY older rows to ARCHIVE and then DELETE from RAW; use a UNION ALL view for dashboards ","explanation":"## Why This Is Asked\nTests end-to-end archival with tenant-scoped retention, integrating streams, tasks, and external storage, plus governance and rollback.\n\n## Key Concepts\n- Streams and Tasks for incremental archival\n- External stages and archival storage\n- Zero-copy cloning and MV for analytics\n- Access controls and masking for archival data\n- Validation and rollback in production\n\n## Code Example\n```javascript\n-- Create archive stage\nCREATE OR REPLACE STAGE ARCHIVE_STAGE\n  URL='s3://archive-bucket/tenant_archive/';\n\nCREATE OR REPLACE SCHEMA ARCHIVE;\nCREATE OR REPLACE TABLE ARCHIVE.TRANSACTIONS LIKE RAW.TRANSACTIONS;\n\n-- Daily archival task (schedules implied)\nCREATE OR REPLACE TASK ARCHIVE_OLDEST_TXN\nWAREHOUSE = WAREHOUSE_X\nSCHEDULE = 'CRON 0 2 * * *'\nAS\nINSERT INTO ARCHIVE.TRANSACTIONS\nSELECT * FROM RAW.TRANSACTIONS\nWHERE tx_ts < DATEADD(month, -18, CURRENT_TIMESTAMP());\nCOPY INTO ARCHIVE_STAGE\nFROM ARCHIVE.TRANSACTIONS\nFILE_FORMAT = (TYPE = 'PARQUET' COMPRESS = 'SNAPPY');\nDELETE FROM RAW.TRANSACTIONS\nWHERE tx_ts < DATEADD(month, -18, CURRENT_TIMESTAMP());\n```\n\n## Follow-up Questions\n- How would you test archival in a non-prod environment?\n- What monitoring would you add to guard against data loss?","diagram":"flowchart TD\n  RAW_TRANSACTIONS[RAW.TRANSACTIONS] --> STREAMS[Streams on RAW]\n  STREAMS --> ARCHIVE_TRANSACTIONS[ARCHIVE.TRANSACTIONS]\n  ARCHIVE_TRANSACTIONS --> ARCHIVE_STAGE[ARCHIVE_STAGE]\n  RAW_TRANSACTIONS --> DASH[Dashboards via VIEW]\n  ARCHIVE_TRANSACTIONS --> RESTORE[Restore/Clone for audits]","difficulty":"advanced","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Tesla","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T14:53:53.565Z","createdAt":"2026-01-26T14:53:53.565Z"},{"id":"q-7735","question":"Beginner Snowflake-core: Implement cost controls for a test environment using Resource Monitors. The test warehouse TEST_WAREHOUSE must stay under 300 credits per month. Provide exact SQL to: 1) create a RESOURCE MONITOR TEST_MON with CREDIT_QUOTA = 300, TRIGGERS ON 50 PERCENT DO NOTIFY, ON 80 PERCENT DO SUSPEND; 2) attach TEST_MON to TEST_WAREHOUSE; 3) verify with SHOW RESOURCE_MONITORS and SHOW WAREHOUSES; 4) describe a simple nightly test plan to confirm NOTIFY and SUSPEND actions?","answer":"CREATE RESOURCE MONITOR TEST_MON WITH CREDIT_QUOTA = 300 TRIGGERS ON 50 PERCENT DO NOTIFY, ON 80 PERCENT DO SUSPEND; ALTER WAREHOUSE TEST_WAREHOUSE SET RESOURCE_MONITORS = ('TEST_MON'); SHOW RESOURCE_","explanation":"## Why This Is Asked\n\nCost control is a core operational skill. This question tests understanding of Snowflake Resource Monitors, thresholds, and practical follow-through from creation to verification.\n\n## Key Concepts\n\n- Resource Monitors and credit quotas\n- TRIGGERS and actions (NOTIFY vs SUSPEND)\n- Attaching monitors to warehouses\n- Basic verification via SHOW commands\n\n## Code Example\n\n```javascript\nCREATE RESOURCE MONITOR TEST_MON\n  WITH CREDIT_QUOTA = 300\n  TRIGGERS\n    ON 50 PERCENT DO NOTIFY,\n    ON 80 PERCENT DO SUSPEND;\n\nALTER WAREHOUSE TEST_WAREHOUSE SET RESOURCE_MONITORS = ('TEST_MON');\n\nSHOW RESOURCE_MONITORS;\nSHOW WAREHOUSES;\n```\n\n## Follow-up Questions\n\n- How would you handle multiple test warehouses with shared budgets?\n- What are the caveats of NOTIFY vs SUSPEND in production review cycles?","diagram":null,"difficulty":"beginner","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Citadel","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T15:44:08.347Z","createdAt":"2026-01-26T15:44:08.348Z"},{"id":"q-7790","question":"Intermediate Snowflake-core: Ingest events from RAW_API_EVENTS(tenant_id STRING, event_id STRING, payload VARIANT, event_ts TIMESTAMP_NTZ). Build a schema-evolving ETL that converts payload keys into dedicated columns on a target table FLATTENED_API_EVENTS, automatically adding new columns when new keys appear, and handles late-arriving data via Time Travel. Include exact SQL for: (a) creating RAW_API_EVENTS and a STREAM, (b) creating the initial FLATTENED_API_EVENTS with a minimal column set, (c) a nightly TASK that runs a JavaScript stored procedure to detect new keys and emit ALTER TABLE ADD COLUMN statements, (d) the INSERT into FLATTENED_API_EVENTS by expanding payload via OBJECT_PAIRS, (e) an audit table CHANGE_LOG recording added columns and reprocess events. Provide exact SQL for key steps?","answer":"Design a streaming ingest: RAW_API_EVENTS with STREAM; FLATTENED_API_EVENTS initially with a minimal set (tenant_id, event_id, event_ts, key, value); a nightly JavaScript stored procedure detects new ","explanation":"## Why This Is Asked\nEvaluates ability to design robust, evolving schemas for semi-structured data, combining Snowflake features like Streams, OBJECT_PAIRS, and dynamic DDL via JavaScript.\n\n## Key Concepts\n- Schema evolution with dynamic DDL via JavaScript SPs\n- OBJECT_PAIRS and FLATTEN for payload extraction\n- Streams and Tasks for incremental ingest\n- Time Travel for late-arrival scenarios\n- Audit trail for schema changes\n\n## Code Example\n```javascript\nCREATE OR REPLACE PROCEDURE EVOLVE_SCHEMA()\nRETURNS STRING\nLANGUAGE JAVASCRIPT\nAS $$\n  // Build and execute dynamic ALTER TABLE statements for new keys\n  var sql = '';\n  // detect new keys from RAW_API_EVENTS payloads and append to sql\n  // sanitize key names to valid identifiers\n  snowflake.execute({sql: sql});\n  return 'ok';\n$$;\n```\n\n## Follow-up Questions\n- How would you test late-arrival reprocessing and ensure idempotence?\n- How would you scale key discovery as payload variety grows across tenants?","diagram":null,"difficulty":"intermediate","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Discord","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T17:49:23.629Z","createdAt":"2026-01-26T17:49:23.629Z"},{"id":"q-7920","question":"Beginner Snowflake-core: Build a minimal end-to-end JSON log ingest from an external stage into Snowflake. Files are daily_log_YYYYMMDD.json with fields: id, cid, act, ts. Provide exact SQL for: (a) JSON file format, (b) external stage, (c) RAW.LOGS table, (d) COPY INTO RAW.LOGS FROM the stage, (e) a STREAM on RAW.LOGS, (f) a MERGE into PROD.LOGS on id, (g) a QA table QA.INGEST_QC with counts, and (h) a nightly TASK and a simple ALERT if zero rows loaded in last run. Include full SQL commands?","answer":"Plan: create json_fmt, stage, RAW.LOGS(id,cid,act,ts); COPY INTO RAW.LOGS FROM @logs_stage PATTERN='daily_log_.*.json'; CREATE STREAM s_raw ON TABLE RAW.LOGS; MERGE PROD.LOGS AS p USING RAW.LOGS AS r ","explanation":"## Why This Is Asked\nTests ability to design a small ingestion pipeline using Snowflake primitives: file formats, stages, bulk load, CDC via streams, upserts with MERGE, quality checks, and a basic alert path.\n\n## Key Concepts\n- JSON file format and external stages\n- Bulk load with COPY INTO\n- Streams for change data capture\n- MERGE for upserts into production table\n- QA checks and simple alerting trigger\n\n## Code Example\n```sql\nCREATE FILE FORMAT json_fmt TYPE='JSON';\nCREATE STAGE logs_stage URL='s3://bucket/logs/' FILE_FORMAT=json_fmt;\nCREATE TABLE RAW.LOGS (id STRING, cid STRING, act STRING, ts TIMESTAMP_NTZ);\nCOPY INTO RAW.LOGS FROM @logs_stage PATTERN='daily_log_.*.json';\nCREATE STREAM s_raw ON TABLE RAW.LOGS;\nMERGE INTO PROD.LOGS AS p USING RAW.LOGS AS r ON p.id = r.id\nWHEN MATCHED THEN UPDATE SET cid = r.cid, act = r.act, ts = r.ts\nWHEN NOT MATCHED THEN INSERT (id, cid, act, ts) VALUES (r.id, r.cid, r.act, r.ts);\nINSERT INTO QA.INGEST_QC SELECT CURRENT_TIMESTAMP(), COUNT(*) FROM RAW.LOGS WHERE ts > DATEADD(hour,-24,CURRENT_TIMESTAMP());\n```\n\n## Follow-up Questions\n- How to handle late-arriving files?\n- How would you scale this for higher volumes or parallelism?","diagram":"flowchart TD\n  A[External Stage] --> B[RAW.LOGS]\n  B --> C[PROD.LOGS via MERGE]\n  B --> D[QA.INGEST_QC]\n  D --> E[ALERT/Monitor]","difficulty":"beginner","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Cloudflare","Databricks"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T22:57:36.763Z","createdAt":"2026-01-26T22:57:36.763Z"},{"id":"q-868","question":"Design a cross-account data-sharing solution in Snowflake for a multinational fintech requiring regional affiliates to access a shared dataset containing PII. How would you implement Secure Data Sharing, dynamic data masking, region-specific RBAC, and auditable access, while enabling updates to masking policies without breaking consumer queries?","answer":"Use a provider-to-consumer Secure Data Share, attach region-specific masking policies to PII columns, and grant region roles in each consumer account. Admins with higher roles see full data; regular r","explanation":"## Why This Is Asked\nThis question tests practical data-sharing governance at scale—cross-account sharing, dynamic masking, RBAC, and auditable access, plus policy evolution without breaking consumers.\n\n## Key Concepts\n- Secure Data Sharing architecture across provider and consumer accounts\n- Dynamic Data Masking policies with role-based access\n- Cross-account RBAC mapping and masking policy scoping\n- Auditing using Snowflake ACCOUNT_USAGE and ACCESS_HISTORY\n- Policy versioning and non-breaking updates\n\n## Code Example\n```sql\n-- Masking policy\nCREATE MASKING POLICY pii_mask AS (VAL STRING) RETURNS STRING ->\nCASE WHEN CURRENT_ROLE() IN ('REGION_A_ANALYST','REGION_B_ADMIN') THEN VAL ELSE 'REDACTED' END;\n\n-- Provider shares\nCREATE SHARE fintech_share;\nGRANT USAGE ON DATABASE fintech_db TO SHARE fintech_share;\nGRANT SELECT ON ALL TABLES IN SCHEMA fintech_db.public TO SHARE fintech_share;\n\n-- Consumer startup\nCREATE DATABASE region_a_db FROM SHARE provider.fintech_share;\n```\n\n## Follow-up Questions\n- How would you test policy updates in a greenfield environment?\n- How would you monitor leakage risk and performance impact of masking at scale?","diagram":"flowchart TD\n  A[Provider] --> B[Secure Data Share]\n  B --> C[Consumer Region A]\n  B --> D[Consumer Region B]\n  C --> E[Dynamic Masking Applied]\n  D --> F[Admins See Full Data]","difficulty":"advanced","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:48:59.902Z","createdAt":"2026-01-12T13:48:59.902Z"},{"id":"q-981","question":"How would you configure Snowflake so regional analysts can run ad-hoc queries on a shared dataset while ensuring isolation, predictable performance, and cost control? Include concrete settings for: (a) warehouse topology (min/max clusters, auto-suspend/resume), (b) RBAC (roles, grant scopes on databases/schemas/tables), (c) cost governance (resource monitors and credit caps), (d) a sample GRANT script giving REGIONAL_ANALYST access to only SALES and EVENTS schemas, and (e) auditing and reproducibility considerations?","answer":"Configure a dedicated ANALYST_WAREHOUSE as a multi-cluster warehouse with MIN 1, MAX 4, AUTO_SUSPEND 300, AUTO_RESUME ON. Create a REGIONAL_ANALYST role with USAGE on the DB and SELECT on SALES and EV","explanation":"## Why This Is Asked\nTests practical Snowflake fundamentals: per-region access, isolation, cost governance, and auditing in a beginner-friendly way.\n\n## Key Concepts\n- Multi-cluster warehouses and auto-suspend/resume\n- RBAC with granular schema/table permissions\n- Resource monitors and credit caps for cost control\n- Auditing via query tags and reproducibility\n- Safe read-only ad-hoc access to shared data\n\n## Code Example\n```javascript\n-- Grants (SQL flavored in a JS code block due to formatting)\nCREATE ROLE REGIONAL_ANALYST;\nGRANT USAGE ON DATABASE FINTECH_DEMO TO ROLE REGIONAL_ANALYST;\nGRANT USAGE ON SCHEMA FINTECH_DEMO.SALES TO ROLE REGIONAL_ANALYST;\nGRANT SELECT ON ALL TABLES IN SCHEMA FINTECH_DEMO.SALES TO ROLE REGIONAL_ANALYST;\nGRANT USAGE ON SCHEMA FINTECH_DEMO.EVENTS TO ROLE REGIONAL_ANALYST;\nGRANT SELECT ON ALL TABLES IN SCHEMA FINTECH_DEMO.EVENTS TO ROLE REGIONAL_ANALYST;\n```\n\n## Follow-up Questions\n- How would you extend permissions if analysts need aggregated results across both schemas?\n- How would you monitor per-user query latency and credit usage, and alert on anomalies?","diagram":"flowchart TD\n  A[Analyst] --> B[ANALYST_WAREHOUSE]\n  B --> C[SALES, EVENTS schemas]\n  C --> D[READ-ONLY access]\n  E[Resource Monitor] --> B\n  F[Auditing] --> A","difficulty":"beginner","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Cloudflare","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T17:44:36.792Z","createdAt":"2026-01-12T17:44:36.792Z"}],"subChannels":["general"],"companies":["Adobe","Airbnb","Amazon","Anthropic","Apple","Bloomberg","Citadel","Cloudflare","Coinbase","Databricks","Discord","DoorDash","Goldman Sachs","Google","Hashicorp","Hugging Face","IBM","Instacart","LinkedIn","Lyft","Meta","Microsoft","MongoDB","NVIDIA","Netflix","OpenAI","Oracle","PayPal","Plaid","Robinhood","Salesforce","Scale Ai","Slack","Snap","Snowflake","Square","Stripe","Tesla","Twitter","Two Sigma","Uber","Zoom"],"stats":{"total":88,"beginner":29,"intermediate":29,"advanced":30,"newThisWeek":37}}