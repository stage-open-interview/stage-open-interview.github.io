{"questions":[{"id":"q-1086","question":"Design a Snowflake CDC pattern for an SCD2 customer_dim using a stream on the source table. Explain how to implement an idempotent upsert via MERGE, how deletes are represented, and how to handle late-arriving changes to preserve history?","answer":"Use Snowflake Streams on the source table and a MERGE into the SCD2 target to achieve idempotent CDC. Use src.metadata$action to distinguish INSERT, UPDATE, DELETE. For deletes, set end_date on the ex","explanation":"## Why This Is Asked\n\nDemonstrates pragmatic CDC design in Snowflake, including stream-driven MERGE, and SCD2 history guarantees.\n\n## Key Concepts\n\n- Snowflake Streams and metadata$action to detect DML\n- MERGE pattern for upserts and tombstone deletes\n- SCD Type 2 history with start_date and end_date\n- Handling late-arriving data and idempotence\n\n## Code Example\n\n```javascript\n// Snowflake MERGE with STREAM snippet\nconst mergeSql = `\nMERGE INTO customer_dim AS target\nUSING customers_stream AS src\nON target.customer_id = src.customer_id\nWHEN MATCHED AND src.metadata$action = 'DELETE' THEN DELETE\nWHEN MATCHED THEN UPDATE SET\n  name = src.name, email = src.email, ...\nWHEN NOT MATCHED THEN INSERT (customer_id, name, email, start_date)\nVALUES (src.customer_id, src.name, src.email, src.metadata$start_time);\n`;\n```\n\n## Follow-up Questions\n\n- How would you test this CDC pipeline end-to-end? \n- How do you handle conflicts if key updates arrive out of order?","diagram":null,"difficulty":"intermediate","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T22:20:06.609Z","createdAt":"2026-01-12T22:20:06.609Z"},{"id":"q-1152","question":"Design a least-privilege access layer in Snowflake for a multi-tenant data lake spanning five domains (marketing, sales, finance, analytics, operations). Describe how you would implement ROW ACCESS POLICIES and MASKING POLICIES on a payments table (columns: id, customer_id, amount, card_number, region) to restrict data by region and user role, and how you would audit access?","answer":"Define domain roles and a service role; apply a masking policy on card_number to reveal full value only to ADMIN_ROLE and mask others; implement a row access policy using a user-context REGION value t","explanation":"## Why This Is Asked\nThis question verifies practical application of Snowflake security primitives (masking, RLS, views) in a multi-tenant context, including auditing and governance considerations.\n\n## Key Concepts\n- ROW ACCESS POLICY and MASKING POLICY\n- USER_CONTEXT/SESSION_CONTEXT for dynamic access decisions\n- SECURE_VIEW vs direct table access for enforcement\n- Auditing via QUERY_TAGs and ACCESS_HISTORY\n\n## Code Example\n```sql\n-- Masking policy: full card only for admins\nCREATE MASKING POLICY mask_card_number AS (val STRING) RETURNS STRING ->\n  CASE WHEN CURRENT_ROLE() = 'ADMIN_ROLE' THEN val\n       ELSE CONCAT('XXXX-XXXX-XXXX-', RIGHT(val, 4))\n  END;\n\nALTER TABLE payments MODIFY COLUMN card_number SET MASKING POLICY mask_card_number;\n\n-- Row access policy by region from user-context (conceptual)\nCREATE OR REPLACE ROW ACCESS POLICY region_rls AS (region STRING) RETURNS BOOLEAN ->\n  region = CURRENT_SESSION_REGION();\n\n-- Secure view to surface allowed columns\nCREATE VIEW payments_secure AS\nSELECT id, customer_id, amount, region\nFROM payments;\n``` \n\n## Follow-up Questions\n- How would you test masking and region filtering for multiple roles? \n- What are the performance implications of masking and RLS at scale, and how would you mitigate them?","diagram":null,"difficulty":"advanced","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Databricks","Lyft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T01:36:23.332Z","createdAt":"2026-01-13T01:36:23.332Z"},{"id":"q-1239","question":"You’re building a beginner Snowflake task: a large SALES table is routinely queried with date-range filters. Propose a minimal clustering solution to improve pruning. Include exact commands to add a single clustering key on SALE_DATE, how to monitor its effectiveness, and how to decide if reclustering is needed. Keep changes focused and explain validation steps with a simple test?","answer":"ALTER TABLE SALES CLUSTER BY (SALE_DATE);\n-- Inspect clustering depth\nSELECT SYSTEM$CLUSTERING_INFORMATION('MY_DB.MY_SCHEMA.SALES');\n-- If depth indicates suboptimal pruning, recluster\nALTER TABLE SAL","explanation":"## Why This Is Asked\nTests practical use of manual clustering for query pruning, plus pragmatic validation and cost awareness for beginners.\n\n## Key Concepts\n- Clustering keys: SALE_DATE to aid pruning\n- SYSTEM$CLUSTERING_INFORMATION: evaluate clustering depth\n- Reclustering threshold: avoid over-clustering; measure impact with a test query\n\n## Code Example\n```javascript\nALTER TABLE SALES CLUSTER BY (SALE_DATE);\nSELECT SYSTEM$CLUSTERING_INFORMATION('MY_DB.MY_SCHEMA.SALES');\nALTER TABLE SALES RECLUSTER;\n-- Simple validation query (date range) to compare performance pre/post\n```\n\n## Follow-up Questions\n- How would you adjust clustering if the query pattern includes filters on region in addition to date?\n- What metrics would you monitor over time to assess clustering maintenance needs?","diagram":"flowchart TD\n  A[SALES Table] --> B[Add clustering key: SALE_DATE]\n  B --> C[Monitor: SYSTEM$CLUSTERING_INFORMATION]\n  C --> D[Evaluate clustering depth]\n  D --> E[Option: RECLUSTER]","difficulty":"beginner","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T06:34:44.338Z","createdAt":"2026-01-13T06:34:44.338Z"},{"id":"q-1337","question":"You're ingesting data daily into table sensor_readings(device_id STRING, ts TIMESTAMP_NTZ, reading FLOAT). You frequently query last 24 hours per device. Propose a beginner-friendly optimization path, choosing clustering key, automatic clustering, or a materialized view, and include an exact query to compute per-device count and average reading for the last 24 hours?","answer":"Best path: add a clustering key on (ts, device_id) to improve pruning for the last-24h window and rely on Snowflake's automatic clustering to maintain it. Avoid premature materialized views in a begin","explanation":"## Why This Is Asked\nTests understanding of Snowflake data pruning, clustering strategies, and practical trade-offs for lightweight optimization.\n\n## Key Concepts\n- Clustering keys and partition pruning\n- Automatic vs manual clustering maintenance\n- Trade-offs of materialized views for beginners\n- 24-hour window query patterns\n\n## Code Example\n```sql\nSELECT device_id, COUNT(*) AS n, AVG(reading) AS avg_reading\nFROM sensor_readings\nWHERE ts >= DATEADD(hour, -24, CURRENT_TIMESTAMP())\nGROUP BY device_id;\n```\n\n## Follow-up Questions\n- How would you monitor clustering impact and costs?\n- When would a materialized view be preferable to clustering in this scenario?","diagram":null,"difficulty":"beginner","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["PayPal","Plaid","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T11:43:36.716Z","createdAt":"2026-01-13T11:43:36.716Z"},{"id":"q-1475","question":"You have a Snowflake table raw_events with columns file_name STRING, payload VARIANT containing an array at payload:'events'. Each event is an object { event, timestamp, user: { id }, region, amount }. Design a beginner-friendly approach to compute daily total_amount and per-user purchases for the last 7 days using a temporary VIEW and LATERAL FLATTEN. Provide the exact query you would run?","answer":"Create a lightweight view that normalizes the JSON events and use LATERAL FLATTEN on payload:'events' to explode each event. Filter where event='purchase', cast timestamp to date, and group by day, us","explanation":"## Why This Is Asked\n\nTests ability to work with semi-structured data, practice using VIEW for readability, and apply LATERAL FLATTEN to explode arrays, all at a beginner level.\n\n## Key Concepts\n\n- VARIANT and semi-structured data access\n- LATERAL FLATTEN for arrays\n- VIEWs to avoid repeated computation\n- Date/time conversion and basic aggregation\n\n## Code Example\n\n```sql\nCREATE OR REPLACE VIEW v_purchases_last7 AS\nSELECT\n  DATE_TRUNC('day', TO_TIMESTAMP_NTZ(evt.value:\"timestamp\"))::DATE AS day,\n  evt.value:\"user\":\"id\" AS user_id,\n  evt.value:\"region\" AS region,\n  evt.value:\"amount\"::FLOAT AS amount\nFROM raw_events r,\nLATERAL FLATTEN(input => r.payload:\"events\") AS evt\nWHERE evt.value:\"event\" = 'purchase'\n  AND TO_TIMESTAMP_NTZ(evt.value:\"timestamp\") >= DATEADD(day, -7, CURRENT_DATE());\n\nSELECT day, user_id, region, SUM(amount) AS total_amount, COUNT(*) AS purchases\nFROM v_purchases_last7\nGROUP BY day, user_id, region\nORDER BY day, region, user_id;\n```\n\n## Follow-up Questions\n\n- How would you handle missing fields in some events?\n- How would you index or cluster this data for faster queries on the last 7 days?","diagram":null,"difficulty":"beginner","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Cloudflare","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T18:49:36.671Z","createdAt":"2026-01-13T18:49:36.671Z"},{"id":"q-1499","question":"You're building a Snowflake-based telemetry store: table `raw.events` (device_id STRING, ts TIMESTAMP_NTZ, payload VARIANT) ingests 50M+ rows daily. BI dashboards query per-device event_count and median reading for last 7 days and last 24 hours. Propose a pragmatic optimization, selecting between clustering, automatic clustering, and a rolling aggregate MV/table. Include exact commands to implement your approach and how you would validate improvements?","answer":"Cluster by (device_id, DATE(ts)); enable AUTOMATIC_CLUSTERING. Create 7-day rolling MV mv_last7: SELECT device_id, COUNT(*) AS event_count, APPROX_PERCENTILE(CAST(payload:reading AS FLOAT), 0.5) AS me","explanation":"## Why This Is Asked\nTests understanding of Snowflake data-pruning strategies and trade-offs between manual clustering, auto clustering, and pre-aggregation for rolling windows.\n\n## Key Concepts\n- Clustering keys and pruning effectiveness\n- Automatic clustering maintenance\n- Rolling aggregates via materialized views\n- Validation via cost/latency experiments\n\n## Code Example\n```sql\nALTER TABLE raw.events CLUSTER BY (device_id, DATE(ts));\nALTER TABLE raw.events SET AUTOMATIC_CLUSTERING = TRUE;\nCREATE MATERIALIZED VIEW mv_last7 AS\nSELECT device_id, COUNT(*) AS event_count,\n       APPROX_PERCENTILE(CAST(payload:reading AS FLOAT), 0.5) AS median_reading\nFROM raw.events\nWHERE ts >= DATEADD(DAY, -7, CURRENT_TIMESTAMP())\nGROUP BY device_id;\n```\n\n## Follow-up Questions\n- How would you adapt this for the 24-hour window?\n- What are the trade-offs of MV vs a rolling summary table in Snowflake?","diagram":"flowchart TD\n  A[Ingest] --> B[raw.events]\n  B --> C[Cluster by device_id, DATE(ts)]\n  C --> D[Automatic Clustering]\n  B --> E[MV mv_last7]\n  E --> F[BI queries]","difficulty":"intermediate","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Tesla","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T19:35:08.508Z","createdAt":"2026-01-13T19:35:08.508Z"},{"id":"q-1567","question":"You're operating a Snowflake telemetry store with table raw.events(device_id STRING, ts TIMESTAMP_NTZ, payload VARIANT) ingesting 100M+ rows daily. BI requires per-device metrics: (a) last hour event_count, (b) last 24h median of payload.metrics.reading. Propose a production-ready strategy: data model, clustering choices vs automatic clustering vs materialized views, and how to validate performance. Include exact commands to implement and how to verify improvements?","answer":"Implement a two-layer architecture: maintain raw.events for ingestion while creating per-device rolling aggregates. Use a STREAM on raw.events with a TASK to populate agg_hour(device_id, hour_ts, cnt) for the last hour, and a materialized view for the 24-hour median. Cluster by device_id to optimize query pruning.","explanation":"## Why This Is Asked\nThis question evaluates end-to-end design capabilities for streaming analytics at scale, encompassing data modeling, pruning strategies, and performance validation.\n\n## Key Concepts\n- Streams and Tasks for incremental processing\n- Materialized views versus rolling aggregates\n- Clustering versus automatic clustering for hot queries\n- MEDIAN calculations over VARIANT payloads with per-device rollups\n\n## Code Example\n```sql\nCREATE STREAM s_raw_events ON TABLE raw.events;\nCREATE TASK t_last_hour WAREHOUSE = WH AS\n  INSERT INTO agg_hour\n  SELECT device_id, DATE_TRUNC('hour', ts) AS hour_ts, COUNT(*)\n  FROM s_raw_events\n  GROUP BY device_id, DATE_TRUNC('hour', ts);\n```","diagram":null,"difficulty":"advanced","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T06:18:13.265Z","createdAt":"2026-01-13T22:30:34.610Z"},{"id":"q-1605","question":"Design a Snowflake-based near real-time anomaly-detection pipeline for telemetry events. Ingested table: raw_events(device_id STRING, ts TIMESTAMP_NTZ, payload VARIANT). Ingests 200M+ rows/day. You need to detect spikes in a metric inside payload (e.g., latency) on a per-device basis with a 5-minute window and surface alerts to an alerts table when a spike exceeds a dynamic threshold. Describe the architecture using Streams, Tasks, and possibly a Snowflake procedure; include DDLs to create the stream, a task schedule, sample SQL for the rolling compute, and how you would validate the pipeline end-to-end?","answer":"Design a Snowflake-based near real-time anomaly-detection pipeline using Streams and Tasks. Ingest: raw_events(device_id STRING, ts TIMESTAMP_NTZ, payload VARIANT). Build a stream on raw_events, a staging table, and an alerts table. Create a scheduled task that processes new stream data every minute, computes rolling 5-minute windows per device, extracts metrics from the payload (e.g., payload.latency_ms), compares against dynamic thresholds, and inserts alerts using MERGE for idempotency. Include error handling and monitoring.","explanation":"## Why This Is Asked\n\nTests practical use of Streams and Tasks for near real-time analytics, including per-device rolling metrics, dynamic thresholds, and idempotent alerts. Evaluates fault handling, scheduling, and test strategies under high-volume ingestion.\n\n## Key Concepts\n- Snowflake Streams & Tasks\n- Rolling window analytics per device\n- Variant payload parsing (e.g., payload.latency_ms)\n- Idempotent alerting with merge/upsert\n- Validation with synthetic data and backfill handling\n\n## Code Example\n```sql\nCREATE OR REPLACE TABLE raw_events (\n  device_id STRING,\n  ts TIMESTAMP_NTZ,\n  payload VARIANT\n);\n\nCREATE OR REPLACE STREAM raw_events_stream ON TABLE raw_events;\n\nCREATE OR REPLACE TABLE device_metrics (\n  device_id STRING,\n  window_start TIMESTAMP_NTZ,\n  metric_name STRING,\n  avg_value FLOAT,\n  threshold FLOAT,\n  processed_at TIMESTAMP_NTZ\n);\n\nCREATE OR REPLACE TABLE alerts (\n  alert_id STRING DEFAULT UUID_STRING(),\n  device_id STRING,\n  metric_name STRING,\n  detected_at TIMESTAMP_NTZ,\n  actual_value FLOAT,\n  threshold FLOAT,\n  created_at TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP(),\n  PRIMARY KEY (alert_id)\n);\n\nCREATE OR REPLACE PROCEDURE detect_anomalies()\nRETURNS STRING\nLANGUAGE JAVASCRIPT\nAS\n$$\n  var sql = `\n    MERGE INTO alerts a\n    USING (\n      SELECT \n        e.device_id,\n        'latency_ms' as metric_name,\n        AVG(e.payload:latency_ms::FLOAT) as avg_latency,\n        3 * STDDEV(e.payload:latency_ms::FLOAT) as threshold,\n        DATEADD('minute', 5, MIN(e.ts)) as window_end\n      FROM raw_events_stream e\n      WHERE e.ts >= DATEADD('minute', -5, CURRENT_TIMESTAMP())\n      GROUP BY e.device_id, DATE_TRUNC('minute', e.ts, 5)\n      HAVING AVG(e.payload:latency_ms::FLOAT) > 3 * STDDEV(e.payload:latency_ms::FLOAT)\n    ) src\n    ON (a.device_id = src.device_id AND a.detected_at = src.window_end)\n    WHEN NOT MATCHED THEN INSERT (\n      device_id, metric_name, detected_at, actual_value, threshold\n    ) VALUES (\n      src.device_id, src.metric_name, src.window_end, src.avg_latency, src.threshold\n    );\n    \n    ALTER STREAM raw_events_stream SET OFFSET = CURRENT_TIMESTAMP();\n  `;\n  \n  snowflake.execute({sqlText: sql});\n  return 'Anomaly detection completed';\n$$;\n\nCREATE OR REPLACE TASK anomaly_detection_task\n  WAREHOUSE = COMPUTE_WH\n  SCHEDULE = '1 MINUTE'\n  AS\n  CALL detect_anomalies();\n\nALTER TASK anomaly_detection_task RESUME;\n```\n\n## Validation Strategy\n1. **Synthetic Data Generation**: Insert test events with known anomaly patterns\n2. **Backfill Testing**: Process historical data to verify threshold calculations\n3. **End-to-End Monitoring**: Track stream lag, task execution times, and alert volumes\n4. **Threshold Calibration**: Validate dynamic thresholds against historical baselines","diagram":"flowchart TD\n  A[raw_events] --> B[raw_events_stream]\n  B --> C[staging_events]\n  C --> D[rolling_metrics_per_device]\n  D --> E[alerts_table]\n  E --> F[downstream_consumer]","difficulty":"intermediate","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Scale Ai","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T04:20:20.721Z","createdAt":"2026-01-14T02:32:59.488Z"},{"id":"q-1678","question":"You're provisioning a Snowflake warehouse for a multi-tenant SaaS app. All tenant data sits in one table: raw_events (tenant_id STRING, event_ts TIMESTAMP_NTZ, event_type STRING, payload VARIANT) ingesting 40M+ rows/day. Propose a hybrid approach: (1) add a composite clustering key (tenant_id, event_ts) for pruning, and (2) a rolling 7-day materialized view with per-tenant metrics. Include exact SQL commands to implement clustering, MV, and a validation query showing the improvement?","answer":"Add clustering on (tenant_id, event_ts) to prune by tenant and date, and create a rolling 7-day MV mv_events_7d with per-tenant day counts. Implement with: create table ... CLUSTER BY (tenant_id, even","explanation":"## Why This Is Asked\n\nTests ability to design scalable, tenant-aware analytics in Snowflake, balancing manual clustering with a rolling materialized view for time-bound queries. It also probes validation strategies and drift monitoring.\n\n## Key Concepts\n\n- Composite clustering keys for multi-tenant pruning\n- Rolling 7-day materialized views for per-tenant aggregates\n- Validation via query plans (EXPLAIN) and performance comparisons\n- MV refresh considerations and clustering drift monitoring\n\n## Code Example\n\n```sql\nCREATE TABLE raw_events (\n  tenant_id STRING,\n  event_ts TIMESTAMP_NTZ,\n  event_type STRING,\n  payload VARIANT\n)\nCLUSTER BY (tenant_id, event_ts);\n\nCREATE MATERIALIZED VIEW mv_events_7d AS\nSELECT tenant_id,\n       DATE_TRUNC('day', event_ts) AS day,\n       COUNT(*) AS event_cnt\nFROM analytics.raw_events\nWHERE event_ts >= DATEADD('day', -7, CURRENT_TIMESTAMP())\nGROUP BY tenant_id, day;\n\nEXPLAIN SELECT tenant_id, day, event_cnt\nFROM mv_events_7d\nWHERE day >= DATE_TRUNC('day', CURRENT_TIMESTAMP()) - 1;\n```\n\n```sql\n-- Basic validation: inspect MV contents for recent days\nSELECT tenant_id, day, event_cnt\nFROM mv_events_7d\nORDER BY tenant_id, day;\n```\n\n## Follow-up Questions\n\n- How would you adapt this approach for hot vs cold data and potential tiering?\n- What metrics and alerts would you set to detect MV staleness or clustering drift?","diagram":null,"difficulty":"intermediate","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Goldman Sachs","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T06:50:55.919Z","createdAt":"2026-01-14T06:50:55.919Z"},{"id":"q-1711","question":"In a Snowflake telemetry store with table raw.events(device_id STRING, ts TIMESTAMP_NTZ, payload VARIANT) ingesting 50M+ rows daily, implement a production-grade data retention pipeline that archives data older than 365 days to an external stage and keeps only the last 365 days in the main table, while preserving query performance on recent data. Describe the architecture and provide exact SQL commands to create the stage, stream, task, and archive procedure, plus a validation plan?","answer":"Architect a retention pipeline using a STREAM on raw.events, a TASK to purge data older than 365 days, and an external archive via COPY INTO to the S3 stage. The plan preserves recent data in Snowflak","explanation":"## Why This Is Asked\nAssess lifecycle design, automation, and validation for production Snowflake workloads.\n\n## Key Concepts\n- Data retention windows, Streams, Tasks\n- External stages and COPY INTO\n- Validation strategies and data correctness\n\n## Code Example\n```javascript\n-- Archive older data to external stage\nCREATE OR REPLACE STAGE archive_stage URL='s3://bucket/archive/';\nCREATE OR REPLACE STREAM raw.events_stream ON TABLE raw.events (APPEND_ONLY = FALSE);\nCREATE OR REPLACE TASK archive_old_events\n  WAREHOUSE = compute_wh\n  SCHEDULE = 'USING CRON 0 2 * * *'\nAS\n  COPY INTO @archive_stage/events FROM (\n    SELECT * FROM raw.events WHERE ts < DATEADD(DAY, -365, CURRENT_TIMESTAMP())\n  ) FILE_FORMAT=(TYPE=CSV);\nDELETE FROM raw.events WHERE ts < DATEADD(DAY, -365, CURRENT_TIMESTAMP());\n```\n\n## Follow-up Questions\n- How would you test idempotency and failure recovery for the archive task?\n- What metrics would you monitor to ensure retention does not impact recent-data queries?","diagram":null,"difficulty":"advanced","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Microsoft","Oracle","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T07:46:52.119Z","createdAt":"2026-01-14T07:46:52.119Z"},{"id":"q-868","question":"Design a cross-account data-sharing solution in Snowflake for a multinational fintech requiring regional affiliates to access a shared dataset containing PII. How would you implement Secure Data Sharing, dynamic data masking, region-specific RBAC, and auditable access, while enabling updates to masking policies without breaking consumer queries?","answer":"Use a provider-to-consumer Secure Data Share, attach region-specific masking policies to PII columns, and grant region roles in each consumer account. Admins with higher roles see full data; regular r","explanation":"## Why This Is Asked\nThis question tests practical data-sharing governance at scale—cross-account sharing, dynamic masking, RBAC, and auditable access, plus policy evolution without breaking consumers.\n\n## Key Concepts\n- Secure Data Sharing architecture across provider and consumer accounts\n- Dynamic Data Masking policies with role-based access\n- Cross-account RBAC mapping and masking policy scoping\n- Auditing using Snowflake ACCOUNT_USAGE and ACCESS_HISTORY\n- Policy versioning and non-breaking updates\n\n## Code Example\n```sql\n-- Masking policy\nCREATE MASKING POLICY pii_mask AS (VAL STRING) RETURNS STRING ->\nCASE WHEN CURRENT_ROLE() IN ('REGION_A_ANALYST','REGION_B_ADMIN') THEN VAL ELSE 'REDACTED' END;\n\n-- Provider shares\nCREATE SHARE fintech_share;\nGRANT USAGE ON DATABASE fintech_db TO SHARE fintech_share;\nGRANT SELECT ON ALL TABLES IN SCHEMA fintech_db.public TO SHARE fintech_share;\n\n-- Consumer startup\nCREATE DATABASE region_a_db FROM SHARE provider.fintech_share;\n```\n\n## Follow-up Questions\n- How would you test policy updates in a greenfield environment?\n- How would you monitor leakage risk and performance impact of masking at scale?","diagram":"flowchart TD\n  A[Provider] --> B[Secure Data Share]\n  B --> C[Consumer Region A]\n  B --> D[Consumer Region B]\n  C --> E[Dynamic Masking Applied]\n  D --> F[Admins See Full Data]","difficulty":"advanced","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T13:48:59.902Z","createdAt":"2026-01-12T13:48:59.902Z"},{"id":"q-981","question":"How would you configure Snowflake so regional analysts can run ad-hoc queries on a shared dataset while ensuring isolation, predictable performance, and cost control? Include concrete settings for: (a) warehouse topology (min/max clusters, auto-suspend/resume), (b) RBAC (roles, grant scopes on databases/schemas/tables), (c) cost governance (resource monitors and credit caps), (d) a sample GRANT script giving REGIONAL_ANALYST access to only SALES and EVENTS schemas, and (e) auditing and reproducibility considerations?","answer":"Configure a dedicated ANALYST_WAREHOUSE as a multi-cluster warehouse with MIN 1, MAX 4, AUTO_SUSPEND 300, AUTO_RESUME ON. Create a REGIONAL_ANALYST role with USAGE on the DB and SELECT on SALES and EV","explanation":"## Why This Is Asked\nTests practical Snowflake fundamentals: per-region access, isolation, cost governance, and auditing in a beginner-friendly way.\n\n## Key Concepts\n- Multi-cluster warehouses and auto-suspend/resume\n- RBAC with granular schema/table permissions\n- Resource monitors and credit caps for cost control\n- Auditing via query tags and reproducibility\n- Safe read-only ad-hoc access to shared data\n\n## Code Example\n```javascript\n-- Grants (SQL flavored in a JS code block due to formatting)\nCREATE ROLE REGIONAL_ANALYST;\nGRANT USAGE ON DATABASE FINTECH_DEMO TO ROLE REGIONAL_ANALYST;\nGRANT USAGE ON SCHEMA FINTECH_DEMO.SALES TO ROLE REGIONAL_ANALYST;\nGRANT SELECT ON ALL TABLES IN SCHEMA FINTECH_DEMO.SALES TO ROLE REGIONAL_ANALYST;\nGRANT USAGE ON SCHEMA FINTECH_DEMO.EVENTS TO ROLE REGIONAL_ANALYST;\nGRANT SELECT ON ALL TABLES IN SCHEMA FINTECH_DEMO.EVENTS TO ROLE REGIONAL_ANALYST;\n```\n\n## Follow-up Questions\n- How would you extend permissions if analysts need aggregated results across both schemas?\n- How would you monitor per-user query latency and credit usage, and alert on anomalies?","diagram":"flowchart TD\n  A[Analyst] --> B[ANALYST_WAREHOUSE]\n  B --> C[SALES, EVENTS schemas]\n  C --> D[READ-ONLY access]\n  E[Resource Monitor] --> B\n  F[Auditing] --> A","difficulty":"beginner","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Cloudflare","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T17:44:36.792Z","createdAt":"2026-01-12T17:44:36.792Z"}],"subChannels":["general"],"companies":["Adobe","Airbnb","Anthropic","Citadel","Cloudflare","Coinbase","Databricks","Goldman Sachs","Google","Hugging Face","Lyft","Microsoft","MongoDB","NVIDIA","Oracle","PayPal","Plaid","Robinhood","Scale Ai","Snap","Snowflake","Stripe","Tesla","Zoom"],"stats":{"total":12,"beginner":4,"intermediate":4,"advanced":4,"newThisWeek":12}}