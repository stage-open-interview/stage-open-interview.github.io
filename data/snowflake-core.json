{"questions":[{"id":"q-1086","question":"Design a Snowflake CDC pattern for an SCD2 customer_dim using a stream on the source table. Explain how to implement an idempotent upsert via MERGE, how deletes are represented, and how to handle late-arriving changes to preserve history?","answer":"Use Snowflake Streams on the source table and a MERGE into the SCD2 target to achieve idempotent CDC. Use src.metadata$action to distinguish INSERT, UPDATE, DELETE. For deletes, set end_date on the ex","explanation":"## Why This Is Asked\n\nDemonstrates pragmatic CDC design in Snowflake, including stream-driven MERGE, and SCD2 history guarantees.\n\n## Key Concepts\n\n- Snowflake Streams and metadata$action to detect DML\n- MERGE pattern for upserts and tombstone deletes\n- SCD Type 2 history with start_date and end_date\n- Handling late-arriving data and idempotence\n\n## Code Example\n\n```javascript\n// Snowflake MERGE with STREAM snippet\nconst mergeSql = `\nMERGE INTO customer_dim AS target\nUSING customers_stream AS src\nON target.customer_id = src.customer_id\nWHEN MATCHED AND src.metadata$action = 'DELETE' THEN DELETE\nWHEN MATCHED THEN UPDATE SET\n  name = src.name, email = src.email, ...\nWHEN NOT MATCHED THEN INSERT (customer_id, name, email, start_date)\nVALUES (src.customer_id, src.name, src.email, src.metadata$start_time);\n`;\n```\n\n## Follow-up Questions\n\n- How would you test this CDC pipeline end-to-end? \n- How do you handle conflicts if key updates arrive out of order?","diagram":null,"difficulty":"intermediate","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T22:20:06.609Z","createdAt":"2026-01-12T22:20:06.609Z"},{"id":"q-1152","question":"Design a least-privilege access layer in Snowflake for a multi-tenant data lake spanning five domains (marketing, sales, finance, analytics, operations). Describe how you would implement ROW ACCESS POLICIES and MASKING POLICIES on a payments table (columns: id, customer_id, amount, card_number, region) to restrict data by region and user role, and how you would audit access?","answer":"Define domain roles and a service role; apply a masking policy on card_number to reveal full value only to ADMIN_ROLE and mask others; implement a row access policy using a user-context REGION value t","explanation":"## Why This Is Asked\nThis question verifies practical application of Snowflake security primitives (masking, RLS, views) in a multi-tenant context, including auditing and governance considerations.\n\n## Key Concepts\n- ROW ACCESS POLICY and MASKING POLICY\n- USER_CONTEXT/SESSION_CONTEXT for dynamic access decisions\n- SECURE_VIEW vs direct table access for enforcement\n- Auditing via QUERY_TAGs and ACCESS_HISTORY\n\n## Code Example\n```sql\n-- Masking policy: full card only for admins\nCREATE MASKING POLICY mask_card_number AS (val STRING) RETURNS STRING ->\n  CASE WHEN CURRENT_ROLE() = 'ADMIN_ROLE' THEN val\n       ELSE CONCAT('XXXX-XXXX-XXXX-', RIGHT(val, 4))\n  END;\n\nALTER TABLE payments MODIFY COLUMN card_number SET MASKING POLICY mask_card_number;\n\n-- Row access policy by region from user-context (conceptual)\nCREATE OR REPLACE ROW ACCESS POLICY region_rls AS (region STRING) RETURNS BOOLEAN ->\n  region = CURRENT_SESSION_REGION();\n\n-- Secure view to surface allowed columns\nCREATE VIEW payments_secure AS\nSELECT id, customer_id, amount, region\nFROM payments;\n``` \n\n## Follow-up Questions\n- How would you test masking and region filtering for multiple roles? \n- What are the performance implications of masking and RLS at scale, and how would you mitigate them?","diagram":null,"difficulty":"advanced","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Databricks","Lyft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T01:36:23.332Z","createdAt":"2026-01-13T01:36:23.332Z"},{"id":"q-1239","question":"You’re building a beginner Snowflake task: a large SALES table is routinely queried with date-range filters. Propose a minimal clustering solution to improve pruning. Include exact commands to add a single clustering key on SALE_DATE, how to monitor its effectiveness, and how to decide if reclustering is needed. Keep changes focused and explain validation steps with a simple test?","answer":"ALTER TABLE SALES CLUSTER BY (SALE_DATE);\n-- Inspect clustering depth\nSELECT SYSTEM$CLUSTERING_INFORMATION('MY_DB.MY_SCHEMA.SALES');\n-- If depth indicates suboptimal pruning, recluster\nALTER TABLE SAL","explanation":"## Why This Is Asked\nTests practical use of manual clustering for query pruning, plus pragmatic validation and cost awareness for beginners.\n\n## Key Concepts\n- Clustering keys: SALE_DATE to aid pruning\n- SYSTEM$CLUSTERING_INFORMATION: evaluate clustering depth\n- Reclustering threshold: avoid over-clustering; measure impact with a test query\n\n## Code Example\n```javascript\nALTER TABLE SALES CLUSTER BY (SALE_DATE);\nSELECT SYSTEM$CLUSTERING_INFORMATION('MY_DB.MY_SCHEMA.SALES');\nALTER TABLE SALES RECLUSTER;\n-- Simple validation query (date range) to compare performance pre/post\n```\n\n## Follow-up Questions\n- How would you adjust clustering if the query pattern includes filters on region in addition to date?\n- What metrics would you monitor over time to assess clustering maintenance needs?","diagram":"flowchart TD\n  A[SALES Table] --> B[Add clustering key: SALE_DATE]\n  B --> C[Monitor: SYSTEM$CLUSTERING_INFORMATION]\n  C --> D[Evaluate clustering depth]\n  D --> E[Option: RECLUSTER]","difficulty":"beginner","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T06:34:44.338Z","createdAt":"2026-01-13T06:34:44.338Z"},{"id":"q-1337","question":"You're ingesting data daily into table sensor_readings(device_id STRING, ts TIMESTAMP_NTZ, reading FLOAT). You frequently query last 24 hours per device. Propose a beginner-friendly optimization path, choosing clustering key, automatic clustering, or a materialized view, and include an exact query to compute per-device count and average reading for the last 24 hours?","answer":"Best path: add a clustering key on (ts, device_id) to improve pruning for the last-24h window and rely on Snowflake's automatic clustering to maintain it. Avoid premature materialized views in a begin","explanation":"## Why This Is Asked\nTests understanding of Snowflake data pruning, clustering strategies, and practical trade-offs for lightweight optimization.\n\n## Key Concepts\n- Clustering keys and partition pruning\n- Automatic vs manual clustering maintenance\n- Trade-offs of materialized views for beginners\n- 24-hour window query patterns\n\n## Code Example\n```sql\nSELECT device_id, COUNT(*) AS n, AVG(reading) AS avg_reading\nFROM sensor_readings\nWHERE ts >= DATEADD(hour, -24, CURRENT_TIMESTAMP())\nGROUP BY device_id;\n```\n\n## Follow-up Questions\n- How would you monitor clustering impact and costs?\n- When would a materialized view be preferable to clustering in this scenario?","diagram":null,"difficulty":"beginner","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["PayPal","Plaid","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T11:43:36.716Z","createdAt":"2026-01-13T11:43:36.716Z"},{"id":"q-1475","question":"You have a Snowflake table raw_events with columns file_name STRING, payload VARIANT containing an array at payload:'events'. Each event is an object { event, timestamp, user: { id }, region, amount }. Design a beginner-friendly approach to compute daily total_amount and per-user purchases for the last 7 days using a temporary VIEW and LATERAL FLATTEN. Provide the exact query you would run?","answer":"Create a lightweight view that normalizes the JSON events and use LATERAL FLATTEN on payload:'events' to explode each event. Filter where event='purchase', cast timestamp to date, and group by day, us","explanation":"## Why This Is Asked\n\nTests ability to work with semi-structured data, practice using VIEW for readability, and apply LATERAL FLATTEN to explode arrays, all at a beginner level.\n\n## Key Concepts\n\n- VARIANT and semi-structured data access\n- LATERAL FLATTEN for arrays\n- VIEWs to avoid repeated computation\n- Date/time conversion and basic aggregation\n\n## Code Example\n\n```sql\nCREATE OR REPLACE VIEW v_purchases_last7 AS\nSELECT\n  DATE_TRUNC('day', TO_TIMESTAMP_NTZ(evt.value:\"timestamp\"))::DATE AS day,\n  evt.value:\"user\":\"id\" AS user_id,\n  evt.value:\"region\" AS region,\n  evt.value:\"amount\"::FLOAT AS amount\nFROM raw_events r,\nLATERAL FLATTEN(input => r.payload:\"events\") AS evt\nWHERE evt.value:\"event\" = 'purchase'\n  AND TO_TIMESTAMP_NTZ(evt.value:\"timestamp\") >= DATEADD(day, -7, CURRENT_DATE());\n\nSELECT day, user_id, region, SUM(amount) AS total_amount, COUNT(*) AS purchases\nFROM v_purchases_last7\nGROUP BY day, user_id, region\nORDER BY day, region, user_id;\n```\n\n## Follow-up Questions\n\n- How would you handle missing fields in some events?\n- How would you index or cluster this data for faster queries on the last 7 days?","diagram":null,"difficulty":"beginner","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Cloudflare","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T18:49:36.671Z","createdAt":"2026-01-13T18:49:36.671Z"},{"id":"q-1499","question":"You're building a Snowflake-based telemetry store: table `raw.events` (device_id STRING, ts TIMESTAMP_NTZ, payload VARIANT) ingests 50M+ rows daily. BI dashboards query per-device event_count and median reading for last 7 days and last 24 hours. Propose a pragmatic optimization, selecting between clustering, automatic clustering, and a rolling aggregate MV/table. Include exact commands to implement your approach and how you would validate improvements?","answer":"Cluster by (device_id, DATE(ts)); enable AUTOMATIC_CLUSTERING. Create 7-day rolling MV mv_last7: SELECT device_id, COUNT(*) AS event_count, APPROX_PERCENTILE(CAST(payload:reading AS FLOAT), 0.5) AS me","explanation":"## Why This Is Asked\nTests understanding of Snowflake data-pruning strategies and trade-offs between manual clustering, auto clustering, and pre-aggregation for rolling windows.\n\n## Key Concepts\n- Clustering keys and pruning effectiveness\n- Automatic clustering maintenance\n- Rolling aggregates via materialized views\n- Validation via cost/latency experiments\n\n## Code Example\n```sql\nALTER TABLE raw.events CLUSTER BY (device_id, DATE(ts));\nALTER TABLE raw.events SET AUTOMATIC_CLUSTERING = TRUE;\nCREATE MATERIALIZED VIEW mv_last7 AS\nSELECT device_id, COUNT(*) AS event_count,\n       APPROX_PERCENTILE(CAST(payload:reading AS FLOAT), 0.5) AS median_reading\nFROM raw.events\nWHERE ts >= DATEADD(DAY, -7, CURRENT_TIMESTAMP())\nGROUP BY device_id;\n```\n\n## Follow-up Questions\n- How would you adapt this for the 24-hour window?\n- What are the trade-offs of MV vs a rolling summary table in Snowflake?","diagram":"flowchart TD\n  A[Ingest] --> B[raw.events]\n  B --> C[Cluster by device_id, DATE(ts)]\n  C --> D[Automatic Clustering]\n  B --> E[MV mv_last7]\n  E --> F[BI queries]","difficulty":"intermediate","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Tesla","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T19:35:08.508Z","createdAt":"2026-01-13T19:35:08.508Z"},{"id":"q-1567","question":"You're operating a Snowflake telemetry store with table raw.events(device_id STRING, ts TIMESTAMP_NTZ, payload VARIANT) ingesting 100M+ rows daily. BI requires per-device metrics: (a) last hour event_count, (b) last 24h median of payload.metrics.reading. Propose a production-ready strategy: data model, clustering choices vs automatic clustering vs materialized views, and how to validate performance. Include exact commands to implement and how to verify improvements?","answer":"Implement a two-layer architecture: maintain raw.events for ingestion while creating per-device rolling aggregates. Use a STREAM on raw.events with a TASK to populate agg_hour(device_id, hour_ts, cnt) for the last hour, and a materialized view for the 24-hour median. Cluster by device_id to optimize query pruning.","explanation":"## Why This Is Asked\nThis question evaluates end-to-end design capabilities for streaming analytics at scale, encompassing data modeling, pruning strategies, and performance validation.\n\n## Key Concepts\n- Streams and Tasks for incremental processing\n- Materialized views versus rolling aggregates\n- Clustering versus automatic clustering for hot queries\n- MEDIAN calculations over VARIANT payloads with per-device rollups\n\n## Code Example\n```sql\nCREATE STREAM s_raw_events ON TABLE raw.events;\nCREATE TASK t_last_hour WAREHOUSE = WH AS\n  INSERT INTO agg_hour\n  SELECT device_id, DATE_TRUNC('hour', ts) AS hour_ts, COUNT(*)\n  FROM s_raw_events\n  GROUP BY device_id, DATE_TRUNC('hour', ts);\n```","diagram":null,"difficulty":"advanced","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T06:18:13.265Z","createdAt":"2026-01-13T22:30:34.610Z"},{"id":"q-1605","question":"Design a Snowflake-based near real-time anomaly-detection pipeline for telemetry events. Ingested table: raw_events(device_id STRING, ts TIMESTAMP_NTZ, payload VARIANT). Ingests 200M+ rows/day. You need to detect spikes in a metric inside payload (e.g., latency) on a per-device basis with a 5-minute window and surface alerts to an alerts table when a spike exceeds a dynamic threshold. Describe the architecture using Streams, Tasks, and possibly a Snowflake procedure; include DDLs to create the stream, a task schedule, sample SQL for the rolling compute, and how you would validate the pipeline end-to-end?","answer":"Design a Snowflake-based near real-time anomaly-detection pipeline using Streams and Tasks. Ingest: raw_events(device_id STRING, ts TIMESTAMP_NTZ, payload VARIANT). Build a stream on raw_events, a staging table, and an alerts table. Create a scheduled task that processes new stream data every minute, computes rolling 5-minute windows per device, extracts metrics from the payload (e.g., payload.latency_ms), compares against dynamic thresholds, and inserts alerts using MERGE for idempotency. Include error handling and monitoring.","explanation":"## Why This Is Asked\n\nTests practical use of Streams and Tasks for near real-time analytics, including per-device rolling metrics, dynamic thresholds, and idempotent alerts. Evaluates fault handling, scheduling, and test strategies under high-volume ingestion.\n\n## Key Concepts\n- Snowflake Streams & Tasks\n- Rolling window analytics per device\n- Variant payload parsing (e.g., payload.latency_ms)\n- Idempotent alerting with merge/upsert\n- Validation with synthetic data and backfill handling\n\n## Code Example\n```sql\nCREATE OR REPLACE TABLE raw_events (\n  device_id STRING,\n  ts TIMESTAMP_NTZ,\n  payload VARIANT\n);\n\nCREATE OR REPLACE STREAM raw_events_stream ON TABLE raw_events;\n\nCREATE OR REPLACE TABLE device_metrics (\n  device_id STRING,\n  window_start TIMESTAMP_NTZ,\n  metric_name STRING,\n  avg_value FLOAT,\n  threshold FLOAT,\n  processed_at TIMESTAMP_NTZ\n);\n\nCREATE OR REPLACE TABLE alerts (\n  alert_id STRING DEFAULT UUID_STRING(),\n  device_id STRING,\n  metric_name STRING,\n  detected_at TIMESTAMP_NTZ,\n  actual_value FLOAT,\n  threshold FLOAT,\n  created_at TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP(),\n  PRIMARY KEY (alert_id)\n);\n\nCREATE OR REPLACE PROCEDURE detect_anomalies()\nRETURNS STRING\nLANGUAGE JAVASCRIPT\nAS\n$$\n  var sql = `\n    MERGE INTO alerts a\n    USING (\n      SELECT \n        e.device_id,\n        'latency_ms' as metric_name,\n        AVG(e.payload:latency_ms::FLOAT) as avg_latency,\n        3 * STDDEV(e.payload:latency_ms::FLOAT) as threshold,\n        DATEADD('minute', 5, MIN(e.ts)) as window_end\n      FROM raw_events_stream e\n      WHERE e.ts >= DATEADD('minute', -5, CURRENT_TIMESTAMP())\n      GROUP BY e.device_id, DATE_TRUNC('minute', e.ts, 5)\n      HAVING AVG(e.payload:latency_ms::FLOAT) > 3 * STDDEV(e.payload:latency_ms::FLOAT)\n    ) src\n    ON (a.device_id = src.device_id AND a.detected_at = src.window_end)\n    WHEN NOT MATCHED THEN INSERT (\n      device_id, metric_name, detected_at, actual_value, threshold\n    ) VALUES (\n      src.device_id, src.metric_name, src.window_end, src.avg_latency, src.threshold\n    );\n    \n    ALTER STREAM raw_events_stream SET OFFSET = CURRENT_TIMESTAMP();\n  `;\n  \n  snowflake.execute({sqlText: sql});\n  return 'Anomaly detection completed';\n$$;\n\nCREATE OR REPLACE TASK anomaly_detection_task\n  WAREHOUSE = COMPUTE_WH\n  SCHEDULE = '1 MINUTE'\n  AS\n  CALL detect_anomalies();\n\nALTER TASK anomaly_detection_task RESUME;\n```\n\n## Validation Strategy\n1. **Synthetic Data Generation**: Insert test events with known anomaly patterns\n2. **Backfill Testing**: Process historical data to verify threshold calculations\n3. **End-to-End Monitoring**: Track stream lag, task execution times, and alert volumes\n4. **Threshold Calibration**: Validate dynamic thresholds against historical baselines","diagram":"flowchart TD\n  A[raw_events] --> B[raw_events_stream]\n  B --> C[staging_events]\n  C --> D[rolling_metrics_per_device]\n  D --> E[alerts_table]\n  E --> F[downstream_consumer]","difficulty":"intermediate","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Scale Ai","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T04:20:20.721Z","createdAt":"2026-01-14T02:32:59.488Z"},{"id":"q-1678","question":"You're provisioning a Snowflake warehouse for a multi-tenant SaaS app. All tenant data sits in one table: raw_events (tenant_id STRING, event_ts TIMESTAMP_NTZ, event_type STRING, payload VARIANT) ingesting 40M+ rows/day. Propose a hybrid approach: (1) add a composite clustering key (tenant_id, event_ts) for pruning, and (2) a rolling 7-day materialized view with per-tenant metrics. Include exact SQL commands to implement clustering, MV, and a validation query showing the improvement?","answer":"Add clustering on (tenant_id, event_ts) to prune by tenant and date, and create a rolling 7-day MV mv_events_7d with per-tenant day counts. Implement with: create table ... CLUSTER BY (tenant_id, even","explanation":"## Why This Is Asked\n\nTests ability to design scalable, tenant-aware analytics in Snowflake, balancing manual clustering with a rolling materialized view for time-bound queries. It also probes validation strategies and drift monitoring.\n\n## Key Concepts\n\n- Composite clustering keys for multi-tenant pruning\n- Rolling 7-day materialized views for per-tenant aggregates\n- Validation via query plans (EXPLAIN) and performance comparisons\n- MV refresh considerations and clustering drift monitoring\n\n## Code Example\n\n```sql\nCREATE TABLE raw_events (\n  tenant_id STRING,\n  event_ts TIMESTAMP_NTZ,\n  event_type STRING,\n  payload VARIANT\n)\nCLUSTER BY (tenant_id, event_ts);\n\nCREATE MATERIALIZED VIEW mv_events_7d AS\nSELECT tenant_id,\n       DATE_TRUNC('day', event_ts) AS day,\n       COUNT(*) AS event_cnt\nFROM analytics.raw_events\nWHERE event_ts >= DATEADD('day', -7, CURRENT_TIMESTAMP())\nGROUP BY tenant_id, day;\n\nEXPLAIN SELECT tenant_id, day, event_cnt\nFROM mv_events_7d\nWHERE day >= DATE_TRUNC('day', CURRENT_TIMESTAMP()) - 1;\n```\n\n```sql\n-- Basic validation: inspect MV contents for recent days\nSELECT tenant_id, day, event_cnt\nFROM mv_events_7d\nORDER BY tenant_id, day;\n```\n\n## Follow-up Questions\n\n- How would you adapt this approach for hot vs cold data and potential tiering?\n- What metrics and alerts would you set to detect MV staleness or clustering drift?","diagram":null,"difficulty":"intermediate","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Goldman Sachs","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T06:50:55.919Z","createdAt":"2026-01-14T06:50:55.919Z"},{"id":"q-1711","question":"In a Snowflake telemetry store with table raw.events(device_id STRING, ts TIMESTAMP_NTZ, payload VARIANT) ingesting 50M+ rows daily, implement a production-grade data retention pipeline that archives data older than 365 days to an external stage and keeps only the last 365 days in the main table, while preserving query performance on recent data. Describe the architecture and provide exact SQL commands to create the stage, stream, task, and archive procedure, plus a validation plan?","answer":"Architect a retention pipeline using a STREAM on raw.events, a TASK to purge data older than 365 days, and an external archive via COPY INTO to the S3 stage. The plan preserves recent data in Snowflak","explanation":"## Why This Is Asked\nAssess lifecycle design, automation, and validation for production Snowflake workloads.\n\n## Key Concepts\n- Data retention windows, Streams, Tasks\n- External stages and COPY INTO\n- Validation strategies and data correctness\n\n## Code Example\n```javascript\n-- Archive older data to external stage\nCREATE OR REPLACE STAGE archive_stage URL='s3://bucket/archive/';\nCREATE OR REPLACE STREAM raw.events_stream ON TABLE raw.events (APPEND_ONLY = FALSE);\nCREATE OR REPLACE TASK archive_old_events\n  WAREHOUSE = compute_wh\n  SCHEDULE = 'USING CRON 0 2 * * *'\nAS\n  COPY INTO @archive_stage/events FROM (\n    SELECT * FROM raw.events WHERE ts < DATEADD(DAY, -365, CURRENT_TIMESTAMP())\n  ) FILE_FORMAT=(TYPE=CSV);\nDELETE FROM raw.events WHERE ts < DATEADD(DAY, -365, CURRENT_TIMESTAMP());\n```\n\n## Follow-up Questions\n- How would you test idempotency and failure recovery for the archive task?\n- What metrics would you monitor to ensure retention does not impact recent-data queries?","diagram":null,"difficulty":"advanced","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Microsoft","Oracle","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T07:46:52.119Z","createdAt":"2026-01-14T07:46:52.119Z"},{"id":"q-2024","question":"Implement row-level security for regional data access in Snowflake. Table `customer.sales` has `customer_id`, `region`, `amount`, `last_purchase_ts`. Roles `SA_US` and `SA_EU` should only see rows for their region. Describe and implement a Snowflake ROW ACCESS POLICY using a role-context and attach it to the table, then provide a sample query that would be allowed for role `SA_US` and a test plan to verify enforcement?","answer":"Create a ROW ACCESS POLICY that maps roles to regions and apply it to the region column. For example: region_rlp (region STRING) RETURNS BOOLEAN AS CASE WHEN CURRENT_ROLE() = 'SA_US' THEN region = 'US","explanation":"## Why This Is Asked\nTests practical use of Snowflake Row Access Policies, role-based access, and testing.\n\n## Key Concepts\n- Row Access Policies\n- CURRENT_ROLE()\n- ALTER TABLE ADD ROW ACCESS POLICY\n- Validation via role switching and selective querying\n\n## Code Example\n```javascript\nCREATE OR REPLACE ROW ACCESS POLICY region_rlp AS (region STRING) RETURNS BOOLEAN ->\n  CASE\n    WHEN CURRENT_ROLE() = 'SA_US' THEN region = 'US'\n    WHEN CURRENT_ROLE() = 'SA_EU' THEN region = 'EU'\n    ELSE FALSE\n  END;\nALTER TABLE customer.sales ADD ROW ACCESS POLICY region_rlp ON (region);\n```\n\n## Follow-up Questions\n- How would you test policy coverage with automated tests?\n- How would you extend to additional regions or dynamic role mappings?","diagram":"flowchart TD\n  A[Role SA_US] --> B[Query on region column]\n  B --> C{Policy allows?}\n  C -->|Yes| D[Rows returned]\n  C -->|No| E[Zero rows]","difficulty":"beginner","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Hugging Face","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T21:33:15.741Z","createdAt":"2026-01-14T21:33:15.741Z"},{"id":"q-2048","question":"You're building a multi-tenant analytics warehouse in Snowflake. Ingested events go into raw.events(tenant_id STRING, event_ts TIMESTAMP_NTZ, payload VARIANT). BI needs per-tenant metrics: (a) count of events in the last 15 minutes, (b) 95th percentile of latency from payload.metrics.latency in the last 24 hours. Propose a production-ready architecture: data model (partitioning, clustering), streaming vs batch paths (streams/tasks vs MV), decision between automatic clustering and materialized views, and a concrete validation plan with exact SQL commands?","answer":"Implement a streaming architecture using Snowflake's STREAM and TASK constructs to incrementally process raw events into per-tenant metrics. Create a STREAM on raw.events to capture new records, then schedule a TASK that continuously updates tenant_metrics with 15-minute event counts and 24-hour 95th percentile latency calculations extracted from payload.metrics.latency. This approach delivers real-time analytics while optimizing compute costs through incremental processing.","explanation":"## Why This Is Asked\nTests ability to design multi-tenant analytics architectures with streaming ingestion, cost-efficient aggregation strategies, and proper utilization of Snowflake's native constructs (streams, tasks, clustering, materialized views).\n\n## Key Concepts\n- Multi-tenant data modeling and isolation strategies\n- Streams and tasks for incremental ETL pipelines\n- Per-tenant windowed aggregates and percentile calculations\n- Clustering strategies versus materialized views versus automatic clustering\n- Real-time analytics with cost optimization\n\n## Code Example\n```sql\n-- Core tables\nCREATE TABLE raw.events (\n  tenant_id STRING, \n  event_ts TIMESTAMP_NTZ, \n  payload VARIANT\n);\n\n-- Streaming infrastructure\nCREATE STREAM s_raw AS SELECT * FROM raw.events;\n\n-- Target metrics table\nCREATE TABLE tenant_metrics (\n  tenant_id STRING,\n  event_count_15min NUMBER,\n  p95_latency_24h NUMBER,\n  updated_at TIMESTAMP_NTZ\n);\n\n-- Incremental processing task\nCREATE OR REPLACE TASK process_metrics\n  WAREHOUSE = analytics_wh\n  SCHEDULE = '1 MINUTE'\nAS\nINSERT INTO tenant_metrics\nSELECT \n  tenant_id,\n  COUNT(*) as event_count_15min,\n  PERCENTILE_CONT(0.95) WITHIN GROUP (\n    ORDER BY payload.metrics.latency\n  ) as p95_latency_24h,\n  CURRENT_TIMESTAMP()\nFROM s_raw \nWHERE event_ts >= DATEADD(minute, -15, CURRENT_TIMESTAMP())\nGROUP BY tenant_id;\n```","diagram":"flowchart TD\nA[Ingest] --> B[RAW.EVENTS]\nB --> C[STREAM S_RAW]\nC --> D[TENANT_METRICS]\nD --> E[BI/Reports]","difficulty":"advanced","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Scale Ai","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T05:53:18.231Z","createdAt":"2026-01-14T22:32:57.077Z"},{"id":"q-2229","question":"You're operating a Snowflake event store table raw.events(device_id STRING, ts TIMESTAMP_NTZ, payload VARIANT). Implement GDPR-like retention: keep 90 days of live data, purge older rows by archiving to raw.events_archive and deleting from live daily via a Snowflake Task, while preserving time travel history and audits. How would you design the architecture, what SQL would you use, and how would you validate?","answer":"Architect a 90-day live retention with archival: create raw.events_archive LIKE raw.events; daily TASK moves rows with ts < dateadd(day,-90,current_timestamp()) to archive and deletes from live; enabl","explanation":"## Why This Is Asked\nTests practical data lifecycle design, governance, and cost trade-offs in Snowflake.\n\n## Key Concepts\n- Time Travel, Fail-Safe, archival workflow, idempotent tasks, cost-aware data movement.\n- Data-archival pipelines and rollback strategies.\n\n## Code Example\n```javascript\n-- Archive old data\nCREATE TABLE raw.events_archive LIKE raw.events;\n\nCREATE OR REPLACE TASK archive_old_events\n  WAREHOUSE = 'COMPUTE_WH'\n  SCHEDULE = '1 DAY'\nAS\n  INSERT INTO raw.events_archive\n  SELECT * FROM raw.events WHERE ts < DATEADD(day, -90, CURRENT_TIMESTAMP());\n\n-- Purge from live\nDELETE FROM raw.events WHERE ts < DATEADD(day, -90, CURRENT_TIMESTAMP());\n```\n\n## Follow-up Questions\n- How would you handle schema changes in the payload VARIANT over time?\n- How would you validate the completeness of archival after a rollback event?","diagram":"flowchart TD\n  A[Ingest] --> B[Live: raw.events]\n  B --> C[Archive: raw.events_archive]\n  B --> D[Daily purge task]\n  B --> E[Time Travel: 1d]\n  C --> F[Audit via archive TZ]","difficulty":"intermediate","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Apple","DoorDash"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T08:45:10.570Z","createdAt":"2026-01-15T08:45:10.570Z"},{"id":"q-2277","question":"In Snowflake, ingest telemetry into RAW.events (device_id STRING, ts TIMESTAMP_NTZ, payload VARIANT) from two sources: real-time stream and nightly batch. You must deliver per-device 15-minute latency metrics for the last hour (avg_reading, cnt). Propose an end-to-end architecture using a Stream, a Materialized View (MV) or incremental aggregation, and a TASK scheduled every 15 minutes to refresh. Include exact SQL to create the stream, MV, task, and a validation query to prove latency targets?","answer":"Adopt a stream-based MV with a 15-minute refresh. Create stream RAW.events_strm on RAW.events; create MV mv_device_hour as SELECT device_id, DATE_TRUNC('hour', ts) AS hh, AVG(payload:reading::FLOAT) A","explanation":"## Why This Is Asked\nTests design of real-time-then-batch pipelines using Snowflake features.\n\n## Key Concepts\n- Streams, Materialized Views, Tasks, VARIANT payloads, per-device windowing, latency validation\n\n## Code Example\n```sql\n-- Create stream\nCREATE OR REPLACE STREAM RAW.events_strm ON TABLE RAW.events;\n\n-- Materialized View\nCREATE MATERIALIZED VIEW mv_device_hour AS\nSELECT device_id,\n       DATE_TRUNC('hour', ts) AS hh,\n       AVG(payload:reading::FLOAT) AS avg_reading,\n       COUNT(*) AS cnt\nFROM RAW.events_strm\nGROUP BY device_id, hh;\n\n-- Task\nCREATE OR REPLACE TASK t_refresh_mv\n  WAREHOUSE = WH\n  SCHEDULE = '15 MINUTE'\nAS\nREFRESH MATERIALIZED VIEW mv_device_hour;\n\n-- Validation\nWITH lag AS (\n  SELECT device_id,\n         DATEDIFF(second, MAX(ts), CURRENT_TIMESTAMP()) AS lag_s\n  FROM RAW.events\n  GROUP BY device_id\n)\nSELECT APPROX_PERCENTILE(lag_s, 0.95) AS p95_lag_sec\nFROM lag;\n```\n\n## Follow-up Questions\n- How would you handle late-arriving data affecting MV freshness?\n- What are cost implications of MV refresh vs. manual reshaping?","diagram":null,"difficulty":"advanced","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Slack","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T10:38:45.778Z","createdAt":"2026-01-15T10:38:45.778Z"},{"id":"q-2303","question":"Scenario: a Snowflake account holds prod_sales.customers with PII. External partners at Oracle, Microsoft, and Nvidia need a sanitized subset for analytics. Propose a beginner-friendly data-share plan: masking policies for PII, a region-limited secure view, and an account-based share. Include commands to implement?","answer":"Share a sanitized dataset via Snowflake data sharing. Create a masking policy for PII, apply it to emails/phones, and expose an EU-only secure view. Add that view to an account-based share granted to ","explanation":"## Why This Is Asked\nInterview context explanation.\n\n## Key Concepts\n- Data sharing basics; account-based vs reader accounts\n- Secure views for row-level access; masking policies\n- Grant hierarchy and testing access\n\n## Code Example\n```sql\n-- mask policy\nCREATE MASKING POLICY pii_mask AS (VAL STRING) RETURNS STRING\n  CASE WHEN CURRENT_ROLE() IN ('EU_ANALYST') THEN 'REDACTED' ELSE VAL END;\n\n-- apply mask\nALTER TABLE prod_sales.customers MODIFY COLUMN email SET MASKING POLICY pii_mask;\n```\n\n## Follow-up Questions\n- How would you audit share usage?\n- How would you revoke access?","diagram":"flowchart TD\n  Internal[Internal Snowflake] --> Share[Data Share eu_sales]\n  Share --> Oracle[Oracle Partner]\n  Share --> Microsoft[Microsoft Partner]\n  Share --> Nvidia[Nvidia Partner]","difficulty":"beginner","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Microsoft","NVIDIA","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T11:31:03.043Z","createdAt":"2026-01-15T11:31:03.043Z"},{"id":"q-2371","question":"You're building a Snowflake-based telemetry store loaded into a shared table raw.events (tenant_id STRING, ts TIMESTAMP_NTZ, device_id STRING, payload VARIANT). Design a production-ready, multi-tenant design ensuring strict data isolation and fast dashboards for last 30 days across hundreds of tenants. Outline RAP rules, masking, secure views or materialized aggregates, and a scalable data-sharing approach. Include concrete SQL patterns and validation steps?","answer":"Use a single shared table raw.events with a TENANT_ID column. Enforce row-level access via a ROW ACCESS POLICY that binds tenant_id to a user context (CURRENT_USER) via a central mapping table. Apply ","explanation":"## Why This Is Asked\n\nTests ability to design a scalable, isolation-first multi-tenant architecture in Snowflake using RAP, masking, and secure views, plus performance strategies.\n\n## Key Concepts\n\n- Row Access Policies and tenant-context binding\n- Secure views and dynamic data masking\n- Time-series clustering and auto-clustering\n- Data-sharing controls and audits\n\n## Code Example\n\n```javascript\n-- Pseudo-SQL illustrating the approach (not real Snowflake syntax)\nALTER TABLE raw.events ADD COLUMN tenant_id STRING;\nCREATE ROW ACCESS POLICY tenant_rap AS\n  (tenant_id STRING) RETURNS BOOLEAN ->\n  tenant_id = get_tenant_for_user(CURRENT_USER());\n\nCREATE VIEW v_last_30d AS\nSELECT * FROM raw.events\nWHERE tenant_id = get_tenant_for_user(CURRENT_USER())\n  AND ts >= CURRENT_DATE() - INTERVAL '30' DAY;\n```\n\n## Follow-up Questions\n\n- How to test for cross-tenant leakage at scale?\n- What monitoring would you add to ensure sustained performance?","diagram":null,"difficulty":"advanced","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","PayPal","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T15:37:21.200Z","createdAt":"2026-01-15T15:37:21.200Z"},{"id":"q-2483","question":"Intermediate Snowflake-core: Ingest 200M rows daily into raw.sessions(user_id STRING, session_id STRING, started_at TIMESTAMP_NTZ, ended_at TIMESTAMP_NTZ, events VARIANT). Need per-user risk scores updated every 5 minutes and dashboards for (a) count of high-risk sessions in last 15 minutes, (b) average duration of high-risk sessions, (c) top 10 user cohorts by risk. Propose a production plan covering data model (raw vs curated), clustering, streaming vs materialized views, schema evolution, validation, and rollback strategy with exact SQL commands?","answer":"Build a curated per-user risk table curate.user_risk updated by a STREAM over raw.sessions and a TASK every 5 minutes. Use a compact SQL expression over the last 5–10 minutes of sessions to compute ri","explanation":"## Why This Is Asked\nTests end-to-end productionizing of streaming + batch in Snowflake, including data model separation, clustering, Streams/Tasks vs MV, schema evolution, validation, and rollback.\n\n## Key Concepts\n- Streams and tasks for near-real-time processing\n- Raw vs curated data separation\n- Composite clustering and pruning\n- Time Travel and rollback\n\n## Code Example\n```sql\n-- example: create stream, task, and insert into curated table (pseudo)\n```\n\n## Follow-up Questions\n- How to handle late-arriving events that modify risk scores?\n- What operational metrics ensure per-user score freshness and data skew detection?","diagram":"flowchart TD\n  A[raw.sessions insert] --> B[streams]\n  B --> C[tasks: compute risk]\n  C --> D[curate.user_risk insert]\n  D --> E[dashboards]","difficulty":"intermediate","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Amazon"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T19:44:46.336Z","createdAt":"2026-01-15T19:44:46.337Z"},{"id":"q-2522","question":"Snowflake-core intermediate: You're ingesting telemetry into Snowflake with raw.events(tenant_id STRING, user_id STRING, ts TIMESTAMP_NTZ, payload VARIANT). In a multi-tenant SaaS, implement per-tenant latency metrics (p95 over last 30 min), dynamic masking of sensitive payload fields for dashboards, and fast per-user drilldown without scanning raw payloads. Propose a production plan: curated schema, partitioning, streaming vs materialized path, VARIANT schema evolution, validation, and rollback with exact commands?","answer":"Implement a curated analytics path with incremental processing: create `curated.events_by_tenant(tenant_id, ts, user_id, latency_ms, payload_masked)` with a STREAM on `raw.events` and a TASK that computes per-tenant p95 latency over rolling 30-minute windows. Apply dynamic masking policies to sensitive payload fields and create materialized views for fast per-user drilldown without scanning raw payloads.","explanation":"## Why This Is Asked\nTests ability to design a production-ready, multi-tenant analytics pipeline with governance, performance optimization, and operational safety in Snowflake.\n\n## Key Concepts\n- Streams and Tasks for incremental ETL processing\n- Per-tenant rolling window metrics (p95 calculations)\n- VARIANT payload masking and secure dashboard views\n- Data governance with dynamic masking policies\n- Performance optimization through materialized views\n- Time Travel and rollback strategies for production safety\n\n## Code Example\n```javascript\n-- Create curated table\nCREATE TABLE curated.events_by_tenant (\n  tenant_id STRING,\n  ts TIMESTAMP_NTZ,\n  user_id STRING,\n  latency_ms NUMBER,\n  payload_masked VARIANT\n);\n\n-- Create stream over raw\nCREATE OR REPLACE STREAM raw_events_stream\n  ON TABLE raw.events;\n\n-- Create task for incremental processing\nCREATE OR REPLACE TASK process_tenant_metrics\n  WAREHOUSE = analytics_wh\n  SCHEDULE = '1 minute'\nAS\nBEGIN\n  INSERT INTO curated.events_by_tenant\n  SELECT \n    tenant_id,\n    ts,\n    user_id,\n    latency_ms,\n    mask_sensitive_fields(payload) as payload_masked\n  FROM raw_events_stream\n  WHERE METADATA$ACTION = 'INSERT';\n  \n  -- Update per-tenant p95 metrics\n  MERGE INTO tenant_latency_metrics t\n  USING (\n    SELECT tenant_id, \n           PERCENTILE_CONT(latency_ms, 0.95) OVER (\n             PARTITION BY tenant_id \n             ORDER BY ts \n             RANGE BETWEEN INTERVAL '30 minutes' PRECEDING AND CURRENT ROW\n           ) as p95_latency\n    FROM curated.events_by_tenant\n  ) s ON t.tenant_id = s.tenant_id\n  WHEN MATCHED THEN UPDATE SET t.p95_latency = s.p95_latency;\nEND;\n```","diagram":null,"difficulty":"intermediate","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Oracle","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T05:46:29.573Z","createdAt":"2026-01-15T21:32:29.212Z"},{"id":"q-2591","question":"You're designing a multi-tenant analytics layer in Snowflake. A single table raw.events stores events for all tenants (tenant_id, ts, payload). BI dashboards must be isolated per tenant and support sub-second queries for the last 7 days. Propose and implement a scalable access-control strategy that avoids per-tenant copies, considering growth to hundreds of tenants. Include how you'd implement Row Access Policies, a Secure View, and validation steps with concrete SQL commands?","answer":"Implement a single shared raw.events table with tenant_id and enforce access through a Row Access Policy bound to session tenant_id, complemented by a Secure View that projects only authorized rows for BI dashboards. Map tenants to Snowflake roles and establish session context to enable dynamic tenant isolation without data duplication.","explanation":"## Why This Is Asked\nAssesses ability to design scalable, auditable tenant isolation using Snowflake features like Row Access Policies and Secure Views while maintaining performance.\n\n## Key Concepts\n- Row Access Policies\n- Secure Views\n- Role mapping vs dynamic tenant mapping\n- Validation: cross-tenant isolation and performance\n\n## Code Example\n\n```sql\n-- Skeleton policy (illustrative)\nCREATE OR REPLACE ROW ACCESS POLICY ra_events AS\n  (tenant_id STRING) RETURNS BOOLEAN ->\n  CASE WHEN CURRENT_ROLE() = 'ADMIN' THEN TRUE\n       ELSE tenant_id = CURRENT_TENANT()\n  END;\n```\n\n## Follow-up Question","diagram":null,"difficulty":"advanced","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Discord","DoorDash"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T05:05:41.403Z","createdAt":"2026-01-15T23:49:04.445Z"},{"id":"q-2592","question":"You operate a multi-tenant Snowflake data lake: table raw.orders(tenant_id STRING, order_id STRING, amount NUMBER, credit_card VARIANT). Implement per-tenant data isolation using row access policies and a masking policy for credit_card so tenants see only last 4 digits. Provide exact SQL to create masking policy, row access policy, a secure view that enforces both policies, and grants. Include how you'd test with two tenants?","answer":"Implement comprehensive per-tenant data isolation using Snowflake's governance features: a masking policy to obscure credit card numbers showing only the last 4 digits, a row access policy to filter data by tenant_id, and a secure view that enforces both policies while granting appropriate privileges to tenant-specific roles.","explanation":"## Why This Is Asked\nThis question tests practical implementation of Snowflake's multi-tenant security model, evaluating understanding of governance features including masking policies, row access policies, and secure views. It assesses ability to implement proper data isolation while maintaining functional access patterns.\n\n## Key Concepts\n- Dynamic data masking using policy-based approach\n- Row-level security through session context validation\n- Secure views for centralized policy enforcement\n- Role-based access control with least privilege principles\n- Cross-tenant isolation and testing methodologies\n\n## Code Example\n```sql\n-- Create masking policy for credit card data\nCREATE MASKING POLICY credit_card_mask AS (val VARIANT) RETURNS VARIANT ->\n  CASE\n    WHEN CURRENT_ROLE() IN ('ACCOUNTADMIN', 'SECURITYADMIN') THEN val\n    ELSE SUBSTR(TO_STRING(val), -4)::VARIANT\n  END;\n\n-- Create row access policy for tenant isolation\nCREATE ROW ACCESS POLICY tenant_isolation AS (tenant_id_col STRING) ->\n  CASE\n    WHEN CURRENT_ROLE() = 'ACCOUNTADMIN' THEN TRUE\n    ELSE tenant_id_col = CURRENT_USER()\n  END;\n\n-- Apply policies to base table\nALTER TABLE raw.orders \n  MODIFY COLUMN credit_card SET MASKING POLICY credit_card_mask;\nALTER TABLE raw.orders \n  ADD ROW ACCESS POLICY tenant_isolation ON (tenant_id);\n\n-- Create secure view with policy enforcement\nCREATE SECURE VIEW orders_secure AS\nSELECT \n  tenant_id,\n  order_id,\n  amount,\n  credit_card\nFROM raw.orders;\n\n-- Create tenant roles and grants\nCREATE ROLE tenant_a_role;\nCREATE ROLE tenant_b_role;\nGRANT SELECT ON orders_secure TO ROLE tenant_a_role;\nGRANT SELECT ON orders_secure TO ROLE tenant_b_role;\n```\n\n## Testing Strategy\n```sql\n-- Test tenant isolation\nUSE ROLE tenant_a_role;\nSET user_tenant = 'tenant_a';\nSELECT COUNT(*) FROM orders_secure; -- Should return only tenant_a rows\n\nUSE ROLE tenant_b_role;\nSET user_tenant = 'tenant_b';\nSELECT COUNT(*) FROM orders_secure; -- Should return only tenant_b rows\n\n-- Verify masking\nSELECT credit_card FROM orders_secure LIMIT 1; -- Should show only last 4 digits\n```","diagram":null,"difficulty":"advanced","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T05:05:23.988Z","createdAt":"2026-01-16T02:21:35.370Z"},{"id":"q-2657","question":"Beginner Snowflake-core: daily load from staged.orders_stage into production.orders, dedup by order_id. Columns: order_id STRING, customer_id STRING, amount NUMBER, load_date DATE. Write a single MERGE UPSERT that keeps only the latest load_date per order_id. Include exact MERGE SQL and a quick verify query?","answer":"MERGE INTO production.orders AS t USING ( SELECT order_id, customer_id, amount, load_date FROM ( SELECT *, ROW_NUMBER() OVER (PARTITION BY order_id ORDER BY load_date DESC) AS rn FROM staging.orders_s","explanation":"## Why This Is Asked\nTests ability to perform a safe upsert with de-dup logic using ROW_NUMBER and a single MERGE, a common beginner data-ops task.\n\n## Key Concepts\n- MERGE UPSERT\n- ROW_NUMBER windowing for de-dup\n- verification queries for data integrity\n\n## Code Example\n```sql\nMERGE INTO production.orders AS t\nUSING (\n  SELECT order_id, customer_id, amount, load_date\n  FROM (\n    SELECT *, ROW_NUMBER() OVER (PARTITION BY order_id ORDER BY load_date DESC) AS rn\n    FROM staging.orders_stage\n  ) s WHERE rn = 1\n) AS s\nON t.order_id = s.order_id\nWHEN MATCHED THEN UPDATE SET customer_id = s.customer_id, amount = s.amount, load_date = s.load_date\nWHEN NOT MATCHED THEN INSERT (order_id, customer_id, amount, load_date) VALUES (s.order_id, s.customer_id, s.amount, s.load_date);\n```\n\n## Follow-up Questions\n- How would you adjust for late-arriving data with different load_date semantics?\n- How would you test this in a non-prod environment?","diagram":null,"difficulty":"beginner","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","Oracle","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T05:42:34.776Z","createdAt":"2026-01-16T05:42:34.776Z"},{"id":"q-2692","question":"You're designing a beginner Snowflake data-curation task: given a raw table RAW.events(device_id string, event_ts timestamp_ntz, payload variant) ingesting 50M rows daily with a JSON payload containing fields event_type, region, and email (PII). Propose a concrete end-to-end approach to surface a curated table ANALYTICS.curated.events_daily with columns device_id, event_type, region, user_email where user_email is masked for external queries via a masking policy. Include exact SQL statements to (1) create the curated table, (2) extract fields, (3) define and apply masking policy on user_email, and (4) create a daily REFRESH TASK at 01:00 UTC that recomputes the curated table from the last 24 hours?","answer":"Create curated table ANALYTICS.curated.events_daily (device_id STRING, event_type STRING, region STRING, user_email STRING); load last 24h from RAW.events extracting payload fields: SELECT device_id, ","explanation":"## Why This Is Asked\nTests practical use of Snowflake’s data-curation primitives: using VARIANT payloads, risk-free exposure for external audiences, and automation with Tasks.\n\n## Key Concepts\n- Extracting fields from VARIANT payloads\n- Masking policies on table columns\n- ALTER TABLE to bind masking policies\n- Snowflake TASKs for scheduled refreshes\n- Lightweight, deterministic daily refresh pattern\n\n## Code Example\n```sql\n-- Create curated table\nCREATE OR REPLACE TABLE ANALYTICS.curated.events_daily (\n  device_id STRING,\n  event_type STRING,\n  region STRING,\n  user_email STRING\n);\n\n-- Load last 24h from RAW.events\nINSERT INTO ANALYTICS.curated.events_daily\nSELECT\n  device_id,\n  payload:event_type::STRING,\n  payload:region::STRING,\n  payload:email::STRING\nFROM RAW.events\nWHERE event_ts >= DATEADD(day,-1,CURRENT_TIMESTAMP());\n\n-- Masking policy\nCREATE OR REPLACE MASKING POLICY analytics.mask_email AS\n(VAL STRING) RETURNS STRING ->\nCASE\n  WHEN current_role() IN ('EXTERNAL_ANALYST') THEN\n    CONCAT(SUBSTR(VAL,1,1), '***', SUBSTR(VAL, -1))\n  ELSE VAL\nEND;\n\nALTER TABLE ANALYTICS.curated.events_daily\n  MODIFY COLUMN user_email SET MASKING POLICY analytics.mask_email;\n\n-- Daily refresh task\nCREATE OR REPLACE TASK ANALYTICS.daily_refresh.refresh_events\n  WAREHOUSE = 'COMPUTE_WH'\n  SCHEDULE = 'CRON 0 1 * * *'\nAS\n  TRUNCATE TABLE ANALYTICS.curated.events_daily;\n  INSERT INTO ANALYTICS.curated.events_daily\n  SELECT\n    device_id,\n    payload:event_type::STRING,\n    payload:region::STRING,\n    payload:email::STRING\n  FROM RAW.events\n  WHERE event_ts >= DATEADD(day,-1,CURRENT_TIMESTAMP());\n```\n\n## Follow-up Questions\n- How would you test masking behavior for different roles?\n- How would you handle schema evolution if payload fields change?","diagram":"flowchart TD\n  A[Raw events] --> B[Extract fields]\n  B --> C[Curated table]\n  C --> D[Masking policy]\n  D --> E[Daily refresh TASK]","difficulty":"beginner","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Discord","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T07:03:15.412Z","createdAt":"2026-01-16T07:03:15.412Z"},{"id":"q-868","question":"Design a cross-account data-sharing solution in Snowflake for a multinational fintech requiring regional affiliates to access a shared dataset containing PII. How would you implement Secure Data Sharing, dynamic data masking, region-specific RBAC, and auditable access, while enabling updates to masking policies without breaking consumer queries?","answer":"Use a provider-to-consumer Secure Data Share, attach region-specific masking policies to PII columns, and grant region roles in each consumer account. Admins with higher roles see full data; regular r","explanation":"## Why This Is Asked\nThis question tests practical data-sharing governance at scale—cross-account sharing, dynamic masking, RBAC, and auditable access, plus policy evolution without breaking consumers.\n\n## Key Concepts\n- Secure Data Sharing architecture across provider and consumer accounts\n- Dynamic Data Masking policies with role-based access\n- Cross-account RBAC mapping and masking policy scoping\n- Auditing using Snowflake ACCOUNT_USAGE and ACCESS_HISTORY\n- Policy versioning and non-breaking updates\n\n## Code Example\n```sql\n-- Masking policy\nCREATE MASKING POLICY pii_mask AS (VAL STRING) RETURNS STRING ->\nCASE WHEN CURRENT_ROLE() IN ('REGION_A_ANALYST','REGION_B_ADMIN') THEN VAL ELSE 'REDACTED' END;\n\n-- Provider shares\nCREATE SHARE fintech_share;\nGRANT USAGE ON DATABASE fintech_db TO SHARE fintech_share;\nGRANT SELECT ON ALL TABLES IN SCHEMA fintech_db.public TO SHARE fintech_share;\n\n-- Consumer startup\nCREATE DATABASE region_a_db FROM SHARE provider.fintech_share;\n```\n\n## Follow-up Questions\n- How would you test policy updates in a greenfield environment?\n- How would you monitor leakage risk and performance impact of masking at scale?","diagram":"flowchart TD\n  A[Provider] --> B[Secure Data Share]\n  B --> C[Consumer Region A]\n  B --> D[Consumer Region B]\n  C --> E[Dynamic Masking Applied]\n  D --> F[Admins See Full Data]","difficulty":"advanced","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T13:48:59.902Z","createdAt":"2026-01-12T13:48:59.902Z"},{"id":"q-981","question":"How would you configure Snowflake so regional analysts can run ad-hoc queries on a shared dataset while ensuring isolation, predictable performance, and cost control? Include concrete settings for: (a) warehouse topology (min/max clusters, auto-suspend/resume), (b) RBAC (roles, grant scopes on databases/schemas/tables), (c) cost governance (resource monitors and credit caps), (d) a sample GRANT script giving REGIONAL_ANALYST access to only SALES and EVENTS schemas, and (e) auditing and reproducibility considerations?","answer":"Configure a dedicated ANALYST_WAREHOUSE as a multi-cluster warehouse with MIN 1, MAX 4, AUTO_SUSPEND 300, AUTO_RESUME ON. Create a REGIONAL_ANALYST role with USAGE on the DB and SELECT on SALES and EV","explanation":"## Why This Is Asked\nTests practical Snowflake fundamentals: per-region access, isolation, cost governance, and auditing in a beginner-friendly way.\n\n## Key Concepts\n- Multi-cluster warehouses and auto-suspend/resume\n- RBAC with granular schema/table permissions\n- Resource monitors and credit caps for cost control\n- Auditing via query tags and reproducibility\n- Safe read-only ad-hoc access to shared data\n\n## Code Example\n```javascript\n-- Grants (SQL flavored in a JS code block due to formatting)\nCREATE ROLE REGIONAL_ANALYST;\nGRANT USAGE ON DATABASE FINTECH_DEMO TO ROLE REGIONAL_ANALYST;\nGRANT USAGE ON SCHEMA FINTECH_DEMO.SALES TO ROLE REGIONAL_ANALYST;\nGRANT SELECT ON ALL TABLES IN SCHEMA FINTECH_DEMO.SALES TO ROLE REGIONAL_ANALYST;\nGRANT USAGE ON SCHEMA FINTECH_DEMO.EVENTS TO ROLE REGIONAL_ANALYST;\nGRANT SELECT ON ALL TABLES IN SCHEMA FINTECH_DEMO.EVENTS TO ROLE REGIONAL_ANALYST;\n```\n\n## Follow-up Questions\n- How would you extend permissions if analysts need aggregated results across both schemas?\n- How would you monitor per-user query latency and credit usage, and alert on anomalies?","diagram":"flowchart TD\n  A[Analyst] --> B[ANALYST_WAREHOUSE]\n  B --> C[SALES, EVENTS schemas]\n  C --> D[READ-ONLY access]\n  E[Resource Monitor] --> B\n  F[Auditing] --> A","difficulty":"beginner","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Cloudflare","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T17:44:36.792Z","createdAt":"2026-01-12T17:44:36.792Z"}],"subChannels":["general"],"companies":["Adobe","Airbnb","Amazon","Anthropic","Apple","Citadel","Cloudflare","Coinbase","Databricks","Discord","DoorDash","Goldman Sachs","Google","Hugging Face","IBM","LinkedIn","Lyft","Microsoft","MongoDB","NVIDIA","Oracle","PayPal","Plaid","Robinhood","Scale Ai","Slack","Snap","Snowflake","Square","Stripe","Tesla","Two Sigma","Zoom"],"stats":{"total":24,"beginner":8,"intermediate":7,"advanced":9,"newThisWeek":24}}