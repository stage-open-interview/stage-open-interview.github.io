{"questions":[{"id":"snowflake-core-account-access-1768170043454-0","question":"You use SSO with Okta and MFA for interactive logins in Snowflake and you need a non-interactive service account for a daily ETL job running on AWS. Which authentication approach is recommended to minimize risk and simplify credential rotation?","answer":"[{\"id\":\"a\",\"text\":\"Create a dedicated Snowflake user with a password and MFA disabled\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use OAuth tokens issued via Okta for the service account\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Create a dedicated user and enable key pair authentication for non-interactive login\",\"isCorrect\":true},{\"id\":\"d\",\"text\":\"Use a temporary, one-time password for each run via a script\",\"isCorrect\":false}]","explanation":"Correct answer: c. Rationale: Key pair authentication enables non-interactive logins suitable for automated ETL jobs and does not depend on interactive MFA flows. A is insecure because MFA is bypassed, and passwords may be exposed. B is viable but adds OAuth setup and token management complexity; for a simple, low-risk automation pattern, key pair is preferred. D is impractical and insecure since it reintroduces credentials management and is not supported by Snowflake as a standard practice for automation.","diagram":null,"difficulty":"intermediate","tags":["Snowflake","Key-Pair-Login","Okta","SAML-2.0","AWS","Terraform-Snowflake-Provider","certification-mcq","domain-weight-20"],"channel":"snowflake-core","subChannel":"account-access","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T22:20:43.455Z","createdAt":"2026-01-11 22:20:43"},{"id":"snowflake-core-account-access-1768170043454-1","question":"Which configuration best ensures admin actions in Snowflake are auditable and require MFA, while still enabling routine admin tasks?","answer":"[{\"id\":\"a\",\"text\":\"Enforce MFA for admin logins via the IdP and enable ACCOUNT_USAGE views (LOGIN_HISTORY, ACCESS_HISTORY) for auditing\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Disable MFA for admins to simplify logins and rely on IP allowlists\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use a single global admin account with full privileges for all tasks\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Disable auditing to avoid performance overhead\",\"isCorrect\":false}]","explanation":"Correct answer: a. Rationale: Enforcing MFA via the IdP ensures strong authentication for admin access, while ACCOUNT_USAGE (LOGIN_HISTORY, ACCESS_HISTORY) provides an auditable trail of admin actions. B weakens security by removing MFA. C concentrates power in one account increasing risk. D hides important activity data and is non-compliant.","diagram":null,"difficulty":"intermediate","tags":["Snowflake","MFA","Okta","SAML-2.0","AWS","Terraform-Snowflake-Provider","certification-mcq","domain-weight-20"],"channel":"snowflake-core","subChannel":"account-access","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T22:20:43.972Z","createdAt":"2026-01-11 22:20:44"},{"id":"snowflake-core-account-access-1768170043454-2","question":"To restrict access to Snowflake from a vendor network and ensure credentials rotate, which configuration provides the strongest, practical security posture?","answer":"[{\"id\":\"a\",\"text\":\"Create a NETWORK POLICY restricting access to the vendor IP and configure a SECURITY INTEGRATION with OAuth so tokens rotate and are short‑lived\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Rely solely on a network policy with IP allowlisting and use a static long‑lived password for the vendor user\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\" Blindly trust the vendor’s network and disable token rotation\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use a password rotation script without network controls\",\"isCorrect\":false}]","explanation":"Correct answer: a. Rationale: A NETWORK POLICY restricts access to a specific IP range, while a SECURITY INTEGRATION with OAuth provides short-lived tokens that rotate, reducing risk if credentials are compromised. B lacks dynamic token security and ignores network restrictions. C is unsafe and non-compliant. D ignores network scope and credential revocation.","diagram":null,"difficulty":"intermediate","tags":["Snowflake","OAuth","Network-Policy","Okta","AWS","Terraform-Snowflake-Provider","certification-mcq","domain-weight-20"],"channel":"snowflake-core","subChannel":"account-access","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T22:20:44.442Z","createdAt":"2026-01-11 22:20:44"},{"id":"snowflake-core-data-loading-1768228424646-0","question":"You're loading CSV files from an S3 stage into a Snowflake table named orders(id INT, amount DECIMAL(10,2), order_date DATE, status STRING). The files include a header row and are compressed as gzip, and you want to skip any files that cause a load error while continuing with others. Which COPY INTO statement correctly loads the data?","answer":"[{\"id\":\"a\",\"text\":\"COPY INTO mydb.public.orders FROM @my_s3_stage/orders/ FILE_FORMAT = (FORMAT_NAME = 'CSV_WITH_HEADER') HEADER = TRUE ON_ERROR = 'CONTINUE';\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"COPY INTO mydb.public.orders FROM @my_s3_stage/orders/ FILE_FORMAT = (TYPE = 'CSV', FIELD_DELIMITER = ',') HEADER = TRUE ON_ERROR = 'SKIP_FILE';\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"COPY INTO mydb.public.orders FROM @my_s3_stage/orders/ FILE_FORMAT = (FORMAT_NAME = 'CSV_WITH_HEADER') ON_ERROR = 'ABORT_STATEMENT';\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"COPY INTO mydb.public.orders FROM @my_s3_stage/orders/ FILE_FORMAT = (FORMAT_NAME = 'CSV_WITH_HEADER') VALIDATE_ONLY = TRUE;\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA loads the data using a defined CSV format that handles headers and continues processing other files on error.\n\n## Why Other Options Are Wrong\n- Option B uses a CSV type format but omits the header handling as defined in the named format, which could misinterpret the header as data.\n- Option C sets ON_ERROR to ABORT_STATEMENT, which stops on the first error instead of continuing.\n- Option D uses VALIDATE_ONLY, which only validates the load and does not perform the actual data load.\n\n## Key Concepts\n- COPY INTO with FORMAT_NAME references a predefined file format; including HEADER handles header rows correctly.\n- ON_ERROR controls whether processing continues after encountering bad files.\n- Validating options (VALIDATE_ONLY) do not load data.\n\n## Real-World Application\n- In production data pipelines, you typically want to keep processing other files even if one file is malformed; this reduces downtime and ensures larger batch loads complete.","diagram":null,"difficulty":"intermediate","tags":["AWS S3","Snowflake","Terraform","Kubernetes","certification-mcq","domain-weight-20"],"channel":"snowflake-core","subChannel":"data-loading","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T14:33:44.647Z","createdAt":"2026-01-12 14:33:45"},{"id":"snowflake-core-data-loading-1768228424646-1","question":"You have a newline-delimited JSON file stored in an external stage and you want to load each JSON object into a single VARIANT column named payload in table events. Which statement correctly performs this load?","answer":"[{\"id\":\"a\",\"text\":\"COPY INTO events(payload) FROM @json_stage/events.json FILE_FORMAT = (TYPE = 'JSON');\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"COPY INTO events FROM @json_stage/events.json FILE_FORMAT = (TYPE = 'JSON') PAYLOAD_COLUMN = 'payload';\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"COPY INTO events(payload) FROM @json_stage/events.json FILE_FORMAT = (TYPE = 'JSON', STRIP_NULL_VALUES = TRUE);\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"COPY INTO events(payload) FROM (SELECT * FROM @json_stage/events.json) FILE_FORMAT = (TYPE = 'JSON');\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA correctly loads JSON objects line-by-line into a VARIANT column named payload using a JSON file format.\n\n## Why Other Options Are Wrong\n- B references a non-existent PAYLOAD_COLUMN option for COPY INTO.\n- C includes STRIP_NULL_VALUES, which is not a standard inline option for this operation.\n- D attempts to load from a subquery of a stage path, which is not how COPY INTO reads external stage files.\n\n## Key Concepts\n- JSON file format type maps to VARIANT payloads in Snowflake.\n- COPY INTO can target specific columns when using inline file format options.\n\n## Real-World Application\n- This pattern is common when ingesting semi-structured JSON logs where each line is a separate JSON object and you want to store raw JSON per row for later parsing.","diagram":null,"difficulty":"intermediate","tags":["AWS S3","Snowflake","Terraform","Kubernetes","certification-mcq","domain-weight-20"],"channel":"snowflake-core","subChannel":"data-loading","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T14:33:45.126Z","createdAt":"2026-01-12 14:33:45"},{"id":"snowflake-core-data-loading-1768228424646-2","question":"For real-time ingestion of many small JSON files arriving in S3, which Snowflake pattern is most appropriate?","answer":"[{\"id\":\"a\",\"text\":\"Use periodic COPY INTO runs scheduled to run as new files arrive.\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Set up Snowpipe with S3 event notifications to auto-ingest new files as they arrive.\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Accumulate files locally and load them in a single batch at the end of the day.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use external tables with manual refresh to ingest the data.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nB uses Snowpipe with S3 event notifications to automatically ingest new small files as they arrive, which is ideal for real-time-like ingestion.\n\n## Why Other Options Are Wrong\n- A relies on manual or scheduled loads and cannot guarantee real-time ingestion of each new file.\n- C delays ingestion, reducing timeliness and increasing latency.\n- D external tables are useful for querying external data sources, not for automatic ingestion workflows.\n\n## Key Concepts\n- Snowpipe auto-ingest.\n- Cloud storage event notifications trigger loads.\n\n## Real-World Application\n- Enables near real-time analytics on streaming-like JSON payloads from IoT devices or app logs.","diagram":null,"difficulty":"intermediate","tags":["AWS S3","Snowflake","Terraform","Kubernetes","certification-mcq","domain-weight-20"],"channel":"snowflake-core","subChannel":"data-loading","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T14:33:45.611Z","createdAt":"2026-01-12 14:33:45"},{"id":"snowflake-core-data-loading-1768228424646-3","question":"You want to map incoming CSV data to an existing table's columns by name rather than by position, and you expect differences in case and surrounding whitespace in the header. Which COPY INTO option enables this behavior?","answer":"[{\"id\":\"a\",\"text\":\"MATCH_BY_COLUMN_NAME = FALSE\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"MATCH_BY_COLUMN_NAME = TRUE\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"IGNORE_HEADER = TRUE\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"UNORDERED_COLUMN_MAPPINGS = TRUE\",\"isCorrect\":false}]","explanation":"## Correct Answer\nB enables Snowflake to map CSV columns by name rather than position, which handles case and whitespace differences.\n\n## Why Other Options Are Wrong\n- A disables name-based matching and reverts to positional mapping.\n- C IGNORE_HEADER is not the same as column-name matching; it only controls header handling.\n- D UNORDERED_COLUMN_MAPPINGS is not a valid COPY INTO option.\n\n## Key Concepts\n- MATCH_BY_COLUMN_NAME true enables name-based mapping during COPY INTO.\n\n## Real-World Application\n- Useful when consuming CSVs from upstream systems that vary in header case or spacing but share column semantics.","diagram":null,"difficulty":"intermediate","tags":["AWS S3","Snowflake","Terraform","Kubernetes","certification-mcq","domain-weight-20"],"channel":"snowflake-core","subChannel":"data-loading","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T14:33:45.773Z","createdAt":"2026-01-12 14:33:45"},{"id":"q-868","question":"Design a cross-account data-sharing solution in Snowflake for a multinational fintech requiring regional affiliates to access a shared dataset containing PII. How would you implement Secure Data Sharing, dynamic data masking, region-specific RBAC, and auditable access, while enabling updates to masking policies without breaking consumer queries?","answer":"Use a provider-to-consumer Secure Data Share, attach region-specific masking policies to PII columns, and grant region roles in each consumer account. Admins with higher roles see full data; regular r","explanation":"## Why This Is Asked\nThis question tests practical data-sharing governance at scale—cross-account sharing, dynamic masking, RBAC, and auditable access, plus policy evolution without breaking consumers.\n\n## Key Concepts\n- Secure Data Sharing architecture across provider and consumer accounts\n- Dynamic Data Masking policies with role-based access\n- Cross-account RBAC mapping and masking policy scoping\n- Auditing using Snowflake ACCOUNT_USAGE and ACCESS_HISTORY\n- Policy versioning and non-breaking updates\n\n## Code Example\n```sql\n-- Masking policy\nCREATE MASKING POLICY pii_mask AS (VAL STRING) RETURNS STRING ->\nCASE WHEN CURRENT_ROLE() IN ('REGION_A_ANALYST','REGION_B_ADMIN') THEN VAL ELSE 'REDACTED' END;\n\n-- Provider shares\nCREATE SHARE fintech_share;\nGRANT USAGE ON DATABASE fintech_db TO SHARE fintech_share;\nGRANT SELECT ON ALL TABLES IN SCHEMA fintech_db.public TO SHARE fintech_share;\n\n-- Consumer startup\nCREATE DATABASE region_a_db FROM SHARE provider.fintech_share;\n```\n\n## Follow-up Questions\n- How would you test policy updates in a greenfield environment?\n- How would you monitor leakage risk and performance impact of masking at scale?","diagram":"flowchart TD\n  A[Provider] --> B[Secure Data Share]\n  B --> C[Consumer Region A]\n  B --> D[Consumer Region B]\n  C --> E[Dynamic Masking Applied]\n  D --> F[Admins See Full Data]","difficulty":"advanced","tags":["snowflake-core"],"channel":"snowflake-core","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:48:59.902Z","createdAt":"2026-01-12T13:48:59.902Z"},{"id":"snowflake-core-performance-optimization-1768210296923-0","question":"A large Snowflake table is loaded daily and used by many concurrent analytic queries filtered by a date column. You want to improve performance with minimal ongoing maintenance. Which approach provides the best balance of performance and maintenance cost?","answer":"[{\"id\":\"a\",\"text\":\"Enable Automatic Clustering on the table\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Create a clustering key on (order_date)\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Create a materialized view for the most common query patterns\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Increase the warehouse size permanently and disable auto-suspend\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA — Enable Automatic Clustering on the table; Snowflake automatically maintains clustering for large tables, improving pruning for date-based predicates with far less ongoing maintenance.\n\n## Why Other Options Are Wrong\n- B: Creating a clustering key on a single column is a manual approach and requires ongoing tuning and maintenance, increasing administrative overhead.\n- C: Materialized views can speed specific queries but add maintenance cost and may not cover all date-based patterns; not as low-maintenance as automatic clustering for broad date-range queries.\n- D: Simply increasing warehouse size improves runtime but does not address data organization and increases cost without reducing maintenance.\n\n## Key Concepts\n- Automatic Clustering\n- Large-table performance\n\n## Real-World Application\n- In environments with frequent date-range scans on huge datasets, turning on automatic clustering reduces admin effort while delivering more predictable performance during peak loads.","diagram":null,"difficulty":"intermediate","tags":["Snowflake","Performance","AutomaticClustering","AWS","Terraform","Kubernetes","certification-mcq","domain-weight-20"],"channel":"snowflake-core","subChannel":"performance-optimization","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T09:31:36.925Z","createdAt":"2026-01-12 09:31:37"},{"id":"snowflake-core-performance-optimization-1768210296923-1","question":"A nightly ETL pipeline produces a flattened wide fact table consumed by dashboards. A commonly run query aggregates weekly sales by product and scans large volumes of data. Which approach delivers the best balance of speed and maintenance?","answer":"[{\"id\":\"a\",\"text\":\"Create a materialized view that pre-aggregates weekly sales by product\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Create a clustering key on (product_id, week)\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use a larger warehouse to reduce runtime\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Maintain a separate denormalized summary table via an ETL job\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA — Create a materialized view that pre-aggregates weekly sales by product; MV precomputes results and accelerates common aggregations with low ongoing maintenance.\n\n## Why Other Options Are Wrong\n- B: Clustering on product_id, week can help some queries but requires ongoing maintenance and may not provide as consistent a win as a pre-aggregated MV for weekly totals.\n- C: A larger warehouse speeds up a single run but increases cost and doesn’t scale as cleanly for repeated workloads and multiple dashboards.\n- D: A manually maintained denormalized summary table is effective but requires additional ETL maintenance and scheduling; MV generally offers lower ongoing maintenance.\n\n## Key Concepts\n- Materialized views\n- Pre-aggregation\n\n## Real-World Application\n- For recurring weekly dashboards, a pre-aggregated MV dramatically reduces compute time and simplifies maintenance compared to ad-hoc clustering or per-query tuning.","diagram":null,"difficulty":"intermediate","tags":["Snowflake","Performance","MaterializedViews","AWS","Terraform","Kubernetes","certification-mcq","domain-weight-20"],"channel":"snowflake-core","subChannel":"performance-optimization","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T09:31:37.466Z","createdAt":"2026-01-12 09:31:37"},{"id":"snowflake-core-performance-optimization-1768210296923-2","question":"During month-end reporting, there is high concurrency on a single Snowflake warehouse. Which approach prevents contention and maintains predictable latency without paying for idle capacity?","answer":"[{\"id\":\"a\",\"text\":\"Configure a multi-cluster warehouse with concurrency scaling and auto-suspend\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Increase the single cluster size\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Create separate warehouses for each report group and route queries accordingly\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Disable result caching and rely on on-demand compute\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA — Configure a multi-cluster warehouse with concurrency scaling and auto-suspend; this setup provides automatic scaling for concurrency spikes without paying for idle capacity.\n\n## Why Other Options Are Wrong\n- B: Simply increasing a single cluster can reduce latency for some queries but does not address high concurrency across many users and can lead to wasted capacity.\n- C: Splitting workloads across multiple warehouses can help, but it adds operational complexity and still may not optimally share resources during spikes.\n- D: Disabling caching eliminates a key performance mechanism Snowflake uses to accelerate repeated queries, worsening latency under concurrency.\n\n## Key Concepts\n- Concurrency scaling\n- Multi-cluster warehouses\n\n## Real-World Application\n- Month-end reports typically spike concurrency; a multi-cluster warehouse with concurrency scaling ensures stable response times without provisioning excess idle capacity.","diagram":null,"difficulty":"intermediate","tags":["Snowflake","Performance","ConcurrencyScaling","AWS","Terraform","Kubernetes","certification-mcq","domain-weight-20"],"channel":"snowflake-core","subChannel":"performance-optimization","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T09:31:37.974Z","createdAt":"2026-01-12 09:31:38"}],"subChannels":["account-access","data-loading","general","performance-optimization"],"companies":["Coinbase","Microsoft"],"stats":{"total":11,"beginner":0,"intermediate":10,"advanced":1,"newThisWeek":11}}