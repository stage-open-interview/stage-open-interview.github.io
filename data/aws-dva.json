{"questions":[{"id":"aws-dva-development-1768181316350-0","question":"An application processes messages from an SQS standard queue via an AWS Lambda consumer. The backlog fluctuates and some messages fail processing occasionally. Which design best ensures at-least-once processing, avoids poison pill issues, and minimizes latency?","answer":"[{\"id\":\"a\",\"text\":\"Attach a Dead-Letter Queue to the SQS queue with a maximum receive count and ensure the Lambda function is idempotent.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Move to an SQS FIFO queue to guarantee order and use a single worker.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Disable DLQ and implement fixed retry logic in Lambda with no backoff.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Switch to an SNS topic fanout with Lambda subscribers.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A: Attach a Dead-Letter Queue to the SQS queue with a maximum receive count and ensure the Lambda function is idempotent. This combination provides a safety net for failed messages and prevents repeated harmful retries, while idempotence ensures duplicates don't impact results.\n\n## Why Other Options Are Wrong\n\n- Option B: FIFO queues preserve order but limit throughput and do not inherently solve poison-pill handling or at-least-once semantics for sporadic failures.\n- Option C: Disabling the DLQ removes a critical safety mechanism for failed messages and does not provide controlled retry handling.\n- Option D: SNS fanout changes the delivery model and does not provide per-message dead-lettering or reliable retry semantics for SQS-backed consumers.\n\n## Key Concepts\n- SQS Dead-Letter Queue\n- At-least-once delivery model\n- Idempotent processing in Lambda\n- Backoff and retry handling\n\n## Real-World Application\n- Implement DLQs to quarantine problematic messages and reprocess after fixes; design consumers to be idempotent to tolerate retries without duplication.","diagram":null,"difficulty":"intermediate","tags":["AWS","SQS","Lambda","Dead-Letter-Queue","Idempotency","certification-mcq","domain-weight-32"],"channel":"aws-dva","subChannel":"development","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T01:28:36.352Z","createdAt":"2026-01-12 01:28:36"},{"id":"aws-dva-development-1768181316350-1","question":"In an EKS cluster running on AWS, you want pods in a namespace to access DynamoDB without embedding credentials and with least privilege. Which approach best achieves this?","answer":"[{\"id\":\"a\",\"text\":\"Create an IAM role for service accounts (IRSA) and annotate the Kubernetes service account used by the pods with the role ARN.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Store AWS credentials in a Kubernetes secret and mount it as environment variables in pods.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Include DynamoDB credentials in the container image and rotate them via CI/CD.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Expose DynamoDB publicly and use an IAM user in the code.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A: Use IRSA by associating an IAM role with the Kubernetes service account used by the pods, granting only DynamoDB permissions. This avoids secret management in Kubernetes and follows least-privilege.\n\n## Why Other Options Are Wrong\n\n- Option B: Storing credentials in Kubernetes Secrets is less secure and requires ongoing secret management; it also risks exposure if Secrets are compromised.\n- Option C: Embedding credentials in the container image is insecure and violates best practices for secret management and rotation.\n- Option D: Publicly exposing DynamoDB undermines least-privilege and security; using IAM users embedded in code is hard to rotate and distribute securely.\n\n## Key Concepts\n- IRSA (IAM Roles for Service Accounts)\n- Least-privilege IAM policies for DynamoDB\n- Kubernetes ServiceAccounts and AWS IAM integration\n\n## Real-World Application\n- Secure pod access to AWS services without secret management; enables automatic credential rotation and strict access control.","diagram":null,"difficulty":"intermediate","tags":["AWS","EKS","IRSA","IAM","DynamoDB","Kubernetes","certification-mcq","domain-weight-32"],"channel":"aws-dva","subChannel":"development","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T01:28:36.807Z","createdAt":"2026-01-12 01:28:37"},{"id":"aws-dva-development-1768181316350-2","question":"For a REST API backed by API Gateway and Lambda with sporadic traffic, which pattern best achieves low latency during bursts while controlling cost?","answer":"[{\"id\":\"a\",\"text\":\"Enable Lambda provisioned concurrency for the API handler and optionally enable API Gateway caching.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Move to an EC2-based API behind a Load Balancer to guarantee performance.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Increase the Lambda memory size to reduce cold starts without provisioning concurrency.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use Lambda destinations to route failures to SQS for retry handling.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A: Enable Lambda provisioned concurrency for the API handler to keep a set number of instances warm, ensuring low latency during bursts while API Gateway caching can further reduce backend load. This directly addresses latency with a measurable cost control mechanism.\n\n## Why Other Options Are Wrong\n\n- Option B: EC2-based solutions add scaling and maintenance overhead and may increase cost for sporadic traffic.\n- Option C: Increasing memory can reduce cold starts but does not guarantee low latency during bursts due to potential scaling limits.\n- Option D: Routing failures to SQS handles errors, not latency optimization for normal traffic.\n\n## Key Concepts\n- Lambda provisioned concurrency\n- API Gateway integration\n- Cost vs latency optimization\n\n## Real-World Application\n- Use provisioned concurrency for predictable latency in API backends with variable traffic; monitor and adjust to balance performance and cost.","diagram":null,"difficulty":"intermediate","tags":["AWS","API-Gateway","Lambda","ProvisionedConcurreny","Serverless","certification-mcq","domain-weight-32"],"channel":"aws-dva","subChannel":"development","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T01:28:37.270Z","createdAt":"2026-01-12 01:28:37"},{"id":"q-674","question":"You're building an AWS DVA data-analytics pipeline ingesting telemetry from 2000 devices/sec via Kinesis Data Streams. A Spark/Glue path processes windows and writes Parquet to S3; aggregation state in DynamoDB. Data loss or duplicates occur during retries and outages; costs spike at peak. Design a resilient, cost-efficient approach: shard sizing, processing path, idempotent writes, error handling with DLQ, and observability. What would you implement and why?","answer":"Scale by configuring 4 Kinesis shards to sustain ~4000 RPS; use a real-time analytics path (Kinesis Data Analytics or Spark) for windowed aggregation, then write Parquet to S3 via Glue streaming. Dedu","explanation":"## Why This Is Asked\n\nAssesses real-world ability to design a resilient streaming analytics pipeline on AWS, balancing throughput, cost, and correctness. Candidates must justify shard sizing, choice of processing path (KDA vs Glue streaming), idempotent writes, error handling, and observability.\n\n## Key Concepts\n\n- Streaming ingestion and scaling\n- Exactly-once semantics with DynamoDB\n- DLQ and backpressure\n- Observability and cost control\n\n## Code Example\n\n```javascript\n// Pseudocode: deduplicate by composite key and conditional writes\n```\n\n## Follow-up Questions\n\n- How would you migrate this to a multi-region setup?\n- How would you test failure modes and simulate burst traffic?","diagram":null,"difficulty":"intermediate","tags":["aws-dva"],"channel":"aws-dva","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Microsoft","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T14:45:55.246Z","createdAt":"2026-01-11T14:45:55.246Z"},{"id":"aws-dva-security-1768213569922-0","question":"You are deploying a new application that stores sensitive data in an S3 bucket used by a single service role. To enforce least privilege, you want to ensure encryption at rest with a customer-managed CMK, block public access, and restrict PutObject to only the service role. Which configuration best achieves this?","answer":"[{\"id\":\"a\",\"text\":\"Enable SSE-KMS with a CMK on the bucket, attach a bucket policy that allows PutObject only from arn:aws:iam::123456789012:role/my-app-role, enable Block Public Access settings on the bucket, and disable ACLs.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Enable SSE-S3 for the bucket, attach a bucket policy that allows PutObject only from arn:aws:iam::123456789012:role/my-app-role, and enable Block Public Access settings.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Enable SSE-KMS with a CMK, attach a bucket policy that allows PutObject from any role in the account, and enable public read access.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Do not set a bucket policy; rely on IAM permissions to PutObject, and leave Block Public Access off.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA. The correct answer is A because it directly enforces encryption with a CMK, restricts PutObject to the specific role via bucket policy, and blocks public access via Block Public Access settings.\n\n## Why Other Options Are Wrong\n- Option B: It uses SSE-S3 and does not enforce CMK-based encryption, which is required per the scenario, and still allows potential misconfig if policy isn't strict.\n\n- Option C: Although CMK is used, it allows PutObject from any role, failing the least-privilege requirement, and lacks public access controls.\n\n- Option D: Relies on IAM permissions alone and omits bucket-level controls and encryption, increasing risk of public exposure.\n\n## Key Concepts\n- SSE-KMS CMK encryption\n- Bucket policies for resource-based access control\n- Block Public Access settings\n- ACLs vs bucket policies\n\n## Real-World Application\n- In production, apply a CMK and restrict access to a specific service role; disable ACLs and enforce public access block to prevent accidental public exposure.","diagram":null,"difficulty":"intermediate","tags":["AWS IAM","AWS S3","AWS KMS","Encryption","certification-mcq","domain-weight-26"],"channel":"aws-dva","subChannel":"security","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T10:26:09.924Z","createdAt":"2026-01-12 10:26:10"},{"id":"aws-dva-security-1768213569922-1","question":"In an AWS Organization with multiple accounts, a security team detects unusual API activity in a member account. What is the most scalable approach to detect, investigate, and respond to such threats across all accounts?","answer":"[{\"id\":\"a\",\"text\":\"Enable GuardDuty in the security account only, and route alerts to CloudWatch.\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Enable GuardDuty in all member accounts and configure centralized findings in the security account; enable a multi-account CloudTrail trail and Security Hub integration.\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Use IAM Access Analyzer to generate access reports for each account.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Enable CloudWatch Alarms on CloudTrail logs to trigger notifications.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nB. The correct answer is B because enabling GuardDuty in all accounts with centralized findings and a multi-account CloudTrail trail combined with Security Hub provides scalable, centralized threat detection and faster incident response.\n\n## Why Other Options Are Wrong\n- Option A: Only enabling in the security account misses detections in member accounts and is not scalable.\n\n- Option C: IAM Access Analyzer helps with permission analysis but does not provide ongoing threat detection across all accounts.\n\n- Option D: CloudWatch Alarms on CloudTrail are reactive and less comprehensive for broad threat detection compared with GuardDuty + Security Hub.\n\n## Key Concepts\n- GuardDuty multi-account deployment\n- Centralized findings in Security Hub\n- Multi-account CloudTrail\n- AWS Organizations\n\n## Real-World Application\n- For large organizations, deploy GuardDuty per account and centralize findings to enable rapid, coordinated response across the fleet.","diagram":null,"difficulty":"intermediate","tags":["AWS GuardDuty","AWS CloudTrail","AWS Security Hub","AWS Organizations","certification-mcq","domain-weight-26"],"channel":"aws-dva","subChannel":"security","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T10:26:10.439Z","createdAt":"2026-01-12 10:26:10"},{"id":"aws-dva-security-1768213569922-2","question":"To enforce configuration compliance across S3 buckets in a multi-account environment, which approach provides continuous, scalable evaluation and automatic remediation for public access misconfigurations?","answer":"[{\"id\":\"a\",\"text\":\"Enable AWS Config managed rules s3-bucket-public-read-prohibited and s3-bucket-public-write-prohibited across all accounts; configure AWS Config Remediation to automatically apply a non-public bucket policy.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Create an IAM policy that denies s3:PutObjectPublic across all buckets.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use AWS Security Hub to monitor buckets for public access.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use S3 Block Public Access settings at the account level only.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA. The correct answer is A because AWS Config managed rules provide continuous evaluation of bucket public access configuration across accounts, and with remediation configured, misconfigurations can be automatically corrected.\n\n## Why Other Options Are Wrong\n- Option B: A denial policy without continuous evaluation cannot fix existing misconfigurations or detect them promptly.\n\n- Option C: Security Hub aggregates findings but does not perform continuous configuration evaluation or automatic remediation.\n\n- Option D: Block Public Access helps guardrails but does not provide continuous evaluation or automated remediation across multiple accounts.\n\n## Key Concepts\n- AWS Config managed rules\n- S3 public access configuration\n- AWS Config Remediation\n- Cross-account governance\n\n## Real-World Application\n- Implement Config rules to continuously enforce non-public buckets and automatically remediate when non-compliant resources are detected.","diagram":null,"difficulty":"intermediate","tags":["AWS Config","AWS S3","IAM","certification-mcq","domain-weight-26"],"channel":"aws-dva","subChannel":"security","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T10:26:10.979Z","createdAt":"2026-01-12 10:26:11"}],"subChannels":["development","general","security"],"companies":["Microsoft","OpenAI"],"stats":{"total":7,"beginner":0,"intermediate":7,"advanced":0,"newThisWeek":7}}