{"questions":[{"id":"q-674","question":"You're building an AWS DVA data-analytics pipeline ingesting telemetry from 2000 devices/sec via Kinesis Data Streams. A Spark/Glue path processes windows and writes Parquet to S3; aggregation state in DynamoDB. Data loss or duplicates occur during retries and outages; costs spike at peak. Design a resilient, cost-efficient approach: shard sizing, processing path, idempotent writes, error handling with DLQ, and observability. What would you implement and why?","answer":"Scale by configuring 4 Kinesis shards to sustain ~4000 RPS; use a real-time analytics path (Kinesis Data Analytics or Spark) for windowed aggregation, then write Parquet to S3 via Glue streaming. Dedu","explanation":"## Why This Is Asked\n\nAssesses real-world ability to design a resilient streaming analytics pipeline on AWS, balancing throughput, cost, and correctness. Candidates must justify shard sizing, choice of processing path (KDA vs Glue streaming), idempotent writes, error handling, and observability.\n\n## Key Concepts\n\n- Streaming ingestion and scaling\n- Exactly-once semantics with DynamoDB\n- DLQ and backpressure\n- Observability and cost control\n\n## Code Example\n\n```javascript\n// Pseudocode: deduplicate by composite key and conditional writes\n```\n\n## Follow-up Questions\n\n- How would you migrate this to a multi-region setup?\n- How would you test failure modes and simulate burst traffic?","diagram":null,"difficulty":"intermediate","tags":["aws-dva"],"channel":"aws-dva","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Microsoft","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T14:45:55.246Z","createdAt":"2026-01-11T14:45:55.246Z"}],"subChannels":["general"],"companies":["Microsoft","OpenAI"],"stats":{"total":1,"beginner":0,"intermediate":1,"advanced":0,"newThisWeek":1}}