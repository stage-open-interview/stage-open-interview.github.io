{"questions":[{"id":"aws-dva-deployment-1768228144730-0","question":"You are deploying a containerized app on ECS Fargate behind an Application Load Balancer and want zero-downtime updates with automatic rollback if the new version fails?","answer":"[{\"id\":\"a\",\"text\":\"Rolling updates of the ECS service with deployment circuit breaker enabled\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Blue/Green deployment with AWS CodeDeploy for ECS\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Canary deployment with traffic shifting via ALB\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"All-at-once deployment with immediate traffic switch\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct answer is B. A Blue/Green deployment with AWS CodeDeploy for ECS provides separate environments, traffic shifting, and built-in rollback handling, which supports zero-downtime updates when issues arise in the new task set.\n\n## Why Other Options Are Wrong\n- A: Rolling updates with a circuit breaker can rollback, but it does not inherently guarantee zero-downtime traffic cutover and may still involve risk during the switch.\n- C: Canary deployments can progressively route a small portion of traffic but do not guarantee complete zero-downtime delivery and require additional routing logic.\n- D: All-at-once deployments switch all traffic at once, causing downtime if the new version has issues.\n\n## Key Concepts\n- ECS Blue/Green with CodeDeploy\n- Traffic shifting and rollback capabilities\n- Zero-downtime deployments in ECS/Fargate\n\n## Real-World Application\n- Use Blue/Green with CodeDeploy when deploying critical services where a quick rollback and clean traffic cutover are required, such as customer-facing APIs.\n","diagram":null,"difficulty":"intermediate","tags":["AWS","ECS","CodeDeploy","ALB","Fargate","CI/CD","certification-mcq","domain-weight-24"],"channel":"aws-dva","subChannel":"deployment","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T14:29:04.732Z","createdAt":"2026-01-12 14:29:05"},{"id":"aws-dva-deployment-1768228144730-1","question":"During a blue/green ECS deployment, which AWS resource defines the traffic-shift settings (e.g., canary vs all-at-once) and monitors health to promote or rollback the new task set?","answer":"[{\"id\":\"a\",\"text\":\"AWS CodeDeploy DeploymentGroup\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"AWS CloudFormation DeploymentConfig\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"AWS ECS Service deploymentConfiguration\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"AWS Application Load Balancer listener rule\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA. AWS CodeDeploy DeploymentGroup defines the deployment configuration for ECS blue/green deployments, including traffic-shift settings (e.g., TimeBasedCanary, AllAtOnce) and health-based promotion/rollback.\n\n## Why Other Options Are Wrong\n- B: CloudFormation DeploymentConfig is not used to control traffic-shift during ECS blue/green deployments.\n- C: ECS Service deploymentConfiguration controls rolling updates but not the CodeDeploy traffic-shift strategy.\n- D: ALB listener rules route traffic but do not define deployment traffic-shift strategies or rollback behavior.\n\n## Key Concepts\n- CodeDeploy deployment groups\n- Traffic routing configurations\n- ECS blue/green deployment workflow\n\n## Real-World Application\n- Use CodeDeploy DeploymentGroup to fine-tune traffic shifting during blue/green releases and to enforce controlled promotion or rollback in production.\n","diagram":null,"difficulty":"intermediate","tags":["AWS","CodeDeploy","ECS","BlueGreen","CI/CD","certification-mcq","domain-weight-24"],"channel":"aws-dva","subChannel":"deployment","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T14:29:05.247Z","createdAt":"2026-01-12 14:29:05"},{"id":"aws-dva-deployment-1768228144730-2","question":"To minimize blast radius in a CI/CD pipeline that deploys to ECS and ECR, which IAM pattern should you implement?","answer":"[{\"id\":\"a\",\"text\":\"Attach AdministratorAccess to the CI role\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Create a dedicated deployment role with least privilege and allow the CI to assume it\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Use the root account for deployment\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use a separate IAM user per service and rotate credentials\",\"isCorrect\":false}]","explanation":"## Correct Answer\nB. Use a dedicated deployment role with least privilege and allow the CI system to assume it (via STS AssumeRole). This confines permissions to whatâ€™s necessary for ECS/ECR deployment, reducing blast radius.\n\n## Why Other Options Are Wrong\n- A: Admin access grants excessive permissions, increasing risk.\n- C: Root credentials are dangerous and not allowed for automation.\n- D: Separate IAM users per service is cumbersome and still risks broad permissions if not carefully scoped; role assumption is cleaner and auditable.\n\n## Key Concepts\n- Least privilege IAM design\n- STS AssumeRole for CI/CD pipelines\n- Scoped permissions for ECR and ECS operations\n\n## Real-World Application\n- Implement a deployment role with specific actions (e.g., ecr:GetAuthorizationToken, ecr:BatchCheckLayerAvailability, ecr:PutImage, ecs:RegisterTaskDefinition, ecs:UpdateService) and require the CI to assume it.\n","diagram":null,"difficulty":"intermediate","tags":["AWS","IAM","ECS","ECR","Security","certification-mcq","domain-weight-24"],"channel":"aws-dva","subChannel":"deployment","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T14:29:05.734Z","createdAt":"2026-01-12 14:29:05"},{"id":"aws-dva-deployment-1768228144730-3","question":"Which mechanism ensures automatic rollback of a CloudFormation stack update if the update fails and rollback is not disabled?","answer":"[{\"id\":\"a\",\"text\":\"DisableRollback set to true\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Rollback on failure by default\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Manual rollback after failure\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use Terraform instead of CloudFormation\",\"isCorrect\":false}]","explanation":"## Correct Answer\nB. CloudFormation automatically rolls back stack updates when a update fails, unless Rollback is explicitly disabled (DisableRollback). The default behavior is to perform a rollback to the previous stable state.\n\n## Why Other Options Are Wrong\n- A: DisableRollback prevents automatic rollback.\n- C: Manual rollback is not automatic.\n- D: Terraform vs CloudFormation is unrelated to the automatic rollback behavior of CFN.\n\n## Key Concepts\n- CloudFormation rollback behavior\n- DisableRollback option\n- Deployment safety in IaC\n\n## Real-World Application\n- Rely on CloudFormation's built-in rollback to maintain prod stability during template changes; only override with explicit deliberate steps.\n","diagram":null,"difficulty":"intermediate","tags":["AWS","CloudFormation","IaC","DeploymentSafety","certification-mcq","domain-weight-24"],"channel":"aws-dva","subChannel":"deployment","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T14:29:05.896Z","createdAt":"2026-01-12 14:29:05"},{"id":"aws-dva-deployment-1768228144730-4","question":"When deploying to a Kubernetes cluster on Amazon EKS, which deployment strategy enables progressive traffic shifting and automated rollback between revisions?","answer":"[{\"id\":\"a\",\"text\":\"All-at-once update of the Deployment\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"RollingUpdate with maxUnavailable=0\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Canary deployment with Istio or Argo Rollouts\",\"isCorrect\":true},{\"id\":\"d\",\"text\":\"Blue/Green using external DNS switch\",\"isCorrect\":false}]","explanation":"## Correct Answer\nC. Canary deployments using tools like Istio or Argo Rollouts enable progressive traffic shifting and automated rollback between revisions in an EKS cluster, aligning with safe, incremental delivery practices.\n\n## Why Other Options Are Wrong\n- A: All-at-once updates do not provide progressive traffic shifting or automated rollback.\n- B: RollingUpdate with maxUnavailable=0 is a Kubernetes update strategy but does not inherently provide automated rollback between revisions without additional tooling.\n- D: Blue/Green with external DNS can work but is more complex to set up in Kubernetes without additional CD tooling; Canary is typically the targeted approach for progressive delivery in Kubernetes.\n\n## Key Concepts\n- Canary deployments in Kubernetes (Istio/Argo Rollouts)\n- Progressive delivery and automated rollback\n- EKS integration with CD tooling\n\n## Real-World Application\n- Use canary deployments to gradually shift traffic to a new revision, monitor metrics, and automatically rollback on failure in production-grade EKS workloads.\n","diagram":null,"difficulty":"intermediate","tags":["AWS","EKS","Kubernetes","Istio","ArgoRollouts","CD","certification-mcq","domain-weight-24"],"channel":"aws-dva","subChannel":"deployment","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T14:29:06.057Z","createdAt":"2026-01-12 14:29:06"},{"id":"aws-dva-development-1768181316350-0","question":"An application processes messages from an SQS standard queue via an AWS Lambda consumer. The backlog fluctuates and some messages fail processing occasionally. Which design best ensures at-least-once processing, avoids poison pill issues, and minimizes latency?","answer":"[{\"id\":\"a\",\"text\":\"Attach a Dead-Letter Queue to the SQS queue with a maximum receive count and ensure the Lambda function is idempotent.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Move to an SQS FIFO queue to guarantee order and use a single worker.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Disable DLQ and implement fixed retry logic in Lambda with no backoff.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Switch to an SNS topic fanout with Lambda subscribers.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A: Attach a Dead-Letter Queue to the SQS queue with a maximum receive count and ensure the Lambda function is idempotent. This combination provides a safety net for failed messages and prevents repeated harmful retries, while idempotence ensures duplicates don't impact results.\n\n## Why Other Options Are Wrong\n\n- Option B: FIFO queues preserve order but limit throughput and do not inherently solve poison-pill handling or at-least-once semantics for sporadic failures.\n- Option C: Disabling the DLQ removes a critical safety mechanism for failed messages and does not provide controlled retry handling.\n- Option D: SNS fanout changes the delivery model and does not provide per-message dead-lettering or reliable retry semantics for SQS-backed consumers.\n\n## Key Concepts\n- SQS Dead-Letter Queue\n- At-least-once delivery model\n- Idempotent processing in Lambda\n- Backoff and retry handling\n\n## Real-World Application\n- Implement DLQs to quarantine problematic messages and reprocess after fixes; design consumers to be idempotent to tolerate retries without duplication.","diagram":null,"difficulty":"intermediate","tags":["AWS","SQS","Lambda","Dead-Letter-Queue","Idempotency","certification-mcq","domain-weight-32"],"channel":"aws-dva","subChannel":"development","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T01:28:36.352Z","createdAt":"2026-01-12 01:28:36"},{"id":"aws-dva-development-1768181316350-1","question":"In an EKS cluster running on AWS, you want pods in a namespace to access DynamoDB without embedding credentials and with least privilege. Which approach best achieves this?","answer":"[{\"id\":\"a\",\"text\":\"Create an IAM role for service accounts (IRSA) and annotate the Kubernetes service account used by the pods with the role ARN.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Store AWS credentials in a Kubernetes secret and mount it as environment variables in pods.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Include DynamoDB credentials in the container image and rotate them via CI/CD.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Expose DynamoDB publicly and use an IAM user in the code.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A: Use IRSA by associating an IAM role with the Kubernetes service account used by the pods, granting only DynamoDB permissions. This avoids secret management in Kubernetes and follows least-privilege.\n\n## Why Other Options Are Wrong\n\n- Option B: Storing credentials in Kubernetes Secrets is less secure and requires ongoing secret management; it also risks exposure if Secrets are compromised.\n- Option C: Embedding credentials in the container image is insecure and violates best practices for secret management and rotation.\n- Option D: Publicly exposing DynamoDB undermines least-privilege and security; using IAM users embedded in code is hard to rotate and distribute securely.\n\n## Key Concepts\n- IRSA (IAM Roles for Service Accounts)\n- Least-privilege IAM policies for DynamoDB\n- Kubernetes ServiceAccounts and AWS IAM integration\n\n## Real-World Application\n- Secure pod access to AWS services without secret management; enables automatic credential rotation and strict access control.","diagram":null,"difficulty":"intermediate","tags":["AWS","EKS","IRSA","IAM","DynamoDB","Kubernetes","certification-mcq","domain-weight-32"],"channel":"aws-dva","subChannel":"development","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T01:28:36.807Z","createdAt":"2026-01-12 01:28:37"},{"id":"aws-dva-development-1768181316350-2","question":"For a REST API backed by API Gateway and Lambda with sporadic traffic, which pattern best achieves low latency during bursts while controlling cost?","answer":"[{\"id\":\"a\",\"text\":\"Enable Lambda provisioned concurrency for the API handler and optionally enable API Gateway caching.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Move to an EC2-based API behind a Load Balancer to guarantee performance.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Increase the Lambda memory size to reduce cold starts without provisioning concurrency.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use Lambda destinations to route failures to SQS for retry handling.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A: Enable Lambda provisioned concurrency for the API handler to keep a set number of instances warm, ensuring low latency during bursts while API Gateway caching can further reduce backend load. This directly addresses latency with a measurable cost control mechanism.\n\n## Why Other Options Are Wrong\n\n- Option B: EC2-based solutions add scaling and maintenance overhead and may increase cost for sporadic traffic.\n- Option C: Increasing memory can reduce cold starts but does not guarantee low latency during bursts due to potential scaling limits.\n- Option D: Routing failures to SQS handles errors, not latency optimization for normal traffic.\n\n## Key Concepts\n- Lambda provisioned concurrency\n- API Gateway integration\n- Cost vs latency optimization\n\n## Real-World Application\n- Use provisioned concurrency for predictable latency in API backends with variable traffic; monitor and adjust to balance performance and cost.","diagram":null,"difficulty":"intermediate","tags":["AWS","API-Gateway","Lambda","ProvisionedConcurreny","Serverless","certification-mcq","domain-weight-32"],"channel":"aws-dva","subChannel":"development","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T01:28:37.270Z","createdAt":"2026-01-12 01:28:37"},{"id":"q-674","question":"You're building an AWS DVA data-analytics pipeline ingesting telemetry from 2000 devices/sec via Kinesis Data Streams. A Spark/Glue path processes windows and writes Parquet to S3; aggregation state in DynamoDB. Data loss or duplicates occur during retries and outages; costs spike at peak. Design a resilient, cost-efficient approach: shard sizing, processing path, idempotent writes, error handling with DLQ, and observability. What would you implement and why?","answer":"Scale by configuring 4 Kinesis shards to sustain ~4000 RPS; use a real-time analytics path (Kinesis Data Analytics or Spark) for windowed aggregation, then write Parquet to S3 via Glue streaming. Dedu","explanation":"## Why This Is Asked\n\nAssesses real-world ability to design a resilient streaming analytics pipeline on AWS, balancing throughput, cost, and correctness. Candidates must justify shard sizing, choice of processing path (KDA vs Glue streaming), idempotent writes, error handling, and observability.\n\n## Key Concepts\n\n- Streaming ingestion and scaling\n- Exactly-once semantics with DynamoDB\n- DLQ and backpressure\n- Observability and cost control\n\n## Code Example\n\n```javascript\n// Pseudocode: deduplicate by composite key and conditional writes\n```\n\n## Follow-up Questions\n\n- How would you migrate this to a multi-region setup?\n- How would you test failure modes and simulate burst traffic?","diagram":null,"difficulty":"intermediate","tags":["aws-dva"],"channel":"aws-dva","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Microsoft","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T14:45:55.246Z","createdAt":"2026-01-11T14:45:55.246Z"},{"id":"q-916","question":"In an AWS-based DVA pipeline ingesting telemetry from 2000 devices/sec via Kinesis Data Streams, with a Spark/Glue path writing Parquet to S3 and cataloged in Glue, design an end-to-end strategy for robust schema evolution, data validation, and partitioning that minimizes reprocessing and supports backfills. Include schema registry, idempotence, DLQ, and observability?","answer":"Adopt Glue Schema Registry with versioned Avro schemas; Spark Structured Streaming validates records against the latest compatible schema and writes Parquet to S3 partitioned by device/hour. Route inc","explanation":"## Why This Is Asked\nThe question probes practical UX for evolving data schemas in streaming pipelines, ensuring data quality and low reprocessing costs during changes.\n\n## Key Concepts\n- Glue Schema Registry and versioning\n- Schema compatibility strategies (backward/forward)\n- Spark Structured Streaming validation and checkpointing\n- Idempotent writes and deduplication stores\n- DLQ routing and backfill procedures\n- Observability with CloudWatch and Glue metrics\n\n## Code Example\n```javascript\n// Spark reads latest compatible Avro schema from Glue, writes Parquet partitioned by device/hour\ndf.write\n  .partitionBy(\"device_id\", \"hour\")\n  .format(\"parquet\").save(\"s3://bucket/data/\");\n```\n\n## Follow-up Questions\n- How would you handle backward-incompatible schema changes without downtime?\n- What metrics signal schema drift or data quality issues?","diagram":null,"difficulty":"intermediate","tags":["aws-dva"],"channel":"aws-dva","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","Snowflake","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T15:29:41.744Z","createdAt":"2026-01-12T15:29:41.744Z"},{"id":"aws-dva-security-1768213569922-0","question":"You are deploying a new application that stores sensitive data in an S3 bucket used by a single service role. To enforce least privilege, you want to ensure encryption at rest with a customer-managed CMK, block public access, and restrict PutObject to only the service role. Which configuration best achieves this?","answer":"[{\"id\":\"a\",\"text\":\"Enable SSE-KMS with a CMK on the bucket, attach a bucket policy that allows PutObject only from arn:aws:iam::123456789012:role/my-app-role, enable Block Public Access settings on the bucket, and disable ACLs.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Enable SSE-S3 for the bucket, attach a bucket policy that allows PutObject only from arn:aws:iam::123456789012:role/my-app-role, and enable Block Public Access settings.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Enable SSE-KMS with a CMK, attach a bucket policy that allows PutObject from any role in the account, and enable public read access.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Do not set a bucket policy; rely on IAM permissions to PutObject, and leave Block Public Access off.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA. The correct answer is A because it directly enforces encryption with a CMK, restricts PutObject to the specific role via bucket policy, and blocks public access via Block Public Access settings.\n\n## Why Other Options Are Wrong\n- Option B: It uses SSE-S3 and does not enforce CMK-based encryption, which is required per the scenario, and still allows potential misconfig if policy isn't strict.\n\n- Option C: Although CMK is used, it allows PutObject from any role, failing the least-privilege requirement, and lacks public access controls.\n\n- Option D: Relies on IAM permissions alone and omits bucket-level controls and encryption, increasing risk of public exposure.\n\n## Key Concepts\n- SSE-KMS CMK encryption\n- Bucket policies for resource-based access control\n- Block Public Access settings\n- ACLs vs bucket policies\n\n## Real-World Application\n- In production, apply a CMK and restrict access to a specific service role; disable ACLs and enforce public access block to prevent accidental public exposure.","diagram":null,"difficulty":"intermediate","tags":["AWS IAM","AWS S3","AWS KMS","Encryption","certification-mcq","domain-weight-26"],"channel":"aws-dva","subChannel":"security","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T10:26:09.924Z","createdAt":"2026-01-12 10:26:10"},{"id":"aws-dva-security-1768213569922-1","question":"In an AWS Organization with multiple accounts, a security team detects unusual API activity in a member account. What is the most scalable approach to detect, investigate, and respond to such threats across all accounts?","answer":"[{\"id\":\"a\",\"text\":\"Enable GuardDuty in the security account only, and route alerts to CloudWatch.\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Enable GuardDuty in all member accounts and configure centralized findings in the security account; enable a multi-account CloudTrail trail and Security Hub integration.\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Use IAM Access Analyzer to generate access reports for each account.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Enable CloudWatch Alarms on CloudTrail logs to trigger notifications.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nB. The correct answer is B because enabling GuardDuty in all accounts with centralized findings and a multi-account CloudTrail trail combined with Security Hub provides scalable, centralized threat detection and faster incident response.\n\n## Why Other Options Are Wrong\n- Option A: Only enabling in the security account misses detections in member accounts and is not scalable.\n\n- Option C: IAM Access Analyzer helps with permission analysis but does not provide ongoing threat detection across all accounts.\n\n- Option D: CloudWatch Alarms on CloudTrail are reactive and less comprehensive for broad threat detection compared with GuardDuty + Security Hub.\n\n## Key Concepts\n- GuardDuty multi-account deployment\n- Centralized findings in Security Hub\n- Multi-account CloudTrail\n- AWS Organizations\n\n## Real-World Application\n- For large organizations, deploy GuardDuty per account and centralize findings to enable rapid, coordinated response across the fleet.","diagram":null,"difficulty":"intermediate","tags":["AWS GuardDuty","AWS CloudTrail","AWS Security Hub","AWS Organizations","certification-mcq","domain-weight-26"],"channel":"aws-dva","subChannel":"security","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T10:26:10.439Z","createdAt":"2026-01-12 10:26:10"},{"id":"aws-dva-security-1768213569922-2","question":"To enforce configuration compliance across S3 buckets in a multi-account environment, which approach provides continuous, scalable evaluation and automatic remediation for public access misconfigurations?","answer":"[{\"id\":\"a\",\"text\":\"Enable AWS Config managed rules s3-bucket-public-read-prohibited and s3-bucket-public-write-prohibited across all accounts; configure AWS Config Remediation to automatically apply a non-public bucket policy.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Create an IAM policy that denies s3:PutObjectPublic across all buckets.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use AWS Security Hub to monitor buckets for public access.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use S3 Block Public Access settings at the account level only.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA. The correct answer is A because AWS Config managed rules provide continuous evaluation of bucket public access configuration across accounts, and with remediation configured, misconfigurations can be automatically corrected.\n\n## Why Other Options Are Wrong\n- Option B: A denial policy without continuous evaluation cannot fix existing misconfigurations or detect them promptly.\n\n- Option C: Security Hub aggregates findings but does not perform continuous configuration evaluation or automatic remediation.\n\n- Option D: Block Public Access helps guardrails but does not provide continuous evaluation or automated remediation across multiple accounts.\n\n## Key Concepts\n- AWS Config managed rules\n- S3 public access configuration\n- AWS Config Remediation\n- Cross-account governance\n\n## Real-World Application\n- Implement Config rules to continuously enforce non-public buckets and automatically remediate when non-compliant resources are detected.","diagram":null,"difficulty":"intermediate","tags":["AWS Config","AWS S3","IAM","certification-mcq","domain-weight-26"],"channel":"aws-dva","subChannel":"security","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T10:26:10.979Z","createdAt":"2026-01-12 10:26:11"}],"subChannels":["deployment","development","general","security"],"companies":["Microsoft","MongoDB","OpenAI","Snowflake","Tesla"],"stats":{"total":13,"beginner":0,"intermediate":13,"advanced":0,"newThisWeek":13}}