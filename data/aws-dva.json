{"questions":[{"id":"q-1025","question":"In a multi-account AWS DVA data platform, streaming IoT telemetry into Kinesis Data Firehose feeding an S3 data lake with Glue catalog. Data schemas evolve and backfills are needed without full reprocessing. Design an end-to-end approach for schema evolution, idempotent writes, and backfill using Glue Schema Registry, Iceberg on S3, and partition pruning. Include data validation, DLQ, and observability?","answer":"Use Glue Schema Registry for versioned schemas, write to Iceberg tables on S3 to support schema evolution and upserts with a deterministic key, implement idempotent writes via a normalized primary key","explanation":"## Why This Is Asked\nTests ability to design scalable, cross-account data pipelines with evolving schemas.\n\n## Key Concepts\n- Glue Schema Registry versioned schemas\n- Iceberg on S3 for upserts and schema evolution\n- Deterministic keys for idempotent writes\n- Partition-driven backfill without full reprocessing\n- DLQ and data quality observability\n\n## Code Example\n```python\n# Pseudocode: write with Iceberg upsert using key\ndef upsert(record):\n  key = record['device_id']\n  iceberg.upsert(table='lake.telemetry', key=key, record=record)\n```\n\n## Follow-up Questions\n- How do you partition backfills to minimize shards?\n- How would you enforce cross-account access controls for Lake Formation?\n","diagram":"flowchart TD\n  Ingest[Kinesis Data Streams] --> Validate[Glue Schema Registry]\n  Validate --> Catalog[(Glue Catalog/Iceberg)]\n  Catalog --> Backfill[Incremental Backfill by Partition]\n  Backfill --> Observability[DLQ & CloudWatch Logs]\n","difficulty":"advanced","tags":["aws-dva"],"channel":"aws-dva","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Instacart","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T19:37:01.899Z","createdAt":"2026-01-12T19:37:01.899Z"},{"id":"q-1268","question":"Design an AWS data lake pattern for multi-tenant analytics where each tenant's data sits under /tenants/{tenantId} in S3 and is exposed to Athena and QuickSight. How would you implement strict tenant isolation, least-privilege access, and automated policy-driven discovery and auditing using Lake Formation, IAM, CMKs, and SCPs? Include governance, testing, and performance considerations?","answer":"Store tenant data in S3 under /tenants/{tenantId} and enforce isolation with Lake Formation grants tied to per-tenant tables. Use per-tenant IAM roles mapped to a central admin for governance, apply S","explanation":"## Why This Is Asked\nTests practical mastery of multi-tenant data governance, isolation, and scalable access control in a real analytics stack. It also probes knowledge of Lake Formation, IAM role mapping, encryption strategy, and cross-account governance.\n\n## Key Concepts\n- Lake Formation granular permissions on per-tenant resources\n- Prefix-based data isolation in S3\n- IAM role mapping per tenant and centralized admin\n- SCPs for cross-account restriction\n- Per-tenant KMS CMKs and Lake Formation audit logs\n\n## Code Example\n\n```yaml\nResources:\n  TenantReadsGrant:\n    Type: AWS::LakeFormation::Grant\n    Properties:\n      DataLakePrincipal:\n        DataLakePrincipalIdentifier: arn:aws:iam::ACCOUNT:role/TenantReader\n      Permissions: [SELECT]\n      Resource:\n        TableResource:\n          DatabaseName: tenant_db\n          TableName: events\n```\n\n## Follow-up Questions\n- How would you test data isolation across tenants? \n- How would you onboard new tenants at scale while preserving least privilege?","diagram":"flowchart TD\n  A[TenantId] --> B[S3: /tenants/{TenantId}/data]\n  B --> C[Lake Formation grants on per-tenant tables]\n  C --> D[Athena/QuickSight]\n  D --> E[Audit logs]","difficulty":"advanced","tags":["aws-dva"],"channel":"aws-dva","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Oracle","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T07:33:46.616Z","createdAt":"2026-01-13T07:33:46.616Z"},{"id":"q-1381","question":"A serverless data ingestion pipeline: an S3 PUT triggers a Lambda that transforms JSON logs into CSV and writes to a separate bucket; failed records go to a DLQ. Explain how you implement idempotent writes, choose between DLQ mechanisms (Lambda DLQ vs SQS), and set up minimal monitoring/alerts to catch processing failures?","answer":"Use a stable composite key from source object key and a per-record id, store seen IDs in DynamoDB, and upsert only when not seen to avoid duplicates. Route failures to an SQS DLQ for decoupled retries","explanation":"## Why This Is Asked\nTests practical handling of retries, deduplication, and observability in a common serverless ELT setup. It prompts decision between DLQ patterns and concrete idempotency strategy.\n\n## Key Concepts\n- Idempotent writes with deterministic IDs\n- Dead-letter queues: Lambda DLQ vs SQS DLQ\n- Observability: metrics, alarms, and alerting\n- Minimal tooling: DynamoDB for seen IDs, CloudWatch for alerts\n\n## Code Example\n```javascript\n// Lambda pseudo-code (Node.js)\nconst AWS = require('aws-sdk');\nconst ddb = new AWS.DynamoDB.DocumentClient();\nexports.handler = async (event) => {\n  for (const rec of event.Records) {\n    const body = JSON.parse(rec.body); // or rec.kinesis, etc.\n    const id = `${rec.s3.object.key}:${body.recordId}`;\n    // Idempotency: check or upsert\n    const exists = await ddb.get({TableName:'SeenIds', Key:{id}}).promise();\n    if (exists.Item) continue;\n    await ddb.put({TableName:'SeenIds', Item:{id}}).promise();\n    // transform and write to target bucket\n    // ... write CSV to target bucket\n  }\n  return {status: 'done'};\n};\n```\n\n## Follow-up Questions\n- How would you adapt for out-of-order arrivals?\n- What changes if the per-record id is not available in the source payload?","diagram":"flowchart TD\n  S3(S3 Put Event) --> L(Lambda)\n  L --> D{DLQ}\n  D --> S(Success Bucket)\n  L --> M(Metrics/Alerts)","difficulty":"beginner","tags":["aws-dva"],"channel":"aws-dva","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T14:44:34.734Z","createdAt":"2026-01-13T14:44:34.734Z"},{"id":"q-1436","question":"In a beginner AWS DVA workflow, JSON logs arrive to S3 at s3://data-logs/raw/. Propose a minimal pipeline where a Lambda validates each record against a JSON schema, writes valid records as Parquet to s3://data-logs/processed/YYYY/MM/DD/, and routes invalid ones to a DLQ. Explain idempotent writes, choose between Lambda DLQ vs SQS, and basic monitoring setup?","answer":"Use an S3-triggered Lambda to validate JSONs against a JSON schema. On success, write a Parquet file to s3://data-logs/processed/YYYY/MM/DD/ with a deterministic key (hash of content) for idempotency.","explanation":"## Why This Is Asked\nTests a practical, beginner-friendly approach to a common DVA pattern: simple validation, durable writes, and observability.\n\n## Key Concepts\n- JSON Schema validation\n- Idempotent writes via deterministic object keys\n- DLQ choices: Lambda DLQ vs SQS\n- Observability: CloudWatch metrics and alarms\n\n## Code Example\n```javascript\n// Example Lambda handler sketch validating JSON and routing\n```\n\n## Follow-up Questions\n- How would schema evolution be managed without breaking retrofits?\n- What adjustments for higher ingest rates or partition pruning would you consider?\n","diagram":null,"difficulty":"beginner","tags":["aws-dva"],"channel":"aws-dva","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","Meta","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T16:57:20.985Z","createdAt":"2026-01-13T16:57:20.985Z"},{"id":"q-1549","question":"Design a secure, scalable cross-account analytics pattern for a multi-tenant data lake. Data for tenants live at /tenants/{tenantId}/ in S3 and must be queryable via Athena/QuickSight with strict isolation. Explain how Lake Formation, per-tenant LF permissions, and cross-account IAM roles control access from a central analytics account. Include encryption (CMKs), cross-account RAM/trust, schema evolution handling, and a testing plan for isolation and governance?","answer":"Implement a Lake Formation-based architecture with per-tenant databases and tables mapped to /tenants/{tenantId}/ S3 locations, accessible through tenant-specific IAM roles assumed from a central analytics account. Enforce strict isolation using Lake Formation fine-grained permissions, cross-account trust relationships, and tenant-specific customer-managed keys.","explanation":"## Why This Is Asked\nThis question evaluates expertise in designing multi-tenant data governance with strict isolation, cross-account analytics, and comprehensive security controls using Lake Formation, CMKs, and Resource Access Manager.\n\n## Key Concepts\n- Lake Formation fine-grained access control and permissions\n- Cross-account IAM roles with trust relationships\n- Customer-managed keys and S3 encryption strategies\n- Schema evolution handling and governance policies\n- Resource Access Manager for cross-account resource sharing\n\n## Code Example\n```javascript\n// Example: IAM trust policy snippet (pseudocode)\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\"AWS\": \"arn:aws:iam::CENTRAL:role/AnalyticsRole\"},\n      \"Action\": \"sts:AssumeRole\",\n      \"Condition\": {\n        \"StringEquals\": {\n          \"aws:PrincipalTag/TenantId\": \"${tenantId}\"\n        }\n      }\n    }\n  ]\n}\n```","diagram":"flowchart TD\nA[Tenant Account] -->|Assumes| B[Analytics Role in Central Account]\nB --> C[Lake Formation Permissions]\nC --> D[Athena/QuickSight Access to /tenants/{tenantId}/]\nD --> E[Audit via LF + CloudWatch]","difficulty":"advanced","tags":["aws-dva"],"channel":"aws-dva","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","Meta","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T05:02:17.418Z","createdAt":"2026-01-13T21:38:07.194Z"},{"id":"q-1575","question":"In a multi-tenant data platform on AWS, tenants stream JSON events into a single Kinesis Data Stream; you aggregate into per-tenant Parquet files in S3 using Firehose. Design an end-to-end pipeline with idempotence, schema validation, and auditing. Include DLQ handling and isolation via Lake Formation. What are your concrete steps?","answer":"Design an end-to-end multi-tenant pipeline: a single Kinesis Data Stream ingests JSON; a Lambda deduplicates by tenant_id + event_id and writes per-tenant Parquet to S3 with daily partitions; register","explanation":"## Why This Is Asked\nThis question probes practical experience with multi-tenant data pipelines, schema governance, and robust error handling in AWS.\n\n## Key Concepts\n- Kinesis Data Streams, Firehose, S3 parquet partitions\n- Lambda dedup by composite key, idempotent writes\n- Glue Schema Registry, Lake Formation for isolation\n- DLQ strategy and observability\n\n## Code Example\n```python\nimport boto3\nimport time\n\n# Pseudo-idempotent dedup check using a DynamoDB table\ndef is_duplicate(tenant_id, event_id):\n    table = boto3.resource('dynamodb').Table('tenant_event_dedup')\n    try:\n        table.put_item(\n            Item={'tenant_id': tenant_id, 'event_id': event_id, 'ts': int(time.time())},\n            ConditionExpression='attribute_not_exists(tenant_id) AND attribute_not_exists(event_id)'\n        )\n        return False\n    except Exception:\n        return True\n```\n\n## Follow-up Questions\n- How would you validate schema evolution across partitions?\n- How would you monitor cross-tenant access and audit logs?","diagram":"flowchart TD\n  A[Kinesis Stream] --> B[Lambda Dedup & Transform]\n  B --> C[S3 per-tenant Parquet partitions]\n  C --> D[Glue Catalog & Lake Formation]\n  D --> E[Athena/Quicksight/Glue Analytics]","difficulty":"intermediate","tags":["aws-dva"],"channel":"aws-dva","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","Lyft","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T22:42:59.619Z","createdAt":"2026-01-13T22:42:59.619Z"},{"id":"q-1617","question":"Design a per-tenant data lake access system on AWS where billions of Parquet files in S3 are consumed by Athena/Glue; describe how you would implement tenant isolation, masking, and row-level security using Lake Formation, and how you would validate auditing and performance under burst workloads?","answer":"Implement per-tenant isolation using Lake Formation LF-tags with policy-based grants, expose only non-sensitive columns through masked views, and enforce row-level security filters in Athena queries. Leverage CloudTrail, Lake Formation logs, and CloudWatch for comprehensive auditing, and validate performance through load testing utilizing Athena's query concurrency capabilities and S3 request metrics during burst workloads.","explanation":"## Why This Is Asked\nMulti-tenant data lakes require fine-grained access control, data masking, and robust auditing; this evaluates policy design and operational validation skills.\n\n## Key Concepts\n- Lake Formation LF-tags and permissions for tenant isolation\n- Data masking and restricted views in Athena/Glue\n- Auditing via CloudTrail, Lake Formation logs, and CloudWatch metrics\n- Performance validation under burst workloads\n\n## Code Example\n```sql\n-- Example: masked view for a tenant\nCREATE VIEW tenant_view AS\nSELECT tenant_id,\n       CASE WHEN is_sensitive(col1) THEN 'REDACTED' ELSE col1 END\n```","diagram":"flowchart TD\n  Ingest[Ingest to S3 Parquet] --> Catalog[Glue Catalog & LF Policies]\n  Catalog --> Query[Athena/Redshift Spectrum]\n  Access[Role-based LF perms] --> Query\n  Mask[Masking Views] --> Query\n  Audit[Audit Trails] --> Ingest","difficulty":"intermediate","tags":["aws-dva"],"channel":"aws-dva","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","IBM","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T05:28:49.686Z","createdAt":"2026-01-14T02:43:19.048Z"},{"id":"q-1637","question":"In a beginner AWS DVA ingestion pipeline, a CSV file uploads to s3://data/tenant-{tenantId}/uploads/YYYY/MM/DD/file.csv triggers a Lambda that validates the header has exactly [timestamp, tenant_id, metric], converts to Parquet, and writes to s3://data/tenant-{tenantId}/processed/YYYY/MM/DD/file.parquet; design for idempotent writes (no duplicates), choose a DLQ strategy, and add minimal CloudWatch alarms for failures. How would you implement this end-to-end, and why?","answer":"Trigger via S3 to Lambda; validate header exactly as [timestamp, tenant_id, metric], error otherwise. Convert CSV to Parquet and write to a partitioned path tenantId/date/file.parquet. Ensure idempote","explanation":"## Why This Is Asked\nTests end-to-end data ingestion basics: event triggers, schema validation, format conversion, partitioned storage, idempotency, DLQ choices, and basic observability.\n\n## Key Concepts\n- S3 event triggers and Lambda orchestration\n- CSV header validation against a strict schema\n- CSV to Parquet transformation and partitioned S3 storage\n- Idempotent writes using a dedupe key (e.g., digest) and a dedupe store\n- DLQ choice between Lambda DLQ vs SQS\n- Minimal CloudWatch alarms for errors and throttling\n\n## Code Example\n```javascript\n// Lambda handler sketch\nconst expected = ['timestamp','tenant_id','metric'];\nexports.handler = async (event) => {\n  // parse S3 event, read CSV, validate headers, convert to Parquet, write partitioned path\n  // implement dedupe via DynamoDB using a composite key (tenant/date/fileDigest)\n};\n```\n\n## Follow-up Questions\n- How would you scale for bursty uploads?\n- How would you test idempotency and DLQ behavior locally?","diagram":"flowchart TD\n  S3[Upload] --> Lambda[Trigger Lambda]\n  Lambda --> Validate[Validate header]\n  Validate --> Transform[CSV to Parquet]\n  Transform --> Write[Write to partitioned path]\n  Lambda --> DLQ[DLQ for errors]\n  DLQ --> Alarms[CloudWatch Alarms]","difficulty":"beginner","tags":["aws-dva"],"channel":"aws-dva","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","LinkedIn","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T04:22:55.624Z","createdAt":"2026-01-14T04:22:55.624Z"},{"id":"q-1706","question":"Design a beginner-friendly, end-to-end data quality check for a daily Parquet dataset stored in S3: s3://telemetry/processed/YYYY/MM/DD/. The data is produced by a Glue job from JSON input. Propose a minimal workflow (using Lambda, Glue, or Athena) that validates schema conformance, computes a day-over-day row-count delta, and emits a quality score JSON to s3://telemetry/quality-reports/YYYY/MM/DD/. Include how you would trigger, idempotency, and basic monitoring?","answer":"Leverage a daily Glue ETL to validate schema and output Parquet, plus an Athena-based delta check to compare day-over-day row counts. Emit a quality score JSON to s3://telemetry/quality-reports/YYYY/M","explanation":"## Why This Is Asked\nTests practical ability to design a simple quality pipeline with AWS primitives, focusing on schema validation, drift detection, and observability.\n\n## Key Concepts\n- Glue ETL and Crawlers\n- Parquet partitioning\n- Athena for ad-hoc checks\n- Idempotent checkpoint (DynamoDB)\n- CloudWatch alerts and dashboards\n\n## Code Example\n```javascript\n// Lightweight Lambda skeleton for quality report write\nexports.handler = async () => {\n  // read latest processed parquet manifests from S3\n  // fetch previous day count from DynamoDB\n  // compute delta and schema conformance\n  // write quality JSON to quality-reports path\n};\n```\n\n## Follow-up Questions\n- How would you handle late-arriving data?\n- How would you scale checks for larger datasets?\n","diagram":"flowchart TD\n  A[Ingest JSON] --> B[Glue ETL → Parquet]\n  B --> C[Parquet in s3://telemetry/processed/YYYY/MM/DD]\n  C --> D[Athena checks: schema + row counts]\n  D --> E[Quality report in s3://telemetry/quality-reports/YYYY/MM/DD]\n  E --> F[CloudWatch alerts/ dashboards]","difficulty":"beginner","tags":["aws-dva"],"channel":"aws-dva","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Discord","DoorDash"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T07:41:59.302Z","createdAt":"2026-01-14T07:41:59.303Z"},{"id":"q-1772","question":"In a multi-account AWS data lake, ingested data lands in s3://lake/raw from several tenants via Kinesis Firehose into a shared account. Propose an end-to-end DVA pipeline that enforces per-tenant isolation, uses Lake Formation for access control, handles schema evolution with Parquet, and provides tenant-aware lineage and auditing. Include how you test isolation and how you monitor for cross-tenant data leakage?","answer":"Adopt per-tenant IAM roles and Lake Formation permissions mapped to a tenant tag in the data catalog; route raw data into partitioned Parquet under /tenants/{tenant}/year=YYYY/month=MM/day=DD; Glue jo","explanation":"## Why This Is Asked\n\nTests multi-account data-lake governance, tenant isolation, and lineage in a realistic AWS setup.\n\n## Key Concepts\n\n- Lake Formation data permissions per-tenant\n- Cross-account IAM roles and resource-based policies\n- Parquet with partition pruning and schema evolution\n- Data lineage, auditing, and governance\n\n## Code Example\n\n```javascript\n// Placeholder: high-level policy example\n```\n\n## Follow-up Questions\n\n- How would you validate no cross-tenant data leakage in production? \n- How would you swap tenants without downtime during schema changes?\n","diagram":"flowchart TD\n  A[Ingest Stream] --> B[S3 lake/raw]\n  B --> C[Lake Formation grants by tenant]\n  C --> D[Glue Data Catalog /tenants/{tenant}]\n  D --> E[Analytics / Athena / Quicksight]","difficulty":"advanced","tags":["aws-dva"],"channel":"aws-dva","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Google","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T10:35:30.591Z","createdAt":"2026-01-14T10:35:30.591Z"},{"id":"q-1801","question":"Ingest JSON events from multiple payment rails into a single streaming layer, partitioned by exchange, and store per-exchange Parquet data in an Iceberg-backed table on S3. Describe the end-to-end design focusing on idempotent writes, late-arriving data, schema evolution, and auditability with time travel. Include concrete steps and trade-offs between Iceberg vs Glue Catalog?","answer":"Key ideas: use MERGE INTO Iceberg with event_id as PK for upserts; route late data with a bounded lateness window and a retry path; leverage Iceberg schema evolution with defaults for new cols; ensure","explanation":"## Why This Is Asked\n\nThis question probes streaming design with lakehouse patterns, idempotence, late data handling, schema evolution, and auditability—crucial for fintech data platforms at scale.\n\n## Key Concepts\n\n- Iceberg upserts and MERGE INTO\n- Late-arrival data handling with watermarks and bounded windows\n- Schema evolution with backward/forward compatibility\n- Auditability via immutable data and changelog tables\n- Catalog backend choices: Iceberg with Glue vs alternatives\n\n## Code Example\n\n```sql\nMERGE INTO iceberg_db.exchanges AS t\nUSING staged_exchanges AS s\nON t.exchange_id = s.exchange_id AND t.event_time = s.event_time\nWHEN MATCHED THEN UPDATE SET t.payload = s.payload\nWHEN NOT MATCHED THEN INSERT (exchange_id, event_time, payload) VALUES (s.exchange_id, s.event_time, s.payload)\n```\n\n## Follow-up Questions\n\n- How would you implement exactly-once semantics across multiple streams?\n- How would you monitor late-arrival data and alert on schema drift?","diagram":null,"difficulty":"intermediate","tags":["aws-dva"],"channel":"aws-dva","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Square","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T11:37:30.742Z","createdAt":"2026-01-14T11:37:30.742Z"},{"id":"q-1821","question":"In a real-world data platform on AWS, streaming JSON events from many producers land in a single Kinesis Data Stream and are ingested into per-tenant Parquet files in S3. Over time the schema evolves and late data arrives. Describe an end-to-end approach that ensures idempotent writes, supports schema evolution, isolates tenants with Lake Formation, and provides reliable auditing and monitoring. Include specific services, data formats, and trade-offs?","answer":"Use Glue Streaming or Spark on Glue to read from Kinesis, validate against Glue Schema Registry, and write per-tenant Parquet with Hudi for upserts. Maintain a dedup table in DynamoDB keyed by tenant+","explanation":"## Why This Is Asked\nTests ability to design end-to-end pipelines with schema evolution, idempotence, and governance in AWS data platforms.\n\n## Key Concepts\n- AWS Glue Streaming / Spark\n- AWS Glue Schema Registry\n- Parquet with per-tenant partitioning\n- Apache Hudi for upserts on S3\n- Lake Formation isolation\n- Event deduplication strategy (event_id in DynamoDB)\n- Late-arrival handling and monitoring\n\n## Code Example\n```javascript\n// Pseudocode: Spark Structured Streaming consuming Kinesis and upserting with Hudi\n```\n\n## Follow-up Questions\n- How would you backfill historical data after a schema change?\n- How would you test idempotence and dedup logic in CI/CD?","diagram":"flowchart TD\n  A[Kinesis] --> B[Glue Streaming]\n  B --> C[Parquet S3 (per-tenant)]\n  C --> D[Hudi Upserts]\n  D --> E[Lake Formation Isolation]\n  E --> F[CloudWatch Monitoring]","difficulty":"intermediate","tags":["aws-dva"],"channel":"aws-dva","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Databricks","Goldman Sachs"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T13:02:43.582Z","createdAt":"2026-01-14T13:02:43.584Z"},{"id":"q-1881","question":"Design an advanced cross-region data ingestion and governance pattern on AWS for a high-volume streaming platform. In us-east-1 raw JSON events arrive via Kinesis Firehose into S3, then replicate to us-west-2 with minimal latency. Propose an architecture that guarantees exactly-once ingestion, supports schema evolution via Iceberg, and enforces per-tenant isolation with Lake Formation. Explain idempotent writes, cross-region replication, date-based partitioning, and robust monitoring (lag, drift, backfills) with minimal tenant impact?","answer":"Idempotency uses a DynamoDB dedupe table keyed by tenantId-recordId with a TTL. Cross-region replication enables S3 us-east-1->us-west-2 and Iceberg tables via Glue catalog to support upserts and sche","explanation":"## Why This Is Asked\nTests ability to architect cross-region data pipelines, enforce idempotency, and govern multi-tenant data lakes at scale. It probes Iceberg-based schema evolution and Lake Formation access control under real-world constraints, plus operational observability for backfills and drift.\n\n## Key Concepts\n- Cross-region replication with S3\n- Idempotent ingestion in streaming data\n- Iceberg for upserts and schema evolution\n- Lake Formation per-tenant isolation\n- Backfill strategy and replay windows\n- Monitoring: lag, drift, DLQ, alerts\n\n## Code Example\n```javascript\n// Example dedupe item (conceptual)\n{\n  \"TenantId\": \"t1\",\n  \"RecordId\": \"r123\",\n  \"PayloadHash\": \"sha256\",\n  \"IngestedAt\": \"2026-01-14T12:34:56Z\"\n}\n```\n\n## Follow-up Questions\n- How would you test the idempotent path under backfill scenarios?\n- How would you handle Iceberg schema evolution compatibility with existing queries?","diagram":null,"difficulty":"advanced","tags":["aws-dva"],"channel":"aws-dva","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T15:41:59.981Z","createdAt":"2026-01-14T15:41:59.981Z"},{"id":"q-1925","question":"You're building a beginner AWS DVA pipeline: 2,000 devices emit JSON telemetry to S3 as daily JSONL; a Lambda validates lines and writes valid records as Parquet to a partitioned dataset, while invalid lines go to a DLQ. Propose a minimal approach to ensure idempotent processing, schema evolution, and late-data handling, with concrete services, data formats, and a simple monitoring plan?","answer":"Use a DynamoDB dedup table keyed by recordId to guard a per-record idempotent path in Lambda; write valid records to Parquet with date-based partitions in S3, and store re-ingested late data via date ","explanation":"## Why This Is Asked\nTests ability to design a robust, beginner-friendly DVA pipeline with idempotency, late-data handling, and schema evolution using common AWS services.\n\n## Key Concepts\n- Idempotent processing using a dedup store (DynamoDB)\n- Parquet partitions by date\n- Schema evolution via Glue Catalog / optional fields\n- Late-arriving data handling and re-ingest strategy\n- Observability via DLQ and CloudWatch alarms\n\n## Code Example\n````javascript\n// Pseudo-code for idempotent ingest\nconst exists = await dynDB.get({ PK: recordId });\nif (exists) return; // duplicate\nawait dynDB.put({ PK: recordId, ts: now });\nawait writeParquet(validRecord, datePartition);\n````\n\n## Follow-up Questions\n- How would you scale the dedup store and handle hot partitions?\n- How to test idempotency and schema evolution safely?\n- What are alternative approaches for late data without reprocessing all history?","diagram":null,"difficulty":"beginner","tags":["aws-dva"],"channel":"aws-dva","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T17:39:52.400Z","createdAt":"2026-01-14T17:39:52.400Z"},{"id":"q-2001","question":"Design a cross-account, multi-region ingestion pipeline: devices in Account A stream telemetry via Kinesis to a central data lake in Account B. Use Firehose to write Parquet to S3, Glue Catalog for schema, and Lake Formation for access control. Ensure idempotent writes with a dedup key and conditional Put, support schema evolution via Glue Schema Registry, handle late data with watermarks, and implement observability with CloudWatch metrics and a DLQ?","answer":"Multi-account ingestion: devices in Account A stream telemetry via Kinesis to a central data lake in Account B. Firehose writes Parquet to S3; Glue Catalog tracks schema; Lake Formation enforces cross","explanation":"Why This Is Asked\nTests multi-account data sharing, schema evolution, and idempotent ingestion. It probes cross-account IAM, Lake Formation permissions, and data reliability under late-arriving data.\n\nKey Concepts\n- Cross-account data access; Lake Formation\n- Kinesis, Firehose to S3 Parquet\n- Glue Catalog/Schema Registry and Iceberg/partitioning\n- Idempotent writes via dedup keys and conditional writes\n- Late data handling with watermarks and reprocess DLQ\n\nCode Example\n```python\n# idempotent write sketch (AWS Lambda with DynamoDB dedup)\nimport boto3\nimport json\n\nddb = boto3.resource('dynamodb')\ntbl = ddb.Table('IngestDedup')\n\ndef handler(event, ctx):\n  for r in json.loads(event['body']):\n    tbl.put_item(Item={'dedup_id': r['id'], 'ts': r['ts']}, ConditionExpression='attribute_not_exists(dedup_id)')\n```\n\nFollow-up Questions\n- How would you test this pipeline for duplicate suppression? \n- How do you adjust for schema evolution without breaking downstream queries?","diagram":"flowchart TD\n  A[Devices (Account A)] --> B[Kinesis Stream]\n  B --> C[Cross-Account Delivery to Account B]\n  C --> D[S3 Parquet in data-lake]\n  D --> E[Glue Catalog]\n  E --> F[Lake Formation Permissions]","difficulty":"advanced","tags":["aws-dva"],"channel":"aws-dva","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","Lyft","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T20:35:58.538Z","createdAt":"2026-01-14T20:35:58.538Z"},{"id":"q-2064","question":"In a cross-region telemetry pipeline for millions of devices, ensure exactly-once processing, schema evolution with optional fields, and late-arriving data backfill within 24 hours. Propose a concrete architecture using AWS DVA primitives (Kinesis Data Streams, Lambda/Fargate, S3, Glue, Athena, DynamoDB), describe idempotent writes, partitioning, late data handling, monitoring, and failure plans. Include trade-offs?","answer":"Implement a cross-region telemetry pipeline using Kinesis Data Streams as the ingestion backbone with per-region Fargate consumers. Store raw events in S3 with partitioned Parquet files organized by date/hour for efficient querying. Write to DynamoDB with idempotent upserts keyed by (deviceId, eventId) using conditional Put requests to prevent duplicates. Implement schema evolution using AWS Glue Schema Registry with optional fields and backward compatibility. Handle late-arriving data within 24 hours through a reprocessing window and idempotent updates. Monitor with CloudWatch metrics, DLQ alerts, and automated failure recovery.","explanation":"Why This Is Asked\n\nTests cross-region DVA design with production-grade guarantees.\n\nKey Concepts\n- Exactly-once semantics via per-eventId deduplication and conditional writes\n- Schema evolution using a registry and partitioned Parquet sinks\n- Late-arrival handling with 24-hour backfill window and idempotent reprocessing\n- Observability: CloudWatch metrics, DLQ, and alerting\n- Trade-offs: cost, latency, and cross-region consistency\n\nCode Example\n\n```javascript\nconst AWS = require('aws-sdk');\nconst ddb = new AWS.DynamoDB.DocumentClient();\n\nasync function saveEvent(event) {\n  const params = {\n    TableName: process.env.TABLE_NAME,\n    Item: {\n      deviceId: event.deviceId,\n      eventId: event.eventId,\n      timestamp: event.timestamp,\n      data: event.data\n    },\n    ConditionExpression: 'attribute_not_exists(eventId)'\n  };\n  \n  try {\n    await ddb.put(params).promise();\n  } catch (error) {\n    if (error.code !== 'ConditionalCheckFailedException') {\n      throw error;\n    }\n  }\n}\n```","diagram":null,"difficulty":"advanced","tags":["aws-dva"],"channel":"aws-dva","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Netflix","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T05:39:43.538Z","createdAt":"2026-01-14T22:51:27.773Z"},{"id":"q-2113","question":"Design a minimal AWS DVA ingestion for 2,000 devices publishing JSON to a Kinesis Data Stream. Implement a Lambda dedupe layer using DynamoDB (keyed by deviceId and eventId), then write validated records as Parquet to S3 partitioned by date/deviceId. Use Glue Schema Registry for optional fields and versioning. Backfill late data within 24 hours; include a basic monitoring plan and DLQ for invalid records?","answer":"Design a minimal AWS DVA ingestion for 2,000 devices publishing JSON to a Kinesis Data Stream. Implement Lambda deduplication using DynamoDB (keyed by deviceId and eventId), then write validated records as Parquet to S3 partitioned by date/deviceId. Use Glue Schema Registry for optional fields and versioning. Backfill late data within 24 hours; include basic monitoring and DLQ for invalid records.","explanation":"Why This Is Asked\n- Tests practical use of deduplication, schema evolution, and late data handling in a beginner-friendly AWS DVA setup.\n- Evaluates ability to compose KDS, Lambda, DynamoDB, S3, Glue, and CloudWatch with clear data ownership.\n\nKey Concepts\n- Idempotent processing via DynamoDB-based deduplication using composite keys.\n- End-to-end flow: KDS -> Lambda -> Parquet in S3, partitioned by date/deviceId.\n- Schema evolution with Glue Schema Registry (nullable/optional fields and versioning).\n- Late-arriving data handling within 24 hours and basic monitoring/alerts.\n\nCode Example\n```python\n# Lambda handler for deduplication and Parquet writing\nimport json\nimport boto3\nfrom datetime import datetime\nimport pyarrow as pa\nimport pyarrow.parquet as pq\nfrom awsglue.schema_registry import SchemaRegistryClient\n\nlambda_client = boto3.client('lambda')\ndynamodb = boto3.resource('dynamodb')\ns3 = boto3.client('s3')\nschema_registry = SchemaRegistryClient()\n\ndedupe_table = dynamodb.Table('device-events-dedupe')\n\ndef lambda_handler(event, context):\n    for record in event['Records']:\n        try:\n            payload = json.loads(record['kinesis']['data'])\n            device_id = payload['deviceId']\n            event_id = payload['eventId']\n            event_time = payload['timestamp']\n            \n            # Check for duplicates\n            response = dedupe_table.get_item(\n                Key={'deviceId': device_id, 'eventId': event_id}\n            )\n            if 'Item' in response:\n                continue  # Skip duplicate\n            \n            # Validate schema\n            schema = schema_registry.get_schema_by_version_id('device-events-schema', 'latest')\n            validated_data = schema.validate(payload)\n            \n            # Write to DynamoDB for deduplication\n            dedupe_table.put_item(\n                Item={'deviceId': device_id, 'eventId': event_id, 'ttl': int(event_time/1000) + 86400}\n            )\n            \n            # Convert to Parquet and write to S3\n            date_partition = datetime.fromtimestamp(event_time/1000).strftime('%Y/%m/%d')\n            table = pa.Table.from_pydict(validated_data)\n            \n            s3_path = f's3://device-events-raw/date={date_partition}/deviceId={device_id}/{event_id}.parquet'\n            \n            # Write Parquet to S3\n            buffer = pa.BufferOutputStream()\n            pq.write_table(table, buffer)\n            s3.put_object(\n                Bucket='device-events-raw',\n                Key=f'date={date_partition}/deviceId={device_id}/{event_id}.parquet',\n                Body=buffer.getvalue().to_pybytes()\n            )\n            \n        except Exception as e:\n            # Send to DLQ for manual review\n            lambda_client.invoke(\n                FunctionName='device-events-dlq-handler',\n                InvocationType='Event',\n                Payload=json.dumps({'error': str(e), 'record': record})\n            )\n    \n    return {'statusCode': 200, 'body': 'Processing completed'}\n```\n\nMonitoring Plan\n- CloudWatch metrics: Lambda invocations, errors, duration\n- DynamoDB consumed capacity and throttling\n- S3 object count and size monitoring\n- CloudWatch alerts for error rates > 5% and Lambda timeouts\n- DLQ size monitoring for manual intervention","diagram":"flowchart TD\n  A[Device Telemetry] -->|Publish| B[Kinesis Data Stream]\n  B --> C[Lambda: dedupe & validate]\n  C --> D[S3: Parquet (partition by date/deviceId)]\n  D --> E[Glue Data Catalog]\n  C --> F[DLQ: invalid records]","difficulty":"beginner","tags":["aws-dva"],"channel":"aws-dva","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Robinhood","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T04:57:06.050Z","createdAt":"2026-01-15T02:23:34.823Z"},{"id":"q-2202","question":"Design a real-time payments fraud detector handling 100k events/sec peak with Kinesis Data Streams. End-to-end latency <200 ms, exactly-once processing, and cross-region DR. Data schemas evolve with optional fields; late data allowed within 10 minutes backfill. Propose concrete AWS DVA architecture using KDS, Lambda/Fargate, S3, Glue, Athena, DynamoDB; explain idempotent writes, partitioning, backpressure, monitoring, and failure plans?","answer":"Use a Kinesis Data Stream with per-record keys and sequence numbers to enable idempotent writes; fan-out to Lambda for fast scoring and to Fargate for ML-based scoring. Maintain online state in Dynamo","explanation":"## Why This Is Asked\nTests design of real-time fraud pipelines at scale with strong reliability, schema evolution, late data handling, and DR.\n\n## Key Concepts\n- Exactly-once semantics with sequence numbers and dedup keys\n- Split real-time scoring (Lambda) and heavier ML (Fargate)\n- DynamoDB as online feature/state store; S3/Parquet for history\n- Glue catalog for evolving schema; Parquet lineage in Athena\n- Late-data backfill within 10 minutes; cross-region DR\n\n## Code Example\n```javascript\n// Pseudo: idempotent write to DynamoDB\ndynamo.put({TableName, Item, ConditionExpression: 'attribute_not_exists(transaction_id)'}, cb)\n```\n\n## Follow-up Questions\n- How would you validate end-to-end latency under burst traffic?\n- What failure modes exist and how would you mitigate them?","diagram":"flowchart TD\nA[KDS] --> B[Lambda]\nA --> C[Fargate ML]\nB --> D[DynamoDB]\nC --> D\nD --> E[S3 Parquet]\n","difficulty":"advanced","tags":["aws-dva"],"channel":"aws-dva","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Stripe","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T07:34:06.093Z","createdAt":"2026-01-15T07:34:06.094Z"},{"id":"q-2316","question":"Beginner AWS DVA task: 2,000 devices send telemetry as JSON lines to a Kinesis Data Stream, consumed by a Lambda that writes Parquet to S3 and uses a DynamoDB table for dedupe. A burst from one device creates hot shards and latency. Propose concrete steps to identify/mitigate shard hotspots, adjust partition keys, tune Lambda concurrency, implement idempotent writes, preserve schema evolution, and plan late-data backfill within 24 hours. Include basic monitoring?","answer":"Candidates should outline: (1) monitor shard metrics (GetRecords per shard) and set shard count/upscale; (2) implement partition key by device_type/date to distribute shards; (3) set Lambda reserved c","explanation":"## Why This Is Asked\nTests practical debugging of shard hot spots, partitioning strategies, and idempotent writes in a beginner DVA context, with realistic latency and backfill constraints.\n\n## Key Concepts\n- Shard hot-spot detection in Kinesis Data Streams\n- Partition keys and data modeling for distribution\n- Lambda concurrency and fault tolerance\n- DynamoDB-based idempotent dedup\n- Glue catalog schema evolution and Parquet partitions\n- Late-data backfill with Glue jobs\n\n## Code Example\n```javascript\n// pseudo-code for dedupe in Lambda\nconst key = deviceId + ':' + eventTs;\nconst exists = await dynamo.get({Key: {id: key}});\nif (exists) return; // idempotent\nawait dynamo.put({Item: {id: key, ...record}});\n```\n\n```\n```\n\n## Follow-up Questions\n- How would you choose partition keys to balance shard utilization?\n- How would you detect late data and trigger a 24-hour backfill?\n- What alerts would you configure for latency and error rates?","diagram":null,"difficulty":"beginner","tags":["aws-dva"],"channel":"aws-dva","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Google","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T11:42:44.850Z","createdAt":"2026-01-15T11:42:44.850Z"},{"id":"q-2336","question":"Beginner AWS DVA task: 2,000 devices emit JSON telemetry to a Kinesis Data Stream. Build a minimal pipeline that (a) validates and redacts PII fields (e.g., userId, deviceId) in real time, (b) writes sanitized records to S3 as Parquet partitioned by date and region, (c) preserves optional fields for schema evolution, and (d) routes and logs any failed records. Propose concrete services, data formats, and a simple monitoring plan?","answer":"Use a streaming pipeline: Kinesis Data Streams -> Lambda function with a JSON Schema validator that redacts PII fields (userId, deviceId) in real-time, preserving optional fields for future schema evo","explanation":"## Why This Is Asked\nTests ability to design a compliant, observable streaming pipeline at beginner level, including PII redaction and schema evolution.\n\n## Key Concepts\n- PII redaction in streaming\n- JSON Schema validation\n- Parquet partitioning by date/region\n- Optional fields and schema evolution\n- DLQ handling and CloudWatch auditing\n\n## Code Example\n```javascript\nfunction redact(record) {\n  if (record.userId) record.userId = \"***REDACTED***\";\n  if (record.deviceId) record.deviceId = \"***REDACTED***\";\n  return record;\n}\n```\n\n## Follow-up Questions\n- How would you test redaction with evolving schemas?\n- What changes would you make to handle large payloads?","diagram":null,"difficulty":"beginner","tags":["aws-dva"],"channel":"aws-dva","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Hugging Face","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T13:10:46.686Z","createdAt":"2026-01-15T13:10:46.686Z"},{"id":"q-2412","question":"**Beginner** AWS DVA task: 10k devices emit telemetry as JSON to Kinesis in Account A. Propose a minimal cross-account pipeline (A->B) using KDS, Lambda/Fargate, S3, Glue, Athena, DynamoDB that validates schema, deduplicates, enriches with device metadata, and writes partitioned Parquet to S3. Include data lineage, cross-account access controls, late-arrival handling within 48 hours, and a simple monitoring plan. Outline trade-offs?","answer":"Proposed pipeline: data lands as JSON in S3 raw (Account A); a Glue crawler catalogs, a Lambda function copies to Account B; a Glue Spark job validates schema, enriches with device metadata from Dynam","explanation":"## Why This Is Asked\nTests ability to design cross-account data pipelines, consider lineage and access control, and handle late data with basic monitoring in a beginner-friendly way.\n\n## Key Concepts\n- Cross-account data transfer\n- Data lineage and provenance\n- Schema validation and enrichment\n- Late-arrival handling\n- Cost/throughput trade-offs\n\n## Code Example\n```javascript\n// Pseudocode: basic schema check\nfunction isValid(record, schema) { /* validate fields, types, and required flags */ }\n```\n\n## Follow-up Questions\n- How would you validate cross-account IAM permissions and least privilege?\n- What strategies minimize data duplication costs while preserving lineage?","diagram":null,"difficulty":"beginner","tags":["aws-dva"],"channel":"aws-dva","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T17:00:23.256Z","createdAt":"2026-01-15T17:00:23.256Z"},{"id":"q-2716","question":"Beginner AWS DVA task: 2,000 devices emit telemetry as JSON lines to a Kinesis Data Stream. Build a minimal pipeline with Lambda to validate and redact PII, and to write Parquet to S3 partitioned by region and date. Use DynamoDB for dedupe (deviceId+seq). Plan schema evolution for optional firmware.version and geo, and a 7-day backfill. Include monitoring and cost considerations?","answer":"Leverage Kinesis Data Stream + Lambda to ingest; DynamoDB for dedupe (deviceId+seq); Lambda validates JSON, redacts PII, writes Parquet to S3 partitioned by region/date; Glue catalog + Athena for quer","explanation":"## Why This Is Asked\nTests practical data ingestion, dedupe, schema evolution, and backfill in a beginner-level AWS DVA context with real primitives.\n\n## Key Concepts\n- Kinesis Data Streams, Lambda, DynamoDB dedupe\n- Parquet on S3 with Glue/Athena\n- Schema evolution via optional fields\n- Backfill strategies and monitoring\n\n## Code Example\n```javascript\n// transformation sketch\n```\n\n## Follow-up Questions\n- How would you validate late data handling? \n- How would you cost-optimize shard provisioning?","diagram":"flowchart TD\nA[Devices] --> B[Kinesis Data Stream]\nB --> C[Lambda Processor]\nC --> D[DynamoDB (dedupe)]\nC --> E[S3 Parquet (region/date)]\nE --> F[Glue Catalog] --> G[Athena]\nH[Backfill] --> E","difficulty":"beginner","tags":["aws-dva"],"channel":"aws-dva","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Snap","Twitter","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T07:48:55.918Z","createdAt":"2026-01-16T07:48:55.918Z"},{"id":"q-674","question":"You're building an AWS DVA data-analytics pipeline ingesting telemetry from 2000 devices/sec via Kinesis Data Streams. A Spark/Glue path processes windows and writes Parquet to S3; aggregation state in DynamoDB. Data loss or duplicates occur during retries and outages; costs spike at peak. Design a resilient, cost-efficient approach: shard sizing, processing path, idempotent writes, error handling with DLQ, and observability. What would you implement and why?","answer":"Scale by configuring 4 Kinesis shards to sustain ~4000 RPS; use a real-time analytics path (Kinesis Data Analytics or Spark) for windowed aggregation, then write Parquet to S3 via Glue streaming. Dedu","explanation":"## Why This Is Asked\n\nAssesses real-world ability to design a resilient streaming analytics pipeline on AWS, balancing throughput, cost, and correctness. Candidates must justify shard sizing, choice of processing path (KDA vs Glue streaming), idempotent writes, error handling, and observability.\n\n## Key Concepts\n\n- Streaming ingestion and scaling\n- Exactly-once semantics with DynamoDB\n- DLQ and backpressure\n- Observability and cost control\n\n## Code Example\n\n```javascript\n// Pseudocode: deduplicate by composite key and conditional writes\n```\n\n## Follow-up Questions\n\n- How would you migrate this to a multi-region setup?\n- How would you test failure modes and simulate burst traffic?","diagram":null,"difficulty":"intermediate","tags":["aws-dva"],"channel":"aws-dva","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Microsoft","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-11T14:45:55.246Z","createdAt":"2026-01-11T14:45:55.246Z"},{"id":"q-916","question":"In an AWS-based DVA pipeline ingesting telemetry from 2000 devices/sec via Kinesis Data Streams, with a Spark/Glue path writing Parquet to S3 and cataloged in Glue, design an end-to-end strategy for robust schema evolution, data validation, and partitioning that minimizes reprocessing and supports backfills. Include schema registry, idempotence, DLQ, and observability?","answer":"Adopt Glue Schema Registry with versioned Avro schemas; Spark Structured Streaming validates records against the latest compatible schema and writes Parquet to S3 partitioned by device/hour. Route inc","explanation":"## Why This Is Asked\nThe question probes practical UX for evolving data schemas in streaming pipelines, ensuring data quality and low reprocessing costs during changes.\n\n## Key Concepts\n- Glue Schema Registry and versioning\n- Schema compatibility strategies (backward/forward)\n- Spark Structured Streaming validation and checkpointing\n- Idempotent writes and deduplication stores\n- DLQ routing and backfill procedures\n- Observability with CloudWatch and Glue metrics\n\n## Code Example\n```javascript\n// Spark reads latest compatible Avro schema from Glue, writes Parquet partitioned by device/hour\ndf.write\n  .partitionBy(\"device_id\", \"hour\")\n  .format(\"parquet\").save(\"s3://bucket/data/\");\n```\n\n## Follow-up Questions\n- How would you handle backward-incompatible schema changes without downtime?\n- What metrics signal schema drift or data quality issues?","diagram":null,"difficulty":"intermediate","tags":["aws-dva"],"channel":"aws-dva","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","Snowflake","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T15:29:41.744Z","createdAt":"2026-01-12T15:29:41.744Z"}],"subChannels":["general"],"companies":["Adobe","Amazon","Apple","Coinbase","Databricks","Discord","DoorDash","Goldman Sachs","Google","Hugging Face","IBM","Instacart","LinkedIn","Lyft","Meta","Microsoft","MongoDB","NVIDIA","Netflix","OpenAI","Oracle","PayPal","Robinhood","Salesforce","Slack","Snap","Snowflake","Square","Stripe","Tesla","Twitter","Two Sigma","Uber","Zoom"],"stats":{"total":24,"beginner":10,"intermediate":6,"advanced":8,"newThisWeek":24}}