{"questions":[{"id":"aws-dva-deployment-1768228144730-0","question":"You are deploying a containerized app on ECS Fargate behind an Application Load Balancer and want zero-downtime updates with automatic rollback if the new version fails?","answer":"[{\"id\":\"a\",\"text\":\"Rolling updates of the ECS service with deployment circuit breaker enabled\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Blue/Green deployment with AWS CodeDeploy for ECS\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Canary deployment with traffic shifting via ALB\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"All-at-once deployment with immediate traffic switch\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct answer is B. A Blue/Green deployment with AWS CodeDeploy for ECS provides separate environments, traffic shifting, and built-in rollback handling, which supports zero-downtime updates when issues arise in the new task set.\n\n## Why Other Options Are Wrong\n- A: Rolling updates with a circuit breaker can rollback, but it does not inherently guarantee zero-downtime traffic cutover and may still involve risk during the switch.\n- C: Canary deployments can progressively route a small portion of traffic but do not guarantee complete zero-downtime delivery and require additional routing logic.\n- D: All-at-once deployments switch all traffic at once, causing downtime if the new version has issues.\n\n## Key Concepts\n- ECS Blue/Green with CodeDeploy\n- Traffic shifting and rollback capabilities\n- Zero-downtime deployments in ECS/Fargate\n\n## Real-World Application\n- Use Blue/Green with CodeDeploy when deploying critical services where a quick rollback and clean traffic cutover are required, such as customer-facing APIs.\n","diagram":null,"difficulty":"intermediate","tags":["AWS","ECS","CodeDeploy","ALB","Fargate","CI/CD","certification-mcq","domain-weight-24"],"channel":"aws-dva","subChannel":"deployment","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T14:29:04.732Z","createdAt":"2026-01-12 14:29:05"},{"id":"aws-dva-deployment-1768228144730-1","question":"During a blue/green ECS deployment, which AWS resource defines the traffic-shift settings (e.g., canary vs all-at-once) and monitors health to promote or rollback the new task set?","answer":"[{\"id\":\"a\",\"text\":\"AWS CodeDeploy DeploymentGroup\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"AWS CloudFormation DeploymentConfig\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"AWS ECS Service deploymentConfiguration\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"AWS Application Load Balancer listener rule\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA. AWS CodeDeploy DeploymentGroup defines the deployment configuration for ECS blue/green deployments, including traffic-shift settings (e.g., TimeBasedCanary, AllAtOnce) and health-based promotion/rollback.\n\n## Why Other Options Are Wrong\n- B: CloudFormation DeploymentConfig is not used to control traffic-shift during ECS blue/green deployments.\n- C: ECS Service deploymentConfiguration controls rolling updates but not the CodeDeploy traffic-shift strategy.\n- D: ALB listener rules route traffic but do not define deployment traffic-shift strategies or rollback behavior.\n\n## Key Concepts\n- CodeDeploy deployment groups\n- Traffic routing configurations\n- ECS blue/green deployment workflow\n\n## Real-World Application\n- Use CodeDeploy DeploymentGroup to fine-tune traffic shifting during blue/green releases and to enforce controlled promotion or rollback in production.\n","diagram":null,"difficulty":"intermediate","tags":["AWS","CodeDeploy","ECS","BlueGreen","CI/CD","certification-mcq","domain-weight-24"],"channel":"aws-dva","subChannel":"deployment","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T14:29:05.247Z","createdAt":"2026-01-12 14:29:05"},{"id":"aws-dva-deployment-1768228144730-2","question":"To minimize blast radius in a CI/CD pipeline that deploys to ECS and ECR, which IAM pattern should you implement?","answer":"[{\"id\":\"a\",\"text\":\"Attach AdministratorAccess to the CI role\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Create a dedicated deployment role with least privilege and allow the CI to assume it\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Use the root account for deployment\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use a separate IAM user per service and rotate credentials\",\"isCorrect\":false}]","explanation":"## Correct Answer\nB. Use a dedicated deployment role with least privilege and allow the CI system to assume it (via STS AssumeRole). This confines permissions to whatâ€™s necessary for ECS/ECR deployment, reducing blast radius.\n\n## Why Other Options Are Wrong\n- A: Admin access grants excessive permissions, increasing risk.\n- C: Root credentials are dangerous and not allowed for automation.\n- D: Separate IAM users per service is cumbersome and still risks broad permissions if not carefully scoped; role assumption is cleaner and auditable.\n\n## Key Concepts\n- Least privilege IAM design\n- STS AssumeRole for CI/CD pipelines\n- Scoped permissions for ECR and ECS operations\n\n## Real-World Application\n- Implement a deployment role with specific actions (e.g., ecr:GetAuthorizationToken, ecr:BatchCheckLayerAvailability, ecr:PutImage, ecs:RegisterTaskDefinition, ecs:UpdateService) and require the CI to assume it.\n","diagram":null,"difficulty":"intermediate","tags":["AWS","IAM","ECS","ECR","Security","certification-mcq","domain-weight-24"],"channel":"aws-dva","subChannel":"deployment","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T14:29:05.734Z","createdAt":"2026-01-12 14:29:05"},{"id":"aws-dva-deployment-1768228144730-3","question":"Which mechanism ensures automatic rollback of a CloudFormation stack update if the update fails and rollback is not disabled?","answer":"[{\"id\":\"a\",\"text\":\"DisableRollback set to true\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Rollback on failure by default\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Manual rollback after failure\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use Terraform instead of CloudFormation\",\"isCorrect\":false}]","explanation":"## Correct Answer\nB. CloudFormation automatically rolls back stack updates when a update fails, unless Rollback is explicitly disabled (DisableRollback). The default behavior is to perform a rollback to the previous stable state.\n\n## Why Other Options Are Wrong\n- A: DisableRollback prevents automatic rollback.\n- C: Manual rollback is not automatic.\n- D: Terraform vs CloudFormation is unrelated to the automatic rollback behavior of CFN.\n\n## Key Concepts\n- CloudFormation rollback behavior\n- DisableRollback option\n- Deployment safety in IaC\n\n## Real-World Application\n- Rely on CloudFormation's built-in rollback to maintain prod stability during template changes; only override with explicit deliberate steps.\n","diagram":null,"difficulty":"intermediate","tags":["AWS","CloudFormation","IaC","DeploymentSafety","certification-mcq","domain-weight-24"],"channel":"aws-dva","subChannel":"deployment","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T14:29:05.896Z","createdAt":"2026-01-12 14:29:05"},{"id":"aws-dva-deployment-1768228144730-4","question":"When deploying to a Kubernetes cluster on Amazon EKS, which deployment strategy enables progressive traffic shifting and automated rollback between revisions?","answer":"[{\"id\":\"a\",\"text\":\"All-at-once update of the Deployment\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"RollingUpdate with maxUnavailable=0\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Canary deployment with Istio or Argo Rollouts\",\"isCorrect\":true},{\"id\":\"d\",\"text\":\"Blue/Green using external DNS switch\",\"isCorrect\":false}]","explanation":"## Correct Answer\nC. Canary deployments using tools like Istio or Argo Rollouts enable progressive traffic shifting and automated rollback between revisions in an EKS cluster, aligning with safe, incremental delivery practices.\n\n## Why Other Options Are Wrong\n- A: All-at-once updates do not provide progressive traffic shifting or automated rollback.\n- B: RollingUpdate with maxUnavailable=0 is a Kubernetes update strategy but does not inherently provide automated rollback between revisions without additional tooling.\n- D: Blue/Green with external DNS can work but is more complex to set up in Kubernetes without additional CD tooling; Canary is typically the targeted approach for progressive delivery in Kubernetes.\n\n## Key Concepts\n- Canary deployments in Kubernetes (Istio/Argo Rollouts)\n- Progressive delivery and automated rollback\n- EKS integration with CD tooling\n\n## Real-World Application\n- Use canary deployments to gradually shift traffic to a new revision, monitor metrics, and automatically rollback on failure in production-grade EKS workloads.\n","diagram":null,"difficulty":"intermediate","tags":["AWS","EKS","Kubernetes","Istio","ArgoRollouts","CD","certification-mcq","domain-weight-24"],"channel":"aws-dva","subChannel":"deployment","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T14:29:06.057Z","createdAt":"2026-01-12 14:29:06"},{"id":"aws-dva-deployment-1768289129778-0","question":"When updating an ECS Fargate service with a new image, which deployment configuration minimizes downtime by ensuring at least a portion of tasks remain healthy during the rollout?","answer":"[{\"id\":\"a\",\"text\":\"minimumHealthyPercent: 100, maximumPercent: 200\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"minimumHealthyPercent: 50, maximumPercent: 200\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"minimumHealthyPercent: 75, maximumPercent: 200\",\"isCorrect\":true},{\"id\":\"d\",\"text\":\"minimumHealthyPercent: 25, maximumPercent: 100\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption C is correct because setting minimumHealthyPercent to 75% while allowing up to 200% maximumPercent enables the rollout to run new tasks while maintaining at least 75% of the desired task count healthy, reducing downtime during the deployment.\n\n## Why Other Options Are Wrong\n- A would require all tasks to be healthy before replacing them, increasing downtime if the old tasks terminate before new ones are healthy.\n- B uses only 50% healthy, which risks service disruption during the rollout.\n- D uses only 25% healthy and a low maximum, potentially causing insufficient capacity during updates.\n\n## Key Concepts\n- ECS Deployment Configuration\n- minimumHealthyPercent\n- maximumPercent\n\n## Real-World Application\n- Use this setting for production services where maintaining baseline capacity during deployments is critical to meet SLAs.","diagram":null,"difficulty":"intermediate","tags":["AWS","ECS","Fargate","Deployment","certification-mcq","domain-weight-24"],"channel":"aws-dva","subChannel":"deployment","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T07:25:29.779Z","createdAt":"2026-01-13 07:25:30"},{"id":"aws-dva-deployment-1768289129778-1","question":"You want to deploy a new Lambda function version behind an API Gateway with a canary rollout; which CodeDeploy strategy would you choose to gradually shift 10% of traffic to the new version first?","answer":"[{\"id\":\"a\",\"text\":\"Canary deployment with 10% traffic to the new version for a 5-minute interval\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Linear deployment with 20% traffic shift every 2 minutes\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"All-at-once deployment\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Blue/green deployment by swapping API Gateway stages\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A is correct because CodeDeploy canary deployments start with a small percentage of traffic to the new Lambda version and progressively increase it, enabling risk-aware rollout.\n\n## Why Other Options Are Wrong\n- B describes a linear rollout with larger, staged shifts that deviate from the standard canary approach and may complicate safety checks.\n- C would route all traffic to the new version immediately, negating the canary safety mechanism.\n- D describes a blue/green pattern, which is different from a canary deployment model.\n\n## Key Concepts\n- CodeDeploy canary deployments\n- Traffic shifting and rollout intervals\n- Lambda alias routing\n\n## Real-World Application\n- Use canary deployments to validate new function behavior under real load with minimal risk.","diagram":null,"difficulty":"intermediate","tags":["AWS","Lambda","CodeDeploy","Deployment","certification-mcq","domain-weight-24"],"channel":"aws-dva","subChannel":"deployment","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T07:25:30.555Z","createdAt":"2026-01-13 07:25:30"},{"id":"aws-dva-deployment-1768289129778-2","question":"In Kubernetes on EKS, which rolling update strategy settings ensure zero downtime during a deployment by avoiding pod unavailability?","answer":"[{\"id\":\"a\",\"text\":\"maxUnavailable: 1, maxSurge: 1\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"maxUnavailable: 0, maxSurge: 1\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"strategy: Recreate\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"pause rollout after each pod\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct because setting maxUnavailable to 0 ensures no pods are terminated before new pods are ready, while maxSurge allows extra pods to run during the update, enabling zero-downtime rolling updates.\n\n## Why Other Options Are Wrong\n- A allows a brief period where one pod can be unavailable, risking downtime.\n- C uses the Recreate strategy, which terminates all old pods before new ones start, causing downtime.\n- D is not a valid Deployment strategy configuration in Kubernetes.\n\n## Key Concepts\n- Kubernetes RollingUpdate strategy\n- maxUnavailable\n- maxSurge\n\n## Real-World Application\n- Essential for critical services running on EKS where any downtime is unacceptable.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","EKS","Deployment","certification-mcq","domain-weight-24"],"channel":"aws-dva","subChannel":"deployment","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T07:25:31.066Z","createdAt":"2026-01-13 07:25:31"},{"id":"aws-dva-deployment-1768289129778-3","question":"Which AWS service pattern is best for blue/green deployments to EC2 instances behind a single load balancer to minimize risk?","answer":"[{\"id\":\"a\",\"text\":\"AWS CodeDeploy for EC2/On-Prem with blue/green deployments\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Route 53 weighted routing alone\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Elastic Beanstalk single environment\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"CloudFormation change sets\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A is correct because CodeDeploy supports blue/green deployments for EC2/On-Prem environments, enabling two identical environments behind a single load balancer and swapping traffic between them safely.\n\n## Why Other Options Are Wrong\n- B can perform routing between environments but does not manage the deployment lifecycle and traffic swap as a blue/green pattern.\n- C Elastic Beanstalk abstracts deployment but does not provide explicit blue/green swap control for EC2 behind a single LB.\n- D Change Sets help preview changes but do not implement blue/green traffic swapping.\n\n## Key Concepts\n- Blue/green deployments\n- EC2/On-Prem with CodeDeploy\n- Traffic switching via load balancer\n\n## Real-World Application\n- Reduces risk for production releases by isolating new versions in a separate environment before cutover.","diagram":null,"difficulty":"intermediate","tags":["AWS","CodeDeploy","EC2","BlueGreen","certification-mcq","domain-weight-24"],"channel":"aws-dva","subChannel":"deployment","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T07:25:31.250Z","createdAt":"2026-01-13 07:25:31"},{"id":"aws-dva-deployment-1768289129778-4","question":"When provisioning infrastructure with CloudFormation, which feature allows you to preview changes before applying them to a stack?","answer":"[{\"id\":\"a\",\"text\":\"Change Sets\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"StackSets\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Drift detection\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Nested stacks\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A is correct because Change Sets let you preview the impact of a CloudFormation change before applying it to a stack, reducing the risk of unintended updates.\n\n## Why Other Options Are Wrong\n- B StackSets are for multi-account/region orchestration, not change previews.\n- C Drift detection checks for drift after deployment, not for previewing changes.\n- D Nested stacks organize templates but do not provide pre-apply change previews.\n\n## Key Concepts\n- CloudFormation Change Sets\n- Change previews vs. live updates\n\n## Real-World Application\n- Use Change Sets in production to validate changes before executing them.","diagram":null,"difficulty":"intermediate","tags":["AWS","CloudFormation","IaC","ChangeSets","certification-mcq","domain-weight-24"],"channel":"aws-dva","subChannel":"deployment","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T07:25:31.431Z","createdAt":"2026-01-13 07:25:31"},{"id":"aws-dva-development-1768181316350-0","question":"An application processes messages from an SQS standard queue via an AWS Lambda consumer. The backlog fluctuates and some messages fail processing occasionally. Which design best ensures at-least-once processing, avoids poison pill issues, and minimizes latency?","answer":"[{\"id\":\"a\",\"text\":\"Attach a Dead-Letter Queue to the SQS queue with a maximum receive count and ensure the Lambda function is idempotent.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Move to an SQS FIFO queue to guarantee order and use a single worker.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Disable DLQ and implement fixed retry logic in Lambda with no backoff.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Switch to an SNS topic fanout with Lambda subscribers.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A: Attach a Dead-Letter Queue to the SQS queue with a maximum receive count and ensure the Lambda function is idempotent. This combination provides a safety net for failed messages and prevents repeated harmful retries, while idempotence ensures duplicates don't impact results.\n\n## Why Other Options Are Wrong\n\n- Option B: FIFO queues preserve order but limit throughput and do not inherently solve poison-pill handling or at-least-once semantics for sporadic failures.\n- Option C: Disabling the DLQ removes a critical safety mechanism for failed messages and does not provide controlled retry handling.\n- Option D: SNS fanout changes the delivery model and does not provide per-message dead-lettering or reliable retry semantics for SQS-backed consumers.\n\n## Key Concepts\n- SQS Dead-Letter Queue\n- At-least-once delivery model\n- Idempotent processing in Lambda\n- Backoff and retry handling\n\n## Real-World Application\n- Implement DLQs to quarantine problematic messages and reprocess after fixes; design consumers to be idempotent to tolerate retries without duplication.","diagram":null,"difficulty":"intermediate","tags":["AWS","SQS","Lambda","Dead-Letter-Queue","Idempotency","certification-mcq","domain-weight-32"],"channel":"aws-dva","subChannel":"development","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T01:28:36.352Z","createdAt":"2026-01-12 01:28:36"},{"id":"aws-dva-development-1768181316350-1","question":"In an EKS cluster running on AWS, you want pods in a namespace to access DynamoDB without embedding credentials and with least privilege. Which approach best achieves this?","answer":"[{\"id\":\"a\",\"text\":\"Create an IAM role for service accounts (IRSA) and annotate the Kubernetes service account used by the pods with the role ARN.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Store AWS credentials in a Kubernetes secret and mount it as environment variables in pods.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Include DynamoDB credentials in the container image and rotate them via CI/CD.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Expose DynamoDB publicly and use an IAM user in the code.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A: Use IRSA by associating an IAM role with the Kubernetes service account used by the pods, granting only DynamoDB permissions. This avoids secret management in Kubernetes and follows least-privilege.\n\n## Why Other Options Are Wrong\n\n- Option B: Storing credentials in Kubernetes Secrets is less secure and requires ongoing secret management; it also risks exposure if Secrets are compromised.\n- Option C: Embedding credentials in the container image is insecure and violates best practices for secret management and rotation.\n- Option D: Publicly exposing DynamoDB undermines least-privilege and security; using IAM users embedded in code is hard to rotate and distribute securely.\n\n## Key Concepts\n- IRSA (IAM Roles for Service Accounts)\n- Least-privilege IAM policies for DynamoDB\n- Kubernetes ServiceAccounts and AWS IAM integration\n\n## Real-World Application\n- Secure pod access to AWS services without secret management; enables automatic credential rotation and strict access control.","diagram":null,"difficulty":"intermediate","tags":["AWS","EKS","IRSA","IAM","DynamoDB","Kubernetes","certification-mcq","domain-weight-32"],"channel":"aws-dva","subChannel":"development","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T01:28:36.807Z","createdAt":"2026-01-12 01:28:37"},{"id":"aws-dva-development-1768181316350-2","question":"For a REST API backed by API Gateway and Lambda with sporadic traffic, which pattern best achieves low latency during bursts while controlling cost?","answer":"[{\"id\":\"a\",\"text\":\"Enable Lambda provisioned concurrency for the API handler and optionally enable API Gateway caching.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Move to an EC2-based API behind a Load Balancer to guarantee performance.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Increase the Lambda memory size to reduce cold starts without provisioning concurrency.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use Lambda destinations to route failures to SQS for retry handling.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A: Enable Lambda provisioned concurrency for the API handler to keep a set number of instances warm, ensuring low latency during bursts while API Gateway caching can further reduce backend load. This directly addresses latency with a measurable cost control mechanism.\n\n## Why Other Options Are Wrong\n\n- Option B: EC2-based solutions add scaling and maintenance overhead and may increase cost for sporadic traffic.\n- Option C: Increasing memory can reduce cold starts but does not guarantee low latency during bursts due to potential scaling limits.\n- Option D: Routing failures to SQS handles errors, not latency optimization for normal traffic.\n\n## Key Concepts\n- Lambda provisioned concurrency\n- API Gateway integration\n- Cost vs latency optimization\n\n## Real-World Application\n- Use provisioned concurrency for predictable latency in API backends with variable traffic; monitor and adjust to balance performance and cost.","diagram":null,"difficulty":"intermediate","tags":["AWS","API-Gateway","Lambda","ProvisionedConcurreny","Serverless","certification-mcq","domain-weight-32"],"channel":"aws-dva","subChannel":"development","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T01:28:37.270Z","createdAt":"2026-01-12 01:28:37"},{"id":"aws-dva-development-1768253194383-0","question":"Which API Gateway configuration provides the most cost-effective and simplest API surface for a serverless microservice that forwards traffic to a Lambda function during bursty workloads?","answer":"[{\"id\":\"a\",\"text\":\"REST API with Lambda proxy integration\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"REST API with Lambda non-proxy integration\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"HTTP API (v2) with Lambda proxy integration\",\"isCorrect\":true},{\"id\":\"d\",\"text\":\"Application Load Balancer in front of Lambda\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption C is correct because HTTP API (v2) with Lambda proxy integration provides lower cost per request and simpler configuration for serverless workloads; REST API options are more costly and offer capabilities that are unnecessary for simple proxy-style use, while an ALB in front of Lambda adds unnecessary complexity and cost.\n\n## Why Other Options Are Wrong\n- Option A: REST API with Lambda proxy integration is more expensive per request and requires more configuration than HTTP API for bursty traffic.\n- Option B: REST API with Lambda non-proxy integration requires explicit request/response mappings, increasing maintenance.\n- Option D: Application Load Balancer is not optimized for API-style orchestration with Lambda and adds extra networking overhead.\n\n## Key Concepts\n- HTTP API vs REST API in API Gateway\n- Lambda proxy integration\n- Cost and latency considerations for serverless endpoints\n\n## Real-World Application\n- Use HTTP API with Lambda proxy for rapid-prototyping microservices that experience unpredictable request bursts.\n","diagram":null,"difficulty":"intermediate","tags":["AWS","API Gateway","Lambda","Serverless","certification-mcq","domain-weight-32"],"channel":"aws-dva","subChannel":"development","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T21:26:34.384Z","createdAt":"2026-01-12 21:26:34"},{"id":"aws-dva-development-1768253194383-1","question":"A fast-moving startup experiences highly bursty, unpredictable read/write traffic to a DynamoDB table; you want pay-per-request and no capacity planning. Which mode should you choose?","answer":"[{\"id\":\"a\",\"text\":\"Provisioned capacity with manual scaling\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"On-Demand capacity mode\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Provisioned capacity with auto-scaling\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"DAX-accelerated provisioned capacity\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct because DynamoDB On-Demand capacity mode charges per request and automatically scales to accommodate sudden bursts without provisioning capacity units in advance.\n\n## Why Other Options Are Wrong\n- Option A: Provisioned capacity with manual scaling requires planning and can throttle during spikes.\n- Option C: Provisioned capacity with auto-scaling still relies on defined targets and can lag during sudden bursts.\n- Option D: DAX is a cache that speeds reads but does not replace the need for capacity or handle write throughput directly.\n\n## Key Concepts\n- DynamoDB On-Demand\n- Pay-per-request pricing\n- Bursty workloads\n\n## Real-World Application\n- Startups facing unpredictable traffic patterns should adopt On-Demand to simplify operations and reduce throttling risk.\n","diagram":null,"difficulty":"intermediate","tags":["AWS","DynamoDB","certification-mcq","domain-weight-32"],"channel":"aws-dva","subChannel":"development","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T21:26:34.944Z","createdAt":"2026-01-12 21:26:35"},{"id":"aws-dva-development-1768253194383-2","question":"You operate an EKS cluster with mixed workloads and want nodes to scale automatically in response to pod scheduling needs. Which autoscaling approach is most suitable in AWS today?","answer":"[{\"id\":\"a\",\"text\":\"Karpenter\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"EKS Managed Node Groups with auto-scaling enabled\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Kubernetes Horizontal Pod Autoscaler\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Manual resizing of EC2 instances\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A is correct because Karpenter provides a modern, efficient autoscaler for Kubernetes on AWS that provisions only the necessary EC2 instances based on actual pod needs, reducing cost and improving scale responsiveness.\n\n## Why Other Options Are Wrong\n- Option B: EKS Managed Node Groups can autoscale but are less responsive than Karpenter for dynamic workloads.\n- Option C: Horizontal Pod Autoscaler scales pods, not cluster nodes, so it cannot alone adjust node capacity.\n- Option D: Manual resizing is slow and error-prone for dynamic workloads.\n\n## Key Concepts\n- Kubernetes cluster autoscaling\n- Karpenter vs Cluster Autoscaler\n- Node provisioning on AWS\n\n## Real-World Application\n- Use Karpenter in an EKS environment to support variable workloads with efficient node provisioning.\n","diagram":null,"difficulty":"intermediate","tags":["AWS","EKS","Karpenter","Kubernetes","certification-mcq","domain-weight-32"],"channel":"aws-dva","subChannel":"development","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T21:26:35.497Z","createdAt":"2026-01-12 21:26:35"},{"id":"aws-dva-development-1768253194383-3","question":"Your pods running on EKS need to access an S3 bucket with the least privilege. Which approach securely enables this without embedding credentials in the pods?","answer":"[{\"id\":\"a\",\"text\":\"Storing AWS credentials in pod environment variables\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"IAM roles for service accounts (IRSA) using an OpenID Connect identity provider\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Attach an EC2 instance profile to the worker nodes\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Store IAM user credentials in the repository used by the pod\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct because IRSA leverages an OIDC identity provider to bind a Kubernetes service account to an IAM role, allowing pods to securely assume the role and access AWS resources without hard-coded credentials.\n\n## Why Other Options Are Wrong\n- Option A: Storing credentials in environment variables risks exposure and leaks.\n- Option C: Node instance profiles grant access to the node, not scoped to specific pods, increasing blast radius.\n- Option D: IAM user credentials in a repo are insecure and violate best practices.\n\n## Key Concepts\n- IRSA (IAM Roles for Service Accounts)\n- OIDC with EKS\n- Least privilege access\n\n## Real-World Application\n- Use IRSA to grant fine-grained S3 access per application component without secret rotation concerns.\n","diagram":null,"difficulty":"intermediate","tags":["AWS","IRSA","EKS","S3","certification-mcq","domain-weight-32"],"channel":"aws-dva","subChannel":"development","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T21:26:35.679Z","createdAt":"2026-01-12 21:26:35"},{"id":"aws-dva-development-1768253194383-4","question":"For an application that stores database credentials and needs automatic rotation with minimal operational overhead, which AWS service best supports this requirement?","answer":"[{\"id\":\"a\",\"text\":\"SSM Parameter Store with manual rotation\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Secrets Manager with automatic rotation for the database\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Environment variables in Lambda configuration\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Storing credentials in a Git repository\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct because Secrets Manager provides built-in rotation for database credentials, integrates with RDS and other services, and minimizes exposure risk through automatic rotation.\n\n## Why Other Options Are Wrong\n- Option A: SSM Parameter Store can store secrets but lacks seamless automatic rotation for databases.\n- Option C: Environment variables are static and require manual secret management; rotation is not automatic.\n- Option D: Storing credentials in a Git repository is insecure and not suitable for secret management.\n\n## Key Concepts\n- Secrets Manager rotation\n- Automatic credential rotation\n- Secure secret storage\n\n## Real-World Application\n- Use Secrets Manager to rotate RDS credentials automatically and securely without operator intervention.\n","diagram":null,"difficulty":"intermediate","tags":["AWS","Secrets Manager","RDS","certification-mcq","domain-weight-32"],"channel":"aws-dva","subChannel":"development","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T21:26:35.859Z","createdAt":"2026-01-12 21:26:35"},{"id":"q-1025","question":"In a multi-account AWS DVA data platform, streaming IoT telemetry into Kinesis Data Firehose feeding an S3 data lake with Glue catalog. Data schemas evolve and backfills are needed without full reprocessing. Design an end-to-end approach for schema evolution, idempotent writes, and backfill using Glue Schema Registry, Iceberg on S3, and partition pruning. Include data validation, DLQ, and observability?","answer":"Use Glue Schema Registry for versioned schemas, write to Iceberg tables on S3 to support schema evolution and upserts with a deterministic key, implement idempotent writes via a normalized primary key","explanation":"## Why This Is Asked\nTests ability to design scalable, cross-account data pipelines with evolving schemas.\n\n## Key Concepts\n- Glue Schema Registry versioned schemas\n- Iceberg on S3 for upserts and schema evolution\n- Deterministic keys for idempotent writes\n- Partition-driven backfill without full reprocessing\n- DLQ and data quality observability\n\n## Code Example\n```python\n# Pseudocode: write with Iceberg upsert using key\ndef upsert(record):\n  key = record['device_id']\n  iceberg.upsert(table='lake.telemetry', key=key, record=record)\n```\n\n## Follow-up Questions\n- How do you partition backfills to minimize shards?\n- How would you enforce cross-account access controls for Lake Formation?\n","diagram":"flowchart TD\n  Ingest[Kinesis Data Streams] --> Validate[Glue Schema Registry]\n  Validate --> Catalog[(Glue Catalog/Iceberg)]\n  Catalog --> Backfill[Incremental Backfill by Partition]\n  Backfill --> Observability[DLQ & CloudWatch Logs]\n","difficulty":"advanced","tags":["aws-dva"],"channel":"aws-dva","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Instacart","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T19:37:01.899Z","createdAt":"2026-01-12T19:37:01.899Z"},{"id":"q-1268","question":"Design an AWS data lake pattern for multi-tenant analytics where each tenant's data sits under /tenants/{tenantId} in S3 and is exposed to Athena and QuickSight. How would you implement strict tenant isolation, least-privilege access, and automated policy-driven discovery and auditing using Lake Formation, IAM, CMKs, and SCPs? Include governance, testing, and performance considerations?","answer":"Store tenant data in S3 under /tenants/{tenantId} and enforce isolation with Lake Formation grants tied to per-tenant tables. Use per-tenant IAM roles mapped to a central admin for governance, apply S","explanation":"## Why This Is Asked\nTests practical mastery of multi-tenant data governance, isolation, and scalable access control in a real analytics stack. It also probes knowledge of Lake Formation, IAM role mapping, encryption strategy, and cross-account governance.\n\n## Key Concepts\n- Lake Formation granular permissions on per-tenant resources\n- Prefix-based data isolation in S3\n- IAM role mapping per tenant and centralized admin\n- SCPs for cross-account restriction\n- Per-tenant KMS CMKs and Lake Formation audit logs\n\n## Code Example\n\n```yaml\nResources:\n  TenantReadsGrant:\n    Type: AWS::LakeFormation::Grant\n    Properties:\n      DataLakePrincipal:\n        DataLakePrincipalIdentifier: arn:aws:iam::ACCOUNT:role/TenantReader\n      Permissions: [SELECT]\n      Resource:\n        TableResource:\n          DatabaseName: tenant_db\n          TableName: events\n```\n\n## Follow-up Questions\n- How would you test data isolation across tenants? \n- How would you onboard new tenants at scale while preserving least privilege?","diagram":"flowchart TD\n  A[TenantId] --> B[S3: /tenants/{TenantId}/data]\n  B --> C[Lake Formation grants on per-tenant tables]\n  C --> D[Athena/QuickSight]\n  D --> E[Audit logs]","difficulty":"advanced","tags":["aws-dva"],"channel":"aws-dva","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Oracle","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T07:33:46.616Z","createdAt":"2026-01-13T07:33:46.616Z"},{"id":"q-674","question":"You're building an AWS DVA data-analytics pipeline ingesting telemetry from 2000 devices/sec via Kinesis Data Streams. A Spark/Glue path processes windows and writes Parquet to S3; aggregation state in DynamoDB. Data loss or duplicates occur during retries and outages; costs spike at peak. Design a resilient, cost-efficient approach: shard sizing, processing path, idempotent writes, error handling with DLQ, and observability. What would you implement and why?","answer":"Scale by configuring 4 Kinesis shards to sustain ~4000 RPS; use a real-time analytics path (Kinesis Data Analytics or Spark) for windowed aggregation, then write Parquet to S3 via Glue streaming. Dedu","explanation":"## Why This Is Asked\n\nAssesses real-world ability to design a resilient streaming analytics pipeline on AWS, balancing throughput, cost, and correctness. Candidates must justify shard sizing, choice of processing path (KDA vs Glue streaming), idempotent writes, error handling, and observability.\n\n## Key Concepts\n\n- Streaming ingestion and scaling\n- Exactly-once semantics with DynamoDB\n- DLQ and backpressure\n- Observability and cost control\n\n## Code Example\n\n```javascript\n// Pseudocode: deduplicate by composite key and conditional writes\n```\n\n## Follow-up Questions\n\n- How would you migrate this to a multi-region setup?\n- How would you test failure modes and simulate burst traffic?","diagram":null,"difficulty":"intermediate","tags":["aws-dva"],"channel":"aws-dva","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Microsoft","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T14:45:55.246Z","createdAt":"2026-01-11T14:45:55.246Z"},{"id":"q-916","question":"In an AWS-based DVA pipeline ingesting telemetry from 2000 devices/sec via Kinesis Data Streams, with a Spark/Glue path writing Parquet to S3 and cataloged in Glue, design an end-to-end strategy for robust schema evolution, data validation, and partitioning that minimizes reprocessing and supports backfills. Include schema registry, idempotence, DLQ, and observability?","answer":"Adopt Glue Schema Registry with versioned Avro schemas; Spark Structured Streaming validates records against the latest compatible schema and writes Parquet to S3 partitioned by device/hour. Route inc","explanation":"## Why This Is Asked\nThe question probes practical UX for evolving data schemas in streaming pipelines, ensuring data quality and low reprocessing costs during changes.\n\n## Key Concepts\n- Glue Schema Registry and versioning\n- Schema compatibility strategies (backward/forward)\n- Spark Structured Streaming validation and checkpointing\n- Idempotent writes and deduplication stores\n- DLQ routing and backfill procedures\n- Observability with CloudWatch and Glue metrics\n\n## Code Example\n```javascript\n// Spark reads latest compatible Avro schema from Glue, writes Parquet partitioned by device/hour\ndf.write\n  .partitionBy(\"device_id\", \"hour\")\n  .format(\"parquet\").save(\"s3://bucket/data/\");\n```\n\n## Follow-up Questions\n- How would you handle backward-incompatible schema changes without downtime?\n- What metrics signal schema drift or data quality issues?","diagram":null,"difficulty":"intermediate","tags":["aws-dva"],"channel":"aws-dva","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","Snowflake","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T15:29:41.744Z","createdAt":"2026-01-12T15:29:41.744Z"},{"id":"aws-dva-security-1768213569922-0","question":"You are deploying a new application that stores sensitive data in an S3 bucket used by a single service role. To enforce least privilege, you want to ensure encryption at rest with a customer-managed CMK, block public access, and restrict PutObject to only the service role. Which configuration best achieves this?","answer":"[{\"id\":\"a\",\"text\":\"Enable SSE-KMS with a CMK on the bucket, attach a bucket policy that allows PutObject only from arn:aws:iam::123456789012:role/my-app-role, enable Block Public Access settings on the bucket, and disable ACLs.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Enable SSE-S3 for the bucket, attach a bucket policy that allows PutObject only from arn:aws:iam::123456789012:role/my-app-role, and enable Block Public Access settings.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Enable SSE-KMS with a CMK, attach a bucket policy that allows PutObject from any role in the account, and enable public read access.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Do not set a bucket policy; rely on IAM permissions to PutObject, and leave Block Public Access off.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA. The correct answer is A because it directly enforces encryption with a CMK, restricts PutObject to the specific role via bucket policy, and blocks public access via Block Public Access settings.\n\n## Why Other Options Are Wrong\n- Option B: It uses SSE-S3 and does not enforce CMK-based encryption, which is required per the scenario, and still allows potential misconfig if policy isn't strict.\n\n- Option C: Although CMK is used, it allows PutObject from any role, failing the least-privilege requirement, and lacks public access controls.\n\n- Option D: Relies on IAM permissions alone and omits bucket-level controls and encryption, increasing risk of public exposure.\n\n## Key Concepts\n- SSE-KMS CMK encryption\n- Bucket policies for resource-based access control\n- Block Public Access settings\n- ACLs vs bucket policies\n\n## Real-World Application\n- In production, apply a CMK and restrict access to a specific service role; disable ACLs and enforce public access block to prevent accidental public exposure.","diagram":null,"difficulty":"intermediate","tags":["AWS IAM","AWS S3","AWS KMS","Encryption","certification-mcq","domain-weight-26"],"channel":"aws-dva","subChannel":"security","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T10:26:09.924Z","createdAt":"2026-01-12 10:26:10"},{"id":"aws-dva-security-1768213569922-1","question":"In an AWS Organization with multiple accounts, a security team detects unusual API activity in a member account. What is the most scalable approach to detect, investigate, and respond to such threats across all accounts?","answer":"[{\"id\":\"a\",\"text\":\"Enable GuardDuty in the security account only, and route alerts to CloudWatch.\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Enable GuardDuty in all member accounts and configure centralized findings in the security account; enable a multi-account CloudTrail trail and Security Hub integration.\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Use IAM Access Analyzer to generate access reports for each account.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Enable CloudWatch Alarms on CloudTrail logs to trigger notifications.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nB. The correct answer is B because enabling GuardDuty in all accounts with centralized findings and a multi-account CloudTrail trail combined with Security Hub provides scalable, centralized threat detection and faster incident response.\n\n## Why Other Options Are Wrong\n- Option A: Only enabling in the security account misses detections in member accounts and is not scalable.\n\n- Option C: IAM Access Analyzer helps with permission analysis but does not provide ongoing threat detection across all accounts.\n\n- Option D: CloudWatch Alarms on CloudTrail are reactive and less comprehensive for broad threat detection compared with GuardDuty + Security Hub.\n\n## Key Concepts\n- GuardDuty multi-account deployment\n- Centralized findings in Security Hub\n- Multi-account CloudTrail\n- AWS Organizations\n\n## Real-World Application\n- For large organizations, deploy GuardDuty per account and centralize findings to enable rapid, coordinated response across the fleet.","diagram":null,"difficulty":"intermediate","tags":["AWS GuardDuty","AWS CloudTrail","AWS Security Hub","AWS Organizations","certification-mcq","domain-weight-26"],"channel":"aws-dva","subChannel":"security","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T10:26:10.439Z","createdAt":"2026-01-12 10:26:10"},{"id":"aws-dva-security-1768213569922-2","question":"To enforce configuration compliance across S3 buckets in a multi-account environment, which approach provides continuous, scalable evaluation and automatic remediation for public access misconfigurations?","answer":"[{\"id\":\"a\",\"text\":\"Enable AWS Config managed rules s3-bucket-public-read-prohibited and s3-bucket-public-write-prohibited across all accounts; configure AWS Config Remediation to automatically apply a non-public bucket policy.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Create an IAM policy that denies s3:PutObjectPublic across all buckets.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use AWS Security Hub to monitor buckets for public access.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use S3 Block Public Access settings at the account level only.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA. The correct answer is A because AWS Config managed rules provide continuous evaluation of bucket public access configuration across accounts, and with remediation configured, misconfigurations can be automatically corrected.\n\n## Why Other Options Are Wrong\n- Option B: A denial policy without continuous evaluation cannot fix existing misconfigurations or detect them promptly.\n\n- Option C: Security Hub aggregates findings but does not perform continuous configuration evaluation or automatic remediation.\n\n- Option D: Block Public Access helps guardrails but does not provide continuous evaluation or automated remediation across multiple accounts.\n\n## Key Concepts\n- AWS Config managed rules\n- S3 public access configuration\n- AWS Config Remediation\n- Cross-account governance\n\n## Real-World Application\n- Implement Config rules to continuously enforce non-public buckets and automatically remediate when non-compliant resources are detected.","diagram":null,"difficulty":"intermediate","tags":["AWS Config","AWS S3","IAM","certification-mcq","domain-weight-26"],"channel":"aws-dva","subChannel":"security","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T10:26:10.979Z","createdAt":"2026-01-12 10:26:11"},{"id":"aws-dva-security-1768275076190-0","question":"An S3 bucket in your AWS account contains sensitive data and is publicly accessible due to a legacy bucket policy. What is the most effective immediate control to block public access across the account and any future misconfigurations?","answer":"[{\"id\":\"a\",\"text\":\"Enable Block Public Access settings at both the account and bucket levels\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Delete the bucket and recreate it as private\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Apply a bucket policy that only allows specific IAM users to access the objects\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Move data to Glacier and delete the bucket\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct option is A: Enable Block Public Access settings at both the account and bucket levels. This centralizes and enforces blocking of public access across S3, reducing risk from legacy policies or ACL configurations.\n\n## Why Other Options Are Wrong\n- B: Deleting the bucket is disruptive and not a practical remediation for governance; it also risks data loss.\n- C: A restrictive bucket policy could still allow public access via ACLs or other misconfigurations; Block Public Access prevents this at the source.\n- D: Glacier is not a remediation for public access and does not address access controls; it hides data, not secures it.\n\n## Key Concepts\n- S3 Block Public Access\n- Least privilege and centralized enforcement\n- Misconfiguration risk from legacy policies\n\n## Real-World Application\n- Apply S3 Block Public Access at account and bucket levels across multiple buckets and review legacy policies to ensure no public access is possible.\n","diagram":null,"difficulty":"intermediate","tags":["S3","IAM","Security","BucketPolicy","Risk-Management","certification-mcq","domain-weight-26"],"channel":"aws-dva","subChannel":"security","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T03:31:16.191Z","createdAt":"2026-01-13 03:31:16"},{"id":"aws-dva-security-1768275076190-1","question":"An EC2 instance role currently grants s3:* on a bucket with the object paths that workload uses. To enforce least privilege for that role, which approach is best?","answer":"[{\"id\":\"a\",\"text\":\"Replace the role policy with a least-privilege policy that only allows the necessary actions (e.g., s3:ListBucket, s3:GetObject) and remove broad permissions\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Attach an additional broad AWS managed policy and rely on Deny to restrict access\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Keep the current policy and add an explicit Deny for all actions except the needed ones\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Create a new IAM user and assign it to the EC2 instance\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A is correct because it replaces the over-permissive policy with a tailored, least-privilege policy, aligning permissions with what the workload actually requires. \n\n## Why Other Options Are Wrong\n- B: Adding a broader policy and relying on Deny is ineffective and can be confusing; AWS evaluates allows before denies, and a broader policy increases risk.\n- C: Explicit Deny is not needed when you can simply limit the allowed actions; Deny can cause maintenance complexity and unexpected results.\n- D: Creating a separate IAM user for the EC2 instance is not applicable to instance roles and would complicate credential management.\n\n## Key Concepts\n- Least privilege for IAM roles\n- Policy scoping and s3:* minimization\n- Trust policy vs. permission policy separation\n\n## Real-World Application\n- Regularly review role permissions; implement least-privilege IAM roles for EC2 workloads and validate with IAM Access Analyzer.\n","diagram":null,"difficulty":"intermediate","tags":["IAM","STS","EC2","LeastPrivilege","S3","certification-mcq","domain-weight-26"],"channel":"aws-dva","subChannel":"security","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T03:31:16.712Z","createdAt":"2026-01-13 03:31:17"},{"id":"aws-dva-security-1768275076190-2","question":"To ensure continuous visibility and tamper-evidence of audit logs across multiple AWS accounts and regions, which configuration provides the strongest baseline?","answer":"[{\"id\":\"a\",\"text\":\"Enable CloudTrail in all regions and deliver logs to a single, immutable S3 bucket with cross-region replication\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Enable VPC Flow Logs in each VPC and store locally in each account\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use CloudWatch Logs to monitor logs on the respective accounts only\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Enable Config history only for the root account and rely on it for all regions\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A is correct because it centralizes logs, ensures immutability in an S3-based archive, and enforces cross-region replication for resilience and tamper-evidence across accounts.\n\n## Why Other Options Are Wrong\n- B: VPC Flow Logs are valuable but do not provide centralized, tamper-evident audit trails spanning accounts/regions.\n- C: CloudWatch Logs alone do not guarantee tamper-evidence or centralized storage for forensics.\n- D: Config history does not provide comprehensive, cross-account regional log integrity.\n\n## Key Concepts\n- CloudTrail multi-region delivery\n- Centralized, immutable logs in S3\n- Cross-account log aggregation\n\n## Real-World Application\n- Implement a multi-account logging architecture with a central log bucket and lifecycle policies, enabling security teams to review across regions and accounts.\n","diagram":null,"difficulty":"intermediate","tags":["CloudTrail","S3","Cross-Account","SecurityHub","KMS","certification-mcq","domain-weight-26"],"channel":"aws-dva","subChannel":"security","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T03:31:17.292Z","createdAt":"2026-01-13 03:31:17"},{"id":"aws-dva-security-1768275076190-3","question":"A public API behind CloudFront is experiencing bursts of automated traffic. Which single WAF configuration best mitigates this risk while minimizing false positives?","answer":"[{\"id\":\"a\",\"text\":\"Create and tune a rate-based rule to throttle IPs that exceed a threshold within a 5-minute window\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Whitelist all IPs temporarily to bypass WAF checks\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Disable WAF and rely on security groups only\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Increase CloudFront origin shield to maximum to absorb traffic\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A is correct because rate-based rules are designed to throttle abusive clients while minimizing false positives when thresholds are tuned appropriately.\n\n## Why Other Options Are Wrong\n- B: Whitelisting all IPs defeats the purpose of protection and exposes to risk.\n- C: Disabling WAF removes a crucial line of defense against automated traffic.\n- D: Origin Shield does not directly mitigate bot traffic; it helps with cache hit ratios, not request-level threat filtering.\n\n## Key Concepts\n- AWS WAF rate-based rules\n- Bot mitigation strategies\n- False positive risk management\n\n## Real-World Application\n- Implement rate-based rules and monitor logs to fine-tune thresholds, ensuring legitimate users arenâ€™t blocked.\n","diagram":null,"difficulty":"intermediate","tags":["WAF","CloudFront","DDoS","Bot-Mitigation","certification-mcq","domain-weight-26"],"channel":"aws-dva","subChannel":"security","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T03:31:17.475Z","createdAt":"2026-01-13 03:31:17"},{"id":"aws-dva-security-1768275076190-4","question":"You are operating an AWS EKS cluster. A workload runs with a container image that needs access to a subset of AWS services. Which approach provides the best security control by granting least privilege to that workload?","answer":"[{\"id\":\"a\",\"text\":\"Use IAM Roles for Service Accounts (IRSA) to bind a restricted IAM role to the Kubernetes service account\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Attach the cluster's node IAM role to the service account\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Store credentials in a Kubernetes Secret and mount them into the pod\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use a Kubernetes RoleBinding with cluster-admin privileges to the pod\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A is correct because IRSA binds a narrowly-scoped IAM role to a service account, enabling fine-grained permissions for the workload and avoiding broad node or cluster permissions.\n\n## Why Other Options Are Wrong\n- B: Node IAM role provides access at node level and is rarely restricted to a subset of services; it increases blast radius.\n- C: Storing credentials in Secrets does not follow best practices for short-lived, rotated credentials and is less auditable.\n- D: Cluster-admin privileges are excessive and endanger the entire cluster if compromised.\n\n## Key Concepts\n- IAM Roles for Service Accounts (IRSA)\n- Least privilege in Kubernetes workloads\n- Pod identity and credentials management\n\n## Real-World Application\n- Map service accounts to restricted IAM roles and validate with least-privilege checks during deployment pipelines.\n","diagram":null,"difficulty":"intermediate","tags":["EKS","Kubernetes","IRSA","IAM","ServiceAccounts","certification-mcq","domain-weight-26"],"channel":"aws-dva","subChannel":"security","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T03:31:17.656Z","createdAt":"2026-01-13 03:31:17"},{"id":"aws-dva-troubleshooting-1768242732899-0","question":"An application behind an Application Load Balancer intermittently returns 503 errors during peak traffic, and some targets are marked unhealthy even though the app logs show healthy responses. What is the most likely root cause and fix?","answer":"[{\"id\":\"a\",\"text\":\"Increase the health check interval in the target group.\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Change the target group health check path to a valid endpoint that returns 200 for all instances.\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Disable health checks on the target group.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Migrate all instances to a single Availability Zone.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nChange the target group health check path to a valid endpoint that returns 200 for all instances.\n\n## Why Other Options Are Wrong\n- Option A: Increasing health check interval delays detection of unhealthy instances and doesn't fix the root cause.\n- Option C: Disabling health checks hides real issues and can degrade availability.\n- Option D: Collapsing to a single AZ reduces redundancy and doesn't solve unhealthy targets.\n\n## Key Concepts\n- ALB target group health checks\n- Health check path availability and response 200\n- Impact of unhealthy targets on load balancing\n\n## Real-World Application\n- Implement a simple /health endpoint that returns 200 and is reachable from the ALB; verify by simulating load and observing target health in the console.","diagram":null,"difficulty":"intermediate","tags":["ALB","EC2","AutoScaling","CloudWatch","HealthChecks","certification-mcq","domain-weight-18"],"channel":"aws-dva","subChannel":"troubleshooting","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T18:32:12.901Z","createdAt":"2026-01-12 18:32:13"},{"id":"aws-dva-troubleshooting-1768242732899-1","question":"Your API uses DynamoDB with read capacity units provisioned. During sudden spikes, you see throttling and increased latency. What is the best approach to handle variable traffic while controlling costs?","answer":"[{\"id\":\"a\",\"text\":\"Permanently double the RCUs to 20k.\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Enable DynamoDB On-Demand capacity or Auto Scaling for RCUs/WCUs.\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Switch to an RDS database with read replicas.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Add a caching layer with ElastiCache; this alone fixes DynamoDB throttling.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nEnable DynamoDB On-Demand capacity or Auto Scaling for RCUs/WCUs.\n\n## Why Other Options Are Wrong\n- Option a: Permanently doubling RCUs is inflexible and costly for variable traffic.\n- Option c: RDS does not address DynamoDB throttling directly and adds operational complexity.\n- Option d: Caching helps reduce load but does not directly solve DynamoDB throttling without capacity changes; on-demand/autoscaling is the primary fix.\n\n## Key Concepts\n- DynamoDB capacity modes: provisioned vs on-demand\n- Auto Scaling for DynamoDB RCUs/WCUs\n- Throttling behavior under high throughput\n\n## Real-World Application\n- Implement on-demand or autoscaling to handle unpredictable traffic and maintain latency targets.","diagram":null,"difficulty":"intermediate","tags":["DynamoDB","AutoScaling","On-Demand","CloudWatch","certification-mcq","domain-weight-18"],"channel":"aws-dva","subChannel":"troubleshooting","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T18:32:13.426Z","createdAt":"2026-01-12 18:32:13"},{"id":"aws-dva-troubleshooting-1768242732899-2","question":"A web application behind an Application Load Balancer runs on T3 instances and experiences sustained high CPU usage during business hours, even with auto scaling. What is the most effective optimization?","answer":"[{\"id\":\"a\",\"text\":\"Move to a larger T3 instance size within the same family.\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Right-size to a more capable instance family (eg, C5 or M5) and adjust the scaling policy.\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Increase the EBS volume size to improve IOPS.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Disable autoscaling to avoid instance churn.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nRight-size to a more capable instance family (eg, C5 or M5) and adjust the scaling policy to match load.\n\n## Why Other Options Are Wrong\n- Option a: Within the same family may still be insufficient for sustained loads.\n- Option c: EBS IOPS won't reduce CPU bottlenecks.\n- Option d: Disabling autoscaling reduces agility to handle traffic spikes.\n\n## Key Concepts\n- Instance type selection by workload (compute vs memory)\n- Auto scaling policy tuning\n- CPU utilization as a scaling signal\n\n## Real-World Application\n- Evaluate workload patterns and migrate to a compute-optimized family with adjusted min/max replicas.","diagram":null,"difficulty":"intermediate","tags":["EC2","ALB","AutoScaling","ComputeOptimized","certification-mcq","domain-weight-18"],"channel":"aws-dva","subChannel":"troubleshooting","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T18:32:13.914Z","createdAt":"2026-01-12 18:32:13"},{"id":"aws-dva-troubleshooting-1768242732899-3","question":"Static assets stored in S3 and served via CloudFront experience frequent origin fetches, causing latency. Which action most directly improves CloudFront cache hits?","answer":"[{\"id\":\"a\",\"text\":\"Increase CloudFront distribution price class to cache more globally.\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Serve assets directly from S3 and bypass CloudFront.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Set Cache-Control max-age headers on S3 objects and configure CloudFront to respect origin cache headers.\",\"isCorrect\":true},{\"id\":\"d\",\"text\":\"Enable Lambda@Edge on every request to rewrite headers.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nSet Cache-Control max-age headers on S3 objects and configure CloudFront to respect origin cache headers.\n\n## Why Other Options Are Wrong\n- Option a: Price class doesn't affect cacheability.\n- Option b: Removing CloudFront increases origin calls; not caching benefits.\n- Option d: Lambda@Edge adds latency and complexity; not first fix.\n\n## Key Concepts\n- CloudFront caching behavior\n- Cache-Control headers and origin headers\n- S3 object metadata\n\n## Real-World Application\n- Add 1- to 1-day TTLs for static assets and verify cache-hit ratios in CloudFront.","diagram":null,"difficulty":"intermediate","tags":["CloudFront","S3","Caching","CDN","certification-mcq","domain-weight-18"],"channel":"aws-dva","subChannel":"troubleshooting","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T18:32:14.080Z","createdAt":"2026-01-12 18:32:14"},{"id":"aws-dva-troubleshooting-1768242732899-4","question":"An EKS deployment shows sporadic 429/503 errors under load; metrics-server lag delays Horizontal Pod Autoscaler scaling. What configuration best resolves this issue?","answer":"[{\"id\":\"a\",\"text\":\"Install metrics-server and configure HorizontalPodAutoscaler with a CPU target and sensible min/max replicas.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Remove metrics-server and rely on custom Prometheus metrics only.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Increase the Kubernetes API server timeout to 60 seconds.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Prewarm all pods by manually scaling up before expected traffic.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nInstall metrics-server and configure HorizontalPodAutoscaler with a CPU target and sensible min/max replicas.\n\n## Why Other Options Are Wrong\n- Option B: Prometheus metrics may be slower or not integrated; metrics-server is the standard source for HPA CPU metrics.\n- Option C: API server timeout does not address per-pod scaling latency.\n- Option D: Manual prewarming is brittle and not scalable.\n\n## Key Concepts\n- Metrics-server for Kubernetes HPA\n- HPA configuration and min/max replicas\n- Load-driven autoscaling in EKS\n\n## Real-World Application\n- Ensure HPA scales quickly enough to meet demand during traffic spikes.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","EKS","MetricsServer","HPA","certification-mcq","domain-weight-18"],"channel":"aws-dva","subChannel":"troubleshooting","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T18:32:14.244Z","createdAt":"2026-01-12 18:32:14"}],"subChannels":["deployment","development","general","security","troubleshooting"],"companies":["Discord","Goldman Sachs","Instacart","Microsoft","MongoDB","OpenAI","Oracle","Salesforce","Snap","Snowflake","Tesla"],"stats":{"total":35,"beginner":0,"intermediate":33,"advanced":2,"newThisWeek":35}}