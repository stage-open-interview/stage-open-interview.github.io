{"questions":[{"id":"q-1025","question":"In a multi-account AWS DVA data platform, streaming IoT telemetry into Kinesis Data Firehose feeding an S3 data lake with Glue catalog. Data schemas evolve and backfills are needed without full reprocessing. Design an end-to-end approach for schema evolution, idempotent writes, and backfill using Glue Schema Registry, Iceberg on S3, and partition pruning. Include data validation, DLQ, and observability?","answer":"Use Glue Schema Registry for versioned schemas, write to Iceberg tables on S3 to support schema evolution and upserts with a deterministic key, implement idempotent writes via a normalized primary key","explanation":"## Why This Is Asked\nTests ability to design scalable, cross-account data pipelines with evolving schemas.\n\n## Key Concepts\n- Glue Schema Registry versioned schemas\n- Iceberg on S3 for upserts and schema evolution\n- Deterministic keys for idempotent writes\n- Partition-driven backfill without full reprocessing\n- DLQ and data quality observability\n\n## Code Example\n```python\n# Pseudocode: write with Iceberg upsert using key\ndef upsert(record):\n  key = record['device_id']\n  iceberg.upsert(table='lake.telemetry', key=key, record=record)\n```\n\n## Follow-up Questions\n- How do you partition backfills to minimize shards?\n- How would you enforce cross-account access controls for Lake Formation?\n","diagram":"flowchart TD\n  Ingest[Kinesis Data Streams] --> Validate[Glue Schema Registry]\n  Validate --> Catalog[(Glue Catalog/Iceberg)]\n  Catalog --> Backfill[Incremental Backfill by Partition]\n  Backfill --> Observability[DLQ & CloudWatch Logs]\n","difficulty":"advanced","tags":["aws-dva"],"channel":"aws-dva","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Instacart","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T19:37:01.899Z","createdAt":"2026-01-12T19:37:01.899Z"},{"id":"q-1268","question":"Design an AWS data lake pattern for multi-tenant analytics where each tenant's data sits under /tenants/{tenantId} in S3 and is exposed to Athena and QuickSight. How would you implement strict tenant isolation, least-privilege access, and automated policy-driven discovery and auditing using Lake Formation, IAM, CMKs, and SCPs? Include governance, testing, and performance considerations?","answer":"Store tenant data in S3 under /tenants/{tenantId} and enforce isolation with Lake Formation grants tied to per-tenant tables. Use per-tenant IAM roles mapped to a central admin for governance, apply S","explanation":"## Why This Is Asked\nTests practical mastery of multi-tenant data governance, isolation, and scalable access control in a real analytics stack. It also probes knowledge of Lake Formation, IAM role mapping, encryption strategy, and cross-account governance.\n\n## Key Concepts\n- Lake Formation granular permissions on per-tenant resources\n- Prefix-based data isolation in S3\n- IAM role mapping per tenant and centralized admin\n- SCPs for cross-account restriction\n- Per-tenant KMS CMKs and Lake Formation audit logs\n\n## Code Example\n\n```yaml\nResources:\n  TenantReadsGrant:\n    Type: AWS::LakeFormation::Grant\n    Properties:\n      DataLakePrincipal:\n        DataLakePrincipalIdentifier: arn:aws:iam::ACCOUNT:role/TenantReader\n      Permissions: [SELECT]\n      Resource:\n        TableResource:\n          DatabaseName: tenant_db\n          TableName: events\n```\n\n## Follow-up Questions\n- How would you test data isolation across tenants? \n- How would you onboard new tenants at scale while preserving least privilege?","diagram":"flowchart TD\n  A[TenantId] --> B[S3: /tenants/{TenantId}/data]\n  B --> C[Lake Formation grants on per-tenant tables]\n  C --> D[Athena/QuickSight]\n  D --> E[Audit logs]","difficulty":"advanced","tags":["aws-dva"],"channel":"aws-dva","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Oracle","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T07:33:46.616Z","createdAt":"2026-01-13T07:33:46.616Z"},{"id":"q-674","question":"You're building an AWS DVA data-analytics pipeline ingesting telemetry from 2000 devices/sec via Kinesis Data Streams. A Spark/Glue path processes windows and writes Parquet to S3; aggregation state in DynamoDB. Data loss or duplicates occur during retries and outages; costs spike at peak. Design a resilient, cost-efficient approach: shard sizing, processing path, idempotent writes, error handling with DLQ, and observability. What would you implement and why?","answer":"Scale by configuring 4 Kinesis shards to sustain ~4000 RPS; use a real-time analytics path (Kinesis Data Analytics or Spark) for windowed aggregation, then write Parquet to S3 via Glue streaming. Dedu","explanation":"## Why This Is Asked\n\nAssesses real-world ability to design a resilient streaming analytics pipeline on AWS, balancing throughput, cost, and correctness. Candidates must justify shard sizing, choice of processing path (KDA vs Glue streaming), idempotent writes, error handling, and observability.\n\n## Key Concepts\n\n- Streaming ingestion and scaling\n- Exactly-once semantics with DynamoDB\n- DLQ and backpressure\n- Observability and cost control\n\n## Code Example\n\n```javascript\n// Pseudocode: deduplicate by composite key and conditional writes\n```\n\n## Follow-up Questions\n\n- How would you migrate this to a multi-region setup?\n- How would you test failure modes and simulate burst traffic?","diagram":null,"difficulty":"intermediate","tags":["aws-dva"],"channel":"aws-dva","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Microsoft","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T14:45:55.246Z","createdAt":"2026-01-11T14:45:55.246Z"},{"id":"q-916","question":"In an AWS-based DVA pipeline ingesting telemetry from 2000 devices/sec via Kinesis Data Streams, with a Spark/Glue path writing Parquet to S3 and cataloged in Glue, design an end-to-end strategy for robust schema evolution, data validation, and partitioning that minimizes reprocessing and supports backfills. Include schema registry, idempotence, DLQ, and observability?","answer":"Adopt Glue Schema Registry with versioned Avro schemas; Spark Structured Streaming validates records against the latest compatible schema and writes Parquet to S3 partitioned by device/hour. Route inc","explanation":"## Why This Is Asked\nThe question probes practical UX for evolving data schemas in streaming pipelines, ensuring data quality and low reprocessing costs during changes.\n\n## Key Concepts\n- Glue Schema Registry and versioning\n- Schema compatibility strategies (backward/forward)\n- Spark Structured Streaming validation and checkpointing\n- Idempotent writes and deduplication stores\n- DLQ routing and backfill procedures\n- Observability with CloudWatch and Glue metrics\n\n## Code Example\n```javascript\n// Spark reads latest compatible Avro schema from Glue, writes Parquet partitioned by device/hour\ndf.write\n  .partitionBy(\"device_id\", \"hour\")\n  .format(\"parquet\").save(\"s3://bucket/data/\");\n```\n\n## Follow-up Questions\n- How would you handle backward-incompatible schema changes without downtime?\n- What metrics signal schema drift or data quality issues?","diagram":null,"difficulty":"intermediate","tags":["aws-dva"],"channel":"aws-dva","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","Snowflake","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T15:29:41.744Z","createdAt":"2026-01-12T15:29:41.744Z"}],"subChannels":["general"],"companies":["Discord","Goldman Sachs","Instacart","Microsoft","MongoDB","OpenAI","Oracle","Salesforce","Snap","Snowflake","Tesla"],"stats":{"total":4,"beginner":0,"intermediate":2,"advanced":2,"newThisWeek":4}}