{"questions":[{"id":"q-1003","question":"Design a multi-region DR plan for a Cloud Run API that reads from Cloud SQL and writes results to Cloud Storage and BigQuery. Define RPO/RTO targets, cross-region replication strategy for Cloud SQL, data residency constraints, traffic failover through a global load balancer, and automated DR tests. Include monitoring, IAM least privilege, and post-failover reconciliation steps?","answer":"Two-region DR: primary in us-central1; standby in europe-west1. Cloud SQL cross-region replicas; continuous backups to Cloud Storage; BigQuery exported datasets replicated or periodically copied to st","explanation":"## Why This Is Asked\n\nTests DR planning across regions, data residency, and automated validation, emphasizing real-world trade-offs between RPO, RTO, cost, and regulatory constraints.\n\n## Key Concepts\n\n- Multi-region DR design\n- Cross-region Cloud SQL replication\n- Data residency and egress controls\n- Global load balancing and DNS failover\n- Automated DR testing and reconciliation\n\n## Code Example\n\n```javascript\n// Example: promote DR region\ngcloud sql instances failover mydb-us-central1\n```\n\n## Follow-up Questions\n\n- What metrics would you monitor during DR tests?\n- How would you handle ongoing writes during failover to maintain consistency?","diagram":"flowchart TD\n  A[User requests] --> LB[Global Load Balancer]\n  LB --> US[Cloud Run - us-central1]\n  US --> SQL_US[Cloud SQL - us-central1]\n  DR[DR standby - europe-west1] --> PREP[State sync]\n  PREP --> DR_RUN[Cloud Run - europe-west1]","difficulty":"advanced","tags":["gcp-cloud-architect"],"channel":"gcp-cloud-architect","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Netflix","Tesla","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T18:49:12.996Z","createdAt":"2026-01-12T18:49:12.996Z"},{"id":"q-1017","question":"Design a beginner-friendly ingestion workflow on GCP for daily CSV exports delivered via partner-signed URLs into a Cloud Storage bucket. Implement a Cloud Function (Python) triggered on finalization to validate, parse, and load aggregates into BigQuery, with structured Cloud Logging including a correlation_id. Outline per-project isolation (dev/stage/prod), idempotent replay, and a simple test plan?","answer":"Create a Cloud Function (Python) with a dedicated service account that has Storage Object Admin on the bucket, BigQuery DataEditor on the target dataset, and Cloud Logging rights. Validate the partner","explanation":"## Why This Is Asked\nThis tests practical ingestion design with GCP primitives, least-privilege IAM, idempotency, and observability in a beginner-friendly context. It covers per-project isolation and simple testing.\n\n## Key Concepts\n- Cloud Functions triggered by Cloud Storage events\n- IAM least privilege service accounts\n- idempotent upserts in BigQuery\n- structured logs with correlation_id\n- per-project isolation (dev/stage/prod)\n\n## Code Example\n```python\n# Python snippet for simple CSV parse\ndef parse_csv_to_rows(file_obj):\n    import csv\n    reader = csv.DictReader(file_obj)\n    for row in reader:\n        yield row\n```\n\n## Follow-up Questions\n- How would you handle schema drift in CSVs?\n- How would you test locally without deploying?","diagram":"flowchart TD\n  A[Partner signs URL] --> B[GCS bucket finalization]\n  B --> C[Cloud Function (Python)]\n  C --> D[BigQuery load (idempotent)]\n  C --> E[Cloud Logging with correlation_id]\n  D --> F[Observability dashboard]","difficulty":"beginner","tags":["gcp-cloud-architect"],"channel":"gcp-cloud-architect","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Snap","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T19:31:22.276Z","createdAt":"2026-01-12T19:31:22.276Z"},{"id":"q-1092","question":"Design a cross-tenant, multi-region data ingestion pipeline on Google Cloud to handle telemetry from partner apps. Data arrives as daily compressed NDJSON in per-tenant Cloud Storage buckets. Build end-to-end using Cloud Storage triggers or Pub/Sub, Dataflow (Beam) for parsing/transformations, and BigQuery with per-tenant datasets. Enforce least-privilege IAM, strict per-project isolation, Private Service Connect, data residency, auditable Cloud Logging, and idempotent replay with watermarking. Include architecture, data mapping, and a practical test plan?","answer":"Propose a cross-tenant, multi-region ingestion using per-tenant GCS storage, a Dataflow Beam pipeline for parsing and transformations, and per-tenant BigQuery datasets. Enforce least-privilege IAM, VP","explanation":"## Why This Is Asked\nExplores multi-tenant isolation, data residency, and secure ingestion at scale on GCP.\n\n## Key Concepts\n- Data residency and multi-region design\n- Least-privilege IAM and service accounts\n- Dataflow pipelines and watermarking\n- Private Service Connect and VPC Service Controls\n- Per-tenant datasets and auditing via Cloud Logging\n\n## Code Example\n```javascript\n// Dataflow pseudo skeleton\nimport apache_beam as beam\nimport json\n\ndef parse_ndjson(line):\n  return json.loads(line)\n\nwith beam.Pipeline() as p:\n  lines = p | 'Read' >> beam.io.ReadFromText('gs://bucket/tenant*/data.ndjson.gz')\n  parsed = lines | 'Parse' >> beam.Map(parse_ndjson)\n  parsed | 'Write' >> beam.io.WriteToBigQuery('project:dataset.table')\n```\n\n## Follow-up Questions\n- How would you validate idempotency across replays?\n- How to test IAM least-privilege and data residency policies?","diagram":"flowchart TD\n  A[Partner NDJSON uploads] --> B[Cloud Storage: per-tenant]\n  B --> C[Dataflow (Beam): parse/transform]\n  C --> D[BigQuery: per-tenant datasets]\n  D --> E[Cloud Logging & Monitoring]\n  B --> F[Private Service Connect / PSC to core services]\n  D --> G[BI Tools / dashboards]","difficulty":"intermediate","tags":["gcp-cloud-architect"],"channel":"gcp-cloud-architect","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Microsoft","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T22:25:21.070Z","createdAt":"2026-01-12T22:25:21.070Z"},{"id":"q-1195","question":"Design a beginner data-retention automation on GCP for a daily CSV export that lands in a shared Cloud Storage bucket and is ingested into a partitioned BigQuery table. Implement per-environment isolation (dev/stage/prod) by separate buckets and datasets. Create a Cloud Scheduler job that triggers a Cloud Function (Python) to apply 30-day retention on storage objects, prune BigQuery partitions, and log using Cloud Logging with a correlation_id. Outline validation tests and rollback plan?","answer":"Implement per-environment resources (dev, stage, prod) with separate buckets and datasets. Enable bucket lifecycle to auto-delete after 30 days. A Cloud Scheduler triggers a Python Cloud Function that","explanation":"## Why This Is Asked\nTests ability to design cross-service retention with observable, auditable changes across GCS and BigQuery using managed schedulers.\n\n## Key Concepts\n- Cloud Storage lifecycle rules\n- Cloud Scheduler and Cloud Functions (Python)\n- BigQuery table partition pruning\n- Cloud Logging with correlation_id for traceability\n- Per-environment isolation (dev/stage/prod)\n\n## Code Example\n```javascript\n# Python Cloud Function (skeleton)\nfrom google.cloud import storage, bigquery\nimport os\n\ndef retention(event, context):\n    bucket = os.environ['BUCKET']\n    table = os.environ['BQ_TABLE']\n    # compute cutoff, delete old objects, prune partitions, log\n```\n\n## Follow-up Questions\n- How would you test the dry-run mode and rollback strategy?\n- What failure modes would you validate (permissions, time zone, partition handling)?","diagram":"flowchart TD\n  A[Scheduler] --> B[Cloud Function]\n  B --> C[Storage Delete]\n  B --> D[BigQuery Partition Prune]\n  B --> E[Cloud Logging correlation_id]","difficulty":"beginner","tags":["gcp-cloud-architect"],"channel":"gcp-cloud-architect","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T04:44:57.166Z","createdAt":"2026-01-13T04:44:57.166Z"},{"id":"q-1233","question":"Design an advanced, per-tenant data lake on Google Cloud for a SaaS platform serving 100 enterprise customers. Ingest on‑prem JSON logs via Pub/Sub to regional Cloud Storage, and use Dataflow to write per‑tenant BigQuery datasets with CMEK; ensure data locality, least‑privilege IAM, and exfiltration controls via VPC Service Controls and Private Service Connect. Outline observability, auditability, and a test plan for IAM changes and retention?","answer":"Per-tenant isolation via separate BigQuery datasets with CMEK per tenant, and IAM conditions scoped to tenant resources. Ingest: on-prem JSON to Pub/Sub; Dataflow streaming to tenant partitions and re","explanation":"## Why This Is Asked\n\nTests end-to-end multi-tenant data lake design with compliance, locality, and advanced GCP controls. It probes isolation, security boundaries, and auditable access.\n\n## Key Concepts\n\n- Per-tenant data isolation via datasets and CMEK\n- IAM conditions and least privilege\n- Dataflow ingestion from Pub/Sub to regional storage\n- VPC Service Controls and Private Service Connect\n- Cloud Audit Logs and Cloud Logging observability\n- Retention, idempotency, and rollback strategy\n\n## Code Example\n\n```yaml\ntenant_id: TENANT_001\ndataset: tenant_TENANT_001\n```\n\n## Follow-up Questions\n\n- How would you scale to 1000 tenants?\n- How would you monitor cross-tenant access attempts?","diagram":"flowchart TD\n  A[On-prem logs] --> B[Pub/Sub]\n  B --> C[Dataflow]\n  C --> D[BigQuery (per-tenant)]\n  C --> E[GCS regional storage]\n  D --> F[Cloud Logging/Audit]","difficulty":"advanced","tags":["gcp-cloud-architect"],"channel":"gcp-cloud-architect","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Lyft","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T06:30:57.020Z","createdAt":"2026-01-13T06:30:57.020Z"},{"id":"q-1308","question":"Design a beginner-friendly, cost-aware data ingestion pipeline on GCP for a fleet of devices sending JSON events to Pub/Sub. Create per-environment isolation with separate projects, topics, and BigQuery datasets (dev/stage/prod). Use Dataflow for streaming processing into BigQuery, optimize costs with regional resources, and implement Cloud Billing budgets with alerts plus a simple rollback and test plan?","answer":"Propose separate GCP projects for dev/stage/prod, dedicated Pub/Sub topics and BigQuery datasets per environment, and a Dataflow streaming job that reads Pub/Sub and writes to a partitioned BigQuery t","explanation":"## Why This Is Asked\n\nTests the ability to design a practical, beginner-friendly, cost-aware streaming pipeline with clear environment separation and governance using core GCP services.\n\n## Key Concepts\n\n- Per-environment isolation (dev/stage/prod) via separate projects, topics, and datasets\n- Pub/Sub + Dataflow streaming ingestion\n- BigQuery best practices (partitioned tables, optional clustering)\n- Cost governance (Cloud Billing budgets and alerts)\n- Rollback and test strategy (idempotent processing, reconciliation, staging tests)\n\n## Code Example\n\n```yaml\nbilling_account: \"ACCT-XXXXXX\"\nbudgets:\n  - name: \"DevBudget\"\n    amount: 100\n    interval: monthly\n    enabled: true\n```\n\n## Follow-up Questions\n\n- How would you validate rollback in a staging environment prior to prod?\n- How would you adjust Dataflow autoscaling to meet SLAs while controlling cost?","diagram":null,"difficulty":"beginner","tags":["gcp-cloud-architect"],"channel":"gcp-cloud-architect","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T10:34:58.153Z","createdAt":"2026-01-13T10:34:58.153Z"},{"id":"q-878","question":"How would you implement a beginner-friendly, auditable deployment pipeline in Google Cloud for a Cloud Run app that reads from Cloud SQL and writes logs to Cloud Logging, ensuring least-privilege IAM, per-project isolation, and no public endpoints?","answer":"Create a dedicated deploy service account per project with minimal roles (roles/run.admin, roles/iam.serviceAccountUser) and grant it to Cloud Build to trigger Cloud Run revisions. Use Private Service","explanation":"## Why This Is Asked\nThis question tests practical knowledge of basic CI/CD in GCP with security and auditability for sensitive workloads.\n\n## Key Concepts\n- Least-privilege IAM across projects\n- Cloud Run, Cloud Build, Cloud SQL, Cloud Logging\n- Private Service Connect and internal networking\n- Cloud Audit Logs for governance\n\n## Code Example\n```javascript\n// Example: create SA and grant roles\ngcloud iam service-accounts create deploy-sa --display-name \"Deploy Service Account\"\ngcloud projects add-iam-policy-binding your-project-id --member \"serviceAccount:deploy-sa@your-project-id.iam.gserviceaccount.com\" --role \"roles/run.admin\"\n```\n\n## Follow-up Questions\n- How would you verify there are no public endpoints exposed to the internet for the deployed app?\n- Which logs and metrics would you route and store for auditability of deployments?","diagram":"flowchart TD\n  A[CI/CD Trigger] --> B[Cloud Build]\n  B --> C[Cloud Run Deployment]\n  C --> D[Private Service Connect to Cloud SQL]\n  D --> E[Cloud Logging & Cloud Audit Logs]\n  E --> F[Internal Load Balancer]","difficulty":"beginner","tags":["gcp-cloud-architect"],"channel":"gcp-cloud-architect","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Goldman Sachs","IBM"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T13:57:48.051Z","createdAt":"2026-01-12T13:57:48.052Z"},{"id":"q-907","question":"Design a private, regional data pipeline for a global fintech platform: events land in regional Pub/Sub topics, Dataflow performs streaming ETL, results stored in per-region BigQuery, and audit logs go to Cloud Logging. Enforce per-region IAM, least privilege, CMEK, Private Service Connect, and no public egress. Describe data flow, security controls, disaster recovery, and cost implications. How would you implement this pipeline?","answer":"Ingest regional events to regional Pub/Sub, process with Dataflow streaming templates, write results to regionally isolated BigQuery datasets encrypted with CMEK, and emit audit trails to Cloud Loggin","explanation":"## Why This Is Asked\nAssesses ability to design geo-aware, secure, cost-conscious GCP architectures for real-time pipelines with strict data residency.\n\n## Key Concepts\n- Regional data locality and isolation\n- Pub/Sub and Dataflow streaming integration\n- CMEK encryption and key management\n- Private connectivity (Private Service Connect) and no public egress\n- IAM least privilege and cross-project boundaries\n- DR strategies and observability\n\n## Code Example\n```python\n# Dataflow streaming skeleton (simplified)\nimport apache_beam as beam\nfrom apache_beam.options.pipeline_options import PipelineOptions\n\nopts = PipelineOptions(\n    streaming=True,\n    project='my-gcp-project',\n    runner='DataflowRunner',\n    temp_location='gs://my-temp-bucket/tmp'\n)\nwith beam.Pipeline(options=opts) as p:\n    (p\n     | 'Read' >> beam.io.ReadFromPubSub(topic='projects/PROJECT/topics/region-a')\n     | 'Parse' >> beam.Map(lambda x: x)  # parse logic here\n     | 'WriteToBQ' >> beam.io.WriteToBigQuery('region-a.dataset.table', mode='append')\n    )\n```\n\n## Follow-up Questions\n- How would you test regional failover and data residency constraints?\n- What monitoring dashboards and SLOs would you implement?","diagram":null,"difficulty":"intermediate","tags":["gcp-cloud-architect"],"channel":"gcp-cloud-architect","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Hugging Face","Instacart"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T14:46:52.185Z","createdAt":"2026-01-12T14:46:52.185Z"},{"id":"q-977","question":"In a beginner setup, you deploy a Cloud Run API behind Private Service Connect, with logs going to Cloud Logging and traces to Cloud Trace. Outline a practical observability plan: which metrics, logs, and traces to collect; how to build a useful dashboard; how to configure a low-noise alert for 5xx latency; and a simple test to validate instrumentation and alerting?","answer":"Instrument a Cloud Run API with: metrics (request count, latency, 5xx rate), structured logs with trace IDs, enabled Cloud Trace; a single Monitoring dashboard; a low-noise alert for p95 latency > 500","explanation":"## Why This Is Asked\n\nAssesses practical setup of observability for a basic Cloud Run service, focusing on end-to-end data flows, dashboards, and alerting to catch regressions early.\n\n## Key Concepts\n\n- Cloud Logging: structured logs with trace context\n- Cloud Monitoring: dashboards and alerting policies\n- Cloud Trace: distributed tracing for latency breakdown\n- Private Service Connect: private connectivity for isolation\n- Health checks and end-to-end tests\n\n## Code Example\n\n```javascript\n// Example instrumentation snippet for Cloud Logging\nconst {Logging} = require('@google-cloud/logging');\nconst logging = new Logging();\nconst log = logging.log('api-logs');\nconst metadata = {resource: {type: 'cloud_run_revision', labels: {service: 'my-api', revision: 'rev-1'}}, severity: 'INFO'};\nconst traceId = 'TRACE_ID';\nconst entry = log.entry(metadata, {message: 'request received', trace: `projects/PROJECT_ID/traces/${traceId}`});\nlog.write(entry);\n```\n\n## Follow-up Questions\n\n- How would you adjust the dashboard and alerting as traffic scales?\n- What lightweight sampling strategy would you apply to avoid alert fatigue?","diagram":"flowchart TD\n  CloudRun[Cloud Run API]\n  Logging[Cloud Logging]\n  Monitoring[Cloud Monitoring]\n  Tracing[Cloud Trace]\n  PSC[Private Service Connect]\n  Dashboard[Monitoring Dashboard]\n  Alert[Alert Policy]\n\n  CloudRun --> Logging\n  CloudRun --> Monitoring\n  CloudRun --> Tracing\n  PSC --> CloudRun\n  Dashboard --> Alert","difficulty":"beginner","tags":["gcp-cloud-architect"],"channel":"gcp-cloud-architect","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T17:40:49.986Z","createdAt":"2026-01-12T17:40:49.986Z"}],"subChannels":["general"],"companies":["Amazon","Anthropic","Cloudflare","Coinbase","Goldman Sachs","Hugging Face","IBM","Instacart","Lyft","Microsoft","NVIDIA","Netflix","PayPal","Robinhood","Snap","Snowflake","Tesla","Two Sigma"],"stats":{"total":9,"beginner":5,"intermediate":2,"advanced":2,"newThisWeek":9}}