{"questions":[{"id":"q-1029","question":"In a global IoT telemetry pipeline ingesting 100k events/s per region into ADLS Gen2 and Databricks Delta Lake, implement schema drift tolerant ingestion, end-to-end data lineage via Purview, RBAC, and cross-region DR replication. How would you architect the data contracts, partitioning, watermarking, retention, and governance to meet data residency and fault-tolerance requirements?","answer":"Architect a multi-region lakehouse: per-region ADLS Gen2 containers, Databricks Delta Lake with mergeSchema and cloudFiles.inferColumnTypes for schema evolution, partition by region/device_type/event_date for optimized queries, watermark-based streaming with exactly-once semantics, Unity Catalog for RBAC with fine-grained permissions, Purview for end-to-end lineage, and Azure Data Factory for cross-region DR replication. Implement data contracts through Delta Lake constraints and schema enforcement, retention policies via Delta Lake VACUUM and ADLS lifecycle management, and monitoring through Log Analytics and Azure Monitor alerts.","explanation":"## Why This Is Asked\nAssesses multi-region lakehouse design, schema drift handling, data governance, and disaster recovery in Azure at enterprise scale.\n\n## Key Concepts\n- Schema drift tolerant ingestion (Delta Lake mergeSchema, cloudFiles.inferColumnTypes)\n- Data lineage (Purview) and RBAC (Unity Catalog)\n- Cross-region replication and data residency\n- Partitioning strategy and watermarking for IoT streams\n- Data contracts, retention, and failure handling\n\n## Implementation Details\n```python\n# Auto schema evolution with Delta Lake\ndf = spark.readStream.format('cloudFiles') \\\n  .option('cloudFiles.format', 'json') \\\n  .option('cloudFiles.inferColumnTypes', 'true') \\\n  .option('cloudFiles.schemaLocation', '/mnt/schema/') \\\n  .load('/mnt/iot/raw/')\n\n# Write with schema evolution and partitioning\nquery = df.writeStream.format('delta') \\\n  .option('checkpointLocation', '/mnt/checkpoints/iot') \\\n  .option('mergeSchema', 'true') \\\n  .partitionBy('region', 'device_type', 'event_date') \\\n  .trigger(processingTime='30 seconds') \\\n  .start('/mnt/iot/processed/')\n\n# Unity Catalog RBAC setup\nCREATE CATALOG IF NOT EXISTS iot_catalog;\nCREATE SCHEMA IF NOT EXISTS iot_catalog.telemetry;\nGRANT USE CATALOG ON CATALOG iot_catalog TO `data-engineers`;\nGRANT USE SCHEMA ON SCHEMA iot_catalog.telemetry TO `data-engineers`;\nGRANT SELECT ON TABLE iot_catalog.telemetry.iot_events TO `analysts`;\n```\n\n## Data Contracts & Governance\n- Delta Lake constraints for data quality validation\n- Schema enforcement via Delta Lake schema registry\n- Purview integration for automated lineage tracking\n- Retention: 7 years hot (ADLS), 30 days Delta Lake VACUUM\n- Data residency: Region-specific containers with geo-replication disabled\n\n## DR Strategy\n- Azure Data Factory copy activities for cross-region sync\n- Active-passive pattern with automated failover\n- Point-in-time restore via Delta Lake time travel","diagram":"flowchart TD\n  A[IoT Devices] --> B[Event Hub / IoT Hub]\n  B --> C[ADLS Gen2 Region A Raw]\n  C --> D[Databricks Delta Lake Region A]\n  D --> E[Purview Lineage]\n  D --> F[Unity Catalog RBAC]\n  D --> G[Cross-Region Replication to Region B]\n  G --> H[DR ADLS Gen2 Region B]","difficulty":"advanced","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Twitter","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":["schema drift tolerant ingestion","delta lake mergeschema","cloudfiles.infercolumntypes","end-to-end data lineage","cross-region dr replication","data contracts","partitioning strategy","watermark-based streaming","unity catalog rbac","azure purview governance","exactly-once semantics","retention policies"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2026-01-17T04:45:30.593Z","createdAt":"2026-01-12T19:42:48.929Z"},{"id":"q-1043","question":"Design an end-to-end Azure data-ecosystem pipeline that ingests incremental changes from a MongoDB Atlas collection into Delta Lake on ADLS Gen2, preserving SCD Type 2 history for a customers dimension, and handling schema drift. Include data lineage to Azure Purview, late-arriving updates, and on-demand reprocessing without data loss. Which components and config would you choose, and why?","answer":"Use MongoDB Atlas Change Streams with Debezium to stream inserts/updates into Kafka, then Spark Structured Streaming consumes from Kafka and writes to Delta Lake on ADLS Gen2 with Delta schema evoluti","explanation":"## Why This Is Asked\nThis question tests practical design decisions for real-time CDC pipelines across Azure, including lineage, schema drift, and reprocessing.\n\n## Key Concepts\n- MongoDB Atlas Change Streams and Debezium for CDC\n- Delta Lake schema evolution and SCD Type 2\n- Spark Structured Streaming from Kafka to Delta Lake\n- Azure Purview for end-to-end lineage\n- Late-arriving data handling and replay strategies\n\n## Code Example\n```javascript\n// Pseudo-implementation illustrating flow (language-agnostic)\nconst inStream = Kafka.read(\"mongodb-change-stream\");\nDelta.write(inStream, \"/mnt/adls/delta/customers\");\n```\n\n## Follow-up Questions\n- How would you implement schema drift handling in Delta Lake?\n- How do you validate and monitor Purview lineage end-to-end?\n- What are failure modes and how would you recover from late data or CDC gaps?\n","diagram":null,"difficulty":"intermediate","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","NVIDIA","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T20:30:13.903Z","createdAt":"2026-01-12T20:30:13.903Z"},{"id":"q-1311","question":"Design an end-to-end incremental ELT pipeline on Azure that ingests 2 TB/day of nested JSON telemetry from Event Hubs into a Delta Lake on ADLS Gen2, then loads a star schema in Azure Synapse. Requirements: (1) schema drift tolerant writes and schema evolution, (2) upserts by a composite key (tenant_id, event_id), (3) end-to-end data lineage to Purview, (4) late-arriving data support via watermarking, (5) partitioning by region and day with file-size/compaction strategies. Which components and patterns would you use, and why? Compare Delta Lake MERGE vs Delete+Insert for upserts?","answer":"Use Event Hubs → Databricks Structured Streaming → Delta Lake on ADLS Gen2 with schema evolution enabled. Upsert into a Delta table on (tenant_id, event_id) using MERGE; apply a 5-minute watermark for","explanation":"## Why This Is Asked\nTests practical ability to architect an Azure data pipeline handling schema drift, incremental loads, and governance across a hybrid stack. It also probes trade-offs between MERGE and Delete+Insert for upserts and how to manage late data and file sizing in a real-world volume.\n\n## Key Concepts\n- Event Hubs, Databricks Structured Streaming, Delta Lake on ADLS Gen2\n- Schema evolution and drift tolerance for nested JSON\n- Upserts via MERGE vs Delete+Insert\n- Watermarking for late data, partitioning strategy\n- Data lineage with Purview, serving layer in Azure Synapse\n\n## Code Example\n```python\n# PySpark sketch for streaming write with MERGE via foreachBatch\ndef upsert_to_delta(microBatchDF, batchId):\n    microBatchDF.createOrReplaceTempView('updates')\n    spark.sql(\"\"\"\n      MERGE INTO delta_table AS t\n      USING updates AS s\n      ON t.tenant_id = s.tenant_id AND t.event_id = s.event_id\n      WHEN MATCHED THEN UPDATE SET *\n      WHEN NOT MATCHED THEN INSERT *\n    \"\"\")\n\nstreamingDF.writeStream.foreachBatch(upsert_to_delta).option('checkpointLocation','/chkpt/path').start('/delta/path')\n```\n\n## Follow-up Questions\n- How would you tune for high cardinality keys without hotspotting?\n- How do you validate lineage in Purview across pipelines and schemas?","diagram":null,"difficulty":"intermediate","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Google","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T10:37:25.505Z","createdAt":"2026-01-13T10:37:25.505Z"},{"id":"q-1326","question":"Design an end-to-end pipeline that streams user activity from Azure Event Hubs into Delta Lake on ADLS Gen2, supports schema evolution, and updates an SCD Type 2 customer dimension in Synapse. Include exactly-once guarantees, late-arriving data handling, idempotent sinks, and end-to-end data lineage via Purview in a hybrid estate. List components, data flows, and governance approach?","answer":"Use Spark Structured Streaming on Azure Databricks to read Event Hubs, write to Delta Lake on ADLS Gen2 with auto schema evolution. Apply MERGE INTO to a Type 2 customer dimension in Synapse for upser","explanation":"## Why This Is Asked\nTests production-grade streaming & lakehouse design: schema evolution, upserts, lineage, and hybrid governance.\n\n## Key Concepts\n- Delta Lake schema evolution on ADLS Gen2\n- Structured Streaming with exactly-once guarantees\n- MERGE INTO for SCD Type 2 in Synapse\n- Purview data lineage across on-prem and cloud\n- Hybrid lakehouse governance and security\n\n## Code Example\n```python\n# PySpark pseudo\nfrom delta.tables import DeltaTable\nsource_df = spark.readStream.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"...\").load()[...]\ndelta_path = \"/mnt/delta/customers\"\ndelta_table = DeltaTable.forPath(spark, delta_path)\n# streaming merge example (simplified)\ndelta_table.alias(\"t\").merge(\n  source_df.alias(\"s\"),\n  \"t.customer_id = s.customer_id\"\n).whenMatchedUpdate(set={\"name\":\"s.name\",\"address\":\"s.address\",\"end_date\":\"current_timestamp()\"})\n .whenNotMatchedInsert(values={\"customer_id\":\"s.customer_id\",\"name\":\"s.name\",\"start_date\":\"current_timestamp()\"})\n .execute()\n```\n\n## Follow-up Questions\n- How would you handle late-arriving events with out-of-order progress?\n- What changes would you make to support global data cataloging and cross-region replication?","diagram":"flowchart TD\n  A[Event Hubs] --> B[Databricks Spark Structured Streaming]\n  B --> C[Delta Lake on ADLS Gen2]\n  C --> D[Synapse SCD Type 2]\n  D --> E[Purview Lineage]\n  E --> F[Hybrid Serving Layer]","difficulty":"advanced","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T11:36:41.027Z","createdAt":"2026-01-13T11:36:41.028Z"},{"id":"q-1453","question":"You are starting a beginner-friendly Azure data engineer task: daily telemetry logs arrive in ADLS Gen2 under /raw/telemetry/ in mixed formats (CSV and nested JSON). Design an event-driven pipeline using Azure Data Factory that fires on blob creation, ingests files, flattens nested structures to a canonical schema, handles optional fields and few schema drift cases, and loads into a simple star schema in Azure Synapse Analytics. Include incremental loading with a watermark and basic data quality checks. What components and steps would you implement?","answer":"Trigger with blob-created events to start an ADF pipeline; Data Flow flattens nested JSON/CSV into canonical columns, applying defaults for missing fields and handling drift; load curated Parquet to /","explanation":"## Why This Is Asked\nThis question evaluates ability to design an event-driven ingest pipeline that handles multi-format data, schema drift, and incremental loads with a simple star schema.\n\n## Key Concepts\n- Event-driven ingestion with BlobCreated events\n- Data Flow for schema normalization and drift handling\n- Staging area and Parquet/Delta-lite storage\n- MERGE-based upserts into a star schema in Synapse\n- Incremental loading using a watermark\n- Basic data quality checks and monitoring\n\n## Code Example\n```sql\nMERGE INTO dw.dbo.FactTelemetry AS t\nUSING staging.FactTelemetry AS s\nON t.event_id = s.event_id\nWHEN MATCHED THEN UPDATE SET\n  t.value = s.value,\n  t.event_time = s.event_time\nWHEN NOT MATCHED THEN INSERT (event_id, value, event_time) VALUES (s.event_id, s.value, s.event_time);\n```\n\n## Follow-up Questions\n- How would you test idempotency of the MERGE against late-arriving data?\n- What changes would you make to handle more complex nested payloads without breaking existing pipelines?","diagram":"flowchart TD\n  A[Blob Created] --> B[ADF Trigger]\n  B --> C[Data Flow: Normalize]\n  C --> D[Staging: Parquet in /curated]\n  D --> E[MERGE into Star Schema in Synapse]\n  E --> F[Partition by date]","difficulty":"beginner","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T17:48:57.023Z","createdAt":"2026-01-13T17:48:57.023Z"},{"id":"q-1474","question":"Design a nightly Azure data pipeline that ingests delta updates from a SaaS source into ADLS Gen2, handles schema drift with automatic inference, and materializes a star schema in Azure Synapse. Ensure end-to-end data lineage, idempotent upserts, and reliable rollback after failures. Which services and patterns would you deploy, and how would you verify correctness and lineage end-to-end?","answer":"Use Azure Data Factory to orchestrate nightly ingests from the SaaS source into ADLS Gen2; Spark on Synapse performs drift-tolerant transforms with automatic schema inference and writes to a Delta tab","explanation":"## Why This Is Asked\nAssesses practical ability to design a robust Azure data pipeline: ingestion, drift handling, lineage, and reliability.\n\n## Key Concepts\n- Orchestration: Azure Data Factory\n- Drift handling: Spark schema inference\n- Storage and warehousing: ADLS Gen2 and Delta Lake on Synapse\n- Lineage: Purview\n- Reliability: MERGE upserts and replay semantics\n\n## Code Example\n```sql\nMERGE INTO dim_sales AS target\nUSING staging_sales AS src\nON target.sale_id = src.sale_id\nWHEN MATCHED THEN UPDATE SET target.amount = src.amount, target.date = src.date\nWHEN NOT MATCHED THEN INSERT (sale_id, amount, date) VALUES (src.sale_id, src.amount, src.date);\n```\n\n## Follow-up Questions\n- How would you monitor data quality and lineage across components?\n- How would you implement rollback and replay when a batch fails?","diagram":"flowchart TD\n  A[ADF] --> B[Ingest to ADLS Gen2]\n  B --> C[Spark (Synapse) drift-aware transform]\n  C --> D[Delta table in Synapse]\n  D --> E[Purview lineage]","difficulty":"intermediate","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Oracle","Plaid","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T18:48:47.638Z","createdAt":"2026-01-13T18:48:47.638Z"},{"id":"q-1524","question":"You operate a global event lake: multiple producers push JSON events into Event Hubs. Ingest to ADLS Gen2, apply drift-tolerant transformations, and publish a canonical star schema in Azure Synapse with incremental loads. Propose a production-ready architecture leveraging Data Factory, Databricks Delta Lake, and Purview; detail schema evolution, late-arriving data handling, data quality gates, and privacy masking?","answer":"Use Event Hubs for streaming ingestion into ADLS Gen2, Databricks Delta Lake for streaming+batch with schema evolution, and Azure Synapse for serving. Orchestrate with Data Factory; Purview for lineag","explanation":"## Why This Is Asked\nAssesses ability to design an end-to-end, scalable, governed data lake with real-time and batch components, plus practical handling of schema drift and privacy.\n\n## Key Concepts\n- Event Hubs ingestion and streaming pipelines\n- Delta Lake schema evolution and merge on write\n- Incremental loads and late-arriving data handling\n- End-to-end data lineage with Purview\n- Data quality gates and privacy masking strategies\n\n## Code Example\n```javascript\n// Pseudo-implementation: enable schema evolution during write\nfunction writeDelta(df, path){\n  df.write\n    .format('delta')\n    .option('mergeSchema','true')\n    .mode('append')\n    .save(path)\n}\n```\n\n## Follow-up Questions\n- How would you monitor lineage and quality across pipelines?\n- What tests would you add for schema evolution compatibility across producers?","diagram":"flowchart TD\n  A[Event Hubs] --> B[Databricks Delta Lake (ADLS Gen2)]\n  B --> C[Azure Synapse (Serving Layer)]\n  D[ADF Orchestration] --> B\n  E[Purview] --> B\n  B --> F[Quality Masks & Privacy UDFs]","difficulty":"advanced","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","Scale Ai","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T20:41:30.601Z","createdAt":"2026-01-13T20:41:30.601Z"},{"id":"q-1563","question":"Daily JSON feed arrives in ADLS Gen2. Design a beginner Data Factory pipeline to stage, flatten nested fields, validate data quality, and write a partitioned Parquet table in Azure Synapse. Handle minor schema drift by defaults and ignoring new fields. Which components and steps would you use?","answer":"I would implement a two-step Data Factory pipeline: (1) Copy activity to stage the JSON files with schema-drift awareness enabled; (2) Mapping Data Flow to flatten nested counterparty fields, validate data quality (price > 0 and quantity > 0), and write partitioned Parquet files to Azure Synapse.","explanation":"## Why This Is Asked\nAssesses practical Data Factory usage, incremental loading capabilities, and basic schema drift handling for real-world JSON feeds.\n\n## Key Concepts\n- Data Factory Copy activity combined with Mapping Data Flow\n- Schema drift handling, default values, and nested field flattening\n- Incremental loading via control table; Parquet partitioning in Synapse\n\n## Code Example\n```sql\nMERGE INTO dbo.Trades AS t\nUSING staging.Trades AS s\nON t.TradeDate = s.TradeDate AND t.TradeId = s.TradeId\nWHEN MATCHED THEN UPDATE SET t.Price = s.Price, t.Qty = s.Qty\nWHEN NOT MATCHED THEN INSERT (TradeDate, TradeId, Price, Qty)\nVALUES (s.TradeDate, s.TradeId, s.Price, s.Qty)\n```","diagram":null,"difficulty":"beginner","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","DoorDash","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T06:18:48.949Z","createdAt":"2026-01-13T21:48:38.152Z"},{"id":"q-1638","question":"Design a streaming ingestion from an on-prem ERP into ADLS Gen2 for real-time analytics. Use Delta Lake with Auto Loader-like ingestion, bronze-silver-gold data path, exact-once delivery, and schema-drift tolerant processing. The silver layer feeds a star schema in Azure Synapse. Ensure end-to-end lineage in Purview, data quality gates, and robust late-arriving data handling. Describe architecture, contracts, and rollback strategy?","answer":"Use Databricks Auto Loader to ingest streaming data into Delta bronze on ADLS Gen2; transform with schema-drift tolerant MERGE into silver; feed a star schema in Azure Synapse; capture lineage in Purv","explanation":"## Why This Is Asked\nTests ability to design streaming pipeline across Azure components with real-time data, schema drift, CDC-like behavior, and governance.\n\n## Key Concepts\n- Databricks Auto Loader, Delta Lake, mergeSchema\n- Bronze-Silver-Gold streaming path\n- Azure Purview for end-to-end lineage\n- Azure Synapse Analytics star schema serving\n- Late-arriving data handling, watermarking, checkpointing, rollback\n\n## Code Example\n```javascript\n// Bronze: streaming read (Auto Loader-like)\nconst bronze = spark.readStream\n  .format('cloudFiles')\n  .option('cloudFiles.format','json')\n  .option('cloudFiles.inferColumnTypes','true')\n  .load('abfss://container@storage.dfs.core.windows.net/bronze');\n\n// Silver: upsert into Delta with schema drift tolerance\nbronze.writeStream\n  .format('delta')\n  .option('checkpointLocation','/mnt/checkpoints/silver')\n  .start('/mnt/delta/silver');\n\nDeltaTable.forPath(spark, '/mnt/delta/silver').as('s').merge(\n  bronze.alias('b'), 's.id = b.id'\n).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute();\n```\n\n## Follow-up Questions\n- How would you verify end-to-end lineage across Purview and Synapse?\n- How would you validate schema drift scenarios and rollback correctness?","diagram":"flowchart TD\n  Bronze[Bronze Delta] --> Silver[Silver Delta]\n  Silver --> Gold[Azure Synapse Star Schema]\n  Purview[Purview] --> Silver\n  Bronze --> Purview","difficulty":"advanced","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","Snowflake","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T04:24:07.043Z","createdAt":"2026-01-14T04:24:07.043Z"},{"id":"q-1682","question":"Design an end-to-end near-real-time data pipeline for a global adtech dataset with mixed sources (on-prem SQL Server, Azure SQL, and SaaS APIs). Implement CDC, incremental loads, and schema drift tolerance, load into Delta Lake on Synapse, and enforce end-to-end lineage in Purview. Include data masking for PII, data quality gates, and a GitOps workflow for pipelines?","answer":"CDC across mixed sources using Data Factory, streaming to ADLS Gen2 Raw; transform via Databricks Spark with Delta Lake on Synapse; publish curated layer to serverless SQL. Purview provides end-to-end","explanation":"## Why This Is Asked\n\nAssesses ability to architect a heterogeneous, scale-out data lakehouse with real-time ingress, schema drift handling, governance, and privacy controls, plus an actionable CI/CD model.\n\n## Key Concepts\n\n- CDC from multi-source systems into ADLS Gen2\n- Delta Lake on Synapse for unified storage and ACID semantics\n- Purview for lineage, classification, and policy enforcement\n- PII masking in the processing layer (e.g., Spark UDFs)\n- Data quality gates (e.g., Great Expectations) integrated into pipelines\n- GitOps-driven CI/CD for reproducible deployments\n\n## Code Example\n\n```python\n# Spark masking example (pseudo)\nfrom pyspark.sql.functions import when, col\n\ndf = df.withColumn(\n    'ssn_masked',\n    when(col('ssn').isNotNull(), 'XXX-XX-' + col('ssn').substr( -4, 4 )).otherwise(None)\n)\n```\n\n## Follow-up Questions\n\n- How would you handle GDPR data Subject Rights requests in this pipeline?\n- What are the trade-offs between serverless SQL vs provisioned Spark pools for the curated layer?","diagram":null,"difficulty":"advanced","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T06:54:31.901Z","createdAt":"2026-01-14T06:54:31.901Z"},{"id":"q-1782","question":"Design a beginner-level Azure data ingestion task: In a fintech scenario, ingest daily transaction rows from an on-premises SQL Server into Azure Data Lake Storage Gen2 as Parquet files using Azure Data Factory. Include steps to handle date partitioning, security, and reliable runs. What would your pipeline look like, and what minimal code or configuration would you provide?","answer":"Proposed: An ADF pipeline ingesting daily transactions from on-prem SQL Server to ADLS Gen2 as Parquet. Steps: 1) linked services for on-prem SQL and ADLS Gen2, 2) dataset with a loadDate param, 3) Co","explanation":"## Why This Is Asked\nThis question tests practical data ingestion skills using Azure Data Factory, including linked services, parameterized datasets, copy activities with predicates, and basic monitoring. It mirrors fintech use cases where day-partitioned Parquet blobs enable efficient downstream queries.\n\n## Key Concepts\n- Azure Data Factory pipelines\n- Parquet sink and date partitioning\n- Self-hosted integration runtime\n- Copy activity performance and retries\n\n## Code Example\n```javascript\n// Example: pseudo-JSON/ARM-like config is expected, not full code\n```\n\n## Follow-up Questions\n- How would you handle schema drift?\n- How would you implement idempotent loads and error retries?\n","diagram":null,"difficulty":"beginner","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Coinbase"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T10:45:04.491Z","createdAt":"2026-01-14T10:45:04.491Z"},{"id":"q-1916","question":"You manage a multi-region analytics platform with data from Salesforce, Workday, and IoT devices. Data lands in ADLS Gen2; you need to apply policy-driven redaction for PII, support incremental loads, enforce schema drift tolerance, and publish end-to-end lineage to Azure Purview and downstream consumers (Power BI, Synapse). Propose an Azure-native pipeline design using Data Factory, Databricks Delta Live Tables, Purview, and Synapse; highlight design decisions, governance, and potential pitfalls?","answer":"Use Delta Live Tables on Databricks for streaming and batch, with auto-loader to ingest from ADLS Gen2, implement structured streaming with incremental watermarking; apply UDF-based redaction for PII ","explanation":"## Why This Is Asked\nThis question probes cross-service data integration, governance, and privacy controls in a realistic multi-region setup. It tests practical tradeoffs between streaming vs batch, and lineage coverage.\n\n## Key Concepts\n- Delta Live Tables, Auto Loader, schema drift handling, incremental loads\n- Azure Purview for lineage, RBAC, data masking and masking policies\n- Synapse external tables and Power BI consumption\n\n## Code Example\n```python\n# Databricks pseudo\nfrom delta.tables import DeltaTable\n# redaction example\ndef mask_pii(s):\n    return re.sub(r\"\\\\d{12}\", \"XXX-XX-XXXX\", s)\n```\n\n## Follow-up Questions\n- How would you adapt this for GDPR data deletion requests?\n- How do you validate lineage coverage across new sources?","diagram":"flowchart TD\nA[Data sources] --> B[ADLS Gen2 landing]\nB --> C[Databricks Delta Live Tables]\nC --> D[Azure Purview lineage]\nC --> E[Synapse external tables]","difficulty":"advanced","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","MongoDB","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T17:00:08.420Z","createdAt":"2026-01-14T17:00:08.420Z"},{"id":"q-1979","question":"In Azure, design a production-grade data pipeline for streaming ecommerce telemetry from Azure Event Hubs into ADLS Gen2, with downstream star schema in Synapse, enabling incremental loads, automatic schema drift handling, and end-to-end data lineage via Purview. Include details on Delta Lake usage, MERGE-based upserts, late-arriving data handling, and a minimal Spark code snippet for a MERGE with schema drift tolerance?","answer":"Architect a streaming-to-batch hybrid pipeline: Event Hubs to ADLS Gen2 raw, Databricks Delta Lake as canonical layer, and MERGE-based upserts into a Synapse star schema. Include Purview for end-to-en","explanation":"## Why This Is Asked\nThis question probes end-to-end data engineering skills: streaming ingestion, lakehouse architecture, dimensional modeling in Synapse, and governance. It also tests ability to implement schema evolution and data contracts while maintaining lineage.\n\n## Key Concepts\n- Delta Lake schema evolution and MERGE-based upserts\n- Data lineage with Purview across ADLS Gen2, Databricks, and Synapse\n- Late-arriving data handling with watermarking\n- Star schema design in Synapse for analytics\n- Integration of Event Hubs, ADLS Gen2, Databricks, and Purview\n- Testing and monitoring in production\n\n## Code Example\n```javascript\nfrom delta.tables import DeltaTable\n# pseudo-spark snippet illustrating MERGE with schema drift handling\nsrc_df = spark.readStream.format(\"json\").load(\"abfss://container@account.dfs.core.windows.net/input/\")\ndelta = DeltaTable.forPath(spark, \"/mnt/delta/fact_telemetry\")\ndelta.alias(\"t\").merge(\n  src_df.alias(\"s\"),\n  \"t.id = s.id\"\n).whenMatchedUpdate(set = {\"value\": \"s.value\", \"ts\": \"s.ts\"})\\\\n .whenNotMatchedInsertAll().execute()\n```\n\n## Follow-up Questions\n- How would you validate schema drift events and automate alerts?\n- How would you test idempotency of MERGE loads in a CI/CD pipeline?","diagram":"flowchart TD\n  A[Event Hubs] --> B[ADLS Gen2 Raw]\n  B --> C[Databricks Delta Lake]\n  C --> D[Synapse Star Schema]\n  C --> E[Purview Lineage]","difficulty":"advanced","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T19:34:09.515Z","createdAt":"2026-01-14T19:34:09.515Z"},{"id":"q-2102","question":"A batch of daily JSON click events arrives in ADLS Gen2 under /raw/events/ from an on‑prem source via Self-Hosted IR. Build a beginner Data Factory pipeline that (1) copies raw JSON to /staging/events/, (2) uses a schema-drift-tolerant mapping to normalize fields and cast ts to timestamp, (3) writes to a daily-partitioned Parquet table in Azure Synapse, and (4) loads incrementally by tracking max ts in a control table and filtering ts > last_ts. Include a basic data quality check for userId?","answer":"Use a Self-Hosted Integration Runtime to copy daily JSON files from `/raw/events/*.json` to `/staging/events/`. A Data Flow handles schema drift: map known fields, cast `ts` to timestamp, and tolerate new fields. Sink to a daily-partitioned Parquet table in Azure Synapse. Implement incremental loading by tracking the maximum timestamp in a control table and filtering for `ts > last_ts`. Add a basic data quality check to ensure `userId` is not null.","explanation":"Why This Is Asked\nTests practical skills in building a beginner-friendly end-to-end ELT pipeline on Azure: moving data from on-premises to the data lake, handling simple schema drift, and implementing incremental loads without over-engineering.\n\nKey Concepts\n- Self-Hosted Integration Runtime for on-premises connectivity\n- Copy Activity and Data Flow in Azure Data Factory\n- Schema drift tolerance and type casting in mapping data flows\n- Parquet sinks with daily partitioning\n- Incremental loads via control table tracking\n- Basic data quality checks for non-null key fields\n\nCode Example\n```javascript","diagram":"flowchart TD\n  A[Raw events in ADLS Gen2] --> B[Copy to staging]\n  B --> C[Data Flow: schema drift tolerant mapping]\n  C --> D[Parquet sink: partition by date(ts)]\n  D --> E[Incremental: last_ts control table]","difficulty":"beginner","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","NVIDIA","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T05:00:21.037Z","createdAt":"2026-01-15T02:16:32.970Z"},{"id":"q-2378","question":"Design a scalable pipeline to ingest daily customer event logs from an on-prem Kafka cluster into Azure Data Lake Gen2, apply schema drift tolerant transformations, and maintain an SCD Type 2 star schema in Azure Synapse, while providing end-to-end data lineage via Purview. Which Azure services and patterns would you use, and how would you implement incremental loads?","answer":"Ingest daily event logs from on‑prem Kafka into ADLS Gen2 via ADF or Dataflow, store as Parquet, then use Databricks Delta Lake to implement SCD Type 2 within a star schema in Azure Synapse. Use Purvi","explanation":"## Why This Is Asked\nThis question tests real-world Azure data pipelines with hybrid sources, schema drift, incremental loads, and lineage, aligning with roles at Snowflake, DoorDash, and Lyft.\n\n## Key Concepts\n- Ingest from on-prem Kafka to ADLS Gen2 with incremental offsets\n- Delta Lake SCD Type 2 and schema evolution\n- Star schema design in Azure Synapse\n- End-to-end data lineage with Purview across hybrid estate\n- Secure, scalable access via managed identities and RBAC\n\n## Code Example\n```javascript\n// Pseudo-Spark-like pseudocode for incremental load and SCD2 merge\nval source = spark.read.format(\"parquet\").load(\"abfss://landing@storage.dfs.core.windows.net/events/\")\nval target = spark.read.table(\"analytics.star_fact\")\nval merged = target.alias(\"t\").merge(\n  source.alias(\"s\"), \"t.id = s.id\"\n).whenMatchedUpdate(set = mapFromColumns(\"t\", \"s\"))\n  .whenNotMatchedInsertAll()\n```\n\n## Follow-up Questions\n- How would you handle late-arriving or out-of-order events in the stream?\n- How would you validate lineage accuracy across Purview and ensure changes propagate to downstream analytics?\n","diagram":null,"difficulty":"intermediate","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Lyft","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T15:43:20.714Z","createdAt":"2026-01-15T15:43:20.714Z"},{"id":"q-2495","question":"Ingest JSON telemetry from multiple partners into ADLS Gen2 and upsert into a Delta Lake on Synapse while preserving end-to-end lineage and data quality. Outline an Azure-based architecture using IoT Hub/Event Hubs, Data Factory or Synapse pipelines, Databricks, Purview, and Delta Lake features to handle schema drift, late data, and incremental upserts. Include security, cost considerations, and observability?","answer":"Ingest JSON telemetry via IoT Hub/Event Hubs into ADLS Gen2 as raw. Use Databricks Spark with schema drift tolerant transforms, then write curated data to Delta Lake on Synapse and use MERGE for incre","explanation":"## Why This Is Asked\nTests end-to-end Azure data lakehouse skills: streaming ingestion, schema drift, Delta Lake upserts, data governance, and observability. It stresses hybrid data governance via Purview and practical security/cost tradeoffs.\n\n## Key Concepts\n- Ingestion: IoT Hub/Event Hubs with ADLS Gen2 landing zones\n- Processing: Databricks Spark with schema evolution\n- Storage: Delta Lake on Synapse with MERGE\n- Governance: Purview lineage and data catalog\n- Quality/Observability: expectations, monitoring, RBAC\n\n## Code Example\n```python\n# PySpark sketch: read raw, infer schema, write delta, MERGE on upsert\nraw = spark.read.json('/mnt/raw/telemetry')\ncurated = raw.select('deviceId','ts','metrics.*')\ncurated.write.format('delta').mode('append').save('/delta/telemetry')\n# MERGE example (pseudo)\ndeltaTable = DeltaTable.forPath(spark, '/delta/telemetry')\ndeltaTable.alias('t').merge(source=curated.alias('s'), condition='t.id = s.id').whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n```\n\n## Follow-up Questions\n- How would you implement late-arriving data handling?\n- How would you test/validate lineage across layers in Purview?","diagram":"flowchart TD\n  Ingest[/IoT Hub/Event Hubs/] --> RAW[ADLS Gen2 Raw]\n  RAW --> SPARK[Databricks Spark]\n  SPARK --> DELTA[Delta Lake on Synapse]\n  DELTA --> PURVIEW[Purview Lineage]\n  DELTA --> QA[Quality Checks]","difficulty":"intermediate","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["OpenAI","Plaid","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T20:36:20.587Z","createdAt":"2026-01-15T20:36:20.588Z"},{"id":"q-2529","question":"Design an Azure data engineering pipeline for streaming IoT telemetry from store sensors into ADLS Gen2 and Synapse, with schema drift tolerant transformations, incremental loads, and end-to-end data lineage. Use Event Hubs to ADLS Gen2, Delta Lake in Synapse, and Purview for lineage. Address late data, schema evolution, security, and cross-region replication. What services and data contracts would you implement, and how would you validate correctness?","answer":"Ingest telemetry from Event Hubs using Spark Structured Streaming in Synapse, writing to a Delta Lake raw zone on ADLS Gen2. Enable schema auto-merge for evolution, use watermarks to handle late data, then apply incremental transformations through curated zones. Implement Purview for end-to-end lineage, enforce RBAC and Key Vault for security, and configure cross-region replication with Azure Geo-Redundant Storage. Validate correctness through data quality rules, schema validation tests, and end-to-end pipeline monitoring.","explanation":"## Why This Is Asked\nInterview context evaluates streaming design, Delta Lake, and governance integration in Azure.\n\n## Key Concepts\n- Event Hubs, Spark Structured Streaming\n- Delta Lake autoMerge, schema evolution\n- Purview for lineage and data catalog\n- RBAC, Key Vault security\n- Late data handling and end-to-end validation\n\n## Code Example\n```javascript\n// Pseudo Spark streaming snippet (Scala/Python-like)\nconst df = spark.readStream.format(\"eventhubs\").option(\"startingPosition\",\"latest\").load()\nconst curated = df.selectExpr(\"cast(body as string) as payload\")\ncurated.writeStream.format(\"delta\").option(\"checkpointLocation\",\"/checkpoints\").start()\n```","diagram":"flowchart TD\n  A[Event Hubs] --> B[ADLS Gen2 Raw]\n  B --> C[Delta Lake (Synapse)]\n  C --> D[Purview Lineage]\n  C --> E[Curated Tables]\n  E --> F[BI/Analytics]\n","difficulty":"intermediate","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Hashicorp","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T05:41:55.547Z","createdAt":"2026-01-15T21:39:09.922Z"},{"id":"q-2605","question":"Design an end-to-end real-time ingestion and analytics pipeline for a live market tick feed (timestamp, symbol, price, volume) arriving from multiple brokers at high throughput. Ingest to ADLS Gen2 as Delta Lake, with schema drift tolerance and automatic schema evolution, and deliver upserts to a central ticks table. Ensure exactly-once streaming, late-arriving data handling, end-to-end data lineage via Purview, and scalable partitioning by date and symbol. Outline components (Event Hubs, Databricks, Delta Lake, Purview) and provide a workable configuration sketch?","answer":"Ingest tick data from brokers into Azure Event Hubs, consume with Databricks Structured Streaming, and write to Delta Lake on ADLS Gen2 with schema evolution enabled. Use MERGE for upserts, a watermark for late-arriving data, and configure exactly-once semantics. Enable Purview integration for end-to-end lineage and partition by date/symbol for scalability.","explanation":"## Why This Is Asked\nTests real-time Azure data-engineering skills with schema drift, exactly-once semantics, and lineage.\n\n## Key Concepts\n- Streaming ingestion (Event Hubs)\n- Delta Lake schema evolution\n- Idempotent upserts (MERGE)\n- Late data handling (watermarks)\n- Data lineage (Purview)\n\n## Code Example\n```javascript\n// pseudo-config illustrating options, not runnable\n```\n\n## Follow-up Questions\n- How would you validate end-to-end latency?\n- What failure modes and compensating actions for late data?","diagram":null,"difficulty":"advanced","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Slack","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T05:02:41.068Z","createdAt":"2026-01-16T02:33:46.893Z"},{"id":"q-2625","question":"Daily on-prem JSON event files arrive; design a beginner ELT pipeline using Data Factory that ingests JSON, enforces a basic schema, converts to Parquet, partitions by event_date, and loads a simple star schema into Azure Synapse. Explain how you’d implement basic data lineage and a 7-day retention policy, without advanced schema drift handling?","answer":"Use Data Factory with a self-hosted integration runtime to pull daily on-prem JSON logs into ADLS Gen2 staging, then a Data Flow to enforce a basic schema and flatten objects, write as Parquet partiti","explanation":"## Why This Is Asked\nInterview context explains end-to-end data movement from on-prem to ADLS Gen2, then to Synapse, plus basic governance and lifecycle considerations.\n\n## Key Concepts\n- On-prem to cloud data ingestion with Data Factory\n- JSON to Parquet conversion and partitioning by date\n- Star schema loading into Azure Synapse\n- Basic data lineage with Purview\n- Simple retention via ADLS lifecycle\n\n## Code Example\n```sql\n-- Skeleton for star schema loading\nCREATE TABLE dim_customer (...);\nCREATE TABLE dim_product (...);\nCREATE TABLE fact_orders (...);\n```\n\n## Follow-up Questions\n- How would you adapt for evolving JSON schemas?\n- How would you monitor ingestion reliability and latency?","diagram":"flowchart TD\n  A[On-prem JSON] --> B[Self-hosted IR]\n  B --> C[ADLS Gen2 (staging)]\n  C --> D[Data Flow Parquet]\n  D --> E[Azure Synapse (Star schema)]\n  E --> F[Purview lineage]\n  F --> G[ADLS Lifecycle 7d]","difficulty":"beginner","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Goldman Sachs","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T04:10:40.457Z","createdAt":"2026-01-16T04:10:40.457Z"},{"id":"q-2669","question":"Design an Azure streaming data pipeline for a global fleet of IoT devices sending JSON telemetry to Event Hubs, with data landing in ADLS Gen2 and a Delta Lake lakehouse. Requirements: sub-5 second ingestion latency, end-to-end latency <15 seconds for dashboards, automatic schema evolution, idempotent upserts, and end-to-end lineage with Purview. Outline components, dataflow, and a minimal Delta table evolution example?","answer":"Use Event Hubs as ingress, Databricks Structured Streaming to Delta Lake on ADLS Gen2. Bronze raw ingest, Silver with schema-evolution via mergeSchema, and upserts via MERGE INTO. Enable checkpoints, ","explanation":"## Why This Is Asked\nThis probes implementing a robust streaming lakehouse with schema drift handling, governance, and cost control across regions.\n\n## Key Concepts\n- Event Hubs ingress, Delta Lake on ADLS Gen2\n- Structured Streaming, mergeSchema for evolution, MERGE INTO for upserts\n- Checkpointing, watermarks, idempotent loads\n- Purview lineage and Azure RBAC\n\n## Code Example\n```python\n# PySpark pseudo\nbronze = (spark.readStream.format('eventhubs').options(**ehConf).load())\n# parse, then write bronze\nbronze.writeStream.format('delta').option('checkpointLocation','/cp/bronze').start('/deltalake/bronze')\n\nsilver = spark.readStream.format('delta').load('/deltalake/bronze')\n# transform and upsert into silver with schema evolution\nsilver.writeStream.format('delta').option('checkpointLocation','/cp/silver').option('mergeSchema','true').start('/deltalake/silver')\n```\n\n## Follow-up Questions\n- How would you implement late-arrival handling and data quality checks in this stack?\n- What monitoring and alerting would you add for schema drift and job failures?","diagram":"flowchart TD\n  EH[Event Hubs] --> Bronze[Bronze Delta]\n  Bronze --> Silver[Silver Delta (mergeSchema)]\n  Silver --> Dash[Dashboards/BI]\n  Purview[Purview] --> Lineage[Lineage tracking]","difficulty":"intermediate","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Discord","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T05:52:01.037Z","createdAt":"2026-01-16T05:52:01.038Z"},{"id":"q-2680","question":"Design an end-to-end Azure data platform to ingest per-tenant e-commerce clickstream data from an on-prem SFTP into ADLS Gen2, handle schema drift, and deliver per-tenant analytics in Synapse while preserving end-to-end lineage in Purview and enforcing RBAC/column masking. Implement incremental loads, late-data handling, and cost/perf tradeoffs between serverless vs dedicated pools?","answer":"Ingest per-tenant data from on-prem SFTP into ADLS Gen2. Use Spark-based ETL to apply schema-drift tolerant transforms and write Delta Lake tables. Enforce per-tenant RBAC and column masking in Synaps","explanation":"## Why This Is Asked\n\nTests ability to design a scalable, governed lakehouse with multi-tenant isolation, schema drift handling, incremental loads, and integration with Purview and Synapse. Also probes trade-offs between serverless and dedicated pools.\n\n## Key Concepts\n\n- Delta Lake on ADLS Gen2\n- Synapse RBAC and dynamic masking\n- Purview data lineage\n- SFTP ingestion and Spark ETL\n- Incremental loads and late-arrival handling\n\n## Code Example\n\n```python\nfrom delta.tables import DeltaTable\n# sample MERGE snippet demonstrating upsert\ndelta_table = DeltaTable.forPath(spark, \"abfss://...@container.dfs.core.windows.net/tenant.delta\")\nsrc = spark.read.parquet(\"abfss://.../incoming/tenant/\")\n\ndelta_table.alias(\"t\").merge(\n  src.alias(\"s\"),\n  \"t.id = s.id\"\n).whenMatchedUpdate(set={\"t.value\": \"s.value\"}).whenNotMatchedInsertAll().execute()\n```\n\n## Follow-up Questions\n\n- How would you audit lineage across on-prem and cloud landing zones?\n- What are cost controls for large multi-tenant workloads?\n- How to validate schema drift and auto-infer schemas safely?\n","diagram":"flowchart TD\n  A[On-prem SFTP] --> B[ADF/ETL]\n  B --> C[ADLS Gen2 Delta Lake]\n  C --> D[Azure Synapse RBAC/Masking]\n  D --> E[Purview Lineage]\n  E --> F[Consumption Layer]","difficulty":"advanced","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Coinbase","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T06:53:36.282Z","createdAt":"2026-01-16T06:53:36.282Z"},{"id":"q-2712","question":"Design a global Azure data lakehouse for real-time fraud analytics. Ingest streaming transaction events from on-prem ERP into ADLS Gen2, apply schema-drift tolerant transformations, and store as Delta Lake tables with governance via Unity Catalog. Implement incremental loads with CDC and watermark, ensure end-to-end lineage via Purview, and support multi-region reads with geo-replication. Which components and patterns would you choose, and how would you configure them to balance latency, cost, and compliance?","answer":"CDC from on-prem ERP to Event Hubs; Databricks Structured Streaming reads Event Hubs, performs schema-evolution-aware transformations, and writes to Delta Lake on ADLS Gen2. Use Unity Catalog for fine","explanation":"Why This Is Asked\n\nThis question probes knowledge of end-to-end Azure data lakehouse architectures, focusing on real-time ingestion, schema drift handling, and governance at scale across regions.\n\nKey Concepts\n\n- CDC from on-prem systems via Event Hubs or Debezium\n- Spark Structured Streaming and Delta Lake on ADLS Gen2 with schema evolution\n- Unity Catalog for access control and governance\n- Purview for lineage and data governance\n- Watermarking, incremental upserts, and state management\n- Geo-replication and multi-region reads\n\nCode Example\n\n```javascript\n// Pseudocode: read from Event Hubs, parse, upsert to Delta Lake\nconst spark = SparkSession.builder().getOrCreate();\nlet raw = spark.readStream\n  .format('eventhubs')\n  .option('startingOffsets', 'latest')\n  .load();\nlet parsed = raw.selectExpr(\"CAST(body AS STRING) AS json\").selectExpr(\"from_json(json, <schema>) AS data\").select('data.*');\nparsed.writeStream()\n  .format('delta')\n  .option('checkpointLocation', '/checkpoints/transactions')\n  .option('path', 'abfss://<container>@<account>.dfs.core.windows.net/transactions')\n  .outputMode('append')\n  .start();\n```\n\nFollow-up Questions\n\n- How would you handle late-arriving data and schema drift in production?\n- What are the trade-offs between Unity Catalog vs Purview for lineage in this setup?\n- How would you monitor SLAs for latency and data freshness across regions?","diagram":null,"difficulty":"advanced","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Google","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T07:45:28.158Z","createdAt":"2026-01-16T07:45:28.159Z"},{"id":"q-2741","question":"Scenario: A REST API provides hourly weather data as JSON. Ingest to ADLS Gen2 and load a simple star schema in Synapse. Outline a beginner Azure Data Factory pipeline that calls the API hourly, lands raw JSON under /data/raw/weather/YYYY/MM/DD/HH/, normalizes to fixed fields (date, location, temp, humidity, wind, precip) with nulls for missing, writes Parquet partitioned by date, and validates via a nightly row-count check. Include components and steps?","answer":"Design an Azure Data Factory pipeline that calls the hourly REST API, lands raw JSON under /data/raw/weather/YYYY/MM/DD/HH/, uses a Data Flow to normalize to fixed fields (date, location, temp, humidi","explanation":"## Why This Is Asked\nTests practical REST ingestion, schema variation handling, and end-to-end flow from landing to a simple star schema with basic validation.\n\n## Key Concepts\n- REST ingestion with Data Factory Web activity\n- Data Flow for field mapping and type casting\n- Parquet partitioning in ADLS Gen2\n- Star schema design in Synapse\n- Nightly data quality validation\n\n## Code Example\n```python\n# Mapping sketch for Data Flow (conceptual)\nmap_weather = lambda rec: {\n  'date': rec.get('date'),\n  'location': rec.get('location'),\n  'temp': float(rec['temp']) if rec.get('temp') is not None else None,\n  'humidity': float(rec['humidity']) if rec.get('humidity') is not None else None,\n  'wind': float(rec['wind']) if rec.get('wind') is not None else None,\n  'precip': float(rec['precip']) if rec.get('precip') is not None else None\n}\n```\n\n## Follow-up Questions\n- How would you adapt for additional fields without breaking the pipeline?\n- How would you monitor failures and re-run only failed partitions?\n","diagram":"flowchart TD\nA[REST API Call (hourly)] --> B[ Land Raw JSON in ADLS /data/raw/weather/ ]\nB --> C[ Data Flow: Normalize fields to fixed set ]\nC --> D[ Parquet Sink: /data/warehouse/weather/date=YYYY-MM-DD ]\nD --> E[ Synapse: WeatherFact star schema ]","difficulty":"beginner","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","NVIDIA","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T09:49:24.647Z","createdAt":"2026-01-16T09:49:24.647Z"},{"id":"q-2915","question":"Scenario: Ingest IoT JSON logs stored in ADLS Gen2 under /iot/logs/ into a clean, queryable table in Azure Synapse. The schema evolves as new fields are added over time. Design a beginner-friendly pipeline using Synapse Pipelines or Data Factory that: 1) handles schema drift, 2) writes normalized data to Parquet in /processed/iot/, 3) loads incrementally via watermark strategy into a small fact table (device_id, event_date, event_type, duration), and 4) logs data quality metrics and lineage. Which services and steps would you implement, and how would you validate success?","answer":"Use Synapse Pipelines or Data Factory with Mapping Data Flows to read JSON from ADLS Gen2, enable schema drift handling to auto-extend schemas, write to Parquet at /processed/iot/, and load incrementa","explanation":"## Why This Is Asked\nTests practical, beginner-friendly mastery of drift-tolerant ingestion, incremental loading, and governance using familiar Azure tools. It also probes practical tradeoffs between Data Flows and Spark in a real-world IoT scenario.\n\n## Key Concepts\n- Schema drift handling in Mapping Data Flows\n- Watermark-based incremental load strategy\n- Parquet storage in ADLS Gen2 and path conventions\n- Basic data quality checks and failure logging\n- Data governance and lineage with Purview\n\n## Code Example\n```python\n# placeholder snippet illustrating a watermark-based incremental load sketch\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\ndf = spark.read.json(\"abfss://container@account.dfs.core.windows.net/iot/logs/\")\ndf.write.parquet(\"/processed/iot/\", mode=\"append\")\n```\n\n## Follow-up Questions\n- How would you test schema drift and backward compatibility?\n- What are the trade-offs between Data Flow and Spark for this pipeline?","diagram":"flowchart TD\n  Ingest[Ingest JSON from ADLS Gen2] --> Drift[Schema Drift Handling in Data Flow]\n  Drift --> Parquet[Write Parquet to /processed/iot/]\n  Parquet --> Load[Incremental Load to Synapse Gold]\n  Load --> Quality[Quality Logging & lineage to Purview]","difficulty":"beginner","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["NVIDIA","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T17:33:46.851Z","createdAt":"2026-01-16T17:33:46.851Z"},{"id":"q-2961","question":"Design an end-to-end data pipeline for a global fintech platform (Coinbase/Airbnb). Ingest real-time trade ticks from an on-prem SFTP feed and batch customer events into ADLS Gen2, leverage Delta Lake on Databricks, and load a star schema into Azure Synapse. Ensure end-to-end data lineage, automatic schema drift handling, incremental loads, and row-level security for PII. Outline components, Purview governance, and testing/rollback strategy?","answer":"Leverage ADF to stage on-prem files to ADLS Gen2, and Databricks Delta Lake with Auto Loader for batch+streaming. Enable schema drift tolerance, and use MERGE for incremental upserts into a Synapse-ba","explanation":"## Why This Is Asked\nProbes practical skill in building scalable, compliant data pipelines with real-time + batch data, governance, and security.\n\n## Key Concepts\n- Ingestion patterns: on-prem SFTP to ADLS Gen2, batch + streaming.\n- Delta Lake features: schema drift tolerance, MERGE upserts, time travel.\n- Orchestration: ADF, Databricks, Synapse integration.\n- Governance: Purview lineage/classification, access controls, data masking.\n\n## Code Example\n```sql\n-- example MERGE for incremental load into target star schema\nMERGE INTO target_schema.fact_trades AS t\nUSING staging.trades_updates AS s\nON t.trade_id = s.trade_id\nWHEN MATCHED THEN UPDATE SET ...\nWHEN NOT MATCHED THEN INSERT (...);\n```\n\n## Follow-up Questions\n- How would you test schema drift handling in this pipeline?\n- How would you implement rollback across regional replicas?","diagram":null,"difficulty":"advanced","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Coinbase"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T19:25:27.656Z","createdAt":"2026-01-16T19:25:27.658Z"},{"id":"q-3019","question":"Design an end-to-end Azure data pipeline for streaming IoT telemetry from on-prem JSON feeds into ADLS Gen2 and a star schema in Synapse. Use Delta Live Tables or Databricks to handle schema drift, implement idempotent upserts, ensure exactly-once semantics, and surface data lineage to Purview. Include data contracts, quality checks, alerting, and recovery from late data?","answer":"Use Azure Data Factory to stage on-prem JSON feeds to ADLS Gen2 as Parquet, then leverage Databricks Delta Live Tables to ingest as Delta Lake with automatic schema drift handling and evolution. Construct a star schema in Synapse and implement MERGE operations for idempotent upserts. Configure exactly-once semantics using Delta Lake's ACID transactions combined with watermarking. Integrate with Azure Purview for comprehensive data lineage through automatic asset registration. Define data contracts using JSON schemas and enforce quality checks with DLT expectations. Establish alerting via Azure Monitor and implement late data recovery using Delta Lake time travel and reprocessing capabilities.","explanation":"## Why This Is Asked\n\nTo evaluate the ability to design end-to-end Azure data pipelines for streaming scenarios with schema drift, upserts, lineage, and governance in a realistic enterprise setting.\n\n## Key Concepts\n\n- Streaming ETL with Delta Lake and Delta Live Tables\n- Schema evolution and drift handling\n- Data contracts and quality checks\n- Data lineage and Purview integration\n- Late-arriving data strategies\n\n## Code Example\n\n```javascript\n// Pseudocode: DLT table creation with schema drift\ncreateDeltaTable(\"facts_payments\", mergeOn=\"payment_id\")\n  .withSchemaEvolution(true)\n  .withExpectations({\n    quality: \"payment_amount > 0\",\n    completeness: \"payment_id IS NOT NULL\"\n  })\n  .withWatermark(\"event_timestamp\", \"5 minutes\")\n  .withPurviewLineage(true);\n```","diagram":null,"difficulty":"advanced","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","IBM","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T06:02:58.545Z","createdAt":"2026-01-16T21:38:22.569Z"},{"id":"q-3107","question":"Design a streaming ELT pipeline that ingests customer events from Azure Event Hubs and a SQL Server CDC feed, normalizes to a Delta Lake star schema on ADLS Gen2, supports schema evolution, idempotent upserts, and end-to-end lineage via Azure Purview. Outline components, dataflow, and quality checks?","answer":"Leverage Azure Databricks Structured Streaming to ingest data from Azure Event Hubs and SQL Server CDC feeds. Implement schema drift handling with automatic schema evolution, and perform idempotent upserts into Delta Lake on ADLS Gen2 using MERGE operations with watermarking and transactional writes. Establish a star schema through dimensional modeling, and capture comprehensive end-to-end data lineage via Azure Purview integration.","explanation":"## Why This Is Asked\n\nTests the ability to design enterprise-grade streaming data pipelines that integrate heterogeneous sources, handle schema evolution, maintain data quality, and provide governance in Azure's modern data stack.\n\n## Key Concepts\n\n- Structured Streaming for continuous data processing\n- Delta Lake MERGE operations for idempotent upserts\n- Schema evolution and automatic schema enforcement\n- Change Data Capture (CDC) from SQL Server\n- Watermarking for late-arriving data handling\n- Azure Purview for comprehensive data lineage\n- ADLS Gen2 for scalable cloud storage\n- Star schema dimensional modeling\n\n## Components\n\n1. **Ingestion Layer**: Azure Event Hubs for streaming events, SQL Server CDC for relational changes\n2. **Processing Layer**: Azure Databricks Structured Streaming jobs with schema evolution\n3. **Storage Layer**: Delta Lake on ADLS Gen2 with ACID transactions\n4. **Governance Layer**: Azure Purview for data cataloging and lineage\n5. **Quality Layer**: Data validation rules and monitoring\n\n## Dataflow\n\nEvent Hubs/CDC → Structured Streaming → Schema Validation → Delta Lake MERGE → Star Schema → Purview Lineage\n\n## Quality Checks\n\n- Schema validation and evolution policies\n- Data completeness and accuracy checks\n- Duplicate detection and idempotency validation\n- Performance monitoring and alerting\n- Lineage verification across pipeline stages","diagram":"flowchart TD\n  A[Event Hubs] --> B[Databricks Structured Streaming]\n  C[SQL CDC] --> B\n  B --> D[Delta Lake on ADLS Gen2]\n  D --> E[Azure Purview]","difficulty":"intermediate","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Microsoft","Robinhood","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T04:52:37.085Z","createdAt":"2026-01-17T02:25:15.775Z"},{"id":"q-3120","question":"In a multinational fintech, ingest both daily batch and streaming micro-batch events into Delta Lake on ADLS Gen2, enforce schema evolution, and capture end-to-end data lineage across on-prem, Event Hubs, and Azure Synapse analytics. Which Azure components would you choose, what patterns ensure correctness and rollback, and how would you validate lineage end-to-end at scale?","answer":"Design uses Azure Databricks + Delta Lake on ADLS Gen2; batch via Delta Live Tables; streaming via Structured Streaming; enable schema evolution with mergeSchema; lineage via Azure Purview with source","explanation":"## Why This Is Asked\n\nThis question probes the candidate's ability to architect a hybrid batch/streaming data pipeline on Azure, with strong governance, lineage, and rollback requirements across disparate sources.\n\n## Key Concepts\n\n- Delta Lake schema evolution and mergeSchema\n- Delta Live Tables and Structured Streaming\n- Azure Purview lineage and data map integration\n- Unity Catalog for fine-grained access\n- Time travel / PITR in Delta Lake\n\n## Code Example\n\n```scala\n// Delta Lake streaming with schema evolution\nval ds = spark.readStream.format(\"cloudFiles\").option(\"cloudFiles.format\",\"parquet\").load(\"/path/stream\")\nds.writeStream.format(\"delta\").option(\"mergeSchema\",\"true\").start(\"/mnt/delta/sink\")\n```\n\n## Follow-up Questions\n\n- How would you test lineage in Purview after data ingestion?\n- How would you handle out-of-order events and late data?","diagram":"flowchart TD\n  A[On-prem/Event Hubs] --> B[Databricks/Delta Lake]\n  B --> C[Delta Lake on ADLS Gen2]\n  C --> D[Azure Purview lineage]\n  C --> E[Azure Synapse/BI]","difficulty":"advanced","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Coinbase","Instacart"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T04:02:49.759Z","createdAt":"2026-01-17T04:02:49.759Z"},{"id":"q-3175","question":"You receive daily JSON event files from three SaaS apps landing in ADLS Gen2 at /raw/events/{saas}/{date}. Each file shape differs slightly. Design a beginner Data Factory pipeline to ingest, normalize to a common schema, and write to Parquet in /processed/events/{date}/. Include: (1) a schema-drift tolerant mapping or inference approach, (2) a simple incremental load using a date watermark, (3) basic data quality checks (not-null on key fields, duplicates), (4) automatic partitioning by date, (5) basic monitoring/logging. Which Azure components and steps would you use?","answer":"Leverage ADLS Gen2, Azure Data Factory v2, and Mapping Data Flows. Ingest JSON per SaaS to a Parquet sink partitioned by date. Enable schema-drift tolerant mapping (runtime field inference to a unifie","explanation":"## Why This Is Asked\n\nTests ability to design a practical beginner data ingestion and normalization pipeline with varying schemas, demonstrating schema drift handling, incremental loading, and basic quality checks using Azure Data Factory.\n\n## Key Concepts\n\n- Schema drift handling in Mapping Data Flows\n- Incremental loading using file path date watermark\n- Parquet sink with partitioning by date\n- Basic data quality checks (not-null, dedup)\n\n## Code Example\n\n```javascript\n{\n  \"source\": \"JSON\",\n  \"transform\": \"unify to schema_v1\",\n  \"sink\": {\n    \"format\": \"Parquet\",\n    \"partitionBy\": [\"date\"]\n  }\n}\n```\n\n## Follow-up Questions\n\n- How would you adapt this for streaming sources?\n- What are potential late-arriving data pitfalls and how would you handle them?","diagram":null,"difficulty":"beginner","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","Snap","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T05:36:59.077Z","createdAt":"2026-01-17T05:36:59.077Z"},{"id":"q-3229","question":"Scenario: A global retailer streams daily sales events and batch inventory feeds from on-prem sources into ADLS Gen2. You must implement end-to-end data lineage, incremental loads with watermark, schema drift tolerant transforms, and PII masking at ingestion, while enabling analytics in Synapse. Which Azure components would you choose to ensure idempotency and auditability across batch and streaming modes?","answer":"Ingest batch and streaming data with Azure Data Factory and Databricks, store in ADLS Gen2 as Delta Lake. Use Spark structured streaming with watermark for incremental loads, and MERGE for upserts. Ma","explanation":"## Why This Is Asked\nTests ability to design an integrated data platform with batch+stream, governance, privacy, and operational reliability across Azure services.\n\n## Key Concepts\n- End-to-end lineage with Purview\n- Delta Lake idempotent upserts\n- Ingestion-time masking of PII\n- Schema drift tolerant transforms\n- Watermark-based incremental processing across batch/stream\n- Cross-service orchestration (Data Factory, Databricks, Synapse)\n\n## Code Example\n```javascript\n// Pseudo orchestration snippet illustrating MERGE against Delta Lake\n```\n\n## Follow-up Questions\n- How would you handle late-arriving data and out-of-order events?\n- How would you validate lineage accuracy if Purview crawls change feed?","diagram":null,"difficulty":"intermediate","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Apple"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T07:40:21.141Z","createdAt":"2026-01-17T07:40:21.142Z"},{"id":"q-3444","question":"You run a multi-tenant data lake on ADLS Gen2 for three business units. Each unit provides JSON with evolving schemas; you must produce a per-tenant SCD Type 2 customer dimension in Delta Lake, with automatic schema drift handling, tenant-level access control, and end-to-end lineage to Purview. Outline architecture, data contracts, and how you guarantee idempotent upserts and secure access?","answer":"Use Databricks Auto Loader to ingest per-tenant JSON into Delta Lake. Use MERGE for SCD Type 2 upserts; handle schema drift with a central schema registry and Purview lineage; enforce tenant isolation","explanation":"## Why This Is Asked\nTests multi-tenant data lake design, schema evolution handling, and governance integration across Azure services.\n\n## Key Concepts\n- Delta Lake MERGE for SCD Type 2\n- Auto Loader with schema drift handling\n- Unity Catalog ABAC for tenant isolation\n- Purview lineage and data contracts\n\n## Code Example\n```javascript\n-- Pseudo SQL for MERGE (illustrative)\nMERGE INTO delta_table AS t\nUSING staging AS s\nON t.tenant = s.tenant AND t.id = s.id\nWHEN MATCHED THEN UPDATE SET ...\nWHEN NOT MATCHED THEN INSERT ...\n```\n\n## Follow-up Questions\n- How would you handle late-arriving data and out-of-order events?\n- How would you test schema drift and contract violations in CI/CD?","diagram":"flowchart TD\n  Ingest[Ingest per-tenant JSON] --> Drift[Schema Drift Handling]\n  Drift --> Upsert[Delta MERGE for SCD2]\n  Upsert --> Lineage[Purview Lineage]\n  Lineage --> UC[Unity Catalog Security]","difficulty":"intermediate","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Meta","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T16:42:42.413Z","createdAt":"2026-01-17T16:42:42.413Z"},{"id":"q-3467","question":"NEW ANGLE: Daily NDJSON invoices land in ADLS Gen2 under /raw/payments/. Build a beginner Data Factory pipeline that stages to /staging/payments, flattens nested fields (payment_id, amount, currency, customer.id, customer.name, ts), runs basic data quality checks (payment_id not null, amount > 0, ts valid), and upserts into a partitioned Parquet table in Azure Synapse (Payments by date(ts)). Explain incremental loading using a watermark and describe datasets/activities?","answer":"Use Data Factory with a Mapping Data Flow to stage NDJSON from /raw/payments into /staging/payments, flattening nested fields and applying quality checks (payment_id not null, amount > 0, ts valid). U","explanation":"## Why This Is Asked\nTests a practical, end-to-end beginner pattern: ingesting NDJSON, flattening nested JSON, validating data quality, and performing an incremental upsert into a partitioned Parquet table in Synapse. Emphasizes familiar Azure tooling and real-world trade-offs.\n\n## Key Concepts\n- Azure Data Factory and Mapping Data Flows\n- NDJSON parsing and JSON flattening\n- Data quality checks (nulls, ranges, timestamp validity)\n- Parquet sink and date partitioning in Synapse\n- Watermark-based incremental loading\n\n## Code Example\n```sql\nMERGE INTO Payments AS t\nUSING staging.Payments AS s\nON t.payment_id = s.payment_id\nWHEN MATCHED THEN UPDATE SET t.amount = s.amount, t.currency = s.currency, t.ts = s.ts\nWHEN NOT MATCHED THEN INSERT (payment_id, amount, currency, customer_id, customer_name, ts)\nVALUES (s.payment_id, s.amount, s.currency, s.customer_id, s.customer_name, s.ts);\n```\n\n## Follow-up Questions\n- How would you extend this to handle schema evolution in the NDJSON fields?\n- What monitoring would you add to verify every day’s watermark advances correctly and handles replays gracefully?","diagram":"flowchart TD\n  A[NDJSON in ADLS Gen2 /raw/payments] --> B[Stage into /staging/payments]\n  B --> C[Flatten fields: payment_id, amount, currency, customer.id, customer.name, ts]\n  C --> D[Quality checks: payment_id != null, amount > 0, ts valid]\n  D --> E[Write partitioned Parquet to Synapse: Payments by date(ts)]\n  E --> F[Incremental load via watermark]","difficulty":"beginner","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Airbnb","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T17:32:01.354Z","createdAt":"2026-01-17T17:32:01.354Z"},{"id":"q-3498","question":"You receive daily JSON logs from a SaaS app stored in ADLS Gen2 under /logs/yyyy/MM/dd/. Each file contains an array of events with nestedUser and nestedItems arrays. Design a beginner ELT using Data Factory and Spark (Synapse) to flatten to a users dimension and a facts table, implement an incremental load based on a date watermark, and ensure idempotent upserts with simple checks. What components, data flow, and validations would you implement?","answer":"Stage: Ingest with Data Factory Mapping Data Flow on Spark; flatten events to fields: user_id, user_email, event_ts; create dim_user and fact_events in Synapse; incremental load using a watermark stor","explanation":"## Why This Is Asked\n\nThis tests practical use of Azure Data Factory, Synapse Spark, and SQL MERGE for a simple ELT with nested JSON, including incremental loads and basic data quality.\n\n## Key Concepts\n\n- JSON flattening in Spark\n- Incremental loads with watermark\n- Upserts via MERGE\n- Basic data quality checks\n\n## Code Example\n\n```javascript\n// No code, design-level answer\n```\n\n## Follow-up Questions\n\n- How would you handle schema evolution for nested fields?\n- What are performance tips for large JSON arrays?","diagram":"flowchart TD\n  A[ADLS Gen2 /logs] --> B[DF Mapping Data Flow]\n  B --> C{Flattened tables}\n  C --> D[dim_user]\n  C --> E[fact_events]","difficulty":"beginner","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Citadel","Databricks"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T18:53:18.844Z","createdAt":"2026-01-17T18:53:18.844Z"},{"id":"q-3572","question":"Daily JSON logs arrive in ADLS Gen2 with nested fields and occasional missing keys. Design a beginner Data Factory pipeline that uses a Mapping Data Flow to flatten and unify to a flat schema, load incrementally into Azure Synapse (or Azure SQL DB) with a watermark on event_time, and route invalid records to /rejects. Include a simple schema-drift tolerant approach and a basic test plan?","answer":"Leverage Azure Data Factory Mapping Data Flows to flatten nested JSON structures, union overlapping fields into a unified flat schema, and load incrementally using a watermark on event_time into Azure Synapse. Handle missing fields by nulling them out, route malformed records to /rejects, and implement schema drift tolerance through dynamic column handling.","explanation":"Why This Is Asked\n\nThis question tests practical data engineering skills for handling schema drift, nested JSON structures, incremental loads, and basic data quality implementation using a reject path within the familiar Azure stack.\n\nKey Concepts\n\n- Mapping Data Flows for JSON flattening and schema unification\n- Schema drift tolerance with null handling for missing fields\n- Watermark-based incremental loading strategies\n- Data quality implementation via reject paths in ADLS\n\nCode Example\n\n```javascript\n// Pseudo-test: ensure essential fields exist after load\nconst rows = readFromSynapse(\"target_table\");\nassert(rows.every(row => row.event_time !== null), \"event_time should not be null\");\nassert(rows.every(row => row.id !== null), \"id should not be null\");\n```","diagram":"flowchart TD\n  A[ADLS Gen2: JSON files] --> B[ADF: Mapping Data Flow]\n  B --> C[Flatten/Union to flat schema]\n  C --> D[Incremental load via watermark]\n  D --> E[Azure Synapse table]\n  B --> F[Rejects: /rejects]","difficulty":"beginner","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T05:43:09.105Z","createdAt":"2026-01-17T21:37:48.243Z"},{"id":"q-3623","question":"In a globally distributed analytics platform, you must implement a multi-geo data mesh on Azure with data products exposed via Delta Lake tables on ADLS Gen2, governed by Purview, and queried through Synapse. How would you ensure discoverability, end-to-end lineage, contract-driven access, and schema evolution across regions while controlling cost and latency?","answer":"Implement a multi-geo data mesh architecture using Azure Purview for centralized catalog and lineage, deploy Delta Lake tables on ADLS Gen2 across regions with cross-region replication, expose data products through Synapse serverless SQL endpoints with contract-driven access via Azure Active Directory integration, and leverage Delta Lake's mergeSchema capabilities for schema evolution while optimizing cost and latency through region-local data access patterns.","explanation":"## Why This Is Asked\nThis question evaluates expertise in enterprise data architecture, specifically testing knowledge of Azure's data platform capabilities, data mesh principles, and production-grade governance patterns.\n\n## Key Concepts\n- Data mesh principles and data product thinking\n- Azure Purview for unified catalog, lineage, and governance\n- Delta Lake schema evolution with mergeSchema\n- Cross-region replication strategies and cost optimization\n- Contract-driven access via Azure AD RBAC and ACLs\n- Synapse integration for unified querying\n\n## Code Example\n```python\n# PySpark example: wri","diagram":"flowchart TD\n  A[Source Data Producers] --> B(Ingestion Pipelines in region R1)\n  B --> C[Delta Lake in ADLS Gen2]\n  C --> D[Purview Catalog & Lineage]\n  D --> E[Data Product APIs / Synapse]\n  E --> F[End-User Analytics/BI]","difficulty":"advanced","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Coinbase","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T05:08:42.119Z","createdAt":"2026-01-17T23:43:28.921Z"},{"id":"q-3643","question":"Design a real-time data lakehouse pipeline that ingests high-velocity financial events from an on-prem Oracle database into ADLS Gen2, processes with Spark Structured Streaming, and upserts into a Delta table consumed by Azure Synapse Analytics. Requirements: end-to-end lineage via Purview, automatic schema drift tolerance, incremental loads, tenant isolation, and robust observability. Which components and data contracts would you choose, and why?","answer":"Use Oracle GoldenGate Microservices to stream DML changes to Azure Event Hubs, Databricks Structured Streaming to consume the CDC stream and write to Delta Lake on ADLS Gen2 with schema evolution enabled, and Azure Synapse Analytics to query the Delta table directly. Implement tenant isolation through separate Event Hub namespaces and Delta Lake partitions, with Purview providing end-to-end lineage and data contracts enforced via Unity Catalog.","explanation":"## Why This Is Asked\n\nTests ability to architect a real-time lakehouse with CDC, lineage, schema drift handling, multi-tenant isolation, and observability across Azure services.\n\n## Key Concepts\n\n- Change Data Capture from on-prem Oracle to streaming sink\n- Delta Lake schema evolution and upserts\n- End-to-end data lineage with Purview\n- Multi-tenant data contracts and isolation\n- Observability, retries, backpressure, and failure handling\n\n## Code Example\n\n```javascript\n// foreachBatch upsert (pseudo)\nfunction upsertDelta(batchDF) {\n  // Real code would call DeltaTable.forPath(...).merge(...)\n}\n```","diagram":"flowchart TD\n  A[On-Prem Oracle CDC] --> B[Event Hubs]\n  B --> C[Databricks Structured Streaming]\n  C --> D[Delta Lake on ADLS Gen2]\n  D --> E[Delta table (Synapse consumption)]\n  E --> F[Purview lineage]","difficulty":"advanced","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Goldman Sachs","IBM"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T05:02:09.019Z","createdAt":"2026-01-18T02:45:17.591Z"},{"id":"q-3656","question":"You receive daily JSONL logs in ADLS Gen2 under /logs/appX/. Each file may add new fields over time. Build a beginner Data Factory pipeline that ingests these logs into a Delta Lake table in ADLS Gen2, using Spark in Synapse or Databricks. Implement schema drift handling, partition by ingest_date, and a simple data-quality check. Describe components and steps?","answer":"Use ADF to orchestrate a Databricks Spark job that reads JSONL from /logs/appX, infers a changing schema with mergeSchema, and writes to Delta at /delta/logs/appX with partitionBy ingest_date. Impleme","explanation":"## Why This Is Asked\nTests practical data ingestion with schema drift, Delta Lake, and lineage.\n\n## Key Concepts\n- Schema evolution with Spark mergeSchema\n- Delta Lake storage on ADLS Gen2\n- Data quality checks and Purview lineage\n\n## Code Example\n```javascript\n// Pseudocode: spark read + write with mergeSchema\nconst df = spark.read.format('json').load('/logs/appX')\ndf.write\n  .format('delta')\n  .partitionBy('ingest_date')\n  .mode('append')\n  .save('/delta/logs/appX')\n```\n\n## Follow-up Questions\n- How would you handle duplicates in event_id during MERGE?\n- How would you test schema drift handling locally?","diagram":"flowchart TD\n  A[Ingest JSONL] --> B[Infer/merge schema]\n  B --> C[Write Delta with partition]\n  C --> D[MERGE into target]\n  D --> E[Purview lineage]","difficulty":"beginner","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Robinhood","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T04:11:40.404Z","createdAt":"2026-01-18T04:11:40.404Z"},{"id":"q-3683","question":"Design a data ingestion and governance pattern for a mixed batch/streaming pipeline into ADLS Gen2 and Synapse: landing, silver Delta Lake, and curated views. Propose concrete Azure components (ADF, Databricks, Purview, Synapse), and describe how you enforce data contracts, enable schema evolution, and implement idempotent upserts across regions while controlling costs. How would you handle a breaking source schema change?","answer":"Propose: ADLS Gen2 landing, Databricks Delta Lake (Silver), Synapse for curated marts, ADF for orchestration, Purview for lineage and contracts; enable Delta schema evolution and upserts via MERGE; ve","explanation":"## Why This Is Asked\n\nTests practical integration of governance, schema evolution, and upserts in a mixed workload with real Azure services.\n\n## Key Concepts\n\n- Data contracts and lineage via Purview across batch/stream paths\n- Delta Lake schema evolution and drift tolerance\n- Idempotent MERGE upserts and cross-region deployment\n- Orchestration with ADF; compute via Databricks; curated views in Synapse\n- Cost controls through autoscale, materialization decisions, and caching\n\n## Code Example\n\n```python\n# Pseudo: upsert into Delta table with MERGE\nMERGE INTO silver AS s\nUSING updates AS u\nON s key = u key\nWHEN MATCHED THEN UPDATE SET *\nWHEN NOT MATCHED THEN INSERT *\n```\n\n## Follow-up Questions\n\n- How would you version and enforce contracts in Purview across teams?\n- How do you validate schema evolution compatibility before deployment?","diagram":"flowchart TD\n  A[Source Systems] --> B[Landing (ADLS Gen2)]\n  B --> C[Silver (Delta Lake)]\n  C --> D[Curated (Synapse)]\n  subgraph Governance\n  E[Purview] --> D\n  end","difficulty":"intermediate","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Google","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T05:34:58.430Z","createdAt":"2026-01-18T05:34:58.431Z"},{"id":"q-3705","question":"You're given daily JSON event files arriving in ADLS Gen2 under /raw/events/. Files contain arrays of events with optional fields; you must flatten to a simple star schema (DimUser: user_id, country; FactEvents: event_id, timestamp, user_id, event_type). Design a beginner end-to-end pipeline using Azure Data Factory and Synapse to stage raw JSON, handle schema drift, perform incremental loads with a watermark, and publish lineage to Purview. Which components, steps, and dataflow logic would you implement?","answer":"Copy raw JSON to ADLS staging, then a Mapping Data Flow flattens nested arrays into DimUser(user_id, country) and FactEvents(event_id, timestamp, user_id, event_type). Implement incremental load with ","explanation":"## Why This Is Asked\nTests practical data engineering workflow on modern Azure services, focusing on real-world JSON with drift and governance.\n\n## Key Concepts\n- Mapping Data Flows for flattening and drift tolerance\n- Incremental load using a watermark (fileModifiedTime)\n- Staging in ADLS Gen2, star schema in Synapse\n- Purview/Data Factory lineage integration for end-to-end lineage\n\n## Code Example\n```sql\n-- Simple incremental MERGE example for loading FactEvents\nMERGE INTO SynapseDB.dbo.FactEvents AS t\nUSING Staging.dbo.FactEvents AS s\nON t.event_id = s.event_id\nWHEN NOT MATCHED THEN INSERT (...);\n```\n\n## Follow-up Questions\n- How would you handle arrays and missing fields during flattening?\n- How would you validate data quality before upserts and surface lineage in Purview?","diagram":"flowchart TD\n  A[Ingest JSON files] --> B[Stage Raw in ADLS]\n  B --> C[Flatten with Data Flow]\n  C --> D[Load DimUser]\n  C --> E[Load FactEvents]\n  D --> F[Synapse DW]\n  E --> F","difficulty":"beginner","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","NVIDIA","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T06:44:31.851Z","createdAt":"2026-01-18T06:44:31.851Z"},{"id":"q-3728","question":"Design a real-time streaming ELT: ingest JSON events from Azure Event Hubs into Delta Lake on ADLS Gen2 via Databricks Structured Streaming; apply schema drift tolerant transforms; upsert into a Star schema in Azure Synapse using MERGE-based SCD2. How would you ensure end-to-end data lineage (Purview), late-data handling, and automatic schema evolution while controlling latency and costs?","answer":"Ingest JSON from Event Hubs with Databricks Structured Streaming into Delta Lake on ADLS Gen2, enabling Delta schema evolution; use MERGE-based SCD2 into a Star schema in Synapse. Enable Purview linea","explanation":"## Why This Is Asked\n\nEvaluates end-to-end real-time ELT design across Event Hubs, Delta Lake, and Synapse, plus governance with Purview. Tests handling of schema drift, late data, and accurate SCD2 upserts at scale.\n\n## Key Concepts\n\n- Streaming ingestion with Databricks Spark\n- Delta Lake schema evolution and drift tolerance\n- MERGE-based SCD2 in a star schema\n- End-to-end data lineage with Purview\n- Late-data handling, watermarks, and checkpointing\n\n## Code Example\n\n```javascript\n// Pseudo-Spark SQL MERGE for SCD2\nval mergeQuery = `\nMERGE INTO delta.`/mnt/delta/sales` AS target\nUSING updates AS src\nON target.id = src.id\nWHEN MATCHED THEN UPDATE SET *\nWHEN NOT MATCHED THEN INSERT *\n` \n```\n\n## Follow-up Questions\n\n- How would you handle schema evolution breaking changes in upstream producers?\n- What are potential cost/latency trade-offs of using Purview for lineage across streaming jobs?","diagram":"flowchart TD\n  EH[Azure Event Hubs] --> DS[Databricks Structured Streaming]\n  DS --> DL[Delta Lake on ADLS Gen2]\n  DL --> DW[Azure Synapse Analytics]\n  DW --> Purview[Purview Lineage]","difficulty":"advanced","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Robinhood","Salesforce","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T07:28:56.492Z","createdAt":"2026-01-18T07:28:56.492Z"},{"id":"q-3781","question":"Design an end-to-end data observability solution for a multi-source Azure data lakehouse. Ingested data lands in ADLS Gen2 and is consumed by Synapse; require automatic lineage, schema drift alerts, data freshness checks, and auto-remediation that re-runs failed pipelines or blocks downstream jobs. Describe architecture, thresholds, data quality checks, and how you would implement with Purview, Delta Lake, Data Factory, and Synapse?","answer":"Architect a observability layer across ADLS Gen2, Purview, and Synapse that monitors data freshness, record counts, schema drift, and end-to-end lineage. Leverage Delta Lake checkpoints, Data Factory/","explanation":"## Why This Is Asked\n\nTests ability to design a cohesive observability and governance layer spanning ingestion, storage, catalog, and compute, plus automated remediation in production.\n\n## Key Concepts\n\n- Data observability metrics: freshness, volume, schema drift, lineage\n- Delta Lake governance and time-travel\n- Event-driven remediation with Event Grid\n- Integration across ADLS Gen2, Purview, Data Factory, Synapse\n- Cost-aware alerting and automated rollback\n\n## Code Example\n\n```javascript\n// Pseudo logic for anomaly detection\nconst rules = [\n  {metric: 'freshness', maxLagMin: 60},\n  {metric: 'records', minDeltaPct: 20}\n];\n// on violation -> trigger workflow via Event Grid\n```\n\n## Follow-up Questions\n\n- How would you test this observability layer before production?\n- What would you monitor to distinguish data quality issues vs source outages?","diagram":null,"difficulty":"advanced","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Goldman Sachs","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T09:33:27.782Z","createdAt":"2026-01-18T09:33:27.782Z"},{"id":"q-3888","question":"You get daily JSON event logs from multiple sources placed under /events/campaigns/ in ADLS Gen2. Schema drift is possible with missing fields and occasional new fields. Build a beginner Azure Data Factory pipeline that ingests, flattens, and loads into a simple star schema in Azure Synapse Analytics; partition by eventDate; store details as JSON for drift tolerance; include basic data quality checks (not-null userId and parseable timestamp) and a minimal lineage/logging. What components and steps would you implement?","answer":"Implement a beginner ADF pipeline: read daily JSONs from /events/campaigns/ in ADLS Gen2, copy to a staging area, flatten with a Mapping Data Flow (or simple JSON path) and emit a tabular view. Load i","explanation":"## Why This Is Asked\nTests practical data engineering with semi-structured data, drift-tolerant ingest, and a clear path to a star schema in Synapse using Azure Data Factory and ADLS Gen2.\n\n## Key Concepts\n- Azure Data Factory pipelines and Mapping Data Flows\n- Ingesting JSON from ADLS Gen2 with schema drift\n- Flattening nested JSON; storing details as JSON for drift tolerance\n- Star schema design in Synapse (DimUser, DimDate, FactEvents)\n- Basic data quality checks and simple lineage logging\n\n## Code Example\n```sql\n-- Create simplified star schema (example)\nCREATE TABLE DimUser (UserId VARCHAR(256) PRIMARY KEY);\nCREATE TABLE DimDate (DateKey DATE PRIMARY KEY);\nCREATE TABLE FactEvents (\n  EventId INT IDENTITY(1,1) PRIMARY KEY,\n  UserId VARCHAR(256) FOREIGN KEY REFERENCES DimUser(UserId),\n  EventDate DATE,\n  EventType VARCHAR(64),\n  Details NVARCHAR(MAX)\n);\n```\n\n## Follow-up Questions\n- How would you handle incremental loads and late-arriving data with this design?\n- What lightweight monitoring would you add to catch JSON schema drift before it affects downstream tables?","diagram":null,"difficulty":"beginner","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Meta","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T13:49:50.928Z","createdAt":"2026-01-18T13:49:50.928Z"},{"id":"q-4212","question":"You are given daily Parquet files containing user activity logs arriving into ADLS Gen2 at /data/activity/. The schema drifts as new fields appear over time. Design a beginner Data Factory pipeline that: 1) copies raw Parquet to a landing zone preserving data; 2) transforms to a simple star schema in Azure Synapse (DimUsers and FactEvents); 3) handles drift by storing extra fields as a JSON blob in a drift column; 4) enforces basic data quality (not-null user_id, parseable event_time) and logs lineage. Which Azure components and steps would you implement?","answer":"Use Data Factory for orchestration with a two-stage flow: (1) Copy Parquet into /landing/parquet as raw. (2) Spark/DF or Data Flow to map fields to a star schema in Synapse: DimUsers and FactEvents. T","explanation":"## Why This Is Asked\\n\\nTests ability to design a beginner-friendly Azure data pipeline that handles schema drift, produces a dimensional model, and captures lineage. It also probes how to isolate raw data, apply transformations, and enforce simple data quality gates with observable provenance.\\n\\n## Key Concepts\\n- Schema drift handling in Parquet\\n- Dimensional modeling (DimUsers, FactEvents)\\n- Drift capture via JSON blob\\n- Data quality checks (NOT NULL, timestamp parse)\\n- Data lineage with Purview integration\\n\\n## Code Example\\n```javascript\n// Pseudocode: drift handling in a dataflow step\nfunction transform(record) {\n  const known = { user_id, event_time, event_type };\n  const drift = _.omit(record, Object.keys(known));\n  return { ...record, drift_json: JSON.stringify(drift) };\n}\n```\n\\n## Follow-up Questions\\n- How would you test drift handling with evolving schemas?\\n- How would you extend this to handle very high throughput volumes?","diagram":null,"difficulty":"beginner","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Scale Ai","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T08:54:14.176Z","createdAt":"2026-01-19T08:54:14.176Z"},{"id":"q-4228","question":"You're building a cross-region retail analytics data product in Azure. Ingest on-prem SQL Server CDC into ADLS Gen2 in each region, publish Delta Lake tables to Databricks and Synapse with incremental loads, end-to-end lineage, and contract-based access. Which Azure components and patterns would you use to guarantee incremental loads, schema drift tolerance, cross-region discovery via Purview, and secure data sharing with minimal latency and cost?","answer":"Use a reproducible multi-region pattern: ADF with a self-hosted IR to ingest CDC from on-prem SQL Server into region ADLS Gen2; Databricks to MERGE incremental rows into Delta Lake with schema evoluti","explanation":"## Why This Is Asked\nTests end-to-end data product design across regions, including CDC ingestion, Delta Lake success criteria, governance, and secure sharing patterns.\n\n## Key Concepts\n- Cross-region data products with incremental loads\n- CDC ingestion patterns and Delta Lake MERGE for upserts\n- Schema drift tolerance and evolution strategies\n- Purview lineage, classifications, and data contracts\n- Delta Sharing and Synapse external tables for controlled access\n\n## Code Example\n```javascript\nfrom delta.tables import DeltaTable\n# example: MERGE into Delta Lake in Databricks\ndeltaTable = DeltaTable.forPath(spark, '/mnt/delta/regionA/sales')\ndeltaTable.alias('t').merge(\n  source=df.alias('s'),\n  condition='t.id = s.id'\n).whenMatchedUpdate(set={'amount':'s.amount','modified':'current_timestamp()'}).whenNotMatchedInsert(values={'id':'s.id','amount':'s.amount','modified':'current_timestamp()'}).execute()\n```\n\n## Follow-up Questions\n- How would you validate data contracts across regions and rollback on contract violations?\n- What monitoring and alerting would you implement to detect latency or schema drift issues in near real-time?","diagram":"flowchart TD\n  A[On‑prem CDC] --> B[ADF Self-hosted IR]\n  B --> C[ADLS Gen2 Region]\n  C --> D[Databricks Delta Lake MERGE]\n  D --> E[Synapse External Tables]\n  E --> F[Purview Data Contracts/Lineage]","difficulty":"advanced","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Google","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T09:43:11.066Z","createdAt":"2026-01-19T09:43:11.068Z"},{"id":"q-4278","question":"Design a real-time feature store on Azure for a globally distributed ride-hailing platform: ingest streaming vehicle telemetry from Azure IoT Hub into ADLS Gen2, compute offline features with Databricks Delta Lake, and serve online features from Cosmos DB. Ensure feature versioning, schema evolution, data quality checks, end-to-end lineage via Purview, and multi-region consistency with latency constraints. What would you implement and why?","answer":"Ingress via IoT Hub to Event Hubs; Databricks Structured Streaming computes offline features stored in Delta Lake on ADLS Gen2; Databricks Feature Store handles feature definitions and versioning; Cos","explanation":"## Why This Is Asked\nIngesting streaming data, building a versioned feature store, and ensuring governance at scale across regions is a core, challenging skill.\n\n## Key Concepts\n- Databricks, Delta Lake, Delta Live Tables\n- Databricks Feature Store, online/offline separation\n- IoT Hub, Event Hubs, Structured Streaming\n- Purview lineage, Unity Catalog RBAC\n- Multi-region consistency, feature versioning\n\n## Code Example\n```python\n# PySpark sketch: define a versioned feature and write to offline/online stores\n```\n\n## Follow-up Questions\n- How would you detect and alert on feature drift across regions?\n- How would you upgrade feature schemas without breaking consumers?\n- What tests validate data quality at ingest and feature compute stages?","diagram":"flowchart TD\n  A[IoT Hub] --> B[Event Hubs/Stream]\n  B --> C[Databricks Structured Streaming]\n  C --> D[Delta Lake offline (ADLS Gen2)]\n  C --> E[Databricks Feature Store (versioned)]\n  D --> F[Cosmos DB online store]\n  G[Purview] --> H[Lineage]\n  I[Unity Catalog] --> J[RBAC]","difficulty":"advanced","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["NVIDIA","Snowflake","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T11:29:49.718Z","createdAt":"2026-01-19T11:29:49.718Z"},{"id":"q-4439","question":"You receive daily CSV and JSON logs from two partners via SFTP and REST, landing into ADLS Gen2 under /landing. Schemas drift over time. Design a beginner Data Factory pipeline that stages raw data, handles drift with Parquet schema evolution, loads to a simple star schema in Azure Synapse, and captures lineage in Purview plus basic data quality checks. What components and steps would you implement?","answer":"Two ingestion paths in Data Factory: Copy Activities for SFTP and REST to ADLS landing. A Data Flow normalizes to a common Parquet schema with drift-tolerant fields, then load to Azure Synapse star sc","explanation":"## Why This Is Asked\nTests ability to design a beginner pipeline with multiple data sources, drift handling, and governance.\n\n## Key Concepts\n- Azure Data Factory pipelines, Copy Activity, Data Flow\n- Parquet with schema evolution for drift\n- Azure Synapse (dim/fact) loading\n- Purview for data lineage; basic data quality checks\n\n## Code Example\n```sql\nSELECT partner, COUNT(*) AS recs\nFROM staging.sales_raw\nGROUP BY partner;\n```\n\n## Follow-up Questions\n- How would you extend for new fields without downtime?\n- How would you validate Purview lineage after runs?","diagram":null,"difficulty":"beginner","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Hugging Face","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T19:00:58.617Z","createdAt":"2026-01-19T19:00:58.617Z"},{"id":"q-4542","question":"You operate a global data lake on Azure hosting per-tenant telemetry from multiple SaaS apps via Event Hubs into ADLS Gen2. Each tenant must have isolated data, incremental loads with late-arriving events, and fast analytics in Power BI/Synapse. Propose an end-to-end design using Delta Lake on Databricks, per-tenant namespaces, RBAC, data drift handling, and lineage with Purview. What components, schemas, and processes would you implement to satisfy isolation, consistency, and cost constraints?","answer":"Delta Lake on ADLS Gen2 with Databricks Unity Catalog for per-tenant isolation, implementing separate databases per tenant within a unified catalog. Ingest streaming data from Event Hubs using Autoloader into a bronze layer, then apply MERGE operations with watermarks into silver tables for incremental loads with late-arriving event handling. Implement per-tenant namespaces in Unity Catalog with RBAC for granular access control, leverage Delta Lake ACID transactions for consistency, and configure retention policies for cost optimization. Enable Azure Purview for end-to-end lineage tracking and data drift detection through automated schema monitoring. Serve analytics via Synapse Link and Power BI DirectQuery over Delta tables with materialized views for optimal performance.","explanation":"## Why This Is Asked\nMulti-tenant isolation, drift handling, and end-to-end lineage in a global data lake architecture. Cost-aware streaming with late data requires concrete trade-offs and robust design patterns.\n\n## Key Concepts\n- Delta Lake, Unity Catalog, Autoloader, MERGE operations, watermarking\n- Azure Purview lineage, RBAC, retention policies\n- ADLS Gen2, Synapse/Power BI integration, cost controls\n\n## Code Example\n```python\n# PySpark pseudo-code for upsert with watermark\nfrom delta.tables import DeltaTable\nbronze = spark.readStream.format('delta').load('/bronze')\nsilver_path = '/silver'\n```","diagram":null,"difficulty":"intermediate","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["OpenAI","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T06:15:39.312Z","createdAt":"2026-01-19T22:53:09.635Z"},{"id":"q-4583","question":"Design an Azure data pipeline to ingest 100k events/sec from Azure Event Hubs into a Delta Lake on ADLS Gen2, with schema drift handled via Azure Schema Registry, end-to-end lineage in Purview, and upserts into Delta Lake using Spark (Synapse or Databricks). Address late events, out-of-order data, and cost with auto-scaling. What components and steps would you implement?","answer":"Implement a Spark Structured Streaming solution (Synapse or Databricks) to consume events from Azure Event Hubs, validate schemas through Azure Schema Registry integration, and perform upserts into Delta Lake on ADLS Gen2 using MERGE operations on the business key. Enable dynamic schema evolution via Schema Registry, apply watermarking to handle late events, and leverage checkpointing for exactly-once processing guarantees. Configure auto-scaling for Spark clusters and Event Hubs throughput units to optimize costs while maintaining required throughput.","explanation":"## Why This Is Asked\nAssess practical streaming lakehouse design capabilities including schema registry integration, Delta Lake upserts, Purview lineage tracking, late data handling, and cost optimization strategies.\n\n## Key Concepts\n- Spark Structured Streaming with Event Hubs connector\n- Azure Schema Registry for schema drift management\n- Delta Lake on ADLS Gen2 with MERGE-based upserts\n- Purview lineage integration for end-to-end tracking\n- Watermarking for late event handling and checkpointing for exactly-once semantics\n- Auto-scaling configurations for cost-effective throughput management\n\n## Code Example\n```javascript\n// Pseudo: upsert into Delta Lake from a micro-batch\nfunction upsert(deltaTablePath, batch) {\n  // merge on business_key\n  deltaTable.as('target')\n    .merge(batch.as('source'), 'target.business_key = source.business_key')\n    .whenMatched().updateAll()\n    .whenNotMatched().insertAll()\n    .execute();\n}\n```","diagram":"flowchart TD\n  A[Event Hubs] --> B[Azure Schema Registry]\n  B --> C[Spark Structured Streaming]\n  C --> D[Delta Lake on ADLS Gen2]\n  D --> E[Purview lineage]","difficulty":"intermediate","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Instacart"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T05:54:53.302Z","createdAt":"2026-01-20T02:35:26.737Z"},{"id":"q-4629","question":"Context: You’re building an Azure data platform for a global ad-tech customer. Data arrives from partners as CSV via SFTP, JSON via REST, and a streaming source via Event Hubs. Ingest into ADLS Gen2, publish a Delta Lake lakehouse, and expose data products with end-to-end lineage in Purview. You must enforce per‑product data contracts, handle schema drift, and keep upserts into a star schema in Synapse. Design the pipeline, contracts, and monitoring strategy?","answer":"Architect a pipeline with ADLS Gen2 landing, Databricks Delta Lake upserts into a Synapse star schema, and Purview for end-to-end lineage and per‑product data contracts. Ingest SFTP CSV, REST JSON, an","explanation":"## Why This Is Asked\nTests end-to-end Azure data platform design: multi-source ingestion, lakehouse with Delta Lake, data contracts per product, lineage via Purview, and practical handling of drift and upserts.\n\n## Key Concepts\n- Delta Lake upserts into Synapse lakehouse\n- Data contracts per product to govern schemas and metadata\n- Drift handling with versioned schemas and schema evolution\n- Multi-source ingestion: SFTP, REST, Event Hubs\n- Purview for lineage, governance, and access contracts\n- Monitoring, alerting, and cost optimization\n\n## Code Example\n```python\n# PySpark upsert example (Delta Lake)\nfrom delta.tables import DeltaTable\n\ndelta_path = \"/data/facts/delta\"\nnew_df = spark.read.format(\"json\").load(\"/path/to/new_facts.json\")\ndelta = DeltaTable.forPath(spark, delta_path)\ndelta.alias(\"t\").merge(\n  new_df.alias(\"s\"),\n  \"t.id = s.id\"\n).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n```\n\n## Follow-up Questions\n- How would you test and enforce cross‑product data contracts in Purview?\n- How would you handle late-arriving data and schema drift across regions?","diagram":"flowchart TD\n  A[Partner data] --> B[Landing ADLS Gen2]\n  B --> C[Databricks Delta Lake]\n  C --> D[Synapse Star Schema]\n  D --> E[Purview Lineage & Data Products]\n","difficulty":"intermediate","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","LinkedIn","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T05:47:07.572Z","createdAt":"2026-01-20T05:47:07.572Z"},{"id":"q-4749","question":"Design an end-to-end pipeline to ingest real-time CDC from an on-prem SQL Server into ADLS Gen2, then populate a star schema in Azure Synapse. The solution must handle schema drift, late-arriving data, and ensure end-to-end lineage in Purview. Specify components, data formats, upsert strategy, partitioning, and validation/monitoring?","answer":"Use Databricks Structured Streaming to read SQL Server CDC via a CDC connector, write to Delta Lake on ADLS Gen2 with schema evolution enabled, then MERGE into a star schema in Azure Synapse. Preserve","explanation":"## Why This Is Asked\n\nThis question tests practical design of real-time CDC pipelines across on-prem to Azure, handling drift, late data, upserts, and governance.\n\n## Key Concepts\n\n- CDC ingestion, Spark Structured Streaming, Delta Lake schema evolution, MERGE upserts, Purview lineage, watermarking, late data handling, idempotent writes, monitoring.\n\n## Code Example\n\n```sql\nMERGE INTO delta.`/datalake/sa/star/fact_sales` AS target\nUSING updates AS source\nON target.sale_id = source.sale_id\nWHEN MATCHED THEN UPDATE SET target.* = source.*\nWHEN NOT MATCHED THEN INSERT *;\n```\n\n## Follow-up Questions\n\n- How would you validate lineage end-to-end across sources and sinks in Purview?\n- How would you implement backfill for late data without duplicate counts?","diagram":"flowchart TD\n  A[On-prem SQL Server CDC] --> B[Databricks Structured Streaming]\n  B --> C[Delta Lake on ADLS Gen2]\n  C --> D[Azure Synapse Star Schema]\n  C --> Purview[Purview lineage]\n  D --> Monitor[Azure Monitor / Alerts]","difficulty":"advanced","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T10:50:14.556Z","createdAt":"2026-01-20T10:50:14.556Z"},{"id":"q-4768","question":"Scenario: design a real-time analytics pipeline for a global retailer. Ingest events from Event Hubs into ADLS Gen2 as JSON; auto-evolve schema; enforce per-tenant isolation and dynamic masking; capture end-to-end lineage in Purview; feed near real-time dashboards in Synapse within 5 seconds. Describe dataflow, components, schema governance, masking, latency monitoring, and cost/trade-offs of Parquet vs Delta with streaming?","answer":"Event Hubs -> ADLS Gen2 landing (JSON) with strict schema on read; Databricks Structured Streaming writes to Delta Lake with auto schema evolution and MERGE UPSERT into a tenant-scoped fact table; per","explanation":"## Why This Is Asked\\n\\nTests ability to design a compliant, low-latency lakehouse with streaming, governance, and tenant isolation. It blends data engineering, cost optimization, and operational monitoring.\\n\\n## Key Concepts\\n\\n- Real-time ingestion and Delta Lake upserts\\n- Schema drift handling and evolution\\n- Tenant isolation and dynamic masking\\n- Purview lineage and RBAC governance\\n- Latency monitoring and cost trade-offs\\n\\n## Code Example\\n\\n```javascript\\n// Example merge pattern (conceptual)\\nMERGE INTO target t USING source s ON t.id = s.id WHEN MATCHED THEN UPDATE SET t.value = s.value;\\n```\\n\\n## Follow-up Questions\\n\\n- How would you validate latency under backpressure?\\n- How would you scale to 10x event rate while preserving SLAs?","diagram":null,"difficulty":"advanced","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Microsoft","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T11:39:40.833Z","createdAt":"2026-01-20T11:39:40.834Z"},{"id":"q-4993","question":"Scenario: A high-velocity telemetry feed from IoT devices streams JSON events to Azure Event Hubs. Schema evolves weekly. Design an end-to-end pipeline that ingests to ADLS Gen2 with drift-tolerant parsing, materializes a star schema in Azure Synapse, and delivers near-real analytics in under 2 minutes. Ensure end-to-end data lineage with Purview, incremental loads, upserts for dimensions, and late-arriving data handling. Describe components, dataflow, and failure modes?","answer":"Ingest through Azure Event Hubs into Spark Structured Streaming with Delta Lake on ADLS Gen2, enabling mergeSchema=true for schema evolution. Store data in Delta tables partitioned by date; implement MERGE operations for dimension table upserts and append-only writes for fact tables. Apply event-time watermarks to handle late-arriving data, enable Azure Purview for end-to-end data lineage, and establish comprehensive monitoring with retry policies to ensure fault tolerance.","explanation":"## Why This Is Asked\nTests the ability to design a scalable streaming and batch pipeline that handles schema drift, implements upserts, maintains data lineage, and addresses late data with proper failure handling.\n\n## Key Concepts\n- Azure Event Hubs for high-throughput ingestion\n- Spark Structured Streaming with Delta Lake on ADLS Gen2\n- Schema drift tolerance using mergeSchema=true\n- MERGE operations for dimension table upserts\n- Event-time watermarks for late-arriving data handling\n- End-to-end data lineage with Azure Purview\n- Comprehensive monitoring and retry policies\n\n## Code Example\n```","diagram":"flowchart TD\n  A[Event Hubs] --> B[Spark Structured Streaming]\n  B --> C[Delta Lake on ADLS Gen2]\n  C --> D[Delta tables in Synapse (star schema)]\n  D --> E[Purview Lineage]","difficulty":"advanced","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","Microsoft","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T05:47:59.645Z","createdAt":"2026-01-20T22:45:09.003Z"},{"id":"q-5016","question":"Scenario: Ingest daily JSON event logs from three partners delivered to ADLS Gen2 under /landing/events/. Each file may gain new fields over time. Build a beginner Data Factory pipeline to (1) stage raw data in /landing/staged, (2) normalize schema with a Mapping Data Flow to a stable Parquet schema, (3) load to a simple star schema in Azure Synapse with a date dim and a fact_event table. Use a watermark for incremental loads, implement basic data quality checks (not-null event_id, valid timestamp), and capture lineage in Purview. What services and steps would you implement?","answer":"Implement a two-pipeline Azure Data Factory solution: (1) an ingestion pipeline that copies raw JSON files from /landing/events/ to /landing/staged/ using a Copy activity with fault tolerance, and (2) a processing pipeline featuring a Mapping Data Flow that normalizes evolving schemas to a stable Parquet format before loading to Azure Synapse via MERGE operations into a star schema with dim_date and fact_event tables. Enable incremental processing through watermark-based change detection on event timestamps, incorporate data quality validation for non-null event_id and valid timestamp fields, and establish comprehensive lineage tracking with Azure Purview for end-to-end governance.","explanation":"## Why This Is Asked\nThis assesses practical experience with schema drift management, end-to-end data orchestration, and governance capabilities in a foundational Azure data engineering scenario that mirrors real-world enterprise challenges.\n\n## Key Concepts\n- Azure Data Factory pipelines with Copy activities and Mapping Data Flows\n- Schema normalization from JSON to Parquet format with drift handling\n- Watermark-based incremental loading strategies for efficiency\n- MERGE operations for dimensional modeling in Synapse\n- Data quality validation frameworks and Purview lineage integration\n- Star schema design patterns with date dimensions and fact tables\n\n## Implementation Approach\nThe solution demonstrates understanding of incremental processing patterns, schema evolution challenges, and data governance requirements while maintaining scalability and reliability in a production environment.","diagram":null,"difficulty":"beginner","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T05:23:40.119Z","createdAt":"2026-01-20T23:56:19.730Z"},{"id":"q-5070","question":"In a global Azure data lake for a food delivery platform, ingest streaming events from Azure Event Hubs into Delta Lake on ADLS Gen2. Design an end-to-end pipeline that handles late data via watermarking, performs schema evolution on nested JSON payloads, and upserts a slowly changing dimension in Synapse. Include data lineage in Purview and cost controls across regions. What components and approach would you choose, and why?","answer":"Spark Structured Streaming on Azure Databricks reads from Event Hubs and writes Delta Lake tables on ADLS Gen2 with schema evolution enabled. Use MERGE INTO in Synapse for SCD Type 2 upserts. Purview ","explanation":"## Why This Is Asked\nTests real-world streaming ingestion, schema drift handling, and robust upserts with governance in a multi-region Azure setup.\n\n## Key Concepts\n- Delta Lake schema evolution and nested JSON handling\n- Spark Structured Streaming from Event Hubs\n- MERGE for SCD Type 2 in Synapse\n- Purview lineage and governance in a hybrid/multi-region environment\n- Cost controls via regional compute sizing and auto-scaling\n\n## Code Example\n```python\nfrom pyspark.sql.functions import from_json, col\nfrom pyspark.sql.types import StructType, StructField, StringType\n\nschema = StructType([\n  StructField('orderId', StringType()),\n  StructField('restaurantId', StringType()),\n  StructField('payload', StructType([\n    StructField('items', StringType())\n  ]))\n])\n\ndf = (spark.readStream\n       .format('eventhubs')\n       .options(**{ 'eventhubs.connectionString': '<conn-string>' })\n       .load()\n       .select(from_json(col('body').cast('string'), schema).alias('data'))\n       .select('data.*')\n)\n\ndeltaPath = '/mnt/delta/events'\ncheckpoint = '/mnt/checkpoints/events'\n\nquery = (df.writeStream\n         .format('delta')\n         .option('checkpointLocation', checkpoint)\n         .option('path', deltaPath)\n         .start())\n```","diagram":null,"difficulty":"intermediate","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","MongoDB","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T04:28:35.966Z","createdAt":"2026-01-21T04:28:35.966Z"},{"id":"q-5090","question":"In an Azure-based data lakehouse for a global e-commerce platform, ingest high-velocity clickstream from Event Hubs and batch product data from partner REST APIs into ADLS Gen2. Use Apache Hudi to support upserts and time-travel, implement bronze-silver-gold layers, and expose clean data products in Synapse. Describe your end-to-end design: ingestion, schema evolution, CDC propagation, governance with Purview, data masking for PII, and cost-aware storage tiering?","answer":"Propose bronze landing in ADLS Gen2 ingesting Event Hubs streams and partner REST data with Apache Hudi on Spark to enable upserts and time-travel. Silver layer enforces a conformed schema with date-b","explanation":"## Why This Is Asked\nTests ability to design a lakehouse on Azure with upserts, time travel, schema evolution, CDC propagation, governance, and cost-aware storage.\n\n## Key Concepts\n- Apache Hudi on Azure Databricks/Spark\n- Bronze-Silver-Gold data lakehouse pattern\n- Upserts, CDC, time-travel support\n- ADLS Gen2 and ABFSS paths\n- Purview for lineage and governance\n- Data masking for PII in pipelines\n- Storage tiering and lifecycle management\n\n## Code Example\n\n```python\n# PySpark example: write to Hudi bronze path (simplified)\ndf.write.format(\"hudi\").options(**{\n  \"hoodie.datasource.write.table_type\": \"COPY_ON_WRITE\",\n  \"hoodie.datasource.write.recordkey\": \"id\",\n  \"hoodie.datasource.write.precombine.field\": \"ts\"\n}).mode(\"append\").save(\"abfss://data@storage.dfs.core.windows.net/bronze/events\")\n```\n\n## Follow-up Questions\n- How would you scale the architecture for multi-terrabyte daily ingest?\n- What are the trade-offs between COPY_ON_WRITE and MERGE_ON_READ in this setup?","diagram":"flowchart TD\n  Bronze[Bronze: Landing] --> Silver[Silver: Conformed]\n  Silver --> Gold[Gold: Data Products]\n  Bronze --> Purview[Purview: Lineage]\n  Gold --> Storage[Storage Tiering: HOT->COOL]","difficulty":"intermediate","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T05:51:20.522Z","createdAt":"2026-01-21T05:51:20.522Z"},{"id":"q-5104","question":"Design a cost-aware real-time ELT pipeline: ingest multi-region telemetry from Event Hubs into ADLS Gen2, process with Synapse Spark Structured Streaming, and upsert into Delta Lake tables; ensure end-to-end Purview lineage and automated data quality checks. Describe components, partitioning, idempotency, and failure handling?","answer":"Ingest multi-region telemetry via Event Hubs, land to ADLS Gen2, run Synapse Spark Structured Streaming with mergeSchema=true, and upsert into Delta Lake tables. Partition by region/date; watermark la","explanation":"## Why This Is Asked\nAssess ability to architect a real-time, cost-aware Azure data stack with schema drift handling, upserts, and governance.\n\n## Key Concepts\n- Event Hubs ingestion, ADLS Gen2 tiering\n- Spark Structured Streaming, Delta Lake MERGE\n- Schema drift handling with mergeSchema and late data via watermark\n- Purview for lineage; cost controls via autoscale and partitioning\n- Data quality gates (Deequ/constraints) in streaming jobs\n\n## Code Example\n```python\n# PySpark snippet for incremental upserts into Delta Lake\nfrom delta.tables import DeltaTable\n\ndef upsert(batch_df, batch_id):\n    delta = DeltaTable.forPath(spark, \"/mnt/delta/telemetry\")\n    delta.alias(\"t\").merge(batch_df.alias(\"s\"), \"t.id = s.id\") \\\n        .whenMatchedUpdate(set={\"value\": \"s.value\", \"ts\": \"s.ts\"}) \\\n        .whenNotMatchedInsertAll() \\\n        .execute()\n\nstreaming_df.writeStream.foreachBatch(upsert).start()\n```\n\n## Follow-up Questions\n- How would you test quality gates in streaming?\n- How would you handle late-arriving schema changes in Delta Lake?\n","diagram":null,"difficulty":"advanced","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["NVIDIA","Oracle","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T06:50:17.397Z","createdAt":"2026-01-21T06:50:17.397Z"},{"id":"q-5434","question":"Design a streaming ELT pipeline that ingests real-time user activity from Azure Event Hubs into Delta Lake on ADLS Gen2, handles schema drift with dynamic inference, maintains an SCD Type 2 customer dimension, and captures end-to-end lineage in Purview. Which processing engine would you choose (Synapse Spark vs Databricks) and why, and how would you configure partitioning, watermarking, upserts, and autoscale to balance latency, cost, and reliability?","answer":"Use Azure Databricks with Delta Lake. Ingest via Structured Streaming from Azure Event Hubs, implement MERGE operations into Delta tables for SCD Type 2 tracking, and capture comprehensive lineage through Azure Purview integration. Leverage Delta's automatic schema evolution to handle dynamic schema drift, configure partitioning based on event timestamp and customer ID for optimal query performance, apply appropriate watermarking to balance data completeness with processing latency, utilize auto-scaling to optimize cost-efficiency, and implement upserts through Delta Lake's MERGE capabilities to maintain dimensional accuracy.","explanation":"## Why This Is Asked\nTests ability to design enterprise-grade streaming pipelines with dynamic schema handling, dimensional modeling requirements, comprehensive lineage tracking, and strategic platform selection between major Spark offerings.\n\n## Key Concepts\n- Structured Streaming integration with Azure Event Hubs\n- Delta Lake schema evolution and automated drift handling\n- MERGE-based SCD Type 2 dimensional implementations\n- Azure Purview lineage capture and metadata management\n- Auto-scaling and cost optimization strategies for Spark platforms\n- Strategic partitioning and watermarking configuration\n- Platform comparison: Synapse Spark vs Databricks trade-offs\n\n## Code Example\n```javascript\n// Pseudo-code: Databricks Delta Lake SCD2 implementation\nconst customerDimension = DeltaTable.forPath(spark, 'abfss://container@account.dfs.core.windows.net/delta/customers')\nconst streamingDF = spark.readStream.format('eventhubs')\n  .option('eventhubs.connectionString', connectionString)\n  .load()\n  .select(from_json(col('body').cast('string'), schema).alias('data'))\n  .select('data.*')\n\nstreamingDF.writeStream.format('delta')\n  .foreachBatch((batchDF, batchId) => {\n    customerDimension.as('target')\n      .merge(batchDF.as('source'), 'target.customer_id = source.customer_id')\n      .whenMatched().updateAll()\n      .whenNotMatched().insertAll()\n      .execute()\n  })\n  .option('checkpointLocation', '/checkpoints/customers')\n  .start()\n```\n\n## Strategic Considerations\nChoose Databricks over Synapse for superior Delta Lake integration, advanced auto-scaling capabilities, and optimized streaming performance. Configure partitioning by event date and customer hash distribution, set watermarking at 5-10 minutes for latency/completeness balance, enable Photon acceleration for cost efficiency, and implement Purview lineage through Delta's automatic metadata capture.","diagram":null,"difficulty":"intermediate","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","LinkedIn"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T04:53:06.843Z","createdAt":"2026-01-21T22:11:14.384Z"},{"id":"q-5529","question":"You manage telemetry from millions of IoT devices feeding Azure IoT Hub. Design a streaming ELT path to ingest into ADLS Gen2, apply drift-tolerant transformations, and expose a star schema in Azure Synapse. Requirements: end-to-end lineage in Purview, incremental loads with late-arriving data, schema evolution handling, and integrated data quality checks with alerting. Which components, data flow, and governance placements would you propose?","answer":"Streaming-first design: devices → IoT Hub → Event Hubs → Azure Databricks with Delta Lake on ADLS Gen2; use schema evolution for drift, and MERGE-based upserts into a Dim/Fact star schema in Azure Syn","explanation":"## Why This Is Asked\n\nTests ability to design a real-time data path with schema drift handling, incremental loading, and governance across Azure services. Focuses on streaming ETL, upserts, and end-to-end lineage—areas often probed in senior roles at data-centric companies.\n\n## Key Concepts\n\n- Streaming ELT patterns with IoT data\n- Delta Lake schema evolution and upserts\n- Incremental loads into Azure Synapse (MERGE)\n- Late-arriving data handling via watermarking\n- Data quality with Deequ and alerting\n- Purview lineage and data governance\n\n## Code Example\n\n```javascript\n// Pseudo-pipeline skeleton (illustrative, not executable)\nconst stream = readFromEventHub();\nconst curated = stream.transform(driftTolerantSchema);\ncurated.writeDeltaLake('adlgen2/curated');\nlet upsert = curated.mergeIntoStarSchema('synapse.dw.fact_dim');\nmonitorQuality(curated, 'purview');\n```\n\n## Follow-up Questions\n\n- How would you handle backfill for late-arriving events without breaking lineage?\n- What latency targets would you set, and how would you tune Spark pools and MERGE operations to meet them?","diagram":"flowchart TD\n  A[IoT devices] --> B[Azure IoT Hub]\n  B --> C[Event Hubs / Stream]\n  C --> D[Azure Databricks (Delta Lake on ADLS Gen2)]\n  D --> E[Azure Synapse Analytics (Star Schema)]\n  E --> F[Purview lineage]\n  D --> G[Quality checks: Deequ]","difficulty":"advanced","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Netflix","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T04:30:01.668Z","createdAt":"2026-01-22T04:30:01.669Z"},{"id":"q-5602","question":"Design an Azure-based streaming ETL for real-time finance events: events arrive via Azure Event Hubs in JSON with evolving fields. Ingest to ADLS Gen2 using Delta Lake, create a Bronze raw layer, a Silver layer with drift-tolerant transformations and upserts, and a curated Analytics table. Ensure end-to-end lineage in Purview, schema evolution, and data quality checks. Handle late data and tombstones; optimize for sub-2s latency?","answer":"Use Event Hubs → Spark streaming (Databricks or Synapse) to ADLS Gen2. Bronze stores raw JSON; Silver applies drift-tolerant transforms and upserts into Delta tables, with a separate curated table for","explanation":"## Why This Is Asked\nGauges practical streaming data engineering skills: ingestion from Event Hubs, Delta Lake layering, schema drift handling, and data governance.\n\n## Key Concepts\n- Streaming ELT with Delta Lake on ADLS Gen2\n- Drift-tolerant transforms and upserts\n- Data lineage with Purview\n- Late data handling and tombstones\n- Latency and cost trade-offs\n\n## Code Example\n```javascript\n// Example: define a streaming read and write with Delta Lake\n```\n\n## Follow-up Questions\n- How would you test schema drift and implement automatic evolution in production?\n- How would you validate end-to-end lineage across all layers?","diagram":"flowchart TD\n  EH[Event Hubs] --> SB[Streaming Job]\n  SB --> BR[Bronze Delta]\n  BR --> SV[Silver Delta (curated/validated)]\n  SV --> GL[Analytics/Feature Delta]\n  GL --> Pur[Purview Lineage]","difficulty":"intermediate","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Anthropic","Coinbase"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T07:55:22.604Z","createdAt":"2026-01-22T07:55:22.605Z"},{"id":"q-5621","question":"Design an Azure data platform for a global streaming app that ingests real-time events from Azure Event Hubs and nightly batches via SFTP into ADLS Gen2. Deliver a star schema in Azure Synapse and maintain end-to-end lineage in Purview. Specify components, data model, and pipeline steps, including schema drift handling, late-arrival data, and GDPR purge semantics. What would you implement?","answer":"Use Databricks Autoloader with Delta Lake on ADLS Gen2 to ingest Event Hubs and SFTP batches, applying schema drift tolerant MERGE into a Delta table, then upsert into a Synapse star schema. Enable en","explanation":"## Why This Is Asked\n- Tests multi-source ingestion, schema drift handling, and governance in Azure.\n\n## Key Concepts\n- Event Hubs + Autoloader, Delta Lake, Delta Live Tables, Synapse,\n  Purview, watermarking, GDPR purge, incremental upserts.\n\n## Code Example\n```python\n# (Pseudo) Spark upsert into Delta lake and MERGE into star schema\n```\n\n## Follow-up Questions\n- How would you test end-to-end lineage sanity? \n- How would you optimize for latency and cost?","diagram":"flowchart TD\n  A[Event Hubs real-time] --> B[Databricks Autoloader + Structured Streaming]\n  B --> C[Delta Lake on ADLS Gen2]\n  D[SFTP batch via ADF] --> C\n  C --> E[Azure Synapse Star Schema]\n  E --> F[Purview for lineage]","difficulty":"advanced","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Netflix","Snowflake","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T08:54:39.534Z","createdAt":"2026-01-22T08:54:39.535Z"},{"id":"q-5664","question":"You are building a beginner Azure data ingestion workflow: daily data arrives as CSV and JSON from multiple partners into ADLS Gen2 under /landing. Design a concrete Data Factory pipeline that stages raw data, flattens nested JSON, handles schema drift, and loads into a date-partitioned star schema in Azure Synapse. Use a watermark-based incremental load and include basic data quality checks. What components and steps would you implement?","answer":"Ingest daily CSV/JSON from multiple partners into ADLS Gen2 under /landing; use a Data Factory pipeline with a Data Flow to flatten nested JSON, infer/handle drift, stage to Parquet, and load into a d","explanation":"## Why This Is Asked\nAssesses practical, beginner-friendly ability to design end-to-end Azure data ingestion with multi-source inputs, drift tolerance, and incremental loads.\n\n## Key Concepts\n- Multi-source Ingestion with Data Factory\n- Data Flow flattening for nested JSON\n- Schema drift handling and Parquet staging\n- Incremental loads via watermark\n- Star schema in Azure Synapse\n- Basic data quality checks and tests\n\n## Code Example\n```sql\nMERGE INTO dbo.FactSales AS target\nUSING (SELECT * FROM staging.dbo.FactSales WHERE ingestion_date > (SELECT MAX(ingestion_date) FROM dbo.FactSales)) AS src\nON target.SalesKey = src.SalesKey\nWHEN MATCHED THEN UPDATE SET ...\nWHEN NOT MATCHED THEN INSERT (...);\n```\n\n## Follow-up Questions\n- How would you test incremental loads and drift handling in CI?\n- What monitoring and alerting would you add for data quality failures?","diagram":"flowchart TD\n  A[Partner CSV/JSON] --> B[ADLS Landing /landing]\n  B --> C[Data Factory Pipeline]\n  C --> D[Staging Parquet/Delta]\n  D --> E[Flattened JSON and drift handling]\n  E --> F[Azure Synapse (Star Schema)]","difficulty":"beginner","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T10:46:20.178Z","createdAt":"2026-01-22T10:46:20.178Z"},{"id":"q-5713","question":"Global IoT telemetry streams into ADLS Gen2 via Event Hubs. Data arrives JSON and Parquet with evolving schemas. Design an end-to-end pipeline: structured streaming intake, Delta Lake with schema evolution, an SCD2 device dimension in the curated layer, downstream dashboards in Synapse, Purview lineage, and protections for late data/out-of-order events plus cost controls?","answer":"Use Event Hubs ingestion with Spark Structured Streaming to process JSON/Parquet, write to Delta Lake bronze/silver/gold with schema evolution, implement an SCD2 device dimension in silver, publish mo","explanation":"## Why This Is Asked\nTests ability to design real-time data pipelines with evolving schemas, Delta Lake capabilities, SCD2 modeling in lakehouse, Purview lineage integration, and cost/latency trade-offs.\n\n## Key Concepts\n- Structured Streaming with Delta Lake schema evolution\n- SCD Type 2 in lakehouse\n- Late data handling and watermarking\n- Purview lineage integration\n- Cost optimization with autoscaling\n\n## Code Example\n```python\n# PySpark Delta Lake upsert example\nfrom delta.tables import DeltaTable\n\ndelta_path = \"/mnt/delta/devices_silver\"\ndeltaTable = DeltaTable.forPath(spark, delta_path)\n\nnew_df = bronze_df.filter(\"event_type = 'device_update'\")\n\ndeltaTable.alias(\"t\").merge(\n  new_df.alias(\"s\"),\n  \"t.device_id = s.device_id\"\n).whenMatchedUpdate(set={\n  \"device_type\": \"s.device_type\",\n  \"last_seen\": \"s.timestamp\"\n}).whenNotMatchedInsert(values={\n  \"device_id\": \"s.device_id\",\n  \"device_type\": \"s.device_type\",\n  \"first_seen\": \"s.timestamp\",\n  \"last_seen\": \"s.timestamp\"\n}).execute()\n```\n\n## Follow-up Questions\n- How would you test schema evolution compatibility across regions?\n- How would you implement data quality gates before the gold layer?","diagram":"flowchart TD\n  A[Event Hubs] --> B[Structured Streaming]\n  B --> C[Bronze Delta]\n  C --> D[Silver Delta (Schema Evolution, SCD2)]\n  D --> E[Gold Delta]\n  E --> F[Synapse Dashboards]\n  F --> G[Purview]","difficulty":"intermediate","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Salesforce","Square","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T13:10:07.266Z","createdAt":"2026-01-22T13:10:07.266Z"},{"id":"q-5843","question":"Scenario: You run a multi-tenant analytics platform on Azure. Telemetry from devices arrives via Event Hubs (JSON with evolving schemas). Ingest into ADLS Gen2, store in Delta Lake with schema drift tolerance, and incremental MERGE upserts. Ensure end-to-end lineage in Purview and tenant-based RBAC with dynamic masking. Describe architecture, data contracts, and governance trade-offs?","answer":"Proposed architecture: Ingest from Event Hubs into ADLS Gen2 raw, use Databricks Delta Lake with schema drift tolerance and MERGE-based upserts for incremental loads, and maintain end-to-end lineage i","explanation":"## Why This Is Asked\nTests ability to design scalable, governance-aware data pipelines on Azure with schema drift handling, lineage, and tenant isolation.\n\n## Key Concepts\n- Streaming + batch integration\n- Delta Lake schema evolution and MERGE upserts\n- Purview lineage, RBAC, masking\n\n## Code Example\n```javascript\n// Pseudo: demonstrate a simple upsert-like merge\nfunction upsert(target, source) {\n  source.forEach(s => {\n    const i = target.findIndex(t => t.id === s.id);\n    if (i >= 0) target[i] = { ...target[i], ...s };\n    else target.push(s);\n  });\n  return target;\n}\n```\n\n## Follow-up Questions\n- How would you test schema drift handling across tenants?\n- What are trade-offs between Databricks vs Synapse for this workload?","diagram":"flowchart TD\n  A[Event Hubs: Ingest] --> B[Raw Gen2 Landing]\n  B --> C[Delta Lake: Schema Drift]\n  C --> D[MERGE Upserts]\n  D --> E[Purview: Lineage & Policies]\n  E --> F[Tenant RBAC + Dynamic Masking]\n  F --> G[Analytics: Synapse/Power BI]","difficulty":"advanced","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","IBM","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T19:03:23.414Z","createdAt":"2026-01-22T19:03:23.414Z"},{"id":"q-5855","question":"Design a near-real-time data product pipeline to ingest CDN and in-app telemetry into ADLS Gen2 at scale, producing per-customer gold tables while preserving lineage and governance. What components do you pick, how do you handle schema drift and late data, and how do you expose the data as governed, discoverable data products?","answer":"Use Databricks Delta Live Tables for streaming ingestion into ADLS Gen2 with bronze landing, silver curated using Delta Lake schema evolution, and gold per-customer data products. Implement MERGE upse","explanation":"## Why This Is Asked\nThis question probes practical streaming lakehouse design, data productization, and governance in Azure. It tests selection of Delta Live Tables, Delta Lake schema evolution, and a clean bronze-silver-gold path, plus lineage via Purview and serving through Synapse.\n\n## Key Concepts\n- Streaming lakehouse with Delta Live Tables\n- Schema drift handling and Delta Lake schema evolution\n- Late-arriving data with watermarking and idempotent upserts\n- Data products per customer and discoverability\n- End-to-end lineage and RBAC via Purview; BI exposure via Synapse\n\n## Code Example\n```sql\nMERGE INTO silver.customer_events AS s\nUSING bronze.customer_events AS b\nON s.event_id = b.event_id\nWHEN MATCHED THEN UPDATE SET s.value = b.value, s.event_ts = b.event_ts\nWHEN NOT MATCHED THEN INSERT (event_id, customer_id, value, event_ts) VALUES (b.event_id, b.customer_id, b.value, b.event_ts)\n```\n\n## Follow-up Questions\n- How would you test the data quality and lineage as new data products are added?\n- What monitoring and alerting would you implement for schema drift and late data scenarios?","diagram":"flowchart TD\n  A[CDN & In-app Telemetry] --> B[Ingestion: Delta Live Tables]\n  B --> C[Bronze Landing /landing]\n  C --> D[Silver Curated /silver (schema evo)]\n  D --> E[Gold Data Products per Customer]\n  E --> F[Purview Lineage & RBAC]\n  E --> G[Synapse BI Layer]","difficulty":"intermediate","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Netflix","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T19:33:42.023Z","createdAt":"2026-01-22T19:33:42.023Z"},{"id":"q-5927","question":"You are a beginner Azure data engineer tasked with integrating three regional partners delivering daily data dumps via SFTP, each with evolving schemas. Design a low-maintenance Azure data pipeline that lands raw CSVs to ADLS Gen2 (per region), infers schema drift into a canonical web_events schema, and loads to a simple star schema in Azure Synapse. Include incremental loads using per-file watermark, data lineage in Purview, and DR replication to a secondary ADLS region. What components and steps would you implement?","answer":"Use Azure Data Factory to orchestrate three regional pipelines. Ingest daily SFTP CSVs to ADLS Gen2 in /landing/region directories. Apply a schema-drift tolerant Data Flow to map to a canonical Parquet web_events schema, then load incrementally to a star schema in Azure Synapse using per-file watermarks. Configure Azure Purview for data lineage tracking and enable ADLS Geo-replication to a secondary region for disaster recovery.","explanation":"Why This Is Asked: Evaluates practical familiarity with basic Azure data integration patterns, handling schema drift, and ensuring lineage and DR in a beginner-friendly scenario. Key concepts: data ingestion with ADF, schema drift tolerance via Data Flows, canonical schema design, incremental loads with per-file watermark, Purview lineage, and DR via ADLS geo-replication. Code Example: See the minimal pipeline skeleton below. Follow-up Questions: 1) How would you test drift tolerance? 2) How would you monitor incremental loads?","diagram":"flowchart TD\n  A[Partner Regions] --> B[ADF Pipelines per Region]\n  B --> C[Landing in ADLS Gen2 /landing/region]\n  C --> D[Schema Drift Tolerant Data Flow -> Canonical web_events Parquet]\n  D --> E[Load to Synapse Star Schema]\n  E --> F[Purview Lineage] --> G[Geo-Replication to DR ADLS]\n  style A fill:#f9f,stroke:#333,stroke-width:1px\n  style G fill:#bbf,stroke:#333,stroke-width:1px","difficulty":"beginner","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Lyft","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T05:44:55.331Z","createdAt":"2026-01-22T22:35:03.833Z"},{"id":"q-6014","question":"You are tasked with a beginner-friendly Azure data pipeline: daily CSVs arrive via SFTP into a landing folder under /incoming. Design a Data Factory workflow that stages raw data in ADLS Gen2, handles schema drift using Parquet and schema evolution, and loads a simple star schema into Azure Synapse. Add a lightweight data quality gate (not-null user_id, valid event_date) and a basic lineage capture to Purview. Include an incremental load approach and a plan for alerting if ingestion lags?","answer":"Use Data Factory to copy CSVs from SFTP to ADLS Gen2 landing, then a Mapping Data Flow to infer/handle drift and write as Parquet. MERGE into a Star schema in Synapse (fact/dim) using a last_modified ","explanation":"## Why This Is Asked\nTests end-to-end ingestion, drift tolerance, incremental loading, data quality gates, and governance.\n\n## Key Concepts\n- Data Factory mapping data flows for drift\n- Parquet schema evolution\n- Incremental load with watermark\n- MERGE into Synapse star schema\n- Purview lineage and basic monitoring\n\n## Code Example\n```json\n{ \"pipeline\": \"IngestCSVToSynapse\", \"stages\": [\"SFTP -> ADLS landing\", \"Data Flow: infer schema -> Parquet\", \"Synapse MERGE into star schema\", \"Purview lineage\", \"Monitor alerts\"] }\n```\n\n## Follow-up Questions\n- How would you adapt for varying file sizes to avoid Synapse hot spots?\n- How would you handle late-arriving data or corrupted files without breaking the pipeline?","diagram":null,"difficulty":"beginner","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","Microsoft","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T04:30:37.836Z","createdAt":"2026-01-23T04:30:37.836Z"},{"id":"q-6141","question":"Describe an end-to-end pipeline in Azure Data Factory that ingests CSV and JSON dumps from SFTP into ADLS Gen2 /landing, normalizes to a canonical web_events Parquet schema in /staging, and MERGEs into a Synapse star schema. Use per-file watermark for incremental loads, capture lineage in Purview, and add basic data quality checks (nulls, timestamp validity)?","answer":"Implement an ADF pipeline that copies CSV/JSON from SFTP to ADLS Gen2 /landing, uses a Data Flow to map both formats to a canonical web_events Parquet schema in /staging, then MERGEs into a Synapse st","explanation":"## Why This Is Asked\n\nThis question checks practical ability to wire Azure services for a common beginner scenario and reason about drift, idempotence, lineage, and quality checks.\n\n## Key Concepts\n\n- Azure Data Factory: Copy + Data Flow\n- ADLS Gen2 landing and staging with Parquet canonicalization\n- Synapse MERGE-based upserts\n- Per-file watermark for incremental loads\n- Purview lineage and basic data quality checks\n\n## Code Example\n\n```javascript\nMERGE INTO dw.web_events AS t\nUSING staging.web_events_parquet AS s\nON t.event_id = s.event_id\nWHEN MATCHED THEN UPDATE SET t.user_id = s.user_id, t.event_type = s.event_type, t.timestamp = s.timestamp\nWHEN NOT MATCHED THEN INSERT (event_id, user_id, event_type, timestamp) VALUES (s.event_id, s.user_id, s.event_type, s.timestamp);\n```\n\n## Follow-up Questions\n\n- How would you handle missing fields or nested JSON properties across partners?\n- How would you test incremental loads and name-based partitioning for Purview lineage?","diagram":"flowchart TD\n  A[Partner data: CSV/JSON via SFTP] --> B[Landing: /landing ADLS Gen2]\n  B --> C[ADF: Copy to /staging and Data Flow: canonical web_events Parquet]\n  C --> D[Synapse: MERGE into Star Schema]\n  D --> E[Purview: Lineage]\n  E --> F[DR to secondary region]","difficulty":"beginner","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Cloudflare","Discord"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T10:48:03.167Z","createdAt":"2026-01-23T10:48:03.167Z"},{"id":"q-6492","question":"Design a cross-region Azure Data Lakehouse for a payment platform: real-time transaction events flow from Azure Event Hubs (JSON) at ~2–5 MB/s; daily partner CSVs land via SFTP into ADLS Gen2. Requirements: (1) Delta Lake with schema evolution and upserts for SCDs, (2) under 2-minute latency for dashboards, (3) end-to-end data lineage to Purview, (4) cost-efficient storage with tiering and retention, (5) disaster recovery via cross-region replication. Which components and data flow would you implement and why?","answer":"Azure Databricks Delta Lake on ADLS Gen2; Event Hubs -> Structured Streaming to bronze Delta; MERGE upserts to silver (SCD1/2). Ingest SFTP CSVs via Data Factory into bronze and MERGE to silver. Purvi","explanation":"## Why This Is Asked\nThis question tests cross-region lakehouse design with real-time + batch ingestion, governance, drift handling, and DR.\n\n## Key Concepts\n- Delta Lake MERGE for upserts and SCDs\n- Spark Structured Streaming + Event Hubs in Azure Databricks\n- SFTP ingestion via Data Factory\n- Purview lineage and data governance\n- Cross-region replication (CRR) for DR\n- Storage tiering and lifecycle management\n\n## Code Example\n```python\nfrom delta.tables import DeltaTable\n# upsert into silver\nsilver_path = 'abfss://.../silver'\ndeltaTable = DeltaTable.forPath(spark, silver_path)\ndf = spark.read.format('delta').load('path_to_bronze_batch')\ndeltaTable.alias('t').merge(\n  df.alias('s'),\n  't.id = s.id'\n).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n```\n\n## Follow-up Questions\n- How would you test end-to-end latency and data quality?\n- What are failure modes in streaming vs batch paths and mitigations?","diagram":"flowchart TD\n  A[Event Hubs] --> B[Databricks: Structured Streaming]\n  B --> C[Delta Bronze]\n  C --> D[Delta Silver: upserts (SCD)]\n  E[SFTP land] --> F[Data Factory] --> G[Delta Bronze] \n  G --> H[Delta Silver]\n  I[Purview] --> J[Lineage]\n  K[ADLS Gen2 CRR] --> L[DR]\n  M[Synapse/BI] --> N[Dashboards]\n","difficulty":"advanced","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Slack","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T04:31:58.135Z","createdAt":"2026-01-24T04:31:58.135Z"},{"id":"q-6623","question":"You're a beginner Azure data engineer tasked with building a data lake and warehouse for partner data arriving via SFTP and HTTP in CSV/JSON. Data lands in ADLS Gen2, is transformed into a simple star schema in Azure Synapse, and you must automatically classify and mask PII at ingestion using built-in Azure tools, plus register datasets in Purview with basic lineage. Which components and steps would you implement?","answer":"Use Azure Data Factory to ingest from SFTP and HTTP into ADLS Gen2 landing; apply Mapping Data Flows to infer schema and mask PII (redact SSN, hash emails) and write to a curated Parquet lake; catalog","explanation":"## Why This Is Asked\nTests practical use of data ingestion, governance, masking, and a simple warehouse, plus Purview lineage.\n\n## Key Concepts\n- Ingestion from multiple sources into ADLS Gen2\n- Schema inference and drift handling via Data Flows\n- PII masking strategies in pipelines\n- Data catalog and lineage in Purview\n- Star schema in Synapse\n\n## Code Example\n```javascript\nfunction maskPII(value, type){\n  switch(type){\n    case 'email': return value.replace(/[^@]+@/, '***@');\n    case 'ssn': return value.replace(/\\d{3}-\\d{2}-\\d{4}/, 'XXX-XX-XXXX');\n    default: return 'MASKED';\n  }\n}\n```\n\n## Follow-up Questions\n- How would you test masking rules across schemas?\n- How would you validate lineage completeness in Purview?","diagram":"flowchart TD\n  A[Ingest: SFTP & HTTP] --> B[Landing in ADLS Gen2]\n  B --> C[DF: Schema inference & PII masking]\n  C --> D[Curated lake in ADLS Gen2]\n  D --> E[Azure Synapse: Star schema]\n  E --> F[Purview: Lineage & classification]","difficulty":"beginner","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Databricks"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T10:39:32.898Z","createdAt":"2026-01-24T10:39:32.898Z"},{"id":"q-6674","question":"Design an end-to-end Azure data platform for a multi-geo financial partner network. Ingest batch CSV/Parquet and streaming JSON records from three partners into ADLS Gen2 in two regions, support automatic schema evolution, build a customer data warehouse with a two-tenant star schema in Azure Synapse, maintain end-to-end lineage in Purview, enforce per-tenant access controls, implement PII masking and encryption at rest, manage cost with hot/cold storage and lifecycle rules, and enable DR failover with a 1-hour RPO. Outline architecture, components, data flows, and trade-offs?","answer":"Two-region ADLS Gen2 with geo-redundancy. Ingest batch via ADF + Spark (Synapse/Databricks) with Delta auto-evolution; stream via Event Hubs to landing zone. Build a curated star schema in Synapse; en","explanation":"## Why This Is Asked\n\nThis question probes multi-geo Azure data platform design, advanced governance, and production-readiness concerns (schema drift, security, DR, cost).\n\n## Key Concepts\n\n- Delta Lake auto-evolution and schema drift handling\n- Purview lineage and data cataloging across regions\n- Row-Level Security in Synapse for multi-tenant access\n- PII masking, encryption at rest with Key Vault\n- Cross-region DR with meet RPO targets and cost-aware storage tiers\n\n## Code Example\n\n```sql\n-- example: enable Delta auto schema evolution\nALTER TABLE curated.fact_transactions SET TBLPROPERTIES ('delta.autoOptimize.optimizeWrite' = 'true');\n```\n\n## Follow-up Questions\n\n- How would you validate DR failover latency and RPO in production?\n- What metrics would you collect to detect stale lineage or drift?","diagram":null,"difficulty":"advanced","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T13:12:39.701Z","createdAt":"2026-01-24T13:12:39.701Z"},{"id":"q-6710","question":"You're building an Azure-based data platform for a multinational retailer with EU residency rules. Ingest streaming event data into region-specific ADLS Gen2 lakes, apply Delta Lake with schema evolution, expose curated data products via Synapse, and track end-to-end lineage with Purview. How would you design region partitions, data contracts, and access control to satisfy residency, latency, and governance while supporting BI and ML workloads?","answer":"Design per-region ADLS Gen2 lakes (EU/US) with Delta Lake for curated layers; Purview tracks lineage and classifications; region-scoped Synapse workspaces serve BI/ML. Enforce schema evolution with De","explanation":"## Why This Is Asked\n\nGauges ability to balance data residency, governance, and analytics in a real-world, multi-region Azure setup. Tests architecture decisions, contract-first thinking, and operational considerations.\n\n## Key Concepts\n\n- Region-specific data lakes with ADLS Gen2\n- Delta Lake schema evolution and enforcement\n- Purview for lineage, classification, and policy\n- Data contracts and versioning across regions\n- Synapse workspaces and RBAC-based access control\n- Latency optimization via regional processing and caching\n\n## Code Example\n\n```python\n# Delta Lake schema enforcement example (high level)\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\ndf = spark.read.format(\"parquet\").load(\"abfss://curated@eu.datalake.dfs.core.windows.net/events/\")\n# In Delta table, enforce schema changes via ALTER TABLE and schema Evolution\n# Pseudo-code for drift check and apply\n```\n\n## Follow-up Questions\n\n- How would you implement data drift detection and automated schema evolution across regions?\n- What metrics would you monitor to ensure SLAs for BI/ML workloads are met, and how would you adjust provisioning?","diagram":"flowchart TD\nEU[EU Region Data Lake] --> CURATED[Curated Delta Lake]\nUS[US Region Data Lake] --> CURATED\nCURATED --> BI[BI/ML Access]\nPurview[Purview for lineage & policy] --> CURATED","difficulty":"intermediate","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Microsoft","NVIDIA","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T14:28:26.481Z","createdAt":"2026-01-24T14:28:26.483Z"},{"id":"q-6748","question":"Design an end-to-end, multi-tenant data lake on Azure Gen2 that ingests per-tenant streaming data from Azure Event Hubs and batch data from AWS S3, stores it as Delta Lake with per-tenant Unity Catalog schemas and Purview lineage. How would you handle schema drift, late-arriving data, incremental loads, and cross-cloud governance?","answer":"Leverage Databricks Delta Lake on ADLS Gen2 with Unity Catalog for per-tenant schemas and Purview for lineage. Ingest streaming from Azure Event Hubs and batch data from AWS S3 into Delta tables using","explanation":"## Why This Is Asked\n\nTests cross-cloud multi-tenant governance and schema evolution.\n\n## Key Concepts\n\n- Delta Lake schema evolution and MERGE\n- Unity Catalog per-tenant schemas\n- Purview lineage across Azure and AWS\n- Late data handling and incremental loads\n- Cross-cloud IAM and network security\n\n## Code Example\n\n```python\n# PySpark pseudo\ndf = spark.readStream.format(\"delta\").load(\"/mnt/tenant/events\")\n```\n\n## Follow-up Questions\n\n- How to validate Purview lineage during deployment?\n- How to enforce per-tenant retention and access controls in Unity Catalog?","diagram":null,"difficulty":"advanced","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Hashicorp"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T15:41:26.273Z","createdAt":"2026-01-24T15:41:26.273Z"},{"id":"q-6841","question":"Design an end-to-end streaming ELT from Azure Event Hubs to a Delta Lake on ADLS Gen2 using Spark Structured Streaming (Databricks) with exactly-once delivery, on-the-fly schema evolution, and sub-2-second latency. Include checkpointing, watermarking for late data, MERGE upserts into a star schema in Synapse, and Purview lineage. Specify components, config, and failure testing?","answer":"Ingest from Event Hubs with Spark Structured Streaming, write to Delta Lake with checkpointing on ADLS Gen2, enable schema evolution, and use foreachBatch to MERGE into a Synapse-based target star sch","explanation":"## Why This Is Asked\nThis question probes streaming ELT design, exactly-once semantics, schema evolution, and lineage integration in Azure at scale.\n\n## Key Concepts\n- Spark Structured Streaming, Delta Lake schema evolution, Event Hubs, ADLS Gen2, MERGE upserts, watermarking, checkpointing, Purview lineage, failure handling.\n\n## Code Example\n```javascript\n// Pseudocode for Spark Streaming to Delta with MERGE\nval df = spark.readStream.format(\"eventhubs\").options(...).load()\nval transformed = df.selectExpr(\"cast(body as string) as json\").transform(parseJson)\nval query = transformed.writeStream\n  .format(\"delta\")\n  .option(\"checkpointLocation\",\"/mnt/checkpoints/evt\")\n  .start(\"/mnt/delta/stream\")\n```\n\n## Follow-up Questions\n- How would you implement idempotent MERGE into the star schema and validate end-to-end exactly-once?\n- How would you simulate late data and ensure correct watermarking and retries?","diagram":"flowchart TD\n  A[Event Hubs] --> B[Databricks Structured Streaming]\n  B --> C[Delta Lake (ADLS Gen2)]\n  C --> D[Purview lineage]\n  B --> E[MERGE to Synapse Star Schema]\n  F[Checkpoint] --> B","difficulty":"advanced","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T19:35:14.706Z","createdAt":"2026-01-24T19:35:14.706Z"},{"id":"q-6945","question":"Beginner Azure Data Engineer: A daily feed from two partners arrives as CSV and JSON in /landing with evolving schemas. Design an end-to-end Data Factory pipeline that stages raw data, quarantines files that fail schema validation, validates drift against a defined envelope, and upserts into a single dimension table in Azure Synapse using a watermark-based incremental load. Include Purview lineage and basic data quality checks?","answer":"Ingest CSV and JSON files from /landing; implement a Data Flow to validate against a schema envelope with automatic routing of schema-drifting files to /quarantine; convert conforming files to Parquet format in /staging; execute watermark-based incremental MERGE into Azure Synapse dimension table; integrate Purview for data lineage and implement basic data quality validation checks.","explanation":"## Why This Is Asked\nThis evaluates practical Azure Data Factory skills for handling real-world data engineering challenges including schema evolution, data quality enforcement, and lineage tracking in a scalable, production-ready architecture.\n\n## Key Concepts\n- Azure Data Factory pipeline orchestration and Data Flow transformations\n- Schema envelope validation with drift detection and quarantine routing\n- Parquet staging for optimized storage and performance\n- Watermark-based incremental loading with MERGE operations\n- Azure Purview integration for end-to-end data lineage\n- Data quality validation and monitoring\n\n## Code Example\n```sql\nMERGE INTO dim_partner AS target\nUSING staging AS source\nON target.partner_id = source.partner_id\nWHEN MATCHED AND target.watermark < source.watermark THEN\n  UPDATE SET partner_name = source.partner_name, last_updated = GETDATE()\nWHEN NOT MATCHED THEN\n  INSERT (partner_id, partner_name, watermark, last_updated)\n  VALUES (source.partner_id, source.partner_name, source.watermark, GETDATE());\n```","diagram":"flowchart TD\n  A[Landing: /landing] --> B[Ingest & Drift Check]\n  B --> C{Drift OK?}\n  C -- Yes --> D[Stage: /staging as Parquet]\n  C -- No --> E[Quarantine: /quarantine]\n  D --> F[Incremental Upsert to Dim in Synapse]\n  F --> G[Purview Lineage & QC Reports]","difficulty":"beginner","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T04:27:29.925Z","createdAt":"2026-01-24T23:45:14.206Z"},{"id":"q-6967","question":"Design a streaming ELT to ingest JSON events from a Kafka-compatible source into ADLS Gen2, maintain event-time semantics, and upsert into Delta Lake with schema drift handling. Ensure end-to-end lineage to Purview, support late data, and provide testable rollback. What architecture and components would you choose, and why?","answer":"Design a comprehensive streaming ELT pipeline that ingests JSON events from a Kafka-compatible source using Databricks Structured Streaming, processes them with event-time semantics through watermarking, performs idempotent upserts into Delta Lake using MERGE operations, handles schema drift with Auto Loader and evolution capabilities, maintains end-to-end lineage with Azure Purview integration, supports late data arrival scenarios, and implements testable rollback mechanisms for operational reliability.","explanation":"## Why This Is Asked\nThis question evaluates the ability to architect a robust streaming ingestion solution that addresses real-time data challenges including late-arriving data, schema evolution, data governance, and operational reliability.\n\n## Key Concepts\n- Event-time processing with watermarks in Spark Structured Streaming for accurate temporal semantics\n- Delta Lake upserts using MERGE operations for idempotent, deduplicated writes\n- Schema evolution and Auto Loader for seamless handling of schema drift\n- Azure Purview integration for comprehensive data lineage and governance\n- Late data handling strategies and testable rollback mechanisms","diagram":null,"difficulty":"advanced","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Goldman Sachs"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T05:18:09.508Z","createdAt":"2026-01-25T02:46:58.811Z"},{"id":"q-7004","question":"New angle: Ingest daily partner shipments from two sources (CSV/JSON) into ADLS Gen2. Build a beginner Data Factory pipeline that (1) stages raw files under /landing/shipments/raw, (2) validates a lightweight schema (shipment_id, date, amount, partner_id) and data types, (3) on drift or faults, moves files to /landing/shipments/errors and raises an alert, (4) on success, loads conformant data into a simple star schema in Azure Synapse, and (5) registers provenance in Purview. What steps and components would you implement?","answer":"Data Factory with two pipelines: landing/validation and load. Stage raw to /landing/shipments/raw, validate schema (shipment_id, date, amount, partner_id) and types via Mapping Data Flow, route valid ","explanation":"## Why This Is Asked\n\nThis angle tests hands-on skills in data validation, error handling, and governance early in the pipeline lifecycle, not just ELT design.\n\n## Key Concepts\n\n- Lightweight schema validation in Mapping Data Flows\n- Drift detection and error routing to an errors layer\n- Parquet conversion and loading into a star schema\n- Alerting and provenance via Purview\n\n## Code Example\n\n```json\n{\n  \"activities\": [\n    {\"name\": \"ValidateSchema\", \"type\": \"MappingDataFlow\"}\n  ]\n}\n```\n\n## Follow-up Questions\n\n- How would you test drift thresholds and alerting?\n- How would you scale the validation pipeline for larger file sizes?","diagram":"flowchart TD\n  A[Landing: /landing/shipments/raw] --> B[Validation: Mapping Data Flow]\n  B --> C{Status}\n  C -->|Valid| D[Load to Synapse (Star Schema)]\n  C -->|Invalid| E[Move to /landing/shipments/errors]\n  D --> F[Purview Lineage]\n  E --> G[Alert]\n","difficulty":"beginner","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Instacart","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T05:35:51.479Z","createdAt":"2026-01-25T05:35:51.479Z"},{"id":"q-7086","question":"You’re receiving daily JSON telemetry payloads from multiple mobile apps into ADLS Gen2. Schema drift occurs as new fields appear. Propose an end-to-end pipeline that (1) ingests to Delta Lake with automatic schema evolution, (2) runs Great Expectations validations in Databricks before committing to the lakehouse, (3) upserts into a central fact table in Azure Synapse, and (4) records end-to-end lineage in Purview. Include how you’d handle late-arriving data, cost, and observability?","answer":"Ingest via a Databricks job to Delta Lake with schema evolution enabled, then run Great Expectations validations before MERGE into the central fact table in Synapse. Use partitioning by ingest_date, a","explanation":"## Why This Is Asked\nAssesses ability to design a resilient, observable data lakehouse pipeline that tolerates schema drift, late data, and cost constraints while integrating governance and quality checks.\n\n## Key Concepts\n- Delta Lake schema evolution\n- Great Expectations integration with Databricks\n- Upserts via MERGE into Synapse\n- Purview lineage and data cataloging\n- Late-arriving data handling and cost observability\n\n## Code Example\n```python\n# PySpark sketch (conceptual)\nfrom delta.tables import DeltaTable\n\n# write to Delta with schema evolution\nnew_df.write.format('delta').mode('append').option('mergeSchema','true').save('/delta/facts')\n\n# upsert into target\n_delta = DeltaTable.forPath(spark, '/delta/facts')\ndelta_table.alias('t').merge(\n  new_df.alias('s'),\n  't.key = s.key'\n).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n```\n\n## Follow-up Questions\n- How would you implement incremental schema drift detection and rollbacks?\n- How would you test the end-to-end lineage in Purview across multiple environments?","diagram":"flowchart TD\n  A[Ingest JSON to ADLS Gen2] --> B[Databricks Delta Lake with schema evolution]\n  B --> C[Great Expectations validations]\n  C --> D[MERGE into Synapse fact table]\n  D --> E[Purview lineage & cataloging]\n  E --> F[Monitoring & alerts]","difficulty":"intermediate","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T08:48:22.606Z","createdAt":"2026-01-25T08:48:22.606Z"},{"id":"q-7184","question":"Design a streaming ingestion pipeline for a high-volume clickstream from Azure Event Hubs into a Delta Lake on ADLS Gen2, with schema drift tolerance and incremental upserts (SCD Type 2). Include end-to-end lineage in Purview, and a data product surface via Synapse. How would you configure the components, handle schema evolution, and ensure governance across regions within cost constraints?","answer":"Ingest via Event Hubs into Databricks Delta Live Tables, output to Delta Lake on ADLS Gen2 with auto-evolution and MERGE-based SCD Type 2. Catalog assets in Purview for end-to-end lineage; surface dat","explanation":"## Why This Is Asked\n\nEvaluates knowledge of streaming ingestion, Delta Lake schema evolution, and SCD2 upserts, plus governance across regions using Purview and Synapse.\n\n## Key Concepts\n\n- Streaming ingestion (Event Hubs)\n- Delta Lake on ADLS Gen2 with schema evolution\n- MERGE for SCD Type 2\n- End-to-end lineage in Purview\n- Data product exposure via Synapse\n- Cross-region governance and cost control\n\n## Code Example\n\n```sql\n-- Example MERGE for SCD Type 2 (illustrative)\nMERGE INTO dim_customer AS target\nUSING staging.dim_customer AS source\nON target.customer_id = source.customer_id\nWHEN MATCHED AND (target.hash <> source.hash) THEN\n  UPDATE SET target.end_date = CURRENT_DATE, target.active = 0\nWHEN NOT MATCHED THEN\n  INSERT (customer_id, name, hash, start_date, end_date, active)\n  VALUES (source.customer_id, source.name, source.hash, CURRENT_DATE, NULL, 1);\n```\n\n## Follow-up Questions\n\n- How would you monitor data drift and trigger reprocessing?\n- How would you handle late-arriving data and tombstone events across regions?","diagram":"flowchart TD\n  A[Event Hubs] --> B[Databricks Delta Live Tables]\n  B --> C[Delta Lake on ADLS Gen2 (SCD2)]\n  C --> D[Synapse External Tables / BI]\n  D --> E[Purview lineage]\n  E --> F[Cross-region governance]","difficulty":"intermediate","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Microsoft","Plaid","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T13:14:53.353Z","createdAt":"2026-01-25T13:14:53.353Z"},{"id":"q-7321","question":"You're a beginner Azure data engineer tasked with building a cross-source ingestion for a marketing analytics product. Partner CSVs arrive via SFTP and internal JSONs via REST into ADLS Gen2 under /landing/marketing. Design a drift-tolerant ELT pipeline that stages raw data, upserts a Campaign dimension with SCD Type 1 and loads a Fact table with idempotent upserts into Azure Synapse, while creating Parquet-based stores for cost-efficient analytics. Include basic data quality gates (row counts, null rates) and register datasets in Purview with lineage. Outline components, steps, and testing strategy?","answer":"Propose using ADLS Gen2 landing, Data Factory orchestrations, Data Flows for drift-tolerant mapping to Parquet, then MERGE-based upserts into a dedicated SQL pool for a star schema. Implement SCD Type","explanation":"## Why This Is Asked\nAddresses drift-tolerant ingestion, basic upserts, and governance in a beginner-friendly path.\n\n## Key Concepts\n- Drift-tolerant mapping in Data Flows\n- MERGE-based upserts in Synapse\n- SCD Type 1 and simple fact upserts\n- Parquet storage in ADLS Gen2\n- Purview lineage\n- Basic data quality gates and audits\n\n## Code Example\n```javascript\nMERGE INTO dim_campaign AS target\nUSING staging_campaign AS src\nON target.campaign_id = src.campaign_id\nWHEN MATCHED THEN UPDATE SET target.name = src.name\nWHEN NOT MATCHED THEN INSERT (campaign_id, name, start_date, end_date) VALUES (src.campaign_id, src.name, src.start_date, src.end_date);\n```\n\n## Follow-up Questions\n- How would you extend to SCD Type 2?\n- How would you test data quality gates in Dev and CI/CD?","diagram":"flowchart TD\n  A[Ingest: SFTP/REST -> /landing/marketing] --> B[Schemata: Drift-tolerant mapping]\n  B --> C[Stage: Raw Parquet in /staging]\n  C --> D[Load: Dim Campaign (SCD1) and Fact (MERGE)]\n  D --> E[Analytics: Star schema in Synapse]\n  E --> F[Governance: Purview lineage]\n  F --> G[Quality Gates: row counts/null checks]","difficulty":"beginner","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T18:43:27.589Z","createdAt":"2026-01-25T18:43:27.589Z"},{"id":"q-7401","question":"You must build a near-real-time ingestion pipeline for a payments analytics platform. Streams from Confluent Cloud (JSON/Avro with evolving schemas) are ingested into ADLS Gen2 via Databricks and Delta Lake. Implement schema drift tolerance, auto-evolving schemas, and idempotent upserts into a Delta Lake fact table, while late data is handled and end-to-end lineage is captured in Purview. Which Azure components and design choices would you use, and how would you implement it?","answer":"Leverage Azure Databricks with Delta Lake on ADLS Gen2, reading Kafka from Confluent Cloud via Spark Structured Streaming; enable schema evolution with merged schemas, apply watermarking for late data handling, and perform idempotent upserts using MERGE operations. Implement Purview integration for end-to-end lineage and data quality gates to ensure governance and traceability.","explanation":"## Why This Is Asked\nAssesses ability to design a near-real-time lakehouse solution that handles evolving schemas, maintains data accuracy, and provides comprehensive governance.\n\n## Key Concepts\n- Streaming ingestion from Kafka to Delta Lake\n- Delta schema evolution and upsert operations\n- Late data handling with watermarking\n- Purview lineage and data quality enforcement\n\n## Code Example\n```python\n# PySpark example for schema evolution and upserts\nstreaming_df = (spark.readStream\n    .format(\"kafka\")\n    .option(\"kafka.bootstrap.servers\", confluent_servers)\n    .load())\n\n# Write with schema evolution and upserts\n(streaming_df.writeStream\n    .format(\"delta\")\n    .option(\"mergeSchema\", \"true\")\n    .outputMode(\"append\")\n    .trigger(processingTime=\"30 seconds\")\n    .start(\"/path/to/delta/table\"))\n```","diagram":"flowchart TD\n  K[Confluent Cloud Kafka] --> D[Databricks Structured Streaming]\n  D --> T[Delta Lake Transactions]\n  T --> L[Purview Lineage]\n  L --> Q[Queries in Synapse/BI]\n  D --> E[Late Data Path]","difficulty":"intermediate","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Google","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T06:51:08.936Z","createdAt":"2026-01-25T21:39:03.183Z"},{"id":"q-7475","question":"Design a streaming ELT pipeline to ingest real-time JSON events from a REST webhook into ADLS Gen2, apply schema drift tolerant transformations, store as Delta Lake tables on ADLS Gen2, and surface aggregates in Azure Synapse with Purview lineage. Explain how you would handle schema drift, versioned schemas, tombstone events, and how you would implement error handling, retry, and backpressure, including component choices for ingestion, processing, and governance?","answer":"Ingest JSON events from the REST webhook through Azure Event Hubs; process with Spark Structured Streaming using schema registry integration to enable schema evolution and drift tolerance; write transformed data to Delta Lake tables on ADLS Gen2 with intelligent partitioning by event date and maintain versioned schemas; expose aggregated datasets in Azure Synapse with automatic Purview lineage tracking. Implement idempotent processing patterns with comprehensive tombstone event handling via CDC mechanisms, utilize exponential backoff strategies for retry logic, and apply backpressure controls through Spark's streaming rate limiting combined with Event Hubs consumer group management.","explanation":"## Why This Is Asked\nStreaming ingestion with schema drift and governance represents a critical production challenge; this question evaluates comprehensive system design abilities and practical implementation trade-offs.\n\n## Key Concepts\n- Streaming ingestion (REST webhook → Event Hub)\n- Schema drift management using Delta Lake schema evolution and optional schema registry\n- Tombstone event handling through change data capture (CDC) patterns\n- Idempotent processing with deduplication strategies\n- Purview lineage integration across ADLS Gen2 and Synapse\n- Error handling with exponential backoff","diagram":"flowchart TD\nA[Webhook] --> B[Event Hubs / Kafka]\nB --> C[Spark Structured Streaming]\nC --> D[Delta Lake on ADLS Gen2]\nD --> E[Azure Synapse]\nE --> F[Purview lineage]","difficulty":"intermediate","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","OpenAI","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T05:31:21.159Z","createdAt":"2026-01-26T02:48:41.946Z"},{"id":"q-7501","question":"You are handed a daily JSON event feed via SFTP with nested fields and frequent schema drift. Design a beginner-friendly end-to-end pipeline using Azure Data Factory, ADLS Gen2, and Azure Synapse Serverless to flatten, store, and query the data as a simple star schema. Include incremental loads with a watermark, basic data quality checks, and Purview lineage, avoiding dedicated Spark clusters. What components and steps would you implement?","answer":"Architect a cost-conscious end-to-end: land raw JSON in ADLS Gen2, use Azure Data Factory Data Flow to flatten nested fields and cope with drift, write partitioned Parquet to /landing/events/date, and","explanation":"## Why This Is Asked\nDemonstrates cost-aware data engineering using serverless compute, handling nested JSON drift, and governance.\n\n## Key Concepts\n- Data Flow flatten and drift tolerance\n- Watermark-based incremental load with serverless\n- Purview lineage and basic data quality checks\n\n## Code Example\n```javascript\n// Pseudo: SQL merge for upserts into dw.dim_users\nconst sql = `\nMERGE INTO dw.dim_users AS target\nUSING stg.Users AS source\nON (target.user_id = source.user_id)\nWHEN MATCHED THEN UPDATE SET\n  name = source.name, email = source.email\nWHEN NOT MATCHED THEN INSERT (user_id, name, email) VALUES (source.user_id, source.name, source.email);\n`;\n```\n\n## Follow-up Questions\n- How would you monitor and retry failed incremental loads in a serverless setup?\n- What failover considerations exist for the SFTP landing scenario?","diagram":"flowchart TD\n  A[Source: SFTP JSON] --> B[Landing in ADLS Gen2]\n  B --> C[Data Flow Flatten in ADF]\n  C --> D[Partitioned Parquet in /landing/date]\n  D --> E[Azure Synapse Serverless: Star Schema]\n  E --> F[Purview Lineage & QC]","difficulty":"beginner","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","Snap","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T04:39:32.599Z","createdAt":"2026-01-26T04:39:32.599Z"},{"id":"q-7579","question":"You're designing a streaming telemetry pipeline for 100k IoT devices sending data to Azure via Event Hubs, landing into ADLS Gen2. Propose an end-to-end approach that handles frequent schema drift, performs near real-time upserts into a Delta Lake, enforces data quality gates, and provides end-to-end lineage in Purview. Specify components, dataflow, and failure handling?","answer":"Event Hubs with Azure Schema Registry for evolution; Databricks Spark streaming writes to Delta Lake on ADLS Gen2, MERGE upserts with auto schema evolution; data quality via Spark constraints and Deeq","explanation":"## Why This Is Asked\nTests real-world streaming ingestion with schema drift, Delta Lake upserts, governance, and reliability in Azure.\n\n## Key Concepts\n- Azure Event Hubs + Schema Registry\n- Delta Lake MERGE for upserts and schema evolution\n- Databricks Spark Streaming or Synapse Spark\n- Data quality gates (constraints, Deequ)\n- Purview lineage, Synapse Pipelines orchestration\n\n## Code Example\n```python\n# Delta MERGE example (Databricks)\ndeltaTable = DeltaTable.forPath(spark, \"/mnt/delta/telemetry\")\n(deltaTable.alias(\"t\").merge(\n  source_df.alias(\"s\"), \"t.device_id = s.device_id\"\n).whenMatchedUpdate(set={\"t.metrics\": \"s.metrics\", \"t.ts\": \"s.ts\"})\n .whenNotMatchedInsertAll()\n .execute())\n```\n\n## Follow-up Questions\n- How would you test schema evolution in a CI/CD pipeline without breaking production?\n- What are the latency and cost trade-offs of MERGE vs append-only with periodic compaction?","diagram":"flowchart TD\n  IoT[IoT Devices] --> EH[Event Hubs]\n  EH --> SR[Schema Registry]\n  SR --> DS[Databricks Spark Streaming]\n  DS --> DL[Delta Lake on ADLS Gen2]\n  DL --> Purview[Purview Lineage]\n  DL --> Synapse[Synapse Pipelines/External Tables]","difficulty":"intermediate","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Goldman Sachs"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T08:43:24.883Z","createdAt":"2026-01-26T08:43:24.885Z"},{"id":"q-7672","question":"In an Azure data lakehouse, data arrives from two sources: a SaaS API ingested via Azure Data Factory into ADLS Gen2 as Parquet, and streaming events via Azure Event Hubs into a Delta table in Synapse. You need end-to-end lineage to Purview, upserts with CDC into a single Delta Lake, automatic schema drift handling, PII masking in the landing zone, and late-arriving data handling. Design the architecture, formats, partitioning, CDC approach, and how you implement quality checks and monitoring?","answer":"Two ingestion paths: batch from Data Factory to ADLS Gen2 Parquet and streaming from Event Hubs into a Delta table in Synapse. Use Delta MERGE for CDC upserts with Change Feed, and enable schema evolu","explanation":"## Why This Is Asked\n\nAssesses ability to design a scalable, governed Azure data lakehouse with mixed ingestion patterns, CDC, schema drift handling, and data privacy at scale.\n\n## Key Concepts\n\n- Delta Lake Change Data Feed for CDC\n- MERGE upserts into a lakehouse table\n- Schema evolution with Delta\n- PII masking in landing zone\n- End-to-end Purview lineage\n- Monitoring and lightweight quality gates\n\n## How to Implement\n\n- Batch: Data Factory to ADLS Gen2 Parquet; Streaming: Event Hubs to Spark pool writing to Delta in Synapse\n- Partition by ingest_date; enable Change Feed for CDC\n- Apply PII masking with Spark UDF before write\n- Connect Purview for lineage; set up alerts for data quality\n\n## Follow-up Questions\n\n- How would you detect and handle drift in nested fields?\n- How would you validate lineage accuracy across batch and streaming paths?","diagram":"flowchart TD\n  A[SaaS API via Data Factory] --> B[ADLS Gen2 Landing Parquet]\n  C[Event Hubs Ingest] --> D[Delta Lake in Synapse]\n  B --> E[Delta Table Merge]\n  E --> F[Purview Lineage]\n  G[PII Masking] --> B","difficulty":"advanced","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","MongoDB","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T12:02:05.898Z","createdAt":"2026-01-26T12:02:05.898Z"},{"id":"q-7754","question":"You're building a real-time trade-data platform for a Robinhood-like app. Ingest JSON tick events from 3 exchanges into ADLS Gen2 with Delta Lake on Databricks; support schema drift, sub-minute windowed analytics in Synapse, and end-to-end lineage in Purview. Explain ingestion, schema-evolution, and per-product access controls?","answer":"Use Event Hubs to ingest per-exchange JSON ticks, Databricks Structured Streaming to sink to Delta Lake on ADLS Gen2 with mergeSchema = true; enable checkpointing. Use Unity Catalog for per-product RB","explanation":"## Why This Is Asked\nTests ability to design a real-time data product on Azure with cross-exchange ingestion, Delta Lake schema evolution, and governance at scale. It also probes how to balance latency, cost, and correctness in production.\n\n## Key Concepts\n- Streaming ingestion from Event Hubs\n- Delta Lake schema evolution with mergeSchema\n- Unity Catalog RBAC for per-product access\n- Purview lineage across events → tables\n- Azure Synapse windowed analytics and latency control\n\n## Code Example\n\n```javascript\n// Streaming ingestion sketch (pseudocode)\nconst query = spark.readStream\n  .format(\"eventhubs\")\n  .load()\n  // further transform steps would go here\n```\n\n## Follow-up Questions\n- How would you handle backpressure and missing data during spikes?\n- What are cost-tuning strategies for continuous streaming pipelines in Databricks? ","diagram":"flowchart TD\n  A[Ingest: Exchange Tick Events] --> B[Databricks Structured Streaming]\n  B --> C[Delta Lake on ADLS Gen2]\n  C --> D[Synapse for windowed analytics]\n  C --> E[Purview lineage]\n  E --> F[Unity Catalog RBAC]","difficulty":"intermediate","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["NVIDIA","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T16:53:01.663Z","createdAt":"2026-01-26T16:53:01.664Z"},{"id":"q-7866","question":"Advanced: design an Azure data platform for three tenants where telemetry lands in ADLS Gen2 from on‑prem and Kafka, ingested with ADF, transformed in Databricks Delta Lake with schema evolution, and published as data products via Delta Sharing to external tenants; include Purview lineage and multi‑region replication with RBAC and private endpoints. What components and workflow would you implement?","answer":"Ingest on‑prem SFTP and Kafka into ADLS Gen2, stage with ADF, load to Delta Lake on Databricks with schema evolution; tie lineage to Purview; publish data products via Delta Sharing for external tenan","explanation":"## Why This Is Asked\nTests advanced Azure data engineering skills: end‑to‑end data pipelines, governance, and cross‑tenant sharing. It probes practical trade‑offs for schema drift, lineage, and multi‑region ops.\n\n## Key Concepts\n- Delta Lake schema evolution and drift handling\n- Purview lineage and classifications\n- Delta Sharing for external data products\n- Multi‑region replication and cost controls\n- RBAC, private endpoints, and security boundaries\n\n## Code Example\n```javascript\n// Example configuration sketch (illustrative)\nconst config = {\n  table: 'telemetry',\n  enableSchemaEvolution: true,\n  mergeOnRead: false\n};\n```\n\n## Follow-up Questions\n- How would you handle late‑arriving data in streaming paths?\n- How would you quantify and bound cross‑tenant egress costs?","diagram":null,"difficulty":"advanced","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Hashicorp","IBM"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T20:52:39.195Z","createdAt":"2026-01-26T20:52:39.195Z"},{"id":"q-940","question":"Daily CSV exports arrive in an ADLS Gen2 container at /incoming/sales/. Files may evolve over time as columns drift. Build a beginner pipeline using Data Factory to stage raw data, infer/handle schema changes, and load a simple star schema in Azure Synapse Analytics. Include a watermark-based incremental load and basic scheduling. Which services and steps would you implement?","answer":"Use Azure Data Factory: Copy to land raw CSVs as Parquet in ADLS Gen2, Data Flow with allowSchemaDrift to map evolving columns to a fixed star-schema in Azure Synapse, sink into a dedicated SQL pool. ","explanation":"## Why This Is Asked\nTests end-to-end basics: file landing, simple schema evolution, incremental loads, and reporting schema in Synapse.\n\n## Key Concepts\n- Data Factory pipelines: Copy, Data Flow, Triggers\n- ADLS Gen2 and Parquet landing\n- Synapse star schema design and incremental loads\n- Watermark-based change capture\n\n## Code Example\n```javascript\n// JSON-like skeleton of pipeline stages\n{\n  name: 'SalesIngest',\n  activities: [\n    {copy: 'landing/parquet'},\n    {dataFlow: 'map-to-star', drift: true},\n    {sink: 'Synapse.star'},\n    {trigger: 'tumblingWindow', interval: 1}\n  ]\n}\n```\n\n## Follow-up Questions\n- How would you handle late-arriving data?\n- How would you monitor data quality and failures?","diagram":"flowchart TD\n  A[CSV arrives] --> B[Copy to landing Parquet]\n  B --> C[Data Flow: map to star schema]\n  C --> D[Sink to Synapse star schema]\n  D --> E[Incremental load by watermark]\n  E --> F[Update metadata log]","difficulty":"beginner","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Amazon","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T16:30:40.009Z","createdAt":"2026-01-12T16:30:40.009Z"},{"id":"q-980","question":"Design a global streaming-to-batch data pipeline for a PayPal/Adobe/Netflix-scale analytics platform: ingest real-time clickstream from Event Hubs into ADLS Gen2, maintain a Delta Lake with drift-tolerant schema, mask PII at ingestion, expose masked aggregates via serverless SQL pool, and enforce end-to-end lineage with Purview while supporting cross-region DR and data residency. Which Azure components and patterns would you use, and how would you handle schema evolution and failure modes?","answer":"Ingest raw clickstream from Event Hubs into ADLS Gen2 staging, use Spark Structured Streaming to write to Delta Lake with drift-aware schema evolution, apply a masking UDF before storing in curated zo","explanation":"## Why This Is Asked\nThis probes practical data governance, real-time-to-batch pipelines, and DR considerations in Azure for large-scale workloads.\n\n## Key Concepts\n- Event Hubs ingestion and Spark Structured Streaming\n- Delta Lake with schema drift handling\n- PII masking at ingestion\n- Purview lineage and data residency compliance\n- Cross-region DR and serverless analytics\n\n## Code Example\n```javascript\n// PII masking pseudo-code\nfunction maskPII(record) {\n  if (!record?.email) return record;\n  record.email = record.email.replace(/[^@]+@/, '*****@');\n  return record;\n}\n```\n\n## Follow-up Questions\n- How would you validate end-to-end lineage across regions?\n- What are failure modes in cross-region DR and mitigations?","diagram":"flowchart TD\n  A[Event Hubs] --> B[ADLS Gen2 staging]\n  B --> C[Delta Lake curated]\n  C --> D[Serverless SQL pool dashboards]\n  A --> E[Purview lineage]\n  F[Geo-DR] --> G[Region failover]","difficulty":"intermediate","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Netflix","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T17:44:06.541Z","createdAt":"2026-01-12T17:44:06.541Z"}],"subChannels":["general"],"companies":["Adobe","Airbnb","Amazon","Anthropic","Apple","Bloomberg","Citadel","Cloudflare","Coinbase","Databricks","Discord","DoorDash","Goldman Sachs","Google","Hashicorp","Hugging Face","IBM","Instacart","LinkedIn","Lyft","Meta","Microsoft","MongoDB","NVIDIA","Netflix","OpenAI","Oracle","PayPal","Plaid","Robinhood","Salesforce","Scale Ai","Slack","Snap","Snowflake","Square","Stripe","Tesla","Twitter","Two Sigma","Uber","Zoom"],"stats":{"total":88,"beginner":27,"intermediate":28,"advanced":33,"newThisWeek":38}}