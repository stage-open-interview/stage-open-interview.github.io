{"questions":[{"id":"q-1029","question":"In a global IoT telemetry pipeline ingesting 100k events/s per region into ADLS Gen2 and Databricks Delta Lake, implement schema drift tolerant ingestion, end-to-end data lineage via Purview, RBAC, and cross-region DR replication. How would you architect the data contracts, partitioning, watermarking, retention, and governance to meet data residency and fault-tolerance requirements?","answer":"Architect a multi-region lakehouse: per-region ADLS Gen2, Databricks Delta Lake with mergeSchema and cloudFiles.inferColumnTypes, partition by region and device_type, watermark-based streaming, and Pu","explanation":"## Why This Is Asked\nAssesses multi-region lakehouse design, schema drift handling, data governance, and disaster recovery in Azure.\n\n## Key Concepts\n- Schema drift tolerant ingestion (Delta Lake, mergeSchema)\n- Data lineage (Purview) and RBAC (Unity Catalog)\n- Cross-region replication and data residency\n- Partitioning strategy and watermarking for IoT streams\n- Data contracts, retention, and failure handling\n\n## Code Example\n```python\nfrom pyspark.sql import functions as F\n\n# Ingest with auto schema, write to Delta with evolution\ndf = (\n  spark.readStream\n     .format(\"cloudFiles\")\n     .option(\"cloudFiles.format\",\"json\")\n     .option(\"cloudFiles.inferColumnTypes\",\"true\")\n     .load(\"/mnt/iot/raw/\")\n)\n\nquery = df.writeStream \\\n  .format(\"delta\") \\\n  .option(\"checkpointLocation\",\"/mnt/checkpoints/iot/stream\") \\\n  .option(\"mergeSchema\",\"true\") \\\n  .partitionBy(\"region\",\"device_type\") \\\n  .start(\"/mnt/delta/iot/region/\")\n```\n\n## Follow-up Questions\n- How would you validate cross-region data residency and schema compatibility across regions?\n- What testing strategy ensures Purview lineage remains intact after DR failover?","diagram":"flowchart TD\n  A[IoT Devices] --> B[Event Hub / IoT Hub]\n  B --> C[ADLS Gen2 Region A Raw]\n  C --> D[Databricks Delta Lake Region A]\n  D --> E[Purview Lineage]\n  D --> F[Unity Catalog RBAC]\n  D --> G[Cross-Region Replication to Region B]\n  G --> H[DR ADLS Gen2 Region B]","difficulty":"advanced","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Twitter","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T19:42:48.929Z","createdAt":"2026-01-12T19:42:48.929Z"},{"id":"q-1043","question":"Design an end-to-end Azure data-ecosystem pipeline that ingests incremental changes from a MongoDB Atlas collection into Delta Lake on ADLS Gen2, preserving SCD Type 2 history for a customers dimension, and handling schema drift. Include data lineage to Azure Purview, late-arriving updates, and on-demand reprocessing without data loss. Which components and config would you choose, and why?","answer":"Use MongoDB Atlas Change Streams with Debezium to stream inserts/updates into Kafka, then Spark Structured Streaming consumes from Kafka and writes to Delta Lake on ADLS Gen2 with Delta schema evoluti","explanation":"## Why This Is Asked\nThis question tests practical design decisions for real-time CDC pipelines across Azure, including lineage, schema drift, and reprocessing.\n\n## Key Concepts\n- MongoDB Atlas Change Streams and Debezium for CDC\n- Delta Lake schema evolution and SCD Type 2\n- Spark Structured Streaming from Kafka to Delta Lake\n- Azure Purview for end-to-end lineage\n- Late-arriving data handling and replay strategies\n\n## Code Example\n```javascript\n// Pseudo-implementation illustrating flow (language-agnostic)\nconst inStream = Kafka.read(\"mongodb-change-stream\");\nDelta.write(inStream, \"/mnt/adls/delta/customers\");\n```\n\n## Follow-up Questions\n- How would you implement schema drift handling in Delta Lake?\n- How do you validate and monitor Purview lineage end-to-end?\n- What are failure modes and how would you recover from late data or CDC gaps?\n","diagram":null,"difficulty":"intermediate","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","NVIDIA","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T20:30:13.903Z","createdAt":"2026-01-12T20:30:13.903Z"},{"id":"q-940","question":"Daily CSV exports arrive in an ADLS Gen2 container at /incoming/sales/. Files may evolve over time as columns drift. Build a beginner pipeline using Data Factory to stage raw data, infer/handle schema changes, and load a simple star schema in Azure Synapse Analytics. Include a watermark-based incremental load and basic scheduling. Which services and steps would you implement?","answer":"Use Azure Data Factory: Copy to land raw CSVs as Parquet in ADLS Gen2, Data Flow with allowSchemaDrift to map evolving columns to a fixed star-schema in Azure Synapse, sink into a dedicated SQL pool. ","explanation":"## Why This Is Asked\nTests end-to-end basics: file landing, simple schema evolution, incremental loads, and reporting schema in Synapse.\n\n## Key Concepts\n- Data Factory pipelines: Copy, Data Flow, Triggers\n- ADLS Gen2 and Parquet landing\n- Synapse star schema design and incremental loads\n- Watermark-based change capture\n\n## Code Example\n```javascript\n// JSON-like skeleton of pipeline stages\n{\n  name: 'SalesIngest',\n  activities: [\n    {copy: 'landing/parquet'},\n    {dataFlow: 'map-to-star', drift: true},\n    {sink: 'Synapse.star'},\n    {trigger: 'tumblingWindow', interval: 1}\n  ]\n}\n```\n\n## Follow-up Questions\n- How would you handle late-arriving data?\n- How would you monitor data quality and failures?","diagram":"flowchart TD\n  A[CSV arrives] --> B[Copy to landing Parquet]\n  B --> C[Data Flow: map to star schema]\n  C --> D[Sink to Synapse star schema]\n  D --> E[Incremental load by watermark]\n  E --> F[Update metadata log]","difficulty":"beginner","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Amazon","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T16:30:40.009Z","createdAt":"2026-01-12T16:30:40.009Z"},{"id":"q-980","question":"Design a global streaming-to-batch data pipeline for a PayPal/Adobe/Netflix-scale analytics platform: ingest real-time clickstream from Event Hubs into ADLS Gen2, maintain a Delta Lake with drift-tolerant schema, mask PII at ingestion, expose masked aggregates via serverless SQL pool, and enforce end-to-end lineage with Purview while supporting cross-region DR and data residency. Which Azure components and patterns would you use, and how would you handle schema evolution and failure modes?","answer":"Ingest raw clickstream from Event Hubs into ADLS Gen2 staging, use Spark Structured Streaming to write to Delta Lake with drift-aware schema evolution, apply a masking UDF before storing in curated zo","explanation":"## Why This Is Asked\nThis probes practical data governance, real-time-to-batch pipelines, and DR considerations in Azure for large-scale workloads.\n\n## Key Concepts\n- Event Hubs ingestion and Spark Structured Streaming\n- Delta Lake with schema drift handling\n- PII masking at ingestion\n- Purview lineage and data residency compliance\n- Cross-region DR and serverless analytics\n\n## Code Example\n```javascript\n// PII masking pseudo-code\nfunction maskPII(record) {\n  if (!record?.email) return record;\n  record.email = record.email.replace(/[^@]+@/, '*****@');\n  return record;\n}\n```\n\n## Follow-up Questions\n- How would you validate end-to-end lineage across regions?\n- What are failure modes in cross-region DR and mitigations?","diagram":"flowchart TD\n  A[Event Hubs] --> B[ADLS Gen2 staging]\n  B --> C[Delta Lake curated]\n  C --> D[Serverless SQL pool dashboards]\n  A --> E[Purview lineage]\n  F[Geo-DR] --> G[Region failover]","difficulty":"intermediate","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Netflix","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T17:44:06.541Z","createdAt":"2026-01-12T17:44:06.541Z"}],"subChannels":["general"],"companies":["Adobe","Amazon","Cloudflare","MongoDB","NVIDIA","Netflix","PayPal","Robinhood","Twitter","Uber","Zoom"],"stats":{"total":4,"beginner":1,"intermediate":2,"advanced":1,"newThisWeek":4}}