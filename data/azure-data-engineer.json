{"questions":[{"id":"q-1029","question":"In a global IoT telemetry pipeline ingesting 100k events/s per region into ADLS Gen2 and Databricks Delta Lake, implement schema drift tolerant ingestion, end-to-end data lineage via Purview, RBAC, and cross-region DR replication. How would you architect the data contracts, partitioning, watermarking, retention, and governance to meet data residency and fault-tolerance requirements?","answer":"Architect a multi-region lakehouse: per-region ADLS Gen2, Databricks Delta Lake with mergeSchema and cloudFiles.inferColumnTypes, partition by region and device_type, watermark-based streaming, and Pu","explanation":"## Why This Is Asked\nAssesses multi-region lakehouse design, schema drift handling, data governance, and disaster recovery in Azure.\n\n## Key Concepts\n- Schema drift tolerant ingestion (Delta Lake, mergeSchema)\n- Data lineage (Purview) and RBAC (Unity Catalog)\n- Cross-region replication and data residency\n- Partitioning strategy and watermarking for IoT streams\n- Data contracts, retention, and failure handling\n\n## Code Example\n```python\nfrom pyspark.sql import functions as F\n\n# Ingest with auto schema, write to Delta with evolution\ndf = (\n  spark.readStream\n     .format(\"cloudFiles\")\n     .option(\"cloudFiles.format\",\"json\")\n     .option(\"cloudFiles.inferColumnTypes\",\"true\")\n     .load(\"/mnt/iot/raw/\")\n)\n\nquery = df.writeStream \\\n  .format(\"delta\") \\\n  .option(\"checkpointLocation\",\"/mnt/checkpoints/iot/stream\") \\\n  .option(\"mergeSchema\",\"true\") \\\n  .partitionBy(\"region\",\"device_type\") \\\n  .start(\"/mnt/delta/iot/region/\")\n```\n\n## Follow-up Questions\n- How would you validate cross-region data residency and schema compatibility across regions?\n- What testing strategy ensures Purview lineage remains intact after DR failover?","diagram":"flowchart TD\n  A[IoT Devices] --> B[Event Hub / IoT Hub]\n  B --> C[ADLS Gen2 Region A Raw]\n  C --> D[Databricks Delta Lake Region A]\n  D --> E[Purview Lineage]\n  D --> F[Unity Catalog RBAC]\n  D --> G[Cross-Region Replication to Region B]\n  G --> H[DR ADLS Gen2 Region B]","difficulty":"advanced","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Twitter","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T19:42:48.929Z","createdAt":"2026-01-12T19:42:48.929Z"},{"id":"q-1043","question":"Design an end-to-end Azure data-ecosystem pipeline that ingests incremental changes from a MongoDB Atlas collection into Delta Lake on ADLS Gen2, preserving SCD Type 2 history for a customers dimension, and handling schema drift. Include data lineage to Azure Purview, late-arriving updates, and on-demand reprocessing without data loss. Which components and config would you choose, and why?","answer":"Use MongoDB Atlas Change Streams with Debezium to stream inserts/updates into Kafka, then Spark Structured Streaming consumes from Kafka and writes to Delta Lake on ADLS Gen2 with Delta schema evoluti","explanation":"## Why This Is Asked\nThis question tests practical design decisions for real-time CDC pipelines across Azure, including lineage, schema drift, and reprocessing.\n\n## Key Concepts\n- MongoDB Atlas Change Streams and Debezium for CDC\n- Delta Lake schema evolution and SCD Type 2\n- Spark Structured Streaming from Kafka to Delta Lake\n- Azure Purview for end-to-end lineage\n- Late-arriving data handling and replay strategies\n\n## Code Example\n```javascript\n// Pseudo-implementation illustrating flow (language-agnostic)\nconst inStream = Kafka.read(\"mongodb-change-stream\");\nDelta.write(inStream, \"/mnt/adls/delta/customers\");\n```\n\n## Follow-up Questions\n- How would you implement schema drift handling in Delta Lake?\n- How do you validate and monitor Purview lineage end-to-end?\n- What are failure modes and how would you recover from late data or CDC gaps?\n","diagram":null,"difficulty":"intermediate","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","NVIDIA","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T20:30:13.903Z","createdAt":"2026-01-12T20:30:13.903Z"},{"id":"q-1311","question":"Design an end-to-end incremental ELT pipeline on Azure that ingests 2 TB/day of nested JSON telemetry from Event Hubs into a Delta Lake on ADLS Gen2, then loads a star schema in Azure Synapse. Requirements: (1) schema drift tolerant writes and schema evolution, (2) upserts by a composite key (tenant_id, event_id), (3) end-to-end data lineage to Purview, (4) late-arriving data support via watermarking, (5) partitioning by region and day with file-size/compaction strategies. Which components and patterns would you use, and why? Compare Delta Lake MERGE vs Delete+Insert for upserts?","answer":"Use Event Hubs → Databricks Structured Streaming → Delta Lake on ADLS Gen2 with schema evolution enabled. Upsert into a Delta table on (tenant_id, event_id) using MERGE; apply a 5-minute watermark for","explanation":"## Why This Is Asked\nTests practical ability to architect an Azure data pipeline handling schema drift, incremental loads, and governance across a hybrid stack. It also probes trade-offs between MERGE and Delete+Insert for upserts and how to manage late data and file sizing in a real-world volume.\n\n## Key Concepts\n- Event Hubs, Databricks Structured Streaming, Delta Lake on ADLS Gen2\n- Schema evolution and drift tolerance for nested JSON\n- Upserts via MERGE vs Delete+Insert\n- Watermarking for late data, partitioning strategy\n- Data lineage with Purview, serving layer in Azure Synapse\n\n## Code Example\n```python\n# PySpark sketch for streaming write with MERGE via foreachBatch\ndef upsert_to_delta(microBatchDF, batchId):\n    microBatchDF.createOrReplaceTempView('updates')\n    spark.sql(\"\"\"\n      MERGE INTO delta_table AS t\n      USING updates AS s\n      ON t.tenant_id = s.tenant_id AND t.event_id = s.event_id\n      WHEN MATCHED THEN UPDATE SET *\n      WHEN NOT MATCHED THEN INSERT *\n    \"\"\")\n\nstreamingDF.writeStream.foreachBatch(upsert_to_delta).option('checkpointLocation','/chkpt/path').start('/delta/path')\n```\n\n## Follow-up Questions\n- How would you tune for high cardinality keys without hotspotting?\n- How do you validate lineage in Purview across pipelines and schemas?","diagram":null,"difficulty":"intermediate","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Google","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T10:37:25.505Z","createdAt":"2026-01-13T10:37:25.505Z"},{"id":"q-1326","question":"Design an end-to-end pipeline that streams user activity from Azure Event Hubs into Delta Lake on ADLS Gen2, supports schema evolution, and updates an SCD Type 2 customer dimension in Synapse. Include exactly-once guarantees, late-arriving data handling, idempotent sinks, and end-to-end data lineage via Purview in a hybrid estate. List components, data flows, and governance approach?","answer":"Use Spark Structured Streaming on Azure Databricks to read Event Hubs, write to Delta Lake on ADLS Gen2 with auto schema evolution. Apply MERGE INTO to a Type 2 customer dimension in Synapse for upser","explanation":"## Why This Is Asked\nTests production-grade streaming & lakehouse design: schema evolution, upserts, lineage, and hybrid governance.\n\n## Key Concepts\n- Delta Lake schema evolution on ADLS Gen2\n- Structured Streaming with exactly-once guarantees\n- MERGE INTO for SCD Type 2 in Synapse\n- Purview data lineage across on-prem and cloud\n- Hybrid lakehouse governance and security\n\n## Code Example\n```python\n# PySpark pseudo\nfrom delta.tables import DeltaTable\nsource_df = spark.readStream.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"...\").load()[...]\ndelta_path = \"/mnt/delta/customers\"\ndelta_table = DeltaTable.forPath(spark, delta_path)\n# streaming merge example (simplified)\ndelta_table.alias(\"t\").merge(\n  source_df.alias(\"s\"),\n  \"t.customer_id = s.customer_id\"\n).whenMatchedUpdate(set={\"name\":\"s.name\",\"address\":\"s.address\",\"end_date\":\"current_timestamp()\"})\n .whenNotMatchedInsert(values={\"customer_id\":\"s.customer_id\",\"name\":\"s.name\",\"start_date\":\"current_timestamp()\"})\n .execute()\n```\n\n## Follow-up Questions\n- How would you handle late-arriving events with out-of-order progress?\n- What changes would you make to support global data cataloging and cross-region replication?","diagram":"flowchart TD\n  A[Event Hubs] --> B[Databricks Spark Structured Streaming]\n  B --> C[Delta Lake on ADLS Gen2]\n  C --> D[Synapse SCD Type 2]\n  D --> E[Purview Lineage]\n  E --> F[Hybrid Serving Layer]","difficulty":"advanced","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T11:36:41.027Z","createdAt":"2026-01-13T11:36:41.028Z"},{"id":"q-1453","question":"You are starting a beginner-friendly Azure data engineer task: daily telemetry logs arrive in ADLS Gen2 under /raw/telemetry/ in mixed formats (CSV and nested JSON). Design an event-driven pipeline using Azure Data Factory that fires on blob creation, ingests files, flattens nested structures to a canonical schema, handles optional fields and few schema drift cases, and loads into a simple star schema in Azure Synapse Analytics. Include incremental loading with a watermark and basic data quality checks. What components and steps would you implement?","answer":"Trigger with blob-created events to start an ADF pipeline; Data Flow flattens nested JSON/CSV into canonical columns, applying defaults for missing fields and handling drift; load curated Parquet to /","explanation":"## Why This Is Asked\nThis question evaluates ability to design an event-driven ingest pipeline that handles multi-format data, schema drift, and incremental loads with a simple star schema.\n\n## Key Concepts\n- Event-driven ingestion with BlobCreated events\n- Data Flow for schema normalization and drift handling\n- Staging area and Parquet/Delta-lite storage\n- MERGE-based upserts into a star schema in Synapse\n- Incremental loading using a watermark\n- Basic data quality checks and monitoring\n\n## Code Example\n```sql\nMERGE INTO dw.dbo.FactTelemetry AS t\nUSING staging.FactTelemetry AS s\nON t.event_id = s.event_id\nWHEN MATCHED THEN UPDATE SET\n  t.value = s.value,\n  t.event_time = s.event_time\nWHEN NOT MATCHED THEN INSERT (event_id, value, event_time) VALUES (s.event_id, s.value, s.event_time);\n```\n\n## Follow-up Questions\n- How would you test idempotency of the MERGE against late-arriving data?\n- What changes would you make to handle more complex nested payloads without breaking existing pipelines?","diagram":"flowchart TD\n  A[Blob Created] --> B[ADF Trigger]\n  B --> C[Data Flow: Normalize]\n  C --> D[Staging: Parquet in /curated]\n  D --> E[MERGE into Star Schema in Synapse]\n  E --> F[Partition by date]","difficulty":"beginner","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T17:48:57.023Z","createdAt":"2026-01-13T17:48:57.023Z"},{"id":"q-1474","question":"Design a nightly Azure data pipeline that ingests delta updates from a SaaS source into ADLS Gen2, handles schema drift with automatic inference, and materializes a star schema in Azure Synapse. Ensure end-to-end data lineage, idempotent upserts, and reliable rollback after failures. Which services and patterns would you deploy, and how would you verify correctness and lineage end-to-end?","answer":"Use Azure Data Factory to orchestrate nightly ingests from the SaaS source into ADLS Gen2; Spark on Synapse performs drift-tolerant transforms with automatic schema inference and writes to a Delta tab","explanation":"## Why This Is Asked\nAssesses practical ability to design a robust Azure data pipeline: ingestion, drift handling, lineage, and reliability.\n\n## Key Concepts\n- Orchestration: Azure Data Factory\n- Drift handling: Spark schema inference\n- Storage and warehousing: ADLS Gen2 and Delta Lake on Synapse\n- Lineage: Purview\n- Reliability: MERGE upserts and replay semantics\n\n## Code Example\n```sql\nMERGE INTO dim_sales AS target\nUSING staging_sales AS src\nON target.sale_id = src.sale_id\nWHEN MATCHED THEN UPDATE SET target.amount = src.amount, target.date = src.date\nWHEN NOT MATCHED THEN INSERT (sale_id, amount, date) VALUES (src.sale_id, src.amount, src.date);\n```\n\n## Follow-up Questions\n- How would you monitor data quality and lineage across components?\n- How would you implement rollback and replay when a batch fails?","diagram":"flowchart TD\n  A[ADF] --> B[Ingest to ADLS Gen2]\n  B --> C[Spark (Synapse) drift-aware transform]\n  C --> D[Delta table in Synapse]\n  D --> E[Purview lineage]","difficulty":"intermediate","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Oracle","Plaid","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T18:48:47.638Z","createdAt":"2026-01-13T18:48:47.638Z"},{"id":"q-1524","question":"You operate a global event lake: multiple producers push JSON events into Event Hubs. Ingest to ADLS Gen2, apply drift-tolerant transformations, and publish a canonical star schema in Azure Synapse with incremental loads. Propose a production-ready architecture leveraging Data Factory, Databricks Delta Lake, and Purview; detail schema evolution, late-arriving data handling, data quality gates, and privacy masking?","answer":"Use Event Hubs for streaming ingestion into ADLS Gen2, Databricks Delta Lake for streaming+batch with schema evolution, and Azure Synapse for serving. Orchestrate with Data Factory; Purview for lineag","explanation":"## Why This Is Asked\nAssesses ability to design an end-to-end, scalable, governed data lake with real-time and batch components, plus practical handling of schema drift and privacy.\n\n## Key Concepts\n- Event Hubs ingestion and streaming pipelines\n- Delta Lake schema evolution and merge on write\n- Incremental loads and late-arriving data handling\n- End-to-end data lineage with Purview\n- Data quality gates and privacy masking strategies\n\n## Code Example\n```javascript\n// Pseudo-implementation: enable schema evolution during write\nfunction writeDelta(df, path){\n  df.write\n    .format('delta')\n    .option('mergeSchema','true')\n    .mode('append')\n    .save(path)\n}\n```\n\n## Follow-up Questions\n- How would you monitor lineage and quality across pipelines?\n- What tests would you add for schema evolution compatibility across producers?","diagram":"flowchart TD\n  A[Event Hubs] --> B[Databricks Delta Lake (ADLS Gen2)]\n  B --> C[Azure Synapse (Serving Layer)]\n  D[ADF Orchestration] --> B\n  E[Purview] --> B\n  B --> F[Quality Masks & Privacy UDFs]","difficulty":"advanced","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","Scale Ai","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T20:41:30.601Z","createdAt":"2026-01-13T20:41:30.601Z"},{"id":"q-1563","question":"Daily JSON feed arrives in ADLS Gen2. Design a beginner Data Factory pipeline to stage, flatten nested fields, validate data quality, and write a partitioned Parquet table in Azure Synapse. Handle minor schema drift by defaults and ignoring new fields. Which components and steps would you use?","answer":"I would implement a two-step Data Factory pipeline: (1) Copy activity to stage the JSON files with schema-drift awareness enabled; (2) Mapping Data Flow to flatten nested counterparty fields, validate data quality (price > 0 and quantity > 0), and write partitioned Parquet files to Azure Synapse.","explanation":"## Why This Is Asked\nAssesses practical Data Factory usage, incremental loading capabilities, and basic schema drift handling for real-world JSON feeds.\n\n## Key Concepts\n- Data Factory Copy activity combined with Mapping Data Flow\n- Schema drift handling, default values, and nested field flattening\n- Incremental loading via control table; Parquet partitioning in Synapse\n\n## Code Example\n```sql\nMERGE INTO dbo.Trades AS t\nUSING staging.Trades AS s\nON t.TradeDate = s.TradeDate AND t.TradeId = s.TradeId\nWHEN MATCHED THEN UPDATE SET t.Price = s.Price, t.Qty = s.Qty\nWHEN NOT MATCHED THEN INSERT (TradeDate, TradeId, Price, Qty)\nVALUES (s.TradeDate, s.TradeId, s.Price, s.Qty)\n```","diagram":null,"difficulty":"beginner","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","DoorDash","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T06:18:48.949Z","createdAt":"2026-01-13T21:48:38.152Z"},{"id":"q-1638","question":"Design a streaming ingestion from an on-prem ERP into ADLS Gen2 for real-time analytics. Use Delta Lake with Auto Loader-like ingestion, bronze-silver-gold data path, exact-once delivery, and schema-drift tolerant processing. The silver layer feeds a star schema in Azure Synapse. Ensure end-to-end lineage in Purview, data quality gates, and robust late-arriving data handling. Describe architecture, contracts, and rollback strategy?","answer":"Use Databricks Auto Loader to ingest streaming data into Delta bronze on ADLS Gen2; transform with schema-drift tolerant MERGE into silver; feed a star schema in Azure Synapse; capture lineage in Purv","explanation":"## Why This Is Asked\nTests ability to design streaming pipeline across Azure components with real-time data, schema drift, CDC-like behavior, and governance.\n\n## Key Concepts\n- Databricks Auto Loader, Delta Lake, mergeSchema\n- Bronze-Silver-Gold streaming path\n- Azure Purview for end-to-end lineage\n- Azure Synapse Analytics star schema serving\n- Late-arriving data handling, watermarking, checkpointing, rollback\n\n## Code Example\n```javascript\n// Bronze: streaming read (Auto Loader-like)\nconst bronze = spark.readStream\n  .format('cloudFiles')\n  .option('cloudFiles.format','json')\n  .option('cloudFiles.inferColumnTypes','true')\n  .load('abfss://container@storage.dfs.core.windows.net/bronze');\n\n// Silver: upsert into Delta with schema drift tolerance\nbronze.writeStream\n  .format('delta')\n  .option('checkpointLocation','/mnt/checkpoints/silver')\n  .start('/mnt/delta/silver');\n\nDeltaTable.forPath(spark, '/mnt/delta/silver').as('s').merge(\n  bronze.alias('b'), 's.id = b.id'\n).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute();\n```\n\n## Follow-up Questions\n- How would you verify end-to-end lineage across Purview and Synapse?\n- How would you validate schema drift scenarios and rollback correctness?","diagram":"flowchart TD\n  Bronze[Bronze Delta] --> Silver[Silver Delta]\n  Silver --> Gold[Azure Synapse Star Schema]\n  Purview[Purview] --> Silver\n  Bronze --> Purview","difficulty":"advanced","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","Snowflake","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T04:24:07.043Z","createdAt":"2026-01-14T04:24:07.043Z"},{"id":"q-1682","question":"Design an end-to-end near-real-time data pipeline for a global adtech dataset with mixed sources (on-prem SQL Server, Azure SQL, and SaaS APIs). Implement CDC, incremental loads, and schema drift tolerance, load into Delta Lake on Synapse, and enforce end-to-end lineage in Purview. Include data masking for PII, data quality gates, and a GitOps workflow for pipelines?","answer":"CDC across mixed sources using Data Factory, streaming to ADLS Gen2 Raw; transform via Databricks Spark with Delta Lake on Synapse; publish curated layer to serverless SQL. Purview provides end-to-end","explanation":"## Why This Is Asked\n\nAssesses ability to architect a heterogeneous, scale-out data lakehouse with real-time ingress, schema drift handling, governance, and privacy controls, plus an actionable CI/CD model.\n\n## Key Concepts\n\n- CDC from multi-source systems into ADLS Gen2\n- Delta Lake on Synapse for unified storage and ACID semantics\n- Purview for lineage, classification, and policy enforcement\n- PII masking in the processing layer (e.g., Spark UDFs)\n- Data quality gates (e.g., Great Expectations) integrated into pipelines\n- GitOps-driven CI/CD for reproducible deployments\n\n## Code Example\n\n```python\n# Spark masking example (pseudo)\nfrom pyspark.sql.functions import when, col\n\ndf = df.withColumn(\n    'ssn_masked',\n    when(col('ssn').isNotNull(), 'XXX-XX-' + col('ssn').substr( -4, 4 )).otherwise(None)\n)\n```\n\n## Follow-up Questions\n\n- How would you handle GDPR data Subject Rights requests in this pipeline?\n- What are the trade-offs between serverless SQL vs provisioned Spark pools for the curated layer?","diagram":null,"difficulty":"advanced","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T06:54:31.901Z","createdAt":"2026-01-14T06:54:31.901Z"},{"id":"q-1782","question":"Design a beginner-level Azure data ingestion task: In a fintech scenario, ingest daily transaction rows from an on-premises SQL Server into Azure Data Lake Storage Gen2 as Parquet files using Azure Data Factory. Include steps to handle date partitioning, security, and reliable runs. What would your pipeline look like, and what minimal code or configuration would you provide?","answer":"Proposed: An ADF pipeline ingesting daily transactions from on-prem SQL Server to ADLS Gen2 as Parquet. Steps: 1) linked services for on-prem SQL and ADLS Gen2, 2) dataset with a loadDate param, 3) Co","explanation":"## Why This Is Asked\nThis question tests practical data ingestion skills using Azure Data Factory, including linked services, parameterized datasets, copy activities with predicates, and basic monitoring. It mirrors fintech use cases where day-partitioned Parquet blobs enable efficient downstream queries.\n\n## Key Concepts\n- Azure Data Factory pipelines\n- Parquet sink and date partitioning\n- Self-hosted integration runtime\n- Copy activity performance and retries\n\n## Code Example\n```javascript\n// Example: pseudo-JSON/ARM-like config is expected, not full code\n```\n\n## Follow-up Questions\n- How would you handle schema drift?\n- How would you implement idempotent loads and error retries?\n","diagram":null,"difficulty":"beginner","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Coinbase"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T10:45:04.491Z","createdAt":"2026-01-14T10:45:04.491Z"},{"id":"q-1916","question":"You manage a multi-region analytics platform with data from Salesforce, Workday, and IoT devices. Data lands in ADLS Gen2; you need to apply policy-driven redaction for PII, support incremental loads, enforce schema drift tolerance, and publish end-to-end lineage to Azure Purview and downstream consumers (Power BI, Synapse). Propose an Azure-native pipeline design using Data Factory, Databricks Delta Live Tables, Purview, and Synapse; highlight design decisions, governance, and potential pitfalls?","answer":"Use Delta Live Tables on Databricks for streaming and batch, with auto-loader to ingest from ADLS Gen2, implement structured streaming with incremental watermarking; apply UDF-based redaction for PII ","explanation":"## Why This Is Asked\nThis question probes cross-service data integration, governance, and privacy controls in a realistic multi-region setup. It tests practical tradeoffs between streaming vs batch, and lineage coverage.\n\n## Key Concepts\n- Delta Live Tables, Auto Loader, schema drift handling, incremental loads\n- Azure Purview for lineage, RBAC, data masking and masking policies\n- Synapse external tables and Power BI consumption\n\n## Code Example\n```python\n# Databricks pseudo\nfrom delta.tables import DeltaTable\n# redaction example\ndef mask_pii(s):\n    return re.sub(r\"\\\\d{12}\", \"XXX-XX-XXXX\", s)\n```\n\n## Follow-up Questions\n- How would you adapt this for GDPR data deletion requests?\n- How do you validate lineage coverage across new sources?","diagram":"flowchart TD\nA[Data sources] --> B[ADLS Gen2 landing]\nB --> C[Databricks Delta Live Tables]\nC --> D[Azure Purview lineage]\nC --> E[Synapse external tables]","difficulty":"advanced","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","MongoDB","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T17:00:08.420Z","createdAt":"2026-01-14T17:00:08.420Z"},{"id":"q-1979","question":"In Azure, design a production-grade data pipeline for streaming ecommerce telemetry from Azure Event Hubs into ADLS Gen2, with downstream star schema in Synapse, enabling incremental loads, automatic schema drift handling, and end-to-end data lineage via Purview. Include details on Delta Lake usage, MERGE-based upserts, late-arriving data handling, and a minimal Spark code snippet for a MERGE with schema drift tolerance?","answer":"Architect a streaming-to-batch hybrid pipeline: Event Hubs to ADLS Gen2 raw, Databricks Delta Lake as canonical layer, and MERGE-based upserts into a Synapse star schema. Include Purview for end-to-en","explanation":"## Why This Is Asked\nThis question probes end-to-end data engineering skills: streaming ingestion, lakehouse architecture, dimensional modeling in Synapse, and governance. It also tests ability to implement schema evolution and data contracts while maintaining lineage.\n\n## Key Concepts\n- Delta Lake schema evolution and MERGE-based upserts\n- Data lineage with Purview across ADLS Gen2, Databricks, and Synapse\n- Late-arriving data handling with watermarking\n- Star schema design in Synapse for analytics\n- Integration of Event Hubs, ADLS Gen2, Databricks, and Purview\n- Testing and monitoring in production\n\n## Code Example\n```javascript\nfrom delta.tables import DeltaTable\n# pseudo-spark snippet illustrating MERGE with schema drift handling\nsrc_df = spark.readStream.format(\"json\").load(\"abfss://container@account.dfs.core.windows.net/input/\")\ndelta = DeltaTable.forPath(spark, \"/mnt/delta/fact_telemetry\")\ndelta.alias(\"t\").merge(\n  src_df.alias(\"s\"),\n  \"t.id = s.id\"\n).whenMatchedUpdate(set = {\"value\": \"s.value\", \"ts\": \"s.ts\"})\\\\n .whenNotMatchedInsertAll().execute()\n```\n\n## Follow-up Questions\n- How would you validate schema drift events and automate alerts?\n- How would you test idempotency of MERGE loads in a CI/CD pipeline?","diagram":"flowchart TD\n  A[Event Hubs] --> B[ADLS Gen2 Raw]\n  B --> C[Databricks Delta Lake]\n  C --> D[Synapse Star Schema]\n  C --> E[Purview Lineage]","difficulty":"advanced","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T19:34:09.515Z","createdAt":"2026-01-14T19:34:09.515Z"},{"id":"q-2102","question":"A batch of daily JSON click events arrives in ADLS Gen2 under /raw/events/ from an on‑prem source via Self-Hosted IR. Build a beginner Data Factory pipeline that (1) copies raw JSON to /staging/events/, (2) uses a schema-drift-tolerant mapping to normalize fields and cast ts to timestamp, (3) writes to a daily-partitioned Parquet table in Azure Synapse, and (4) loads incrementally by tracking max ts in a control table and filtering ts > last_ts. Include a basic data quality check for userId?","answer":"Use a Self-Hosted Integration Runtime to copy daily JSON files from `/raw/events/*.json` to `/staging/events/`. A Data Flow handles schema drift: map known fields, cast `ts` to timestamp, and tolerate new fields. Sink to a daily-partitioned Parquet table in Azure Synapse. Implement incremental loading by tracking the maximum timestamp in a control table and filtering for `ts > last_ts`. Add a basic data quality check to ensure `userId` is not null.","explanation":"Why This Is Asked\nTests practical skills in building a beginner-friendly end-to-end ELT pipeline on Azure: moving data from on-premises to the data lake, handling simple schema drift, and implementing incremental loads without over-engineering.\n\nKey Concepts\n- Self-Hosted Integration Runtime for on-premises connectivity\n- Copy Activity and Data Flow in Azure Data Factory\n- Schema drift tolerance and type casting in mapping data flows\n- Parquet sinks with daily partitioning\n- Incremental loads via control table tracking\n- Basic data quality checks for non-null key fields\n\nCode Example\n```javascript","diagram":"flowchart TD\n  A[Raw events in ADLS Gen2] --> B[Copy to staging]\n  B --> C[Data Flow: schema drift tolerant mapping]\n  C --> D[Parquet sink: partition by date(ts)]\n  D --> E[Incremental: last_ts control table]","difficulty":"beginner","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","NVIDIA","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T05:00:21.037Z","createdAt":"2026-01-15T02:16:32.970Z"},{"id":"q-940","question":"Daily CSV exports arrive in an ADLS Gen2 container at /incoming/sales/. Files may evolve over time as columns drift. Build a beginner pipeline using Data Factory to stage raw data, infer/handle schema changes, and load a simple star schema in Azure Synapse Analytics. Include a watermark-based incremental load and basic scheduling. Which services and steps would you implement?","answer":"Use Azure Data Factory: Copy to land raw CSVs as Parquet in ADLS Gen2, Data Flow with allowSchemaDrift to map evolving columns to a fixed star-schema in Azure Synapse, sink into a dedicated SQL pool. ","explanation":"## Why This Is Asked\nTests end-to-end basics: file landing, simple schema evolution, incremental loads, and reporting schema in Synapse.\n\n## Key Concepts\n- Data Factory pipelines: Copy, Data Flow, Triggers\n- ADLS Gen2 and Parquet landing\n- Synapse star schema design and incremental loads\n- Watermark-based change capture\n\n## Code Example\n```javascript\n// JSON-like skeleton of pipeline stages\n{\n  name: 'SalesIngest',\n  activities: [\n    {copy: 'landing/parquet'},\n    {dataFlow: 'map-to-star', drift: true},\n    {sink: 'Synapse.star'},\n    {trigger: 'tumblingWindow', interval: 1}\n  ]\n}\n```\n\n## Follow-up Questions\n- How would you handle late-arriving data?\n- How would you monitor data quality and failures?","diagram":"flowchart TD\n  A[CSV arrives] --> B[Copy to landing Parquet]\n  B --> C[Data Flow: map to star schema]\n  C --> D[Sink to Synapse star schema]\n  D --> E[Incremental load by watermark]\n  E --> F[Update metadata log]","difficulty":"beginner","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Amazon","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T16:30:40.009Z","createdAt":"2026-01-12T16:30:40.009Z"},{"id":"q-980","question":"Design a global streaming-to-batch data pipeline for a PayPal/Adobe/Netflix-scale analytics platform: ingest real-time clickstream from Event Hubs into ADLS Gen2, maintain a Delta Lake with drift-tolerant schema, mask PII at ingestion, expose masked aggregates via serverless SQL pool, and enforce end-to-end lineage with Purview while supporting cross-region DR and data residency. Which Azure components and patterns would you use, and how would you handle schema evolution and failure modes?","answer":"Ingest raw clickstream from Event Hubs into ADLS Gen2 staging, use Spark Structured Streaming to write to Delta Lake with drift-aware schema evolution, apply a masking UDF before storing in curated zo","explanation":"## Why This Is Asked\nThis probes practical data governance, real-time-to-batch pipelines, and DR considerations in Azure for large-scale workloads.\n\n## Key Concepts\n- Event Hubs ingestion and Spark Structured Streaming\n- Delta Lake with schema drift handling\n- PII masking at ingestion\n- Purview lineage and data residency compliance\n- Cross-region DR and serverless analytics\n\n## Code Example\n```javascript\n// PII masking pseudo-code\nfunction maskPII(record) {\n  if (!record?.email) return record;\n  record.email = record.email.replace(/[^@]+@/, '*****@');\n  return record;\n}\n```\n\n## Follow-up Questions\n- How would you validate end-to-end lineage across regions?\n- What are failure modes in cross-region DR and mitigations?","diagram":"flowchart TD\n  A[Event Hubs] --> B[ADLS Gen2 staging]\n  B --> C[Delta Lake curated]\n  C --> D[Serverless SQL pool dashboards]\n  A --> E[Purview lineage]\n  F[Geo-DR] --> G[Region failover]","difficulty":"intermediate","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Netflix","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T17:44:06.541Z","createdAt":"2026-01-12T17:44:06.541Z"}],"subChannels":["general"],"companies":["Adobe","Airbnb","Amazon","Bloomberg","Citadel","Cloudflare","Coinbase","Databricks","DoorDash","Google","IBM","LinkedIn","MongoDB","NVIDIA","Netflix","OpenAI","Oracle","PayPal","Plaid","Robinhood","Scale Ai","Snap","Snowflake","Square","Twitter","Two Sigma","Uber","Zoom"],"stats":{"total":16,"beginner":5,"intermediate":4,"advanced":7,"newThisWeek":16}}