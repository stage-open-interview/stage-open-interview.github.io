{"questions":[{"id":"q-1029","question":"In a global IoT telemetry pipeline ingesting 100k events/s per region into ADLS Gen2 and Databricks Delta Lake, implement schema drift tolerant ingestion, end-to-end data lineage via Purview, RBAC, and cross-region DR replication. How would you architect the data contracts, partitioning, watermarking, retention, and governance to meet data residency and fault-tolerance requirements?","answer":"Architect a multi-region lakehouse: per-region ADLS Gen2, Databricks Delta Lake with mergeSchema and cloudFiles.inferColumnTypes, partition by region and device_type, watermark-based streaming, and Pu","explanation":"## Why This Is Asked\nAssesses multi-region lakehouse design, schema drift handling, data governance, and disaster recovery in Azure.\n\n## Key Concepts\n- Schema drift tolerant ingestion (Delta Lake, mergeSchema)\n- Data lineage (Purview) and RBAC (Unity Catalog)\n- Cross-region replication and data residency\n- Partitioning strategy and watermarking for IoT streams\n- Data contracts, retention, and failure handling\n\n## Code Example\n```python\nfrom pyspark.sql import functions as F\n\n# Ingest with auto schema, write to Delta with evolution\ndf = (\n  spark.readStream\n     .format(\"cloudFiles\")\n     .option(\"cloudFiles.format\",\"json\")\n     .option(\"cloudFiles.inferColumnTypes\",\"true\")\n     .load(\"/mnt/iot/raw/\")\n)\n\nquery = df.writeStream \\\n  .format(\"delta\") \\\n  .option(\"checkpointLocation\",\"/mnt/checkpoints/iot/stream\") \\\n  .option(\"mergeSchema\",\"true\") \\\n  .partitionBy(\"region\",\"device_type\") \\\n  .start(\"/mnt/delta/iot/region/\")\n```\n\n## Follow-up Questions\n- How would you validate cross-region data residency and schema compatibility across regions?\n- What testing strategy ensures Purview lineage remains intact after DR failover?","diagram":"flowchart TD\n  A[IoT Devices] --> B[Event Hub / IoT Hub]\n  B --> C[ADLS Gen2 Region A Raw]\n  C --> D[Databricks Delta Lake Region A]\n  D --> E[Purview Lineage]\n  D --> F[Unity Catalog RBAC]\n  D --> G[Cross-Region Replication to Region B]\n  G --> H[DR ADLS Gen2 Region B]","difficulty":"advanced","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Twitter","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T19:42:48.929Z","createdAt":"2026-01-12T19:42:48.929Z"},{"id":"q-1043","question":"Design an end-to-end Azure data-ecosystem pipeline that ingests incremental changes from a MongoDB Atlas collection into Delta Lake on ADLS Gen2, preserving SCD Type 2 history for a customers dimension, and handling schema drift. Include data lineage to Azure Purview, late-arriving updates, and on-demand reprocessing without data loss. Which components and config would you choose, and why?","answer":"Use MongoDB Atlas Change Streams with Debezium to stream inserts/updates into Kafka, then Spark Structured Streaming consumes from Kafka and writes to Delta Lake on ADLS Gen2 with Delta schema evoluti","explanation":"## Why This Is Asked\nThis question tests practical design decisions for real-time CDC pipelines across Azure, including lineage, schema drift, and reprocessing.\n\n## Key Concepts\n- MongoDB Atlas Change Streams and Debezium for CDC\n- Delta Lake schema evolution and SCD Type 2\n- Spark Structured Streaming from Kafka to Delta Lake\n- Azure Purview for end-to-end lineage\n- Late-arriving data handling and replay strategies\n\n## Code Example\n```javascript\n// Pseudo-implementation illustrating flow (language-agnostic)\nconst inStream = Kafka.read(\"mongodb-change-stream\");\nDelta.write(inStream, \"/mnt/adls/delta/customers\");\n```\n\n## Follow-up Questions\n- How would you implement schema drift handling in Delta Lake?\n- How do you validate and monitor Purview lineage end-to-end?\n- What are failure modes and how would you recover from late data or CDC gaps?\n","diagram":null,"difficulty":"intermediate","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","NVIDIA","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T20:30:13.903Z","createdAt":"2026-01-12T20:30:13.903Z"},{"id":"q-1311","question":"Design an end-to-end incremental ELT pipeline on Azure that ingests 2 TB/day of nested JSON telemetry from Event Hubs into a Delta Lake on ADLS Gen2, then loads a star schema in Azure Synapse. Requirements: (1) schema drift tolerant writes and schema evolution, (2) upserts by a composite key (tenant_id, event_id), (3) end-to-end data lineage to Purview, (4) late-arriving data support via watermarking, (5) partitioning by region and day with file-size/compaction strategies. Which components and patterns would you use, and why? Compare Delta Lake MERGE vs Delete+Insert for upserts?","answer":"Use Event Hubs → Databricks Structured Streaming → Delta Lake on ADLS Gen2 with schema evolution enabled. Upsert into a Delta table on (tenant_id, event_id) using MERGE; apply a 5-minute watermark for","explanation":"## Why This Is Asked\nTests practical ability to architect an Azure data pipeline handling schema drift, incremental loads, and governance across a hybrid stack. It also probes trade-offs between MERGE and Delete+Insert for upserts and how to manage late data and file sizing in a real-world volume.\n\n## Key Concepts\n- Event Hubs, Databricks Structured Streaming, Delta Lake on ADLS Gen2\n- Schema evolution and drift tolerance for nested JSON\n- Upserts via MERGE vs Delete+Insert\n- Watermarking for late data, partitioning strategy\n- Data lineage with Purview, serving layer in Azure Synapse\n\n## Code Example\n```python\n# PySpark sketch for streaming write with MERGE via foreachBatch\ndef upsert_to_delta(microBatchDF, batchId):\n    microBatchDF.createOrReplaceTempView('updates')\n    spark.sql(\"\"\"\n      MERGE INTO delta_table AS t\n      USING updates AS s\n      ON t.tenant_id = s.tenant_id AND t.event_id = s.event_id\n      WHEN MATCHED THEN UPDATE SET *\n      WHEN NOT MATCHED THEN INSERT *\n    \"\"\")\n\nstreamingDF.writeStream.foreachBatch(upsert_to_delta).option('checkpointLocation','/chkpt/path').start('/delta/path')\n```\n\n## Follow-up Questions\n- How would you tune for high cardinality keys without hotspotting?\n- How do you validate lineage in Purview across pipelines and schemas?","diagram":null,"difficulty":"intermediate","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Google","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T10:37:25.505Z","createdAt":"2026-01-13T10:37:25.505Z"},{"id":"q-1326","question":"Design an end-to-end pipeline that streams user activity from Azure Event Hubs into Delta Lake on ADLS Gen2, supports schema evolution, and updates an SCD Type 2 customer dimension in Synapse. Include exactly-once guarantees, late-arriving data handling, idempotent sinks, and end-to-end data lineage via Purview in a hybrid estate. List components, data flows, and governance approach?","answer":"Use Spark Structured Streaming on Azure Databricks to read Event Hubs, write to Delta Lake on ADLS Gen2 with auto schema evolution. Apply MERGE INTO to a Type 2 customer dimension in Synapse for upser","explanation":"## Why This Is Asked\nTests production-grade streaming & lakehouse design: schema evolution, upserts, lineage, and hybrid governance.\n\n## Key Concepts\n- Delta Lake schema evolution on ADLS Gen2\n- Structured Streaming with exactly-once guarantees\n- MERGE INTO for SCD Type 2 in Synapse\n- Purview data lineage across on-prem and cloud\n- Hybrid lakehouse governance and security\n\n## Code Example\n```python\n# PySpark pseudo\nfrom delta.tables import DeltaTable\nsource_df = spark.readStream.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"...\").load()[...]\ndelta_path = \"/mnt/delta/customers\"\ndelta_table = DeltaTable.forPath(spark, delta_path)\n# streaming merge example (simplified)\ndelta_table.alias(\"t\").merge(\n  source_df.alias(\"s\"),\n  \"t.customer_id = s.customer_id\"\n).whenMatchedUpdate(set={\"name\":\"s.name\",\"address\":\"s.address\",\"end_date\":\"current_timestamp()\"})\n .whenNotMatchedInsert(values={\"customer_id\":\"s.customer_id\",\"name\":\"s.name\",\"start_date\":\"current_timestamp()\"})\n .execute()\n```\n\n## Follow-up Questions\n- How would you handle late-arriving events with out-of-order progress?\n- What changes would you make to support global data cataloging and cross-region replication?","diagram":"flowchart TD\n  A[Event Hubs] --> B[Databricks Spark Structured Streaming]\n  B --> C[Delta Lake on ADLS Gen2]\n  C --> D[Synapse SCD Type 2]\n  D --> E[Purview Lineage]\n  E --> F[Hybrid Serving Layer]","difficulty":"advanced","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T11:36:41.027Z","createdAt":"2026-01-13T11:36:41.028Z"},{"id":"q-1453","question":"You are starting a beginner-friendly Azure data engineer task: daily telemetry logs arrive in ADLS Gen2 under /raw/telemetry/ in mixed formats (CSV and nested JSON). Design an event-driven pipeline using Azure Data Factory that fires on blob creation, ingests files, flattens nested structures to a canonical schema, handles optional fields and few schema drift cases, and loads into a simple star schema in Azure Synapse Analytics. Include incremental loading with a watermark and basic data quality checks. What components and steps would you implement?","answer":"Trigger with blob-created events to start an ADF pipeline; Data Flow flattens nested JSON/CSV into canonical columns, applying defaults for missing fields and handling drift; load curated Parquet to /","explanation":"## Why This Is Asked\nThis question evaluates ability to design an event-driven ingest pipeline that handles multi-format data, schema drift, and incremental loads with a simple star schema.\n\n## Key Concepts\n- Event-driven ingestion with BlobCreated events\n- Data Flow for schema normalization and drift handling\n- Staging area and Parquet/Delta-lite storage\n- MERGE-based upserts into a star schema in Synapse\n- Incremental loading using a watermark\n- Basic data quality checks and monitoring\n\n## Code Example\n```sql\nMERGE INTO dw.dbo.FactTelemetry AS t\nUSING staging.FactTelemetry AS s\nON t.event_id = s.event_id\nWHEN MATCHED THEN UPDATE SET\n  t.value = s.value,\n  t.event_time = s.event_time\nWHEN NOT MATCHED THEN INSERT (event_id, value, event_time) VALUES (s.event_id, s.value, s.event_time);\n```\n\n## Follow-up Questions\n- How would you test idempotency of the MERGE against late-arriving data?\n- What changes would you make to handle more complex nested payloads without breaking existing pipelines?","diagram":"flowchart TD\n  A[Blob Created] --> B[ADF Trigger]\n  B --> C[Data Flow: Normalize]\n  C --> D[Staging: Parquet in /curated]\n  D --> E[MERGE into Star Schema in Synapse]\n  E --> F[Partition by date]","difficulty":"beginner","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T17:48:57.023Z","createdAt":"2026-01-13T17:48:57.023Z"},{"id":"q-1474","question":"Design a nightly Azure data pipeline that ingests delta updates from a SaaS source into ADLS Gen2, handles schema drift with automatic inference, and materializes a star schema in Azure Synapse. Ensure end-to-end data lineage, idempotent upserts, and reliable rollback after failures. Which services and patterns would you deploy, and how would you verify correctness and lineage end-to-end?","answer":"Use Azure Data Factory to orchestrate nightly ingests from the SaaS source into ADLS Gen2; Spark on Synapse performs drift-tolerant transforms with automatic schema inference and writes to a Delta tab","explanation":"## Why This Is Asked\nAssesses practical ability to design a robust Azure data pipeline: ingestion, drift handling, lineage, and reliability.\n\n## Key Concepts\n- Orchestration: Azure Data Factory\n- Drift handling: Spark schema inference\n- Storage and warehousing: ADLS Gen2 and Delta Lake on Synapse\n- Lineage: Purview\n- Reliability: MERGE upserts and replay semantics\n\n## Code Example\n```sql\nMERGE INTO dim_sales AS target\nUSING staging_sales AS src\nON target.sale_id = src.sale_id\nWHEN MATCHED THEN UPDATE SET target.amount = src.amount, target.date = src.date\nWHEN NOT MATCHED THEN INSERT (sale_id, amount, date) VALUES (src.sale_id, src.amount, src.date);\n```\n\n## Follow-up Questions\n- How would you monitor data quality and lineage across components?\n- How would you implement rollback and replay when a batch fails?","diagram":"flowchart TD\n  A[ADF] --> B[Ingest to ADLS Gen2]\n  B --> C[Spark (Synapse) drift-aware transform]\n  C --> D[Delta table in Synapse]\n  D --> E[Purview lineage]","difficulty":"intermediate","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Oracle","Plaid","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T18:48:47.638Z","createdAt":"2026-01-13T18:48:47.638Z"},{"id":"q-1524","question":"You operate a global event lake: multiple producers push JSON events into Event Hubs. Ingest to ADLS Gen2, apply drift-tolerant transformations, and publish a canonical star schema in Azure Synapse with incremental loads. Propose a production-ready architecture leveraging Data Factory, Databricks Delta Lake, and Purview; detail schema evolution, late-arriving data handling, data quality gates, and privacy masking?","answer":"Use Event Hubs for streaming ingestion into ADLS Gen2, Databricks Delta Lake for streaming+batch with schema evolution, and Azure Synapse for serving. Orchestrate with Data Factory; Purview for lineag","explanation":"## Why This Is Asked\nAssesses ability to design an end-to-end, scalable, governed data lake with real-time and batch components, plus practical handling of schema drift and privacy.\n\n## Key Concepts\n- Event Hubs ingestion and streaming pipelines\n- Delta Lake schema evolution and merge on write\n- Incremental loads and late-arriving data handling\n- End-to-end data lineage with Purview\n- Data quality gates and privacy masking strategies\n\n## Code Example\n```javascript\n// Pseudo-implementation: enable schema evolution during write\nfunction writeDelta(df, path){\n  df.write\n    .format('delta')\n    .option('mergeSchema','true')\n    .mode('append')\n    .save(path)\n}\n```\n\n## Follow-up Questions\n- How would you monitor lineage and quality across pipelines?\n- What tests would you add for schema evolution compatibility across producers?","diagram":"flowchart TD\n  A[Event Hubs] --> B[Databricks Delta Lake (ADLS Gen2)]\n  B --> C[Azure Synapse (Serving Layer)]\n  D[ADF Orchestration] --> B\n  E[Purview] --> B\n  B --> F[Quality Masks & Privacy UDFs]","difficulty":"advanced","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","Scale Ai","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T20:41:30.601Z","createdAt":"2026-01-13T20:41:30.601Z"},{"id":"q-1563","question":"Daily JSON feed arrives in ADLS Gen2. Design a beginner Data Factory pipeline to stage, flatten nested fields, validate data quality, and write a partitioned Parquet table in Azure Synapse. Handle minor schema drift by defaults and ignoring new fields. Which components and steps would you use?","answer":"I would implement a two-step Data Factory pipeline: (1) Copy activity to stage the JSON files with schema-drift awareness enabled; (2) Mapping Data Flow to flatten nested counterparty fields, validate data quality (price > 0 and quantity > 0), and write partitioned Parquet files to Azure Synapse.","explanation":"## Why This Is Asked\nAssesses practical Data Factory usage, incremental loading capabilities, and basic schema drift handling for real-world JSON feeds.\n\n## Key Concepts\n- Data Factory Copy activity combined with Mapping Data Flow\n- Schema drift handling, default values, and nested field flattening\n- Incremental loading via control table; Parquet partitioning in Synapse\n\n## Code Example\n```sql\nMERGE INTO dbo.Trades AS t\nUSING staging.Trades AS s\nON t.TradeDate = s.TradeDate AND t.TradeId = s.TradeId\nWHEN MATCHED THEN UPDATE SET t.Price = s.Price, t.Qty = s.Qty\nWHEN NOT MATCHED THEN INSERT (TradeDate, TradeId, Price, Qty)\nVALUES (s.TradeDate, s.TradeId, s.Price, s.Qty)\n```","diagram":null,"difficulty":"beginner","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","DoorDash","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T06:18:48.949Z","createdAt":"2026-01-13T21:48:38.152Z"},{"id":"q-1638","question":"Design a streaming ingestion from an on-prem ERP into ADLS Gen2 for real-time analytics. Use Delta Lake with Auto Loader-like ingestion, bronze-silver-gold data path, exact-once delivery, and schema-drift tolerant processing. The silver layer feeds a star schema in Azure Synapse. Ensure end-to-end lineage in Purview, data quality gates, and robust late-arriving data handling. Describe architecture, contracts, and rollback strategy?","answer":"Use Databricks Auto Loader to ingest streaming data into Delta bronze on ADLS Gen2; transform with schema-drift tolerant MERGE into silver; feed a star schema in Azure Synapse; capture lineage in Purv","explanation":"## Why This Is Asked\nTests ability to design streaming pipeline across Azure components with real-time data, schema drift, CDC-like behavior, and governance.\n\n## Key Concepts\n- Databricks Auto Loader, Delta Lake, mergeSchema\n- Bronze-Silver-Gold streaming path\n- Azure Purview for end-to-end lineage\n- Azure Synapse Analytics star schema serving\n- Late-arriving data handling, watermarking, checkpointing, rollback\n\n## Code Example\n```javascript\n// Bronze: streaming read (Auto Loader-like)\nconst bronze = spark.readStream\n  .format('cloudFiles')\n  .option('cloudFiles.format','json')\n  .option('cloudFiles.inferColumnTypes','true')\n  .load('abfss://container@storage.dfs.core.windows.net/bronze');\n\n// Silver: upsert into Delta with schema drift tolerance\nbronze.writeStream\n  .format('delta')\n  .option('checkpointLocation','/mnt/checkpoints/silver')\n  .start('/mnt/delta/silver');\n\nDeltaTable.forPath(spark, '/mnt/delta/silver').as('s').merge(\n  bronze.alias('b'), 's.id = b.id'\n).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute();\n```\n\n## Follow-up Questions\n- How would you verify end-to-end lineage across Purview and Synapse?\n- How would you validate schema drift scenarios and rollback correctness?","diagram":"flowchart TD\n  Bronze[Bronze Delta] --> Silver[Silver Delta]\n  Silver --> Gold[Azure Synapse Star Schema]\n  Purview[Purview] --> Silver\n  Bronze --> Purview","difficulty":"advanced","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","Snowflake","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T04:24:07.043Z","createdAt":"2026-01-14T04:24:07.043Z"},{"id":"q-1682","question":"Design an end-to-end near-real-time data pipeline for a global adtech dataset with mixed sources (on-prem SQL Server, Azure SQL, and SaaS APIs). Implement CDC, incremental loads, and schema drift tolerance, load into Delta Lake on Synapse, and enforce end-to-end lineage in Purview. Include data masking for PII, data quality gates, and a GitOps workflow for pipelines?","answer":"CDC across mixed sources using Data Factory, streaming to ADLS Gen2 Raw; transform via Databricks Spark with Delta Lake on Synapse; publish curated layer to serverless SQL. Purview provides end-to-end","explanation":"## Why This Is Asked\n\nAssesses ability to architect a heterogeneous, scale-out data lakehouse with real-time ingress, schema drift handling, governance, and privacy controls, plus an actionable CI/CD model.\n\n## Key Concepts\n\n- CDC from multi-source systems into ADLS Gen2\n- Delta Lake on Synapse for unified storage and ACID semantics\n- Purview for lineage, classification, and policy enforcement\n- PII masking in the processing layer (e.g., Spark UDFs)\n- Data quality gates (e.g., Great Expectations) integrated into pipelines\n- GitOps-driven CI/CD for reproducible deployments\n\n## Code Example\n\n```python\n# Spark masking example (pseudo)\nfrom pyspark.sql.functions import when, col\n\ndf = df.withColumn(\n    'ssn_masked',\n    when(col('ssn').isNotNull(), 'XXX-XX-' + col('ssn').substr( -4, 4 )).otherwise(None)\n)\n```\n\n## Follow-up Questions\n\n- How would you handle GDPR data Subject Rights requests in this pipeline?\n- What are the trade-offs between serverless SQL vs provisioned Spark pools for the curated layer?","diagram":null,"difficulty":"advanced","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T06:54:31.901Z","createdAt":"2026-01-14T06:54:31.901Z"},{"id":"q-1782","question":"Design a beginner-level Azure data ingestion task: In a fintech scenario, ingest daily transaction rows from an on-premises SQL Server into Azure Data Lake Storage Gen2 as Parquet files using Azure Data Factory. Include steps to handle date partitioning, security, and reliable runs. What would your pipeline look like, and what minimal code or configuration would you provide?","answer":"Proposed: An ADF pipeline ingesting daily transactions from on-prem SQL Server to ADLS Gen2 as Parquet. Steps: 1) linked services for on-prem SQL and ADLS Gen2, 2) dataset with a loadDate param, 3) Co","explanation":"## Why This Is Asked\nThis question tests practical data ingestion skills using Azure Data Factory, including linked services, parameterized datasets, copy activities with predicates, and basic monitoring. It mirrors fintech use cases where day-partitioned Parquet blobs enable efficient downstream queries.\n\n## Key Concepts\n- Azure Data Factory pipelines\n- Parquet sink and date partitioning\n- Self-hosted integration runtime\n- Copy activity performance and retries\n\n## Code Example\n```javascript\n// Example: pseudo-JSON/ARM-like config is expected, not full code\n```\n\n## Follow-up Questions\n- How would you handle schema drift?\n- How would you implement idempotent loads and error retries?\n","diagram":null,"difficulty":"beginner","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Coinbase"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T10:45:04.491Z","createdAt":"2026-01-14T10:45:04.491Z"},{"id":"q-1916","question":"You manage a multi-region analytics platform with data from Salesforce, Workday, and IoT devices. Data lands in ADLS Gen2; you need to apply policy-driven redaction for PII, support incremental loads, enforce schema drift tolerance, and publish end-to-end lineage to Azure Purview and downstream consumers (Power BI, Synapse). Propose an Azure-native pipeline design using Data Factory, Databricks Delta Live Tables, Purview, and Synapse; highlight design decisions, governance, and potential pitfalls?","answer":"Use Delta Live Tables on Databricks for streaming and batch, with auto-loader to ingest from ADLS Gen2, implement structured streaming with incremental watermarking; apply UDF-based redaction for PII ","explanation":"## Why This Is Asked\nThis question probes cross-service data integration, governance, and privacy controls in a realistic multi-region setup. It tests practical tradeoffs between streaming vs batch, and lineage coverage.\n\n## Key Concepts\n- Delta Live Tables, Auto Loader, schema drift handling, incremental loads\n- Azure Purview for lineage, RBAC, data masking and masking policies\n- Synapse external tables and Power BI consumption\n\n## Code Example\n```python\n# Databricks pseudo\nfrom delta.tables import DeltaTable\n# redaction example\ndef mask_pii(s):\n    return re.sub(r\"\\\\d{12}\", \"XXX-XX-XXXX\", s)\n```\n\n## Follow-up Questions\n- How would you adapt this for GDPR data deletion requests?\n- How do you validate lineage coverage across new sources?","diagram":"flowchart TD\nA[Data sources] --> B[ADLS Gen2 landing]\nB --> C[Databricks Delta Live Tables]\nC --> D[Azure Purview lineage]\nC --> E[Synapse external tables]","difficulty":"advanced","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","MongoDB","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T17:00:08.420Z","createdAt":"2026-01-14T17:00:08.420Z"},{"id":"q-1979","question":"In Azure, design a production-grade data pipeline for streaming ecommerce telemetry from Azure Event Hubs into ADLS Gen2, with downstream star schema in Synapse, enabling incremental loads, automatic schema drift handling, and end-to-end data lineage via Purview. Include details on Delta Lake usage, MERGE-based upserts, late-arriving data handling, and a minimal Spark code snippet for a MERGE with schema drift tolerance?","answer":"Architect a streaming-to-batch hybrid pipeline: Event Hubs to ADLS Gen2 raw, Databricks Delta Lake as canonical layer, and MERGE-based upserts into a Synapse star schema. Include Purview for end-to-en","explanation":"## Why This Is Asked\nThis question probes end-to-end data engineering skills: streaming ingestion, lakehouse architecture, dimensional modeling in Synapse, and governance. It also tests ability to implement schema evolution and data contracts while maintaining lineage.\n\n## Key Concepts\n- Delta Lake schema evolution and MERGE-based upserts\n- Data lineage with Purview across ADLS Gen2, Databricks, and Synapse\n- Late-arriving data handling with watermarking\n- Star schema design in Synapse for analytics\n- Integration of Event Hubs, ADLS Gen2, Databricks, and Purview\n- Testing and monitoring in production\n\n## Code Example\n```javascript\nfrom delta.tables import DeltaTable\n# pseudo-spark snippet illustrating MERGE with schema drift handling\nsrc_df = spark.readStream.format(\"json\").load(\"abfss://container@account.dfs.core.windows.net/input/\")\ndelta = DeltaTable.forPath(spark, \"/mnt/delta/fact_telemetry\")\ndelta.alias(\"t\").merge(\n  src_df.alias(\"s\"),\n  \"t.id = s.id\"\n).whenMatchedUpdate(set = {\"value\": \"s.value\", \"ts\": \"s.ts\"})\\\\n .whenNotMatchedInsertAll().execute()\n```\n\n## Follow-up Questions\n- How would you validate schema drift events and automate alerts?\n- How would you test idempotency of MERGE loads in a CI/CD pipeline?","diagram":"flowchart TD\n  A[Event Hubs] --> B[ADLS Gen2 Raw]\n  B --> C[Databricks Delta Lake]\n  C --> D[Synapse Star Schema]\n  C --> E[Purview Lineage]","difficulty":"advanced","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T19:34:09.515Z","createdAt":"2026-01-14T19:34:09.515Z"},{"id":"q-2102","question":"A batch of daily JSON click events arrives in ADLS Gen2 under /raw/events/ from an on‑prem source via Self-Hosted IR. Build a beginner Data Factory pipeline that (1) copies raw JSON to /staging/events/, (2) uses a schema-drift-tolerant mapping to normalize fields and cast ts to timestamp, (3) writes to a daily-partitioned Parquet table in Azure Synapse, and (4) loads incrementally by tracking max ts in a control table and filtering ts > last_ts. Include a basic data quality check for userId?","answer":"Use a Self-Hosted Integration Runtime to copy daily JSON files from `/raw/events/*.json` to `/staging/events/`. A Data Flow handles schema drift: map known fields, cast `ts` to timestamp, and tolerate new fields. Sink to a daily-partitioned Parquet table in Azure Synapse. Implement incremental loading by tracking the maximum timestamp in a control table and filtering for `ts > last_ts`. Add a basic data quality check to ensure `userId` is not null.","explanation":"Why This Is Asked\nTests practical skills in building a beginner-friendly end-to-end ELT pipeline on Azure: moving data from on-premises to the data lake, handling simple schema drift, and implementing incremental loads without over-engineering.\n\nKey Concepts\n- Self-Hosted Integration Runtime for on-premises connectivity\n- Copy Activity and Data Flow in Azure Data Factory\n- Schema drift tolerance and type casting in mapping data flows\n- Parquet sinks with daily partitioning\n- Incremental loads via control table tracking\n- Basic data quality checks for non-null key fields\n\nCode Example\n```javascript","diagram":"flowchart TD\n  A[Raw events in ADLS Gen2] --> B[Copy to staging]\n  B --> C[Data Flow: schema drift tolerant mapping]\n  C --> D[Parquet sink: partition by date(ts)]\n  D --> E[Incremental: last_ts control table]","difficulty":"beginner","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","NVIDIA","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T05:00:21.037Z","createdAt":"2026-01-15T02:16:32.970Z"},{"id":"q-2378","question":"Design a scalable pipeline to ingest daily customer event logs from an on-prem Kafka cluster into Azure Data Lake Gen2, apply schema drift tolerant transformations, and maintain an SCD Type 2 star schema in Azure Synapse, while providing end-to-end data lineage via Purview. Which Azure services and patterns would you use, and how would you implement incremental loads?","answer":"Ingest daily event logs from on‑prem Kafka into ADLS Gen2 via ADF or Dataflow, store as Parquet, then use Databricks Delta Lake to implement SCD Type 2 within a star schema in Azure Synapse. Use Purvi","explanation":"## Why This Is Asked\nThis question tests real-world Azure data pipelines with hybrid sources, schema drift, incremental loads, and lineage, aligning with roles at Snowflake, DoorDash, and Lyft.\n\n## Key Concepts\n- Ingest from on-prem Kafka to ADLS Gen2 with incremental offsets\n- Delta Lake SCD Type 2 and schema evolution\n- Star schema design in Azure Synapse\n- End-to-end data lineage with Purview across hybrid estate\n- Secure, scalable access via managed identities and RBAC\n\n## Code Example\n```javascript\n// Pseudo-Spark-like pseudocode for incremental load and SCD2 merge\nval source = spark.read.format(\"parquet\").load(\"abfss://landing@storage.dfs.core.windows.net/events/\")\nval target = spark.read.table(\"analytics.star_fact\")\nval merged = target.alias(\"t\").merge(\n  source.alias(\"s\"), \"t.id = s.id\"\n).whenMatchedUpdate(set = mapFromColumns(\"t\", \"s\"))\n  .whenNotMatchedInsertAll()\n```\n\n## Follow-up Questions\n- How would you handle late-arriving or out-of-order events in the stream?\n- How would you validate lineage accuracy across Purview and ensure changes propagate to downstream analytics?\n","diagram":null,"difficulty":"intermediate","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Lyft","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T15:43:20.714Z","createdAt":"2026-01-15T15:43:20.714Z"},{"id":"q-2495","question":"Ingest JSON telemetry from multiple partners into ADLS Gen2 and upsert into a Delta Lake on Synapse while preserving end-to-end lineage and data quality. Outline an Azure-based architecture using IoT Hub/Event Hubs, Data Factory or Synapse pipelines, Databricks, Purview, and Delta Lake features to handle schema drift, late data, and incremental upserts. Include security, cost considerations, and observability?","answer":"Ingest JSON telemetry via IoT Hub/Event Hubs into ADLS Gen2 as raw. Use Databricks Spark with schema drift tolerant transforms, then write curated data to Delta Lake on Synapse and use MERGE for incre","explanation":"## Why This Is Asked\nTests end-to-end Azure data lakehouse skills: streaming ingestion, schema drift, Delta Lake upserts, data governance, and observability. It stresses hybrid data governance via Purview and practical security/cost tradeoffs.\n\n## Key Concepts\n- Ingestion: IoT Hub/Event Hubs with ADLS Gen2 landing zones\n- Processing: Databricks Spark with schema evolution\n- Storage: Delta Lake on Synapse with MERGE\n- Governance: Purview lineage and data catalog\n- Quality/Observability: expectations, monitoring, RBAC\n\n## Code Example\n```python\n# PySpark sketch: read raw, infer schema, write delta, MERGE on upsert\nraw = spark.read.json('/mnt/raw/telemetry')\ncurated = raw.select('deviceId','ts','metrics.*')\ncurated.write.format('delta').mode('append').save('/delta/telemetry')\n# MERGE example (pseudo)\ndeltaTable = DeltaTable.forPath(spark, '/delta/telemetry')\ndeltaTable.alias('t').merge(source=curated.alias('s'), condition='t.id = s.id').whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n```\n\n## Follow-up Questions\n- How would you implement late-arriving data handling?\n- How would you test/validate lineage across layers in Purview?","diagram":"flowchart TD\n  Ingest[/IoT Hub/Event Hubs/] --> RAW[ADLS Gen2 Raw]\n  RAW --> SPARK[Databricks Spark]\n  SPARK --> DELTA[Delta Lake on Synapse]\n  DELTA --> PURVIEW[Purview Lineage]\n  DELTA --> QA[Quality Checks]","difficulty":"intermediate","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["OpenAI","Plaid","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T20:36:20.587Z","createdAt":"2026-01-15T20:36:20.588Z"},{"id":"q-2529","question":"Design an Azure data engineering pipeline for streaming IoT telemetry from store sensors into ADLS Gen2 and Synapse, with schema drift tolerant transformations, incremental loads, and end-to-end data lineage. Use Event Hubs to ADLS Gen2, Delta Lake in Synapse, and Purview for lineage. Address late data, schema evolution, security, and cross-region replication. What services and data contracts would you implement, and how would you validate correctness?","answer":"Ingest telemetry from Event Hubs using Spark Structured Streaming in Synapse, writing to a Delta Lake raw zone on ADLS Gen2. Enable schema auto-merge for evolution, use watermarks to handle late data, then apply incremental transformations through curated zones. Implement Purview for end-to-end lineage, enforce RBAC and Key Vault for security, and configure cross-region replication with Azure Geo-Redundant Storage. Validate correctness through data quality rules, schema validation tests, and end-to-end pipeline monitoring.","explanation":"## Why This Is Asked\nInterview context evaluates streaming design, Delta Lake, and governance integration in Azure.\n\n## Key Concepts\n- Event Hubs, Spark Structured Streaming\n- Delta Lake autoMerge, schema evolution\n- Purview for lineage and data catalog\n- RBAC, Key Vault security\n- Late data handling and end-to-end validation\n\n## Code Example\n```javascript\n// Pseudo Spark streaming snippet (Scala/Python-like)\nconst df = spark.readStream.format(\"eventhubs\").option(\"startingPosition\",\"latest\").load()\nconst curated = df.selectExpr(\"cast(body as string) as payload\")\ncurated.writeStream.format(\"delta\").option(\"checkpointLocation\",\"/checkpoints\").start()\n```","diagram":"flowchart TD\n  A[Event Hubs] --> B[ADLS Gen2 Raw]\n  B --> C[Delta Lake (Synapse)]\n  C --> D[Purview Lineage]\n  C --> E[Curated Tables]\n  E --> F[BI/Analytics]\n","difficulty":"intermediate","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Hashicorp","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T05:41:55.547Z","createdAt":"2026-01-15T21:39:09.922Z"},{"id":"q-2605","question":"Design an end-to-end real-time ingestion and analytics pipeline for a live market tick feed (timestamp, symbol, price, volume) arriving from multiple brokers at high throughput. Ingest to ADLS Gen2 as Delta Lake, with schema drift tolerance and automatic schema evolution, and deliver upserts to a central ticks table. Ensure exactly-once streaming, late-arriving data handling, end-to-end data lineage via Purview, and scalable partitioning by date and symbol. Outline components (Event Hubs, Databricks, Delta Lake, Purview) and provide a workable configuration sketch?","answer":"Ingest tick data from brokers into Azure Event Hubs, consume with Databricks Structured Streaming, and write to Delta Lake on ADLS Gen2 with schema evolution enabled. Use MERGE for upserts, a watermark for late-arriving data, and configure exactly-once semantics. Enable Purview integration for end-to-end lineage and partition by date/symbol for scalability.","explanation":"## Why This Is Asked\nTests real-time Azure data-engineering skills with schema drift, exactly-once semantics, and lineage.\n\n## Key Concepts\n- Streaming ingestion (Event Hubs)\n- Delta Lake schema evolution\n- Idempotent upserts (MERGE)\n- Late data handling (watermarks)\n- Data lineage (Purview)\n\n## Code Example\n```javascript\n// pseudo-config illustrating options, not runnable\n```\n\n## Follow-up Questions\n- How would you validate end-to-end latency?\n- What failure modes and compensating actions for late data?","diagram":null,"difficulty":"advanced","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Slack","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T05:02:41.068Z","createdAt":"2026-01-16T02:33:46.893Z"},{"id":"q-2625","question":"Daily on-prem JSON event files arrive; design a beginner ELT pipeline using Data Factory that ingests JSON, enforces a basic schema, converts to Parquet, partitions by event_date, and loads a simple star schema into Azure Synapse. Explain how you’d implement basic data lineage and a 7-day retention policy, without advanced schema drift handling?","answer":"Use Data Factory with a self-hosted integration runtime to pull daily on-prem JSON logs into ADLS Gen2 staging, then a Data Flow to enforce a basic schema and flatten objects, write as Parquet partiti","explanation":"## Why This Is Asked\nInterview context explains end-to-end data movement from on-prem to ADLS Gen2, then to Synapse, plus basic governance and lifecycle considerations.\n\n## Key Concepts\n- On-prem to cloud data ingestion with Data Factory\n- JSON to Parquet conversion and partitioning by date\n- Star schema loading into Azure Synapse\n- Basic data lineage with Purview\n- Simple retention via ADLS lifecycle\n\n## Code Example\n```sql\n-- Skeleton for star schema loading\nCREATE TABLE dim_customer (...);\nCREATE TABLE dim_product (...);\nCREATE TABLE fact_orders (...);\n```\n\n## Follow-up Questions\n- How would you adapt for evolving JSON schemas?\n- How would you monitor ingestion reliability and latency?","diagram":"flowchart TD\n  A[On-prem JSON] --> B[Self-hosted IR]\n  B --> C[ADLS Gen2 (staging)]\n  C --> D[Data Flow Parquet]\n  D --> E[Azure Synapse (Star schema)]\n  E --> F[Purview lineage]\n  F --> G[ADLS Lifecycle 7d]","difficulty":"beginner","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Goldman Sachs","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T04:10:40.457Z","createdAt":"2026-01-16T04:10:40.457Z"},{"id":"q-2669","question":"Design an Azure streaming data pipeline for a global fleet of IoT devices sending JSON telemetry to Event Hubs, with data landing in ADLS Gen2 and a Delta Lake lakehouse. Requirements: sub-5 second ingestion latency, end-to-end latency <15 seconds for dashboards, automatic schema evolution, idempotent upserts, and end-to-end lineage with Purview. Outline components, dataflow, and a minimal Delta table evolution example?","answer":"Use Event Hubs as ingress, Databricks Structured Streaming to Delta Lake on ADLS Gen2. Bronze raw ingest, Silver with schema-evolution via mergeSchema, and upserts via MERGE INTO. Enable checkpoints, ","explanation":"## Why This Is Asked\nThis probes implementing a robust streaming lakehouse with schema drift handling, governance, and cost control across regions.\n\n## Key Concepts\n- Event Hubs ingress, Delta Lake on ADLS Gen2\n- Structured Streaming, mergeSchema for evolution, MERGE INTO for upserts\n- Checkpointing, watermarks, idempotent loads\n- Purview lineage and Azure RBAC\n\n## Code Example\n```python\n# PySpark pseudo\nbronze = (spark.readStream.format('eventhubs').options(**ehConf).load())\n# parse, then write bronze\nbronze.writeStream.format('delta').option('checkpointLocation','/cp/bronze').start('/deltalake/bronze')\n\nsilver = spark.readStream.format('delta').load('/deltalake/bronze')\n# transform and upsert into silver with schema evolution\nsilver.writeStream.format('delta').option('checkpointLocation','/cp/silver').option('mergeSchema','true').start('/deltalake/silver')\n```\n\n## Follow-up Questions\n- How would you implement late-arrival handling and data quality checks in this stack?\n- What monitoring and alerting would you add for schema drift and job failures?","diagram":"flowchart TD\n  EH[Event Hubs] --> Bronze[Bronze Delta]\n  Bronze --> Silver[Silver Delta (mergeSchema)]\n  Silver --> Dash[Dashboards/BI]\n  Purview[Purview] --> Lineage[Lineage tracking]","difficulty":"intermediate","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Discord","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T05:52:01.037Z","createdAt":"2026-01-16T05:52:01.038Z"},{"id":"q-2680","question":"Design an end-to-end Azure data platform to ingest per-tenant e-commerce clickstream data from an on-prem SFTP into ADLS Gen2, handle schema drift, and deliver per-tenant analytics in Synapse while preserving end-to-end lineage in Purview and enforcing RBAC/column masking. Implement incremental loads, late-data handling, and cost/perf tradeoffs between serverless vs dedicated pools?","answer":"Ingest per-tenant data from on-prem SFTP into ADLS Gen2. Use Spark-based ETL to apply schema-drift tolerant transforms and write Delta Lake tables. Enforce per-tenant RBAC and column masking in Synaps","explanation":"## Why This Is Asked\n\nTests ability to design a scalable, governed lakehouse with multi-tenant isolation, schema drift handling, incremental loads, and integration with Purview and Synapse. Also probes trade-offs between serverless and dedicated pools.\n\n## Key Concepts\n\n- Delta Lake on ADLS Gen2\n- Synapse RBAC and dynamic masking\n- Purview data lineage\n- SFTP ingestion and Spark ETL\n- Incremental loads and late-arrival handling\n\n## Code Example\n\n```python\nfrom delta.tables import DeltaTable\n# sample MERGE snippet demonstrating upsert\ndelta_table = DeltaTable.forPath(spark, \"abfss://...@container.dfs.core.windows.net/tenant.delta\")\nsrc = spark.read.parquet(\"abfss://.../incoming/tenant/\")\n\ndelta_table.alias(\"t\").merge(\n  src.alias(\"s\"),\n  \"t.id = s.id\"\n).whenMatchedUpdate(set={\"t.value\": \"s.value\"}).whenNotMatchedInsertAll().execute()\n```\n\n## Follow-up Questions\n\n- How would you audit lineage across on-prem and cloud landing zones?\n- What are cost controls for large multi-tenant workloads?\n- How to validate schema drift and auto-infer schemas safely?\n","diagram":"flowchart TD\n  A[On-prem SFTP] --> B[ADF/ETL]\n  B --> C[ADLS Gen2 Delta Lake]\n  C --> D[Azure Synapse RBAC/Masking]\n  D --> E[Purview Lineage]\n  E --> F[Consumption Layer]","difficulty":"advanced","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Coinbase","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T06:53:36.282Z","createdAt":"2026-01-16T06:53:36.282Z"},{"id":"q-2712","question":"Design a global Azure data lakehouse for real-time fraud analytics. Ingest streaming transaction events from on-prem ERP into ADLS Gen2, apply schema-drift tolerant transformations, and store as Delta Lake tables with governance via Unity Catalog. Implement incremental loads with CDC and watermark, ensure end-to-end lineage via Purview, and support multi-region reads with geo-replication. Which components and patterns would you choose, and how would you configure them to balance latency, cost, and compliance?","answer":"CDC from on-prem ERP to Event Hubs; Databricks Structured Streaming reads Event Hubs, performs schema-evolution-aware transformations, and writes to Delta Lake on ADLS Gen2. Use Unity Catalog for fine","explanation":"Why This Is Asked\n\nThis question probes knowledge of end-to-end Azure data lakehouse architectures, focusing on real-time ingestion, schema drift handling, and governance at scale across regions.\n\nKey Concepts\n\n- CDC from on-prem systems via Event Hubs or Debezium\n- Spark Structured Streaming and Delta Lake on ADLS Gen2 with schema evolution\n- Unity Catalog for access control and governance\n- Purview for lineage and data governance\n- Watermarking, incremental upserts, and state management\n- Geo-replication and multi-region reads\n\nCode Example\n\n```javascript\n// Pseudocode: read from Event Hubs, parse, upsert to Delta Lake\nconst spark = SparkSession.builder().getOrCreate();\nlet raw = spark.readStream\n  .format('eventhubs')\n  .option('startingOffsets', 'latest')\n  .load();\nlet parsed = raw.selectExpr(\"CAST(body AS STRING) AS json\").selectExpr(\"from_json(json, <schema>) AS data\").select('data.*');\nparsed.writeStream()\n  .format('delta')\n  .option('checkpointLocation', '/checkpoints/transactions')\n  .option('path', 'abfss://<container>@<account>.dfs.core.windows.net/transactions')\n  .outputMode('append')\n  .start();\n```\n\nFollow-up Questions\n\n- How would you handle late-arriving data and schema drift in production?\n- What are the trade-offs between Unity Catalog vs Purview for lineage in this setup?\n- How would you monitor SLAs for latency and data freshness across regions?","diagram":null,"difficulty":"advanced","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Google","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T07:45:28.158Z","createdAt":"2026-01-16T07:45:28.159Z"},{"id":"q-940","question":"Daily CSV exports arrive in an ADLS Gen2 container at /incoming/sales/. Files may evolve over time as columns drift. Build a beginner pipeline using Data Factory to stage raw data, infer/handle schema changes, and load a simple star schema in Azure Synapse Analytics. Include a watermark-based incremental load and basic scheduling. Which services and steps would you implement?","answer":"Use Azure Data Factory: Copy to land raw CSVs as Parquet in ADLS Gen2, Data Flow with allowSchemaDrift to map evolving columns to a fixed star-schema in Azure Synapse, sink into a dedicated SQL pool. ","explanation":"## Why This Is Asked\nTests end-to-end basics: file landing, simple schema evolution, incremental loads, and reporting schema in Synapse.\n\n## Key Concepts\n- Data Factory pipelines: Copy, Data Flow, Triggers\n- ADLS Gen2 and Parquet landing\n- Synapse star schema design and incremental loads\n- Watermark-based change capture\n\n## Code Example\n```javascript\n// JSON-like skeleton of pipeline stages\n{\n  name: 'SalesIngest',\n  activities: [\n    {copy: 'landing/parquet'},\n    {dataFlow: 'map-to-star', drift: true},\n    {sink: 'Synapse.star'},\n    {trigger: 'tumblingWindow', interval: 1}\n  ]\n}\n```\n\n## Follow-up Questions\n- How would you handle late-arriving data?\n- How would you monitor data quality and failures?","diagram":"flowchart TD\n  A[CSV arrives] --> B[Copy to landing Parquet]\n  B --> C[Data Flow: map to star schema]\n  C --> D[Sink to Synapse star schema]\n  D --> E[Incremental load by watermark]\n  E --> F[Update metadata log]","difficulty":"beginner","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Amazon","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T16:30:40.009Z","createdAt":"2026-01-12T16:30:40.009Z"},{"id":"q-980","question":"Design a global streaming-to-batch data pipeline for a PayPal/Adobe/Netflix-scale analytics platform: ingest real-time clickstream from Event Hubs into ADLS Gen2, maintain a Delta Lake with drift-tolerant schema, mask PII at ingestion, expose masked aggregates via serverless SQL pool, and enforce end-to-end lineage with Purview while supporting cross-region DR and data residency. Which Azure components and patterns would you use, and how would you handle schema evolution and failure modes?","answer":"Ingest raw clickstream from Event Hubs into ADLS Gen2 staging, use Spark Structured Streaming to write to Delta Lake with drift-aware schema evolution, apply a masking UDF before storing in curated zo","explanation":"## Why This Is Asked\nThis probes practical data governance, real-time-to-batch pipelines, and DR considerations in Azure for large-scale workloads.\n\n## Key Concepts\n- Event Hubs ingestion and Spark Structured Streaming\n- Delta Lake with schema drift handling\n- PII masking at ingestion\n- Purview lineage and data residency compliance\n- Cross-region DR and serverless analytics\n\n## Code Example\n```javascript\n// PII masking pseudo-code\nfunction maskPII(record) {\n  if (!record?.email) return record;\n  record.email = record.email.replace(/[^@]+@/, '*****@');\n  return record;\n}\n```\n\n## Follow-up Questions\n- How would you validate end-to-end lineage across regions?\n- What are failure modes in cross-region DR and mitigations?","diagram":"flowchart TD\n  A[Event Hubs] --> B[ADLS Gen2 staging]\n  B --> C[Delta Lake curated]\n  C --> D[Serverless SQL pool dashboards]\n  A --> E[Purview lineage]\n  F[Geo-DR] --> G[Region failover]","difficulty":"intermediate","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Netflix","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T17:44:06.541Z","createdAt":"2026-01-12T17:44:06.541Z"}],"subChannels":["general"],"companies":["Adobe","Airbnb","Amazon","Apple","Bloomberg","Citadel","Cloudflare","Coinbase","Databricks","Discord","DoorDash","Goldman Sachs","Google","Hashicorp","IBM","LinkedIn","Lyft","Meta","Microsoft","MongoDB","NVIDIA","Netflix","OpenAI","Oracle","PayPal","Plaid","Robinhood","Scale Ai","Slack","Snap","Snowflake","Square","Tesla","Twitter","Two Sigma","Uber","Zoom"],"stats":{"total":24,"beginner":6,"intermediate":8,"advanced":10,"newThisWeek":24}}