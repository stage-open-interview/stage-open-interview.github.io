{"questions":[{"id":"azure-data-engineer-design-data-integration-1768203065598-0","question":"Scenario: You have an on-premises SQL Server as the source and Azure Data Lake Storage Gen2 as the sink. You need near real-time ingestion with about 5-minute latency and late-arriving data handling. Which approach in Azure Data Factory best meets these requirements?","answer":"[{\"id\":\"a\",\"text\":\"Schedule a Copy Activity every 5 minutes using a watermark column to load only new data\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Build a streaming pipeline with Event Hubs and Spark streaming to land raw events into ADLS\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Run a full table load every 5 minutes and overwrite the destination\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use a Mapping Data Flow to apply CDC logic directly on the source\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA. Schedule a Copy Activity every 5 minutes using a watermark column to load only new data.\n\n## Why Other Options Are Wrong\n- B: While streaming is powerful, Event Hubs/Spark streaming add complexity and cost for 5-minute batch-like latency, and Data Factory is not a native streaming sink for ADLS ingestion to this pattern.\n- C: Full table loads every 5 minutes are resource-intensive and violate near-real-time efficiency goals.\n- D: Mapping Data Flow does not inherently provide a simple, reliable incremental load pattern with late-arriving data handling for this scenario.\n\n## Key Concepts\n- Incremental loading with watermark columns\n- Copy Activity orchestration and time-based triggers\n- Late-arriving data handling in ingestion pipelines\n\n## Real-World Application\n- Enables near-real-time analytics by delta-ing only new/updated rows and scheduling regular runs, reducing source load and improving refresh cadence.","diagram":null,"difficulty":"intermediate","tags":["Azure Data Factory","Azure Data Lake Storage Gen2","Incremental load","Terraform","Kubernetes","certification-mcq","domain-weight-20"],"channel":"azure-data-engineer","subChannel":"design-data-integration","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T07:31:05.599Z","createdAt":"2026-01-12 07:31:06"},{"id":"azure-data-engineer-design-data-integration-1768203065598-1","question":"Scenario: In Azure Synapse Analytics, you need to load data from ADLS Gen2 into a dedicated SQL pool to populate a Slowly Changing Dimension Type 2 (SCD2) dimension table. What is the recommended approach?","answer":"[{\"id\":\"a\",\"text\":\"Use PolyBase to load data from ADLS into a staging table, then MERGE into the target with SCD2 logic\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Use a Data Flow in Synapse to perform SCD2 transformations directly in the dedicated SQL pool\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Load data into a staging table in the dedicated SQL pool and perform an UPDATE/INSERT in real time on the production table\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use SSIS integration with a linked server to handle SCD2 inside the SQL pool\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA. Use PolyBase to load data from ADLS into a staging table, then MERGE into the target with SCD2 logic.\n\n## Why Other Options Are Wrong\n- B: Data Flow in Synapse can implement SCD2 but is not the canonical pattern for dedicated SQL pool (which excels with PolyBase staging and MERGE for SCD2).\n- C: Real-time UPDATE/INSERT on the production SCD2 table is brittle and can lead to fragmentation and locking in a dedicated SQL pool.\n- D: SSIS on a linked server is not the recommended path for scalable SCD2 in Synapse today.\n\n## Key Concepts\n- PolyBase data loading into staging in dedicated SQL pool\n- MERGE-based SCD2 pattern\n- Staging vs production separation in Synapse\n\n## Real-World Application\n- Enables scalable, auditable history preservation for dimensions while leveraging the performance characteristics of a dedicated SQL pool.","diagram":null,"difficulty":"intermediate","tags":["Azure Synapse Analytics","PolyBase","MERGE","Terraform","Kubernetes","certification-mcq","domain-weight-20"],"channel":"azure-data-engineer","subChannel":"design-data-integration","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T07:31:06.157Z","createdAt":"2026-01-12 07:31:06"},{"id":"azure-data-engineer-design-data-integration-1768203065598-2","question":"Scenario: Your data integration design combines sources from on-prem databases and blob storage with a lakehouse target in Azure. You want to minimize compute costs while reusing established patterns. Which architecture pattern is most appropriate?","answer":"[{\"id\":\"a\",\"text\":\"Use Data Factory pipelines with Mapping Data Flows for all transformations\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use Data Factory to orchestrate pipelines, stage raw data in ADLS Gen2, and perform transformations in Synapse Serverless SQL pool with external tables and views\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Push all transforms to a self-managed Hadoop cluster and copy results to ADLS Gen2\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use a single ETL script in a VM with Python and schedule it with Windows Task Scheduler\",\"isCorrect\":false}]","explanation":"## Correct Answer\nB. Use Data Factory to orchestrate pipelines, stage raw data in ADLS Gen2, and perform transformations in Synapse Serverless SQL pool with external tables and views.\n\n## Why Other Options Are Wrong\n- A: Mapping Data Flows can transform data but may not provide the most cost-effective pattern when serverless SQL patterns can share compute across queries.\n- C: A self-managed Hadoop cluster introduces unnecessary maintenance and cost; not aligned with a lakehouse-centric, serverless approach.\n- D: A VM-based Python ETL is less scalable and harder to maintain for heterogeneous sources and lakehouse patterns.\n\n## Key Concepts\n- Data Factory orchestration with serverless transforms\n- Synapse Serverless SQL pool and external tables\n- Lakehouse architecture patterns\n\n## Real-World Application\n- Reduces compute costs by leveraging serverless processing for transforms while preserving familiar patterns and data lineage in the lakehouse.","diagram":null,"difficulty":"intermediate","tags":["Azure Data Factory","Azure Synapse Analytics","Azure Data Lake Storage Gen2","Terraform","Kubernetes","certification-mcq","domain-weight-20"],"channel":"azure-data-engineer","subChannel":"design-data-integration","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T07:31:06.700Z","createdAt":"2026-01-12 07:31:06"},{"id":"azure-data-engineer-design-data-processing-1768166200349-0","question":"You design a batch ELT pipeline that ingests large CSV files from an on-premises SFTP server into ADLS Gen2, applies schema drift tolerant transformations, and loads a star schema in Azure Synapse Analytics. You require end-to-end data lineage, automatic schema inference, and incremental loads. Which Azure components should you use together to meet these goals?","answer":"[{\"id\":\"a\",\"text\":\"Azure Data Factory pipelines + Azure Synapse Analytics + Azure Purview\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Azure Databricks + Azure Data Factory + Azure Data Lake Storage Gen2\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Azure Data Factory pipelines + Azure SQL Data Warehouse + Azure Purview\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Azure Stream Analytics + Azure Synapse Analytics + Azure Data Catalog\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A is correct because it combines data pipelines with scalable transform capabilities, a modern data warehouse in Synapse for serving data, and Purview for end-to-end data lineage. Mapping Data Flows in Data Factory provide drift-tolerant transformations and support incremental loads when designed with watermarking and upserts, while Purview captures lineage across the pipeline and storage layers.\n\n## Why Other Options Are Wrong\n- Option B lacks an explicit data governance/lineage component and relies on Databricks without confirming lineage integration. While Databricks is powerful, the combination misses the built-in lineage visibility provided by Purview in this scenario.\n- Option C uses Azure SQL Data Warehouse, which is an older term for Synapse; it misrepresents the recommended modern serving layer in this architecture.\n- Option D relies on Stream Analytics (real-time) and a data catalog rather than a holistic batch ELT design with end-to-end lineage; it does not optimally address schema drift handling and incremental batch loads.\n\n## Key Concepts\n- Data Factory Pipelines\n- Mapping Data Flows (schema drift tolerant)\n- Azure Synapse Analytics\n- Azure Purview (data lineage)\n\n## Real-World Application\n- Deploy a batch ELT that ingests, mutates, and upserts large CSV batches while maintaining lineage visibility for audits and compliance.","diagram":null,"difficulty":"intermediate","tags":["Azure","Azure Data Factory","Azure Synapse","Purview","Delta Lake","Databricks","AWS","Kubernetes","Terraform","certification-mcq","domain-weight-40"],"channel":"azure-data-engineer","subChannel":"design-data-processing","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T21:16:40.351Z","createdAt":"2026-01-11 21:16:40"},{"id":"azure-data-engineer-design-data-processing-1768166200349-1","question":"To process IoT telemetry in real-time with strict consistency, you compare Azure Stream Analytics and Databricks Structured Streaming with Delta Lake. The requirement is end-to-end exactly-once semantics and tolerance for late-arriving data across a multi-step pipeline. Which approach best satisfies these requirements?","answer":"[{\"id\":\"a\",\"text\":\"Azure Stream Analytics\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Azure Databricks Structured Streaming with Delta Lake upserts\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Azure Data Factory mapping data flow with deterministic transformations\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Azure Functions orchestrating incremental loads\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct because Databricks Structured Streaming with Delta Lake provides ACID transactions and upsert capabilities, enabling true end-to-end exactly-once semantics when writing to Delta Lake tables via merge/upsert operations. Delta Lake supports schema enforcement and time-travel, which helps with late-arriving data.\n\n## Why Other Options Are Wrong\n- Option A (Azure Stream Analytics) offers low-latency real-time processing but typically does not guarantee exactly-once semantics across complex multi-stage pipelines.\n- Option C (Data Factory mapping data flow) is powerful for batch/near-real-time ETL but does not inherently guarantee exactly-once semantics across streaming states.\n- Option D (Azure Functions) can orchestrate steps but does not provide built-in guarantees for exactly-once processing or ACID transactions in storage.\n\n## Key Concepts\n- Databricks Structured Streaming\n- Delta Lake ACID transactions\n- Exactly-once semantics in streaming pipelines\n- Handling late-arriving data\n\n## Real-World Application\n- Deploy a streaming pipeline for sensor data where deduplication and idempotent writes are critical for accurate dashboards and downstream analytics.","diagram":null,"difficulty":"intermediate","tags":["Azure","Azure Databricks","Delta Lake","Structured Streaming","AWS","Kubernetes","Terraform","certification-mcq","domain-weight-40"],"channel":"azure-data-engineer","subChannel":"design-data-processing","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T21:16:40.877Z","createdAt":"2026-01-11 21:16:41"},{"id":"azure-data-engineer-design-data-processing-1768166200349-2","question":"You store raw data in ADLS Gen2 and need automated archival and deletion based on retention policies while ensuring easy auditability and compliant access controls. Which approach best implements data lifecycle and governance requirements?","answer":"[{\"id\":\"a\",\"text\":\"Configure lifecycle management policies on the ADLS Gen2 container with tier transitions to Archive and auto-delete\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Create a nightly Data Factory job to delete old files\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Store copies in a separate object store and delete old ones manually\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use Purview retention policies to automatically purge data\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A is correct because ADLS Gen2 lifecycle policies can automatically move data to Archive tier and delete it after the configured retention period, providing cost-effective storage and auditable data lifecycle without custom scripting.\n\n## Why Other Options Are Wrong\n- Option B relies on manually scheduled deletes, which is error-prone and harder to audit.\n- Option C moves data externally and relies on manual deletion, increasing complexity and risk of data loss.\n- Option D Purview provides governance and metadata about retention policies but does not automatically purge or tier data in ADLS Gen2; lifecycle policies are the actual mechanism for automatic data movement and deletion.\n\n## Key Concepts\n- ADLS Gen2 lifecycle management\n- Archive tier and automatic deletion\n- Data governance and auditability\n\n## Real-World Application\n- Implement automatic cost-optimized retention for raw data while keeping an auditable trail for compliance reviews.","diagram":null,"difficulty":"intermediate","tags":["Azure","ADLS Gen2","Lifecycle Management","Azure Purview","Terraform","AWS","Kubernetes","certification-mcq","domain-weight-40"],"channel":"azure-data-engineer","subChannel":"design-data-processing","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T21:16:41.395Z","createdAt":"2026-01-11 21:16:41"},{"id":"azure-data-engineer-design-data-storage-1768225521938-0","question":"You are designing a data lake architecture on Azure for analytics. You require a storage solution with hierarchical namespace and POSIX-style access control lists, and compatibility with Hadoop/Spark workloads. Which storage option best meets these requirements?","answer":"[{\"id\":\"a\",\"text\":\"Azure Data Lake Storage Gen2\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Azure Blob Storage\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Azure Data Lake Storage Gen1\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Azure SQL Database\",\"isCorrect\":false}]","explanation":"## Correct Answer\nAzure Data Lake Storage Gen2 provides a hierarchical namespace and POSIX-style access control lists, and is designed for Hadoop/Spark workloads.\n\n## Why Other Options Are Wrong\n- Azure Blob Storage does not support hierarchical namespaces or POSIX ACLs, so it cannot provide the required lakehouse features.\n- Data Lake Storage Gen1 is an older generation; Gen2 is the current recommended platform with better integration.\n- Azure SQL Database is a relational storage service and not a storage option for data lakes.\n\n## Key Concepts\n- Azure Data Lake Storage Gen2 features: hierarchical namespace, POSIX ACLs, optimized for big data analytics\n- Data lake design considerations: compatibility with Hadoop/Spark, scalable analytics storage\n\n## Real-World Application\nIn a real project, you would deploy ADLS Gen2 for raw landing zones and transformed data, grant data engineers fine-grained ACLs, and connect your Spark or Hive jobs to the same storage to minimize data movement.","diagram":null,"difficulty":"intermediate","tags":["Azure","AzureDataLakeStorageGen2","S3","Kubernetes","Terraform","certification-mcq","domain-weight-15"],"channel":"azure-data-engineer","subChannel":"design-data-storage","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:45:21.939Z","createdAt":"2026-01-12 13:45:22"},{"id":"azure-data-engineer-design-data-storage-1768225521938-1","question":"As part of a lakehouse pattern, you need to run on-demand SQL queries over data stored in Azure Data Lake Storage Gen2 without provisioning dedicated clusters. Which configuration provides cost-effective, scalable query over the data?","answer":"[{\"id\":\"a\",\"text\":\"Databricks SQL Analytics on Databricks clusters\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Azure Synapse Analytics serverless SQL pool with external tables over ADLS Gen2\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Azure Data Factory Data Flow\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Azure SQL Database\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption b is correct because Azure Synapse serverless SQL pool enables on-demand queries over data in ADLS Gen2 without managing compute clusters.\n\n## Why Other Options Are Wrong\n- Databricks SQL Analytics on Databricks clusters can query ADLS Gen2 but relies on a managed cluster; it is not the serverless on-demand option emphasized for lakehouse patterns in Synapse.\n- Data Factory Data Flow is for data transformation, not primarily for ad-hoc lakehouse queries over data lakes.\n- Azure SQL Database is a relational engine not optimized for directly querying large unstructured data stored in ADLS Gen2 in lakehouse fashion.\n\n## Key Concepts\n- Serverless SQL pool in Synapse provides on-demand querying of external data in ADLS Gen2\n- External tables and data lakehouse patterns enable scalable analytics without heavy compute provisioning\n\n## Real-World Application\nTeams use Synapse serverless SQL to perform BI and data science queries directly against lakehouse data in ADLS Gen2, reducing costs and simplifying governance.","diagram":null,"difficulty":"intermediate","tags":["Azure","AzureSynapseAnalytics","ADLSGen2","Databricks","Kubernetes","Terraform","AWS_S3","certification-mcq","domain-weight-15"],"channel":"azure-data-engineer","subChannel":"design-data-storage","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:45:22.644Z","createdAt":"2026-01-12 13:45:23"},{"id":"azure-data-engineer-design-data-storage-1768225521938-2","question":"To meet compliance requiring encryption keys to be rotated and managed centrally, you want to use customer-managed keys with a centralized KMS. Which approach ensures consistent encryption across ADLS Gen2 and Synapse?","answer":"[{\"id\":\"a\",\"text\":\"Store keys per service in application configuration\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use Customer-Managed Keys in Azure Key Vault with BYOK for both ADLS Gen2 and Synapse\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Rely on Microsoft-managed keys and rotate them manually per service\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use on-premises HSM and periodically import keys\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption b is correct because BYOK with a centralized Key Vault enables unified key management and rotation across multiple Azure services like ADLS Gen2 and Synapse.\n\n## Why Other Options Are Wrong\n- Storing keys per service in app config fragments key management and introduces drift and inconsistent rotation.\n- Microsoft-managed keys do not meet BYOK or centralized rotation requirements across services.\n- On-prem HSMs add complexity and latency for cloud-native services and reduce cloud agility.\n\n## Key Concepts\n- Customer-Managed Keys (BYOK)\n- Centralized key management via Azure Key Vault\n\n## Real-World Application\nImplement a single Key Vault with a defined rotation policy and managed identities for ADLS Gen2 and Synapse to reference the same keys, ensuring consistent rotation schedules and improved compliance posture.","diagram":null,"difficulty":"intermediate","tags":["Azure","AzureKeyVault","BYOK","Terraform","AWS_S3","certification-mcq","domain-weight-15"],"channel":"azure-data-engineer","subChannel":"design-data-storage","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:45:23.162Z","createdAt":"2026-01-12 13:45:23"},{"id":"azure-data-engineer-design-data-storage-1768225521938-3","question":"You need cross-region data availability for analytics workloads; which option provides cross-region replication with read access to the secondary region?","answer":"[{\"id\":\"a\",\"text\":\"LRS\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"ZRS\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"GRS\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"RA-GRS\",\"isCorrect\":true}]","explanation":"## Correct Answer\nOption d is correct because Read-Access Geo-Redundant Storage (RA-GRS) provides geo-redundant replication with read access to the secondary region, enabling cross-region read availability for analytics.\n\n## Why Other Options Are Wrong\n- LRS offers only local redundancy with no cross-region replication.\n- ZRS provides zone redundancy within a single region, not cross-region.\n- GRS replicates to a paired region but does not provide read access to the secondary by default.\n\n## Key Concepts\n- GRS vs RA-GRS differences\n- Cross-region replication and read access patterns for analytics\n\n## Real-World Application\nIn a global analytics deployment, RA-GRS ensures analysts can read data from a nearby secondary region during outages or latency spikes, improving resilience and performance.","diagram":null,"difficulty":"intermediate","tags":["Azure","AzureStorage","GRS","RA-GRS","AWS_S3","certification-mcq","domain-weight-15"],"channel":"azure-data-engineer","subChannel":"design-data-storage","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:45:23.341Z","createdAt":"2026-01-12 13:45:23"},{"id":"azure-data-engineer-design-data-storage-1768225521938-4","question":"To enable point-in-time restore for data stored in ADLS Gen2, you want to retain previous versions of objects. What is the recommended approach?","answer":"[{\"id\":\"a\",\"text\":\"Enable blob versioning on the storage account backing ADLS Gen2\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Enable change feed\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Rely on soft delete for blobs\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use planned manual snapshots on each file\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption a is correct because blob versioning stores previous versions of objects, enabling point-in-time restore for ADLS Gen2 data.\n\n## Why Other Options Are Wrong\n- Change feed tracks changes but does not provide per-object versioning suitable for point-in-time restores.\n- Soft delete protects against accidental deletions but does not preserve historical object versions.\n- Manual snapshots are error-prone and require ongoing user action; built-in versioning provides automatic retention of versions.\n\n## Key Concepts\n- Blob versioning for ADLS Gen2\n- Point-in-time restore capabilities in data lakes\n\n## Real-World Application\nWith versioning enabled, data engineers can restore a previous version of a blob after a faulty ingestion, reducing downtime and data-loss risk.","diagram":null,"difficulty":"intermediate","tags":["Azure","AzureBlobStorage","ADLSGen2","AWS_S3","certification-mcq","domain-weight-15"],"channel":"azure-data-engineer","subChannel":"design-data-storage","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:45:23.521Z","createdAt":"2026-01-12 13:45:23"}],"subChannels":["design-data-integration","design-data-processing","design-data-storage"],"companies":[],"stats":{"total":11,"beginner":0,"intermediate":11,"advanced":0,"newThisWeek":11}}