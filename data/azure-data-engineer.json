{"questions":[{"id":"q-1029","question":"In a global IoT telemetry pipeline ingesting 100k events/s per region into ADLS Gen2 and Databricks Delta Lake, implement schema drift tolerant ingestion, end-to-end data lineage via Purview, RBAC, and cross-region DR replication. How would you architect the data contracts, partitioning, watermarking, retention, and governance to meet data residency and fault-tolerance requirements?","answer":"Architect a multi-region lakehouse: per-region ADLS Gen2, Databricks Delta Lake with mergeSchema and cloudFiles.inferColumnTypes, partition by region and device_type, watermark-based streaming, and Pu","explanation":"## Why This Is Asked\nAssesses multi-region lakehouse design, schema drift handling, data governance, and disaster recovery in Azure.\n\n## Key Concepts\n- Schema drift tolerant ingestion (Delta Lake, mergeSchema)\n- Data lineage (Purview) and RBAC (Unity Catalog)\n- Cross-region replication and data residency\n- Partitioning strategy and watermarking for IoT streams\n- Data contracts, retention, and failure handling\n\n## Code Example\n```python\nfrom pyspark.sql import functions as F\n\n# Ingest with auto schema, write to Delta with evolution\ndf = (\n  spark.readStream\n     .format(\"cloudFiles\")\n     .option(\"cloudFiles.format\",\"json\")\n     .option(\"cloudFiles.inferColumnTypes\",\"true\")\n     .load(\"/mnt/iot/raw/\")\n)\n\nquery = df.writeStream \\\n  .format(\"delta\") \\\n  .option(\"checkpointLocation\",\"/mnt/checkpoints/iot/stream\") \\\n  .option(\"mergeSchema\",\"true\") \\\n  .partitionBy(\"region\",\"device_type\") \\\n  .start(\"/mnt/delta/iot/region/\")\n```\n\n## Follow-up Questions\n- How would you validate cross-region data residency and schema compatibility across regions?\n- What testing strategy ensures Purview lineage remains intact after DR failover?","diagram":"flowchart TD\n  A[IoT Devices] --> B[Event Hub / IoT Hub]\n  B --> C[ADLS Gen2 Region A Raw]\n  C --> D[Databricks Delta Lake Region A]\n  D --> E[Purview Lineage]\n  D --> F[Unity Catalog RBAC]\n  D --> G[Cross-Region Replication to Region B]\n  G --> H[DR ADLS Gen2 Region B]","difficulty":"advanced","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Twitter","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T19:42:48.929Z","createdAt":"2026-01-12T19:42:48.929Z"},{"id":"q-1043","question":"Design an end-to-end Azure data-ecosystem pipeline that ingests incremental changes from a MongoDB Atlas collection into Delta Lake on ADLS Gen2, preserving SCD Type 2 history for a customers dimension, and handling schema drift. Include data lineage to Azure Purview, late-arriving updates, and on-demand reprocessing without data loss. Which components and config would you choose, and why?","answer":"Use MongoDB Atlas Change Streams with Debezium to stream inserts/updates into Kafka, then Spark Structured Streaming consumes from Kafka and writes to Delta Lake on ADLS Gen2 with Delta schema evoluti","explanation":"## Why This Is Asked\nThis question tests practical design decisions for real-time CDC pipelines across Azure, including lineage, schema drift, and reprocessing.\n\n## Key Concepts\n- MongoDB Atlas Change Streams and Debezium for CDC\n- Delta Lake schema evolution and SCD Type 2\n- Spark Structured Streaming from Kafka to Delta Lake\n- Azure Purview for end-to-end lineage\n- Late-arriving data handling and replay strategies\n\n## Code Example\n```javascript\n// Pseudo-implementation illustrating flow (language-agnostic)\nconst inStream = Kafka.read(\"mongodb-change-stream\");\nDelta.write(inStream, \"/mnt/adls/delta/customers\");\n```\n\n## Follow-up Questions\n- How would you implement schema drift handling in Delta Lake?\n- How do you validate and monitor Purview lineage end-to-end?\n- What are failure modes and how would you recover from late data or CDC gaps?\n","diagram":null,"difficulty":"intermediate","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","NVIDIA","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T20:30:13.903Z","createdAt":"2026-01-12T20:30:13.903Z"},{"id":"q-1311","question":"Design an end-to-end incremental ELT pipeline on Azure that ingests 2 TB/day of nested JSON telemetry from Event Hubs into a Delta Lake on ADLS Gen2, then loads a star schema in Azure Synapse. Requirements: (1) schema drift tolerant writes and schema evolution, (2) upserts by a composite key (tenant_id, event_id), (3) end-to-end data lineage to Purview, (4) late-arriving data support via watermarking, (5) partitioning by region and day with file-size/compaction strategies. Which components and patterns would you use, and why? Compare Delta Lake MERGE vs Delete+Insert for upserts?","answer":"Use Event Hubs → Databricks Structured Streaming → Delta Lake on ADLS Gen2 with schema evolution enabled. Upsert into a Delta table on (tenant_id, event_id) using MERGE; apply a 5-minute watermark for","explanation":"## Why This Is Asked\nTests practical ability to architect an Azure data pipeline handling schema drift, incremental loads, and governance across a hybrid stack. It also probes trade-offs between MERGE and Delete+Insert for upserts and how to manage late data and file sizing in a real-world volume.\n\n## Key Concepts\n- Event Hubs, Databricks Structured Streaming, Delta Lake on ADLS Gen2\n- Schema evolution and drift tolerance for nested JSON\n- Upserts via MERGE vs Delete+Insert\n- Watermarking for late data, partitioning strategy\n- Data lineage with Purview, serving layer in Azure Synapse\n\n## Code Example\n```python\n# PySpark sketch for streaming write with MERGE via foreachBatch\ndef upsert_to_delta(microBatchDF, batchId):\n    microBatchDF.createOrReplaceTempView('updates')\n    spark.sql(\"\"\"\n      MERGE INTO delta_table AS t\n      USING updates AS s\n      ON t.tenant_id = s.tenant_id AND t.event_id = s.event_id\n      WHEN MATCHED THEN UPDATE SET *\n      WHEN NOT MATCHED THEN INSERT *\n    \"\"\")\n\nstreamingDF.writeStream.foreachBatch(upsert_to_delta).option('checkpointLocation','/chkpt/path').start('/delta/path')\n```\n\n## Follow-up Questions\n- How would you tune for high cardinality keys without hotspotting?\n- How do you validate lineage in Purview across pipelines and schemas?","diagram":null,"difficulty":"intermediate","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Google","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T10:37:25.505Z","createdAt":"2026-01-13T10:37:25.505Z"},{"id":"q-1326","question":"Design an end-to-end pipeline that streams user activity from Azure Event Hubs into Delta Lake on ADLS Gen2, supports schema evolution, and updates an SCD Type 2 customer dimension in Synapse. Include exactly-once guarantees, late-arriving data handling, idempotent sinks, and end-to-end data lineage via Purview in a hybrid estate. List components, data flows, and governance approach?","answer":"Use Spark Structured Streaming on Azure Databricks to read Event Hubs, write to Delta Lake on ADLS Gen2 with auto schema evolution. Apply MERGE INTO to a Type 2 customer dimension in Synapse for upser","explanation":"## Why This Is Asked\nTests production-grade streaming & lakehouse design: schema evolution, upserts, lineage, and hybrid governance.\n\n## Key Concepts\n- Delta Lake schema evolution on ADLS Gen2\n- Structured Streaming with exactly-once guarantees\n- MERGE INTO for SCD Type 2 in Synapse\n- Purview data lineage across on-prem and cloud\n- Hybrid lakehouse governance and security\n\n## Code Example\n```python\n# PySpark pseudo\nfrom delta.tables import DeltaTable\nsource_df = spark.readStream.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"...\").load()[...]\ndelta_path = \"/mnt/delta/customers\"\ndelta_table = DeltaTable.forPath(spark, delta_path)\n# streaming merge example (simplified)\ndelta_table.alias(\"t\").merge(\n  source_df.alias(\"s\"),\n  \"t.customer_id = s.customer_id\"\n).whenMatchedUpdate(set={\"name\":\"s.name\",\"address\":\"s.address\",\"end_date\":\"current_timestamp()\"})\n .whenNotMatchedInsert(values={\"customer_id\":\"s.customer_id\",\"name\":\"s.name\",\"start_date\":\"current_timestamp()\"})\n .execute()\n```\n\n## Follow-up Questions\n- How would you handle late-arriving events with out-of-order progress?\n- What changes would you make to support global data cataloging and cross-region replication?","diagram":"flowchart TD\n  A[Event Hubs] --> B[Databricks Spark Structured Streaming]\n  B --> C[Delta Lake on ADLS Gen2]\n  C --> D[Synapse SCD Type 2]\n  D --> E[Purview Lineage]\n  E --> F[Hybrid Serving Layer]","difficulty":"advanced","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T11:36:41.027Z","createdAt":"2026-01-13T11:36:41.028Z"},{"id":"q-940","question":"Daily CSV exports arrive in an ADLS Gen2 container at /incoming/sales/. Files may evolve over time as columns drift. Build a beginner pipeline using Data Factory to stage raw data, infer/handle schema changes, and load a simple star schema in Azure Synapse Analytics. Include a watermark-based incremental load and basic scheduling. Which services and steps would you implement?","answer":"Use Azure Data Factory: Copy to land raw CSVs as Parquet in ADLS Gen2, Data Flow with allowSchemaDrift to map evolving columns to a fixed star-schema in Azure Synapse, sink into a dedicated SQL pool. ","explanation":"## Why This Is Asked\nTests end-to-end basics: file landing, simple schema evolution, incremental loads, and reporting schema in Synapse.\n\n## Key Concepts\n- Data Factory pipelines: Copy, Data Flow, Triggers\n- ADLS Gen2 and Parquet landing\n- Synapse star schema design and incremental loads\n- Watermark-based change capture\n\n## Code Example\n```javascript\n// JSON-like skeleton of pipeline stages\n{\n  name: 'SalesIngest',\n  activities: [\n    {copy: 'landing/parquet'},\n    {dataFlow: 'map-to-star', drift: true},\n    {sink: 'Synapse.star'},\n    {trigger: 'tumblingWindow', interval: 1}\n  ]\n}\n```\n\n## Follow-up Questions\n- How would you handle late-arriving data?\n- How would you monitor data quality and failures?","diagram":"flowchart TD\n  A[CSV arrives] --> B[Copy to landing Parquet]\n  B --> C[Data Flow: map to star schema]\n  C --> D[Sink to Synapse star schema]\n  D --> E[Incremental load by watermark]\n  E --> F[Update metadata log]","difficulty":"beginner","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Amazon","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T16:30:40.009Z","createdAt":"2026-01-12T16:30:40.009Z"},{"id":"q-980","question":"Design a global streaming-to-batch data pipeline for a PayPal/Adobe/Netflix-scale analytics platform: ingest real-time clickstream from Event Hubs into ADLS Gen2, maintain a Delta Lake with drift-tolerant schema, mask PII at ingestion, expose masked aggregates via serverless SQL pool, and enforce end-to-end lineage with Purview while supporting cross-region DR and data residency. Which Azure components and patterns would you use, and how would you handle schema evolution and failure modes?","answer":"Ingest raw clickstream from Event Hubs into ADLS Gen2 staging, use Spark Structured Streaming to write to Delta Lake with drift-aware schema evolution, apply a masking UDF before storing in curated zo","explanation":"## Why This Is Asked\nThis probes practical data governance, real-time-to-batch pipelines, and DR considerations in Azure for large-scale workloads.\n\n## Key Concepts\n- Event Hubs ingestion and Spark Structured Streaming\n- Delta Lake with schema drift handling\n- PII masking at ingestion\n- Purview lineage and data residency compliance\n- Cross-region DR and serverless analytics\n\n## Code Example\n```javascript\n// PII masking pseudo-code\nfunction maskPII(record) {\n  if (!record?.email) return record;\n  record.email = record.email.replace(/[^@]+@/, '*****@');\n  return record;\n}\n```\n\n## Follow-up Questions\n- How would you validate end-to-end lineage across regions?\n- What are failure modes in cross-region DR and mitigations?","diagram":"flowchart TD\n  A[Event Hubs] --> B[ADLS Gen2 staging]\n  B --> C[Delta Lake curated]\n  C --> D[Serverless SQL pool dashboards]\n  A --> E[Purview lineage]\n  F[Geo-DR] --> G[Region failover]","difficulty":"intermediate","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Netflix","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T17:44:06.541Z","createdAt":"2026-01-12T17:44:06.541Z"}],"subChannels":["general"],"companies":["Adobe","Amazon","Bloomberg","Cloudflare","Google","IBM","MongoDB","NVIDIA","Netflix","PayPal","Robinhood","Snap","Snowflake","Twitter","Uber","Zoom"],"stats":{"total":6,"beginner":1,"intermediate":3,"advanced":2,"newThisWeek":6}}