{"questions":[{"id":"azure-data-engineer-design-data-integration-1768203065598-0","question":"Scenario: You have an on-premises SQL Server as the source and Azure Data Lake Storage Gen2 as the sink. You need near real-time ingestion with about 5-minute latency and late-arriving data handling. Which approach in Azure Data Factory best meets these requirements?","answer":"[{\"id\":\"a\",\"text\":\"Schedule a Copy Activity every 5 minutes using a watermark column to load only new data\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Build a streaming pipeline with Event Hubs and Spark streaming to land raw events into ADLS\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Run a full table load every 5 minutes and overwrite the destination\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use a Mapping Data Flow to apply CDC logic directly on the source\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA. Schedule a Copy Activity every 5 minutes using a watermark column to load only new data.\n\n## Why Other Options Are Wrong\n- B: While streaming is powerful, Event Hubs/Spark streaming add complexity and cost for 5-minute batch-like latency, and Data Factory is not a native streaming sink for ADLS ingestion to this pattern.\n- C: Full table loads every 5 minutes are resource-intensive and violate near-real-time efficiency goals.\n- D: Mapping Data Flow does not inherently provide a simple, reliable incremental load pattern with late-arriving data handling for this scenario.\n\n## Key Concepts\n- Incremental loading with watermark columns\n- Copy Activity orchestration and time-based triggers\n- Late-arriving data handling in ingestion pipelines\n\n## Real-World Application\n- Enables near-real-time analytics by delta-ing only new/updated rows and scheduling regular runs, reducing source load and improving refresh cadence.","diagram":null,"difficulty":"intermediate","tags":["Azure Data Factory","Azure Data Lake Storage Gen2","Incremental load","Terraform","Kubernetes","certification-mcq","domain-weight-20"],"channel":"azure-data-engineer","subChannel":"design-data-integration","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T07:31:05.599Z","createdAt":"2026-01-12 07:31:06"},{"id":"azure-data-engineer-design-data-integration-1768203065598-1","question":"Scenario: In Azure Synapse Analytics, you need to load data from ADLS Gen2 into a dedicated SQL pool to populate a Slowly Changing Dimension Type 2 (SCD2) dimension table. What is the recommended approach?","answer":"[{\"id\":\"a\",\"text\":\"Use PolyBase to load data from ADLS into a staging table, then MERGE into the target with SCD2 logic\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Use a Data Flow in Synapse to perform SCD2 transformations directly in the dedicated SQL pool\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Load data into a staging table in the dedicated SQL pool and perform an UPDATE/INSERT in real time on the production table\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use SSIS integration with a linked server to handle SCD2 inside the SQL pool\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA. Use PolyBase to load data from ADLS into a staging table, then MERGE into the target with SCD2 logic.\n\n## Why Other Options Are Wrong\n- B: Data Flow in Synapse can implement SCD2 but is not the canonical pattern for dedicated SQL pool (which excels with PolyBase staging and MERGE for SCD2).\n- C: Real-time UPDATE/INSERT on the production SCD2 table is brittle and can lead to fragmentation and locking in a dedicated SQL pool.\n- D: SSIS on a linked server is not the recommended path for scalable SCD2 in Synapse today.\n\n## Key Concepts\n- PolyBase data loading into staging in dedicated SQL pool\n- MERGE-based SCD2 pattern\n- Staging vs production separation in Synapse\n\n## Real-World Application\n- Enables scalable, auditable history preservation for dimensions while leveraging the performance characteristics of a dedicated SQL pool.","diagram":null,"difficulty":"intermediate","tags":["Azure Synapse Analytics","PolyBase","MERGE","Terraform","Kubernetes","certification-mcq","domain-weight-20"],"channel":"azure-data-engineer","subChannel":"design-data-integration","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T07:31:06.157Z","createdAt":"2026-01-12 07:31:06"},{"id":"azure-data-engineer-design-data-integration-1768203065598-2","question":"Scenario: Your data integration design combines sources from on-prem databases and blob storage with a lakehouse target in Azure. You want to minimize compute costs while reusing established patterns. Which architecture pattern is most appropriate?","answer":"[{\"id\":\"a\",\"text\":\"Use Data Factory pipelines with Mapping Data Flows for all transformations\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use Data Factory to orchestrate pipelines, stage raw data in ADLS Gen2, and perform transformations in Synapse Serverless SQL pool with external tables and views\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Push all transforms to a self-managed Hadoop cluster and copy results to ADLS Gen2\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use a single ETL script in a VM with Python and schedule it with Windows Task Scheduler\",\"isCorrect\":false}]","explanation":"## Correct Answer\nB. Use Data Factory to orchestrate pipelines, stage raw data in ADLS Gen2, and perform transformations in Synapse Serverless SQL pool with external tables and views.\n\n## Why Other Options Are Wrong\n- A: Mapping Data Flows can transform data but may not provide the most cost-effective pattern when serverless SQL patterns can share compute across queries.\n- C: A self-managed Hadoop cluster introduces unnecessary maintenance and cost; not aligned with a lakehouse-centric, serverless approach.\n- D: A VM-based Python ETL is less scalable and harder to maintain for heterogeneous sources and lakehouse patterns.\n\n## Key Concepts\n- Data Factory orchestration with serverless transforms\n- Synapse Serverless SQL pool and external tables\n- Lakehouse architecture patterns\n\n## Real-World Application\n- Reduces compute costs by leveraging serverless processing for transforms while preserving familiar patterns and data lineage in the lakehouse.","diagram":null,"difficulty":"intermediate","tags":["Azure Data Factory","Azure Synapse Analytics","Azure Data Lake Storage Gen2","Terraform","Kubernetes","certification-mcq","domain-weight-20"],"channel":"azure-data-engineer","subChannel":"design-data-integration","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T07:31:06.700Z","createdAt":"2026-01-12 07:31:06"},{"id":"azure-data-engineer-design-data-processing-1768166200349-0","question":"You design a batch ELT pipeline that ingests large CSV files from an on-premises SFTP server into ADLS Gen2, applies schema drift tolerant transformations, and loads a star schema in Azure Synapse Analytics. You require end-to-end data lineage, automatic schema inference, and incremental loads. Which Azure components should you use together to meet these goals?","answer":"[{\"id\":\"a\",\"text\":\"Azure Data Factory pipelines + Azure Synapse Analytics + Azure Purview\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Azure Databricks + Azure Data Factory + Azure Data Lake Storage Gen2\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Azure Data Factory pipelines + Azure SQL Data Warehouse + Azure Purview\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Azure Stream Analytics + Azure Synapse Analytics + Azure Data Catalog\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A is correct because it combines data pipelines with scalable transform capabilities, a modern data warehouse in Synapse for serving data, and Purview for end-to-end data lineage. Mapping Data Flows in Data Factory provide drift-tolerant transformations and support incremental loads when designed with watermarking and upserts, while Purview captures lineage across the pipeline and storage layers.\n\n## Why Other Options Are Wrong\n- Option B lacks an explicit data governance/lineage component and relies on Databricks without confirming lineage integration. While Databricks is powerful, the combination misses the built-in lineage visibility provided by Purview in this scenario.\n- Option C uses Azure SQL Data Warehouse, which is an older term for Synapse; it misrepresents the recommended modern serving layer in this architecture.\n- Option D relies on Stream Analytics (real-time) and a data catalog rather than a holistic batch ELT design with end-to-end lineage; it does not optimally address schema drift handling and incremental batch loads.\n\n## Key Concepts\n- Data Factory Pipelines\n- Mapping Data Flows (schema drift tolerant)\n- Azure Synapse Analytics\n- Azure Purview (data lineage)\n\n## Real-World Application\n- Deploy a batch ELT that ingests, mutates, and upserts large CSV batches while maintaining lineage visibility for audits and compliance.","diagram":null,"difficulty":"intermediate","tags":["Azure","Azure Data Factory","Azure Synapse","Purview","Delta Lake","Databricks","AWS","Kubernetes","Terraform","certification-mcq","domain-weight-40"],"channel":"azure-data-engineer","subChannel":"design-data-processing","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T21:16:40.351Z","createdAt":"2026-01-11 21:16:40"},{"id":"azure-data-engineer-design-data-processing-1768166200349-1","question":"To process IoT telemetry in real-time with strict consistency, you compare Azure Stream Analytics and Databricks Structured Streaming with Delta Lake. The requirement is end-to-end exactly-once semantics and tolerance for late-arriving data across a multi-step pipeline. Which approach best satisfies these requirements?","answer":"[{\"id\":\"a\",\"text\":\"Azure Stream Analytics\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Azure Databricks Structured Streaming with Delta Lake upserts\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Azure Data Factory mapping data flow with deterministic transformations\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Azure Functions orchestrating incremental loads\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct because Databricks Structured Streaming with Delta Lake provides ACID transactions and upsert capabilities, enabling true end-to-end exactly-once semantics when writing to Delta Lake tables via merge/upsert operations. Delta Lake supports schema enforcement and time-travel, which helps with late-arriving data.\n\n## Why Other Options Are Wrong\n- Option A (Azure Stream Analytics) offers low-latency real-time processing but typically does not guarantee exactly-once semantics across complex multi-stage pipelines.\n- Option C (Data Factory mapping data flow) is powerful for batch/near-real-time ETL but does not inherently guarantee exactly-once semantics across streaming states.\n- Option D (Azure Functions) can orchestrate steps but does not provide built-in guarantees for exactly-once processing or ACID transactions in storage.\n\n## Key Concepts\n- Databricks Structured Streaming\n- Delta Lake ACID transactions\n- Exactly-once semantics in streaming pipelines\n- Handling late-arriving data\n\n## Real-World Application\n- Deploy a streaming pipeline for sensor data where deduplication and idempotent writes are critical for accurate dashboards and downstream analytics.","diagram":null,"difficulty":"intermediate","tags":["Azure","Azure Databricks","Delta Lake","Structured Streaming","AWS","Kubernetes","Terraform","certification-mcq","domain-weight-40"],"channel":"azure-data-engineer","subChannel":"design-data-processing","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T21:16:40.877Z","createdAt":"2026-01-11 21:16:41"},{"id":"azure-data-engineer-design-data-processing-1768166200349-2","question":"You store raw data in ADLS Gen2 and need automated archival and deletion based on retention policies while ensuring easy auditability and compliant access controls. Which approach best implements data lifecycle and governance requirements?","answer":"[{\"id\":\"a\",\"text\":\"Configure lifecycle management policies on the ADLS Gen2 container with tier transitions to Archive and auto-delete\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Create a nightly Data Factory job to delete old files\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Store copies in a separate object store and delete old ones manually\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use Purview retention policies to automatically purge data\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A is correct because ADLS Gen2 lifecycle policies can automatically move data to Archive tier and delete it after the configured retention period, providing cost-effective storage and auditable data lifecycle without custom scripting.\n\n## Why Other Options Are Wrong\n- Option B relies on manually scheduled deletes, which is error-prone and harder to audit.\n- Option C moves data externally and relies on manual deletion, increasing complexity and risk of data loss.\n- Option D Purview provides governance and metadata about retention policies but does not automatically purge or tier data in ADLS Gen2; lifecycle policies are the actual mechanism for automatic data movement and deletion.\n\n## Key Concepts\n- ADLS Gen2 lifecycle management\n- Archive tier and automatic deletion\n- Data governance and auditability\n\n## Real-World Application\n- Implement automatic cost-optimized retention for raw data while keeping an auditable trail for compliance reviews.","diagram":null,"difficulty":"intermediate","tags":["Azure","ADLS Gen2","Lifecycle Management","Azure Purview","Terraform","AWS","Kubernetes","certification-mcq","domain-weight-40"],"channel":"azure-data-engineer","subChannel":"design-data-processing","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T21:16:41.395Z","createdAt":"2026-01-11 21:16:41"}],"subChannels":["design-data-integration","design-data-processing"],"companies":[],"stats":{"total":6,"beginner":0,"intermediate":6,"advanced":0,"newThisWeek":6}}