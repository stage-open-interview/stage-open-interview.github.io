{"questions":[{"id":"q-1029","question":"In a global IoT telemetry pipeline ingesting 100k events/s per region into ADLS Gen2 and Databricks Delta Lake, implement schema drift tolerant ingestion, end-to-end data lineage via Purview, RBAC, and cross-region DR replication. How would you architect the data contracts, partitioning, watermarking, retention, and governance to meet data residency and fault-tolerance requirements?","answer":"Architect a multi-region lakehouse: per-region ADLS Gen2 containers, Databricks Delta Lake with mergeSchema and cloudFiles.inferColumnTypes for schema evolution, partition by region/device_type/event_date for optimized queries, watermark-based streaming with exactly-once semantics, Unity Catalog for RBAC with fine-grained permissions, Purview for end-to-end lineage, and Azure Data Factory for cross-region DR replication. Implement data contracts through Delta Lake constraints and schema enforcement, retention policies via Delta Lake VACUUM and ADLS lifecycle management, and monitoring through Log Analytics and Azure Monitor alerts.","explanation":"## Why This Is Asked\nAssesses multi-region lakehouse design, schema drift handling, data governance, and disaster recovery in Azure at enterprise scale.\n\n## Key Concepts\n- Schema drift tolerant ingestion (Delta Lake mergeSchema, cloudFiles.inferColumnTypes)\n- Data lineage (Purview) and RBAC (Unity Catalog)\n- Cross-region replication and data residency\n- Partitioning strategy and watermarking for IoT streams\n- Data contracts, retention, and failure handling\n\n## Implementation Details\n```python\n# Auto schema evolution with Delta Lake\ndf = spark.readStream.format('cloudFiles') \\\n  .option('cloudFiles.format', 'json') \\\n  .option('cloudFiles.inferColumnTypes', 'true') \\\n  .option('cloudFiles.schemaLocation', '/mnt/schema/') \\\n  .load('/mnt/iot/raw/')\n\n# Write with schema evolution and partitioning\nquery = df.writeStream.format('delta') \\\n  .option('checkpointLocation', '/mnt/checkpoints/iot') \\\n  .option('mergeSchema', 'true') \\\n  .partitionBy('region', 'device_type', 'event_date') \\\n  .trigger(processingTime='30 seconds') \\\n  .start('/mnt/iot/processed/')\n\n# Unity Catalog RBAC setup\nCREATE CATALOG IF NOT EXISTS iot_catalog;\nCREATE SCHEMA IF NOT EXISTS iot_catalog.telemetry;\nGRANT USE CATALOG ON CATALOG iot_catalog TO `data-engineers`;\nGRANT USE SCHEMA ON SCHEMA iot_catalog.telemetry TO `data-engineers`;\nGRANT SELECT ON TABLE iot_catalog.telemetry.iot_events TO `analysts`;\n```\n\n## Data Contracts & Governance\n- Delta Lake constraints for data quality validation\n- Schema enforcement via Delta Lake schema registry\n- Purview integration for automated lineage tracking\n- Retention: 7 years hot (ADLS), 30 days Delta Lake VACUUM\n- Data residency: Region-specific containers with geo-replication disabled\n\n## DR Strategy\n- Azure Data Factory copy activities for cross-region sync\n- Active-passive pattern with automated failover\n- Point-in-time restore via Delta Lake time travel","diagram":"flowchart TD\n  A[IoT Devices] --> B[Event Hub / IoT Hub]\n  B --> C[ADLS Gen2 Region A Raw]\n  C --> D[Databricks Delta Lake Region A]\n  D --> E[Purview Lineage]\n  D --> F[Unity Catalog RBAC]\n  D --> G[Cross-Region Replication to Region B]\n  G --> H[DR ADLS Gen2 Region B]","difficulty":"advanced","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Twitter","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":["schema drift tolerant ingestion","delta lake mergeschema","cloudfiles.infercolumntypes","end-to-end data lineage","cross-region dr replication","data contracts","partitioning strategy","watermark-based streaming","unity catalog rbac","azure purview governance","exactly-once semantics","retention policies"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2026-01-17T04:45:30.593Z","createdAt":"2026-01-12T19:42:48.929Z"},{"id":"q-1043","question":"Design an end-to-end Azure data-ecosystem pipeline that ingests incremental changes from a MongoDB Atlas collection into Delta Lake on ADLS Gen2, preserving SCD Type 2 history for a customers dimension, and handling schema drift. Include data lineage to Azure Purview, late-arriving updates, and on-demand reprocessing without data loss. Which components and config would you choose, and why?","answer":"Use MongoDB Atlas Change Streams with Debezium to stream inserts/updates into Kafka, then Spark Structured Streaming consumes from Kafka and writes to Delta Lake on ADLS Gen2 with Delta schema evoluti","explanation":"## Why This Is Asked\nThis question tests practical design decisions for real-time CDC pipelines across Azure, including lineage, schema drift, and reprocessing.\n\n## Key Concepts\n- MongoDB Atlas Change Streams and Debezium for CDC\n- Delta Lake schema evolution and SCD Type 2\n- Spark Structured Streaming from Kafka to Delta Lake\n- Azure Purview for end-to-end lineage\n- Late-arriving data handling and replay strategies\n\n## Code Example\n```javascript\n// Pseudo-implementation illustrating flow (language-agnostic)\nconst inStream = Kafka.read(\"mongodb-change-stream\");\nDelta.write(inStream, \"/mnt/adls/delta/customers\");\n```\n\n## Follow-up Questions\n- How would you implement schema drift handling in Delta Lake?\n- How do you validate and monitor Purview lineage end-to-end?\n- What are failure modes and how would you recover from late data or CDC gaps?\n","diagram":null,"difficulty":"intermediate","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","NVIDIA","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T20:30:13.903Z","createdAt":"2026-01-12T20:30:13.903Z"},{"id":"q-1311","question":"Design an end-to-end incremental ELT pipeline on Azure that ingests 2 TB/day of nested JSON telemetry from Event Hubs into a Delta Lake on ADLS Gen2, then loads a star schema in Azure Synapse. Requirements: (1) schema drift tolerant writes and schema evolution, (2) upserts by a composite key (tenant_id, event_id), (3) end-to-end data lineage to Purview, (4) late-arriving data support via watermarking, (5) partitioning by region and day with file-size/compaction strategies. Which components and patterns would you use, and why? Compare Delta Lake MERGE vs Delete+Insert for upserts?","answer":"Use Event Hubs → Databricks Structured Streaming → Delta Lake on ADLS Gen2 with schema evolution enabled. Upsert into a Delta table on (tenant_id, event_id) using MERGE; apply a 5-minute watermark for","explanation":"## Why This Is Asked\nTests practical ability to architect an Azure data pipeline handling schema drift, incremental loads, and governance across a hybrid stack. It also probes trade-offs between MERGE and Delete+Insert for upserts and how to manage late data and file sizing in a real-world volume.\n\n## Key Concepts\n- Event Hubs, Databricks Structured Streaming, Delta Lake on ADLS Gen2\n- Schema evolution and drift tolerance for nested JSON\n- Upserts via MERGE vs Delete+Insert\n- Watermarking for late data, partitioning strategy\n- Data lineage with Purview, serving layer in Azure Synapse\n\n## Code Example\n```python\n# PySpark sketch for streaming write with MERGE via foreachBatch\ndef upsert_to_delta(microBatchDF, batchId):\n    microBatchDF.createOrReplaceTempView('updates')\n    spark.sql(\"\"\"\n      MERGE INTO delta_table AS t\n      USING updates AS s\n      ON t.tenant_id = s.tenant_id AND t.event_id = s.event_id\n      WHEN MATCHED THEN UPDATE SET *\n      WHEN NOT MATCHED THEN INSERT *\n    \"\"\")\n\nstreamingDF.writeStream.foreachBatch(upsert_to_delta).option('checkpointLocation','/chkpt/path').start('/delta/path')\n```\n\n## Follow-up Questions\n- How would you tune for high cardinality keys without hotspotting?\n- How do you validate lineage in Purview across pipelines and schemas?","diagram":null,"difficulty":"intermediate","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Google","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T10:37:25.505Z","createdAt":"2026-01-13T10:37:25.505Z"},{"id":"q-1326","question":"Design an end-to-end pipeline that streams user activity from Azure Event Hubs into Delta Lake on ADLS Gen2, supports schema evolution, and updates an SCD Type 2 customer dimension in Synapse. Include exactly-once guarantees, late-arriving data handling, idempotent sinks, and end-to-end data lineage via Purview in a hybrid estate. List components, data flows, and governance approach?","answer":"Use Spark Structured Streaming on Azure Databricks to read Event Hubs, write to Delta Lake on ADLS Gen2 with auto schema evolution. Apply MERGE INTO to a Type 2 customer dimension in Synapse for upser","explanation":"## Why This Is Asked\nTests production-grade streaming & lakehouse design: schema evolution, upserts, lineage, and hybrid governance.\n\n## Key Concepts\n- Delta Lake schema evolution on ADLS Gen2\n- Structured Streaming with exactly-once guarantees\n- MERGE INTO for SCD Type 2 in Synapse\n- Purview data lineage across on-prem and cloud\n- Hybrid lakehouse governance and security\n\n## Code Example\n```python\n# PySpark pseudo\nfrom delta.tables import DeltaTable\nsource_df = spark.readStream.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"...\").load()[...]\ndelta_path = \"/mnt/delta/customers\"\ndelta_table = DeltaTable.forPath(spark, delta_path)\n# streaming merge example (simplified)\ndelta_table.alias(\"t\").merge(\n  source_df.alias(\"s\"),\n  \"t.customer_id = s.customer_id\"\n).whenMatchedUpdate(set={\"name\":\"s.name\",\"address\":\"s.address\",\"end_date\":\"current_timestamp()\"})\n .whenNotMatchedInsert(values={\"customer_id\":\"s.customer_id\",\"name\":\"s.name\",\"start_date\":\"current_timestamp()\"})\n .execute()\n```\n\n## Follow-up Questions\n- How would you handle late-arriving events with out-of-order progress?\n- What changes would you make to support global data cataloging and cross-region replication?","diagram":"flowchart TD\n  A[Event Hubs] --> B[Databricks Spark Structured Streaming]\n  B --> C[Delta Lake on ADLS Gen2]\n  C --> D[Synapse SCD Type 2]\n  D --> E[Purview Lineage]\n  E --> F[Hybrid Serving Layer]","difficulty":"advanced","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T11:36:41.027Z","createdAt":"2026-01-13T11:36:41.028Z"},{"id":"q-1453","question":"You are starting a beginner-friendly Azure data engineer task: daily telemetry logs arrive in ADLS Gen2 under /raw/telemetry/ in mixed formats (CSV and nested JSON). Design an event-driven pipeline using Azure Data Factory that fires on blob creation, ingests files, flattens nested structures to a canonical schema, handles optional fields and few schema drift cases, and loads into a simple star schema in Azure Synapse Analytics. Include incremental loading with a watermark and basic data quality checks. What components and steps would you implement?","answer":"Trigger with blob-created events to start an ADF pipeline; Data Flow flattens nested JSON/CSV into canonical columns, applying defaults for missing fields and handling drift; load curated Parquet to /","explanation":"## Why This Is Asked\nThis question evaluates ability to design an event-driven ingest pipeline that handles multi-format data, schema drift, and incremental loads with a simple star schema.\n\n## Key Concepts\n- Event-driven ingestion with BlobCreated events\n- Data Flow for schema normalization and drift handling\n- Staging area and Parquet/Delta-lite storage\n- MERGE-based upserts into a star schema in Synapse\n- Incremental loading using a watermark\n- Basic data quality checks and monitoring\n\n## Code Example\n```sql\nMERGE INTO dw.dbo.FactTelemetry AS t\nUSING staging.FactTelemetry AS s\nON t.event_id = s.event_id\nWHEN MATCHED THEN UPDATE SET\n  t.value = s.value,\n  t.event_time = s.event_time\nWHEN NOT MATCHED THEN INSERT (event_id, value, event_time) VALUES (s.event_id, s.value, s.event_time);\n```\n\n## Follow-up Questions\n- How would you test idempotency of the MERGE against late-arriving data?\n- What changes would you make to handle more complex nested payloads without breaking existing pipelines?","diagram":"flowchart TD\n  A[Blob Created] --> B[ADF Trigger]\n  B --> C[Data Flow: Normalize]\n  C --> D[Staging: Parquet in /curated]\n  D --> E[MERGE into Star Schema in Synapse]\n  E --> F[Partition by date]","difficulty":"beginner","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T17:48:57.023Z","createdAt":"2026-01-13T17:48:57.023Z"},{"id":"q-1474","question":"Design a nightly Azure data pipeline that ingests delta updates from a SaaS source into ADLS Gen2, handles schema drift with automatic inference, and materializes a star schema in Azure Synapse. Ensure end-to-end data lineage, idempotent upserts, and reliable rollback after failures. Which services and patterns would you deploy, and how would you verify correctness and lineage end-to-end?","answer":"Use Azure Data Factory to orchestrate nightly ingests from the SaaS source into ADLS Gen2; Spark on Synapse performs drift-tolerant transforms with automatic schema inference and writes to a Delta tab","explanation":"## Why This Is Asked\nAssesses practical ability to design a robust Azure data pipeline: ingestion, drift handling, lineage, and reliability.\n\n## Key Concepts\n- Orchestration: Azure Data Factory\n- Drift handling: Spark schema inference\n- Storage and warehousing: ADLS Gen2 and Delta Lake on Synapse\n- Lineage: Purview\n- Reliability: MERGE upserts and replay semantics\n\n## Code Example\n```sql\nMERGE INTO dim_sales AS target\nUSING staging_sales AS src\nON target.sale_id = src.sale_id\nWHEN MATCHED THEN UPDATE SET target.amount = src.amount, target.date = src.date\nWHEN NOT MATCHED THEN INSERT (sale_id, amount, date) VALUES (src.sale_id, src.amount, src.date);\n```\n\n## Follow-up Questions\n- How would you monitor data quality and lineage across components?\n- How would you implement rollback and replay when a batch fails?","diagram":"flowchart TD\n  A[ADF] --> B[Ingest to ADLS Gen2]\n  B --> C[Spark (Synapse) drift-aware transform]\n  C --> D[Delta table in Synapse]\n  D --> E[Purview lineage]","difficulty":"intermediate","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Oracle","Plaid","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T18:48:47.638Z","createdAt":"2026-01-13T18:48:47.638Z"},{"id":"q-1524","question":"You operate a global event lake: multiple producers push JSON events into Event Hubs. Ingest to ADLS Gen2, apply drift-tolerant transformations, and publish a canonical star schema in Azure Synapse with incremental loads. Propose a production-ready architecture leveraging Data Factory, Databricks Delta Lake, and Purview; detail schema evolution, late-arriving data handling, data quality gates, and privacy masking?","answer":"Use Event Hubs for streaming ingestion into ADLS Gen2, Databricks Delta Lake for streaming+batch with schema evolution, and Azure Synapse for serving. Orchestrate with Data Factory; Purview for lineag","explanation":"## Why This Is Asked\nAssesses ability to design an end-to-end, scalable, governed data lake with real-time and batch components, plus practical handling of schema drift and privacy.\n\n## Key Concepts\n- Event Hubs ingestion and streaming pipelines\n- Delta Lake schema evolution and merge on write\n- Incremental loads and late-arriving data handling\n- End-to-end data lineage with Purview\n- Data quality gates and privacy masking strategies\n\n## Code Example\n```javascript\n// Pseudo-implementation: enable schema evolution during write\nfunction writeDelta(df, path){\n  df.write\n    .format('delta')\n    .option('mergeSchema','true')\n    .mode('append')\n    .save(path)\n}\n```\n\n## Follow-up Questions\n- How would you monitor lineage and quality across pipelines?\n- What tests would you add for schema evolution compatibility across producers?","diagram":"flowchart TD\n  A[Event Hubs] --> B[Databricks Delta Lake (ADLS Gen2)]\n  B --> C[Azure Synapse (Serving Layer)]\n  D[ADF Orchestration] --> B\n  E[Purview] --> B\n  B --> F[Quality Masks & Privacy UDFs]","difficulty":"advanced","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","Scale Ai","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T20:41:30.601Z","createdAt":"2026-01-13T20:41:30.601Z"},{"id":"q-1563","question":"Daily JSON feed arrives in ADLS Gen2. Design a beginner Data Factory pipeline to stage, flatten nested fields, validate data quality, and write a partitioned Parquet table in Azure Synapse. Handle minor schema drift by defaults and ignoring new fields. Which components and steps would you use?","answer":"I would implement a two-step Data Factory pipeline: (1) Copy activity to stage the JSON files with schema-drift awareness enabled; (2) Mapping Data Flow to flatten nested counterparty fields, validate data quality (price > 0 and quantity > 0), and write partitioned Parquet files to Azure Synapse.","explanation":"## Why This Is Asked\nAssesses practical Data Factory usage, incremental loading capabilities, and basic schema drift handling for real-world JSON feeds.\n\n## Key Concepts\n- Data Factory Copy activity combined with Mapping Data Flow\n- Schema drift handling, default values, and nested field flattening\n- Incremental loading via control table; Parquet partitioning in Synapse\n\n## Code Example\n```sql\nMERGE INTO dbo.Trades AS t\nUSING staging.Trades AS s\nON t.TradeDate = s.TradeDate AND t.TradeId = s.TradeId\nWHEN MATCHED THEN UPDATE SET t.Price = s.Price, t.Qty = s.Qty\nWHEN NOT MATCHED THEN INSERT (TradeDate, TradeId, Price, Qty)\nVALUES (s.TradeDate, s.TradeId, s.Price, s.Qty)\n```","diagram":null,"difficulty":"beginner","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","DoorDash","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T06:18:48.949Z","createdAt":"2026-01-13T21:48:38.152Z"},{"id":"q-1638","question":"Design a streaming ingestion from an on-prem ERP into ADLS Gen2 for real-time analytics. Use Delta Lake with Auto Loader-like ingestion, bronze-silver-gold data path, exact-once delivery, and schema-drift tolerant processing. The silver layer feeds a star schema in Azure Synapse. Ensure end-to-end lineage in Purview, data quality gates, and robust late-arriving data handling. Describe architecture, contracts, and rollback strategy?","answer":"Use Databricks Auto Loader to ingest streaming data into Delta bronze on ADLS Gen2; transform with schema-drift tolerant MERGE into silver; feed a star schema in Azure Synapse; capture lineage in Purv","explanation":"## Why This Is Asked\nTests ability to design streaming pipeline across Azure components with real-time data, schema drift, CDC-like behavior, and governance.\n\n## Key Concepts\n- Databricks Auto Loader, Delta Lake, mergeSchema\n- Bronze-Silver-Gold streaming path\n- Azure Purview for end-to-end lineage\n- Azure Synapse Analytics star schema serving\n- Late-arriving data handling, watermarking, checkpointing, rollback\n\n## Code Example\n```javascript\n// Bronze: streaming read (Auto Loader-like)\nconst bronze = spark.readStream\n  .format('cloudFiles')\n  .option('cloudFiles.format','json')\n  .option('cloudFiles.inferColumnTypes','true')\n  .load('abfss://container@storage.dfs.core.windows.net/bronze');\n\n// Silver: upsert into Delta with schema drift tolerance\nbronze.writeStream\n  .format('delta')\n  .option('checkpointLocation','/mnt/checkpoints/silver')\n  .start('/mnt/delta/silver');\n\nDeltaTable.forPath(spark, '/mnt/delta/silver').as('s').merge(\n  bronze.alias('b'), 's.id = b.id'\n).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute();\n```\n\n## Follow-up Questions\n- How would you verify end-to-end lineage across Purview and Synapse?\n- How would you validate schema drift scenarios and rollback correctness?","diagram":"flowchart TD\n  Bronze[Bronze Delta] --> Silver[Silver Delta]\n  Silver --> Gold[Azure Synapse Star Schema]\n  Purview[Purview] --> Silver\n  Bronze --> Purview","difficulty":"advanced","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","Snowflake","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T04:24:07.043Z","createdAt":"2026-01-14T04:24:07.043Z"},{"id":"q-1682","question":"Design an end-to-end near-real-time data pipeline for a global adtech dataset with mixed sources (on-prem SQL Server, Azure SQL, and SaaS APIs). Implement CDC, incremental loads, and schema drift tolerance, load into Delta Lake on Synapse, and enforce end-to-end lineage in Purview. Include data masking for PII, data quality gates, and a GitOps workflow for pipelines?","answer":"CDC across mixed sources using Data Factory, streaming to ADLS Gen2 Raw; transform via Databricks Spark with Delta Lake on Synapse; publish curated layer to serverless SQL. Purview provides end-to-end","explanation":"## Why This Is Asked\n\nAssesses ability to architect a heterogeneous, scale-out data lakehouse with real-time ingress, schema drift handling, governance, and privacy controls, plus an actionable CI/CD model.\n\n## Key Concepts\n\n- CDC from multi-source systems into ADLS Gen2\n- Delta Lake on Synapse for unified storage and ACID semantics\n- Purview for lineage, classification, and policy enforcement\n- PII masking in the processing layer (e.g., Spark UDFs)\n- Data quality gates (e.g., Great Expectations) integrated into pipelines\n- GitOps-driven CI/CD for reproducible deployments\n\n## Code Example\n\n```python\n# Spark masking example (pseudo)\nfrom pyspark.sql.functions import when, col\n\ndf = df.withColumn(\n    'ssn_masked',\n    when(col('ssn').isNotNull(), 'XXX-XX-' + col('ssn').substr( -4, 4 )).otherwise(None)\n)\n```\n\n## Follow-up Questions\n\n- How would you handle GDPR data Subject Rights requests in this pipeline?\n- What are the trade-offs between serverless SQL vs provisioned Spark pools for the curated layer?","diagram":null,"difficulty":"advanced","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T06:54:31.901Z","createdAt":"2026-01-14T06:54:31.901Z"},{"id":"q-1782","question":"Design a beginner-level Azure data ingestion task: In a fintech scenario, ingest daily transaction rows from an on-premises SQL Server into Azure Data Lake Storage Gen2 as Parquet files using Azure Data Factory. Include steps to handle date partitioning, security, and reliable runs. What would your pipeline look like, and what minimal code or configuration would you provide?","answer":"Proposed: An ADF pipeline ingesting daily transactions from on-prem SQL Server to ADLS Gen2 as Parquet. Steps: 1) linked services for on-prem SQL and ADLS Gen2, 2) dataset with a loadDate param, 3) Co","explanation":"## Why This Is Asked\nThis question tests practical data ingestion skills using Azure Data Factory, including linked services, parameterized datasets, copy activities with predicates, and basic monitoring. It mirrors fintech use cases where day-partitioned Parquet blobs enable efficient downstream queries.\n\n## Key Concepts\n- Azure Data Factory pipelines\n- Parquet sink and date partitioning\n- Self-hosted integration runtime\n- Copy activity performance and retries\n\n## Code Example\n```javascript\n// Example: pseudo-JSON/ARM-like config is expected, not full code\n```\n\n## Follow-up Questions\n- How would you handle schema drift?\n- How would you implement idempotent loads and error retries?\n","diagram":null,"difficulty":"beginner","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Coinbase"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T10:45:04.491Z","createdAt":"2026-01-14T10:45:04.491Z"},{"id":"q-1916","question":"You manage a multi-region analytics platform with data from Salesforce, Workday, and IoT devices. Data lands in ADLS Gen2; you need to apply policy-driven redaction for PII, support incremental loads, enforce schema drift tolerance, and publish end-to-end lineage to Azure Purview and downstream consumers (Power BI, Synapse). Propose an Azure-native pipeline design using Data Factory, Databricks Delta Live Tables, Purview, and Synapse; highlight design decisions, governance, and potential pitfalls?","answer":"Use Delta Live Tables on Databricks for streaming and batch, with auto-loader to ingest from ADLS Gen2, implement structured streaming with incremental watermarking; apply UDF-based redaction for PII ","explanation":"## Why This Is Asked\nThis question probes cross-service data integration, governance, and privacy controls in a realistic multi-region setup. It tests practical tradeoffs between streaming vs batch, and lineage coverage.\n\n## Key Concepts\n- Delta Live Tables, Auto Loader, schema drift handling, incremental loads\n- Azure Purview for lineage, RBAC, data masking and masking policies\n- Synapse external tables and Power BI consumption\n\n## Code Example\n```python\n# Databricks pseudo\nfrom delta.tables import DeltaTable\n# redaction example\ndef mask_pii(s):\n    return re.sub(r\"\\\\d{12}\", \"XXX-XX-XXXX\", s)\n```\n\n## Follow-up Questions\n- How would you adapt this for GDPR data deletion requests?\n- How do you validate lineage coverage across new sources?","diagram":"flowchart TD\nA[Data sources] --> B[ADLS Gen2 landing]\nB --> C[Databricks Delta Live Tables]\nC --> D[Azure Purview lineage]\nC --> E[Synapse external tables]","difficulty":"advanced","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","MongoDB","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T17:00:08.420Z","createdAt":"2026-01-14T17:00:08.420Z"},{"id":"q-1979","question":"In Azure, design a production-grade data pipeline for streaming ecommerce telemetry from Azure Event Hubs into ADLS Gen2, with downstream star schema in Synapse, enabling incremental loads, automatic schema drift handling, and end-to-end data lineage via Purview. Include details on Delta Lake usage, MERGE-based upserts, late-arriving data handling, and a minimal Spark code snippet for a MERGE with schema drift tolerance?","answer":"Architect a streaming-to-batch hybrid pipeline: Event Hubs to ADLS Gen2 raw, Databricks Delta Lake as canonical layer, and MERGE-based upserts into a Synapse star schema. Include Purview for end-to-en","explanation":"## Why This Is Asked\nThis question probes end-to-end data engineering skills: streaming ingestion, lakehouse architecture, dimensional modeling in Synapse, and governance. It also tests ability to implement schema evolution and data contracts while maintaining lineage.\n\n## Key Concepts\n- Delta Lake schema evolution and MERGE-based upserts\n- Data lineage with Purview across ADLS Gen2, Databricks, and Synapse\n- Late-arriving data handling with watermarking\n- Star schema design in Synapse for analytics\n- Integration of Event Hubs, ADLS Gen2, Databricks, and Purview\n- Testing and monitoring in production\n\n## Code Example\n```javascript\nfrom delta.tables import DeltaTable\n# pseudo-spark snippet illustrating MERGE with schema drift handling\nsrc_df = spark.readStream.format(\"json\").load(\"abfss://container@account.dfs.core.windows.net/input/\")\ndelta = DeltaTable.forPath(spark, \"/mnt/delta/fact_telemetry\")\ndelta.alias(\"t\").merge(\n  src_df.alias(\"s\"),\n  \"t.id = s.id\"\n).whenMatchedUpdate(set = {\"value\": \"s.value\", \"ts\": \"s.ts\"})\\\\n .whenNotMatchedInsertAll().execute()\n```\n\n## Follow-up Questions\n- How would you validate schema drift events and automate alerts?\n- How would you test idempotency of MERGE loads in a CI/CD pipeline?","diagram":"flowchart TD\n  A[Event Hubs] --> B[ADLS Gen2 Raw]\n  B --> C[Databricks Delta Lake]\n  C --> D[Synapse Star Schema]\n  C --> E[Purview Lineage]","difficulty":"advanced","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T19:34:09.515Z","createdAt":"2026-01-14T19:34:09.515Z"},{"id":"q-2102","question":"A batch of daily JSON click events arrives in ADLS Gen2 under /raw/events/ from an on‑prem source via Self-Hosted IR. Build a beginner Data Factory pipeline that (1) copies raw JSON to /staging/events/, (2) uses a schema-drift-tolerant mapping to normalize fields and cast ts to timestamp, (3) writes to a daily-partitioned Parquet table in Azure Synapse, and (4) loads incrementally by tracking max ts in a control table and filtering ts > last_ts. Include a basic data quality check for userId?","answer":"Use a Self-Hosted Integration Runtime to copy daily JSON files from `/raw/events/*.json` to `/staging/events/`. A Data Flow handles schema drift: map known fields, cast `ts` to timestamp, and tolerate new fields. Sink to a daily-partitioned Parquet table in Azure Synapse. Implement incremental loading by tracking the maximum timestamp in a control table and filtering for `ts > last_ts`. Add a basic data quality check to ensure `userId` is not null.","explanation":"Why This Is Asked\nTests practical skills in building a beginner-friendly end-to-end ELT pipeline on Azure: moving data from on-premises to the data lake, handling simple schema drift, and implementing incremental loads without over-engineering.\n\nKey Concepts\n- Self-Hosted Integration Runtime for on-premises connectivity\n- Copy Activity and Data Flow in Azure Data Factory\n- Schema drift tolerance and type casting in mapping data flows\n- Parquet sinks with daily partitioning\n- Incremental loads via control table tracking\n- Basic data quality checks for non-null key fields\n\nCode Example\n```javascript","diagram":"flowchart TD\n  A[Raw events in ADLS Gen2] --> B[Copy to staging]\n  B --> C[Data Flow: schema drift tolerant mapping]\n  C --> D[Parquet sink: partition by date(ts)]\n  D --> E[Incremental: last_ts control table]","difficulty":"beginner","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","NVIDIA","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T05:00:21.037Z","createdAt":"2026-01-15T02:16:32.970Z"},{"id":"q-2378","question":"Design a scalable pipeline to ingest daily customer event logs from an on-prem Kafka cluster into Azure Data Lake Gen2, apply schema drift tolerant transformations, and maintain an SCD Type 2 star schema in Azure Synapse, while providing end-to-end data lineage via Purview. Which Azure services and patterns would you use, and how would you implement incremental loads?","answer":"Ingest daily event logs from on‑prem Kafka into ADLS Gen2 via ADF or Dataflow, store as Parquet, then use Databricks Delta Lake to implement SCD Type 2 within a star schema in Azure Synapse. Use Purvi","explanation":"## Why This Is Asked\nThis question tests real-world Azure data pipelines with hybrid sources, schema drift, incremental loads, and lineage, aligning with roles at Snowflake, DoorDash, and Lyft.\n\n## Key Concepts\n- Ingest from on-prem Kafka to ADLS Gen2 with incremental offsets\n- Delta Lake SCD Type 2 and schema evolution\n- Star schema design in Azure Synapse\n- End-to-end data lineage with Purview across hybrid estate\n- Secure, scalable access via managed identities and RBAC\n\n## Code Example\n```javascript\n// Pseudo-Spark-like pseudocode for incremental load and SCD2 merge\nval source = spark.read.format(\"parquet\").load(\"abfss://landing@storage.dfs.core.windows.net/events/\")\nval target = spark.read.table(\"analytics.star_fact\")\nval merged = target.alias(\"t\").merge(\n  source.alias(\"s\"), \"t.id = s.id\"\n).whenMatchedUpdate(set = mapFromColumns(\"t\", \"s\"))\n  .whenNotMatchedInsertAll()\n```\n\n## Follow-up Questions\n- How would you handle late-arriving or out-of-order events in the stream?\n- How would you validate lineage accuracy across Purview and ensure changes propagate to downstream analytics?\n","diagram":null,"difficulty":"intermediate","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Lyft","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T15:43:20.714Z","createdAt":"2026-01-15T15:43:20.714Z"},{"id":"q-2495","question":"Ingest JSON telemetry from multiple partners into ADLS Gen2 and upsert into a Delta Lake on Synapse while preserving end-to-end lineage and data quality. Outline an Azure-based architecture using IoT Hub/Event Hubs, Data Factory or Synapse pipelines, Databricks, Purview, and Delta Lake features to handle schema drift, late data, and incremental upserts. Include security, cost considerations, and observability?","answer":"Ingest JSON telemetry via IoT Hub/Event Hubs into ADLS Gen2 as raw. Use Databricks Spark with schema drift tolerant transforms, then write curated data to Delta Lake on Synapse and use MERGE for incre","explanation":"## Why This Is Asked\nTests end-to-end Azure data lakehouse skills: streaming ingestion, schema drift, Delta Lake upserts, data governance, and observability. It stresses hybrid data governance via Purview and practical security/cost tradeoffs.\n\n## Key Concepts\n- Ingestion: IoT Hub/Event Hubs with ADLS Gen2 landing zones\n- Processing: Databricks Spark with schema evolution\n- Storage: Delta Lake on Synapse with MERGE\n- Governance: Purview lineage and data catalog\n- Quality/Observability: expectations, monitoring, RBAC\n\n## Code Example\n```python\n# PySpark sketch: read raw, infer schema, write delta, MERGE on upsert\nraw = spark.read.json('/mnt/raw/telemetry')\ncurated = raw.select('deviceId','ts','metrics.*')\ncurated.write.format('delta').mode('append').save('/delta/telemetry')\n# MERGE example (pseudo)\ndeltaTable = DeltaTable.forPath(spark, '/delta/telemetry')\ndeltaTable.alias('t').merge(source=curated.alias('s'), condition='t.id = s.id').whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n```\n\n## Follow-up Questions\n- How would you implement late-arriving data handling?\n- How would you test/validate lineage across layers in Purview?","diagram":"flowchart TD\n  Ingest[/IoT Hub/Event Hubs/] --> RAW[ADLS Gen2 Raw]\n  RAW --> SPARK[Databricks Spark]\n  SPARK --> DELTA[Delta Lake on Synapse]\n  DELTA --> PURVIEW[Purview Lineage]\n  DELTA --> QA[Quality Checks]","difficulty":"intermediate","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["OpenAI","Plaid","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T20:36:20.587Z","createdAt":"2026-01-15T20:36:20.588Z"},{"id":"q-2529","question":"Design an Azure data engineering pipeline for streaming IoT telemetry from store sensors into ADLS Gen2 and Synapse, with schema drift tolerant transformations, incremental loads, and end-to-end data lineage. Use Event Hubs to ADLS Gen2, Delta Lake in Synapse, and Purview for lineage. Address late data, schema evolution, security, and cross-region replication. What services and data contracts would you implement, and how would you validate correctness?","answer":"Ingest telemetry from Event Hubs using Spark Structured Streaming in Synapse, writing to a Delta Lake raw zone on ADLS Gen2. Enable schema auto-merge for evolution, use watermarks to handle late data, then apply incremental transformations through curated zones. Implement Purview for end-to-end lineage, enforce RBAC and Key Vault for security, and configure cross-region replication with Azure Geo-Redundant Storage. Validate correctness through data quality rules, schema validation tests, and end-to-end pipeline monitoring.","explanation":"## Why This Is Asked\nInterview context evaluates streaming design, Delta Lake, and governance integration in Azure.\n\n## Key Concepts\n- Event Hubs, Spark Structured Streaming\n- Delta Lake autoMerge, schema evolution\n- Purview for lineage and data catalog\n- RBAC, Key Vault security\n- Late data handling and end-to-end validation\n\n## Code Example\n```javascript\n// Pseudo Spark streaming snippet (Scala/Python-like)\nconst df = spark.readStream.format(\"eventhubs\").option(\"startingPosition\",\"latest\").load()\nconst curated = df.selectExpr(\"cast(body as string) as payload\")\ncurated.writeStream.format(\"delta\").option(\"checkpointLocation\",\"/checkpoints\").start()\n```","diagram":"flowchart TD\n  A[Event Hubs] --> B[ADLS Gen2 Raw]\n  B --> C[Delta Lake (Synapse)]\n  C --> D[Purview Lineage]\n  C --> E[Curated Tables]\n  E --> F[BI/Analytics]\n","difficulty":"intermediate","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Hashicorp","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T05:41:55.547Z","createdAt":"2026-01-15T21:39:09.922Z"},{"id":"q-2605","question":"Design an end-to-end real-time ingestion and analytics pipeline for a live market tick feed (timestamp, symbol, price, volume) arriving from multiple brokers at high throughput. Ingest to ADLS Gen2 as Delta Lake, with schema drift tolerance and automatic schema evolution, and deliver upserts to a central ticks table. Ensure exactly-once streaming, late-arriving data handling, end-to-end data lineage via Purview, and scalable partitioning by date and symbol. Outline components (Event Hubs, Databricks, Delta Lake, Purview) and provide a workable configuration sketch?","answer":"Ingest tick data from brokers into Azure Event Hubs, consume with Databricks Structured Streaming, and write to Delta Lake on ADLS Gen2 with schema evolution enabled. Use MERGE for upserts, a watermark for late-arriving data, and configure exactly-once semantics. Enable Purview integration for end-to-end lineage and partition by date/symbol for scalability.","explanation":"## Why This Is Asked\nTests real-time Azure data-engineering skills with schema drift, exactly-once semantics, and lineage.\n\n## Key Concepts\n- Streaming ingestion (Event Hubs)\n- Delta Lake schema evolution\n- Idempotent upserts (MERGE)\n- Late data handling (watermarks)\n- Data lineage (Purview)\n\n## Code Example\n```javascript\n// pseudo-config illustrating options, not runnable\n```\n\n## Follow-up Questions\n- How would you validate end-to-end latency?\n- What failure modes and compensating actions for late data?","diagram":null,"difficulty":"advanced","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Slack","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T05:02:41.068Z","createdAt":"2026-01-16T02:33:46.893Z"},{"id":"q-2625","question":"Daily on-prem JSON event files arrive; design a beginner ELT pipeline using Data Factory that ingests JSON, enforces a basic schema, converts to Parquet, partitions by event_date, and loads a simple star schema into Azure Synapse. Explain how you’d implement basic data lineage and a 7-day retention policy, without advanced schema drift handling?","answer":"Use Data Factory with a self-hosted integration runtime to pull daily on-prem JSON logs into ADLS Gen2 staging, then a Data Flow to enforce a basic schema and flatten objects, write as Parquet partiti","explanation":"## Why This Is Asked\nInterview context explains end-to-end data movement from on-prem to ADLS Gen2, then to Synapse, plus basic governance and lifecycle considerations.\n\n## Key Concepts\n- On-prem to cloud data ingestion with Data Factory\n- JSON to Parquet conversion and partitioning by date\n- Star schema loading into Azure Synapse\n- Basic data lineage with Purview\n- Simple retention via ADLS lifecycle\n\n## Code Example\n```sql\n-- Skeleton for star schema loading\nCREATE TABLE dim_customer (...);\nCREATE TABLE dim_product (...);\nCREATE TABLE fact_orders (...);\n```\n\n## Follow-up Questions\n- How would you adapt for evolving JSON schemas?\n- How would you monitor ingestion reliability and latency?","diagram":"flowchart TD\n  A[On-prem JSON] --> B[Self-hosted IR]\n  B --> C[ADLS Gen2 (staging)]\n  C --> D[Data Flow Parquet]\n  D --> E[Azure Synapse (Star schema)]\n  E --> F[Purview lineage]\n  F --> G[ADLS Lifecycle 7d]","difficulty":"beginner","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Goldman Sachs","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T04:10:40.457Z","createdAt":"2026-01-16T04:10:40.457Z"},{"id":"q-2669","question":"Design an Azure streaming data pipeline for a global fleet of IoT devices sending JSON telemetry to Event Hubs, with data landing in ADLS Gen2 and a Delta Lake lakehouse. Requirements: sub-5 second ingestion latency, end-to-end latency <15 seconds for dashboards, automatic schema evolution, idempotent upserts, and end-to-end lineage with Purview. Outline components, dataflow, and a minimal Delta table evolution example?","answer":"Use Event Hubs as ingress, Databricks Structured Streaming to Delta Lake on ADLS Gen2. Bronze raw ingest, Silver with schema-evolution via mergeSchema, and upserts via MERGE INTO. Enable checkpoints, ","explanation":"## Why This Is Asked\nThis probes implementing a robust streaming lakehouse with schema drift handling, governance, and cost control across regions.\n\n## Key Concepts\n- Event Hubs ingress, Delta Lake on ADLS Gen2\n- Structured Streaming, mergeSchema for evolution, MERGE INTO for upserts\n- Checkpointing, watermarks, idempotent loads\n- Purview lineage and Azure RBAC\n\n## Code Example\n```python\n# PySpark pseudo\nbronze = (spark.readStream.format('eventhubs').options(**ehConf).load())\n# parse, then write bronze\nbronze.writeStream.format('delta').option('checkpointLocation','/cp/bronze').start('/deltalake/bronze')\n\nsilver = spark.readStream.format('delta').load('/deltalake/bronze')\n# transform and upsert into silver with schema evolution\nsilver.writeStream.format('delta').option('checkpointLocation','/cp/silver').option('mergeSchema','true').start('/deltalake/silver')\n```\n\n## Follow-up Questions\n- How would you implement late-arrival handling and data quality checks in this stack?\n- What monitoring and alerting would you add for schema drift and job failures?","diagram":"flowchart TD\n  EH[Event Hubs] --> Bronze[Bronze Delta]\n  Bronze --> Silver[Silver Delta (mergeSchema)]\n  Silver --> Dash[Dashboards/BI]\n  Purview[Purview] --> Lineage[Lineage tracking]","difficulty":"intermediate","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Discord","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T05:52:01.037Z","createdAt":"2026-01-16T05:52:01.038Z"},{"id":"q-2680","question":"Design an end-to-end Azure data platform to ingest per-tenant e-commerce clickstream data from an on-prem SFTP into ADLS Gen2, handle schema drift, and deliver per-tenant analytics in Synapse while preserving end-to-end lineage in Purview and enforcing RBAC/column masking. Implement incremental loads, late-data handling, and cost/perf tradeoffs between serverless vs dedicated pools?","answer":"Ingest per-tenant data from on-prem SFTP into ADLS Gen2. Use Spark-based ETL to apply schema-drift tolerant transforms and write Delta Lake tables. Enforce per-tenant RBAC and column masking in Synaps","explanation":"## Why This Is Asked\n\nTests ability to design a scalable, governed lakehouse with multi-tenant isolation, schema drift handling, incremental loads, and integration with Purview and Synapse. Also probes trade-offs between serverless and dedicated pools.\n\n## Key Concepts\n\n- Delta Lake on ADLS Gen2\n- Synapse RBAC and dynamic masking\n- Purview data lineage\n- SFTP ingestion and Spark ETL\n- Incremental loads and late-arrival handling\n\n## Code Example\n\n```python\nfrom delta.tables import DeltaTable\n# sample MERGE snippet demonstrating upsert\ndelta_table = DeltaTable.forPath(spark, \"abfss://...@container.dfs.core.windows.net/tenant.delta\")\nsrc = spark.read.parquet(\"abfss://.../incoming/tenant/\")\n\ndelta_table.alias(\"t\").merge(\n  src.alias(\"s\"),\n  \"t.id = s.id\"\n).whenMatchedUpdate(set={\"t.value\": \"s.value\"}).whenNotMatchedInsertAll().execute()\n```\n\n## Follow-up Questions\n\n- How would you audit lineage across on-prem and cloud landing zones?\n- What are cost controls for large multi-tenant workloads?\n- How to validate schema drift and auto-infer schemas safely?\n","diagram":"flowchart TD\n  A[On-prem SFTP] --> B[ADF/ETL]\n  B --> C[ADLS Gen2 Delta Lake]\n  C --> D[Azure Synapse RBAC/Masking]\n  D --> E[Purview Lineage]\n  E --> F[Consumption Layer]","difficulty":"advanced","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Coinbase","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T06:53:36.282Z","createdAt":"2026-01-16T06:53:36.282Z"},{"id":"q-2712","question":"Design a global Azure data lakehouse for real-time fraud analytics. Ingest streaming transaction events from on-prem ERP into ADLS Gen2, apply schema-drift tolerant transformations, and store as Delta Lake tables with governance via Unity Catalog. Implement incremental loads with CDC and watermark, ensure end-to-end lineage via Purview, and support multi-region reads with geo-replication. Which components and patterns would you choose, and how would you configure them to balance latency, cost, and compliance?","answer":"CDC from on-prem ERP to Event Hubs; Databricks Structured Streaming reads Event Hubs, performs schema-evolution-aware transformations, and writes to Delta Lake on ADLS Gen2. Use Unity Catalog for fine","explanation":"Why This Is Asked\n\nThis question probes knowledge of end-to-end Azure data lakehouse architectures, focusing on real-time ingestion, schema drift handling, and governance at scale across regions.\n\nKey Concepts\n\n- CDC from on-prem systems via Event Hubs or Debezium\n- Spark Structured Streaming and Delta Lake on ADLS Gen2 with schema evolution\n- Unity Catalog for access control and governance\n- Purview for lineage and data governance\n- Watermarking, incremental upserts, and state management\n- Geo-replication and multi-region reads\n\nCode Example\n\n```javascript\n// Pseudocode: read from Event Hubs, parse, upsert to Delta Lake\nconst spark = SparkSession.builder().getOrCreate();\nlet raw = spark.readStream\n  .format('eventhubs')\n  .option('startingOffsets', 'latest')\n  .load();\nlet parsed = raw.selectExpr(\"CAST(body AS STRING) AS json\").selectExpr(\"from_json(json, <schema>) AS data\").select('data.*');\nparsed.writeStream()\n  .format('delta')\n  .option('checkpointLocation', '/checkpoints/transactions')\n  .option('path', 'abfss://<container>@<account>.dfs.core.windows.net/transactions')\n  .outputMode('append')\n  .start();\n```\n\nFollow-up Questions\n\n- How would you handle late-arriving data and schema drift in production?\n- What are the trade-offs between Unity Catalog vs Purview for lineage in this setup?\n- How would you monitor SLAs for latency and data freshness across regions?","diagram":null,"difficulty":"advanced","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Google","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T07:45:28.158Z","createdAt":"2026-01-16T07:45:28.159Z"},{"id":"q-2741","question":"Scenario: A REST API provides hourly weather data as JSON. Ingest to ADLS Gen2 and load a simple star schema in Synapse. Outline a beginner Azure Data Factory pipeline that calls the API hourly, lands raw JSON under /data/raw/weather/YYYY/MM/DD/HH/, normalizes to fixed fields (date, location, temp, humidity, wind, precip) with nulls for missing, writes Parquet partitioned by date, and validates via a nightly row-count check. Include components and steps?","answer":"Design an Azure Data Factory pipeline that calls the hourly REST API, lands raw JSON under /data/raw/weather/YYYY/MM/DD/HH/, uses a Data Flow to normalize to fixed fields (date, location, temp, humidi","explanation":"## Why This Is Asked\nTests practical REST ingestion, schema variation handling, and end-to-end flow from landing to a simple star schema with basic validation.\n\n## Key Concepts\n- REST ingestion with Data Factory Web activity\n- Data Flow for field mapping and type casting\n- Parquet partitioning in ADLS Gen2\n- Star schema design in Synapse\n- Nightly data quality validation\n\n## Code Example\n```python\n# Mapping sketch for Data Flow (conceptual)\nmap_weather = lambda rec: {\n  'date': rec.get('date'),\n  'location': rec.get('location'),\n  'temp': float(rec['temp']) if rec.get('temp') is not None else None,\n  'humidity': float(rec['humidity']) if rec.get('humidity') is not None else None,\n  'wind': float(rec['wind']) if rec.get('wind') is not None else None,\n  'precip': float(rec['precip']) if rec.get('precip') is not None else None\n}\n```\n\n## Follow-up Questions\n- How would you adapt for additional fields without breaking the pipeline?\n- How would you monitor failures and re-run only failed partitions?\n","diagram":"flowchart TD\nA[REST API Call (hourly)] --> B[ Land Raw JSON in ADLS /data/raw/weather/ ]\nB --> C[ Data Flow: Normalize fields to fixed set ]\nC --> D[ Parquet Sink: /data/warehouse/weather/date=YYYY-MM-DD ]\nD --> E[ Synapse: WeatherFact star schema ]","difficulty":"beginner","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","NVIDIA","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T09:49:24.647Z","createdAt":"2026-01-16T09:49:24.647Z"},{"id":"q-2915","question":"Scenario: Ingest IoT JSON logs stored in ADLS Gen2 under /iot/logs/ into a clean, queryable table in Azure Synapse. The schema evolves as new fields are added over time. Design a beginner-friendly pipeline using Synapse Pipelines or Data Factory that: 1) handles schema drift, 2) writes normalized data to Parquet in /processed/iot/, 3) loads incrementally via watermark strategy into a small fact table (device_id, event_date, event_type, duration), and 4) logs data quality metrics and lineage. Which services and steps would you implement, and how would you validate success?","answer":"Use Synapse Pipelines or Data Factory with Mapping Data Flows to read JSON from ADLS Gen2, enable schema drift handling to auto-extend schemas, write to Parquet at /processed/iot/, and load incrementa","explanation":"## Why This Is Asked\nTests practical, beginner-friendly mastery of drift-tolerant ingestion, incremental loading, and governance using familiar Azure tools. It also probes practical tradeoffs between Data Flows and Spark in a real-world IoT scenario.\n\n## Key Concepts\n- Schema drift handling in Mapping Data Flows\n- Watermark-based incremental load strategy\n- Parquet storage in ADLS Gen2 and path conventions\n- Basic data quality checks and failure logging\n- Data governance and lineage with Purview\n\n## Code Example\n```python\n# placeholder snippet illustrating a watermark-based incremental load sketch\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\ndf = spark.read.json(\"abfss://container@account.dfs.core.windows.net/iot/logs/\")\ndf.write.parquet(\"/processed/iot/\", mode=\"append\")\n```\n\n## Follow-up Questions\n- How would you test schema drift and backward compatibility?\n- What are the trade-offs between Data Flow and Spark for this pipeline?","diagram":"flowchart TD\n  Ingest[Ingest JSON from ADLS Gen2] --> Drift[Schema Drift Handling in Data Flow]\n  Drift --> Parquet[Write Parquet to /processed/iot/]\n  Parquet --> Load[Incremental Load to Synapse Gold]\n  Load --> Quality[Quality Logging & lineage to Purview]","difficulty":"beginner","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["NVIDIA","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T17:33:46.851Z","createdAt":"2026-01-16T17:33:46.851Z"},{"id":"q-2961","question":"Design an end-to-end data pipeline for a global fintech platform (Coinbase/Airbnb). Ingest real-time trade ticks from an on-prem SFTP feed and batch customer events into ADLS Gen2, leverage Delta Lake on Databricks, and load a star schema into Azure Synapse. Ensure end-to-end data lineage, automatic schema drift handling, incremental loads, and row-level security for PII. Outline components, Purview governance, and testing/rollback strategy?","answer":"Leverage ADF to stage on-prem files to ADLS Gen2, and Databricks Delta Lake with Auto Loader for batch+streaming. Enable schema drift tolerance, and use MERGE for incremental upserts into a Synapse-ba","explanation":"## Why This Is Asked\nProbes practical skill in building scalable, compliant data pipelines with real-time + batch data, governance, and security.\n\n## Key Concepts\n- Ingestion patterns: on-prem SFTP to ADLS Gen2, batch + streaming.\n- Delta Lake features: schema drift tolerance, MERGE upserts, time travel.\n- Orchestration: ADF, Databricks, Synapse integration.\n- Governance: Purview lineage/classification, access controls, data masking.\n\n## Code Example\n```sql\n-- example MERGE for incremental load into target star schema\nMERGE INTO target_schema.fact_trades AS t\nUSING staging.trades_updates AS s\nON t.trade_id = s.trade_id\nWHEN MATCHED THEN UPDATE SET ...\nWHEN NOT MATCHED THEN INSERT (...);\n```\n\n## Follow-up Questions\n- How would you test schema drift handling in this pipeline?\n- How would you implement rollback across regional replicas?","diagram":null,"difficulty":"advanced","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Coinbase"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T19:25:27.656Z","createdAt":"2026-01-16T19:25:27.658Z"},{"id":"q-3019","question":"Design an end-to-end Azure data pipeline for streaming IoT telemetry from on-prem JSON feeds into ADLS Gen2 and a star schema in Synapse. Use Delta Live Tables or Databricks to handle schema drift, implement idempotent upserts, ensure exactly-once semantics, and surface data lineage to Purview. Include data contracts, quality checks, alerting, and recovery from late data?","answer":"Use Azure Data Factory to stage on-prem JSON feeds to ADLS Gen2 as Parquet, then leverage Databricks Delta Live Tables to ingest as Delta Lake with automatic schema drift handling and evolution. Construct a star schema in Synapse and implement MERGE operations for idempotent upserts. Configure exactly-once semantics using Delta Lake's ACID transactions combined with watermarking. Integrate with Azure Purview for comprehensive data lineage through automatic asset registration. Define data contracts using JSON schemas and enforce quality checks with DLT expectations. Establish alerting via Azure Monitor and implement late data recovery using Delta Lake time travel and reprocessing capabilities.","explanation":"## Why This Is Asked\n\nTo evaluate the ability to design end-to-end Azure data pipelines for streaming scenarios with schema drift, upserts, lineage, and governance in a realistic enterprise setting.\n\n## Key Concepts\n\n- Streaming ETL with Delta Lake and Delta Live Tables\n- Schema evolution and drift handling\n- Data contracts and quality checks\n- Data lineage and Purview integration\n- Late-arriving data strategies\n\n## Code Example\n\n```javascript\n// Pseudocode: DLT table creation with schema drift\ncreateDeltaTable(\"facts_payments\", mergeOn=\"payment_id\")\n  .withSchemaEvolution(true)\n  .withExpectations({\n    quality: \"payment_amount > 0\",\n    completeness: \"payment_id IS NOT NULL\"\n  })\n  .withWatermark(\"event_timestamp\", \"5 minutes\")\n  .withPurviewLineage(true);\n```","diagram":null,"difficulty":"advanced","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","IBM","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T06:02:58.545Z","createdAt":"2026-01-16T21:38:22.569Z"},{"id":"q-3107","question":"Design a streaming ELT pipeline that ingests customer events from Azure Event Hubs and a SQL Server CDC feed, normalizes to a Delta Lake star schema on ADLS Gen2, supports schema evolution, idempotent upserts, and end-to-end lineage via Azure Purview. Outline components, dataflow, and quality checks?","answer":"Leverage Azure Databricks Structured Streaming to ingest data from Azure Event Hubs and SQL Server CDC feeds. Implement schema drift handling with automatic schema evolution, and perform idempotent upserts into Delta Lake on ADLS Gen2 using MERGE operations with watermarking and transactional writes. Establish a star schema through dimensional modeling, and capture comprehensive end-to-end data lineage via Azure Purview integration.","explanation":"## Why This Is Asked\n\nTests the ability to design enterprise-grade streaming data pipelines that integrate heterogeneous sources, handle schema evolution, maintain data quality, and provide governance in Azure's modern data stack.\n\n## Key Concepts\n\n- Structured Streaming for continuous data processing\n- Delta Lake MERGE operations for idempotent upserts\n- Schema evolution and automatic schema enforcement\n- Change Data Capture (CDC) from SQL Server\n- Watermarking for late-arriving data handling\n- Azure Purview for comprehensive data lineage\n- ADLS Gen2 for scalable cloud storage\n- Star schema dimensional modeling\n\n## Components\n\n1. **Ingestion Layer**: Azure Event Hubs for streaming events, SQL Server CDC for relational changes\n2. **Processing Layer**: Azure Databricks Structured Streaming jobs with schema evolution\n3. **Storage Layer**: Delta Lake on ADLS Gen2 with ACID transactions\n4. **Governance Layer**: Azure Purview for data cataloging and lineage\n5. **Quality Layer**: Data validation rules and monitoring\n\n## Dataflow\n\nEvent Hubs/CDC → Structured Streaming → Schema Validation → Delta Lake MERGE → Star Schema → Purview Lineage\n\n## Quality Checks\n\n- Schema validation and evolution policies\n- Data completeness and accuracy checks\n- Duplicate detection and idempotency validation\n- Performance monitoring and alerting\n- Lineage verification across pipeline stages","diagram":"flowchart TD\n  A[Event Hubs] --> B[Databricks Structured Streaming]\n  C[SQL CDC] --> B\n  B --> D[Delta Lake on ADLS Gen2]\n  D --> E[Azure Purview]","difficulty":"intermediate","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Microsoft","Robinhood","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T04:52:37.085Z","createdAt":"2026-01-17T02:25:15.775Z"},{"id":"q-3120","question":"In a multinational fintech, ingest both daily batch and streaming micro-batch events into Delta Lake on ADLS Gen2, enforce schema evolution, and capture end-to-end data lineage across on-prem, Event Hubs, and Azure Synapse analytics. Which Azure components would you choose, what patterns ensure correctness and rollback, and how would you validate lineage end-to-end at scale?","answer":"Design uses Azure Databricks + Delta Lake on ADLS Gen2; batch via Delta Live Tables; streaming via Structured Streaming; enable schema evolution with mergeSchema; lineage via Azure Purview with source","explanation":"## Why This Is Asked\n\nThis question probes the candidate's ability to architect a hybrid batch/streaming data pipeline on Azure, with strong governance, lineage, and rollback requirements across disparate sources.\n\n## Key Concepts\n\n- Delta Lake schema evolution and mergeSchema\n- Delta Live Tables and Structured Streaming\n- Azure Purview lineage and data map integration\n- Unity Catalog for fine-grained access\n- Time travel / PITR in Delta Lake\n\n## Code Example\n\n```scala\n// Delta Lake streaming with schema evolution\nval ds = spark.readStream.format(\"cloudFiles\").option(\"cloudFiles.format\",\"parquet\").load(\"/path/stream\")\nds.writeStream.format(\"delta\").option(\"mergeSchema\",\"true\").start(\"/mnt/delta/sink\")\n```\n\n## Follow-up Questions\n\n- How would you test lineage in Purview after data ingestion?\n- How would you handle out-of-order events and late data?","diagram":"flowchart TD\n  A[On-prem/Event Hubs] --> B[Databricks/Delta Lake]\n  B --> C[Delta Lake on ADLS Gen2]\n  C --> D[Azure Purview lineage]\n  C --> E[Azure Synapse/BI]","difficulty":"advanced","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Coinbase","Instacart"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T04:02:49.759Z","createdAt":"2026-01-17T04:02:49.759Z"},{"id":"q-3175","question":"You receive daily JSON event files from three SaaS apps landing in ADLS Gen2 at /raw/events/{saas}/{date}. Each file shape differs slightly. Design a beginner Data Factory pipeline to ingest, normalize to a common schema, and write to Parquet in /processed/events/{date}/. Include: (1) a schema-drift tolerant mapping or inference approach, (2) a simple incremental load using a date watermark, (3) basic data quality checks (not-null on key fields, duplicates), (4) automatic partitioning by date, (5) basic monitoring/logging. Which Azure components and steps would you use?","answer":"Leverage ADLS Gen2, Azure Data Factory v2, and Mapping Data Flows. Ingest JSON per SaaS to a Parquet sink partitioned by date. Enable schema-drift tolerant mapping (runtime field inference to a unifie","explanation":"## Why This Is Asked\n\nTests ability to design a practical beginner data ingestion and normalization pipeline with varying schemas, demonstrating schema drift handling, incremental loading, and basic quality checks using Azure Data Factory.\n\n## Key Concepts\n\n- Schema drift handling in Mapping Data Flows\n- Incremental loading using file path date watermark\n- Parquet sink with partitioning by date\n- Basic data quality checks (not-null, dedup)\n\n## Code Example\n\n```javascript\n{\n  \"source\": \"JSON\",\n  \"transform\": \"unify to schema_v1\",\n  \"sink\": {\n    \"format\": \"Parquet\",\n    \"partitionBy\": [\"date\"]\n  }\n}\n```\n\n## Follow-up Questions\n\n- How would you adapt this for streaming sources?\n- What are potential late-arriving data pitfalls and how would you handle them?","diagram":null,"difficulty":"beginner","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","Snap","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T05:36:59.077Z","createdAt":"2026-01-17T05:36:59.077Z"},{"id":"q-3229","question":"Scenario: A global retailer streams daily sales events and batch inventory feeds from on-prem sources into ADLS Gen2. You must implement end-to-end data lineage, incremental loads with watermark, schema drift tolerant transforms, and PII masking at ingestion, while enabling analytics in Synapse. Which Azure components would you choose to ensure idempotency and auditability across batch and streaming modes?","answer":"Ingest batch and streaming data with Azure Data Factory and Databricks, store in ADLS Gen2 as Delta Lake. Use Spark structured streaming with watermark for incremental loads, and MERGE for upserts. Ma","explanation":"## Why This Is Asked\nTests ability to design an integrated data platform with batch+stream, governance, privacy, and operational reliability across Azure services.\n\n## Key Concepts\n- End-to-end lineage with Purview\n- Delta Lake idempotent upserts\n- Ingestion-time masking of PII\n- Schema drift tolerant transforms\n- Watermark-based incremental processing across batch/stream\n- Cross-service orchestration (Data Factory, Databricks, Synapse)\n\n## Code Example\n```javascript\n// Pseudo orchestration snippet illustrating MERGE against Delta Lake\n```\n\n## Follow-up Questions\n- How would you handle late-arriving data and out-of-order events?\n- How would you validate lineage accuracy if Purview crawls change feed?","diagram":null,"difficulty":"intermediate","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Apple"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T07:40:21.141Z","createdAt":"2026-01-17T07:40:21.142Z"},{"id":"q-3444","question":"You run a multi-tenant data lake on ADLS Gen2 for three business units. Each unit provides JSON with evolving schemas; you must produce a per-tenant SCD Type 2 customer dimension in Delta Lake, with automatic schema drift handling, tenant-level access control, and end-to-end lineage to Purview. Outline architecture, data contracts, and how you guarantee idempotent upserts and secure access?","answer":"Use Databricks Auto Loader to ingest per-tenant JSON into Delta Lake. Use MERGE for SCD Type 2 upserts; handle schema drift with a central schema registry and Purview lineage; enforce tenant isolation","explanation":"## Why This Is Asked\nTests multi-tenant data lake design, schema evolution handling, and governance integration across Azure services.\n\n## Key Concepts\n- Delta Lake MERGE for SCD Type 2\n- Auto Loader with schema drift handling\n- Unity Catalog ABAC for tenant isolation\n- Purview lineage and data contracts\n\n## Code Example\n```javascript\n-- Pseudo SQL for MERGE (illustrative)\nMERGE INTO delta_table AS t\nUSING staging AS s\nON t.tenant = s.tenant AND t.id = s.id\nWHEN MATCHED THEN UPDATE SET ...\nWHEN NOT MATCHED THEN INSERT ...\n```\n\n## Follow-up Questions\n- How would you handle late-arriving data and out-of-order events?\n- How would you test schema drift and contract violations in CI/CD?","diagram":"flowchart TD\n  Ingest[Ingest per-tenant JSON] --> Drift[Schema Drift Handling]\n  Drift --> Upsert[Delta MERGE for SCD2]\n  Upsert --> Lineage[Purview Lineage]\n  Lineage --> UC[Unity Catalog Security]","difficulty":"intermediate","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Meta","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T16:42:42.413Z","createdAt":"2026-01-17T16:42:42.413Z"},{"id":"q-3467","question":"NEW ANGLE: Daily NDJSON invoices land in ADLS Gen2 under /raw/payments/. Build a beginner Data Factory pipeline that stages to /staging/payments, flattens nested fields (payment_id, amount, currency, customer.id, customer.name, ts), runs basic data quality checks (payment_id not null, amount > 0, ts valid), and upserts into a partitioned Parquet table in Azure Synapse (Payments by date(ts)). Explain incremental loading using a watermark and describe datasets/activities?","answer":"Use Data Factory with a Mapping Data Flow to stage NDJSON from /raw/payments into /staging/payments, flattening nested fields and applying quality checks (payment_id not null, amount > 0, ts valid). U","explanation":"## Why This Is Asked\nTests a practical, end-to-end beginner pattern: ingesting NDJSON, flattening nested JSON, validating data quality, and performing an incremental upsert into a partitioned Parquet table in Synapse. Emphasizes familiar Azure tooling and real-world trade-offs.\n\n## Key Concepts\n- Azure Data Factory and Mapping Data Flows\n- NDJSON parsing and JSON flattening\n- Data quality checks (nulls, ranges, timestamp validity)\n- Parquet sink and date partitioning in Synapse\n- Watermark-based incremental loading\n\n## Code Example\n```sql\nMERGE INTO Payments AS t\nUSING staging.Payments AS s\nON t.payment_id = s.payment_id\nWHEN MATCHED THEN UPDATE SET t.amount = s.amount, t.currency = s.currency, t.ts = s.ts\nWHEN NOT MATCHED THEN INSERT (payment_id, amount, currency, customer_id, customer_name, ts)\nVALUES (s.payment_id, s.amount, s.currency, s.customer_id, s.customer_name, s.ts);\n```\n\n## Follow-up Questions\n- How would you extend this to handle schema evolution in the NDJSON fields?\n- What monitoring would you add to verify every day’s watermark advances correctly and handles replays gracefully?","diagram":"flowchart TD\n  A[NDJSON in ADLS Gen2 /raw/payments] --> B[Stage into /staging/payments]\n  B --> C[Flatten fields: payment_id, amount, currency, customer.id, customer.name, ts]\n  C --> D[Quality checks: payment_id != null, amount > 0, ts valid]\n  D --> E[Write partitioned Parquet to Synapse: Payments by date(ts)]\n  E --> F[Incremental load via watermark]","difficulty":"beginner","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Airbnb","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T17:32:01.354Z","createdAt":"2026-01-17T17:32:01.354Z"},{"id":"q-3498","question":"You receive daily JSON logs from a SaaS app stored in ADLS Gen2 under /logs/yyyy/MM/dd/. Each file contains an array of events with nestedUser and nestedItems arrays. Design a beginner ELT using Data Factory and Spark (Synapse) to flatten to a users dimension and a facts table, implement an incremental load based on a date watermark, and ensure idempotent upserts with simple checks. What components, data flow, and validations would you implement?","answer":"Stage: Ingest with Data Factory Mapping Data Flow on Spark; flatten events to fields: user_id, user_email, event_ts; create dim_user and fact_events in Synapse; incremental load using a watermark stor","explanation":"## Why This Is Asked\n\nThis tests practical use of Azure Data Factory, Synapse Spark, and SQL MERGE for a simple ELT with nested JSON, including incremental loads and basic data quality.\n\n## Key Concepts\n\n- JSON flattening in Spark\n- Incremental loads with watermark\n- Upserts via MERGE\n- Basic data quality checks\n\n## Code Example\n\n```javascript\n// No code, design-level answer\n```\n\n## Follow-up Questions\n\n- How would you handle schema evolution for nested fields?\n- What are performance tips for large JSON arrays?","diagram":"flowchart TD\n  A[ADLS Gen2 /logs] --> B[DF Mapping Data Flow]\n  B --> C{Flattened tables}\n  C --> D[dim_user]\n  C --> E[fact_events]","difficulty":"beginner","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Citadel","Databricks"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T18:53:18.844Z","createdAt":"2026-01-17T18:53:18.844Z"},{"id":"q-3572","question":"Daily JSON logs arrive in ADLS Gen2 with nested fields and occasional missing keys. Design a beginner Data Factory pipeline that uses a Mapping Data Flow to flatten and unify to a flat schema, load incrementally into Azure Synapse (or Azure SQL DB) with a watermark on event_time, and route invalid records to /rejects. Include a simple schema-drift tolerant approach and a basic test plan?","answer":"Leverage Azure Data Factory Mapping Data Flows to flatten nested JSON structures, union overlapping fields into a unified flat schema, and load incrementally using a watermark on event_time into Azure Synapse. Handle missing fields by nulling them out, route malformed records to /rejects, and implement schema drift tolerance through dynamic column handling.","explanation":"Why This Is Asked\n\nThis question tests practical data engineering skills for handling schema drift, nested JSON structures, incremental loads, and basic data quality implementation using a reject path within the familiar Azure stack.\n\nKey Concepts\n\n- Mapping Data Flows for JSON flattening and schema unification\n- Schema drift tolerance with null handling for missing fields\n- Watermark-based incremental loading strategies\n- Data quality implementation via reject paths in ADLS\n\nCode Example\n\n```javascript\n// Pseudo-test: ensure essential fields exist after load\nconst rows = readFromSynapse(\"target_table\");\nassert(rows.every(row => row.event_time !== null), \"event_time should not be null\");\nassert(rows.every(row => row.id !== null), \"id should not be null\");\n```","diagram":"flowchart TD\n  A[ADLS Gen2: JSON files] --> B[ADF: Mapping Data Flow]\n  B --> C[Flatten/Union to flat schema]\n  C --> D[Incremental load via watermark]\n  D --> E[Azure Synapse table]\n  B --> F[Rejects: /rejects]","difficulty":"beginner","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T05:43:09.105Z","createdAt":"2026-01-17T21:37:48.243Z"},{"id":"q-3623","question":"In a globally distributed analytics platform, you must implement a multi-geo data mesh on Azure with data products exposed via Delta Lake tables on ADLS Gen2, governed by Purview, and queried through Synapse. How would you ensure discoverability, end-to-end lineage, contract-driven access, and schema evolution across regions while controlling cost and latency?","answer":"Implement a multi-geo data mesh architecture using Azure Purview for centralized catalog and lineage, deploy Delta Lake tables on ADLS Gen2 across regions with cross-region replication, expose data products through Synapse serverless SQL endpoints with contract-driven access via Azure Active Directory integration, and leverage Delta Lake's mergeSchema capabilities for schema evolution while optimizing cost and latency through region-local data access patterns.","explanation":"## Why This Is Asked\nThis question evaluates expertise in enterprise data architecture, specifically testing knowledge of Azure's data platform capabilities, data mesh principles, and production-grade governance patterns.\n\n## Key Concepts\n- Data mesh principles and data product thinking\n- Azure Purview for unified catalog, lineage, and governance\n- Delta Lake schema evolution with mergeSchema\n- Cross-region replication strategies and cost optimization\n- Contract-driven access via Azure AD RBAC and ACLs\n- Synapse integration for unified querying\n\n## Code Example\n```python\n# PySpark example: wri","diagram":"flowchart TD\n  A[Source Data Producers] --> B(Ingestion Pipelines in region R1)\n  B --> C[Delta Lake in ADLS Gen2]\n  C --> D[Purview Catalog & Lineage]\n  D --> E[Data Product APIs / Synapse]\n  E --> F[End-User Analytics/BI]","difficulty":"advanced","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Coinbase","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T05:08:42.119Z","createdAt":"2026-01-17T23:43:28.921Z"},{"id":"q-3643","question":"Design a real-time data lakehouse pipeline that ingests high-velocity financial events from an on-prem Oracle database into ADLS Gen2, processes with Spark Structured Streaming, and upserts into a Delta table consumed by Azure Synapse Analytics. Requirements: end-to-end lineage via Purview, automatic schema drift tolerance, incremental loads, tenant isolation, and robust observability. Which components and data contracts would you choose, and why?","answer":"Use Oracle GoldenGate Microservices to stream DML changes to Azure Event Hubs, Databricks Structured Streaming to consume the CDC stream and write to Delta Lake on ADLS Gen2 with schema evolution enabled, and Azure Synapse Analytics to query the Delta table directly. Implement tenant isolation through separate Event Hub namespaces and Delta Lake partitions, with Purview providing end-to-end lineage and data contracts enforced via Unity Catalog.","explanation":"## Why This Is Asked\n\nTests ability to architect a real-time lakehouse with CDC, lineage, schema drift handling, multi-tenant isolation, and observability across Azure services.\n\n## Key Concepts\n\n- Change Data Capture from on-prem Oracle to streaming sink\n- Delta Lake schema evolution and upserts\n- End-to-end data lineage with Purview\n- Multi-tenant data contracts and isolation\n- Observability, retries, backpressure, and failure handling\n\n## Code Example\n\n```javascript\n// foreachBatch upsert (pseudo)\nfunction upsertDelta(batchDF) {\n  // Real code would call DeltaTable.forPath(...).merge(...)\n}\n```","diagram":"flowchart TD\n  A[On-Prem Oracle CDC] --> B[Event Hubs]\n  B --> C[Databricks Structured Streaming]\n  C --> D[Delta Lake on ADLS Gen2]\n  D --> E[Delta table (Synapse consumption)]\n  E --> F[Purview lineage]","difficulty":"advanced","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Goldman Sachs","IBM"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T05:02:09.019Z","createdAt":"2026-01-18T02:45:17.591Z"},{"id":"q-3656","question":"You receive daily JSONL logs in ADLS Gen2 under /logs/appX/. Each file may add new fields over time. Build a beginner Data Factory pipeline that ingests these logs into a Delta Lake table in ADLS Gen2, using Spark in Synapse or Databricks. Implement schema drift handling, partition by ingest_date, and a simple data-quality check. Describe components and steps?","answer":"Use ADF to orchestrate a Databricks Spark job that reads JSONL from /logs/appX, infers a changing schema with mergeSchema, and writes to Delta at /delta/logs/appX with partitionBy ingest_date. Impleme","explanation":"## Why This Is Asked\nTests practical data ingestion with schema drift, Delta Lake, and lineage.\n\n## Key Concepts\n- Schema evolution with Spark mergeSchema\n- Delta Lake storage on ADLS Gen2\n- Data quality checks and Purview lineage\n\n## Code Example\n```javascript\n// Pseudocode: spark read + write with mergeSchema\nconst df = spark.read.format('json').load('/logs/appX')\ndf.write\n  .format('delta')\n  .partitionBy('ingest_date')\n  .mode('append')\n  .save('/delta/logs/appX')\n```\n\n## Follow-up Questions\n- How would you handle duplicates in event_id during MERGE?\n- How would you test schema drift handling locally?","diagram":"flowchart TD\n  A[Ingest JSONL] --> B[Infer/merge schema]\n  B --> C[Write Delta with partition]\n  C --> D[MERGE into target]\n  D --> E[Purview lineage]","difficulty":"beginner","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Robinhood","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T04:11:40.404Z","createdAt":"2026-01-18T04:11:40.404Z"},{"id":"q-3683","question":"Design a data ingestion and governance pattern for a mixed batch/streaming pipeline into ADLS Gen2 and Synapse: landing, silver Delta Lake, and curated views. Propose concrete Azure components (ADF, Databricks, Purview, Synapse), and describe how you enforce data contracts, enable schema evolution, and implement idempotent upserts across regions while controlling costs. How would you handle a breaking source schema change?","answer":"Propose: ADLS Gen2 landing, Databricks Delta Lake (Silver), Synapse for curated marts, ADF for orchestration, Purview for lineage and contracts; enable Delta schema evolution and upserts via MERGE; ve","explanation":"## Why This Is Asked\n\nTests practical integration of governance, schema evolution, and upserts in a mixed workload with real Azure services.\n\n## Key Concepts\n\n- Data contracts and lineage via Purview across batch/stream paths\n- Delta Lake schema evolution and drift tolerance\n- Idempotent MERGE upserts and cross-region deployment\n- Orchestration with ADF; compute via Databricks; curated views in Synapse\n- Cost controls through autoscale, materialization decisions, and caching\n\n## Code Example\n\n```python\n# Pseudo: upsert into Delta table with MERGE\nMERGE INTO silver AS s\nUSING updates AS u\nON s key = u key\nWHEN MATCHED THEN UPDATE SET *\nWHEN NOT MATCHED THEN INSERT *\n```\n\n## Follow-up Questions\n\n- How would you version and enforce contracts in Purview across teams?\n- How do you validate schema evolution compatibility before deployment?","diagram":"flowchart TD\n  A[Source Systems] --> B[Landing (ADLS Gen2)]\n  B --> C[Silver (Delta Lake)]\n  C --> D[Curated (Synapse)]\n  subgraph Governance\n  E[Purview] --> D\n  end","difficulty":"intermediate","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Google","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T05:34:58.430Z","createdAt":"2026-01-18T05:34:58.431Z"},{"id":"q-3705","question":"You're given daily JSON event files arriving in ADLS Gen2 under /raw/events/. Files contain arrays of events with optional fields; you must flatten to a simple star schema (DimUser: user_id, country; FactEvents: event_id, timestamp, user_id, event_type). Design a beginner end-to-end pipeline using Azure Data Factory and Synapse to stage raw JSON, handle schema drift, perform incremental loads with a watermark, and publish lineage to Purview. Which components, steps, and dataflow logic would you implement?","answer":"Copy raw JSON to ADLS staging, then a Mapping Data Flow flattens nested arrays into DimUser(user_id, country) and FactEvents(event_id, timestamp, user_id, event_type). Implement incremental load with ","explanation":"## Why This Is Asked\nTests practical data engineering workflow on modern Azure services, focusing on real-world JSON with drift and governance.\n\n## Key Concepts\n- Mapping Data Flows for flattening and drift tolerance\n- Incremental load using a watermark (fileModifiedTime)\n- Staging in ADLS Gen2, star schema in Synapse\n- Purview/Data Factory lineage integration for end-to-end lineage\n\n## Code Example\n```sql\n-- Simple incremental MERGE example for loading FactEvents\nMERGE INTO SynapseDB.dbo.FactEvents AS t\nUSING Staging.dbo.FactEvents AS s\nON t.event_id = s.event_id\nWHEN NOT MATCHED THEN INSERT (...);\n```\n\n## Follow-up Questions\n- How would you handle arrays and missing fields during flattening?\n- How would you validate data quality before upserts and surface lineage in Purview?","diagram":"flowchart TD\n  A[Ingest JSON files] --> B[Stage Raw in ADLS]\n  B --> C[Flatten with Data Flow]\n  C --> D[Load DimUser]\n  C --> E[Load FactEvents]\n  D --> F[Synapse DW]\n  E --> F","difficulty":"beginner","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","NVIDIA","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T06:44:31.851Z","createdAt":"2026-01-18T06:44:31.851Z"},{"id":"q-3728","question":"Design a real-time streaming ELT: ingest JSON events from Azure Event Hubs into Delta Lake on ADLS Gen2 via Databricks Structured Streaming; apply schema drift tolerant transforms; upsert into a Star schema in Azure Synapse using MERGE-based SCD2. How would you ensure end-to-end data lineage (Purview), late-data handling, and automatic schema evolution while controlling latency and costs?","answer":"Ingest JSON from Event Hubs with Databricks Structured Streaming into Delta Lake on ADLS Gen2, enabling Delta schema evolution; use MERGE-based SCD2 into a Star schema in Synapse. Enable Purview linea","explanation":"## Why This Is Asked\n\nEvaluates end-to-end real-time ELT design across Event Hubs, Delta Lake, and Synapse, plus governance with Purview. Tests handling of schema drift, late data, and accurate SCD2 upserts at scale.\n\n## Key Concepts\n\n- Streaming ingestion with Databricks Spark\n- Delta Lake schema evolution and drift tolerance\n- MERGE-based SCD2 in a star schema\n- End-to-end data lineage with Purview\n- Late-data handling, watermarks, and checkpointing\n\n## Code Example\n\n```javascript\n// Pseudo-Spark SQL MERGE for SCD2\nval mergeQuery = `\nMERGE INTO delta.`/mnt/delta/sales` AS target\nUSING updates AS src\nON target.id = src.id\nWHEN MATCHED THEN UPDATE SET *\nWHEN NOT MATCHED THEN INSERT *\n` \n```\n\n## Follow-up Questions\n\n- How would you handle schema evolution breaking changes in upstream producers?\n- What are potential cost/latency trade-offs of using Purview for lineage across streaming jobs?","diagram":"flowchart TD\n  EH[Azure Event Hubs] --> DS[Databricks Structured Streaming]\n  DS --> DL[Delta Lake on ADLS Gen2]\n  DL --> DW[Azure Synapse Analytics]\n  DW --> Purview[Purview Lineage]","difficulty":"advanced","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Robinhood","Salesforce","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T07:28:56.492Z","createdAt":"2026-01-18T07:28:56.492Z"},{"id":"q-3781","question":"Design an end-to-end data observability solution for a multi-source Azure data lakehouse. Ingested data lands in ADLS Gen2 and is consumed by Synapse; require automatic lineage, schema drift alerts, data freshness checks, and auto-remediation that re-runs failed pipelines or blocks downstream jobs. Describe architecture, thresholds, data quality checks, and how you would implement with Purview, Delta Lake, Data Factory, and Synapse?","answer":"Architect a observability layer across ADLS Gen2, Purview, and Synapse that monitors data freshness, record counts, schema drift, and end-to-end lineage. Leverage Delta Lake checkpoints, Data Factory/","explanation":"## Why This Is Asked\n\nTests ability to design a cohesive observability and governance layer spanning ingestion, storage, catalog, and compute, plus automated remediation in production.\n\n## Key Concepts\n\n- Data observability metrics: freshness, volume, schema drift, lineage\n- Delta Lake governance and time-travel\n- Event-driven remediation with Event Grid\n- Integration across ADLS Gen2, Purview, Data Factory, Synapse\n- Cost-aware alerting and automated rollback\n\n## Code Example\n\n```javascript\n// Pseudo logic for anomaly detection\nconst rules = [\n  {metric: 'freshness', maxLagMin: 60},\n  {metric: 'records', minDeltaPct: 20}\n];\n// on violation -> trigger workflow via Event Grid\n```\n\n## Follow-up Questions\n\n- How would you test this observability layer before production?\n- What would you monitor to distinguish data quality issues vs source outages?","diagram":null,"difficulty":"advanced","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Goldman Sachs","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T09:33:27.782Z","createdAt":"2026-01-18T09:33:27.782Z"},{"id":"q-3888","question":"You get daily JSON event logs from multiple sources placed under /events/campaigns/ in ADLS Gen2. Schema drift is possible with missing fields and occasional new fields. Build a beginner Azure Data Factory pipeline that ingests, flattens, and loads into a simple star schema in Azure Synapse Analytics; partition by eventDate; store details as JSON for drift tolerance; include basic data quality checks (not-null userId and parseable timestamp) and a minimal lineage/logging. What components and steps would you implement?","answer":"Implement a beginner ADF pipeline: read daily JSONs from /events/campaigns/ in ADLS Gen2, copy to a staging area, flatten with a Mapping Data Flow (or simple JSON path) and emit a tabular view. Load i","explanation":"## Why This Is Asked\nTests practical data engineering with semi-structured data, drift-tolerant ingest, and a clear path to a star schema in Synapse using Azure Data Factory and ADLS Gen2.\n\n## Key Concepts\n- Azure Data Factory pipelines and Mapping Data Flows\n- Ingesting JSON from ADLS Gen2 with schema drift\n- Flattening nested JSON; storing details as JSON for drift tolerance\n- Star schema design in Synapse (DimUser, DimDate, FactEvents)\n- Basic data quality checks and simple lineage logging\n\n## Code Example\n```sql\n-- Create simplified star schema (example)\nCREATE TABLE DimUser (UserId VARCHAR(256) PRIMARY KEY);\nCREATE TABLE DimDate (DateKey DATE PRIMARY KEY);\nCREATE TABLE FactEvents (\n  EventId INT IDENTITY(1,1) PRIMARY KEY,\n  UserId VARCHAR(256) FOREIGN KEY REFERENCES DimUser(UserId),\n  EventDate DATE,\n  EventType VARCHAR(64),\n  Details NVARCHAR(MAX)\n);\n```\n\n## Follow-up Questions\n- How would you handle incremental loads and late-arriving data with this design?\n- What lightweight monitoring would you add to catch JSON schema drift before it affects downstream tables?","diagram":null,"difficulty":"beginner","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Meta","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T13:49:50.928Z","createdAt":"2026-01-18T13:49:50.928Z"},{"id":"q-4212","question":"You are given daily Parquet files containing user activity logs arriving into ADLS Gen2 at /data/activity/. The schema drifts as new fields appear over time. Design a beginner Data Factory pipeline that: 1) copies raw Parquet to a landing zone preserving data; 2) transforms to a simple star schema in Azure Synapse (DimUsers and FactEvents); 3) handles drift by storing extra fields as a JSON blob in a drift column; 4) enforces basic data quality (not-null user_id, parseable event_time) and logs lineage. Which Azure components and steps would you implement?","answer":"Use Data Factory for orchestration with a two-stage flow: (1) Copy Parquet into /landing/parquet as raw. (2) Spark/DF or Data Flow to map fields to a star schema in Synapse: DimUsers and FactEvents. T","explanation":"## Why This Is Asked\\n\\nTests ability to design a beginner-friendly Azure data pipeline that handles schema drift, produces a dimensional model, and captures lineage. It also probes how to isolate raw data, apply transformations, and enforce simple data quality gates with observable provenance.\\n\\n## Key Concepts\\n- Schema drift handling in Parquet\\n- Dimensional modeling (DimUsers, FactEvents)\\n- Drift capture via JSON blob\\n- Data quality checks (NOT NULL, timestamp parse)\\n- Data lineage with Purview integration\\n\\n## Code Example\\n```javascript\n// Pseudocode: drift handling in a dataflow step\nfunction transform(record) {\n  const known = { user_id, event_time, event_type };\n  const drift = _.omit(record, Object.keys(known));\n  return { ...record, drift_json: JSON.stringify(drift) };\n}\n```\n\\n## Follow-up Questions\\n- How would you test drift handling with evolving schemas?\\n- How would you extend this to handle very high throughput volumes?","diagram":null,"difficulty":"beginner","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Scale Ai","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-19T08:54:14.176Z","createdAt":"2026-01-19T08:54:14.176Z"},{"id":"q-4228","question":"You're building a cross-region retail analytics data product in Azure. Ingest on-prem SQL Server CDC into ADLS Gen2 in each region, publish Delta Lake tables to Databricks and Synapse with incremental loads, end-to-end lineage, and contract-based access. Which Azure components and patterns would you use to guarantee incremental loads, schema drift tolerance, cross-region discovery via Purview, and secure data sharing with minimal latency and cost?","answer":"Use a reproducible multi-region pattern: ADF with a self-hosted IR to ingest CDC from on-prem SQL Server into region ADLS Gen2; Databricks to MERGE incremental rows into Delta Lake with schema evoluti","explanation":"## Why This Is Asked\nTests end-to-end data product design across regions, including CDC ingestion, Delta Lake success criteria, governance, and secure sharing patterns.\n\n## Key Concepts\n- Cross-region data products with incremental loads\n- CDC ingestion patterns and Delta Lake MERGE for upserts\n- Schema drift tolerance and evolution strategies\n- Purview lineage, classifications, and data contracts\n- Delta Sharing and Synapse external tables for controlled access\n\n## Code Example\n```javascript\nfrom delta.tables import DeltaTable\n# example: MERGE into Delta Lake in Databricks\ndeltaTable = DeltaTable.forPath(spark, '/mnt/delta/regionA/sales')\ndeltaTable.alias('t').merge(\n  source=df.alias('s'),\n  condition='t.id = s.id'\n).whenMatchedUpdate(set={'amount':'s.amount','modified':'current_timestamp()'}).whenNotMatchedInsert(values={'id':'s.id','amount':'s.amount','modified':'current_timestamp()'}).execute()\n```\n\n## Follow-up Questions\n- How would you validate data contracts across regions and rollback on contract violations?\n- What monitoring and alerting would you implement to detect latency or schema drift issues in near real-time?","diagram":"flowchart TD\n  A[On‑prem CDC] --> B[ADF Self-hosted IR]\n  B --> C[ADLS Gen2 Region]\n  C --> D[Databricks Delta Lake MERGE]\n  D --> E[Synapse External Tables]\n  E --> F[Purview Data Contracts/Lineage]","difficulty":"advanced","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Google","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-19T09:43:11.066Z","createdAt":"2026-01-19T09:43:11.068Z"},{"id":"q-4278","question":"Design a real-time feature store on Azure for a globally distributed ride-hailing platform: ingest streaming vehicle telemetry from Azure IoT Hub into ADLS Gen2, compute offline features with Databricks Delta Lake, and serve online features from Cosmos DB. Ensure feature versioning, schema evolution, data quality checks, end-to-end lineage via Purview, and multi-region consistency with latency constraints. What would you implement and why?","answer":"Ingress via IoT Hub to Event Hubs; Databricks Structured Streaming computes offline features stored in Delta Lake on ADLS Gen2; Databricks Feature Store handles feature definitions and versioning; Cos","explanation":"## Why This Is Asked\nIngesting streaming data, building a versioned feature store, and ensuring governance at scale across regions is a core, challenging skill.\n\n## Key Concepts\n- Databricks, Delta Lake, Delta Live Tables\n- Databricks Feature Store, online/offline separation\n- IoT Hub, Event Hubs, Structured Streaming\n- Purview lineage, Unity Catalog RBAC\n- Multi-region consistency, feature versioning\n\n## Code Example\n```python\n# PySpark sketch: define a versioned feature and write to offline/online stores\n```\n\n## Follow-up Questions\n- How would you detect and alert on feature drift across regions?\n- How would you upgrade feature schemas without breaking consumers?\n- What tests validate data quality at ingest and feature compute stages?","diagram":"flowchart TD\n  A[IoT Hub] --> B[Event Hubs/Stream]\n  B --> C[Databricks Structured Streaming]\n  C --> D[Delta Lake offline (ADLS Gen2)]\n  C --> E[Databricks Feature Store (versioned)]\n  D --> F[Cosmos DB online store]\n  G[Purview] --> H[Lineage]\n  I[Unity Catalog] --> J[RBAC]","difficulty":"advanced","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["NVIDIA","Snowflake","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-19T11:29:49.718Z","createdAt":"2026-01-19T11:29:49.718Z"},{"id":"q-4439","question":"You receive daily CSV and JSON logs from two partners via SFTP and REST, landing into ADLS Gen2 under /landing. Schemas drift over time. Design a beginner Data Factory pipeline that stages raw data, handles drift with Parquet schema evolution, loads to a simple star schema in Azure Synapse, and captures lineage in Purview plus basic data quality checks. What components and steps would you implement?","answer":"Two ingestion paths in Data Factory: Copy Activities for SFTP and REST to ADLS landing. A Data Flow normalizes to a common Parquet schema with drift-tolerant fields, then load to Azure Synapse star sc","explanation":"## Why This Is Asked\nTests ability to design a beginner pipeline with multiple data sources, drift handling, and governance.\n\n## Key Concepts\n- Azure Data Factory pipelines, Copy Activity, Data Flow\n- Parquet with schema evolution for drift\n- Azure Synapse (dim/fact) loading\n- Purview for data lineage; basic data quality checks\n\n## Code Example\n```sql\nSELECT partner, COUNT(*) AS recs\nFROM staging.sales_raw\nGROUP BY partner;\n```\n\n## Follow-up Questions\n- How would you extend for new fields without downtime?\n- How would you validate Purview lineage after runs?","diagram":null,"difficulty":"beginner","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Hugging Face","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-19T19:00:58.617Z","createdAt":"2026-01-19T19:00:58.617Z"},{"id":"q-4542","question":"You operate a global data lake on Azure hosting per-tenant telemetry from multiple SaaS apps via Event Hubs into ADLS Gen2. Each tenant must have isolated data, incremental loads with late-arriving events, and fast analytics in Power BI/Synapse. Propose an end-to-end design using Delta Lake on Databricks, per-tenant namespaces, RBAC, data drift handling, and lineage with Purview. What components, schemas, and processes would you implement to satisfy isolation, consistency, and cost constraints?","answer":"Delta Lake on ADLS Gen2 with Databricks Unity Catalog for per-tenant isolation, implementing separate databases per tenant within a unified catalog. Ingest streaming data from Event Hubs using Autoloader into a bronze layer, then apply MERGE operations with watermarks into silver tables for incremental loads with late-arriving event handling. Implement per-tenant namespaces in Unity Catalog with RBAC for granular access control, leverage Delta Lake ACID transactions for consistency, and configure retention policies for cost optimization. Enable Azure Purview for end-to-end lineage tracking and data drift detection through automated schema monitoring. Serve analytics via Synapse Link and Power BI DirectQuery over Delta tables with materialized views for optimal performance.","explanation":"## Why This Is Asked\nMulti-tenant isolation, drift handling, and end-to-end lineage in a global data lake architecture. Cost-aware streaming with late data requires concrete trade-offs and robust design patterns.\n\n## Key Concepts\n- Delta Lake, Unity Catalog, Autoloader, MERGE operations, watermarking\n- Azure Purview lineage, RBAC, retention policies\n- ADLS Gen2, Synapse/Power BI integration, cost controls\n\n## Code Example\n```python\n# PySpark pseudo-code for upsert with watermark\nfrom delta.tables import DeltaTable\nbronze = spark.readStream.format('delta').load('/bronze')\nsilver_path = '/silver'\n```","diagram":null,"difficulty":"intermediate","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["OpenAI","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T06:15:39.312Z","createdAt":"2026-01-19T22:53:09.635Z"},{"id":"q-4583","question":"Design an Azure data pipeline to ingest 100k events/sec from Azure Event Hubs into a Delta Lake on ADLS Gen2, with schema drift handled via Azure Schema Registry, end-to-end lineage in Purview, and upserts into Delta Lake using Spark (Synapse or Databricks). Address late events, out-of-order data, and cost with auto-scaling. What components and steps would you implement?","answer":"Implement a Spark Structured Streaming solution (Synapse or Databricks) to consume events from Azure Event Hubs, validate schemas through Azure Schema Registry integration, and perform upserts into Delta Lake on ADLS Gen2 using MERGE operations on the business key. Enable dynamic schema evolution via Schema Registry, apply watermarking to handle late events, and leverage checkpointing for exactly-once processing guarantees. Configure auto-scaling for Spark clusters and Event Hubs throughput units to optimize costs while maintaining required throughput.","explanation":"## Why This Is Asked\nAssess practical streaming lakehouse design capabilities including schema registry integration, Delta Lake upserts, Purview lineage tracking, late data handling, and cost optimization strategies.\n\n## Key Concepts\n- Spark Structured Streaming with Event Hubs connector\n- Azure Schema Registry for schema drift management\n- Delta Lake on ADLS Gen2 with MERGE-based upserts\n- Purview lineage integration for end-to-end tracking\n- Watermarking for late event handling and checkpointing for exactly-once semantics\n- Auto-scaling configurations for cost-effective throughput management\n\n## Code Example\n```javascript\n// Pseudo: upsert into Delta Lake from a micro-batch\nfunction upsert(deltaTablePath, batch) {\n  // merge on business_key\n  deltaTable.as('target')\n    .merge(batch.as('source'), 'target.business_key = source.business_key')\n    .whenMatched().updateAll()\n    .whenNotMatched().insertAll()\n    .execute();\n}\n```","diagram":"flowchart TD\n  A[Event Hubs] --> B[Azure Schema Registry]\n  B --> C[Spark Structured Streaming]\n  C --> D[Delta Lake on ADLS Gen2]\n  D --> E[Purview lineage]","difficulty":"intermediate","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Instacart"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T05:54:53.302Z","createdAt":"2026-01-20T02:35:26.737Z"},{"id":"q-4629","question":"Context: You’re building an Azure data platform for a global ad-tech customer. Data arrives from partners as CSV via SFTP, JSON via REST, and a streaming source via Event Hubs. Ingest into ADLS Gen2, publish a Delta Lake lakehouse, and expose data products with end-to-end lineage in Purview. You must enforce per‑product data contracts, handle schema drift, and keep upserts into a star schema in Synapse. Design the pipeline, contracts, and monitoring strategy?","answer":"Architect a pipeline with ADLS Gen2 landing, Databricks Delta Lake upserts into a Synapse star schema, and Purview for end-to-end lineage and per‑product data contracts. Ingest SFTP CSV, REST JSON, an","explanation":"## Why This Is Asked\nTests end-to-end Azure data platform design: multi-source ingestion, lakehouse with Delta Lake, data contracts per product, lineage via Purview, and practical handling of drift and upserts.\n\n## Key Concepts\n- Delta Lake upserts into Synapse lakehouse\n- Data contracts per product to govern schemas and metadata\n- Drift handling with versioned schemas and schema evolution\n- Multi-source ingestion: SFTP, REST, Event Hubs\n- Purview for lineage, governance, and access contracts\n- Monitoring, alerting, and cost optimization\n\n## Code Example\n```python\n# PySpark upsert example (Delta Lake)\nfrom delta.tables import DeltaTable\n\ndelta_path = \"/data/facts/delta\"\nnew_df = spark.read.format(\"json\").load(\"/path/to/new_facts.json\")\ndelta = DeltaTable.forPath(spark, delta_path)\ndelta.alias(\"t\").merge(\n  new_df.alias(\"s\"),\n  \"t.id = s.id\"\n).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n```\n\n## Follow-up Questions\n- How would you test and enforce cross‑product data contracts in Purview?\n- How would you handle late-arriving data and schema drift across regions?","diagram":"flowchart TD\n  A[Partner data] --> B[Landing ADLS Gen2]\n  B --> C[Databricks Delta Lake]\n  C --> D[Synapse Star Schema]\n  D --> E[Purview Lineage & Data Products]\n","difficulty":"intermediate","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","LinkedIn","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T05:47:07.572Z","createdAt":"2026-01-20T05:47:07.572Z"},{"id":"q-4749","question":"Design an end-to-end pipeline to ingest real-time CDC from an on-prem SQL Server into ADLS Gen2, then populate a star schema in Azure Synapse. The solution must handle schema drift, late-arriving data, and ensure end-to-end lineage in Purview. Specify components, data formats, upsert strategy, partitioning, and validation/monitoring?","answer":"Use Databricks Structured Streaming to read SQL Server CDC via a CDC connector, write to Delta Lake on ADLS Gen2 with schema evolution enabled, then MERGE into a star schema in Azure Synapse. Preserve","explanation":"## Why This Is Asked\n\nThis question tests practical design of real-time CDC pipelines across on-prem to Azure, handling drift, late data, upserts, and governance.\n\n## Key Concepts\n\n- CDC ingestion, Spark Structured Streaming, Delta Lake schema evolution, MERGE upserts, Purview lineage, watermarking, late data handling, idempotent writes, monitoring.\n\n## Code Example\n\n```sql\nMERGE INTO delta.`/datalake/sa/star/fact_sales` AS target\nUSING updates AS source\nON target.sale_id = source.sale_id\nWHEN MATCHED THEN UPDATE SET target.* = source.*\nWHEN NOT MATCHED THEN INSERT *;\n```\n\n## Follow-up Questions\n\n- How would you validate lineage end-to-end across sources and sinks in Purview?\n- How would you implement backfill for late data without duplicate counts?","diagram":"flowchart TD\n  A[On-prem SQL Server CDC] --> B[Databricks Structured Streaming]\n  B --> C[Delta Lake on ADLS Gen2]\n  C --> D[Azure Synapse Star Schema]\n  C --> Purview[Purview lineage]\n  D --> Monitor[Azure Monitor / Alerts]","difficulty":"advanced","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T10:50:14.556Z","createdAt":"2026-01-20T10:50:14.556Z"},{"id":"q-4768","question":"Scenario: design a real-time analytics pipeline for a global retailer. Ingest events from Event Hubs into ADLS Gen2 as JSON; auto-evolve schema; enforce per-tenant isolation and dynamic masking; capture end-to-end lineage in Purview; feed near real-time dashboards in Synapse within 5 seconds. Describe dataflow, components, schema governance, masking, latency monitoring, and cost/trade-offs of Parquet vs Delta with streaming?","answer":"Event Hubs -> ADLS Gen2 landing (JSON) with strict schema on read; Databricks Structured Streaming writes to Delta Lake with auto schema evolution and MERGE UPSERT into a tenant-scoped fact table; per","explanation":"## Why This Is Asked\\n\\nTests ability to design a compliant, low-latency lakehouse with streaming, governance, and tenant isolation. It blends data engineering, cost optimization, and operational monitoring.\\n\\n## Key Concepts\\n\\n- Real-time ingestion and Delta Lake upserts\\n- Schema drift handling and evolution\\n- Tenant isolation and dynamic masking\\n- Purview lineage and RBAC governance\\n- Latency monitoring and cost trade-offs\\n\\n## Code Example\\n\\n```javascript\\n// Example merge pattern (conceptual)\\nMERGE INTO target t USING source s ON t.id = s.id WHEN MATCHED THEN UPDATE SET t.value = s.value;\\n```\\n\\n## Follow-up Questions\\n\\n- How would you validate latency under backpressure?\\n- How would you scale to 10x event rate while preserving SLAs?","diagram":null,"difficulty":"advanced","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Microsoft","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T11:39:40.833Z","createdAt":"2026-01-20T11:39:40.834Z"},{"id":"q-940","question":"Daily CSV exports arrive in an ADLS Gen2 container at /incoming/sales/. Files may evolve over time as columns drift. Build a beginner pipeline using Data Factory to stage raw data, infer/handle schema changes, and load a simple star schema in Azure Synapse Analytics. Include a watermark-based incremental load and basic scheduling. Which services and steps would you implement?","answer":"Use Azure Data Factory: Copy to land raw CSVs as Parquet in ADLS Gen2, Data Flow with allowSchemaDrift to map evolving columns to a fixed star-schema in Azure Synapse, sink into a dedicated SQL pool. ","explanation":"## Why This Is Asked\nTests end-to-end basics: file landing, simple schema evolution, incremental loads, and reporting schema in Synapse.\n\n## Key Concepts\n- Data Factory pipelines: Copy, Data Flow, Triggers\n- ADLS Gen2 and Parquet landing\n- Synapse star schema design and incremental loads\n- Watermark-based change capture\n\n## Code Example\n```javascript\n// JSON-like skeleton of pipeline stages\n{\n  name: 'SalesIngest',\n  activities: [\n    {copy: 'landing/parquet'},\n    {dataFlow: 'map-to-star', drift: true},\n    {sink: 'Synapse.star'},\n    {trigger: 'tumblingWindow', interval: 1}\n  ]\n}\n```\n\n## Follow-up Questions\n- How would you handle late-arriving data?\n- How would you monitor data quality and failures?","diagram":"flowchart TD\n  A[CSV arrives] --> B[Copy to landing Parquet]\n  B --> C[Data Flow: map to star schema]\n  C --> D[Sink to Synapse star schema]\n  D --> E[Incremental load by watermark]\n  E --> F[Update metadata log]","difficulty":"beginner","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Amazon","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T16:30:40.009Z","createdAt":"2026-01-12T16:30:40.009Z"},{"id":"q-980","question":"Design a global streaming-to-batch data pipeline for a PayPal/Adobe/Netflix-scale analytics platform: ingest real-time clickstream from Event Hubs into ADLS Gen2, maintain a Delta Lake with drift-tolerant schema, mask PII at ingestion, expose masked aggregates via serverless SQL pool, and enforce end-to-end lineage with Purview while supporting cross-region DR and data residency. Which Azure components and patterns would you use, and how would you handle schema evolution and failure modes?","answer":"Ingest raw clickstream from Event Hubs into ADLS Gen2 staging, use Spark Structured Streaming to write to Delta Lake with drift-aware schema evolution, apply a masking UDF before storing in curated zo","explanation":"## Why This Is Asked\nThis probes practical data governance, real-time-to-batch pipelines, and DR considerations in Azure for large-scale workloads.\n\n## Key Concepts\n- Event Hubs ingestion and Spark Structured Streaming\n- Delta Lake with schema drift handling\n- PII masking at ingestion\n- Purview lineage and data residency compliance\n- Cross-region DR and serverless analytics\n\n## Code Example\n```javascript\n// PII masking pseudo-code\nfunction maskPII(record) {\n  if (!record?.email) return record;\n  record.email = record.email.replace(/[^@]+@/, '*****@');\n  return record;\n}\n```\n\n## Follow-up Questions\n- How would you validate end-to-end lineage across regions?\n- What are failure modes in cross-region DR and mitigations?","diagram":"flowchart TD\n  A[Event Hubs] --> B[ADLS Gen2 staging]\n  B --> C[Delta Lake curated]\n  C --> D[Serverless SQL pool dashboards]\n  A --> E[Purview lineage]\n  F[Geo-DR] --> G[Region failover]","difficulty":"intermediate","tags":["azure-data-engineer"],"channel":"azure-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Netflix","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T17:44:06.541Z","createdAt":"2026-01-12T17:44:06.541Z"}],"subChannels":["general"],"companies":["Adobe","Airbnb","Amazon","Anthropic","Apple","Bloomberg","Citadel","Cloudflare","Coinbase","Databricks","Discord","DoorDash","Goldman Sachs","Google","Hashicorp","Hugging Face","IBM","Instacart","LinkedIn","Lyft","Meta","Microsoft","MongoDB","NVIDIA","Netflix","OpenAI","Oracle","PayPal","Plaid","Robinhood","Salesforce","Scale Ai","Slack","Snap","Snowflake","Square","Stripe","Tesla","Twitter","Two Sigma","Uber","Zoom"],"stats":{"total":53,"beginner":17,"intermediate":15,"advanced":21,"newThisWeek":47}}