{"questions":[{"id":"gh-26","question":"What are the essential Linux commands every DevOps engineer should master for system administration, troubleshooting, and automation?","answer":"Essential Linux commands span file operations (ls, cp, mv, rm, find), text processing (grep, sed, awk, cut, sort, uniq), process management (ps, top, htop, kill, nice, jobs), network diagnostics (netstat, ss, curl, wget, ssh, nc), permissions (chmod, chown, chgrp, setfacl), package management (apt, yum, dnf, snap, flatpak), system monitoring (df, du, free, iostat, vmstat, lsof), and containerization tools (docker, podman, systemd, journalctl), forming the comprehensive foundation for modern DevOps automation and system administration.","explanation":"## Interview Context\nThis question evaluates practical Linux proficiency for DevOps engineers, testing knowledge beyond basic commands to include modern infrastructure tools and best practices.\n\n## Key Areas to Cover\n- **File System Operations**: Advanced navigation with find, locate, which; manipulation with dd, rsync; permissions including ACLs and extended attributes\n- **Process Management**: Real-time monitoring with htop, process control with kill signals, job scheduling with cron and systemd timers\n- **Network Tools**: Connection monitoring with ss, packet analysis with tcpdump, service discovery with dig/nslookup\n- **Text Processing**: Advanced pattern matching with regex, data transformation pipelines, log analysis techniques\n- **System Monitoring**: Performance metrics with sar, resource tracking, disk usage analysis\n- **Container Orchestration**: Docker commands for image/container lifecycle, systemd service management, journalctl for log analysis\n- **Security**: User management, audit trails, SELinux/AppArmor basics\n\n## Example Responses\nDemonstrate chaining commands like `ps aux | grep nginx | awk '{print $2}' | xargs kill -9`, or Docker workflows: `docker build -t app . && docker run -d --restart=unless-stopped app`. Show understanding of systemd unit files and service management commands.","diagram":"\ngraph TD\n    Linux --> Files[File Ops: ls, cp, mv]\n    Linux --> Sys[System: top, df, ps]\n    Linux --> Text[Text: grep, awk, sed]\n","difficulty":"beginner","tags":["linux","shell"],"channel":"linux","subChannel":"commands","sourceUrl":null,"videos":{"shortVideo":"https://www.youtube.com/watch?v=zB_3FIGRWRU"},"companies":["Amazon","Cloudflare","Google","Hashicorp","Microsoft","Netflix"],"eli5":"Imagine you're the boss of a giant playground with lots of toys and games. Linux commands are like your magic words to control everything! 'ls' is like looking at all your toys on the shelf. 'cp' is like making copies of your favorite drawings. 'mv' is like moving toys to different boxes. 'grep' is like finding specific words in your storybooks. 'ps' is like checking which friends are playing on the swings. 'curl' is like sending secret messages to other playgrounds. 'chmod' is like deciding who can play with which toys - you, your friends, or everyone. These magic words help you keep the playground running smoothly, fix broken toys, and make games happen automatically while you eat snacks!","relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-29T07:00:08.286Z","createdAt":"2025-12-26 12:51:05"},{"id":"q-1000","question":"A Linux host runs several Docker containers; during peak load, API responses slow and some requests time out. Describe a beginner-friendly, concrete diagnostic workflow to (1) confirm whether host saturation is CPU, memory, or I/O, (2) identify the container most responsible, and (3) apply a safe mitigation (e.g., graceful restart or CPU/memory throttling) while monitoring impact. Include exact commands and expected outputs?","answer":"Use top/vmstat to identify bottleneck, then docker stats --no-stream to spot the hogging container. Correlate with docker ps. If CPU or memory is the issue, throttle with docker update --cpu-shares 51","explanation":"## Why This Is Asked\nThis tests practical, beginner-friendly diagnostic thinking for containerized workloads.\n\n## Key Concepts\n- Metrics: CPU/memory/IO\n- Per-container accounting via docker stats\n- Safe mitigations: CPU shares, memory limits, graceful restarts\n\n## Code Example\n```javascript\ntop -b -n1\ndocker stats --no-stream\n```\n\n## Follow-up Questions\n- How would you verify the mitigation actually improved latency?\n- What are edge cases when throttling hurts user experience?","diagram":null,"difficulty":"beginner","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","LinkedIn"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T18:45:16.170Z","createdAt":"2026-01-12T18:45:16.170Z"},{"id":"q-1142","question":"Scenario: A Linux server hosting a small web service used by customers suddenly shows disk usage climbing on the root filesystem. Provide a beginner-friendly, concrete diagnostic workflow to (1) locate the directories/files consuming the most space, (2) identify top offenders, and (3) apply a safe mitigation (e.g., rotate/compress/archive logs or purge old data) while keeping the service up. Include exact commands and expected outputs?","answer":"Check space: df -h. Find large files/dirs: du -hx --max-depth=1 / 2>/dev/null | sort -rh | head -n 5. Focus on /var/log: du -xh /var/log | sort -rh | head -n 5. Mitigate safely: tar czf /backup/logs-$","explanation":"## Why This Is Asked\n\nAssesses practical diagnostic skills for a common sysadmin pain point: disk pressure. Tests ability to use core CLI tools and perform safe, non-downtime mitigations.\n\n## Key Concepts\n\n- Disk usage analytics with df and du\n- Identifying large offenders efficiently\n- Safe log rotation/archiving and selective truncation\n- Lightweight monitoring after mitigation\n\n## Code Example\n\n```bash\n# Safe archival of logs\nLOG_ARCHIVE=/backup/logs-$(date +%F).tar.gz\ntar czf \"$LOG_ARCHIVE\" /var/log\n\n# Purge old logs (age > 30 days)\nfind /var/log -type f -iname '*.log' -mtime +30 -delete\n```\n\n## Follow-up Questions\n\n- How would you adapt this for a system with logrotation already in place?\n- What risks exist when deleting or truncating logs, and how to mitigate?","diagram":null,"difficulty":"beginner","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Google","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T01:28:46.981Z","createdAt":"2026-01-13T01:28:46.981Z"},{"id":"q-1205","question":"You're operating a fleet of Linux hosts running a low-latency web API. Intermittent 100–200 ms latency spikes appear under load. Design a practical, end-to-end diagnostic plan using eBPF/BPFtrace or perf, iostat/vmstat, and container metrics to collect data, identify root cause (CPU scheduling, IO wait, or network), and propose minimal-disruption mitigations?","answer":"Baseline with iostat -dx 1, vmstat 1, and kubectl top; use eBPFtrace to map per-process IO latency: tracepoint:block:block_rq_issue { @s[pid] = nsecs; } tracepoint:block:block_rq_complete { @d[pid] = ","explanation":"## Why This Is Asked\nDemonstrates practical, instrumented thinking for production Linux latency issues using modern tracing tools.\n\n## Key Concepts\n- eBPF/BPFtrace, perf, ftrace\n- IO schedulers, MQ, quotas\n- Container metrics and baseline-vs-spike analysis\n\n## Code Example\n```bash\ntracepoint:block:block_rq_issue { @s[pid] = nsecs; }\ntracepoint:block:block_rq_complete { @d[pid] = hist(nsecs - @s[pid]); }\n```\n\n## Follow-up Questions\n- How would you extend this to a multi-tenant cluster?\n- What are the risks of tracing in production and how would you mitigate them?","diagram":null,"difficulty":"advanced","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Salesforce","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T04:53:37.702Z","createdAt":"2026-01-13T04:53:37.702Z"},{"id":"q-1244","question":"Scenario: A Linux host runs a MongoDB primary in a high-traffic cluster. During peak hours, latency spikes and tail latency increases while CPU and memory appear stable. Using only default tooling, no downtime, describe a concrete, actionable diagnostic workflow to (1) determine if I/O wait, CPU, or memory is the bottleneck, (2) pinpoint the offending disk/device or process, and (3) apply a safe mitigation (e.g., adjust I/O scheduler, tune dirty writeback parameters, or throttle MongoDB) while monitoring impact. Include exact commands and expected outputs?","answer":"Run vmstat, iostat, and top in batch to correlate latency with system metrics, then pidstat to isolate the offender. If disk I/O wait dominates, switch a block device’s scheduler to deadline/bfq and a","explanation":"## Why This Is Asked\nThis probes practical Linux performance diagnosis under live load, focusing on I/O bottlenecks and safe mitigations in a MongoDB context.\n\n## Key Concepts\n- I/O wait, disk scheduling, and writeback pressure\n- Process-level bottleneck identification with pidstat/top\n- Safe live mitigations (scheduler changes, sysctl tweaks, application-level throttling)\n\n## Code Example\n```javascript\n// Simple Node.js snippet to run common diagnostics sequentially\nconst {execSync} = require('child_process');\nconsole.log(execSync('vmstat 1 5; iostat -xz 1 5; top -bn1', {encoding: 'utf8'}));\n```\n\n## Follow-up Questions\n- How would you validate the effectiveness of the scheduler change?\n- What risks exist when adjusting dirty_writeback_ratio on a live DB?","diagram":"flowchart TD\n  Start[Start] --> Metrics[Collect metrics: vmstat, iostat, top]\n  Metrics --> Bottleneck{Bottleneck detected?}\n  Bottleneck -->|Yes| Isolate[Identify device/process via pidstat / proc/diskstats]\n  Bottleneck -->|No| End[End]\n  Isolate --> Mitigate[Apply safe mitigation: scheduler or sysctl]\n  Mitigate --> Verify[Reobserve metrics to confirm]\n  Verify --> End","difficulty":"intermediate","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Databricks","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T06:38:24.003Z","createdAt":"2026-01-13T06:38:24.003Z"},{"id":"q-1266","question":"Context: Linux node in a Kubernetes cluster hosting a high-throughput data ingestion service. Intermittent tail latency spikes (>1s) appear during peak traffic, affecting processing. Without downtime, design a concrete troubleshooting workflow to (1) confirm whether CPU, I/O, or network is the bottleneck, (2) identify the exact subsystem or process responsible, and (3) implement a safe mitigation with minimal impact while maintaining observability. Include exact commands and realistic outputs?","answer":"Begin with per-second metrics: iostat -xz 1 2; mpstat -P ALL 1; vmstat 1. Identify hot processes with pidstat -p ALL 1 | head. If IO lag appears, run perf stat -p <pid> -e cycles,instructions,cache-re","explanation":"## Why This Is Asked\nAssesses practical, end-to-end debugging in real-world Linux systems, including multi-subsystem correlation and safe mitigations with minimal downtime.\n\n## Key Concepts\n- Tail latency diagnosis across CPU, IO, and network\n- Per-second monitoring with iostat/mpstat/vmstat\n- Process-level tracing with pidstat and perf\n- I/O schedulers and CPU pinning as mitigations\n\n## Code Example\n```javascript\n// Example: parse perf output snapshot (illustrative)\nfunction parsePerf(line){ /* simplistic parser */ }\n```\n\n## Follow-up Questions\n- How would you adapt this workflow for containerized workloads (Kubernetes, CNI, cgroups)?\n- How would you validate the mitigation under synthetic peak load?","diagram":null,"difficulty":"intermediate","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Anthropic","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T07:32:47.290Z","createdAt":"2026-01-13T07:32:47.290Z"},{"id":"q-1364","question":"A Linux host running multiple microservices behind NGINX exhibits intermittent latency spikes; new connections occasionally fail and FD usage appears high. Using only default tools, describe a beginner-friendly workflow to (1) confirm FD exhaustion is the bottleneck, (2) identify the offending process by per‑process FD usage, and (3) apply a safe mitigation (increase NOFILE limits, adjust per-service limits, or set a systemd limit) while monitoring impact. Include exact commands and expected outputs?","answer":"Check limits and usage: cat /proc/sys/fs/file-max; ulimit -n; grep OpenFiles /proc/$(pgrep -f nginx)/limits 2>/dev/null. Identify top FDs: for p in /proc/[0-9]*; do [ -r \\\"$p/fd\\\" ] && echo -n \\\"${p##","explanation":"## Why This Is Asked\nFD exhaustion can silently throttle connections under load. This question probes practical techniques to verify limits, count per‑process FDs, and apply safe, scoped mitigations without downtime.\n\n## Key Concepts\n- File descriptor limits (nofile, file-max, nr_open)\n- /proc/<pid>/limits and per‑process FD counts\n- systemd LimitNOFILE and limits.conf\n- Safe, incremental changes and live monitoring\n\n## Code Example\n```javascript\n// Example Node.js utility to count open fds for a pid\nconst fs = require('fs');\nconst pid = process.argv[2];\nconsole.log('Open fds for', pid, ':', fs.readdirSync(`/proc/${pid}/fd`).length);\n```\n\n## Follow-up Questions\n- How would you automate per-service FD limits with systemd?\n- What are risks of increasing global limits?","diagram":"flowchart TD\n  A[FD exhaustion suspected] --> B[Check system limits]\n  B --> C[Count per-process FDs]\n  C --> D[Mitigate via limits or systemd]\n  D --> E[Monitor impact]","difficulty":"beginner","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","PayPal","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T13:17:53.743Z","createdAt":"2026-01-13T13:17:53.743Z"},{"id":"q-1410","question":"On a Linux host, a data-processing job occasionally stalls during bursts. Using only default tooling and no downtime, outline a concrete, beginner-friendly diagnostic flow to (1) confirm CPU, memory, or I/O bound, (2) identify the offending process and its operation, and (3) apply a safe mitigation (e.g., adjust priority, pause/resume, or throttle) with monitoring. Which commands would you run and what outputs would you expect?","answer":"Run top or ps to locate high CPU/mem processes, then vmstat 1 5 to watch IO and swap, and iostat -dx 1 5 to confirm IO waits. Note PID and cmd. Mitigate by renice -n 10 -p <pid> or pause with kill -SI","explanation":"## Why This Is Asked\nThis checks practical triage using default tools and safe mitigations. It also tests how the candidate identifies bottlenecks and minimizes disruption.\n\n## Key Concepts\n- Resource bottlenecks vs bursts\n- Observability: top/ps, vmstat, iostat\n- Process control: renice, SIGSTOP/SIGCONT\n\n## Code Example\n```bash\ntop -b -n1 | head\nps -eo pid,ppid,cmd,%cpu --sort=-%cpu | head\nvmstat 1 5\niostat -dx 1 5\nrenice -n 10 -p <pid>\n```\n\n## Follow-up Questions\n- What if iostat isn't installed?\n- How would you verify impact after mitigation?","diagram":"flowchart TD\n  A[Start] --> B{Identify bottleneck}\n  B --> C[CPU]\n  B --> D[Memory]\n  B --> E[IO]\n  C --> F[Lower priority]\n  D --> G[Tune cache/allocations]\n  E --> H[Adjust IO scheduler]","difficulty":"beginner","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","OpenAI","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T15:50:44.422Z","createdAt":"2026-01-13T15:50:44.422Z"},{"id":"q-1424","question":"Scenario: a high-traffic Linux service behind a reverse proxy experiences sporadic tail latency spikes during peak hours. CPU and IO look normal in aggregate. Design a practical end-to-end diagnostic using eBPF to identify root causes quickly: (1) instrument per-request latency across kernel and user-space, (2) attribute latency to network, disk, or app code, (3) propose safe mitigations and validate impact. Include specific probes, sample commands, and expected outputs?","answer":"Use an end-to-end eBPF trace with bpftrace: attach kprobes/kretprobes on accept, recv, and send, plus the HTTP handler; store start times per tid and compute deltas to build a latency histogram keyed ","explanation":"## Why This Is Asked\nTests practical eBPF observability, end-to-end tracing, and ability to propose concrete mitigations in production-like settings.\n\n## Key Concepts\n- eBPF tracing for low-overhead observability\n- kprobes/kretprobes and tracepoints\n- Per-request latency histograms and map keying (tid, IP, URL)\n- Tail latency analysis and cross-layer attribution\n- Safe mitigations: TCP tuning, NIC offloads, queue depths\n\n## Code Example\n```javascript\n#!/usr/bin/env bpftrace\nBEGIN { @start[tid] = 0; }\ntracepoint:syscalls:sys_enter_accept { @start[tid] = nsecs; }\ntracepoint:syscalls:sys_exit_accept { @lat[tid] = nsecs - @start[tid]; delete(@start[tid]); }\n```\n\n## Follow-up Questions\n- How would you extend this to multi-node correlation with time sync?\n- What are risks of running eBPF in production and how would you mitigate them?","diagram":null,"difficulty":"advanced","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Cloudflare","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T16:46:26.464Z","createdAt":"2026-01-13T16:46:26.464Z"},{"id":"q-1445","question":"A Linux host running GPU-accelerated video processing containers shows intermittent frame latency spikes during peak load, with no obvious CPU/memory/I/O bottlenecks. Without downtime, describe a concrete, real-world diagnostic workflow using only default tools to (1) verify if latency is caused by page cache pressure or Transparent Huge Pages, (2) identify the subsystem or container involved, and (3) apply a safe mitigation (e.g., disable THP on affected NUMA nodes, tune swappiness, or pin tasks) with minimal impact and observable results. Include exact commands and expected outputs?","answer":"Workflow: verify spike with top/iostat, inspect THP and page cache, locate offending container, and apply a safe mitigation. Commands: cat /sys/kernel/mm/transparent_hugepage/enabled; vmstat 1 5; iost","explanation":"## Why This Is Asked\nAssesses hands-on Linux troubleshooting for latency with containerized workloads, focusing on memory reclamation, THP behavior, and safe, minimal-impact mitigations.\n\n## Key Concepts\n- Transparent Huge Pages (THP) behavior and toggling\n- Page cache vs memory pressure\n- Container isolation and CPU pinning\n- Default tooling for on-call debugging\n\n## Code Example\n```bash\n# Check THP state\ncat /sys/kernel/mm/transparent_hugepage/enabled\n# Observe memory/caching behavior\nvmstat 1 5\n# Disk and I/O detail\niostat -dx 1\n# Identify running containers\ndocker ps\n# See heavy processes inside a container\ndocker top <id>\n# Mitigation: disable THP and pin a pid to CPUs\nsudo tee /sys/kernel/mm/transparent_hugepage/enabled <<< 'never'\ntaskset -cp 0-7 <pid>\n```\n\n## Follow-up Questions\n- How would you validate no performance regression after disabling THP? \n- How could you automate this workflow for on-call use?\n","diagram":"flowchart TD\n  A[Latency Event] --> B[Collect Metrics]\n  B --> C[Check THP/Cache]\n  C --> D[Identify Container]\n  D --> E[Apply Mitigation]\n  E --> F[Monitor Metrics]","difficulty":"intermediate","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","DoorDash","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T17:38:30.594Z","createdAt":"2026-01-13T17:38:30.594Z"},{"id":"q-1570","question":"A Linux CI node running multiple git builds experiences occasional stalls during parallel jobs. Using only default tooling and no downtime, describe a concrete diagnostic workflow to (1) determine if CPU, memory, or I/O is the bottleneck, (2) identify the specific component (e.g., git, filesystem, network) causing the stall, and (3) apply a safe mitigation (e.g., throttle parallel jobs, adjust I/O scheduler, or raise file descriptor limits) while monitoring impact. Include exact commands and expected outputs?","answer":"Run `vmstat 1 60` and `iostat -xz 1 60`, then `top -b -n1` to identify CPU, memory, and I/O pressure. If iowait is elevated, map I/O to processes with `pidstat -d 1`. If a git/build task is causing the issue, throttle parallel jobs using `make -j$(nproc)` or adjust CI job concurrency.","explanation":"## Why This Is Asked\nTests practical skills in diagnosing common Linux bottlenecks using default tools and linking observations to concrete mitigations in a CI context.\n\n## Key Concepts\n- Baseline and interpret vmstat/iostat outputs\n- I/O wait (iowait) vs CPU\n- PID-level I/O mapping with pidstat\n- Safe mitigations: adjust concurrency, swappiness, fd limits, scheduler\n\n## Code Example\n```bash\nvmstat 1 60\niostat -xz 1 60\ntop -b -n1 | head\n```\n\n## Follow-up Questions\n- How would you verify that mitigation didn't degrade other workloads?\n- Which metrics would you log for post-change validation?","diagram":null,"difficulty":"beginner","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","Tesla","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T06:12:46.493Z","createdAt":"2026-01-13T22:33:22.994Z"},{"id":"q-1705","question":"You have a Linux host running a Rust-based data-processing daemon that stalls for 30–60 seconds under peak load. Using only default tooling, design a concrete diagnostic workflow to (1) determine if the stall is CPU-, I/O-, or memory-bound, (2) identify the exact subsystem or process causing the stall, and (3) apply a safe mitigation (e.g., cgroup throttling or IO-scheduler tweaks) while preserving observability. Include exact commands and expected outputs?","answer":"Begin with a system-wide view, then drill down:\n- top -b -n1\n- vmstat 1\n- iostat -xz 1\n- pidstat -p ALL 1\nCompare CPU idle, IO wait, and memory pressure. If IO-wait is high, enable BFQ (or deadline) a","explanation":"## Why This Is Asked\nTests practical Linux troubleshooting under realistic load, emphasizing observable symptoms, bottleneck identification, and safe mitigations that preserve service continuity.\n\n## Key Concepts\n- Distinguish CPU, IO, and memory bottlenecks via metrics\n- Map waits to processes with pidstat and vmstat\n- Safe mitigations: IO scheduler tuning, cgroups throttling, memory settings\n\n## Code Example\n```javascript\n// Shell-like workflow sketch (illustrative)\nconst steps = [\n  'top -b -n1',\n  'vmstat 1',\n  'iostat -xz 1',\n  'pidstat -p ALL 1'\n];\n```\n\n## Follow-up Questions\n- How would you revert mitigations if they worsen latency?\n- Which metrics indicate readiness for throttling vs. scheduler change?","diagram":"flowchart TD\n  A[Start] --> B[Collect metrics]\n  B --> C{Bottleneck?}\n  C -- CPU --> D[Throttle CPU via cgroups]\n  C -- IO --> E[Tune IO scheduler & limit writers]\n  C -- Mem --> F[Adjust swapiness/min_free_kbytes]\n  D --> G[Validate metrics]\n  E --> G\n  F --> G\n  G --> H[Continue monitoring]","difficulty":"intermediate","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Hugging Face","Lyft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T07:41:25.531Z","createdAt":"2026-01-14T07:41:25.531Z"},{"id":"q-1873","question":"Context: A Linux host runs a Kubernetes-deployed real-time inference service behind Nginx; latency tail spikes occur during bursts. Without downtime, describe a concrete, repeatable workflow to determine whether latency is caused by CPU throttling, memory pressure, or network queueing, identify the offending container/pod, and apply a safe mitigation (e.g., adjust cgroup limits, scale the deployment, or tune kernel parameters) while preserving observability. Include exact commands, expected outputs, and a simple rollback plan?","answer":"Start by capturing latency and per-pod resource use: kubectl top pod my-infer and kubectl top node. Inspect cgroups for the pod: read quotas with cat /sys/fs/cgroup/cpu/kubepods/.../cpu.stat and memor","explanation":"## Why This Is Asked\nTests practical troubleshooting in a containerized Linux environment, focusing on per-pod resource contention and observability. It differentiates CPU throttling, memory pressure, and network effects.\n\n## Key Concepts\n- Kubernetes resource requests/limits and QoS\n- Linux cgroups CPU and memory accounting\n- I/O and network bottlenecks in a container\n- Safe mitigation with observable rollback\n\n## Code Example\n```javascript\nkubectl top pod my-infer\nkubectl top node\ncat /sys/fs/cgroup/cpu/kubepods/.../cpu.stat\niostat -dx 1 5\ntc qdisc show\nss -tuln\npidstat -p ALL 1 5\n```\n\n## Follow-up Questions\n- How would you automate this workflow for future bursts?\n- What metrics indicate rollback safety vs. risk of thrashing?","diagram":"flowchart TD\n  A[Latency Spike] --> B{Check CPU}\n  B --> C[CPU throttling or limits]\n  A --> D{Check Memory}\n  D --> E[Memory pressure / OOM]\n  A --> F{Check Network}\n  F --> G[Queueing / NAPI]","difficulty":"intermediate","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Hugging Face","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T15:34:57.283Z","createdAt":"2026-01-14T15:34:57.283Z"},{"id":"q-2038","question":"Scenario: A Linux host running a batch queue occasionally fails to start new jobs with 'Too many open files' under moderate load. Without downtime, describe a concrete, beginner-friendly diagnostic workflow using default tools to (1) confirm FD limits are the bottleneck, (2) identify the process or user hitting the limit, and (3) apply a safe mitigation (e.g., raise per-user limits, adjust LimitNOFILE for the service) while keeping service available. Include exact commands and expected outputs?","answer":"To diagnose 'Too many open files' errors without downtime, follow this systematic workflow:\n\n**1. Confirm FD limits are the bottleneck:**\n```bash\nulimit -n                    # Check current user soft limit\npidof queue                  # Get the queue process PID\nlsof | wc -l                # Count total open files system-wide\ncat /proc/sys/fs/file-max   # Check system-wide maximum\ncat /proc/$(pidof queue)/limits | grep 'Max open files'  # Check process-specific limits\n```\n\n**2. Identify the process or user hitting the limit:**\n```bash\nlsof -p $(pidof queue) | wc -l                    # Count files opened by queue process\nps -eo pid,user,comm --sort=-pid | head -20        # List top processes by PID\nlsof -u $(whoami) | wc -l                          # Count files per current user\nss -tulpn | head -10                               # Check network connections consuming FDs\n```\n\n**3. Apply safe mitigation while keeping service available:**\n\nFor per-user limits:\n```bash\necho '* soft nofile 4096' >> /etc/security/limits.conf\necho '* hard nofile 8192' >> /etc/security/limits.conf\nulimit -n 4096                                      # Apply immediately for current session\n```\n\nFor systemd services:\n```bash\nsystemctl edit queue.service\n# Add: [Service] LimitNOFILE=8192\nsystemctl daemon-reload && systemctl restart queue.service\n```\n\nExpected outputs should show increased limits and reduced errors in `/var/log/syslog` or service logs.","explanation":"## Why This Is Asked\nTests understanding of Linux file descriptor management and practical troubleshooting techniques that maintain service availability.\n\n## Key Concepts\n- File descriptor limits (soft vs hard constraints)\n- Per-user versus system-wide resource management\n- systemd service configuration with LimitNOFILE\n- Runtime diagnostic tools: lsof, /proc filesystem, ulimit\n- Non-disruptive mitigation strategies\n\n## Code Example\n```bash\n# Quick diagnostic sequence\nulimit -n                          # Current soft limit\npidof batchd                       # Process identification\nlsof -p $(pidof batchd) | wc -l    # Process FD count\ncat /proc/$(pidof batchd)/limits   # Process limits view\n```\n\n## Follow-up Questions\n- How do container runtimes affect FD limits?\n- What monitoring would you implement for early detection?\n- When would you choose ephemeral versus permanent limit changes?","diagram":"flowchart TD\n  A[FD limit check] --> B{Limit hit?}\n  B -- Yes --> C[Raise per-user limits / LimitNOFILE]\n  B -- No --> D[Check other bottlenecks]\n  C --> E[Restart service and validate]\n","difficulty":"beginner","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Goldman Sachs","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T06:13:46.746Z","createdAt":"2026-01-14T21:44:38.759Z"},{"id":"q-2273","question":"Context: A Linux server hosts a real-time ingestion service and a Redis cache on the same box. During peak ingestion, tail latency spikes. Using only default tools, outline a concrete, reproducible workflow to (1) confirm whether CPU, IO, memory, or network is the bottleneck, (2) pinpoint the offender process or disk, and (3) apply a safe mitigation with minimal disruption. Include exact commands and expected outputs?","answer":"Run: top -b -n1; vmstat 1 5; cat /proc/loadavg; if iowait >5%, inspect /proc/diskstats and ps -eo pid,pcpu,pmem,cmd --sort=-pcpu | head. If IO bound, throttle ingestion via CPU cgroups: mkdir -p /sys/","explanation":"## Why This Is Asked\nAssesses practical diagnosis under contention using only core, universally-available tools. Emphasizes observability, root-cause isolation, and safe mitigation with minimal disruption.\n\n## Key Concepts\n- Resource bottleneck identification (CPU/IO/memory/network)\n- Per-process isolation via CPU cgroups\n- Kernel tunables (swappiness) and observability via /proc\n- Safe, incremental mitigation with immediate verification\n\n## Code Example\n```\ntop -b -n1\nvmstat 1 5\ncat /proc/loadavg\n```\n```\n# Example commands shown in discussion; real outputs will vary\n```\n\n## Follow-up Questions\n- How would you scale the approach for multiple ingestion workers?\n- What non-disruptive rollback steps would you take if latency worsens?","diagram":"flowchart TD\n  A[Start monitoring] --> B{Bottleneck detected?}\n  B -- CPU --> C[Check per-process CPU with ps/top]\n  B -- IO --> D[Inspect /proc/diskstats and /proc/disk/queue]\n  C --> E[Apply CPU cgroup throttling]\n  D --> E\n  E --> F[Adjust swappiness if memory pressure]\n  F --> G[Monitor latency via logs/health endpoints]\n  G --> H[Done]","difficulty":"intermediate","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","Slack","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T09:55:39.961Z","createdAt":"2026-01-15T09:55:39.961Z"},{"id":"q-2318","question":"Scenario: A Linux host in a high-throughput UDP ingestion pipeline experiences random latency spikes under peak traffic. Using only default tooling, design a concrete diagnostic workflow to (1) determine whether NIC, IRQs, or queue saturation is the bottleneck, (2) identify the offending interface/driver and IRQ affinity, and (3) apply a safe mitigation (e.g., adjust IRQ affinity, enable RSS, increase Rx queue depth) while preserving observability. Include exact commands and expected outputs?","answer":"Run: cat /proc/interrupts; ip -s link; for ethN: ethtool -S ethN; sar -n DEV 1 1; check /proc/net/dev for drops. If IRQs spike on ethN, assign its IRQs to dedicated CPUs: echo <mask> > /proc/irq/N/smp","explanation":"## Why This Is Asked\n\nTests ability to triage network path bottlenecks at the kernel level using built-in tools. Requires correlating NIC stats, interrupts, and queue depth to locate root cause without third-party software.\n\n## Key Concepts\n\n- NIC interrupt handling and RSS\n- IRQ affinity and CPU isolation\n- Rx/Tx queue depth and latency\n\n## Code Example\n\n```javascript\n#!/bin/bash\n# collect interrupts per NIC\ngrep -E 'eth|enp' /proc/interrupts\n# show per-interface stats\nfor i in /sys/class/net/*; do echo \"$i\"; cat \"$i/statistics/rx_bytes\"; done\n```\n\n## Follow-up Questions\n\n- How would you monitor after changes to ensure no regressions?\n- How would you adapt this in a multi-NIC, containerized environment?","diagram":"flowchart TD\n  A[Peak UDP ingestion] --> B{Check /proc/interrupts}\n  B --> C[High interrupts on NIC?]\n  C -->|Yes| D[Analyze IRQ affinity & RSS]\n  C -->|No| E[Check Rx/Tx queue depth]\n  D --> F[Apply safe mitigation]\n  E --> F\n  F --> G[Verify observability: latency & drops]","difficulty":"advanced","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Goldman Sachs","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T11:44:13.285Z","createdAt":"2026-01-15T11:44:13.285Z"},{"id":"q-2353","question":"A Linux host runs three LXC containers. During peak hours, one container's API latency spikes while others stay responsive. Using only default tooling, describe a concrete, beginner-friendly diagnostic workflow to (1) determine if bottleneck is CPU, memory, or I/O at host or container level, (2) identify the container and the exact process responsible for the spike, and (3) propose a safe mitigation (e.g., adjust container limits, throttle I/O, or relocate service) with observability. Include exact commands and expected outputs?","answer":"Run ps -eo pid,pcpu,pmem,cmd --sort=-pcpu | head -n 5. For each PID, check /proc/$PID/cgroup to map to the container. Then confirm per-container usage with systemd-cgtop -m 1 and systemd-cgls. If Cont","explanation":"## Why This Is Asked\n\nAssesses ability to triage container-level resource contention using native Linux tools, mapping system metrics back to the container boundary, and choosing safe, low-downtime mitigations.\n\n## Key Concepts\n\n- cgroups and per-container isolation\n- mapping processes to containers via /proc/$pid/cgroup\n- systemd-cgtop/systemd-cgls for per-container metrics\n- safe mitigations: CPU quotas, memory limits, IO throttling\n\n## Code Example\n\n```javascript\nps -eo pid,pcpu,pmem,cmd --sort=-pcpu | head -n 5\nfor pid in $(ps -eo pid --no-headers); do\n  cat /proc/$pid/cgroup 2>/dev/null\ndone | head -n 5\nsystemd-cgtop -m 1\n```\n\n## Follow-up Questions\n\n- If containers are not managed by systemd, how would you adapt?\n- How would you validate that mitigations had the intended effect without risking service downtime?","diagram":"flowchart TD\n  Host[/Host/] --> ContainerA[Container A]\n  ContainerA --> API[API Endpoint]\n  Host --> ContainerB[Container B]\n  ContainerB --> API","difficulty":"beginner","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","OpenAI","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T14:40:30.617Z","createdAt":"2026-01-15T14:40:30.617Z"},{"id":"q-2426","question":"Context: A Linux host runs multiple containers on a single NIC behind a load balancer. During a traffic spike, a critical API shows increased latency and occasional 5xx errors. Using only default Linux tools, outline a concrete, practical diagnostic workflow to (1) determine whether latency comes from CPU, memory pressure, disk I/O, or network contention, (2) identify the exact container or process responsible, and (3) apply a safe mitigation (e.g., throttle or move to a different cgroup, adjust IO scheduler) with minimal disruption and full observability. Include exact commands and expected outputs?","answer":"I’d start by collecting signals and mapping PIDs to containers: vmstat 1 5; iostat -xz 1; pidstat -ru 1; docker stats --no-stream. Then map PIDs to containers: grep -R '/docker/' /proc/$PID/cgroup. If","explanation":"## Why This Is Asked\nTests hands-on diagnostic reasoning for multi-tenant containers using only default tools, plus safe live mitigations without downtime.\n\n## Key Concepts\n- Container-to-process mapping via /proc/$PID/cgroup\n- Resource controllers: CPUQuota, memory.max\n- Observability: vmstat, iostat, pidstat, docker stats\n- Safe throttling to preserve availability\n\n## Code Example\n```\nvmstat 1 5\niostat -xz 1\npidstat -ru 1\ndocker stats --no-stream\ngrep -R '/docker/' /proc/$PID/cgroup\n```\n\n## Follow-up Questions\n- What if no container shows high CPU but latency persists?\n- How to verify no downtime after applying throttling (e.g., gradual ramp, canary container)?","diagram":"flowchart TD\n  A[Start] --> B[Collect signals]\n  B --> C[Map PIDs to containers]\n  C --> D[Identify bottleneck]\n  D --> E[Apply throttling or memory limits]\n  E --> F[Re-run load test]\n  F --> G[Validate results]","difficulty":"intermediate","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Netflix","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T17:46:29.845Z","createdAt":"2026-01-15T17:46:29.845Z"},{"id":"q-2598","question":"On a Linux host running a high-throughput data-ingestion service, intermittent 5–10s stalls appear under sustained load. Using only default tooling, design a concrete diagnostic workflow to determine whether stalls are caused by Transparent Huge Pages, memory fragmentation, or IO pressure; identify the exact subsystem responsible; and apply a safe mitigation (e.g., disable THP, adjust NUMA binding) while preserving observability. Include exact commands and expected outputs?","answer":"Use `iostat -xz 1 3` to monitor I/O wait metrics, `pidstat -p ALL 1` to identify CPU contention patterns, and `slabtop -s object` to observe memory allocation churn. Inspect THP configuration with `cat /sys/kernel/mm/transparent_hugepage/enabled` and check kernel messages via `dmesg | grep -i hugepage` for THP-related events.","explanation":"## Why This Is Asked\nTests practical ability to diagnose kernel memory behavior and I/O contention using built-in tools, focusing on THP impact and safe mitigations.\n\n## Key Concepts\n- Transparent Huge Pages (THP)\n- Memory fragmentation vs allocation failures\n- I/O wait vs CPU contention\n\n## Code Example\n```bash\n#!/bin/bash\necho \"THP status:\"\ncat /sys/kernel/mm/transparent_hugepage/enabled\necho \"I/O wait (sample):\"\niostat -xz 1 3\n```\n\n## Follow-up Questions\n- How to persist THP disable across reboots?\n- What factors could confound THP diagnostics (defragmentation, memory pressure, NUMA)?","diagram":"flowchart TD\n  A[Stall observed] --> B[Check THP status]\n  B --> C{THP enabled?}\n  C -->|Yes| D[Check dmesg for THP events]\n  C -->|No| E[Assess IO & memory churn]\n  D --> F[If THP events align, mitigate]\n  F --> G[Validate latency post-mitigation]","difficulty":"advanced","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Databricks","IBM"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T05:04:08.066Z","createdAt":"2026-01-16T02:27:13.001Z"},{"id":"q-2740","question":"Scenario: A Linux server's boot time has increased after a kernel update. Using only default tooling, describe a concrete diagnostic workflow to (1) determine which boot phase takes the longest, (2) pinpoint the slowest unit or driver, and (3) apply a safe mitigation (e.g., mask a service, adjust a TimeoutStartSec, or blacklist a module) while preserving observability. Include exact commands and expected outputs?","answer":"Begin with systemd-analyze blame to list boot delays, then systemd-analyze critical-chain to map the boot path. Check messages with dmesg | tail -n 100 and journalctl -b | tail -n 200. If a service is","explanation":"## Why This Is Asked\n\nTests practical debugging of boot-time regressions, requiring use of native systemd tooling, kernel logs, and safe mitigations. It assesses the ability to isolate phases, identify the root cause, and implement a reversible fix with observability intact.\n\n## Key Concepts\n\n- systemd-analyze blame\n- systemd-analyze critical-chain\n- dmesg and journalctl for boot logs\n- /etc/modprobe.d/ blacklist for drivers\n- safe mitigations: masking a unit, timeout tweaks, or module blacklist\n\n## Code Example\n\n```javascript\nsystemd-analyze blame\nsystemd-analyze critical-chain\ndmesg | tail -n 100\njournalctl -b | tail -n 200\n```\n\n## Follow-up Questions\n\n- How would you verify the fix across reboots?\n- What if the delay recurs after a kernel update again, what’s your rollback plan?","diagram":"flowchart TD\n  A[Boot Start] --> B[systemd-analyze blame]\n  B --> C[Identify slow unit]\n  C --> D{Mitigation}\n  D -->|Mask service| E[Mask unit]\n  D -->|TimeoutStartSec| F[Adjust timeout]\n  D -->|Blacklist module| G[Blacklist module]","difficulty":"beginner","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Citadel"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T09:48:54.388Z","createdAt":"2026-01-16T09:48:54.388Z"},{"id":"q-2819","question":"A Linux server runs a Python data-collection daemon under systemd; during bursts the daemon slows dramatically while overall CPU and memory appear normal. Using only default tools, outline a beginner-friendly diagnostic workflow to (1) determine if CPU throttling via cgroups or the scheduler is the bottleneck, (2) identify the throttled process, and (3) apply a safe mitigation (e.g., adjust cpu.max or Nice value) while preserving observability. Include exact commands and expected outputs?","answer":"Check for throttling with cgroups and the scheduler using only default tools. Steps: 1) cat /sys/fs/cgroup/cpu.slice/mydaemon.service/cpu.max; 2) top -b -n1 | head; 3) pidstat -p ALL 1; 4) dmesg | tai","explanation":"## Why This Is Asked\nTests practical, beginner-friendly diagnostics for CPU throttling and safe mitigations using default Linux tools.\n\n## Key Concepts\n- cgroups CPUQuota and cpu.max\n- /sys/fs/cgroup inspection\n- per-process metrics with pidstat/top\n- kernel messages with dmesg\n- systemctl set-property for safe quota adjustments\n\n## Code Example\n```javascript\n// Example: raise CPU quota for a service\nsystemctl set-property mydaemon.service CPUQuota=50%\nsystemctl restart mydaemon.service\n```\n\n## Follow-up Questions\n- What monitoring would you add to ensure throttling doesn’t recur?\n- How would you handle multiple services sharing the same CPU pool?","diagram":null,"difficulty":"beginner","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","LinkedIn","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T13:53:04.947Z","createdAt":"2026-01-16T13:53:04.947Z"},{"id":"q-2900","question":"Context: A Linux host running dozens of containers under cgroups v2 experiences intermittent tail-latency spikes on one service during peak load. Without downtime, design a precise diagnostic workflow to (1) verify if cpu.max throttling is the culprit, (2) locate the offending container/process, and (3) apply a safe mitigation (adjust quotas, pin CPUs, or throttle traffic) while preserving QoS. Include exact commands and expected outputs?","answer":"Run: cat /sys/fs/cgroup/cpu.max -> 50000 100000 (50% of one CPU). Then: ps -eo pid,comm,%cpu --sort=-%cpu | head -n5 -> shows 1234 myservice 46.2. Map PIDs to cgroups via cat /proc/1234/cgroup. Mitiga","explanation":"## Why This Is Asked\nTests practical skills diagnosing cgroup-based contention under real workload.\n\n## Key Concepts\n- cgroups v2 cpu.max and quotas\n- mapping processes to their cgroups\n- safe throttling vs QoS guarantees\n\n## Code Example\n```\ncat /sys/fs/cgroup/cpu.max\nps -eo pid,cmd,%cpu --sort=-%cpu | head -n5\n```\n\n## Follow-up Questions\n- How would you handle multiple services hitting limits simultaneously?\n- What are risks of aggressive throttling on latency-sensitive apps?","diagram":"flowchart TD\n  Start --> Identify[cgroup v2 status]\n  Identify --> Top[Top consumer processes]\n  Top --> Mitigate[Apply quotas or pin CPUs]\n  Mitigate --> Validate[Validate latency drop]","difficulty":"intermediate","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","Instacart"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T16:51:51.422Z","createdAt":"2026-01-16T16:51:51.422Z"},{"id":"q-2917","question":"You're running a Linux host in a high-load microservice environment. Under peak traffic, 95th percentile latency on a critical path spikes 2–5x while CPU, I/O, and memory appear nominal. Using only default tools, design a concrete diagnostic workflow to (1) determine if bottleneck is CPU, memory, disk I/O, or network, (2) identify the exact offender (process, device, or subsystem), and (3) apply a safe, observable mitigation (e.g., cgroup throttling, scheduler tuning, or buffering changes) with minimal downtime. Include exact commands and expected outputs?","answer":"Run quick baselines: `top -b -n1`, `vmstat 1 5`, `iostat -xd 1 5`, and `ps -eo pid,pcpu,pmem,cmd --sort=-pcpu | head -n 6`. If CPU is hot, assign affinity with `taskset -pc <cpus> <pid>`; if I/O bound","explanation":"## Why This Is Asked\nTests practical troubleshooting under latency pressure using built-in tools, not hypothetical fixes.\n\n## Key Concepts\n- Tail-latency analysis under load\n- Baseline capture with defaults (top/vmstat/iostat/ps)\n- Safe mitigations (CPU pinning, IO priorities, scheduler tweaks) and observability\n\n## Code Example\n```javascript\n// Diagnostics commands to run\ntop -b -n1\nvmstat 1 5\niostat -xd 1 5\nps -eo pid,pcpu,pmem,cmd --sort=-pcpu | head -n 6\n```\n\n## Follow-up Questions\n- How would you validate mitigation effectiveness?\n- What risks do throttling or pinning introduce, and how mitigate them?","diagram":"flowchart TD\n  A[Start] --> B{Bottleneck?}\n  B --> C[CPU]\n  B --> D[I/O]\n  B --> E[Memory]\n  B --> F[Network]","difficulty":"advanced","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Lyft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T17:36:44.989Z","createdAt":"2026-01-16T17:36:44.989Z"},{"id":"q-3310","question":"Linux server runs a Redis cluster and a busy web app; tail latency spikes under load while CPU and memory look healthy. Without downtime, design a concrete, repeatable diagnostic workflow to (1) pinpoint if network I/O, disk I/O, or interrupts are the bottleneck, (2) identify the offender, and (3) apply a safe mitigation with minimal disruption. Include exact commands and expected outputs?","answer":"Execute a triage: 1) iostat -xz 1 60 to confirm IO wait; 2) mpstat -P ALL 1 60 to spot CPU contexts; 3) cat /proc/interrupts to identify hot IRQs; 4) ethtool -k eth0 and -K eth0 to verify NIC offloads","explanation":"## Why This Is Asked\n\nTests practical Linux performance triage under realistic constraints, blending IO, network, and interrupt angles with safe mitigations and observable outcomes.\n\n## Key Concepts\n\n- IO wait detection with iostat, CPU context with mpstat, interrupts via /proc/interrupts\n- NIC offloads and wake paths using ethtool\n- Disk I/O schedulers (mq-deadline) and safe mitigations\n- IRQ affinity and minimal-downtime interventions\n\n## Code Example\n\n```javascript\n// Example: simple parser skeleton (not executed here) to map iostat lines into metrics\nfunction parseIostat(line){ const parts=line.trim().split(/\\s+/); return {device: parts[0], await: parseFloat(parts[11])}; }\n```\n\n## Follow-up Questions\n\n- How would you automate this triage in a production monitoring script?\n- What changes if workload shifts to NVMe SSDs with high queue depth?","diagram":"flowchart TD\n  A[Start: Peak latency] --> B[Triage IO and interrupts]\n  B --> C{IO wait high?}\n  C -- Yes --> D[NIC/disk mitigations]\n  C -- No --> E[Check interrupts/affinity]\n  D --> F[Observe impact]\n  E --> F","difficulty":"intermediate","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Salesforce","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T11:23:32.602Z","createdAt":"2026-01-17T11:23:32.604Z"},{"id":"q-3489","question":"You have a Linux server hosting a real-time data ingestion service and a multi-stage database on a 2-socket NUMA machine. Under peak load, tail latency spikes. Using only default tooling, design a concrete diagnostic workflow to (1) determine if stalls are CPU, memory, or I/O bound, (2) identify the NUMA locality or device causing pressure and the offending process, and (3) apply a safe mitigation (e.g., CPU pinning, NUMA binding, or IO scheduling tweaks) while preserving observability. Include exact commands and expected outputs?","answer":"Run vmstat 1 5 to gauge CPU/mem, sample /proc/diskstats for IO waits, read /proc/meminfo for pressure, and ps -eo pid,cmd,%cpu --sort=-%cpu | head to spot hot processes. Check NUMA with numactl --hard","explanation":"## Why This Is Asked\n\nTests real-world debugging in NUMA-heavy hosts with minimal tooling, linking to performance trade-offs.\n\n## Key Concepts\n\n- NUMA locality\n- Safe process pinning\n- Kernel I/O paths and disk stats\n- Observability with standard Linux tools\n\n## Code Example\n\n```bash\nvmstat 1 5\ncat /proc/diskstats\nps -eo pid,cmd,%cpu --sort=-%cpu | head\nnumactl --hardware\n```\n\n## Follow-up Questions\n\n- How would you extend the approach for containers with shared memory?\n- What pitfalls could NUMA pinning cause in multi-tenant nodes?","diagram":null,"difficulty":"advanced","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","MongoDB","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T18:46:42.504Z","createdAt":"2026-01-17T18:46:42.505Z"},{"id":"q-3535","question":"Scenario: A Linux host running several VMs experiences intermittent memory pressure during business hours, causing guest pauses. Using only default tools, outline a beginner-friendly diagnostic workflow to (1) determine if host memory pressure or VM ballooning is the bottleneck, (2) identify which VM(s) are affected and estimate their memory footprint, and (3) apply a safe mitigation (e.g., cap VM memory via libvirt, adjust host swappiness) while preserving observability. Include exact commands and expected outputs?","answer":"Check host memory pressure: free -m; vmstat 1 5; grep Swap /proc/meminfo. If swap used or MemAvailable is low, locate VMs with high RSS: ps -eo pid,comm,rss,cmd | grep -E 'qemu|kvm'. For each VM, run ","explanation":"## Why This Is Asked\n\nTests ability to reason about virtualization memory pressure and safe mitigations using default tools; demonstrates identifying bottlenecks and applying safe mitigations without downtime.\n\n## Key Concepts\n\n- Host memory pressure and swap\n- VM ballooning and per-VM memory metrics via virsh dommemstat\n- Safe mitigations: cap VM memory with virsh setmem; adjust swappiness; observability\n\n## Code Example\n\n```bash\n#!/usr/bin/env bash\nfree -m\nvmstat 1 5\nfor vm in $(virsh list --name); do\n  virsh dommemstat \"$vm\" --memory\ndone\n```\n\n## Follow-up Questions\n\n- What are the risks of capping VM memory in production and how would you rollback?\n- How would you validate that the mitigation worked without redeploying workloads?","diagram":null,"difficulty":"beginner","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","NVIDIA","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T20:35:53.377Z","createdAt":"2026-01-17T20:35:53.378Z"},{"id":"q-3610","question":"You run a Linux host with a small API service behind Nginx. During bursts, tail latency spikes even though CPU, memory, and network look normal. Using only default tooling, design a beginner-friendly diagnostic workflow to (1) determine if CPU, memory, or I/O wait is the bottleneck, (2) identify the exact process or disk causing the bottleneck, and (3) apply a safe mitigation (e.g., adjust I/O scheduler, tune dirty writeback, or scale workers) while preserving observability. Include exact commands and expected outputs?","answer":"Execute this diagnostic workflow using default system tools:\n\n1. **Identify bottleneck type**: Run `iostat -xz 1 2` to check device await times, `vmstat 1` to monitor I/O wait (wa) and memory pressure\n2. **Map to processes**: Use `pidstat -p ALL 1` to correlate CPU wait with specific PIDs, `ss -tlnp` to examine socket states\n3. **Apply safe mitigation**: If I/O wait dominates, switch to deadline/noop scheduler with `echo deadline > /sys/block/sdX/queue/scheduler`, tune dirty writeback via `/proc/sys/vm/dirty_*` parameters, or scale worker processes while preserving existing monitoring","explanation":"## Why This Is Asked\nThis evaluates practical OS troubleshooting skills under realistic production load, testing ability to diagnose performance issues using only default system tools. It assesses understanding of bottleneck identification, process/device correlation, and safe mitigations that maintain observability.\n\n## Key Concepts\n- Bottleneck detection via iostat (device await) and vmstat (I/O wait, memory)\n- Process-level correlation using pidstat for CPU wait mapping\n- Network and socket state analysis with ss\n- Safe production mitigations: I/O scheduler tuning, dirty writeback optimization, worker scaling, swappiness adjustment\n\n## Code Example\n```bash\n# Diagnostic sequence\niostat -xz 1 2    # Device-level I/O metrics\nvmstat 1          # System-wide wait and memory\npidstat -p ALL 1  # Process-level CPU wait\nss -tlnp          # Socket states and processes\n```\n\n## Follow-up Considerations\n- Monitor mitigation impact using same toolset\n- Document changes for rollback\n- Consider sysctl.conf persistence for tunable parameters","diagram":"flowchart TD\n  Start([Start]) --> Bottleneck{Bottleneck? CPU/IO/Memory}\n  Bottleneck --> Identify[Identify process or device]\n  Identify --> Mitigate[Apply safe mitigation]\n  Mitigate --> Observe[Observe impact]\n  Observe --> Bottleneck","difficulty":"beginner","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","IBM","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T05:15:40.230Z","createdAt":"2026-01-17T23:34:59.321Z"},{"id":"q-4263","question":"On a single Linux host running multiple Python services in separate cgroups, tail latency spikes under peak load even though aggregate CPU usage stays under 70%. Using only default tools, design a concrete diagnostic workflow to (1) determine if tail latency is due to CPU throttling by cgroups, (2) identify which cgroup and process is hitting quotas, and (3) propose a safe mitigation (e.g., adjust quotas, shares, or migrate a heavy service to its own host) while preserving observability. Include exact commands and expected outputs?","answer":"Inspect each cgroup's throttling: cat /sys/fs/cgroup/cpu/MyService/cpu.stat and cat /sys/fs/cgroup/cpu/MyService/cpu.cfs_quota_us; compare with cpu.cfs_period_us to derive throttled_percentage. Identi","explanation":"## Why This Is Asked\nThe question probes practical troubleshooting of cgroup-based CPU throttling, tail latency, and observability in a multi-service Linux host.\n\n## Key Concepts\n- cgroups v1 vs v2, cpu quota/period, throttled_time\n- diagnosing throttling vs genuine load, identifying offending PIDs\n- mitigations: adjust quota/shares, isolate on dedicated hosts, verify impact\n\n## Code Example\n```javascript\n// Node.js snippet to read cpu.stat for a given service\nconst fs=require('fs');\nconst path='/sys/fs/cgroup/cpu/MyService/cpu.stat';\nconsole.log(fs.readFileSync(path,'utf8'));\n```\n\n## Follow-up Questions\n- How would you monitor the effectiveness of quota adjustments across services?\n- What risks exist when increasing quotas on a busy host?","diagram":"flowchart TD\n  A[Start] --> B[Check cgroup cpu.stat throttled_time]\n  B --> C[Identify which group is throttled]\n  C --> D[Find offending process via ps with high CPU time]\n  D --> E[Adjust quotas or migrate service]\n  E --> F[Observe metrics]","difficulty":"intermediate","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Airbnb","Google","LinkedIn"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T10:55:04.783Z","createdAt":"2026-01-19T10:55:04.783Z"},{"id":"q-4294","question":"On a Linux host serving low-latency ML inference REST endpoints, tail latency spikes under peak load while average latency stays acceptable. Using only default tools, design a concrete diagnostic workflow to (1) determine if tail latency is due to CPU, memory, IO, or interrupts, (2) identify the bottlenecked core/NUMA domain and the offending process, and (3) propose a safe mitigation (e.g., CPU pinning, shielding, IO scheduler tweaks) while preserving observability. Include exact commands and expected outputs?","answer":"Run a triage: (a) vmstat 1; iostat -xz 1; cat /proc/interrupts to spot IO/IRQ pressure; (b) pidstat -p ALL 1; ps -eo pid,ppid,cmd,%cpu --sort=-%cpu | head; mpstat -P ALL 1 to map cores to load; (c) mi","explanation":"## Why This Is Asked\nReal latency issues demand structured triage with OS visibility, using only built-in tools.\n\n## Key Concepts\n- diagnose tail latency sources: CPU contention, memory pressure, IO wait, interrupts\n- map per-core activity to NUMA domains and processes\n- safe mitigations: CPU pinning, shielding, IRQ balance, IO scheduler tweaks, observability\n\n## Code Example\n```bash\nvmstat 1\niostat -xz 1\ncat /proc/interrupts\n```\n\n## Follow-up Questions\n- How would you validate observability after applying mitigations?\n- What risks arise from CPU shielding on a production host?","diagram":"flowchart TD\n  A[Tail latency spike] --> B{Root cause}\n  B --> C[CPU contention]\n  B --> D[Memory pressure]\n  B --> E[IO wait]\n  B --> F[Interrupt storm]\n  C --> G[Identify hot cores and processes]\n  E --> H[Check IO devices and schedulers]\n  F --> I[Inspect NIC IRQs and balance]\n","difficulty":"intermediate","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Anthropic","MongoDB","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T11:43:10.324Z","createdAt":"2026-01-19T11:43:10.324Z"},{"id":"q-4319","question":"**Scenario:** A Linux host runs a multi-tenant service and experiences intermittent network latency spikes during peak load. Using only default tooling, design a concrete diagnostic workflow to (1) determine if latency is kernel-network-stack, NIC, or user-space, (2) identify the exact TX/RX queue or IRQ causing it, and (3) apply a safe mitigation (e.g., adjust IRQ affinity, interrupt coalescing, or RPS) while preserving availability. Include exact commands and expected outputs?","answer":"Baseline latency with ping; NIC stats: ethtool -S eth0; IRQ overview: cat /proc/interrupts; per-queue stats: ls /sys/class/net/eth0/queues; NIC coalescing: ethtool -C eth0 rx-usecs 50 rx-frames 256; e","explanation":"## Why This Is Asked\nAssesses real-world networking diagnosis using stock Linux tools, focusing on where latency originates and how to safely mitigate without downtime.\n\n## Key Concepts\n- NIC queues, ethtool stats, /proc/interrupts\n- IRQ affinity, RPS, interrupt coalescing trade-offs\n- Observability and rollback validation\n\n## Code Example\n```javascript\n// Note: commands shown above are real shell steps; this block illustrates flow\nconsole.log('Diagnose latency by querying NIC stats, IRQs, and coalescing');\n```\n\n## Follow-up Questions\n- How would you revert changes if latency worsened?\n- Which metrics would you monitor long-term to prevent regressions?","diagram":null,"difficulty":"advanced","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Airbnb","Databricks","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T13:28:06.394Z","createdAt":"2026-01-19T13:28:06.394Z"},{"id":"q-4465","question":"On a dual-socket Linux server running a CPU-bound analytics workload, latency spikes occur under peak load even when CPU usage seems normal. Using only default tooling, design a concrete diagnostic workflow to (1) determine if the spike is NUMA memory locality, CPU throttling, or kernel I/O, (2) pinpoint the offending subsystem or process, and (3) apply a safe mitigation (e.g., bind memory/threads with numactl, isolate CPUs, or adjust memory policies) while preserving observability. Include exact commands and expected outputs?","answer":"Run numactl --hardware and numastat -m to verify NUMA topology and locality. Use mpstat -P ALL 1 and iostat -xz 1 to detect CPU throttling or IO waits; inspect /proc/<pid>/numa_maps for hotspots. If l","explanation":"## Why This Is Asked\n\nAssesses NUMA-aware debugging and safe mitigation using defaults, critical in high-saturation production.\n\n## Key Concepts\n\n- NUMA locality\n- /proc/<pid>/numa_maps\n- mpstat, iostat\n- numactl, taskset\n\n## Code Example\n\n```bash\nnumactl --hardware\nnumastat -m\n```\n\n## Follow-up Questions\n\n- How would you validate that the mitigation didn't impact cross-node memory access?","diagram":null,"difficulty":"advanced","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Citadel","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T19:51:45.297Z","createdAt":"2026-01-19T19:51:45.297Z"},{"id":"q-466","question":"You're debugging a production Linux server where processes are randomly dying with 'Out of memory' errors, but `free -m` shows 8GB available RAM. How would you diagnose and fix this issue?","answer":"Check `dmesg | grep -i oom-killer` for OOM events. Use `cat /proc/meminfo` to examine memory fragmentation. Review `overcommit_memory` and `overcommit_ratio` in `/proc/sys/vm/`. Monitor with `sar -r` ","explanation":"## Memory Management Issues\n\nLinux OOM killer activates when available memory + swap < pages_min * 4, not when RAM is fully used.\n\n## Diagnostic Steps\n\n- Check OOM killer logs: `dmesg | grep -i oom-killer`\n- Examine memory fragmentation: `cat /proc/meminfo | grep -E '(MemFree|MemAvailable|Slab|PageTables)'`\n- Monitor memory pressure: `cat /proc/pressure/memory`\n- Review overcommit settings: `cat /proc/sys/vm/overcommit_memory`\n\n## Common Causes\n\n- Memory fragmentation preventing large allocations\n- Overcommitment allowing more memory than physically available\n- Kernel memory usage (slabs, page tables) not visible in `free`\n- Memory leaks in kernel modules\n\n## Solutions\n\n```bash\n# Disable overcommit\necho 0 > /proc/sys/vm/overcommit_memory\n\n# Add swap space\nfallocate -l 2G /swapfile\nchmod 600 /swapfile\nmkswap /swapfile\nswapon /swapfile\n\n# Tune memory management\necho 65536 > /proc/sys/vm/min_free_kbytes\n```","diagram":"flowchart TD\n  A[Process Memory Request] --> B{Physical RAM Available?}\n  B -->|No| C[Check Swap]\n  B -->|Yes| D[Allocate Memory]\n  C -->|Swap Available| E[Use Swap]\n  C -->|No Swap| F{Overcommit Enabled?}\n  F -->|Yes| G[Allow Allocation]\n  F -->|No| H[OOM Killer Activates]\n  G --> I[Memory Pressure Builds]\n  I --> H\n  H --> J[Kill Process]\n  J --> K[Log to dmesg]","difficulty":"advanced","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":["oom killer","memory fragmentation","overcommit_memory","dmesg","proc/meminfo","sar","system diagnostics"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2025-12-27T04:55:16.000Z","createdAt":"2025-12-26 12:51:05"},{"id":"q-4703","question":"Scenario: A Linux host running a containerized payment service experiences intermittent latency spikes under peak load. Using only default tooling, design a concrete diagnostic workflow to (1) determine if latency is CPU-, IO-, or network-bound, (2) identify the exact process, namespace, or block device involved, and (3) apply a safe mitigation (e.g., CPU shielding/cgroup throttling, IO scheduler tweak, or tc-based traffic shaping) while preserving observability. Include exact commands and example outputs?","answer":"Start with iostat -xd 1 5; vmstat 1 5; mpstat -P ALL 1 5; pidstat -d -p ALL 1 5; ss -s; ip -s link. Map spikes to per-process CPU/IO; if CPU-bound, set quotas via cgset/cgexec or cpuset; if IO-bound, ","explanation":"## Why This Is Asked\nTests triage skills for live Linux environments, focusing on observable cues and safe mitigations.\n\n## Key Concepts\n- Linux performance triage with default tools\n- CPU/IO/network bottlenecks correlation\n- cgroups CPU quotas, IO schedulers, tc shaping\n\n## Code Example\n```bash\niostat -xd 1 5\nvmstat 1 5\nmpstat -P ALL 1 5\npidstat -d -p ALL 1 5\nss -s\n```\n\n## Follow-up Questions\n- How would you automate this workflow for periodic runs?\n- What are the risks of aggressive IO throttling in a payment service?","diagram":"flowchart TD\n  A[Start Diagnostic] --> B{Bound Type?}\n  B --> C[CPU-bound]\n  B --> D[IO-bound]\n  B --> E[Network-bound]\n  C --> F[Identify process with pidstat/top]\n  D --> G[Check block devices with iostat]\n  E --> H[Inspect TCP queues with ss/tc]\n  F --> I[Mitigate with CPU quotas/cpuset]\n  G --> J[Mitigate with io-scheduler tuning]\n  H --> K[Mitigate with tc traffic shaping]\n  I --> L[Re-measure]\n  J --> L\n  K --> L","difficulty":"advanced","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Microsoft","MongoDB","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T08:58:25.598Z","createdAt":"2026-01-20T08:58:25.598Z"},{"id":"q-4748","question":"Scenario: A Linux host runs multiple containers with a unified cgroup v2 hierarchy. A payment service experiences intermittent latency under sustained load, with memory pressure but no OOM. Using only default tools, design a concrete diagnostic workflow to (1) confirm memory pressure origin (anonymous vs file cache), (2) identify the specific container/cgroup causing pressure, and (3) apply a safe mitigation (e.g., adjust memory.max for that cgroup or tune swap accounting) while preserving observability. Include exact commands and example outputs?","answer":"Telemetry over 60s: free -h; vmstat 1 60; slabtop -o; check MemAvailable and SwapUsed to decide if pressure is from anonymous memory or cache. Map cgroups: systemd-cgls | grep -i memory; for each path","explanation":"## Why This Is Asked\nThis question probes real-world memory pressure diagnosis under containers with cgroup v2, forcing candidates to combine system monitoring, per‑cgroup accounting, and safe constraints.\n\n## Key Concepts\n- Memory pressure sources (anonymous vs cache)\n- cgroup v2 per‑container accounting\n- Safe mitigations (memory.max, swap accounting) with observability\n\n## Code Example\n```bash\nfree -h\nvmstat 1 60\nsystemd-cgls | grep -i memory\n```\n\n## Follow-up Questions\n- How would you verify observability after mitigation?\n- How would you handle a multi-tenant node with noisy neighbors?","diagram":"flowchart TD\n  A[Telemetry] --> B[Identify pressure]\n  B --> C[Find offender cgroup]\n  C --> D[Apply memory.max cap]\n  D --> E[Monitor & logs]","difficulty":"advanced","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Airbnb","DoorDash"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T10:49:35.614Z","createdAt":"2026-01-20T10:49:35.614Z"},{"id":"q-4769","question":"On a single Linux host with NUMA, tail latency spikes under sustained load from a real-time event processor while CPU usage remains moderate. Using only default Linux tools, design a concrete diagnostic workflow to (1) confirm whether stalls are caused by Transparent Huge Pages or page cache thrashing, (2) identify the offending memory consumer and its allocations, and (3) apply a safe mitigation (e.g., disable THP, adjust swappiness, pin the hot thread) while preserving observability. Include exact commands and example outputs?","answer":"Check THP: cat /sys/kernel/mm/transparent_hugepage/enabled; memory pressure: free -m; vmstat 1 5; Identify heavy consumer: ps -eo pid,cmd,%mem,%cpu --sort=-%mem | head; inspect mappings: head -n 20 /p","explanation":"This question tests practical debugging of latency while using default tooling. Candidates must verify THP/page cache impact, locate the heavy memory consumer via ps and smaps, and apply safe mitigations (disable THP, tune swappiness, pin CPU affinity) while preserving observability.","diagram":"flowchart TD\n  Start[Start] --> THP[Check THP]\n  THP --> MEM[Memory checks]\n  MEM --> FIND[Identify heavy consumer]\n  FIND --> MIT[Mitigate]\n  MIT --> VERIFY[Verify latency]","difficulty":"intermediate","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Oracle","Plaid","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T11:43:21.409Z","createdAt":"2026-01-20T11:43:21.409Z"},{"id":"q-4818","question":"On a Linux server delivering real-time analytics, tail latency spikes to ~200 ms under sustained load. The setup uses a NVMe drive and a multi-queue NIC. Using only default Linux tools, design a concrete diagnostic workflow to (1) confirm whether latency ties to IO scheduling or NIC interrupts, (2) identify the exact IRQ or kernel thread involved, and (3) propose safe mitigations (e.g., pinning IRQs, tuning the I/O scheduler, adjusting CPU affinity) while preserving observability. Include exact commands and example outputs?","answer":"Run iostat -xz 1 to observe device latency; inspect /proc/interrupts to locate NIC IRQs and their counts; check per-process IO wait with pidstat -d 1 -p <pid>; view /proc/sched_debug for CPU throttlin","explanation":"## Why This Is Asked\nThis checks practical debugging of latency in a real sysadmin context, not theory.\n\n## Key Concepts\n- IO scheduling latency, IRQ affinity, per-process IO waits, observability\n- Using default tools to correlate device latency with CPU or IRQ activity\n- Safe mitigations: CPU pinning, IRQ affinity, IO scheduler tweaks\n\n## Code Example\n```javascript\n# Commands to run\niostat -xz 1\ncat /proc/interrupts\npidstat -d 1 -p <pid>\n```\n\n## Follow-up Questions\n- How would you validate the effectiveness of PINning and affinity changes over time?\n- What risks do IRQ pinning and CPU affinity introduce in a multi-tenant environment?\n","diagram":null,"difficulty":"intermediate","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Airbnb","MongoDB","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T14:52:58.021Z","createdAt":"2026-01-20T14:52:58.021Z"},{"id":"q-4844","question":"On a Linux server hosting a Redis cluster, tail latency spikes under sustained write load while CPU and memory look normal. Using only default tools, design a concrete diagnostic workflow to (1) confirm if tail latency is due to I/O wait, NIC queuing, or page cache, (2) pinpoint the exact subsystem causing it, and (3) propose safe mitigations (IO scheduler, CPU pinning, or cache tuning) with observability. Include exact commands and sample outputs?","answer":"Start with iostat -x 1 to gauge I/O wait, vmstat 1 for page/cache pressure, and sar -B for blocked processes. Use iotop -aoW to identify heavy I/O consumers. Inspect /proc/diskstats and, if needed, /p","explanation":"## Why This Is Asked\nTests practical debugging of tail latency under load with standard tools, emphasizing observable signals and safe mitigations.\n\n## Key Concepts\n- I/O wait vs CPU wait diagnosis\n- Disk scheduler impact on latency\n- NIC queueing and IRQ balancing\n- Observability post-mitigation\n\n## Code Example\n```bash\niostat -x 1\nvmstat 1\nsar -B 1 5\n```\n\n## Follow-up Questions\n- How would you roll back changes if latency returns after mitigation?\n- Which metrics would you monitor long term to detect regressions?","diagram":"flowchart TD\n  A[Start: Data path under load] --> B[Measure I/O wait with iostat/vmstat]\n  B --> C{I/O wait high?}\n  C -- Yes --> D[Identify bottleneck: disk vs NIC](\"/proc/diskstats and /proc/interrupts\")\n  D --> E[Mitigations: change IO scheduler, pin CPUs, balance IRQs]\n  E --> F[Observe metrics to confirm]\n  C -- No --> G[Check cache and NIC latency separately]\n  G --> H[Mitigations: cache tuning, NIC queue tuning]\n  H --> F","difficulty":"intermediate","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Amazon","Citadel"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T16:04:34.225Z","createdAt":"2026-01-20T16:04:34.225Z"},{"id":"q-4877","question":"Context: A Linux host running a data ingestion batch occasionally stalls as the number of open files grows toward the system limit during ETL. Using only default tooling, design a beginner-friendly diagnostic workflow to (1) confirm whether file descriptor exhaustion is the bottleneck versus CPU/IO/memory, (2) identify the process with the most open FDs and the top offending file/socket types, and (3) apply a safe mitigation (e.g., raise limits, tune backlog, adjust file usage) while preserving observability. Include exact commands and expected outputs?","answer":"Begin by confirming FD exhaustion: run ulimit -n; cat /proc/sys/fs/file-max; then enumerate per-PID FDs: for p in /proc/[0-9]*/fd 2>/dev/null; do echo -n \"$p: \"; ls -l $p 2>/dev/null | wc -l; done | s","explanation":"## Why This Is Asked\nThis question tests practical FD exhaustion diagnosis using only default tooling, focusing on identifying offenders and choosing safe mitigations while preserving observability.\n\n## Key Concepts\n- File descriptors, ulimit, /proc, limits.conf\n- Per-process FD accounting, /proc/[pid]/fd\n- Observability: ss, logs, process introspection\n- Mitigations: raise limits, adjust service, optimize FD usage\n\n## Code Example\n```bash\nulimit -n\ncat /proc/sys/fs/file-max\nfor p in /proc/[0-9]*/fd 2>/dev/null; do echo -n \"$p: \"; ls -l $p 2>/dev/null | wc -l; done | sort -nr | head -n 5\n```\n\n## Follow-up Questions\n- How would you monitor FD growth over time?\n- What risks come with increasing limits in production?\n","diagram":null,"difficulty":"beginner","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Anthropic","Discord","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T17:10:33.009Z","createdAt":"2026-01-20T17:10:33.009Z"},{"id":"q-496","question":"How would you find all processes running on port 8080 and terminate them safely?","answer":"Use `lsof -i :8080` to identify processes, then `kill -15 PID` for graceful termination. If unresponsive, use `kill -9 PID`. For multiple processes: `pkill -f ':8080'` sends SIGTERM to all matching processes.","explanation":"## Process Identification\n- `lsof -i :8080` lists processes using the port\n- `netstat -tulpn | grep :8080` alternative method\n\n## Safe Termination\n- `kill -15 PID` (SIGTERM) allows graceful shutdown\n- `kill -9 PID` (SIGKILL) forces immediate termination\n\n## Batch Operations\n- `pkill -f ':8080'` terminates all matching processes\n- `fuser -k 8080/tcp` kills processes by port","diagram":"flowchart TD\n  A[Identify Port Usage] --> B[lsof -i :8080]\n  B --> C{Process Responding?}\n  C -->|Yes| D[kill -15 PID]\n  C -->|No| E[kill -9 PID]\n  D --> F[Verify Termination]\n  E --> F","difficulty":"beginner","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-08T11:59:14.782Z","createdAt":"2025-12-26 12:51:06"},{"id":"q-4967","question":"On a Linux host delivering real-time API responses, tail latency spikes during bursts even though overall CPU and network usage look reasonable. Using only default tooling, design a concrete diagnostic workflow to (1) determine if stalls are caused by IRQ/NAPI imbalance or driver contention, (2) identify the exact NIC IRQ lines and CPU cores involved, and (3) implement a safe mitigation (e.g., pin IRQs to specific CPUs, toggle NIC offloads, or adjust irqbalance) while preserving observability. Include exact commands and example outputs?","answer":"Begin by examining /proc/interrupts to identify active NIC IRQ lines and their distribution across CPU cores, then verify SMP affinity settings and temporarily bind high-frequency IRQs to dedicated CPUs while monitoring latency changes through controlled offload adjustments.","explanation":"## Why This Is Asked\nTail latency in production networks frequently originates from interrupt contention rather than application code paths. This question evaluates practical debugging skills using only system-default tools.\n\n## Key Concepts\nIRQ/NAPI mechanisms, CPU affinity, NIC offloads, irqbalance service, per-CPU latency measurement\n\n## Code Example\n```bash\n# Identify active NIC IRQs\ngrep -E 'eth|eno' /proc/interrupts\n# Verify current affinity mask\ncat /proc/irq/45/smp_affinity_list\n# Bind IRQ 45 to CPUs 0-1\necho 0-1 | sudo tee /proc/irq/45/smp_affinity_list\n# Disable RX offload for testing\n```","diagram":"flowchart TD\n  A[Start] --> B[Inspect /proc/interrupts for hot NIC IRQs]\n  B --> C[Check /proc/irq/XX/smp_affinity]\n  C --> D[Pin IRQs to dedicated CPUs]\n  D --> E[Test with NIC offloads]\n  E --> F[Rerun latency measurements]\n  F --> G[Decide on long-term policy]","difficulty":"intermediate","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":null,"companies":["OpenAI","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-21T06:05:26.395Z","createdAt":"2026-01-20T21:53:39.872Z"},{"id":"q-5034","question":"On a Linux host serving a high-traffic web app, tail latency spikes under sustained load with CPU idle and memory normal. Using only default Linux tools, design a concrete diagnostic workflow to (1) confirm I/O wait is the choke, (2) identify the exact disk path and process contributing IO, and (3) apply a safe mitigation (e.g., switch IO scheduler, adjust read-ahead, or tune block layer params) while preserving observability. Include exact commands and example outputs?","answer":"Start with `vmstat 1 5` to confirm I/O wait spikes while CPU remains idle; run `iostat -xz 1 5` to identify the device with high await. Use `iotop -oaP` to map I/O by PID, then inspect `/sys/block/sdX/queue/scheduler` and apply mitigations like switching to mq-deadline or adjusting read-ahead.","explanation":"## Why This Is Asked\n\nAssesses ability to reason about I/O bottlenecks with realistic constraints and to use standard tooling to localize I/O pressure without external agents.\n\n## Key Concepts\n\n- I/O wait vs CPU bound\n- Block layer: mq-deadline, bfq, read-ahead\n- Per-PID I/O correlation and device paths\n\n## Code Example\n\n```bash\nvmstat 1 5\niostat -xz 1 5\n```\n\n## Follow-up Questions\n\n- How would you verify long-term I/O degradation without sacrificing observability?\n- What are the risks of changing I/O scheduler on a production disk?","diagram":"flowchart TD\n  A[Start] --> B[vmstat/iostat]\n  B --> C{Is iowait high?}\n  C -->|Yes| D[Identify device]\n  D --> E[Check per-PID IO]\n  E --> F[Mitigate Scheduler/Read-Ahead]\n  F --> G[Recheck metrics]\n  G --> H[Log and observe]","difficulty":"intermediate","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Airbnb","DoorDash","LinkedIn"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-21T05:13:15.863Z","createdAt":"2026-01-21T02:40:37.637Z"},{"id":"q-5184","question":"A Linux host running a small CI worker pool intermittently experiences failures to start new jobs with EMFILE: too many open files. Using only default tooling, design a beginner-friendly diagnostic workflow to (1) determine if the issue is per-user limits, system-wide file descriptor usage, or a FD leak, (2) identify the offending process and its FD usage, and (3) apply a safe mitigation (e.g., raise limits via limits.conf or systemd, increase fs.file-max, or fix a leak) while preserving observability. Include exact commands and expected outputs?","answer":"Begin by confirming per-user limits (ulimit -n; cat /proc/$(pidof ci)/limits) and system-wide limits (cat /proc/sys/fs/file-max). Then tally FDs with lsof | wc -l and ls /proc/*/fd | wc -l. Identify s","explanation":"## Why This Is Asked\nTests practical knowledge of Linux file descriptor limits, both per-user and system-wide, and how to diagnose FD exhaustion using default tooling.\n\n## Key Concepts\n- File descriptors and system limits (ulimit, fs.file-max)\n- Per-process vs global FD accounting (ls /proc/$PID/fd, lsof)\n- Safe mitigations (systemd overrides, limits.conf, service restarts) and observability\n\n## Code Example\n```bash\n# Check per-user limit and per-process limits\nulimit -n\ncat /proc/1234/limits\n\n# System-wide limit\ncat /proc/sys/fs/file-max\n\n# FD usage and offender lookup\nlsof | wc -l\nps -eo pid,cmd | grep -i ci\nls -l /proc/1234/fd | wc -l\n```\n\n## Follow-up Questions\n- How would you detect and prevent FD leaks in a long-running service?\n- What are trade-offs of raising limits vs fixing leaks?","diagram":"flowchart TD\n  A[Start] --> B{EMFILE observed?}\n  B --> C[Check per-user limits]\n  C --> D[Check system-wide fd usage]\n  D --> E[Identify offending process]\n  E --> F[Mitigate: systemd override or limits.conf]\n  F --> G[Verify observability]","difficulty":"beginner","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Goldman Sachs","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T10:10:28.854Z","createdAt":"2026-01-21T10:10:28.854Z"},{"id":"q-5217","question":"A Linux host running a Node.js API behind Nginx experiences intermittent 1–2s cold-start delays during bursts, while CPU, memory, and disk I/O look normal. Using only default tooling, provide a concrete diagnostic workflow to (1) determine if stalls are CPU scheduling, I/O wait, or memory pressure, (2) identify the exact process or kernel subsystem causing the stall, and (3) apply a safe mitigation (e.g., adjust cpu shares, tune dirty_writeback, or reorder startup) while preserving observability. Include exact commands and example outputs?","answer":"Run vmstat 1 10; iostat -xz 1 10; free -m; pidstat 1 1 -p ALL; top -b -n1. If iowait is elevated and disk queue long, correlate pidstat results to the offending process. Mitigate by pinning the heavy ","explanation":"## Why This Is Asked\nThis question tests practical, beginner-friendly Linux performance diagnosis using default tools, focusing on I/O, CPU, and memory bottlenecks in a live service.\n\n## Key Concepts\n- vmstat, iostat, free, pidstat basics\n- Interpreting iowait and swap pressure\n- Safe mitigations with taskset/cgroups and VM tunables\n\n## Code Example\n```bash\n#!/bin/bash\nvmstat 1 5\niostat -xz 1 5\nfree -m\npidstat 1 1 -p ALL\n```\n\n## Follow-up Questions\n- How would containerization affect this workflow?\n- How to adapt for swap thrashing scenarios?\n","diagram":"flowchart TD\n  A[Start] --> B[Run vmstat/iostat/free]\n  B --> C[Is wa or swap high?]\n  C -->|Yes| D[Identify offending process via pidstat]\n  C -->|No| E[Check startup and cgroups]\n  D --> F[Apply mitigation (taskset/cgroups, vm tunables)]\n  F --> G[Re-check metrics]]","difficulty":"beginner","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Meta","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T11:05:59.529Z","createdAt":"2026-01-21T11:05:59.529Z"},{"id":"q-527","question":"How would you find and kill a process that's using port 8080 on a Linux system?","answer":"Use `lsof -i :8080` or `netstat -tulpn | grep 8080` to find the PID, then `kill -9 <PID>` to terminate it. For a safer approach, try `kill -15 <PID>` first to allow graceful shutdown.","explanation":"## Finding the Process\n- `lsof -i :8080` lists processes using that port\n- `netstat -tulpn` shows all listening processes with PIDs\n- `ss -tulpn | grep 8080` is a modern alternative\n\n## Terminating the Process\n- `kill -15 <PID>` sends SIGTERM for graceful shutdown\n- `kill -9 <PID>` sends SIGKILL for force termination\n- `pkill -f \"process_name\"` kills by name pattern\n\n## Verification\n- `lsof -i :8080` confirms process is killed\n- `netstat -tulpn | grep 8080` should return empty","diagram":"flowchart TD\n  A[Port 8080 Issue] --> B[lsof -i :8080]\n  B --> C[Get PID]\n  C --> D[kill -15 PID]\n  D --> E{Process Killed?}\n  E -->|No| F[kill -9 PID]\n  E -->|Yes| G[Verify with lsof]","difficulty":"beginner","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Oracle","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-25T15:01:25.772Z","createdAt":"2025-12-26 12:51:06"},{"id":"q-5273","question":"A Linux host runs a Python data-ingest daemon that intermittently stalls under bursts. CPU, memory, and disk I/O appear normal, but new files fail to start for 1-2 seconds. Using only default tooling, outline a concrete diagnostic workflow to (1) decide if the bottleneck is per-process limits, I/O wait, or a kernel resource, (2) identify the exact process or resource causing the stall, and (3) implement a safe mitigation (e.g., raise nofile limits, throttle workers, or tune IO scheduler) while preserving observability?","answer":"Start with triage: check per-process limits with ulimit -n and /proc/$PID/limits; monitor fds in use via ls /proc/*/fd | wc -l; watch I/O and CPU wait with iostat -dx 1 2 and vmstat 1; inspect logs vi","explanation":"## Why This Is Asked\nThis tests practical, no-frills debugging using default tools, triaging limits vs I/O vs kernel resources.\n\n## Key Concepts\n- File descriptor limits and ulimit\n- I/O wait and disk scheduling\n- Observability with journalctl and /proc\n\n## Code Example\n```bash\n# quick checks\nulimit -n\ncat /proc/$(pgrep -f ingest)/limits\niostat -dx 1 2\n```\n\n## Follow-up Questions\n- How would you validate changes without restarting long-running services?\n- What logs or metrics confirm a mitigation worked?","diagram":"flowchart TD\nA[Bursts] --> B{Bottleneck?}\nB -->|FD limits| C[Raise nofile]\nB -->|I/O wait| D[Tune IO scheduler]\nB -->|Other| E[Observe logs]","difficulty":"beginner","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Citadel","Coinbase","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T14:49:23.981Z","createdAt":"2026-01-21T14:49:23.981Z"},{"id":"q-5318","question":"On a Linux server hosting multiple services, intermittent latency spikes occur under memory pressure with THP enabled. Using only default tools, design a concrete diagnostic workflow to (1) verify if hugepage fragmentation is causing allocator delays, (2) identify the specific process or container consuming memory aggressively, and (3) apply a safe mitigation (e.g., disable THP on affected zone, adjust min_free_kbytes, or migrate memory pressure) while preserving observability. Include exact commands and sample outputs?","answer":"Baseline THP state and memory pressure signals, then correlate with latency:\n- cat /sys/kernel/mm/transparent_hugepage/enabled\n- dmesg | grep -i thp\n- cat /proc/meminfo | grep -iHuge\n- for each cgroup","explanation":"## Why This Is Asked\nMemory allocator stalls under pressure can stem from THP fragmentation; diagnosing with default tools tests hypothesis without specialized profilers.\n\n## Key Concepts\n- Transparent Huge Pages (THP)\n- Memory fragmentation and allocator latency\n- Per‑cgroup memory accounting\n- Safe kernel knob changes with observability\n\n## Code Example\n```bash\n# Commands mentioned in answer (for reference)\ncat /sys/kernel/mm/transparent_hugepage/enabled\ndmesg | grep -i thp\ngrep -i Huge /proc/meminfo\nfor c in /sys/fs/cgroup/memory/*; do echo \"$c\"; cat \"$c/memory.stat\"; done\n```\n\n## Follow-up Questions\n- After disabling THP, how would you validate no regression across services?\n- What persistent config would you apply to avoid regressions in future bursts?","diagram":"flowchart TD\n  A[Latency spikes observed] --> B{THP fragmentation?}\n  B --> C[Check THP state]\n  B --> D[Inspect per-cgroup memory.stat]\n  C --> E{Fragmentation evident?}\n  E --> F[Disable THP: echo never > /sys/kernel/mm/transparent_hugepage/enabled]\n  F --> G[Re-test latency and observability]\n  D --> G","difficulty":"intermediate","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Amazon","Apple","LinkedIn"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T17:14:33.406Z","createdAt":"2026-01-21T17:14:33.406Z"},{"id":"q-5398","question":"Scenario: A Linux host runs a lightweight server that writes logs to /var/log/app.log. During bursts, log writes slow and latency spikes. Using only default tooling, describe a concrete diagnostic workflow to (1) determine if I/O wait, CPU, or memory is the bottleneck, (2) identify the disk or process causing the bottleneck, and (3) apply a safe mitigation (e.g., adjust logging buffering or fsync behavior) while keeping observability. Include exact commands and example outputs?","answer":"Run iostat -xz 1 to confirm IO wait; vmstat 1 to monitor memory and swaps; pid=$(pgrep -f 'myserver'); pidstat -d 1 -p $pid; ls -l /proc/$pid/fd | grep app.log; strace -e write -p $pid -s 256 2>&1 | h","explanation":"## Why This Is Asked\nTests practical, beginner-friendly ability to diagnose IO-bound symptoms via standard tools. It emphasizes mapping IO to processes and safe mitigations without downtime.\n\n## Key Concepts\n- IO wait diagnosis with iostat/vmstat\n- Process-level IO attribution via pidstat/lsproc fd\n- Tracing writes with strace; safe mitigations: buffering, fsync timing, log rotation\n\n## Code Example\n\n```bash\n# example steps\n```\n\n## Follow-up Questions\n- How would you adapt this for a containerized service?\n- Which risks come with removing fsync in a production logger?","diagram":null,"difficulty":"beginner","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Snap","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T20:55:06.943Z","createdAt":"2026-01-21T20:55:06.943Z"},{"id":"q-5428","question":"On a Linux host running a single critical service with heavy disk I/O, latency spikes occur under load, yet CPU utilization and overall I/O wait seem inconclusive. Using only default Linux tools, design a concrete diagnostic workflow to determine whether the bottleneck is the IO scheduler, device queue depth, or page cache, and propose safe mitigations that preserve observability. Include exact commands and example outputs?","answer":"Begin with baseline monitoring using `iostat -dx 1 5` and `vmstat 1 5` to capture latency patterns. If `await` spikes while `svctm` remains stable, investigate the device queue: `cat /sys/block/sda/queue/nr_requests` and `cat /sys/block/sda/queue/read_ahead_kb`. Check the current scheduler with `cat /sys/block/sda/queue/scheduler`. For page cache analysis, use `sar -B 1 5` to examine page cache hit rates and `free -h` to monitor cache utilization. Test scheduler impact by switching to mq-deadline: `echo mq-deadline | sudo tee /sys/block/sda/queue/scheduler`. Safely mitigate by incrementally adjusting `nr_requests` (echo '256' | sudo tee /sys/block/sda/queue/nr_requests) and `read_ahead_kb` (echo '128' | sudo tee /sys/block/sda/queue/read_ahead_kb), monitoring changes after each adjustment.","explanation":"## Why This Is Asked\n\nTests ability to reason about storage bottlenecks with limited tooling, focusing on IO scheduler, queue depth, and page cache observability while maintaining system safety.\n\n## Key Concepts\n\n- IO scheduling and multi-queue devices\n- Observability with iostat/vmstat and /sys/block\n- Safe mitigations: adjust read_ahead_kb, nr_requests, mq-deadline\n- Page cache efficiency analysis\n\n## Code Example\n\n```bash\n# Baseline monitoring\niostat -dx 1 5\nvmstat 1 5\n\n# Device queue inspection\ncat /sys/block/sda/queue/nr_requests\ncat /sys/block/sda/queue/read_ahead_kb\ncat /sys/block/sda/queue/scheduler\n\n# Page cache analysis\nsar -B 1 5\nfree -h\n\n# Safe mitigation\necho mq-deadline | sudo tee /sys/block/sda/queue/scheduler\necho '256' | sudo tee /sys/block/sda/queue/nr_requests\necho '128' | sudo tee /sys/block/sda/queue/read_ahead_kb\n```\n\n## Follow-up Questions\n\n- How would you validate that the mitigation didn't degrade other IO paths?\n- What metrics would you use to determine if page cache is the primary bottleneck?\n- How do you balance performance improvements against system stability?","diagram":null,"difficulty":"intermediate","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Instacart","PayPal","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T06:04:25.797Z","createdAt":"2026-01-21T21:57:52.925Z"},{"id":"q-5472","question":"Scenario: A Linux host running a data-processing daemon intermittently reports EMFILE when creating thousands of temp files during bursts. CPU and I/O appear normal. Using only default tooling, outline a beginner-friendly diagnostic workflow to (1) confirm if file descriptors are the bottleneck vs CPU/memory/disk, (2) identify the offending limit or process, and (3) apply a safe mitigation (e.g., raise soft/hard limits, adjust fs.file-max, or tune the app) while preserving observability. Include exact commands and example outputs?","answer":"Begin by checking whether file descriptors are exhausted. Commands: cat /proc/sys/fs/file-max; ulimit -n; lsof | wc -l; for pid in /proc/[0-9]*/; do echo -n $pid: ; ls -1 /proc/$pid/fd 2>/dev/null | wc -l; done","explanation":"## Why This Is Asked\nTests practical, beginner-friendly debugging of a common Linux constraint: file descriptor exhaustion under bursts, using default tools only.\n\n## Key Concepts\n- File descriptors, limits (ulimit, fs.file-max)\n- Process fd usage (lsof, /proc/<pid>/fd)\n- Safe mitigations (limits.conf, sysctl, service restart)\n\n## Code Example\n```\n# Check current limits\ncat /proc/sys/fs/file-max\nulimit -n\nlsof | wc -l\n```\n\n## Follow-up Questions\n- How would you monitor fd usage over time in production?\n- What would you change to avoid starvation under spikes without compromising security?","diagram":"flowchart TD\n  A[User burst] --> B{Check fds}\n  B --> C{Fd exhausted?}\n  C -- Yes --> D[Increase limits & restart]\n  C -- No --> E[Investigate other bottlenecks]","difficulty":"beginner","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Lyft","Microsoft","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T05:40:53.862Z","createdAt":"2026-01-21T23:45:17.953Z"},{"id":"q-553","question":"You're troubleshooting a production server where a critical process keeps getting killed. How would you diagnose if it's an OOM kill versus other issues, and what specific commands would you use to investigate?","answer":"Check dmesg for 'Out of memory' messages and /var/log/messages. Use `free -h` to see memory usage, `ps aux --sort=-%mem` to find memory hogs, and `cat /proc/<pid>/status` for VmRSS. If OOM, check `/proc/sys/vm/panic_on_oom` and `/proc/sys/vm/oom_kill_allocating_task` to understand kernel behavior.","explanation":"## Diagnosis Steps\n- Check system logs for OOM kill indicators\n- Analyze current memory usage patterns\n- Identify memory-intensive processes\n\n## Key Commands\n```bash\ndmesg | grep -i oom\nfree -h\nps aux --sort=-%mem | head -10\ncat /proc/$(pidof process)/status | grep Vm\n```\n\n## Prevention\n- Monitor memory usage with `top`/`htop`\n- Set up alerts for high memory usage\n- Configure appropriate swap space\n- Tune kernel parameters like `vm.swappiness`","diagram":"flowchart TD\n  A[Process Killed] --> B{Check dmesg}\n  B -->|OOM messages| C[Analyze memory usage]\n  B -->|No OOM| D[Check other signals]\n  C --> E[Identify memory hogs]\n  E --> F[Tune kernel params]\n  D --> G[Check logs/crash dumps]","difficulty":"intermediate","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Scale Ai","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-08T11:55:47.944Z","createdAt":"2025-12-26 12:51:07"},{"id":"q-5646","question":"Scenario: A Linux host serving a Python API experiences sudden tail latency spikes during bursts; CPU, memory, and disk I/O look normal. Using only default tooling, design a concrete diagnostic workflow to (1) determine if the bottleneck is network (sockets, NIC IRQs, or queue depth) or compute/IO, (2) identify the offending interface, process, or socket, and (3) apply a safe mitigation (e.g., adjust socket buffers, net core tunables, or scale workers) while preserving observability. Include exact commands and sample outputs?","answer":"Run iostat -dx 1 to spot IO wait, sar -n DEV 1 for NIC pressure, and ss -tpan to inspect sockets. Inspect /proc/interrupts and ethtool -S ethX to locate hot IRQs and RX/TX queues. Use pidstat -p ALL 1","explanation":"## Why This Is Asked\nDesigning a diagnostic workflow to distinguish network vs CPU/IO bottlenecks on a live Linux host is essential. The candidate should justify using baseline observability, then converge on a bottleneck with concrete commands.\n\n## Key Concepts\n- IO wait vs network pressure\n- Tools: iostat, sar, ss, ethtool, pidstat\n- Safe mitigations: socket buffers, backlog, worker tuning\n\n## Code Example\n```javascript\n// pseudo parsing of iostat outputs in JS (illustrative) \n```\n\n## Follow-up Questions\n- How would you validate the mitigation's effectiveness?\n- How would you scale this approach to multiple hosts?","diagram":null,"difficulty":"beginner","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Citadel","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T09:55:04.113Z","createdAt":"2026-01-22T09:55:04.113Z"},{"id":"q-5680","question":"Scenario: A Linux node in a high-throughput data-processing cluster runs a CUDA-accelerated workload and intermittently stalls under peak load. Memory usage looks normal, but latency spikes coincide with page faults and NUMA imbalance. Using only default tooling, design a concrete diagnostic workflow to (1) determine if the stall is NUMA memory pressure vs page-cache or swap, (2) identify the exact process or subsystem responsible, and (3) apply a safe mitigation (e.g., pinning workloads with NUMA/cgroups, adjusting swappiness or THP, or disabling swap) while preserving observability. Include exact commands and example outputs?","answer":"Run vmstat 1, free -m, and slabtop to detect memory pressure or kernel caches. Use top/ps to find large consumers; inspect /proc/<pid>/numa_maps and run numactl -H to verify NUMA layout. If imbalance,","explanation":"## Why This Is Asked\nTests proficiency in diagnosing subtle Linux memory issues with real-world workload patterns (NUMA pressure, page cache, THP) using only default tooling. It also probes how candidates identify root causes and implement safe mitigations that preserve observability.\n\n## Key Concepts\n- NUMA topology and memory binding\n- Page cache vs. swap pressure\n- Kernel memory caches (slab) and THP\n- Cgroup and NUMA pinning for safe mitigations\n\n## Code Example\n```bash\nvmstat 1 5\nfree -m\ntop -b -n1 | head -n 20\n```\n\n## Follow-up Questions\n- How would you adapt this to a multi-node Kubernetes cluster with GPU workloads?\n- What safeguards ensure observability during mitigation rollout?","diagram":"flowchart TD\n  A[Start] --> B[Collect metrics: vmstat, free, slabtop]\n  B --> C[Check NUMA: /proc or numactl -H]\n  C --> D[Identify process: top/ps; /proc/<pid>/numa_maps]\n  D --> E[Mitigate: pin with numactl/cgroups or swap/THP tweaks]\n  E --> F[Verify: metrics stable, observability]","difficulty":"advanced","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":null,"companies":["MongoDB","NVIDIA","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T11:06:50.324Z","createdAt":"2026-01-22T11:06:50.324Z"},{"id":"q-5748","question":"Scenario: A Linux host runs a MongoDB primary. During bursts, new data file creation occasionally fails with ENOSPC even though df shows free space. Using only default tooling, describe a concrete, beginner-friendly diagnostic workflow to (1) determine whether space, inodes, or reserved blocks are the bottleneck, (2) identify the offending filesystem or mount, and (3) apply a safe mitigation (e.g., reduce reserved blocks, prune files, or expand storage) while preserving observability. Include exact commands and example outputs?","answer":"Run df -h and df -i to distinguish space vs inodes. If ENOSPC persists, run du -xS /var/lib/mongo | sort -n -r | head -n 20 to locate large dirs. Check mount with mount | grep mongo and inspect filesy","explanation":"## Why This Is Asked\nTests practical disk/resource troubleshooting on Linux with MongoDB, focusing on a subtle ENOSPC case where space looks available.\n\n## Key Concepts\n- Space vs inode vs reserved blocks\n- Filesystem tuning (reserved blocks) and safe mitigation\n- Using default tools to locate large dirs and verify mounts\n\n## Code Example\n```bash\ndf -h\ndf -i\ndu -xS /var/lib/mongo | sort -n -r | head -n 20\nsudo tune2fs -m 1 /dev/sdXN\n```\n\n## Follow-up Questions\n- How would you validate the safety of reducing reserved blocks in production?\n- What alerts would you add to detect reoccurrence?","diagram":"flowchart TD\n  A[MongoDB ENOSPC during writes] --> B[Check df -h and df -i]\n  B --> C{Space or inodes OK?}\n  C -->|Yes| D[Find large dirs with du -xS / | sort -n -r | head]\n  C -->|No| E[Inspect mount and filesystem]\n  D --> F[If reserved blocks high: tune2fs -m 1 /dev/sdXN]\n  F --> G[Expand storage / resize FS as needed]\n  G --> H[Observe mongod logs and dmesg]","difficulty":"beginner","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Discord","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T15:03:02.390Z","createdAt":"2026-01-22T15:03:02.390Z"},{"id":"q-580","question":"How would you find all processes using a specific port and terminate one safely?","answer":"Use `lsof -i :8080` to list processes on port 8080, then `kill -15 <PID>` for graceful shutdown. If unresponsive, use `kill -9 <PID>` as last resort. Always check process dependencies first.","explanation":"## Finding Processes\n- `lsof -i :<port>` lists processes using the port\n- `netstat -tulpn | grep :<port>` alternative method\n\n## Safe Termination\n- `kill -15 <PID>` sends SIGTERM for graceful shutdown\n- Allows process to cleanup resources and save state\n- Wait reasonable time before escalating\n\n## Force Termination\n- `kill -9 <PID>` sends SIGKILL as last resort\n- Immediate termination without cleanup\n- Can cause data corruption or resource leaks","diagram":"flowchart TD\n  A[Identify Port] --> B[lsof -i :port]\n  B --> C[List PIDs]\n  C --> D{Process Responsive?}\n  D -->|Yes| E[kill -15 PID]\n  D -->|No| F[kill -9 PID]\n  E --> G[Monitor Shutdown]\n  F --> H[Force Terminate]","difficulty":"beginner","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","MongoDB","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":["lsof","process termination","graceful shutdown","pid","port","dependencies","kill command"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2025-12-27T04:58:08.288Z","createdAt":"2025-12-27T01:13:40.807Z"},{"id":"q-5830","question":"NUMA-Aware Latency: A Linux host in a multi-tenant analytics cluster shows intermittent 5–20 ms tail latency on a service under sustained I/O. Using only default tooling, design a concrete diagnostic workflow to (1) confirm if latency is caused by remote NUMA memory access, (2) identify the offending process or memory pattern, and (3) apply a safe mitigation such as binding to a local NUMA node or selective interleaving, while preserving observability. Include exact commands and expected outputs?","answer":"Plan: 1) numactl --hardware to map NUMA nodes; 2) numastat -p <pid> and cat /proc/<pid>/numa_maps to spot remote allocations; correlate spikes with remote pages; 3) if remote access dominates, apply m","explanation":"## Why This Is Asked\nTests NUMA topology reasoning and safe production mitigation under load.\n\n## Key Concepts\n- NUMA topology and memory policies\n- Per-process NUMA accounting with /proc and numastat\n- Safe mitigation via membind/cpunodebind and thread pinning\n\n## Code Example\n```bash\nnumactl --hardware\nnumastat -p 12345\ncat /proc/12345/numa_maps\n```\n\n## Follow-up Questions\n- How would you scale this to hundreds of processes?\n- What are the risks of binding to a single node under mixed workloads?","diagram":"flowchart TD\nA[Start] --> B[Check NUMA topology: numactl --hardware]\nB --> C[Inspect per-PID NUMA usage: numastat -p PID, /proc/PID/numa_maps]\nC --> D{Remote allocations present?}\nD -->|Yes| E[Bind to local node: numactl --membind=0 --cpunodebind=0 -p PID]\nD -->|No| F[Monitor and log observability]\nE --> G[Reassess latency with pidstat/iostat]\nF --> G\nG --> H[Done]","difficulty":"advanced","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Meta","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T18:49:31.352Z","createdAt":"2026-01-22T18:49:31.352Z"},{"id":"q-5956","question":"Scenario: A Linux host running a distributed log-collector experiences sporadic 100-300 ms pauses during bursts. CPU, I/O, and free memory look normal. Using only default tooling, design a concrete diagnostic workflow to (1) prove whether cache/TLB misses or lock contention cause the pauses, (2) pinpoint the offending thread(s) and kernel path (e.g., mmap region, page faults, or IRQs), and (3) apply a safe mitigation (e.g., pin frequently woken threads, adjust CPU affinity, NUMA locality, or reduce contention) while preserving observability. Include exact commands and expected outputs?","answer":"Run: `top -b -n1`; `vmstat 1`; `iostat -xz 1 2`; `pidstat -p ALL 1` to identify hot processes; then `perf stat -e cache-references,cache-misses,cycles -p <pid> -I 1000`; `perf top -p <pid>`. If cache misses are high, pin frequently woken threads using `taskset -c <cpu-list> <pid>` and verify with `perf record -e cache-misses -p <pid> && perf report`. For lock contention, use `perf record -e sched:sched_switch -e lock:lock_acquire -p <pid> && perf report` to identify contended locks and offending threads.","explanation":"## Why This Is Asked\nAdvanced troubleshooting for subtle kernel/CPU/cache bottlenecks in distributed systems.\n\n## Key Concepts\n- perf cache metrics and TLB analysis\n- CPU affinity and thread pinning\n- NUMA locality optimization\n- Observability through repeated sampling\n\n## Code Example\n```bash\n# baseline metrics\ntop -b -n1\nvmstat 1\niostat -xz 1 2\n# process identification\npidstat -p ALL 1\n# cache analysis\nperf stat -e cache-references,cache-misses,cycles -p <pid> -I 1000\n```\n\n## Follow-up Questions\n- How would you generalize to a multi-tenant cluster?\n- What if perf is patched out on your distribution?","diagram":"flowchart TD\n  A[Baseline] --> B[Narrowing]\n  B --> C[Identify culprit]\n  C --> D[Mitigate]\n  D --> E[Observability]","difficulty":"advanced","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Hugging Face","Lyft","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T05:17:08.689Z","createdAt":"2026-01-22T23:43:46.076Z"},{"id":"q-6123","question":"A Linux host with multiple NICs experiences sporadic 100–200 ms latency spikes under peak traffic. Using only default tooling, design a diagnostic workflow to determine if IRQ affinity, RSS/XPS mapping, or CPU shielding is causing the latency, identify the exact NIC queue and CPU involved, and apply a safe mitigation (e.g., pin IRQs, adjust RPS/XPS, or isolate CPUs) while preserving observability. Include exact commands and example outputs?","answer":"Audit IRQ flow and NIC queue mapping to diagnose tail latency. Key steps: 1) show IRQs: cat /proc/interrupts | grep -i eth0; 2) read per-IRQ affinity: for irq in $(grep -i eth0 /proc/interrupts | awk ","explanation":"## Why This Is Asked\n\nTests practical, low-level Linux networking diagnosis under load, focusing on real-world tail-latency issues.\n\n## Key Concepts\n\n- IRQ affinity and per-IRQ cpus\n- RSS/XPS mappings per NIC queue\n- CPU isolation and observability during mitigations\n\n## Code Example\n\n```bash\n# Inspect IRQs for eth0\ncat /proc/interrupts | grep -i eth0\n\n# Show per-IRQ affinity when possible\nfor irq in $(grep -i eth0 /proc/interrupts | awk '{print $1}'); do echo IRQ $irq:; cat /proc/irq/$irq/smp_affinity 2>/dev/null; done\n\n# Check NIC queue mappings\ncat /sys/class/net/eth0/queues/rx-*/rps_cpus\n```\n\n## Follow-up Questions\n\n- How would you verify improvement after a mitigation?\n- What are risks of pinning IRQs and how to rollback?","diagram":"flowchart TD\n  A[Start] --> B[Identify RX IRQs per NIC] \n  B --> C[Check per-IRQ smp_affinity] \n  C --> D[Examine RPS/XPS mappings] \n  D --> E[Apply mitigations (IRQ pinning, RPS, CPU isolation)] \n  E --> F[Re-test and observe] ","difficulty":"advanced","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Airbnb","NVIDIA","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T09:55:05.692Z","createdAt":"2026-01-23T09:55:05.692Z"},{"id":"q-6192","question":"A Linux host running GPU-accelerated inference workloads in containers shows sporadic 0.5–2 ms latency spikes at the 99th percentile under peak traffic. Using only default tools, design a concrete diagnostic workflow to (1) determine if latency spikes are CPU-, I/O-, memory-, or PCIe/NVMe-related, (2) identify the exact component causing the stall (process, driver, IRQ, or device), and (3) apply a safe mitigation (e.g., CPU shielding with cpuset/isolated CPUs, IRQ affinity tuning, or I/O scheduler changes) while preserving observability. Include exact commands and sample outputs?","answer":"Begin with baseline metrics from perf and I/O tools. Commands: perf stat -a -e cycles,instructions,branch-misses -I 1000 sleep 5; iostat -xz 1; vmstat 1. Then inspect interrupts and scheduling: cat /p","explanation":"Why This Is Asked\n\nTests practical, end-to-end skills in diagnosing production Linux bottlenecks with GPUs using only built-in tools. Emphasizes real-world sequencing, safe mitigations, and maintaining observability across containers.\n\nKey Concepts\n\n- Performance baselining with perf and iostat\n- Kernel IRQ and scheduling introspection\n- CPU isolation and IRQ affinity for low-latency workloads\n- I/O scheduler choices and NUMA considerations\n- Observability during containerized workloads\n\nCode Example\n\n```javascript\n# Baseline metrics\nperf stat -a -e cycles,instructions,branch-misses -I 1000 sleep 5\niostat -xz 1\n\n# Inspect kernel events\ncat /proc/interrupts\ncat /proc/schedstat\n\n# Isolation and affinity (examples)\necho 1 > /proc/sys/kernel/softirq_depth\n# Pin latency-critical CPUs to a cpuset, then test\n```","diagram":"flowchart TD\n  A(Start) --> B(Baseline with perf/iostat)\n  B --> C(Check /proc/interrupts and schedstat)\n  C --> D(Isolate latency-critical CPUs)\n  D --> E(Tune IRQ affinity and IO scheduler)\n  E --> F(Validate with repeated runs)","difficulty":"advanced","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Citadel","IBM","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T13:24:51.179Z","createdAt":"2026-01-23T13:24:51.179Z"},{"id":"q-6237","question":"A Linux host running a Java service shows a slow boot: after reboot, systemd reports the service starting late and the entire boot takes about 90 seconds. Using only default tooling, describe a beginner-friendly diagnostic workflow to (1) identify the slow boot unit and root cause, (2) determine if it is a dependency, timeout, or hardware wait, and (3) apply a safe mitigation while preserving observability. Include exact commands and expected outputs?","answer":"Use systemd-analyze blame to find the slow boot unit, then systemd-analyze critical-chain to map dependencies. Inspect logs with journalctl -b -u <service> and dmesg for hardware waits. For mitigation","explanation":"Why This Is Asked\nTests practical boot-time debugging using systemd tooling, journald, and kernel logs. It checks ability to isolate a slow unit, understand dependency or timeout causes, and apply minimal, observable mitigations without downtime.\n\nKey Concepts\n- systemd-analyze blame\n- systemd-analyze critical-chain\n- journalctl -b\n- dmesg\n- unit dependencies (After/Requires)\n\nCode Example\n```bash\nsystemd-analyze blame\nsystemd-analyze critical-chain\njournalctl -b -u myservice -e\n```\n\nFollow-up Questions\n- How would you diagnose a slow mount or network dependency delaying boot?\n- What would you monitor after applying a mitigation to ensure observability stays intact?","diagram":"flowchart TD\n  Boot[Boot Process] --> SlowUnit[Slow boot unit]\n  SlowUnit --> Logs[Review Logs]\n  Logs --> Mitigation[Apply safe mitigation]\n  Mitigation --> Verify[Verify boot timing]","difficulty":"beginner","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Google","Square","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T15:52:04.694Z","createdAt":"2026-01-23T15:52:04.694Z"},{"id":"q-6322","question":"Scenario: A Linux server hosting several containerized services suffers intermittent network latency spikes during bursts. The system shows high soft IRQ load on various CPUs. Using only default tools, design a concrete diagnostic workflow to (1) confirm IRQ/NET-related latency, (2) identify the offending NIC or interrupt vector, and (3) apply a safe mitigation (e.g., isolate IRQs to dedicated CPUs, adjust affinity, disable irqbalance) while preserving observability. Include exact commands and example outputs?","answer":"Verify IRQ impact with vmstat 1 and mpstat -P ALL 1, then inspect /proc/interrupts to find the top vector: cat /proc/interrupts | sort -nrk2 | head -n1. Note its device (e.g., eth0). Pin it: echo <mas","explanation":"## Why This Is Asked\nThis question probes practical debugging of multi-tenant Linux systems where network latency spikes are driven by IRQ contention. It tests observation, diagnosis, and safe mitigation without extra tooling.\n\n## Key Concepts\n- Linux IRQs, /proc/interrupts, smp_affinity, isolcpus, irqbalance\n- Observability: vmstat, mpstat, sar, ping, baseline vs post-change\n- Caution: avoid starving essential interrupts during isolation\n\n## Code Example\n```bash\nvmstat 1\nmpstat -P ALL 1\ncat /proc/interrupts | sort -nrk2 | head -n1\n```\n\n## Follow-up Questions\n- How would you validate multi-NIC environments with overlapping interrupts?\n- What trade-offs exist between isolation and CPU utilization?","diagram":null,"difficulty":"intermediate","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Adobe","IBM"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T19:12:17.787Z","createdAt":"2026-01-23T19:12:17.787Z"},{"id":"q-6373","question":"Scenario: A Linux host running a MongoDB shard experiences sporadic 2–5 second stalls under high write load. Using only default Linux tools, design a concrete diagnostic workflow that (1) confirms storage I/O as the stall source, (2) identifies the exact device/queue causing latency, and (3) prescribes a safe mitigation (e.g., io-scheduler tuning, queue depth adjustments, or multi-queue enablement) while preserving observability. Include exact commands and example outputs?","answer":"Start with `iostat -dx 1 2` to confirm elevated await times on the storage device. Use `iotop -P -o -d 1` to identify the specific process responsible for heavy I/O. Run `blktrace -d /dev/nvme0n1 -D /tmp/blktrace` followed by `blkparse` to analyze the block I/O patterns and pinpoint queue-level bottlenecks. Cross-reference with MongoDB's `db.serverStatus().metrics.document` to correlate I/O stalls with database operations.","explanation":"## Why This Is Asked\nAdvanced storage-path diagnostics are critical for latency-sensitive workloads like MongoDB. This question probes practical use of default tools to isolate I/O bottlenecks, map them to devices and queues, and implement safe, observable mitigations.\n\n## Key Concepts\n- I/O wait diagnostics with iostat and iotop\n- Block-layer tracing with blktrace/blkparse\n- MongoDB latency visibility via serverStatus\n- Kernel I/O schedulers and per-device tuning\n\n## Code Example\n```bash\n# Example commands for quick reference\niostat -dx 1 2\niotop -P -o -d 1\nblktrace -d /dev/nvme0n1 -D /tmp/blktrace\nblkparse /tmp/blktrace\n```","diagram":"flowchart TD\n  A[Start] --> B{Check I/O wait}\n  B --> C[Identify device/queue]\n  C --> D{High latency?}\n  D --> E[Apply mitigations]\n  E --> F[Observe impact]\n  F --> G[End]","difficulty":"advanced","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":null,"companies":["MongoDB","Tesla","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T05:35:37.567Z","createdAt":"2026-01-23T21:35:19.515Z"},{"id":"q-6547","question":"You manage a Linux server with multiple NVIDIA GPUs in a NUMA system, running containerized ML inference workloads. At peak load, one container shows tail latency spikes due to CPU cache/memory bandwidth contention that plausibly affects PCIe DMA to GPUs. Using only default Linux tools, design a concrete diagnostic workflow to: (1) confirm cache/bandwidth contention on the GPU path, (2) identify the offending container/process/NUMA region, (3) apply a safe mitigation (e.g., NUMA policy changes, isolcpus, memory bandwidth throttling via cgroups/mem policy, or cpuset placement) while preserving observability. Include exact commands and sample outputs?","answer":"Use perf and NUMA tools to confirm cache/bandwidth contention and pin the workload. Steps: 1) perf stat -e cycles,instructions,cache-references,cache-misses -a 5; sar -B 1 5; 2) numactl --hardware; ps","explanation":"## Why This Is Asked\n\nTests the ability to design a reproducible, data-driven workflow for complex hardware interference in multi-tenant Linux environments, integrating performance counters, NUMA awareness, and safe resource isolation.\n\n## Key Concepts\n\n- NUMA affinity and memory locality\n- CPU cache/memory bandwidth contention and PCIe DMA paths\n- cpuset/cgroups2 for workload isolation\n- perf, sar, numactl, ps/pidstat for observability\n\n## Code Example\n\n```javascript\n// Example: tiny parser for perf output (pseudo)\nfunction parsePerf(line){const [e,v]=line.split(':'); return {event:e, value:parseInt(v)}}\n```\n\n## Follow-up Questions\n\n- How would you adapt the workflow for a live Kubernetes cluster?\n- What kernel flags or cgroup adjustments would you consider and why?","diagram":"flowchart TD\n  A[Start] --> B[Run perf stat and sar]\n  B --> C{Contention observed?}\n  C -->|Yes| D[Identify offender via ps/numactl maps]\n  D --> E[Create cpuset and bind tasks]\n  E --> F[Re-measure and monitor observability]\n  C -->|No| G[Check alternative bottlenecks]","difficulty":"advanced","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":null,"companies":["NVIDIA","Netflix","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T07:40:56.095Z","createdAt":"2026-01-24T07:40:56.096Z"},{"id":"q-6616","question":"Scenario: A Linux host runs multiple containers on a NUMA machine with memory-hungry workloads. Under peak load, tail latency spikes appear and memory reclaim metrics rise, yet there is no swapping. Using only default tools, design a concrete diagnostic workflow to (1) confirm memory reclaim/bandwidth as the limiter, (2) identify the container/NUMA node responsible, (3) apply safe mitigations (e.g., isolcpus, mempolicy, or cpuset changes) while preserving observability. Include exact commands and example outputs?","answer":"Begin by confirming memory pressure and reclaim delay: vmstat 1 5; free -m; sar -r 1 60; dmesg | tail -n 100 | grep -i memory -m 5. Identify the culprit with docker stats and PID mapping, then tie to ","explanation":"## Why This Is Asked\nTests depth in diagnosing memory pressure on NUMA systems with container workloads using only default tools.\n\n## Key Concepts\n- Memory pressure vs reclaim latency\n- NUMA topology, cpuset, mempolicy\n- Observability with vmstat/sar/dmesg/logs\n\n## Code Example\n```bash\n#!/bin/bash\nvmstat 1 5\n```\n\n## Follow-up Questions\n- How would you adapt if swapping is enabled?\n- What ootb metrics validate long-term mitigations?","diagram":"flowchart TD\n  Start[Start] --> Verify memory pressure\n  Verify memory pressure --> Identify container/NUMA node\n  Identify container/NUMA node --> Mitigate\n  Mitigate --> Validate observability","difficulty":"advanced","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Plaid","Salesforce","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T10:34:41.264Z","createdAt":"2026-01-24T10:34:41.264Z"},{"id":"q-6727","question":"Scenario: A Linux server's boot sequence is intermittently slow, delaying a critical daemon by several seconds. Using only default tooling, design a beginner-friendly diagnostic workflow to (1) determine if the delay is due to systemd unit dependencies, sockets, or timers, (2) identify the bottleneck unit, and (3) apply a safe mitigation (e.g., adjust TimeoutStartSec, reorder dependencies, or tweak StartLimitInterval) while preserving observability. Include exact commands and sample outputs?","answer":"To diagnose boot delay, run systemd-analyze blame; systemd-analyze critical-chain; and journalctl -b -u <slow-unit> to inspect events. Identify the bottleneck by the dependency chain and by checking d","explanation":"## Why This Is Asked\n\nNew angle focusing on systemd boot delays, not covered by prior questions, and highly relevant in large-scale Linux deployments.\n\n## Key Concepts\n\n- systemd analyze utilities\n- unit dependencies and parallel startup\n- journalctl filtering and unit logs\n- safe mitigations: TimeoutStartSec, StartLimitInterval, dependency reordering\n\n## Code Example\n\n```bash\nsystemd-analyze blame\nsystemd-analyze critical-chain\njournalctl -b -u slow.service\n```\n\n## Follow-up Questions\n\n- How would you verify a fix doesn't impact other services?\n- What are the trade-offs of increasing TimeoutStartSec on a critical daemon?","diagram":"flowchart TD\n  Boot[Boot Process] --> A[Identify bottleneck with systemd-analyze blame]\n  A --> B[Inspect critical chain]\n  B --> C[Inspect unit logs with journalctl]\n  C --> D[Mitigate by TimeoutStartSec or dependencies]\n  D --> End[Verify with same tooling]","difficulty":"beginner","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Cloudflare","Goldman Sachs","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T14:46:49.902Z","createdAt":"2026-01-24T14:46:49.902Z"},{"id":"q-6789","question":"Scenario: A Linux host serving a MongoDB shard in containers on a NUMA machine shows 95th percentile latency spikes under peak load. Using only default Linux tools, design a concrete diagnostic workflow to (1) confirm if CPU frequency scaling or C-state transitions align with latency tails, (2) locate the exact CPUs/NUMA nodes involved, (3) apply safe mitigations (e.g., set governors to performance, pin critical processes, disable deep C-states) while preserving observability. Include exact commands and example outputs?","answer":"Design a workflow to determine if latency tails follow CPU power transitions. (1) correlate latency with CPUfreq/C-states via pidstat, iostat, and /sys/devices/system/cpu/cpu*/cpufreq/scaling_cur_freq","explanation":"## Why This Is Asked\nTests advanced Linux diagnostics for production latency issues in NUMA/containerized workloads, focusing on power management and scheduling interactions rather than generic performance.\n\n## Key Concepts\n- CPU freq scaling, C-states, and latency tail effects\n- NUMA topology and per-node contention\n- Safe mitigations: CPU isolation, governor tuning, process pinning, observability hooks\n\n## Code Example\n```bash\n# Example commands\ngrep -H . /sys/devices/system/cpu/cpu*/cpufreq/scaling_cur_freq\ncat /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor\nnumactl -H\npidstat 1\nperf stat -e cycles,instructions,cache-references,cache-misses -p <pid> & sleep 5; kill $!;```\n```\n\n## Follow-up Questions\n- How would you validate a remediation without regressing throughput?\n- Which metrics would you surface to SRE dashboards to detect reoccurrence?","diagram":null,"difficulty":"advanced","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Instacart","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T17:40:04.119Z","createdAt":"2026-01-24T17:40:04.119Z"},{"id":"q-6880","question":"Scenario: A Linux host runs a Node.js API behind Nginx. During bursts, tail latency spikes while CPU, memory, and disk I/O look normal. Using only default tooling, design a beginner-friendly diagnostic workflow to (1) determine if stalls are due to Node.js event loop, Nginx worker, or kernel scheduling, (2) identify the exact component responsible, and (3) apply a safe mitigation (e.g., adjust Nginx worker_processes, backlog, or CPU affinity; or tune Node.js clustering) while preserving observability. Include exact commands and example outputs?","answer":"Establish performance baselines during normal and burst periods: `top -b -n1`, `pidstat -p ALL 1`, `iostat -dx 1`, `vmstat 1`. Correlate latency spikes with these metrics. Use `strace -tt -p <pid>` to identify blocking system calls in the problematic process. Apply targeted mitigations: for Nginx, adjust `worker_processes`, `worker_connections`, or `net.core.somaxconn`; for Node.js, enable clustering or tune event loop parameters. Validate changes maintain observability while reducing tail latency.","explanation":"## Why This Is Asked\n\nTests practical Linux troubleshooting skills using default system tools for diagnosing tail latency in Node.js/Nginx deployments.\n\n## Key Concepts\n\n- Performance baseline measurement using top/pidstat/iostat/vmstat\n- Distinguishing user-space vs kernel bottlenecks through strace analysis\n- Safe configuration tuning: Nginx worker optimization, Linux socket parameters, Node.js clustering\n\n## Code Example\n\n```bash\n# Establish baseline metrics\ntop -b -n1\npidstat -p ALL 1\niostat -dx 1\nvmstat 1\n```\n\n## Follow-up Questions\n\n- How would this approach adapt to containerized environments?","diagram":"flowchart TD\n  A[Start] --> B[Baseline metrics]\n  B --> C[Culprit]\n  C --> D[Apply mitigations]\n  D --> E[Verify observability]","difficulty":"beginner","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":null,"companies":["PayPal","Snap","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T06:18:08.079Z","createdAt":"2026-01-24T21:32:30.951Z"},{"id":"q-6949","question":"Scenario: A Linux host with two CPU sockets runs several containerized services. Under sustained load, tail latency spikes for one service despite normal CPU and I/O metrics. Using only default tools, design a concrete diagnostic workflow to (1) confirm NUMA-related memory access latency as the cause, (2) identify the offending NUMA node(s) and memory policy, and (3) apply a safe mitigation (e.g., bind the culprit process/container to its local NUMA node with numactl, adjust memory policy, or move the container) while preserving observability. Include exact commands and example outputs?","answer":"Use default tools to diagnose NUMA tail latency: 1) `numactl --hardware` to show topology; 2) `pid=$(pgrep -f 'service-name')`; `cat /proc/$pid/numa_maps | head` to inspect locality; 3) `numastat` to gauge NUMA node imbalance; 4) `numactl --cpunodebind=X --membind=X -p $pid` to bind process to local node","explanation":"## Why This Is Asked\nTests practical NUMA awareness and diagnostics under latency pressure using only default tools.\n\n## Key Concepts\n- NUMA topology and memory policy\n- /proc/[pid]/numa_maps and numastat\n- Locality impact on latency\n- Safe mitigation via numactl bindings\n\n## Code Example\n```bash\nnumactl --hardware\npid=$(pgrep -f 'service-name')\nawk '{print $1, $2, $3}' /proc/$pid/numa_maps | head\nnumastat\npmap -x $pid | head\nnumactl --cpunodebind=0 --membind=0 -p $pid\n```\n\n## Follow-up Questions\n- How would you monitor long-term NUMA effects across reboots?\n- How to handle multi-threaded containers with NUMA awareness?","diagram":"flowchart TD\n  A[Identify tail latency] --> B[Check NUMA topology]\n  B --> C[Inspect per-pid NUMA maps]\n  C --> D[Survey NUMA memory stats]\n  D --> E[Apply NUMA binding]\n  E --> F[Validate latency change]","difficulty":"intermediate","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Apple","Coinbase","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T05:34:27.789Z","createdAt":"2026-01-24T23:52:08.229Z"},{"id":"q-7122","question":"A Linux server runs a simple Python data-worker started via systemd. During bursts, response times deteriorate for 1–2 minutes while CPU and memory appear idle. Using only default tooling, describe a concrete diagnostic workflow to (1) determine if stalls are CPU scheduling, I/O wait, or memory pressure, (2) pinpoint the exact resource or process causing the stall, and (3) apply a safe mitigation (e.g., adjust I/O priority, pin workers, or cap memory) while maintaining observability. Include exact commands and sample outputs?","answer":"Run vmstat 1 5 to capture swpd, free, buff/cache; iostat -xz 1 5 to observe device util and wa; top -b -n1 | head to spot hogs; ps -eo pid,ppid,cmd,%mem,%cpu --sort=-%cpu | head to identify culprit. I","explanation":"## Why This Is Asked\nTests practical Linux diagnostic skills using default tools to distinguish CPU, IO, and memory stalls.\n\n## Key Concepts\n- vmstat, iostat, top/ps, I/O priorities, cgroups/memory limits, per-process bottlenecks\n\n## Code Example\n```bash\n# Capture a quick snapshot\nvmstat 1 5\niostat -xz 1 5\n```\n\n## Follow-up Questions\n- How would you automate this data collection?\n- What tooling limitations might you encounter and how would you work around them?","diagram":"flowchart TD\n  A[Start] --> B[Run vmstat and iostat]\n  B --> C{Is wa high?}\n  C -- Yes --> D[Identify culprit with top/ps]\n  C -- No --> E{Is swap growing?}\n  E -- Yes --> F[Limit memory with cgroups/systemd]\n  E -- No --> G[Tune IO priority or workers]\n  D --> H[Re-run stats]\n  F --> H\n  G --> H\n  H --> I[Done]","difficulty":"beginner","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Instacart","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T09:51:44.087Z","createdAt":"2026-01-25T09:51:44.087Z"},{"id":"q-7175","question":"Scenario: A Linux host runs a Redis service behind a reverse proxy. During bursts, write latency tails spike even though CPU, memory, and network look normal. Using only default tooling, outline a concrete diagnostic workflow to (1) confirm if I/O wait or CPU is the bottleneck, (2) pinpoint the disk IO pattern and the process driving it, and (3) apply a safe mitigation (e.g., switch IO scheduler, adjust Redis fsync, or buffer writes) while preserving observability. Include exact commands and sample outputs?","answer":"Use iostat -x 1 to detect iowait, pidstat -d 1 -p $(pgrep redis-server) to map IO to Redis, and vmstat 1 to watch swapping. If iowait spikes, switch the device scheduler to deadline and adjust Redis p","explanation":"## Why This Is Asked\\nGauges practical OS-level diagnostics for a common web service; uses beginner-friendly tools.\\n\\n## Key Concepts\\n- iostat, pidstat, vmstat; IO schedulers; Redis persistence knobs\\n\\n## Code Example\\n```javascript\\n// Example commands used in workflow\\niostat -x 1\\npidstat -d 1 -p $(pgrep redis-server)\\nvmstat 1\\n```\\n\\n```bash\\n# Switch IO scheduler (example)\\necho deadline > /sys/block/sdX/queue/scheduler\\n# Redis persistence tweak (in redis.conf)\\nappendfsync everysec\\n```\\n\\n## Follow-up Questions\\n- What checks if latency remains after scheduler change?\\n- How to revert scheduler change safely?","diagram":null,"difficulty":"beginner","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Cloudflare","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T13:02:16.547Z","createdAt":"2026-01-25T13:02:16.547Z"},{"id":"q-7231","question":"Scenario: A Linux host runs multiple containerized microservices on a dense NUMA machine. At peak IO, 99th percentile latency spikes for a database-like service. Using only default tools, design a concrete diagnostic workflow to (1) confirm whether memory pressure or page cache eviction contributes to latency tails, (2) identify the NUMA node(s) and container/process(es) contributing to the pressure, and (3) apply safe mitigations (e.g., cap containers with cgroups, adjust swapiness, enforce per-NUMA memory binding) while preserving observability. Include exact commands and example outputs?","answer":"Begin by collecting memory and IO metrics with vmstat, free/meminfo, slabtop, and iostat; map NUMA layout with numactl --hardware; identify pressure sources per container via /proc/*/smaps or docker s","explanation":"## Why This Is Asked\nThis question tests deep knowledge of Linux memory management, NUMA behavior, and containerization under load. The responder must reason about memory reclaim, page cache vs swap, kernel slab allocations, and per-container memory pressure, plus practical mitigations that preserve observability.\n\n## Key Concepts\n- Memory pressure signals in /proc/meminfo, vmstat, slabtop\n- NUMA node binding and cpuset/numactl usage\n- Per-container memory limits and swapping behavior\n- Observability with iostat/vmstat/sar and container metrics\n\n## Code Example\n```javascript\n#!/usr/bin/env bash\n# collect metrics for 60s\nvmstat 1 60 > /tmp/vmstat.log\niostat -dx 1 60 > /tmp/iostat.log\n```\n\n## Follow-up Questions\n- How would you test the effect of increasing MemAvailable vs lowering swapiness?\n- What pitfalls exist when binding memory to a NUMA node across many containers?","diagram":null,"difficulty":"advanced","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Adobe","Cloudflare"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T14:46:18.035Z","createdAt":"2026-01-25T14:46:18.035Z"},{"id":"q-7259","question":"On a Linux host running a Kubernetes node pool, tail latency spikes appear in service responses under load. Using only default Linux tools, design a concrete diagnostic workflow to (1) confirm tail latency is tied to I/O or interrupt handling, (2) identify the offending IRQ or NIC queue, and (3) apply a safe mitigation (e.g., isolate CPUs, pin IRQs, disable irqbalance) while preserving observability. Include exact commands and sample outputs?","answer":"Begin by correlating latency with IO wait and IRQ activity, using iostat -xz 1 and inspecting /proc/interrupts. Locate the hot NIC IRQ, then pin it to dedicated CPUs (echo 0x000000FF > /proc/irq/NNN/s","explanation":"## Why This Is Asked\nTests practical diagnostic ability to connect latency tails to kernel IO paths and interrupts, and to apply safe mitigations without losing observability.\n\n## Key Concepts\n- IO wait vs tail latency\n- IRQ affinity and CPU isolation\n- Observability with iostat, pidstat, perf\n- Safe production mitigations (irqbalance, isolcpus, kernel cmdline)\n\n## Code Example\n```bash\n# Surface IO wait and interrupts\niostat -xz 1\ngrep -E '^[0-9]+' /proc/interrupts\n# Pin hot IRQ to CPUs 0-7 (example IRQ nn)\necho 0x000000FF > /proc/irq/nn/smp_affinity\n# Disable irqbalance\nsystemctl stop irqbalance\n# Persist CPU isolation via kernel parameter (grub)\n# then reboot and verify\npidstat -d 1\nperf stat -e cycles,instructions,cache-references -p <pid> & sleep 5; kill $! \n```\n\n## Follow-up Questions\n- What are the risks of isolcpus and IRQ pinning in a multi-tenant cluster?\n- How would you validate that fixes persist under churn and node maintenance?","diagram":null,"difficulty":"advanced","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Amazon","MongoDB","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T15:42:04.689Z","createdAt":"2026-01-25T15:42:04.689Z"},{"id":"q-7289","question":"Scenario: A Linux server hosting a data-processing pipeline experiences intermittent tail I/O latency spikes under sustained load when using a networked block storage (iSCSI). Using only default tools, design a concrete diagnostic workflow to (1) confirm IO-bound vs CPU-bound latency, (2) identify whether the bottleneck is the storage device, NIC, or network path, and (3) apply a safe mitigation (e.g., switch I/O scheduler to deadline, adjust per-device queue depth, rebalance CPU affinity for IO, or disable irqbalance) while preserving observability. Include exact commands and example outputs?","answer":"Begin with per-device IO wait using iostat -dx 1 5; verify CPU contention with top or pidstat; inspect block scheduler via cat /sys/block/sd*/queue/scheduler; if disk latency dominates, switch to dead","explanation":"## Why This Is Asked\nTests practical, end-to-end diagnostic skills for real-world Linux infra issues: distinguishing IO-bound from CPU-bound latency, identifying bottlenecks across storage and network layers, and applying safe mitigations without breaking observability.\n\n## Key Concepts\n- IO scheduling and per-device queues\n- Distinguishing storage vs network bottlenecks\n- Safe, observable mitigations (scheduler changes, queue depths, CPU affinity)\n\n## Code Example\n```javascript\n// Note: This is a conceptual snippet showing the workflow steps\nconst diagnoseIO = () => {\n  // 1) collect IO metrics\n  // 2) inspect schedulers\n  // 3) apply a mitigation\n}\n```\n\n## Follow-up Questions\n- How would you automate this workflow for a fleet of nodes?\n- What metrics would you export to central dashboards for correlation?","diagram":null,"difficulty":"intermediate","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Adobe","Google","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T17:01:00.386Z","createdAt":"2026-01-25T17:01:00.386Z"},{"id":"q-7347","question":"Scenario: A Linux host runs a microservice mesh with many containers. During load spikes, response latency intermittently jumps to 2-3s. The latency seems to shift between CPU saturation and log I/O contention. Using only default Linux tools, design a concrete diagnostic workflow to (1) confirm if latency is tied to log I/O or CPU cycles, (2) locate the container/process responsible for bursty log writes and file-descriptor pressure, (3) apply safe mitigations (e.g., separate log device, journald settings, cgroup tweaks) while preserving observability. Include exact commands and example outputs?","answer":"Start by correlating latency with disk I/O using: iostat -dx 1 5; monitor CPU and I/O per process: pidstat -p ALL 1; check pressure: cat /proc/pressure/{cpu,memory}; identify the burst by inspecting c","explanation":"## Why This Is Asked\nTests diagnosing cross-cutting contention between CPU and log I/O in a real multi-tenant workload, plus practical mitigation with observability.\n\n## Key Concepts\n- Latency causality: CPU vs disk I/O vs logging\n- Per-container accounting and file descriptor pressure\n- Safe mitigations that preserve observability\n\n## Code Example\n```javascript\n# Commands (shown in answer)\niostat -dx 1 5\npidstat -p ALL 1\ncat /proc/pressure/cpu\njournalctl -f --since \"5 seconds ago\"\n```\n\n## Follow-up Questions\n- How would you automate this workflow across nodes?\n- What metrics would you surface in a dashboard to detect regression early?","diagram":"flowchart TD\n  A[Latency spike] --> B[Disk I/O spike]\n  B --> C[Container log burst]\n  C --> D[Mitigation: separate log device / journald tweaks]\n  D --> E[Restore observability]","difficulty":"advanced","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Cloudflare","Google","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T19:38:09.721Z","createdAt":"2026-01-25T19:38:09.721Z"},{"id":"q-7437","question":"Scenario: A multi-tenant Linux host with containerized services runs on a dual-socket NUMA system. Under sustained load, tail latency spikes appear for a payment service. Using only default tools, design a concrete diagnostic workflow to (1) verify NUMA imbalance and cross-node memory access, (2) identify the containers and memory nodes involved, and (3) apply a safe mitigation (e.g., pin memory/CPUs with numactl, isolate a socket, or adjust memory policy) while preserving observability. Include exact commands and example outputs?","answer":"Propose a NUMA-aware diagnostic workflow. Steps: 1) Verify topology and per-node memory distribution using `lscpu; numactl --hardware; numastat -m` to establish baseline NUMA configuration and identify existing memory imbalances. 2) Analyze container memory access patterns with `pidstat -p ALL` and `pmap` to map processes to specific NUMA nodes, then use `numastat -p <PID>` to track cross-node memory references. 3) Apply targeted mitigation by pinning critical services using `numactl --cpunodebind=0 --membind=0 <start_container>` or `systemd-run --slice=payment.slice --property=CPUAffinity=0-15 --property=MemoryPolicy=prefer --property=MemoryNode=0`, followed by monitoring with `numastat -m` and `pidstat -p <PID> 1` to verify improvements while maintaining full observability.","explanation":"## Why This Is Asked\n\nReal-world systems often suffer tail latency due to NUMA memory locality on multi-socket hosts. This question probes practical detection, attribution, and mitigation actions using default tooling, without relying on vendor-specific utilities.\n\n## Key Concepts\n\n- NUMA topology and memory locality\n- Per-node memory pressure and cross-node references\n- CPU pinning and memory binding\n- Observability of latency vs locality interactions\n\n## Code Example\n\n```bash\n# Step 1: Verify NUMA topology\nlscpu\nnumactl --hardware\nnumastat -m\n\n# Step 2: Monitor process-level memory access\npidstat -p ALL 1\npmap -x <PID>\nnumastat -p <PID>\n\n# Step 3: Apply mitigation\nnumactl --cpunodebind=0 --membind=0 <start_container>\n# OR\nsystemd-run --slice=payment.slice --property=CPUAffinity=0-15 --property=MemoryPolicy=prefer --property=MemoryNode=0\n```","diagram":"flowchart TD\n  A[Check NUMA topology] --> B[Check per-node memory usage]\n  B --> C[Identify container/memory nodes]\n  C --> D[Apply pinning/isolation]\n  D --> E[Validate observability]","difficulty":"intermediate","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":null,"companies":["PayPal","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T05:58:37.420Z","createdAt":"2026-01-25T23:32:51.368Z"},{"id":"q-7551","question":"Scenario: A Linux host runs Redis in a container and a data-processor in another container. Under peak load, tail latency spikes in Redis and memory bandwidth saturates on NUMA node0. Using only default tools, design a concrete diagnostic workflow to (1) confirm CPU/memory pressure is the cause, (2) pinpoint the exact CPUs and NUMA nodes involved, and (3) apply safe mitigations (e.g., pin critical services with cpuset, adjust CPU frequency governor, disable THP, enforce memcg limits) while preserving observability. Include exact commands and example outputs?","answer":"Use: mpstat -P ALL 1; pidstat -p $(pidof redis-server) 1; iostat -xd 1; vmstat 1; numastat 1; top -b -d 1 -n 1 -H. If pressure is confirmed, pin Redis to CPUs 2-5 on node0 with a cpuset, set governor ","explanation":"## Why This Is Asked\nTests depth in diagnosing production Linux issues with no specialized tools, emphasizing observability, NUMA awareness, and safe mitigations.\n\n## Key Concepts\n- CPU affinity and cpuset\n- NUMA topology and memcg limits\n- I/O and memory bandwidth patterns\n- Observability and safe containment\n\n## Code Example\n```bash\n# Example monitoring commands\nmpstat -P ALL 1\npidstat -p $(pidof redis-server) 1\n```\n```\n## Follow-up Questions\n- What would you do if latency remains after pinning and THP is already disabled?\n- How would you adapt this workflow for Kubernetes pods and cgroup v2?\n```","diagram":"flowchart TD\n  A[Collect metrics] --> B[Confirm pressure]\n  B --> C[Identify CPUs/NUMA nodes]\n  C --> D[Apply mitigations]\n  D --> E[Verify observability]","difficulty":"advanced","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Microsoft","PayPal","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T07:16:34.261Z","createdAt":"2026-01-26T07:16:34.261Z"},{"id":"q-7606","question":"A Linux host runs a lightweight observability agent that intermittently spikes CPU during metric scrapes; all other metrics look normal. Using only default tooling, outline a concrete diagnostic workflow to (1) verify if the spike is caused by a cron/timer job, (2) identify the exact script or process triggering it, and (3) apply a safe mitigation (e.g., adjust cron schedule, set CPU affinity, or throttle scrapes) while preserving observability?","answer":"Check timers and cron for triggers, then pin and throttle the culprit. Steps: systemctl list-timers --all; crontab -l; top -b -n1; ps -eo pid,pcpu,cmd --sort=-pcpu | head. If a timer/cron is the cause","explanation":"## Why This Is Asked\n\nBursty CPU from scheduled tasks is a common beginner debugging scenario linking OS tooling to performance.\n\n## Key Concepts\n\n- systemd timers and cron\n- Runtime process monitoring\n- Safe mitigation: throttle, pin, reschedule\n\n## Code Example\n\n```bash\nsystemctl list-timers --all\ncrontab -l\ntop -b -n1\nps -eo pid,pcpu,cmd --sort=-pcpu | head\n```\n\n## Follow-up Questions\n\n- How would you test the mitigation without impacting other timers?\n- How would you scale this to multiple agents across hosts?","diagram":"flowchart TD\n  Start[Start] --> Timers[Check systemd timers and cron]\n  Timers --> Identify[Identify triggering process]\n  Identify --> Mitigate[Apply mitigation (throttle, pin, reschedule)]\n  Mitigate --> Verify[Verify observability]","difficulty":"beginner","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Hugging Face","Robinhood","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T09:47:26.555Z","createdAt":"2026-01-26T09:47:26.555Z"},{"id":"q-7641","question":"Scenario: A Linux host running a Rust web service behind systemd occasionally fails to accept new TCP connections under moderate load while CPU, memory, and disk I/O appear normal. Using only default tooling, describe a concrete, beginner-friendly diagnostic workflow to (1) verify whether the issue is socket TIME_WAIT state saturation vs file descriptor exhaustion, (2) identify the exact limit or per-process FD usage causing the bottleneck, and (3) apply a safe mitigation (e.g., raise soft/hard limits, adjust systemd or service limits, or tune the application) while preserving observability. Include exact commands and example outputs?","answer":"Begin by checking socket states and FD saturation: - ss -anp | awk '/LISTEN|TIME-WAIT|ESTAB/ {print $1,$2,$5}' - ulimit -n - cat /proc/sys/fs/file-max - ls -1 /proc/$(pidof yourservice)/fd | wc -l If ","explanation":"## Why This Is Asked\nTests practical FD and socket troubleshooting using only basics.\n\n## Key Concepts\n- File descriptor limits (soft/hard)\n- TIME_WAIT vs ESTABLISHED states\n- per-process vs system-wide limits; systemd overrides\n\n## Code Example\n```bash\nss -anp | awk '/LISTEN|TIME-WAIT|ESTAB/ {print $1,$2,$5}'\nulimit -n\ncat /proc/sys/fs/file-max\nls -1 /proc/$(pidof yourservice)/fd | wc -l\n```\n\n## Follow-up Questions\n- How to automate a one-liner to detect FD exhaustion?\n- Risks of raising limits on production services?","diagram":null,"difficulty":"beginner","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Amazon","Google","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T10:54:13.985Z","createdAt":"2026-01-26T10:54:13.985Z"},{"id":"q-7654","question":"A Linux host with two NVMe drives, one in RAID-1 and one standalone, experiences occasional I/O latency spikes under sustained load. Using only default tools, design a concrete diagnostic workflow to (1) confirm I/O bound latency, (2) identify the device/driver path responsible, and (3) apply a safe mitigation (e.g., tune io scheduler, pin IO-heavy tasks to a specific device, or stagger IO) while preserving observability. Include exact commands and example outputs?","answer":"Start by confirming IO bottlenecks with: iostat -xz 1; iotop -oa; and check schedulers: cat /sys/block/nvme0n1/queue/scheduler. If await is high on a device, switch to deadline: echo deadline > /sys/b","explanation":"## Why This Is Asked\nThis tests real-world triage: distinguish IO waits from CPU/memory, map to a block device, and implement a safe, observable mitigation.\n\n## Key Concepts\n- IO wait profiling with iostat and iotop\n- Block scheduler tuning and device paths\n- Task affinity with ionice/blkio cgroups\n\n## Code Example\n```javascript\n// Example snippet: switch scheduler and assign IO heavy process\nconsole.log('sudo sh -c \"echo deadline > /sys/block/nvme0n1/queue/scheduler\"')\n```\n\n## Follow-up Questions\n- How would you verify no regression after changing the scheduler?\n- What are trade-offs of different schedulers on mixed-workloads?","diagram":"flowchart TD\n  A[Start] --> B[Run iostat/iotop]\n  B --> C[Identify device with high await]\n  C --> D[Check /sys/block/.../queue/scheduler]\n  D --> E[Apply mitigation]\n  E --> F[Validate with metrics]","difficulty":"intermediate","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Anthropic","Square","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T11:34:07.505Z","createdAt":"2026-01-26T11:34:07.505Z"},{"id":"q-7684","question":"Scenario: A Linux host running dozens of containers exhibits intermittent tail latency spikes during bursts. You suspect kernel memory reclaim thrash. Using only default tools, design a concrete diagnostic workflow to (1) confirm reclaim activity, (2) identify whether page cache, slab, or allocator is most impacted and which container correlates, and (3) apply safe mitigations (e.g., tune min_free_kbytes, vfs_cache_pressure, container memory limits) while preserving observability. Include exact commands and example outputs?","answer":"Diagnose kernel memory reclaim thrash causing latency spikes in a container-heavy host using only default tools: 1) vmstat 1 10 and free -m to confirm swap/page activity; 2) inspect /proc/vmstat and /","explanation":"## Why This Is Asked\nTests knowledge of kernel memory management, particularly memory reclaim pathways, and practical skills to map reclaim pressure to containers and apply safe tunables without breaking observability. Emphasizes real-world debugging under load and safe containment.\n\n## Key Concepts\n- Kernel memory reclaim (kswapd) and page cache pressure\n- /proc/vmstat and /proc/meminfo signals for reclaim activity\n- cgroups-based container memory limits and RSS mapping\n- Safe tunables: vm.min_free_kbytes, vm.vfs_cache_pressure\n- Observability: dmesg, vmstat, baseline metrics\n\n## Code Example\n```bash\n#!/bin/bash\nset -e\n\necho \"=== vmstat 1 5 ===\"\nvmstat 1 5\necho \"=== MemInfo ===\"\ngrep -E 'MemTotal|MemAvailable|Cached|SwapTotal|SwapFree' /proc/meminfo\n```\n\n## Follow-up Questions\n- How would you baseline reclaim pressure across a fleet to spot anomalies quickly?\n- If increasing min_free_kbytes impacts node utilization, what trade-offs and safeguards would you implement before applying in production?","diagram":null,"difficulty":"advanced","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Robinhood","Scale Ai","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T13:21:52.669Z","createdAt":"2026-01-26T13:21:52.669Z"},{"id":"q-7997","question":"Scenario: A Linux host runs dozens of containers with memory limits. Under peak load, the system sporadically kills processes via the OOM killer even though free memory appears available. Using only default tools, design a concrete diagnostic workflow to (1) confirm OOM events and identify the victim process and container, (2) determine which cgroup or kernel memory controller is triggering, (3) apply a safe mitigation (e.g., adjust container memory limits, tweak overcommit/swappiness, enable per‑cgroup memory accounting) while preserving observability. Include exact commands and example outputs?","answer":"Check OOM: dmesg | grep -i oom; dmesg | tail -n 60; free -m; grep -i MemAvailable /proc/meminfo; for cg in /sys/fs/cgroup/memory/*; do [ -f $cg/memory.usage_in_bytes ] && echo '---' $cg && cat $cg/mem","explanation":"## Why This Is Asked\n\nTests ability to diagnose memory pressure and OOM in multi-tenant Linux; requires hands-on with kernel memory accounting, cgroups, and safe mitigations.\n\n## Key Concepts\n\n- OOM killer triggers and victims\n- Per‑cgroup memory accounting (memory.usage_in_bytes, memory.max)\n- System memory metrics: MemAvailable, swap pressure, overcommit\n- Safe mitigations: hard vs soft limits, swappiness tweaks\n\n## Code Example\n\n```text\ndmesg | grep -i oom\nfor cg in /sys/fs/cgroup/memory/*; do [ -f \"$cg/memory.usage_in_bytes\" ] && echo \"$cg\" && cat \"$cg/memory.usage_in_bytes\"; done\n```\n\n## Follow-up Questions\n\n- How would you handle CGv2 unified hierarchies?\n- What telemetry would you add to detect regressive memory growth?","diagram":"flowchart TD\n  A Identify_OOM_events --> B Trace_per-container_memory_usage\n  B --> C Find_victim_container\n  C --> D Check_memory_limits\n  D --> E Apply_limits_or_swappiness\n  E --> F Verify_observability","difficulty":"advanced","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Robinhood","Salesforce","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T04:40:19.718Z","createdAt":"2026-01-27T04:40:19.718Z"},{"id":"q-8002","question":"Scenario: A Linux host running a Python microservice in Docker experiences occasional 1-2s latency spikes under sustained load. With default tools only, design a concrete diagnostic workflow to (1) confirm if latency is kernel scheduler or memory pressure, (2) locate offending process/container, (3) apply safe mitigations (cpuset/timeslice/cgroup limits) while preserving observability. Include exact commands and sample outputs?","answer":"Begin with system-wide metrics: vmstat 1 60 to spot wa and swapping; if wa or swaps spike, run pidstat -u -p ALL 1 60 to identify cpu hogs. Map PID to container via /proc/[pid]/cgroup, then pin with t","explanation":"## Why This Is Asked\n\nTests practical Linux performance diagnostics using only default tools, covering CPU, memory, and container boundaries in production-like setups.\n\n## Key Concepts\n\n- vmstat and pidstat for system and per-process metrics\n- /proc/[pid]/cgroup to map hosts to containers\n- taskset and container cpuset updates for safe isolation\n- memory.max / memory.limit_in_bytes for memory pressure mitigation\n\n## Code Example\n\n```bash\nvmstat 1 60\npidstat -u -p ALL 1 60\n```\n\n## Follow-up Questions\n\n- How would you handle a multi-tenant host with noisy neighbor containers?\n- What non-disruptive observability improvements would you add to prevent regression?","diagram":"flowchart TD\n  Start[Start] --> Metrics[Metrics collection]\n  Metrics --> Identify[Identify offender]\n  Identify --> Mitigate[Apply mitigation]\n  Mitigate --> Verify[Verify observability]","difficulty":"intermediate","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Citadel","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T05:38:34.857Z","createdAt":"2026-01-27T05:38:34.857Z"},{"id":"q-8106","question":"Diagnose NUMA-related tail latency on a dual-socket Linux host running many containers. Using only default tools, design a concrete diagnostic workflow to (1) confirm NUMA-induced remote memory access latency, (2) identify which containers touch memory on non-local nodes, and (3) apply a safe mitigation (e.g., bind workloads to local NUMA nodes) while preserving observability. Include exact commands and example outputs?","answer":"Use default tools to prove NUMA locality issues and pin workloads: 1) check topology: numactl --hardware; 2) map per-container memory: for each container pid, cat /proc/PID/numa_maps and numastat -p P","explanation":"## Why This Is Asked\nIntermittent tail latency can stem from memory locality in NUMA systems. This question tests practical debugging and containment skills using only default tools.\n\n## Key Concepts\n- NUMA topology and memory locality\n- per-process NUMA mappings\n- cpuset and numactl-based affinity\n- observability while applying changes\n\n## Code Example\n```javascript\n# show topology\nnumactl --hardware\n# per-container memory usage (example PID 1234)\ncat /proc/1234/numa_maps\nnumastat -p 1234\n```\n\n## Follow-up Questions\n- How would you scale this diagnosis across hundreds of containers?\n- What are trade-offs of binding to a single NUMA node in a growing cluster?","diagram":"flowchart TD\n  A[Start: NUMA issue] --> B[Check topology]\n  B --> C[Map container memory]\n  C --> D[Correlation with latency]\n  D --> E[Apply affinity mitigation]\n  E --> F[Re-measure observability]","difficulty":"intermediate","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Citadel","Google","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T09:58:35.548Z","createdAt":"2026-01-27T09:58:35.548Z"},{"id":"q-8125","question":"A Linux host running a Go API behind Nginx shows intermittent 50–200 ms connection setup delays during bursts, while CPU, RAM, and disk I/O appear normal. Using only default tools, describe a concrete workflow to (1) confirm ephemeral port exhaustion/TIME_WAIT saturation as the cause, (2) identify the offending connections or service, and (3) apply a safe mitigation (e.g., raise IP local port range, enable tcp_tw_reuse, or adjust nf_conntrack_max) with minimal impact and maintain observability. Include exact commands and expected outputs?","answer":"Diagnosis steps: 1) ss -s to check socket counts; 2) ss -a | awk '/TIME-WAIT/ {print}' to quantify TIME_WAIT; 3) check ephemeral range: cat /proc/sys/net/ipv4/ip_local_port_range; 4) view nf_conntrack","explanation":"## Why This Is Asked\n\nTests practical, focused on a networking edge case that beginners can reason about using standard tools.\n\n## Key Concepts\n\n- Ephemeral port exhaustion and TIME_WAIT dynamics\n- sS and socket lifecycle monitoring\n- Basic sysctl tuning for port ranges and reuse\n\n## Code Example\n\n```bash\nss -s\nss -a | awk '/TIME-WAIT/ {print}'\ncat /proc/sys/net/ipv4/ip_local_port_range\ncat /proc/sys/net/netfilter/nf_conntrack_max\n```\n\n## Follow-up Questions\n\n- How would you revert changes if issues arise?\n- What monitoring would you add to verify stability after changes?","diagram":null,"difficulty":"beginner","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Discord","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T10:50:45.394Z","createdAt":"2026-01-27T10:50:45.394Z"},{"id":"q-8224","question":"Scenario: A Linux host with 2 NUMA nodes runs a latency-sensitive service and a background job. During bursts, tail latency spikes and cross-node memory traffic appears. Using only default tools, design a concrete diagnostic workflow to (1) confirm NUMA memory bandwidth contention as the cause, (2) map per-thread NUMA placement, and (3) apply safe mitigation (bind the service to one node, fix CPU affinity, or adjust memory policy) while preserving observability. Include exact commands and outputs?","answer":"Use numactl --hardware to reveal topology; run numastat -m to show per-node memory usage; inspect per-thread placement with ps -eo pid,psr,comm; view /proc/<pid>/numa_maps or /proc/<pid>/smaps for pag","explanation":"## Why This Is Asked\nTests practical NUMA troubleshooting, not theory, and expects concrete commands, observability steps, and safe mitigations.\n\n## Key Concepts\n- NUMA topology discovery\n- Per-thread memory placement\n- Safe process pinning and policy changes\n- Observability after mitigation\n\n## Code Example\n```\n# Discover topology\nnumactl --hardware\n\n# See memory usage per node\nnumastat -m\n\n# Bind a service to node0 and CPUs 0-7\nsudo numactl --membind=0 --physcpubind=0-7 /path/to/service\n```\n\n## Follow-up Questions\n- How would you verify no regressions after binding?\n- What are trade-offs of strict binding in a virtualized environment?","diagram":"flowchart TD\nA[Start] --> B[Check NUMA topology: numactl --hardware]\nB --> C[Monitor per-node mem: numastat -m]\nC --> D[Map thread placement: ps -eo pid,psr,comm]\nD --> E[Inspect pid numa_maps: /proc/<pid>/numa_maps]\nE --> F[Mitigate: numactl/taskset bindings]\nF --> G[Observe metrics: top/iostat/sar]\nG --> H[End]","difficulty":"intermediate","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Google","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T15:50:26.110Z","createdAt":"2026-01-27T15:50:26.111Z"},{"id":"q-8301","question":"A Linux host running a high-throughput API behind Nginx experiences intermittent 20–100 ms latency bursts under load. CPU, memory, and disk I/O look normal. Using only default tools, design a concrete diagnostic workflow to (1) confirm the TCP stack is the bottleneck (e.g., rwnd, slow start, or queue backlog), (2) determine whether the bottleneck is client, server, or network queue, and (3) implement a safe mitigation (e.g., adjust tcp_rmem/tcp_wmem or somaxconn) with minimal observability impact. Include exact commands and example outputs?","answer":"Run a quick probe and per-connection check, then inspect the TCP stack state. Commands: curl -sS -w '%{time_starttransfer}\\n' http://localhost/health -o /dev/null; ss -tnae | grep ESTAB; cat /proc/net","explanation":"## Why This Is Asked\nTests the ability to diagnose TCP pathologies with built-in Linux tools under realistic bursts.\n\n## Key Concepts\n- TCP window, backlog, queueing\n- Distinguishing client/server/network bottlenecks\n- Safe, reversible sysctl tuning with observability checks\n\n## Code Example\n```bash\n# sample commands\ncurl -sS -w '%{time_starttransfer}\\n' http://localhost/health -o /dev/null\nss -tnae | grep ESTAB\ncat /proc/net/tcp | head\ntcpdump -i eth0 -c 200 'tcp and port 80'\nsysctl -w net.core.somaxconn=1024\nsysctl -w net.ipv4.tcp_rmem='4096 87380 6291456'\nsysctl -w net.ipv4.tcp_wmem='4096 87380 6291456'\n```\n\n## Follow-up Questions\n- How would you roll back if no improvement is observed?\n- What other netstats would you monitor after changes?","diagram":null,"difficulty":"intermediate","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Google","PayPal","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T19:13:18.559Z","createdAt":"2026-01-27T19:13:18.559Z"},{"id":"q-8346","question":"Scenario: A Linux host with two NUMA nodes runs a latency‑sensitive service in containers. During bursts, 99th percentile latency climbs from ~1 ms to 40 ms while CPU, I/O, and network look fine. Using only default Linux tools, design a concrete diagnostic workflow to (1) confirm cross‑NUMA memory access or bandwidth saturation as the cause, (2) identify the container(s)/cgroups driving the contention, and (3) apply a safe mitigation (e.g., pin critical services to node0 and migrate others) while preserving observability. Include exact commands and example outputs?","answer":"Plan: verify cross‑NUMA pressure with numactl --hardware and numastat -c; inspect per‑container memory usage via /sys/fs/cgroup/memory/docker*/memory.stat; once culprit is found, pin the critical cont","explanation":"## Why This Is Asked\nAssesses practical debugging of NUMA-related latency with containers using only built‑in tools, focusing on cross‑node memory access and per‑cgroup pressure.\n\n## Key Concepts\n- NUMA topology and policy\n- per‑container memory usage reporting\n- pinning and migrating workloads without losing observability\n\n## Code Example\n```bash\nnumactl --hardware\nnumastat -c\n```\n\n## Follow-up Questions\n- How would you scale this approach in a Kubernetes cluster?\n- What are risks of pinning and how would you monitor for regressions?\n","diagram":null,"difficulty":"intermediate","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Meta","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T20:48:37.196Z","createdAt":"2026-01-27T20:48:37.196Z"},{"id":"q-8397","question":"A Linux host running an Nginx API stack intermittently rejects new connections during bursts, while existing connections stay healthy. Using only default tools, describe a concrete diagnostic workflow to (1) determine whether the bottleneck is TCP backlog, ephemeral ports, or per-process file descriptor limits, (2) identify the exact culprit and process, and (3) apply a safe mitigation (e.g., raise somaxconn, adjust ip_local_port_range, or increase per-process FD limits) while keeping observability. Include exact commands and expected outputs?","answer":"Diagnose by systematically checking each potential bottleneck:\n\n1. **TCP Backlog Analysis:**\n   ```bash\n   ss -ltnp | head -5        # Check listening queues\n   ss -s                      # Show socket statistics\n   cat /proc/sys/net/core/somaxconn  # Current backlog limit\n   ```\n   Expected outputs: large Recv-Q values indicate backlog exhaustion.\n\n2. **Ephemeral Port Availability:**\n   ```bash\n   sysctl net.ipv4.ip_local_port_range  # Current port range\n   ss -tan state TIME-WAIT | wc -l      # Count TIME_WAIT connections\n   ```\n   Expected: TIME_WAIT count approaching available port range suggests exhaustion.\n\n3. **File Descriptor Limits:**\n   ```bash\n   ulimit -n                               # Per-process soft limit\n   cat /proc/sys/fs/file-max               # System-wide limit\n   ls -l /proc/$(pgrep nginx)/fd | wc -l   # Current FD usage by nginx\n   ```\n   Expected: Current FD usage approaching the limit.\n\n4. **Safe Mitigation:**\n   ```bash\n   # For backlog issues:\n   sysctl -w net.core.somaxconn=65535\n   echo 'net.core.somaxconn=65535' >> /etc/sysctl.conf\n   \n   # For ephemeral ports:\n   sysctl -w net.ipv4.ip_local_port_range='1024 65535'\n   echo 'net.ipv4.ip_local_port_range=1024 65535' >> /etc/sysctl.conf\n   \n   # For FD limits:\n   echo 'nginx soft nofile 65535' >> /etc/security/limits.conf\n   echo 'nginx hard nofile 65535' >> /etc/security/limits.conf\n   ```\n\n5. **Observability:**\n   ```bash\n   watch 'ss -ltnp; ss -s; cat /proc/$(pgrep nginx)/status | grep FD'\n   ```","explanation":"## Why This Is Asked\nTests practical networking diagnosis on a common web stack. It targets backlog, ephemeral ports, and FD limits—areas often misdiagnosed. It also checks safe, observable remediation steps.\n\n## Key Concepts\n- TCP backlog, somaxconn, port ranges\n- Ephemeral ports and per-process fd limits\n- Safe, observable mitigations in system and service configs\n\n## Code Example\n```bash\nss -ltnp\n```\n\n## Follow-up Questions\n- How would you monitor the effect of your change over the next hour?\n- What would you do if curl still fails despite larger backlog?","diagram":null,"difficulty":"beginner","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Google","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-28T05:00:13.096Z","createdAt":"2026-01-27T22:56:03.458Z"},{"id":"q-917","question":"Scenario: a Linux server hosting a web app experiences sporadic high response times during peak hours. Using only default tools and no downtime, describe a concrete, beginner-friendly diagnostic workflow to (1) determine whether CPU, memory, or I/O is the bottleneck, (2) identify the offending process, and (3) apply a safe mitigation (e.g., graceful restart) while monitoring impact. Include exact commands and expected outputs?","answer":"Run top in batch to spot heavy CPU: top -b -n1; then dump top processes: ps -eo pid,comm,pcpu,pmem --sort=-pcpu | head -5. Check IO with iostat -dx 1 2 and memory with free -m. If a process hogs, grac","explanation":"## Why This Is Asked\n\\nTests practical, beginner-friendly triage in production-like Linux environments, focusing on real commands and safe mitigations without downtime.\\n\\n## Key Concepts\n- Basic bottleneck triage (CPU, memory, I/O)\\n- Identifying culprits with top and ps\\n- Verifying I/O and memory health with iostat and free\\n- Safe mitigation via graceful restart and live monitoring\\n\\n## Code Example\n```javascript\n// Diagnostics snippet (commands)\ntop -b -n1 | head -20\nps -eo pid,comm,pcpu,pmem --sort=-pcpu | head -5\niostat -dx 1 2\nfree -m\njournalctl -u service -n 100 --no-pager\n```\n\n## Follow-up Questions\n- How would you automate this workflow for nightly checks?\\n- What differences would you expect between CPU-bound and I/O-bound symptoms?","diagram":null,"difficulty":"beginner","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Microsoft","PayPal","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T15:30:13.738Z","createdAt":"2026-01-12T15:30:13.738Z"}],"subChannels":["commands","general"],"companies":["Adobe","Airbnb","Amazon","Anthropic","Apple","Bloomberg","Citadel","Cloudflare","Coinbase","Databricks","Discord","DoorDash","Goldman Sachs","Google","Hashicorp","Hugging Face","IBM","Instacart","LinkedIn","Lyft","Meta","Microsoft","MongoDB","NVIDIA","Netflix","OpenAI","Oracle","PayPal","Plaid","Robinhood","Salesforce","Scale Ai","Slack","Snap","Snowflake","Square","Tesla","Twitter","Two Sigma","Uber","Zoom"],"stats":{"total":88,"beginner":33,"intermediate":29,"advanced":26,"newThisWeek":43}}