{"questions":[{"id":"gh-26","question":"What are the essential Linux commands every DevOps engineer should master for system administration, troubleshooting, and automation?","answer":"Essential Linux commands span file operations (ls, cp, mv, rm, find), text processing (grep, sed, awk, cut, sort, uniq), process management (ps, top, htop, kill, nice, jobs), network diagnostics (netstat, ss, curl, wget, ssh, nc), permissions (chmod, chown, chgrp, setfacl), package management (apt, yum, dnf, snap, flatpak), system monitoring (df, du, free, iostat, vmstat, lsof), and containerization tools (docker, podman, systemd, journalctl), forming the comprehensive foundation for modern DevOps automation and system administration.","explanation":"## Interview Context\nThis question evaluates practical Linux proficiency for DevOps engineers, testing knowledge beyond basic commands to include modern infrastructure tools and best practices.\n\n## Key Areas to Cover\n- **File System Operations**: Advanced navigation with find, locate, which; manipulation with dd, rsync; permissions including ACLs and extended attributes\n- **Process Management**: Real-time monitoring with htop, process control with kill signals, job scheduling with cron and systemd timers\n- **Network Tools**: Connection monitoring with ss, packet analysis with tcpdump, service discovery with dig/nslookup\n- **Text Processing**: Advanced pattern matching with regex, data transformation pipelines, log analysis techniques\n- **System Monitoring**: Performance metrics with sar, resource tracking, disk usage analysis\n- **Container Orchestration**: Docker commands for image/container lifecycle, systemd service management, journalctl for log analysis\n- **Security**: User management, audit trails, SELinux/AppArmor basics\n\n## Example Responses\nDemonstrate chaining commands like `ps aux | grep nginx | awk '{print $2}' | xargs kill -9`, or Docker workflows: `docker build -t app . && docker run -d --restart=unless-stopped app`. Show understanding of systemd unit files and service management commands.","diagram":"\ngraph TD\n    Linux --> Files[File Ops: ls, cp, mv]\n    Linux --> Sys[System: top, df, ps]\n    Linux --> Text[Text: grep, awk, sed]\n","difficulty":"beginner","tags":["linux","shell"],"channel":"linux","subChannel":"commands","sourceUrl":null,"videos":{"shortVideo":"https://www.youtube.com/watch?v=zB_3FIGRWRU"},"companies":["Amazon","Cloudflare","Google","Hashicorp","Microsoft","Netflix"],"eli5":"Imagine you're the boss of a giant playground with lots of toys and games. Linux commands are like your magic words to control everything! 'ls' is like looking at all your toys on the shelf. 'cp' is like making copies of your favorite drawings. 'mv' is like moving toys to different boxes. 'grep' is like finding specific words in your storybooks. 'ps' is like checking which friends are playing on the swings. 'curl' is like sending secret messages to other playgrounds. 'chmod' is like deciding who can play with which toys - you, your friends, or everyone. These magic words help you keep the playground running smoothly, fix broken toys, and make games happen automatically while you eat snacks!","relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-29T07:00:08.286Z","createdAt":"2025-12-26 12:51:05"},{"id":"q-1000","question":"A Linux host runs several Docker containers; during peak load, API responses slow and some requests time out. Describe a beginner-friendly, concrete diagnostic workflow to (1) confirm whether host saturation is CPU, memory, or I/O, (2) identify the container most responsible, and (3) apply a safe mitigation (e.g., graceful restart or CPU/memory throttling) while monitoring impact. Include exact commands and expected outputs?","answer":"Use top/vmstat to identify bottleneck, then docker stats --no-stream to spot the hogging container. Correlate with docker ps. If CPU or memory is the issue, throttle with docker update --cpu-shares 51","explanation":"## Why This Is Asked\nThis tests practical, beginner-friendly diagnostic thinking for containerized workloads.\n\n## Key Concepts\n- Metrics: CPU/memory/IO\n- Per-container accounting via docker stats\n- Safe mitigations: CPU shares, memory limits, graceful restarts\n\n## Code Example\n```javascript\ntop -b -n1\ndocker stats --no-stream\n```\n\n## Follow-up Questions\n- How would you verify the mitigation actually improved latency?\n- What are edge cases when throttling hurts user experience?","diagram":null,"difficulty":"beginner","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","LinkedIn"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T18:45:16.170Z","createdAt":"2026-01-12T18:45:16.170Z"},{"id":"q-1142","question":"Scenario: A Linux server hosting a small web service used by customers suddenly shows disk usage climbing on the root filesystem. Provide a beginner-friendly, concrete diagnostic workflow to (1) locate the directories/files consuming the most space, (2) identify top offenders, and (3) apply a safe mitigation (e.g., rotate/compress/archive logs or purge old data) while keeping the service up. Include exact commands and expected outputs?","answer":"Check space: df -h. Find large files/dirs: du -hx --max-depth=1 / 2>/dev/null | sort -rh | head -n 5. Focus on /var/log: du -xh /var/log | sort -rh | head -n 5. Mitigate safely: tar czf /backup/logs-$","explanation":"## Why This Is Asked\n\nAssesses practical diagnostic skills for a common sysadmin pain point: disk pressure. Tests ability to use core CLI tools and perform safe, non-downtime mitigations.\n\n## Key Concepts\n\n- Disk usage analytics with df and du\n- Identifying large offenders efficiently\n- Safe log rotation/archiving and selective truncation\n- Lightweight monitoring after mitigation\n\n## Code Example\n\n```bash\n# Safe archival of logs\nLOG_ARCHIVE=/backup/logs-$(date +%F).tar.gz\ntar czf \"$LOG_ARCHIVE\" /var/log\n\n# Purge old logs (age > 30 days)\nfind /var/log -type f -iname '*.log' -mtime +30 -delete\n```\n\n## Follow-up Questions\n\n- How would you adapt this for a system with logrotation already in place?\n- What risks exist when deleting or truncating logs, and how to mitigate?","diagram":null,"difficulty":"beginner","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Google","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T01:28:46.981Z","createdAt":"2026-01-13T01:28:46.981Z"},{"id":"q-1205","question":"You're operating a fleet of Linux hosts running a low-latency web API. Intermittent 100–200 ms latency spikes appear under load. Design a practical, end-to-end diagnostic plan using eBPF/BPFtrace or perf, iostat/vmstat, and container metrics to collect data, identify root cause (CPU scheduling, IO wait, or network), and propose minimal-disruption mitigations?","answer":"Baseline with iostat -dx 1, vmstat 1, and kubectl top; use eBPFtrace to map per-process IO latency: tracepoint:block:block_rq_issue { @s[pid] = nsecs; } tracepoint:block:block_rq_complete { @d[pid] = ","explanation":"## Why This Is Asked\nDemonstrates practical, instrumented thinking for production Linux latency issues using modern tracing tools.\n\n## Key Concepts\n- eBPF/BPFtrace, perf, ftrace\n- IO schedulers, MQ, quotas\n- Container metrics and baseline-vs-spike analysis\n\n## Code Example\n```bash\ntracepoint:block:block_rq_issue { @s[pid] = nsecs; }\ntracepoint:block:block_rq_complete { @d[pid] = hist(nsecs - @s[pid]); }\n```\n\n## Follow-up Questions\n- How would you extend this to a multi-tenant cluster?\n- What are the risks of tracing in production and how would you mitigate them?","diagram":null,"difficulty":"advanced","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Salesforce","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T04:53:37.702Z","createdAt":"2026-01-13T04:53:37.702Z"},{"id":"q-1244","question":"Scenario: A Linux host runs a MongoDB primary in a high-traffic cluster. During peak hours, latency spikes and tail latency increases while CPU and memory appear stable. Using only default tooling, no downtime, describe a concrete, actionable diagnostic workflow to (1) determine if I/O wait, CPU, or memory is the bottleneck, (2) pinpoint the offending disk/device or process, and (3) apply a safe mitigation (e.g., adjust I/O scheduler, tune dirty writeback parameters, or throttle MongoDB) while monitoring impact. Include exact commands and expected outputs?","answer":"Run vmstat, iostat, and top in batch to correlate latency with system metrics, then pidstat to isolate the offender. If disk I/O wait dominates, switch a block device’s scheduler to deadline/bfq and a","explanation":"## Why This Is Asked\nThis probes practical Linux performance diagnosis under live load, focusing on I/O bottlenecks and safe mitigations in a MongoDB context.\n\n## Key Concepts\n- I/O wait, disk scheduling, and writeback pressure\n- Process-level bottleneck identification with pidstat/top\n- Safe live mitigations (scheduler changes, sysctl tweaks, application-level throttling)\n\n## Code Example\n```javascript\n// Simple Node.js snippet to run common diagnostics sequentially\nconst {execSync} = require('child_process');\nconsole.log(execSync('vmstat 1 5; iostat -xz 1 5; top -bn1', {encoding: 'utf8'}));\n```\n\n## Follow-up Questions\n- How would you validate the effectiveness of the scheduler change?\n- What risks exist when adjusting dirty_writeback_ratio on a live DB?","diagram":"flowchart TD\n  Start[Start] --> Metrics[Collect metrics: vmstat, iostat, top]\n  Metrics --> Bottleneck{Bottleneck detected?}\n  Bottleneck -->|Yes| Isolate[Identify device/process via pidstat / proc/diskstats]\n  Bottleneck -->|No| End[End]\n  Isolate --> Mitigate[Apply safe mitigation: scheduler or sysctl]\n  Mitigate --> Verify[Reobserve metrics to confirm]\n  Verify --> End","difficulty":"intermediate","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Databricks","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T06:38:24.003Z","createdAt":"2026-01-13T06:38:24.003Z"},{"id":"q-1266","question":"Context: Linux node in a Kubernetes cluster hosting a high-throughput data ingestion service. Intermittent tail latency spikes (>1s) appear during peak traffic, affecting processing. Without downtime, design a concrete troubleshooting workflow to (1) confirm whether CPU, I/O, or network is the bottleneck, (2) identify the exact subsystem or process responsible, and (3) implement a safe mitigation with minimal impact while maintaining observability. Include exact commands and realistic outputs?","answer":"Begin with per-second metrics: iostat -xz 1 2; mpstat -P ALL 1; vmstat 1. Identify hot processes with pidstat -p ALL 1 | head. If IO lag appears, run perf stat -p <pid> -e cycles,instructions,cache-re","explanation":"## Why This Is Asked\nAssesses practical, end-to-end debugging in real-world Linux systems, including multi-subsystem correlation and safe mitigations with minimal downtime.\n\n## Key Concepts\n- Tail latency diagnosis across CPU, IO, and network\n- Per-second monitoring with iostat/mpstat/vmstat\n- Process-level tracing with pidstat and perf\n- I/O schedulers and CPU pinning as mitigations\n\n## Code Example\n```javascript\n// Example: parse perf output snapshot (illustrative)\nfunction parsePerf(line){ /* simplistic parser */ }\n```\n\n## Follow-up Questions\n- How would you adapt this workflow for containerized workloads (Kubernetes, CNI, cgroups)?\n- How would you validate the mitigation under synthetic peak load?","diagram":null,"difficulty":"intermediate","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Anthropic","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T07:32:47.290Z","createdAt":"2026-01-13T07:32:47.290Z"},{"id":"q-1364","question":"A Linux host running multiple microservices behind NGINX exhibits intermittent latency spikes; new connections occasionally fail and FD usage appears high. Using only default tools, describe a beginner-friendly workflow to (1) confirm FD exhaustion is the bottleneck, (2) identify the offending process by per‑process FD usage, and (3) apply a safe mitigation (increase NOFILE limits, adjust per-service limits, or set a systemd limit) while monitoring impact. Include exact commands and expected outputs?","answer":"Check limits and usage: cat /proc/sys/fs/file-max; ulimit -n; grep OpenFiles /proc/$(pgrep -f nginx)/limits 2>/dev/null. Identify top FDs: for p in /proc/[0-9]*; do [ -r \\\"$p/fd\\\" ] && echo -n \\\"${p##","explanation":"## Why This Is Asked\nFD exhaustion can silently throttle connections under load. This question probes practical techniques to verify limits, count per‑process FDs, and apply safe, scoped mitigations without downtime.\n\n## Key Concepts\n- File descriptor limits (nofile, file-max, nr_open)\n- /proc/<pid>/limits and per‑process FD counts\n- systemd LimitNOFILE and limits.conf\n- Safe, incremental changes and live monitoring\n\n## Code Example\n```javascript\n// Example Node.js utility to count open fds for a pid\nconst fs = require('fs');\nconst pid = process.argv[2];\nconsole.log('Open fds for', pid, ':', fs.readdirSync(`/proc/${pid}/fd`).length);\n```\n\n## Follow-up Questions\n- How would you automate per-service FD limits with systemd?\n- What are risks of increasing global limits?","diagram":"flowchart TD\n  A[FD exhaustion suspected] --> B[Check system limits]\n  B --> C[Count per-process FDs]\n  C --> D[Mitigate via limits or systemd]\n  D --> E[Monitor impact]","difficulty":"beginner","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","PayPal","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T13:17:53.743Z","createdAt":"2026-01-13T13:17:53.743Z"},{"id":"q-1410","question":"On a Linux host, a data-processing job occasionally stalls during bursts. Using only default tooling and no downtime, outline a concrete, beginner-friendly diagnostic flow to (1) confirm CPU, memory, or I/O bound, (2) identify the offending process and its operation, and (3) apply a safe mitigation (e.g., adjust priority, pause/resume, or throttle) with monitoring. Which commands would you run and what outputs would you expect?","answer":"Run top or ps to locate high CPU/mem processes, then vmstat 1 5 to watch IO and swap, and iostat -dx 1 5 to confirm IO waits. Note PID and cmd. Mitigate by renice -n 10 -p <pid> or pause with kill -SI","explanation":"## Why This Is Asked\nThis checks practical triage using default tools and safe mitigations. It also tests how the candidate identifies bottlenecks and minimizes disruption.\n\n## Key Concepts\n- Resource bottlenecks vs bursts\n- Observability: top/ps, vmstat, iostat\n- Process control: renice, SIGSTOP/SIGCONT\n\n## Code Example\n```bash\ntop -b -n1 | head\nps -eo pid,ppid,cmd,%cpu --sort=-%cpu | head\nvmstat 1 5\niostat -dx 1 5\nrenice -n 10 -p <pid>\n```\n\n## Follow-up Questions\n- What if iostat isn't installed?\n- How would you verify impact after mitigation?","diagram":"flowchart TD\n  A[Start] --> B{Identify bottleneck}\n  B --> C[CPU]\n  B --> D[Memory]\n  B --> E[IO]\n  C --> F[Lower priority]\n  D --> G[Tune cache/allocations]\n  E --> H[Adjust IO scheduler]","difficulty":"beginner","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","OpenAI","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T15:50:44.422Z","createdAt":"2026-01-13T15:50:44.422Z"},{"id":"q-1424","question":"Scenario: a high-traffic Linux service behind a reverse proxy experiences sporadic tail latency spikes during peak hours. CPU and IO look normal in aggregate. Design a practical end-to-end diagnostic using eBPF to identify root causes quickly: (1) instrument per-request latency across kernel and user-space, (2) attribute latency to network, disk, or app code, (3) propose safe mitigations and validate impact. Include specific probes, sample commands, and expected outputs?","answer":"Use an end-to-end eBPF trace with bpftrace: attach kprobes/kretprobes on accept, recv, and send, plus the HTTP handler; store start times per tid and compute deltas to build a latency histogram keyed ","explanation":"## Why This Is Asked\nTests practical eBPF observability, end-to-end tracing, and ability to propose concrete mitigations in production-like settings.\n\n## Key Concepts\n- eBPF tracing for low-overhead observability\n- kprobes/kretprobes and tracepoints\n- Per-request latency histograms and map keying (tid, IP, URL)\n- Tail latency analysis and cross-layer attribution\n- Safe mitigations: TCP tuning, NIC offloads, queue depths\n\n## Code Example\n```javascript\n#!/usr/bin/env bpftrace\nBEGIN { @start[tid] = 0; }\ntracepoint:syscalls:sys_enter_accept { @start[tid] = nsecs; }\ntracepoint:syscalls:sys_exit_accept { @lat[tid] = nsecs - @start[tid]; delete(@start[tid]); }\n```\n\n## Follow-up Questions\n- How would you extend this to multi-node correlation with time sync?\n- What are risks of running eBPF in production and how would you mitigate them?","diagram":null,"difficulty":"advanced","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Cloudflare","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T16:46:26.464Z","createdAt":"2026-01-13T16:46:26.464Z"},{"id":"q-1445","question":"A Linux host running GPU-accelerated video processing containers shows intermittent frame latency spikes during peak load, with no obvious CPU/memory/I/O bottlenecks. Without downtime, describe a concrete, real-world diagnostic workflow using only default tools to (1) verify if latency is caused by page cache pressure or Transparent Huge Pages, (2) identify the subsystem or container involved, and (3) apply a safe mitigation (e.g., disable THP on affected NUMA nodes, tune swappiness, or pin tasks) with minimal impact and observable results. Include exact commands and expected outputs?","answer":"Workflow: verify spike with top/iostat, inspect THP and page cache, locate offending container, and apply a safe mitigation. Commands: cat /sys/kernel/mm/transparent_hugepage/enabled; vmstat 1 5; iost","explanation":"## Why This Is Asked\nAssesses hands-on Linux troubleshooting for latency with containerized workloads, focusing on memory reclamation, THP behavior, and safe, minimal-impact mitigations.\n\n## Key Concepts\n- Transparent Huge Pages (THP) behavior and toggling\n- Page cache vs memory pressure\n- Container isolation and CPU pinning\n- Default tooling for on-call debugging\n\n## Code Example\n```bash\n# Check THP state\ncat /sys/kernel/mm/transparent_hugepage/enabled\n# Observe memory/caching behavior\nvmstat 1 5\n# Disk and I/O detail\niostat -dx 1\n# Identify running containers\ndocker ps\n# See heavy processes inside a container\ndocker top <id>\n# Mitigation: disable THP and pin a pid to CPUs\nsudo tee /sys/kernel/mm/transparent_hugepage/enabled <<< 'never'\ntaskset -cp 0-7 <pid>\n```\n\n## Follow-up Questions\n- How would you validate no performance regression after disabling THP? \n- How could you automate this workflow for on-call use?\n","diagram":"flowchart TD\n  A[Latency Event] --> B[Collect Metrics]\n  B --> C[Check THP/Cache]\n  C --> D[Identify Container]\n  D --> E[Apply Mitigation]\n  E --> F[Monitor Metrics]","difficulty":"intermediate","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","DoorDash","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T17:38:30.594Z","createdAt":"2026-01-13T17:38:30.594Z"},{"id":"q-1570","question":"A Linux CI node running multiple git builds experiences occasional stalls during parallel jobs. Using only default tooling and no downtime, describe a concrete diagnostic workflow to (1) determine if CPU, memory, or I/O is the bottleneck, (2) identify the specific component (e.g., git, filesystem, network) causing the stall, and (3) apply a safe mitigation (e.g., throttle parallel jobs, adjust I/O scheduler, or raise file descriptor limits) while monitoring impact. Include exact commands and expected outputs?","answer":"Run `vmstat 1 60` and `iostat -xz 1 60`, then `top -b -n1` to identify CPU, memory, and I/O pressure. If iowait is elevated, map I/O to processes with `pidstat -d 1`. If a git/build task is causing the issue, throttle parallel jobs using `make -j$(nproc)` or adjust CI job concurrency.","explanation":"## Why This Is Asked\nTests practical skills in diagnosing common Linux bottlenecks using default tools and linking observations to concrete mitigations in a CI context.\n\n## Key Concepts\n- Baseline and interpret vmstat/iostat outputs\n- I/O wait (iowait) vs CPU\n- PID-level I/O mapping with pidstat\n- Safe mitigations: adjust concurrency, swappiness, fd limits, scheduler\n\n## Code Example\n```bash\nvmstat 1 60\niostat -xz 1 60\ntop -b -n1 | head\n```\n\n## Follow-up Questions\n- How would you verify that mitigation didn't degrade other workloads?\n- Which metrics would you log for post-change validation?","diagram":null,"difficulty":"beginner","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","Tesla","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T06:12:46.493Z","createdAt":"2026-01-13T22:33:22.994Z"},{"id":"q-1705","question":"You have a Linux host running a Rust-based data-processing daemon that stalls for 30–60 seconds under peak load. Using only default tooling, design a concrete diagnostic workflow to (1) determine if the stall is CPU-, I/O-, or memory-bound, (2) identify the exact subsystem or process causing the stall, and (3) apply a safe mitigation (e.g., cgroup throttling or IO-scheduler tweaks) while preserving observability. Include exact commands and expected outputs?","answer":"Begin with a system-wide view, then drill down:\n- top -b -n1\n- vmstat 1\n- iostat -xz 1\n- pidstat -p ALL 1\nCompare CPU idle, IO wait, and memory pressure. If IO-wait is high, enable BFQ (or deadline) a","explanation":"## Why This Is Asked\nTests practical Linux troubleshooting under realistic load, emphasizing observable symptoms, bottleneck identification, and safe mitigations that preserve service continuity.\n\n## Key Concepts\n- Distinguish CPU, IO, and memory bottlenecks via metrics\n- Map waits to processes with pidstat and vmstat\n- Safe mitigations: IO scheduler tuning, cgroups throttling, memory settings\n\n## Code Example\n```javascript\n// Shell-like workflow sketch (illustrative)\nconst steps = [\n  'top -b -n1',\n  'vmstat 1',\n  'iostat -xz 1',\n  'pidstat -p ALL 1'\n];\n```\n\n## Follow-up Questions\n- How would you revert mitigations if they worsen latency?\n- Which metrics indicate readiness for throttling vs. scheduler change?","diagram":"flowchart TD\n  A[Start] --> B[Collect metrics]\n  B --> C{Bottleneck?}\n  C -- CPU --> D[Throttle CPU via cgroups]\n  C -- IO --> E[Tune IO scheduler & limit writers]\n  C -- Mem --> F[Adjust swapiness/min_free_kbytes]\n  D --> G[Validate metrics]\n  E --> G\n  F --> G\n  G --> H[Continue monitoring]","difficulty":"intermediate","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Hugging Face","Lyft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T07:41:25.531Z","createdAt":"2026-01-14T07:41:25.531Z"},{"id":"q-466","question":"You're debugging a production Linux server where processes are randomly dying with 'Out of memory' errors, but `free -m` shows 8GB available RAM. How would you diagnose and fix this issue?","answer":"Check `dmesg | grep -i oom-killer` for OOM events. Use `cat /proc/meminfo` to examine memory fragmentation. Review `overcommit_memory` and `overcommit_ratio` in `/proc/sys/vm/`. Monitor with `sar -r` ","explanation":"## Memory Management Issues\n\nLinux OOM killer activates when available memory + swap < pages_min * 4, not when RAM is fully used.\n\n## Diagnostic Steps\n\n- Check OOM killer logs: `dmesg | grep -i oom-killer`\n- Examine memory fragmentation: `cat /proc/meminfo | grep -E '(MemFree|MemAvailable|Slab|PageTables)'`\n- Monitor memory pressure: `cat /proc/pressure/memory`\n- Review overcommit settings: `cat /proc/sys/vm/overcommit_memory`\n\n## Common Causes\n\n- Memory fragmentation preventing large allocations\n- Overcommitment allowing more memory than physically available\n- Kernel memory usage (slabs, page tables) not visible in `free`\n- Memory leaks in kernel modules\n\n## Solutions\n\n```bash\n# Disable overcommit\necho 0 > /proc/sys/vm/overcommit_memory\n\n# Add swap space\nfallocate -l 2G /swapfile\nchmod 600 /swapfile\nmkswap /swapfile\nswapon /swapfile\n\n# Tune memory management\necho 65536 > /proc/sys/vm/min_free_kbytes\n```","diagram":"flowchart TD\n  A[Process Memory Request] --> B{Physical RAM Available?}\n  B -->|No| C[Check Swap]\n  B -->|Yes| D[Allocate Memory]\n  C -->|Swap Available| E[Use Swap]\n  C -->|No Swap| F{Overcommit Enabled?}\n  F -->|Yes| G[Allow Allocation]\n  F -->|No| H[OOM Killer Activates]\n  G --> I[Memory Pressure Builds]\n  I --> H\n  H --> J[Kill Process]\n  J --> K[Log to dmesg]","difficulty":"advanced","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":["oom killer","memory fragmentation","overcommit_memory","dmesg","proc/meminfo","sar","system diagnostics"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2025-12-27T04:55:16.000Z","createdAt":"2025-12-26 12:51:05"},{"id":"q-496","question":"How would you find all processes running on port 8080 and terminate them safely?","answer":"Use `lsof -i :8080` to identify processes, then `kill -15 PID` for graceful termination. If unresponsive, use `kill -9 PID`. For multiple processes: `pkill -f ':8080'` sends SIGTERM to all matching processes.","explanation":"## Process Identification\n- `lsof -i :8080` lists processes using the port\n- `netstat -tulpn | grep :8080` alternative method\n\n## Safe Termination\n- `kill -15 PID` (SIGTERM) allows graceful shutdown\n- `kill -9 PID` (SIGKILL) forces immediate termination\n\n## Batch Operations\n- `pkill -f ':8080'` terminates all matching processes\n- `fuser -k 8080/tcp` kills processes by port","diagram":"flowchart TD\n  A[Identify Port Usage] --> B[lsof -i :8080]\n  B --> C{Process Responding?}\n  C -->|Yes| D[kill -15 PID]\n  C -->|No| E[kill -9 PID]\n  D --> F[Verify Termination]\n  E --> F","difficulty":"beginner","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-08T11:59:14.782Z","createdAt":"2025-12-26 12:51:06"},{"id":"q-527","question":"How would you find and kill a process that's using port 8080 on a Linux system?","answer":"Use `lsof -i :8080` or `netstat -tulpn | grep 8080` to find the PID, then `kill -9 <PID>` to terminate it. For a safer approach, try `kill -15 <PID>` first to allow graceful shutdown.","explanation":"## Finding the Process\n- `lsof -i :8080` lists processes using that port\n- `netstat -tulpn` shows all listening processes with PIDs\n- `ss -tulpn | grep 8080` is a modern alternative\n\n## Terminating the Process\n- `kill -15 <PID>` sends SIGTERM for graceful shutdown\n- `kill -9 <PID>` sends SIGKILL for force termination\n- `pkill -f \"process_name\"` kills by name pattern\n\n## Verification\n- `lsof -i :8080` confirms process is killed\n- `netstat -tulpn | grep 8080` should return empty","diagram":"flowchart TD\n  A[Port 8080 Issue] --> B[lsof -i :8080]\n  B --> C[Get PID]\n  C --> D[kill -15 PID]\n  D --> E{Process Killed?}\n  E -->|No| F[kill -9 PID]\n  E -->|Yes| G[Verify with lsof]","difficulty":"beginner","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Oracle","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-25T15:01:25.772Z","createdAt":"2025-12-26 12:51:06"},{"id":"q-553","question":"You're troubleshooting a production server where a critical process keeps getting killed. How would you diagnose if it's an OOM kill versus other issues, and what specific commands would you use to investigate?","answer":"Check dmesg for 'Out of memory' messages and /var/log/messages. Use `free -h` to see memory usage, `ps aux --sort=-%mem` to find memory hogs, and `cat /proc/<pid>/status` for VmRSS. If OOM, check `/proc/sys/vm/panic_on_oom` and `/proc/sys/vm/oom_kill_allocating_task` to understand kernel behavior.","explanation":"## Diagnosis Steps\n- Check system logs for OOM kill indicators\n- Analyze current memory usage patterns\n- Identify memory-intensive processes\n\n## Key Commands\n```bash\ndmesg | grep -i oom\nfree -h\nps aux --sort=-%mem | head -10\ncat /proc/$(pidof process)/status | grep Vm\n```\n\n## Prevention\n- Monitor memory usage with `top`/`htop`\n- Set up alerts for high memory usage\n- Configure appropriate swap space\n- Tune kernel parameters like `vm.swappiness`","diagram":"flowchart TD\n  A[Process Killed] --> B{Check dmesg}\n  B -->|OOM messages| C[Analyze memory usage]\n  B -->|No OOM| D[Check other signals]\n  C --> E[Identify memory hogs]\n  E --> F[Tune kernel params]\n  D --> G[Check logs/crash dumps]","difficulty":"intermediate","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Scale Ai","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-08T11:55:47.944Z","createdAt":"2025-12-26 12:51:07"},{"id":"q-580","question":"How would you find all processes using a specific port and terminate one safely?","answer":"Use `lsof -i :8080` to list processes on port 8080, then `kill -15 <PID>` for graceful shutdown. If unresponsive, use `kill -9 <PID>` as last resort. Always check process dependencies first.","explanation":"## Finding Processes\n- `lsof -i :<port>` lists processes using the port\n- `netstat -tulpn | grep :<port>` alternative method\n\n## Safe Termination\n- `kill -15 <PID>` sends SIGTERM for graceful shutdown\n- Allows process to cleanup resources and save state\n- Wait reasonable time before escalating\n\n## Force Termination\n- `kill -9 <PID>` sends SIGKILL as last resort\n- Immediate termination without cleanup\n- Can cause data corruption or resource leaks","diagram":"flowchart TD\n  A[Identify Port] --> B[lsof -i :port]\n  B --> C[List PIDs]\n  C --> D{Process Responsive?}\n  D -->|Yes| E[kill -15 PID]\n  D -->|No| F[kill -9 PID]\n  E --> G[Monitor Shutdown]\n  F --> H[Force Terminate]","difficulty":"beginner","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","MongoDB","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":["lsof","process termination","graceful shutdown","pid","port","dependencies","kill command"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2025-12-27T04:58:08.288Z","createdAt":"2025-12-27T01:13:40.807Z"},{"id":"q-917","question":"Scenario: a Linux server hosting a web app experiences sporadic high response times during peak hours. Using only default tools and no downtime, describe a concrete, beginner-friendly diagnostic workflow to (1) determine whether CPU, memory, or I/O is the bottleneck, (2) identify the offending process, and (3) apply a safe mitigation (e.g., graceful restart) while monitoring impact. Include exact commands and expected outputs?","answer":"Run top in batch to spot heavy CPU: top -b -n1; then dump top processes: ps -eo pid,comm,pcpu,pmem --sort=-pcpu | head -5. Check IO with iostat -dx 1 2 and memory with free -m. If a process hogs, grac","explanation":"## Why This Is Asked\n\\nTests practical, beginner-friendly triage in production-like Linux environments, focusing on real commands and safe mitigations without downtime.\\n\\n## Key Concepts\n- Basic bottleneck triage (CPU, memory, I/O)\\n- Identifying culprits with top and ps\\n- Verifying I/O and memory health with iostat and free\\n- Safe mitigation via graceful restart and live monitoring\\n\\n## Code Example\n```javascript\n// Diagnostics snippet (commands)\ntop -b -n1 | head -20\nps -eo pid,comm,pcpu,pmem --sort=-pcpu | head -5\niostat -dx 1 2\nfree -m\njournalctl -u service -n 100 --no-pager\n```\n\n## Follow-up Questions\n- How would you automate this workflow for nightly checks?\\n- What differences would you expect between CPU-bound and I/O-bound symptoms?","diagram":null,"difficulty":"beginner","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Microsoft","PayPal","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T15:30:13.738Z","createdAt":"2026-01-12T15:30:13.738Z"}],"subChannels":["commands","general"],"companies":["Adobe","Airbnb","Amazon","Anthropic","Apple","Citadel","Cloudflare","Databricks","DoorDash","Google","Hashicorp","Hugging Face","IBM","Instacart","LinkedIn","Lyft","Microsoft","MongoDB","NVIDIA","Netflix","OpenAI","Oracle","PayPal","Salesforce","Scale Ai","Snap","Snowflake","Square","Tesla","Twitter","Two Sigma","Uber"],"stats":{"total":18,"beginner":10,"intermediate":5,"advanced":3,"newThisWeek":12}}