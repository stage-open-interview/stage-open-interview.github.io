{"questions":[{"id":"gh-26","question":"What are the essential Linux commands every DevOps engineer should master for system administration, troubleshooting, and automation?","answer":"Essential Linux commands span file operations (ls, cp, mv, rm, find), text processing (grep, sed, awk, cut, sort, uniq), process management (ps, top, htop, kill, nice, jobs), network diagnostics (netstat, ss, curl, wget, ssh, nc), permissions (chmod, chown, chgrp, setfacl), package management (apt, yum, dnf, snap, flatpak), system monitoring (df, du, free, iostat, vmstat, lsof), and containerization tools (docker, podman, systemd, journalctl), forming the comprehensive foundation for modern DevOps automation and system administration.","explanation":"## Interview Context\nThis question evaluates practical Linux proficiency for DevOps engineers, testing knowledge beyond basic commands to include modern infrastructure tools and best practices.\n\n## Key Areas to Cover\n- **File System Operations**: Advanced navigation with find, locate, which; manipulation with dd, rsync; permissions including ACLs and extended attributes\n- **Process Management**: Real-time monitoring with htop, process control with kill signals, job scheduling with cron and systemd timers\n- **Network Tools**: Connection monitoring with ss, packet analysis with tcpdump, service discovery with dig/nslookup\n- **Text Processing**: Advanced pattern matching with regex, data transformation pipelines, log analysis techniques\n- **System Monitoring**: Performance metrics with sar, resource tracking, disk usage analysis\n- **Container Orchestration**: Docker commands for image/container lifecycle, systemd service management, journalctl for log analysis\n- **Security**: User management, audit trails, SELinux/AppArmor basics\n\n## Example Responses\nDemonstrate chaining commands like `ps aux | grep nginx | awk '{print $2}' | xargs kill -9`, or Docker workflows: `docker build -t app . && docker run -d --restart=unless-stopped app`. Show understanding of systemd unit files and service management commands.","diagram":"\ngraph TD\n    Linux --> Files[File Ops: ls, cp, mv]\n    Linux --> Sys[System: top, df, ps]\n    Linux --> Text[Text: grep, awk, sed]\n","difficulty":"beginner","tags":["linux","shell"],"channel":"linux","subChannel":"commands","sourceUrl":null,"videos":{"shortVideo":"https://www.youtube.com/watch?v=zB_3FIGRWRU"},"companies":["Amazon","Cloudflare","Google","Hashicorp","Microsoft","Netflix"],"eli5":"Imagine you're the boss of a giant playground with lots of toys and games. Linux commands are like your magic words to control everything! 'ls' is like looking at all your toys on the shelf. 'cp' is like making copies of your favorite drawings. 'mv' is like moving toys to different boxes. 'grep' is like finding specific words in your storybooks. 'ps' is like checking which friends are playing on the swings. 'curl' is like sending secret messages to other playgrounds. 'chmod' is like deciding who can play with which toys - you, your friends, or everyone. These magic words help you keep the playground running smoothly, fix broken toys, and make games happen automatically while you eat snacks!","relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-29T07:00:08.286Z","createdAt":"2025-12-26 12:51:05"},{"id":"q-1000","question":"A Linux host runs several Docker containers; during peak load, API responses slow and some requests time out. Describe a beginner-friendly, concrete diagnostic workflow to (1) confirm whether host saturation is CPU, memory, or I/O, (2) identify the container most responsible, and (3) apply a safe mitigation (e.g., graceful restart or CPU/memory throttling) while monitoring impact. Include exact commands and expected outputs?","answer":"Use top/vmstat to identify bottleneck, then docker stats --no-stream to spot the hogging container. Correlate with docker ps. If CPU or memory is the issue, throttle with docker update --cpu-shares 51","explanation":"## Why This Is Asked\nThis tests practical, beginner-friendly diagnostic thinking for containerized workloads.\n\n## Key Concepts\n- Metrics: CPU/memory/IO\n- Per-container accounting via docker stats\n- Safe mitigations: CPU shares, memory limits, graceful restarts\n\n## Code Example\n```javascript\ntop -b -n1\ndocker stats --no-stream\n```\n\n## Follow-up Questions\n- How would you verify the mitigation actually improved latency?\n- What are edge cases when throttling hurts user experience?","diagram":null,"difficulty":"beginner","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","LinkedIn"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T18:45:16.170Z","createdAt":"2026-01-12T18:45:16.170Z"},{"id":"q-1142","question":"Scenario: A Linux server hosting a small web service used by customers suddenly shows disk usage climbing on the root filesystem. Provide a beginner-friendly, concrete diagnostic workflow to (1) locate the directories/files consuming the most space, (2) identify top offenders, and (3) apply a safe mitigation (e.g., rotate/compress/archive logs or purge old data) while keeping the service up. Include exact commands and expected outputs?","answer":"Check space: df -h. Find large files/dirs: du -hx --max-depth=1 / 2>/dev/null | sort -rh | head -n 5. Focus on /var/log: du -xh /var/log | sort -rh | head -n 5. Mitigate safely: tar czf /backup/logs-$","explanation":"## Why This Is Asked\n\nAssesses practical diagnostic skills for a common sysadmin pain point: disk pressure. Tests ability to use core CLI tools and perform safe, non-downtime mitigations.\n\n## Key Concepts\n\n- Disk usage analytics with df and du\n- Identifying large offenders efficiently\n- Safe log rotation/archiving and selective truncation\n- Lightweight monitoring after mitigation\n\n## Code Example\n\n```bash\n# Safe archival of logs\nLOG_ARCHIVE=/backup/logs-$(date +%F).tar.gz\ntar czf \"$LOG_ARCHIVE\" /var/log\n\n# Purge old logs (age > 30 days)\nfind /var/log -type f -iname '*.log' -mtime +30 -delete\n```\n\n## Follow-up Questions\n\n- How would you adapt this for a system with logrotation already in place?\n- What risks exist when deleting or truncating logs, and how to mitigate?","diagram":null,"difficulty":"beginner","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Google","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T01:28:46.981Z","createdAt":"2026-01-13T01:28:46.981Z"},{"id":"q-1205","question":"You're operating a fleet of Linux hosts running a low-latency web API. Intermittent 100–200 ms latency spikes appear under load. Design a practical, end-to-end diagnostic plan using eBPF/BPFtrace or perf, iostat/vmstat, and container metrics to collect data, identify root cause (CPU scheduling, IO wait, or network), and propose minimal-disruption mitigations?","answer":"Baseline with iostat -dx 1, vmstat 1, and kubectl top; use eBPFtrace to map per-process IO latency: tracepoint:block:block_rq_issue { @s[pid] = nsecs; } tracepoint:block:block_rq_complete { @d[pid] = ","explanation":"## Why This Is Asked\nDemonstrates practical, instrumented thinking for production Linux latency issues using modern tracing tools.\n\n## Key Concepts\n- eBPF/BPFtrace, perf, ftrace\n- IO schedulers, MQ, quotas\n- Container metrics and baseline-vs-spike analysis\n\n## Code Example\n```bash\ntracepoint:block:block_rq_issue { @s[pid] = nsecs; }\ntracepoint:block:block_rq_complete { @d[pid] = hist(nsecs - @s[pid]); }\n```\n\n## Follow-up Questions\n- How would you extend this to a multi-tenant cluster?\n- What are the risks of tracing in production and how would you mitigate them?","diagram":null,"difficulty":"advanced","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Salesforce","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T04:53:37.702Z","createdAt":"2026-01-13T04:53:37.702Z"},{"id":"q-1244","question":"Scenario: A Linux host runs a MongoDB primary in a high-traffic cluster. During peak hours, latency spikes and tail latency increases while CPU and memory appear stable. Using only default tooling, no downtime, describe a concrete, actionable diagnostic workflow to (1) determine if I/O wait, CPU, or memory is the bottleneck, (2) pinpoint the offending disk/device or process, and (3) apply a safe mitigation (e.g., adjust I/O scheduler, tune dirty writeback parameters, or throttle MongoDB) while monitoring impact. Include exact commands and expected outputs?","answer":"Run vmstat, iostat, and top in batch to correlate latency with system metrics, then pidstat to isolate the offender. If disk I/O wait dominates, switch a block device’s scheduler to deadline/bfq and a","explanation":"## Why This Is Asked\nThis probes practical Linux performance diagnosis under live load, focusing on I/O bottlenecks and safe mitigations in a MongoDB context.\n\n## Key Concepts\n- I/O wait, disk scheduling, and writeback pressure\n- Process-level bottleneck identification with pidstat/top\n- Safe live mitigations (scheduler changes, sysctl tweaks, application-level throttling)\n\n## Code Example\n```javascript\n// Simple Node.js snippet to run common diagnostics sequentially\nconst {execSync} = require('child_process');\nconsole.log(execSync('vmstat 1 5; iostat -xz 1 5; top -bn1', {encoding: 'utf8'}));\n```\n\n## Follow-up Questions\n- How would you validate the effectiveness of the scheduler change?\n- What risks exist when adjusting dirty_writeback_ratio on a live DB?","diagram":"flowchart TD\n  Start[Start] --> Metrics[Collect metrics: vmstat, iostat, top]\n  Metrics --> Bottleneck{Bottleneck detected?}\n  Bottleneck -->|Yes| Isolate[Identify device/process via pidstat / proc/diskstats]\n  Bottleneck -->|No| End[End]\n  Isolate --> Mitigate[Apply safe mitigation: scheduler or sysctl]\n  Mitigate --> Verify[Reobserve metrics to confirm]\n  Verify --> End","difficulty":"intermediate","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Databricks","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T06:38:24.003Z","createdAt":"2026-01-13T06:38:24.003Z"},{"id":"q-1266","question":"Context: Linux node in a Kubernetes cluster hosting a high-throughput data ingestion service. Intermittent tail latency spikes (>1s) appear during peak traffic, affecting processing. Without downtime, design a concrete troubleshooting workflow to (1) confirm whether CPU, I/O, or network is the bottleneck, (2) identify the exact subsystem or process responsible, and (3) implement a safe mitigation with minimal impact while maintaining observability. Include exact commands and realistic outputs?","answer":"Begin with per-second metrics: iostat -xz 1 2; mpstat -P ALL 1; vmstat 1. Identify hot processes with pidstat -p ALL 1 | head. If IO lag appears, run perf stat -p <pid> -e cycles,instructions,cache-re","explanation":"## Why This Is Asked\nAssesses practical, end-to-end debugging in real-world Linux systems, including multi-subsystem correlation and safe mitigations with minimal downtime.\n\n## Key Concepts\n- Tail latency diagnosis across CPU, IO, and network\n- Per-second monitoring with iostat/mpstat/vmstat\n- Process-level tracing with pidstat and perf\n- I/O schedulers and CPU pinning as mitigations\n\n## Code Example\n```javascript\n// Example: parse perf output snapshot (illustrative)\nfunction parsePerf(line){ /* simplistic parser */ }\n```\n\n## Follow-up Questions\n- How would you adapt this workflow for containerized workloads (Kubernetes, CNI, cgroups)?\n- How would you validate the mitigation under synthetic peak load?","diagram":null,"difficulty":"intermediate","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Anthropic","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T07:32:47.290Z","createdAt":"2026-01-13T07:32:47.290Z"},{"id":"q-1364","question":"A Linux host running multiple microservices behind NGINX exhibits intermittent latency spikes; new connections occasionally fail and FD usage appears high. Using only default tools, describe a beginner-friendly workflow to (1) confirm FD exhaustion is the bottleneck, (2) identify the offending process by per‑process FD usage, and (3) apply a safe mitigation (increase NOFILE limits, adjust per-service limits, or set a systemd limit) while monitoring impact. Include exact commands and expected outputs?","answer":"Check limits and usage: cat /proc/sys/fs/file-max; ulimit -n; grep OpenFiles /proc/$(pgrep -f nginx)/limits 2>/dev/null. Identify top FDs: for p in /proc/[0-9]*; do [ -r \\\"$p/fd\\\" ] && echo -n \\\"${p##","explanation":"## Why This Is Asked\nFD exhaustion can silently throttle connections under load. This question probes practical techniques to verify limits, count per‑process FDs, and apply safe, scoped mitigations without downtime.\n\n## Key Concepts\n- File descriptor limits (nofile, file-max, nr_open)\n- /proc/<pid>/limits and per‑process FD counts\n- systemd LimitNOFILE and limits.conf\n- Safe, incremental changes and live monitoring\n\n## Code Example\n```javascript\n// Example Node.js utility to count open fds for a pid\nconst fs = require('fs');\nconst pid = process.argv[2];\nconsole.log('Open fds for', pid, ':', fs.readdirSync(`/proc/${pid}/fd`).length);\n```\n\n## Follow-up Questions\n- How would you automate per-service FD limits with systemd?\n- What are risks of increasing global limits?","diagram":"flowchart TD\n  A[FD exhaustion suspected] --> B[Check system limits]\n  B --> C[Count per-process FDs]\n  C --> D[Mitigate via limits or systemd]\n  D --> E[Monitor impact]","difficulty":"beginner","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","PayPal","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T13:17:53.743Z","createdAt":"2026-01-13T13:17:53.743Z"},{"id":"q-1410","question":"On a Linux host, a data-processing job occasionally stalls during bursts. Using only default tooling and no downtime, outline a concrete, beginner-friendly diagnostic flow to (1) confirm CPU, memory, or I/O bound, (2) identify the offending process and its operation, and (3) apply a safe mitigation (e.g., adjust priority, pause/resume, or throttle) with monitoring. Which commands would you run and what outputs would you expect?","answer":"Run top or ps to locate high CPU/mem processes, then vmstat 1 5 to watch IO and swap, and iostat -dx 1 5 to confirm IO waits. Note PID and cmd. Mitigate by renice -n 10 -p <pid> or pause with kill -SI","explanation":"## Why This Is Asked\nThis checks practical triage using default tools and safe mitigations. It also tests how the candidate identifies bottlenecks and minimizes disruption.\n\n## Key Concepts\n- Resource bottlenecks vs bursts\n- Observability: top/ps, vmstat, iostat\n- Process control: renice, SIGSTOP/SIGCONT\n\n## Code Example\n```bash\ntop -b -n1 | head\nps -eo pid,ppid,cmd,%cpu --sort=-%cpu | head\nvmstat 1 5\niostat -dx 1 5\nrenice -n 10 -p <pid>\n```\n\n## Follow-up Questions\n- What if iostat isn't installed?\n- How would you verify impact after mitigation?","diagram":"flowchart TD\n  A[Start] --> B{Identify bottleneck}\n  B --> C[CPU]\n  B --> D[Memory]\n  B --> E[IO]\n  C --> F[Lower priority]\n  D --> G[Tune cache/allocations]\n  E --> H[Adjust IO scheduler]","difficulty":"beginner","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","OpenAI","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T15:50:44.422Z","createdAt":"2026-01-13T15:50:44.422Z"},{"id":"q-1424","question":"Scenario: a high-traffic Linux service behind a reverse proxy experiences sporadic tail latency spikes during peak hours. CPU and IO look normal in aggregate. Design a practical end-to-end diagnostic using eBPF to identify root causes quickly: (1) instrument per-request latency across kernel and user-space, (2) attribute latency to network, disk, or app code, (3) propose safe mitigations and validate impact. Include specific probes, sample commands, and expected outputs?","answer":"Use an end-to-end eBPF trace with bpftrace: attach kprobes/kretprobes on accept, recv, and send, plus the HTTP handler; store start times per tid and compute deltas to build a latency histogram keyed ","explanation":"## Why This Is Asked\nTests practical eBPF observability, end-to-end tracing, and ability to propose concrete mitigations in production-like settings.\n\n## Key Concepts\n- eBPF tracing for low-overhead observability\n- kprobes/kretprobes and tracepoints\n- Per-request latency histograms and map keying (tid, IP, URL)\n- Tail latency analysis and cross-layer attribution\n- Safe mitigations: TCP tuning, NIC offloads, queue depths\n\n## Code Example\n```javascript\n#!/usr/bin/env bpftrace\nBEGIN { @start[tid] = 0; }\ntracepoint:syscalls:sys_enter_accept { @start[tid] = nsecs; }\ntracepoint:syscalls:sys_exit_accept { @lat[tid] = nsecs - @start[tid]; delete(@start[tid]); }\n```\n\n## Follow-up Questions\n- How would you extend this to multi-node correlation with time sync?\n- What are risks of running eBPF in production and how would you mitigate them?","diagram":null,"difficulty":"advanced","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Cloudflare","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T16:46:26.464Z","createdAt":"2026-01-13T16:46:26.464Z"},{"id":"q-1445","question":"A Linux host running GPU-accelerated video processing containers shows intermittent frame latency spikes during peak load, with no obvious CPU/memory/I/O bottlenecks. Without downtime, describe a concrete, real-world diagnostic workflow using only default tools to (1) verify if latency is caused by page cache pressure or Transparent Huge Pages, (2) identify the subsystem or container involved, and (3) apply a safe mitigation (e.g., disable THP on affected NUMA nodes, tune swappiness, or pin tasks) with minimal impact and observable results. Include exact commands and expected outputs?","answer":"Workflow: verify spike with top/iostat, inspect THP and page cache, locate offending container, and apply a safe mitigation. Commands: cat /sys/kernel/mm/transparent_hugepage/enabled; vmstat 1 5; iost","explanation":"## Why This Is Asked\nAssesses hands-on Linux troubleshooting for latency with containerized workloads, focusing on memory reclamation, THP behavior, and safe, minimal-impact mitigations.\n\n## Key Concepts\n- Transparent Huge Pages (THP) behavior and toggling\n- Page cache vs memory pressure\n- Container isolation and CPU pinning\n- Default tooling for on-call debugging\n\n## Code Example\n```bash\n# Check THP state\ncat /sys/kernel/mm/transparent_hugepage/enabled\n# Observe memory/caching behavior\nvmstat 1 5\n# Disk and I/O detail\niostat -dx 1\n# Identify running containers\ndocker ps\n# See heavy processes inside a container\ndocker top <id>\n# Mitigation: disable THP and pin a pid to CPUs\nsudo tee /sys/kernel/mm/transparent_hugepage/enabled <<< 'never'\ntaskset -cp 0-7 <pid>\n```\n\n## Follow-up Questions\n- How would you validate no performance regression after disabling THP? \n- How could you automate this workflow for on-call use?\n","diagram":"flowchart TD\n  A[Latency Event] --> B[Collect Metrics]\n  B --> C[Check THP/Cache]\n  C --> D[Identify Container]\n  D --> E[Apply Mitigation]\n  E --> F[Monitor Metrics]","difficulty":"intermediate","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","DoorDash","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T17:38:30.594Z","createdAt":"2026-01-13T17:38:30.594Z"},{"id":"q-1570","question":"A Linux CI node running multiple git builds experiences occasional stalls during parallel jobs. Using only default tooling and no downtime, describe a concrete diagnostic workflow to (1) determine if CPU, memory, or I/O is the bottleneck, (2) identify the specific component (e.g., git, filesystem, network) causing the stall, and (3) apply a safe mitigation (e.g., throttle parallel jobs, adjust I/O scheduler, or raise file descriptor limits) while monitoring impact. Include exact commands and expected outputs?","answer":"Run `vmstat 1 60` and `iostat -xz 1 60`, then `top -b -n1` to identify CPU, memory, and I/O pressure. If iowait is elevated, map I/O to processes with `pidstat -d 1`. If a git/build task is causing the issue, throttle parallel jobs using `make -j$(nproc)` or adjust CI job concurrency.","explanation":"## Why This Is Asked\nTests practical skills in diagnosing common Linux bottlenecks using default tools and linking observations to concrete mitigations in a CI context.\n\n## Key Concepts\n- Baseline and interpret vmstat/iostat outputs\n- I/O wait (iowait) vs CPU\n- PID-level I/O mapping with pidstat\n- Safe mitigations: adjust concurrency, swappiness, fd limits, scheduler\n\n## Code Example\n```bash\nvmstat 1 60\niostat -xz 1 60\ntop -b -n1 | head\n```\n\n## Follow-up Questions\n- How would you verify that mitigation didn't degrade other workloads?\n- Which metrics would you log for post-change validation?","diagram":null,"difficulty":"beginner","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","Tesla","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T06:12:46.493Z","createdAt":"2026-01-13T22:33:22.994Z"},{"id":"q-1705","question":"You have a Linux host running a Rust-based data-processing daemon that stalls for 30–60 seconds under peak load. Using only default tooling, design a concrete diagnostic workflow to (1) determine if the stall is CPU-, I/O-, or memory-bound, (2) identify the exact subsystem or process causing the stall, and (3) apply a safe mitigation (e.g., cgroup throttling or IO-scheduler tweaks) while preserving observability. Include exact commands and expected outputs?","answer":"Begin with a system-wide view, then drill down:\n- top -b -n1\n- vmstat 1\n- iostat -xz 1\n- pidstat -p ALL 1\nCompare CPU idle, IO wait, and memory pressure. If IO-wait is high, enable BFQ (or deadline) a","explanation":"## Why This Is Asked\nTests practical Linux troubleshooting under realistic load, emphasizing observable symptoms, bottleneck identification, and safe mitigations that preserve service continuity.\n\n## Key Concepts\n- Distinguish CPU, IO, and memory bottlenecks via metrics\n- Map waits to processes with pidstat and vmstat\n- Safe mitigations: IO scheduler tuning, cgroups throttling, memory settings\n\n## Code Example\n```javascript\n// Shell-like workflow sketch (illustrative)\nconst steps = [\n  'top -b -n1',\n  'vmstat 1',\n  'iostat -xz 1',\n  'pidstat -p ALL 1'\n];\n```\n\n## Follow-up Questions\n- How would you revert mitigations if they worsen latency?\n- Which metrics indicate readiness for throttling vs. scheduler change?","diagram":"flowchart TD\n  A[Start] --> B[Collect metrics]\n  B --> C{Bottleneck?}\n  C -- CPU --> D[Throttle CPU via cgroups]\n  C -- IO --> E[Tune IO scheduler & limit writers]\n  C -- Mem --> F[Adjust swapiness/min_free_kbytes]\n  D --> G[Validate metrics]\n  E --> G\n  F --> G\n  G --> H[Continue monitoring]","difficulty":"intermediate","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Hugging Face","Lyft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T07:41:25.531Z","createdAt":"2026-01-14T07:41:25.531Z"},{"id":"q-1873","question":"Context: A Linux host runs a Kubernetes-deployed real-time inference service behind Nginx; latency tail spikes occur during bursts. Without downtime, describe a concrete, repeatable workflow to determine whether latency is caused by CPU throttling, memory pressure, or network queueing, identify the offending container/pod, and apply a safe mitigation (e.g., adjust cgroup limits, scale the deployment, or tune kernel parameters) while preserving observability. Include exact commands, expected outputs, and a simple rollback plan?","answer":"Start by capturing latency and per-pod resource use: kubectl top pod my-infer and kubectl top node. Inspect cgroups for the pod: read quotas with cat /sys/fs/cgroup/cpu/kubepods/.../cpu.stat and memor","explanation":"## Why This Is Asked\nTests practical troubleshooting in a containerized Linux environment, focusing on per-pod resource contention and observability. It differentiates CPU throttling, memory pressure, and network effects.\n\n## Key Concepts\n- Kubernetes resource requests/limits and QoS\n- Linux cgroups CPU and memory accounting\n- I/O and network bottlenecks in a container\n- Safe mitigation with observable rollback\n\n## Code Example\n```javascript\nkubectl top pod my-infer\nkubectl top node\ncat /sys/fs/cgroup/cpu/kubepods/.../cpu.stat\niostat -dx 1 5\ntc qdisc show\nss -tuln\npidstat -p ALL 1 5\n```\n\n## Follow-up Questions\n- How would you automate this workflow for future bursts?\n- What metrics indicate rollback safety vs. risk of thrashing?","diagram":"flowchart TD\n  A[Latency Spike] --> B{Check CPU}\n  B --> C[CPU throttling or limits]\n  A --> D{Check Memory}\n  D --> E[Memory pressure / OOM]\n  A --> F{Check Network}\n  F --> G[Queueing / NAPI]","difficulty":"intermediate","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Hugging Face","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T15:34:57.283Z","createdAt":"2026-01-14T15:34:57.283Z"},{"id":"q-2038","question":"Scenario: A Linux host running a batch queue occasionally fails to start new jobs with 'Too many open files' under moderate load. Without downtime, describe a concrete, beginner-friendly diagnostic workflow using default tools to (1) confirm FD limits are the bottleneck, (2) identify the process or user hitting the limit, and (3) apply a safe mitigation (e.g., raise per-user limits, adjust LimitNOFILE for the service) while keeping service available. Include exact commands and expected outputs?","answer":"To diagnose 'Too many open files' errors without downtime, follow this systematic workflow:\n\n**1. Confirm FD limits are the bottleneck:**\n```bash\nulimit -n                    # Check current user soft limit\npidof queue                  # Get the queue process PID\nlsof | wc -l                # Count total open files system-wide\ncat /proc/sys/fs/file-max   # Check system-wide maximum\ncat /proc/$(pidof queue)/limits | grep 'Max open files'  # Check process-specific limits\n```\n\n**2. Identify the process or user hitting the limit:**\n```bash\nlsof -p $(pidof queue) | wc -l                    # Count files opened by queue process\nps -eo pid,user,comm --sort=-pid | head -20        # List top processes by PID\nlsof -u $(whoami) | wc -l                          # Count files per current user\nss -tulpn | head -10                               # Check network connections consuming FDs\n```\n\n**3. Apply safe mitigation while keeping service available:**\n\nFor per-user limits:\n```bash\necho '* soft nofile 4096' >> /etc/security/limits.conf\necho '* hard nofile 8192' >> /etc/security/limits.conf\nulimit -n 4096                                      # Apply immediately for current session\n```\n\nFor systemd services:\n```bash\nsystemctl edit queue.service\n# Add: [Service] LimitNOFILE=8192\nsystemctl daemon-reload && systemctl restart queue.service\n```\n\nExpected outputs should show increased limits and reduced errors in `/var/log/syslog` or service logs.","explanation":"## Why This Is Asked\nTests understanding of Linux file descriptor management and practical troubleshooting techniques that maintain service availability.\n\n## Key Concepts\n- File descriptor limits (soft vs hard constraints)\n- Per-user versus system-wide resource management\n- systemd service configuration with LimitNOFILE\n- Runtime diagnostic tools: lsof, /proc filesystem, ulimit\n- Non-disruptive mitigation strategies\n\n## Code Example\n```bash\n# Quick diagnostic sequence\nulimit -n                          # Current soft limit\npidof batchd                       # Process identification\nlsof -p $(pidof batchd) | wc -l    # Process FD count\ncat /proc/$(pidof batchd)/limits   # Process limits view\n```\n\n## Follow-up Questions\n- How do container runtimes affect FD limits?\n- What monitoring would you implement for early detection?\n- When would you choose ephemeral versus permanent limit changes?","diagram":"flowchart TD\n  A[FD limit check] --> B{Limit hit?}\n  B -- Yes --> C[Raise per-user limits / LimitNOFILE]\n  B -- No --> D[Check other bottlenecks]\n  C --> E[Restart service and validate]\n","difficulty":"beginner","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Goldman Sachs","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T06:13:46.746Z","createdAt":"2026-01-14T21:44:38.759Z"},{"id":"q-2273","question":"Context: A Linux server hosts a real-time ingestion service and a Redis cache on the same box. During peak ingestion, tail latency spikes. Using only default tools, outline a concrete, reproducible workflow to (1) confirm whether CPU, IO, memory, or network is the bottleneck, (2) pinpoint the offender process or disk, and (3) apply a safe mitigation with minimal disruption. Include exact commands and expected outputs?","answer":"Run: top -b -n1; vmstat 1 5; cat /proc/loadavg; if iowait >5%, inspect /proc/diskstats and ps -eo pid,pcpu,pmem,cmd --sort=-pcpu | head. If IO bound, throttle ingestion via CPU cgroups: mkdir -p /sys/","explanation":"## Why This Is Asked\nAssesses practical diagnosis under contention using only core, universally-available tools. Emphasizes observability, root-cause isolation, and safe mitigation with minimal disruption.\n\n## Key Concepts\n- Resource bottleneck identification (CPU/IO/memory/network)\n- Per-process isolation via CPU cgroups\n- Kernel tunables (swappiness) and observability via /proc\n- Safe, incremental mitigation with immediate verification\n\n## Code Example\n```\ntop -b -n1\nvmstat 1 5\ncat /proc/loadavg\n```\n```\n# Example commands shown in discussion; real outputs will vary\n```\n\n## Follow-up Questions\n- How would you scale the approach for multiple ingestion workers?\n- What non-disruptive rollback steps would you take if latency worsens?","diagram":"flowchart TD\n  A[Start monitoring] --> B{Bottleneck detected?}\n  B -- CPU --> C[Check per-process CPU with ps/top]\n  B -- IO --> D[Inspect /proc/diskstats and /proc/disk/queue]\n  C --> E[Apply CPU cgroup throttling]\n  D --> E\n  E --> F[Adjust swappiness if memory pressure]\n  F --> G[Monitor latency via logs/health endpoints]\n  G --> H[Done]","difficulty":"intermediate","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","Slack","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T09:55:39.961Z","createdAt":"2026-01-15T09:55:39.961Z"},{"id":"q-2318","question":"Scenario: A Linux host in a high-throughput UDP ingestion pipeline experiences random latency spikes under peak traffic. Using only default tooling, design a concrete diagnostic workflow to (1) determine whether NIC, IRQs, or queue saturation is the bottleneck, (2) identify the offending interface/driver and IRQ affinity, and (3) apply a safe mitigation (e.g., adjust IRQ affinity, enable RSS, increase Rx queue depth) while preserving observability. Include exact commands and expected outputs?","answer":"Run: cat /proc/interrupts; ip -s link; for ethN: ethtool -S ethN; sar -n DEV 1 1; check /proc/net/dev for drops. If IRQs spike on ethN, assign its IRQs to dedicated CPUs: echo <mask> > /proc/irq/N/smp","explanation":"## Why This Is Asked\n\nTests ability to triage network path bottlenecks at the kernel level using built-in tools. Requires correlating NIC stats, interrupts, and queue depth to locate root cause without third-party software.\n\n## Key Concepts\n\n- NIC interrupt handling and RSS\n- IRQ affinity and CPU isolation\n- Rx/Tx queue depth and latency\n\n## Code Example\n\n```javascript\n#!/bin/bash\n# collect interrupts per NIC\ngrep -E 'eth|enp' /proc/interrupts\n# show per-interface stats\nfor i in /sys/class/net/*; do echo \"$i\"; cat \"$i/statistics/rx_bytes\"; done\n```\n\n## Follow-up Questions\n\n- How would you monitor after changes to ensure no regressions?\n- How would you adapt this in a multi-NIC, containerized environment?","diagram":"flowchart TD\n  A[Peak UDP ingestion] --> B{Check /proc/interrupts}\n  B --> C[High interrupts on NIC?]\n  C -->|Yes| D[Analyze IRQ affinity & RSS]\n  C -->|No| E[Check Rx/Tx queue depth]\n  D --> F[Apply safe mitigation]\n  E --> F\n  F --> G[Verify observability: latency & drops]","difficulty":"advanced","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Goldman Sachs","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T11:44:13.285Z","createdAt":"2026-01-15T11:44:13.285Z"},{"id":"q-2353","question":"A Linux host runs three LXC containers. During peak hours, one container's API latency spikes while others stay responsive. Using only default tooling, describe a concrete, beginner-friendly diagnostic workflow to (1) determine if bottleneck is CPU, memory, or I/O at host or container level, (2) identify the container and the exact process responsible for the spike, and (3) propose a safe mitigation (e.g., adjust container limits, throttle I/O, or relocate service) with observability. Include exact commands and expected outputs?","answer":"Run ps -eo pid,pcpu,pmem,cmd --sort=-pcpu | head -n 5. For each PID, check /proc/$PID/cgroup to map to the container. Then confirm per-container usage with systemd-cgtop -m 1 and systemd-cgls. If Cont","explanation":"## Why This Is Asked\n\nAssesses ability to triage container-level resource contention using native Linux tools, mapping system metrics back to the container boundary, and choosing safe, low-downtime mitigations.\n\n## Key Concepts\n\n- cgroups and per-container isolation\n- mapping processes to containers via /proc/$pid/cgroup\n- systemd-cgtop/systemd-cgls for per-container metrics\n- safe mitigations: CPU quotas, memory limits, IO throttling\n\n## Code Example\n\n```javascript\nps -eo pid,pcpu,pmem,cmd --sort=-pcpu | head -n 5\nfor pid in $(ps -eo pid --no-headers); do\n  cat /proc/$pid/cgroup 2>/dev/null\ndone | head -n 5\nsystemd-cgtop -m 1\n```\n\n## Follow-up Questions\n\n- If containers are not managed by systemd, how would you adapt?\n- How would you validate that mitigations had the intended effect without risking service downtime?","diagram":"flowchart TD\n  Host[/Host/] --> ContainerA[Container A]\n  ContainerA --> API[API Endpoint]\n  Host --> ContainerB[Container B]\n  ContainerB --> API","difficulty":"beginner","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","OpenAI","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T14:40:30.617Z","createdAt":"2026-01-15T14:40:30.617Z"},{"id":"q-2426","question":"Context: A Linux host runs multiple containers on a single NIC behind a load balancer. During a traffic spike, a critical API shows increased latency and occasional 5xx errors. Using only default Linux tools, outline a concrete, practical diagnostic workflow to (1) determine whether latency comes from CPU, memory pressure, disk I/O, or network contention, (2) identify the exact container or process responsible, and (3) apply a safe mitigation (e.g., throttle or move to a different cgroup, adjust IO scheduler) with minimal disruption and full observability. Include exact commands and expected outputs?","answer":"I’d start by collecting signals and mapping PIDs to containers: vmstat 1 5; iostat -xz 1; pidstat -ru 1; docker stats --no-stream. Then map PIDs to containers: grep -R '/docker/' /proc/$PID/cgroup. If","explanation":"## Why This Is Asked\nTests hands-on diagnostic reasoning for multi-tenant containers using only default tools, plus safe live mitigations without downtime.\n\n## Key Concepts\n- Container-to-process mapping via /proc/$PID/cgroup\n- Resource controllers: CPUQuota, memory.max\n- Observability: vmstat, iostat, pidstat, docker stats\n- Safe throttling to preserve availability\n\n## Code Example\n```\nvmstat 1 5\niostat -xz 1\npidstat -ru 1\ndocker stats --no-stream\ngrep -R '/docker/' /proc/$PID/cgroup\n```\n\n## Follow-up Questions\n- What if no container shows high CPU but latency persists?\n- How to verify no downtime after applying throttling (e.g., gradual ramp, canary container)?","diagram":"flowchart TD\n  A[Start] --> B[Collect signals]\n  B --> C[Map PIDs to containers]\n  C --> D[Identify bottleneck]\n  D --> E[Apply throttling or memory limits]\n  E --> F[Re-run load test]\n  F --> G[Validate results]","difficulty":"intermediate","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Netflix","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T17:46:29.845Z","createdAt":"2026-01-15T17:46:29.845Z"},{"id":"q-2598","question":"On a Linux host running a high-throughput data-ingestion service, intermittent 5–10s stalls appear under sustained load. Using only default tooling, design a concrete diagnostic workflow to determine whether stalls are caused by Transparent Huge Pages, memory fragmentation, or IO pressure; identify the exact subsystem responsible; and apply a safe mitigation (e.g., disable THP, adjust NUMA binding) while preserving observability. Include exact commands and expected outputs?","answer":"Use `iostat -xz 1 3` to monitor I/O wait metrics, `pidstat -p ALL 1` to identify CPU contention patterns, and `slabtop -s object` to observe memory allocation churn. Inspect THP configuration with `cat /sys/kernel/mm/transparent_hugepage/enabled` and check kernel messages via `dmesg | grep -i hugepage` for THP-related events.","explanation":"## Why This Is Asked\nTests practical ability to diagnose kernel memory behavior and I/O contention using built-in tools, focusing on THP impact and safe mitigations.\n\n## Key Concepts\n- Transparent Huge Pages (THP)\n- Memory fragmentation vs allocation failures\n- I/O wait vs CPU contention\n\n## Code Example\n```bash\n#!/bin/bash\necho \"THP status:\"\ncat /sys/kernel/mm/transparent_hugepage/enabled\necho \"I/O wait (sample):\"\niostat -xz 1 3\n```\n\n## Follow-up Questions\n- How to persist THP disable across reboots?\n- What factors could confound THP diagnostics (defragmentation, memory pressure, NUMA)?","diagram":"flowchart TD\n  A[Stall observed] --> B[Check THP status]\n  B --> C{THP enabled?}\n  C -->|Yes| D[Check dmesg for THP events]\n  C -->|No| E[Assess IO & memory churn]\n  D --> F[If THP events align, mitigate]\n  F --> G[Validate latency post-mitigation]","difficulty":"advanced","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Databricks","IBM"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T05:04:08.066Z","createdAt":"2026-01-16T02:27:13.001Z"},{"id":"q-466","question":"You're debugging a production Linux server where processes are randomly dying with 'Out of memory' errors, but `free -m` shows 8GB available RAM. How would you diagnose and fix this issue?","answer":"Check `dmesg | grep -i oom-killer` for OOM events. Use `cat /proc/meminfo` to examine memory fragmentation. Review `overcommit_memory` and `overcommit_ratio` in `/proc/sys/vm/`. Monitor with `sar -r` ","explanation":"## Memory Management Issues\n\nLinux OOM killer activates when available memory + swap < pages_min * 4, not when RAM is fully used.\n\n## Diagnostic Steps\n\n- Check OOM killer logs: `dmesg | grep -i oom-killer`\n- Examine memory fragmentation: `cat /proc/meminfo | grep -E '(MemFree|MemAvailable|Slab|PageTables)'`\n- Monitor memory pressure: `cat /proc/pressure/memory`\n- Review overcommit settings: `cat /proc/sys/vm/overcommit_memory`\n\n## Common Causes\n\n- Memory fragmentation preventing large allocations\n- Overcommitment allowing more memory than physically available\n- Kernel memory usage (slabs, page tables) not visible in `free`\n- Memory leaks in kernel modules\n\n## Solutions\n\n```bash\n# Disable overcommit\necho 0 > /proc/sys/vm/overcommit_memory\n\n# Add swap space\nfallocate -l 2G /swapfile\nchmod 600 /swapfile\nmkswap /swapfile\nswapon /swapfile\n\n# Tune memory management\necho 65536 > /proc/sys/vm/min_free_kbytes\n```","diagram":"flowchart TD\n  A[Process Memory Request] --> B{Physical RAM Available?}\n  B -->|No| C[Check Swap]\n  B -->|Yes| D[Allocate Memory]\n  C -->|Swap Available| E[Use Swap]\n  C -->|No Swap| F{Overcommit Enabled?}\n  F -->|Yes| G[Allow Allocation]\n  F -->|No| H[OOM Killer Activates]\n  G --> I[Memory Pressure Builds]\n  I --> H\n  H --> J[Kill Process]\n  J --> K[Log to dmesg]","difficulty":"advanced","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":["oom killer","memory fragmentation","overcommit_memory","dmesg","proc/meminfo","sar","system diagnostics"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2025-12-27T04:55:16.000Z","createdAt":"2025-12-26 12:51:05"},{"id":"q-496","question":"How would you find all processes running on port 8080 and terminate them safely?","answer":"Use `lsof -i :8080` to identify processes, then `kill -15 PID` for graceful termination. If unresponsive, use `kill -9 PID`. For multiple processes: `pkill -f ':8080'` sends SIGTERM to all matching processes.","explanation":"## Process Identification\n- `lsof -i :8080` lists processes using the port\n- `netstat -tulpn | grep :8080` alternative method\n\n## Safe Termination\n- `kill -15 PID` (SIGTERM) allows graceful shutdown\n- `kill -9 PID` (SIGKILL) forces immediate termination\n\n## Batch Operations\n- `pkill -f ':8080'` terminates all matching processes\n- `fuser -k 8080/tcp` kills processes by port","diagram":"flowchart TD\n  A[Identify Port Usage] --> B[lsof -i :8080]\n  B --> C{Process Responding?}\n  C -->|Yes| D[kill -15 PID]\n  C -->|No| E[kill -9 PID]\n  D --> F[Verify Termination]\n  E --> F","difficulty":"beginner","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-08T11:59:14.782Z","createdAt":"2025-12-26 12:51:06"},{"id":"q-527","question":"How would you find and kill a process that's using port 8080 on a Linux system?","answer":"Use `lsof -i :8080` or `netstat -tulpn | grep 8080` to find the PID, then `kill -9 <PID>` to terminate it. For a safer approach, try `kill -15 <PID>` first to allow graceful shutdown.","explanation":"## Finding the Process\n- `lsof -i :8080` lists processes using that port\n- `netstat -tulpn` shows all listening processes with PIDs\n- `ss -tulpn | grep 8080` is a modern alternative\n\n## Terminating the Process\n- `kill -15 <PID>` sends SIGTERM for graceful shutdown\n- `kill -9 <PID>` sends SIGKILL for force termination\n- `pkill -f \"process_name\"` kills by name pattern\n\n## Verification\n- `lsof -i :8080` confirms process is killed\n- `netstat -tulpn | grep 8080` should return empty","diagram":"flowchart TD\n  A[Port 8080 Issue] --> B[lsof -i :8080]\n  B --> C[Get PID]\n  C --> D[kill -15 PID]\n  D --> E{Process Killed?}\n  E -->|No| F[kill -9 PID]\n  E -->|Yes| G[Verify with lsof]","difficulty":"beginner","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Oracle","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-25T15:01:25.772Z","createdAt":"2025-12-26 12:51:06"},{"id":"q-553","question":"You're troubleshooting a production server where a critical process keeps getting killed. How would you diagnose if it's an OOM kill versus other issues, and what specific commands would you use to investigate?","answer":"Check dmesg for 'Out of memory' messages and /var/log/messages. Use `free -h` to see memory usage, `ps aux --sort=-%mem` to find memory hogs, and `cat /proc/<pid>/status` for VmRSS. If OOM, check `/proc/sys/vm/panic_on_oom` and `/proc/sys/vm/oom_kill_allocating_task` to understand kernel behavior.","explanation":"## Diagnosis Steps\n- Check system logs for OOM kill indicators\n- Analyze current memory usage patterns\n- Identify memory-intensive processes\n\n## Key Commands\n```bash\ndmesg | grep -i oom\nfree -h\nps aux --sort=-%mem | head -10\ncat /proc/$(pidof process)/status | grep Vm\n```\n\n## Prevention\n- Monitor memory usage with `top`/`htop`\n- Set up alerts for high memory usage\n- Configure appropriate swap space\n- Tune kernel parameters like `vm.swappiness`","diagram":"flowchart TD\n  A[Process Killed] --> B{Check dmesg}\n  B -->|OOM messages| C[Analyze memory usage]\n  B -->|No OOM| D[Check other signals]\n  C --> E[Identify memory hogs]\n  E --> F[Tune kernel params]\n  D --> G[Check logs/crash dumps]","difficulty":"intermediate","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Scale Ai","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-08T11:55:47.944Z","createdAt":"2025-12-26 12:51:07"},{"id":"q-580","question":"How would you find all processes using a specific port and terminate one safely?","answer":"Use `lsof -i :8080` to list processes on port 8080, then `kill -15 <PID>` for graceful shutdown. If unresponsive, use `kill -9 <PID>` as last resort. Always check process dependencies first.","explanation":"## Finding Processes\n- `lsof -i :<port>` lists processes using the port\n- `netstat -tulpn | grep :<port>` alternative method\n\n## Safe Termination\n- `kill -15 <PID>` sends SIGTERM for graceful shutdown\n- Allows process to cleanup resources and save state\n- Wait reasonable time before escalating\n\n## Force Termination\n- `kill -9 <PID>` sends SIGKILL as last resort\n- Immediate termination without cleanup\n- Can cause data corruption or resource leaks","diagram":"flowchart TD\n  A[Identify Port] --> B[lsof -i :port]\n  B --> C[List PIDs]\n  C --> D{Process Responsive?}\n  D -->|Yes| E[kill -15 PID]\n  D -->|No| F[kill -9 PID]\n  E --> G[Monitor Shutdown]\n  F --> H[Force Terminate]","difficulty":"beginner","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","MongoDB","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":["lsof","process termination","graceful shutdown","pid","port","dependencies","kill command"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2025-12-27T04:58:08.288Z","createdAt":"2025-12-27T01:13:40.807Z"},{"id":"q-917","question":"Scenario: a Linux server hosting a web app experiences sporadic high response times during peak hours. Using only default tools and no downtime, describe a concrete, beginner-friendly diagnostic workflow to (1) determine whether CPU, memory, or I/O is the bottleneck, (2) identify the offending process, and (3) apply a safe mitigation (e.g., graceful restart) while monitoring impact. Include exact commands and expected outputs?","answer":"Run top in batch to spot heavy CPU: top -b -n1; then dump top processes: ps -eo pid,comm,pcpu,pmem --sort=-pcpu | head -5. Check IO with iostat -dx 1 2 and memory with free -m. If a process hogs, grac","explanation":"## Why This Is Asked\n\\nTests practical, beginner-friendly triage in production-like Linux environments, focusing on real commands and safe mitigations without downtime.\\n\\n## Key Concepts\n- Basic bottleneck triage (CPU, memory, I/O)\\n- Identifying culprits with top and ps\\n- Verifying I/O and memory health with iostat and free\\n- Safe mitigation via graceful restart and live monitoring\\n\\n## Code Example\n```javascript\n// Diagnostics snippet (commands)\ntop -b -n1 | head -20\nps -eo pid,comm,pcpu,pmem --sort=-pcpu | head -5\niostat -dx 1 2\nfree -m\njournalctl -u service -n 100 --no-pager\n```\n\n## Follow-up Questions\n- How would you automate this workflow for nightly checks?\\n- What differences would you expect between CPU-bound and I/O-bound symptoms?","diagram":null,"difficulty":"beginner","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Microsoft","PayPal","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T15:30:13.738Z","createdAt":"2026-01-12T15:30:13.738Z"}],"subChannels":["commands","general"],"companies":["Adobe","Airbnb","Amazon","Anthropic","Apple","Citadel","Cloudflare","Coinbase","Databricks","DoorDash","Goldman Sachs","Google","Hashicorp","Hugging Face","IBM","Instacart","LinkedIn","Lyft","Microsoft","MongoDB","NVIDIA","Netflix","OpenAI","Oracle","PayPal","Salesforce","Scale Ai","Slack","Snap","Snowflake","Square","Tesla","Twitter","Two Sigma","Uber"],"stats":{"total":25,"beginner":12,"intermediate":8,"advanced":5,"newThisWeek":19}}