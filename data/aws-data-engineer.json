{"questions":[{"id":"aws-data-engineer-data-ingestion-1768148517041-0","question":"You are designing a data ingestion pipeline for streaming IoT telemetry. The system must ingest up to 15,000 events per second, apply on-the-fly transformations to standardize the payloads, and store the results in S3 in Parquet format for Athena queries. The input schema evolves over time with new fields, and you want minimal maintenance when fields are added. Which architecture best meets these requirements?","answer":"[{\"id\":\"a\",\"text\":\"Store raw JSON in S3 and run a nightly EMR job to parse and convert to Parquet\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Ingest with Kinesis Data Streams, transform with AWS Glue Streaming ETL, and write Parquet to S3; use Glue Data Catalog for schema evolution\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Ingest with SQS, process with Lambda, and store as CSV in S3\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use a custom on-prem ETL tool to batch process data daily\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct because it uses a high-throughput streaming path (Kinesis Data Streams) with streaming ETL (Glue Streaming ETL) to perform on-the-fly transformations and write Parquet to S3 for Athena. It also leverages Glue Data Catalog for schema evolution, allowing new fields to be incorporated without breaking existing pipelines. The other options rely on batch processing (A), or consume events with SQS + Lambda in a manner that doesn't scale to 15k events/sec or provide robust schema evolution (C), or depend on an on-prem batch tool with no streaming capability (D).\n\n## Why Other Options Are Wrong\n- Option A: Batch processing cannot satisfy near real-time ingestion and lacks incremental transformation.\n- Option C: SQS + Lambda is not designed for such high event throughput and lacks built-in schema evolution support.\n- Option D: On-prem batch processing introduces latency and maintenance burden not aligned with cloud-native scalability.\n\n## Key Concepts\n- Streaming ingestion with Kinesis Data Streams\n- AWS Glue Streaming ETL or Glue pipelines\n- Parquet format and Athena compatibility\n- Glue Data Catalog for schema evolution\n\n## Real-World Application\n- This pattern is used for real-time analytics on IoT telemetry, enabling fast queries on clean Parquet data without rebuilding pipelines on schema changes.","diagram":null,"difficulty":"intermediate","tags":["AWS","Kinesis","Glue","S3","Parquet","Athena","DataIngestion","Streaming","certification-mcq","domain-weight-34"],"channel":"aws-data-engineer","subChannel":"data-ingestion","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T16:21:57.044Z","createdAt":"2026-01-11 16:21:57"},{"id":"aws-data-engineer-data-ingestion-1768148517041-1","question":"You have multiple teams ingesting JSON logs into S3 with evolving fields. You want to query with Athena without breaking existing pipelines, and you want to support schema evolution. Which approach is best?","answer":"[{\"id\":\"a\",\"text\":\"Enforce a fixed schema by rewriting all files to a single schema upon ingestion\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use AWS Glue Schema Registry to maintain schema evolution and use Glue Tables with dynamic frames\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Convert all data to CSV before storing in S3\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Ignore schema evolution and adjust queries to handle optional fields\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct because AWS Glue Schema Registry provides centralized schema governance that supports evolution across multiple producers. When combined with Glue Tables and dynamic frames, downstream queries (e.g., Athena) can adapt to changing fields without breaking pipelines.\n\n## Why Other Options Are Wrong\n- Option A: Forcing a fixed schema increases maintenance, causes churn with new fields, and breaks multi-team ingestion.\n- Option C: CSV reduces schema flexibility and complicates semi-structured JSON handling in Athena.\n- Option D: Ignoring evolution leads to fractured schemas and failed queries.\n\n## Key Concepts\n- AWS Glue Schema Registry\n- Glue Data Catalog / Tables\n- Schema evolution in data lakes\n\n## Real-World Application\n- Enables multiple teams to contribute JSON logs with evolving schemas while keeping Athena queries stable and future-proof.","diagram":null,"difficulty":"intermediate","tags":["AWS","Glue","SchemaRegistry","Athena","S3","DataIngestion","DataCatalog","certification-mcq","domain-weight-34"],"channel":"aws-data-engineer","subChannel":"data-ingestion","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T16:21:57.390Z","createdAt":"2026-01-11 16:21:57"},{"id":"aws-data-engineer-data-ingestion-1768148517041-2","question":"You operate an on-prem Oracle database and a Redshift data warehouse. You need near real-time changes to reflect in Redshift and preserve historical rows (SCD Type 2). Which approach best accomplishes this with minimal custom coding?","answer":"[{\"id\":\"a\",\"text\":\"Configure an AWS DMS CDC task to Redshift and implement MERGE statements in Redshift to perform SCD Type 2\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Configure DMS CDC to S3 and run a Glue job to merge into Redshift\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Build a custom log-based replication service on-prem and push changes to Redshift\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Schedule hourly full table dumps from Oracle to Redshift\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A is correct because AWS DMS can capture CDC from an on-prem Oracle source and replicate to Redshift with low latency. Implementing MERGE statements in Redshift enables SCD Type 2 to preserve history with minimal custom code beyond the MERGE logic.\n\n## Why Other Options Are Wrong\n- Option B: Adds an extra staging step (S3) and delayed merging, increasing latency and complexity.\n- Option C: Requires building a custom solution with significant maintenance.\n- Option D: Full dumps cause high latency and data staleness, not near real-time.\n\n## Key Concepts\n- AWS DMS CDC for Oracle to Redshift\n- Redshift MERGE for SCD Type 2\n- Change data capture patterns\n\n## Real-World Application\n- Keeps dimensional history accurate in a data warehouse while minimizing custom ETL development.","diagram":null,"difficulty":"intermediate","tags":["AWS","DMS","Redshift","CDC","SCD","DataIngestion","Oracle","certification-mcq","domain-weight-34"],"channel":"aws-data-engineer","subChannel":"data-ingestion","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T16:21:57.734Z","createdAt":"2026-01-11 16:21:57"},{"id":"aws-data-engineer-data-operations-1768216823632-0","question":"You have an analytics data lake on S3 using daily partitions (year, month, day) registered in the AWS Glue Data Catalog and queried via Athena. The partition count has grown to tens of thousands, causing long crawler runs and slow query planning. Which design approach reduces maintenance while preserving fast partition pruning?","answer":"[{\"id\":\"a\",\"text\":\"Implement partition projection on the Glue table for year, month, and day so Athena can prune partitions without listing all partitions.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Reorganize data into a single table with no partitions and rely on file-level filters.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Switch to monthly partitions and keep the same table metadata.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Keep the current partitioning and increase crawler frequency to refresh partitions more often.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct choice is A because partition projection lets Athena prune partitions without enumerating all partitions, reducing maintenance and query planning cost. B would still require manual partition handling; C eliminates partitioning and sacrifices pruning; D increases costs and does not address pruning efficiency.\n\n## Why Other Options Are Wrong\n- B: Moving to monthly partitions reduces count but still requires partition discovery and misses fine-grained pruning.\n- C: A non-partitioned table degrades performance for time-bounded filters.\n- D: Frequent crawls add cost and do not address pruning efficiency.\n\n## Key Concepts\n- AWS Glue Data Catalog\n- Athena partition pruning and partition projection\n\n## Real-World Application\nUsed to scale data lake catalogs with high partition counts while preserving fast ad-hoc query performance.","diagram":null,"difficulty":"intermediate","tags":["AWS","S3","Athena","Glue","PartitionProjection","certification-mcq","domain-weight-22"],"channel":"aws-data-engineer","subChannel":"data-operations","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T11:20:23.634Z","createdAt":"2026-01-12 11:20:23"},{"id":"aws-data-engineer-data-operations-1768216823632-1","question":"A Glue ETL job ingests data from a MySQL RDS source and writes Parquet to S3 before loading into Redshift. A new column is added in the source and occasionally causes a failure due to a schema mismatch. Which approach best ensures the job remains resilient to new columns without failing?","answer":"[{\"id\":\"a\",\"text\":\"Use AWS Glue DynamicFrame with ResolveChoice and ApplyMapping to map to the target schema and ignore extra columns via DropFields.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Hard-code the target schema to include the new column to catch up automatically.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Disable schema checks entirely and always write as string type.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Switch to a different ETL tool that ignores schema drift.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A is correct because Glue DynamicFrame and ResolveChoice allow the pipeline to adapt to drift and map to the stable target schema while ignoring unexpected new columns, preventing failures. B locks the target schema to a new field, causing more frequent changes and potential failures; C sacrifices data typing and can lead to data quality issues; D avoids Glue's native, scalable handling of schema drift.\n\n## Why Other Options Are Wrong\n- B: Forcing the new column into the target schema requires schema changes and redeploys, causing failures.\n- C: Dropping type safety leads to downstream ambiguity and issues.\n- D: Introducing another tool adds complexity and may not integrate with existing catalog and load steps.\n\n## Key Concepts\n- AWS Glue DynamicFrame\n- ResolveChoice, ApplyMapping, DropFields\n- Schema drift handling in ETL pipelines\n\n## Real-World Application\nKeeps data pipelines resilient to evolving source schemas with minimal downtime and manual intervention.","diagram":null,"difficulty":"intermediate","tags":["AWS","Glue","Parquet","Redshift","SchemaDrift","certification-mcq","domain-weight-22"],"channel":"aws-data-engineer","subChannel":"data-operations","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T11:20:23.993Z","createdAt":"2026-01-12 11:20:24"},{"id":"aws-data-engineer-data-operations-1768216823632-2","question":"In a multi-environment AWS data lake deployed with Terraform, you need dev/stage/prod with isolated infrastructure states. Which practice best prevents cross-environment drift and accidental changes?","answer":"[{\"id\":\"a\",\"text\":\"Maintain separate remote state per environment, using distinct backends and state files.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Use a single backend and rely on a single state for all environments.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Only rely on Git branches; state remains in local files.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Manually reproduce resources in each environment without state management.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A is correct because maintaining separate remote states per environment (or distinct backends) prevents drift and unintentional cross-environment changes, allowing safe, repeatable deployments. B, C, and D either merge states or bypass state management, increasing risk of drift and misconfiguration.\n\n## Why Other Options Are Wrong\n- B: A single shared state risks cross-environment contamination.\n- C: Local state is not shareable or reproducible across teammates or CI.\n- D: Manual reproduction increases inconsistency and drift.\n\n## Key Concepts\n- Terraform state management\n- Remote backends (S3, Terraform Cloud/Enterprise)\n- Environment isolation\n\n## Real-World Application\nEnsures predictable provisioning across dev/stage/prod for data lake components like S3, Glue, and Redshift.","diagram":null,"difficulty":"intermediate","tags":["Terraform","AWS","S3","Glue","Redshift","Kubernetes","certification-mcq","domain-weight-22"],"channel":"aws-data-engineer","subChannel":"data-operations","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T11:20:24.344Z","createdAt":"2026-01-12 11:20:24"},{"id":"aws-data-engineer-data-store-management-1768189639780-0","question":"A data lake on Amazon S3 stores hundreds of terabytes of unstructured data and logs. You need cost-effective long-term retention for analytics with Athena while keeping hot data quickly accessible for recent queries. Which approach best balances cost and query performance?","answer":"[{\"id\":\"a\",\"text\":\"Store all data in S3 Standard and avoid any lifecycle transitions\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Enable S3 Intelligent-Tiering for the entire dataset to automatically move items between tiers\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Configure S3 Lifecycle policies to transition hot data to S3 Standard/IA and cold data to Glacier Deep Archive, enabling cost-effective retention with Athena\",\"isCorrect\":true},{\"id\":\"d\",\"text\":\"Migrate data to EFS and run analytics directly from there\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption C. Implementing S3 Lifecycle policies to move hot data to more accessible tiers (Standard/IA) and cold data to Glacier Deep Archive provides the best balance of cost and analytics performance for Athena.\n\n## Why Other Options Are Wrong\n- A: Keeps all data in Standard storage, leading to higher long-term costs for archival data.\n- B: Intelligent-Tiering helps with access-pattern optimization but is not as cost-effective for long-term archival as tiered lifecycle with Glacier Deep Archive.\n- D: Migrating to EFS is inappropriate for petabyte-scale analytics and adds unnecessary complexity and cost.\n\n## Key Concepts\n- S3 storage classes and lifecycle policies\n- Cost optimization for data lakes with Athena\n- Glacier Deep Archive for long-term retention\n\n## Real-World Application\nUsed to retain decades of raw logs for compliance while preserving fast access to recent data for ad-hoc analytics.","diagram":null,"difficulty":"intermediate","tags":["S3","Glacier","Athena","Data-Lake","certification-mcq","domain-weight-26"],"channel":"aws-data-engineer","subChannel":"data-store-management","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T03:47:19.783Z","createdAt":"2026-01-12 03:47:20"},{"id":"aws-data-engineer-data-store-management-1768189639780-1","question":"An IoT telemetry pipeline generates high-volume time-series events with intermittent bursts. You want low-latency reads for dashboards and scalable storage for long-term analytics. Which storage configuration best achieves cost-efficiency, scalability, and near-real-time access?","answer":"[{\"id\":\"a\",\"text\":\"Store events in Amazon S3 with partitioned Parquet files and query via Athena\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Store events in DynamoDB with on-demand capacity, a composite key (deviceId as partition key, timestamp as sort key), TTL on aging data, and an optional GSI on timestamp for range queries\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Use Amazon RDS with a time-series schema and daily snapshotting\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Push data into Redshift and perform all queries there in real time\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B. DynamoDB with on-demand capacity provides virtually unlimited scalability and low-latency reads/writes, and modeling time-series data with a composite (partition, sort) key supports efficient range queries. TTL helps manage aging data.\n\n## Why Other Options Are Wrong\n- A: S3 + Athena is cost-effective for batch analytics but not optimal for near real-time dashboards due to higher-latency access and batch-oriented queries.\n- C: RDS is not ideal for high-velocity time-series data at scale and may incur higher costs and management overhead.\n- D: Redshift is optimized for analytical workloads, not real-time ingestion and low-latency point reads for dashboards.\n\n## Key Concepts\n- DynamoDB on-demand capacity and time-series data modeling\n- Composite primary keys for time-based queries\n- TTL for data lifecycle\n\n## Real-World Application\nIdeal for IoT dashboards requiring fast access to recent data while keeping historical data cost-effectively stored for analytics.","diagram":null,"difficulty":"intermediate","tags":["DynamoDB","IoT","Time-Series","TTL","certification-mcq","domain-weight-26"],"channel":"aws-data-engineer","subChannel":"data-store-management","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T03:47:20.199Z","createdAt":"2026-01-12 03:47:20"},{"id":"aws-data-engineer-data-store-management-1768189639780-2","question":"A regulated data store uses S3 to provide input to Redshift for analytics. You must ensure encryption at rest with strong key management and auditable key usage. Which approach provides best security and manageability?","answer":"[{\"id\":\"a\",\"text\":\"Use SSE-S3 with AWS managed keys for all objects\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use SSE-KMS with a customer-managed AWS KMS key and implement a bucket policy requiring encryption for all objects\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Encrypt data client-side before uploading and keep keys in an on-premises vault\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Rely on Redshift encryption only and skip S3-level encryption\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B. SSE-KMS with a customer-managed KMS key provides strong, auditable key management, and a bucket policy enforcing encryption ensures all data at rest is protected consistently.\n\n## Why Other Options Are Wrong\n- A: SSE-S3 uses AWS managed keys, offering weaker control and auditing capabilities than customer-managed keys.\n- C: Client-side encryption shifts encryption responsibility to clients and can complicate key management and access control.\n- D: Redshift encryption protects data at rest in Redshift, but S3 objects feeding Redshift require separate encryption controls; relying solely on Redshift misses S3-level protection and auditing.\n\n## Key Concepts\n- SSE-KMS with customer-managed keys\n- Bucket policies enforcing encryption\n- Auditing key usage with KMS IAM policies and CloudTrail\n\n## Real-World Application\nEnsures regulatory compliance with auditable key usage and centralized control over encryption keys for data ingested from S3 into analytic warehouses.","diagram":null,"difficulty":"intermediate","tags":["S3","KMS","SSE-KMS","Redshift","Compliance","certification-mcq","domain-weight-26"],"channel":"aws-data-engineer","subChannel":"data-store-management","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T03:47:20.637Z","createdAt":"2026-01-12 03:47:20"}],"subChannels":["data-ingestion","data-operations","data-store-management"],"companies":[],"stats":{"total":9,"beginner":0,"intermediate":9,"advanced":0,"newThisWeek":9}}