{"questions":[{"id":"q-1336","question":"In a multi-account AWS data lake used by a global product analytics team, ingest semi-structured data from several SaaS APIs into S3, enforce schema evolution and data quality checks, and expose curated tables with time-travel queries. Describe the end-to-end design including data formats, upsert strategy, and governance controls you would apply using AWS services?","answer":"Leverage AWS Glue Data Catalog with Lake Formation, store data in S3 using Apache Iceberg for upserts and time travel, orchestrate with Step Functions, transform in Glue Spark jobs, validate quality v","explanation":"## Why This Is Asked\nTests ability to design multi-account governance, schema evolution, upserts, and cross-service orchestration for a scalable data lake.\n\n## Key Concepts\n- Multi-account governance with Lake Formation\n- Apache Iceberg on S3 for upserts and time travel\n- Glue Data Catalog as central metadata store\n- Data quality tooling (Glue Data Quality or DataBrew)\n- Orchestration with Step Functions\n- Access controls and encryption\n\n## Code Example\n```javascript\n// Spark SQL pseudo: upsert into Iceberg table using MERGE\nspark.sql(\"MERGE INTO lake.product USING staging.product AS s ON t.id = s.id WHEN MATCHED THEN UPDATE SET ... WHEN NOT MATCHED THEN INSERT...\")\n```\n\n## Follow-up Questions\n- How would you handle schema drift across SaaS sources with Iceberg?\n- How would you enforce time-travel/query lineage access controls for data consumers?","diagram":null,"difficulty":"advanced","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Apple"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T11:43:06.203Z","createdAt":"2026-01-13T11:43:06.203Z"},{"id":"q-1419","question":"Design an advanced, multi-region data ingestion and governance pipeline for a global fintech platform. Data from partners arrives as JSON and Parquet; must enforce per-tenant isolation, support near real-time analytics, and GDPR erasure. Outline architecture using AWS Kinesis (streams), DMS for CDC, Glue (ETL), Iceberg on S3 for schema-evolving tables, Lake Formation/IAM for access, and Athena/Redshift Spectrum for queries. Include format choices, CDC vs batch mix, lineage, and failure modes?","answer":"Use Iceberg-on-S3 for per-tenant, time-travel capable tables; stream data through Kinesis and batch via Glue; CDC via DMS from partners; catalog in Glue Data Catalog; enforce isolation with Lake Forma","explanation":"## Why This Is Asked\nTests ability to design end-to-end data governance at scale across regions and tenants.\n\n## Key Concepts\n- Multi-region data lake with Iceberg on S3\n- Per-tenant isolation with Lake Formation\n- Streaming + batch ingestion\n- Data lineage and GDPR erasure\n- Schema evolution and time travel\n\n## Code Example\n```javascript\n// PySpark-like example writing to Iceberg\nconst df = spark.read.format(\"json\").load(\"s3://bucket/partner/*.json\");\ndf.write.format(\"iceberg\").mode(\"append\").save(\"db.tenants.events\");\n```\n\n## Follow-up Questions\n- How would you test data erasure requests end-to-end?\n- What metrics alerting would you implement for cross-region replication lag?","diagram":"flowchart TD\n  A[Partner Sources] --> B[Ingest: Kinesis/Batch]\n  B --> C[Glue ETL]\n  C --> D[Iceberg Tables on S3]\n  D --> E[Queries: Athena/Redshift]\n  D --> F[Access: Lake Formation]\n  F --> G[Lineage & Audits]\n  D --> H[Region Replication (S3)]","difficulty":"advanced","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T16:42:34.969Z","createdAt":"2026-01-13T16:42:34.969Z"},{"id":"q-1487","question":"You operate an AWS data lake where streaming user activity flows from Kinesis Data Streams into S3 via Firehose, with Glue Data Catalog, Athena queries, and dashboards. Schemas evolve and data quality is mission-critical. Design a real-time data quality and drift detection framework: schema drift, per-record quality checks, quarantine failing files, alert owners, and preserve cross-account auditability?","answer":"Leverage Glue Schema Registry for evolution, and real-time validators in Kinesis Data Analytics or Lambda to enforce rules (non-null IDs, valid timestamps, sane ranges) and tag bad records. Quarantine","explanation":"## Why This Is Asked\nAssesses ability to design real-time data quality and drift detection at scale, balancing schema evolution, validation, quarantine, alerting, and cross-account governance using AWS services.\n\n## Key Concepts\n- Schema evolution and registry management\n- Real-time validation and data quarantining\n- Observability: CloudWatch metrics and SNS alerts\n- Cross-account auditability via Glue Data Catalog and Lake Formation\n\n## Code Example\n```javascript\n// Pseudo validator for a streaming record\nfunction isRecordValid(r){\n  if(!r.user_id || !r.timestamp) return false;\n  if(typeof r.value !== 'number' || r.value < 0) return false;\n  return true;\n}\n```\n\n## Follow-up Questions\n- How would you test drift detection across regions and accounts?\n- What are trade-offs of using a separate quarantine bucket vs inline rejection for analytics workloads?\n","diagram":"flowchart TD\n  A[Kinesis] --> B[Firehose]\n  B --> C[S3 + Glue Catalog]\n  C --> D[Athena]\n  D --> E[Alerts/Owners]","difficulty":"advanced","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Coinbase","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T18:59:52.005Z","createdAt":"2026-01-13T18:59:52.005Z"},{"id":"q-1506","question":"Scenario: A partner API provides daily batches of image assets with evolving metadata. Build an end-to-end ingest to a multi-region data lake: landing in S3, metadata in a versioned, upsertable table, idempotent processing, schema evolution, and governance across accounts. Describe data formats, partitioning, dedupe strategy, and cross-account orchestration?","answer":"Ingest images via multipart S3 uploads behind a deduplicated manifest; store metadata in a versioned, upsertable Iceberg table on S3 (via Glue/Spark) keyed by asset_id + batch_id; handle schema evolut","explanation":"## Why This Is Asked\nThis question tests practical design for a cross-region data lake with large binary assets, evolving metadata, and strict governance. It probes hands-on trade-offs and tooling choices that show depth beyond basic ingestion.\n\n## Key Concepts\n- Ingesting large assets with multipart uploads and manifests\n- Idempotent upserts using a composite key (asset_id + batch_id)\n- Schema evolution supported by Iceberg/Delta on S3\n- Cross-account governance via Lake Formation and Glue Data Catalog\n- Multi-region orchestration with Step Functions\n\n## Code Example\n```javascript\n// Example: upsert metadata into a versioned Iceberg-like table (pseudo)\nasync function upsertMetadata(client, rec) {\n  const current = await client.read(rec.asset_id);\n  const merged = { ...current, ...rec };\n  await client.write(rec.asset_id, merged);\n}\n```\n\n## Follow-up Questions\n- How would you handle API rate limits and retries?\n- How would you validate data quality, schema drift, and auditability in production?","diagram":null,"difficulty":"intermediate","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","Slack","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T19:43:56.091Z","createdAt":"2026-01-13T19:43:56.091Z"},{"id":"q-1521","question":"You manage a global, multi-tenant data platform where streaming events bucket into S3 and downstream engines like Redshift and Athena rely on per-tenant schemas. Design a resilient end-to-end architecture that versions datasets, enforces per-tenant schema contracts, detects drift, quarantines bad records, and supports rollback without downtime. Specify services, data formats, governance, and observability?","answer":"Propose using Firehose/Kinesis for streaming into S3 with per-tenant prefixes; Glue Schema Registry to enforce per-tenant contracts; Iceberg on S3 for versioned tables; Lake Formation for tenant isola","explanation":"## Why This Is Asked\nTests multi-tenant data governance, schema contracts, drift detection, and rollback—critical for scalable data products.\n\n## Key Concepts\n- Multi-tenant governance with Glue Schema Registry and Lake Formation.\n- Versioned storage with Iceberg on S3.\n- Drift detection and quality checks via Glue jobs and Deequ.\n- Quarantine workflows and rollback with Step Functions.\n- Observability via CloudWatch and Athena-based lineage.\n\n## Code Example\n```javascript\n// Placeholder for a data quality check wiring example\n```\n\n## Follow-up Questions\n- How would you handle schema evolution while preserving historic queries?\n- What metrics and alerts signal a drift or ingestion failure?","diagram":"flowchart TD\n  Ingest[Kinesis/Firehose] --> Stage[S3 (tenant prefixes)]\n  Stage --> Catalog[Glue Catalog & Iceberg Tables]\n  Catalog --> Validate[Deequ drift checks]\n  Validate --> Quarantine[Quarantine bucket]\n  Quarantine --> Orchestrator[Step Functions]\n  Orchestrator --> Observability[CloudWatch]","difficulty":"advanced","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Google","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T20:39:24.546Z","createdAt":"2026-01-13T20:39:24.546Z"},{"id":"q-1543","question":"Design an event-driven data platform to ingest order and driver events from regional services into a centralized data lake for near-real-time analytics. Data arrives as evolving JSON schemas; must handle schema evolution, efficient partitioning, cross-account access, and cost. Which AWS services and data design patterns would you implement, and how would you validate data quality and enable fast ad-hoc queries?","answer":"Use Kinesis Data Streams for event ingestion, a Glue streaming job with Glue Schema Registry to normalize evolving JSON schemas, and write Parquet to S3 partitioned by date/region/service. Employ Lake Formation for fine-grained cross-account access, implement data quality validation with Glue job assertions and Deequ checks, and enable fast ad-hoc queries through Athena with optimized partitioning and columnar formats. Apply cost controls through file compaction, lifecycle policies, and intelligent tiering.","explanation":"## Why This Is Asked\n\nExamines real-time data ingestion, schema evolution, cross-account governance, and cost-aware storage patterns in a practical, production context.\n\n## Key Concepts\n\n- Streaming ingestion: Kinesis Data Streams + Glue Schema Registry\n- Real-time normalization: Glue Streaming or Kinesis Data Analytics\n- Storage design: Parquet on S3 with partitioning by date/region/service\n- Governance: Lake Formation fine-grained access, Glue Data Catalog, drift monitoring\n- Quality & cost: deduplication, compaction, managed small files\n\n## Code Example\n\n```python\n# Glue Spark job snippet for schema evolution and data quality\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.types import StructType\n\n# Initialize Glue context\nargs = getResolvedOptions(__SYS__, ['JOB_NAME'])\nsc = SparkContext()\nglue_context = GlueContext(sc)\nspark = glue_context.spark_session\n\n# Read from Kinesis with schema registry\ndynamic_frame = glue_context.create_dynamic_frame.from_options(\n    connection_type=\"kinesis\",\n    connection_options={\n        \"streamARN\": \"arn:aws:kinesis:region:account:stream/order-events\",\n        \"classification\": \"json\",\n        \"endpointUrl\": \"https://kinesis.region.amazonaws.com\"\n    },\n    transformation_ctx=\"kinesis_source\"\n)\n\n# Apply schema evolution and data quality checks\ndf = dynamic_frame.toDF()\n\n# Data quality validation\ndf = df.filter(\n    F.col(\"order_id\").isNotNull() &\n    F.col(\"timestamp\").isNotNull() &\n    F.col(\"region\").isNotNull()\n)\n\n# Write optimized Parquet to S3\ndf.write.mode(\"append\").partitionBy(\n    \"date\", \"region\", \"service_type\"\n).parquet(\"s3://data-lake/orders/\")\n```","diagram":"flowchart TD\n  A[Ingest: Kinesis Data Streams] --> B[Normalize: Glue Schema Registry & Glue Streaming]\n  B --> C[Store: S3 Parquet with partitions: date/region/service]\n  C --> D[Catalog: Glue Data Catalog + Lake Formation policies]\n  D --> E[Query: Athena / Redshift Spectrum]\n  E --> F[Quality & Monitoring: Glue jobs + CloudWatch]","difficulty":"intermediate","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T06:31:51.252Z","createdAt":"2026-01-13T21:31:34.885Z"},{"id":"q-1608","question":"Scenario: A new source streams JSON events from a mobile app via Kinesis Data Streams. Design a beginner-level ingestion pipeline to land data in S3 as Parquet with daily partitions, using AWS Glue for ETL and cataloging. Include services, data formats, schema evolution approach, basic data quality checks, and how you would test end-to-end before analytics?","answer":"Design a streaming ingestion pipeline using Kinesis Data Streams to capture JSON events from the mobile app, with AWS Glue ETL jobs that process the data, convert it to Parquet format, and write to S3 with daily date partitions. The Glue Data Catalog will register the resulting tables for analytics. Implement simple schema evolution by allowing new optional fields while maintaining backward compatibility, and include basic data quality checks for required fields, data types, and null values.","explanation":"## Why This Is Asked\nEvaluates practical, beginner-appropriate streaming ingestion using common AWS data tooling and basic quality controls.\n\n## Key Concepts\n- Kinesis Data Streams for real-time data ingestion\n- AWS Glue ETL and Data Catalog for data processing and metadata management\n- Parquet format with date-based partitioning for optimized storage and queries\n- Simple schema evolution strategies for handling changing data structures\n- Data quality checks (required fields, types, nulls)\n- CloudWatch alarms and end-to-end testing\n\n## Code Example\n```javascript\n// Pseudo-code to start a Glue job\nconst glue = new AWS.Glue();\nawait glue.startJobRun({\n  JobName: 'mobile-events-etl',\n  Arguments: {\n    '--input-stream': 'mobile-app-events',\n    '--output-path': 's3://analytics-bucket/events/',\n    '--partition-format': 'yyyy/MM/dd'\n  }\n});\n```","diagram":null,"difficulty":"beginner","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["NVIDIA","Netflix","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T05:30:45.364Z","createdAt":"2026-01-14T02:34:42.960Z"},{"id":"q-1631","question":"You receive a daily nested JSON file and a separate CSV in an S3 bucket. Design a beginner-level ingestion using AWS Glue Studio to flatten the JSON, normalize types, parse the CSV, and write Parquet under a date-partitioned path s3://data-lake/events/date=YYYY-MM-DD/. Register a Glue catalog table, outline simple schema-drift handling, and include basic data-quality checks plus a quick end-to-end test plan with sample files?","answer":"Describe a beginner-level ingestion using AWS Glue Studio for two daily files in S3: a nested JSON and a CSV. Build a Glue job to flatten JSON, normalize types, parse CSV, and write Parquet to s3://da","explanation":"## Why This Is Asked\nBeginners must design practical ETL workflows that handle mixed formats and small drift, using Glue Studio in repeatable ways.\n\n## Key Concepts\n- Glue Studio ETL\n- JSON flattening and CSV parsing\n- Parquet partitioning by date\n- Glue Data Catalog table\n- Basic data quality checks and schema drift\n\n## Code Example\n```javascript\nfunction flatten(obj, prefix = '', res = {}) {\n  for (const k in obj) {\n    const v = obj[k];\n    const key = prefix ? prefix + '.' + k : k;\n    if (v && typeof v === 'object' && !Array.isArray(v)) flatten(v, key, res);\n    else res[key] = v;\n  }\n  return res;\n}\n```\n\n## Follow-up Questions\n- How would you test handling of occasionally missing fields?\n- What would you do to scale the job for larger batches?","diagram":"flowchart TD\n  A[Ingest Files to S3] --> B[Glue Studio Job: JSON flatten] \n  B --> C[Glue Studio Job: CSV parse] \n  C --> D[Write Parquet to date partition] \n  D --> E[Glue Catalog Table with metadata] \n  E --> F[Quality checks & Schema drift handling] \n  F --> G[Test plan & validation]","difficulty":"beginner","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","Oracle","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T04:18:19.843Z","createdAt":"2026-01-14T04:18:19.843Z"},{"id":"q-1654","question":"New data source writes daily CSVs to S3. Design a beginner pipeline to validate a defined schema, fail-fast on missing required fields, and route bad records to a quarantine bucket with a reason. Store valid data as Parquet in S3 with daily partitions, and catalog via Glue so Athena can query. Include a basic end-to-end test plan?","answer":"Set up a Glue Spark job triggered by S3 PUT events to read CSVs with a defined schema, filter out rows missing required fields, and write invalid rows to a quarantine bucket with a validation_reason f","explanation":"## Why This Is Asked\nPractical beginner task validating end-to-end ingestion, data quality, and cataloging.\n\n## Key Concepts\n- S3 event-driven ETL\n- Glue Spark/DynamicFrame validation\n- Quarantine routing for bad records\n- Parquet partitioning by date\n- Glue Data Catalog and Athena access\n\n## Code Example\n```javascript\n// Pseudo Glue-like JS snippet for validation\nfunction process(batch){\n  const valid=[], quarantined=[];\n  for (const r of batch){\n    if (r.a != null && r.b != null) valid.push(transform(r));\n    else quarantined.push({row:r, validation_reason:'missing_required_fields'});\n  }\n  writeParquet(valid, datePartition);\n  writeJson(quarantined, quarantineBucket);\n}\n```\n\n## Follow-up Questions\n- How would you test schema evolution if new fields are added?\n- How would you monitor the quarantine rate and alert if it spikes?","diagram":"flowchart TD\n  A[CSV Ingest] --> B[Glue ETL]\n  B --> C[Parquet in S3]\n  B --> D[Quarantine bucket]\n  C --> E[Glue Catalog]\n  E --> F[Athena]","difficulty":"beginner","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Plaid","Two Sigma","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T05:36:30.080Z","createdAt":"2026-01-14T05:36:30.080Z"},{"id":"q-1785","question":"Two-account AWS data lake (us-east-1 and eu-west-1) ingests partner JSON events via API into S3. Design an end-to-end pipeline using Glue for ETL and cataloging, store Parquet with daily partitions by country/date, enable Athena queries with Lake Formation access controls, and handle schema evolution, data quality, late-arrivals, and cross-region replication trade-offs?","answer":"Land partner JSON events into S3 landing bucket, run a Glue ETL to convert to Parquet, partition by country and date, and catalog with Glue. Use Lake Formation for fine‑grained access. Implement schem","explanation":"## Why This Is Asked\n\nTests ability to design a cross-account, cross-region data pipeline with end-to-end data quality and governance considerations. It emphasizes real-world friction points like schema drift, late-arrival data, and access controls.\n\n## Key Concepts\n\n- AWS Glue ETL and Data Catalog\n- Parquet, partition pruning (country/date)\n- Lake Formation access controls and sharing\n- Schema evolution and Glue Schema Registry\n- Data quality checks and late-arrival handling\n- Cross-region replication implications (latency, consistency, cost)\n\n## Code Example\n\n```python\nfrom awsglue.context import GlueContext\nfrom pyspark.context import SparkContext\nfrom pyspark.sql.functions import to_date, col\n\nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\ndf = glueContext.read.json(\"s3://landing-bucket/partner-events/\")\ndf = df.withColumn(\"date\", to_date(col(\"event_time\"))).select(\"country\", \"date\", *df.columns)\ndf.write.partitionBy(\"country\", \"date\").mode(\"append\").parquet(\"s3://curated-bucket/parquet/partner-events/\")\n```\n\n## Follow-up Questions\n\n- How would you test end-to-end with backfills for the last 7 days?\n- How would you automate schema drift detection and backfill compatibility?\n- What Lake Formation grants would you set for analysts vs admins, across accounts?","diagram":"flowchart TD\n  S[Partner API events] --> I[Ingest to S3 land bucket]\n  I --> ETL[Glue ETL to Parquet]\n  ETL --> P[Partitioned Parquet: country/date]\n  P --> Q[Athena via Glue Catalog]\n  Q --> L[Lake Formation access controls]","difficulty":"intermediate","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T10:47:46.380Z","createdAt":"2026-01-14T10:47:46.380Z"},{"id":"q-1816","question":"You collect protobuf-encoded telemetry from thousands of IoT devices via AWS IoT Core; design an end-to-end ingestion that lands as daily-partitioned Parquet in S3, uses AWS Glue Schema Registry for evolving schemas, and implements idempotent deduplication plus a 3-sigma anomaly check. Include data formats, partition keys, testing strategy?","answer":"Proposed approach: IoT Core receives protobuf telemetry; push to Kinesis Data Streams; a Lambda decodes protobuf using the Glue Schema Registry schema and writes to S3 via Firehose as Parquet. Partiti","explanation":"## Why This Is Asked\n\nTests ability to design real-time to batch pipelines, handle protobuf, schema evolution, dedup, and quality checks in a multi-service AWS environment, with testing considerations.\n\n## Key Concepts\n\n- IoT Core to Kinesis to Lambda decoding\n- Glue Schema Registry usage\n- Parquet partitioning by date and device_type\n- Idempotent deduplication (device_id, message_id)\n- 3-sigma anomaly checks and end-to-end testing\n\n## Code Example\n\n```python\n# PySpark snippet for 3-sigma anomaly detection\nfrom pyspark.sql import functions as F\ndf = spark.read.parquet(\"s3://bucket/path\")\nstats = df.agg(F.mean(\"value\").alias(\"mean\"), F.stddev(\"value\").alias(\"stddev\")).collect()[0]\nmean, std = stats[\"mean\"], stats[\"stddev\"]\ndf.filter((F.abs(df.value - mean) > 3*std)).select(...)\n```\n\n## Follow-up Questions\n\n- How would you handle schema evolution without breaking consumers?\n- What are the failure modes and how would you monitor dedup accuracy?\n","diagram":"flowchart TD\n  IoTCore(IoT Core) --> KDS[Kinesis Data Streams]\n  KDS --> Lambda[Decode protobuf]\n  Lambda --> Firehose[Firehose to S3 Parquet]\n  Firehose --> S3[S3: daily partitions by date, device_type]\n  S3 --> Catalog[Glue Data Catalog]\n  Catalog --> Dedup[DynamoDB dedupe table]","difficulty":"intermediate","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Meta","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T11:50:43.897Z","createdAt":"2026-01-14T11:50:43.897Z"},{"id":"q-1883","question":"Design a streaming data contract and quality workflow for events arriving from multiple teams via MSK and Kinesis. Use AWS Glue Schema Registry to enforce evolving Avro/JSON schemas with versioning and compatibility (backward/forward). Describe publishing schemas, validating payloads at ingest, storing Parquet in S3 with daily partitions, and monitoring for drift and quality across regions?","answer":"Use Glue Schema Registry with per-stream Avro/JSON schemas, versioned and backward/forward compatible. Producers send schemaId in headers; a validation step enforces the contract at ingest. Write to S","explanation":"## Why This Is Asked\n\nTests practical use of Glue Schema Registry, schema evolution governance, and cross-region data quality workflows. It also covers integration with streaming sources (MSK/Kinesis) and storage in S3, with error handling and drift monitoring.\n\n## Key Concepts\n\n- Glue Schema Registry and Avro/JSON\n- Schema versioning and compatibility (backward/forward)\n- Ingest validation and header schemaId\n- Parquet ingestion with date-based partitions\n- Glue Data Quality and CloudWatch drift alerts\n\n## Code Example\n\n```javascript\n// Pseudo-code: register and validate a schema, then publish event with header schemaId\n```\n\n## Follow-up Questions\n\n- How would you test schema evolution changes across producers without downtime?\n- How do you handle breaking changes in existing pipelines?","diagram":null,"difficulty":"advanced","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T15:43:31.534Z","createdAt":"2026-01-14T15:43:31.534Z"},{"id":"q-1968","question":"Design an end-to-end streaming-to-lake pipeline: ingest real-time events from AWS MSK, consolidate into Apache Iceberg tables on S3 with daily partitions, support CDC-style upserts/deletes, and implement schema evolution. Include governance with Lake Formation/IAM, testing strategy (canaries, synthetic data), and data quality/lineage monitoring?","answer":"Ingest from MSK to Spark Streaming, write to Iceberg tables on S3 with daily partitions; implement upserts/deletes using Iceberg MERGE; enable schema evolution via Iceberg schema migrations and Glue C","explanation":"## Why This Is Asked\nAssesses ability to design a scalable, governed streaming-to-warehouse pipeline using Iceberg, CDC semantics, and robust testing.\n\n## Key Concepts\n- Apache Iceberg on S3 with partitioning and schema evolution\n- CDC upserts/deletes in Iceberg\n- MSK ingest and Spark Structured Streaming or Glue jobs\n- Data governance with Lake Formation and IAM\n- Data quality checks and lineage (canaries, synthetic data, metadata catalogs)\n\n## Code Example\n```python\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import from_json, col\nspark = SparkSession.builder.getOrCreate()\n# schema would be defined per event\nschema = ...\ndf = spark.readStream.format(\"kafka\").option(\"kafka.bootstrap.servers\",\"host:9092\").option(\"subscribe\",\"rides\").load()\nevents = df.selectExpr(\"CAST(value AS STRING) as json\").select(from_json(col(\"json\"), schema).alias(\"r\")).select(\"r.*\")\nquery = events.writeStream.format(\"iceberg\").option(\"path\",\"s3://bucket/iceberg/rides_events\").option(\"checkpointLocation\",\"s3://bucket/checkpoints/rides\").outputMode(\"append\").start(\"analytics.rides_events\")\nquery.awaitTermination()\n```\n\n## Follow-up Questions\n- How would you handle late data and retractions?\n- What are the trade-offs between Iceberg vs. Delta in this context?","diagram":"flowchart TD\n  MSK(MSK) --> Iceberg[Iceberg Tables on S3]\n  Iceberg --> Catalog[Glue Data Catalog]\n  Catalog --> BI[Athena/Presto]\n  Iceberg --> CDC[CDC Upserts/Deletes]\n  Iceberg --> Governance[Data Governance]\n  Governance --> LF[Lake Formation]\n  BI --> Monitor[Data Quality & Lineage Alerts]","difficulty":"advanced","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T19:01:30.560Z","createdAt":"2026-01-14T19:01:30.561Z"},{"id":"q-1981","question":"Design a data lake ingestion pipeline on S3 for semi-structured SaaS data that must support upserts and deletes with time-travel queries; choose between Apache Iceberg, Hudi, or Delta Lake on AWS (Glue, EMR, Athena) and justify: schema evolution, compaction, CDC handling, partitioning, and data quality checks; include testing plan?","answer":"Use Apache Iceberg on S3 (Parquet) with a Glue Data Catalog. Implement upserts/deletes via MERGE, with tombstones for deletes and partition pruning for cost. Ingest via CDC streams from SaaS sources, ","explanation":"## Why This Is Asked\nThe candidate must reason about ACID on object storage and multi-source ingestion.\n\n## Key Concepts\n- Iceberg vs. alternatives for upserts\n- CDC, dedupe, tombstones\n- Schema evolution and time travel\n- Partitioning and compaction costs\n- Validation via Athena/QA dashboards\n\n## Code Example\n```javascript\n// placeholder example of MERGE operation with Iceberg\n```\n\n## Follow-up Questions\n- How would you instrument data quality checks and alerting?\n- How would you test schema evolution across deployments?\n","diagram":"flowchart TD\n  A[Source SaaS] --> B[CDC Stream]\n  B --> C[Iceberg Table on S3]\n  C --> D[Athena/Glue Access]\n  D --> E[Time Travel / QA]","difficulty":"intermediate","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","Instacart","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T19:35:55.962Z","createdAt":"2026-01-14T19:35:55.962Z"},{"id":"q-2019","question":"Scenario: IoT sensors on factory floors emit JSON telemetry with evolving schemas (new metrics appear over time). Design a streaming ingestion pipeline that reads from Kinesis Data Streams, writes to S3 as Parquet with daily partitions, and uses an Iceberg catalog (Glue-backed) to handle schema evolution. Compare upsert strategies for late data (Iceberg MERGE INTO vs Hudi), outline data quality checks, and a testing plan including sandbox data and end-to-end validation. Include governance via Lake Formation?","answer":"Leverage Glue Streaming (Spark) to ingest JSON from Kinesis Data Streams and write daily-partitioned Parquet in S3, backed by an Iceberg catalog (Glue). Use MERGE INTO for late data; rely on Iceberg f","explanation":"Why This Is Asked\nTests experience with streaming ingestion, advanced file formats, and schema evolution in production.\n\nKey Concepts\n- Streaming ETL with Glue or Spark\n- Parquet with daily partitions\n- Iceberg catalog in Glue and schema evolution\n- Upserts for late data (MERGE INTO vs Hudi)\n- Data quality and drift detection\n- End-to-end testing in sandbox; governance via Lake Formation\n\nCode Example\n```javascript\n// Pseudo Spark SQL for Iceberg MERGE\nMERGE INTO iceberg_db.telemetry AS t\nUSING staged_telemetry AS s\nON t.device_id = s.device_id AND t.ts = s.ts\nWHEN MATCHED THEN UPDATE SET *\nWHEN NOT MATCHED THEN INSERT *\n```\n\nFollow-up Questions\n- How would you test schema evolution with additive fields?\n- How would you monitor ingestion quality and alert on drift?","diagram":"flowchart TD\n  A[Kinesis Data Streams] --> B[Glue Streaming (Spark)]\n  B --> C[S3 Parquet daily partitions]\n  C --> D[Iceberg Catalog (Glue)]\n  D --> E[Athena/Redshift Spectrum]","difficulty":"intermediate","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","IBM","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T20:55:38.216Z","createdAt":"2026-01-14T20:55:38.216Z"},{"id":"q-2060","question":"Design a multi-region, streaming-plus-batch ingestion pipeline for a mobile app that emits JSON events at high volume. Ingest via Kinesis Data Streams to S3 Parquet using Apache Iceberg tables with a Glue catalog, supporting upserts, deletes, and schema evolution. Include multi-source CDC, data quality checks, governance via Lake Formation, and end-to-end testing?","answer":"Leverage Iceberg tables on S3 with a Glue catalog, landing JSON from Kinesis via a streaming job that MERGEs into Iceberg to support upserts/deletes; replicate CDC from multiple sources using DMS across regions with schema evolution and governance.","explanation":"## Why This Is Asked\nThis question probes design for scalable, auditable data lakes across regions, balancing mutating events with schema changes and governance.\n\n## Key Concepts\n- Apache Iceberg on S3 with Glue catalog\n- Upserts/deletes via MERGE semantics\n- Multi-source CDC (DMS) across regions\n- Schema evolution and data quality checks (Deequ)\n- Lake Formation governance and end-to-end testing\n\n## Code Example\n```javascript\nMERGE INTO iceberg_db.events AS t\nUSING staging.events AS s\nON t.event_id = s.event_id\nWHEN MATCHED THEN UPDATE SET t.payload = s.payload, t.ts = s.ts\nWHEN NOT MATCHED THEN INSERT (event_id, payload, ts) VALUES (s.event_id, s.payload, s.ts)\n```","diagram":null,"difficulty":"advanced","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","Meta","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T04:34:48.217Z","createdAt":"2026-01-14T22:47:50.677Z"},{"id":"q-2095","question":"You receive a financial dataset hourly from a partner API in JSON with nested fields. Design a beginner-level ingestion pipeline to land data in S3 as Parquet with hourly partitions, using AWS Glue for ETL and cataloging. Include how you flatten nested JSON, define a stable schema, enable basic data quality checks, ensure encryption with KMS, and implement least-privilege IAM roles and Lake Formation permissions. Outline how you'd test end-to-end before analytics?","answer":"Implement a secure data ingestion pipeline using a KMS-encrypted S3 bucket to store raw JSON files with hourly prefixes. Configure a Glue Crawler to infer the initial schema and populate the Glue Data Catalog. Develop a Glue ETL job in Python to flatten nested JSON fields, convert to Parquet format, and write to a processed S3 location with hourly partitioning. Enable basic data quality checks using Glue DataBrew rulesets. Apply least-privilege IAM roles and Lake Formation permissions to control access. Validate the pipeline end-to-end using sample data before production deployment.","explanation":"## Why This Is Asked\nTests practical data ingestion setup with common AWS services and security considerations.\n\n## Key Concepts\n- JSON flattening, Parquet storage, hourly partitions\n- Glue Crawlers, Glue ETL (Python), Catalog with Lake Formation\n- KMS encryption, IAM least privilege, data quality checks\n\n## Code Example\n```python\n# flatten nested json\nimport json\n\ndef flatten(record):\n    flat = {}\n    for k, v in record.items():\n        if isinstance(v, dict):\n            for kk, vv in v.items():\n                flat[f\"{k}_{kk}\"] = vv\n        else:\n            flat[k] = v\n    return flat\n```","diagram":"flowchart TD\n  A[Partner JSON hourly] --> B[Glue Crawler infer schema]\n  B --> C[Glue ETL flatten to Parquet]\n  C --> D[S3/processed hourly partitions]\n  D --> E[Athena/Quicksight access]","difficulty":"beginner","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Robinhood","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T05:02:22.343Z","createdAt":"2026-01-14T23:45:36.307Z"},{"id":"q-2144","question":"Design a real-time fraud signals pipeline: ingest 50 banks via Kinesis Data Streams, use Glue Schema Registry for evolving Avro schemas, run Spark Streaming to land Parquet in S3 partitioned by date and merchantId, with a raw zone and a masked curated zone. Apply IP masking and immutability retention; catalog with Glue Data Catalog; enforce access via Lake Formation; emit lineage/audit logs. Include failover tests and latency considerations?","answer":"Architect a real-time fraud signals pipeline: ingest 50 banks via Kinesis Data Streams, use Glue Schema Registry for evolving Avro schemas, Spark Streaming to write Parquet to S3 with date and merchan","explanation":"## Why This Is Asked\nTests end-to-end streaming design, schema evolution, privacy controls, data governance, and resilience in production.\n\n## Key Concepts\n- Real-time ingestion via Kinesis Data Streams with Glue Schema Registry for evolving Avro schemas\n- Streaming transform to Parquet with partitioning by date and merchantId\n- Data masking, raw vs curated zones, immutable retention\n- Cataloging with Glue Data Catalog and access control via Lake Formation\n- Data lineage and audit logging; end-to-end validation\n\n## Code Example\n```python\n# PySpark sketch: mask IP and write to curated zone\nfrom pyspark.sql.functions import sha2, col\n\ndf = spark.readStream.format('kinesis').option('streamName','fraud').load()\nmasked = df.withColumn('ipAddress', sha2(col('ipAddress'), 256))\nmasked.writeStream.format('parquet').option('path','s3://bucket/curated/')\n```\n\n## Follow-up Questions\n- How would you handle schema drift in Glue Schema Registry during peak load?\n- How would you simulate regional outages and validate failover?","diagram":"flowchart TD\n  A[Ingest: Kinesis Data Streams] --> B[Spark Streaming]\n  B --> C[S3: Raw Parquet]\n  B --> D[S3: Curated Parquet]\n  C --> E[Glue Catalog]\n  D --> E\n  E --> F[Lake Formation Access]","difficulty":"advanced","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Plaid","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T04:23:42.398Z","createdAt":"2026-01-15T04:23:42.399Z"},{"id":"q-2204","question":"Design a real-world, multi-tenant, multi-account ingestion pipeline: Ingest JSON events from microservices via Kinesis Data Streams, store as Parquet in S3 with daily partitioning by tenant_id, and catalog with AWS Glue. Enforce tenant isolation with Lake Formation across accounts, handle schema evolution, embed data quality checks, and outline end-to-end testing strategies?","answer":"In a multi-tenant, multi-account data lake, ingest JSON events from microservices via Kinesis Data Streams, land to S3 as Parquet partitioned by date and tenant_id. Use Glue for ETL and cataloging; en","explanation":"## Why This Is Asked\nTests cross-account data governance, real-world multi-tenant isolation, and evolving schemas in a streaming-to-batch pipeline.\n\n## Key Concepts\n- Multi-account data governance with Lake Formation\n- Parquet partitioning by date and tenant_id\n- Glue ETL + Data Catalog with schema evolution\n- Data quality checks in ETL and streaming\n- End-to-end testing with synthetic tenants\n\n## Code Example\n```javascript\n// Pseudocode: ETL outline for Glue-like job (conceptual)\nasync function runETL(inputStream){\n  const batch = await readJSONEvents(inputStream)\n  const transformed = batch.map(e => ({...e, tenant_id: e.tenantId, date: toDate(e.ts)}))\n  await writeParquet(transformed, `/s3/bucket/tenant/date`, {partitionBy: ['tenant_id','date']})\n  await updateGlueCatalog(transformed)\n}\n```\n\n## Follow-up Questions\n- How would you implement schema evolution without breaking downstream jobs?\n- What monitoring and alerting would you add for data-skew or partition churn?","diagram":"flowchart TD\n  A[Ingest JSON events via Kinesis] --> B[Transform & Partition by tenant_id/date]\n  B --> C[S3 Parquet Landing Zone]\n  C --> D[Glue Catalog & Lake Formation Policies]\n  D --> E[Analytics & BI Access]\n  E --> F[Monitoring & Alerts]","difficulty":"intermediate","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Bloomberg","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T07:35:14.749Z","createdAt":"2026-01-15T07:35:14.749Z"},{"id":"q-2244","question":"You receive daily JSON event files from a mobile app stored in S3 with nested arrays. Design a beginner-friendly ingestion pipeline to flatten into Parquet with daily partitions, catalog in AWS Glue, and implement a basic data quality gate (required fields, non-null user_id, ISO8601 event_time). Include testing approach and handling of schema drift?","answer":"Implement a Glue Spark job that reads daily JSON files from s3://partner-logs/incoming/YYYY/MM/DD, flattens nested arrays with explode, and maps to a canonical schema (user_id, event_time, device, act","explanation":"## Why This Is Asked\nTests practical workflow design for beginner to apply Glue, S3, and Parquet with schema evolution and data quality.\n\n## Key Concepts\n- Flattening nested JSON, Parquet partitions, Glue Catalog\n- Simple data quality rules, alerting, basic deduplication\n- Testing with sample data and end-to-end validation\n\n## Code Example\n```javascript\n// Pseudo Glue job invocation using AWS SDK\nimport { GlueClient, StartJobRunCommand } from \"@aws-sdk/client-glue\";\nconst client = new GlueClient({ region: \"us-east-1\" });\nconst cmd = new StartJobRunCommand({ JobName: \"FlattenJsonToParquet\" });\nclient.send(cmd);\n```\n\n## Follow-up Questions\n- How would you extend this with schema drift handling across daily files?\n- How would you test idempotency for repeated runs","diagram":"flowchart TD\n  A[S3 Incoming daily JSON] --> B[Glue ETL Job]\n  B --> C[Parquet in S3 with daily partitions]\n  C --> D[Glue Data Catalog]\n  D --> E[Athena/Quicksight dashboards]","difficulty":"beginner","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","LinkedIn","Lyft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T09:01:35.624Z","createdAt":"2026-01-15T09:01:35.624Z"},{"id":"q-2264","question":"In a cross-account AWS data lake spanning us-east-1 and eu-west-1, ingest streaming events from Kinesis into an Apache Hudi dataset on S3 to support upserts and time-travel. Design the end-to-end architecture including how to implement upserts, schema evolution, compaction, cross-account Lake Formation policies, and an end-to-end test and rollback plan, with concrete services and data formats?","answer":"Use Kinesis to trigger a Spark job that writes to S3 as an Apache Hudi MOR table, enabling upserts and time-travel. Catalog with Glue; enforce cross-account Lake Formation grants; handle schema evolut","explanation":"## Why This Is Asked\nTests ability to design an upsert-capable lake using Hudi, cross-account access, and testing/rollback.\n\n## Key Concepts\n- Apache Hudi MOR on S3 with Glue Catalog\n- Kinesis streaming and Spark ETL\n- Lake Formation cross-account access\n- Compaction and late-arrival handling\n- Backfill and rollback strategies\n\n## Code Example\n```javascript\n# PySpark/Hudi upsert example (conceptual)\ndf.write.format(\"org.apache.hudi\").options({\n  \"hoodie.datasource.write.recordkey\": \"id\",\n  \"hoodie.datasource.write.table_type\": \"MERGE_ON_READ\",\n  \"hoodie.datasource.write.precombine_field\": \"ts\",\n  \"hoodie.datasource.write.payload.use_object_digest\": \"true\"\n}).mode(\"append\").save(\"s3://bucket/path/hudi-table\")\n```\n\n## Follow-up Questions\n- How would you test compaction performance at scale?\n- What are cross-account Lake Formation pitfalls and mitigations for this pattern?","diagram":null,"difficulty":"advanced","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T09:48:00.369Z","createdAt":"2026-01-15T09:48:00.369Z"},{"id":"q-2312","question":"Scenario: you manage a multi-tenant SaaS data lake across three AWS accounts. Ingest telemetry events via Kinesis Data Streams, land as Parquet in S3 with daily partitions, and register metadata in the Glue Data Catalog. Enforce schema evolution and tenant-level masking using Lake Formation; enable cross-account analytics with Athena. Describe architecture, governance, and testing strategy?","answer":"Leverage Lake Formation with per-tenant databases and table permissions, and use IAM roles for cross-account access. Land data as Parquet in S3 with daily partitions, register in the Glue Data Catalog","explanation":"## Why This Is Asked\nTests cross‑account governance, fine‑grained access, and scalable analytics in a multi‑tenant data lake. It probes masking, schema evolution, and end‑to‑end validation.\n\n## Key Concepts\n- Cross‑account data lake governance with Lake Formation\n- Fine‑grained access and tenant isolation\n- Schema evolution in Glue Data Catalog\n- Parquet partitioning by date\n- Data masking for PII\n- Cross‑account analytics with Athena\n- Data lineage and end‑to‑end testing\n\n## Code Example\n```json\n{\n  \"MaskingPolicy\": {\n    \"Column\": \"email\",\n    \"MaskType\": \"EMAIL\",\n    \"AppliedTo\": \"TenantA\"\n  }\n}\n```\n\n## Follow-up Questions\n- How would you validate masking in dev vs prod environments?\n- How do you propagate schema changes without breaking downstream queries?\n- What monitoring would you implement for cross‑account access and data lineage?","diagram":"flowchart TD\n  A[Telemetry Source (Kinesis Data Streams)]\n  B[S3 Parquet (daily partitions)]\n  C[Glue Data Catalog]\n  D[Lake Formation Masking]\n  E[Athena Cross-Account Access]\n  F[BI/Dashboards]\nA --> B\nB --> C\nC --> D\nD --> E\nE --> F","difficulty":"intermediate","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","Microsoft","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T11:40:27.776Z","createdAt":"2026-01-15T11:40:27.776Z"},{"id":"q-2350","question":"Design an end-to-end ingestion and upsert pipeline for customer records with SCD Type 2 using Apache Iceberg on S3. Ingest JSON events from Kinesis and batch CDC from RDS; ensure schema evolution, partitioning, and time travel; discuss how to reconcile late-arriving changes and record-level lineage. Include services, table format, upsert strategy, and testing?","answer":"Use Apache Iceberg on S3 with Glue Catalog. Real-time: Kinesis Data Streams feed a Spark Structured Streaming job writing to an Iceberg table partitioned by day. Batch: AWS DMS captures RDS CDC to S3,","explanation":"## Why This Is Asked\nTests practical lakehouse design with Iceberg on S3, SCD Type 2, and hybrid real-time/batch ingestion, plus schema evolution and lineage.\n\n## Key Concepts\n- Iceberg on S3 with Glue Catalog\n- Real-time ingestion via Kinesis and Spark Structured Streaming\n- Batch CDC via AWS DMS from RDS to S3\n- SCD Type 2 upsert semantics (start_date, end_date, current)\n- Schema evolution and time travel in Iceberg\n- Data lineage via catalog and metadata\n\n## Code Example\n```javascript\nMERGE INTO iceberg.customers AS target\nUSING staging AS source\nON target.cust_id = source.cust_id AND target.current = true\nWHEN MATCHED THEN UPDATE SET end_date = source.batch_date, current = false\nWHEN NOT MATCHED THEN INSERT (..., start_date, end_date, current) VALUES (...)\n```\n\n## Follow-up Questions\n- How would you validate late-arriving changes to avoid duplicates?\n- How would you test schema evolution conflicts across real-time and batch paths?\n- How would you monitor and prove end-to-end data lineage across services?","diagram":"flowchart TD\n  A[Kinesis Real-time Ingest] --> B[Spark Streaming to Iceberg @ S3]\n  C[DMS CDC to S3] --> B\n  B --> D[Iceberg Table with Glue Catalog]\n  D --> E[Time Travel & Schema Evolution]","difficulty":"advanced","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Apple","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T14:38:32.079Z","createdAt":"2026-01-15T14:38:32.079Z"},{"id":"q-2440","question":"Scenario: Daily JSON app-logs arrive in S3 at s3://bucket/raw/app-logs with fields userId, timestamp, action, and optional device {model,os}. For a beginner, design an ingestion pipeline that flattens to Parquet with daily partitions into s3://bucket/curated/app-logs/yyyy/mm/dd/, catalogs in Glue, and uses a defined schema (userId string, timestamp timestamp, action string, device_model string, device_os string) with schema evolution enabled. Include data quality gates (required userId, non-null timestamp, restricted action set), drift handling, and end-to-end testing plan?","answer":"Use a Glue ETL job to read JSON from s3://bucket/raw/app-logs, flatten device fields, and write Parquet to s3://bucket/curated/app-logs/yyyy/mm/dd with daily partitions. Enforce schema: userId string,","explanation":"## Why This Is Asked\nTests practical use of AWS Glue for end-to-end ingestion, Parquet formatting, and schema evolution with basic data quality.\n\n## Key Concepts\n- AWS Glue ETL and Data Catalog\n- Parquet with partitioning by date\n- Flattening nested JSON to a flat schema\n- Schema evolution and data quality gates\n\n## Code Example\n```python\n# Minimal Glue ETL outline (conceptual)\nfrom awsglue.transforms import *\nfrom awsglue.dynamicframe import DynamicFrame\n\ndef flatten(frame):\n    # flatten device fields\n    return frame\n```\n\n## Follow-up Questions\n- How would you test for schema drift and roll back changes?\n- How would you handle new fields arriving in device or action?\n","diagram":null,"difficulty":"beginner","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Plaid","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T17:57:24.761Z","createdAt":"2026-01-15T17:57:24.761Z"},{"id":"q-2494","question":"Ingest a hybrid workload into an Iceberg table on S3: streaming JSON events and nightly relational extracts, partitioned by date. Outline a concrete pipeline using AWS Glue, S3, and query engines (Athena/Databricks), with data formats, schema evolution, upserts, late-arriving data, and end-to-end validation. How would you monitor partitions and performance and enforce access controls?","answer":"Use a single Iceberg-backed table on S3 with daily partitions. Stream JSON via Kinesis Firehose to Parquet, batch PostgreSQL exports to Parquet, all merged into Iceberg. Implement MERGE on event_id fo","explanation":"## Why This Is Asked\nTests ability to design a hybrid batch/stream pipeline, choose Iceberg on S3, and reason about schema evolution, upserts, late data handling, validation, and governance in AWS.\n\n## Key Concepts\n- Apache Iceberg on S3 with Delta-like upserts\n- Schema evolution and partitioning\n- Hybrid ingestion (stream + batch)\n- Data quality and end-to-end testing\n- Observability: partition counts, lag, job metrics\n\n## Code Example\n```sql\nMERGE INTO iceberg_db.iceberg_table AS t\nUSING staging_view AS s\nON t.event_id = s.event_id\nWHEN MATCHED THEN UPDATE SET t.payload = s.payload, t.updated_at = s.updated_at\nWHEN NOT MATCHED THEN INSERT (event_id, payload, updated_at) VALUES (s.event_id, s.payload, s.updated_at);\n```\n\n## Follow-up Questions\n- How would you handle schema changes that affect downstream schemas?\n- How would you implement access control and data sharing in this setup?","diagram":null,"difficulty":"intermediate","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Databricks","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T20:35:49.838Z","createdAt":"2026-01-15T20:35:49.838Z"},{"id":"q-2594","question":"Design an end-to-end AWS data lake ingestion and governance solution that provides complete data lineage across multiple sources (RDS, SaaS API, IoT), transformations in AWS Glue, and consumption in Athena. Include how you model lineage metadata (source, transform, target), handle schema evolution, and how you validate lineage during failing ETL runs and across re-processing, with Lake Formation fencing?","answer":"Instrument each ETL run with a unique run_id and emit comprehensive lineage records (source→transformation→target) to a centralized lineage ledger stored in S3, with corresponding metadata managed in the Glue Data Catalog. Orchestrate the ingestion pipeline using Step Functions, while Lake Formation enforces access controls and implements fencing mechanisms to prevent concurrent modifications. For schema evolution, maintain versioned schemas in the Glue Data Catalog and leverage AWS Glue Schema Registry to validate compatibility. During ETL failures, Step Functions' built-in retry mechanisms and error handling ensure lineage records are committed exclusively for successful transformations, preserving data integrity across re-processing scenarios.","explanation":"## Why This Is Asked\nTests capability to design production-grade lineage and governance across sources, transforms, and targets, with robust testing for failures.\n\n## Key Concepts\n- Data lineage modeling across sources, transforms, targets\n- Glue Data Catalog + S3 lineage ledger\n- Step Functions orchestration, Lake Formation controls\n- Schema evolution handling and validation\n- End-to-end testing of lineage integrity in ETL runs\n\n## Code Example\n```javascript\n// Pseudo-code: emitLineage(runId, source, transform, target)\nfunction emitLineage(runId, source, transform, target) {\n  const entry = {\n    run_id: runId,\n    timestamp: new Date().toISOString(),\n    source: source,\n    transformation: transform,\n    target: target,\n    status: 'success'\n  };\n  // Write to lineage ledger in S3\n  s3.putObject({\n    Bucket: 'lineage-ledger',\n    Key: `lineage/${runId}/${Date.now()}.json`,\n    Body: JSON.stringify(entry)\n  });\n  // Update Glue Data Catalog metadata\n  glue.updateTable({ ... });\n}\n```\n\nThis approach ensures complete traceability while handling failures gracefully and maintaining data governance through Lake Formation's fencing capabilities.","diagram":"flowchart TD\n  A[Source Systems] --> B[Ingestion Layer]\n  B --> C[Glue ETL Jobs]\n  C --> D[Lineage Ledger (S3)]\n  C --> E[Glue Data Catalog]\n  D --> F[Athena/Curated Tables]\n  E --> F","difficulty":"advanced","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["PayPal","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T05:04:47.209Z","createdAt":"2026-01-16T02:22:44.228Z"},{"id":"q-2653","question":"Scenario: A global product analytics data lake ingests streaming clickstream events from mobile apps with evolving JSON schemas containing PII. You must mask PII in the shared dataset before analytics, support cross-account access, and keep raw data auditable for compliance. Design end-to-end ingestion using AWS (Kinesis or MSK, Glue, S3, Lake Formation/Macie, KMS). Address schema evolution, late data, data quality, testing, and rollout?","answer":"Two-branch streaming: ingest with Kinesis (or MSK); route to a masking Spark job in Glue that redacts PII but preserves IDs; emit masked Parquet to S3 in daily partitions (date, region, app). Use Glue","explanation":"## Why This Is Asked\\nThis question probes practical privacy-preserving data ingestion at scale, cross-account access, and governance.\\n\\n## Key Concepts\\n- Streaming data pipelines (Kinesis/MSK)\\n- PII masking in ETL (Glue Spark UDFs)\\n- Data lake governance (Glue Catalog, Lake Formation)\\n- PII discovery (Macie) and KMS-based encryption\\n\\n## Code Example\\n```python\\n# Glue Spark masking example (conceptual)\\ndef mask(record):\\n  record['email'] = hash_value(record.get('email'))\\n  record['phone'] = mask_phone(record.get('phone'))\\n  return record\\n```\\n\\n## Follow-up Questions\\n- How would you test schema evolution and backward compatibility across partitions?\\n- How would you monitor data quality and alert on masking failures in production?","diagram":null,"difficulty":"advanced","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","MongoDB","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T05:39:51.621Z","createdAt":"2026-01-16T05:39:51.621Z"},{"id":"q-2686","question":"Design an ingestion pipeline for a new SaaS data stream of JSON events that lands in S3 with daily partitions. Use AWS Glue (Spark) and Apache Iceberg on S3 to materialize updatable tables with idempotent merges, handle schema evolution, and cope with late-arriving data. Describe partitioning, upsert strategy, schema evolution, data quality checks, testing, and monitoring?","answer":"Propose a pipeline: Glue Spark reads JSON with nested fields, flattens it, and writes to an Iceberg table on S3 with daily partitions. Upsert by a stable PK using MERGE INTO, and enable Iceberg schema","explanation":"## Why This Is Asked\n\nInterview context explanation.\n\n## Key Concepts\n\n- Iceberg on S3 with Glue catalog\n- MERGE INTO upserts by PK\n- Schema evolution and partitioning strategies\n- Late-arrivals handling with watermark\n- Data quality testing and monitoring\n\n## Code Example\n\n```javascript\n// Pseudo: validate a record's key fields before merge\nfunction valid(rec){\n  return rec && rec.userId && typeof rec.eventTime === 'string'\n}\n```\n\n## Follow-up Questions\n\n- How would you handle schema drift detection for Iceberg tables?\n- How would you implement backfills when source schema changes?","diagram":"flowchart TD\n  Ingest[JSON Ingest] --> Spark[Glue Spark Job]\n  Spark --> Iceberg[Iceberg Table on S3]\n  Iceberg --> Upsert[MERGE INTO by PK]\n  Ingest --> Late[Late Arrivals] --> Iceberg","difficulty":"intermediate","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","Snap","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T06:58:16.227Z","createdAt":"2026-01-16T06:58:16.228Z"},{"id":"q-2875","question":"In a regulated financial environment, design an end-to-end AWS data pipeline that ingests daily batch CSV exports and real-time credit-event streams into a centralized data lake. Explain how you would land data in S3 with partitions, implement idempotent upserts and schema evolution, enforce governance via Lake Formation and the Glue Data Catalog, implement data quality checks and tests, and observe and rollback on failures?","answer":"Hybrid ingestion design: batch feeds land CSVs into S3 under date partitions; a real-time stream of events arrives on Kinesis Data Streams and is written to S3 as JSON via Firehose. Glue Spark jobs pe","explanation":"## Why This Is Asked\nThis question probes real-world data ingestion, transformation, governance, and rollback strategies under regulatory constraints.\n\n## Key Concepts\n- Batch + streaming ingestion patterns\n- Idempotent upserts and schema evolution\n- Lake Formation governance and Glue Catalog\n- Data quality checks and testing strategies\n- Observability and rollback plans\n\n## Code Example\n```javascript\n// Pseudo Glue job outline for Spark\nimport sys\nfrom awsglue.context import GlueContext\nfrom pyspark.context import SparkContext\n\nsc = SparkContext()\ngc = GlueContext(sc)\ndf = gc.create_dynamic_frame.from_options(\"s3\", {\"path\": \"s3://bucket/path\"}, \"csv\")\ndf = df.drop_fields([\"unneeded\"])\ndf = df.resolveChoice(\"cast\", {\"column\": \"int\"})\n# upsert logic would be implemented here using a sink with a manifest\n```\n\n## Follow-up Questions\n- How would you surface schema evolution without breaking downstream dashboards?\n- What tests validate idempotence and rollback behavior?","diagram":"flowchart TD\n  A[Ingest] --> B[S3 Landing]\n  B --> C[Glue ETL]\n  C --> D[Parquet Lake]\n  A --> E[Firehose to S3]\n  D --> F[Athena/Glue Catalog]\n  F --> G[Lake Formation Governance]","difficulty":"intermediate","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","MongoDB","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T15:44:50.072Z","createdAt":"2026-01-16T15:44:50.073Z"},{"id":"q-2925","question":"You have three data sources: a REST API streaming JSON via Kinesis Data Streams, a CSV feed landed to S3, and an on-prem CDC stream to AWS. Design an end-to-end pipeline that lands data in S3 as Parquet with per-source/date partitions, uses Apache Hudi for upserts, supports schema evolution via the Glue Catalog, handles late data, includes data quality checks, and a practical end-to-end test plan?","answer":"Use a source-agnostic ingestion: Kinesis Data Streams for JSON, S3 event-based for CSV; land raw data in per-source/date prefixes; Glue Spark jobs write to Apache Hudi datasets in S3 Parquet with upse","explanation":"## Why This Is Asked\nTests ability to integrate heterogeneous sources into a single lake, with transactional-like upserts on object storage, and resilient schema management.\n\n## Key Concepts\n- Upserts on object storage using Apache Hudi (MERGE_ON_READ or COPY_ON_WRITE)\n- Glue Catalog-backed schema evolution and partition pruning\n- Late-arriving data handling and watermarking\n- Data quality checks (Glue Data Quality or Great Expectations) with DLQ routing\n- End-to-end testing with synthetic data and staged environments\n\n## Code Example\n```javascript\n// Pseudocode: Hudi upsert in Spark (Glue)\ndf.write.format(\"hudi\")\n  .option(\"hoodie.datasource.write.operation\",\"upsert\")\n  .option(\"hoodie.datasource.write.table_type\",\"MERGE_ON_READ\")\n  .option(\"hoodie.datasource.write.recordkey\",\"id\")\n  .option(\"hoodie.datasource.write.partitionpath.field\",\"source,date\")\n  .mode(\"append\").save(\"s3://bucket/data-sourceA/ parquets/\");\n```\n\n## Follow-up Questions\n- How would you validate idempotency and late-arrival correctness across sources?\n- What cost/throughput trade-offs exist between Kinesis+Glue vs a streaming-first lake strategy?","diagram":"flowchart TD\n  A(Source) --> B(Ingestion)\n  B --> C(Glue ETL)\n  C --> D(Hudi Parquet in S3)\n  D --> E(Analytics Layer)","difficulty":"intermediate","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Meta","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T17:49:06.966Z","createdAt":"2026-01-16T17:49:06.966Z"},{"id":"q-2985","question":"Scenario: You must ingest daily JSONL logs from an on-prem SFTP into S3. Design a beginner-friendly, serverless ingestion using AWS Transfer Family to pull files, convert to Parquet with daily partitions, catalog via Glue, and orchestrate via Step Functions. What steps would you implement, including naming, partitioning, quality checks, and testing?","answer":"Serverless SFTP ingest: Use AWS Transfer Family to pull daily JSONL from on-prem SFTP into S3 at logs/date=YYYY-MM-DD/. Run a Glue Spark ETL to convert to Parquet with daily partitions and register in","explanation":"## Why This Is Asked\nTests practical use of serverless ingest for on-prem sources, with a safe beginner path using Transfer Family, Glue, and Step Functions.\n\n## Key Concepts\n- AWS Transfer Family for SFTP pulls\n- Glue Spark ETL and Data Catalog\n- Parquet with daily partitions\n- Step Functions orchestration and basic data quality\n\n## Code Example\n```python\n# Glue ETL outline to convert JSONL to Parquet and write daily partitions\nimport sys\nfrom awsglue.context import GlueContext\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\n# ... minimal skeleton\n```\n\n## Follow-up Questions\n- How would you handle schema drift or missing fields in JSONL?\n- How would you monitor Step Functions failures and alert on retries?","diagram":"flowchart TD\n  A[SFTP Ingest] --> B[Transfer Family Pull]\n  B --> C[Glue ETL (JSONL->Parquet)]\n  C --> D[S3 Partitioned by date]\n  D --> E[Glue Catalog]\n  E --> F[Step Functions Orchestrator]","difficulty":"beginner","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Microsoft","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T19:47:48.168Z","createdAt":"2026-01-16T19:47:48.168Z"},{"id":"q-3199","question":"In a 3-region data lake ingesting mobile-app events, design an end-to-end pipeline to ingest JSON events from Kinesis Data Streams, normalize to Parquet, partition by tenant_id/date, and mask PII. Ensure schema evolution, data lineage, and cross-account sharing with Snowflake, IBM, and Airbnb. Include services, upsert strategy (CDC-based) for updates, testing, and monitoring?","answer":"Ingest via Kinesis Data Streams; Glue Spark parses JSON, masks PII, and writes Parquet to S3 partitioned by tenant_id/date. Catalog with Glue; enable cross-account Lake Formation grants for Snowflake/","explanation":"## Why This Is Asked\nAssesses streaming ingestion, cross-account data sharing, and governance in a multi-region data lake with PII handling.\n\n## Key Concepts\n- Kinesis Data Streams\n- AWS Glue Spark ETL\n- Parquet data format and partitioning\n- Glue Data Catalog and Lake Formation cross-account sharing\n- CDC-based upserts and schema evolution\n- Data lineage and monitoring\n\n## Code Example\n```javascript\nfunction maskPII(s) {\n  if (typeof s !== 'string') return s\n  return s.replace(/.(?=.{4})/g, '*')\n}\n```\n\n## Follow-up Questions\n- How would you monitor data quality and drift across regions?\n- How would you validate compatibility of schema changes with Snowflake and IBM dashboards?\n","diagram":null,"difficulty":"advanced","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","IBM","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T06:48:33.161Z","createdAt":"2026-01-17T06:48:33.161Z"},{"id":"q-3302","question":"In a cross-account AWS data lake, ingest real-time financial trades (JSON) from Kinesis Data Streams, land as Parquet in S3 with daily partitioning by trade_date and tenant_id for tenant isolation, and catalog with Glue. Enforce column-level PII masking and cross-account Lake Formation access, support schema evolution, and provide an end-to-end test plan. What is your architecture and approach?","answer":"Streaming design: Kinesis Data Streams feeds a Glue Spark job (or Kinesis Data Analytics) that writes Parquet to S3 with daily partition by trade_date and tenant_id. Catalog via Glue Data Catalog. Imp","explanation":"## Why This Is Asked\nTests ability to design a cross-account, governed data pipeline with real-time ingestion, data sanitization, and schema evolution. It probes tool choices, partitioning strategy, and testing rigor.\n\n## Key Concepts\n- Cross‑account Lake Formation and IAM permissions\n- Kinesis to S3 (Parquet) with partitioning\n- Column-level masking and masked views\n- Glue Data Catalog and schema evolution\n- End-to-end testing in a regulated data lake\n\n## Code Example\n```python\n# PySpark masking example (conceptual)\nfrom pyspark.sql.functions import col, when\ndf2 = df.withColumn(\"masked\", when(col(\"ssn\").isNotNull(), \"MASKED\").otherwise(None))\n```\n\n## Follow-up Questions\n- How would you validate cross-account permissions during testing?\n- How would you handle schema drift and evolving masked columns across partitions?","diagram":null,"difficulty":"advanced","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Databricks","IBM"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T10:39:13.775Z","createdAt":"2026-01-17T10:39:13.775Z"},{"id":"q-3358","question":"In a multi-account data lake for regulated analytics, ingest data from multiple SaaS apps into S3 as Parquet. Implement PII masking at ingest, data lineage, and cross-account access controls using Lake Formation. Design end-to-end pipeline with per-source ingestion paths, masking rules, schema evolution, and a robust test plan. Include the roles, services, and hand-off between ingestion, masking, and BI layers?","answer":"Ingest from each SaaS app into S3 using Firehose, then run a Glue Spark job to mask PII (SSN, emails) and write masked data as Parquet partitioned by source/date. Expose raw and masked tables in a Glu","explanation":"## Why This Is Asked\n\nAssesses ability to design secure, observable data pipelines across accounts, with PII masking, lineage, and governance using Lake Formation, Glue, and S3 Parquet. Focuses on practical patterns for multi-tenant data sharing and compliance.\n\n## Key Concepts\n\n- Lake Formation permissions and masking\n- Glue Catalog and Spark-based data masking\n- Parquet with schema evolution and partitioning\n- Data lineage, audit via Glue Catalog/CloudTrail\n- Cross-account access via RAM and trusted roles\n\n## Code Example\n\n```javascript\nfunction maskPII(value, type) {\n  if (type === 'ssn') return value ? value.replace(/\\d{3}-\\d{2}-\\d{4}/g, 'XXX-XX-XXXX') : value;\n  if (type === 'email') {\n    const [name, domain] = (value||'').split('@');\n    return (name ? name[0] + '***' : '') + '@' + (domain||'');\n  }\n  return value;\n}\n```\n\n## Follow-up Questions\n\n- How would you test masking coverage and lineage, and what metrics?\n- How would you adapt this design for tenants with different data schemas?","diagram":null,"difficulty":"intermediate","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","LinkedIn","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T13:17:49.491Z","createdAt":"2026-01-17T13:17:49.491Z"},{"id":"q-3422","question":"A SaaS provider drops a daily CSV export into S3 at s3://data-logs/saasA/YYYY/MM/DD/export.csv.gz. Outline a beginner-level AWS data pipeline using AWS Glue to ingest, convert to Parquet, partition by date, store in s3://data-lake/saasA/YYYY/MM/DD/, register in the Glue Data Catalog, and query with Athena. Include how you’d handle a new column 'region' added today (schema evolution), basic data quality checks, and a minimal end-to-end test plan?","answer":"Set up a Glue crawler to infer the CSV schema, then a Glue ETL job (Python Spark) that reads s3://data-logs/saasA/YYYY/MM/DD/export.csv.gz, writes Parquet to s3://data-lake/saasA/YYYY/MM/DD/ with dail","explanation":"## Why This Is Asked\n\nTests ability to translate a raw CSV drop into a managed, queryable Lake with partitioning, cataloging, and basic quality checks. It also checks handling of simple schema drift in a beginner-friendly way.\n\n## Key Concepts\n\n- AWS Glue ETL (Python Spark) for CSV->Parquet\n- Glue Data Catalog and Athena for querying\n- Daily partitioning by year/month/day in S3\n- Simple schema evolution: nullable fields and catalog versioning\n- Data quality checks: non-empty, key integrity\n\n## Code Example\n\n```python\n# Glue ETL skeleton (conceptual)\nfrom awsglue.context import GlueContext\nfrom awsglue.utils import getResolvedOptions\nfrom awsglue.job import Job\n\n# setup and read CSV -> Parquet logic here\n```\n\n## Follow-up Questions\n\n- How would you test schema evolution with a new column in a CI pipeline?\n- What are the minimal IAM permissions required for the Glue job to read and write?","diagram":null,"difficulty":"beginner","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Hashicorp"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T15:37:42.173Z","createdAt":"2026-01-17T15:37:42.173Z"},{"id":"q-3464","question":"Design a hybrid streaming/batch data pipeline on AWS for a multi-tenant analytics platform. Ingest streaming events from Kinesis and batch exports to S3; normalize to Parquet with per-tenant prefixes and date partitions. Implement upserts on S3 with Apache Hudi (Glue/EMR), enforce per-tenant RBAC via Lake Formation, and manage schema evolution with Glue Data Catalog. Include data quality using Deequ and an end-to-end test strategy?","answer":"Design a hybrid streaming/batch pipeline in AWS for a multi-tenant analytics platform. Ingest streaming events from Kinesis and batch exports to S3; normalize to Parquet with per-tenant prefixes and d","explanation":"## Why This Is Asked\n\nAssesses ability to design a scalable, secure, multi-tenant data lake with hybrid ingestion, upsert semantics, and governance across streaming and batch sources.\n\n## Key Concepts\n\n- Hybrid ingestion (stream + batch)\n- Tenant isolation and RBAC (Lake Formation)\n- Upserts on object storage (Apache Hudi)\n- Parquet with tenant/date partitioning\n- Schema evolution (Glue Catalog/Registry)\n- Data quality checks (Deequ)\n- End-to-end testing strategies\n\n## Code Example\n\n```python\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\ndf = spark.read.json(\"s3://incoming/events/\")\ndf.write.format(\"org.apache.hudi\").options(\n  {\n    \"hoodie.datasource.write.recordkey\": \"event_id\",\n    \"hoodie.datasource.write.table_type\": \"MERGE_ON_READ\",\n    \"hoodie.datasource.write.precombine_field\": \"ts\"\n  }\n).mode(\"append\").save(\"s3://lake/tenant_id/events\")\n```\n\n## Follow-up Questions\n\n- How would you validate schema drift without breaking downstream queries?\n- What metrics and alarms would you set for data freshness and quality?","diagram":"flowchart TD\n  A[Ingest: Kinesis] --> B[Stream to S3 Parquet (tenant/date)]\n  B --> C[Upsert: Apache Hudi (Glue/EMR)]\n  C --> D[Glue Data Catalog + Lake Formation]\n  D --> E[Query: Athena / Redshift Spectrum]\n  E --> F[Quality & Governance]","difficulty":"advanced","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Databricks","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T17:30:00.794Z","createdAt":"2026-01-17T17:30:00.794Z"},{"id":"q-3560","question":"Ingest data from three SaaS feeds arriving in S3 with mixed timestamp formats and time zones. Design a beginner AWS data pipeline that uses AWS Glue to normalize timestamps to UTC, convert to Parquet, partition by ingestion date (YYYY/MM/DD), and catalog with Glue Data Catalog; ensure schema evolution with a new 'deviceId' field; implement a basic data quality check (null counts) and a minimal end-to-end test plan. Include how you'd validate a sample dataset and what tests you’d run at each stage?","answer":"Use an AWS Glue Spark job to read multi-source JSON/CSV files from S3, parse multiple timestamp formats into UTC, normalize to a canonical schema, write Parquet to s3://lake/partner/YYYY/MM/DD/ with partitioning by ingestion date, and configure a Glue Crawler with schema evolution policy for the new 'deviceId' field.","explanation":"## Why This Is Asked\nTests multi-source timestamp normalization, Parquet partitioning strategies, and foundational data quality practices—core competencies for entry-level data engineers working with real-world SaaS integrations.\n\n## Key Concepts\n- Cross-source timestamp normalization to UTC\n- AWS Glue ETL jobs and Data Catalog integration\n- Parquet file format with date-based partitioning\n- Schema evolution via crawler configuration\n- Basic data quality validation and testing frameworks\n\n## Code Example\n```javascript\nfunction normalizeTimestamp(ts, formats) {\n  // Parse multiple timestamp formats\n  for (const format of formats) {\n    const parsed = moment(ts, format, true);\n    if (parsed.isValid()) {\n      return parsed.utc().format();\n    }\n  }\n  throw new Error(`Unable to parse timestamp: ${ts}`);\n}\n```","diagram":"flowchart TD\n  A[Sources: SaaS 1-3] --> B[Glue Job: Normalize]\n  B --> C[Parquet: s3://lake/partner/YYYY/MM/DD]\n  C --> D[Glue Catalog]\n  D --> E[Athena/Quicksight] ","difficulty":"beginner","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","LinkedIn"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T05:45:42.083Z","createdAt":"2026-01-17T21:30:36.259Z"},{"id":"q-3654","question":"Ingest multi-source JSON logs from trading platforms into a centralized data lake on AWS. Platforms evolve schema weekly; events arrive out of order and duplicates exist. Design an end-to-end pipeline using S3, Glue, and Athena that handles schema evolution, deduplication, daily Parquet partitions, and data quality checks. Include testing and cross-region reliability?","answer":"Design a multi-source ingest using Kinesis Data Streams (or MSK) to collect JSON from each platform, de-duplicate by (platform,event_id), and land into S3 as Parquet with daily partitions. Use Glue Sc","explanation":"## Why This Is Asked\nValidates handling varied data sources, schema evolution, and data quality in a real AWS data lake.\n\n## Key Concepts\n- Multi-source ingestion, idempotent dedup, schema evolution, Parquet partitioning\n- Glue Schema Registry, Glue ETL, Data Catalog, Athena\n- End-to-end testing, cross-region reliability, monitoring\n\n## Code Example\n```javascript\n// Placeholder: integration wiring example would reside in infra-as-code\n```\n\n## Follow-up Questions\n- How would you implement exactly-once semantics across regions?\n- Which failure modes require manually replaying data and how to detect them?","diagram":"flowchart TD\n  A[Ingest: KDS/MSK] --> B[Dedup: DynamoDB]\n  B --> C[Glue ETL]\n  C --> D[S3: Parquet]\n  D --> E[Catalog: Glue Data Catalog]\n  E --> F[Query: Athena]","difficulty":"intermediate","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Citadel","Goldman Sachs"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T04:10:41.071Z","createdAt":"2026-01-18T04:10:41.074Z"},{"id":"q-3708","question":"A fintech app emits daily NDJSON logs (nested fields) to S3: s3://transact-logs/TEAM/YYYY/MM/DD/tx.ndjson.gz. Outline a beginner pipeline using AWS Glue to crawl, flatten, and convert to Parquet with daily partitions at s3://lake/fintech/TEAM/YYYY/MM/DD/, cataloged in Glue. Include simple data quality checks (null rates, range checks), how to handle future schema drift (new fields), and a minimal end-to-end test plan with Athena validation?","answer":"A fintech app emits daily NDJSON logs (nested fields) to S3: s3://transact-logs/TEAM/YYYY/MM/DD/tx.ndjson.gz. Outline a beginner pipeline using AWS Glue to crawl, flatten, and convert to Parquet with ","explanation":"Why This Is Asked\n- Tests a practical, end-to-end Glue-based ingestion flow for nested JSON without overcomplication.\n- Checks handling of NDJSON, flattening logic, Parquet partitioning, and Glue Data Catalog registration.\n- Assesses basic data quality checks and a simple schema drift plan suitable for beginners.\n\nKey Concepts\n- AWS Glue Crawlers and ETL\n- NDJSON flattening and Parquet conversion\n- S3 daily partitioning and Glue Catalog integration\n- Athena for validation and schema drift handling\n- Basic data quality checks (null rates, value ranges)\n\nCode Example\n```python\n# Pseudo-Glue ETL skeleton (PySpark)\nfrom awsglue.context import GlueContext\nfrom awsglue.transforms import *\nfrom pyspark.context import SparkContext\nsc = SparkContext()\ngc = GlueContext(sc)\ndf = gc.create_dataframe_from_s3(\"s3://path\", format=\"ndjson\")\nflat = df.flatten()  # flatten nested fields\nparquet = flat.cast(\"decimal(10,2)\", [\"amount\"])  # example cast\nparquet.write(\"s3://lake/fintech/TEAM/YYYY/MM/DD/\", partitionKeys=[\"date\"]) \n```\n\nFollow-up Questions\n- How would you adjust for partial failures across days?\n- What small changes ensure compatibility when a new field appears in the NDJSON?","diagram":"flowchart TD\n  A[Raw NDJSON in S3] --> B[Glue Crawler]\n  B --> C[Glue ETL: flatten + cast]\n  C --> D[Parquet in S3 with daily partitions]\n  D --> E[Glue Catalog]\n  E --> F[Athena validation]","difficulty":"beginner","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Meta","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T06:46:57.266Z","createdAt":"2026-01-18T06:46:57.266Z"},{"id":"q-3725","question":"Beginner-level: Daily multi-tenant event logs arrive as JSON in s3://raw/tenant_id/date/. Design an end-to-end pipeline using S3, AWS Glue (ETL + Data Catalog), and Athena to land data as Parquet with tenant/date partitions, handle evolving schemas with new optional fields, add a lineage table capturing source_path and tenant_id, implement basic data quality checks, and provide a minimal test plan?","answer":"Use S3 as the raw landing, a Glue crawler to create the catalog, and a Glue ETL job (Python) to flatten JSON to a tabular schema, write Parquet to s3://lake/{tenant_id}/{date}/, and generate a lineage","explanation":"## Why This Is Asked\nTests end-to-end AWS data ingestion with cataloging, Parquet, and Athena, plus lineage basics.\n\n## Key Concepts\n- S3 landing, Glue Crawlers, Glue ETL, Parquet, partitioning by tenant/date\n- Glue Data Catalog, Athena\n- Schema evolution with new optional fields\n- Basic data quality checks and lineage tracking\n\n## Code Example\n```python\n# Pseudo Glue ETL outline\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\n# Flatten nested JSON, map fields, and write Parquet\n```\n\n## Follow-up Questions\n- How would you scale for thousands of tenants?\n- How would you handle late-arriving data and schema drift?","diagram":"flowchart TD\n  RawLogs[s3://raw/tenant/date] --> Crawler[Crawler creates Catalog]\n  Crawler --> ETL[Glue ETL -> Parquet]\n  ETL --> Lake[s3://lake/tenant/date]\n  Lake --> Athena[Athena/Quicksight]","difficulty":"beginner","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Hashicorp"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T07:27:17.689Z","createdAt":"2026-01-18T07:27:17.691Z"},{"id":"q-3912","question":"Land daily CSVs via AWS Transfer Family to s3://reports/raw/YYYY/MM/DD/report.csv.gz. Glue Spark job reads, enforces types, writes Parquet to s3://reports/curated/YYYY/MM/DD/report.parquet partitioned by date. Use Glue Crawler for catalog; handle schema evolution for a new 'region' column. Data quality: row count >0 and non-null keys. End-to-end test with sample file and a quick Athena query?","answer":"Land daily CSVs via AWS Transfer Family to S3 raw, then a Glue Spark job to cast types, write Parquet to curated with date partitions. Crawlers update the Data Catalog; enabled schema evolution to acc","explanation":"## Why This Is Asked\nTests practical ingestion from an SFTP-like source using AWS Transfer Family, plus ETL, partitioned Parquet, and catalog management with schema evolution. It also touches basic data quality and end-to-end testing.\n\n## Key Concepts\n- AWS Transfer Family for secure file landing\n- AWS Glue ETL (Spark) for transformation\n- Parquet with partitioning by date\n- Glue Data Catalog and schema evolution\n- Data quality checks and end-to-end testing\n\n## Code Example\n```python\nimport sys\nfrom awsglue.context import GlueContext\nfrom pyspark.context import SparkContext\nsc = SparkContext()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\n\n# Read CSV from raw location\ndf = spark.read.option(\"header\", \"true\").csv(\"s3://reports/raw/YYYY/MM/DD/report.csv.gz\")\n# Simple type casting could be added here\n\n# Write to curated with date partitions\ndf.write.partitionBy(\"year\", \"month\", \"day\").parquet(\"s3://reports/curated/YYYY/MM/DD/report.parquet\")\n```\n\n## Follow-up Questions\n- How would you validate idempotency and handle re-ingestion of the same file?\n- What failure modes exist with Transfer Family and how would you monitor them?","diagram":"flowchart TD\n  A[Ingest] --> B[Parse CSV]\n  B --> C[Write Parquet]\n  C --> D[Catalog Update]\n  D --> E[Athena/Queryability]","difficulty":"beginner","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","Lyft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T14:45:26.659Z","createdAt":"2026-01-18T14:45:26.659Z"},{"id":"q-4017","question":"Context: Three vendors publish daily JSON files to S3 at s3://fin-data/vendorX/YYYY/MM/DD/data.json.gz. Design a compliant analytics pipeline that preserves data lineage and field-level provenance, supports schema evolution, and enables time-travel queries. Propose end-to-end design using AWS Glue, Glue Data Catalog, Lake Formation, Athena/Redshift Spectrum, and CloudTrail. Include governance, upsert strategy, and tests?","answer":"Use Glue to read the daily JSON, enable schema evolution, and write Parquet partitioned by vendor/date to S3; register tables in the Glue Data Catalog; enforce fine-grained access with Lake Formation;","explanation":"## Why This Is Asked\n\nTests the ability to build a governance-aware, scalable data pipeline with lineage and schema evolution in AWS. It also probes knowledge of Lake Formation permissions, Glue Data Catalog, and test strategies for drift and upserts.\n\n## Key Concepts\n\n- AWS Glue ETL and Data Catalog\n- Lake Formation governance and fine-grained permissions\n- Data lineage and field-level provenance\n- Schema evolution and time-travel with Athena/Redshift Spectrum\n- CloudTrail data events and job bookmarks\n\n## Code Example\n\n```javascript\n// Skeleton AWS Glue-like ETL job structure\nfunction runGlueJob(event) {\n  // read JSON from S3, infer schema, write Parquet, partition by vendor/date\n}\n```\n\n## Follow-up Questions\n\n- How would you implement field-level lineage for a new field in a downstream table?\n- How would you test schema drift and upsert correctness end-to-end?","diagram":"flowchart TD\n  A[Vendor JSON in S3] --> B[Glue ETL Job]\n  B --> C[Parquet in S3 (vendor/date)]\n  C --> D[Glue Data Catalog]\n  D --> E[Athena/Redshift Spectrum]\n  G[Lake Formation governance] --> E\n  H[CloudTrail lineage] --> B\n  E --> F[Analytics]","difficulty":"intermediate","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","IBM","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T19:34:41.671Z","createdAt":"2026-01-18T19:34:41.672Z"},{"id":"q-4060","question":"You’re building a beginner-level data ingestion workflow for a fintech telemetry app. Daily JSON logs land in S3; design a pipeline using AWS Glue and Athena that ingests to Parquet with daily partitions, supports simple schema evolution, and emits a lightweight data-lineage record (source, transform, destination) to a separate S3 bucket. Include a basic test plan and cost considerations?","answer":"Use an AWS Glue Spark job to read JSON logs from s3://telemetry/logs/YYYY/MM/DD, convert them to Parquet format with daily partitioning, and write to s3://telemetry/lake/YYYY/MM/DD. Register the resulting table in the Glue Data Catalog for Athena querying. Enable schema evolution using dynamic frame resolution and mergeSchema options to handle changing log structures. Emit lightweight lineage records as JSON files to s3://telemetry/lineage/, capturing source path, transformation timestamp, and destination location. Schedule the pipeline using Glue triggers or Step Functions for daily execution.","explanation":"## Why This Is Asked\n\nThis question evaluates practical AWS Glue implementation skills, Parquet partitioning strategies, and lightweight data lineage mechanisms essential for fintech data governance and compliance.\n\n## Key Concepts\n\n- AWS Glue Spark ETL jobs for data transformation\n- Parquet format with daily partitioning for optimal query performance\n- Schema evolution using dynamic frames to handle changing data structures\n- Glue Data Catalog integration with Athena for serverless querying\n- Simple lineage tracking to S3 for auditability and governance\n\n## Code Example\n\n```python\n# Glue ETL script skeleton\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\nfrom awsglue.transforms import *\nfrom pyspark.context import SparkContext\n\nsc = SparkContext()\nglue_context = GlueContext(sc)\nspark = glue_context.spark_session\n\n# Read JSON logs with schema evolution\ndynamic_frame = glue_context.create_dynamic_frame.from_options(\n    connection_type=\"s3\",\n    connection_options={\"paths\": [\"s3://telemetry/logs/\"], \"recurse\": True},\n    format=\"json\"\n)\n\n# Apply transformations and write to Parquet\napply_mapping = ...\n```\n\n## Test Plan\n\n1. Validate JSON parsing with sample log files\n2. Test Parquet output structure and partitioning\n3. Verify schema evolution with modified log formats\n4. Confirm Athena query performance on partitioned data\n5. Validate lineage record generation and S3 delivery\n\n## Cost Considerations\n\n- Glue job runtime: $0.44 per DPU-hour\n- Athena querying: $5 per TB scanned\n- S3 storage: Parquet compression reduces costs vs JSON\n- Data Catalog: Free for first 1M objects, then $1 per 100K objects\n\nOptimize costs by using columnar format, partition pruning, and appropriate Glue worker sizing.","diagram":"flowchart TD\n  A[S3: telemetry/logs] --> B[Glue Ingest Job]\n  B --> C[Parquet Lake: telemetry/lake/YYYY/MM/DD]\n  C --> D[Glue Data Catalog]\n  B --> E[Lineage Ledger: s3://telemetry/lineage/]","difficulty":"beginner","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T05:57:32.991Z","createdAt":"2026-01-18T21:50:21.455Z"},{"id":"q-4078","question":"Design an end-to-end AWS ingestion and governance pipeline for a multi-tenant SaaS data lake. Data from partners arrives as streaming JSON/Avro; land in S3 as Parquet with daily partitions; govern across accounts using Lake Formation; support schema evolution, late-arriving data, and Athena time-travel queries. Outline architecture, formats, partitioning, quality checks, and testing strategy?","answer":"Organize data by partner under a unified lake structure (s3://lake/partner/YYYY/MM/DD) and implement cross-account governance using Lake Formation. Ingest streaming data via Kinesis Firehose, converting to Parquet format for S3 storage, and manage metadata through the Glue Catalog with schema evolution support.","explanation":"## Why This Is Asked\nTests ability to design a scalable, governed, multi-tenant data lake with schema evolution, late-arrival handling, and cross-account control.\n\n## Key Concepts\n- Lake Formation governance across accounts\n- Parquet columnar storage with per-tenant partitions\n- Glue Catalog + Glue Schema Registry for evolving schemas\n- Late-arriving data handling and partition replay\n- Athena time-travel queries and data quality checks\n\n## Code Example\n```javascript\nfunction validate(record) {\n  if (!record.partner_id || !record.event_time) return false;\n  return true;\n}\n```\n\n## Follow-up Questions","diagram":null,"difficulty":"intermediate","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["OpenAI","PayPal","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T05:52:02.627Z","createdAt":"2026-01-18T22:48:55.537Z"},{"id":"q-4098","question":"A two-account AWS data lake receives a daily CSV telemetry feed from a 3rd-party vendor into s3://telemetry/vendorX/YYYY/MM/DD/data.csv.gz. Design an end-to-end pipeline to ingest, clean, and store as Parquet in the data lake, with per-tenant isolation, schema evolution for new columns, basic data quality checks, and time-travel via Athena. Include services, formats, partitioning, governance, and a testing plan?","answer":"To satisfy these requirements: Use a two-account Lake Formation data lake. Ingest daily CSVs with a Glue ETL job (Python/DynamicFrame) that reads CSV, applies drift-tolerant schema evolution (merges new columns into existing schema), performs data quality validation, and writes partitioned Parquet files. Implement per-tenant isolation using Lake Formation permissions and S3 prefix-based partitioning. Enable time-travel queries via Athena with versioned Parquet files and proper table configuration.","explanation":"## Why This Is Asked\nDesigning cross-account data ingestion with schema evolution and per-tenant isolation tests production readiness for a real data lake.\n\n## Key Concepts\n- AWS Lake Formation, Glue ETL, DynamicFrame, Parquet, Athena\n- Schema evolution and drift detection\n- Tenant isolation, access controls\n- Data quality checks, test strategy\n\n## Code Example\n```python\n# Pseudo Glue script: merge schema with new columns\nfrom awsglue.dynamicframe import DynamicFrame\n# Load CSV, infer/merge with existing schema, and write Parquet\n```\n\n## Follow-up Questions\n- How would you handle late-arriving","diagram":"flowchart TD\n  A(Source CSV) --> B(Glue ETL Job)\n  B --> C(S3 Parquet with date partition)\n  C --> D(Athena catalogs)\n  D --> E(Per-tenant access via Lake Formation)","difficulty":"intermediate","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Plaid","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T05:33:34.504Z","createdAt":"2026-01-18T23:40:36.615Z"},{"id":"q-4186","question":"Design an end-to-end ingestion pipeline for hourly encrypted ORC files from a third-party source that requires upsert-like behavior with deletes. Implement schema evolution, partitioning by hour, and cross-account access using AWS Glue Data Catalog, Lake Formation, S3, Athena, and optional DynamoDB for offset tracking. Include testing, backfill strategy, and rollback plans?","answer":"Set up an hourly ETL: decrypt ORC, normalize to Parquet, and write to S3 with hourly partitions. Track lastProcessedHour in DynamoDB; emit tombstones for deletes to propagate removals. Use Glue for ca","explanation":"## Why This Is Asked\n\nTests handling of incremental, tombstone-aware ingestion, schema drift, cross-account governance, and robust testing in a scalable AWS data lake.\n\n## Key Concepts\n\n- Tombstones for deletes and upserts in a lake\n- Hourly partitioning and offset tracking\n- Schema evolution with Glue and registry\n- Lake Formation cross-account access and fine‑grained permissions\n- End-to-end testing including backfills and rollback plans\n\n## Code Example\n\n```javascript\n// Pseudo-code: read offset, decrypt, write Parquet hourly, handle tombstones\nconst last = db.get(\"offset\");\nconst files = listOrcFilesSince(last);\nfor (const f of files) {\n  const data = decrypt(read(f));\n  const upserted = transformToParquet(data);\n  s3.putParquet(upserted, `hourly/${f.hour}`);\n  if (data.deleted) tombstonePut(f.id);\n  db.set(\"offset\", f.hour);\n}\n```\n\n## Follow-up Questions\n\n- How would you detect and handle schema drift across multiple source versions?\n- How would you test rollback and backfill scenarios across accounts?","diagram":null,"difficulty":"advanced","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Databricks","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T07:02:45.437Z","createdAt":"2026-01-19T07:02:45.437Z"},{"id":"q-4444","question":"Design a cross-account, multi-region ingestion and governance pipeline: a partner app streams JSON to Kinesis in Account A; use Kinesis Firehose to land Parquet in S3 with daily partitions; Glue ETL masks PII via Lake Formation permissions and handles schema evolution; grant Account B analytics access via trusted roles; include end-to-end tests, dedupe, backfill, and cost controls?","answer":"Design a cross-account, multi-region ingestion and governance pipeline: a partner app streams JSON to Kinesis in Account A; use Kinesis Firehose to land Parquet in S3 with daily partitions; Glue ETL m","explanation":"Why This Is Asked\nTests ability to architect a secure, scalable data lake with cross-account access, schema evolution, and data masking at scale.\n\nKey Concepts\n- Cross-account IAM roles and trust policies\n- Lake Formation column-level permissions and data masking\n- Glue ETL for masking, Parquet storage, and schema evolution\n- Kinesis Firehose as ingestion to S3 with daily partitions\n- Deduplication, backfill, and cost controls (lifecycle policies, data tiering)\n\nCode Example\n```python\n# PySpark dedupe example (conceptual)\ndf = df.dropDuplicates([\"event_id\"])  # ensure idempotent upserts before write\n```\n\nFollow-up Questions\n- How would you validate cross-account access in a non-prod environment?\n- What metrics would you monitor to catch schema drift and data masking failures?","diagram":"flowchart TD\n  A[Partner App] --> B[Kinesis in Account A]\n  B --> C[Firehose to S3 Parquet]\n  C --> D[Glue ETL + Lake Formation]\n  D --> E[Catalog & Daily Partitions]\n  E --> F[Account B Analytics]\n","difficulty":"advanced","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Microsoft","PayPal","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T19:05:51.441Z","createdAt":"2026-01-19T19:05:51.441Z"},{"id":"q-4592","question":"Scenario: A SaaS vendor streams daily JSON events into a central S3 data lake across accounts. Data arrives with nested fields and occasional new keys. Design an end-to-end ingestion and governance plan to land data as Apache Iceberg tables in S3 (partitioned by event_date), with schema evolution via Glue Schema Registry, and upserts for dedup by (customer_id, event_id). Include ingestion path, late-arriving data handling, testing plan, monitoring, and trade-offs between Iceberg vs Glue tables?","answer":"Design a pipeline where JSON events are ingested into S3 as Apache Iceberg tables partitioned by event_date, with schema evolution via Glue Schema Registry and upserts via MERGE on (customer_id, event_id). The pipeline includes Kinesis Data Streams for real-time ingestion, AWS Lambda for preprocessing, and Glue ETL jobs for batch processing. Late-arriving data is handled through watermarking and reprocessing windows, with data quality checks using AWS Deequ and monitoring via CloudWatch.","explanation":"## Why This Is Asked\nTests real-world proficiency with modern data lakes, schema evolution, and upsert workflows, plus governance and reliability considerations.\n\n## Key Concepts\n- Apache Iceberg on S3 for time travel and upserts\n- Glue Schema Registry for evolving schemas\n- MERGE semantics for deduplication\n- Late-arriving data handling with watermark and backfill windows\n- Data quality checks and observability (CloudWatch, Lake Formation)\n\n## Code Example\n```python\n# PySpark pseudo-code: read, write to Iceberg, and perform MERGE-like upsert\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder \\\n    .appName('EventIngestion') \\\n    .config('spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions') \\\n    .config('spark.sql.catalog.spark_catalog', 'org.apache.iceberg.spark.SparkCatalog') \\\n    .getOrCreate()\n\n# Read from schema registry and write to Iceberg\ndf = spark.read.format('json').load('s3://raw-events/')\ndf.write.format('iceberg').partitionBy('event_date').save('s3://processed-events/events')\n\n# MERGE for upserts\nspark.sql(\"\"\"\n    MERGE INTO events AS target\n    USING new_events AS source\n    ON target.customer_id = source.customer_id \n       AND target.event_id = source.event_id\n    WHEN MATCHED THEN UPDATE SET *\n    WHEN NOT MATCHED THEN INSERT *\n\"\"\")\n```","diagram":"flowchart TD\n  Ingest[Ingest JSON] --> Validate[Schema validation]\n  Validate --> Load[Write to Iceberg]\n  Load --> Upsert[MERGE by key]\n  Upsert --> Quality[Data quality checks]\n  Quality --> Monitor[Observability]","difficulty":"intermediate","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Hugging Face","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T04:31:33.081Z","createdAt":"2026-01-20T02:50:21.749Z"},{"id":"q-4618","question":"You manage a beginner-level AWS data lake for a daily JSON feed arriving into s3://fin-data/raw/apiX/YYYY/MM/DD/*.json. Design a cost-conscious pipeline to convert to Parquet, partition by date and product, and catalog in Glue. Add basic data quality checks (nulls, data types) and a schema evolution plan for added fields. Include a monitoring/alert plan using CloudWatch/SNS for missing daily files or failed runs?","answer":"Use Glue Crawlers to populate the Data Catalog, ETL with Glue Studio (Python) to convert raw JSON to Parquet in s3://fin-data/processed/apiX/YYYY/MM/DD, partition by date and product, and query via At","explanation":"## Why This Is Asked\nThis tests practical data ingestion and governance basics for beginners in a real-world lake.\n\n## Key Concepts\n- Glue Crawlers, Glue ETL, Parquet, partitioning\n- Data quality checks, schema evolution\n- CloudWatch/SNS for monitoring, end-to-end testing\n\n## Code Example\n```javascript\n// Example: quick data quality check sketch\nfunction checkNullRate(records){\n  const total = records.length;\n  const nulls = records.filter(r=> Object.values(r).includes(null)).length;\n  return (nulls/total) < 0.05;\n}\n```\n\n## Follow-up Questions\n- How would you test schema evolution with a new optional field?\n- How would you adapt this for multi-region replication and data freshness?","diagram":"flowchart TD\n  A[Raw JSON in s3://fin-data/raw/apiX/...] --> B[Glue Crawler/Data Catalog]\n  B --> C[Glue ETL: JSON -> Parquet in s3://fin-data/processed/apiX/YYYY/MM/DD]\n  C --> D[Partition: date/product]\n  D --> E[Athena for queries]\n  E --> F[CloudWatch metrics]\n  F --> G[SNS alerts for missing days or failures]","difficulty":"beginner","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T04:37:11.945Z","createdAt":"2026-01-20T04:37:11.945Z"},{"id":"q-4797","question":"New SaaS nightly export: a gzipped CSV lands in s3://cust-data/raw/saasBeta/YYYY/MM/DD/export.csv.gz. The schema may gain a new column over time (eg region). Design a beginner-level, cost-conscious AWS data ingestion pipeline that lands raw data, converts to Parquet, partitions by date, and catalogs in Glue. Include a schema evolution strategy, basic data quality checks (nulls, types, date parsing), and a test plan for end-to-end verification with alerts via CloudWatch/SNS?","answer":"Use a serverless pipeline: raw CSV.gz lands in s3://cust-data/raw/saasBeta/YYYY/MM/DD/export.csv.gz. A Glue ETL job reads it, applies a defined schema (mergeSchema), and writes Parquet to s3://cust-da","explanation":"## Why This Is Asked\n\nTests serverless ingestion, schema evolution, and basic data quality in a realistic SaaS feed. It also checks how candidates design end-to-end testing and monitoring with AWS services.\n\n## Key Concepts\n\n- AWS Glue ETL (Python/Spark)\n- Parquet with date partitioning\n- Glue Data Catalog and mergeSchema for evolving schemas\n- Data quality checks (nulls, types, date parsing)\n- Event-driven testing and CloudWatch/SNS alerts\n\n## Code Example\n\n```python\n# Pseudo-Glue ETL outline\nfrom awsglue.context import GlueContext\nfrom awsglue.transforms import *\n# ... load CSV.gz, apply schema, write Parquet with dynamic schema merge\n```\n\n## Follow-up Questions\n\n- How would you handle schema drift if 80% of the fields stay the same but types vary?\n- What changes would you make to support real-time ingestion later?","diagram":null,"difficulty":"beginner","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T13:19:30.660Z","createdAt":"2026-01-20T13:19:30.660Z"},{"id":"q-4835","question":"Design an end-to-end pipeline to ingest CDC updates from three operational databases (MySQL, PostgreSQL, DynamoDB) into an S3 data lake as Parquet, using Apache Iceberg for upserts and deletes. Include partitioning strategy, schema evolution, late data handling, compaction, and multi-region replication with Glue Data Catalog and Lake Formation permissions. Compare against Delta/Lakehouse trade-offs?","answer":"Use Apache Iceberg on S3 to enable atomic upserts/deletes. Ingest CDC from MySQL, PostgreSQL, and DynamoDB via Spark Structured Streaming (on EMR or Glue 4.x) into an Iceberg table. Partition by date ","explanation":"## Why This Is Asked\nTests advanced data-lake correctness with upserts, schema evolution, and multi-region security. It also probes trade-offs between Iceberg and Delta formats in AWS.\n\n## Key Concepts\n- CDC integration across heterogeneous sources\n- Upserts/deletes on object stores via Iceberg\n- Schema evolution, partitioning, and compaction policies\n- Lake Formation permissions and cross-region replication\n\n## Code Example\n```javascript\n// Pseudo-Spark/Iceberg write (illustrative)\ndf.write\n  .format(\"iceberg\")\n  .mode(\"append\")\n  .partitionBy(\"dt\", \"source\")\n  .save(\"s3://bucket/iceberg_db/events\")\n```\n\n## Follow-up Questions\n- How would you monitor CDC lag and data quality across regions?\n- What 전략 would you use if a new field is added with a nested type?","diagram":"flowchart TD\n  A[CDC sources: MySQL, PostgreSQL, DynamoDB] --> B[Spark Structured Streaming] \n  B --> C[S3: iceberg_db/events Parquet] \n  C --> D[Glue Catalog / Iceberg] \n  D --> E[Athena/Quicksight]\n  E --> F[Security: Lake Formation + IAM roles]","difficulty":"advanced","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Apple","Instacart"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T15:41:08.851Z","createdAt":"2026-01-20T15:41:08.852Z"},{"id":"q-4929","question":"A new data source emits daily JSON events with nested arrays and optional fields, arriving to s3://data/lake/raw/sourceB/YYYY/MM/DD/.json. Design a beginner-level ingestion pipeline to flatten the JSON, store as Parquet in s3://data-lake/sourceB/YYYY/MM/DD/, partition by date, and register in the Glue Data Catalog. How would you handle a schema evolution for added fields, implement basic data quality checks, and create a minimal end-to-end test plan?","answer":"Use S3 raw -> AWS Glue Spark job (Python) to flatten JSON, write Parquet to s3://data-lake/sourceB/YYYY/MM/DD/, partition by date; register in Glue Data Catalog; implement schema evolution by allowing","explanation":"## Why This Is Asked\nTests flattening of nested JSON, schema drift handling, and end-to-end data flow basics for a beginner.\n\n## Key Concepts\n- Glue ETL with Spark for JSON normalization\n- Parquet, partitioning by date\n- Glue Data Catalog, schema evolution strategy\n- Basic data quality checks and end-to-end validation\n\n## Code Example\n```python\n# pseudo: read json, flatten, write parquet with partitioning\n```\n\n## Follow-up Questions\n- How would you handle very large nested arrays to avoid data skew?\n- How would you validate idempotency and deduplication during re-ingest?","diagram":"flowchart TD\n  A[Raw JSON in S3] --> B[Glue ETL Job Flatten]\n  B --> C[Parquet in S3 (partition by date)]\n  C --> D[Glue Catalog]\n  D --> E[Athena/BI Queries]","difficulty":"beginner","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","NVIDIA","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T20:20:38.336Z","createdAt":"2026-01-20T20:20:38.338Z"},{"id":"q-5191","question":"Scenario: A new data source streams JSON logs into Kinesis Data Streams at high velocity. Design an end-to-end pipeline to land enriched data in S3 as Parquet with daily partitions, register in Glue Data Catalog, support schema evolution, and enable time-travel queries. Include enrichment, idempotency, data quality checks, testing plan, and governance controls (encryption, access)?","answer":"Use Kinesis Data Analytics to enrich JSON streams, then write Parquet to S3 with daily partitions and register in Glue Catalog. Leverage Glue Schema Registry for evolution; implement idempotent upsert","explanation":"## Why This Is Asked\nAssesses ability to design a scalable streaming ingestion with enrichment, schema evolution, and governance in AWS.\n\n## Key Concepts\n- Streaming ingestion (Kinesis Data Streams, Kinesis Data Analytics)\n- Parquet on S3 with partitioning\n- Glue Data Catalog and Iceberg table\n- Schema evolution and data quality\n- Idempotent upserts and testing\n- Governance (Lake Formation, KMS)\n\n## Code Example\n```javascript\n// Pseudo Spark Structured Streaming (illustrative)\nconst df = spark.readStream.format(\"kinesis\").option(\"streamName\",\"...\").load()\nval enriched = df.selectExpr(\"CAST(data AS STRING) as json\").transform(parseAndEnrich)\nenriched.writeStream.format(\"iceberg\").option(\"path\",\"s3://data-lake/partner/iceberg\").start()\n```\n\n## Follow-up Questions\n- How would you handle late-arriving data and duplicate IDs?\n- How would you test schema evolution across multiple partitions?\n","diagram":null,"difficulty":"intermediate","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["NVIDIA","Robinhood","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T10:15:57.315Z","createdAt":"2026-01-21T10:15:57.315Z"},{"id":"q-5278","question":"You operate a high-throughput ecommerce platform emitting JSON order events to Kinesis Data Streams. Design an end-to-end AWS data platform that ingests, stores, and analyzes these events in real time and in batch. Requirements: upserts and time travel in S3 (choose Apache Hudi or Iceberg), schema evolution via Glue Schema Registry, late-arrival handling, data quality checks, centralized cataloging, and both real-time dashboards (Athena/QuickSight) and batch analytics; include testing and rollback strategy?","answer":"Use Kinesis Data Streams with a Flink app (KDA) for streaming enrichment, writing upserts to S3 via Apache Hudi (or Iceberg) to enable time travel and current views. Glue Schema Registry manages evolv","explanation":"## Why This Is Asked\nAssesses ability to design a scalable, maintainable pipeline supporting upserts, schema evolution, late data, and observability in AWS.\n\n## Key Concepts\n- Streaming + Lakehouse on S3 with Hudi/Iceberg for upserts and time travel\n- Glue Schema Registry for evolving schemas; Glue Data Catalog\n- Late-arrival handling with watermarks and lateness tolerances\n- Data quality checks via Glue Data Quality or Deequ\n- Observability and testing: canary deployments, cross-region failover\n\n## Code Example\n```javascript\n// Pseudo-configuration for upsert job\n{\n  \"hoodie.datasource.write.operation\": \"upsert\",\n  \"hoodie.datasource.write.recordkey\": \"order_id\",\n  \"hoodie.datasource.write.table.name\": \"orders\",\n  \"hoodie.upsert.shuffle.use.last.commit\": true\n}\n```\n\n## Follow-up Questions\n- How would you handle schema drift if a field type changes?\n- How would you validate data quality prior to dashboards and rollback plans?","diagram":null,"difficulty":"advanced","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","MongoDB","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T14:53:10.873Z","createdAt":"2026-01-21T14:53:10.873Z"},{"id":"q-5315","question":"Scenario: Global retailer emits orders and real-time pricing events into two AWS accounts across multiple regions. Design an end-to-end ingestion and governance pipeline that lands data in S3 as Parquet with daily partitions, registers in Glue Data Catalog (preferably Iceberg/Parquet), enforces strict access via Lake Formation, and supports schema evolution and data quality checks. Include services, data formats, partition strategy, and how you would validate end-to-end?","answer":"Multi-region, multi-account design using Kinesis Data Streams for real-time and Glue Spark jobs for batch. Raw data lands in S3 in Parquet with daily partitions; Glue Catalog (Iceberg tables) enforces","explanation":"## Why This Is Asked\n\nAssesses ability to design cross-account, governed ingestion with real-time and batch components, schema evolution, and testability.\n\n## Key Concepts\n- Multi-account governance and cross-region data movement\n- Parquet + Iceberg in Glue Catalog\n- Lake Formation for access control and masking\n- Real-time (Kinesis) + batch ETL (Glue)\n- Data quality checks and end-to-end testing\n\n## Code Example\n```javascript\n// Pseudo-code: basic schema evolution handling\nconst df = readParquet('s3://raw/orders/')\nconst evolved = df.withColumn('order_total', df['order_total'].cast('double'))\nwriteParquet(evolved, 's3://warehouse/orders/', {partitionBy: ['order_date']})\n```\n\n## Follow-up Questions\n- How would you monitor schema drift across regions?\n- How would you test partial failures and retries in the streaming path?","diagram":"flowchart TD\n  A[Source Systems] --> B[Kinesis Data Streams]\n  B --> C[S3 Raw Landing]\n  C --> D[Glue ETL]\n  D --> E[Parquet in S3 with daily partitions]\n  E --> F[Glue Catalog (Iceberg)]\n  F --> G[Analytics: Athena/Redshift Spectrum]","difficulty":"intermediate","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Microsoft","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T16:09:12.136Z","createdAt":"2026-01-21T16:09:12.136Z"},{"id":"q-5439","question":"Design an end-to-end ingestion pipeline that ingests JSON events from three SaaS vendors into an S3 data lake, upserts them into a unified Parquet dataset partitioned by date and vendor using AWS Glue and Apache Hudi, supports per-vendor schema evolution, enforces access with Lake Formation, and includes a robust testing plan for late-arriving data?","answer":"Leverage AWS Glue Spark jobs with Apache Hudi to upsert JSON events from three SaaS vendors into a single Parquet dataset in S3, partitioned by date and vendor. Use Glue Data Catalog and Lake Formatio","explanation":"## Why This Is Asked\n\nTests ability to design an end-to-end AWS data pipeline that handles upserts, multi-vendor schema evolution, and strict governance in a realistic, multi-tenant setting. Emphasizes lineage, testing, and late-arrival handling beyond basic ingestion.\n\n## Key Concepts\n\n- Apache Hudi on S3 for upserts and time-travel queries\n- Glue Spark jobs orchestrating ETL with a raw → curated data lake pattern\n- Partitioning by date and vendor for performance and governance\n- Lake Formation for fine-grained access control per vendor/tenant\n- Schema evolution per source, with validation and minimal downtime\n- Data quality checks and end-to-end test strategy including late-arriving data\n\n## Code Example\n\n```python\n# PySpark (Glue) - simplified illustration\nfrom pyspark.context import SparkContext\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.getOrCreate()\nraw = spark.read.json(\"s3://lake/raw/vendorA/2026-01-01/*.json\")\n# transformations omitted for brevity\ncurated_path = \"s3://lake/curated/vendorA/date_partition\"\nraw.write.format(\"hudi\").options(\n  path=curated_path,\n  table_name=\"vendorA_events\",\n  recordKey=\"event_id\",\n  preCombineField=\"event_ts\",\n  HoodieWriteConfigReverse=true\n).mode(\"append\").save()\n```\n\n## Follow-up Questions\n\n- How would you test schema evolution across vendors without breaking historical queries?\n- What monitoring would you implement to detect schema drift and late-arriving data anomalies?","diagram":"flowchart TD\n  A[S3 raw events] --> B[Glue Spark (Hudi) upsert]\n  B --> C[S3 curated data lake]\n  C --> D[Athena/Redshift Spectrum]\n  subgraph Governance\n    E[Lake Formation access controls]\n  end\n  C --> E","difficulty":"intermediate","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Robinhood","Square","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T22:36:03.110Z","createdAt":"2026-01-21T22:36:03.110Z"},{"id":"q-5538","question":"In a daily multi-tenant JSON feed, design an end-to-end ingestion pipeline using AWS data services to land data into S3 as Parquet with daily partitions and per-tenant isolation. Include: partitioning strategy (date + tenant_id), Glue ETL to normalize and handle schema evolution, catalog registration, basic data quality checks, and a test plan. Also specify access controls (Lake Formation/IAM) per tenant?","answer":"Design an end-to-end pipeline for a daily multi-tenant JSON feed. Ingest to S3 with partitions by date and tenant_id, convert to Parquet, and register in the Glue Data Catalog. Use Glue ETL to normali","explanation":"## Why This Is Asked\nAssesses practical data ingestion for multi-tenant data lakes, with governance and basic quality checks.\n\n## Key Concepts\n- Partitioning strategy: date + tenant_id\n- Glue ETL: normalization, Parquet emission\n- Schema evolution: catalog versioning, add-field handling\n- Data quality: nulls, types, ranges\n- Access control: IAM/Lake Formation per tenant\n\n## Code Example\n```javascript\n// Pseudo-logic: not required in real environment\n```\n\n## Follow-up Questions\n- How would you automate schema drift detection across daily runs?\n- How would you test tenant isolation at scale?","diagram":"flowchart TD\n  S[Daily JSON source] --> I[Ingest to s3/raw/tenant/date/Parquet]\n  I --> C[Glue ETL: normalize and Parquet]\n  C --> Cat[Glue Data Catalog]\n  Cat --> A[Access control: Lake Formation per tenant]\n  A --> QA[Quality checks and alerts]","difficulty":"beginner","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Goldman Sachs"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T04:37:56.251Z","createdAt":"2026-01-22T04:37:56.251Z"},{"id":"q-5587","question":"You run a multi-tenant SaaS that writes per-tenant JSON events to S3 at s3://saas-events/raw/{tenantId}/{YYYY}/{MM}/{DD}. Design a beginner–level pipeline to flatten and convert to Parquet, land in s3://saas-events/warehouse/{tenantId}/{YYYY}/{MM}/{DD}/, and catalog in Glue. Include partitioning by tenant and date, simple data quality checks, a schema evolution plan for new fields, a lightweight test plan, and tenant isolation in the IAM/Data Catalog?","answer":"Outline a practical solution: use a Spark-based AWS Glue job to read s3://saas-events/raw/{tenantId}/{YYYY}/{MM}/{DD}.json, flatten nested fields, write Parquet to s3://saas-events/warehouse/{tenantId","explanation":"## Why This Is Asked\n\nTests ability to design a tenant-isolated, cost-conscious data pipeline using standard AWS tools, focusing on beginner-level ETL, data quality, and cataloging.\n\n## Key Concepts\n\n- Per-tenant partitioning and isolation in S3 and Glue\n- Glue Spark (Python/Scala) ETL to flatten JSON and write Parquet\n- Glue Data Catalog integration and partitioning\n- Simple data quality checks and schema evolution (nullable new fields)\n- Lightweight end-to-end test plan with synthetic data\n\n## Code Example\n\n```python\n# PySpark-like pseudocode for ETL\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, explode, from_json\n\nspark = SparkSession.builder.getOrCreate()\n# read per-tenant data placeholder\ndf = spark.read.json(\"s3://saas-events/raw/*/*/*/*.json\")\n# flatten example (adjust to actual schema)\ndf_flat = df.select(\"tenantId\", \"eventTime\", \"payload.*\")\n# write partitioned Parquet\nout_path = \"s3://saas-events/warehouse/\" + \"${tenantId}\" + \"/\" + \"${YYYY}\" + \"/\" + \"${MM}\" + \"/\" + \"${DD}\" + \"/\"\ndf_flat.write.partitionBy(\"tenantId\", \"date\").parquet(out_path)\n```\n\n## Follow-up Questions\n\n- How would you test late-arriving data and re-partition crawls?\n- How do you enforce tenant isolation in IAM and Glue Catalog?\n- What would you monitor for failures (alerts, retries, backfills)?","diagram":"flowchart TD\n  A[Tenant data event] --> B[Ingestion Job]\n  B --> C[Flatten & Parquet]\n  C --> D[Partitioned Parquet in warehouse]\n  D --> E[Glue Data Catalog & IAM controls]","difficulty":"beginner","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Airbnb","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T07:35:55.388Z","createdAt":"2026-01-22T07:35:55.389Z"},{"id":"q-5727","question":"In a multi-region data lake where thousands of IoT devices publish telemetry via AWS IoT Core, design an end-to-end streaming ingestion pipeline that lands Parquet-encoded data in S3 with daily partitions, uses Apache Hudi for upserts and time-travel, registers metadata in the Glue Data Catalog, handles late-arriving data and schema evolution (new fields), enforces data-quality checks, and supports cross-account access with least-privilege IAM and Lake Formation policies. Outline services, data formats, partitioning, upgrade path, testing, and rollback?","answer":"Route IoT Core data to Kinesis Firehose, writing Parquet to s3://lake/iot/... partitions by date. Enable Apache Hudi on an EMR Spark job to upsert and provide time-travel, with Glue Data Catalog integ","explanation":"## Why This Is Asked\nTests designing a real-world streaming ingestion with upserts, schema evolution, and cross-account access in AWS.\n\n## Key Concepts\n- AWS IoT Core, Kinesis Firehose, Parquet, S3\n- Apache Hudi, EMR Spark, Glue Data Catalog\n- Schema evolution, data quality checks (Deequ), idempotency\n- Lake Formation, fine-grained IAM, cross-account access\n\n## Code Example\n```javascript\n// Pseudocode: Hudi upsert config on Spark\nval df = spark.read.format(\"parquet\").load(src)\ndf.write.format(\"hudi\").options(Map(\n  \"hoodie.datasource.write.operation\" -> \"upsert\",\n  \"hoodie.datasource.write recordkey\" -> \"device_id,ts\",\n  \"hoodie.datasource.write.precombine field\" -> \"ts\"\n)).save(\"s3://lake/iot/\")\n```\n\n## Follow-up Questions\n- How would you test schema evolution without downtime?\n- How do you monitor data quality and roll back corrupted partitions?","diagram":"flowchart TD\n  A[IoT Core] --> B[Kinesis Firehose]\n  B --> C[S3 Parquet daily partitions]\n  C --> D[EMR Spark + Hudi upsert]\n  D --> E[Glue Data Catalog]\n  E --> F[Athena/Quicksight]\n","difficulty":"intermediate","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Meta","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T13:21:40.977Z","createdAt":"2026-01-22T13:21:40.977Z"},{"id":"q-5828","question":"Design a multi-tenant data lake ingestion: route JSON events from all products into a Kinesis Data Streams consumer, land Parquet in S3 partitioned by tenant_id/date, cataloged in Glue, and restrict per-tenant access via Lake Formation. Describe services, partitioning, schema evolution, data quality checks, testing, and how you'd ensure tenant isolation and cost efficiency?","answer":"Use Kinesis Data Streams + Firehose to land JSON into S3 as Parquet, partitioned by tenant_id/date. Catalog in Glue; enforce per-tenant isolation with Lake Formation by granting database/table permiss","explanation":"## Why This Is Asked\n\nAssesses practical multi-tenant data governance, partitioning strategy, and end-to-end reliability in a real AWS data lake.\n\n## Key Concepts\n\n- Multi-tenant isolation with Lake Formation and per-tenant IAM\n- Parquet partitioning by tenant and date\n- Glue Data Catalog and schema evolution with a registry\n- Data quality checks at ingest (nonnulls, type validation, dedup)\n- End-to-end CI/CD validation across tenants\n\n## Code Example\n\n```javascript\n// Example: grant Lake Formation permissions for a tenant (conceptual)\nconst { LakeFormationClient, GrantPermissionsCommand } = require(\"@aws-sdk/client-lakeformation\");\n// Pseudo-implementation for illustration\n```\n\n## Follow-up Questions\n\n- How would you test schema evolution with existing partitions?\n- What changes if a new tenant is added mid-day with different schema?\n","diagram":null,"difficulty":"intermediate","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Robinhood","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T18:09:34.563Z","createdAt":"2026-01-22T18:09:34.563Z"},{"id":"q-5838","question":"Two vendor Parquet feeds land in s3://telemetry/raw/YYYY/MM/DD/vendor=V1/ and vendor=V2/ with slight schema drift. Design a beginner pipeline to map to a canonical schema, write to s3://telemetry/curated/YYYY/MM/DD/ (partition by day/region), and catalog in Glue. Include simple schema-evolution rules, QC (nulls, types, duplicates) and an end-to-end test. Add CloudWatch alert for failures and missing days?","answer":"Map both Parquet schemas to a canonical schema in a Glue Spark job, with a mapping dictionary for fields, dtype casting, and default values. Read from both vendor folders, union, drop extraneous field","explanation":"## Why This Is Asked\n\nThis question tests practical data engineering skills for handling schema drift, canonicalization, and cataloging in AWS using Glue, Spark, and S3. It also probes data quality checks, end-to-end testing, and operational monitoring with CloudWatch.\n\n## Key Concepts\n\n- Schema drift handling across vendors\n- Canonical schema mapping and Parquet storage\n- Glue Data Catalog integration and schema evolution (mergeSchema)\n- Data quality checks: nulls, types, duplicates\n- End-to-end testing and CloudWatch alerts\n\n## Code Example\n\n```javascript\n// Mapping skeleton for canonical fields\n```\n\n## Follow-up Questions\n\n- How would you handle breaking changes that require backfilling?\n- How would you monitor data freshness across vendors?\n","diagram":null,"difficulty":"beginner","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","NVIDIA","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T18:58:11.540Z","createdAt":"2026-01-22T18:58:11.541Z"},{"id":"q-5921","question":"**BEGINNER**: Scenario: A daily CSV feed of e-commerce transactions arrives in s3://data-lake/raw/transactions/YYYY/MM/DD/transactions.csv. Design a beginner-level ingestion pipeline to convert to Parquet, partition by date, and register in Glue. Explain idempotent processing with deduplication, handle schema drift (new columns), data quality checks, and a simple end-to-end test plan?","answer":"Implement an idempotent pipeline: Glue ETL reads the daily CSV via dynamic frame, deduplicates by transaction_id, writes Parquet to s3://data-lake/transactions/YYYY/MM/DD/, overwriting the partition to ensure idempotency. Configure Glue to automatically handle schema drift by adding new columns without breaking existing jobs. Include basic data quality checks using Glue's built-in rules to validate required fields and data types. For testing, implement a simple end-to-end test that processes a sample CSV through the pipeline and validates the resulting Parquet files contain expected data with correct partitioning.","explanation":"## Why This Is Asked\nTests ability to design end-to-end ingestion with idempotency, schema evolution, and basic quality checks using Glue + S3 + Parquet.\n\n## Key Concepts\n- Idempotent processing\n- Schema evolution in Glue\n- Data quality checks\n- End-to-end testing\n\n## Code Example\n```python\n# Pseudo Glue ETL skeleton\nimport sys\nfrom awsglue.transforms import *\n# ... read CSV, drop duplicates by transaction_id, write Parquet, update catalog\n```\n\n## Follow-up Questions\n- How would you detect and handle late-arriving files?\n- How would you scale this for higher throughput?","diagram":"flowchart TD\n  A[S3 raw: transactions.csv] --> B[Glue ETL job]\n  B --> C[Parquet commit: s3://data-lake/transactions/YYYY/MM/DD/]\n  C --> D[Glue Data Catalog]\n  D --> E[Athena/Quicksight queries]","difficulty":"beginner","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T05:51:04.137Z","createdAt":"2026-01-22T22:06:05.636Z"},{"id":"q-5980","question":"For a multi-account AWS data lake used by Adobe/Tesla, design an end-to-end data governance solution that uses Lake Formation column-level permissions, Glue Data Catalog, and S3 tagging to restrict PII in a customer_events dataset. Show how you would grant a data scientist read access to non-PII columns only, implement masking in Athena/Glue queries, and verify access and auditability with tests?","answer":"Grant data scientist read access only to non-PII columns via Lake Formation column-level permissions on the Glue Data Catalog, explicitly denying access to PII columns. Implement data masking by creating views or UDFs that redact PII in Athena/Glue queries. Verify access controls and auditability through Lake Formation audit logs and CloudTrail events.","explanation":"## Why This Is Asked\nThis question evaluates practical data governance design in a multi-account AWS data lake, focusing on column-level access controls, data masking, and auditability—critical capabilities for enterprise-scale data programs at companies like Adobe and Tesla.\n\n## Key Concepts\n- Lake Formation column-level permissions\n- Glue Data Catalog and cross-account access\n- S3 object tagging for data classification\n- Data masking with views and UDFs\n- Auditing with Lake Formation logs and CloudTrail\n\n## Code Example\n```javascript\n// Pseudo CLI: grant non-PII column access (LF) and illustrate masking\n```","diagram":"flowchart TD\n  A[Source Systems] --> B[S3 Raw Data Lake]\n  B --> C[Glue Data Catalog]\n  C --> D[Lake Formation Permissions]\n  D --> E[Athena/Glue Queries]\n  E --> F[Audit Logs & Compliance]","difficulty":"intermediate","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T05:01:01.111Z","createdAt":"2026-01-23T02:38:26.731Z"},{"id":"q-6045","question":"Streaming JSON events arrive in Kinesis across regions. Design a cost-aware, auditable pipeline to land data in S3 Parquet with upserts and time-travel, while preserving deterministic data lineage. Use Kinesis -> Glue Spark (Hudi) -> S3 Parquet, with a lineage table mapping event_id to source_region and partition, plus object tags for governance. Include testing and rollback plan?","answer":"Leverage per-record IDs and region metadata; store as Hudi upserts in S3 Parquet; maintain a lineage table in Glue Catalog mapping event_id to source_region and partition; tag objects with region and ","explanation":"## Why This Is Asked\n\nThis tests designing a lineage-aware streaming pipeline with upsert semantics and time travel under cost pressure.\n\n## Key Concepts\n\n- Streaming ingestion with idempotence\n- Data lineage and governance\n- Upserts and time-travel with Hudi/Iceberg\n\n## Code Example\n\n```javascript\n// Placeholder Spark upsert example\n```\n\n## Follow-up Questions\n\n- How would you validate lineage accuracy end-to-end?\n- What monitoring would you put in place for data drift?","diagram":"flowchart TD\n  Kinesis[Kinesis] --> GLUE[Glue Spark (Hudi)]\n  GLUE --> S3[S3 Parquet (Hudi)]\n  S3 --> CATALOG[Glue Catalog / Athena]\n  CATALOG --> LINEAGE[Lineage table]","difficulty":"intermediate","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Coinbase","IBM"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T06:49:26.522Z","createdAt":"2026-01-23T06:49:26.523Z"},{"id":"q-6084","question":"Multi-tenant ingestion: three partners push JSON events to a shared S3 bucket. Design a pipeline that isolates each partner’s data, supports independent schema evolution, and enables cross-tenant analytics with a unified catalog. Detail data model, per-partner partitions, Lake Formation permissions, and validation tests, including late-arriving data handling and lineage?","answer":"Partition by partner_id and date; store under s3://data-lake/partner_id/date/. Use a single Glue Data Catalog with per-partner databases and Lake Formation grants so each partner can only query its da","explanation":"## Why This Is Asked\n\nTests ability to design multi-tenant data lakes with isolation and governance, a common real-world requirement.\n\n## Key Concepts\n\n- Multi-tenant data isolation with Lake Formation\n- Per-partner prefixes and partitioning by date\n- Unified Glue Data Catalog with per-partner databases\n- Schema evolution and data quality gates\n- Late-arriving data handling and lineage\n\n## Code Example\n\n```javascript\n// Pseudo-schema validation\nfunction validate(record, schema) {\n  for (const [k, t] of Object.entries(schema)) {\n    if (t === 'string' && typeof record[k] !== 'string') return false;\n  }\n  return true;\n}\n```\n\n## Follow-up Questions\n- How would you monitor schema drift across partners?\n- How would you enforce PII masking for analytics?","diagram":"flowchart TD\n  A[JSON Source] --> B[Ingestion/ETL]\n  B --> C[S3 Partitioned by partner/date]\n  C --> D[Glue Data Catalog]\n  D --> E[Lake Formation Permissions]\n  E --> F[Analytics/BI]","difficulty":"intermediate","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Meta","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T07:57:23.271Z","createdAt":"2026-01-23T07:57:23.271Z"},{"id":"q-6128","question":"You receive a daily feed of CSV files arriving at s3://corp-data/raw/sales/YYYY/MM/DD/sales.csv.gz. Files may have different column orders and some rows with missing values. Design a beginner-level ingestion pipeline to convert to Parquet, partition by date, and register in Glue Data Catalog. Requirements: 1) which AWS services and steps; 2) how to handle schema evolution when new columns appear; 3) how to implement data quality checks (nulls, data types, duplicates) and simple test data; 4) testing strategy end-to-end; 5) monitoring/alerts for missing files or failed runs; 6) how to make downstream queries robust to column reordering?","answer":"Use a Glue Spark job to read s3://corp-data/raw/sales/YYYY/MM/DD/sales.csv.gz with header and a defined schema; write Parquet to s3://corp-data/warehouse/sales/YYYY/MM/DD/ and register in Glue Data Ca","explanation":"## Why This Is Asked\nTests practical ability to design an end-to-end, beginner-friendly ingestion that handles common real-world cruft: variable column order, missing values, and schema evolution, using AWS data tools.\n\n## Key Concepts\n- AWS Glue (Spark jobs, Data Catalog, workflows)\n- Parquet partitioning by date\n- Schema evolution via defined schema and column names\n- Data quality checks: null ratios, type validation, deduplication\n- Monitoring: CloudWatch, EventBridge/SNS alerts\n- End-to-end testing: small sample, end-to-end run\n\n## Code Example\n```python\nfrom pyspark.sql.types import StructType, StructField, StringType, DoubleType\nschema = StructType([\n  StructField('sale_id', StringType(), True),\n  StructField('customer_id', StringType(), True),\n  StructField('amount', DoubleType(), True),\n  StructField('currency', StringType(), True)\n])\npath = 's3://corp-data/raw/sales/YYYY/MM/DD/sales.csv.gz'\nout = 's3://corp-data/warehouse/sales/YYYY/MM/DD/'\ndf = spark.read.csv(path, header=True, schema=schema)\ndf = df.dropDuplicates(['sale_id'])\ndf.write.partitionBy('YYYY','MM','DD').parquet(out, mode='append')\n```\n\n## Follow-up Questions\n- How to handle late-arriving files?\n- How would you test drift in schema vs. data quality?\n- How to scale to hundreds of daily partitions and large files?","diagram":null,"difficulty":"beginner","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Microsoft","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T10:01:03.297Z","createdAt":"2026-01-23T10:01:03.297Z"},{"id":"q-6172","question":"Design a scalable ML feature store pipeline in AWS for a fraud-detection model. Ingest raw data from Kinesis and S3, design SageMaker Feature Store feature groups with offline/online stores, manage schema evolution, implement drift checks, ensure data governance with Lake Formation and Glue Catalog, and outline end-to-end testing and rollback?","answer":"Design a SageMaker Feature Store pipeline with offline/online stores; ingest raw data from Kinesis and S3 into feature groups; define tenant-scoped access via Lake Formation; implement schema evolutio","explanation":"## Why This Is Asked\n\nAssess ability to design an ML feature pipeline end-to-end, covering data ingestion, storage, governance, and observability in real-world multi-tenant environments.\n\n## Key Concepts\n\n- SageMaker Feature Store: online and offline stores, feature groups, primary keys, and batch/stream ingestion patterns.\n- Glue Data Catalog integration and Lake Formation for fine-grained access control.\n- Schema evolution: backward/forward compatibility, versioned feature schemas, data replay.\n- Data quality and drift: feature drift monitoring via SageMaker Model Monitor or custom validators.\n- End-to-end testing: synthetic data, canary jobs, rollback/compaction strategy.\n\n## Code Example\n\n```python\n# Example: create a feature group in SageMaker Feature Store\nimport boto3\nfrom sagemaker import Session\nfrom sagemaker.feature_store.feature_group import FeatureGroup\n\nsession = Session()\nfg = FeatureGroup(name='fraud_features', sagemaker_session=session)\n# Define feature metadata and record payloads for ingestions\n```\n\n## Follow-up Questions\n\n- How would you enforce tenant isolation across Feature Store feature groups and Lake Formation permissions?\n- What metrics and tests would you implement to detect feature drift and ensure data quality over time?\n","diagram":null,"difficulty":"intermediate","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Scale Ai","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T11:40:58.318Z","createdAt":"2026-01-23T11:40:58.318Z"},{"id":"q-6226","question":"Scenario: An IoT fleet in multiple regions streams telemetry in CSV and JSON via Kinesis Firehose into S3 across several AWS accounts. Design an end-to-end ingestion path that lands data as Parquet with daily partitions, using AWS Glue for ETL and the data catalog, and Lake Formation for access controls. Address schema evolution with mixed formats, data quality checks, partition strategy and compaction, testing and rollback, and cost monitoring. Which design choices and trade-offs would you make?","answer":"I’d route Firehose to a raw S3 bucket, run a Glue ETL job to normalize CSV/JSON to Parquet, and partition by date/region. Glue Catalog tracks schemas; Lake Formation enforces per‑user access. Implemen","explanation":"## Why This Is Asked\n\nTests practical multi-account, multi-region data lake design with mixed formats, governance, and cost discipline. It probes ETL design, schema evolution handling, data quality, and rollback strategies under real-world constraints.\n\n## Key Concepts\n\n- Multi-account, multi-region data lake orchestration\n- Firehose ingestion to raw S3 and Glue ETL to Parquet\n- Glue Data Catalog + Lake Formation access controls\n- Schema evolution with mixed CSV/JSON formats\n- Data quality checks, partition strategy, and compaction\n- Canary testing, rollback procedures, cost monitoring\n\n## Code Example\n\n```javascript\nfunction validateSchema(record, schema) {\n  // naive schema validation example\n  for (const key of Object.keys(schema)) {\n    if (!(key in record)) return false\n  }\n  return true\n}\n```\n\n## Follow-up Questions\n\n- How would you implement end-to-end testing and canary launches for schema changes?\n- What strategies would you use to reconcile schema drift between CSV and JSON inputs?\n- How would you monitor and optimize query performance with partition pruning across regions?","diagram":"flowchart TD\n  Ingest[Kinesis Firehose] --> ETL[Glue ETL Job]\n  ETL --> Parquet[S3 Parquet with daily partitions]\n  Parquet --> Catalog[Glue Data Catalog]\n  Catalog --> Access[Lake Formation Access Controls]","difficulty":"intermediate","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","IBM","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T15:01:38.225Z","createdAt":"2026-01-23T15:01:38.225Z"},{"id":"q-6314","question":"Scenario: A SaaS platform emits per-tenant user activity as JSON events at high scale across regions. Design an end-to-end landing pipeline that guarantees strict per-tenant isolation, supports schema evolution, handles late data and duplicates, and is cost-conscious. Describe ingestion, storage, cataloging, access control (Lake Formation), data quality checks, testing, and cross-region replication considerations?","answer":"Ingest JSON via Kinesis Data Firehose, convert to Parquet, partition by tenant_id/date, and land in S3. Use Glue Data Catalog with Lake Formation to enforce per-tenant row-level access. Implement dedu","explanation":"## Why This Is Asked\nTests ability to design a scalable, multi-tenant data lake with strict isolation, evolving schemas, and late-arrival handling on AWS.\n\n## Key Concepts\n- Ingestion path: Kinesis Firehose with data format conversion to Parquet\n- Storage: S3 partitioned by tenant_id/date for efficient pruning\n- Cataloging and governance: Glue Data Catalog + Lake Formation for per-tenant access\n- Data quality: dedup on event_id, late data handling, schema evolution checks\n- Testing: synthetic tenants and cross-region replication validation\n\n## Code Example\n```javascript\n// Example: spark path would handle schema evolution and dedup in Glue job\n```\n\n## Follow-up Questions\n- How would you implement per-tenant masking in queries?\n- How would you monitor schema drift and automate remediation?","diagram":null,"difficulty":"advanced","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","Microsoft","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T19:01:39.906Z","createdAt":"2026-01-23T19:01:39.906Z"},{"id":"q-6378","question":"Design an end-to-end streaming ingestion pipeline that consumes JSON events from a Kinesis Data Streams source, lands Parquet files partitioned by date in S3, and exposes an updatable Iceberg table via Glue/Athena. Address schema evolution, late-arriving data, and upserts, plus governance (PII masking, access controls). Include testing steps and cost considerations?","answer":"Use Spark Structured Streaming (Glue or EMR) to read from Kinesis Data Streams, write to an Iceberg table on S3 with daily partitions, register in Glue Catalog, enable schema evolution with MERGE operations for upserts, and implement governance via Lake Formation for PII masking and IAM-based access controls.","explanation":"## Why This Is Asked\n\nAssesses the ability to design a real-world data lake pipeline with streaming ingestion, upserts, and governance. Tests knowledge of integrating Kinesis, Iceberg on S3, Glue Catalog, and access controls for enterprise data platforms.\n\n## Key Concepts\n\n- Streaming ingestion from Kinesis using Spark Structured Streaming (Glue/EMR)\n- Iceberg on S3 with Glue Catalog for upserts and schema evolution\n- Daily partitioning and tombstone handling for late-arriving data\n- Governance: Lake Formation masking, IAM roles, and fine-grained access controls\n- Testing: end-to-end validation, data quality checks, and cost optimization strategies\n\n## Code Example\n\n```\nMERGE INTO iceberg_db.events AS target\nUSING staged AS source\nON target.event_id = source.event_id\nWHEN MATCHED THEN UPDATE SET *\nWHEN NOT MATCHED THEN INSERT *\n```","diagram":"flowchart TD\n  A[Kinesis Data Streams] --> B[Spark Structured Streaming]\n  B --> C[Iceberg Table on S3]\n  C --> D[Glue Catalog / Athena Access]\n  D --> E[Monitoring & Governance]","difficulty":"intermediate","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","NVIDIA","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T04:34:02.643Z","createdAt":"2026-01-23T21:40:49.512Z"},{"id":"q-6514","question":"Scenario: A daily CSV feed from a partner is uploaded to s3://partner-logs/raw/partnerA/YYYY/MM/DD/, with fields user_id, action, ts and an optional ip_address. Design a beginner-level ingestion pipeline using AWS Glue to land Parquet in s3://partner-logs/curated/partnerA/YYYY/MM/DD partitioned by date and action, register in Glue Data Catalog, and implement a simple schema evolution strategy, plus basic data quality checks and a minimal end-to-end test plan. How would you implement this?","answer":"Implement a Glue Spark job to ingest daily partner CSVs from s3://partner-logs/raw/partnerA/YYYY/MM/DD/, flatten optional ip_address, and output Parquet to s3://partner-logs/curated/partnerA/YYYY/MM/D","explanation":"## Why This Is Asked\nTests practical Glue ETL skills: handling simple schema changes, incremental loads, and data quality in a real-world pipeline.\n\n## Key Concepts\n- AWS Glue Spark jobs for ETL\n- DynamicFrame applyMapping, mergeSchema\n- Parquet partitioning by date and field\n- Job bookmarks for idempotent loads\n- Basic data quality: not-null checks, type validation\n\n## Code Example\n```javascript\n// Pseudo-outline: read CSV, map types, write Parquet with partitions\nconst df = spark.read.csv(path, {header:true, inferSchema:false})\nval mapped = df.withColumn('user_id', df['user_id'].cast('string'))\n  .withColumn('ts', to_timestamp(df['ts']))\n  .withColumn('ip_address', when(col('ip_address').isNotNull(), col('ip_address')).otherwise(lit(null)))\nmapped.write.partitionBy('date','action').parquet(outPath)\n```\n\n## Follow-up Questions\n- How would you monitor the pipeline for failures and missing files?\n- How would you adapt to a new field added by the partner without breaking queries?","diagram":null,"difficulty":"beginner","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T06:04:21.782Z","createdAt":"2026-01-24T06:04:21.782Z"},{"id":"q-6714","question":"Scenario: A single AWS data lake serves multiple customers (tenants) writing JSON events to a shared Kinesis Data Stream. Design an end-to-end pipeline to land events in S3 as Parquet, partitioned by tenant_id and event_date, with weekly schema evolution, deduplication, and strict per-tenant access controls using Lake Formation. Explain how you would enforce tenant isolation, apply dynamic masking for PII, validate data lineage, and test end-to-end under schema drift and late arrivals. Specify services, formats, and testing strategy?","answer":"Use KDS with a streaming ETL (Glue or Lambda) to write Parquet to S3 partitioned by tenant_id/date; catalog in Glue Data Catalog; enforce Lake Formation permissions per tenant; apply dynamic masking f","explanation":"## Why This Is Asked\nMulti-tenant isolation, schema drift, and streaming ETL matter at scale.\n\n## Key Concepts\n- Kinesis Data Streams / Glue Streaming ETL\n- Lake Formation permissions, per-tenant access\n- Parquet partitions by tenant_id and event_date\n- Glue Catalog schema evolution; lineage checks\n- Dedup via event_id; idempotent writes; masking for PII\n- Testing: synthetic tenants, late arrivals, cross-tenant audits, cost watch\n\n## Code Example\n```javascript\nfunction maskPII(record){\n  if(record.ssn) record.ssn = 'XXX-XX-XXXX';\n  if(record.email) record.email = record.email.replace(/[^@]+@/, 'user@');\n  return record;\n}\n```\n\n## Follow-up Questions\n- How would you test schema evolution across partitions without downtime?\n- How would you handle event order and late arrivals in a cross-tenant dataset?\n","diagram":"flowchart TD\n  A[Kinesis Data Stream] --> B[Glue Streaming ETL]\n  B --> C[S3 Parquet: tenant_id/date partitions]\n  C --> D[Glue Data Catalog + Lake Formation perms]\n  D --> E[Athena/Glue queries with masking]\n  E --> F[Audit, lineage, and alerts]","difficulty":"advanced","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T14:31:53.963Z","createdAt":"2026-01-24T14:31:53.963Z"},{"id":"q-6770","question":"Scenario: A daily CSV feed arrives at s3://vendor-data/raw/sales/YYYY/MM/DD/*.csv where column order may vary and new optional columns appear over time. Design a beginner-level ingestion to store Parquet with daily partitions in s3://vendor-data/curated/sales/YYYY/MM/DD/, register in Glue, and implement a robust schema-evolution strategy that tolerates added or reordered columns without breaking ETL. Include a simple test plan and monitoring?","answer":"Use a Glue Spark job that reads the daily CSV with header, applies a base schema for required fields, and captures unknown columns into a JSON blob column. Write Parquet to s3://vendor-data/curated/sa","explanation":"## Why This Is Asked\nTests handling dynamic CSV schemas in a beginner-friendly way, ensuring practical exposure to Glue ETL, Parquet partitioning, and catalog updates.\n\n## Key Concepts\n- AWS Glue Spark ETL\n- Parquet partitioning\n- Schema evolution tolerant parsing\n- Lightweight data quality checks\n- End-to-end testing and CloudWatch monitoring\n\n## Code Example\n```python\n# Simplified Spark read with permissive mode\nfrom pyspark.sql.types import StructType, StructField, StringType, DoubleType\nbase = StructType([\n  StructField(\"order_id\", StringType(), True),\n  StructField(\"customer_id\", StringType(), True),\n  StructField(\"amount\", DoubleType(), True)\n])\ndf = spark.read.format(\"csv\").option(\"header\",\"true\").schema(base).option(\"mode\",\"PERMISSIVE\").load(\"s3://vendor-data/raw/sales/.../*.csv\")\ndf.write.partitionBy(\"year\",\"month\",\"day\").format(\"parquet\").save(\"s3://vendor-data/curated/sales/.../\")\n```\n\n## Follow-up Questions\n- How would you adapt for stricter validation if new required columns appear?\n- How would you validate lineage from raw to curated data?","diagram":"flowchart TD\n  A[Ingest CSV] --> B[Parse header and base schema]\n  B --> C[Capture extras as JSON blob]\n  C --> D[Write Parquet with daily partitions]\n  D --> E[Glue Data Catalog update]\n  E --> F[CloudWatch alert for missing days]","difficulty":"beginner","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T16:57:11.088Z","createdAt":"2026-01-24T16:57:11.088Z"},{"id":"q-6795","question":"Design an AWS-native data ingestion and processing pipeline for a multi-tenant SaaS that streams petabytes of telemetry from per-tenant devices to a centralized data lake on S3. The catch: you must support upserts and deletes, low-latency queries for dashboards, per-tenant data residency, and cost controls. Propose a data layout using Apache Hudi or Iceberg on S3, with Glue Catalog integration, Athena/Redshift Spectrum for querying, and a schema evolution strategy, along with testing, monitoring, and failure handling?","answer":"Use Apache Hudi on S3 to enable upserts and deletes with per-tenant partitions. Ingest telemetry via streaming into S3, process with Spark to write to Hudi tables, and run periodic compaction. Glue Da","explanation":"## Why This Is Asked\nTests ability to design a scalable, cost-aware data lake with upserts/deletes, multi-tenancy, and end-to-end testing in an AWS-heavy environment.\n\n## Key Concepts\n- Upserts/deletes on S3 using Apache Hudi or Iceberg\n- Per-tenant partitioning and residency controls\n- Glue Data Catalog integration; Athena/Redshift Spectrum for queries\n- Schema evolution strategy and data quality/testing\n\n## Code Example\n```scala\n// Spark Structured Streaming write to Hudi table\nimport org.apache.spark.sql.functions._\nval df = spark.readStream.format(\"json\").load(\"s3://bucket/raw/telemetry/\")\ndf.writeStream\n  .format(\"org.apache.hudi\")\n  .option(\"hoodie.datasource.write.storage.type\",\"MERGE_ON_READ\")\n  .option(\"hoodie.datasource.write.recordkey\",\"tenant_id,device_id,timestamp\")\n  .option(\"hoodie.datasource.write.partitionpath.field\",\"tenant_id\")\n  .option(\"hoodie.datasource.write.table_name\",\"telemetry\")\n  .option(\"hoodie.upsert.shuffle.parallelism\",\"200\")\n  .start(\"s3://bucket/warehouse/telemetry\")\n```\n\n## Follow-up Questions\n- How would you validate upsert/delete correctness with late-arriving events?\n- What strategies ensure tenant data residency during regional migrations?","diagram":"flowchart TD\n  Ingest[Ingest Telemetry] --> Stage[Stage in S3]\n  Stage --> Upsert[Apply Upserts/Deletes with Hudi]\n  Upsert --> Catalog[Glue Data Catalog]\n  Catalog --> Query[Query via Athena/Redshift Spectrum]\n  Query --> Monitor[Monitoring & Cost Controls]","difficulty":"advanced","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Salesforce","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T17:44:12.167Z","createdAt":"2026-01-24T17:44:12.167Z"},{"id":"q-6876","question":"Scenario: Ingest chat transcripts and user events from a mobile app via Kinesis Data Streams into S3 as daily-partitioned Parquet. The ML/Analytics teams will use Snowflake for analytics and Hugging Face models for NLP features. Design the end-to-end pipeline: ingestion and schema evolution, Parquet partitioning strategy, offline/online feature store approach, data governance, and a testing plan to validate end-to-end reliability before serving?","answer":"Ingest with Kinesis to S3 Parquet (daily partitions). Use Glue ETL to enforce schema evolution and register schemas in Glue Data Catalog. Build offline features in Snowflake on top of Parquet tables; ","explanation":"## Why This Is Asked\nTests end-to-end streaming to lakehouse with ML feature exposure and governance.\n\n## Key Concepts\n- Kinesis streaming; Parquet on S3; daily partitions\n- Glue ETL and Data Catalog; schema evolution\n- Snowflake offline analytics; Hugging Face NLP features online\n- Lake Formation governance and access controls\n- End-to-end testing: synthetic data, drift, lineage\n\n## Code Example\n```javascript\n// Pseudo Glue Spark job sketch\nconst df = spark.readStream.format('kinesis').load(\"stream-name\");\nval out = df.writeStream.format('parquet').option('partitionBy','date').start(\"s3://bucket/ Parquet/\");\n```\n\n## Follow-up Questions\n- How would you test schema drift between old and new events?\n- How would you handle feature store consistency if the Hugging Face model updates?","diagram":"flowchart TD\n  A[Kinesis Ingest] --> B[S3 Parquet (daily)]\n  B --> C[Glue Data Catalog]\n  C --> D[Snowflake (Offline analytics)]\n  C --> E[Hugging Face NLP service (Online features)]\n  E --> F[Serving layer with cache]\n  D --> G[Analytics/Models]","difficulty":"intermediate","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T21:04:12.407Z","createdAt":"2026-01-24T21:04:12.407Z"},{"id":"q-6990","question":"Advanced: You run a multi-region data lake for telemetry from edge devices. Data arrives as JSON with evolving schema, occasional late data, and strict per-tenant access. Design an end-to-end pipeline using S3 with Apache Iceberg, Glue catalog, and Athena across accounts; include schema evolution, deduplication, data quality checks, cross-region replication, and auditing. How would you ensure low-latency analytics and governance under cost constraints?","answer":"Use Apache Iceberg on S3 with a Glue catalog, ingested via Kinesis Data Streams and Spark on Glue/EMR. Partition by tenant and day; enable Iceberg schema evolution; deduplicate by (tenant,id,ts); hand","explanation":"## Why This Is Asked\n\nRigor: tests mastery of multi-region data lakes, Iceberg on S3, and governance at scale. It evaluates handling evolving schemas, late data, real-time ingestion, and cross-account access.\n\n## Key Concepts\n\n- Apache Iceberg on S3 with Glue catalog\n- Kinesis + Spark (Glue/EMR)\n- Lake Formation per-tenant access\n- Cross-region S3 replication\n- Data quality checks and deduplication\n\n## Code Example\n\n```scala\n// Pseudo-Spark dedupe\nimport org.apache.spark.sql.functions._\nval df = spark.readStream.format(\"json\").load(\"s3://...\")\nval deduped = df.withWatermark(\"timestamp\",\"1h\").dropDuplicates(\"tenant\",\"device_id\",\"event_id\")\n```\n\n## Follow-up Questions\n\n- How would you test late data handling?\n- How would you implement tombstones in Iceberg?\n- How to validate governance across accounts?\n","diagram":"flowchart TD\n  A[Edge devices] --> B[Kinesis Data Streams]\n  B --> C[Spark on Glue/EMR]\n  C --> D[Iceberg tables on S3]\n  D --> E[Athena/Glue]\n  E --> F[S3 cross-region replication]\n  F --> G[Lake Formation auditing]","difficulty":"advanced","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Cloudflare","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T04:26:43.789Z","createdAt":"2026-01-25T04:26:43.790Z"},{"id":"q-7014","question":"Design a regionally distributed real-time data ingestion pipeline that streams JSON events from fintech apps into S3 as partitioned Parquet with hourly partitions, handling nested schema evolution, upserts, and late data, while enabling fast analytics in Athena and governance via Lake Formation. Include services, data formats, testing, and failure handling?","answer":"Ingest JSON events regionally via Kinesis Data Streams, fan out to S3 Parquet with hourly partitions using Apache Hudi for upserts and schema evolution. Use Glue Schema Registry + Glue Data Catalog; q","explanation":"## Why This Is Asked\nThis question probes end-to-end streaming + lakehouse design at scale, schema evolution, upserts, dedup, late data, cross-region replication, and governance. It tests practical choices (KDS vs MSK, Parquet vs ORC, Hoodie, Glue Registry, Lake Formation, Athena) and how you validate with tests.\n\n## Key Concepts\n- Regionally distributed streaming and cross-region replication\n- Schema evolution with Glue Registry\n- Upserts/dedup using Apache Hudi on S3\n- Partitioning strategy: hourly Parquet\n- Data quality and testing strategy\n\n## Code Example\n```javascript\n// CDK-like pseudo snippet to define a Kinesis stream, an S3 bucket, and a Glue catalog table\nconst stream = new kinesis.Stream(this, 'IngestStream');\nconst bucket = new s3.Bucket(this, 'RawBucket');\nconst table = new glue.Table(this, 'EventTable', {\n  database: glue.Database.fromDatabaseName(this, 'db', 'analytics'),\n  tableName: 'device_events',\n  columns: [\n    {name: 'device_id', type: glue.Schema.STRING},\n    {name: 'event_time', type: glue.Schema.TIMESTAMP},\n    {name: 'payload', type: glue.Schema.STRING},\n  ],\n  partitionKeys: [{name: 'hour', type: glue.Schema.STRING}],\n  dataFormat: glue.DataFormat.PARQUET\n});\n```\n\n## Follow-up Questions\n- How would you test end-to-end latency and data quality under bursty traffic?\n- How would you handle nested JSON schema drift and ensure backward compatibility?","diagram":"flowchart TD\n  A[Ingest: Regional KDS/MSK] --> B[Shard: hourly Parquet on S3]\n  B --> C[Schema: Glue Registry & Catalog]\n  C --> D[Query: Athena/Redshift Spectrum]\n  D --> E[Governance: Lake Formation & IAM]\n  E --> F[Monitoring: CloudWatch]","difficulty":"advanced","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T05:48:03.452Z","createdAt":"2026-01-25T05:48:03.452Z"},{"id":"q-7222","question":"Context: Global e-commerce, real-time dashboards. Ingest mobile/web events via Kinesis Data Streams, land in S3 as Parquet with daily partitions, cataloged by Glue, and governed with Lake Formation. Data arrives with evolving schema, duplicates, and late data. Design an end-to-end pipeline: data models, upsert strategy (Hudi/Delta), schema evolution approach, data quality checks, testing plan, cross-region replication, and cost controls?","answer":"Use Kinesis Data Streams for ingestion; land raw events in S3 as Parquet with daily partitions and Glue catalog. Run Apache Hudi on Glue Spark jobs to upsert into target Parquet, enabling dedup and la","explanation":"## Why This Is Asked\n\nTests ability to design a production-ready data pipeline addressing schema evolution, dedup, late data, multi-region, and cost.\n\n## Key Concepts\n\n- Ingestion and stream processing with Kinesis\n- Parquet with daily partitions in S3\n- Hudi upserts, schema evolution, tombstones\n- Glue Data Catalog and Lake Formation\n- Cross-region replication, cost controls\n\n## Code Example\n\n```javascript\nconst hoodieOptions = {\n  \"hoodie.datasource.write.recordkey.field\": \"event_id\",\n  \"hoodie.datasource.write.table.type\": \"MERGE_ON_READ\"\n};\n```\n\n## Follow-up Questions\n\n- How would you monitor latency from event ingress to analytics?\n- How would you validate schema drift and enforce compatibility across teams?\n","diagram":null,"difficulty":"advanced","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Anthropic","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T14:27:39.395Z","createdAt":"2026-01-25T14:27:39.396Z"},{"id":"q-7275","question":"Design a hybrid real-time/batch data platform for multi-tenant fraud detection. Ingest via MSK, run KDA for sub-second signals into DynamoDB and land transformed events to S3 Parquet via Glue. Use Lake Formation for tenant isolation and masking, with Glue Data Catalog, lineage, and end-to-end tests?","answer":"Architect a hybrid real-time/batch data platform for multi-tenant fraud detection. Ingest via MSK, run KDA for sub-second signals into DynamoDB and land transformed events to S3 Parquet via Glue, with","explanation":"## Why This Is Asked\nTests ability to design hybrid streaming/batch pipelines, multi-tenant governance, and observability under real-world constraints.\n\n## Key Concepts\n- Hybrid streaming + batch architecture with MSK, Kinesis Data Analytics, Glue\n- Low-latency storage (DynamoDB/ElastiCache) and long-term storage (S3 Parquet)\n- Tenant isolation and masking via Lake Formation\n- Data catalog, lineage, monitoring, and tests\n\n## Code Example\n```python\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.getOrCreate()\n# streaming source\nstream = spark.readStream.format(\"kinesis\").option(\"streamName\",\"fraud-stream\").load()\n# static reference data\nstatic = spark.read.parquet(\"s3://bucket/fraud/static/\")\njoined = stream.join(static, on=\"tenant_id\")\nquery = joined.writeStream.format(\"console\").start()\n```\n\n## Follow-up Questions\n- How would you validate per-tenant masking and lineage during deployments?\n- How would you test scaling under peak MSK throughput and ensure partition pruning in Athena?","diagram":"flowchart TD\n  A[MSK Ingest] --> B[KDA] \n  B --> C[DynamoDB]\n  A --> D[S3 Parquet Landing via Glue]\n  C -- data lineage --> E[Glue Data Catalog]\n  E --> F[Athena/Redshift Spectrum] \n  G[Lake Formation] --> H[Per-tenant masking and access control]","difficulty":"advanced","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","PayPal","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T16:40:09.565Z","createdAt":"2026-01-25T16:40:09.565Z"},{"id":"q-7307","question":"Design a beginner-level ingestion pipeline for a daily JSON Lines file from a partner. The file sits at s3://partner-data/raw/date=YYYY-MM-DD/file.jsonl and contains nested arrays and optional fields. Create a pipeline to transform to Parquet, store at s3://lake/partner/YYYY/MM/DD/, partition by date, and register in Glue Data Catalog. Include schema evolution for added fields, basic data quality checks, and a simple idempotency mechanism using per-file checksums to avoid reprocessing duplicates. Outline services, formats, and a test plan?","answer":"Leverage a Glue ETL Spark job to read s3://partner-data/raw/date=YYYY-MM-DD/file.jsonl, explode nested arrays, cast to Parquet types, and write to s3://lake/partner/YYYY/MM/DD/ with daily partitioning","explanation":"## Why This Is Asked\nTests practical, beginner-friendly use of Glue for JSON ingestion, Parquet output, and cataloging, plus a starter approach to schema evolution and data quality. The idempotency pattern with per-file checksums introduces a realistic reliability constraint.\n\n## Key Concepts\n- AWS Glue ETL (Spark) for JSON to Parquet\n- Partitioned Parquet in S3 (YYYY/MM/DD)\n- Glue Data Catalog integration and schema evolution (mergeSchema)\n- Simple idempotency using per-file checksums in DynamoDB\n- Basic data quality checks (null counts, type validation)\n\n## Code Example\n```python\n# PySpark (Glue) sketch\nfrom pyspark.sql.functions import explode, col\ndf = spark.read.json(\"s3://partner-data/raw/date=YYYY-MM-DD/file.jsonl\")\n# flatten nested arrays if present\ndf2 = df.withColumn(\"items\", explode(col(\"nested\").getItem(\"array\")))\n# write with partitioning\nout_path = \"s3://lake/partner/YYYY/MM/DD/\"\ndf2.write.partitionBy(\"year\",\"month\",\"day\").mode(\"overwrite\").parquet(out_path)\n```\n\n## Follow-up Questions\n- How would you handle late-arriving files or schema drift over time?\n- What monitoring would you add to detect missing daily files or failed runs?","diagram":"flowchart TD\n  A[S3 partner raw] --> B[Glue ETL Job]\n  B --> C[Flatten & Cast]\n  C --> D[Write Parquet to lake]\n  D --> E[Glue Catalog Registration]\n  E --> F[DynamoDB checksum store]\n  F --> G[Athena/BI queries]","difficulty":"beginner","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Coinbase","Hugging Face"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T17:38:09.153Z","createdAt":"2026-01-25T17:38:09.153Z"},{"id":"q-7353","question":"Design a beginner-friendly ingestion pipeline for two app sources that emit daily JSON logs into separate S3 prefixes: app1/logs/YYYY/MM/DD/log.json and app2/logs/YYYY/MM/DD/log.json. Create a pipeline to land a unified Parquet dataset in s3://data-lake/cleaned/logs/YYYY/MM/DD/, partitioned by date, using AWS Glue for ETL and cataloging. Include schema evolution handling for new optional fields, deduplication by event_id, basic data quality checks, and a minimal end-to-end test plan. Which steps and services would you choose?","answer":"Use two Glue crawlers to infer schemas, a PySpark Glue job to union both sources, deduplicate on event_id, normalize optional fields into a unified schema, and write Parquet partitioned by date to s3:","explanation":"## Why This Is Asked\nTests ability to design a practical, beginner-friendly ETL using AWS Glue that handles multiple sources, schema drift, dedup, and validation, plus a concrete end-to-end test plan.\n\n## Key Concepts\n- Glue crawlers and dynamic frames for schema inference and evolution\n- Merging JSON sources with a unified Parquet target and date partitioning\n- Deduplication on event_id to handle replays\n- Basic data quality checks (nulls, types) and schema evolution strategy\n- End-to-end validation using Athena queries and simple alerts\n\n## Code Example\n```python\n# PySpark skeleton (Glue job) to deduplicate and unify schemas\nfrom awsglue.context import GlueContext\nfrom pyspark.context import SparkContext\nsc = SparkContext()\ngc = GlueContext(sc)\nspark = gc.spark_session\npaths = [\"s3://data-lake/raw/app1/\", \"s3://data-lake/raw/app2/\"]\nf = spark.read.json(paths).selectExpr(\"*\", \"cast(event_id as string) as event_id\")\ndf = f.dropDuplicates([\"event_id\"]).withColumnRenamed(\"date\", \"partition_date\")\ndf.write.parquet(\"s3://data-lake/cleaned/logs/YYYY/MM/DD/\", mode=\"append\")\n```\n\n## Follow-up Questions\n- How would you handle late-arriving data or corrections after initial ingest?\n- How would you monitor data quality and schema drift over time?","diagram":"flowchart TD\n  App1JSON[App1 JSON] --> ETL[Glue ETL Job]\n  App2JSON[App2 JSON] --> ETL\n  ETL --> Parquet[S3: data-lake/cleaned/logs/YYYY/MM/DD/]\n  Parquet --> Catalog[Glue Data Catalog]\n  Catalog --> Athena","difficulty":"beginner","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Cloudflare"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T19:42:51.312Z","createdAt":"2026-01-25T19:42:51.312Z"},{"id":"q-7536","question":"In a 3-region AWS data lake, ingest real-time user clickstream from Kinesis Data Streams into an Iceberg table on S3. Implement column-level redaction and tokenization for PII at ingestion, while enabling full data access for data science. Detail the architecture, data formats, masking strategy, access controls, and testing/validation approach including cross-region replication and auditability?","answer":"Ingest via KDS and a Lambda/Glue processor that applies column-level masking and tokenization (e.g., redact emails, tokenize 16-digit IDs) before writing to an Iceberg table on S3. Use Glue Data Catal","explanation":"## Why This Is Asked\n\nThis question probes practical data masking at ingestion, multi-region governance, and the end-to-end path from streaming to a structured, queryable Iceberg table. It tests choices around streaming services, masking patterns, and cross-account auditability.\n\n## Key Concepts\n\n- Streaming ingestion (Kinesis Data Streams, Lambda/Glue)\n- Iceberg on S3, schema evolution\n- Column-level masking and tokenization\n- Lake Formation, cross-region replication, data lineage\n- Validation via synthetic data and end-to-end tests\n\n## Code Example\n\n```javascript\n// Masking example\nfunction maskRecord(record) {\n  const r = {...record};\n  if ('email' in r) r.email = 'REDACTED';\n  if ('phone' in r) r.phone = r.phone.substring(0,3) + '****';\n  if ('ssn' in r) r.ssn = '***-**-****';\n  return r;\n}\n```\n\n## Follow-up Questions\n\n- How would you test masking correctness across region boundaries?\n- What if a data scientist needs access to non-PII fields but not masked values?","diagram":"flowchart TD\n  A[Kinesis Data Streams] --> B[Ingestion & Masking (Lambda/Glue)]\n  B --> C[Iceberg Table on S3]\n  C --> D[Athena/Glue Queries]\n  D --> E[Role-based Access (Lake Formation)]\n  E --> F[Cross-region Replication & Audits]","difficulty":"advanced","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","IBM","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T06:55:11.114Z","createdAt":"2026-01-26T06:55:11.114Z"},{"id":"q-7567","question":"Design a beginner-friendly ingestion pipeline for a daily CSV feed arriving to s3://data-lake/raw/activities/YYYY/MM/DD/activities.csv. The schema includes user_id (string), event_type (string), event_ts (timestamp), metadata (string, nullable). Convert to Parquet, partition by date, and publish in s3://data-lake/warehouse/activities/YYYY/MM/DD/ and register in Glue Data Catalog. Include schema evolution plan, basic data quality checks, and a minimal end-to-end test plan?","answer":"Use an AWS Glue Spark job to read the daily CSV with a defined schema (user_id string, event_type string, event_ts timestamp, metadata string nullable), write Parquet to a date-partitioned path, and u","explanation":"## Why This Is Asked\n\nAssesses practical, beginner-friendly data ingestion with Glue, including partitioned Parquet output, catalog updates, and basic quality checks.\n\n## Key Concepts\n\n- AWS Glue Spark jobs\n- Parquet partitioning by date\n- Glue Data Catalog\n- Schema evolution (nullable columns)\n- Data quality checks (not-null, timestamp validity)\n\n## Code Example\n\n```javascript\n// Data quality check example (JS-like pseudocode for illustration)\nfunction validRecord(rec){\n  if (!rec.user_id || typeof rec.user_id !== 'string') return false;\n  if (!rec.event_ts || isNaN(Date.parse(rec.event_ts))) return false;\n  return true;\n}\n```\n\n## Follow-up Questions\n\n- How would you handle late-arriving files or reprocessing failed files without duplicating data?\n- How would you adapt the pipeline for multi-tenant inputs with evolving schemas?","diagram":null,"difficulty":"beginner","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","PayPal","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T07:53:30.514Z","createdAt":"2026-01-26T07:53:30.514Z"},{"id":"q-7725","question":"Design an advanced, multi-region AWS data lake pipeline that ingests streaming and batch sources, enforces strict tenant isolation, and supports upserts with schema drift. Detail your choice between Apache Iceberg on S3 versus Parquet, how you implement governance via Lake Formation, orchestration with Glue and Step Functions, and how you validate end-to-end analytics reproducibility across regions?","answer":"Leverage Iceberg on S3 for upserts and schema evolution, partitioned by tenant and date. Use Glue Catalog and Lake Formation for strict isolation, with KMS-encrypted S3 buckets. Ingest streams via Kin","explanation":"## Why This Is Asked\n\nTests experience designing multi-region data lakes with mixed sources, strict isolation, upserts, and schema drift handling.\n\n## Key Concepts\n\n- Apache Iceberg on S3 for upserts and schema evolution\n- Lake Formation for tenant isolation and fine-grained permissions\n- Glue/Step Functions for orchestration and reproducible pipelines\n- End-to-end testing for cross-region analytics reproducibility\n\n## Code Example\n\n```sql\nMERGE INTO iceberg_table AS t USING staged AS s ON t.id = s.id\nWHEN MATCHED THEN UPDATE SET t.value = s.value\nWHEN NOT MATCHED THEN INSERT (id, value) VALUES (s.id, s.value);\n```\n\n## Follow-up Questions\n\n- How would you monitor for schema drift and trigger a migration workflow?\n- What are the trade-offs between Iceberg and Parquet in this setup?","diagram":null,"difficulty":"advanced","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","DoorDash","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T15:07:38.414Z","createdAt":"2026-01-26T15:07:38.415Z"},{"id":"q-7959","question":"Design an end-to-end pipeline to ingest high-volume IoT telemetry where devices emit protobuf payloads into AWS; propose a streaming path using Kinesis, a Spark/Glue ETL to Parquet in S3 partitioned by day, handle protobuf schema evolution via Glue Schema Registry, ensure idempotent processing and data-quality checks, and outline verification and cost considerations?","answer":"Use Kinesis Data Streams to ingest protobuf payloads, decode with Glue Schema Registry, and run a Glue Spark streaming job to parse and write Parquet to S3 partitioned by date. Manage protobuf evoluti","explanation":"## Why This Is Asked\nTests knowledge of protobuf ingestion, schema evolution, and end-to-end testing in AWS data lakes.\n\n## Key Concepts\n- Streaming ingestion, protobuf decoding, Glue Schema Registry, Parquet partitioning\n- Idempotent processing, upserts, data quality checks\n- End-to-end testing, cost awareness\n\n## Code Example\n```python\n# Pseudocode: Spark Structured Streaming in Glue to decode protobuf and write Parquet\nfrom pyspark.sql.functions import col\ninput_df = spark.readStream.format(\"kinesis\").option(\"streamName\",\"telemetry\").load()\ndecoded = input_df.withColumn(\"payload\", decode_protobuf_udf(col(\"data\")))\nout = decoded.select(\"payload.*\").withColumn(\"date\", col(\"timestamp\").cast(\"date\"))\nquery = out.writeStream.format(\"parquet\").option(\"path\",\"s3://bucket/telemetry/\").partitionBy(\"date\").start()\n```\n\n## Follow-up Questions\n- How would you handle schema evolution conflicts when a new field is added?\n- How would you implement idempotence if a record arrives twice?\n","diagram":"flowchart TD\n  A[IoT protobuf events] --> B[Kinesis Data Streams]\n  B --> C[Glue Streaming job]\n  C --> D[S3 Parquet (date partitions)]\n  D --> E[Athena/Glue catalog]\n","difficulty":"intermediate","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","Netflix","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T02:43:19.166Z","createdAt":"2026-01-27T02:43:19.166Z"},{"id":"q-886","question":"Scenario: you manage a financial data lake with multiple transactional sources feeding S3 via DMS. Stakeholders require upserts, full history for compliance, fast dashboards using a latest-state table, and seamless schema evolution. Compare Iceberg, Hudi, and Delta Lake for this scenario and outline a concrete pipeline (CDC, ETL, compaction, governance)?","answer":"Use Apache Iceberg on S3 to support CDC upserts and full history. Ingest via DMS into a staging Parquet layer, then Spark MERGE INTO to the Iceberg table; maintain a separate 'latest' snapshot for das","explanation":"## Why This Is Asked\nFinancial data lakes require reliable upserts, auditability, and governance. This question probes familiarity with lakehouse formats and practical pipeline design.\n\n## Key Concepts\n- Change Data Capture (CDC) via DMS into S3-backed lakehouse\n- Upserts and history with Iceberg vs Hudi/Delta\n- Schema evolution and partitioning strategy\n- Data governance and validation checks\n\n## Code Example\n```javascript\nCREATE TABLE analytics.fact_sales (\n  id BIGINT,\n  amount DECIMAL(12,2),\n  ts TIMESTAMP,\n  source STRING\n)\nUSING ICEBERG\nPARTITIONED BY (years(ts), months(ts), source);\n\nMERGE INTO analytics.fact_sales_latest AS target\nUSING analytics.fact_sales_staging AS src\nON target.id = src.id\nWHEN MATCHED THEN UPDATE SET amount = src.amount, ts = src.ts\nWHEN NOT MATCHED THEN INSERT (id, amount, ts, source) VALUES (src.id, src.amount, src.ts, src.source);\n```\n\n## Follow-up Questions\n- How would you handle late-arriving data and out-of-order events?\n- What monitoring would you implement for compaction and schema changes?","diagram":"flowchart TD\n  A[CDC Source] --> B[Staging Parquet]\n  B --> C[Spark MERGE to Iceberg]\n  C --> D[Latest Snapshot]\n  D --> E[Athena/BI Dashboards]","difficulty":"intermediate","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T14:25:05.485Z","createdAt":"2026-01-12T14:25:05.485Z"},{"id":"q-948","question":"In a multi-tenant data lake on S3 across two AWS accounts, each tenant's data must be isolated, with auditable access, cost accounting, and fast analytics for dashboards. Design a solution using AWS Lake Formation for governance, S3 prefixes per tenant, and Athena/Glue for analytics. Explain cross-account sharing, tag-based access, and data lineage, plus failure modes?","answer":"Use Lake Formation governance with per-tenant databases mapped to S3 prefixes (s3://lake/tenant-id/...). Expose cross-account access via resource links and tag-based ACLs, audited by Lake Formation an","explanation":"## Why This Is Asked\nTests mastery of cross-account governance, tenant isolation, auditable access, and cost reporting in a real-world data lake.\n\n## Key Concepts\n- Lake Formation governance and resource links\n- Per-tenant S3 prefixes and Glue Data Catalog\n- Tag-based access control and cost allocation\n- Cross-account sharing and CloudTrail/audit trails\n- Data lineage and auditability\n\n## Code Example\n```bash\n# Grant a tenant role read on its catalog table (illustrative)\naws lakeformation grant-permissions --principal '{\"EffectiveFrom\": \"2026-01-12T00:00:00Z\",\"DataLakePrincipalIdentifier\": \"arn:aws:iam::111122223333:role/tenantA\"}' --permissions SELECT --resource '{\"Table\": {\"DatabaseName\": \"tenantA_db\", \"Name\": \"events\"}}'\n```\n\n## Follow-up Questions\n- How would you handle offboarding a tenant's data while preserving analytics history?\n- How would you test isolation and audit completeness in CI/CD?\n","diagram":null,"difficulty":"advanced","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Discord","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T16:36:54.412Z","createdAt":"2026-01-12T16:36:54.412Z"}],"subChannels":["general"],"companies":["Adobe","Airbnb","Amazon","Anthropic","Apple","Bloomberg","Citadel","Cloudflare","Coinbase","Databricks","Discord","DoorDash","Goldman Sachs","Google","Hashicorp","Hugging Face","IBM","Instacart","LinkedIn","Lyft","Meta","Microsoft","MongoDB","NVIDIA","Netflix","OpenAI","Oracle","PayPal","Plaid","Robinhood","Salesforce","Scale Ai","Slack","Snap","Snowflake","Square","Stripe","Tesla","Twitter","Two Sigma","Uber","Zoom"],"stats":{"total":87,"beginner":26,"intermediate":32,"advanced":29,"newThisWeek":37}}