{"questions":[{"id":"aws-data-engineer-data-ingestion-1768148517041-0","question":"You are designing a data ingestion pipeline for streaming IoT telemetry. The system must ingest up to 15,000 events per second, apply on-the-fly transformations to standardize the payloads, and store the results in S3 in Parquet format for Athena queries. The input schema evolves over time with new fields, and you want minimal maintenance when fields are added. Which architecture best meets these requirements?","answer":"[{\"id\":\"a\",\"text\":\"Store raw JSON in S3 and run a nightly EMR job to parse and convert to Parquet\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Ingest with Kinesis Data Streams, transform with AWS Glue Streaming ETL, and write Parquet to S3; use Glue Data Catalog for schema evolution\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Ingest with SQS, process with Lambda, and store as CSV in S3\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use a custom on-prem ETL tool to batch process data daily\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct because it uses a high-throughput streaming path (Kinesis Data Streams) with streaming ETL (Glue Streaming ETL) to perform on-the-fly transformations and write Parquet to S3 for Athena. It also leverages Glue Data Catalog for schema evolution, allowing new fields to be incorporated without breaking existing pipelines. The other options rely on batch processing (A), or consume events with SQS + Lambda in a manner that doesn't scale to 15k events/sec or provide robust schema evolution (C), or depend on an on-prem batch tool with no streaming capability (D).\n\n## Why Other Options Are Wrong\n- Option A: Batch processing cannot satisfy near real-time ingestion and lacks incremental transformation.\n- Option C: SQS + Lambda is not designed for such high event throughput and lacks built-in schema evolution support.\n- Option D: On-prem batch processing introduces latency and maintenance burden not aligned with cloud-native scalability.\n\n## Key Concepts\n- Streaming ingestion with Kinesis Data Streams\n- AWS Glue Streaming ETL or Glue pipelines\n- Parquet format and Athena compatibility\n- Glue Data Catalog for schema evolution\n\n## Real-World Application\n- This pattern is used for real-time analytics on IoT telemetry, enabling fast queries on clean Parquet data without rebuilding pipelines on schema changes.","diagram":null,"difficulty":"intermediate","tags":["AWS","Kinesis","Glue","S3","Parquet","Athena","DataIngestion","Streaming","certification-mcq","domain-weight-34"],"channel":"aws-data-engineer","subChannel":"data-ingestion","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T16:21:57.044Z","createdAt":"2026-01-11 16:21:57"},{"id":"aws-data-engineer-data-ingestion-1768148517041-1","question":"You have multiple teams ingesting JSON logs into S3 with evolving fields. You want to query with Athena without breaking existing pipelines, and you want to support schema evolution. Which approach is best?","answer":"[{\"id\":\"a\",\"text\":\"Enforce a fixed schema by rewriting all files to a single schema upon ingestion\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use AWS Glue Schema Registry to maintain schema evolution and use Glue Tables with dynamic frames\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Convert all data to CSV before storing in S3\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Ignore schema evolution and adjust queries to handle optional fields\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct because AWS Glue Schema Registry provides centralized schema governance that supports evolution across multiple producers. When combined with Glue Tables and dynamic frames, downstream queries (e.g., Athena) can adapt to changing fields without breaking pipelines.\n\n## Why Other Options Are Wrong\n- Option A: Forcing a fixed schema increases maintenance, causes churn with new fields, and breaks multi-team ingestion.\n- Option C: CSV reduces schema flexibility and complicates semi-structured JSON handling in Athena.\n- Option D: Ignoring evolution leads to fractured schemas and failed queries.\n\n## Key Concepts\n- AWS Glue Schema Registry\n- Glue Data Catalog / Tables\n- Schema evolution in data lakes\n\n## Real-World Application\n- Enables multiple teams to contribute JSON logs with evolving schemas while keeping Athena queries stable and future-proof.","diagram":null,"difficulty":"intermediate","tags":["AWS","Glue","SchemaRegistry","Athena","S3","DataIngestion","DataCatalog","certification-mcq","domain-weight-34"],"channel":"aws-data-engineer","subChannel":"data-ingestion","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T16:21:57.390Z","createdAt":"2026-01-11 16:21:57"},{"id":"aws-data-engineer-data-ingestion-1768148517041-2","question":"You operate an on-prem Oracle database and a Redshift data warehouse. You need near real-time changes to reflect in Redshift and preserve historical rows (SCD Type 2). Which approach best accomplishes this with minimal custom coding?","answer":"[{\"id\":\"a\",\"text\":\"Configure an AWS DMS CDC task to Redshift and implement MERGE statements in Redshift to perform SCD Type 2\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Configure DMS CDC to S3 and run a Glue job to merge into Redshift\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Build a custom log-based replication service on-prem and push changes to Redshift\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Schedule hourly full table dumps from Oracle to Redshift\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A is correct because AWS DMS can capture CDC from an on-prem Oracle source and replicate to Redshift with low latency. Implementing MERGE statements in Redshift enables SCD Type 2 to preserve history with minimal custom code beyond the MERGE logic.\n\n## Why Other Options Are Wrong\n- Option B: Adds an extra staging step (S3) and delayed merging, increasing latency and complexity.\n- Option C: Requires building a custom solution with significant maintenance.\n- Option D: Full dumps cause high latency and data staleness, not near real-time.\n\n## Key Concepts\n- AWS DMS CDC for Oracle to Redshift\n- Redshift MERGE for SCD Type 2\n- Change data capture patterns\n\n## Real-World Application\n- Keeps dimensional history accurate in a data warehouse while minimizing custom ETL development.","diagram":null,"difficulty":"intermediate","tags":["AWS","DMS","Redshift","CDC","SCD","DataIngestion","Oracle","certification-mcq","domain-weight-34"],"channel":"aws-data-engineer","subChannel":"data-ingestion","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T16:21:57.734Z","createdAt":"2026-01-11 16:21:57"}],"subChannels":["data-ingestion"],"companies":[],"stats":{"total":3,"beginner":0,"intermediate":3,"advanced":0,"newThisWeek":3}}