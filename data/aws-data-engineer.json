{"questions":[{"id":"aws-data-engineer-data-ingestion-1768148517041-0","question":"You are designing a data ingestion pipeline for streaming IoT telemetry. The system must ingest up to 15,000 events per second, apply on-the-fly transformations to standardize the payloads, and store the results in S3 in Parquet format for Athena queries. The input schema evolves over time with new fields, and you want minimal maintenance when fields are added. Which architecture best meets these requirements?","answer":"[{\"id\":\"a\",\"text\":\"Store raw JSON in S3 and run a nightly EMR job to parse and convert to Parquet\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Ingest with Kinesis Data Streams, transform with AWS Glue Streaming ETL, and write Parquet to S3; use Glue Data Catalog for schema evolution\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Ingest with SQS, process with Lambda, and store as CSV in S3\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use a custom on-prem ETL tool to batch process data daily\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct because it uses a high-throughput streaming path (Kinesis Data Streams) with streaming ETL (Glue Streaming ETL) to perform on-the-fly transformations and write Parquet to S3 for Athena. It also leverages Glue Data Catalog for schema evolution, allowing new fields to be incorporated without breaking existing pipelines. The other options rely on batch processing (A), or consume events with SQS + Lambda in a manner that doesn't scale to 15k events/sec or provide robust schema evolution (C), or depend on an on-prem batch tool with no streaming capability (D).\n\n## Why Other Options Are Wrong\n- Option A: Batch processing cannot satisfy near real-time ingestion and lacks incremental transformation.\n- Option C: SQS + Lambda is not designed for such high event throughput and lacks built-in schema evolution support.\n- Option D: On-prem batch processing introduces latency and maintenance burden not aligned with cloud-native scalability.\n\n## Key Concepts\n- Streaming ingestion with Kinesis Data Streams\n- AWS Glue Streaming ETL or Glue pipelines\n- Parquet format and Athena compatibility\n- Glue Data Catalog for schema evolution\n\n## Real-World Application\n- This pattern is used for real-time analytics on IoT telemetry, enabling fast queries on clean Parquet data without rebuilding pipelines on schema changes.","diagram":null,"difficulty":"intermediate","tags":["AWS","Kinesis","Glue","S3","Parquet","Athena","DataIngestion","Streaming","certification-mcq","domain-weight-34"],"channel":"aws-data-engineer","subChannel":"data-ingestion","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T16:21:57.044Z","createdAt":"2026-01-11 16:21:57"},{"id":"aws-data-engineer-data-ingestion-1768148517041-1","question":"You have multiple teams ingesting JSON logs into S3 with evolving fields. You want to query with Athena without breaking existing pipelines, and you want to support schema evolution. Which approach is best?","answer":"[{\"id\":\"a\",\"text\":\"Enforce a fixed schema by rewriting all files to a single schema upon ingestion\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use AWS Glue Schema Registry to maintain schema evolution and use Glue Tables with dynamic frames\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Convert all data to CSV before storing in S3\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Ignore schema evolution and adjust queries to handle optional fields\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct because AWS Glue Schema Registry provides centralized schema governance that supports evolution across multiple producers. When combined with Glue Tables and dynamic frames, downstream queries (e.g., Athena) can adapt to changing fields without breaking pipelines.\n\n## Why Other Options Are Wrong\n- Option A: Forcing a fixed schema increases maintenance, causes churn with new fields, and breaks multi-team ingestion.\n- Option C: CSV reduces schema flexibility and complicates semi-structured JSON handling in Athena.\n- Option D: Ignoring evolution leads to fractured schemas and failed queries.\n\n## Key Concepts\n- AWS Glue Schema Registry\n- Glue Data Catalog / Tables\n- Schema evolution in data lakes\n\n## Real-World Application\n- Enables multiple teams to contribute JSON logs with evolving schemas while keeping Athena queries stable and future-proof.","diagram":null,"difficulty":"intermediate","tags":["AWS","Glue","SchemaRegistry","Athena","S3","DataIngestion","DataCatalog","certification-mcq","domain-weight-34"],"channel":"aws-data-engineer","subChannel":"data-ingestion","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T16:21:57.390Z","createdAt":"2026-01-11 16:21:57"},{"id":"aws-data-engineer-data-ingestion-1768148517041-2","question":"You operate an on-prem Oracle database and a Redshift data warehouse. You need near real-time changes to reflect in Redshift and preserve historical rows (SCD Type 2). Which approach best accomplishes this with minimal custom coding?","answer":"[{\"id\":\"a\",\"text\":\"Configure an AWS DMS CDC task to Redshift and implement MERGE statements in Redshift to perform SCD Type 2\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Configure DMS CDC to S3 and run a Glue job to merge into Redshift\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Build a custom log-based replication service on-prem and push changes to Redshift\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Schedule hourly full table dumps from Oracle to Redshift\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A is correct because AWS DMS can capture CDC from an on-prem Oracle source and replicate to Redshift with low latency. Implementing MERGE statements in Redshift enables SCD Type 2 to preserve history with minimal custom code beyond the MERGE logic.\n\n## Why Other Options Are Wrong\n- Option B: Adds an extra staging step (S3) and delayed merging, increasing latency and complexity.\n- Option C: Requires building a custom solution with significant maintenance.\n- Option D: Full dumps cause high latency and data staleness, not near real-time.\n\n## Key Concepts\n- AWS DMS CDC for Oracle to Redshift\n- Redshift MERGE for SCD Type 2\n- Change data capture patterns\n\n## Real-World Application\n- Keeps dimensional history accurate in a data warehouse while minimizing custom ETL development.","diagram":null,"difficulty":"intermediate","tags":["AWS","DMS","Redshift","CDC","SCD","DataIngestion","Oracle","certification-mcq","domain-weight-34"],"channel":"aws-data-engineer","subChannel":"data-ingestion","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T16:21:57.734Z","createdAt":"2026-01-11 16:21:57"},{"id":"aws-data-engineer-data-ingestion-1768246149093-0","question":"A real-time streaming pipeline collects user click events from multiple front-end apps and stores them in an S3 data lake for analytics. You want minimal operational overhead and to apply a lightweight transformation during ingestion. Which architecture best meets these requirements?","answer":"[{\"id\":\"a\",\"text\":\"Use Kinesis Data Firehose with a Lambda transformation to S3\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Use a self-managed Spark job on EMR to write to S3\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use Kinesis Data Streams with a custom EC2 consumer to write to S3\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use AWS Glue Elastic Views to replicate data to S3\",\"isCorrect\":false}]","explanation":"## Correct Answer\nKinesis Data Firehose with a Lambda transformation to S3 is correct because Firehose provides serverless, managed ingestion to S3 and Lambda can perform on-the-fly transformations without managing servers.\n\n## Why Other Options Are Wrong\n- Use a self-managed Spark job on EMR to write to S3: EMR requires managing and provisioning clusters, increasing operational overhead.\n- Use Kinesis Data Streams with a custom EC2 consumer to write to S3: This is not serverless and requires managing EC2 instances.\n- Use AWS Glue Elastic Views to replicate data to S3: Elastic Views focuses on materialized views across data stores, not a real-time streaming ingestion path to S3 with lightweight inline transformation.\n\n## Key Concepts\n- Serverless ingestion pipelines (Kinesis Data Firehose)\n- Lightweight streaming transforms (Lambda)\n- Data lake ingestion to S3\n\n## Real-World Application\n- Suitable for real-time log/ clickstream ingestion into a data lake with minimal operations and on-the-fly format adjustments.","diagram":null,"difficulty":"intermediate","tags":["AWS Glue","Amazon Kinesis","Kinesis Data Firehose","Lambda","S3","Serverless","Data Ingestion","certification-mcq","domain-weight-34"],"channel":"aws-data-engineer","subChannel":"data-ingestion","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T19:29:09.094Z","createdAt":"2026-01-12 19:29:09"},{"id":"aws-data-engineer-data-ingestion-1768246149093-1","question":"You are migrating an on-premises MySQL database to AWS and require near-zero downtime while keeping the target data lake up to date. Which AWS service best supports continuous CDC replication from on-premises to AWS with minimal downtime?","answer":"[{\"id\":\"a\",\"text\":\"AWS Data Migration Service (DMS) with ongoing replication to S3 or Redshift\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"AWS Glue ETL job scheduled nightly\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"AWS Snowball Edge\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Amazon MQ\",\"isCorrect\":false}]","explanation":"## Correct Answer\nAWS Data Migration Service (DMS) supports continuous change data capture (CDC) from on-premises databases to AWS targets such as S3 and Redshift, enabling near-zero downtime migrations.\n\n## Why Other Options Are Wrong\n- AWS Glue ETL scheduled nightly cannot provide continuous CDC or zero-downtime migration.\n- AWS Snowball Edge is for offline bulk transfer, not continuous replication.\n- Amazon MQ is a managed messaging service and not suitable for database replication.\n\n## Key Concepts\n- Change data capture (CDC)\n- AWS Data Migration Service (DMS)\n- Near-zero downtime migrations\n\n## Real-World Application\n- Migrating production OLTP to a data lake/Analytics platform with minimal disruption during cutover.","diagram":null,"difficulty":"intermediate","tags":["AWS DMS","CDC","MySQL","S3","Redshift","On-Premises","certification-mcq","domain-weight-34"],"channel":"aws-data-engineer","subChannel":"data-ingestion","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T19:29:09.619Z","createdAt":"2026-01-12 19:29:10"},{"id":"aws-data-engineer-data-ingestion-1768246149093-2","question":"For a telemetry stream from IoT devices, you need to perform Spark-based transformations in a serverless manner and avoid managing clusters. Which AWS service should you choose?","answer":"[{\"id\":\"a\",\"text\":\"AWS Lambda with Kinesis\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"AWS Glue Streaming ETL (Spark)\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Amazon EMR cluster\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"AWS Data Pipeline\",\"isCorrect\":false}]","explanation":"## Correct Answer\nAWS Glue Streaming ETL uses Spark under the hood in a serverless fashion, enabling real-time transformations without managing a Spark cluster.\n\n## Why Other Options Are Wrong\n- AWS Lambda with Kinesis can do stream processing but does not provide Spark-based transformations.\n- Amazon EMR requires managing a cluster, which contradicts the serverless requirement.\n- AWS Data Pipeline is an older service with limited real-time streaming capabilities.\n\n## Key Concepts\n- Serverless streaming ETL\n- AWS Glue Streaming (Spark)\n- IoT data pipelines\n\n## Real-World Application\n- Real-time analytics on IoT telemetry without managing Spark infrastructure.","diagram":null,"difficulty":"intermediate","tags":["AWS Glue","Glue Streaming","Apache Spark","IoT","Serverless","Streaming ETL","certification-mcq","domain-weight-34"],"channel":"aws-data-engineer","subChannel":"data-ingestion","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T19:29:10.218Z","createdAt":"2026-01-12 19:29:10"},{"id":"aws-data-engineer-data-ingestion-1768246149093-3","question":"To enforce a published schema on streaming data before it lands in S3 for analytics, which AWS feature best supports schema governance and runtime validation across producers and consumers?","answer":"[{\"id\":\"a\",\"text\":\"AWS Glue Schema Registry\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Lake Formation Permissions\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"AWS Glue Crawlers\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"S3 Object Lock\",\"isCorrect\":false}]","explanation":"## Correct Answer\nAWS Glue Schema Registry provides a central schema registry for streaming data, enabling schema governance and runtime validation across producers and consumers before data lands in S3.\n\n## Why Other Options Are Wrong\n- Lake Formation Permissions govern access, not schema validation for streaming data.\n- AWS Glue Crawlers infer and catalog schemas but do not enforce runtime validation on write across producers.\n- S3 Object Lock is for object immutability, not schema governance.\n\n## Key Concepts\n- Schema Registry for streaming data\n- Data governance\n- Runtime validation\n\n## Real-World Application\n- Ensures downstream analytics see consistent, validated data formats.","diagram":null,"difficulty":"intermediate","tags":["AWS Glue","Glue Schema Registry","Streaming","Schema Governance","S3","Data Quality","certification-mcq","domain-weight-34"],"channel":"aws-data-engineer","subChannel":"data-ingestion","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T19:29:10.396Z","createdAt":"2026-01-12 19:29:10"},{"id":"aws-data-engineer-data-ingestion-1768246149093-4","question":"You need to orchestrate ingestion and transformation across multiple AWS accounts with parameterized workflows and robust error handling. Which AWS service is best suited for this cross-account orchestration?","answer":"[{\"id\":\"a\",\"text\":\"AWS Data Pipeline\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"AWS Step Functions with cross-account IAM roles\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"AWS CloudFormation with nested stacks\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Amazon MQ\",\"isCorrect\":false}]","explanation":"## Correct Answer\nAWS Step Functions with cross-account IAM roles provides stateful orchestration of workflows across accounts, enabling parameterization, retries, and error handling in a scalable way.\n\n## Why Other Options Are Wrong\n- AWS Data Pipeline is older and less flexible for multi-account orchestration.\n- AWS CloudFormation handles infrastructure deployment, not workflow orchestration at runtime.\n- Amazon MQ is a messaging broker and not suitable for end-to-end orchestration.\n\n## Key Concepts\n- State machines and cross-account access\n- Parameterized workflows\n- Error handling and retries\n\n## Real-World Application\n- Coordinating data ingestion and transformation across a multi-account data lake architecture with centralized monitoring.","diagram":null,"difficulty":"intermediate","tags":["AWS Step Functions","IAM","Cross-Account","Orchestration","Multi-Account","Terraform","certification-mcq","domain-weight-34"],"channel":"aws-data-engineer","subChannel":"data-ingestion","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T19:29:10.585Z","createdAt":"2026-01-12 19:29:10"},{"id":"aws-data-engineer-data-operations-1768216823632-0","question":"You have an analytics data lake on S3 using daily partitions (year, month, day) registered in the AWS Glue Data Catalog and queried via Athena. The partition count has grown to tens of thousands, causing long crawler runs and slow query planning. Which design approach reduces maintenance while preserving fast partition pruning?","answer":"[{\"id\":\"a\",\"text\":\"Implement partition projection on the Glue table for year, month, and day so Athena can prune partitions without listing all partitions.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Reorganize data into a single table with no partitions and rely on file-level filters.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Switch to monthly partitions and keep the same table metadata.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Keep the current partitioning and increase crawler frequency to refresh partitions more often.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct choice is A because partition projection lets Athena prune partitions without enumerating all partitions, reducing maintenance and query planning cost. B would still require manual partition handling; C eliminates partitioning and sacrifices pruning; D increases costs and does not address pruning efficiency.\n\n## Why Other Options Are Wrong\n- B: Moving to monthly partitions reduces count but still requires partition discovery and misses fine-grained pruning.\n- C: A non-partitioned table degrades performance for time-bounded filters.\n- D: Frequent crawls add cost and do not address pruning efficiency.\n\n## Key Concepts\n- AWS Glue Data Catalog\n- Athena partition pruning and partition projection\n\n## Real-World Application\nUsed to scale data lake catalogs with high partition counts while preserving fast ad-hoc query performance.","diagram":null,"difficulty":"intermediate","tags":["AWS","S3","Athena","Glue","PartitionProjection","certification-mcq","domain-weight-22"],"channel":"aws-data-engineer","subChannel":"data-operations","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T11:20:23.634Z","createdAt":"2026-01-12 11:20:23"},{"id":"aws-data-engineer-data-operations-1768216823632-1","question":"A Glue ETL job ingests data from a MySQL RDS source and writes Parquet to S3 before loading into Redshift. A new column is added in the source and occasionally causes a failure due to a schema mismatch. Which approach best ensures the job remains resilient to new columns without failing?","answer":"[{\"id\":\"a\",\"text\":\"Use AWS Glue DynamicFrame with ResolveChoice and ApplyMapping to map to the target schema and ignore extra columns via DropFields.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Hard-code the target schema to include the new column to catch up automatically.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Disable schema checks entirely and always write as string type.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Switch to a different ETL tool that ignores schema drift.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A is correct because Glue DynamicFrame and ResolveChoice allow the pipeline to adapt to drift and map to the stable target schema while ignoring unexpected new columns, preventing failures. B locks the target schema to a new field, causing more frequent changes and potential failures; C sacrifices data typing and can lead to data quality issues; D avoids Glue's native, scalable handling of schema drift.\n\n## Why Other Options Are Wrong\n- B: Forcing the new column into the target schema requires schema changes and redeploys, causing failures.\n- C: Dropping type safety leads to downstream ambiguity and issues.\n- D: Introducing another tool adds complexity and may not integrate with existing catalog and load steps.\n\n## Key Concepts\n- AWS Glue DynamicFrame\n- ResolveChoice, ApplyMapping, DropFields\n- Schema drift handling in ETL pipelines\n\n## Real-World Application\nKeeps data pipelines resilient to evolving source schemas with minimal downtime and manual intervention.","diagram":null,"difficulty":"intermediate","tags":["AWS","Glue","Parquet","Redshift","SchemaDrift","certification-mcq","domain-weight-22"],"channel":"aws-data-engineer","subChannel":"data-operations","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T11:20:23.993Z","createdAt":"2026-01-12 11:20:24"},{"id":"aws-data-engineer-data-operations-1768216823632-2","question":"In a multi-environment AWS data lake deployed with Terraform, you need dev/stage/prod with isolated infrastructure states. Which practice best prevents cross-environment drift and accidental changes?","answer":"[{\"id\":\"a\",\"text\":\"Maintain separate remote state per environment, using distinct backends and state files.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Use a single backend and rely on a single state for all environments.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Only rely on Git branches; state remains in local files.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Manually reproduce resources in each environment without state management.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A is correct because maintaining separate remote states per environment (or distinct backends) prevents drift and unintentional cross-environment changes, allowing safe, repeatable deployments. B, C, and D either merge states or bypass state management, increasing risk of drift and misconfiguration.\n\n## Why Other Options Are Wrong\n- B: A single shared state risks cross-environment contamination.\n- C: Local state is not shareable or reproducible across teammates or CI.\n- D: Manual reproduction increases inconsistency and drift.\n\n## Key Concepts\n- Terraform state management\n- Remote backends (S3, Terraform Cloud/Enterprise)\n- Environment isolation\n\n## Real-World Application\nEnsures predictable provisioning across dev/stage/prod for data lake components like S3, Glue, and Redshift.","diagram":null,"difficulty":"intermediate","tags":["Terraform","AWS","S3","Glue","Redshift","Kubernetes","certification-mcq","domain-weight-22"],"channel":"aws-data-engineer","subChannel":"data-operations","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T11:20:24.344Z","createdAt":"2026-01-12 11:20:24"},{"id":"aws-data-engineer-data-operations-1768279449739-0","question":"In a data lake on S3, you plan to load daily JSON logs into a partitioned Parquet table via Glue. You want partitions created automatically by date and to support schema evolution as new fields appear in the JSON. Which combination achieves this with minimal maintenance?","answer":"[{\"id\":\"a\",\"text\":\"Use Glue Crawlers with Add new columns to the table and enable partition projection for date-based partitions.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Manually maintain a static schema in the catalog and create partitions by hand.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use Redshift Spectrum external tables with a fixed schema and no automatic schema evolution.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Query the data using a view that abstracts partitions without updating the catalog.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A is correct because Glue Crawlers can automatically discover new data and update the Data Catalog, while setting partition projection enables automatic date-based partitions and supports schema evolution when new fields appear in JSON.\n\n## Why Other Options Are Wrong\n- B: Manual schema maintenance is not scalable and misses new fields automatically.\n- C: Redshift Spectrum with a fixed schema cannot adapt to evolving JSON fields without redefining external tables.\n- D: A view does not create partitions or handle schema evolution; it only presents data.\n\n## Key Concepts\n- AWS Glue Data Catalog\n- Glue Crawlers with Add new columns\n- Partition projection\n- Parquet format and Athena compatibility\n\n## Real-World Application\n- Automates ingestion of evolving JSON data into analytics-ready partitions with minimal maintenance.","diagram":null,"difficulty":"intermediate","tags":["AWS","AWS Glue","Amazon Athena","Amazon S3","certification-mcq","domain-weight-22"],"channel":"aws-data-engineer","subChannel":"data-operations","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T04:44:09.740Z","createdAt":"2026-01-13 04:44:10"},{"id":"aws-data-engineer-data-operations-1768279449739-1","question":"An S3 landing zone receives a new batch file every hour. You want to trigger a long-running Glue ETL workflow without manual starts. Which pattern best fits this requirement?","answer":"[{\"id\":\"a\",\"text\":\"An S3 event triggers a Lambda which starts a Glue workflow.\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"An EventBridge rule detects S3 ObjectCreated events and triggers a Step Functions state machine that starts a Glue workflow.\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"SNS topic triggers an EMR cluster to run the ETL job.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"A manual console start of the Glue job after each batch.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct because event-driven orchestration using EventBridge to trigger a Step Functions state machine provides reliable, scalable coordination for long-running Glue workflows without Lambda timeouts.\n\n## Why Other Options Are Wrong\n- A: Lambda-based triggering can work but has timeout limits and lacks robust orchestration for long-running jobs.\n- C: SNS + EMR is heavier infrastructure and less aligned with modern Glue ETL patterns.\n- D: Manual starts are not automated and reduce reliability.\n\n## Key Concepts\n- Event-driven architecture\n- AWS EventBridge, AWS Step Functions, AWS Glue workflows\n\n## Real-World Application\n- Automates hour-by-hour ETL pipelines with reliable orchestration and error handling.","diagram":null,"difficulty":"intermediate","tags":["AWS","Amazon EventBridge","AWS Step Functions","AWS Glue","Amazon S3","certification-mcq","domain-weight-22"],"channel":"aws-data-engineer","subChannel":"data-operations","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T04:44:10.097Z","createdAt":"2026-01-13 04:44:10"},{"id":"aws-data-engineer-data-operations-1768279449739-2","question":"You need to catalog metadata for datasets stored in S3 and grant fine-grained access (table- and column-level) to multiple principals. Which AWS service combination provides this capability?","answer":"[{\"id\":\"a\",\"text\":\"IAM policies and S3 bucket policies with object-level conditions.\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Lake Formation with the Glue Data Catalog to grant table- and column-level permissions.\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Athena workgroup policies to restrict query access by user.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Quicksight data-source permissions for data access.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct because AWS Lake Formation tightly integrates with the Glue Data Catalog to manage fine-grained access at the table and column level.\n\n## Why Other Options Are Wrong\n- A: IAM/S3 policies cannot enforce column-level data access in a scalable lake scenario.\n- C: Athena workgroups control resources and user access at the query level, not column-level data governance.\n- D: QuickSight permissions govern visualization access, not underlying data lake permissions.\n\n## Key Concepts\n- AWS Lake Formation\n- AWS Glue Data Catalog\n- Fine-grained data access control\n\n## Real-World Application\n- Analysts access only the data they are authorized to see, reducing risk of exposure.","diagram":null,"difficulty":"intermediate","tags":["AWS","Lake Formation","AWS Glue","IAM","certification-mcq","domain-weight-22"],"channel":"aws-data-engineer","subChannel":"data-operations","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T04:44:10.441Z","createdAt":"2026-01-13 04:44:10"},{"id":"aws-data-engineer-data-operations-1768279449739-3","question":"You need to profile and clean a structured dataset as part of a data preparation workflow before ingestion into a data warehouse. Which AWS service is best suited for this task?","answer":"[{\"id\":\"a\",\"text\":\"AWS DataBrew\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"AWS Glue Data Catalog\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Amazon Redshift\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Amazon EMR\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A is correct because AWS DataBrew provides data profiling, cleaning, and preparation without coding, ideal for prep workflows.\n\n## Why Other Options Are Wrong\n- B: Glue Data Catalog stores metadata, not data cleansing or profiling.\n- C: Redshift is a data warehouse, not a data prep tool for raw lake data.\n- D: EMR is a general-purpose cluster; DataBrew offers a focused, low-friction data prep experience.\n\n## Key Concepts\n- AWS DataBrew\n- Data profiling and cleaning\n\n## Real-World Application\n- Streamlines data prep before loading into analytics environments.","diagram":null,"difficulty":"intermediate","tags":["AWS","AWS DataBrew","AWS Glue","certification-mcq","domain-weight-22"],"channel":"aws-data-engineer","subChannel":"data-operations","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T04:44:10.567Z","createdAt":"2026-01-13 04:44:10"},{"id":"aws-data-engineer-data-operations-1768279449739-4","question":"Streaming data is ingested from Kinesis Data Firehose into S3 as Parquet, while the data schema evolves over time. You want to enforce a stable, evolving schema contract across producers and consumers. Which approach is recommended?","answer":"[{\"id\":\"a\",\"text\":\"Use AWS Glue Schema Registry with Apache Avro to enforce a contract and manage evolving schemas.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Rely on S3 object tagging to describe schema changes and adjust consumers accordingly.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use Lake Formation permissions to enforce schema changes.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Store schemas in CloudWatch Logs and fetch them when needed.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A is correct because AWS Glue Schema Registry provides a centralized schema registry for streaming data (e.g., Avro) and enforces compatibility across producers and consumers.\n\n## Why Other Options Are Wrong\n- B: S3 object tagging is insufficient for enforcing schema contracts across a streaming pipeline.\n- C: Lake Formation manages access control, not streaming schema governance.\n- D: CloudWatch Logs are for events/logs, not schema management.\n\n## Key Concepts\n- AWS Glue Schema Registry\n- Kinesis Data Firehose integration\n- Avro schemas and compatibility\n\n## Real-World Application\n- Prevents schema drift in real-time analytics by enforcing a contract between data producers and consumers.","diagram":null,"difficulty":"intermediate","tags":["AWS","AWS Glue Schema Registry","Kinesis Data Firehose","Apache Avro","certification-mcq","domain-weight-22"],"channel":"aws-data-engineer","subChannel":"data-operations","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T04:44:10.692Z","createdAt":"2026-01-13 04:44:10"},{"id":"aws-data-engineer-data-security-1768231788418-0","question":"To enforce encryption at rest for a data lake in S3 using a customer-managed CMK and require that all objects are encrypted with SSE-KMS, what configuration should you implement?","answer":"[{\"id\":\"a\",\"text\":\"Enable default encryption with SSE-KMS using a CMK and attach a bucket policy that requires x-amz-server-side-encryption: aws:kms, and grant kms:Encrypt and kms:Decrypt to the relevant IAM roles\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Enable default encryption with SSE-S3 and rely on IAM policies to restrict usage of encryption keys\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use client-side encryption and store keys in Secrets Manager\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Enable bucket encryption using a CMK but do not enforce encryption on uploads\",\"isCorrect\":false}]","explanation":"## Correct Answer\n- a) Enable default encryption with SSE-KMS using a customer-managed CMK and enforce via bucket policy that all PutObject requests include x-amz-server-side-encryption: aws:kms, and grant kms:Encrypt and kms:Decrypt to the relevant IAM roles.\n\n## Why Other Options Are Wrong\n- b) SSE-S3 does not use a CMK, which fails the requirement for customer-managed key-based encryption at rest.\n- c) Client-side encryption happens before data reaches S3 and is not managed by S3/KMS governance.\n- d) Not enforcing encryption on uploads allows unencrypted objects to be stored, defeating the requirement.\n\n## Key Concepts\n- SSE-KMS, CMKs, and key policies\n- Bucket policies and encryption requirements\n- IAM roles with KMS permissions\n\n## Real-World Application\nIn a production data lake, configure a CMK in KMS, set bucket default encryption to aws:kms, enforce via a bucket policy that all uploads specify the KMS header and key, and grant the ingestion roles the necessary KMS permissions to encrypt data.","diagram":null,"difficulty":"intermediate","tags":["AWS","S3","KMS","IAM","Data Encryption","certification-mcq","domain-weight-18"],"channel":"aws-data-engineer","subChannel":"data-security","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T15:29:48.419Z","createdAt":"2026-01-12 15:29:48"},{"id":"aws-data-engineer-data-security-1768231788418-1","question":"Your data lake spans two AWS accounts and you want centralized governance with auditable access controls. Which approach best achieves this using AWS services?","answer":"[{\"id\":\"a\",\"text\":\"Manage all permissions via IAM roles and bucket policies in each account independently\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use Lake Formation permissions to grant cross-account access to the Data Catalog and S3 locations, enabling centralized governance and auditing\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Use AWS RAM to share the underlying S3 bucket across accounts\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Copy data to the partner account and manage access there\",\"isCorrect\":false}]","explanation":"## Correct Answer\n- b) Use Lake Formation permissions to grant cross-account access to the Data Catalog and S3 locations, enabling centralized governance and auditing.\n\n## Why Other Options Are Wrong\n- a) Per-account IAM/bucket policies lead to policy sprawl and weaker centralized governance.\n- c) RAM shares resources but does not provide Lake Formationâ€™s catalog-based governance and auditing for data lake permissions.\n- d) Copying data to another account bypasses centralized governance and increases data duplication risk.\n\n## Key Concepts\n- AWS Lake Formation permissions model\n- Data Catalog and table-level permissions\n- Data location permissions for S3\n\n## Real-World Application\nUsing Lake Formation to centrally manage permissions across accounts reduces policy fragmentation and provides a consistent audit trail for cross-account data access.","diagram":null,"difficulty":"intermediate","tags":["AWS","Lake Formation","IAM","S3","Data Catalog","certification-mcq","domain-weight-18"],"channel":"aws-data-engineer","subChannel":"data-security","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T15:29:48.764Z","createdAt":"2026-01-12 15:29:49"},{"id":"aws-data-engineer-data-security-1768231788418-2","question":"To audit permission changes to data assets across multiple accounts, which combination provides a reliable, centralized audit trail?","answer":"[{\"id\":\"a\",\"text\":\"Enable CloudTrail Data Events for object-level changes in S3 only\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Enable CloudTrail management events for IAM policy changes and AWS Config across accounts, with a centralized trail\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Rely on IAM Access Analyzer outputs after changes\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use GuardDuty findings to infer policy changes\",\"isCorrect\":false}]","explanation":"## Correct Answer\n- b) Enable CloudTrail management events for IAM policy changes and AWS Config across accounts, with a centralized trail to capture configuration changes.\n\n## Why Other Options Are Wrong\n- a) Data Events cover data plane activity, not policy or configuration changes.\n- c) Access Analyzer helps identify access paths but does not provide a centralized audit log of changes.\n- d) GuardDuty detects threats, not governance policy changes.\n\n## Key Concepts\n- CloudTrail management events\n- AWS Config recording across accounts\n- Centralized logging and aggregation\n\n## Real-World Application\nFor a multi-account data lake, enable cross-account CloudTrail and Config aggregators to maintain a single source of truth for who changed what permissions and when.","diagram":null,"difficulty":"intermediate","tags":["AWS","CloudTrail","AWS Config","IAM","Audit","certification-mcq","domain-weight-18"],"channel":"aws-data-engineer","subChannel":"data-security","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T15:29:49.130Z","createdAt":"2026-01-12 15:29:49"},{"id":"aws-data-engineer-data-security-1768231788418-3","question":"To meet regulatory data retention of 7 years and prevent deletion of critical data stored in S3, which combination provides immutable, durable retention?","answer":"[{\"id\":\"a\",\"text\":\"Enable S3 Object Lock in Compliance mode with a retention period and ensure bucket versioning is enabled\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Enable MFA Delete on the bucket and rely on versioning alone\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Archive data to Glacier with a 7-year retention policy without Object Lock\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Rely on encryption to protect data integrity and manually enforce retention\",\"isCorrect\":false}]","explanation":"## Correct Answer\n- a) Enable S3 Object Lock in Compliance mode with a retention period and ensure bucket versioning is enabled.\n\n## Why Other Options Are Wrong\n- b) MFA Delete is deprecated for most workflows and does not guarantee immutability like Object Lock.\n- c) Glacier retention alone does not guarantee immutability or prevent deletion of live objects.\n- d) Encryption protects confidentiality, not retention or immutability.\n\n## Key Concepts\n- S3 Object Lock, Compliance mode\n- Bucket Versioning\n- Data retention policies and governance\n\n## Real-World Application\nFor regulatory compliance requiring long-term immutability, enable Object Lock in Compliance mode with a defined retention period and keep versioning enabled so that deleted or overwritten objects remain recoverable for the retention window.","diagram":null,"difficulty":"intermediate","tags":["AWS","S3","Object Lock","Versioning","Compliance","certification-mcq","domain-weight-18"],"channel":"aws-data-engineer","subChannel":"data-security","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T15:29:49.255Z","createdAt":"2026-01-12 15:29:49"},{"id":"aws-data-engineer-data-security-1768231788418-4","question":"In a data ingestion pipeline from S3 to Redshift, how can you ensure encryption in transit and restrict the data path to your VPC?","answer":"[{\"id\":\"a\",\"text\":\"Force TLS for all connections and rely on public endpoints restricted by IAM\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use VPC endpoints for S3 and Redshift, enforce TLS, and disable public access to those endpoints\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Use public endpoints with TLS only and no VPC isolation\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use Direct Connect to your on-prem network and route traffic over the internet\",\"isCorrect\":false}]","explanation":"## Correct Answer\n- b) Use VPC endpoints for S3 and Redshift, enforce TLS, and disable public access to those endpoints.\n\n## Why Other Options Are Wrong\n- a) Public endpoints can expose traffic to the internet; while TLS helps, private connectivity is safer.\n- c) Public endpoints with TLS do not restrict traffic to your VPC, increasing exposure.\n- d) Direct Connect drives on-prem connectivity, not in-AWS cross-service private data paths for this scenario.\n\n## Key Concepts\n- VPC endpoints (Gateway/Interface)\n- TLS for in-transit encryption\n- Private connectivity between AWS services\n\n## Real-World Application\nBy using VPC endpoints for S3 and Redshift and ensuring TLS, data moves only within the AWS network boundary, reducing exposure and meeting security governance requirements.","diagram":null,"difficulty":"intermediate","tags":["AWS","VPC","S3","Redshift","TLS","certification-mcq","domain-weight-18"],"channel":"aws-data-engineer","subChannel":"data-security","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T15:29:49.382Z","createdAt":"2026-01-12 15:29:49"},{"id":"aws-data-engineer-data-security-1768293084701-0","question":"You are designing access controls for a data lake on S3 with a Glue Data Catalog. You must allow data scientists to run analytics on customer data but ensure PII fields are not accessible to them. Which approach meets this requirement best?","answer":"[{\"id\":\"a\",\"text\":\"Rely on IAM policies attached to data scientists' roles to limit access to PII columns within a single file.\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Enable Lake Formation with database, table, and column-level permissions to grant SELECT on non-PII columns and revoke access to PII columns.\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Use a VPC endpoint policy to restrict access to the S3 bucket.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Store PII data in a separate bucket with a separate KMS key and grant access to the non-PII bucket.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nB: Enable Lake Formation with database, table, and column-level permissions to grant SELECT on non-PII columns and revoke access to PII columns.\n\n## Why Other Options Are Wrong\n- A: IAM policies cannot enforce column-level restrictions inside a single file; hence they cannot provide per-column access control.\n- C: A VPC endpoint policy restricts network access, not per-column data access.\n- D: Splitting data into separate buckets does not provide scalable per-column access controls within a shared file.\n\n## Key Concepts\n- Lake Formation permissions model (database, table, column)\n- Column-level security in data lakes\n- Glue Data Catalog integration for governance\n\n## Real-World Application\n- Implement a Lake Formation catalog, grant SELECT on non-PII columns to data-science roles, revoke PII access, and audit permissions via Lake Formation APIs.","diagram":null,"difficulty":"intermediate","tags":["AWS","LakeFormation","IAM","Glue","S3","DataSecurity","certification-mcq","domain-weight-18"],"channel":"aws-data-engineer","subChannel":"data-security","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T08:31:24.702Z","createdAt":"2026-01-13 08:31:25"},{"id":"aws-data-engineer-data-security-1768293084701-1","question":"You store sensitive data in S3 and want to enforce strong encryption with centralized control over keys. Which approach is the most appropriate?","answer":"[{\"id\":\"a\",\"text\":\"Enable SSE-S3 on the bucket and rely on AWS to rotate keys.\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use SSE-KMS with a customer-managed CMK and a strict key policy that enforces encryption by default for all new objects.\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Encrypt data client-side before upload and manage keys in your application.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use SSE-KMS with a customer-managed CMK but share the CMK with multiple teams to simplify access.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nB: Use SSE-KMS with a customer-managed CMK and a strict key policy that enforces encryption by default for all new objects in S3.\n\n## Why Other Options Are Wrong\n- A: SSE-S3 uses AWS-managed keys and lacks fine-grained control and explicit governance.\n- C: Client-side encryption shifts key management responsibility outside AWS and complicates key rotation and access control.\n- D: Sharing the CMK undermines centralized governance and violates least privilege.\n\n## Key Concepts\n- SSE-KMS vs SSE-S3\n- Customer-managed CMK lifecycle and policy\n- Default encryption enforcement for data governance\n\n## Real-World Application\n- Enforce a policy requiring SSE-KMS for all new objects, maintain a tight CMK policy, and monitor decrypt activity via CloudTrail.","diagram":null,"difficulty":"intermediate","tags":["AWS","KMS","S3","IAM","DataSecurity","certification-mcq","domain-weight-18"],"channel":"aws-data-engineer","subChannel":"data-security","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T08:31:25.186Z","createdAt":"2026-01-13 08:31:25"},{"id":"aws-data-engineer-data-security-1768293084701-2","question":"A data lake contains sensitive data in S3 buckets. You need to automatically detect, classify, and respond to exposures and alert the security team. Which approach is most appropriate?","answer":"[{\"id\":\"a\",\"text\":\"Rely on CloudWatch alarms for bucket activity and manually review findings.\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Enable AWS Macie to classify sensitive data and generate findings, and configure automated remediation with Lambda and alert via SNS.\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Use GuardDuty to monitor S3 bucket data for sensitive exposures.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Maintain a manual data inventory and rely on quarterly audits.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nB: Enable AWS Macie to classify sensitive data and generate findings, and configure automated remediation with Lambda and alert via SNS.\n\n## Why Other Options Are Wrong\n- A: CloudWatch alone does not provide automated classification or actionable findings for sensitive data.\n- C: GuardDuty focuses on threat detection for workloads and networks, not data classification in S3.\n- D: Manual audits are slow and error-prone, not suitable for real-time exposure response.\n\n## Key Concepts\n- AWS Macie data classification and findings\n- Event-driven remediation with Lambda\n- SNS for alerting and incident response\n\n## Real-World Application\n- Run Macie on critical buckets, trigger Lambda-based remediation (e.g., apply bucket policy) and notify security teams when PII is detected.","diagram":null,"difficulty":"intermediate","tags":["AWS","Macie","S3","Lambda","SNS","certification-mcq","domain-weight-18"],"channel":"aws-data-engineer","subChannel":"data-security","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T08:31:25.652Z","createdAt":"2026-01-13 08:31:25"},{"id":"aws-data-engineer-data-security-1768293084701-3","question":"You must retain data for seven years and prevent tampering or deletion in an S3 bucket containing regulatory records. Which configuration best meets this requirement?","answer":"[{\"id\":\"a\",\"text\":\"Enable S3 Object Lock in Governance mode with a seven-year retention period.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Use an S3 Lifecycle rule to delete objects after seven years.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Store data in a relational database and rely on backups for retention.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Rely on KMS encryption alone to prevent deletion or tampering.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA: Enable S3 Object Lock in Governance mode with a seven-year retention period to prevent tampering and meet regulatory retention requirements.\n\n## Why Other Options Are Wrong\n- B: Lifecycle deletion would remove data after seven years, failing retention needs.\n- C: A database with backups does not provide immutable, per-object retention guarantees for S3 objects.\n- D: Encryption does not prevent deletion or tampering by policy; immutability is required.\n\n## Key Concepts\n- S3 Object Lock and Governance mode\n- Immutability and retention policies\n- Regulatory compliance requirements for data retention\n\n## Real-World Application\n- Apply Object Lock on a WORM bucket, set seven-year retention, and enforce retention across all compliant datasets.","diagram":null,"difficulty":"intermediate","tags":["AWS","S3ObjectLock","Governance","Compliance","DataSecurity","certification-mcq","domain-weight-18"],"channel":"aws-data-engineer","subChannel":"data-security","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T08:31:25.816Z","createdAt":"2026-01-13 08:31:25"},{"id":"aws-data-engineer-data-security-1768293084701-4","question":"A dataset in your S3 bucket must be shared with a partner in another AWS account while enforcing least privilege and governance. Which approach best achieves this?","answer":"[{\"id\":\"a\",\"text\":\"Make the bucket publicly readable and grant the partner a link.\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Create a cross-account IAM role in the partner's account and have your team assume it.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Create a cross-account IAM role in your account with a trust policy for the partner's account and grant read on the bucket; the partner assumes the role.\",\"isCorrect\":true},{\"id\":\"d\",\"text\":\"Copy the dataset to the partner's account bucket and revoke access to the original.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nC: Create a cross-account IAM role in your account with a trust policy for the partner's account and grant read on the bucket; the partner assumes the role.\n\n## Why Other Options Are Wrong\n- A: Public access undermines governance and increases risk.\n- B: A role in the partner's account cannot be trusted by your resource policies to grant access.\n- D: Copying data disrupts governance and complicates data consistency.\n\n## Key Concepts\n- Cross-account roles and trust policies\n- Least-privilege data sharing\n- S3 bucket policies and IAM integration\n\n## Real-World Application\n- Establish a read-only cross-account role in your account, share the ARN with the partner, and monitor access with CloudTrail and SCPs.","diagram":null,"difficulty":"intermediate","tags":["AWS","IAM","S3","RAM","CrossAccount","certification-mcq","domain-weight-18"],"channel":"aws-data-engineer","subChannel":"data-security","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T08:31:25.981Z","createdAt":"2026-01-13 08:31:26"},{"id":"aws-data-engineer-data-store-management-1768189639780-0","question":"A data lake on Amazon S3 stores hundreds of terabytes of unstructured data and logs. You need cost-effective long-term retention for analytics with Athena while keeping hot data quickly accessible for recent queries. Which approach best balances cost and query performance?","answer":"[{\"id\":\"a\",\"text\":\"Store all data in S3 Standard and avoid any lifecycle transitions\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Enable S3 Intelligent-Tiering for the entire dataset to automatically move items between tiers\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Configure S3 Lifecycle policies to transition hot data to S3 Standard/IA and cold data to Glacier Deep Archive, enabling cost-effective retention with Athena\",\"isCorrect\":true},{\"id\":\"d\",\"text\":\"Migrate data to EFS and run analytics directly from there\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption C. Implementing S3 Lifecycle policies to move hot data to more accessible tiers (Standard/IA) and cold data to Glacier Deep Archive provides the best balance of cost and analytics performance for Athena.\n\n## Why Other Options Are Wrong\n- A: Keeps all data in Standard storage, leading to higher long-term costs for archival data.\n- B: Intelligent-Tiering helps with access-pattern optimization but is not as cost-effective for long-term archival as tiered lifecycle with Glacier Deep Archive.\n- D: Migrating to EFS is inappropriate for petabyte-scale analytics and adds unnecessary complexity and cost.\n\n## Key Concepts\n- S3 storage classes and lifecycle policies\n- Cost optimization for data lakes with Athena\n- Glacier Deep Archive for long-term retention\n\n## Real-World Application\nUsed to retain decades of raw logs for compliance while preserving fast access to recent data for ad-hoc analytics.","diagram":null,"difficulty":"intermediate","tags":["S3","Glacier","Athena","Data-Lake","certification-mcq","domain-weight-26"],"channel":"aws-data-engineer","subChannel":"data-store-management","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T03:47:19.783Z","createdAt":"2026-01-12 03:47:20"},{"id":"aws-data-engineer-data-store-management-1768189639780-1","question":"An IoT telemetry pipeline generates high-volume time-series events with intermittent bursts. You want low-latency reads for dashboards and scalable storage for long-term analytics. Which storage configuration best achieves cost-efficiency, scalability, and near-real-time access?","answer":"[{\"id\":\"a\",\"text\":\"Store events in Amazon S3 with partitioned Parquet files and query via Athena\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Store events in DynamoDB with on-demand capacity, a composite key (deviceId as partition key, timestamp as sort key), TTL on aging data, and an optional GSI on timestamp for range queries\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Use Amazon RDS with a time-series schema and daily snapshotting\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Push data into Redshift and perform all queries there in real time\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B. DynamoDB with on-demand capacity provides virtually unlimited scalability and low-latency reads/writes, and modeling time-series data with a composite (partition, sort) key supports efficient range queries. TTL helps manage aging data.\n\n## Why Other Options Are Wrong\n- A: S3 + Athena is cost-effective for batch analytics but not optimal for near real-time dashboards due to higher-latency access and batch-oriented queries.\n- C: RDS is not ideal for high-velocity time-series data at scale and may incur higher costs and management overhead.\n- D: Redshift is optimized for analytical workloads, not real-time ingestion and low-latency point reads for dashboards.\n\n## Key Concepts\n- DynamoDB on-demand capacity and time-series data modeling\n- Composite primary keys for time-based queries\n- TTL for data lifecycle\n\n## Real-World Application\nIdeal for IoT dashboards requiring fast access to recent data while keeping historical data cost-effectively stored for analytics.","diagram":null,"difficulty":"intermediate","tags":["DynamoDB","IoT","Time-Series","TTL","certification-mcq","domain-weight-26"],"channel":"aws-data-engineer","subChannel":"data-store-management","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T03:47:20.199Z","createdAt":"2026-01-12 03:47:20"},{"id":"aws-data-engineer-data-store-management-1768189639780-2","question":"A regulated data store uses S3 to provide input to Redshift for analytics. You must ensure encryption at rest with strong key management and auditable key usage. Which approach provides best security and manageability?","answer":"[{\"id\":\"a\",\"text\":\"Use SSE-S3 with AWS managed keys for all objects\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use SSE-KMS with a customer-managed AWS KMS key and implement a bucket policy requiring encryption for all objects\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Encrypt data client-side before uploading and keep keys in an on-premises vault\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Rely on Redshift encryption only and skip S3-level encryption\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B. SSE-KMS with a customer-managed KMS key provides strong, auditable key management, and a bucket policy enforcing encryption ensures all data at rest is protected consistently.\n\n## Why Other Options Are Wrong\n- A: SSE-S3 uses AWS managed keys, offering weaker control and auditing capabilities than customer-managed keys.\n- C: Client-side encryption shifts encryption responsibility to clients and can complicate key management and access control.\n- D: Redshift encryption protects data at rest in Redshift, but S3 objects feeding Redshift require separate encryption controls; relying solely on Redshift misses S3-level protection and auditing.\n\n## Key Concepts\n- SSE-KMS with customer-managed keys\n- Bucket policies enforcing encryption\n- Auditing key usage with KMS IAM policies and CloudTrail\n\n## Real-World Application\nEnsures regulatory compliance with auditable key usage and centralized control over encryption keys for data ingested from S3 into analytic warehouses.","diagram":null,"difficulty":"intermediate","tags":["S3","KMS","SSE-KMS","Redshift","Compliance","certification-mcq","domain-weight-26"],"channel":"aws-data-engineer","subChannel":"data-store-management","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T03:47:20.637Z","createdAt":"2026-01-12 03:47:20"},{"id":"aws-data-engineer-data-store-management-1768259913588-0","question":"A data lake in S3 stores 100 TB of Parquet data partitioned by date but queries frequently scan non-matching partitions. Which approach yields the best reduction in scan costs while preserving current query patterns?","answer":"[{\"id\":\"a\",\"text\":\"Convert data format to ORC\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Introduce partition pruning by date and add partitions to enable predicate pushdown in Athena\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Switch to Redshift Spectrum\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Increase the number of small files\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nB. Partition pruning by date partitions and adding partitions to enable predicate pushdown in Athena. This reduces scanned data by pruning non-matching partitions without requiring changes to query patterns.\n\n## Why Other Options Are Wrong\n\n- A: Converting to ORC can reduce per-file overhead, but it adds operational overhead and may not achieve as much cost reduction as effective partition pruning for existing queries.\n- C: Redshift Spectrum is a separate analytics engine and does not address pruning in the existing data lake used by Athena.\n- D: Increasing the number of small files typically hurts performance due to metadata overhead and less efficient scans.\n\n## Key Concepts\n\n- Partition pruning\n- Predicate pushdown\n\n## Real-World Application\n\n- Example path: s3://data-lake/warehouse/parquet/date=YYYY-MM-DD/; ensure queries include a date predicate to enable partition pruning.\n","diagram":null,"difficulty":"intermediate","tags":["S3","Athena","Parquet","Partitioning","certification-mcq","domain-weight-26"],"channel":"aws-data-engineer","subChannel":"data-store-management","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T23:18:33.589Z","createdAt":"2026-01-12 23:18:33"},{"id":"aws-data-engineer-data-store-management-1768259913588-1","question":"Which mechanism provides immutable data protection for objects stored in S3 to meet regulatory requirements (WORM)?","answer":"[{\"id\":\"a\",\"text\":\"S3 Versioning\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"S3 Object Lock in Compliance mode\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"S3 Replication\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"AES-256 Server-Side Encryption\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nB. S3 Object Lock in Compliance mode provides immutable retention (WORM) for S3 objects, preventing deletion for the retention period and meeting regulatory requirements.\n\n## Why Other Options Are Wrong\n\n- A: Versioning preserves previous versions but does not guarantee immutability against deletion within governance constraints.\n- C: Replication duplicates data but does not enforce immutability on the destination.\n- D: Encryption protects data confidentiality, not immutability.\n\n## Key Concepts\n\n- S3 Object Lock\n- Compliance mode\n- WORM protections\n\n## Real-World Application\n\n- Used to meet regulatory retention for financial records or legal holds where deletions must be prevented for a defined period.\n","diagram":null,"difficulty":"intermediate","tags":["S3","ObjectLock","Compliance","WORM","certification-mcq","domain-weight-26"],"channel":"aws-data-engineer","subChannel":"data-store-management","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T23:18:33.998Z","createdAt":"2026-01-12 23:18:34"},{"id":"aws-data-engineer-data-store-management-1768259913588-2","question":"After migrating data from on-premises to S3 using AWS DataSync, you need to validate data integrity across the entire dataset. Which approach best ensures end-to-end integrity?","answer":"[{\"id\":\"a\",\"text\":\"Compare object counts only\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Compare ETags/MD5 checksums and optionally generate and compare S3 Inventory reports\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Rely solely on DataSync transfer logs\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Re-run the copy until it succeeds\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nB. Compare ETags/MD5 checksums and optionally generate and compare S3 Inventory reports. Checksums verify content integrity, while inventory helps scale verification for large datasets. Logs alone do not guarantee data integrity.\n\n## Why Other Options Are Wrong\n\n- A: Object counts do not confirm data integrity or content fidelity.\n- C: Transfer logs show success but not file correctness.\n- D: Re-copying may fix transient issues but does not guarantee that the final data matches source contents.\n\n## Key Concepts\n\n- Data integrity checksums\n- S3 Inventory\n- ETag/MD5 verification\n\n## Real-World Application\n\n- After a large-scale DataSync migration, run checksum validation against a manifest derived from source XOR destination to ensure no corruption occurred.\n","diagram":null,"difficulty":"intermediate","tags":["DataSync","S3","ETag","MD5","certification-mcq","domain-weight-26"],"channel":"aws-data-engineer","subChannel":"data-store-management","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T23:18:34.385Z","createdAt":"2026-01-12 23:18:34"},{"id":"aws-data-engineer-data-store-management-1768259913588-3","question":"To automate cost optimization and lifecycle management for cold data stored in S3, which configuration should you implement?","answer":"[{\"id\":\"a\",\"text\":\"S3 Lifecycle policy to transition to GLACIER and set Expiration\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Enable S3 Intelligent-Tiering for all objects\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use S3 Versioning with MFA Delete only\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use CloudFront invalidations for lifecycle management\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nA. S3 Lifecycle policy to transition data to GLACIER (or GLACIER Deep Archive) and set an expiration rule to delete after a retention period. This directly reduces storage costs for infrequently accessed data.\n\n## Why Other Options Are Wrong\n\n- B: Intelligent-Tiering helps optimize costs for unknown access patterns but is not as cost-effective for truly cold data that rarely changes or is accessed.\n- C: Versioning with MFA Delete protects against deletion but does not automatically move data to cheaper storage or purge old data.\n- D: CloudFront invalidations do not affect where data is stored; they only control cache, not lifecycle costs.\n\n## Key Concepts\n\n- S3 Lifecycle policies\n- Transition to Glacier/Deep Archive\n- Expiration rules\n\n## Real-World Application\n\n- Apply to datasets older than a defined threshold to automatically move to cheaper storage and delete when retention ends.\n","diagram":null,"difficulty":"intermediate","tags":["S3","Lifecycle","Glacier","CostOptimization","certification-mcq","domain-weight-26"],"channel":"aws-data-engineer","subChannel":"data-store-management","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T23:18:34.524Z","createdAt":"2026-01-12 23:18:34"},{"id":"aws-data-engineer-data-store-management-1768259913588-4","question":"Which AWS service combination provides a centralized metadata catalog and governance controls across multiple data stores (S3, Redshift, RDS) to support data discovery and access control?","answer":"[{\"id\":\"a\",\"text\":\"AWS Glue Data Catalog (with Lake Formation governance)\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"AWS DynamoDB\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"AWS KMS\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"AWS CloudWatch Logs\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nA. AWS Glue Data Catalog provides a centralized metadata repository, and when combined with Lake Formation, offers fine-grained access control and governance across data stores such as S3, Redshift, and RDS.\n\n## Why Other Options Are Wrong\n\n- B: DynamoDB is a NoSQL database, not a metadata catalog or governance layer.\n- C: KMS handles encryption keys, not metadata or governance.\n- D: CloudWatch Logs is for logging and monitoring, not metadata governance.\n\n## Key Concepts\n\n- Glue Data Catalog\n- Lake Formation governance\n- Data discovery and access control\n\n## Real-World Application\n\n- Enables data stewards to discover datasets across S3, Redshift, and RDS and enforce access policies consistently.\n","diagram":null,"difficulty":"intermediate","tags":["Glue","DataCatalog","LakeFormation","Governance","certification-mcq","domain-weight-26"],"channel":"aws-data-engineer","subChannel":"data-store-management","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T23:18:34.665Z","createdAt":"2026-01-12 23:18:34"},{"id":"q-886","question":"Scenario: you manage a financial data lake with multiple transactional sources feeding S3 via DMS. Stakeholders require upserts, full history for compliance, fast dashboards using a latest-state table, and seamless schema evolution. Compare Iceberg, Hudi, and Delta Lake for this scenario and outline a concrete pipeline (CDC, ETL, compaction, governance)?","answer":"Use Apache Iceberg on S3 to support CDC upserts and full history. Ingest via DMS into a staging Parquet layer, then Spark MERGE INTO to the Iceberg table; maintain a separate 'latest' snapshot for das","explanation":"## Why This Is Asked\nFinancial data lakes require reliable upserts, auditability, and governance. This question probes familiarity with lakehouse formats and practical pipeline design.\n\n## Key Concepts\n- Change Data Capture (CDC) via DMS into S3-backed lakehouse\n- Upserts and history with Iceberg vs Hudi/Delta\n- Schema evolution and partitioning strategy\n- Data governance and validation checks\n\n## Code Example\n```javascript\nCREATE TABLE analytics.fact_sales (\n  id BIGINT,\n  amount DECIMAL(12,2),\n  ts TIMESTAMP,\n  source STRING\n)\nUSING ICEBERG\nPARTITIONED BY (years(ts), months(ts), source);\n\nMERGE INTO analytics.fact_sales_latest AS target\nUSING analytics.fact_sales_staging AS src\nON target.id = src.id\nWHEN MATCHED THEN UPDATE SET amount = src.amount, ts = src.ts\nWHEN NOT MATCHED THEN INSERT (id, amount, ts, source) VALUES (src.id, src.amount, src.ts, src.source);\n```\n\n## Follow-up Questions\n- How would you handle late-arriving data and out-of-order events?\n- What monitoring would you implement for compaction and schema changes?","diagram":"flowchart TD\n  A[CDC Source] --> B[Staging Parquet]\n  B --> C[Spark MERGE to Iceberg]\n  C --> D[Latest Snapshot]\n  D --> E[Athena/BI Dashboards]","difficulty":"intermediate","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T14:25:05.485Z","createdAt":"2026-01-12T14:25:05.485Z"},{"id":"q-948","question":"In a multi-tenant data lake on S3 across two AWS accounts, each tenant's data must be isolated, with auditable access, cost accounting, and fast analytics for dashboards. Design a solution using AWS Lake Formation for governance, S3 prefixes per tenant, and Athena/Glue for analytics. Explain cross-account sharing, tag-based access, and data lineage, plus failure modes?","answer":"Use Lake Formation governance with per-tenant databases mapped to S3 prefixes (s3://lake/tenant-id/...). Expose cross-account access via resource links and tag-based ACLs, audited by Lake Formation an","explanation":"## Why This Is Asked\nTests mastery of cross-account governance, tenant isolation, auditable access, and cost reporting in a real-world data lake.\n\n## Key Concepts\n- Lake Formation governance and resource links\n- Per-tenant S3 prefixes and Glue Data Catalog\n- Tag-based access control and cost allocation\n- Cross-account sharing and CloudTrail/audit trails\n- Data lineage and auditability\n\n## Code Example\n```bash\n# Grant a tenant role read on its catalog table (illustrative)\naws lakeformation grant-permissions --principal '{\"EffectiveFrom\": \"2026-01-12T00:00:00Z\",\"DataLakePrincipalIdentifier\": \"arn:aws:iam::111122223333:role/tenantA\"}' --permissions SELECT --resource '{\"Table\": {\"DatabaseName\": \"tenantA_db\", \"Name\": \"events\"}}'\n```\n\n## Follow-up Questions\n- How would you handle offboarding a tenant's data while preserving analytics history?\n- How would you test isolation and audit completeness in CI/CD?\n","diagram":null,"difficulty":"advanced","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Discord","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T16:36:54.412Z","createdAt":"2026-01-12T16:36:54.412Z"}],"subChannels":["data-ingestion","data-operations","data-security","data-store-management","general"],"companies":["Apple","Discord","Goldman Sachs","Netflix","Tesla"],"stats":{"total":36,"beginner":0,"intermediate":35,"advanced":1,"newThisWeek":36}}