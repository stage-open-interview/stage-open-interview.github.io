{"questions":[{"id":"aws-data-engineer-data-ingestion-1768148517041-0","question":"You are designing a data ingestion pipeline for streaming IoT telemetry. The system must ingest up to 15,000 events per second, apply on-the-fly transformations to standardize the payloads, and store the results in S3 in Parquet format for Athena queries. The input schema evolves over time with new fields, and you want minimal maintenance when fields are added. Which architecture best meets these requirements?","answer":"[{\"id\":\"a\",\"text\":\"Store raw JSON in S3 and run a nightly EMR job to parse and convert to Parquet\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Ingest with Kinesis Data Streams, transform with AWS Glue Streaming ETL, and write Parquet to S3; use Glue Data Catalog for schema evolution\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Ingest with SQS, process with Lambda, and store as CSV in S3\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use a custom on-prem ETL tool to batch process data daily\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct because it uses a high-throughput streaming path (Kinesis Data Streams) with streaming ETL (Glue Streaming ETL) to perform on-the-fly transformations and write Parquet to S3 for Athena. It also leverages Glue Data Catalog for schema evolution, allowing new fields to be incorporated without breaking existing pipelines. The other options rely on batch processing (A), or consume events with SQS + Lambda in a manner that doesn't scale to 15k events/sec or provide robust schema evolution (C), or depend on an on-prem batch tool with no streaming capability (D).\n\n## Why Other Options Are Wrong\n- Option A: Batch processing cannot satisfy near real-time ingestion and lacks incremental transformation.\n- Option C: SQS + Lambda is not designed for such high event throughput and lacks built-in schema evolution support.\n- Option D: On-prem batch processing introduces latency and maintenance burden not aligned with cloud-native scalability.\n\n## Key Concepts\n- Streaming ingestion with Kinesis Data Streams\n- AWS Glue Streaming ETL or Glue pipelines\n- Parquet format and Athena compatibility\n- Glue Data Catalog for schema evolution\n\n## Real-World Application\n- This pattern is used for real-time analytics on IoT telemetry, enabling fast queries on clean Parquet data without rebuilding pipelines on schema changes.","diagram":null,"difficulty":"intermediate","tags":["AWS","Kinesis","Glue","S3","Parquet","Athena","DataIngestion","Streaming","certification-mcq","domain-weight-34"],"channel":"aws-data-engineer","subChannel":"data-ingestion","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T16:21:57.044Z","createdAt":"2026-01-11 16:21:57"},{"id":"aws-data-engineer-data-ingestion-1768148517041-1","question":"You have multiple teams ingesting JSON logs into S3 with evolving fields. You want to query with Athena without breaking existing pipelines, and you want to support schema evolution. Which approach is best?","answer":"[{\"id\":\"a\",\"text\":\"Enforce a fixed schema by rewriting all files to a single schema upon ingestion\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use AWS Glue Schema Registry to maintain schema evolution and use Glue Tables with dynamic frames\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Convert all data to CSV before storing in S3\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Ignore schema evolution and adjust queries to handle optional fields\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct because AWS Glue Schema Registry provides centralized schema governance that supports evolution across multiple producers. When combined with Glue Tables and dynamic frames, downstream queries (e.g., Athena) can adapt to changing fields without breaking pipelines.\n\n## Why Other Options Are Wrong\n- Option A: Forcing a fixed schema increases maintenance, causes churn with new fields, and breaks multi-team ingestion.\n- Option C: CSV reduces schema flexibility and complicates semi-structured JSON handling in Athena.\n- Option D: Ignoring evolution leads to fractured schemas and failed queries.\n\n## Key Concepts\n- AWS Glue Schema Registry\n- Glue Data Catalog / Tables\n- Schema evolution in data lakes\n\n## Real-World Application\n- Enables multiple teams to contribute JSON logs with evolving schemas while keeping Athena queries stable and future-proof.","diagram":null,"difficulty":"intermediate","tags":["AWS","Glue","SchemaRegistry","Athena","S3","DataIngestion","DataCatalog","certification-mcq","domain-weight-34"],"channel":"aws-data-engineer","subChannel":"data-ingestion","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T16:21:57.390Z","createdAt":"2026-01-11 16:21:57"},{"id":"aws-data-engineer-data-ingestion-1768148517041-2","question":"You operate an on-prem Oracle database and a Redshift data warehouse. You need near real-time changes to reflect in Redshift and preserve historical rows (SCD Type 2). Which approach best accomplishes this with minimal custom coding?","answer":"[{\"id\":\"a\",\"text\":\"Configure an AWS DMS CDC task to Redshift and implement MERGE statements in Redshift to perform SCD Type 2\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Configure DMS CDC to S3 and run a Glue job to merge into Redshift\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Build a custom log-based replication service on-prem and push changes to Redshift\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Schedule hourly full table dumps from Oracle to Redshift\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A is correct because AWS DMS can capture CDC from an on-prem Oracle source and replicate to Redshift with low latency. Implementing MERGE statements in Redshift enables SCD Type 2 to preserve history with minimal custom code beyond the MERGE logic.\n\n## Why Other Options Are Wrong\n- Option B: Adds an extra staging step (S3) and delayed merging, increasing latency and complexity.\n- Option C: Requires building a custom solution with significant maintenance.\n- Option D: Full dumps cause high latency and data staleness, not near real-time.\n\n## Key Concepts\n- AWS DMS CDC for Oracle to Redshift\n- Redshift MERGE for SCD Type 2\n- Change data capture patterns\n\n## Real-World Application\n- Keeps dimensional history accurate in a data warehouse while minimizing custom ETL development.","diagram":null,"difficulty":"intermediate","tags":["AWS","DMS","Redshift","CDC","SCD","DataIngestion","Oracle","certification-mcq","domain-weight-34"],"channel":"aws-data-engineer","subChannel":"data-ingestion","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T16:21:57.734Z","createdAt":"2026-01-11 16:21:57"},{"id":"aws-data-engineer-data-operations-1768216823632-0","question":"You have an analytics data lake on S3 using daily partitions (year, month, day) registered in the AWS Glue Data Catalog and queried via Athena. The partition count has grown to tens of thousands, causing long crawler runs and slow query planning. Which design approach reduces maintenance while preserving fast partition pruning?","answer":"[{\"id\":\"a\",\"text\":\"Implement partition projection on the Glue table for year, month, and day so Athena can prune partitions without listing all partitions.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Reorganize data into a single table with no partitions and rely on file-level filters.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Switch to monthly partitions and keep the same table metadata.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Keep the current partitioning and increase crawler frequency to refresh partitions more often.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct choice is A because partition projection lets Athena prune partitions without enumerating all partitions, reducing maintenance and query planning cost. B would still require manual partition handling; C eliminates partitioning and sacrifices pruning; D increases costs and does not address pruning efficiency.\n\n## Why Other Options Are Wrong\n- B: Moving to monthly partitions reduces count but still requires partition discovery and misses fine-grained pruning.\n- C: A non-partitioned table degrades performance for time-bounded filters.\n- D: Frequent crawls add cost and do not address pruning efficiency.\n\n## Key Concepts\n- AWS Glue Data Catalog\n- Athena partition pruning and partition projection\n\n## Real-World Application\nUsed to scale data lake catalogs with high partition counts while preserving fast ad-hoc query performance.","diagram":null,"difficulty":"intermediate","tags":["AWS","S3","Athena","Glue","PartitionProjection","certification-mcq","domain-weight-22"],"channel":"aws-data-engineer","subChannel":"data-operations","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T11:20:23.634Z","createdAt":"2026-01-12 11:20:23"},{"id":"aws-data-engineer-data-operations-1768216823632-1","question":"A Glue ETL job ingests data from a MySQL RDS source and writes Parquet to S3 before loading into Redshift. A new column is added in the source and occasionally causes a failure due to a schema mismatch. Which approach best ensures the job remains resilient to new columns without failing?","answer":"[{\"id\":\"a\",\"text\":\"Use AWS Glue DynamicFrame with ResolveChoice and ApplyMapping to map to the target schema and ignore extra columns via DropFields.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Hard-code the target schema to include the new column to catch up automatically.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Disable schema checks entirely and always write as string type.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Switch to a different ETL tool that ignores schema drift.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A is correct because Glue DynamicFrame and ResolveChoice allow the pipeline to adapt to drift and map to the stable target schema while ignoring unexpected new columns, preventing failures. B locks the target schema to a new field, causing more frequent changes and potential failures; C sacrifices data typing and can lead to data quality issues; D avoids Glue's native, scalable handling of schema drift.\n\n## Why Other Options Are Wrong\n- B: Forcing the new column into the target schema requires schema changes and redeploys, causing failures.\n- C: Dropping type safety leads to downstream ambiguity and issues.\n- D: Introducing another tool adds complexity and may not integrate with existing catalog and load steps.\n\n## Key Concepts\n- AWS Glue DynamicFrame\n- ResolveChoice, ApplyMapping, DropFields\n- Schema drift handling in ETL pipelines\n\n## Real-World Application\nKeeps data pipelines resilient to evolving source schemas with minimal downtime and manual intervention.","diagram":null,"difficulty":"intermediate","tags":["AWS","Glue","Parquet","Redshift","SchemaDrift","certification-mcq","domain-weight-22"],"channel":"aws-data-engineer","subChannel":"data-operations","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T11:20:23.993Z","createdAt":"2026-01-12 11:20:24"},{"id":"aws-data-engineer-data-operations-1768216823632-2","question":"In a multi-environment AWS data lake deployed with Terraform, you need dev/stage/prod with isolated infrastructure states. Which practice best prevents cross-environment drift and accidental changes?","answer":"[{\"id\":\"a\",\"text\":\"Maintain separate remote state per environment, using distinct backends and state files.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Use a single backend and rely on a single state for all environments.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Only rely on Git branches; state remains in local files.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Manually reproduce resources in each environment without state management.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A is correct because maintaining separate remote states per environment (or distinct backends) prevents drift and unintentional cross-environment changes, allowing safe, repeatable deployments. B, C, and D either merge states or bypass state management, increasing risk of drift and misconfiguration.\n\n## Why Other Options Are Wrong\n- B: A single shared state risks cross-environment contamination.\n- C: Local state is not shareable or reproducible across teammates or CI.\n- D: Manual reproduction increases inconsistency and drift.\n\n## Key Concepts\n- Terraform state management\n- Remote backends (S3, Terraform Cloud/Enterprise)\n- Environment isolation\n\n## Real-World Application\nEnsures predictable provisioning across dev/stage/prod for data lake components like S3, Glue, and Redshift.","diagram":null,"difficulty":"intermediate","tags":["Terraform","AWS","S3","Glue","Redshift","Kubernetes","certification-mcq","domain-weight-22"],"channel":"aws-data-engineer","subChannel":"data-operations","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T11:20:24.344Z","createdAt":"2026-01-12 11:20:24"},{"id":"aws-data-engineer-data-security-1768231788418-0","question":"To enforce encryption at rest for a data lake in S3 using a customer-managed CMK and require that all objects are encrypted with SSE-KMS, what configuration should you implement?","answer":"[{\"id\":\"a\",\"text\":\"Enable default encryption with SSE-KMS using a CMK and attach a bucket policy that requires x-amz-server-side-encryption: aws:kms, and grant kms:Encrypt and kms:Decrypt to the relevant IAM roles\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Enable default encryption with SSE-S3 and rely on IAM policies to restrict usage of encryption keys\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use client-side encryption and store keys in Secrets Manager\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Enable bucket encryption using a CMK but do not enforce encryption on uploads\",\"isCorrect\":false}]","explanation":"## Correct Answer\n- a) Enable default encryption with SSE-KMS using a customer-managed CMK and enforce via bucket policy that all PutObject requests include x-amz-server-side-encryption: aws:kms, and grant kms:Encrypt and kms:Decrypt to the relevant IAM roles.\n\n## Why Other Options Are Wrong\n- b) SSE-S3 does not use a CMK, which fails the requirement for customer-managed key-based encryption at rest.\n- c) Client-side encryption happens before data reaches S3 and is not managed by S3/KMS governance.\n- d) Not enforcing encryption on uploads allows unencrypted objects to be stored, defeating the requirement.\n\n## Key Concepts\n- SSE-KMS, CMKs, and key policies\n- Bucket policies and encryption requirements\n- IAM roles with KMS permissions\n\n## Real-World Application\nIn a production data lake, configure a CMK in KMS, set bucket default encryption to aws:kms, enforce via a bucket policy that all uploads specify the KMS header and key, and grant the ingestion roles the necessary KMS permissions to encrypt data.","diagram":null,"difficulty":"intermediate","tags":["AWS","S3","KMS","IAM","Data Encryption","certification-mcq","domain-weight-18"],"channel":"aws-data-engineer","subChannel":"data-security","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T15:29:48.419Z","createdAt":"2026-01-12 15:29:48"},{"id":"aws-data-engineer-data-security-1768231788418-1","question":"Your data lake spans two AWS accounts and you want centralized governance with auditable access controls. Which approach best achieves this using AWS services?","answer":"[{\"id\":\"a\",\"text\":\"Manage all permissions via IAM roles and bucket policies in each account independently\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use Lake Formation permissions to grant cross-account access to the Data Catalog and S3 locations, enabling centralized governance and auditing\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Use AWS RAM to share the underlying S3 bucket across accounts\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Copy data to the partner account and manage access there\",\"isCorrect\":false}]","explanation":"## Correct Answer\n- b) Use Lake Formation permissions to grant cross-account access to the Data Catalog and S3 locations, enabling centralized governance and auditing.\n\n## Why Other Options Are Wrong\n- a) Per-account IAM/bucket policies lead to policy sprawl and weaker centralized governance.\n- c) RAM shares resources but does not provide Lake Formationâ€™s catalog-based governance and auditing for data lake permissions.\n- d) Copying data to another account bypasses centralized governance and increases data duplication risk.\n\n## Key Concepts\n- AWS Lake Formation permissions model\n- Data Catalog and table-level permissions\n- Data location permissions for S3\n\n## Real-World Application\nUsing Lake Formation to centrally manage permissions across accounts reduces policy fragmentation and provides a consistent audit trail for cross-account data access.","diagram":null,"difficulty":"intermediate","tags":["AWS","Lake Formation","IAM","S3","Data Catalog","certification-mcq","domain-weight-18"],"channel":"aws-data-engineer","subChannel":"data-security","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T15:29:48.764Z","createdAt":"2026-01-12 15:29:49"},{"id":"aws-data-engineer-data-security-1768231788418-2","question":"To audit permission changes to data assets across multiple accounts, which combination provides a reliable, centralized audit trail?","answer":"[{\"id\":\"a\",\"text\":\"Enable CloudTrail Data Events for object-level changes in S3 only\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Enable CloudTrail management events for IAM policy changes and AWS Config across accounts, with a centralized trail\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Rely on IAM Access Analyzer outputs after changes\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use GuardDuty findings to infer policy changes\",\"isCorrect\":false}]","explanation":"## Correct Answer\n- b) Enable CloudTrail management events for IAM policy changes and AWS Config across accounts, with a centralized trail to capture configuration changes.\n\n## Why Other Options Are Wrong\n- a) Data Events cover data plane activity, not policy or configuration changes.\n- c) Access Analyzer helps identify access paths but does not provide a centralized audit log of changes.\n- d) GuardDuty detects threats, not governance policy changes.\n\n## Key Concepts\n- CloudTrail management events\n- AWS Config recording across accounts\n- Centralized logging and aggregation\n\n## Real-World Application\nFor a multi-account data lake, enable cross-account CloudTrail and Config aggregators to maintain a single source of truth for who changed what permissions and when.","diagram":null,"difficulty":"intermediate","tags":["AWS","CloudTrail","AWS Config","IAM","Audit","certification-mcq","domain-weight-18"],"channel":"aws-data-engineer","subChannel":"data-security","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T15:29:49.130Z","createdAt":"2026-01-12 15:29:49"},{"id":"aws-data-engineer-data-security-1768231788418-3","question":"To meet regulatory data retention of 7 years and prevent deletion of critical data stored in S3, which combination provides immutable, durable retention?","answer":"[{\"id\":\"a\",\"text\":\"Enable S3 Object Lock in Compliance mode with a retention period and ensure bucket versioning is enabled\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Enable MFA Delete on the bucket and rely on versioning alone\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Archive data to Glacier with a 7-year retention policy without Object Lock\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Rely on encryption to protect data integrity and manually enforce retention\",\"isCorrect\":false}]","explanation":"## Correct Answer\n- a) Enable S3 Object Lock in Compliance mode with a retention period and ensure bucket versioning is enabled.\n\n## Why Other Options Are Wrong\n- b) MFA Delete is deprecated for most workflows and does not guarantee immutability like Object Lock.\n- c) Glacier retention alone does not guarantee immutability or prevent deletion of live objects.\n- d) Encryption protects confidentiality, not retention or immutability.\n\n## Key Concepts\n- S3 Object Lock, Compliance mode\n- Bucket Versioning\n- Data retention policies and governance\n\n## Real-World Application\nFor regulatory compliance requiring long-term immutability, enable Object Lock in Compliance mode with a defined retention period and keep versioning enabled so that deleted or overwritten objects remain recoverable for the retention window.","diagram":null,"difficulty":"intermediate","tags":["AWS","S3","Object Lock","Versioning","Compliance","certification-mcq","domain-weight-18"],"channel":"aws-data-engineer","subChannel":"data-security","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T15:29:49.255Z","createdAt":"2026-01-12 15:29:49"},{"id":"aws-data-engineer-data-security-1768231788418-4","question":"In a data ingestion pipeline from S3 to Redshift, how can you ensure encryption in transit and restrict the data path to your VPC?","answer":"[{\"id\":\"a\",\"text\":\"Force TLS for all connections and rely on public endpoints restricted by IAM\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use VPC endpoints for S3 and Redshift, enforce TLS, and disable public access to those endpoints\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Use public endpoints with TLS only and no VPC isolation\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use Direct Connect to your on-prem network and route traffic over the internet\",\"isCorrect\":false}]","explanation":"## Correct Answer\n- b) Use VPC endpoints for S3 and Redshift, enforce TLS, and disable public access to those endpoints.\n\n## Why Other Options Are Wrong\n- a) Public endpoints can expose traffic to the internet; while TLS helps, private connectivity is safer.\n- c) Public endpoints with TLS do not restrict traffic to your VPC, increasing exposure.\n- d) Direct Connect drives on-prem connectivity, not in-AWS cross-service private data paths for this scenario.\n\n## Key Concepts\n- VPC endpoints (Gateway/Interface)\n- TLS for in-transit encryption\n- Private connectivity between AWS services\n\n## Real-World Application\nBy using VPC endpoints for S3 and Redshift and ensuring TLS, data moves only within the AWS network boundary, reducing exposure and meeting security governance requirements.","diagram":null,"difficulty":"intermediate","tags":["AWS","VPC","S3","Redshift","TLS","certification-mcq","domain-weight-18"],"channel":"aws-data-engineer","subChannel":"data-security","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T15:29:49.382Z","createdAt":"2026-01-12 15:29:49"},{"id":"aws-data-engineer-data-store-management-1768189639780-0","question":"A data lake on Amazon S3 stores hundreds of terabytes of unstructured data and logs. You need cost-effective long-term retention for analytics with Athena while keeping hot data quickly accessible for recent queries. Which approach best balances cost and query performance?","answer":"[{\"id\":\"a\",\"text\":\"Store all data in S3 Standard and avoid any lifecycle transitions\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Enable S3 Intelligent-Tiering for the entire dataset to automatically move items between tiers\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Configure S3 Lifecycle policies to transition hot data to S3 Standard/IA and cold data to Glacier Deep Archive, enabling cost-effective retention with Athena\",\"isCorrect\":true},{\"id\":\"d\",\"text\":\"Migrate data to EFS and run analytics directly from there\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption C. Implementing S3 Lifecycle policies to move hot data to more accessible tiers (Standard/IA) and cold data to Glacier Deep Archive provides the best balance of cost and analytics performance for Athena.\n\n## Why Other Options Are Wrong\n- A: Keeps all data in Standard storage, leading to higher long-term costs for archival data.\n- B: Intelligent-Tiering helps with access-pattern optimization but is not as cost-effective for long-term archival as tiered lifecycle with Glacier Deep Archive.\n- D: Migrating to EFS is inappropriate for petabyte-scale analytics and adds unnecessary complexity and cost.\n\n## Key Concepts\n- S3 storage classes and lifecycle policies\n- Cost optimization for data lakes with Athena\n- Glacier Deep Archive for long-term retention\n\n## Real-World Application\nUsed to retain decades of raw logs for compliance while preserving fast access to recent data for ad-hoc analytics.","diagram":null,"difficulty":"intermediate","tags":["S3","Glacier","Athena","Data-Lake","certification-mcq","domain-weight-26"],"channel":"aws-data-engineer","subChannel":"data-store-management","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T03:47:19.783Z","createdAt":"2026-01-12 03:47:20"},{"id":"aws-data-engineer-data-store-management-1768189639780-1","question":"An IoT telemetry pipeline generates high-volume time-series events with intermittent bursts. You want low-latency reads for dashboards and scalable storage for long-term analytics. Which storage configuration best achieves cost-efficiency, scalability, and near-real-time access?","answer":"[{\"id\":\"a\",\"text\":\"Store events in Amazon S3 with partitioned Parquet files and query via Athena\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Store events in DynamoDB with on-demand capacity, a composite key (deviceId as partition key, timestamp as sort key), TTL on aging data, and an optional GSI on timestamp for range queries\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Use Amazon RDS with a time-series schema and daily snapshotting\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Push data into Redshift and perform all queries there in real time\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B. DynamoDB with on-demand capacity provides virtually unlimited scalability and low-latency reads/writes, and modeling time-series data with a composite (partition, sort) key supports efficient range queries. TTL helps manage aging data.\n\n## Why Other Options Are Wrong\n- A: S3 + Athena is cost-effective for batch analytics but not optimal for near real-time dashboards due to higher-latency access and batch-oriented queries.\n- C: RDS is not ideal for high-velocity time-series data at scale and may incur higher costs and management overhead.\n- D: Redshift is optimized for analytical workloads, not real-time ingestion and low-latency point reads for dashboards.\n\n## Key Concepts\n- DynamoDB on-demand capacity and time-series data modeling\n- Composite primary keys for time-based queries\n- TTL for data lifecycle\n\n## Real-World Application\nIdeal for IoT dashboards requiring fast access to recent data while keeping historical data cost-effectively stored for analytics.","diagram":null,"difficulty":"intermediate","tags":["DynamoDB","IoT","Time-Series","TTL","certification-mcq","domain-weight-26"],"channel":"aws-data-engineer","subChannel":"data-store-management","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T03:47:20.199Z","createdAt":"2026-01-12 03:47:20"},{"id":"aws-data-engineer-data-store-management-1768189639780-2","question":"A regulated data store uses S3 to provide input to Redshift for analytics. You must ensure encryption at rest with strong key management and auditable key usage. Which approach provides best security and manageability?","answer":"[{\"id\":\"a\",\"text\":\"Use SSE-S3 with AWS managed keys for all objects\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use SSE-KMS with a customer-managed AWS KMS key and implement a bucket policy requiring encryption for all objects\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Encrypt data client-side before uploading and keep keys in an on-premises vault\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Rely on Redshift encryption only and skip S3-level encryption\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B. SSE-KMS with a customer-managed KMS key provides strong, auditable key management, and a bucket policy enforcing encryption ensures all data at rest is protected consistently.\n\n## Why Other Options Are Wrong\n- A: SSE-S3 uses AWS managed keys, offering weaker control and auditing capabilities than customer-managed keys.\n- C: Client-side encryption shifts encryption responsibility to clients and can complicate key management and access control.\n- D: Redshift encryption protects data at rest in Redshift, but S3 objects feeding Redshift require separate encryption controls; relying solely on Redshift misses S3-level protection and auditing.\n\n## Key Concepts\n- SSE-KMS with customer-managed keys\n- Bucket policies enforcing encryption\n- Auditing key usage with KMS IAM policies and CloudTrail\n\n## Real-World Application\nEnsures regulatory compliance with auditable key usage and centralized control over encryption keys for data ingested from S3 into analytic warehouses.","diagram":null,"difficulty":"intermediate","tags":["S3","KMS","SSE-KMS","Redshift","Compliance","certification-mcq","domain-weight-26"],"channel":"aws-data-engineer","subChannel":"data-store-management","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T03:47:20.637Z","createdAt":"2026-01-12 03:47:20"},{"id":"q-886","question":"Scenario: you manage a financial data lake with multiple transactional sources feeding S3 via DMS. Stakeholders require upserts, full history for compliance, fast dashboards using a latest-state table, and seamless schema evolution. Compare Iceberg, Hudi, and Delta Lake for this scenario and outline a concrete pipeline (CDC, ETL, compaction, governance)?","answer":"Use Apache Iceberg on S3 to support CDC upserts and full history. Ingest via DMS into a staging Parquet layer, then Spark MERGE INTO to the Iceberg table; maintain a separate 'latest' snapshot for das","explanation":"## Why This Is Asked\nFinancial data lakes require reliable upserts, auditability, and governance. This question probes familiarity with lakehouse formats and practical pipeline design.\n\n## Key Concepts\n- Change Data Capture (CDC) via DMS into S3-backed lakehouse\n- Upserts and history with Iceberg vs Hudi/Delta\n- Schema evolution and partitioning strategy\n- Data governance and validation checks\n\n## Code Example\n```javascript\nCREATE TABLE analytics.fact_sales (\n  id BIGINT,\n  amount DECIMAL(12,2),\n  ts TIMESTAMP,\n  source STRING\n)\nUSING ICEBERG\nPARTITIONED BY (years(ts), months(ts), source);\n\nMERGE INTO analytics.fact_sales_latest AS target\nUSING analytics.fact_sales_staging AS src\nON target.id = src.id\nWHEN MATCHED THEN UPDATE SET amount = src.amount, ts = src.ts\nWHEN NOT MATCHED THEN INSERT (id, amount, ts, source) VALUES (src.id, src.amount, src.ts, src.source);\n```\n\n## Follow-up Questions\n- How would you handle late-arriving data and out-of-order events?\n- What monitoring would you implement for compaction and schema changes?","diagram":"flowchart TD\n  A[CDC Source] --> B[Staging Parquet]\n  B --> C[Spark MERGE to Iceberg]\n  C --> D[Latest Snapshot]\n  D --> E[Athena/BI Dashboards]","difficulty":"intermediate","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T14:25:05.485Z","createdAt":"2026-01-12T14:25:05.485Z"}],"subChannels":["data-ingestion","data-operations","data-security","data-store-management","general"],"companies":["Goldman Sachs","Tesla"],"stats":{"total":15,"beginner":0,"intermediate":15,"advanced":0,"newThisWeek":15}}