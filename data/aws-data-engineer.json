{"questions":[{"id":"q-1336","question":"In a multi-account AWS data lake used by a global product analytics team, ingest semi-structured data from several SaaS APIs into S3, enforce schema evolution and data quality checks, and expose curated tables with time-travel queries. Describe the end-to-end design including data formats, upsert strategy, and governance controls you would apply using AWS services?","answer":"Leverage AWS Glue Data Catalog with Lake Formation, store data in S3 using Apache Iceberg for upserts and time travel, orchestrate with Step Functions, transform in Glue Spark jobs, validate quality v","explanation":"## Why This Is Asked\nTests ability to design multi-account governance, schema evolution, upserts, and cross-service orchestration for a scalable data lake.\n\n## Key Concepts\n- Multi-account governance with Lake Formation\n- Apache Iceberg on S3 for upserts and time travel\n- Glue Data Catalog as central metadata store\n- Data quality tooling (Glue Data Quality or DataBrew)\n- Orchestration with Step Functions\n- Access controls and encryption\n\n## Code Example\n```javascript\n// Spark SQL pseudo: upsert into Iceberg table using MERGE\nspark.sql(\"MERGE INTO lake.product USING staging.product AS s ON t.id = s.id WHEN MATCHED THEN UPDATE SET ... WHEN NOT MATCHED THEN INSERT...\")\n```\n\n## Follow-up Questions\n- How would you handle schema drift across SaaS sources with Iceberg?\n- How would you enforce time-travel/query lineage access controls for data consumers?","diagram":null,"difficulty":"advanced","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Apple"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T11:43:06.203Z","createdAt":"2026-01-13T11:43:06.203Z"},{"id":"q-1419","question":"Design an advanced, multi-region data ingestion and governance pipeline for a global fintech platform. Data from partners arrives as JSON and Parquet; must enforce per-tenant isolation, support near real-time analytics, and GDPR erasure. Outline architecture using AWS Kinesis (streams), DMS for CDC, Glue (ETL), Iceberg on S3 for schema-evolving tables, Lake Formation/IAM for access, and Athena/Redshift Spectrum for queries. Include format choices, CDC vs batch mix, lineage, and failure modes?","answer":"Use Iceberg-on-S3 for per-tenant, time-travel capable tables; stream data through Kinesis and batch via Glue; CDC via DMS from partners; catalog in Glue Data Catalog; enforce isolation with Lake Forma","explanation":"## Why This Is Asked\nTests ability to design end-to-end data governance at scale across regions and tenants.\n\n## Key Concepts\n- Multi-region data lake with Iceberg on S3\n- Per-tenant isolation with Lake Formation\n- Streaming + batch ingestion\n- Data lineage and GDPR erasure\n- Schema evolution and time travel\n\n## Code Example\n```javascript\n// PySpark-like example writing to Iceberg\nconst df = spark.read.format(\"json\").load(\"s3://bucket/partner/*.json\");\ndf.write.format(\"iceberg\").mode(\"append\").save(\"db.tenants.events\");\n```\n\n## Follow-up Questions\n- How would you test data erasure requests end-to-end?\n- What metrics alerting would you implement for cross-region replication lag?","diagram":"flowchart TD\n  A[Partner Sources] --> B[Ingest: Kinesis/Batch]\n  B --> C[Glue ETL]\n  C --> D[Iceberg Tables on S3]\n  D --> E[Queries: Athena/Redshift]\n  D --> F[Access: Lake Formation]\n  F --> G[Lineage & Audits]\n  D --> H[Region Replication (S3)]","difficulty":"advanced","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T16:42:34.969Z","createdAt":"2026-01-13T16:42:34.969Z"},{"id":"q-1487","question":"You operate an AWS data lake where streaming user activity flows from Kinesis Data Streams into S3 via Firehose, with Glue Data Catalog, Athena queries, and dashboards. Schemas evolve and data quality is mission-critical. Design a real-time data quality and drift detection framework: schema drift, per-record quality checks, quarantine failing files, alert owners, and preserve cross-account auditability?","answer":"Leverage Glue Schema Registry for evolution, and real-time validators in Kinesis Data Analytics or Lambda to enforce rules (non-null IDs, valid timestamps, sane ranges) and tag bad records. Quarantine","explanation":"## Why This Is Asked\nAssesses ability to design real-time data quality and drift detection at scale, balancing schema evolution, validation, quarantine, alerting, and cross-account governance using AWS services.\n\n## Key Concepts\n- Schema evolution and registry management\n- Real-time validation and data quarantining\n- Observability: CloudWatch metrics and SNS alerts\n- Cross-account auditability via Glue Data Catalog and Lake Formation\n\n## Code Example\n```javascript\n// Pseudo validator for a streaming record\nfunction isRecordValid(r){\n  if(!r.user_id || !r.timestamp) return false;\n  if(typeof r.value !== 'number' || r.value < 0) return false;\n  return true;\n}\n```\n\n## Follow-up Questions\n- How would you test drift detection across regions and accounts?\n- What are trade-offs of using a separate quarantine bucket vs inline rejection for analytics workloads?\n","diagram":"flowchart TD\n  A[Kinesis] --> B[Firehose]\n  B --> C[S3 + Glue Catalog]\n  C --> D[Athena]\n  D --> E[Alerts/Owners]","difficulty":"advanced","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Coinbase","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T18:59:52.005Z","createdAt":"2026-01-13T18:59:52.005Z"},{"id":"q-1506","question":"Scenario: A partner API provides daily batches of image assets with evolving metadata. Build an end-to-end ingest to a multi-region data lake: landing in S3, metadata in a versioned, upsertable table, idempotent processing, schema evolution, and governance across accounts. Describe data formats, partitioning, dedupe strategy, and cross-account orchestration?","answer":"Ingest images via multipart S3 uploads behind a deduplicated manifest; store metadata in a versioned, upsertable Iceberg table on S3 (via Glue/Spark) keyed by asset_id + batch_id; handle schema evolut","explanation":"## Why This Is Asked\nThis question tests practical design for a cross-region data lake with large binary assets, evolving metadata, and strict governance. It probes hands-on trade-offs and tooling choices that show depth beyond basic ingestion.\n\n## Key Concepts\n- Ingesting large assets with multipart uploads and manifests\n- Idempotent upserts using a composite key (asset_id + batch_id)\n- Schema evolution supported by Iceberg/Delta on S3\n- Cross-account governance via Lake Formation and Glue Data Catalog\n- Multi-region orchestration with Step Functions\n\n## Code Example\n```javascript\n// Example: upsert metadata into a versioned Iceberg-like table (pseudo)\nasync function upsertMetadata(client, rec) {\n  const current = await client.read(rec.asset_id);\n  const merged = { ...current, ...rec };\n  await client.write(rec.asset_id, merged);\n}\n```\n\n## Follow-up Questions\n- How would you handle API rate limits and retries?\n- How would you validate data quality, schema drift, and auditability in production?","diagram":null,"difficulty":"intermediate","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","Slack","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T19:43:56.091Z","createdAt":"2026-01-13T19:43:56.091Z"},{"id":"q-1521","question":"You manage a global, multi-tenant data platform where streaming events bucket into S3 and downstream engines like Redshift and Athena rely on per-tenant schemas. Design a resilient end-to-end architecture that versions datasets, enforces per-tenant schema contracts, detects drift, quarantines bad records, and supports rollback without downtime. Specify services, data formats, governance, and observability?","answer":"Propose using Firehose/Kinesis for streaming into S3 with per-tenant prefixes; Glue Schema Registry to enforce per-tenant contracts; Iceberg on S3 for versioned tables; Lake Formation for tenant isola","explanation":"## Why This Is Asked\nTests multi-tenant data governance, schema contracts, drift detection, and rollback—critical for scalable data products.\n\n## Key Concepts\n- Multi-tenant governance with Glue Schema Registry and Lake Formation.\n- Versioned storage with Iceberg on S3.\n- Drift detection and quality checks via Glue jobs and Deequ.\n- Quarantine workflows and rollback with Step Functions.\n- Observability via CloudWatch and Athena-based lineage.\n\n## Code Example\n```javascript\n// Placeholder for a data quality check wiring example\n```\n\n## Follow-up Questions\n- How would you handle schema evolution while preserving historic queries?\n- What metrics and alerts signal a drift or ingestion failure?","diagram":"flowchart TD\n  Ingest[Kinesis/Firehose] --> Stage[S3 (tenant prefixes)]\n  Stage --> Catalog[Glue Catalog & Iceberg Tables]\n  Catalog --> Validate[Deequ drift checks]\n  Validate --> Quarantine[Quarantine bucket]\n  Quarantine --> Orchestrator[Step Functions]\n  Orchestrator --> Observability[CloudWatch]","difficulty":"advanced","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Google","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T20:39:24.546Z","createdAt":"2026-01-13T20:39:24.546Z"},{"id":"q-1543","question":"Design an event-driven data platform to ingest order and driver events from regional services into a centralized data lake for near-real-time analytics. Data arrives as evolving JSON schemas; must handle schema evolution, efficient partitioning, cross-account access, and cost. Which AWS services and data design patterns would you implement, and how would you validate data quality and enable fast ad-hoc queries?","answer":"Use Kinesis Data Streams for event ingestion, a Glue streaming job with Glue Schema Registry to normalize evolving JSON schemas, and write Parquet to S3 partitioned by date/region/service. Employ Lake Formation for fine-grained cross-account access, implement data quality validation with Glue job assertions and Deequ checks, and enable fast ad-hoc queries through Athena with optimized partitioning and columnar formats. Apply cost controls through file compaction, lifecycle policies, and intelligent tiering.","explanation":"## Why This Is Asked\n\nExamines real-time data ingestion, schema evolution, cross-account governance, and cost-aware storage patterns in a practical, production context.\n\n## Key Concepts\n\n- Streaming ingestion: Kinesis Data Streams + Glue Schema Registry\n- Real-time normalization: Glue Streaming or Kinesis Data Analytics\n- Storage design: Parquet on S3 with partitioning by date/region/service\n- Governance: Lake Formation fine-grained access, Glue Data Catalog, drift monitoring\n- Quality & cost: deduplication, compaction, managed small files\n\n## Code Example\n\n```python\n# Glue Spark job snippet for schema evolution and data quality\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.types import StructType\n\n# Initialize Glue context\nargs = getResolvedOptions(__SYS__, ['JOB_NAME'])\nsc = SparkContext()\nglue_context = GlueContext(sc)\nspark = glue_context.spark_session\n\n# Read from Kinesis with schema registry\ndynamic_frame = glue_context.create_dynamic_frame.from_options(\n    connection_type=\"kinesis\",\n    connection_options={\n        \"streamARN\": \"arn:aws:kinesis:region:account:stream/order-events\",\n        \"classification\": \"json\",\n        \"endpointUrl\": \"https://kinesis.region.amazonaws.com\"\n    },\n    transformation_ctx=\"kinesis_source\"\n)\n\n# Apply schema evolution and data quality checks\ndf = dynamic_frame.toDF()\n\n# Data quality validation\ndf = df.filter(\n    F.col(\"order_id\").isNotNull() &\n    F.col(\"timestamp\").isNotNull() &\n    F.col(\"region\").isNotNull()\n)\n\n# Write optimized Parquet to S3\ndf.write.mode(\"append\").partitionBy(\n    \"date\", \"region\", \"service_type\"\n).parquet(\"s3://data-lake/orders/\")\n```","diagram":"flowchart TD\n  A[Ingest: Kinesis Data Streams] --> B[Normalize: Glue Schema Registry & Glue Streaming]\n  B --> C[Store: S3 Parquet with partitions: date/region/service]\n  C --> D[Catalog: Glue Data Catalog + Lake Formation policies]\n  D --> E[Query: Athena / Redshift Spectrum]\n  E --> F[Quality & Monitoring: Glue jobs + CloudWatch]","difficulty":"intermediate","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T06:31:51.252Z","createdAt":"2026-01-13T21:31:34.885Z"},{"id":"q-1608","question":"Scenario: A new source streams JSON events from a mobile app via Kinesis Data Streams. Design a beginner-level ingestion pipeline to land data in S3 as Parquet with daily partitions, using AWS Glue for ETL and cataloging. Include services, data formats, schema evolution approach, basic data quality checks, and how you would test end-to-end before analytics?","answer":"Design a streaming ingestion pipeline using Kinesis Data Streams to capture JSON events from the mobile app, with AWS Glue ETL jobs that process the data, convert it to Parquet format, and write to S3 with daily date partitions. The Glue Data Catalog will register the resulting tables for analytics. Implement simple schema evolution by allowing new optional fields while maintaining backward compatibility, and include basic data quality checks for required fields, data types, and null values.","explanation":"## Why This Is Asked\nEvaluates practical, beginner-appropriate streaming ingestion using common AWS data tooling and basic quality controls.\n\n## Key Concepts\n- Kinesis Data Streams for real-time data ingestion\n- AWS Glue ETL and Data Catalog for data processing and metadata management\n- Parquet format with date-based partitioning for optimized storage and queries\n- Simple schema evolution strategies for handling changing data structures\n- Data quality checks (required fields, types, nulls)\n- CloudWatch alarms and end-to-end testing\n\n## Code Example\n```javascript\n// Pseudo-code to start a Glue job\nconst glue = new AWS.Glue();\nawait glue.startJobRun({\n  JobName: 'mobile-events-etl',\n  Arguments: {\n    '--input-stream': 'mobile-app-events',\n    '--output-path': 's3://analytics-bucket/events/',\n    '--partition-format': 'yyyy/MM/dd'\n  }\n});\n```","diagram":null,"difficulty":"beginner","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["NVIDIA","Netflix","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T05:30:45.364Z","createdAt":"2026-01-14T02:34:42.960Z"},{"id":"q-1631","question":"You receive a daily nested JSON file and a separate CSV in an S3 bucket. Design a beginner-level ingestion using AWS Glue Studio to flatten the JSON, normalize types, parse the CSV, and write Parquet under a date-partitioned path s3://data-lake/events/date=YYYY-MM-DD/. Register a Glue catalog table, outline simple schema-drift handling, and include basic data-quality checks plus a quick end-to-end test plan with sample files?","answer":"Describe a beginner-level ingestion using AWS Glue Studio for two daily files in S3: a nested JSON and a CSV. Build a Glue job to flatten JSON, normalize types, parse CSV, and write Parquet to s3://da","explanation":"## Why This Is Asked\nBeginners must design practical ETL workflows that handle mixed formats and small drift, using Glue Studio in repeatable ways.\n\n## Key Concepts\n- Glue Studio ETL\n- JSON flattening and CSV parsing\n- Parquet partitioning by date\n- Glue Data Catalog table\n- Basic data quality checks and schema drift\n\n## Code Example\n```javascript\nfunction flatten(obj, prefix = '', res = {}) {\n  for (const k in obj) {\n    const v = obj[k];\n    const key = prefix ? prefix + '.' + k : k;\n    if (v && typeof v === 'object' && !Array.isArray(v)) flatten(v, key, res);\n    else res[key] = v;\n  }\n  return res;\n}\n```\n\n## Follow-up Questions\n- How would you test handling of occasionally missing fields?\n- What would you do to scale the job for larger batches?","diagram":"flowchart TD\n  A[Ingest Files to S3] --> B[Glue Studio Job: JSON flatten] \n  B --> C[Glue Studio Job: CSV parse] \n  C --> D[Write Parquet to date partition] \n  D --> E[Glue Catalog Table with metadata] \n  E --> F[Quality checks & Schema drift handling] \n  F --> G[Test plan & validation]","difficulty":"beginner","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","Oracle","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T04:18:19.843Z","createdAt":"2026-01-14T04:18:19.843Z"},{"id":"q-1654","question":"New data source writes daily CSVs to S3. Design a beginner pipeline to validate a defined schema, fail-fast on missing required fields, and route bad records to a quarantine bucket with a reason. Store valid data as Parquet in S3 with daily partitions, and catalog via Glue so Athena can query. Include a basic end-to-end test plan?","answer":"Set up a Glue Spark job triggered by S3 PUT events to read CSVs with a defined schema, filter out rows missing required fields, and write invalid rows to a quarantine bucket with a validation_reason f","explanation":"## Why This Is Asked\nPractical beginner task validating end-to-end ingestion, data quality, and cataloging.\n\n## Key Concepts\n- S3 event-driven ETL\n- Glue Spark/DynamicFrame validation\n- Quarantine routing for bad records\n- Parquet partitioning by date\n- Glue Data Catalog and Athena access\n\n## Code Example\n```javascript\n// Pseudo Glue-like JS snippet for validation\nfunction process(batch){\n  const valid=[], quarantined=[];\n  for (const r of batch){\n    if (r.a != null && r.b != null) valid.push(transform(r));\n    else quarantined.push({row:r, validation_reason:'missing_required_fields'});\n  }\n  writeParquet(valid, datePartition);\n  writeJson(quarantined, quarantineBucket);\n}\n```\n\n## Follow-up Questions\n- How would you test schema evolution if new fields are added?\n- How would you monitor the quarantine rate and alert if it spikes?","diagram":"flowchart TD\n  A[CSV Ingest] --> B[Glue ETL]\n  B --> C[Parquet in S3]\n  B --> D[Quarantine bucket]\n  C --> E[Glue Catalog]\n  E --> F[Athena]","difficulty":"beginner","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Plaid","Two Sigma","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T05:36:30.080Z","createdAt":"2026-01-14T05:36:30.080Z"},{"id":"q-1785","question":"Two-account AWS data lake (us-east-1 and eu-west-1) ingests partner JSON events via API into S3. Design an end-to-end pipeline using Glue for ETL and cataloging, store Parquet with daily partitions by country/date, enable Athena queries with Lake Formation access controls, and handle schema evolution, data quality, late-arrivals, and cross-region replication trade-offs?","answer":"Land partner JSON events into S3 landing bucket, run a Glue ETL to convert to Parquet, partition by country and date, and catalog with Glue. Use Lake Formation for fine‑grained access. Implement schem","explanation":"## Why This Is Asked\n\nTests ability to design a cross-account, cross-region data pipeline with end-to-end data quality and governance considerations. It emphasizes real-world friction points like schema drift, late-arrival data, and access controls.\n\n## Key Concepts\n\n- AWS Glue ETL and Data Catalog\n- Parquet, partition pruning (country/date)\n- Lake Formation access controls and sharing\n- Schema evolution and Glue Schema Registry\n- Data quality checks and late-arrival handling\n- Cross-region replication implications (latency, consistency, cost)\n\n## Code Example\n\n```python\nfrom awsglue.context import GlueContext\nfrom pyspark.context import SparkContext\nfrom pyspark.sql.functions import to_date, col\n\nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\ndf = glueContext.read.json(\"s3://landing-bucket/partner-events/\")\ndf = df.withColumn(\"date\", to_date(col(\"event_time\"))).select(\"country\", \"date\", *df.columns)\ndf.write.partitionBy(\"country\", \"date\").mode(\"append\").parquet(\"s3://curated-bucket/parquet/partner-events/\")\n```\n\n## Follow-up Questions\n\n- How would you test end-to-end with backfills for the last 7 days?\n- How would you automate schema drift detection and backfill compatibility?\n- What Lake Formation grants would you set for analysts vs admins, across accounts?","diagram":"flowchart TD\n  S[Partner API events] --> I[Ingest to S3 land bucket]\n  I --> ETL[Glue ETL to Parquet]\n  ETL --> P[Partitioned Parquet: country/date]\n  P --> Q[Athena via Glue Catalog]\n  Q --> L[Lake Formation access controls]","difficulty":"intermediate","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T10:47:46.380Z","createdAt":"2026-01-14T10:47:46.380Z"},{"id":"q-886","question":"Scenario: you manage a financial data lake with multiple transactional sources feeding S3 via DMS. Stakeholders require upserts, full history for compliance, fast dashboards using a latest-state table, and seamless schema evolution. Compare Iceberg, Hudi, and Delta Lake for this scenario and outline a concrete pipeline (CDC, ETL, compaction, governance)?","answer":"Use Apache Iceberg on S3 to support CDC upserts and full history. Ingest via DMS into a staging Parquet layer, then Spark MERGE INTO to the Iceberg table; maintain a separate 'latest' snapshot for das","explanation":"## Why This Is Asked\nFinancial data lakes require reliable upserts, auditability, and governance. This question probes familiarity with lakehouse formats and practical pipeline design.\n\n## Key Concepts\n- Change Data Capture (CDC) via DMS into S3-backed lakehouse\n- Upserts and history with Iceberg vs Hudi/Delta\n- Schema evolution and partitioning strategy\n- Data governance and validation checks\n\n## Code Example\n```javascript\nCREATE TABLE analytics.fact_sales (\n  id BIGINT,\n  amount DECIMAL(12,2),\n  ts TIMESTAMP,\n  source STRING\n)\nUSING ICEBERG\nPARTITIONED BY (years(ts), months(ts), source);\n\nMERGE INTO analytics.fact_sales_latest AS target\nUSING analytics.fact_sales_staging AS src\nON target.id = src.id\nWHEN MATCHED THEN UPDATE SET amount = src.amount, ts = src.ts\nWHEN NOT MATCHED THEN INSERT (id, amount, ts, source) VALUES (src.id, src.amount, src.ts, src.source);\n```\n\n## Follow-up Questions\n- How would you handle late-arriving data and out-of-order events?\n- What monitoring would you implement for compaction and schema changes?","diagram":"flowchart TD\n  A[CDC Source] --> B[Staging Parquet]\n  B --> C[Spark MERGE to Iceberg]\n  C --> D[Latest Snapshot]\n  D --> E[Athena/BI Dashboards]","difficulty":"intermediate","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T14:25:05.485Z","createdAt":"2026-01-12T14:25:05.485Z"},{"id":"q-948","question":"In a multi-tenant data lake on S3 across two AWS accounts, each tenant's data must be isolated, with auditable access, cost accounting, and fast analytics for dashboards. Design a solution using AWS Lake Formation for governance, S3 prefixes per tenant, and Athena/Glue for analytics. Explain cross-account sharing, tag-based access, and data lineage, plus failure modes?","answer":"Use Lake Formation governance with per-tenant databases mapped to S3 prefixes (s3://lake/tenant-id/...). Expose cross-account access via resource links and tag-based ACLs, audited by Lake Formation an","explanation":"## Why This Is Asked\nTests mastery of cross-account governance, tenant isolation, auditable access, and cost reporting in a real-world data lake.\n\n## Key Concepts\n- Lake Formation governance and resource links\n- Per-tenant S3 prefixes and Glue Data Catalog\n- Tag-based access control and cost allocation\n- Cross-account sharing and CloudTrail/audit trails\n- Data lineage and auditability\n\n## Code Example\n```bash\n# Grant a tenant role read on its catalog table (illustrative)\naws lakeformation grant-permissions --principal '{\"EffectiveFrom\": \"2026-01-12T00:00:00Z\",\"DataLakePrincipalIdentifier\": \"arn:aws:iam::111122223333:role/tenantA\"}' --permissions SELECT --resource '{\"Table\": {\"DatabaseName\": \"tenantA_db\", \"Name\": \"events\"}}'\n```\n\n## Follow-up Questions\n- How would you handle offboarding a tenant's data while preserving analytics history?\n- How would you test isolation and audit completeness in CI/CD?\n","diagram":null,"difficulty":"advanced","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Discord","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T16:36:54.412Z","createdAt":"2026-01-12T16:36:54.412Z"}],"subChannels":["general"],"companies":["Adobe","Amazon","Apple","Coinbase","Discord","DoorDash","Goldman Sachs","Google","Instacart","Lyft","Meta","NVIDIA","Netflix","Oracle","Plaid","Slack","Snap","Square","Stripe","Tesla","Twitter","Two Sigma","Zoom"],"stats":{"total":12,"beginner":3,"intermediate":4,"advanced":5,"newThisWeek":12}}