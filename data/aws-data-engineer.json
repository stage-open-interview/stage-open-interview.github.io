{"questions":[{"id":"q-1336","question":"In a multi-account AWS data lake used by a global product analytics team, ingest semi-structured data from several SaaS APIs into S3, enforce schema evolution and data quality checks, and expose curated tables with time-travel queries. Describe the end-to-end design including data formats, upsert strategy, and governance controls you would apply using AWS services?","answer":"Leverage AWS Glue Data Catalog with Lake Formation, store data in S3 using Apache Iceberg for upserts and time travel, orchestrate with Step Functions, transform in Glue Spark jobs, validate quality v","explanation":"## Why This Is Asked\nTests ability to design multi-account governance, schema evolution, upserts, and cross-service orchestration for a scalable data lake.\n\n## Key Concepts\n- Multi-account governance with Lake Formation\n- Apache Iceberg on S3 for upserts and time travel\n- Glue Data Catalog as central metadata store\n- Data quality tooling (Glue Data Quality or DataBrew)\n- Orchestration with Step Functions\n- Access controls and encryption\n\n## Code Example\n```javascript\n// Spark SQL pseudo: upsert into Iceberg table using MERGE\nspark.sql(\"MERGE INTO lake.product USING staging.product AS s ON t.id = s.id WHEN MATCHED THEN UPDATE SET ... WHEN NOT MATCHED THEN INSERT...\")\n```\n\n## Follow-up Questions\n- How would you handle schema drift across SaaS sources with Iceberg?\n- How would you enforce time-travel/query lineage access controls for data consumers?","diagram":null,"difficulty":"advanced","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Apple"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T11:43:06.203Z","createdAt":"2026-01-13T11:43:06.203Z"},{"id":"q-1419","question":"Design an advanced, multi-region data ingestion and governance pipeline for a global fintech platform. Data from partners arrives as JSON and Parquet; must enforce per-tenant isolation, support near real-time analytics, and GDPR erasure. Outline architecture using AWS Kinesis (streams), DMS for CDC, Glue (ETL), Iceberg on S3 for schema-evolving tables, Lake Formation/IAM for access, and Athena/Redshift Spectrum for queries. Include format choices, CDC vs batch mix, lineage, and failure modes?","answer":"Use Iceberg-on-S3 for per-tenant, time-travel capable tables; stream data through Kinesis and batch via Glue; CDC via DMS from partners; catalog in Glue Data Catalog; enforce isolation with Lake Forma","explanation":"## Why This Is Asked\nTests ability to design end-to-end data governance at scale across regions and tenants.\n\n## Key Concepts\n- Multi-region data lake with Iceberg on S3\n- Per-tenant isolation with Lake Formation\n- Streaming + batch ingestion\n- Data lineage and GDPR erasure\n- Schema evolution and time travel\n\n## Code Example\n```javascript\n// PySpark-like example writing to Iceberg\nconst df = spark.read.format(\"json\").load(\"s3://bucket/partner/*.json\");\ndf.write.format(\"iceberg\").mode(\"append\").save(\"db.tenants.events\");\n```\n\n## Follow-up Questions\n- How would you test data erasure requests end-to-end?\n- What metrics alerting would you implement for cross-region replication lag?","diagram":"flowchart TD\n  A[Partner Sources] --> B[Ingest: Kinesis/Batch]\n  B --> C[Glue ETL]\n  C --> D[Iceberg Tables on S3]\n  D --> E[Queries: Athena/Redshift]\n  D --> F[Access: Lake Formation]\n  F --> G[Lineage & Audits]\n  D --> H[Region Replication (S3)]","difficulty":"advanced","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T16:42:34.969Z","createdAt":"2026-01-13T16:42:34.969Z"},{"id":"q-1487","question":"You operate an AWS data lake where streaming user activity flows from Kinesis Data Streams into S3 via Firehose, with Glue Data Catalog, Athena queries, and dashboards. Schemas evolve and data quality is mission-critical. Design a real-time data quality and drift detection framework: schema drift, per-record quality checks, quarantine failing files, alert owners, and preserve cross-account auditability?","answer":"Leverage Glue Schema Registry for evolution, and real-time validators in Kinesis Data Analytics or Lambda to enforce rules (non-null IDs, valid timestamps, sane ranges) and tag bad records. Quarantine","explanation":"## Why This Is Asked\nAssesses ability to design real-time data quality and drift detection at scale, balancing schema evolution, validation, quarantine, alerting, and cross-account governance using AWS services.\n\n## Key Concepts\n- Schema evolution and registry management\n- Real-time validation and data quarantining\n- Observability: CloudWatch metrics and SNS alerts\n- Cross-account auditability via Glue Data Catalog and Lake Formation\n\n## Code Example\n```javascript\n// Pseudo validator for a streaming record\nfunction isRecordValid(r){\n  if(!r.user_id || !r.timestamp) return false;\n  if(typeof r.value !== 'number' || r.value < 0) return false;\n  return true;\n}\n```\n\n## Follow-up Questions\n- How would you test drift detection across regions and accounts?\n- What are trade-offs of using a separate quarantine bucket vs inline rejection for analytics workloads?\n","diagram":"flowchart TD\n  A[Kinesis] --> B[Firehose]\n  B --> C[S3 + Glue Catalog]\n  C --> D[Athena]\n  D --> E[Alerts/Owners]","difficulty":"advanced","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Coinbase","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T18:59:52.005Z","createdAt":"2026-01-13T18:59:52.005Z"},{"id":"q-1506","question":"Scenario: A partner API provides daily batches of image assets with evolving metadata. Build an end-to-end ingest to a multi-region data lake: landing in S3, metadata in a versioned, upsertable table, idempotent processing, schema evolution, and governance across accounts. Describe data formats, partitioning, dedupe strategy, and cross-account orchestration?","answer":"Ingest images via multipart S3 uploads behind a deduplicated manifest; store metadata in a versioned, upsertable Iceberg table on S3 (via Glue/Spark) keyed by asset_id + batch_id; handle schema evolut","explanation":"## Why This Is Asked\nThis question tests practical design for a cross-region data lake with large binary assets, evolving metadata, and strict governance. It probes hands-on trade-offs and tooling choices that show depth beyond basic ingestion.\n\n## Key Concepts\n- Ingesting large assets with multipart uploads and manifests\n- Idempotent upserts using a composite key (asset_id + batch_id)\n- Schema evolution supported by Iceberg/Delta on S3\n- Cross-account governance via Lake Formation and Glue Data Catalog\n- Multi-region orchestration with Step Functions\n\n## Code Example\n```javascript\n// Example: upsert metadata into a versioned Iceberg-like table (pseudo)\nasync function upsertMetadata(client, rec) {\n  const current = await client.read(rec.asset_id);\n  const merged = { ...current, ...rec };\n  await client.write(rec.asset_id, merged);\n}\n```\n\n## Follow-up Questions\n- How would you handle API rate limits and retries?\n- How would you validate data quality, schema drift, and auditability in production?","diagram":null,"difficulty":"intermediate","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","Slack","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T19:43:56.091Z","createdAt":"2026-01-13T19:43:56.091Z"},{"id":"q-1521","question":"You manage a global, multi-tenant data platform where streaming events bucket into S3 and downstream engines like Redshift and Athena rely on per-tenant schemas. Design a resilient end-to-end architecture that versions datasets, enforces per-tenant schema contracts, detects drift, quarantines bad records, and supports rollback without downtime. Specify services, data formats, governance, and observability?","answer":"Propose using Firehose/Kinesis for streaming into S3 with per-tenant prefixes; Glue Schema Registry to enforce per-tenant contracts; Iceberg on S3 for versioned tables; Lake Formation for tenant isola","explanation":"## Why This Is Asked\nTests multi-tenant data governance, schema contracts, drift detection, and rollback—critical for scalable data products.\n\n## Key Concepts\n- Multi-tenant governance with Glue Schema Registry and Lake Formation.\n- Versioned storage with Iceberg on S3.\n- Drift detection and quality checks via Glue jobs and Deequ.\n- Quarantine workflows and rollback with Step Functions.\n- Observability via CloudWatch and Athena-based lineage.\n\n## Code Example\n```javascript\n// Placeholder for a data quality check wiring example\n```\n\n## Follow-up Questions\n- How would you handle schema evolution while preserving historic queries?\n- What metrics and alerts signal a drift or ingestion failure?","diagram":"flowchart TD\n  Ingest[Kinesis/Firehose] --> Stage[S3 (tenant prefixes)]\n  Stage --> Catalog[Glue Catalog & Iceberg Tables]\n  Catalog --> Validate[Deequ drift checks]\n  Validate --> Quarantine[Quarantine bucket]\n  Quarantine --> Orchestrator[Step Functions]\n  Orchestrator --> Observability[CloudWatch]","difficulty":"advanced","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Google","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T20:39:24.546Z","createdAt":"2026-01-13T20:39:24.546Z"},{"id":"q-1543","question":"Design an event-driven data platform to ingest order and driver events from regional services into a centralized data lake for near-real-time analytics. Data arrives as evolving JSON schemas; must handle schema evolution, efficient partitioning, cross-account access, and cost. Which AWS services and data design patterns would you implement, and how would you validate data quality and enable fast ad-hoc queries?","answer":"Use Kinesis Data Streams for event ingestion, a Glue streaming job with Glue Schema Registry to normalize evolving JSON schemas, and write Parquet to S3 partitioned by date/region/service. Employ Lake Formation for fine-grained cross-account access, implement data quality validation with Glue job assertions and Deequ checks, and enable fast ad-hoc queries through Athena with optimized partitioning and columnar formats. Apply cost controls through file compaction, lifecycle policies, and intelligent tiering.","explanation":"## Why This Is Asked\n\nExamines real-time data ingestion, schema evolution, cross-account governance, and cost-aware storage patterns in a practical, production context.\n\n## Key Concepts\n\n- Streaming ingestion: Kinesis Data Streams + Glue Schema Registry\n- Real-time normalization: Glue Streaming or Kinesis Data Analytics\n- Storage design: Parquet on S3 with partitioning by date/region/service\n- Governance: Lake Formation fine-grained access, Glue Data Catalog, drift monitoring\n- Quality & cost: deduplication, compaction, managed small files\n\n## Code Example\n\n```python\n# Glue Spark job snippet for schema evolution and data quality\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.types import StructType\n\n# Initialize Glue context\nargs = getResolvedOptions(__SYS__, ['JOB_NAME'])\nsc = SparkContext()\nglue_context = GlueContext(sc)\nspark = glue_context.spark_session\n\n# Read from Kinesis with schema registry\ndynamic_frame = glue_context.create_dynamic_frame.from_options(\n    connection_type=\"kinesis\",\n    connection_options={\n        \"streamARN\": \"arn:aws:kinesis:region:account:stream/order-events\",\n        \"classification\": \"json\",\n        \"endpointUrl\": \"https://kinesis.region.amazonaws.com\"\n    },\n    transformation_ctx=\"kinesis_source\"\n)\n\n# Apply schema evolution and data quality checks\ndf = dynamic_frame.toDF()\n\n# Data quality validation\ndf = df.filter(\n    F.col(\"order_id\").isNotNull() &\n    F.col(\"timestamp\").isNotNull() &\n    F.col(\"region\").isNotNull()\n)\n\n# Write optimized Parquet to S3\ndf.write.mode(\"append\").partitionBy(\n    \"date\", \"region\", \"service_type\"\n).parquet(\"s3://data-lake/orders/\")\n```","diagram":"flowchart TD\n  A[Ingest: Kinesis Data Streams] --> B[Normalize: Glue Schema Registry & Glue Streaming]\n  B --> C[Store: S3 Parquet with partitions: date/region/service]\n  C --> D[Catalog: Glue Data Catalog + Lake Formation policies]\n  D --> E[Query: Athena / Redshift Spectrum]\n  E --> F[Quality & Monitoring: Glue jobs + CloudWatch]","difficulty":"intermediate","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T06:31:51.252Z","createdAt":"2026-01-13T21:31:34.885Z"},{"id":"q-1608","question":"Scenario: A new source streams JSON events from a mobile app via Kinesis Data Streams. Design a beginner-level ingestion pipeline to land data in S3 as Parquet with daily partitions, using AWS Glue for ETL and cataloging. Include services, data formats, schema evolution approach, basic data quality checks, and how you would test end-to-end before analytics?","answer":"Design a streaming ingestion pipeline using Kinesis Data Streams to capture JSON events from the mobile app, with AWS Glue ETL jobs that process the data, convert it to Parquet format, and write to S3 with daily date partitions. The Glue Data Catalog will register the resulting tables for analytics. Implement simple schema evolution by allowing new optional fields while maintaining backward compatibility, and include basic data quality checks for required fields, data types, and null values.","explanation":"## Why This Is Asked\nEvaluates practical, beginner-appropriate streaming ingestion using common AWS data tooling and basic quality controls.\n\n## Key Concepts\n- Kinesis Data Streams for real-time data ingestion\n- AWS Glue ETL and Data Catalog for data processing and metadata management\n- Parquet format with date-based partitioning for optimized storage and queries\n- Simple schema evolution strategies for handling changing data structures\n- Data quality checks (required fields, types, nulls)\n- CloudWatch alarms and end-to-end testing\n\n## Code Example\n```javascript\n// Pseudo-code to start a Glue job\nconst glue = new AWS.Glue();\nawait glue.startJobRun({\n  JobName: 'mobile-events-etl',\n  Arguments: {\n    '--input-stream': 'mobile-app-events',\n    '--output-path': 's3://analytics-bucket/events/',\n    '--partition-format': 'yyyy/MM/dd'\n  }\n});\n```","diagram":null,"difficulty":"beginner","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["NVIDIA","Netflix","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T05:30:45.364Z","createdAt":"2026-01-14T02:34:42.960Z"},{"id":"q-1631","question":"You receive a daily nested JSON file and a separate CSV in an S3 bucket. Design a beginner-level ingestion using AWS Glue Studio to flatten the JSON, normalize types, parse the CSV, and write Parquet under a date-partitioned path s3://data-lake/events/date=YYYY-MM-DD/. Register a Glue catalog table, outline simple schema-drift handling, and include basic data-quality checks plus a quick end-to-end test plan with sample files?","answer":"Describe a beginner-level ingestion using AWS Glue Studio for two daily files in S3: a nested JSON and a CSV. Build a Glue job to flatten JSON, normalize types, parse CSV, and write Parquet to s3://da","explanation":"## Why This Is Asked\nBeginners must design practical ETL workflows that handle mixed formats and small drift, using Glue Studio in repeatable ways.\n\n## Key Concepts\n- Glue Studio ETL\n- JSON flattening and CSV parsing\n- Parquet partitioning by date\n- Glue Data Catalog table\n- Basic data quality checks and schema drift\n\n## Code Example\n```javascript\nfunction flatten(obj, prefix = '', res = {}) {\n  for (const k in obj) {\n    const v = obj[k];\n    const key = prefix ? prefix + '.' + k : k;\n    if (v && typeof v === 'object' && !Array.isArray(v)) flatten(v, key, res);\n    else res[key] = v;\n  }\n  return res;\n}\n```\n\n## Follow-up Questions\n- How would you test handling of occasionally missing fields?\n- What would you do to scale the job for larger batches?","diagram":"flowchart TD\n  A[Ingest Files to S3] --> B[Glue Studio Job: JSON flatten] \n  B --> C[Glue Studio Job: CSV parse] \n  C --> D[Write Parquet to date partition] \n  D --> E[Glue Catalog Table with metadata] \n  E --> F[Quality checks & Schema drift handling] \n  F --> G[Test plan & validation]","difficulty":"beginner","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","Oracle","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T04:18:19.843Z","createdAt":"2026-01-14T04:18:19.843Z"},{"id":"q-1654","question":"New data source writes daily CSVs to S3. Design a beginner pipeline to validate a defined schema, fail-fast on missing required fields, and route bad records to a quarantine bucket with a reason. Store valid data as Parquet in S3 with daily partitions, and catalog via Glue so Athena can query. Include a basic end-to-end test plan?","answer":"Set up a Glue Spark job triggered by S3 PUT events to read CSVs with a defined schema, filter out rows missing required fields, and write invalid rows to a quarantine bucket with a validation_reason f","explanation":"## Why This Is Asked\nPractical beginner task validating end-to-end ingestion, data quality, and cataloging.\n\n## Key Concepts\n- S3 event-driven ETL\n- Glue Spark/DynamicFrame validation\n- Quarantine routing for bad records\n- Parquet partitioning by date\n- Glue Data Catalog and Athena access\n\n## Code Example\n```javascript\n// Pseudo Glue-like JS snippet for validation\nfunction process(batch){\n  const valid=[], quarantined=[];\n  for (const r of batch){\n    if (r.a != null && r.b != null) valid.push(transform(r));\n    else quarantined.push({row:r, validation_reason:'missing_required_fields'});\n  }\n  writeParquet(valid, datePartition);\n  writeJson(quarantined, quarantineBucket);\n}\n```\n\n## Follow-up Questions\n- How would you test schema evolution if new fields are added?\n- How would you monitor the quarantine rate and alert if it spikes?","diagram":"flowchart TD\n  A[CSV Ingest] --> B[Glue ETL]\n  B --> C[Parquet in S3]\n  B --> D[Quarantine bucket]\n  C --> E[Glue Catalog]\n  E --> F[Athena]","difficulty":"beginner","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Plaid","Two Sigma","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T05:36:30.080Z","createdAt":"2026-01-14T05:36:30.080Z"},{"id":"q-1785","question":"Two-account AWS data lake (us-east-1 and eu-west-1) ingests partner JSON events via API into S3. Design an end-to-end pipeline using Glue for ETL and cataloging, store Parquet with daily partitions by country/date, enable Athena queries with Lake Formation access controls, and handle schema evolution, data quality, late-arrivals, and cross-region replication trade-offs?","answer":"Land partner JSON events into S3 landing bucket, run a Glue ETL to convert to Parquet, partition by country and date, and catalog with Glue. Use Lake Formation for fine‑grained access. Implement schem","explanation":"## Why This Is Asked\n\nTests ability to design a cross-account, cross-region data pipeline with end-to-end data quality and governance considerations. It emphasizes real-world friction points like schema drift, late-arrival data, and access controls.\n\n## Key Concepts\n\n- AWS Glue ETL and Data Catalog\n- Parquet, partition pruning (country/date)\n- Lake Formation access controls and sharing\n- Schema evolution and Glue Schema Registry\n- Data quality checks and late-arrival handling\n- Cross-region replication implications (latency, consistency, cost)\n\n## Code Example\n\n```python\nfrom awsglue.context import GlueContext\nfrom pyspark.context import SparkContext\nfrom pyspark.sql.functions import to_date, col\n\nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\ndf = glueContext.read.json(\"s3://landing-bucket/partner-events/\")\ndf = df.withColumn(\"date\", to_date(col(\"event_time\"))).select(\"country\", \"date\", *df.columns)\ndf.write.partitionBy(\"country\", \"date\").mode(\"append\").parquet(\"s3://curated-bucket/parquet/partner-events/\")\n```\n\n## Follow-up Questions\n\n- How would you test end-to-end with backfills for the last 7 days?\n- How would you automate schema drift detection and backfill compatibility?\n- What Lake Formation grants would you set for analysts vs admins, across accounts?","diagram":"flowchart TD\n  S[Partner API events] --> I[Ingest to S3 land bucket]\n  I --> ETL[Glue ETL to Parquet]\n  ETL --> P[Partitioned Parquet: country/date]\n  P --> Q[Athena via Glue Catalog]\n  Q --> L[Lake Formation access controls]","difficulty":"intermediate","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T10:47:46.380Z","createdAt":"2026-01-14T10:47:46.380Z"},{"id":"q-1816","question":"You collect protobuf-encoded telemetry from thousands of IoT devices via AWS IoT Core; design an end-to-end ingestion that lands as daily-partitioned Parquet in S3, uses AWS Glue Schema Registry for evolving schemas, and implements idempotent deduplication plus a 3-sigma anomaly check. Include data formats, partition keys, testing strategy?","answer":"Proposed approach: IoT Core receives protobuf telemetry; push to Kinesis Data Streams; a Lambda decodes protobuf using the Glue Schema Registry schema and writes to S3 via Firehose as Parquet. Partiti","explanation":"## Why This Is Asked\n\nTests ability to design real-time to batch pipelines, handle protobuf, schema evolution, dedup, and quality checks in a multi-service AWS environment, with testing considerations.\n\n## Key Concepts\n\n- IoT Core to Kinesis to Lambda decoding\n- Glue Schema Registry usage\n- Parquet partitioning by date and device_type\n- Idempotent deduplication (device_id, message_id)\n- 3-sigma anomaly checks and end-to-end testing\n\n## Code Example\n\n```python\n# PySpark snippet for 3-sigma anomaly detection\nfrom pyspark.sql import functions as F\ndf = spark.read.parquet(\"s3://bucket/path\")\nstats = df.agg(F.mean(\"value\").alias(\"mean\"), F.stddev(\"value\").alias(\"stddev\")).collect()[0]\nmean, std = stats[\"mean\"], stats[\"stddev\"]\ndf.filter((F.abs(df.value - mean) > 3*std)).select(...)\n```\n\n## Follow-up Questions\n\n- How would you handle schema evolution without breaking consumers?\n- What are the failure modes and how would you monitor dedup accuracy?\n","diagram":"flowchart TD\n  IoTCore(IoT Core) --> KDS[Kinesis Data Streams]\n  KDS --> Lambda[Decode protobuf]\n  Lambda --> Firehose[Firehose to S3 Parquet]\n  Firehose --> S3[S3: daily partitions by date, device_type]\n  S3 --> Catalog[Glue Data Catalog]\n  Catalog --> Dedup[DynamoDB dedupe table]","difficulty":"intermediate","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Meta","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T11:50:43.897Z","createdAt":"2026-01-14T11:50:43.897Z"},{"id":"q-1883","question":"Design a streaming data contract and quality workflow for events arriving from multiple teams via MSK and Kinesis. Use AWS Glue Schema Registry to enforce evolving Avro/JSON schemas with versioning and compatibility (backward/forward). Describe publishing schemas, validating payloads at ingest, storing Parquet in S3 with daily partitions, and monitoring for drift and quality across regions?","answer":"Use Glue Schema Registry with per-stream Avro/JSON schemas, versioned and backward/forward compatible. Producers send schemaId in headers; a validation step enforces the contract at ingest. Write to S","explanation":"## Why This Is Asked\n\nTests practical use of Glue Schema Registry, schema evolution governance, and cross-region data quality workflows. It also covers integration with streaming sources (MSK/Kinesis) and storage in S3, with error handling and drift monitoring.\n\n## Key Concepts\n\n- Glue Schema Registry and Avro/JSON\n- Schema versioning and compatibility (backward/forward)\n- Ingest validation and header schemaId\n- Parquet ingestion with date-based partitions\n- Glue Data Quality and CloudWatch drift alerts\n\n## Code Example\n\n```javascript\n// Pseudo-code: register and validate a schema, then publish event with header schemaId\n```\n\n## Follow-up Questions\n\n- How would you test schema evolution changes across producers without downtime?\n- How do you handle breaking changes in existing pipelines?","diagram":null,"difficulty":"advanced","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T15:43:31.534Z","createdAt":"2026-01-14T15:43:31.534Z"},{"id":"q-1968","question":"Design an end-to-end streaming-to-lake pipeline: ingest real-time events from AWS MSK, consolidate into Apache Iceberg tables on S3 with daily partitions, support CDC-style upserts/deletes, and implement schema evolution. Include governance with Lake Formation/IAM, testing strategy (canaries, synthetic data), and data quality/lineage monitoring?","answer":"Ingest from MSK to Spark Streaming, write to Iceberg tables on S3 with daily partitions; implement upserts/deletes using Iceberg MERGE; enable schema evolution via Iceberg schema migrations and Glue C","explanation":"## Why This Is Asked\nAssesses ability to design a scalable, governed streaming-to-warehouse pipeline using Iceberg, CDC semantics, and robust testing.\n\n## Key Concepts\n- Apache Iceberg on S3 with partitioning and schema evolution\n- CDC upserts/deletes in Iceberg\n- MSK ingest and Spark Structured Streaming or Glue jobs\n- Data governance with Lake Formation and IAM\n- Data quality checks and lineage (canaries, synthetic data, metadata catalogs)\n\n## Code Example\n```python\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import from_json, col\nspark = SparkSession.builder.getOrCreate()\n# schema would be defined per event\nschema = ...\ndf = spark.readStream.format(\"kafka\").option(\"kafka.bootstrap.servers\",\"host:9092\").option(\"subscribe\",\"rides\").load()\nevents = df.selectExpr(\"CAST(value AS STRING) as json\").select(from_json(col(\"json\"), schema).alias(\"r\")).select(\"r.*\")\nquery = events.writeStream.format(\"iceberg\").option(\"path\",\"s3://bucket/iceberg/rides_events\").option(\"checkpointLocation\",\"s3://bucket/checkpoints/rides\").outputMode(\"append\").start(\"analytics.rides_events\")\nquery.awaitTermination()\n```\n\n## Follow-up Questions\n- How would you handle late data and retractions?\n- What are the trade-offs between Iceberg vs. Delta in this context?","diagram":"flowchart TD\n  MSK(MSK) --> Iceberg[Iceberg Tables on S3]\n  Iceberg --> Catalog[Glue Data Catalog]\n  Catalog --> BI[Athena/Presto]\n  Iceberg --> CDC[CDC Upserts/Deletes]\n  Iceberg --> Governance[Data Governance]\n  Governance --> LF[Lake Formation]\n  BI --> Monitor[Data Quality & Lineage Alerts]","difficulty":"advanced","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T19:01:30.560Z","createdAt":"2026-01-14T19:01:30.561Z"},{"id":"q-1981","question":"Design a data lake ingestion pipeline on S3 for semi-structured SaaS data that must support upserts and deletes with time-travel queries; choose between Apache Iceberg, Hudi, or Delta Lake on AWS (Glue, EMR, Athena) and justify: schema evolution, compaction, CDC handling, partitioning, and data quality checks; include testing plan?","answer":"Use Apache Iceberg on S3 (Parquet) with a Glue Data Catalog. Implement upserts/deletes via MERGE, with tombstones for deletes and partition pruning for cost. Ingest via CDC streams from SaaS sources, ","explanation":"## Why This Is Asked\nThe candidate must reason about ACID on object storage and multi-source ingestion.\n\n## Key Concepts\n- Iceberg vs. alternatives for upserts\n- CDC, dedupe, tombstones\n- Schema evolution and time travel\n- Partitioning and compaction costs\n- Validation via Athena/QA dashboards\n\n## Code Example\n```javascript\n// placeholder example of MERGE operation with Iceberg\n```\n\n## Follow-up Questions\n- How would you instrument data quality checks and alerting?\n- How would you test schema evolution across deployments?\n","diagram":"flowchart TD\n  A[Source SaaS] --> B[CDC Stream]\n  B --> C[Iceberg Table on S3]\n  C --> D[Athena/Glue Access]\n  D --> E[Time Travel / QA]","difficulty":"intermediate","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","Instacart","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T19:35:55.962Z","createdAt":"2026-01-14T19:35:55.962Z"},{"id":"q-2019","question":"Scenario: IoT sensors on factory floors emit JSON telemetry with evolving schemas (new metrics appear over time). Design a streaming ingestion pipeline that reads from Kinesis Data Streams, writes to S3 as Parquet with daily partitions, and uses an Iceberg catalog (Glue-backed) to handle schema evolution. Compare upsert strategies for late data (Iceberg MERGE INTO vs Hudi), outline data quality checks, and a testing plan including sandbox data and end-to-end validation. Include governance via Lake Formation?","answer":"Leverage Glue Streaming (Spark) to ingest JSON from Kinesis Data Streams and write daily-partitioned Parquet in S3, backed by an Iceberg catalog (Glue). Use MERGE INTO for late data; rely on Iceberg f","explanation":"Why This Is Asked\nTests experience with streaming ingestion, advanced file formats, and schema evolution in production.\n\nKey Concepts\n- Streaming ETL with Glue or Spark\n- Parquet with daily partitions\n- Iceberg catalog in Glue and schema evolution\n- Upserts for late data (MERGE INTO vs Hudi)\n- Data quality and drift detection\n- End-to-end testing in sandbox; governance via Lake Formation\n\nCode Example\n```javascript\n// Pseudo Spark SQL for Iceberg MERGE\nMERGE INTO iceberg_db.telemetry AS t\nUSING staged_telemetry AS s\nON t.device_id = s.device_id AND t.ts = s.ts\nWHEN MATCHED THEN UPDATE SET *\nWHEN NOT MATCHED THEN INSERT *\n```\n\nFollow-up Questions\n- How would you test schema evolution with additive fields?\n- How would you monitor ingestion quality and alert on drift?","diagram":"flowchart TD\n  A[Kinesis Data Streams] --> B[Glue Streaming (Spark)]\n  B --> C[S3 Parquet daily partitions]\n  C --> D[Iceberg Catalog (Glue)]\n  D --> E[Athena/Redshift Spectrum]","difficulty":"intermediate","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","IBM","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T20:55:38.216Z","createdAt":"2026-01-14T20:55:38.216Z"},{"id":"q-2060","question":"Design a multi-region, streaming-plus-batch ingestion pipeline for a mobile app that emits JSON events at high volume. Ingest via Kinesis Data Streams to S3 Parquet using Apache Iceberg tables with a Glue catalog, supporting upserts, deletes, and schema evolution. Include multi-source CDC, data quality checks, governance via Lake Formation, and end-to-end testing?","answer":"Leverage Iceberg tables on S3 with a Glue catalog, landing JSON from Kinesis via a streaming job that MERGEs into Iceberg to support upserts/deletes; replicate CDC from multiple sources using DMS across regions with schema evolution and governance.","explanation":"## Why This Is Asked\nThis question probes design for scalable, auditable data lakes across regions, balancing mutating events with schema changes and governance.\n\n## Key Concepts\n- Apache Iceberg on S3 with Glue catalog\n- Upserts/deletes via MERGE semantics\n- Multi-source CDC (DMS) across regions\n- Schema evolution and data quality checks (Deequ)\n- Lake Formation governance and end-to-end testing\n\n## Code Example\n```javascript\nMERGE INTO iceberg_db.events AS t\nUSING staging.events AS s\nON t.event_id = s.event_id\nWHEN MATCHED THEN UPDATE SET t.payload = s.payload, t.ts = s.ts\nWHEN NOT MATCHED THEN INSERT (event_id, payload, ts) VALUES (s.event_id, s.payload, s.ts)\n```","diagram":null,"difficulty":"advanced","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","Meta","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T04:34:48.217Z","createdAt":"2026-01-14T22:47:50.677Z"},{"id":"q-2095","question":"You receive a financial dataset hourly from a partner API in JSON with nested fields. Design a beginner-level ingestion pipeline to land data in S3 as Parquet with hourly partitions, using AWS Glue for ETL and cataloging. Include how you flatten nested JSON, define a stable schema, enable basic data quality checks, ensure encryption with KMS, and implement least-privilege IAM roles and Lake Formation permissions. Outline how you'd test end-to-end before analytics?","answer":"Implement a secure data ingestion pipeline using a KMS-encrypted S3 bucket to store raw JSON files with hourly prefixes. Configure a Glue Crawler to infer the initial schema and populate the Glue Data Catalog. Develop a Glue ETL job in Python to flatten nested JSON fields, convert to Parquet format, and write to a processed S3 location with hourly partitioning. Enable basic data quality checks using Glue DataBrew rulesets. Apply least-privilege IAM roles and Lake Formation permissions to control access. Validate the pipeline end-to-end using sample data before production deployment.","explanation":"## Why This Is Asked\nTests practical data ingestion setup with common AWS services and security considerations.\n\n## Key Concepts\n- JSON flattening, Parquet storage, hourly partitions\n- Glue Crawlers, Glue ETL (Python), Catalog with Lake Formation\n- KMS encryption, IAM least privilege, data quality checks\n\n## Code Example\n```python\n# flatten nested json\nimport json\n\ndef flatten(record):\n    flat = {}\n    for k, v in record.items():\n        if isinstance(v, dict):\n            for kk, vv in v.items():\n                flat[f\"{k}_{kk}\"] = vv\n        else:\n            flat[k] = v\n    return flat\n```","diagram":"flowchart TD\n  A[Partner JSON hourly] --> B[Glue Crawler infer schema]\n  B --> C[Glue ETL flatten to Parquet]\n  C --> D[S3/processed hourly partitions]\n  D --> E[Athena/Quicksight access]","difficulty":"beginner","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Robinhood","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T05:02:22.343Z","createdAt":"2026-01-14T23:45:36.307Z"},{"id":"q-2144","question":"Design a real-time fraud signals pipeline: ingest 50 banks via Kinesis Data Streams, use Glue Schema Registry for evolving Avro schemas, run Spark Streaming to land Parquet in S3 partitioned by date and merchantId, with a raw zone and a masked curated zone. Apply IP masking and immutability retention; catalog with Glue Data Catalog; enforce access via Lake Formation; emit lineage/audit logs. Include failover tests and latency considerations?","answer":"Architect a real-time fraud signals pipeline: ingest 50 banks via Kinesis Data Streams, use Glue Schema Registry for evolving Avro schemas, Spark Streaming to write Parquet to S3 with date and merchan","explanation":"## Why This Is Asked\nTests end-to-end streaming design, schema evolution, privacy controls, data governance, and resilience in production.\n\n## Key Concepts\n- Real-time ingestion via Kinesis Data Streams with Glue Schema Registry for evolving Avro schemas\n- Streaming transform to Parquet with partitioning by date and merchantId\n- Data masking, raw vs curated zones, immutable retention\n- Cataloging with Glue Data Catalog and access control via Lake Formation\n- Data lineage and audit logging; end-to-end validation\n\n## Code Example\n```python\n# PySpark sketch: mask IP and write to curated zone\nfrom pyspark.sql.functions import sha2, col\n\ndf = spark.readStream.format('kinesis').option('streamName','fraud').load()\nmasked = df.withColumn('ipAddress', sha2(col('ipAddress'), 256))\nmasked.writeStream.format('parquet').option('path','s3://bucket/curated/')\n```\n\n## Follow-up Questions\n- How would you handle schema drift in Glue Schema Registry during peak load?\n- How would you simulate regional outages and validate failover?","diagram":"flowchart TD\n  A[Ingest: Kinesis Data Streams] --> B[Spark Streaming]\n  B --> C[S3: Raw Parquet]\n  B --> D[S3: Curated Parquet]\n  C --> E[Glue Catalog]\n  D --> E\n  E --> F[Lake Formation Access]","difficulty":"advanced","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Plaid","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T04:23:42.398Z","createdAt":"2026-01-15T04:23:42.399Z"},{"id":"q-2204","question":"Design a real-world, multi-tenant, multi-account ingestion pipeline: Ingest JSON events from microservices via Kinesis Data Streams, store as Parquet in S3 with daily partitioning by tenant_id, and catalog with AWS Glue. Enforce tenant isolation with Lake Formation across accounts, handle schema evolution, embed data quality checks, and outline end-to-end testing strategies?","answer":"In a multi-tenant, multi-account data lake, ingest JSON events from microservices via Kinesis Data Streams, land to S3 as Parquet partitioned by date and tenant_id. Use Glue for ETL and cataloging; en","explanation":"## Why This Is Asked\nTests cross-account data governance, real-world multi-tenant isolation, and evolving schemas in a streaming-to-batch pipeline.\n\n## Key Concepts\n- Multi-account data governance with Lake Formation\n- Parquet partitioning by date and tenant_id\n- Glue ETL + Data Catalog with schema evolution\n- Data quality checks in ETL and streaming\n- End-to-end testing with synthetic tenants\n\n## Code Example\n```javascript\n// Pseudocode: ETL outline for Glue-like job (conceptual)\nasync function runETL(inputStream){\n  const batch = await readJSONEvents(inputStream)\n  const transformed = batch.map(e => ({...e, tenant_id: e.tenantId, date: toDate(e.ts)}))\n  await writeParquet(transformed, `/s3/bucket/tenant/date`, {partitionBy: ['tenant_id','date']})\n  await updateGlueCatalog(transformed)\n}\n```\n\n## Follow-up Questions\n- How would you implement schema evolution without breaking downstream jobs?\n- What monitoring and alerting would you add for data-skew or partition churn?","diagram":"flowchart TD\n  A[Ingest JSON events via Kinesis] --> B[Transform & Partition by tenant_id/date]\n  B --> C[S3 Parquet Landing Zone]\n  C --> D[Glue Catalog & Lake Formation Policies]\n  D --> E[Analytics & BI Access]\n  E --> F[Monitoring & Alerts]","difficulty":"intermediate","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Bloomberg","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T07:35:14.749Z","createdAt":"2026-01-15T07:35:14.749Z"},{"id":"q-2244","question":"You receive daily JSON event files from a mobile app stored in S3 with nested arrays. Design a beginner-friendly ingestion pipeline to flatten into Parquet with daily partitions, catalog in AWS Glue, and implement a basic data quality gate (required fields, non-null user_id, ISO8601 event_time). Include testing approach and handling of schema drift?","answer":"Implement a Glue Spark job that reads daily JSON files from s3://partner-logs/incoming/YYYY/MM/DD, flattens nested arrays with explode, and maps to a canonical schema (user_id, event_time, device, act","explanation":"## Why This Is Asked\nTests practical workflow design for beginner to apply Glue, S3, and Parquet with schema evolution and data quality.\n\n## Key Concepts\n- Flattening nested JSON, Parquet partitions, Glue Catalog\n- Simple data quality rules, alerting, basic deduplication\n- Testing with sample data and end-to-end validation\n\n## Code Example\n```javascript\n// Pseudo Glue job invocation using AWS SDK\nimport { GlueClient, StartJobRunCommand } from \"@aws-sdk/client-glue\";\nconst client = new GlueClient({ region: \"us-east-1\" });\nconst cmd = new StartJobRunCommand({ JobName: \"FlattenJsonToParquet\" });\nclient.send(cmd);\n```\n\n## Follow-up Questions\n- How would you extend this with schema drift handling across daily files?\n- How would you test idempotency for repeated runs","diagram":"flowchart TD\n  A[S3 Incoming daily JSON] --> B[Glue ETL Job]\n  B --> C[Parquet in S3 with daily partitions]\n  C --> D[Glue Data Catalog]\n  D --> E[Athena/Quicksight dashboards]","difficulty":"beginner","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","LinkedIn","Lyft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T09:01:35.624Z","createdAt":"2026-01-15T09:01:35.624Z"},{"id":"q-2264","question":"In a cross-account AWS data lake spanning us-east-1 and eu-west-1, ingest streaming events from Kinesis into an Apache Hudi dataset on S3 to support upserts and time-travel. Design the end-to-end architecture including how to implement upserts, schema evolution, compaction, cross-account Lake Formation policies, and an end-to-end test and rollback plan, with concrete services and data formats?","answer":"Use Kinesis to trigger a Spark job that writes to S3 as an Apache Hudi MOR table, enabling upserts and time-travel. Catalog with Glue; enforce cross-account Lake Formation grants; handle schema evolut","explanation":"## Why This Is Asked\nTests ability to design an upsert-capable lake using Hudi, cross-account access, and testing/rollback.\n\n## Key Concepts\n- Apache Hudi MOR on S3 with Glue Catalog\n- Kinesis streaming and Spark ETL\n- Lake Formation cross-account access\n- Compaction and late-arrival handling\n- Backfill and rollback strategies\n\n## Code Example\n```javascript\n# PySpark/Hudi upsert example (conceptual)\ndf.write.format(\"org.apache.hudi\").options({\n  \"hoodie.datasource.write.recordkey\": \"id\",\n  \"hoodie.datasource.write.table_type\": \"MERGE_ON_READ\",\n  \"hoodie.datasource.write.precombine_field\": \"ts\",\n  \"hoodie.datasource.write.payload.use_object_digest\": \"true\"\n}).mode(\"append\").save(\"s3://bucket/path/hudi-table\")\n```\n\n## Follow-up Questions\n- How would you test compaction performance at scale?\n- What are cross-account Lake Formation pitfalls and mitigations for this pattern?","diagram":null,"difficulty":"advanced","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T09:48:00.369Z","createdAt":"2026-01-15T09:48:00.369Z"},{"id":"q-2312","question":"Scenario: you manage a multi-tenant SaaS data lake across three AWS accounts. Ingest telemetry events via Kinesis Data Streams, land as Parquet in S3 with daily partitions, and register metadata in the Glue Data Catalog. Enforce schema evolution and tenant-level masking using Lake Formation; enable cross-account analytics with Athena. Describe architecture, governance, and testing strategy?","answer":"Leverage Lake Formation with per-tenant databases and table permissions, and use IAM roles for cross-account access. Land data as Parquet in S3 with daily partitions, register in the Glue Data Catalog","explanation":"## Why This Is Asked\nTests cross‑account governance, fine‑grained access, and scalable analytics in a multi‑tenant data lake. It probes masking, schema evolution, and end‑to‑end validation.\n\n## Key Concepts\n- Cross‑account data lake governance with Lake Formation\n- Fine‑grained access and tenant isolation\n- Schema evolution in Glue Data Catalog\n- Parquet partitioning by date\n- Data masking for PII\n- Cross‑account analytics with Athena\n- Data lineage and end‑to‑end testing\n\n## Code Example\n```json\n{\n  \"MaskingPolicy\": {\n    \"Column\": \"email\",\n    \"MaskType\": \"EMAIL\",\n    \"AppliedTo\": \"TenantA\"\n  }\n}\n```\n\n## Follow-up Questions\n- How would you validate masking in dev vs prod environments?\n- How do you propagate schema changes without breaking downstream queries?\n- What monitoring would you implement for cross‑account access and data lineage?","diagram":"flowchart TD\n  A[Telemetry Source (Kinesis Data Streams)]\n  B[S3 Parquet (daily partitions)]\n  C[Glue Data Catalog]\n  D[Lake Formation Masking]\n  E[Athena Cross-Account Access]\n  F[BI/Dashboards]\nA --> B\nB --> C\nC --> D\nD --> E\nE --> F","difficulty":"intermediate","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","Microsoft","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T11:40:27.776Z","createdAt":"2026-01-15T11:40:27.776Z"},{"id":"q-2350","question":"Design an end-to-end ingestion and upsert pipeline for customer records with SCD Type 2 using Apache Iceberg on S3. Ingest JSON events from Kinesis and batch CDC from RDS; ensure schema evolution, partitioning, and time travel; discuss how to reconcile late-arriving changes and record-level lineage. Include services, table format, upsert strategy, and testing?","answer":"Use Apache Iceberg on S3 with Glue Catalog. Real-time: Kinesis Data Streams feed a Spark Structured Streaming job writing to an Iceberg table partitioned by day. Batch: AWS DMS captures RDS CDC to S3,","explanation":"## Why This Is Asked\nTests practical lakehouse design with Iceberg on S3, SCD Type 2, and hybrid real-time/batch ingestion, plus schema evolution and lineage.\n\n## Key Concepts\n- Iceberg on S3 with Glue Catalog\n- Real-time ingestion via Kinesis and Spark Structured Streaming\n- Batch CDC via AWS DMS from RDS to S3\n- SCD Type 2 upsert semantics (start_date, end_date, current)\n- Schema evolution and time travel in Iceberg\n- Data lineage via catalog and metadata\n\n## Code Example\n```javascript\nMERGE INTO iceberg.customers AS target\nUSING staging AS source\nON target.cust_id = source.cust_id AND target.current = true\nWHEN MATCHED THEN UPDATE SET end_date = source.batch_date, current = false\nWHEN NOT MATCHED THEN INSERT (..., start_date, end_date, current) VALUES (...)\n```\n\n## Follow-up Questions\n- How would you validate late-arriving changes to avoid duplicates?\n- How would you test schema evolution conflicts across real-time and batch paths?\n- How would you monitor and prove end-to-end data lineage across services?","diagram":"flowchart TD\n  A[Kinesis Real-time Ingest] --> B[Spark Streaming to Iceberg @ S3]\n  C[DMS CDC to S3] --> B\n  B --> D[Iceberg Table with Glue Catalog]\n  D --> E[Time Travel & Schema Evolution]","difficulty":"advanced","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Apple","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T14:38:32.079Z","createdAt":"2026-01-15T14:38:32.079Z"},{"id":"q-2440","question":"Scenario: Daily JSON app-logs arrive in S3 at s3://bucket/raw/app-logs with fields userId, timestamp, action, and optional device {model,os}. For a beginner, design an ingestion pipeline that flattens to Parquet with daily partitions into s3://bucket/curated/app-logs/yyyy/mm/dd/, catalogs in Glue, and uses a defined schema (userId string, timestamp timestamp, action string, device_model string, device_os string) with schema evolution enabled. Include data quality gates (required userId, non-null timestamp, restricted action set), drift handling, and end-to-end testing plan?","answer":"Use a Glue ETL job to read JSON from s3://bucket/raw/app-logs, flatten device fields, and write Parquet to s3://bucket/curated/app-logs/yyyy/mm/dd with daily partitions. Enforce schema: userId string,","explanation":"## Why This Is Asked\nTests practical use of AWS Glue for end-to-end ingestion, Parquet formatting, and schema evolution with basic data quality.\n\n## Key Concepts\n- AWS Glue ETL and Data Catalog\n- Parquet with partitioning by date\n- Flattening nested JSON to a flat schema\n- Schema evolution and data quality gates\n\n## Code Example\n```python\n# Minimal Glue ETL outline (conceptual)\nfrom awsglue.transforms import *\nfrom awsglue.dynamicframe import DynamicFrame\n\ndef flatten(frame):\n    # flatten device fields\n    return frame\n```\n\n## Follow-up Questions\n- How would you test for schema drift and roll back changes?\n- How would you handle new fields arriving in device or action?\n","diagram":null,"difficulty":"beginner","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Plaid","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T17:57:24.761Z","createdAt":"2026-01-15T17:57:24.761Z"},{"id":"q-2494","question":"Ingest a hybrid workload into an Iceberg table on S3: streaming JSON events and nightly relational extracts, partitioned by date. Outline a concrete pipeline using AWS Glue, S3, and query engines (Athena/Databricks), with data formats, schema evolution, upserts, late-arriving data, and end-to-end validation. How would you monitor partitions and performance and enforce access controls?","answer":"Use a single Iceberg-backed table on S3 with daily partitions. Stream JSON via Kinesis Firehose to Parquet, batch PostgreSQL exports to Parquet, all merged into Iceberg. Implement MERGE on event_id fo","explanation":"## Why This Is Asked\nTests ability to design a hybrid batch/stream pipeline, choose Iceberg on S3, and reason about schema evolution, upserts, late data handling, validation, and governance in AWS.\n\n## Key Concepts\n- Apache Iceberg on S3 with Delta-like upserts\n- Schema evolution and partitioning\n- Hybrid ingestion (stream + batch)\n- Data quality and end-to-end testing\n- Observability: partition counts, lag, job metrics\n\n## Code Example\n```sql\nMERGE INTO iceberg_db.iceberg_table AS t\nUSING staging_view AS s\nON t.event_id = s.event_id\nWHEN MATCHED THEN UPDATE SET t.payload = s.payload, t.updated_at = s.updated_at\nWHEN NOT MATCHED THEN INSERT (event_id, payload, updated_at) VALUES (s.event_id, s.payload, s.updated_at);\n```\n\n## Follow-up Questions\n- How would you handle schema changes that affect downstream schemas?\n- How would you implement access control and data sharing in this setup?","diagram":null,"difficulty":"intermediate","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Databricks","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T20:35:49.838Z","createdAt":"2026-01-15T20:35:49.838Z"},{"id":"q-2594","question":"Design an end-to-end AWS data lake ingestion and governance solution that provides complete data lineage across multiple sources (RDS, SaaS API, IoT), transformations in AWS Glue, and consumption in Athena. Include how you model lineage metadata (source, transform, target), handle schema evolution, and how you validate lineage during failing ETL runs and across re-processing, with Lake Formation fencing?","answer":"Instrument each ETL run with a unique run_id and emit comprehensive lineage records (source→transformation→target) to a centralized lineage ledger stored in S3, with corresponding metadata managed in the Glue Data Catalog. Orchestrate the ingestion pipeline using Step Functions, while Lake Formation enforces access controls and implements fencing mechanisms to prevent concurrent modifications. For schema evolution, maintain versioned schemas in the Glue Data Catalog and leverage AWS Glue Schema Registry to validate compatibility. During ETL failures, Step Functions' built-in retry mechanisms and error handling ensure lineage records are committed exclusively for successful transformations, preserving data integrity across re-processing scenarios.","explanation":"## Why This Is Asked\nTests capability to design production-grade lineage and governance across sources, transforms, and targets, with robust testing for failures.\n\n## Key Concepts\n- Data lineage modeling across sources, transforms, targets\n- Glue Data Catalog + S3 lineage ledger\n- Step Functions orchestration, Lake Formation controls\n- Schema evolution handling and validation\n- End-to-end testing of lineage integrity in ETL runs\n\n## Code Example\n```javascript\n// Pseudo-code: emitLineage(runId, source, transform, target)\nfunction emitLineage(runId, source, transform, target) {\n  const entry = {\n    run_id: runId,\n    timestamp: new Date().toISOString(),\n    source: source,\n    transformation: transform,\n    target: target,\n    status: 'success'\n  };\n  // Write to lineage ledger in S3\n  s3.putObject({\n    Bucket: 'lineage-ledger',\n    Key: `lineage/${runId}/${Date.now()}.json`,\n    Body: JSON.stringify(entry)\n  });\n  // Update Glue Data Catalog metadata\n  glue.updateTable({ ... });\n}\n```\n\nThis approach ensures complete traceability while handling failures gracefully and maintaining data governance through Lake Formation's fencing capabilities.","diagram":"flowchart TD\n  A[Source Systems] --> B[Ingestion Layer]\n  B --> C[Glue ETL Jobs]\n  C --> D[Lineage Ledger (S3)]\n  C --> E[Glue Data Catalog]\n  D --> F[Athena/Curated Tables]\n  E --> F","difficulty":"advanced","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["PayPal","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T05:04:47.209Z","createdAt":"2026-01-16T02:22:44.228Z"},{"id":"q-2653","question":"Scenario: A global product analytics data lake ingests streaming clickstream events from mobile apps with evolving JSON schemas containing PII. You must mask PII in the shared dataset before analytics, support cross-account access, and keep raw data auditable for compliance. Design end-to-end ingestion using AWS (Kinesis or MSK, Glue, S3, Lake Formation/Macie, KMS). Address schema evolution, late data, data quality, testing, and rollout?","answer":"Two-branch streaming: ingest with Kinesis (or MSK); route to a masking Spark job in Glue that redacts PII but preserves IDs; emit masked Parquet to S3 in daily partitions (date, region, app). Use Glue","explanation":"## Why This Is Asked\\nThis question probes practical privacy-preserving data ingestion at scale, cross-account access, and governance.\\n\\n## Key Concepts\\n- Streaming data pipelines (Kinesis/MSK)\\n- PII masking in ETL (Glue Spark UDFs)\\n- Data lake governance (Glue Catalog, Lake Formation)\\n- PII discovery (Macie) and KMS-based encryption\\n\\n## Code Example\\n```python\\n# Glue Spark masking example (conceptual)\\ndef mask(record):\\n  record['email'] = hash_value(record.get('email'))\\n  record['phone'] = mask_phone(record.get('phone'))\\n  return record\\n```\\n\\n## Follow-up Questions\\n- How would you test schema evolution and backward compatibility across partitions?\\n- How would you monitor data quality and alert on masking failures in production?","diagram":null,"difficulty":"advanced","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","MongoDB","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T05:39:51.621Z","createdAt":"2026-01-16T05:39:51.621Z"},{"id":"q-2686","question":"Design an ingestion pipeline for a new SaaS data stream of JSON events that lands in S3 with daily partitions. Use AWS Glue (Spark) and Apache Iceberg on S3 to materialize updatable tables with idempotent merges, handle schema evolution, and cope with late-arriving data. Describe partitioning, upsert strategy, schema evolution, data quality checks, testing, and monitoring?","answer":"Propose a pipeline: Glue Spark reads JSON with nested fields, flattens it, and writes to an Iceberg table on S3 with daily partitions. Upsert by a stable PK using MERGE INTO, and enable Iceberg schema","explanation":"## Why This Is Asked\n\nInterview context explanation.\n\n## Key Concepts\n\n- Iceberg on S3 with Glue catalog\n- MERGE INTO upserts by PK\n- Schema evolution and partitioning strategies\n- Late-arrivals handling with watermark\n- Data quality testing and monitoring\n\n## Code Example\n\n```javascript\n// Pseudo: validate a record's key fields before merge\nfunction valid(rec){\n  return rec && rec.userId && typeof rec.eventTime === 'string'\n}\n```\n\n## Follow-up Questions\n\n- How would you handle schema drift detection for Iceberg tables?\n- How would you implement backfills when source schema changes?","diagram":"flowchart TD\n  Ingest[JSON Ingest] --> Spark[Glue Spark Job]\n  Spark --> Iceberg[Iceberg Table on S3]\n  Iceberg --> Upsert[MERGE INTO by PK]\n  Ingest --> Late[Late Arrivals] --> Iceberg","difficulty":"intermediate","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","Snap","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T06:58:16.227Z","createdAt":"2026-01-16T06:58:16.228Z"},{"id":"q-2875","question":"In a regulated financial environment, design an end-to-end AWS data pipeline that ingests daily batch CSV exports and real-time credit-event streams into a centralized data lake. Explain how you would land data in S3 with partitions, implement idempotent upserts and schema evolution, enforce governance via Lake Formation and the Glue Data Catalog, implement data quality checks and tests, and observe and rollback on failures?","answer":"Hybrid ingestion design: batch feeds land CSVs into S3 under date partitions; a real-time stream of events arrives on Kinesis Data Streams and is written to S3 as JSON via Firehose. Glue Spark jobs pe","explanation":"## Why This Is Asked\nThis question probes real-world data ingestion, transformation, governance, and rollback strategies under regulatory constraints.\n\n## Key Concepts\n- Batch + streaming ingestion patterns\n- Idempotent upserts and schema evolution\n- Lake Formation governance and Glue Catalog\n- Data quality checks and testing strategies\n- Observability and rollback plans\n\n## Code Example\n```javascript\n// Pseudo Glue job outline for Spark\nimport sys\nfrom awsglue.context import GlueContext\nfrom pyspark.context import SparkContext\n\nsc = SparkContext()\ngc = GlueContext(sc)\ndf = gc.create_dynamic_frame.from_options(\"s3\", {\"path\": \"s3://bucket/path\"}, \"csv\")\ndf = df.drop_fields([\"unneeded\"])\ndf = df.resolveChoice(\"cast\", {\"column\": \"int\"})\n# upsert logic would be implemented here using a sink with a manifest\n```\n\n## Follow-up Questions\n- How would you surface schema evolution without breaking downstream dashboards?\n- What tests validate idempotence and rollback behavior?","diagram":"flowchart TD\n  A[Ingest] --> B[S3 Landing]\n  B --> C[Glue ETL]\n  C --> D[Parquet Lake]\n  A --> E[Firehose to S3]\n  D --> F[Athena/Glue Catalog]\n  F --> G[Lake Formation Governance]","difficulty":"intermediate","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","MongoDB","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T15:44:50.072Z","createdAt":"2026-01-16T15:44:50.073Z"},{"id":"q-2925","question":"You have three data sources: a REST API streaming JSON via Kinesis Data Streams, a CSV feed landed to S3, and an on-prem CDC stream to AWS. Design an end-to-end pipeline that lands data in S3 as Parquet with per-source/date partitions, uses Apache Hudi for upserts, supports schema evolution via the Glue Catalog, handles late data, includes data quality checks, and a practical end-to-end test plan?","answer":"Use a source-agnostic ingestion: Kinesis Data Streams for JSON, S3 event-based for CSV; land raw data in per-source/date prefixes; Glue Spark jobs write to Apache Hudi datasets in S3 Parquet with upse","explanation":"## Why This Is Asked\nTests ability to integrate heterogeneous sources into a single lake, with transactional-like upserts on object storage, and resilient schema management.\n\n## Key Concepts\n- Upserts on object storage using Apache Hudi (MERGE_ON_READ or COPY_ON_WRITE)\n- Glue Catalog-backed schema evolution and partition pruning\n- Late-arriving data handling and watermarking\n- Data quality checks (Glue Data Quality or Great Expectations) with DLQ routing\n- End-to-end testing with synthetic data and staged environments\n\n## Code Example\n```javascript\n// Pseudocode: Hudi upsert in Spark (Glue)\ndf.write.format(\"hudi\")\n  .option(\"hoodie.datasource.write.operation\",\"upsert\")\n  .option(\"hoodie.datasource.write.table_type\",\"MERGE_ON_READ\")\n  .option(\"hoodie.datasource.write.recordkey\",\"id\")\n  .option(\"hoodie.datasource.write.partitionpath.field\",\"source,date\")\n  .mode(\"append\").save(\"s3://bucket/data-sourceA/ parquets/\");\n```\n\n## Follow-up Questions\n- How would you validate idempotency and late-arrival correctness across sources?\n- What cost/throughput trade-offs exist between Kinesis+Glue vs a streaming-first lake strategy?","diagram":"flowchart TD\n  A(Source) --> B(Ingestion)\n  B --> C(Glue ETL)\n  C --> D(Hudi Parquet in S3)\n  D --> E(Analytics Layer)","difficulty":"intermediate","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Meta","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T17:49:06.966Z","createdAt":"2026-01-16T17:49:06.966Z"},{"id":"q-2985","question":"Scenario: You must ingest daily JSONL logs from an on-prem SFTP into S3. Design a beginner-friendly, serverless ingestion using AWS Transfer Family to pull files, convert to Parquet with daily partitions, catalog via Glue, and orchestrate via Step Functions. What steps would you implement, including naming, partitioning, quality checks, and testing?","answer":"Serverless SFTP ingest: Use AWS Transfer Family to pull daily JSONL from on-prem SFTP into S3 at logs/date=YYYY-MM-DD/. Run a Glue Spark ETL to convert to Parquet with daily partitions and register in","explanation":"## Why This Is Asked\nTests practical use of serverless ingest for on-prem sources, with a safe beginner path using Transfer Family, Glue, and Step Functions.\n\n## Key Concepts\n- AWS Transfer Family for SFTP pulls\n- Glue Spark ETL and Data Catalog\n- Parquet with daily partitions\n- Step Functions orchestration and basic data quality\n\n## Code Example\n```python\n# Glue ETL outline to convert JSONL to Parquet and write daily partitions\nimport sys\nfrom awsglue.context import GlueContext\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\n# ... minimal skeleton\n```\n\n## Follow-up Questions\n- How would you handle schema drift or missing fields in JSONL?\n- How would you monitor Step Functions failures and alert on retries?","diagram":"flowchart TD\n  A[SFTP Ingest] --> B[Transfer Family Pull]\n  B --> C[Glue ETL (JSONL->Parquet)]\n  C --> D[S3 Partitioned by date]\n  D --> E[Glue Catalog]\n  E --> F[Step Functions Orchestrator]","difficulty":"beginner","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Microsoft","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T19:47:48.168Z","createdAt":"2026-01-16T19:47:48.168Z"},{"id":"q-3199","question":"In a 3-region data lake ingesting mobile-app events, design an end-to-end pipeline to ingest JSON events from Kinesis Data Streams, normalize to Parquet, partition by tenant_id/date, and mask PII. Ensure schema evolution, data lineage, and cross-account sharing with Snowflake, IBM, and Airbnb. Include services, upsert strategy (CDC-based) for updates, testing, and monitoring?","answer":"Ingest via Kinesis Data Streams; Glue Spark parses JSON, masks PII, and writes Parquet to S3 partitioned by tenant_id/date. Catalog with Glue; enable cross-account Lake Formation grants for Snowflake/","explanation":"## Why This Is Asked\nAssesses streaming ingestion, cross-account data sharing, and governance in a multi-region data lake with PII handling.\n\n## Key Concepts\n- Kinesis Data Streams\n- AWS Glue Spark ETL\n- Parquet data format and partitioning\n- Glue Data Catalog and Lake Formation cross-account sharing\n- CDC-based upserts and schema evolution\n- Data lineage and monitoring\n\n## Code Example\n```javascript\nfunction maskPII(s) {\n  if (typeof s !== 'string') return s\n  return s.replace(/.(?=.{4})/g, '*')\n}\n```\n\n## Follow-up Questions\n- How would you monitor data quality and drift across regions?\n- How would you validate compatibility of schema changes with Snowflake and IBM dashboards?\n","diagram":null,"difficulty":"advanced","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","IBM","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T06:48:33.161Z","createdAt":"2026-01-17T06:48:33.161Z"},{"id":"q-3302","question":"In a cross-account AWS data lake, ingest real-time financial trades (JSON) from Kinesis Data Streams, land as Parquet in S3 with daily partitioning by trade_date and tenant_id for tenant isolation, and catalog with Glue. Enforce column-level PII masking and cross-account Lake Formation access, support schema evolution, and provide an end-to-end test plan. What is your architecture and approach?","answer":"Streaming design: Kinesis Data Streams feeds a Glue Spark job (or Kinesis Data Analytics) that writes Parquet to S3 with daily partition by trade_date and tenant_id. Catalog via Glue Data Catalog. Imp","explanation":"## Why This Is Asked\nTests ability to design a cross-account, governed data pipeline with real-time ingestion, data sanitization, and schema evolution. It probes tool choices, partitioning strategy, and testing rigor.\n\n## Key Concepts\n- Cross‑account Lake Formation and IAM permissions\n- Kinesis to S3 (Parquet) with partitioning\n- Column-level masking and masked views\n- Glue Data Catalog and schema evolution\n- End-to-end testing in a regulated data lake\n\n## Code Example\n```python\n# PySpark masking example (conceptual)\nfrom pyspark.sql.functions import col, when\ndf2 = df.withColumn(\"masked\", when(col(\"ssn\").isNotNull(), \"MASKED\").otherwise(None))\n```\n\n## Follow-up Questions\n- How would you validate cross-account permissions during testing?\n- How would you handle schema drift and evolving masked columns across partitions?","diagram":null,"difficulty":"advanced","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Databricks","IBM"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T10:39:13.775Z","createdAt":"2026-01-17T10:39:13.775Z"},{"id":"q-3358","question":"In a multi-account data lake for regulated analytics, ingest data from multiple SaaS apps into S3 as Parquet. Implement PII masking at ingest, data lineage, and cross-account access controls using Lake Formation. Design end-to-end pipeline with per-source ingestion paths, masking rules, schema evolution, and a robust test plan. Include the roles, services, and hand-off between ingestion, masking, and BI layers?","answer":"Ingest from each SaaS app into S3 using Firehose, then run a Glue Spark job to mask PII (SSN, emails) and write masked data as Parquet partitioned by source/date. Expose raw and masked tables in a Glu","explanation":"## Why This Is Asked\n\nAssesses ability to design secure, observable data pipelines across accounts, with PII masking, lineage, and governance using Lake Formation, Glue, and S3 Parquet. Focuses on practical patterns for multi-tenant data sharing and compliance.\n\n## Key Concepts\n\n- Lake Formation permissions and masking\n- Glue Catalog and Spark-based data masking\n- Parquet with schema evolution and partitioning\n- Data lineage, audit via Glue Catalog/CloudTrail\n- Cross-account access via RAM and trusted roles\n\n## Code Example\n\n```javascript\nfunction maskPII(value, type) {\n  if (type === 'ssn') return value ? value.replace(/\\d{3}-\\d{2}-\\d{4}/g, 'XXX-XX-XXXX') : value;\n  if (type === 'email') {\n    const [name, domain] = (value||'').split('@');\n    return (name ? name[0] + '***' : '') + '@' + (domain||'');\n  }\n  return value;\n}\n```\n\n## Follow-up Questions\n\n- How would you test masking coverage and lineage, and what metrics?\n- How would you adapt this design for tenants with different data schemas?","diagram":null,"difficulty":"intermediate","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","LinkedIn","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T13:17:49.491Z","createdAt":"2026-01-17T13:17:49.491Z"},{"id":"q-3422","question":"A SaaS provider drops a daily CSV export into S3 at s3://data-logs/saasA/YYYY/MM/DD/export.csv.gz. Outline a beginner-level AWS data pipeline using AWS Glue to ingest, convert to Parquet, partition by date, store in s3://data-lake/saasA/YYYY/MM/DD/, register in the Glue Data Catalog, and query with Athena. Include how you’d handle a new column 'region' added today (schema evolution), basic data quality checks, and a minimal end-to-end test plan?","answer":"Set up a Glue crawler to infer the CSV schema, then a Glue ETL job (Python Spark) that reads s3://data-logs/saasA/YYYY/MM/DD/export.csv.gz, writes Parquet to s3://data-lake/saasA/YYYY/MM/DD/ with dail","explanation":"## Why This Is Asked\n\nTests ability to translate a raw CSV drop into a managed, queryable Lake with partitioning, cataloging, and basic quality checks. It also checks handling of simple schema drift in a beginner-friendly way.\n\n## Key Concepts\n\n- AWS Glue ETL (Python Spark) for CSV->Parquet\n- Glue Data Catalog and Athena for querying\n- Daily partitioning by year/month/day in S3\n- Simple schema evolution: nullable fields and catalog versioning\n- Data quality checks: non-empty, key integrity\n\n## Code Example\n\n```python\n# Glue ETL skeleton (conceptual)\nfrom awsglue.context import GlueContext\nfrom awsglue.utils import getResolvedOptions\nfrom awsglue.job import Job\n\n# setup and read CSV -> Parquet logic here\n```\n\n## Follow-up Questions\n\n- How would you test schema evolution with a new column in a CI pipeline?\n- What are the minimal IAM permissions required for the Glue job to read and write?","diagram":null,"difficulty":"beginner","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Hashicorp"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T15:37:42.173Z","createdAt":"2026-01-17T15:37:42.173Z"},{"id":"q-3464","question":"Design a hybrid streaming/batch data pipeline on AWS for a multi-tenant analytics platform. Ingest streaming events from Kinesis and batch exports to S3; normalize to Parquet with per-tenant prefixes and date partitions. Implement upserts on S3 with Apache Hudi (Glue/EMR), enforce per-tenant RBAC via Lake Formation, and manage schema evolution with Glue Data Catalog. Include data quality using Deequ and an end-to-end test strategy?","answer":"Design a hybrid streaming/batch pipeline in AWS for a multi-tenant analytics platform. Ingest streaming events from Kinesis and batch exports to S3; normalize to Parquet with per-tenant prefixes and d","explanation":"## Why This Is Asked\n\nAssesses ability to design a scalable, secure, multi-tenant data lake with hybrid ingestion, upsert semantics, and governance across streaming and batch sources.\n\n## Key Concepts\n\n- Hybrid ingestion (stream + batch)\n- Tenant isolation and RBAC (Lake Formation)\n- Upserts on object storage (Apache Hudi)\n- Parquet with tenant/date partitioning\n- Schema evolution (Glue Catalog/Registry)\n- Data quality checks (Deequ)\n- End-to-end testing strategies\n\n## Code Example\n\n```python\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\ndf = spark.read.json(\"s3://incoming/events/\")\ndf.write.format(\"org.apache.hudi\").options(\n  {\n    \"hoodie.datasource.write.recordkey\": \"event_id\",\n    \"hoodie.datasource.write.table_type\": \"MERGE_ON_READ\",\n    \"hoodie.datasource.write.precombine_field\": \"ts\"\n  }\n).mode(\"append\").save(\"s3://lake/tenant_id/events\")\n```\n\n## Follow-up Questions\n\n- How would you validate schema drift without breaking downstream queries?\n- What metrics and alarms would you set for data freshness and quality?","diagram":"flowchart TD\n  A[Ingest: Kinesis] --> B[Stream to S3 Parquet (tenant/date)]\n  B --> C[Upsert: Apache Hudi (Glue/EMR)]\n  C --> D[Glue Data Catalog + Lake Formation]\n  D --> E[Query: Athena / Redshift Spectrum]\n  E --> F[Quality & Governance]","difficulty":"advanced","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Databricks","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T17:30:00.794Z","createdAt":"2026-01-17T17:30:00.794Z"},{"id":"q-3560","question":"Ingest data from three SaaS feeds arriving in S3 with mixed timestamp formats and time zones. Design a beginner AWS data pipeline that uses AWS Glue to normalize timestamps to UTC, convert to Parquet, partition by ingestion date (YYYY/MM/DD), and catalog with Glue Data Catalog; ensure schema evolution with a new 'deviceId' field; implement a basic data quality check (null counts) and a minimal end-to-end test plan. Include how you'd validate a sample dataset and what tests you’d run at each stage?","answer":"Use an AWS Glue Spark job to read multi-source JSON/CSV files from S3, parse multiple timestamp formats into UTC, normalize to a canonical schema, write Parquet to s3://lake/partner/YYYY/MM/DD/ with partitioning by ingestion date, and configure a Glue Crawler with schema evolution policy for the new 'deviceId' field.","explanation":"## Why This Is Asked\nTests multi-source timestamp normalization, Parquet partitioning strategies, and foundational data quality practices—core competencies for entry-level data engineers working with real-world SaaS integrations.\n\n## Key Concepts\n- Cross-source timestamp normalization to UTC\n- AWS Glue ETL jobs and Data Catalog integration\n- Parquet file format with date-based partitioning\n- Schema evolution via crawler configuration\n- Basic data quality validation and testing frameworks\n\n## Code Example\n```javascript\nfunction normalizeTimestamp(ts, formats) {\n  // Parse multiple timestamp formats\n  for (const format of formats) {\n    const parsed = moment(ts, format, true);\n    if (parsed.isValid()) {\n      return parsed.utc().format();\n    }\n  }\n  throw new Error(`Unable to parse timestamp: ${ts}`);\n}\n```","diagram":"flowchart TD\n  A[Sources: SaaS 1-3] --> B[Glue Job: Normalize]\n  B --> C[Parquet: s3://lake/partner/YYYY/MM/DD]\n  C --> D[Glue Catalog]\n  D --> E[Athena/Quicksight] ","difficulty":"beginner","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","LinkedIn"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T05:45:42.083Z","createdAt":"2026-01-17T21:30:36.259Z"},{"id":"q-3654","question":"Ingest multi-source JSON logs from trading platforms into a centralized data lake on AWS. Platforms evolve schema weekly; events arrive out of order and duplicates exist. Design an end-to-end pipeline using S3, Glue, and Athena that handles schema evolution, deduplication, daily Parquet partitions, and data quality checks. Include testing and cross-region reliability?","answer":"Design a multi-source ingest using Kinesis Data Streams (or MSK) to collect JSON from each platform, de-duplicate by (platform,event_id), and land into S3 as Parquet with daily partitions. Use Glue Sc","explanation":"## Why This Is Asked\nValidates handling varied data sources, schema evolution, and data quality in a real AWS data lake.\n\n## Key Concepts\n- Multi-source ingestion, idempotent dedup, schema evolution, Parquet partitioning\n- Glue Schema Registry, Glue ETL, Data Catalog, Athena\n- End-to-end testing, cross-region reliability, monitoring\n\n## Code Example\n```javascript\n// Placeholder: integration wiring example would reside in infra-as-code\n```\n\n## Follow-up Questions\n- How would you implement exactly-once semantics across regions?\n- Which failure modes require manually replaying data and how to detect them?","diagram":"flowchart TD\n  A[Ingest: KDS/MSK] --> B[Dedup: DynamoDB]\n  B --> C[Glue ETL]\n  C --> D[S3: Parquet]\n  D --> E[Catalog: Glue Data Catalog]\n  E --> F[Query: Athena]","difficulty":"intermediate","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Citadel","Goldman Sachs"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T04:10:41.071Z","createdAt":"2026-01-18T04:10:41.074Z"},{"id":"q-3708","question":"A fintech app emits daily NDJSON logs (nested fields) to S3: s3://transact-logs/TEAM/YYYY/MM/DD/tx.ndjson.gz. Outline a beginner pipeline using AWS Glue to crawl, flatten, and convert to Parquet with daily partitions at s3://lake/fintech/TEAM/YYYY/MM/DD/, cataloged in Glue. Include simple data quality checks (null rates, range checks), how to handle future schema drift (new fields), and a minimal end-to-end test plan with Athena validation?","answer":"A fintech app emits daily NDJSON logs (nested fields) to S3: s3://transact-logs/TEAM/YYYY/MM/DD/tx.ndjson.gz. Outline a beginner pipeline using AWS Glue to crawl, flatten, and convert to Parquet with ","explanation":"Why This Is Asked\n- Tests a practical, end-to-end Glue-based ingestion flow for nested JSON without overcomplication.\n- Checks handling of NDJSON, flattening logic, Parquet partitioning, and Glue Data Catalog registration.\n- Assesses basic data quality checks and a simple schema drift plan suitable for beginners.\n\nKey Concepts\n- AWS Glue Crawlers and ETL\n- NDJSON flattening and Parquet conversion\n- S3 daily partitioning and Glue Catalog integration\n- Athena for validation and schema drift handling\n- Basic data quality checks (null rates, value ranges)\n\nCode Example\n```python\n# Pseudo-Glue ETL skeleton (PySpark)\nfrom awsglue.context import GlueContext\nfrom awsglue.transforms import *\nfrom pyspark.context import SparkContext\nsc = SparkContext()\ngc = GlueContext(sc)\ndf = gc.create_dataframe_from_s3(\"s3://path\", format=\"ndjson\")\nflat = df.flatten()  # flatten nested fields\nparquet = flat.cast(\"decimal(10,2)\", [\"amount\"])  # example cast\nparquet.write(\"s3://lake/fintech/TEAM/YYYY/MM/DD/\", partitionKeys=[\"date\"]) \n```\n\nFollow-up Questions\n- How would you adjust for partial failures across days?\n- What small changes ensure compatibility when a new field appears in the NDJSON?","diagram":"flowchart TD\n  A[Raw NDJSON in S3] --> B[Glue Crawler]\n  B --> C[Glue ETL: flatten + cast]\n  C --> D[Parquet in S3 with daily partitions]\n  D --> E[Glue Catalog]\n  E --> F[Athena validation]","difficulty":"beginner","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Meta","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T06:46:57.266Z","createdAt":"2026-01-18T06:46:57.266Z"},{"id":"q-3725","question":"Beginner-level: Daily multi-tenant event logs arrive as JSON in s3://raw/tenant_id/date/. Design an end-to-end pipeline using S3, AWS Glue (ETL + Data Catalog), and Athena to land data as Parquet with tenant/date partitions, handle evolving schemas with new optional fields, add a lineage table capturing source_path and tenant_id, implement basic data quality checks, and provide a minimal test plan?","answer":"Use S3 as the raw landing, a Glue crawler to create the catalog, and a Glue ETL job (Python) to flatten JSON to a tabular schema, write Parquet to s3://lake/{tenant_id}/{date}/, and generate a lineage","explanation":"## Why This Is Asked\nTests end-to-end AWS data ingestion with cataloging, Parquet, and Athena, plus lineage basics.\n\n## Key Concepts\n- S3 landing, Glue Crawlers, Glue ETL, Parquet, partitioning by tenant/date\n- Glue Data Catalog, Athena\n- Schema evolution with new optional fields\n- Basic data quality checks and lineage tracking\n\n## Code Example\n```python\n# Pseudo Glue ETL outline\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\n# Flatten nested JSON, map fields, and write Parquet\n```\n\n## Follow-up Questions\n- How would you scale for thousands of tenants?\n- How would you handle late-arriving data and schema drift?","diagram":"flowchart TD\n  RawLogs[s3://raw/tenant/date] --> Crawler[Crawler creates Catalog]\n  Crawler --> ETL[Glue ETL -> Parquet]\n  ETL --> Lake[s3://lake/tenant/date]\n  Lake --> Athena[Athena/Quicksight]","difficulty":"beginner","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Hashicorp"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T07:27:17.689Z","createdAt":"2026-01-18T07:27:17.691Z"},{"id":"q-886","question":"Scenario: you manage a financial data lake with multiple transactional sources feeding S3 via DMS. Stakeholders require upserts, full history for compliance, fast dashboards using a latest-state table, and seamless schema evolution. Compare Iceberg, Hudi, and Delta Lake for this scenario and outline a concrete pipeline (CDC, ETL, compaction, governance)?","answer":"Use Apache Iceberg on S3 to support CDC upserts and full history. Ingest via DMS into a staging Parquet layer, then Spark MERGE INTO to the Iceberg table; maintain a separate 'latest' snapshot for das","explanation":"## Why This Is Asked\nFinancial data lakes require reliable upserts, auditability, and governance. This question probes familiarity with lakehouse formats and practical pipeline design.\n\n## Key Concepts\n- Change Data Capture (CDC) via DMS into S3-backed lakehouse\n- Upserts and history with Iceberg vs Hudi/Delta\n- Schema evolution and partitioning strategy\n- Data governance and validation checks\n\n## Code Example\n```javascript\nCREATE TABLE analytics.fact_sales (\n  id BIGINT,\n  amount DECIMAL(12,2),\n  ts TIMESTAMP,\n  source STRING\n)\nUSING ICEBERG\nPARTITIONED BY (years(ts), months(ts), source);\n\nMERGE INTO analytics.fact_sales_latest AS target\nUSING analytics.fact_sales_staging AS src\nON target.id = src.id\nWHEN MATCHED THEN UPDATE SET amount = src.amount, ts = src.ts\nWHEN NOT MATCHED THEN INSERT (id, amount, ts, source) VALUES (src.id, src.amount, src.ts, src.source);\n```\n\n## Follow-up Questions\n- How would you handle late-arriving data and out-of-order events?\n- What monitoring would you implement for compaction and schema changes?","diagram":"flowchart TD\n  A[CDC Source] --> B[Staging Parquet]\n  B --> C[Spark MERGE to Iceberg]\n  C --> D[Latest Snapshot]\n  D --> E[Athena/BI Dashboards]","difficulty":"intermediate","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T14:25:05.485Z","createdAt":"2026-01-12T14:25:05.485Z"},{"id":"q-948","question":"In a multi-tenant data lake on S3 across two AWS accounts, each tenant's data must be isolated, with auditable access, cost accounting, and fast analytics for dashboards. Design a solution using AWS Lake Formation for governance, S3 prefixes per tenant, and Athena/Glue for analytics. Explain cross-account sharing, tag-based access, and data lineage, plus failure modes?","answer":"Use Lake Formation governance with per-tenant databases mapped to S3 prefixes (s3://lake/tenant-id/...). Expose cross-account access via resource links and tag-based ACLs, audited by Lake Formation an","explanation":"## Why This Is Asked\nTests mastery of cross-account governance, tenant isolation, auditable access, and cost reporting in a real-world data lake.\n\n## Key Concepts\n- Lake Formation governance and resource links\n- Per-tenant S3 prefixes and Glue Data Catalog\n- Tag-based access control and cost allocation\n- Cross-account sharing and CloudTrail/audit trails\n- Data lineage and auditability\n\n## Code Example\n```bash\n# Grant a tenant role read on its catalog table (illustrative)\naws lakeformation grant-permissions --principal '{\"EffectiveFrom\": \"2026-01-12T00:00:00Z\",\"DataLakePrincipalIdentifier\": \"arn:aws:iam::111122223333:role/tenantA\"}' --permissions SELECT --resource '{\"Table\": {\"DatabaseName\": \"tenantA_db\", \"Name\": \"events\"}}'\n```\n\n## Follow-up Questions\n- How would you handle offboarding a tenant's data while preserving analytics history?\n- How would you test isolation and audit completeness in CI/CD?\n","diagram":null,"difficulty":"advanced","tags":["aws-data-engineer"],"channel":"aws-data-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Discord","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T16:36:54.412Z","createdAt":"2026-01-12T16:36:54.412Z"}],"subChannels":["general"],"companies":["Adobe","Airbnb","Amazon","Anthropic","Apple","Bloomberg","Citadel","Cloudflare","Coinbase","Databricks","Discord","DoorDash","Goldman Sachs","Google","Hashicorp","IBM","Instacart","LinkedIn","Lyft","Meta","Microsoft","MongoDB","NVIDIA","Netflix","Oracle","PayPal","Plaid","Robinhood","Salesforce","Scale Ai","Slack","Snap","Snowflake","Square","Stripe","Tesla","Twitter","Two Sigma","Zoom"],"stats":{"total":42,"beginner":11,"intermediate":15,"advanced":16,"newThisWeek":42}}