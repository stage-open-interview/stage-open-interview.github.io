{"questions":[{"id":"q-1001","question":"You're building a beginner-friendly AWS-only pipeline to ingest customer chat transcripts (text files up to 50 KB) uploaded to S3. Design how to automatically redact PII using Amazon Comprehend PII detection, store the redacted transcript back to S3, and index metadata in DynamoDB. Include data flow, IAM permissions, error handling, and privacy considerations?","answer":"Two-bucket flow: uploads go to `transcript-uploads`; a Lambda triggered by S3 reads text, runs `Comprehend.detectPiiEntities` to identify PII, replaces with `[REDACTED]`, writes redacted text to `tran","explanation":"## Why This Is Asked\nTests ability to design a secure, cost-conscious AWS-native pipeline that handles PII responsibly using services like S3, Lambda, Comprehend, and DynamoDB, with proper error handling and auditing.\n\n## Key Concepts\n- PII detection: Amazon Comprehend PII entities\n- Data flow: S3 -> Lambda -> S3 (redacted) -> DynamoDB\n- Privacy controls: encryption at rest (SSE/KMS), data minimization\n- Reliability: dead-letter queues (DLQ), retries, idempotent processing, logging\n\n## Code Example\n```javascript\nconst AWS = require('aws-sdk');\nconst s3 = new AWS.S3();\nconst comprehend = new AWS.Comprehend({region: 'us-east-1'});\nasync function redact(text) {\n  const res = await comprehend.detectPiiEntities({ Text: text, LanguageCode: 'en' }).promise();\n  let redacted = text;\n  // naive approach: replace ranges from end to start to avoid offset shifts\n  const ranges = res.Entities.map(e => ({ s: e.BeginOffset, e: e.EndOffset }));\n  ranges.sort((a,b) => b.s - a.s);\n  for (const r of ranges) {\n    redacted = redacted.substring(0, r.s) + '[REDACTED]' + redacted.substring(r.e);\n  }\n  return redacted;\n}\n```\n\n## Follow-up Questions\n- How would you validate redaction accuracy and handle false positives/negatives? \n- How would you adapt this for higher throughput or multilingual transcripts?","diagram":null,"difficulty":"beginner","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Snowflake","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T18:46:28.747Z","createdAt":"2026-01-12T18:46:28.747Z"},{"id":"q-1058","question":"Design a cross region, multi account AI inference platform for real time pricing and risk scoring in a fintech setting. Ingest streaming data, enforce per tenant data residency, and meet sub 100 ms latency. Describe data flow, services, IAM boundaries, model registry, feature store, drift monitoring, error handling, and cost controls?","answer":"Use a multi region, multi account pattern: stream data via Kinesis to region local Lambda preprocessors, push features to SageMaker Feature Store, and serve models with regional SageMaker endpoints be","explanation":"## Why This Is Asked\nExplores a candidate's ability to design scalable, compliant AI infra across AWS accounts and regions, ensuring tenancy isolation, latency targets, and governance.\n\n## Key Concepts\n- Multi account governance and cross region dataflow\n- SageMaker Feature Store and versioned model registry\n- PrivateLink, per tenant IAM, and API Gateway routing\n- Drift monitoring, error handling, and cost controls\n- Data residency via SCPs and audit trails via CloudTrail/Config\n\n## Code Example\n```javascript\n// Pseudo high level manifest of dataflow and services\nconst flow = [\n  'Kinesis -> Lambda preprocess',\n  'Feature Store write',\n  'Regional SageMaker endoints -> Tenant API',\n  'Step Functions orchestration',\n  'CloudTrail + Config for audit'\n];\n```\n\n## Follow-up Questions\n- How would you implement feature drift detection across regions?\n- What are the security implications of cross account model sharing and how would you mitigate them?","diagram":null,"difficulty":"advanced","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","NVIDIA","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T21:19:24.738Z","createdAt":"2026-01-12T21:19:24.738Z"},{"id":"q-1259","question":"Design an end-to-end AWS-native real-time fraud detection pipeline for a global e-commerce platform. Ingest event streams (Kinesis Data Streams), redact PII, create real-time features stored in SageMaker Feature Store (online) and offline store, with governance, lineage, access control, and cost constraints. Include data flow, IAM, retry logic, backpressure, testing, and incident response?","answer":"Propose an AWS-native real-time fraud pipeline: ingest events with Kinesis Data Streams, redact PII in-stream (Lambda or Kinesis Data Analytics) and publish redacted records to SageMaker Feature Store","explanation":"## Why This Is Asked\n\nTests real-time data flow and governance across streaming, feature store, and model scoring, plus privacy requirements.\n\n## Key Concepts\n\n- Kinesis Data Streams\n- SageMaker Feature Store (online/offline)\n- PII redaction in streaming\n- Data lineage and governance (Glue Data Catalog)\n- IAM, KMS, encryption at rest/in transit\n- Backpressure, retries, circuit breakers\n\n## Code Example\n\n```python\nimport boto3\n\ndef redact_pii(record):\n    # placeholder: call to Comprehend or regex\n    return record  # simplified\n```\n\n## Follow-up Questions\n\n- How would you test data drift in the feature store over time?\n- How would you handle schema evolution for features?\n","diagram":null,"difficulty":"advanced","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Cloudflare","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T07:24:57.938Z","createdAt":"2026-01-13T07:24:57.938Z"},{"id":"q-1292","question":"Design a real-time fraud-detection pipeline on AWS for a FinTech use-case. Ingest streaming transactions via Kinesis Data Streams, preprocess with Lambda, and invoke a SageMaker endpoint for real-time scores. Persist results to DynamoDB with audit logs in S3. Address latency (<200 ms), data privacy (KMS, VPC endpoints), IAM, drift monitoring, error handling, and cost control. Provide concrete components and trade-offs?","answer":"Flow: Ingest streaming transactions via Kinesis Data Streams; preprocess in Lambda; invoke a SageMaker endpoint for real-time fraud scores; persist results to DynamoDB and archive logs to S3. Security","explanation":"## Why This Is Asked\nTests real-world AWS AI deployment decisions: low latency, security, and governance in a streaming inference path.\n\n## Key Concepts\n- Streaming ingestion (Kinesis), serverless preprocessing (Lambda), model hosting (SageMaker), persistent storage (DynamoDB, S3).\n- Privacy: KMS, VPC endpoints, IAM least-privilege.\n- Observability: CloudWatch metrics, drift monitoring, retry/backoff.\n\n## Code Example\n```javascript\n// pseudo: Lambda handler invoked by Kinesis; calls SageMaker endpoint and writes to DynamoDB\n```\n\n## Follow-up Questions\n- How would you implement per-customer data isolation in this flow?\n- What failure modes require DLQ routing and circuit breakers?","diagram":"flowchart TD\n  A[Kinesis Data Streams] -->|Preprocess via Lambda| B[Lambda Preprocessing]\n  B -->|SageMaker Inference| C[SageMaker Endpoint]\n  C -->|Store in| D[DynamoDB]\n  C -->|Archive logs to| E[S3 Logs]","difficulty":"intermediate","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Coinbase"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T08:37:56.616Z","createdAt":"2026-01-13T08:37:56.616Z"},{"id":"q-1313","question":"You're designing a beginner-friendly AWS-only pipeline for user-submitted PDFs (max 5 MB) uploaded to S3. Build an end-to-end workflow that uses Textract to extract text, runs a lightweight topic model via Comprehend or a tiny SageMaker endpoint to derive a topic label, stores a compact summary and a searchable index in DynamoDB, and exposes a read path via API Gateway + Lambda. Include data flow, IAM roles, error handling, and privacy considerations?","answer":"Set up an event-driven pipeline: S3 upload to incoming-pdfs/, Lambda triggers Textract for text extraction, and then sends the text to Comprehend (or a small SageMaker model) for topic labeling. Persi","explanation":"## Why This Is Asked\nTests multi-service wiring (Textract, Comprehend/SageMaker, DynamoDB, API Gateway) and basic privacy, cost, and error handling.\n\n## Key Concepts\n- AWS Textract for PDF text extraction\n- Comprehend or SageMaker for simple topic modeling\n- DynamoDB for metadata index\n- API Gateway + Lambda for reads\n- IAM least privilege, KMS, retry, DLQ\n\n## Code Example\n```python\n# Lambda pseudo-handler sketch\ndef handler(event, context):\n  # parse S3 event, call Textract, then Comprehend/SageMaker, store DynamoDB item\n  return {\"status\": \"ok\"}\n```\n\n## Follow-up Questions\n- How would you handle large PDFs exceeding 5 MB?\n- How would you test data privacy controls in this flow?","diagram":"flowchart TD\n  A[S3 Upload] --> B[Lambda Trigger]\n  B --> C[Textract]\n  C --> D[Topic Label (Comprehend/SageMaker)]\n  D --> E[DynamoDB (summary/index)]\n  E --> F[API Gateway / Lambda read path]","difficulty":"beginner","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T10:38:19.119Z","createdAt":"2026-01-13T10:38:19.119Z"},{"id":"q-1366","question":"You're operating a real-time text classifier API on a SageMaker hosting endpoint behind API Gateway. You need a canary deployment strategy using CodeDeploy for SageMaker endpoints, with 2% of traffic to the new version, ramping to 50% over 2 hours, then full if no regressions. Describe data flow, required services, IAM, drift detection via Model Monitor, and rollback/failover logic, plus privacy considerations?","answer":"Use a canary deployment with CodeDeploy for SageMaker endpoints: two production variants, 2% traffic to v2, then gradual ramp to 50% over 2 hours. Monitor drift with Model Monitor and live metrics; re","explanation":"## Why This Is Asked\nThis question probes real-world deployment discipline: canary rollouts, automated rollback, and monitoring for ML endpoints in production.\n\n## Key Concepts\n- Canary deployments with CodeDeploy for SageMaker endpoints\n- EndpointVariant traffic shifting and health checks\n- Model Monitor drift detection and performance metrics\n- Automated rollback, alarms, and observability\n- IAM least-privilege and privacy considerations\n\n## Code Example\n```yaml\n# AppSpec for SageMaker canary deployment (illustrative)\nversion: 0.0\nResources:\n  SageMakerEndpoint:\n    Type: AWS::SageMaker::Endpoint\n    Properties:\n      EndpointConfigName: MyEndpointConfig\n```\n\n## Follow-up Questions\n- How would you simulate drift in CI/CD?\n- How would you extend to multi-region endpoints and cross-account monitoring?\n","diagram":"flowchart TD\n  A[Client Request] --> B(API Gateway)\n  B --> C[SageMaker Endpoint: v1 Production]\n  B --> D[SageMaker Endpoint: v2 Canary]\n  D --> E[Model Monitor & CloudWatch]\n  E --> F{Drift/Performance OK?}\n  F -- Yes --> G[CodeDeploy traffic shift]\n  F -- No --> H[Rollback to v1]\n  G --> I[Promote to Production when ready]","difficulty":"intermediate","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["NVIDIA","Netflix","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T13:19:25.324Z","createdAt":"2026-01-13T13:19:25.324Z"},{"id":"q-1437","question":"You're building a beginner-friendly, AWS‑only pipeline to process user-submitted emails stored as JSON in S3 (max 20 KB per file). Design an end-to-end workflow that detects language, translates to English, analyzes sentiment, and stores results in DynamoDB, with a simple read path via API Gateway + Lambda. Include data flow, services, error handling, IAM roles, and privacy considerations?","answer":"Propose an AWS-only pipeline: S3 triggers Lambda to enqueue processing; Step Functions coordinates Translate, Comprehend (DetectDominantLanguage and DetectSentiment) and a DynamoDB write; expose a rea","explanation":"## Why This Is Asked\n\nThis asks for a practical, beginner-friendly AWS AI/ML workflow that combines text detection, translation, sentiment, storage, and a lightweight read API, while emphasizing security and cost.\n\n## Key Concepts\n\n- DetectDominantLanguage (Comprehend)\n- TranslateText (Translate)\n- DetectSentiment (Comprehend)\n- S3 → Lambda → Step Functions orchestration\n- DynamoDB indexing by documentId\n- API Gateway + Lambda for reads\n- IAM least privilege and KMS\n- Privacy: PII masking, encryption, and retention\n\n## Code Example\n\n```javascript\nimport { ComprehendClient, DetectDominantLanguageCommand, DetectSentimentCommand } from \"@aws-sdk/client-comprehend\";\nimport { TranslateClient, TranslateTextCommand } from \"@aws-sdk/client-translate\";\n\nconst comprehend = new ComprehendClient({region:\"us-east-1\"});\nconst translate = new TranslateClient({region:\"us-east-1\"});\n\nasync function analyzeText(text){\n  const dl = await comprehend.send(new DetectDominantLanguageCommand({Text: text}));\n  const langCode = (dl.Languages?.[0]?.LanguageCode) ?? \"en\";\n  let english = text;\n  if (langCode !== \"en\"){\n    const tr = await translate.send(new TranslateTextCommand({Text: text, SourceLanguageCode: langCode, TargetLanguageCode: \"en\"}));\n    english = tr.TranslatedText;\n  }\n  const sentiment = await comprehend.send(new DetectSentimentCommand({Text: english, LanguageCode: \"en\"}));\n  return { language: langCode, translated: english, sentiment: sentiment.Sentiment };\n}\n``` \n\n## Follow-up Questions\n\n- How would you test this with mocks and local development?\n- What are the implications for cost at scale and how would you mitigate?\n- How would you handle a batch of emails arriving concurrently?","diagram":null,"difficulty":"beginner","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","Robinhood","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T16:58:46.574Z","createdAt":"2026-01-13T16:58:46.574Z"},{"id":"q-1460","question":"Design an advanced, low-latency fraud-detection pipeline for real-time payments in AWS. Ingest events via Kinesis Data Streams, derive features into SageMaker Feature Store, train/deploy with SageMaker Pipelines, score via a real-time SageMaker Endpoint called from Lambda, and log audits in CloudTrail. Enforce KMS encryption, data minimization/retention policies, IAM least privilege, and a rollback/failover plan. Include data flow, IAM, privacy, and failure handling?","answer":"Ingest payment events via Kinesis Data Streams with enhanced fan-out consumers; derive and store features in SageMaker Feature Store with automatic feature validation; train models using SageMaker Pipelines with automated drift detection and retraining triggers; deploy to real-time SageMaker Endpoint with auto-scaling and A/B testing capabilities; score transactions via Lambda that validates input schema and enforces rate limiting; store results in DynamoDB with TTL for data minimization; audit all actions in CloudTrail with CloudWatch Logs for compliance; encrypt all data at rest and in transit using KMS customer-managed keys; implement IAM least privilege with service-specific policies; enable multi-AZ deployment with automatic failover and blue-green rollback.","explanation":"## Why This Is Asked\nTests comprehensive real-time ML deployment, feature engineering, security compliance, and operational resilience in regulated fintech environments.\n\n## Key Concepts\n- Kinesis Data Streams with enhanced fan-out, SageMaker Feature Store with feature validation\n- SageMaker Pipelines with automated drift detection, real-time endpoints with auto-scaling\n- Lambda integration with schema validation, DynamoDB with TTL for data minimization\n- CloudTrail auditing, KMS customer-managed keys, IAM least privilege with service controls\n- Multi-AZ deployment, blue-green deployments, automated rollback procedures\n\n## Code Example\n```python\nimport boto3, json, os\nfrom botocore.exceptions import ClientError\n\nsm = boto3.client('sagemaker-runtime')\nddb = boto3.client('dynamodb')\ncloudtrail = boto3.client('cloudtrail')\n\ndef lambda_handler(event, context):\n    # Validate input schema\n    required_fields = ['transaction_id', 'amount', 'timestamp', 'user_id']\n    if not all(field in event for field in required_fields):\n        raise ValueError('Missing required fields')\n    \n    # Rate limiting check\n    if event['amount'] > float(os.environ.get('MAX_AMOUNT', 10000)):\n        return {'status': 'rejected', 'reason': 'amount_exceeded'}\n    \n    # Real-time scoring\n    try:\n        payload = json.dumps({k: v for k, v in event.items() if k in required_fields})\n        resp = sm.invoke_endpoint(\n            EndpointName=os.environ['ENDPOINT_NAME'],\n            ContentType='application/json',\n            Body=payload\n        )\n        score = json.loads(resp['Body'].read().decode())\n        \n        # Store result with TTL\n        ddb.put_item(\n            TableName='fraud-scores',\n            Item={\n                'transaction_id': {'S': event['transaction_id']},\n                'score': {'N': str(score['probability'])},\n                'timestamp': {'N': str(event['timestamp'])},\n                'ttl': {'N': str(int(time.time()) + 86400*30)}  # 30-day retention\n            }\n        )\n        \n        return {'status': 'scored', 'score': score['probability']}\n    except ClientError as e:\n        cloudtrail.put_event_selectors(\n            TrailName='fraud-audit-trail',\n            EventSelectors=[{'SourceName': 'lambda.amazonaws.com'}]\n        )\n        return {'status': 'error', 'message': str(e)}\n```\n\n## Follow-up Questions\n- How would you implement feature drift detection and automated retraining?\n- How to design canary deployments for model updates?\n- What monitoring metrics would you set up for operational health?\n- How would you handle GDPR right-to-be-forgotten requests?","diagram":"flowchart TD\n  A[Ingest events] --> B[Kinesis Data Streams]\n  B --> C[SageMaker Feature Store (online)]\n  C --> D[SageMaker Pipelines (train & deploy)]\n  D --> E[SageMaker Real-time Endpoint]\n  E --> F[Lambda Scoring]\n  F --> G[CloudWatch + SageMaker Model Monitor]\n  G --> H[CloudTrail Auditing]","difficulty":"advanced","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":["kinesis data streams","sagemaker feature store","sagemaker pipelines","real-time endpoint","lambda integration","dynamodb ttl","cloudtrail auditing","kms encryption","iam least privilege","multi-az deployment","blue-green rollback","enhanced fan-out"],"voiceSuitable":true,"isNew":true,"lastUpdated":"2026-01-15T04:52:37.096Z","createdAt":"2026-01-13T17:56:45.234Z"},{"id":"q-1482","question":"Design a production-ready, AWS-native content moderation pipeline for a multi-tenant SaaS platform handling images and captions uploaded to S3. Must route per-tenant policies to a custom SageMaker multi-model endpoint, apply per-tenant threshold overrides, enforce data isolation, log audit trails, and keep costs predictable with auto-scaling and caching. Describe data flow, IAM, encryption, error handling, and privacy implications?","answer":"Use a two-branch flow: images to a SageMaker multi-model endpoint and captions to a Lambda-accelerated text classifier; tenant policies in DynamoDB drive per-tenant thresholds and endpoint routing via","explanation":"## Why This Is Asked\nAssess ability to design a scalable, multi-tenant AI moderation pipeline on AWS with policy-driven routing, data isolation, and compliant audit logging.\n\n## Key Concepts\n- SageMaker multi-model endpoints and per-tenant routing\n- DynamoDB for policy storage; S3 for audit logs and artifacts\n- Tenant isolation via prefixes, IAM scoping, KMS encryption\n- Step Functions for orchestration, error handling, retries\n- Cost controls: autoscaling, model cache, off-peak scheduling\n\n## Code Example\n```javascript\n// Pseudo-routing logic for tenant-aware inference\nfunction routeToEndpoint(tenantId, itemType) {\n  const policy = fetchPolicy(tenantId)\n  if (itemType === 'image') return { endpoint: policy.imageEndpoint, threshold: policy.imageThreshold }\n  return { endpoint: policy.textEndpoint, threshold: policy.textThreshold }\n}\n```\n\n## Follow-up Questions\n- How would you test tenant policy overrides without affecting others?\n- How would you enforce data deletion and retention across S3 and DynamoDB?","diagram":"flowchart TD\n  A[Upload to S3 (tenantId)] --> B[EventBridge/SFN]\n  B --> C{Content Type}\n  C --> D[SageMaker multi-model endpoint]\n  C --> E[Text classifier]\n  D & E --> F[Apply per-tenant policy]\n  F --> G[Store result in DynamoDB]\n  G --> H[Audit log in S3]","difficulty":"advanced","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Cloudflare","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T18:54:50.921Z","createdAt":"2026-01-13T18:54:50.921Z"},{"id":"q-1552","question":"You're deploying a privacy-preserving AI rating model for financial support tickets on AWS. Incoming tickets arrive as JSON (up to 10 KB) in S3; design an end-to-end pipeline that uses SageMaker for inference, writes per-ticket audit logs to DynamoDB, encrypts data at rest with SSE-KMS, and supports reliable, rate-limited processing with error handling and drift monitoring. How would you implement this?","answer":"Implement S3 event triggers to invoke Lambda functions that call SageMaker endpoints for inference; maintain comprehensive audit logs in DynamoDB with ticket_id, input hash, model_version, latency, and outcome; enforce SSE-KMS encryption across S3 and DynamoDB; incorporate Dead Letter Queues for reliable error handling with exponential backoff and rate limiting.","explanation":"## Why This Is Asked\nThis question evaluates expertise in designing end-to-end privacy-preserving ML workflows on AWS, with emphasis on security implementation (SSE-KMS), system reliability (DLQs, exponential backoff), governance compliance (audit logging), and cost optimization (autoscaling controls).\n\n## Key Concepts\n- S3 event-driven data ingestion and Lambda orchestration\n- SageMaker endpoints for real-time ML inference\n- DynamoDB for comprehensive audit trail with unique identifiers\n- SSE-KMS encryption implementation across all AWS services\n- Robust error handling with DLQs, backoff strategies, and rate limiting","diagram":"flowchart TD\n  A[Tickets arrive in S3] --> B[S3 Event -> Lambda]\n  B --> C[SageMaker Inference]\n  C --> D[DynamoDB Audit]\n  D --> E[Model Monitor/Alarms]","difficulty":"intermediate","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["PayPal","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T06:29:47.696Z","createdAt":"2026-01-13T21:40:06.374Z"},{"id":"q-1583","question":"You're running a real-time fraud-detection model for payments. Data arrives in a Kinesis stream; design an end-to-end AWS-native pipeline that performs per-record inference, monitors drift and bias with SageMaker Clarify/Model Monitor, archives data in S3 with metadata, and auto-triggers a rollback to a previous model version via Step Functions if drift thresholds are exceeded. Include data flow, IAM, encryption, backpressure handling, and privacy considerations?","answer":"Per-record inference via a SageMaker endpoint fed by Kinesis streams, a Lambda transform layer to normalize features, Model Monitor to detect data and label drift, and Clarify for bias auditing; archive raw and processed data in S3 with SSE-KMS encryption, store metadata in DynamoDB, implement Step Functions for automated rollback with exponential backoff, handle backpressure through Kinesis enhanced fan-out and Lambda provisioned concurrency, and ensure privacy through data masking and PII detection.","explanation":"## Why This Is Asked\nAssessment of end-to-end ML pipelines on AWS, with drift detection, governance, and safe rollback.\n\n## Key Concepts\n- Real-time inference integration (Kinesis + SageMaker Endpoint)\n- Drift and bias monitoring (Model Monitor, Clarify)\n- Data archiving and metadata (S3 with SSE-KMS, DynamoDB)\n- Automated rollback (Step Functions) and retry/backoff\n\n## Code Example\n```javascript\n// Example: Lambda to normalize features before SageMaker inference\nconst AWS = require('aws-sdk');\nconst sage = new AWS.SageMakerRuntime();\nexports.handler = async (event) => {\n  // transform feature\n```","diagram":"flowchart TD\n  Kinesis[Data Stream] --> Lambda[Preprocess]\n  Lambda --> Sage[Inference Endpoint]\n  Sage --> DynamoDB[Prediction Logs]\n  Sage --> S3[Data Archive]\n  SageMonitor[SageMaker Model Monitor] --> Drift[Drift Alert]\n  Drift --> SF[Step Functions]\n  SF --> Rollback[Rollback to previous model]\n  Rollback --> Sage[Endpoint Redeploy]","difficulty":"intermediate","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Anthropic","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T05:57:38.931Z","createdAt":"2026-01-13T22:50:38.700Z"},{"id":"q-1627","question":"You're building an AWS-native ML platform that serves a global analytics product with data residing in Snowflake and Databricks across regions. Design a pipeline that ingests from both sources, uses SageMaker Feature Store, trains/registers with a human approval, serves real-time inference, and uses HashiCorp Vault for secrets. Include data lineage, IAM, encryption, drift monitoring, rollback policy, and privacy considerations?","answer":"Design a cross-region AWS pipeline that ingests batch data from Snowflake and Databricks into S3, feeds features to SageMaker Feature Store, trains and registers models with an approval step, deploys ","explanation":"## Why This Is Asked\nThis question tests cross-cloud data plumbing, security, and production-grade ML governance in a realistic, multi-region setup.\n\n## Key Concepts\n- Cross-region ingestion from Snowflake and Databricks\n- SageMaker Feature Store and Model Registry with human approvals\n- HashiCorp Vault integration and cross-account IAM\n- Data lineage, encryption, privacy controls, and auditability\n- Drift monitoring and automated rollback\n\n## Code Example\n```python\n# Pseudo: orchestrate fetch from sources, store features, train, register, and deploy with approval\n```\n\n## Follow-up Questions\n- How would you validate data provenance across Snowflake/Databricks?\n- How would you test rollback and ensure reproducibility?","diagram":"flowchart TD\n  S[Snowflake] --> D1[(Raw Data S3)]\n  D1 --> F[SageMaker Feature Store]\n  Databricks[Databricks] --> D1\n  F --> T[Training Job]\n  T --> R[Model Registry]\n  R --> E[Endpoint]\n  Vault(HashiCorp Vault) --> Secrets[Secrets Management]\n  Drift[Drift Monitor] --> Roll[Rollback Trigger]","difficulty":"advanced","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Hashicorp","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T04:14:53.187Z","createdAt":"2026-01-14T04:14:53.187Z"},{"id":"q-1750","question":"You're building a beginner-friendly AWS-only pipeline for multilingual customer recordings. Audio files (up to 90 seconds) are uploaded to S3. Design an event-driven flow that uses Amazon Transcribe to produce a transcript in the source language, Amazon Translate to produce a Spanish version, stores both transcripts in S3, and writes a compact index in DynamoDB with fields like customerId, fileKey, sourceLang, translatedKey. Include data flow, IAM, error handling, and privacy considerations?","answer":"Design an event-driven AWS pipeline: S3 upload triggers a workflow (Step Functions or Lambda) that starts Transcribe for the source transcript, calls Translate to generate a Spanish version, writes bo","explanation":"## Why This Is Asked\n\nAssesses practical use of AWS AI services (Transcribe, Translate) in a beginner-friendly, cost-conscious pipeline with data governance and privacy considerations.\n\n## Key Concepts\n\n- Amazon Transcribe for speech-to-text\n- Amazon Translate for multilingual text\n- S3 event-driven processing\n- DynamoDB indexing for quick lookups\n- IAM permissions, encryption, and retry strategies\n\n## Code Example\n\n```javascript\n// Pseudo Lambda orchestrating Transcribe and Translate\nexports.handler = async (event) => {\n  const fileKey = event.Records[0].s3.object.key;\n  // 1) start Transcribe job on the audio\n  // 2) fetch transcript, call Translate to Spanish\n  // 3) upload both transcripts to S3\n  // 4) write DynamoDB index with metadata\n  // 5) handle errors and retries\n};\n```\n\n## Follow-up Questions\n\n- How would you handle long-running transcripts and batching?\n- What changes for multi-language translation beyond Spanish?\n- How would you enforce data privacy (encryption, access controls, vendor logs)?","diagram":"flowchart TD\n  A[S3 Upload] --> B[Event Trigger]\n  B --> C[Transcribe Job]\n  C --> D[Transcript Text]\n  D --> E[Translate to Spanish]\n  E --> F[Translate Text]\n  C --> G[Original Transcript S3]\n  F --> H[Translated Transcript S3]\n  G --> I[DynamoDB Index]\n  H --> I\n","difficulty":"beginner","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T09:39:05.334Z","createdAt":"2026-01-14T09:39:05.334Z"},{"id":"q-1789","question":"Design an AWS-native, real-time fraud-detection pipeline: ingest streaming transactions from Kinesis Data Streams, score risk with a SageMaker Inference endpoint (Transformer-based), store scores in DynamoDB, and trigger a Step Functions workflow for high-risk events. Include data flow, IAM, privacy controls, drift monitoring, and cost governance?","answer":"Real-time fraud-detection pipeline using Kinesis -> SageMaker Inference endpoint -> DynamoDB table with transactionId and score, plus a Step Functions workflow for investigations when score exceeds a ","explanation":"## Why This Is Asked\nThis question evaluates building a low-latency, compliant fraud-detection pipeline with streaming data, model hosting, and integration patterns.\n\n## Key Concepts\n- Real-time ingestion with Kinesis Data Streams\n- SageMaker Inference endpoints (transformer-based models)\n- DynamoDB data modeling and TTL\n- Step Functions orchestration and alerts\n- IAM, KMS, and privacy controls\n- Model drift monitoring (SageMaker Model Monitor)\n- Cost governance and autoscaling\n\n## Code Example\n```javascript\nconst AWS = require('aws-sdk');\nconst sagemaker = new AWS.SageMakerRuntime();\nasync function score(event){\n  const resp = await sagemaker.invokeEndpoint({EndpointName:'FraudDetector', Body: JSON.stringify(event), ContentType:'application/json'}).promise();\n  return JSON.parse(resp.Body.toString());\n}\n```\n\n## Follow-up Questions\n- How would you implement canary deployments and drift monitoring for the endpoint?\n- How would you validate data retention and privacy controls across regions?","diagram":null,"difficulty":"advanced","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Goldman Sachs"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T10:50:22.735Z","createdAt":"2026-01-14T10:50:22.735Z"},{"id":"q-1800","question":"You're building a compliant multi-tenant, multilingual content moderation service on AWS. Users upload video to S3; extract audio, transcribe with Amazon Transcribe, run a SageMaker classifier to flag policy-violating content, redact PII, and store transcripts and decisions, indexing per-tenant metadata in DynamoDB. Design an end-to-end, event-driven pipeline with data isolation, data sovereignty, error handling, and privacy controls?","answer":"Adopt a tenant-scoped data plane (per-tenant S3 prefixes, Lake Formation isolation, KMS keys). EventBridge triggers Step Functions on video upload: extract audio, Transcribe (source language), SageMak","explanation":"## Why This Is Asked\nTests ability to design multi-tenant, compliant AI pipelines in AWS, balancing isolation, privacy, and operational resilience.\n\n## Key Concepts\n- Tenant isolation with Lake Formation and per-tenant KMS keys\n- Event-driven orchestration via EventBridge and Step Functions\n- AWS AI: Transcribe, SageMaker moderation, and PII redaction\n- Data residency and regional replication\n\n## Code Example\n```javascript\n// Pseudo-steps: on S3 PUT -> extract audio -> Transcribe -> SageMaker -> redact -> store -> index\n``` \n\n## Follow-up Questions\n- How would you test data residency constraints across regions?\n- How would you handle tenant policy changes at runtime?","diagram":"flowchart TD\n  A[Video Upload to S3 (Tenant)] --> B[EventBridge]\n  B --> C[Step Functions]\n  C --> D[Transcribe]\n  D --> E[SageMaker Moderation]\n  E --> F[PII Redaction]\n  F --> G[S3: Transcript + Decisions]\n  G --> H[DynamoDB: Tenant Index]\n  H --> I[Regional Replication]","difficulty":"intermediate","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T11:35:55.371Z","createdAt":"2026-01-14T11:35:55.371Z"},{"id":"q-1831","question":"You're building a beginner AWS-only pipeline to process user-uploaded audio stories (up to 120 seconds). When an audio file lands in S3, design an event-driven flow that uses Amazon Transcribe to produce a transcript, Amazon Comprehend to extract keywords, and a SageMaker endpoint to classify genre. Save transcript and metadata to S3 and index in DynamoDB. Include data flow, IAM permissions, error handling, and privacy considerations?","answer":"Orchestrate with Step Functions: on S3 upload (≤120s) run Transcribe to text, Comprehend for keywords, and a SageMaker endpoint to assign a genre. Save transcript to transcripts/{key}.txt and metadata","explanation":"## Why This Is Asked\n- Tests end-to-end orchestration across AI services with Step Functions.\n- Emphasizes data governance, encryption, and privacy in an entry-level pipeline.\n- Assesses DynamoDB schema design and S3 layout for artifacts.\n\n## Key Concepts\n- Event-driven workflows, IAM least privilege, cross-service data handoff\n- Privacy by design: encryption, access controls, retention\n- Simple model hosting via SageMaker endpoint\n\n## Code Example\n```javascript\nconst stateMachine = {\n  Comment: 'Audio genre labeling',\n  StartAt: 'Transcribe',\n  States: {\n    Transcribe: { Type: 'Task', Resource: 'arn:aws:transcribe:...', Next: 'Comprehend' },\n    Comprehend: { Type: 'Task', Resource: 'arn:aws:comprehend:...', Next: 'SageMaker' },\n    SageMaker: { Type: 'Task', Resource: 'arn:aws:sagemaker:...', Next: 'Store' },\n    Store: { Type: 'Pass', Result: 'done', End: true }\n  }\n}\n```\n\n## Follow-up Questions\n- How would you add retry/backoff and DLQ for failed steps?\n- How would multi-language transcripts affect the flow and data model?","diagram":"flowchart TD\n  A[S3 Upload] --> B[Step Functions]\n  B --> C[Transcribe]\n  B --> D[Comprehend]\n  B --> E[SageMaker Genre]\n  C --> F[S3 transcripts/]\n  E --> G[S3 outputs/genre.json]\n  D --> H[S3 outputs/keywords.json]\n  B --> I[DynamoDB AudioIndex]","difficulty":"beginner","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Anthropic","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T13:13:46.119Z","createdAt":"2026-01-14T13:13:46.119Z"},{"id":"q-1902","question":"You're building a beginner AWS-only pipeline to moderate user-uploaded profile pictures (JPEG/PNG, up to 2MB) in S3; on upload, design an event-driven flow using Rekognition to detect unsafe content and top labels, write a JSON report to S3, and upsert a summary in DynamoDB with fields like userId, objectKey, safeFlag, topLabel. Include data flow, IAM, error handling, and privacy considerations?","answer":"Trigger a Lambda from the S3 PUT event, call Rekognition DetectLabels and SafeSearch, store a small JSON report in S3 with labelScore and safety verdict, then upsert a summary item in DynamoDB with us","explanation":"## Why This Is Asked\nTests ability to design a simple yet complete event-driven pipeline using Rekognition, S3, Lambda, and DynamoDB, while accounting for privacy and failure handling.\n\n## Key Concepts\n- S3 event triggers and Lambda integration\n- Rekognition DetectLabels and SafeSearch usage\n- Writing reports to S3 and upserting DynamoDB records\n- IAM least-privilege roles and policy scoping\n- Error handling with DLQ and idempotency, privacy controls\n\n## Code Example\n```javascript\nconst AWS = require('aws-sdk');\nconst rekognition = new AWS.Rekognition();\nconst s3 = new AWS.S3();\nconst dynamodb = new AWS.DynamoDB.DocumentClient();\n\nexports.handler = async (event) => {\n  const bucket = event.Records[0].s3.bucket.name;\n  const key = decodeURIComponent(event.Records[0].s3.object.key.replace(/\\+/g, ' '));\n  const image = { Image: { S3Object: { Bucket: bucket, Name: key } } };\n  const resp = await rekognition.detectLabels({ ...image, MaxLabels: 5 }).promise();\n  const labels = resp.Labels.map(l => l.Name);\n  const safeFlag = resp.Adult?.Value === false && resp.Violence?.Value === false;\n  const report = { objectKey: key, safeFlag, topLabel: labels[0], labels };\n  await s3.putObject({ Bucket: bucket, Key: key + '.report.json', Body: JSON.stringify(report) }).promise();\n  await dynamodb.put({ TableName: 'ProfileModeration', Item: { userId: '<USER_ID>', objectKey: key, safeFlag, topLabel: labels[0] } }).promise();\n  return {};\n}\n```\n\n## Follow-up Questions\n- How would you handle mislabeled content or false positives?\n- How would you scale to millions of uploads with idempotent processing?","diagram":"flowchart TD\n  A[S3 Upload] --> B[Lambda Trigger]\n  B --> C[Rekognition: DetectLabels & SafeSearch]\n  C --> D[Write report.json to S3]\n  D --> E[Update DynamoDB]\n  E --> F[Done]","difficulty":"beginner","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T16:46:44.316Z","createdAt":"2026-01-14T16:46:44.316Z"},{"id":"q-1935","question":"Design a real-time, multi-tenant fraud-detection pipeline for a SaaS platform. Ingestion uses Kinesis Data Streams per tenant; a central SageMaker detector scores each event; PII is redacted before storage in per-tenant S3 buckets; an index is kept in DynamoDB with fields tenantId, sessionId, eventTime, fraudScore. Explain data isolation, cross-account IAM, error handling, privacy controls, and idempotency/replay safety?","answer":"Architect a real-time, multi-tenant fraud-detection pipeline: Kinesis streams per tenant feed a central SageMaker detector; redact PII, write artifacts to tenant-scoped S3, and publish a per-tenant Dy","explanation":"## Why This Is Asked\nTests ability to design scalable, compliant streaming pipelines across accounts, with strict data isolation, privacy, and correctness guarantees.\n\n## Key Concepts\n- Real-time event processing with Kinesis and Lambda/SSM\n- Cross-account IAM roles and least-privilege access\n- Data redaction, encryption at rest (SSE-KMS), and tenant-scoped storage\n- Idempotency, replay safety, and DLQ/error handling\n- Durable auditing and per-tenant indexing in DynamoDB\n\n## Code Example\n```javascript\n// Pseudo: consume Kinesis event, redactPII, invoke SageMaker, write to S3, upsert DynamoDB\n```\n\n## Follow-up Questions\n- How would you implement idempotent processing for duplicate events?\n- What tests ensure privacy requirements are met across tenants?","diagram":"flowchart TD\n  A[Kinesis Streams (per-tenant)] --> B[Processor (Lambda/Step Functions)]\n  B --> C[SageMaker Detector]\n  B --> D[PII Redaction]\n  C --> E[S3 (tenant bucket)]\n  D --> E\n  E --> F[DynamoDB Tenant Index]\n  F --> G[Audit Logs & DLQ]\n","difficulty":"intermediate","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T17:48:24.454Z","createdAt":"2026-01-14T17:48:24.454Z"},{"id":"q-2018","question":"Design an end-to-end, event-driven AWS pipeline for a regulated financial analytics service. Tenants upload encrypted JSON trade logs (up to 5 MB) to S3. Build per-tenant, region-isolated processing: decrypt with KMS, run a SageMaker multi-tenant analytics model, redact PII with Comprehend and regex, and store outputs in region-scoped S3 prefixes. Maintain an immutable audit trail in DynamoDB with versioning, implement retry and DLQ, and enforce strict IAM boundaries?","answer":"Propose a tenant-isolated, multi-region pipeline: S3 event -> Step Functions -> Lambda decrypt with KMS -> SageMaker multi-tenant analytics model -> PII redaction via Comprehend + regex -> outputs to ","explanation":"## Why This Is Asked\n\nTests the ability to design a robust, compliant, multi-region data pipeline with strong tenant isolation, privacy controls, and auditable lineage in AWS.\n\n## Key Concepts\n\n- Event-driven orchestration (Step Functions, Lambda, EventBridge)\n- Multi-tenant isolation (region prefixes, IAM scoping)\n- Data privacy (KMS encryption, Comprehend PII redaction)\n- Auditability (immutable DynamoDB with versioning, Streams for lineage)\n- Resilience (retries, DLQ, monitoring)\n\n## Code Example\n\n```python\n# Pseudo: decrypt with KMS, route to SageMaker\nimport boto3\nkms=boto3.client('kms')\n# decrypt logic placeholder\n```\n\n## Follow-up Questions\n\n- How would you test tenant isolation across regions?\n- How would you validate audit log integrity and versioning?","diagram":"flowchart TD\n  A[S3 Upload] --> B[Step Functions Start]\n  B --> C[Lambda: Decrypt (KMS)]\n  C --> D[SageMaker Analytics (Tenant)]\n  D --> E[PII Redaction (Comprehend/Regex)]\n  E --> F[Region S3 Prefix Output]\n  E --> G[Immutable Audit Trail (DynamoDB)]\n  G --> H[Monitoring & Retries]\n\n  A --> B\n  B --> C\n  C --> D\n  D --> E\n  E --> F\n  E --> G\n  G --> H","difficulty":"intermediate","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Discord","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T20:54:52.927Z","createdAt":"2026-01-14T20:54:52.927Z"},{"id":"q-2045","question":"Design a real-time, AWS-only PII redaction pipeline for streaming chat messages. Ingest messages enter regional Kinesis Data Streams with tenant isolation. Use SageMaker endpoint (or Comprehend) to detect PII, redact spans, store redacted messages in S3, and index per-tenant audit logs in DynamoDB (tenantId, messageId, detectedPII, redacted, timestamp). Include data flow, IAM, error handling, privacy controls, and rate-limiting?","answer":"In real-time, use regional Kinesis Data Streams for ingestion with tenant isolation, a Lambda function to invoke either a SageMaker PII detection endpoint or AWS Comprehend to identify sensitive spans, redact those spans inline, push sanitized events to S3 for storage, and upsert per-tenant audit logs in DynamoDB with fields for tenantId, messageId, detectedPII, redacted status, and timestamp. Implement proper IAM roles with least privilege, encryption at rest and in transit, error handling with dead-letter queues, and rate limiting to prevent service abuse.","explanation":"## Why This Is Asked\nAssesses the ability to design compliant, streaming AI workflows with robust privacy controls and multi-tenant architecture.\n\n## Key Concepts\n- Real-time streaming architecture with Kinesis Data Streams\n- PII detection using AWS Comprehend or custom SageMaker endpoints\n- In-flight redaction and audited persistence mechanisms\n- Multi-tenant data isolation with encryption at rest and in transit\n- IAM least privilege principles and security best practices\n- Comprehensive error handling, retry logic, and rate limiting strategies\n\n## Code Example\n```python\n# Lambda handler skeleton invoking SageMaker for PII detection and redaction\nimport json\nimport boto3\n\ndef detect_pii(text):\n    # Invoke SageMaker endpoint or Comprehend for PII detection\n    pass\n\ndef redact_content(text, pii_spans):\n    # Redact identified PII spans from content\n    pass\n\ndef handler(event, context):\n    for record in event['Records']:\n        payload = json.loads(record['kinesis']['data'])\n        pii_spans = detect_pii(payload['message'])\n        redacted_message = redact_content(payload['message'], pii_spans)\n        # Store redacted message and audit log\n        # Handle errors and implement rate limiting\n```","diagram":null,"difficulty":"intermediate","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Lyft","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T04:52:25.049Z","createdAt":"2026-01-14T21:49:22.754Z"},{"id":"q-2075","question":"Design a real-time fraud detection pipeline for a high-volume platform used by Lyft, PayPal, and Snap. Ingest payment events into Kinesis, enrich with per-tenant risk data from DynamoDB via Lambda, compute a fraud score with a SageMaker endpoint, and enforce decisions in DynamoDB while routing high-risk events to a Step Functions human-in-the-loop workflow and notifying security via SNS. Include data isolation, sub-200 ms latency budget, and privacy controls with PII masking and audit trails?","answer":"Design a real-time fraud detection pipeline: ingest payment events into Kinesis, enrich with per-tenant risk data from DynamoDB via Lambda, compute fraud scores using a SageMaker endpoint, enforce decisions in DynamoDB while routing high-risk events to a Step Functions human-in-the-loop workflow, and notify security teams via SNS. Implement data isolation through tenant-specific DynamoDB tables, maintain sub-200ms latency through optimized Lambda functions and provisioned SageMaker endpoints, and ensure privacy compliance with PII masking, encryption at rest, and comprehensive audit trails.","explanation":"## Why This Is Asked\nTests ability to design a low-latency, multitenant fraud pipeline using AWS AI services, with clear data flow, fault tolerance, and privacy controls.\n\n## Key Concepts\n- Event-driven architectures across Kinesis, Lambda, and Step Functions\n- Real-time scoring with SageMaker endpoints\n- Per-tenant data isolation in DynamoDB\n- Privacy: PII masking, encryption at rest, audit trails\n\n## Code Example\n\n```javascript\n// Lambda enrichment skeleton\nconst AWS = require('aws-sdk');\nconst dynamo = new AWS.DynamoDB.DocumentClient();\n\nexports.handler = async (event) => {\n  // parse event\n```","diagram":"flowchart TD\n  A[Payment Event] --> B[Kinesis Stream]\n  B --> C[Lambda Enrich]\n  C --> D[SageMaker FraudScore]\n  D --> E{Threshold?}\n  E -- Yes --> F[Block/Flag in DynamoDB; SNS]\n  E -- No --> G[Proceed downstream]\n  F --> H[Audit Log & PII Masking]","difficulty":"advanced","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","PayPal","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T05:33:11.408Z","createdAt":"2026-01-14T23:27:14.074Z"},{"id":"q-2175","question":"You're building a real-time, AWS-native, multi-tenant text inference service for content safety at scale. Incoming messages arrive with tenant context and must be isolated either via per-tenant endpoints or tenant-scoped routing. Describe architecture for throughput, deployment strategy (canary/rolling), privacy controls (encryption, PII redaction), drift monitoring, data retention, and cost governance. Include data flow, IAM, and observability?","answer":"Describe an AWS-native, multi-tenant, real-time text inference pipeline for content safety. Messages include tenant context and must be isolated either via per-tenant endpoints or tenant-scoped routin","explanation":"## Why This Is Asked\nAssesses design of scalable, secure, multi-tenant ML inference on AWS with real-time requirements, isolation guarantees, privacy controls, and cost governance.\n\n## Key Concepts\n- Tenancy isolation strategies (per-tenant vs shared endpoints with routing)\n- Real-time inference flow (API Gateway, Kinesis/EventBridge, SageMaker)\n- Privacy controls (KMS, encryption at rest/in transit, PII redaction)\n- Deployment discipline (canary, rolling updates, model monitoring)\n- Observability and cost governance\n\n## Code Example\n```javascript\n// Pseudo-CDK snippet: tenant-aware SageMaker endpoint and routing placeholder\n```\n\n## Follow-up Questions\n- How would you verify tenant isolation under burst traffic?\n- What tests validate drift detection and rollback safety?","diagram":"flowchart TD\n  A[Client Message] --> B[API Gateway]\n  B --> C[Kinesis Data Stream]\n  C --> D[SageMaker Inference Endpoint (tenant-aware)]\n  D --> E[PII Redaction / Labeling]\n  E --> F[S3: raw + redacted]\n  F --> G[DynamoDB: Tenant Index]\n  G --> H[CloudWatch / OpenTelemetry]\n","difficulty":"advanced","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","Meta","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T06:45:09.337Z","createdAt":"2026-01-15T06:45:09.339Z"},{"id":"q-2208","question":"You're building a privacy-preserving, multi-tenant receipt processor on AWS. Clients upload receipts (PDF/JPG up to 5MB) to S3. Design an event-driven flow: Textract extracts fields, a SageMaker endpoint classifies line items, redact PII, and write structured data to DynamoDB with tenant isolation. Include data flow, IAM, SSE-KMS, retries, drift monitoring, audit logs, privacy controls, and testing strategy?","answer":"Design a Step Functions workflow triggered by S3 events: Textract OCR to extract date, total, vendor; SageMaker to classify items; a Lambda for PII redaction; store structured data in DynamoDB with te","explanation":"## Why This Is Asked\nTests end-to-end design of a real-world, privacy-conscious data pipeline with multi-tenant isolation and production-ready concerns.\n\n## Key Concepts\n- Event-driven orchestration (S3, Step Functions, Lambda)\n- Textract for extraction, SageMaker for classification\n- PII redaction and data minimization\n- Tenant isolation in DynamoDB and per-tenant encryption (SSE-KMS)\n- Retries, DLQ, drift monitoring, auditing, privacy controls\n\n## Code Example\n```javascript\n// Pseudocode: State machine skeleton\n{\n  \"Comment\": \"Receipt workflow\",\n  \"StartAt\": \"Textract\",\n  \"States\": {\n    /* ... */\n  }\n}\n```\n\n## Follow-up Questions\n- How would you test per-tenant data isolation and drift detection?\n- What metrics and alarms would you configure for reliability and privacy governance?","diagram":"flowchart TD\n  A[S3 Receipt Upload] --> B[Trigger Step Functions]\n  B --> C[Textract OCR]\n  C --> D[SageMaker Classifier]\n  D --> E[PII Redaction]\n  E --> F[DynamoDB Store]\n  F --> G[Audit Logs (S3/CloudWatch)]","difficulty":"intermediate","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","DoorDash","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T07:37:40.086Z","createdAt":"2026-01-15T07:37:40.086Z"},{"id":"q-2387","question":"You're processing PDFs uploaded to S3 (up to 5 MB). Design an event-driven AWS pipeline that uses Textract to extract text and tables, Comprehend to identify keywords, stores outputs in S3, and writes a compact index to DynamoDB with fields like documentId, s3Key, textSnippet, phrases. Include IAM, error handling, and privacy considerations?","answer":"Trigger on S3 upload of a PDF (<=5 MB). A Lambda orchestrates: Textract AnalyzeDocument to extract text/tables, store plain text in S3, then Comprehend KeyPhrases to build a keyword list. Upsert a Dyn","explanation":"## Why This Is Asked\nAssesses understanding of event-driven AWS AI services integration, data flow, and privacy.\n\n## Key Concepts\n- S3 event triggers\n- Textract AnalyzeDocument\n- Comprehend KeyPhrases\n- DynamoDB indexing\n- Privacy: encryption, PII redaction\n\n## Code Example\n```javascript\nconst AWS = require('aws-sdk');\nconst textract = new AWS.Textract();\n// skeleton: extract, store, and index\n```\n\n## Follow-up Questions\n- How would you test the pipeline end-to-end?\n- How would you handle large PDFs or sensitive data? ","diagram":"flowchart TD\n  S3Input[/PDF uploaded to S3/] --> Lambda[Lambda: Orchestrator]\n  Lambda --> Textract[Textract: AnalyzeDocument]\n  Textract --> TextStore[Store Text in S3]\n  Lambda --> Comprehend[Comprehend: KeyPhrases]\n  Comprehend --> DB[DynamoDB: docIndex]\n  Lambda --> DLQ[SQS DLQ]\n  Textract --> DLQ\n  Comprehend --> DLQ","difficulty":"beginner","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","MongoDB","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T15:51:12.405Z","createdAt":"2026-01-15T15:51:12.406Z"},{"id":"q-2445","question":"You're designing a real-time, multi-tenant chat moderation pipeline on AWS. Ingest messages from multiple tenants via Kinesis Data Streams, score content with a versioned SageMaker endpoint, redact PII, and persist per-tenant results to DynamoDB with TTL, while archiving raw and redacted messages to S3. Design end-to-end architecture with latency target ~150 ms, strong data isolation, model versioning, encryption, and cost controls; include IAM roles, VPC endpoints, and monitoring?","answer":"Architect a streaming, multi-tenant moderation flow: Kinesis ingests messages, SageMaker endpoint scores content (versioned models), PII redaction occurs before storage, DynamoDB stores per-tenant res","explanation":"## Why This Is Asked\nExplores multi-tenant data isolation, real-time latency, and policy governance in production-grade AI pipelines beyond prior questions.\n\n## Key Concepts\n- Real-time streaming with Kinesis Data Streams\n- SageMaker model versioning and endpoints\n- Per-tenant isolation in DynamoDB with TTL and encryption\n- PII redaction before archival in S3\n- IAM, KMS, VPC endpoints, and cost-monitoring\n\n## Code Example\n```javascript\nconst AWS = require('aws-sdk');\nconst sage = new AWS.SageMakerRuntime();\nasync function score(msg) {\n  const res = await sage.invokeEndpoint({\n    EndpointName: 'moderation-v1',\n    ContentType: 'text/plain',\n    Body: msg\n  }).promise();\n  return res.Body.toString();\n}\n```\n\n## Follow-up Questions\n- How would you test tenant isolation and data leakage risks?\n- Describe a canary deployment strategy for new model versions in SageMaker.\n- What monitoring dashboards and SLOs would you implement for latency and accuracy?\n- How would you handle model drift and feature changes across tenants?","diagram":"flowchart TD\n  A[Kinesis Streams] --> B[SageMaker Endpoint]\n  B --> C[DynamoDB (per-tenant)]\n  A --> D[S3 Raw/Redacted Archive]\n  C --> E[Audit & Compliance]\n  D --> E","difficulty":"advanced","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T18:50:57.992Z","createdAt":"2026-01-15T18:50:57.992Z"},{"id":"q-2733","question":"You're building a beginner AWS-only pipeline for customer support call recordings uploaded to S3 as WAV/MP3 (≤2 minutes). Design an event-driven flow using Amazon Transcribe (auto language), translate non-English transcripts to English, generate an SRT subtitle, redact PII in transcripts, store both outputs in S3, and upsert a DynamoDB index with callId, transcriptKey, subtitleKey, language, and sentiment. Include IAM, error handling, privacy, and cost considerations?","answer":"Orchestrate with Step Functions: on S3 upload, start Transcribe with auto language, detect language; if not English, translate to English; generate an SRT from the transcript; redact PII in transcript","explanation":"## Why This Is Asked\nTests ability to design an end-to-end AWS workflow for multilingual audio with transcription, translation, subtitle generation, PII redaction, and indexing while considering privacy and cost.\n\n## Key Concepts\n- Event-driven orchestration with Step Functions and EventBridge\n- Amazon Transcribe for auto language detection\n- Amazon Translate for non-English to English translation\n- PII redaction via Amazon Comprehend\n- SRT generation and storage in S3; DynamoDB indexing\n- IAM least privilege, SSE-KMS, DLQ, and retention policies\n\n## Code Example\n```javascript\n// Simplified Step Functions skeleton (conceptual)\n{\n  \"Comment\": \"Call processing flow\",\n  \"StartAt\": \"Transcribe\",\n  \"States\": {\n    \"Transcribe\": {\"Type\": \"Task\", \"Resource\": \"arn:aws:states:::transcribe:startTranscriptionJob\"},\n    \"TranslateIfNeeded\": {\"Type\": \"Choice\", \"Choices\": [{\"Variable\": \"$.LanguageCode\", \"StringEquals\": \"en\"}]},\n    \"Translate\": {\"Type\": \"Task\", \"Resource\": \"arn:aws:states:::translate:translateText\"},\n    \"Redact\": {\"Type\": \"Task\", \"Resource\": \"arn:aws:states:::comprehend:detectPII\"},\n    \"SRTAndStore\": {\"Type\": \"Task\", \"Resource\": \"arn:aws:states:::s3:putObject\"},\n    \"UpdateIndex\": {\"Type\": \"Task\", \"Resource\": \"arn:aws:states:::dynamodb:putItem\"}\n  }\n}\n```\n\n## Follow-up Questions\n- How would you test idempotency and handle retriable failures?\n- What would you monitor for cost and latency, and which metrics matter most?","diagram":"flowchart TD\n  A[Audio Upload to S3] --> B[Transcribe (auto language)]\n  B --> C{Language == English?}\n  C -->|Yes| D[Generate Transcript] --> F[Redact PII] --> G[Create SRT] --> H[Store Outputs in S3] --> I[Update DynamoDB]\n  C -->|No| E[Translate to English] --> F\n  F --> G\n  style A fill:#f9f,stroke:#333,stroke-width:2px\n  style I fill:#bbf,stroke:#333,stroke-width:2px","difficulty":"beginner","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["NVIDIA","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T09:44:40.560Z","createdAt":"2026-01-16T09:44:40.560Z"},{"id":"q-2754","question":"You're building a beginner AWS-only event-driven pipeline to process user-uploaded handwritten receipts (JPEG/PNG, ≤4MB) stored in S3. Design the flow using Amazon Textract to extract text and key-value data, store outputs in S3, and upsert a DynamoDB index with receiptId, userId, totalAmount, currency, and receiptDate. Include IAM roles, error handling, privacy considerations, and cost controls?","answer":"Event-driven: S3 PUT triggers a Lambda; Lambda invokes Textract AnalyzeDocument for handwriting; save Textract JSON to S3; parse totalAmount/date; upsert DynamoDB with receiptId, userId, totalAmount, ","explanation":"## Why This Is Asked\nTests practical serverless design using Textract for handwritten data, including extraction, indexing, and privacy.\n\n## Key Concepts\n- Textract handwriting support\n- Event-driven Lambda with S3 triggers\n- DynamoDB upsert for indexing\n- IAM least privilege and KMS\n- Privacy and cost considerations\n\n## Code Example\n```javascript\n// Example parsing of Textract response to extract a total amount\nfunction extractTotal(textract) {\n  const blocks = (textract?.Blocks || []).filter(b => b.BlockType === 'LINE');\n  const totalLine = blocks.find(b => /total|amount/i.test(b.Text || ''));\n  const amount = totalLine ? parseFloat(totalLine.Text.replace(/[^0-9.]/g, '')) : null;\n  return amount;\n}\n```\n\n## Follow-up Questions\n- How would you handle multi-page receipts?\n- How would you add unit tests for the Lambda parsing logic?","diagram":"flowchart TD\n  A[Upload to S3] --> B[Lambda Trigger on Put]\n  B --> C[Textract AnalyzeDocument]\n  C --> D[S3 Textract Output]\n  C --> E[DynamoDB Upsert for receiptIndex]","difficulty":"beginner","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Google","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T10:41:57.927Z","createdAt":"2026-01-16T10:41:57.927Z"},{"id":"q-2770","question":"You're building an AWS-based data-lake intake for user-submitted datasets (CSV/Parquet, up to 500MB) uploaded to S3. Design an event-driven flow: validate schema and basic quality, trigger a SageMaker Processing job to compute metadata (columns, data types, missing values, sample stats), store outputs in S3, and upsert a DynamoDB index with datasetId, s3Key, schemaHash, qualityScore, ownerId. Include IAM, error handling, privacy controls, and cost considerations?","answer":"Event flow: S3 Put triggers Lambda to validate schema and basic quality (Glue Schema Registry or lightweight validator). It then starts a SageMaker Processing job to compute metadata (columns, dtypes,","explanation":"## Why This Is Asked\nTests ability to design data-ingestion pipelines that combine validation, feature extraction, storage, and metadata indexing; emphasizes privacy, cost, and error handling; introduces multi-service orchestration beyond the typical Transcribe/Translate tasks; aligns with real-world data-lake governance.\n\n## Key Concepts\n- Event-driven orchestration (S3, Lambda, Step Functions)\n- Data validation and schema management (Glue schema registry)\n- SageMaker Processing for metadata extraction\n- DynamoDB indexing with structured fields\n- Security and privacy: IAM roles, KMS encryption, encryption at rest, data residency\n- Error handling and observability: DLQ, CloudWatch, alarms\n- Cost considerations: batch processing, data retention rules, lifecycle\n\n## Code Example\n```javascript\n// Lambda handler sketch: trigger SageMaker processing after schema check\nexports.handler = async (event) => {\n  const dataset = parse(event);\n  if (!valid(dataset)) throw new Error(\"Invalid\");\n  await startSageMakerProcessing(dataset);\n  return { status: 'started' };\n}\n```\n\n## Follow-up Questions\n- How would you handle schema evolution and backward compatibility?\n- How would you implement retry/backoff policy and idempotency?\n- How would you test this pipeline locally?","diagram":"flowchart TD\n  A[S3 Upload] --> B[Lambda Validator]\n  B --> C[SageMaker Processing]\n  C --> D[S3 Outputs]\n  D --> E[DynamoDB Upsert]","difficulty":"intermediate","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Google","IBM"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T11:28:55.169Z","createdAt":"2026-01-16T11:28:55.169Z"},{"id":"q-2846","question":"You're deploying a real-time text moderation model on AWS that flags policy-violating comments. Design an end-to-end AWS-native pipeline that ingests messages, runs inference on a SageMaker endpoint, uses SageMaker Clarify for per-request attributions, stores input, predictions, and explanations in S3, upserts a DynamoDB audit index (requestId, userId, modelVersion, latency, score, topFeatures), redacts PII before storage, enforces least-privilege IAM, and triggers retraining via SageMaker Pipelines on drift or performance degradation while balancing cost and latency?","answer":"Design an end-to-end AWS-native pipeline for real-time text moderation with explainability and privacy. Ingest from API Gateway to a streaming service; inference on SageMaker; generate per-request att","explanation":"## Why This Is Asked\nTests real-time inference, explainability, privacy controls, drift-aware retraining, and cost discipline across AWS services.\n\n## Key Concepts\n- SageMaker endpoints for inference\n- SageMaker Clarify explainability\n- SageMaker Pipelines for retraining\n- S3 for raw/input/output and explanations\n- DynamoDB audit indexing\n- IAM least privilege and encryption\n- Data redaction for PII\n- Drift detection and cost-aware scaling\n\n## Code Example\n```python\nimport re\n\ndef redact(text):\n  text = re.sub(r'\\b[\\w.-]+@[\\w.-]+\\.[A-Za-z]{2,}\\b', '[REDACTED_EMAIL]', text)\n  text = re.sub(r'\\b(?:\\+?\\d{1,3}[-.\\s]?)?\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}\\b', '[REDACTED_PHONE]', text)\n  return text\n```\n\n## Follow-up Questions\n- How would you validate explainability outputs for regulatory compliance?\n- How would you test end-to-end latency under peak load?","diagram":"flowchart TD\n  Ingest Messages --> Inference[SageMaker Endpoint]\n  Inference --> Explain[SageMaker Clarify]\n  Explain --> Store[S3: Input, Output, Attributions]\n  Store --> Audit[DynamoDB Audit Index]\n  Audit --> Retrain[SageMaker Pipelines: Drift & Retraining]","difficulty":"advanced","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T14:42:14.265Z","createdAt":"2026-01-16T14:42:14.265Z"},{"id":"q-3068","question":"You're processing multilingual chatbot logs uploaded as JSON to S3 (≤1 MB). Design an end-to-end AWS AI/ML pipeline that triggers on upload to: 1) detect language with Comprehend, 2) run a SageMaker endpoint for sentiment and abusive content, 3) translate non-English content to English, 4) redact PII, 5) write a combined NDJSON to a data lake and upsert a DynamoDB index with conversationId, translatedKey, language, sentiment, abuseFlag. Include error handling, idempotent retries, data privacy, and cost considerations; outline monitoring with CloudWatch/X-Ray and EventBridge?","answer":"Trigger a Step Functions workflow on S3 PUT. Use Comprehend to detect language, SageMaker endpoint for sentiment/abuse scoring, Translate to English if needed, Lambda to redact PII, then write NDJSON to the data lake and upsert results to DynamoDB with proper error handling, idempotent retries, and CloudWatch monitoring.","explanation":"## Why This Is Asked\n\nTests orchestration, edge cases, and cost/privacy trade-offs for multi-service AWS AI pipelines.\n\n## Key Concepts\n\n- Event-driven orchestration (S3 → Step Functions)\n- Language detection, sentiment/abuse scoring, translation\n- PII redaction and data privacy controls\n- Idempotency, retries, and error handling\n- Observability (CloudWatch, X-Ray, EventBridge) and cost governance\n\n## Code Example\n\n```json\n{\n  \"Comment\": \"Chatlog ML pipeline\",\n  \"StartAt\": \"DetectLanguage\",\n  \"States\": {\n    \"DetectLanguage\": {\n      \"Type\": \"Task\",\n      \"Resource\": \"arn:aws:states:::comprehend:batchDetectDominantLanguage\",\n      \"Parameters\": {\n        \"TextList.$\": \"$$.Map.Item.Value.messages[*].content\"\n      },\n      \"ResultPath\": \"$.languageDetection\",\n      \"Next\": \"SentimentAnalysis\"\n    },\n    \"SentimentAnalysis\": {\n      \"Type\": \"Task\",\n      \"Resource\": \"arn:aws:states:::sagemaker:invokeEndpoint\",\n      \"Parameters\": {\n        \"EndpointName\": \"chatbot-sentiment-abuse-endpoint\",\n        \"Body.$\": \"$.languageDetection.ResultList[*]\"\n      },\n      \"ResultPath\": \"$.sentimentAbuse\",\n      \"Next\": \"TranslationCheck\"\n    },\n    \"TranslationCheck\": {\n      \"Type\": \"Choice\",\n      \"Choices\": [{\n        \"Variable\": \"$.languageDetection.ResultList[0].DominantLanguage.Code\",\n        \"StringEquals\": \"en\",\n        \"Next\": \"PIIRedaction\"\n      }],\n      \"Default\": \"Translation\"\n    },\n    \"Translation\": {\n      \"Type\": \"Task\",\n      \"Resource\": \"arn:aws:states:::translate:translateText\",\n      \"Parameters\": {\n        \"Text.$\": \"$.languageDetection.ResultList[*].Text\",\n        \"SourceLanguageCode.$\": \"$.languageDetection.ResultList[0].DominantLanguage.Code\",\n        \"TargetLanguageCode\": \"en\"\n      },\n      \"ResultPath\": \"$.translation\",\n      \"Next\": \"PIIRedaction\"\n    },\n    \"PIIRedaction\": {\n      \"Type\": \"Task\",\n      \"Resource\": \"arn:aws:states:::lambda:invoke\",\n      \"Parameters\": {\n        \"FunctionName\": \"redact-pii-function\",\n        \"Payload\": {\n          \"text.$\": \"$.translation.TranslatedText ?? $$.Map.Item.Value.messages[*].content\",\n          \"piiTypes\": [\"EMAIL\", \"PHONE\", \"NAME\", \"CREDIT_CARD\"]\n        }\n      },\n      \"ResultPath\": \"$.redacted\",\n      \"Next\": \"PersistResults\"\n    },\n    \"PersistResults\": {\n      \"Type\": \"Parallel\",\n      \"Branches\": [\n        {\n          \"StartAt\": \"WriteToDataLake\",\n          \"States\": {\n            \"WriteToDataLake\": {\n              \"Type\": \"Task\",\n              \"Resource\": \"arn:aws:states:::lambda:invoke\",\n              \"Parameters\": {\n                \"FunctionName\": \"write-ndjson-function\",\n                \"Payload\": {\n                  \"conversationId.$\": \"$$.Map.Item.Value.conversationId\",\n                  \"originalContent.$\": \"$$.Map.Item.Value.messages[*].content\",\n                  \"translatedContent.$\": \"$.translation.TranslatedText\",\n                  \"language.$\": \"$.languageDetection.ResultList[0].DominantLanguage.Code\",\n                  \"sentiment.$\": \"$.sentimentAbuse.Body.sentiment\",\n                  \"abuseFlag.$\": \"$.sentimentAbuse.Body.abuse_score > 0.7\",\n                  \"redactedContent.$\": \"$.redacted.Payload.redactedText\"\n                }\n              },\n              \"End\": true\n            }\n          }\n        },\n        {\n          \"StartAt\": \"UpdateDynamoDB\",\n          \"States\": {\n            \"UpdateDynamoDB\": {\n              \"Type\": \"Task\",\n              \"Resource\": \"arn:aws:states:::dynamodb:updateItem\",\n              \"Parameters\": {\n                \"TableName\": \"chatbot-conversations-index\",\n                \"Key\": {\n                  \"conversationId\": {\"S.$\": \"$$.Map.Item.Value.conversationId\"}\n                },\n                \"UpdateExpression\": \"SET translatedKey = :tk, language = :lang, sentiment = :sent, abuseFlag = :abuse, lastProcessed = :ts\",\n                \"ExpressionAttributeValues\": {\n                  \":tk\": {\"S.$\": \"$.translation.TranslatedText\"},\n                  \":lang\": {\"S.$\": \"$.languageDetection.ResultList[0].DominantLanguage.Code\"},\n                  \":sent\": {\"S.$\": \"$.sentimentAbuse.Body.sentiment\"},\n                  \":abuse\": {\"BOOL.$\": \"$.sentimentAbuse.Body.abuse_score > 0.7\"},\n                  \":ts\": {\"S.$\": \"$$.State.EnteredTime\"}\n                }\n              },\n              \"End\": true\n            }\n          }\n        }\n      ],\n      \"Next\": \"Success\"\n    },\n    \"Success\": {\n      \"Type\": \"Succeed\"\n    }\n  }\n}\n```\n\n## Error Handling & Retries\n\n```json\n{\n  \"Catch\": [\n    {\n      \"ErrorEquals\": [\"States.TaskFailed\"],\n      \"IntervalSeconds\": 5,\n      \"MaxAttempts\": 3,\n      \"BackoffRate\": 2.0,\n      \"Next\": \"ErrorFallback\"\n    }\n  ]\n}\n```\n\n## Cost & Privacy Controls\n\n- **Batch processing** for Comprehend/Translate to reduce API calls\n- **SageMaker serverless** endpoints for cost optimization\n- **VPC endpoints** for private connectivity\n- **KMS encryption** for S3 and DynamoDB\n- **Data retention policies** and lifecycle management\n\n## Monitoring\n\n- **CloudWatch Metrics**: Success/failure rates, latency, token usage\n- **X-Ray**: End-to-end tracing across services\n- **EventBridge**: Failed execution notifications and auto-retries\n- **Custom Dashboards**: Cost monitoring per conversation and service","diagram":"flowchart TD\n  A[S3 Upload] --> B[Step Functions]\n  B --> C[Detect Language: Comprehend]\n  C --> D[SageMaker: Sentiment/Abuse]\n  D --> E[Translate if needed]\n  E --> F[PII Redaction]\n  F --> G[NDJSON + DynamoDB Upsert]\n","difficulty":"intermediate","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Instacart","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T05:15:52.869Z","createdAt":"2026-01-16T23:37:19.460Z"},{"id":"q-3127","question":"You're building a beginner AWS-only pipeline for multilingual customer support chat transcripts uploaded as JSON to S3 (≤1MB per file). Design an event-driven flow that triggers on upload, translates non-English content to English using Amazon Translate, analyzes sentiment and named entities with Amazon Comprehend, stores both original and translated JSON in S3, and upserts an index in DynamoDB with fields like chatId, originalLang, translatedKey, sentiment, entities. Include IAM, error handling, privacy, and cost considerations?","answer":"Trigger on S3 PUT (≤1MB). Use Lambda/Step Functions: read JSON; if language != en, translate to English; run Comprehend sentiment and entity detection on both versions; store both JSONs in S3; upsert ","explanation":"## Why This Is Asked\n\nTests ability to design a simple, observable data flow that combines text translation and NLP analysis, with an awareness of privacy and cost.\n\n## Key Concepts\n\n- Event-driven architectures in AWS (S3 events, Lambda/Step Functions)\n- Conditional translation and NLP (Translate, Comprehend)\n- Data provenance and indexing (S3 keys, DynamoDB)\n- Security and privacy (KMS, encryption, IAM least privilege)\n\n## Code Example\n\n```javascript\n// pseudo snippet showing conditional translate call and DynamoDB upsert\n```\n\n## Follow-up Questions\n\n- How would you handle rate limits or retries for Translate and Comprehend?\n- How would you scale this to thousands of concurrent files while keeping costs predictable?","diagram":"flowchart TD\n  A[S3 Upload] --> B[Event Trigger]\n  B --> C[Read JSON]\n  C --> D[Translate (if needed)]\n  C --> E[Comprehend on original]\n  D --> F[Comprehend on translated]\n  F --> G[Store outputs in S3]\n  G --> H[DynamoDB index upsert]","difficulty":"beginner","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T04:09:12.043Z","createdAt":"2026-01-17T04:09:12.043Z"},{"id":"q-3138","question":"You're collecting IoT sensor CSV files uploaded to S3 (deviceId, timestamp, temp, humidity). Design an event-driven AWS pipeline that validates CSV schema, computes per-device daily aggregates (minTemp, maxTemp, avgTemp), stores raw and processed outputs in S3, and upserts a DynamoDB index with deviceId, date, minTemp, maxTemp, avgTemp, and status. Include IAM permissions, error handling, and data retention?","answer":"An S3 upload triggers a Lambda to validate CSV schema (deviceId, timestamp, temp, humidity) and compute per-device daily aggregates (min/max/avg temp). Save raw to s3/raw/{deviceId}/{date}.csv and a s","explanation":"## Why This Is Asked\nThis question tests a beginner-friendly, end-to-end design: event-driven flow, simple data validation, aggregation logic, and robust error handling with durable storage and minimal IAM permissions.\n\n## Key Concepts\n- Event-driven AWS workflows with S3 and Lambda\n- CSV parsing, schema validation, and per-key aggregation\n- DynamoDB upsert patterns and conditional writes\n- IAM least privilege and data-retention policies\n\n## Code Example\n```javascript\n// Lambda skeleton: parse CSV, validate header, aggregate by device/date, write outputs, update DynamoDB\nexports.handler = async (event) => {\n  // placeholder for parsing S3 event, streaming CSV, computing aggregates\n};\n```\n\n## Follow-up Questions\n- How would you test end-to-end processing and failure paths?\n- How would you scale for high-throughput streams and ensure idempotent replays?","diagram":null,"difficulty":"beginner","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Hashicorp","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T04:40:14.370Z","createdAt":"2026-01-17T04:40:14.372Z"},{"id":"q-3191","question":"You're building a real-time, event-driven AWS pipeline to ingest live webinar audio streams that arrive as 30-second chunks uploaded to S3. Design an end-to-end flow that uses Amazon Transcribe for streaming transcription with speaker diarization, Amazon Comprehend to compute per-speaker sentiment and key entities, and stores transcripts, per-speaker metrics, and a compact index in DynamoDB. Include privacy controls (PII redaction), data retention, IAM roles, error handling, idempotency, and cost considerations?","answer":"Use a Step Functions workflow: trigger on S3 new chunk; start a Transcribe streaming job with diarization; store the transcript in S3; run Comprehend on the transcript to produce per-speaker sentiment","explanation":"## Why This Is Asked\nThe scenario probes real-world event-driven design, streaming AI services, privacy, and cost controls in AWS.\n\n## Key Concepts\n- Transcribe streaming with diarization and chunking\n- Comprehend per-speaker sentiment and entity extraction\n- PII redaction in transcripts before long-term storage\n- DynamoDB indexing and idempotent upserts\n- IAM least-privilege, encryption, error handling, and DLQ\n- Cost planning for continuous streaming workloads\n\n## Code Example\n```javascript\n// Pseudo-idempotent S3 event handler for webinar chunk\nconst handle = async (record) => {\n  const id = deriveId(record);\n  if (await existsInDynamo(id)) return;\n  // start Transcribe streaming and process results\n};\n```\n\n## Follow-up Questions\n- How would you scale for thousands of concurrent webinars?\n- How would you observe and debug end-to-end latency issues?","diagram":"flowchart TD\nA[S3: webinar chunk] --> B[Transcribe streaming (diarization)]\nB --> C[Transcript in S3]\nB --> D[Comprehend: per-speaker sentiment/entities]\nC --> E[PII redaction]\nE --> F[DynamoDB index: webinarId, segmentKey, transcriptKey, sentiment]\nF --> G[Audit, retention, and cost metrics]","difficulty":"intermediate","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","IBM"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T06:42:39.223Z","createdAt":"2026-01-17T06:42:39.223Z"},{"id":"q-3225","question":"You're building a beginner AWS-only pipeline to ingest sensor CSV uploads into S3 and produce deduplicated aggregates. Each file <= 500KB with headers: timestamp, deviceId, value. On upload, compute SHA-256, check a DynamoDB dedup table; if new, validate schema, compute per-device hourly averages, write aggregates as JSON to S3 under aggregates/{date}/{fileKey}. Upsert a metadata item in DynamoDB with fileKey, hash, deviceCount, earliestTs, latestTs, aggregateKey. Include IAM, error handling, privacy, and cost considerations?","answer":"Trigger: S3 PUT of a CSV <= 500KB. Lambda computes SHA-256 of the file and checks a DynamoDB dedup table. If new, validate headers (timestamp, deviceId, value), streams rows to aggregate per-device ho","explanation":"## Why This Is Asked\nTests idempotent, event-driven pipelines, dedup, validation, per-device aggregation, and DynamoDB metadata design. Also covers IAM least privilege, error handling, and privacy considerations.\n\n## Key Concepts\n- Event-driven Lambda on S3\n- Content hashing for dedup\n- CSV validation and streaming aggregation\n- DynamoDB for dedup and metadata\n- S3 versioning, IAM policies, error retries, cost awareness\n\n## Code Example\n```javascript\nconst crypto = require('crypto');\nconst hash = crypto.createHash('sha256').update(fileBuffer).digest('hex');\n```\n\n## Follow-up Questions\n- How would you test idempotency and handling of partial failures?\n- What are scaling and cost considerations as file rate grows?","diagram":"flowchart TD\n  A[S3 Upload] --> B[Lambda: Validate & Hash]\n  B --> C{New File?}\n  C -- No --> D[End]\n  C -- Yes --> E[Parse CSV & Aggregate]\n  E --> F[Store Aggregates in S3]\n  F --> G[Upsert Metadata in DynamoDB]\n  G --> H[End]","difficulty":"beginner","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Oracle","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T07:38:05.939Z","createdAt":"2026-01-17T07:38:05.939Z"},{"id":"q-3245","question":"You're building a privacy-preserving, real-time data pipeline for streaming customer events from IoT devices into AWS. Ingest via AWS IoT Core, buffer with Kinesis Streams, run online inference in SageMaker endpoints to flag anomalies, redact PII from event payloads, store raw data in S3 and anomalies index in DynamoDB with fields tenantId, eventId, timestamp, anomalyScore, remediationAction, and enforce per-tenant isolation with IAM roles and KMS encryption. What is your design, including error handling, backpressure, and audit trails?","answer":"Route IoT Core messages per tenant to Kinesis Streams, redact PII in a Lambda, write raw events to S3 (tenant-scoped partitions) and sanitized copies to the stream for processing. SageMaker endpoint e","explanation":"## Why This Is Asked\nTests ability to design a real-time, multi-tenant, privacy-preserving data pipeline on AWS with end-to-end data flow, governance, and observability.\n\n## Key Concepts\n- Real-time streaming: IoT Core -> Kinesis -> SageMaker\n- Privacy: PII redaction and masking\n- Isolation: per-tenant IAM roles and KMS encryption\n- Observability: CloudWatch logs, DLQ, audit trails\n- Reliability: backpressure handling and batch processing\n\n## Code Example\n```python\nimport json, re\n\ndef redact(event):\n    s = json.dumps(event)\n    s = re.sub(r'\"email\"\\s*:\\s*\"[^\"]+\"', '\"email\":\"REDACTED\"', s)\n    s = re.sub(r'\"phone\"\\s*:\\s*\"[0-9\\-() ]+\"', '\"phone\":\"REDACTED\"', s)\n    return json.loads(s)\n```\n\n## Follow-up Questions\n- How would you implement tenant-aware KMS key policies?\n- How would you monitor latency and drift in anomaly scores and roll back if needed?","diagram":"flowchart TD\nA[IoT Core Ingest (per-tenant)] --> B[Kinesis Streams]\nB --> C[Lambda: redact & route]\nC --> D[S3 Raw (tenantId partition)]\nC --> E[SageMaker Endpoint (online inference)]\nE --> F[DynamoDB Anomalies (tenantId, eventId, timestamp, score)]\nE --> G[Alerts/Remediation to 1..N systems]","difficulty":"advanced","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Google","IBM"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T08:41:31.923Z","createdAt":"2026-01-17T08:41:31.923Z"},{"id":"q-3373","question":"You're building a beginner AWS-only pipeline to process user event logs uploaded as newline-delimited JSON (JSONL) to S3 (≤1 MB per file). On upload, design an event-driven flow using Lambda to validate schema, extract userId, classify events with a lightweight AI step, redact PII in the payload, write sanitized events back to S3, and upsert a DynamoDB index with logId, s3Key, eventCategory, timestamp, and userHash. Include IAM, error handling, privacy, and cost considerations?","answer":"On S3 upload, trigger Lambda to validate JSONL against a schema; use a small AI step (Comprehend or a tiny SageMaker endpoint) to classify eventCategory; redact PII via regex (emails, phone numbers, I","explanation":"## Why This Is Asked\nTests ability to design an end-to-end event-driven pipeline using basic AWS AI services, data validation, privacy controls, and cost-conscious patterns at a beginner level.\n\n## Key Concepts\n- Event-driven: S3 -> Lambda -> AI service -> S3 -> DynamoDB\n- Data privacy: PII redaction, hashing identifiers\n- IAM least privilege and cross-service access\n- Observability: DLQ, CloudWatch metrics\n- Cost: batch processing to reduce invocations\n\n## Code Example\n```javascript\nfunction redact(record){\n  return record.payload.replace(/([a-zA-Z0-9._%+-]+@[^\\s,]+)|(\\b\\d{10,}\\b)/g, \"[REDACTED]\");\n}\n```\n\n## Follow-up Questions\n- How would you version the JSONL schema and migrate existing logs?\n- How would you test this pipeline for cold starts and failure modes?","diagram":null,"difficulty":"beginner","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T13:47:42.497Z","createdAt":"2026-01-17T13:47:42.498Z"},{"id":"q-3430","question":"You're building an AWS-native pipeline to process enterprise chat transcripts uploaded as JSONL files to S3 (≤5 MB per file). On upload, design an event-driven flow that uses Amazon Comprehend for entity and sentiment analysis, a SageMaker endpoint for topic modeling to classify conversations, redact PII, write a sanitized transcript to S3, and upsert a DynamoDB index with chatId, transcriptKey, topics, sentiment, and redactedFlag. Include IAM, KMS encryption, privacy controls, and cost considerations?","answer":"Design a serverless, event-driven flow: S3 upload triggers a validation Lambda; a SageMaker endpoint runs topic modeling; Comprehend handles entity and sentiment; redact PII; write sanitized transcrip","explanation":"## Why This Is Asked\nTests end-to-end design of an AWS AI pipeline handling privacy, scaling, and cost. Combines managed AI services (Comprehend, SageMaker) with event-driven wiring (S3, Lambda, DynamoDB).\n\n## Key Concepts\n- Event-driven architecture across S3, Lambda, and SageMaker\n- AI services integration: Comprehend for NER/sentiment, SageMaker for topic modeling\n- Data privacy: PII redaction, encryption at rest/in transit, access controls\n- Cost awareness: batching, cold starts, endpoint usage, and streaming vs. per-file processing\n\n## Code Example\n```javascript\n// Pseudo-code: demonstrate sequence of calls to AWS services\nconst AWS = require('aws-sdk');\nconst comprehend = new AWS.Comprehend();\nconst sage = new AWS.SageMakerRuntime();\nasync function analyze(text){\n  const entities = await comprehend.detectEntities({Text: text, LanguageCode:'en'}).promise();\n  const sentiment = await comprehend.detectSentiment({Text: text, LanguageCode:'en'}).promise();\n  const res = await sage.invokeEndpoint({EndpointName:'topic-model', Body: JSON.stringify({text}), ContentType:'application/json'}).promise();\n  return {entities, sentiment, topics: JSON.parse(res.Body.toString())};\n}\n```\n\n## Follow-up Questions\n- How would you handle partial failures and retries across Lambda, SageMaker, and DynamoDB?\n- What security controls and monitoring would you add for regulatory compliance (e.g., encryption, audit logs, IAM least privilege)?","diagram":null,"difficulty":"advanced","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T15:42:07.946Z","createdAt":"2026-01-17T15:42:07.946Z"},{"id":"q-3460","question":"You're building a beginner AWS-only data ingestion pipeline for ML datasets. CSV files (up to 50 MB) are uploaded to S3. On upload, design an event-driven flow using Lambda to validate headers, detect duplicates by datasetId + timestamp, partition data into a date-based path, trigger a Glue crawler to catalog, and write a manifest JSON to S3 with fields datasetId, fileKey, recordCount, partitionPath, and upsert a DynamoDB index with datasetId, s3Root, lastUpdated. Include IAM, error handling, privacy considerations, and cost?","answer":"On S3 CSV upload (≤50 MB), design an event-driven AWS pipeline: a Lambda validates headers and row count, then checks DynamoDB for duplicates (datasetId + timestamp). If new, partition into a date-bas","explanation":"## Why This Is Asked\n\nTests practical data ingestion and cataloging using AWS services beyond basic compute. Emphasizes idempotency, schema validation, and metadata management for ML datasets.\n\n## Key Concepts\n\n- Event-driven integration (S3 -> Lambda -> DynamoDB/Glue)\n- Data cataloging with Glue Crawlers\n- Idempotent deduplication checks\n- Partitioning for efficient queries and cost control\n- IAM least privilege and error handling patterns\n\n## Code Example\n\n```javascript\nfunction validateHeaders(headers) { /* minimal schema check */ }\n```\n\n## Follow-up Questions\n\n- How would you handle evolving CSV schemas without breaking downstream jobs?\n- How would you scale Glue Crawlers for many small files vs. few large files?","diagram":"flowchart TD\nA[S3: CSV upload] --> B[Lambda: validate headers & row count]\nB --> C{Headers valid}\nC -->|Yes| D[Check DynamoDB for duplicates]\nC -->|No| Z[Error: invalid header]\nD --> E{Duplicate?}\nE -->|No| F[Partition: date-based path]\nE -->|Yes| G[Abort: log duplicate]\nF --> H[Trigger Glue crawler]\nH --> I[DynamoDB: upsert datasetId, lastUpdated]\nI --> J[S3: manifest.json]","difficulty":"beginner","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Instacart","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T17:25:31.080Z","createdAt":"2026-01-17T17:25:31.080Z"},{"id":"q-3538","question":"You're building a multi-tenant AWS AI pipeline for incoming chat logs (JSON, up to 10 KB each) uploaded to S3. Design an event-driven flow (S3 -> Step Functions) that detects language, translates non-English to English, performs sentiment and entity analysis, redacts PII, stores both original and sanitized transcripts in S3, and upserts a DynamoDB index with chatId, originalKey, sanitizedKey, language, sentiment. Include IAM, privacy controls, and data isolation?","answer":"Propose a multi-tenant chat-log pipeline: on S3 upload, trigger Step Functions to DetectLanguage (Comprehend), translate non-English to English (Translate), run sentiment and entity analysis (Comprehe","explanation":"## Why This Is Asked\nTests orchestration of multiple AWS AI services for non-audio data, focusing on privacy, data isolation, and dynamic language handling.\n\n## Key Concepts\n- Multi-tenant isolation\n- Comprehend DetectLanguage and Translate\n- Redaction of PII with regex or NLP\n- DynamoDB indexing of metadata\n- IAM permissions least privilege and robust error handling\n\n## Code Example\n```javascript\n// Placeholder: pseudocode for Step Functions state machine or Lambda calls\n```\n\n## Follow-up Questions\n- How would you handle rate limits and retries for Comprehend and Translate?\n- How would you enforce per-tenant data retention policies?","diagram":"flowchart TD\n  S3[\"S3 Upload\"] --> SF[\"Step Functions\"]\n  SF --> DL[\"Detect Language (Comprehend)\"]\n  DL --> TR[\"Translate to English (Translate)\"]\n  SF --> AN[\"Analyze (Sentiment/Entities)\"]\n  TR --> RED[\"Redact PII\"]\n  AN --> RED\n  RED --> O1[\"Store Original Transcript in S3\"]\n  RED --> O2[\"Store Sanitized Transcript in S3\"]\n  O2 --> DB[\"Upsert DynamoDB Index\"]","difficulty":"intermediate","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","OpenAI","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T20:38:26.632Z","createdAt":"2026-01-17T20:38:26.632Z"},{"id":"q-3733","question":"You’re building an AWS-only, event-driven pipeline for PDFs uploaded by external partners to S3 (up to 20 MB). Design a flow using Textract to extract text and tables, a SageMaker classifier to flag risk clauses, and Comprehend for entity detection. Store outputs in partner-scoped S3, and write a per-partner index to DynamoDB (partnerId, docKey, riskFlag, clauses). Enforce data sovereignty with region replication and customer-managed keys, plus IAM least privilege, error handling, and cost controls?","answer":"Trigger on S3 Put for partner PDFs; Step Functions coordinates Textract (text/tables), a SageMaker classifier for risk clauses, and Comprehend for entities; outputs go to a partner-prefixed S3 locatio","explanation":"## Why This Is Asked\nTests real-world data partnerships and privacy in a cross-service AWS pipeline, covering OCR, NLP classification, and entity extraction in a single flow, plus governance and cost awareness.\n\n## Key Concepts\n- Textract for text and tables extraction\n- SageMaker model integration for risk-flag classification\n- Comprehend for entity detection\n- Step Functions for orchestration and retries\n- Data sovereignty, CMKs, IAM least privilege, cost controls\n\n## Code Example\n```javascript\n// Example AWS CDK sketch (pseudo)\n```\n\n## Follow-up Questions\n- How would you handle partial failures and ensure idempotency?\n- What metrics and alarms would you implement for model drift and cost spikes?","diagram":null,"difficulty":"intermediate","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T07:32:27.103Z","createdAt":"2026-01-18T07:32:27.103Z"},{"id":"q-3800","question":"You're ingesting customer reviews stored as JSONL files in S3 (up to 20 MB per file). Design an advanced, privacy-conscious event-driven AWS pipeline that uses SageMaker endpoints for sentiment and topic modeling, applies PII redaction, stores outputs in S3, and upserts a compact DynamoDB index. Include data governance with Lake Formation, KMS, and a drift-aware retraining trigger?","answer":"Design a serverless pipeline: on S3 JSONL upload, trigger a Step Functions workflow; per record, Lambda validates JSON, calls a SageMaker endpoint for sentiment and topic inference, and a redact Lambd","explanation":"## Why This Is Asked\nTests ability to architect a scalable, privacy-first ML pipeline using primarily AWS serverless services; emphasizes model monitoring, governance, and retraining triggers.\n\n## Key Concepts\n- Event-driven orchestration (S3 events, Lambda, Step Functions)\n- SageMaker inference for NLP (sentiment, topics)\n- PII redaction and data masking\n- Compact indexing in DynamoDB\n- Data governance with Lake Formation, KMS, IAM least privilege\n- Drift detection and retraining triggers (Glue/Step Functions)\n\n## Code Example\n```javascript\n// Pseudo: invoke SageMaker endpoint for each JSON line\nconst result = await sageMaker.invokeEndpoint({ EndpointName: 'nlp-sentiment-topic', Body: line });\n```\n\n## Follow-up Questions\n- How would you test PII redaction accuracy at scale?\n- How would you handle backpressure from large uploads and ensure idempotency?","diagram":"flowchart TD\n  S3Upload[S3 Upload] --> L0[Lambda: validate]\n  L0 --> SF[Step Functions]\n  SF --> SM[SageMaker: sentiment/topics]\n  SM --> O[Outputs in S3]\n  O --> D[DynamoDB index]","difficulty":"advanced","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T10:29:42.301Z","createdAt":"2026-01-18T10:29:42.303Z"},{"id":"q-3833","question":"You’re building an AWS-native, event-driven pipeline for ingesting training videos uploaded to S3 (≤1 GB). On upload, design a workflow that uses MediaConvert to transcode to multiple bitrates, Transcribe for captions, Rekognition for scene labels and unsafe content, a SageMaker endpoint to summarize transcripts and extract topics, redact PII, write outputs to S3, and upsert a DynamoDB index with videoId, originalKey, captionKey, summaryKey, topics, and safetyFlag. Include IAM, KMS, privacy controls, and cost considerations?","answer":"Design a serverless, event-driven video pipeline: on S3 upload (≤1 GB), trigger MediaConvert for multi-bitrate transcoding, Transcribe for captions, Rekognition for scene labels and unsafe content, Sa","explanation":"## Why This Is Asked\nTests end-to-end orchestration of managed AI services with privacy and cost in mind. It probes design for idempotency, retries, and failure handling across media workflows. It also checks data lineage, encryption, and least-privilege IAM while considering data-residency.\n\n## Key Concepts\n- Event-driven orchestration (S3 events, EventBridge/Step Functions)\n- MediaConvert, Transcribe, Rekognition, SageMaker\n- Redaction, metadata indexing, PII privacy, KMS\n\n## Code Example\n```javascript\n{\n  Comment: Video processing pipeline\n  StartAt: Transcode\n  States: { ... }\n}\n```\n\n## Follow-up Questions\n- How would you ensure idempotent reuploads and deduplication?\n- How would you monitor costs and implement quotas for large video workloads?","diagram":"flowchart TD\n  A[Video Upload] --> B[MediaConvert]\n  B --> C[Transcribe]\n  C --> D[SageMaker Summary]\n  D --> E[Redact & Store]\n  E --> F[DynamoDB Index]","difficulty":"advanced","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","IBM","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T11:28:46.781Z","createdAt":"2026-01-18T11:28:46.781Z"},{"id":"q-3890","question":"You're building a beginner AWS-only pipeline for user-uploaded product photos (JPEG/PNG, ≤3 MB) stored in S3. On upload, design an event-driven flow that uses Amazon Rekognition to generate the top 3 object labels and a simple color palette, writes a JSON descriptor to S3, and upserts a DynamoDB item with productId, imageKey, labels, colors, and privacyFlag. Include IAM roles, error handling with DLQ, and cost considerations?","answer":"Trigger on S3 PUT for photos (JPEG/PNG ≤3 MB). Lambda calls Rekognition detectLabels (top 3) and derives a basic color palette. Build descriptor JSON: imageKey, labels, colors, timestamp, facesDetecte","explanation":"## Why This Is Asked\nTests practical ability to design an end-to-end, event-driven image analytics pipeline in AWS using beginner-friendly services (S3, Lambda, Rekognition, DynamoDB) with real-world concerns like privacy, cost, and tenancy.\n\n## Key Concepts\n- Event-driven flow triggered by S3 PUT\n- Rekognition detectLabels with top N labels\n- Lightweight color palette extraction in Lambda\n- Descriptor JSON stored in S3 and DynamoDB upsert\n- IAM least privilege, DLQ/backoff, cost awareness\n\n## Code Example\n```javascript\n// Node.js Lambda pseudo\nconst rek = new AWS.Rekognition();\nconst s3 = new AWS.S3();\nconst ddb = new AWS.DynamoDB.DocumentClient();\n\nexports.handler = async (evt) => {\n  const key = evt.Records[0].s3.object.key;\n  const bucket = evt.Records[0].s3.bucket.name;\n  const img = await s3.getObject({Bucket:bucket,Key:key}).promise();\n  const labels = await rek.detectLabels({Image:{Bytes: img.Body}, MaxLabels:3, MinConfidence: 60}).promise();\n  const colors = extractColors(img.Body); // simple utility\n  const descriptor = { imageKey:key, labels: labels.Labels.map(l=>l.Name), colors, timestamp: Date.now(), facesDetected: labels.Labels.some(l=>l.Name==='Face') };\n  await s3.putObject({Bucket: bucket, Key: key + '.json', Body: JSON.stringify(descriptor)}).promise();\n  await ddb.update({TableName:'ProductImages',Key:{productId: determineProductId(key)}, UpdateExpression:'SET labels=:l, colors=:c, imageKey=:k, privacyFlag=:p', ExpressionAttributeValues:{':l':descriptor.labels, ':c':descriptor.colors, ':k':key, ':p':false}}).promise();\n}\n```\n\n## Follow-up Questions\n- How would you validate labels and handle false positives? \n- How would you extend for multi-tenant isolation and cost controls?","diagram":"flowchart TD\n A[S3 Put] --> B[Lambda]\n B --> C[Rekognition]\n B --> D[ColorCalc]\n C --> E[Descriptor]\n D --> E\n E --> F[S3 json]\n E --> G[DynamoDB upsert]","difficulty":"beginner","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","NVIDIA","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T13:55:25.036Z","createdAt":"2026-01-18T13:55:25.036Z"},{"id":"q-951","question":"You're building a low‑ops internal voice assistant for support scripts. Audio files up to 60 seconds are uploaded to S3. Design an AWS‑only pipeline that transcribes, analyzes sentiment, and stores a brief summary plus an index in DynamoDB, with minimal cost and ops. Include data flow, services, error handling, and privacy considerations?","answer":"Use an S3 trigger to start a Step Functions workflow: Transcribe for 60s, then Comprehend for sentiment, store transcript + summary in DynamoDB, and index in a separate small table. Add a compact S3 m","explanation":"## Why This Is Asked\n\nTests ability to design a cost‑aware, low‑ops AWS data pipeline that stitches AI services together with serverless orchestration.\n\n## Key Concepts\n\n- AWS services: S3, Step Functions, Transcribe, Comprehend, DynamoDB\n- Serverless orchestration with error handling and retries\n- Data privacy: encryption, access control, least privilege\n\n## Code Example\n\n```javascript\n// Example AWS CDK snippet (TypeScript) configuring a Step Function state machine trigger\n```\n\n## Follow-up Questions\n\n- How would you scale the pipeline for higher concurrency?\n- How would you monitor latency and alert on failures?","diagram":null,"difficulty":"beginner","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T16:38:46.546Z","createdAt":"2026-01-12T16:38:46.546Z"}],"subChannels":["general"],"companies":["Adobe","Airbnb","Amazon","Anthropic","Apple","Bloomberg","Citadel","Cloudflare","Coinbase","Databricks","Discord","DoorDash","Goldman Sachs","Google","Hashicorp","Hugging Face","IBM","Instacart","LinkedIn","Lyft","Meta","MongoDB","NVIDIA","Netflix","OpenAI","Oracle","PayPal","Plaid","Robinhood","Salesforce","Scale Ai","Slack","Snap","Snowflake","Square","Stripe","Tesla","Twitter","Uber","Zoom"],"stats":{"total":44,"beginner":16,"intermediate":14,"advanced":14,"newThisWeek":44}}