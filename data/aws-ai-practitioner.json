{"questions":[{"id":"q-1001","question":"You're building a beginner-friendly AWS-only pipeline to ingest customer chat transcripts (text files up to 50 KB) uploaded to S3. Design how to automatically redact PII using Amazon Comprehend PII detection, store the redacted transcript back to S3, and index metadata in DynamoDB. Include data flow, IAM permissions, error handling, and privacy considerations?","answer":"Two-bucket flow: uploads go to `transcript-uploads`; a Lambda triggered by S3 reads text, runs `Comprehend.detectPiiEntities` to identify PII, replaces with `[REDACTED]`, writes redacted text to `tran","explanation":"## Why This Is Asked\nTests ability to design a secure, cost-conscious AWS-native pipeline that handles PII responsibly using services like S3, Lambda, Comprehend, and DynamoDB, with proper error handling and auditing.\n\n## Key Concepts\n- PII detection: Amazon Comprehend PII entities\n- Data flow: S3 -> Lambda -> S3 (redacted) -> DynamoDB\n- Privacy controls: encryption at rest (SSE/KMS), data minimization\n- Reliability: dead-letter queues (DLQ), retries, idempotent processing, logging\n\n## Code Example\n```javascript\nconst AWS = require('aws-sdk');\nconst s3 = new AWS.S3();\nconst comprehend = new AWS.Comprehend({region: 'us-east-1'});\nasync function redact(text) {\n  const res = await comprehend.detectPiiEntities({ Text: text, LanguageCode: 'en' }).promise();\n  let redacted = text;\n  // naive approach: replace ranges from end to start to avoid offset shifts\n  const ranges = res.Entities.map(e => ({ s: e.BeginOffset, e: e.EndOffset }));\n  ranges.sort((a,b) => b.s - a.s);\n  for (const r of ranges) {\n    redacted = redacted.substring(0, r.s) + '[REDACTED]' + redacted.substring(r.e);\n  }\n  return redacted;\n}\n```\n\n## Follow-up Questions\n- How would you validate redaction accuracy and handle false positives/negatives? \n- How would you adapt this for higher throughput or multilingual transcripts?","diagram":null,"difficulty":"beginner","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Snowflake","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T18:46:28.747Z","createdAt":"2026-01-12T18:46:28.747Z"},{"id":"q-1058","question":"Design a cross region, multi account AI inference platform for real time pricing and risk scoring in a fintech setting. Ingest streaming data, enforce per tenant data residency, and meet sub 100 ms latency. Describe data flow, services, IAM boundaries, model registry, feature store, drift monitoring, error handling, and cost controls?","answer":"Use a multi region, multi account pattern: stream data via Kinesis to region local Lambda preprocessors, push features to SageMaker Feature Store, and serve models with regional SageMaker endpoints be","explanation":"## Why This Is Asked\nExplores a candidate's ability to design scalable, compliant AI infra across AWS accounts and regions, ensuring tenancy isolation, latency targets, and governance.\n\n## Key Concepts\n- Multi account governance and cross region dataflow\n- SageMaker Feature Store and versioned model registry\n- PrivateLink, per tenant IAM, and API Gateway routing\n- Drift monitoring, error handling, and cost controls\n- Data residency via SCPs and audit trails via CloudTrail/Config\n\n## Code Example\n```javascript\n// Pseudo high level manifest of dataflow and services\nconst flow = [\n  'Kinesis -> Lambda preprocess',\n  'Feature Store write',\n  'Regional SageMaker endoints -> Tenant API',\n  'Step Functions orchestration',\n  'CloudTrail + Config for audit'\n];\n```\n\n## Follow-up Questions\n- How would you implement feature drift detection across regions?\n- What are the security implications of cross account model sharing and how would you mitigate them?","diagram":null,"difficulty":"advanced","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","NVIDIA","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T21:19:24.738Z","createdAt":"2026-01-12T21:19:24.738Z"},{"id":"q-1259","question":"Design an end-to-end AWS-native real-time fraud detection pipeline for a global e-commerce platform. Ingest event streams (Kinesis Data Streams), redact PII, create real-time features stored in SageMaker Feature Store (online) and offline store, with governance, lineage, access control, and cost constraints. Include data flow, IAM, retry logic, backpressure, testing, and incident response?","answer":"Propose an AWS-native real-time fraud pipeline: ingest events with Kinesis Data Streams, redact PII in-stream (Lambda or Kinesis Data Analytics) and publish redacted records to SageMaker Feature Store","explanation":"## Why This Is Asked\n\nTests real-time data flow and governance across streaming, feature store, and model scoring, plus privacy requirements.\n\n## Key Concepts\n\n- Kinesis Data Streams\n- SageMaker Feature Store (online/offline)\n- PII redaction in streaming\n- Data lineage and governance (Glue Data Catalog)\n- IAM, KMS, encryption at rest/in transit\n- Backpressure, retries, circuit breakers\n\n## Code Example\n\n```python\nimport boto3\n\ndef redact_pii(record):\n    # placeholder: call to Comprehend or regex\n    return record  # simplified\n```\n\n## Follow-up Questions\n\n- How would you test data drift in the feature store over time?\n- How would you handle schema evolution for features?\n","diagram":null,"difficulty":"advanced","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Cloudflare","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T07:24:57.938Z","createdAt":"2026-01-13T07:24:57.938Z"},{"id":"q-1292","question":"Design a real-time fraud-detection pipeline on AWS for a FinTech use-case. Ingest streaming transactions via Kinesis Data Streams, preprocess with Lambda, and invoke a SageMaker endpoint for real-time scores. Persist results to DynamoDB with audit logs in S3. Address latency (<200 ms), data privacy (KMS, VPC endpoints), IAM, drift monitoring, error handling, and cost control. Provide concrete components and trade-offs?","answer":"Flow: Ingest streaming transactions via Kinesis Data Streams; preprocess in Lambda; invoke a SageMaker endpoint for real-time fraud scores; persist results to DynamoDB and archive logs to S3. Security","explanation":"## Why This Is Asked\nTests real-world AWS AI deployment decisions: low latency, security, and governance in a streaming inference path.\n\n## Key Concepts\n- Streaming ingestion (Kinesis), serverless preprocessing (Lambda), model hosting (SageMaker), persistent storage (DynamoDB, S3).\n- Privacy: KMS, VPC endpoints, IAM least-privilege.\n- Observability: CloudWatch metrics, drift monitoring, retry/backoff.\n\n## Code Example\n```javascript\n// pseudo: Lambda handler invoked by Kinesis; calls SageMaker endpoint and writes to DynamoDB\n```\n\n## Follow-up Questions\n- How would you implement per-customer data isolation in this flow?\n- What failure modes require DLQ routing and circuit breakers?","diagram":"flowchart TD\n  A[Kinesis Data Streams] -->|Preprocess via Lambda| B[Lambda Preprocessing]\n  B -->|SageMaker Inference| C[SageMaker Endpoint]\n  C -->|Store in| D[DynamoDB]\n  C -->|Archive logs to| E[S3 Logs]","difficulty":"intermediate","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Coinbase"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T08:37:56.616Z","createdAt":"2026-01-13T08:37:56.616Z"},{"id":"q-951","question":"You're building a low‑ops internal voice assistant for support scripts. Audio files up to 60 seconds are uploaded to S3. Design an AWS‑only pipeline that transcribes, analyzes sentiment, and stores a brief summary plus an index in DynamoDB, with minimal cost and ops. Include data flow, services, error handling, and privacy considerations?","answer":"Use an S3 trigger to start a Step Functions workflow: Transcribe for 60s, then Comprehend for sentiment, store transcript + summary in DynamoDB, and index in a separate small table. Add a compact S3 m","explanation":"## Why This Is Asked\n\nTests ability to design a cost‑aware, low‑ops AWS data pipeline that stitches AI services together with serverless orchestration.\n\n## Key Concepts\n\n- AWS services: S3, Step Functions, Transcribe, Comprehend, DynamoDB\n- Serverless orchestration with error handling and retries\n- Data privacy: encryption, access control, least privilege\n\n## Code Example\n\n```javascript\n// Example AWS CDK snippet (TypeScript) configuring a Step Function state machine trigger\n```\n\n## Follow-up Questions\n\n- How would you scale the pipeline for higher concurrency?\n- How would you monitor latency and alert on failures?","diagram":null,"difficulty":"beginner","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T16:38:46.546Z","createdAt":"2026-01-12T16:38:46.546Z"}],"subChannels":["general"],"companies":["Airbnb","Citadel","Cloudflare","Coinbase","MongoDB","NVIDIA","Salesforce","Snap","Snowflake","Stripe","Zoom"],"stats":{"total":5,"beginner":2,"intermediate":1,"advanced":2,"newThisWeek":5}}