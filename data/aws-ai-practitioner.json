{"questions":[{"id":"q-1001","question":"You're building a beginner-friendly AWS-only pipeline to ingest customer chat transcripts (text files up to 50 KB) uploaded to S3. Design how to automatically redact PII using Amazon Comprehend PII detection, store the redacted transcript back to S3, and index metadata in DynamoDB. Include data flow, IAM permissions, error handling, and privacy considerations?","answer":"Two-bucket flow: uploads go to `transcript-uploads`; a Lambda triggered by S3 reads text, runs `Comprehend.detectPiiEntities` to identify PII, replaces with `[REDACTED]`, writes redacted text to `tran","explanation":"## Why This Is Asked\nTests ability to design a secure, cost-conscious AWS-native pipeline that handles PII responsibly using services like S3, Lambda, Comprehend, and DynamoDB, with proper error handling and auditing.\n\n## Key Concepts\n- PII detection: Amazon Comprehend PII entities\n- Data flow: S3 -> Lambda -> S3 (redacted) -> DynamoDB\n- Privacy controls: encryption at rest (SSE/KMS), data minimization\n- Reliability: dead-letter queues (DLQ), retries, idempotent processing, logging\n\n## Code Example\n```javascript\nconst AWS = require('aws-sdk');\nconst s3 = new AWS.S3();\nconst comprehend = new AWS.Comprehend({region: 'us-east-1'});\nasync function redact(text) {\n  const res = await comprehend.detectPiiEntities({ Text: text, LanguageCode: 'en' }).promise();\n  let redacted = text;\n  // naive approach: replace ranges from end to start to avoid offset shifts\n  const ranges = res.Entities.map(e => ({ s: e.BeginOffset, e: e.EndOffset }));\n  ranges.sort((a,b) => b.s - a.s);\n  for (const r of ranges) {\n    redacted = redacted.substring(0, r.s) + '[REDACTED]' + redacted.substring(r.e);\n  }\n  return redacted;\n}\n```\n\n## Follow-up Questions\n- How would you validate redaction accuracy and handle false positives/negatives? \n- How would you adapt this for higher throughput or multilingual transcripts?","diagram":null,"difficulty":"beginner","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Snowflake","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T18:46:28.747Z","createdAt":"2026-01-12T18:46:28.747Z"},{"id":"q-1058","question":"Design a cross region, multi account AI inference platform for real time pricing and risk scoring in a fintech setting. Ingest streaming data, enforce per tenant data residency, and meet sub 100 ms latency. Describe data flow, services, IAM boundaries, model registry, feature store, drift monitoring, error handling, and cost controls?","answer":"Use a multi region, multi account pattern: stream data via Kinesis to region local Lambda preprocessors, push features to SageMaker Feature Store, and serve models with regional SageMaker endpoints be","explanation":"## Why This Is Asked\nExplores a candidate's ability to design scalable, compliant AI infra across AWS accounts and regions, ensuring tenancy isolation, latency targets, and governance.\n\n## Key Concepts\n- Multi account governance and cross region dataflow\n- SageMaker Feature Store and versioned model registry\n- PrivateLink, per tenant IAM, and API Gateway routing\n- Drift monitoring, error handling, and cost controls\n- Data residency via SCPs and audit trails via CloudTrail/Config\n\n## Code Example\n```javascript\n// Pseudo high level manifest of dataflow and services\nconst flow = [\n  'Kinesis -> Lambda preprocess',\n  'Feature Store write',\n  'Regional SageMaker endoints -> Tenant API',\n  'Step Functions orchestration',\n  'CloudTrail + Config for audit'\n];\n```\n\n## Follow-up Questions\n- How would you implement feature drift detection across regions?\n- What are the security implications of cross account model sharing and how would you mitigate them?","diagram":null,"difficulty":"advanced","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","NVIDIA","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T21:19:24.738Z","createdAt":"2026-01-12T21:19:24.738Z"},{"id":"q-1259","question":"Design an end-to-end AWS-native real-time fraud detection pipeline for a global e-commerce platform. Ingest event streams (Kinesis Data Streams), redact PII, create real-time features stored in SageMaker Feature Store (online) and offline store, with governance, lineage, access control, and cost constraints. Include data flow, IAM, retry logic, backpressure, testing, and incident response?","answer":"Propose an AWS-native real-time fraud pipeline: ingest events with Kinesis Data Streams, redact PII in-stream (Lambda or Kinesis Data Analytics) and publish redacted records to SageMaker Feature Store","explanation":"## Why This Is Asked\n\nTests real-time data flow and governance across streaming, feature store, and model scoring, plus privacy requirements.\n\n## Key Concepts\n\n- Kinesis Data Streams\n- SageMaker Feature Store (online/offline)\n- PII redaction in streaming\n- Data lineage and governance (Glue Data Catalog)\n- IAM, KMS, encryption at rest/in transit\n- Backpressure, retries, circuit breakers\n\n## Code Example\n\n```python\nimport boto3\n\ndef redact_pii(record):\n    # placeholder: call to Comprehend or regex\n    return record  # simplified\n```\n\n## Follow-up Questions\n\n- How would you test data drift in the feature store over time?\n- How would you handle schema evolution for features?\n","diagram":null,"difficulty":"advanced","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Cloudflare","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T07:24:57.938Z","createdAt":"2026-01-13T07:24:57.938Z"},{"id":"q-1292","question":"Design a real-time fraud-detection pipeline on AWS for a FinTech use-case. Ingest streaming transactions via Kinesis Data Streams, preprocess with Lambda, and invoke a SageMaker endpoint for real-time scores. Persist results to DynamoDB with audit logs in S3. Address latency (<200 ms), data privacy (KMS, VPC endpoints), IAM, drift monitoring, error handling, and cost control. Provide concrete components and trade-offs?","answer":"Flow: Ingest streaming transactions via Kinesis Data Streams; preprocess in Lambda; invoke a SageMaker endpoint for real-time fraud scores; persist results to DynamoDB and archive logs to S3. Security","explanation":"## Why This Is Asked\nTests real-world AWS AI deployment decisions: low latency, security, and governance in a streaming inference path.\n\n## Key Concepts\n- Streaming ingestion (Kinesis), serverless preprocessing (Lambda), model hosting (SageMaker), persistent storage (DynamoDB, S3).\n- Privacy: KMS, VPC endpoints, IAM least-privilege.\n- Observability: CloudWatch metrics, drift monitoring, retry/backoff.\n\n## Code Example\n```javascript\n// pseudo: Lambda handler invoked by Kinesis; calls SageMaker endpoint and writes to DynamoDB\n```\n\n## Follow-up Questions\n- How would you implement per-customer data isolation in this flow?\n- What failure modes require DLQ routing and circuit breakers?","diagram":"flowchart TD\n  A[Kinesis Data Streams] -->|Preprocess via Lambda| B[Lambda Preprocessing]\n  B -->|SageMaker Inference| C[SageMaker Endpoint]\n  C -->|Store in| D[DynamoDB]\n  C -->|Archive logs to| E[S3 Logs]","difficulty":"intermediate","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Coinbase"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T08:37:56.616Z","createdAt":"2026-01-13T08:37:56.616Z"},{"id":"q-1313","question":"You're designing a beginner-friendly AWS-only pipeline for user-submitted PDFs (max 5 MB) uploaded to S3. Build an end-to-end workflow that uses Textract to extract text, runs a lightweight topic model via Comprehend or a tiny SageMaker endpoint to derive a topic label, stores a compact summary and a searchable index in DynamoDB, and exposes a read path via API Gateway + Lambda. Include data flow, IAM roles, error handling, and privacy considerations?","answer":"Set up an event-driven pipeline: S3 upload to incoming-pdfs/, Lambda triggers Textract for text extraction, and then sends the text to Comprehend (or a small SageMaker model) for topic labeling. Persi","explanation":"## Why This Is Asked\nTests multi-service wiring (Textract, Comprehend/SageMaker, DynamoDB, API Gateway) and basic privacy, cost, and error handling.\n\n## Key Concepts\n- AWS Textract for PDF text extraction\n- Comprehend or SageMaker for simple topic modeling\n- DynamoDB for metadata index\n- API Gateway + Lambda for reads\n- IAM least privilege, KMS, retry, DLQ\n\n## Code Example\n```python\n# Lambda pseudo-handler sketch\ndef handler(event, context):\n  # parse S3 event, call Textract, then Comprehend/SageMaker, store DynamoDB item\n  return {\"status\": \"ok\"}\n```\n\n## Follow-up Questions\n- How would you handle large PDFs exceeding 5 MB?\n- How would you test data privacy controls in this flow?","diagram":"flowchart TD\n  A[S3 Upload] --> B[Lambda Trigger]\n  B --> C[Textract]\n  C --> D[Topic Label (Comprehend/SageMaker)]\n  D --> E[DynamoDB (summary/index)]\n  E --> F[API Gateway / Lambda read path]","difficulty":"beginner","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T10:38:19.119Z","createdAt":"2026-01-13T10:38:19.119Z"},{"id":"q-1366","question":"You're operating a real-time text classifier API on a SageMaker hosting endpoint behind API Gateway. You need a canary deployment strategy using CodeDeploy for SageMaker endpoints, with 2% of traffic to the new version, ramping to 50% over 2 hours, then full if no regressions. Describe data flow, required services, IAM, drift detection via Model Monitor, and rollback/failover logic, plus privacy considerations?","answer":"Use a canary deployment with CodeDeploy for SageMaker endpoints: two production variants, 2% traffic to v2, then gradual ramp to 50% over 2 hours. Monitor drift with Model Monitor and live metrics; re","explanation":"## Why This Is Asked\nThis question probes real-world deployment discipline: canary rollouts, automated rollback, and monitoring for ML endpoints in production.\n\n## Key Concepts\n- Canary deployments with CodeDeploy for SageMaker endpoints\n- EndpointVariant traffic shifting and health checks\n- Model Monitor drift detection and performance metrics\n- Automated rollback, alarms, and observability\n- IAM least-privilege and privacy considerations\n\n## Code Example\n```yaml\n# AppSpec for SageMaker canary deployment (illustrative)\nversion: 0.0\nResources:\n  SageMakerEndpoint:\n    Type: AWS::SageMaker::Endpoint\n    Properties:\n      EndpointConfigName: MyEndpointConfig\n```\n\n## Follow-up Questions\n- How would you simulate drift in CI/CD?\n- How would you extend to multi-region endpoints and cross-account monitoring?\n","diagram":"flowchart TD\n  A[Client Request] --> B(API Gateway)\n  B --> C[SageMaker Endpoint: v1 Production]\n  B --> D[SageMaker Endpoint: v2 Canary]\n  D --> E[Model Monitor & CloudWatch]\n  E --> F{Drift/Performance OK?}\n  F -- Yes --> G[CodeDeploy traffic shift]\n  F -- No --> H[Rollback to v1]\n  G --> I[Promote to Production when ready]","difficulty":"intermediate","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["NVIDIA","Netflix","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T13:19:25.324Z","createdAt":"2026-01-13T13:19:25.324Z"},{"id":"q-1437","question":"You're building a beginner-friendly, AWS‑only pipeline to process user-submitted emails stored as JSON in S3 (max 20 KB per file). Design an end-to-end workflow that detects language, translates to English, analyzes sentiment, and stores results in DynamoDB, with a simple read path via API Gateway + Lambda. Include data flow, services, error handling, IAM roles, and privacy considerations?","answer":"Propose an AWS-only pipeline: S3 triggers Lambda to enqueue processing; Step Functions coordinates Translate, Comprehend (DetectDominantLanguage and DetectSentiment) and a DynamoDB write; expose a rea","explanation":"## Why This Is Asked\n\nThis asks for a practical, beginner-friendly AWS AI/ML workflow that combines text detection, translation, sentiment, storage, and a lightweight read API, while emphasizing security and cost.\n\n## Key Concepts\n\n- DetectDominantLanguage (Comprehend)\n- TranslateText (Translate)\n- DetectSentiment (Comprehend)\n- S3 → Lambda → Step Functions orchestration\n- DynamoDB indexing by documentId\n- API Gateway + Lambda for reads\n- IAM least privilege and KMS\n- Privacy: PII masking, encryption, and retention\n\n## Code Example\n\n```javascript\nimport { ComprehendClient, DetectDominantLanguageCommand, DetectSentimentCommand } from \"@aws-sdk/client-comprehend\";\nimport { TranslateClient, TranslateTextCommand } from \"@aws-sdk/client-translate\";\n\nconst comprehend = new ComprehendClient({region:\"us-east-1\"});\nconst translate = new TranslateClient({region:\"us-east-1\"});\n\nasync function analyzeText(text){\n  const dl = await comprehend.send(new DetectDominantLanguageCommand({Text: text}));\n  const langCode = (dl.Languages?.[0]?.LanguageCode) ?? \"en\";\n  let english = text;\n  if (langCode !== \"en\"){\n    const tr = await translate.send(new TranslateTextCommand({Text: text, SourceLanguageCode: langCode, TargetLanguageCode: \"en\"}));\n    english = tr.TranslatedText;\n  }\n  const sentiment = await comprehend.send(new DetectSentimentCommand({Text: english, LanguageCode: \"en\"}));\n  return { language: langCode, translated: english, sentiment: sentiment.Sentiment };\n}\n``` \n\n## Follow-up Questions\n\n- How would you test this with mocks and local development?\n- What are the implications for cost at scale and how would you mitigate?\n- How would you handle a batch of emails arriving concurrently?","diagram":null,"difficulty":"beginner","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","Robinhood","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T16:58:46.574Z","createdAt":"2026-01-13T16:58:46.574Z"},{"id":"q-1460","question":"Design an advanced, low-latency fraud-detection pipeline for real-time payments in AWS. Ingest events via Kinesis Data Streams, derive features into SageMaker Feature Store, train/deploy with SageMaker Pipelines, score via a real-time SageMaker Endpoint called from Lambda, and log audits in CloudTrail. Enforce KMS encryption, data minimization/retention policies, IAM least privilege, and a rollback/failover plan. Include data flow, IAM, privacy, and failure handling?","answer":"Ingest payment events via Kinesis Data Streams with enhanced fan-out consumers; derive and store features in SageMaker Feature Store with automatic feature validation; train models using SageMaker Pipelines with automated drift detection and retraining triggers; deploy to real-time SageMaker Endpoint with auto-scaling and A/B testing capabilities; score transactions via Lambda that validates input schema and enforces rate limiting; store results in DynamoDB with TTL for data minimization; audit all actions in CloudTrail with CloudWatch Logs for compliance; encrypt all data at rest and in transit using KMS customer-managed keys; implement IAM least privilege with service-specific policies; enable multi-AZ deployment with automatic failover and blue-green rollback.","explanation":"## Why This Is Asked\nTests comprehensive real-time ML deployment, feature engineering, security compliance, and operational resilience in regulated fintech environments.\n\n## Key Concepts\n- Kinesis Data Streams with enhanced fan-out, SageMaker Feature Store with feature validation\n- SageMaker Pipelines with automated drift detection, real-time endpoints with auto-scaling\n- Lambda integration with schema validation, DynamoDB with TTL for data minimization\n- CloudTrail auditing, KMS customer-managed keys, IAM least privilege with service controls\n- Multi-AZ deployment, blue-green deployments, automated rollback procedures\n\n## Code Example\n```python\nimport boto3, json, os\nfrom botocore.exceptions import ClientError\n\nsm = boto3.client('sagemaker-runtime')\nddb = boto3.client('dynamodb')\ncloudtrail = boto3.client('cloudtrail')\n\ndef lambda_handler(event, context):\n    # Validate input schema\n    required_fields = ['transaction_id', 'amount', 'timestamp', 'user_id']\n    if not all(field in event for field in required_fields):\n        raise ValueError('Missing required fields')\n    \n    # Rate limiting check\n    if event['amount'] > float(os.environ.get('MAX_AMOUNT', 10000)):\n        return {'status': 'rejected', 'reason': 'amount_exceeded'}\n    \n    # Real-time scoring\n    try:\n        payload = json.dumps({k: v for k, v in event.items() if k in required_fields})\n        resp = sm.invoke_endpoint(\n            EndpointName=os.environ['ENDPOINT_NAME'],\n            ContentType='application/json',\n            Body=payload\n        )\n        score = json.loads(resp['Body'].read().decode())\n        \n        # Store result with TTL\n        ddb.put_item(\n            TableName='fraud-scores',\n            Item={\n                'transaction_id': {'S': event['transaction_id']},\n                'score': {'N': str(score['probability'])},\n                'timestamp': {'N': str(event['timestamp'])},\n                'ttl': {'N': str(int(time.time()) + 86400*30)}  # 30-day retention\n            }\n        )\n        \n        return {'status': 'scored', 'score': score['probability']}\n    except ClientError as e:\n        cloudtrail.put_event_selectors(\n            TrailName='fraud-audit-trail',\n            EventSelectors=[{'SourceName': 'lambda.amazonaws.com'}]\n        )\n        return {'status': 'error', 'message': str(e)}\n```\n\n## Follow-up Questions\n- How would you implement feature drift detection and automated retraining?\n- How to design canary deployments for model updates?\n- What monitoring metrics would you set up for operational health?\n- How would you handle GDPR right-to-be-forgotten requests?","diagram":"flowchart TD\n  A[Ingest events] --> B[Kinesis Data Streams]\n  B --> C[SageMaker Feature Store (online)]\n  C --> D[SageMaker Pipelines (train & deploy)]\n  D --> E[SageMaker Real-time Endpoint]\n  E --> F[Lambda Scoring]\n  F --> G[CloudWatch + SageMaker Model Monitor]\n  G --> H[CloudTrail Auditing]","difficulty":"advanced","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":["kinesis data streams","sagemaker feature store","sagemaker pipelines","real-time endpoint","lambda integration","dynamodb ttl","cloudtrail auditing","kms encryption","iam least privilege","multi-az deployment","blue-green rollback","enhanced fan-out"],"voiceSuitable":true,"isNew":true,"lastUpdated":"2026-01-15T04:52:37.096Z","createdAt":"2026-01-13T17:56:45.234Z"},{"id":"q-1482","question":"Design a production-ready, AWS-native content moderation pipeline for a multi-tenant SaaS platform handling images and captions uploaded to S3. Must route per-tenant policies to a custom SageMaker multi-model endpoint, apply per-tenant threshold overrides, enforce data isolation, log audit trails, and keep costs predictable with auto-scaling and caching. Describe data flow, IAM, encryption, error handling, and privacy implications?","answer":"Use a two-branch flow: images to a SageMaker multi-model endpoint and captions to a Lambda-accelerated text classifier; tenant policies in DynamoDB drive per-tenant thresholds and endpoint routing via","explanation":"## Why This Is Asked\nAssess ability to design a scalable, multi-tenant AI moderation pipeline on AWS with policy-driven routing, data isolation, and compliant audit logging.\n\n## Key Concepts\n- SageMaker multi-model endpoints and per-tenant routing\n- DynamoDB for policy storage; S3 for audit logs and artifacts\n- Tenant isolation via prefixes, IAM scoping, KMS encryption\n- Step Functions for orchestration, error handling, retries\n- Cost controls: autoscaling, model cache, off-peak scheduling\n\n## Code Example\n```javascript\n// Pseudo-routing logic for tenant-aware inference\nfunction routeToEndpoint(tenantId, itemType) {\n  const policy = fetchPolicy(tenantId)\n  if (itemType === 'image') return { endpoint: policy.imageEndpoint, threshold: policy.imageThreshold }\n  return { endpoint: policy.textEndpoint, threshold: policy.textThreshold }\n}\n```\n\n## Follow-up Questions\n- How would you test tenant policy overrides without affecting others?\n- How would you enforce data deletion and retention across S3 and DynamoDB?","diagram":"flowchart TD\n  A[Upload to S3 (tenantId)] --> B[EventBridge/SFN]\n  B --> C{Content Type}\n  C --> D[SageMaker multi-model endpoint]\n  C --> E[Text classifier]\n  D & E --> F[Apply per-tenant policy]\n  F --> G[Store result in DynamoDB]\n  G --> H[Audit log in S3]","difficulty":"advanced","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Cloudflare","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T18:54:50.921Z","createdAt":"2026-01-13T18:54:50.921Z"},{"id":"q-1552","question":"You're deploying a privacy-preserving AI rating model for financial support tickets on AWS. Incoming tickets arrive as JSON (up to 10 KB) in S3; design an end-to-end pipeline that uses SageMaker for inference, writes per-ticket audit logs to DynamoDB, encrypts data at rest with SSE-KMS, and supports reliable, rate-limited processing with error handling and drift monitoring. How would you implement this?","answer":"Implement S3 event triggers to invoke Lambda functions that call SageMaker endpoints for inference; maintain comprehensive audit logs in DynamoDB with ticket_id, input hash, model_version, latency, and outcome; enforce SSE-KMS encryption across S3 and DynamoDB; incorporate Dead Letter Queues for reliable error handling with exponential backoff and rate limiting.","explanation":"## Why This Is Asked\nThis question evaluates expertise in designing end-to-end privacy-preserving ML workflows on AWS, with emphasis on security implementation (SSE-KMS), system reliability (DLQs, exponential backoff), governance compliance (audit logging), and cost optimization (autoscaling controls).\n\n## Key Concepts\n- S3 event-driven data ingestion and Lambda orchestration\n- SageMaker endpoints for real-time ML inference\n- DynamoDB for comprehensive audit trail with unique identifiers\n- SSE-KMS encryption implementation across all AWS services\n- Robust error handling with DLQs, backoff strategies, and rate limiting","diagram":"flowchart TD\n  A[Tickets arrive in S3] --> B[S3 Event -> Lambda]\n  B --> C[SageMaker Inference]\n  C --> D[DynamoDB Audit]\n  D --> E[Model Monitor/Alarms]","difficulty":"intermediate","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["PayPal","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T06:29:47.696Z","createdAt":"2026-01-13T21:40:06.374Z"},{"id":"q-1583","question":"You're running a real-time fraud-detection model for payments. Data arrives in a Kinesis stream; design an end-to-end AWS-native pipeline that performs per-record inference, monitors drift and bias with SageMaker Clarify/Model Monitor, archives data in S3 with metadata, and auto-triggers a rollback to a previous model version via Step Functions if drift thresholds are exceeded. Include data flow, IAM, encryption, backpressure handling, and privacy considerations?","answer":"Per-record inference via a SageMaker endpoint fed by Kinesis streams, a Lambda transform layer to normalize features, Model Monitor to detect data and label drift, and Clarify for bias auditing; archive raw and processed data in S3 with SSE-KMS encryption, store metadata in DynamoDB, implement Step Functions for automated rollback with exponential backoff, handle backpressure through Kinesis enhanced fan-out and Lambda provisioned concurrency, and ensure privacy through data masking and PII detection.","explanation":"## Why This Is Asked\nAssessment of end-to-end ML pipelines on AWS, with drift detection, governance, and safe rollback.\n\n## Key Concepts\n- Real-time inference integration (Kinesis + SageMaker Endpoint)\n- Drift and bias monitoring (Model Monitor, Clarify)\n- Data archiving and metadata (S3 with SSE-KMS, DynamoDB)\n- Automated rollback (Step Functions) and retry/backoff\n\n## Code Example\n```javascript\n// Example: Lambda to normalize features before SageMaker inference\nconst AWS = require('aws-sdk');\nconst sage = new AWS.SageMakerRuntime();\nexports.handler = async (event) => {\n  // transform feature\n```","diagram":"flowchart TD\n  Kinesis[Data Stream] --> Lambda[Preprocess]\n  Lambda --> Sage[Inference Endpoint]\n  Sage --> DynamoDB[Prediction Logs]\n  Sage --> S3[Data Archive]\n  SageMonitor[SageMaker Model Monitor] --> Drift[Drift Alert]\n  Drift --> SF[Step Functions]\n  SF --> Rollback[Rollback to previous model]\n  Rollback --> Sage[Endpoint Redeploy]","difficulty":"intermediate","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Anthropic","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T05:57:38.931Z","createdAt":"2026-01-13T22:50:38.700Z"},{"id":"q-1627","question":"You're building an AWS-native ML platform that serves a global analytics product with data residing in Snowflake and Databricks across regions. Design a pipeline that ingests from both sources, uses SageMaker Feature Store, trains/registers with a human approval, serves real-time inference, and uses HashiCorp Vault for secrets. Include data lineage, IAM, encryption, drift monitoring, rollback policy, and privacy considerations?","answer":"Design a cross-region AWS pipeline that ingests batch data from Snowflake and Databricks into S3, feeds features to SageMaker Feature Store, trains and registers models with an approval step, deploys ","explanation":"## Why This Is Asked\nThis question tests cross-cloud data plumbing, security, and production-grade ML governance in a realistic, multi-region setup.\n\n## Key Concepts\n- Cross-region ingestion from Snowflake and Databricks\n- SageMaker Feature Store and Model Registry with human approvals\n- HashiCorp Vault integration and cross-account IAM\n- Data lineage, encryption, privacy controls, and auditability\n- Drift monitoring and automated rollback\n\n## Code Example\n```python\n# Pseudo: orchestrate fetch from sources, store features, train, register, and deploy with approval\n```\n\n## Follow-up Questions\n- How would you validate data provenance across Snowflake/Databricks?\n- How would you test rollback and ensure reproducibility?","diagram":"flowchart TD\n  S[Snowflake] --> D1[(Raw Data S3)]\n  D1 --> F[SageMaker Feature Store]\n  Databricks[Databricks] --> D1\n  F --> T[Training Job]\n  T --> R[Model Registry]\n  R --> E[Endpoint]\n  Vault(HashiCorp Vault) --> Secrets[Secrets Management]\n  Drift[Drift Monitor] --> Roll[Rollback Trigger]","difficulty":"advanced","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Hashicorp","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T04:14:53.187Z","createdAt":"2026-01-14T04:14:53.187Z"},{"id":"q-1750","question":"You're building a beginner-friendly AWS-only pipeline for multilingual customer recordings. Audio files (up to 90 seconds) are uploaded to S3. Design an event-driven flow that uses Amazon Transcribe to produce a transcript in the source language, Amazon Translate to produce a Spanish version, stores both transcripts in S3, and writes a compact index in DynamoDB with fields like customerId, fileKey, sourceLang, translatedKey. Include data flow, IAM, error handling, and privacy considerations?","answer":"Design an event-driven AWS pipeline: S3 upload triggers a workflow (Step Functions or Lambda) that starts Transcribe for the source transcript, calls Translate to generate a Spanish version, writes bo","explanation":"## Why This Is Asked\n\nAssesses practical use of AWS AI services (Transcribe, Translate) in a beginner-friendly, cost-conscious pipeline with data governance and privacy considerations.\n\n## Key Concepts\n\n- Amazon Transcribe for speech-to-text\n- Amazon Translate for multilingual text\n- S3 event-driven processing\n- DynamoDB indexing for quick lookups\n- IAM permissions, encryption, and retry strategies\n\n## Code Example\n\n```javascript\n// Pseudo Lambda orchestrating Transcribe and Translate\nexports.handler = async (event) => {\n  const fileKey = event.Records[0].s3.object.key;\n  // 1) start Transcribe job on the audio\n  // 2) fetch transcript, call Translate to Spanish\n  // 3) upload both transcripts to S3\n  // 4) write DynamoDB index with metadata\n  // 5) handle errors and retries\n};\n```\n\n## Follow-up Questions\n\n- How would you handle long-running transcripts and batching?\n- What changes for multi-language translation beyond Spanish?\n- How would you enforce data privacy (encryption, access controls, vendor logs)?","diagram":"flowchart TD\n  A[S3 Upload] --> B[Event Trigger]\n  B --> C[Transcribe Job]\n  C --> D[Transcript Text]\n  D --> E[Translate to Spanish]\n  E --> F[Translate Text]\n  C --> G[Original Transcript S3]\n  F --> H[Translated Transcript S3]\n  G --> I[DynamoDB Index]\n  H --> I\n","difficulty":"beginner","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T09:39:05.334Z","createdAt":"2026-01-14T09:39:05.334Z"},{"id":"q-1789","question":"Design an AWS-native, real-time fraud-detection pipeline: ingest streaming transactions from Kinesis Data Streams, score risk with a SageMaker Inference endpoint (Transformer-based), store scores in DynamoDB, and trigger a Step Functions workflow for high-risk events. Include data flow, IAM, privacy controls, drift monitoring, and cost governance?","answer":"Real-time fraud-detection pipeline using Kinesis -> SageMaker Inference endpoint -> DynamoDB table with transactionId and score, plus a Step Functions workflow for investigations when score exceeds a ","explanation":"## Why This Is Asked\nThis question evaluates building a low-latency, compliant fraud-detection pipeline with streaming data, model hosting, and integration patterns.\n\n## Key Concepts\n- Real-time ingestion with Kinesis Data Streams\n- SageMaker Inference endpoints (transformer-based models)\n- DynamoDB data modeling and TTL\n- Step Functions orchestration and alerts\n- IAM, KMS, and privacy controls\n- Model drift monitoring (SageMaker Model Monitor)\n- Cost governance and autoscaling\n\n## Code Example\n```javascript\nconst AWS = require('aws-sdk');\nconst sagemaker = new AWS.SageMakerRuntime();\nasync function score(event){\n  const resp = await sagemaker.invokeEndpoint({EndpointName:'FraudDetector', Body: JSON.stringify(event), ContentType:'application/json'}).promise();\n  return JSON.parse(resp.Body.toString());\n}\n```\n\n## Follow-up Questions\n- How would you implement canary deployments and drift monitoring for the endpoint?\n- How would you validate data retention and privacy controls across regions?","diagram":null,"difficulty":"advanced","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Goldman Sachs"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T10:50:22.735Z","createdAt":"2026-01-14T10:50:22.735Z"},{"id":"q-1800","question":"You're building a compliant multi-tenant, multilingual content moderation service on AWS. Users upload video to S3; extract audio, transcribe with Amazon Transcribe, run a SageMaker classifier to flag policy-violating content, redact PII, and store transcripts and decisions, indexing per-tenant metadata in DynamoDB. Design an end-to-end, event-driven pipeline with data isolation, data sovereignty, error handling, and privacy controls?","answer":"Adopt a tenant-scoped data plane (per-tenant S3 prefixes, Lake Formation isolation, KMS keys). EventBridge triggers Step Functions on video upload: extract audio, Transcribe (source language), SageMak","explanation":"## Why This Is Asked\nTests ability to design multi-tenant, compliant AI pipelines in AWS, balancing isolation, privacy, and operational resilience.\n\n## Key Concepts\n- Tenant isolation with Lake Formation and per-tenant KMS keys\n- Event-driven orchestration via EventBridge and Step Functions\n- AWS AI: Transcribe, SageMaker moderation, and PII redaction\n- Data residency and regional replication\n\n## Code Example\n```javascript\n// Pseudo-steps: on S3 PUT -> extract audio -> Transcribe -> SageMaker -> redact -> store -> index\n``` \n\n## Follow-up Questions\n- How would you test data residency constraints across regions?\n- How would you handle tenant policy changes at runtime?","diagram":"flowchart TD\n  A[Video Upload to S3 (Tenant)] --> B[EventBridge]\n  B --> C[Step Functions]\n  C --> D[Transcribe]\n  D --> E[SageMaker Moderation]\n  E --> F[PII Redaction]\n  F --> G[S3: Transcript + Decisions]\n  G --> H[DynamoDB: Tenant Index]\n  H --> I[Regional Replication]","difficulty":"intermediate","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T11:35:55.371Z","createdAt":"2026-01-14T11:35:55.371Z"},{"id":"q-1831","question":"You're building a beginner AWS-only pipeline to process user-uploaded audio stories (up to 120 seconds). When an audio file lands in S3, design an event-driven flow that uses Amazon Transcribe to produce a transcript, Amazon Comprehend to extract keywords, and a SageMaker endpoint to classify genre. Save transcript and metadata to S3 and index in DynamoDB. Include data flow, IAM permissions, error handling, and privacy considerations?","answer":"Orchestrate with Step Functions: on S3 upload (≤120s) run Transcribe to text, Comprehend for keywords, and a SageMaker endpoint to assign a genre. Save transcript to transcripts/{key}.txt and metadata","explanation":"## Why This Is Asked\n- Tests end-to-end orchestration across AI services with Step Functions.\n- Emphasizes data governance, encryption, and privacy in an entry-level pipeline.\n- Assesses DynamoDB schema design and S3 layout for artifacts.\n\n## Key Concepts\n- Event-driven workflows, IAM least privilege, cross-service data handoff\n- Privacy by design: encryption, access controls, retention\n- Simple model hosting via SageMaker endpoint\n\n## Code Example\n```javascript\nconst stateMachine = {\n  Comment: 'Audio genre labeling',\n  StartAt: 'Transcribe',\n  States: {\n    Transcribe: { Type: 'Task', Resource: 'arn:aws:transcribe:...', Next: 'Comprehend' },\n    Comprehend: { Type: 'Task', Resource: 'arn:aws:comprehend:...', Next: 'SageMaker' },\n    SageMaker: { Type: 'Task', Resource: 'arn:aws:sagemaker:...', Next: 'Store' },\n    Store: { Type: 'Pass', Result: 'done', End: true }\n  }\n}\n```\n\n## Follow-up Questions\n- How would you add retry/backoff and DLQ for failed steps?\n- How would multi-language transcripts affect the flow and data model?","diagram":"flowchart TD\n  A[S3 Upload] --> B[Step Functions]\n  B --> C[Transcribe]\n  B --> D[Comprehend]\n  B --> E[SageMaker Genre]\n  C --> F[S3 transcripts/]\n  E --> G[S3 outputs/genre.json]\n  D --> H[S3 outputs/keywords.json]\n  B --> I[DynamoDB AudioIndex]","difficulty":"beginner","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Anthropic","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T13:13:46.119Z","createdAt":"2026-01-14T13:13:46.119Z"},{"id":"q-1902","question":"You're building a beginner AWS-only pipeline to moderate user-uploaded profile pictures (JPEG/PNG, up to 2MB) in S3; on upload, design an event-driven flow using Rekognition to detect unsafe content and top labels, write a JSON report to S3, and upsert a summary in DynamoDB with fields like userId, objectKey, safeFlag, topLabel. Include data flow, IAM, error handling, and privacy considerations?","answer":"Trigger a Lambda from the S3 PUT event, call Rekognition DetectLabels and SafeSearch, store a small JSON report in S3 with labelScore and safety verdict, then upsert a summary item in DynamoDB with us","explanation":"## Why This Is Asked\nTests ability to design a simple yet complete event-driven pipeline using Rekognition, S3, Lambda, and DynamoDB, while accounting for privacy and failure handling.\n\n## Key Concepts\n- S3 event triggers and Lambda integration\n- Rekognition DetectLabels and SafeSearch usage\n- Writing reports to S3 and upserting DynamoDB records\n- IAM least-privilege roles and policy scoping\n- Error handling with DLQ and idempotency, privacy controls\n\n## Code Example\n```javascript\nconst AWS = require('aws-sdk');\nconst rekognition = new AWS.Rekognition();\nconst s3 = new AWS.S3();\nconst dynamodb = new AWS.DynamoDB.DocumentClient();\n\nexports.handler = async (event) => {\n  const bucket = event.Records[0].s3.bucket.name;\n  const key = decodeURIComponent(event.Records[0].s3.object.key.replace(/\\+/g, ' '));\n  const image = { Image: { S3Object: { Bucket: bucket, Name: key } } };\n  const resp = await rekognition.detectLabels({ ...image, MaxLabels: 5 }).promise();\n  const labels = resp.Labels.map(l => l.Name);\n  const safeFlag = resp.Adult?.Value === false && resp.Violence?.Value === false;\n  const report = { objectKey: key, safeFlag, topLabel: labels[0], labels };\n  await s3.putObject({ Bucket: bucket, Key: key + '.report.json', Body: JSON.stringify(report) }).promise();\n  await dynamodb.put({ TableName: 'ProfileModeration', Item: { userId: '<USER_ID>', objectKey: key, safeFlag, topLabel: labels[0] } }).promise();\n  return {};\n}\n```\n\n## Follow-up Questions\n- How would you handle mislabeled content or false positives?\n- How would you scale to millions of uploads with idempotent processing?","diagram":"flowchart TD\n  A[S3 Upload] --> B[Lambda Trigger]\n  B --> C[Rekognition: DetectLabels & SafeSearch]\n  C --> D[Write report.json to S3]\n  D --> E[Update DynamoDB]\n  E --> F[Done]","difficulty":"beginner","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T16:46:44.316Z","createdAt":"2026-01-14T16:46:44.316Z"},{"id":"q-1935","question":"Design a real-time, multi-tenant fraud-detection pipeline for a SaaS platform. Ingestion uses Kinesis Data Streams per tenant; a central SageMaker detector scores each event; PII is redacted before storage in per-tenant S3 buckets; an index is kept in DynamoDB with fields tenantId, sessionId, eventTime, fraudScore. Explain data isolation, cross-account IAM, error handling, privacy controls, and idempotency/replay safety?","answer":"Architect a real-time, multi-tenant fraud-detection pipeline: Kinesis streams per tenant feed a central SageMaker detector; redact PII, write artifacts to tenant-scoped S3, and publish a per-tenant Dy","explanation":"## Why This Is Asked\nTests ability to design scalable, compliant streaming pipelines across accounts, with strict data isolation, privacy, and correctness guarantees.\n\n## Key Concepts\n- Real-time event processing with Kinesis and Lambda/SSM\n- Cross-account IAM roles and least-privilege access\n- Data redaction, encryption at rest (SSE-KMS), and tenant-scoped storage\n- Idempotency, replay safety, and DLQ/error handling\n- Durable auditing and per-tenant indexing in DynamoDB\n\n## Code Example\n```javascript\n// Pseudo: consume Kinesis event, redactPII, invoke SageMaker, write to S3, upsert DynamoDB\n```\n\n## Follow-up Questions\n- How would you implement idempotent processing for duplicate events?\n- What tests ensure privacy requirements are met across tenants?","diagram":"flowchart TD\n  A[Kinesis Streams (per-tenant)] --> B[Processor (Lambda/Step Functions)]\n  B --> C[SageMaker Detector]\n  B --> D[PII Redaction]\n  C --> E[S3 (tenant bucket)]\n  D --> E\n  E --> F[DynamoDB Tenant Index]\n  F --> G[Audit Logs & DLQ]\n","difficulty":"intermediate","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T17:48:24.454Z","createdAt":"2026-01-14T17:48:24.454Z"},{"id":"q-2018","question":"Design an end-to-end, event-driven AWS pipeline for a regulated financial analytics service. Tenants upload encrypted JSON trade logs (up to 5 MB) to S3. Build per-tenant, region-isolated processing: decrypt with KMS, run a SageMaker multi-tenant analytics model, redact PII with Comprehend and regex, and store outputs in region-scoped S3 prefixes. Maintain an immutable audit trail in DynamoDB with versioning, implement retry and DLQ, and enforce strict IAM boundaries?","answer":"Propose a tenant-isolated, multi-region pipeline: S3 event -> Step Functions -> Lambda decrypt with KMS -> SageMaker multi-tenant analytics model -> PII redaction via Comprehend + regex -> outputs to ","explanation":"## Why This Is Asked\n\nTests the ability to design a robust, compliant, multi-region data pipeline with strong tenant isolation, privacy controls, and auditable lineage in AWS.\n\n## Key Concepts\n\n- Event-driven orchestration (Step Functions, Lambda, EventBridge)\n- Multi-tenant isolation (region prefixes, IAM scoping)\n- Data privacy (KMS encryption, Comprehend PII redaction)\n- Auditability (immutable DynamoDB with versioning, Streams for lineage)\n- Resilience (retries, DLQ, monitoring)\n\n## Code Example\n\n```python\n# Pseudo: decrypt with KMS, route to SageMaker\nimport boto3\nkms=boto3.client('kms')\n# decrypt logic placeholder\n```\n\n## Follow-up Questions\n\n- How would you test tenant isolation across regions?\n- How would you validate audit log integrity and versioning?","diagram":"flowchart TD\n  A[S3 Upload] --> B[Step Functions Start]\n  B --> C[Lambda: Decrypt (KMS)]\n  C --> D[SageMaker Analytics (Tenant)]\n  D --> E[PII Redaction (Comprehend/Regex)]\n  E --> F[Region S3 Prefix Output]\n  E --> G[Immutable Audit Trail (DynamoDB)]\n  G --> H[Monitoring & Retries]\n\n  A --> B\n  B --> C\n  C --> D\n  D --> E\n  E --> F\n  E --> G\n  G --> H","difficulty":"intermediate","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Discord","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T20:54:52.927Z","createdAt":"2026-01-14T20:54:52.927Z"},{"id":"q-2045","question":"Design a real-time, AWS-only PII redaction pipeline for streaming chat messages. Ingest messages enter regional Kinesis Data Streams with tenant isolation. Use SageMaker endpoint (or Comprehend) to detect PII, redact spans, store redacted messages in S3, and index per-tenant audit logs in DynamoDB (tenantId, messageId, detectedPII, redacted, timestamp). Include data flow, IAM, error handling, privacy controls, and rate-limiting?","answer":"In real-time, use regional Kinesis Data Streams for ingestion with tenant isolation, a Lambda function to invoke either a SageMaker PII detection endpoint or AWS Comprehend to identify sensitive spans, redact those spans inline, push sanitized events to S3 for storage, and upsert per-tenant audit logs in DynamoDB with fields for tenantId, messageId, detectedPII, redacted status, and timestamp. Implement proper IAM roles with least privilege, encryption at rest and in transit, error handling with dead-letter queues, and rate limiting to prevent service abuse.","explanation":"## Why This Is Asked\nAssesses the ability to design compliant, streaming AI workflows with robust privacy controls and multi-tenant architecture.\n\n## Key Concepts\n- Real-time streaming architecture with Kinesis Data Streams\n- PII detection using AWS Comprehend or custom SageMaker endpoints\n- In-flight redaction and audited persistence mechanisms\n- Multi-tenant data isolation with encryption at rest and in transit\n- IAM least privilege principles and security best practices\n- Comprehensive error handling, retry logic, and rate limiting strategies\n\n## Code Example\n```python\n# Lambda handler skeleton invoking SageMaker for PII detection and redaction\nimport json\nimport boto3\n\ndef detect_pii(text):\n    # Invoke SageMaker endpoint or Comprehend for PII detection\n    pass\n\ndef redact_content(text, pii_spans):\n    # Redact identified PII spans from content\n    pass\n\ndef handler(event, context):\n    for record in event['Records']:\n        payload = json.loads(record['kinesis']['data'])\n        pii_spans = detect_pii(payload['message'])\n        redacted_message = redact_content(payload['message'], pii_spans)\n        # Store redacted message and audit log\n        # Handle errors and implement rate limiting\n```","diagram":null,"difficulty":"intermediate","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Lyft","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T04:52:25.049Z","createdAt":"2026-01-14T21:49:22.754Z"},{"id":"q-2075","question":"Design a real-time fraud detection pipeline for a high-volume platform used by Lyft, PayPal, and Snap. Ingest payment events into Kinesis, enrich with per-tenant risk data from DynamoDB via Lambda, compute a fraud score with a SageMaker endpoint, and enforce decisions in DynamoDB while routing high-risk events to a Step Functions human-in-the-loop workflow and notifying security via SNS. Include data isolation, sub-200 ms latency budget, and privacy controls with PII masking and audit trails?","answer":"Design a real-time fraud detection pipeline: ingest payment events into Kinesis, enrich with per-tenant risk data from DynamoDB via Lambda, compute fraud scores using a SageMaker endpoint, enforce decisions in DynamoDB while routing high-risk events to a Step Functions human-in-the-loop workflow, and notify security teams via SNS. Implement data isolation through tenant-specific DynamoDB tables, maintain sub-200ms latency through optimized Lambda functions and provisioned SageMaker endpoints, and ensure privacy compliance with PII masking, encryption at rest, and comprehensive audit trails.","explanation":"## Why This Is Asked\nTests ability to design a low-latency, multitenant fraud pipeline using AWS AI services, with clear data flow, fault tolerance, and privacy controls.\n\n## Key Concepts\n- Event-driven architectures across Kinesis, Lambda, and Step Functions\n- Real-time scoring with SageMaker endpoints\n- Per-tenant data isolation in DynamoDB\n- Privacy: PII masking, encryption at rest, audit trails\n\n## Code Example\n\n```javascript\n// Lambda enrichment skeleton\nconst AWS = require('aws-sdk');\nconst dynamo = new AWS.DynamoDB.DocumentClient();\n\nexports.handler = async (event) => {\n  // parse event\n```","diagram":"flowchart TD\n  A[Payment Event] --> B[Kinesis Stream]\n  B --> C[Lambda Enrich]\n  C --> D[SageMaker FraudScore]\n  D --> E{Threshold?}\n  E -- Yes --> F[Block/Flag in DynamoDB; SNS]\n  E -- No --> G[Proceed downstream]\n  F --> H[Audit Log & PII Masking]","difficulty":"advanced","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","PayPal","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T05:33:11.408Z","createdAt":"2026-01-14T23:27:14.074Z"},{"id":"q-2175","question":"You're building a real-time, AWS-native, multi-tenant text inference service for content safety at scale. Incoming messages arrive with tenant context and must be isolated either via per-tenant endpoints or tenant-scoped routing. Describe architecture for throughput, deployment strategy (canary/rolling), privacy controls (encryption, PII redaction), drift monitoring, data retention, and cost governance. Include data flow, IAM, and observability?","answer":"Describe an AWS-native, multi-tenant, real-time text inference pipeline for content safety. Messages include tenant context and must be isolated either via per-tenant endpoints or tenant-scoped routin","explanation":"## Why This Is Asked\nAssesses design of scalable, secure, multi-tenant ML inference on AWS with real-time requirements, isolation guarantees, privacy controls, and cost governance.\n\n## Key Concepts\n- Tenancy isolation strategies (per-tenant vs shared endpoints with routing)\n- Real-time inference flow (API Gateway, Kinesis/EventBridge, SageMaker)\n- Privacy controls (KMS, encryption at rest/in transit, PII redaction)\n- Deployment discipline (canary, rolling updates, model monitoring)\n- Observability and cost governance\n\n## Code Example\n```javascript\n// Pseudo-CDK snippet: tenant-aware SageMaker endpoint and routing placeholder\n```\n\n## Follow-up Questions\n- How would you verify tenant isolation under burst traffic?\n- What tests validate drift detection and rollback safety?","diagram":"flowchart TD\n  A[Client Message] --> B[API Gateway]\n  B --> C[Kinesis Data Stream]\n  C --> D[SageMaker Inference Endpoint (tenant-aware)]\n  D --> E[PII Redaction / Labeling]\n  E --> F[S3: raw + redacted]\n  F --> G[DynamoDB: Tenant Index]\n  G --> H[CloudWatch / OpenTelemetry]\n","difficulty":"advanced","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","Meta","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T06:45:09.337Z","createdAt":"2026-01-15T06:45:09.339Z"},{"id":"q-2208","question":"You're building a privacy-preserving, multi-tenant receipt processor on AWS. Clients upload receipts (PDF/JPG up to 5MB) to S3. Design an event-driven flow: Textract extracts fields, a SageMaker endpoint classifies line items, redact PII, and write structured data to DynamoDB with tenant isolation. Include data flow, IAM, SSE-KMS, retries, drift monitoring, audit logs, privacy controls, and testing strategy?","answer":"Design a Step Functions workflow triggered by S3 events: Textract OCR to extract date, total, vendor; SageMaker to classify items; a Lambda for PII redaction; store structured data in DynamoDB with te","explanation":"## Why This Is Asked\nTests end-to-end design of a real-world, privacy-conscious data pipeline with multi-tenant isolation and production-ready concerns.\n\n## Key Concepts\n- Event-driven orchestration (S3, Step Functions, Lambda)\n- Textract for extraction, SageMaker for classification\n- PII redaction and data minimization\n- Tenant isolation in DynamoDB and per-tenant encryption (SSE-KMS)\n- Retries, DLQ, drift monitoring, auditing, privacy controls\n\n## Code Example\n```javascript\n// Pseudocode: State machine skeleton\n{\n  \"Comment\": \"Receipt workflow\",\n  \"StartAt\": \"Textract\",\n  \"States\": {\n    /* ... */\n  }\n}\n```\n\n## Follow-up Questions\n- How would you test per-tenant data isolation and drift detection?\n- What metrics and alarms would you configure for reliability and privacy governance?","diagram":"flowchart TD\n  A[S3 Receipt Upload] --> B[Trigger Step Functions]\n  B --> C[Textract OCR]\n  C --> D[SageMaker Classifier]\n  D --> E[PII Redaction]\n  E --> F[DynamoDB Store]\n  F --> G[Audit Logs (S3/CloudWatch)]","difficulty":"intermediate","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","DoorDash","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T07:37:40.086Z","createdAt":"2026-01-15T07:37:40.086Z"},{"id":"q-2387","question":"You're processing PDFs uploaded to S3 (up to 5 MB). Design an event-driven AWS pipeline that uses Textract to extract text and tables, Comprehend to identify keywords, stores outputs in S3, and writes a compact index to DynamoDB with fields like documentId, s3Key, textSnippet, phrases. Include IAM, error handling, and privacy considerations?","answer":"Trigger on S3 upload of a PDF (<=5 MB). A Lambda orchestrates: Textract AnalyzeDocument to extract text/tables, store plain text in S3, then Comprehend KeyPhrases to build a keyword list. Upsert a Dyn","explanation":"## Why This Is Asked\nAssesses understanding of event-driven AWS AI services integration, data flow, and privacy.\n\n## Key Concepts\n- S3 event triggers\n- Textract AnalyzeDocument\n- Comprehend KeyPhrases\n- DynamoDB indexing\n- Privacy: encryption, PII redaction\n\n## Code Example\n```javascript\nconst AWS = require('aws-sdk');\nconst textract = new AWS.Textract();\n// skeleton: extract, store, and index\n```\n\n## Follow-up Questions\n- How would you test the pipeline end-to-end?\n- How would you handle large PDFs or sensitive data? ","diagram":"flowchart TD\n  S3Input[/PDF uploaded to S3/] --> Lambda[Lambda: Orchestrator]\n  Lambda --> Textract[Textract: AnalyzeDocument]\n  Textract --> TextStore[Store Text in S3]\n  Lambda --> Comprehend[Comprehend: KeyPhrases]\n  Comprehend --> DB[DynamoDB: docIndex]\n  Lambda --> DLQ[SQS DLQ]\n  Textract --> DLQ\n  Comprehend --> DLQ","difficulty":"beginner","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","MongoDB","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T15:51:12.405Z","createdAt":"2026-01-15T15:51:12.406Z"},{"id":"q-2445","question":"You're designing a real-time, multi-tenant chat moderation pipeline on AWS. Ingest messages from multiple tenants via Kinesis Data Streams, score content with a versioned SageMaker endpoint, redact PII, and persist per-tenant results to DynamoDB with TTL, while archiving raw and redacted messages to S3. Design end-to-end architecture with latency target ~150 ms, strong data isolation, model versioning, encryption, and cost controls; include IAM roles, VPC endpoints, and monitoring?","answer":"Architect a streaming, multi-tenant moderation flow: Kinesis ingests messages, SageMaker endpoint scores content (versioned models), PII redaction occurs before storage, DynamoDB stores per-tenant res","explanation":"## Why This Is Asked\nExplores multi-tenant data isolation, real-time latency, and policy governance in production-grade AI pipelines beyond prior questions.\n\n## Key Concepts\n- Real-time streaming with Kinesis Data Streams\n- SageMaker model versioning and endpoints\n- Per-tenant isolation in DynamoDB with TTL and encryption\n- PII redaction before archival in S3\n- IAM, KMS, VPC endpoints, and cost-monitoring\n\n## Code Example\n```javascript\nconst AWS = require('aws-sdk');\nconst sage = new AWS.SageMakerRuntime();\nasync function score(msg) {\n  const res = await sage.invokeEndpoint({\n    EndpointName: 'moderation-v1',\n    ContentType: 'text/plain',\n    Body: msg\n  }).promise();\n  return res.Body.toString();\n}\n```\n\n## Follow-up Questions\n- How would you test tenant isolation and data leakage risks?\n- Describe a canary deployment strategy for new model versions in SageMaker.\n- What monitoring dashboards and SLOs would you implement for latency and accuracy?\n- How would you handle model drift and feature changes across tenants?","diagram":"flowchart TD\n  A[Kinesis Streams] --> B[SageMaker Endpoint]\n  B --> C[DynamoDB (per-tenant)]\n  A --> D[S3 Raw/Redacted Archive]\n  C --> E[Audit & Compliance]\n  D --> E","difficulty":"advanced","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T18:50:57.992Z","createdAt":"2026-01-15T18:50:57.992Z"},{"id":"q-951","question":"You're building a low‑ops internal voice assistant for support scripts. Audio files up to 60 seconds are uploaded to S3. Design an AWS‑only pipeline that transcribes, analyzes sentiment, and stores a brief summary plus an index in DynamoDB, with minimal cost and ops. Include data flow, services, error handling, and privacy considerations?","answer":"Use an S3 trigger to start a Step Functions workflow: Transcribe for 60s, then Comprehend for sentiment, store transcript + summary in DynamoDB, and index in a separate small table. Add a compact S3 m","explanation":"## Why This Is Asked\n\nTests ability to design a cost‑aware, low‑ops AWS data pipeline that stitches AI services together with serverless orchestration.\n\n## Key Concepts\n\n- AWS services: S3, Step Functions, Transcribe, Comprehend, DynamoDB\n- Serverless orchestration with error handling and retries\n- Data privacy: encryption, access control, least privilege\n\n## Code Example\n\n```javascript\n// Example AWS CDK snippet (TypeScript) configuring a Step Function state machine trigger\n```\n\n## Follow-up Questions\n\n- How would you scale the pipeline for higher concurrency?\n- How would you monitor latency and alert on failures?","diagram":null,"difficulty":"beginner","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T16:38:46.546Z","createdAt":"2026-01-12T16:38:46.546Z"}],"subChannels":["general"],"companies":["Adobe","Airbnb","Amazon","Anthropic","Bloomberg","Citadel","Cloudflare","Coinbase","Databricks","Discord","DoorDash","Goldman Sachs","Google","Hashicorp","Hugging Face","Instacart","LinkedIn","Lyft","Meta","MongoDB","NVIDIA","Netflix","OpenAI","Oracle","PayPal","Plaid","Robinhood","Salesforce","Scale Ai","Snap","Snowflake","Square","Stripe","Tesla","Twitter","Uber","Zoom"],"stats":{"total":26,"beginner":8,"intermediate":9,"advanced":9,"newThisWeek":26}}