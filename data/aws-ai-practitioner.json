{"questions":[{"id":"q-1001","question":"You're building a beginner-friendly AWS-only pipeline to ingest customer chat transcripts (text files up to 50 KB) uploaded to S3. Design how to automatically redact PII using Amazon Comprehend PII detection, store the redacted transcript back to S3, and index metadata in DynamoDB. Include data flow, IAM permissions, error handling, and privacy considerations?","answer":"Two-bucket flow: uploads go to `transcript-uploads`; a Lambda triggered by S3 reads text, runs `Comprehend.detectPiiEntities` to identify PII, replaces with `[REDACTED]`, writes redacted text to `tran","explanation":"## Why This Is Asked\nTests ability to design a secure, cost-conscious AWS-native pipeline that handles PII responsibly using services like S3, Lambda, Comprehend, and DynamoDB, with proper error handling and auditing.\n\n## Key Concepts\n- PII detection: Amazon Comprehend PII entities\n- Data flow: S3 -> Lambda -> S3 (redacted) -> DynamoDB\n- Privacy controls: encryption at rest (SSE/KMS), data minimization\n- Reliability: dead-letter queues (DLQ), retries, idempotent processing, logging\n\n## Code Example\n```javascript\nconst AWS = require('aws-sdk');\nconst s3 = new AWS.S3();\nconst comprehend = new AWS.Comprehend({region: 'us-east-1'});\nasync function redact(text) {\n  const res = await comprehend.detectPiiEntities({ Text: text, LanguageCode: 'en' }).promise();\n  let redacted = text;\n  // naive approach: replace ranges from end to start to avoid offset shifts\n  const ranges = res.Entities.map(e => ({ s: e.BeginOffset, e: e.EndOffset }));\n  ranges.sort((a,b) => b.s - a.s);\n  for (const r of ranges) {\n    redacted = redacted.substring(0, r.s) + '[REDACTED]' + redacted.substring(r.e);\n  }\n  return redacted;\n}\n```\n\n## Follow-up Questions\n- How would you validate redaction accuracy and handle false positives/negatives? \n- How would you adapt this for higher throughput or multilingual transcripts?","diagram":null,"difficulty":"beginner","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Snowflake","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T18:46:28.747Z","createdAt":"2026-01-12T18:46:28.747Z"},{"id":"q-1058","question":"Design a cross region, multi account AI inference platform for real time pricing and risk scoring in a fintech setting. Ingest streaming data, enforce per tenant data residency, and meet sub 100 ms latency. Describe data flow, services, IAM boundaries, model registry, feature store, drift monitoring, error handling, and cost controls?","answer":"Use a multi region, multi account pattern: stream data via Kinesis to region local Lambda preprocessors, push features to SageMaker Feature Store, and serve models with regional SageMaker endpoints be","explanation":"## Why This Is Asked\nExplores a candidate's ability to design scalable, compliant AI infra across AWS accounts and regions, ensuring tenancy isolation, latency targets, and governance.\n\n## Key Concepts\n- Multi account governance and cross region dataflow\n- SageMaker Feature Store and versioned model registry\n- PrivateLink, per tenant IAM, and API Gateway routing\n- Drift monitoring, error handling, and cost controls\n- Data residency via SCPs and audit trails via CloudTrail/Config\n\n## Code Example\n```javascript\n// Pseudo high level manifest of dataflow and services\nconst flow = [\n  'Kinesis -> Lambda preprocess',\n  'Feature Store write',\n  'Regional SageMaker endoints -> Tenant API',\n  'Step Functions orchestration',\n  'CloudTrail + Config for audit'\n];\n```\n\n## Follow-up Questions\n- How would you implement feature drift detection across regions?\n- What are the security implications of cross account model sharing and how would you mitigate them?","diagram":null,"difficulty":"advanced","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","NVIDIA","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T21:19:24.738Z","createdAt":"2026-01-12T21:19:24.738Z"},{"id":"q-1259","question":"Design an end-to-end AWS-native real-time fraud detection pipeline for a global e-commerce platform. Ingest event streams (Kinesis Data Streams), redact PII, create real-time features stored in SageMaker Feature Store (online) and offline store, with governance, lineage, access control, and cost constraints. Include data flow, IAM, retry logic, backpressure, testing, and incident response?","answer":"Propose an AWS-native real-time fraud pipeline: ingest events with Kinesis Data Streams, redact PII in-stream (Lambda or Kinesis Data Analytics) and publish redacted records to SageMaker Feature Store","explanation":"## Why This Is Asked\n\nTests real-time data flow and governance across streaming, feature store, and model scoring, plus privacy requirements.\n\n## Key Concepts\n\n- Kinesis Data Streams\n- SageMaker Feature Store (online/offline)\n- PII redaction in streaming\n- Data lineage and governance (Glue Data Catalog)\n- IAM, KMS, encryption at rest/in transit\n- Backpressure, retries, circuit breakers\n\n## Code Example\n\n```python\nimport boto3\n\ndef redact_pii(record):\n    # placeholder: call to Comprehend or regex\n    return record  # simplified\n```\n\n## Follow-up Questions\n\n- How would you test data drift in the feature store over time?\n- How would you handle schema evolution for features?\n","diagram":null,"difficulty":"advanced","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Cloudflare","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T07:24:57.938Z","createdAt":"2026-01-13T07:24:57.938Z"},{"id":"q-1292","question":"Design a real-time fraud-detection pipeline on AWS for a FinTech use-case. Ingest streaming transactions via Kinesis Data Streams, preprocess with Lambda, and invoke a SageMaker endpoint for real-time scores. Persist results to DynamoDB with audit logs in S3. Address latency (<200 ms), data privacy (KMS, VPC endpoints), IAM, drift monitoring, error handling, and cost control. Provide concrete components and trade-offs?","answer":"Flow: Ingest streaming transactions via Kinesis Data Streams; preprocess in Lambda; invoke a SageMaker endpoint for real-time fraud scores; persist results to DynamoDB and archive logs to S3. Security","explanation":"## Why This Is Asked\nTests real-world AWS AI deployment decisions: low latency, security, and governance in a streaming inference path.\n\n## Key Concepts\n- Streaming ingestion (Kinesis), serverless preprocessing (Lambda), model hosting (SageMaker), persistent storage (DynamoDB, S3).\n- Privacy: KMS, VPC endpoints, IAM least-privilege.\n- Observability: CloudWatch metrics, drift monitoring, retry/backoff.\n\n## Code Example\n```javascript\n// pseudo: Lambda handler invoked by Kinesis; calls SageMaker endpoint and writes to DynamoDB\n```\n\n## Follow-up Questions\n- How would you implement per-customer data isolation in this flow?\n- What failure modes require DLQ routing and circuit breakers?","diagram":"flowchart TD\n  A[Kinesis Data Streams] -->|Preprocess via Lambda| B[Lambda Preprocessing]\n  B -->|SageMaker Inference| C[SageMaker Endpoint]\n  C -->|Store in| D[DynamoDB]\n  C -->|Archive logs to| E[S3 Logs]","difficulty":"intermediate","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Coinbase"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T08:37:56.616Z","createdAt":"2026-01-13T08:37:56.616Z"},{"id":"q-1313","question":"You're designing a beginner-friendly AWS-only pipeline for user-submitted PDFs (max 5 MB) uploaded to S3. Build an end-to-end workflow that uses Textract to extract text, runs a lightweight topic model via Comprehend or a tiny SageMaker endpoint to derive a topic label, stores a compact summary and a searchable index in DynamoDB, and exposes a read path via API Gateway + Lambda. Include data flow, IAM roles, error handling, and privacy considerations?","answer":"Set up an event-driven pipeline: S3 upload to incoming-pdfs/, Lambda triggers Textract for text extraction, and then sends the text to Comprehend (or a small SageMaker model) for topic labeling. Persi","explanation":"## Why This Is Asked\nTests multi-service wiring (Textract, Comprehend/SageMaker, DynamoDB, API Gateway) and basic privacy, cost, and error handling.\n\n## Key Concepts\n- AWS Textract for PDF text extraction\n- Comprehend or SageMaker for simple topic modeling\n- DynamoDB for metadata index\n- API Gateway + Lambda for reads\n- IAM least privilege, KMS, retry, DLQ\n\n## Code Example\n```python\n# Lambda pseudo-handler sketch\ndef handler(event, context):\n  # parse S3 event, call Textract, then Comprehend/SageMaker, store DynamoDB item\n  return {\"status\": \"ok\"}\n```\n\n## Follow-up Questions\n- How would you handle large PDFs exceeding 5 MB?\n- How would you test data privacy controls in this flow?","diagram":"flowchart TD\n  A[S3 Upload] --> B[Lambda Trigger]\n  B --> C[Textract]\n  C --> D[Topic Label (Comprehend/SageMaker)]\n  D --> E[DynamoDB (summary/index)]\n  E --> F[API Gateway / Lambda read path]","difficulty":"beginner","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T10:38:19.119Z","createdAt":"2026-01-13T10:38:19.119Z"},{"id":"q-1366","question":"You're operating a real-time text classifier API on a SageMaker hosting endpoint behind API Gateway. You need a canary deployment strategy using CodeDeploy for SageMaker endpoints, with 2% of traffic to the new version, ramping to 50% over 2 hours, then full if no regressions. Describe data flow, required services, IAM, drift detection via Model Monitor, and rollback/failover logic, plus privacy considerations?","answer":"Use a canary deployment with CodeDeploy for SageMaker endpoints: two production variants, 2% traffic to v2, then gradual ramp to 50% over 2 hours. Monitor drift with Model Monitor and live metrics; re","explanation":"## Why This Is Asked\nThis question probes real-world deployment discipline: canary rollouts, automated rollback, and monitoring for ML endpoints in production.\n\n## Key Concepts\n- Canary deployments with CodeDeploy for SageMaker endpoints\n- EndpointVariant traffic shifting and health checks\n- Model Monitor drift detection and performance metrics\n- Automated rollback, alarms, and observability\n- IAM least-privilege and privacy considerations\n\n## Code Example\n```yaml\n# AppSpec for SageMaker canary deployment (illustrative)\nversion: 0.0\nResources:\n  SageMakerEndpoint:\n    Type: AWS::SageMaker::Endpoint\n    Properties:\n      EndpointConfigName: MyEndpointConfig\n```\n\n## Follow-up Questions\n- How would you simulate drift in CI/CD?\n- How would you extend to multi-region endpoints and cross-account monitoring?\n","diagram":"flowchart TD\n  A[Client Request] --> B(API Gateway)\n  B --> C[SageMaker Endpoint: v1 Production]\n  B --> D[SageMaker Endpoint: v2 Canary]\n  D --> E[Model Monitor & CloudWatch]\n  E --> F{Drift/Performance OK?}\n  F -- Yes --> G[CodeDeploy traffic shift]\n  F -- No --> H[Rollback to v1]\n  G --> I[Promote to Production when ready]","difficulty":"intermediate","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["NVIDIA","Netflix","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T13:19:25.324Z","createdAt":"2026-01-13T13:19:25.324Z"},{"id":"q-1437","question":"You're building a beginner-friendly, AWS‑only pipeline to process user-submitted emails stored as JSON in S3 (max 20 KB per file). Design an end-to-end workflow that detects language, translates to English, analyzes sentiment, and stores results in DynamoDB, with a simple read path via API Gateway + Lambda. Include data flow, services, error handling, IAM roles, and privacy considerations?","answer":"Propose an AWS-only pipeline: S3 triggers Lambda to enqueue processing; Step Functions coordinates Translate, Comprehend (DetectDominantLanguage and DetectSentiment) and a DynamoDB write; expose a rea","explanation":"## Why This Is Asked\n\nThis asks for a practical, beginner-friendly AWS AI/ML workflow that combines text detection, translation, sentiment, storage, and a lightweight read API, while emphasizing security and cost.\n\n## Key Concepts\n\n- DetectDominantLanguage (Comprehend)\n- TranslateText (Translate)\n- DetectSentiment (Comprehend)\n- S3 → Lambda → Step Functions orchestration\n- DynamoDB indexing by documentId\n- API Gateway + Lambda for reads\n- IAM least privilege and KMS\n- Privacy: PII masking, encryption, and retention\n\n## Code Example\n\n```javascript\nimport { ComprehendClient, DetectDominantLanguageCommand, DetectSentimentCommand } from \"@aws-sdk/client-comprehend\";\nimport { TranslateClient, TranslateTextCommand } from \"@aws-sdk/client-translate\";\n\nconst comprehend = new ComprehendClient({region:\"us-east-1\"});\nconst translate = new TranslateClient({region:\"us-east-1\"});\n\nasync function analyzeText(text){\n  const dl = await comprehend.send(new DetectDominantLanguageCommand({Text: text}));\n  const langCode = (dl.Languages?.[0]?.LanguageCode) ?? \"en\";\n  let english = text;\n  if (langCode !== \"en\"){\n    const tr = await translate.send(new TranslateTextCommand({Text: text, SourceLanguageCode: langCode, TargetLanguageCode: \"en\"}));\n    english = tr.TranslatedText;\n  }\n  const sentiment = await comprehend.send(new DetectSentimentCommand({Text: english, LanguageCode: \"en\"}));\n  return { language: langCode, translated: english, sentiment: sentiment.Sentiment };\n}\n``` \n\n## Follow-up Questions\n\n- How would you test this with mocks and local development?\n- What are the implications for cost at scale and how would you mitigate?\n- How would you handle a batch of emails arriving concurrently?","diagram":null,"difficulty":"beginner","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","Robinhood","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T16:58:46.574Z","createdAt":"2026-01-13T16:58:46.574Z"},{"id":"q-1460","question":"Design an advanced, low-latency fraud-detection pipeline for real-time payments in AWS. Ingest events via Kinesis Data Streams, derive features into SageMaker Feature Store, train/deploy with SageMaker Pipelines, score via a real-time SageMaker Endpoint called from Lambda, and log audits in CloudTrail. Enforce KMS encryption, data minimization/retention policies, IAM least privilege, and a rollback/failover plan. Include data flow, IAM, privacy, and failure handling?","answer":"Ingest via Kinesis Data Streams; features in SageMaker Feature Store; train/deploy with SageMaker Pipelines; real-time scoring via SageMaker Endpoint invoked by Lambda; store results in DynamoDB; audi","explanation":"## Why This Is Asked\nTests real-time ML deployment, feature store usage, privacy, and operational resilience in a regulated fintech context.\n\n## Key Concepts\n- Kinesis Data Streams, SageMaker Feature Store, SageMaker Pipelines\n- SageMaker endpoints, Lambda integration\n- CloudTrail auditing, KMS encryption, IAM least privilege\n- Data minimization, retention, drift detection, rollback\n\n## Code Example\n```python\nimport boto3, json\nsm = boto3.client('sagemaker-runtime')\ndef lambda_handler(event, context):\n    payload = json.dumps(event['payload'])\n    resp = sm.invoke_endpoint(EndpointName=event['endpoint'], ContentType='application/json', Body=payload)\n    return json.loads(resp['Body'].read())\n```\n\n## Follow-up Questions\n- How would you handle feature drift in production? \n- How to test end-to-end latency under peak load? \n- How would you enforce data retention across services?","diagram":"flowchart TD\n  A[Ingest events] --> B[Kinesis Data Streams]\n  B --> C[SageMaker Feature Store (online)]\n  C --> D[SageMaker Pipelines (train & deploy)]\n  D --> E[SageMaker Real-time Endpoint]\n  E --> F[Lambda Scoring]\n  F --> G[CloudWatch + SageMaker Model Monitor]\n  G --> H[CloudTrail Auditing]","difficulty":"advanced","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T17:56:45.234Z","createdAt":"2026-01-13T17:56:45.234Z"},{"id":"q-1482","question":"Design a production-ready, AWS-native content moderation pipeline for a multi-tenant SaaS platform handling images and captions uploaded to S3. Must route per-tenant policies to a custom SageMaker multi-model endpoint, apply per-tenant threshold overrides, enforce data isolation, log audit trails, and keep costs predictable with auto-scaling and caching. Describe data flow, IAM, encryption, error handling, and privacy implications?","answer":"Use a two-branch flow: images to a SageMaker multi-model endpoint and captions to a Lambda-accelerated text classifier; tenant policies in DynamoDB drive per-tenant thresholds and endpoint routing via","explanation":"## Why This Is Asked\nAssess ability to design a scalable, multi-tenant AI moderation pipeline on AWS with policy-driven routing, data isolation, and compliant audit logging.\n\n## Key Concepts\n- SageMaker multi-model endpoints and per-tenant routing\n- DynamoDB for policy storage; S3 for audit logs and artifacts\n- Tenant isolation via prefixes, IAM scoping, KMS encryption\n- Step Functions for orchestration, error handling, retries\n- Cost controls: autoscaling, model cache, off-peak scheduling\n\n## Code Example\n```javascript\n// Pseudo-routing logic for tenant-aware inference\nfunction routeToEndpoint(tenantId, itemType) {\n  const policy = fetchPolicy(tenantId)\n  if (itemType === 'image') return { endpoint: policy.imageEndpoint, threshold: policy.imageThreshold }\n  return { endpoint: policy.textEndpoint, threshold: policy.textThreshold }\n}\n```\n\n## Follow-up Questions\n- How would you test tenant policy overrides without affecting others?\n- How would you enforce data deletion and retention across S3 and DynamoDB?","diagram":"flowchart TD\n  A[Upload to S3 (tenantId)] --> B[EventBridge/SFN]\n  B --> C{Content Type}\n  C --> D[SageMaker multi-model endpoint]\n  C --> E[Text classifier]\n  D & E --> F[Apply per-tenant policy]\n  F --> G[Store result in DynamoDB]\n  G --> H[Audit log in S3]","difficulty":"advanced","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Cloudflare","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T18:54:50.921Z","createdAt":"2026-01-13T18:54:50.921Z"},{"id":"q-1552","question":"You're deploying a privacy-preserving AI rating model for financial support tickets on AWS. Incoming tickets arrive as JSON (up to 10 KB) in S3; design an end-to-end pipeline that uses SageMaker for inference, writes per-ticket audit logs to DynamoDB, encrypts data at rest with SSE-KMS, and supports reliable, rate-limited processing with error handling and drift monitoring. How would you implement this?","answer":"Implement S3 event triggers to invoke Lambda functions that call SageMaker endpoints for inference; maintain comprehensive audit logs in DynamoDB with ticket_id, input hash, model_version, latency, and outcome; enforce SSE-KMS encryption across S3 and DynamoDB; incorporate Dead Letter Queues for reliable error handling with exponential backoff and rate limiting.","explanation":"## Why This Is Asked\nThis question evaluates expertise in designing end-to-end privacy-preserving ML workflows on AWS, with emphasis on security implementation (SSE-KMS), system reliability (DLQs, exponential backoff), governance compliance (audit logging), and cost optimization (autoscaling controls).\n\n## Key Concepts\n- S3 event-driven data ingestion and Lambda orchestration\n- SageMaker endpoints for real-time ML inference\n- DynamoDB for comprehensive audit trail with unique identifiers\n- SSE-KMS encryption implementation across all AWS services\n- Robust error handling with DLQs, backoff strategies, and rate limiting","diagram":"flowchart TD\n  A[Tickets arrive in S3] --> B[S3 Event -> Lambda]\n  B --> C[SageMaker Inference]\n  C --> D[DynamoDB Audit]\n  D --> E[Model Monitor/Alarms]","difficulty":"intermediate","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["PayPal","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T06:29:47.696Z","createdAt":"2026-01-13T21:40:06.374Z"},{"id":"q-1583","question":"You're running a real-time fraud-detection model for payments. Data arrives in a Kinesis stream; design an end-to-end AWS-native pipeline that performs per-record inference, monitors drift and bias with SageMaker Clarify/Model Monitor, archives data in S3 with metadata, and auto-triggers a rollback to a previous model version via Step Functions if drift thresholds are exceeded. Include data flow, IAM, encryption, backpressure handling, and privacy considerations?","answer":"Per-record inference via a SageMaker endpoint fed by Kinesis streams, a Lambda transform layer to normalize features, Model Monitor to detect data and label drift, and Clarify for bias auditing; archive raw and processed data in S3 with SSE-KMS encryption, store metadata in DynamoDB, implement Step Functions for automated rollback with exponential backoff, handle backpressure through Kinesis enhanced fan-out and Lambda provisioned concurrency, and ensure privacy through data masking and PII detection.","explanation":"## Why This Is Asked\nAssessment of end-to-end ML pipelines on AWS, with drift detection, governance, and safe rollback.\n\n## Key Concepts\n- Real-time inference integration (Kinesis + SageMaker Endpoint)\n- Drift and bias monitoring (Model Monitor, Clarify)\n- Data archiving and metadata (S3 with SSE-KMS, DynamoDB)\n- Automated rollback (Step Functions) and retry/backoff\n\n## Code Example\n```javascript\n// Example: Lambda to normalize features before SageMaker inference\nconst AWS = require('aws-sdk');\nconst sage = new AWS.SageMakerRuntime();\nexports.handler = async (event) => {\n  // transform feature\n```","diagram":"flowchart TD\n  Kinesis[Data Stream] --> Lambda[Preprocess]\n  Lambda --> Sage[Inference Endpoint]\n  Sage --> DynamoDB[Prediction Logs]\n  Sage --> S3[Data Archive]\n  SageMonitor[SageMaker Model Monitor] --> Drift[Drift Alert]\n  Drift --> SF[Step Functions]\n  SF --> Rollback[Rollback to previous model]\n  Rollback --> Sage[Endpoint Redeploy]","difficulty":"intermediate","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Anthropic","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T05:57:38.931Z","createdAt":"2026-01-13T22:50:38.700Z"},{"id":"q-1627","question":"You're building an AWS-native ML platform that serves a global analytics product with data residing in Snowflake and Databricks across regions. Design a pipeline that ingests from both sources, uses SageMaker Feature Store, trains/registers with a human approval, serves real-time inference, and uses HashiCorp Vault for secrets. Include data lineage, IAM, encryption, drift monitoring, rollback policy, and privacy considerations?","answer":"Design a cross-region AWS pipeline that ingests batch data from Snowflake and Databricks into S3, feeds features to SageMaker Feature Store, trains and registers models with an approval step, deploys ","explanation":"## Why This Is Asked\nThis question tests cross-cloud data plumbing, security, and production-grade ML governance in a realistic, multi-region setup.\n\n## Key Concepts\n- Cross-region ingestion from Snowflake and Databricks\n- SageMaker Feature Store and Model Registry with human approvals\n- HashiCorp Vault integration and cross-account IAM\n- Data lineage, encryption, privacy controls, and auditability\n- Drift monitoring and automated rollback\n\n## Code Example\n```python\n# Pseudo: orchestrate fetch from sources, store features, train, register, and deploy with approval\n```\n\n## Follow-up Questions\n- How would you validate data provenance across Snowflake/Databricks?\n- How would you test rollback and ensure reproducibility?","diagram":"flowchart TD\n  S[Snowflake] --> D1[(Raw Data S3)]\n  D1 --> F[SageMaker Feature Store]\n  Databricks[Databricks] --> D1\n  F --> T[Training Job]\n  T --> R[Model Registry]\n  R --> E[Endpoint]\n  Vault(HashiCorp Vault) --> Secrets[Secrets Management]\n  Drift[Drift Monitor] --> Roll[Rollback Trigger]","difficulty":"advanced","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Hashicorp","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T04:14:53.187Z","createdAt":"2026-01-14T04:14:53.187Z"},{"id":"q-1750","question":"You're building a beginner-friendly AWS-only pipeline for multilingual customer recordings. Audio files (up to 90 seconds) are uploaded to S3. Design an event-driven flow that uses Amazon Transcribe to produce a transcript in the source language, Amazon Translate to produce a Spanish version, stores both transcripts in S3, and writes a compact index in DynamoDB with fields like customerId, fileKey, sourceLang, translatedKey. Include data flow, IAM, error handling, and privacy considerations?","answer":"Design an event-driven AWS pipeline: S3 upload triggers a workflow (Step Functions or Lambda) that starts Transcribe for the source transcript, calls Translate to generate a Spanish version, writes bo","explanation":"## Why This Is Asked\n\nAssesses practical use of AWS AI services (Transcribe, Translate) in a beginner-friendly, cost-conscious pipeline with data governance and privacy considerations.\n\n## Key Concepts\n\n- Amazon Transcribe for speech-to-text\n- Amazon Translate for multilingual text\n- S3 event-driven processing\n- DynamoDB indexing for quick lookups\n- IAM permissions, encryption, and retry strategies\n\n## Code Example\n\n```javascript\n// Pseudo Lambda orchestrating Transcribe and Translate\nexports.handler = async (event) => {\n  const fileKey = event.Records[0].s3.object.key;\n  // 1) start Transcribe job on the audio\n  // 2) fetch transcript, call Translate to Spanish\n  // 3) upload both transcripts to S3\n  // 4) write DynamoDB index with metadata\n  // 5) handle errors and retries\n};\n```\n\n## Follow-up Questions\n\n- How would you handle long-running transcripts and batching?\n- What changes for multi-language translation beyond Spanish?\n- How would you enforce data privacy (encryption, access controls, vendor logs)?","diagram":"flowchart TD\n  A[S3 Upload] --> B[Event Trigger]\n  B --> C[Transcribe Job]\n  C --> D[Transcript Text]\n  D --> E[Translate to Spanish]\n  E --> F[Translate Text]\n  C --> G[Original Transcript S3]\n  F --> H[Translated Transcript S3]\n  G --> I[DynamoDB Index]\n  H --> I\n","difficulty":"beginner","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T09:39:05.334Z","createdAt":"2026-01-14T09:39:05.334Z"},{"id":"q-1789","question":"Design an AWS-native, real-time fraud-detection pipeline: ingest streaming transactions from Kinesis Data Streams, score risk with a SageMaker Inference endpoint (Transformer-based), store scores in DynamoDB, and trigger a Step Functions workflow for high-risk events. Include data flow, IAM, privacy controls, drift monitoring, and cost governance?","answer":"Real-time fraud-detection pipeline using Kinesis -> SageMaker Inference endpoint -> DynamoDB table with transactionId and score, plus a Step Functions workflow for investigations when score exceeds a ","explanation":"## Why This Is Asked\nThis question evaluates building a low-latency, compliant fraud-detection pipeline with streaming data, model hosting, and integration patterns.\n\n## Key Concepts\n- Real-time ingestion with Kinesis Data Streams\n- SageMaker Inference endpoints (transformer-based models)\n- DynamoDB data modeling and TTL\n- Step Functions orchestration and alerts\n- IAM, KMS, and privacy controls\n- Model drift monitoring (SageMaker Model Monitor)\n- Cost governance and autoscaling\n\n## Code Example\n```javascript\nconst AWS = require('aws-sdk');\nconst sagemaker = new AWS.SageMakerRuntime();\nasync function score(event){\n  const resp = await sagemaker.invokeEndpoint({EndpointName:'FraudDetector', Body: JSON.stringify(event), ContentType:'application/json'}).promise();\n  return JSON.parse(resp.Body.toString());\n}\n```\n\n## Follow-up Questions\n- How would you implement canary deployments and drift monitoring for the endpoint?\n- How would you validate data retention and privacy controls across regions?","diagram":null,"difficulty":"advanced","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Goldman Sachs"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T10:50:22.735Z","createdAt":"2026-01-14T10:50:22.735Z"},{"id":"q-951","question":"You're building a low‑ops internal voice assistant for support scripts. Audio files up to 60 seconds are uploaded to S3. Design an AWS‑only pipeline that transcribes, analyzes sentiment, and stores a brief summary plus an index in DynamoDB, with minimal cost and ops. Include data flow, services, error handling, and privacy considerations?","answer":"Use an S3 trigger to start a Step Functions workflow: Transcribe for 60s, then Comprehend for sentiment, store transcript + summary in DynamoDB, and index in a separate small table. Add a compact S3 m","explanation":"## Why This Is Asked\n\nTests ability to design a cost‑aware, low‑ops AWS data pipeline that stitches AI services together with serverless orchestration.\n\n## Key Concepts\n\n- AWS services: S3, Step Functions, Transcribe, Comprehend, DynamoDB\n- Serverless orchestration with error handling and retries\n- Data privacy: encryption, access control, least privilege\n\n## Code Example\n\n```javascript\n// Example AWS CDK snippet (TypeScript) configuring a Step Function state machine trigger\n```\n\n## Follow-up Questions\n\n- How would you scale the pipeline for higher concurrency?\n- How would you monitor latency and alert on failures?","diagram":null,"difficulty":"beginner","tags":["aws-ai-practitioner"],"channel":"aws-ai-practitioner","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T16:38:46.546Z","createdAt":"2026-01-12T16:38:46.546Z"}],"subChannels":["general"],"companies":["Adobe","Airbnb","Amazon","Anthropic","Bloomberg","Citadel","Cloudflare","Coinbase","Databricks","DoorDash","Goldman Sachs","Google","Hashicorp","LinkedIn","MongoDB","NVIDIA","Netflix","PayPal","Robinhood","Salesforce","Scale Ai","Snap","Snowflake","Stripe","Tesla","Zoom"],"stats":{"total":15,"beginner":5,"intermediate":4,"advanced":6,"newThisWeek":15}}