{"questions":[{"id":"q-1018","question":"You’re building a mobile camera app that auto-captures frames from a live video feed. Describe a practical, beginner-friendly pipeline to decide whether a frame is usable by applying two simple checks: (1) sharpness via variance of Laplacian, (2) exposure via histogram-based brightness. Explain how thresholds would be chosen, how you'd adapt them across lighting, and provide a minimal code snippet illustrating the core checks?","answer":"Compute sharpness as the variance of the Laplacian on grayscale and assess exposure with 2nd and 98th percentile brightness. Require low>=20 and high<=235, and sharpness>100. Adapt thresholds with a 3","explanation":"## Why This Is Asked\n\nTests ability to design a lightweight, production-friendly CV heuristic using simple, well-known metrics, and to reason about robustness to lighting without heavy models.\n\n## Key Concepts\n\n- Variance of Laplacian as a sharpness proxy\n- Histogram-based exposure sensing (2nd/98th percentile)\n- Thresholding plus simple adaptation for devices\n\n- Trade-offs between false positives/negatives\n\n## Code Example\n\n```python\nimport cv2, numpy as np\ndef frame_ok(frame, sharp_th=100.0, low_thr=20, high_thr=235):\n    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n    sharp = cv2.Laplacian(gray, cv2.CV_64F).var()\n    lo, hi = np.percentile(gray, (2, 98))\n    exp_ok = (lo >= low_thr) and (hi <= high_thr)\n    return (sharp > sharp_th) and exp_ok\n```\n\n## Follow-up Questions\n\n- How would you calibrate thresholds across cameras?\n- How would you extend to color balance and motion blur?\n","diagram":"flowchart TD\nFrame[Frame In] --> Sharp[Compute Laplacian Variance]\nSharp --> Decide1{Sharpness > 100?}\nDecide1 -- Yes --> Exp[Compute Exposure]\nExp --> Decide2{Exposure OK?}\nDecide2 -- Yes --> Accept[Accept Frame]\nDecide2 -- No --> Reject[Reject Frame]\nDecide1 -- No --> Reject","difficulty":"beginner","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","LinkedIn","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T19:32:04.521Z","createdAt":"2026-01-12T19:32:04.521Z"},{"id":"q-1093","question":"Design a privacy-preserving, real-time hand-gesture recognition system for video calls on consumer laptops (720p camera) that distinguishes a small set of gestures (hand-raise, thumbs-up, peace) without exposing facial details. Must run on-device at 30–60 FPS, handle lighting/occlusion, and support federated fine-tuning with differential privacy. Outline architecture, data strategy, and evaluation plan?","answer":"Two-stage on-device design: a lightweight hand ROI extractor (tiny CNN such as MobileNetV3) plus a temporal classifier (1D conv or efficient Transformer) operating on a short frame window. Train with ","explanation":"## Why This Is Asked\nThis question probes practical on-device CV with privacy constraints, latency pressure, and data privacy.\n\n## Key Concepts\n- Lightweight models, ROI extraction, temporal reasoning\n- Quantization, pruning, on-device inference\n- Federated learning, differential privacy, privacy budgets\n- Evaluation: latency, memory, F1/precision-recall under occlusion\n\n## Code Example\n```javascript\n// Pseudocode: frame window prep for on-device gesture model\nfunction prepareWindow(frames) {\n  const ROI = detectHandROI(frames[0]);\n  const window = frames.slice(-16).map(f => crop(f, ROI));\n  return stack(window);\n}\n```\n\n## Follow-up Questions\n- How would you measure and mitigate drift when lighting changes across devices?\n- How would you handle new gestures without retraining all devices?","diagram":null,"difficulty":"intermediate","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","Tesla","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T22:25:53.986Z","createdAt":"2026-01-12T22:25:53.986Z"},{"id":"q-1108","question":"Design a real-time, on-device hand pose/gesture system for air-drawing in a video-conferencing app. From a monocular 1080p60 stream, infer 2D/3D hand pose with <40ms latency on a CPU, robust to occlusion and varied skin tones, and support at least 5 gestures (draw, erase, next, previous, pointer). Outline data needs, model architecture, latency optimizations, temporal consistency, and evaluation plan?","answer":"Proposed approach: deploy a lightweight 2D keypoint detector plus a compact 3D lifting head on-device, with a short temporal filter (Kalman/temporal conv). Use INT8 quantization and ONNX Runtime for C","explanation":"## Why This Is Asked\nThis question probes on-device real-time hand pose recognition, latency budgeting, robustness to occlusion and skin tone, and interface with gesture-based controls for conferencing.\n\n## Key Concepts\n- On-device real-time inference\n- 2D keypoint + 3D lifting\n- Temporal smoothing for stability\n- Occlusion and bias robustness\n- Quantization and runtime deployment\n\n## Code Example\n```javascript\nfunction inferHandPose(frame) {\n  const kps2d = detector2D(frame);\n  const pose3D = liftTo3D(kps2d);\n  return smoothPose(pose3D);\n}\n```\n\n## Follow-up Questions\n- How would you handle multiple hands and long occlusions?\n- How would you validate latency and diagnose spikes in production?\n","diagram":"flowchart TD\nA[Capture 1080p60 frame] --> B[2D hand-keypoint detector]\nB --> C[3D pose lifting]\nC --> D[Temporal smoothing]\nD --> E[Gesture classifier]\nE --> F[Action: air-draw / next / prev]","difficulty":"advanced","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Plaid","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T23:17:25.742Z","createdAt":"2026-01-12T23:17:25.742Z"},{"id":"q-1270","question":"Design a real-time monocular 3D detector for an assembly line that estimates 6-DoF pose of tools from a single RGB camera, achieving sub-50ms per frame on embedded hardware. Use a lightweight backbone with self-supervised pretraining plus a small labeled set; recover pose via differentiable PnP from 2D-3D correspondences with a temporal filter and reprojection losses. Monitor drift with streaming per-frame errors?","answer":"Propose a lightweight monocular 3D detector on edge hardware that predicts 2D-3D keypoint correspondences and a scale per tool, then solves 6-DoF via differentiable PnP. Use self-supervised pretrainin","explanation":"## Why This Is Asked\nIndustrial CV on the edge demands real-time 3D understanding from monocular inputs with limited labels. The design must handle latency, occlusion, and drift in long shifts.\n\n## Key Concepts\n- Monocular 3D pose estimation\n- Differentiable PnP\n- Self-supervised learning\n- Temporal filtering\n- Embedded/edge deployment\n\n## Code Example\n```python\n# Simple differentiable PnP pose recovery sketch\nimport torch\n\ndef pose_from_keypoints(pts2d, pts3d, K):\n    # Placeholder for differentiable PnP optimization\n    # Return rotation R and translation t\n    R = torch.eye(3)\n    t = torch.zeros(3)\n    return R, t\n```\n\n## Follow-up Questions\n- How to robustly handle occlusions and outliers in keypoint matches?\n- What evaluation protocol would reliably detect drift over 24h? ","diagram":null,"difficulty":"intermediate","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Hashicorp","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T07:35:09.692Z","createdAt":"2026-01-13T07:35:09.692Z"},{"id":"q-1417","question":"Given a single RGB camera mounted on an autonomous delivery drone operating in urban environments, design a real-time system to detect and track pedestrians and other vulnerable actors at 30 fps under nighttime and rain conditions. Propose data strategy (synthetic rain + real), model backbone, temporal fusion and latency targets, and safety/failover mechanisms?","answer":"Use a lightweight detector such as EfficientDet-D3 with a Kalman-filter-based tracker and a short-term memory for temporal fusion; train on rain-augmented synthetic data plus a small labeled real-worl","explanation":"## Why This Is Asked\nDesigning perception for aerial platforms under adverse weather tests data strategy, real-time constraints, and safety guarantees in production-like settings.\n\n## Key Concepts\n- Single-view detection under rain/night; robustness to weather\n- Temporal fusion and multi-object tracking in a streaming context\n- Domain adaptation: synthetic rain + real rainy data\n- Edge latency budgets and model backbones\n- Safety/failover and occlusion handling\n\n## Code Example\n```python\n# Pseudo-code: simple Kalman-based track update\nclass Track:\n    def update(self, measurement):\n        self.state = self.kf.update(self.state, measurement)\n```\n\n## Follow-up Questions\n- How would you validate performance across rain intensities and lighting?\n- How would you quantify and mitigate false positives in crowded scenes?\n","diagram":"flowchart TD\n  A[Input: video stream] --> B[Preprocessing]\n  B --> C[Detection & Tracking]\n  C --> D[Temporal Fusion & Association]\n  D --> E[Output: Trajectories & Alerts]","difficulty":"advanced","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Cloudflare","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T15:56:01.086Z","createdAt":"2026-01-13T15:56:01.086Z"},{"id":"q-1435","question":"For aerial inspection of solar farms, design a CV system to detect and grade micro-cracks on solar panels from drone video with limited labeled data. Specify an architecture that detects tiny defects, data-augmentation strategies (synthetic crack overlays, texture randomization), domain adaptation, and an edge-friendly output (box, segmentation mask, and severity score). Include evaluation protocol and latency targets?","answer":"I’d use a two-stage detector with a high-res backbone (EfficientNetV2-S + FPN) and a light segmentation head. Train with synthetic crack overlays on panel textures plus self-supervised pretraining on ","explanation":"## Why This Is Asked\nTests ability to design CV solutions under data scarcity, with tiny defect detection, multi-task outputs, and edge deployment constraints.\n\n## Key Concepts\n- Tiny defect detection in high-res imagery\n- Synthetic data and SSL for labels-scarce regimes\n- Domain adaptation and edge-model optimization\n- Multi-task outputs: bbox, mask, severity\n\n## Code Example\n```python\n# Pseudo training loop outline\nfor batch in dataloader:\n  imgs, boxes, masks, sev = batch\n  feats = backbone(imgs)\n  pred_boxes, pred_masks = heads(feats)\n  loss = focal_loss(pred_boxes, boxes) + bce_loss(pred_masks, masks) + mse_loss(sev, severity)\n  loss.backward()\n  opt.step()\n```\n\n## Follow-up Questions\n- How would you quantify model calibration for severity predictions?\n- What ablation would you run to isolate the impact of synthetic data?","diagram":"flowchart TD\n  A[Drone Frame] --> B[Preprocess] \n  B --> C[SSL Pretraining] \n  C --> D[Detector (backbone + heads)] \n  D --> E[Postprocess: NMS + severity] \n  E --> F[Edge Deployment]","difficulty":"advanced","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","MongoDB","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T16:56:34.735Z","createdAt":"2026-01-13T16:56:34.735Z"},{"id":"q-1861","question":"You're given an overhead RGB image of a table with scattered coins. Design a beginner-friendly pipeline to count the number of coins and estimate their approximate denomination from a single image. Include preprocessing, circle/contour detection, radius-based grouping to separate coin types, handling shadows, and a minimal code sketch using OpenCV to detect circular shapes?","answer":"Preprocess: grayscale, CLAHE, and blur to reduce noise. Use HoughCircles (dp=1.2, minDist=20, minRadius=10, maxRadius=60) to detect coin-like circles. Merge duplicates by proximity, bucket by radius i","explanation":"## Why This Is Asked\nTests ability to design a practical CV pipeline using lightweight, non-ML methods for a common task.\n\n## Key Concepts\n- Circle detection with Hough transform\n- Radius-based grouping for coin types\n- Noise/shadow handling with blur and histogram equalization\n- Validation with synthetic data\n\n## Code Example\n```javascript\n// OpenCV.js-like pseudocode\nlet src = cv.imread('table.jpg');\ncv.cvtColor(src, src, cv.COLOR_RGBA2GRAY);\ncv.equalizeHist(src, src);\ncv.GaussianBlur(src, new cv.Size(9,9), 2);\nlet circles = new cv.Mat();\ncv.HoughCircles(src, circles, cv.HOUGH_GRADIENT, 1.2, 20, 50, 30, 10, 60);\n```\n\n## Follow-up Questions\n- How would you address perspective distortions?\n- How would you validate robustness with lighting changes?\n","diagram":"flowchart TD\n  A[Input image] --> B[Preprocess]\n  B --> C[Circle detection]\n  C --> D[Filter duplicates]\n  D --> E[Group by radius]\n  E --> F[Count & report]","difficulty":"beginner","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Instacart","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T14:48:10.414Z","createdAt":"2026-01-14T14:48:10.414Z"},{"id":"q-1945","question":"Design a real-time anomaly-detection pipeline for a single RGB camera monitoring an industrial warehouse. Propose a memory-augmented autoencoder approach that runs on an edge device at 15–20 FPS, uses frame-wise reconstruction error plus optical-flow residuals, maintains a fixed-size normal-pattern memory, and includes a drift-adaptation strategy with minimal labeling?","answer":"Implement a memory-augmented autoencoder (MAE) with a lightweight encoder-decoder and a fixed memory bank of normal latent prototypes. For each frame, retrieve the nearest memory vectors, fuse them wi","explanation":"## Why This Is Asked\\n\\nTests practical CV design under edge constraints, handling concept drift with minimal labels, and fusing appearance with motion cues.\\n\\n## Key Concepts\\n- Memory-augmented autoencoder for normal-pattern modeling\\n- Fixed-size memory bank and online updates\\n- Frame reconstruction error + optical-flow residuals for anomaly scoring\\n- Edge deployment with INT8/quantized backbones\\n\\n## Code Example\\n```javascript\\n// Skeleton: MAE forward pass and anomaly scoring\\nfunction MAEForward(frame, model, memory) {\\n  // encode, memory lookup, decode, compute error\\n}\\n```\\n\\n## Follow-up Questions\\n- How would you quantify drift and decide when to refresh memory?\\n- How would you evaluate latency vs. accuracy trade-offs on Jetson?","diagram":null,"difficulty":"intermediate","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","Hugging Face"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T17:58:29.262Z","createdAt":"2026-01-14T17:58:29.262Z"},{"id":"q-2131","question":"In a factory setting, design a real-time 2-view RGB tool-pose tracking system that achieves sub-60ms per frame on edge hardware. Use two synchronized cameras, a lightweight backbone, and a differentiable PnP with 2D-3D correspondences; include cross-view fusion, a temporal filter, and a self-supervised pretraining strategy with synthetic data. How would you validate drift and occlusion resilience?","answer":"Propose a dual-RGB edge pipeline: detect 2D keypoints on each view with a small CNN (e.g., MobileNetV3) and fuse via a learned cross-view encoder; compute 6-DoF tool pose with differentiable PnP + rob","explanation":"## Why This Is Asked\nTests multi-view real-time pose estimation under strict latency, with practical constraints on edge hardware, including cross-view fusion, differentiable PnP, and self-supervised learning.\n\n## Key Concepts\n- multi-view fusion\n- differentiable PnP and RANSAC\n- edge latency optimization\n- self-supervised pretraining with synthetic data\n- drift and occlusion resilience\n\n## Code Example\n```javascript\n// Skeleton: data flow for two views\nfunction processFrame(view1, view2, intrinsics, extrinsics){\n  // 1) detect 2D keypoints in both views\n  // 2) establish cross-view correspondences\n  // 3) triangulate to 3D points\n  // 4) solve PnP for 6-DoF pose\n  // 5) apply temporal filter for stability\n}\n```\n\n## Follow-up Questions\n- How would you evaluate drift over long streaming sequences?\n- How would you handle calibration drift or a camera failure during inference?","diagram":"flowchart TD\n  A[Two RGB cameras] --> B[2D keypoint detectors]\n  B --> C[Cross-view fusion encoder]\n  C --> D[Triangulation to 3D]\n  D --> E[Differentiable PnP pose]\n  E --> F[Temporal fusion (Kalman)]","difficulty":"advanced","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","OpenAI","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T04:14:43.759Z","createdAt":"2026-01-15T04:14:43.759Z"},{"id":"q-2242","question":"In a warehouse setting, design an edge-friendly CV pipeline that counts and localizes pallets from a single moving RGB camera mounted on a forklift, using self-supervised depth cues from motion and a lightweight 3D detector to output per-pallet 3D bounding boxes with uncertainty at 30 Hz. Address occlusion, dynamic workers, and domain shift between day and night?","answer":"I'd deploy a lightweight 2D pallet detector (EfficientDet-D3) to generate proposals, estimate depth from ego-motion via structure-from-motion, and use a differentiable 3D box head for per-pallet boxes","explanation":"## Why This Is Asked\nAssesses end-to-end design for real-world inventory sensing under occlusion, speed, and lighting variation, emphasizing self-supervised depth, differentiable 3D reasoning, and uncertainty estimation on edge hardware.\n\n## Key Concepts\n- Lightweight 2D detector (EfficientDet)\n- Self-supervised depth from ego-motion\n- Differentiable 3D box regression\n- Uncertainty via MC dropout and aleatoric depth\n- Kalman tracking + data association\n- Edge deployment (latency, memory)\n\n## Code Example\n```javascript\n// Pseudocode\nfunction pipeline(frame, state){\n  proposals = detector(frame)\n  depth = depthFromMotion(frame, state)\n  boxes = reg3D(proposals, depth)\n  tracks = kalmanTrack(boxes, state)\n  return tracks\n}\n```\n\n## Follow-up Questions\n- How would you evaluate robustness to motion blur and occlusion?\n- How would you adapt the system to different pallet shapes/sizes or new warehouses without labeled data?","diagram":null,"difficulty":"advanced","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T09:00:25.390Z","createdAt":"2026-01-15T09:00:25.390Z"},{"id":"q-2333","question":"Design a beginner-friendly pipeline to detect and count red boxes moving on a conveyor using a single RGB camera in real time. Include HSV color space selection, dual-range red thresholds, noise removal with morphology, contour filtering by area/shape, and a simple line-crossing tracker. Provide a minimal OpenCV.js code snippet for core steps?","answer":"Use HSV segmentation for red with two ranges, apply Gaussian blur and morphology to reduce noise, detect contours, filter by area and aspect ratio to identify boxes, compute centroids, and count when ","explanation":"## Why This Is Asked\n\nThis task yields a concrete, end-to-end CV workflow suitable for entry-level projects that run in real time on modest hardware. It tests practical skills in color segmentation, noise handling, contour reasoning, and a simple, robust counting strategy.\n\n## Key Concepts\n\n- HSV color space and red wrap-around handling\n- Morphology for noise reduction\n- Contour area and aspect-ratio filtering\n- Centroid computation and line-crossing tracking\n- Lightweight calibration for varying lighting\n\n## Code Example\n\n```javascript\n// OpenCV.js core steps for red box detection\nfunction detectRedBoxes(src) {\n  // convert to HSV, threshold red in two ranges, combine masks\n  // blur and apply morphology\n  // find contours, filter by area/shape\n  // compute centroids and detect crossing against a line\n}\n```\n\n## Follow-up Questions\n\n- How would you handle occlusion or partial boxes?\n- How would you validate with limited labeled data and choose metrics?","diagram":null,"difficulty":"beginner","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Google","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T13:08:46.849Z","createdAt":"2026-01-15T13:08:46.849Z"},{"id":"q-2377","question":"Scenario: fixed overhead RGB camera watches a single shoebox on a shelf. Design a beginner-friendly, non-deep-learning pipeline to decide whether the box is upright within ±10 degrees using contour-based detection. Explain preprocessing, edge/shape heuristics, thresholds, and how to handle perspective distortion and occlusion. Include a minimal Python OpenCV snippet that outputs 'upright' or 'tilted'?","answer":"Convert to grayscale and blur (3x3). Run Canny to get edges, then find contours and keep the largest near-rectangular contour via minAreaRect and a 4-corner check. Normalize the angle to [-90,90], tak","explanation":"## Why This Is Asked\nTests building a robust, non-ML CV heuristic that handles real-world perspective and noise.\n\n## Key Concepts\n- Contour detection and minAreaRect for orientation\n- Edge extraction and thresholding for robustness\n- Angle normalization and aspect-ratio checks\n- Frame-consistency to reject jitter\n\n## Code Example\n```python\nimport cv2\nimport numpy as np\n\ndef upright_status(img):\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    blur = cv2.GaussianBlur(gray, (3,3), 0)\n    edges = cv2.Canny(blur, 50, 150)\n    cnts, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    if not cnts:\n        return 'tilted'\n    c = max(cnts, key=cv2.contourArea)\n    rect = cv2.minAreaRect(c)\n    angle = abs(rect[2])\n    if angle > 45:\n        angle = 90 - angle\n    w, h = rect[1]\n    aspect = w / h if h else 0\n    if angle <= 10 and 0.8 <= aspect <= 1.2:\n        return 'upright'\n    return 'tilted'\n```\n\n## Follow-up Questions\n- How would you scale this to multiple boxes in a scene? \n- How would you handle strong perspective distortion or occlusion while keeping it beginner-friendly?","diagram":"flowchart TD\n  A[Acquire image] --> B[Preprocess]\n  B --> C[Edge detect]\n  C --> D[Find contours]\n  D --> E[Select largest near-rect]{Is near-rect?}\n  E -->|Yes| F[MinAreaRect and angle]\n  F --> G[Compute upright status]\n  G --> H[Output Upright/Tilted]","difficulty":"beginner","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Oracle","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T15:42:44.654Z","createdAt":"2026-01-15T15:42:44.654Z"},{"id":"q-2546","question":"Design a real-time monocular hand pose and gesture recognition system for AR UI on a battery-constrained headset. Propose a lightweight architecture (e.g., two-branch network with 2D heatmaps and a temporal encoder), latency target <25 ms per frame on edge hardware, occlusion handling, and 3D hand pose estimation—describe data strategy, loss terms, and evaluation?","answer":"Two-branch lightweight model: Branch A generates 2D hand keypoint heatmaps using a MobileNetV3-like backbone; Branch B encodes short-term temporal context. 3D pose is recovered through a differentiable hand model with a PnP solver, achieving <25ms latency via model quantization and efficient temporal fusion. Occlusion handling incorporates visibility prediction and temporal smoothing, while training combines synthetic and real data with domain adaptation.","explanation":"## Why This Is Asked\nTests practical hand tracking for AR UI with tight latency, occlusion handling, and edge deployment, aligning with high-performance computer vision needs at tech leaders.\n\n## Key Concepts\n- Lightweight backbones for 2D heatmaps\n- Temporal encoders (GRU/Transformer) for stability\n- Differentiable hand model with PnP for 3D pose\n- Occlusion handling via visibility prediction and temporal smoothing\n- Synthetic + real data with domain adaptation; real-time evaluation metrics MPJPE, PCK3D\n\n## Code Example\n```python\n# Pseudocode: fuse 2D heatmaps with temporal features to estimate 3D hand pose\nheatmaps = branch_a(image)  # 2D keypoint detection\ntemporal_features = branch_b(heatmaps_sequence)  # temporal context\npose_3d = hand_model_solver(heatmaps, temporal_features)  # 3D pose recovery\n```","diagram":"flowchart TD\n  A[Input: monocular video] --> B[Backbone: 2D heatmaps]\n  A --> C[Temporal encoder]\n  B --> D[3D pose via differentiable hand model]\n  C --> D\n  D --> E[Occlusion vis predictor + smoothing]\n  E --> F[Gesture decoding -> UI events]","difficulty":"intermediate","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","NVIDIA","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T05:30:31.054Z","createdAt":"2026-01-15T22:33:53.343Z"},{"id":"q-456","question":"How would you design a real-time object detection system for a social media platform that processes 10M images/day with 99.9% accuracy and <100ms latency?","answer":"Implement a distributed pipeline with GPU clusters for inference, model quantization using TensorRT/ONNX, edge caching for popular content, and ensemble models. Incorporate batch processing, model versioning, and comprehensive monitoring to achieve the target performance metrics.","explanation":"## Architecture\n- **Inference Layer**: GPU clusters with model quantization and TensorRT optimization\n- **Caching Layer**: Redis for frequent detections and CDN edge caching for popular content\n- **Load Balancing**: Kubernetes with auto-scaling based on queue depth and latency thresholds\n- **Monitoring**: Real-time accuracy metrics and comprehensive latency tracking\n\n## Optimization Strategies\n- **Model Compression**: TensorRT optimization with 8-bit quantization for faster inference\n- **Batch Processing**: Dynamic batching based on system load to maximize GPU utilization\n- **Fallback Mechanism**: CPU inference when GPU resources are unavailable\n- **Model Ensemble**: Combine YOLOv8 with ResNet to achieve 99.9% accuracy\n\n## Production Considerations\n- **Model Versioning**: Implement A/B testing and gradual rollouts for model updates\n- **Scalability**: Horizontal scaling of inference nodes to handle peak loads\n- **Reliability**: Implement circuit breakers and retry mechanisms for fault tolerance","diagram":"flowchart TD\n  A[Image Upload] --> B[Preprocessing]\n  B --> C[GPU Inference]\n  C --> D[Postprocessing]\n  D --> E[Cache Check]\n  E --> F[Result Storage]\n  G[Load Balancer] --> C\n  H[Monitor] --> I[Alert System]\n  J[Model Registry] --> C","difficulty":"advanced","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Microsoft","Salesforce","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-09T08:56:19.980Z","createdAt":"2025-12-26 12:51:05"},{"id":"q-487","question":"Design a real-time object detection system for DoorDash delivery vehicles that must identify packages, license plates, and traffic signs in varying weather conditions. How would you handle model optimization for edge deployment and ensure 99% accuracy?","answer":"Use YOLOv8 with custom-trained heads for each object class. Implement TensorRT optimization for NVIDIA Jetson edge devices. Use multi-scale training with weather augmentation (rain, fog, night). Deploy with model ensemble techniques and confidence thresholding to maintain 99% accuracy.","explanation":"## Architecture\n- **Detection Pipeline**: Multi-task CNN with shared backbone and separate heads\n- **Edge Optimization**: TensorRT INT8 quantization, model pruning, batch size 1\n- **Weather Handling**: Domain adaptation, synthetic data generation\n\n## Implementation\n```python\n# Model optimization example\nimport torch\nfrom torch2trt import torch2trt\n\nmodel = YOLOv8(num_classes=3)\nmodel.load_state_dict(torch.load('best.pth'))\nmodel.cuda().eval()\n\n# TensorRT conversion\nx = torch.ones((1, 3, 640, 640)).cuda()\nmodel_trt = torch2trt(model, [x], fp16_mode=True)\n```\n\n## Performance\n- **Inference**: <50ms per frame\n- **Accuracy**: 99%+ on validation set\n- **Memory**: <2GB on edge device","diagram":"flowchart TD\n  A[Camera Feed] --> B[Preprocessing]\n  B --> C[YOLOv8 Detection]\n  C --> D[TensorRT Inference]\n  D --> E[NMS Filtering]\n  E --> F[Temporal Smoothing]\n  F --> G[Confidence Check]\n  G --> H{Accuracy > 99%?}\n  H -->|Yes| I[Output Results]\n  H -->|No| J[Cloud Fallback]","difficulty":"advanced","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-01T06:41:06.125Z","createdAt":"2025-12-26 12:51:06"},{"id":"q-517","question":"Design a real-time object detection system for cryptocurrency trading terminals that must detect and classify multiple monitor types, trading interfaces, and unauthorized screen recording devices with <100ms latency. How would you optimize YOLOv8 for this specific use case?","answer":"Optimize YOLOv8-nano with TensorRT quantization, custom dataset of trading interfaces, and multi-scale feature fusion. Implement temporal consistency with optical flow tracking, use knowledge distillation from larger models, and deploy with batch inference pipelines to achieve sub-100ms latency.","explanation":"## Architecture Overview\n- **Model Selection**: YOLOv8-nano optimized for speed, custom-trained on comprehensive trading UI datasets\n- **Optimization Pipeline**: TensorRT FP16 quantization with custom CUDA kernels for hardware acceleration\n- **Real-time Processing**: Intelligent frame skipping combined with motion detection for idle periods\n\n## Technical Implementation\n```python\n# Custom trading interface detector\nclass TradingDetector:\n    def __init__(self):\n        self.model = YOLO('yolov8n_trading.pt')\n        self.tracker = DeepSort(max_age=30)\n    \n    def detect_frame(self, frame):\n        results = self.model(frame, conf=0.7)\n        return self.tracker.update(results)","diagram":"flowchart TD\n  A[Camera Feed] --> B[Frame Preprocessing]\n  B --> C[YOLOv8-nano Inference]\n  C --> D[Object Tracking]\n  D --> E[Classification Layer]\n  E --> F[Alert System]\n  F --> G[Security Dashboard]\n  C --> H[Performance Monitor]\n  H --> I[Model Retraining]","difficulty":"advanced","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Coinbase","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-09T08:38:40.181Z","createdAt":"2025-12-26 12:51:06"},{"id":"q-545","question":"How would you detect if an image contains a face using basic computer vision techniques?","answer":"To detect if an image contains a face using basic computer vision techniques, apply Haar cascade classifiers that scan the image using trained rectangular filters to identify facial features like eyes and nose. Alternatively, use HOG feature extraction combined with SVM classifiers to distinguish face patterns from background regions.","explanation":"## Face Detection Methods\n\n### Haar Cascade Approach\n- Uses integral images for fast feature computation\n- Trains on positive/negative face samples\n- Detects facial features through rectangular filters\n\n### Implementation Steps\n- Convert image to grayscale\n- Apply histogram equalization\n- Load pre-trained cascade classifier\n- Use detectMultiScale() with proper parameters\n\n### Common Parameters\n```python\nfaces = face_cascade.detectMultiScale(\n    gray_image,\n    scaleFactor=1.1,\n    minNeighbors=5,\n    minSize=(30, 30)\n)\n```\n\n### Trade-offs\n- Fast but less accurate than deep learning\n- Works well for frontal faces\n- Sensitive to lighting conditions","diagram":"flowchart TD\n  A[Input Image] --> B[Grayscale Conversion]\n  B --> C[Histogram Equalization]\n  C --> D[Haar Cascade Detection]\n  D --> E[Face Bounding Boxes]\n  E --> F[Output Results]","difficulty":"beginner","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","NVIDIA","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":["haar cascades","hog features","svm","opencv","detectmultiscale","integral images"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2026-01-07T03:43:05.970Z","createdAt":"2025-12-26 12:51:07"},{"id":"q-570","question":"How would you design a real-time object detection system for Airbnb's property listing photos that can identify amenities and safety violations while processing 10,000 images per hour?","answer":"I'd design a distributed object detection pipeline using Redis queues for image processing, YOLOv8 for real-time detection, and GPU-accelerated batch inference. The system would employ model ensembling for accuracy, TensorRT optimization for performance, and intelligent result caching.","explanation":"## Architecture\n- **Ingestion**: S3 event triggers Lambda functions to queue images for processing\n- **Processing**: GPU workers execute YOLOv8 models with TensorRT optimization for maximum throughput\n- **Storage**: Detection results and metadata stored in PostgreSQL with vector embeddings for similarity search\n- **API Layer**: FastAPI endpoints with Redis caching for low-latency responses\n\n## Performance Optimization\n- **Batch Processing**: Process 32 images per GPU batch to maximize utilization\n- **Model Selection**: YOLOv8-large achieves 45% mAP at 80 FPS for high accuracy requirements\n- **Auto-scaling**: Dynamic worker scaling based on queue depth and processing latency metrics\n\n## Design Trade-offs\n- **Accuracy vs Speed**: Medium models provide optimal balance with 35% mAP at 120 FPS\n- **Cost Efficiency**: Spot instances reduce GPU costs by 70% with checkpoint recovery\n- **Latency Targets**: P99 processing time under 2 seconds for real-time user experience","diagram":"flowchart TD\n  A[S3 Upload] --> B[Lambda Trigger]\n  B --> C[Redis Queue]\n  C --> D[GPU Workers]\n  D --> E[YOLOv8 Detection]\n  E --> F[PostgreSQL]\n  F --> G[API Cache]\n  G --> H[Client]","difficulty":"advanced","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Instacart"],"eli5":null,"relevanceScore":null,"voiceKeywords":["distributed pipeline","redis queue","yolov8","batch inference","tensorrt","model ensemble"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2026-01-08T11:41:33.371Z","createdAt":"2025-12-27T01:12:04.929Z"},{"id":"q-274","question":"How would you implement a hybrid CNN architecture combining ResNet residual connections with EfficientNet compound scaling for production image classification?","answer":"Implement a hybrid CNN by integrating ResNet residual connections with EfficientNet's compound scaling methodology. This involves using ResNet blocks enhanced with channel attention mechanisms, applying EfficientNet's compound scaling formula φ = α^β · γ^φ to balance depth, width, and resolution, and optimizing the model with mixed precision training for production deployment.","explanation":"## Concept\nA hybrid CNN architecture combines ResNet's residual connections (identity shortcuts) with EfficientNet's compound scaling approach, which systematically balances network depth, width, and resolution using a fixed compound coefficient φ.\n\n## Implementation\n```python\n# Hybrid block with residual + efficient scaling\nclass HybridBlock(nn.Module):\n    def __init__(self, in_ch, out_ch, stride=1):\n        super().__init__()\n        # EfficientNet MBConv with residual\n        self.mbconv = MBConv(in_ch, out_ch, stride)\n        self.shortcut = nn.Identity() if stride == 1 else nn.Conv2d(in_ch, out_ch, 1)\n        \n    def forward(self, x):\n        return self.mbconv(x) + self.shortcut(x)\n```\n\n## Production Optimization\n- Mixed precision training for memory efficiency\n- Channel attention for improved feature representation\n- Compound scaling ensures optimal resource utilization","diagram":"flowchart TD\n    A[Input Image] --> B[Stem Conv 3x3]\n    B --> C[Hybrid Block 1: MBConv + Residual]\n    C --> D[Hybrid Block 2: MBConv + Residual]\n    D --> E[SE Block: Channel Attention]\n    E --> F[Hybrid Block 3: Downsample]\n    F --> G[Global Average Pool]\n    G --> H[Fully Connected]\n    H --> I[Softmax]\n    \n    C --> C1[Identity Shortcut]\n    C1 --> C2[Add]\n    D --> C2\n    E --> C2","difficulty":"intermediate","tags":["cnn","resnet","efficientnet"],"channel":"computer-vision","subChannel":"image-classification","sourceUrl":"https://arxiv.org/abs/1905.11946","videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google","Meta","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-08T11:22:10.106Z","createdAt":"2025-12-26 12:51:07"},{"id":"q-253","question":"How does YOLO implement real-time object detection using grid-based prediction and what are the key components of its architecture?","answer":"YOLO divides image into S×S grid, each cell predicts B bounding boxes with confidence scores, class probabilities, and uses anchor boxes for better localization.","explanation":"## Interview Context\nThis question assesses understanding of modern object detection architectures and trade-offs between speed and accuracy. YOLO's unified approach is fundamental for real-time applications.\n\n## Technical Details\n\n### Core Architecture\n- **Backbone**: Darknet feature extractor (Darknet-53 for YOLOv3)\n- **Grid System**: S×S grid where each cell predicts B bounding boxes\n- **Output Tensor**: [S, S, B*(5+C)] where 5 = (x, y, w, h, confidence), C = class count\n\n### Key Components\n- **Anchor Boxes**: Pre-defined aspect ratios to improve bounding box prediction\n- **IoU Calculation**: Intersection over Union for confidence scoring\n- **Non-Maximum Suppression**: Removes redundant detections above IoU threshold\n- **Multi-Scale Predictions**: Feature maps at different scales for various object sizes\n\n### Loss Function\n```\nLoss = λ_coord * MSE(bbox) + λ_noobj * MSE(confidence) + MSE(class)\n```\n\n### Training Process\n- Single-stage training end-to-end\n- Uses mean squared error for bounding box regression\n- Cross-entropy for classification\n\n## Performance Comparison\n- **YOLOv1**: 45 FPS on Titan X (63.4% mAP)\n- **Faster R-CNN**: 7 FPS (73.2% mAP)\n- **YOLOv3**: 30 FPS (57.9% mAP)\n\n## Follow-up Questions\n1. How does YOLOv5 improve upon YOLOv3's architecture and training methodology?\n2. What are the trade-offs between YOLO and two-stage detectors like Faster R-CNN?\n3. How would you optimize YOLO for edge devices with limited computational resources?","diagram":"graph TD\n    A[Input Image 448×448] --> B[Backbone Network DarkNet-53]\n    B --> C[Feature Maps]\n    C --> D[Detection Head]\n    D --> E[Grid S×S]\n    E --> F[Each Cell Predicts]\n    F --> G[B Bounding Boxes]\n    F --> H[Confidence Scores]\n    F --> I[Class Probabilities]\n    G --> J[Non-Max Suppression]\n    H --> J\n    I --> J\n    J --> K[Final Detections]","difficulty":"beginner","tags":["yolo","rcnn","detr"],"channel":"computer-vision","subChannel":"object-detection","sourceUrl":null,"videos":null,"companies":["Amazon","Google","Meta","Microsoft","NVIDIA","Tesla"],"eli5":"Imagine you have a big chocolate chip cookie and you want to find all the chocolate chips really fast! You cut the cookie into tiny squares, like a checkerboard. Each little square has a special job - it looks for chocolate chips that might be hiding inside it. Each square also draws a box around any chips it finds and says how sure it is that it's really a chip. Some squares might think they see a chip when it's just a crumb, so they have to be very confident! The computer learns to do this super quickly by practicing with lots of cookies, so it can find all the chocolate chips in one quick look instead of searching slowly all over the cookie.","relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-26T16:38:46.236Z","createdAt":"2025-12-26 12:51:07"},{"id":"q-200","question":"How does U-Net's skip connection architecture enable precise medical image segmentation?","answer":"U-Net employs a contracting encoder to extract contextual features and an expanding decoder with skip connections that preserve spatial details, enabling precise pixel-wise segmentation.","explanation":"## Why Asked\nTests understanding of advanced CNN architectures for medical imaging and computer vision segmentation tasks.\n\n## Key Concepts\n- Encoder-decoder architecture\n- Skip connections for feature preservation\n- Contracting and expanding paths\n- Pixel-wise segmentation\n\n## Code Example\n```\ndef unet_block(x, skip):\n    x = Conv2D(64, 3, padding='same')(x)\n    x = concatenate([x, skip])\n    return x\n```\n\n## Follow-up Questions\n- How does U-Net handle class imbalance?\n- What are alternatives to skip connections?\n- How does it compare to FCN?","diagram":"flowchart TD\n  A[Input Image] --> B[Encoder Path]\n  B --> B1[Bottleneck]\n  B1 --> C[Decoder Path]\n  C --> D[Segmentation Output]\n  B -.->|Skip Connections| C","difficulty":"beginner","tags":["unet","mask-rcnn","sam"],"channel":"computer-vision","subChannel":"segmentation","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":"https://www.youtube.com/watch?v=HS3Q_90hnDg"},"companies":["Amazon","Google","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":["u-net","skip connections","encoder","decoder","spatial details","segmentation"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2025-12-30T01:46:54.144Z","createdAt":"2025-12-26 12:51:06"},{"id":"q-228","question":"How would you optimize a real-time medical image segmentation pipeline using SAM with 100ms latency constraint on edge devices?","answer":"Use SAM's lightweight encoder with quantized ViT-B, implement prompt caching, and apply tensorRT optimization for sub-100ms inference.","explanation":"## Concept Overview\nReal-time medical segmentation requires balancing accuracy with strict latency constraints. SAM (Segment Anything Model) provides zero-shot segmentation but needs optimization for edge deployment.\n\n## Implementation Details\n- **Model Optimization**: Use SAM-ViT-B (lightweight) with INT8 quantization\n- **Prompt Engineering**: Implement prompt caching for similar anatomical regions\n- **Hardware Acceleration**: Deploy with TensorRT on NVIDIA Jetson or CoreML on Apple Silicon\n- **Batch Processing**: Process multiple slices in parallel when available\n\n## Code Example\n```python\n# Optimized SAM inference pipeline\nimport torch\nfrom segment_anything import sam_model_registry\n\nclass OptimizedSAM:\n    def __init__(self):\n        self.sam = sam_model_registry['vit_b'](checkpoint='sam_vit_b.pth')\n        self.sam.eval()\n        self.sam.cuda()\n        # Enable TensorRT optimization\n        self.sam = torch.compile(self.sam, mode='max-autotune')\n    \n    def segment_with_cache(self, image, prompt):\n        # Check prompt cache first\n        cache_key = hash(prompt.tobytes())\n        if cache_key in self.prompt_cache:\n            return self.prompt_cache[cache_key]\n        \n        masks = self.sam.predict(image, prompt)\n        self.prompt_cache[cache_key] = masks\n        return masks\n```\n\n## Common Pitfalls\n- **Memory Overhead**: Prompt caching can consume significant memory on edge devices\n- **Quantization Loss**: INT8 quantization may reduce fine-grained segmentation accuracy\n- **Prompt Sensitivity**: Medical images require precise prompt placement for accurate results\n- **Hardware Variability**: Different edge devices have varying compute capabilities","diagram":"graph TD[Input Medical Image] --> A[Preprocessing: Resize/Normalize]\nA --> B[Prompt Detection: Anatomical Region]\nB --> C{Prompt Cache Hit?}\nC -->|Yes| D[Return Cached Mask]\nC -->|No| E[SAM Encoder: ViT-B Lightweight]\nE --> F[Prompt-Guided Decoder]\nF --> G[Post-processing: Refine Boundaries]\nG --> H[Cache Result]\nH --> I[Output Segmentation Mask]\nD --> I\n\nsubgraph Edge Device Optimization\n    J[TensorRT Engine] --> K[INT8 Quantization]\n    K --> L[Memory Pool Management]\nend\n\nE -.-> J\nF -.-> J","difficulty":"advanced","tags":["unet","mask-rcnn","sam"],"channel":"computer-vision","subChannel":"segmentation","sourceUrl":null,"videos":null,"companies":["Apple","Google","Meta","Microsoft","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-26T12:40:51.586Z","createdAt":"2025-12-26 12:51:07"}],"subChannels":["general","image-classification","object-detection","segmentation"],"companies":["Adobe","Airbnb","Amazon","Apple","Bloomberg","Cloudflare","Coinbase","DoorDash","Goldman Sachs","Google","Hashicorp","Hugging Face","Instacart","LinkedIn","Lyft","Meta","Microsoft","MongoDB","NVIDIA","Netflix","OpenAI","Oracle","Plaid","Salesforce","Scale Ai","Snap","Snowflake","Square","Tesla","Twitter","Two Sigma","Uber","Zoom"],"stats":{"total":22,"beginner":7,"intermediate":5,"advanced":10,"newThisWeek":13}}