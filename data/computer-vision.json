{"questions":[{"id":"q-1018","question":"You’re building a mobile camera app that auto-captures frames from a live video feed. Describe a practical, beginner-friendly pipeline to decide whether a frame is usable by applying two simple checks: (1) sharpness via variance of Laplacian, (2) exposure via histogram-based brightness. Explain how thresholds would be chosen, how you'd adapt them across lighting, and provide a minimal code snippet illustrating the core checks?","answer":"Compute sharpness as the variance of the Laplacian on grayscale and assess exposure with 2nd and 98th percentile brightness. Require low>=20 and high<=235, and sharpness>100. Adapt thresholds with a 3","explanation":"## Why This Is Asked\n\nTests ability to design a lightweight, production-friendly CV heuristic using simple, well-known metrics, and to reason about robustness to lighting without heavy models.\n\n## Key Concepts\n\n- Variance of Laplacian as a sharpness proxy\n- Histogram-based exposure sensing (2nd/98th percentile)\n- Thresholding plus simple adaptation for devices\n\n- Trade-offs between false positives/negatives\n\n## Code Example\n\n```python\nimport cv2, numpy as np\ndef frame_ok(frame, sharp_th=100.0, low_thr=20, high_thr=235):\n    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n    sharp = cv2.Laplacian(gray, cv2.CV_64F).var()\n    lo, hi = np.percentile(gray, (2, 98))\n    exp_ok = (lo >= low_thr) and (hi <= high_thr)\n    return (sharp > sharp_th) and exp_ok\n```\n\n## Follow-up Questions\n\n- How would you calibrate thresholds across cameras?\n- How would you extend to color balance and motion blur?\n","diagram":"flowchart TD\nFrame[Frame In] --> Sharp[Compute Laplacian Variance]\nSharp --> Decide1{Sharpness > 100?}\nDecide1 -- Yes --> Exp[Compute Exposure]\nExp --> Decide2{Exposure OK?}\nDecide2 -- Yes --> Accept[Accept Frame]\nDecide2 -- No --> Reject[Reject Frame]\nDecide1 -- No --> Reject","difficulty":"beginner","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","LinkedIn","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T19:32:04.521Z","createdAt":"2026-01-12T19:32:04.521Z"},{"id":"q-1093","question":"Design a privacy-preserving, real-time hand-gesture recognition system for video calls on consumer laptops (720p camera) that distinguishes a small set of gestures (hand-raise, thumbs-up, peace) without exposing facial details. Must run on-device at 30–60 FPS, handle lighting/occlusion, and support federated fine-tuning with differential privacy. Outline architecture, data strategy, and evaluation plan?","answer":"Two-stage on-device design: a lightweight hand ROI extractor (tiny CNN such as MobileNetV3) plus a temporal classifier (1D conv or efficient Transformer) operating on a short frame window. Train with ","explanation":"## Why This Is Asked\nThis question probes practical on-device CV with privacy constraints, latency pressure, and data privacy.\n\n## Key Concepts\n- Lightweight models, ROI extraction, temporal reasoning\n- Quantization, pruning, on-device inference\n- Federated learning, differential privacy, privacy budgets\n- Evaluation: latency, memory, F1/precision-recall under occlusion\n\n## Code Example\n```javascript\n// Pseudocode: frame window prep for on-device gesture model\nfunction prepareWindow(frames) {\n  const ROI = detectHandROI(frames[0]);\n  const window = frames.slice(-16).map(f => crop(f, ROI));\n  return stack(window);\n}\n```\n\n## Follow-up Questions\n- How would you measure and mitigate drift when lighting changes across devices?\n- How would you handle new gestures without retraining all devices?","diagram":null,"difficulty":"intermediate","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","Tesla","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T22:25:53.986Z","createdAt":"2026-01-12T22:25:53.986Z"},{"id":"q-1108","question":"Design a real-time, on-device hand pose/gesture system for air-drawing in a video-conferencing app. From a monocular 1080p60 stream, infer 2D/3D hand pose with <40ms latency on a CPU, robust to occlusion and varied skin tones, and support at least 5 gestures (draw, erase, next, previous, pointer). Outline data needs, model architecture, latency optimizations, temporal consistency, and evaluation plan?","answer":"Proposed approach: deploy a lightweight 2D keypoint detector plus a compact 3D lifting head on-device, with a short temporal filter (Kalman/temporal conv). Use INT8 quantization and ONNX Runtime for C","explanation":"## Why This Is Asked\nThis question probes on-device real-time hand pose recognition, latency budgeting, robustness to occlusion and skin tone, and interface with gesture-based controls for conferencing.\n\n## Key Concepts\n- On-device real-time inference\n- 2D keypoint + 3D lifting\n- Temporal smoothing for stability\n- Occlusion and bias robustness\n- Quantization and runtime deployment\n\n## Code Example\n```javascript\nfunction inferHandPose(frame) {\n  const kps2d = detector2D(frame);\n  const pose3D = liftTo3D(kps2d);\n  return smoothPose(pose3D);\n}\n```\n\n## Follow-up Questions\n- How would you handle multiple hands and long occlusions?\n- How would you validate latency and diagnose spikes in production?\n","diagram":"flowchart TD\nA[Capture 1080p60 frame] --> B[2D hand-keypoint detector]\nB --> C[3D pose lifting]\nC --> D[Temporal smoothing]\nD --> E[Gesture classifier]\nE --> F[Action: air-draw / next / prev]","difficulty":"advanced","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Plaid","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T23:17:25.742Z","createdAt":"2026-01-12T23:17:25.742Z"},{"id":"q-1270","question":"Design a real-time monocular 3D detector for an assembly line that estimates 6-DoF pose of tools from a single RGB camera, achieving sub-50ms per frame on embedded hardware. Use a lightweight backbone with self-supervised pretraining plus a small labeled set; recover pose via differentiable PnP from 2D-3D correspondences with a temporal filter and reprojection losses. Monitor drift with streaming per-frame errors?","answer":"Propose a lightweight monocular 3D detector on edge hardware that predicts 2D-3D keypoint correspondences and a scale per tool, then solves 6-DoF via differentiable PnP. Use self-supervised pretrainin","explanation":"## Why This Is Asked\nIndustrial CV on the edge demands real-time 3D understanding from monocular inputs with limited labels. The design must handle latency, occlusion, and drift in long shifts.\n\n## Key Concepts\n- Monocular 3D pose estimation\n- Differentiable PnP\n- Self-supervised learning\n- Temporal filtering\n- Embedded/edge deployment\n\n## Code Example\n```python\n# Simple differentiable PnP pose recovery sketch\nimport torch\n\ndef pose_from_keypoints(pts2d, pts3d, K):\n    # Placeholder for differentiable PnP optimization\n    # Return rotation R and translation t\n    R = torch.eye(3)\n    t = torch.zeros(3)\n    return R, t\n```\n\n## Follow-up Questions\n- How to robustly handle occlusions and outliers in keypoint matches?\n- What evaluation protocol would reliably detect drift over 24h? ","diagram":null,"difficulty":"intermediate","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Hashicorp","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T07:35:09.692Z","createdAt":"2026-01-13T07:35:09.692Z"},{"id":"q-1417","question":"Given a single RGB camera mounted on an autonomous delivery drone operating in urban environments, design a real-time system to detect and track pedestrians and other vulnerable actors at 30 fps under nighttime and rain conditions. Propose data strategy (synthetic rain + real), model backbone, temporal fusion and latency targets, and safety/failover mechanisms?","answer":"Use a lightweight detector such as EfficientDet-D3 with a Kalman-filter-based tracker and a short-term memory for temporal fusion; train on rain-augmented synthetic data plus a small labeled real-worl","explanation":"## Why This Is Asked\nDesigning perception for aerial platforms under adverse weather tests data strategy, real-time constraints, and safety guarantees in production-like settings.\n\n## Key Concepts\n- Single-view detection under rain/night; robustness to weather\n- Temporal fusion and multi-object tracking in a streaming context\n- Domain adaptation: synthetic rain + real rainy data\n- Edge latency budgets and model backbones\n- Safety/failover and occlusion handling\n\n## Code Example\n```python\n# Pseudo-code: simple Kalman-based track update\nclass Track:\n    def update(self, measurement):\n        self.state = self.kf.update(self.state, measurement)\n```\n\n## Follow-up Questions\n- How would you validate performance across rain intensities and lighting?\n- How would you quantify and mitigate false positives in crowded scenes?\n","diagram":"flowchart TD\n  A[Input: video stream] --> B[Preprocessing]\n  B --> C[Detection & Tracking]\n  C --> D[Temporal Fusion & Association]\n  D --> E[Output: Trajectories & Alerts]","difficulty":"advanced","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Cloudflare","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T15:56:01.086Z","createdAt":"2026-01-13T15:56:01.086Z"},{"id":"q-1435","question":"For aerial inspection of solar farms, design a CV system to detect and grade micro-cracks on solar panels from drone video with limited labeled data. Specify an architecture that detects tiny defects, data-augmentation strategies (synthetic crack overlays, texture randomization), domain adaptation, and an edge-friendly output (box, segmentation mask, and severity score). Include evaluation protocol and latency targets?","answer":"I’d use a two-stage detector with a high-res backbone (EfficientNetV2-S + FPN) and a light segmentation head. Train with synthetic crack overlays on panel textures plus self-supervised pretraining on ","explanation":"## Why This Is Asked\nTests ability to design CV solutions under data scarcity, with tiny defect detection, multi-task outputs, and edge deployment constraints.\n\n## Key Concepts\n- Tiny defect detection in high-res imagery\n- Synthetic data and SSL for labels-scarce regimes\n- Domain adaptation and edge-model optimization\n- Multi-task outputs: bbox, mask, severity\n\n## Code Example\n```python\n# Pseudo training loop outline\nfor batch in dataloader:\n  imgs, boxes, masks, sev = batch\n  feats = backbone(imgs)\n  pred_boxes, pred_masks = heads(feats)\n  loss = focal_loss(pred_boxes, boxes) + bce_loss(pred_masks, masks) + mse_loss(sev, severity)\n  loss.backward()\n  opt.step()\n```\n\n## Follow-up Questions\n- How would you quantify model calibration for severity predictions?\n- What ablation would you run to isolate the impact of synthetic data?","diagram":"flowchart TD\n  A[Drone Frame] --> B[Preprocess] \n  B --> C[SSL Pretraining] \n  C --> D[Detector (backbone + heads)] \n  D --> E[Postprocess: NMS + severity] \n  E --> F[Edge Deployment]","difficulty":"advanced","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","MongoDB","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T16:56:34.735Z","createdAt":"2026-01-13T16:56:34.735Z"},{"id":"q-1861","question":"You're given an overhead RGB image of a table with scattered coins. Design a beginner-friendly pipeline to count the number of coins and estimate their approximate denomination from a single image. Include preprocessing, circle/contour detection, radius-based grouping to separate coin types, handling shadows, and a minimal code sketch using OpenCV to detect circular shapes?","answer":"Preprocess: grayscale, CLAHE, and blur to reduce noise. Use HoughCircles (dp=1.2, minDist=20, minRadius=10, maxRadius=60) to detect coin-like circles. Merge duplicates by proximity, bucket by radius i","explanation":"## Why This Is Asked\nTests ability to design a practical CV pipeline using lightweight, non-ML methods for a common task.\n\n## Key Concepts\n- Circle detection with Hough transform\n- Radius-based grouping for coin types\n- Noise/shadow handling with blur and histogram equalization\n- Validation with synthetic data\n\n## Code Example\n```javascript\n// OpenCV.js-like pseudocode\nlet src = cv.imread('table.jpg');\ncv.cvtColor(src, src, cv.COLOR_RGBA2GRAY);\ncv.equalizeHist(src, src);\ncv.GaussianBlur(src, new cv.Size(9,9), 2);\nlet circles = new cv.Mat();\ncv.HoughCircles(src, circles, cv.HOUGH_GRADIENT, 1.2, 20, 50, 30, 10, 60);\n```\n\n## Follow-up Questions\n- How would you address perspective distortions?\n- How would you validate robustness with lighting changes?\n","diagram":"flowchart TD\n  A[Input image] --> B[Preprocess]\n  B --> C[Circle detection]\n  C --> D[Filter duplicates]\n  D --> E[Group by radius]\n  E --> F[Count & report]","difficulty":"beginner","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Instacart","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T14:48:10.414Z","createdAt":"2026-01-14T14:48:10.414Z"},{"id":"q-1945","question":"Design a real-time anomaly-detection pipeline for a single RGB camera monitoring an industrial warehouse. Propose a memory-augmented autoencoder approach that runs on an edge device at 15–20 FPS, uses frame-wise reconstruction error plus optical-flow residuals, maintains a fixed-size normal-pattern memory, and includes a drift-adaptation strategy with minimal labeling?","answer":"Implement a memory-augmented autoencoder (MAE) with a lightweight encoder-decoder and a fixed memory bank of normal latent prototypes. For each frame, retrieve the nearest memory vectors, fuse them wi","explanation":"## Why This Is Asked\\n\\nTests practical CV design under edge constraints, handling concept drift with minimal labels, and fusing appearance with motion cues.\\n\\n## Key Concepts\\n- Memory-augmented autoencoder for normal-pattern modeling\\n- Fixed-size memory bank and online updates\\n- Frame reconstruction error + optical-flow residuals for anomaly scoring\\n- Edge deployment with INT8/quantized backbones\\n\\n## Code Example\\n```javascript\\n// Skeleton: MAE forward pass and anomaly scoring\\nfunction MAEForward(frame, model, memory) {\\n  // encode, memory lookup, decode, compute error\\n}\\n```\\n\\n## Follow-up Questions\\n- How would you quantify drift and decide when to refresh memory?\\n- How would you evaluate latency vs. accuracy trade-offs on Jetson?","diagram":null,"difficulty":"intermediate","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","Hugging Face"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T17:58:29.262Z","createdAt":"2026-01-14T17:58:29.262Z"},{"id":"q-2131","question":"In a factory setting, design a real-time 2-view RGB tool-pose tracking system that achieves sub-60ms per frame on edge hardware. Use two synchronized cameras, a lightweight backbone, and a differentiable PnP with 2D-3D correspondences; include cross-view fusion, a temporal filter, and a self-supervised pretraining strategy with synthetic data. How would you validate drift and occlusion resilience?","answer":"Propose a dual-RGB edge pipeline: detect 2D keypoints on each view with a small CNN (e.g., MobileNetV3) and fuse via a learned cross-view encoder; compute 6-DoF tool pose with differentiable PnP + rob","explanation":"## Why This Is Asked\nTests multi-view real-time pose estimation under strict latency, with practical constraints on edge hardware, including cross-view fusion, differentiable PnP, and self-supervised learning.\n\n## Key Concepts\n- multi-view fusion\n- differentiable PnP and RANSAC\n- edge latency optimization\n- self-supervised pretraining with synthetic data\n- drift and occlusion resilience\n\n## Code Example\n```javascript\n// Skeleton: data flow for two views\nfunction processFrame(view1, view2, intrinsics, extrinsics){\n  // 1) detect 2D keypoints in both views\n  // 2) establish cross-view correspondences\n  // 3) triangulate to 3D points\n  // 4) solve PnP for 6-DoF pose\n  // 5) apply temporal filter for stability\n}\n```\n\n## Follow-up Questions\n- How would you evaluate drift over long streaming sequences?\n- How would you handle calibration drift or a camera failure during inference?","diagram":"flowchart TD\n  A[Two RGB cameras] --> B[2D keypoint detectors]\n  B --> C[Cross-view fusion encoder]\n  C --> D[Triangulation to 3D]\n  D --> E[Differentiable PnP pose]\n  E --> F[Temporal fusion (Kalman)]","difficulty":"advanced","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","OpenAI","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T04:14:43.759Z","createdAt":"2026-01-15T04:14:43.759Z"},{"id":"q-2242","question":"In a warehouse setting, design an edge-friendly CV pipeline that counts and localizes pallets from a single moving RGB camera mounted on a forklift, using self-supervised depth cues from motion and a lightweight 3D detector to output per-pallet 3D bounding boxes with uncertainty at 30 Hz. Address occlusion, dynamic workers, and domain shift between day and night?","answer":"I'd deploy a lightweight 2D pallet detector (EfficientDet-D3) to generate proposals, estimate depth from ego-motion via structure-from-motion, and use a differentiable 3D box head for per-pallet boxes","explanation":"## Why This Is Asked\nAssesses end-to-end design for real-world inventory sensing under occlusion, speed, and lighting variation, emphasizing self-supervised depth, differentiable 3D reasoning, and uncertainty estimation on edge hardware.\n\n## Key Concepts\n- Lightweight 2D detector (EfficientDet)\n- Self-supervised depth from ego-motion\n- Differentiable 3D box regression\n- Uncertainty via MC dropout and aleatoric depth\n- Kalman tracking + data association\n- Edge deployment (latency, memory)\n\n## Code Example\n```javascript\n// Pseudocode\nfunction pipeline(frame, state){\n  proposals = detector(frame)\n  depth = depthFromMotion(frame, state)\n  boxes = reg3D(proposals, depth)\n  tracks = kalmanTrack(boxes, state)\n  return tracks\n}\n```\n\n## Follow-up Questions\n- How would you evaluate robustness to motion blur and occlusion?\n- How would you adapt the system to different pallet shapes/sizes or new warehouses without labeled data?","diagram":null,"difficulty":"advanced","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T09:00:25.390Z","createdAt":"2026-01-15T09:00:25.390Z"},{"id":"q-2333","question":"Design a beginner-friendly pipeline to detect and count red boxes moving on a conveyor using a single RGB camera in real time. Include HSV color space selection, dual-range red thresholds, noise removal with morphology, contour filtering by area/shape, and a simple line-crossing tracker. Provide a minimal OpenCV.js code snippet for core steps?","answer":"Use HSV segmentation for red with two ranges, apply Gaussian blur and morphology to reduce noise, detect contours, filter by area and aspect ratio to identify boxes, compute centroids, and count when ","explanation":"## Why This Is Asked\n\nThis task yields a concrete, end-to-end CV workflow suitable for entry-level projects that run in real time on modest hardware. It tests practical skills in color segmentation, noise handling, contour reasoning, and a simple, robust counting strategy.\n\n## Key Concepts\n\n- HSV color space and red wrap-around handling\n- Morphology for noise reduction\n- Contour area and aspect-ratio filtering\n- Centroid computation and line-crossing tracking\n- Lightweight calibration for varying lighting\n\n## Code Example\n\n```javascript\n// OpenCV.js core steps for red box detection\nfunction detectRedBoxes(src) {\n  // convert to HSV, threshold red in two ranges, combine masks\n  // blur and apply morphology\n  // find contours, filter by area/shape\n  // compute centroids and detect crossing against a line\n}\n```\n\n## Follow-up Questions\n\n- How would you handle occlusion or partial boxes?\n- How would you validate with limited labeled data and choose metrics?","diagram":null,"difficulty":"beginner","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Google","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T13:08:46.849Z","createdAt":"2026-01-15T13:08:46.849Z"},{"id":"q-2377","question":"Scenario: fixed overhead RGB camera watches a single shoebox on a shelf. Design a beginner-friendly, non-deep-learning pipeline to decide whether the box is upright within ±10 degrees using contour-based detection. Explain preprocessing, edge/shape heuristics, thresholds, and how to handle perspective distortion and occlusion. Include a minimal Python OpenCV snippet that outputs 'upright' or 'tilted'?","answer":"Convert to grayscale and blur (3x3). Run Canny to get edges, then find contours and keep the largest near-rectangular contour via minAreaRect and a 4-corner check. Normalize the angle to [-90,90], tak","explanation":"## Why This Is Asked\nTests building a robust, non-ML CV heuristic that handles real-world perspective and noise.\n\n## Key Concepts\n- Contour detection and minAreaRect for orientation\n- Edge extraction and thresholding for robustness\n- Angle normalization and aspect-ratio checks\n- Frame-consistency to reject jitter\n\n## Code Example\n```python\nimport cv2\nimport numpy as np\n\ndef upright_status(img):\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    blur = cv2.GaussianBlur(gray, (3,3), 0)\n    edges = cv2.Canny(blur, 50, 150)\n    cnts, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    if not cnts:\n        return 'tilted'\n    c = max(cnts, key=cv2.contourArea)\n    rect = cv2.minAreaRect(c)\n    angle = abs(rect[2])\n    if angle > 45:\n        angle = 90 - angle\n    w, h = rect[1]\n    aspect = w / h if h else 0\n    if angle <= 10 and 0.8 <= aspect <= 1.2:\n        return 'upright'\n    return 'tilted'\n```\n\n## Follow-up Questions\n- How would you scale this to multiple boxes in a scene? \n- How would you handle strong perspective distortion or occlusion while keeping it beginner-friendly?","diagram":"flowchart TD\n  A[Acquire image] --> B[Preprocess]\n  B --> C[Edge detect]\n  C --> D[Find contours]\n  D --> E[Select largest near-rect]{Is near-rect?}\n  E -->|Yes| F[MinAreaRect and angle]\n  F --> G[Compute upright status]\n  G --> H[Output Upright/Tilted]","difficulty":"beginner","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Oracle","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T15:42:44.654Z","createdAt":"2026-01-15T15:42:44.654Z"},{"id":"q-2546","question":"Design a real-time monocular hand pose and gesture recognition system for AR UI on a battery-constrained headset. Propose a lightweight architecture (e.g., two-branch network with 2D heatmaps and a temporal encoder), latency target <25 ms per frame on edge hardware, occlusion handling, and 3D hand pose estimation—describe data strategy, loss terms, and evaluation?","answer":"Two-branch lightweight model: Branch A generates 2D hand keypoint heatmaps using a MobileNetV3-like backbone; Branch B encodes short-term temporal context. 3D pose is recovered through a differentiable hand model with a PnP solver, achieving <25ms latency via model quantization and efficient temporal fusion. Occlusion handling incorporates visibility prediction and temporal smoothing, while training combines synthetic and real data with domain adaptation.","explanation":"## Why This Is Asked\nTests practical hand tracking for AR UI with tight latency, occlusion handling, and edge deployment, aligning with high-performance computer vision needs at tech leaders.\n\n## Key Concepts\n- Lightweight backbones for 2D heatmaps\n- Temporal encoders (GRU/Transformer) for stability\n- Differentiable hand model with PnP for 3D pose\n- Occlusion handling via visibility prediction and temporal smoothing\n- Synthetic + real data with domain adaptation; real-time evaluation metrics MPJPE, PCK3D\n\n## Code Example\n```python\n# Pseudocode: fuse 2D heatmaps with temporal features to estimate 3D hand pose\nheatmaps = branch_a(image)  # 2D keypoint detection\ntemporal_features = branch_b(heatmaps_sequence)  # temporal context\npose_3d = hand_model_solver(heatmaps, temporal_features)  # 3D pose recovery\n```","diagram":"flowchart TD\n  A[Input: monocular video] --> B[Backbone: 2D heatmaps]\n  A --> C[Temporal encoder]\n  B --> D[3D pose via differentiable hand model]\n  C --> D\n  D --> E[Occlusion vis predictor + smoothing]\n  E --> F[Gesture decoding -> UI events]","difficulty":"intermediate","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","NVIDIA","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T05:30:31.054Z","createdAt":"2026-01-15T22:33:53.343Z"},{"id":"q-2778","question":"Design a compact on-device system to detect and track pedestrians in urban traffic and estimate their 3D pose (6-DoF) from a monocular video feed. Constraints: sub-40 ms per frame on an embedded GPU, a lightweight backbone with self-supervised pretraining on unlabeled video plus a small labeled set, and robustness to occlusion, motion blur, and adverse weather. Describe architecture, data strategy, losses, and evaluation methodology?","answer":"Design a compact on-device system: a 2D keypoint detector + 3D pose head (MobileNetV3-scale) with temporal fusion. Reconstruct 6‑DoF pedestrian pose from 2D joints with differentiable PnP and a short ","explanation":"## Why This Is Asked\nAssesses on-device CV design, latency budgeting, and robust 3D human pose estimation for safety-critical driving.\n\n## Key Concepts\n- On-device real-time inference with a lightweight backbone\n- 2D-to-3D pose reconstruction and Kalman filtering\n- Self-supervised pretraining from unlabeled video\n- Robustness to occlusion and adverse weather\n- Quantization and latency accounting\n\n## Code Example\n```javascript\n// Pseudocode for pipeline\nclass PedestrianPoseEstimator {\n  constructor() { /* load detector, 3D head, PnP module, tracker */ }\n  forward(frame) {\n    const keypoints2D = this.detector.detect(frame)\n    const pose3D = this.poseHead.estimate(keypoints2D)\n    const pose6DoF = differentiablePnP(pose3D, this.cameraIntrinsics)\n    return this.tracker.update(pose6DoF)\n  }\n}\n```\n\n## Follow-up Questions\n- How would you extend this to multi-pedestrian tracking with identity preservation?\n- What failure modes are most critical in heavy rain and how would you mitigate them?","diagram":"flowchart TD\n  A[Input video frame] --> B[2D keypoint detector]\n  B --> C[3D pose head]\n  C --> D[Differentiable PnP]\n  D --> E[Kalman tracker]\n  E --> F[On-device inference output]\n  F --> G[Evaluation]\n","difficulty":"advanced","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","Lyft","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T11:36:45.771Z","createdAt":"2026-01-16T11:36:45.771Z"},{"id":"q-3001","question":"Design a real-time multi-view RGB-D fusion system to estimate 6-DoF tool poses and hand interactions on a 4-camera rig. Use a two-branch head: per-view 2D heatmaps plus a depth-voxel fusion module feeding a lightweight 3D pose regressor. Train with synthetic data and self-supervised cross-view consistency; monitor drift via reprojection error and pose-trajectory. Latency <40 ms?","answer":"Propose a 4-view RGB-D fusion with a dual-branch head: per-view 2D heatmaps and a depth-voxel fusion module feeding a lightweight 3D pose regressor. Use differentiable fusion, cross-view consistency l","explanation":"## Why This Is Asked\n\nTests multi-view fusion, edge latency, and robust data association in manufacturing/UD scenarios. Probes training with synthetic data and drift control under strict latency budgets.\n\n## Key Concepts\n\n- Multi-view fusion across RGB-D cameras\n- Edge latency and model compression\n- Self-supervised and synthetic data strategies\n- Cross-view consistency and drift monitoring\n- Data association across views\n\n## Code Example\n\n```javascript\n// High-level fusion outline\nfunction fuseViews(viewFeatures, depthMaps){\n  // voxel pooling and fusion\n  return fused3D;\n}\n```\n\n## Follow-up Questions\n\n- How would you handle unseen tools during inference?\n- How would you evaluate latency vs accuracy on an embedded GPU?","diagram":"flowchart TD\n  A[Input: 4 RGB-D streams] --> B[Per-view 2D heatmaps]\n  B --> C[Depth-voxel fusion]\n  C --> D[Lightweight 3D pose regressor]\n  D --> E[Tool pose + hand interaction outputs]\n  E --> F[Metrics: latency, drift, accuracy]","difficulty":"advanced","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","DoorDash","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T20:41:00.651Z","createdAt":"2026-01-16T20:41:00.651Z"},{"id":"q-3027","question":"Scenario: A fixed RGB camera watches a horizontal conveyor belt in a packaging line. Bottles pass one by one; you must count each item and verify the cap is present and seated using only lightweight CV techniques (no deep nets). Propose a beginner-friendly pipeline: (1) background subtraction to segment bottles, (2) contour-based separation and simple tracking to avoid double counting, (3) HSV-based cap presence check, (4) a temporal filter to smooth counts across frames. Explain thresholds, occlusion handling, and provide a minimal Python/OpenCV outline?","answer":"Approach uses a Gaussian Mixture Model (GMM) or MOG2 background subtraction to segment bottles, filters by aspect ratio and area constraints, samples the cap Region of Interest (ROI) in HSV color space to detect cap presence when hue and saturation values fall within calibrated thresholds, and implements a per-bottle tracker to prevent double counting combined with temporal smoothing for robust counting.","explanation":"## Why This Is Asked\n\nThis question tests practical computer vision skills: robust object counting using lightweight methods, occlusion handling strategies, and threshold tuning in real-world industrial applications. The constraint of avoiding deep networks emphasizes the importance of classical CV techniques.\n\n## Key Concepts\n\n- Background subtraction for moving object detection\n- Contour filtering and simple object tracking\n- HSV color space thresholding for component verification\n- Temporal counting logic and line-crossing detection\n- Occlusion handling and threshold robustness\n- Real-time performance considerations\n\n## Code Example\n\n```python\nimport cv2\nimport numpy as np\n\n# Initialize background subtractor\nbackSub = cv2.createBackgroundSubtractorMOG2(detectShadows=True)\n\nframe = cv2.imread('frame.png')\n# Background subtraction to segment bottles\nfg_mask = backSub.apply(frame)\n# Morphological operations to clean mask\nkernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\nfg_mask = cv2.morphologyEx(fg_mask, cv2.MORPH_OPEN, kernel)\n\n# Find and filter contours\ncontours, _ = cv2.findContours(fg_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\nfor contour in contours:\n    area = cv2.contourArea(contour)\n    x, y, w, h = cv2.boundingRect(contour)\n    aspect_ratio = float(w) / h\n    \n    # Filter by area and aspect ratio for bottles\n    if area > 500 and 0.3 < aspect_ratio < 0.7:\n        # Extract cap ROI and check in HSV space\n        cap_roi = frame[y:y+h//4, x:x+w]\n        hsv_cap = cv2.cvtColor(cap_roi, cv2.COLOR_BGR2HSV)\n        # Apply HSV thresholds for cap detection\n        cap_mask = cv2.inRange(hsv_cap, (h_low, s_low, v_low), (h_high, s_high, v_high))\n        cap_present = np.sum(cap_mask) > threshold\n```\n\n## Follow-up Questions\n\n- How would you handle varying lighting conditions across different shifts?\n- What strategies would you implement for overlapping bottles during high throughput?\n- How could you extend this system to detect bottle fill levels?\n- What performance optimizations would you consider for real-time processing?","diagram":"flowchart TD\n  Start[Start] --> Seg[Background Subtraction & Contour Detection]\n  Seg --> Cap[Cap Presence Check (HSV)]\n  Cap --> Count[Temporal Count & Line Crossing]\n  Count --> End[Total Count]\n","difficulty":"beginner","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Oracle","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T05:43:56.300Z","createdAt":"2026-01-16T21:43:48.972Z"},{"id":"q-3066","question":"On a fast, fixed conveyor, design a real-time monocular 6-DoF pose estimator for a robotic gripper picking tools with glossy surfaces and changing lighting. Target <=25 ms per frame on embedded edge hardware; use a lightweight backbone (MobileNetV3+FPN), differentiable PnP from 2D-3D correspondences, and a short temporal fusion. Describe data strategy, losses (reprojection, pose, temporal consistency), and how you monitor drift and recover from failures?","answer":"Implement a lightweight backbone using MobileNetV3+FPN to predict 2D keypoints and estimate 6-DoF pose through a differentiable PnP layer. Train with synthetic domain randomization to handle lighting variations and glossy surface reflections, complemented by real-world fine-tuning. Optimize for embedded deployment via quantization and TensorRT to achieve ≤25ms per frame. Apply short temporal fusion (3-5 frames) using an EKF to smooth pose estimates and detect drift. Monitor reprojection error and keypoint confidence, triggering recovery when thresholds are exceeded. Loss functions combine reprojection error, pose regularization, and temporal consistency terms.","explanation":"## Why This Is Asked\nAssess real-time monocular pose estimation with edge constraints and data strategy under appearance changes.\n\n## Key Concepts\n- Monocular 6-DoF pose with differentiable PnP\n- Temporal fusion and drift monitoring\n- Edge hardware optimization (quantization, TensorRT)\n- Domain randomization for glossy surfaces\n\n## Code Example\n```javascript\n// placeholder snippet showing a differentiable PnP integration\nfunction differentiablePnP(keypoints2D, points3D, intrinsics) {\n  // implement PnP with gradient flow\n  return estimatePose(keypoints2D, points3D, intrinsics);\n}\n```\n\n## Follow-up Questions\n- How do you handle occlusions in keypoint detection?\n- What recovery strategies work best for tracking failures?\n- How do you balance accuracy vs. speed on embedded hardware?","diagram":null,"difficulty":"advanced","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","Microsoft","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T05:18:31.011Z","createdAt":"2026-01-16T23:36:16.161Z"},{"id":"q-3102","question":"Design a privacy-preserving real-time perception pipeline for monocular video on an embedded device that detects cars, pedestrians, and other scene objects while obfuscating faces and license plates in intermediate features. Outline architecture (edge detector, privacy module, differentiable de-identification loss), latency target <50 ms/frame, and evaluation combining object mAP with privacy leakage metrics?","answer":"Edge-first detector processes frames on-device; faces and license plates are passed through a learnable de-identification module that replaces sensitive regions with anonymized tokens before any transmission or logging. The architecture consists of: (1) a lightweight backbone (e.g., MobileNetV3) for feature extraction, (2) a privacy module that applies differentiable obfuscation to face/license plate regions, and (3) detection heads for cars, pedestrians, and other objects. The de-identification loss combines adversarial privacy loss with reconstruction constraints, ensuring sensitive information cannot be recovered while preserving contextual features for detection. Latency optimization includes model quantization, early-exit strategies, and hardware-aware pruning to achieve <50ms/frame on embedded platforms.","explanation":"## Why This Is Asked\n\nAssesses the ability to architect privacy-aware computer vision systems under strict edge constraints while balancing accuracy and privacy guarantees. Requires concrete architecture, differentiable privacy terms, and rigorous evaluation plans.\n\n## Key Concepts\n\n- Privacy-preserving CV and de-identification modules\n- Differentiable privacy loss and threat-model-aware evaluation\n- Edge/on-device inference and latency budgeting\n- Trade-offs between detection accuracy and privacy guarantees\n\n## Code Example\n\n```javascript\n// Lightweight total loss combining detection accuracy and privacy\nfunction computeTotalLoss(detectionPred, privacyPred, targets) {\n    const detectionLoss = computeDetectionLoss(detectionPred, targets);\n    const privacyLoss = computePrivacyLoss(privacyPred, targets.sensitiveRegions);\n    \n    // Weighted combination with privacy budget constraint\n    return detectionLoss + lambdaPrivacy * privacyLoss;\n}\n\n// Differentiable de-identification module\nclass PrivacyModule {\n    obfuscate(features, sensitiveRegions) {\n        // Apply learnable obfuscation to sensitive regions\n        return features.map((feat, i) => \n            sensitiveRegions[i] ? this.anonymize(feat) : feat\n        );\n    }\n}\n```","diagram":"flowchart TD\n  A[Frame] --> B[Edge Detector]\n  B --> C[Privacy Module]\n  C --> D[Object Detector]\n  D --> E[Outputs]","difficulty":"advanced","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Netflix","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T03:46:01.354Z","createdAt":"2026-01-17T02:21:34.230Z"},{"id":"q-3228","question":"Design a real-time on-edge computer vision pipeline to identify grocery items on a fast-moving conveyor using a single RGB camera. The catalog resides in a MongoDB collection; new items arrive weekly. Achieve <40 ms/frame inference on embedded GPUs, handle unknown items with open-set recognition, and support rapid adaptation with a small labeled set plus self-supervised pretraining. Outline architecture, data flow, and evaluation?","answer":"On-edge detector with a lightweight backbone (EfficientNet-Lite) running under 40 ms/frame; two-stage: fast region proposal then compact classifier with an open-set head. Catalog in MongoDB with per-i","explanation":"## Why This Is Asked\\n\\nAdvanced edge inference with open-set handling is realistic for on-site processing and catalog integration. It tests both CV skills and data engineering with a MongoDB-backed catalog, plus incremental learning in a streaming setting.\\n\\n## Key Concepts\\n\\n- Edge latency and hardware constraints\\n- Open-set recognition and calibration\\n- Self-supervised pretraining and incremental learning\\n- Catalog indexing and embedding storage in MongoDB\\n- Evaluation in a streaming, production-like scenario\\n\\n## Code Example\\n\\n```python\\ndef infer(frame, model):\\n  logits = model(frame)\\n  return softmax(logits)\\n```\\n\\n## Follow-up Questions\\n\\n- How would you onboard new items without downtime?\\n- How would you monitor model drift and rollback strategies?","diagram":null,"difficulty":"advanced","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T07:39:42.261Z","createdAt":"2026-01-17T07:39:42.261Z"},{"id":"q-3291","question":"Design a real-time monocular 6-DoF object tracker for a single RGB camera that can track up to 4 tools on an assembly-line in clutter and variable lighting. Propose a lightweight backbone (MobileNetV3) for initial pose, a differentiable renderer-based refinement (e.g., PyTorch3D) with reprojection and silhouette losses, and a streaming UKF to fuse measurements and handle occlusions. Explain data strategy, losses, latency targets (60 FPS), and evaluation protocol?","answer":"A practical answer would describe a two-stage pipeline: 1) an anchor-free detector using MobileNetV3 for fast 2D-keypoint-into-3D hypotheses, 2) a differentiable renderer (PyTorch3D) to refine 6-DoF w","explanation":"## Why This Is Asked\nTests monocular 3D tracking, differentiable rendering, and real-time fusion under occlusion. It probes design choices for embedded constraints and data strategies across synthetic-real domains.\n\n## Key Concepts\n- Real-time monocular 6-DoF tracking\n- Lightweight backbone, differentiable renderer\n- 2D reprojection and silhouette losses\n- Temporal fusion (UKF) and occlusion handling\n- Data augmentation and domain randomization\n\n## Code Example\n```javascript\n// Pseudocode for UKF pose update\nfunction ukfPredict(state, dt){ /* ... */ }\n```\n\n## Follow-up Questions\n- How would you validate drift and set reinitialization thresholds?\n- What ablation studies would you run to justify the renderer-based refinement?","diagram":"flowchart TD\n  A[RGB Frame] --> B[2D Detector: MobileNetV3]\n  B --> C[2D-3D Hypotheses]\n  C --> D[Differentiable Renderer Refinement]\n  D --> E[Temporal Fusion: UKF]\n  E --> F[Tracked Poses]","difficulty":"advanced","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Instacart","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T10:32:11.309Z","createdAt":"2026-01-17T10:32:11.309Z"},{"id":"q-3439","question":"Design a robust multi-view CV pipeline to detect tampering in high-volume financial documents scanned from different angles; propose a light 2D-3D fusion model using a differentiable renderer to verify cross-view consistency, augmented with synthetic tamper data and a small real set, plus a temporal consistency module to tolerate scan noise. Include latency targets and evaluation protocol?","answer":"Proposed solution: a dual-branch CV system that ingests multi-view scans of each document. Branch A uses a lightweight backbone to extract 2D features; Branch B uses a differentiable renderer to enfor","explanation":"## Why This Is Asked\nIn finance, document integrity is critical. This question probes multi-view reasoning, 2D-3D fusion, differentiable rendering, and temporal stability under latency constraints, all on edge hardware.\n\n## Key Concepts\n- Multi-view fusion with differentiable rendering\n- Synthetic data with domain randomization\n- Lightweight backbones for edge latency\n- Temporal consistency (Kalman/UKF) across frames\n\n## Code Example\n```javascript\nfunction fuse(viewFeatures, tamperTemplate) {\n  // pseudo: cross-view attention with a 3D consistency loss\n  return tamperScore;\n}\n```\n\n## Follow-up Questions\n- How would you quantify cross-view inconsistency and its impact on false positives?\n- What ablation study would you run to justify the differentiable renderer component?","diagram":"flowchart TD\n  A[Multi-view Scans] --> B[2D Feature Branch]\n  A --> C[3D Consistency Renderer]\n  B --> D[Fusion & Tamper Scoring]\n  C --> D\n  D --> E[Temporal Filter & Evaluation]","difficulty":"advanced","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T16:36:46.160Z","createdAt":"2026-01-17T16:36:46.160Z"},{"id":"q-3574","question":"Design a real-time 3D object tracker for autonomous warehouse robots that fuses stereo cameras, a lightweight LiDAR, and an IMU to track up to 6 moving pallets in clutter with occlusions. Propose a sensor fusion architecture (learned fusion head plus model-based tracker), data association (JPDA or MHT), and a latency target under 40 ms per frame on edge hardware. Include evaluation and failure handling?","answer":"An on-board multi-sensor fusion tracker employing a two-stage approach: a lightweight CNN-based fusion head that fuses stereo depth, LiDAR echoes, and IMU data to produce per-object 3D state estimates, followed by a model-based tracker (UKF/EKF) for temporal consistency. The fusion head processes synchronized sensor streams at 15-20 Hz, outputting 6-DOF pose and velocity for each detected pallet. For data association under occlusion, implement Joint Probabilistic Data Association (JPDA) with gating based on Mahalanobis distance and motion prediction confidence. The architecture targets <40ms latency per frame on edge hardware (NVIDIA Jetson/Intel NUC) through model quantization (INT8), tensor optimization, and parallel processing pipelines. Track management includes initialization/deletion logic, occlusion handling using last-known-state prediction, and drift detection via residual monitoring. Evaluation uses CLEAR MOT metrics on warehouse datasets with synthetic occlusions, measuring tracking accuracy, ID switches, and computational efficiency. Failure handling incorporates sensor fault detection, fallback to single-sensor tracking, and graceful degradation when association confidence drops below thresholds.","explanation":"## Why This Is Asked\nAssesses multi-sensor fusion design with real-time constraints in a practical logistics setting.\n\n## Key Concepts\n- Multi-sensor fusion (stereo, LiDAR, IMU)\n- Data association under occlusion (JPDA/MHT)\n- Real-time edge latency and lightweight models\n- Track management and drift handling\n\n## Code Example\n```javascript\n// Pseudo: simple UKF update step with JPDA associations\nfunction updateUKF(state, measurements, associations) {\n  // Predict step using motion model\n  const predicted = predictState(state);\n  \n  // JPDA-weighted measurement update\n  const innovation = computeInnovation(predicted, measurements);\n  const kalmanGain = computeKalmanGain(predicted.covariance, innovation.covariance);\n  \n  // Apply JPDA-weighted update\n  const updatedState = applyUpdate(state, innovation, kalmanGain, associations.weights);\n  \n  return updatedState;\n}\n```","diagram":null,"difficulty":"advanced","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Meta","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T05:42:46.884Z","createdAt":"2026-01-17T21:38:52.560Z"},{"id":"q-3576","question":"You have a fixed RGB camera overlooking a supermarket shelf. Design a beginner-friendly pipeline to detect when two items occlude each other in the frame to help picker guidance. Use only a single RGB stream, lightweight cues (color histograms, edge density, simple optical flow), and optionally a tiny classifier trained on a small labeled set. Outline data flow, thresholding strategy, and a minimal evaluation plan?","answer":"Proposed approach: compute color histogram dissimilarity in candidate regions to separate items, track edge density to detect split boundaries, and apply a lightweight optical flow check to verify independent motion between overlapping objects. The pipeline processes the RGB stream through three parallel feature extractors, combines their confidence scores using adaptive thresholds, and outputs occlusion flags for picker guidance.","explanation":"## Why This Is Asked\n\nThis question probes practical computer vision reasoning for occlusion handling with a single RGB stream, touching on feature design, lightweight modeling, and evaluation—common requirements in industrial applications.\n\n## Key Concepts\n\n- Occlusion detection using color, edges, and motion cues\n- Lightweight modeling and thresholding strategies\n- Edge device deployment and latency considerations\n- Evaluation metrics: precision/recall for occlusion flags\n\n## Code Example\n\n```python\n# Minimal feature extraction sketch\nimport cv2\nframe = cv2.imread('frame.jpg')\n# color histogram computation\nhist = cv2.calcHist([frame], [0,1,2], None, [8,8,8], [0,256,0,256,0,256])\n# edge density detection\nedges = cv2.Canny(cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY), 50, 150)\n# optical flow for motion verification\n# (implementation depends on consecutive frames)\n```","diagram":"flowchart TD\n  A[Acquire frame] --> B[Compute color-hist diff] \n  B --> C[Edge-density check] \n  C --> D[Compute optical flow] \n  D --> E[Occlusion flag?] \n  E --> F[Optional small classifier] ","difficulty":"beginner","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Google","Instacart"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T05:42:17.970Z","createdAt":"2026-01-17T22:27:42.129Z"},{"id":"q-3635","question":"Scenario: a fixed RGB camera watches a conveyor; detect a missing screw by comparing live frame to a reference image using a lightweight template matching approach (normalized cross-correlation). Describe a real-time pipeline on a consumer CPU at 24 FPS: ROI selection, lighting normalization, pass/fail thresholding, and a simple occlusion guard. Include a minimal OpenCV NCC-based snippet and a plan for evaluation?","answer":"Leverage normalized cross-correlation (NCC) on a fixed 60x60 pixel ROI centered on the screw location, working in grayscale for computational efficiency. Apply per-frame histogram equalization to mitigate lighting variations across the conveyor. Use cv2.matchTemplate(refROI, currROI, cv2.TM_CCORR_NORMED) to compute similarity scores, with a pass/fail threshold typically set around 0.7-0.8. Implement a simple occlusion guard by rejecting frames with excessive global intensity changes or when the correlation score drops abruptly across consecutive frames, indicating potential obstructions rather than missing screws.","explanation":"## Why This Is Asked\nThis question evaluates practical computer vision engineering skills for real-time quality control systems, specifically testing the ability to design efficient template matching solutions under resource constraints.\n\n## Key Concepts\n- Normalized cross-correlation for template matching\n- ROI selection and computational optimization\n- Lighting normalization techniques\n- Real-time processing constraints (24 FPS on consumer CPU)\n- Temporal filtering for occlusion detection\n- Threshold tuning for pass/fail classification\n\n## Code Example\n```python\nimport cv2\nimport numpy as np\n\n# Load reference template\nref_template = cv2.imread('reference_screw.png', cv2.IMREAD_GRAYSCALE)\n\n# Process current frame\nframe = cv2.imread('current_frame.png', cv2.IMREAD_GRAYSCALE)\n\n# Define fixed ROI (60x60 pixels around screw location)\ny, x, h, w = 50, 30, 60, 60\nroi = frame[y:y+h, x:x+w]\n\n# Lighting normalization\nroi_normalized = cv2.equalizeHist(roi)\n\n# Template matching using NCC\nresult = cv2.matchTemplate(roi_normalized, ref_template, cv2.TM_CCORR_NORMED)\n_, max_score, _, _ = cv2.minMaxLoc(result)\n\n# Pass/fail determination\nthreshold = 0.75\nis_present = max_score > threshold\n```\n\n## Evaluation Plan\n- Collect dataset with known ground truth (present/missing screws)\n- Test across various lighting conditions and conveyor speeds\n- Measure precision/recall and processing latency\n- Optimize threshold values using ROC analysis\n- Validate 24 FPS performance on target consumer CPU hardware","diagram":"flowchart TD\n  Start[Start]\n  ROI[Define ROI around screw]\n  NCC[Compute NCC with ref]\n  Decision{Score >= thr?}\n  Start --> ROI --> NCC --> Decision\n  Decision --> Pass[Pass]\n  Decision --> Fail[Fail]","difficulty":"beginner","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Meta","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T04:02:03.003Z","createdAt":"2026-01-18T02:35:59.615Z"},{"id":"q-3692","question":"With two synchronized RGB cameras observing a factory belt, design a real-time system to detect and estimate 6-DoF poses of articulated tools (e.g., screwdrivers, wrenches) as they move. Use stereo cues, a lightweight backbone, weak supervision (motion, silhouette), and differentiable rendering for pose refinement. Target sub-60 ms per frame on edge hardware, include uncertainty estimates and a practical evaluation protocol?","answer":"Propose a stereo-based pipeline: lightweight backbone (MobileNetV3), epipolar-guided feature matching, and a parametric 6-DoF tool model to initialize pose. Refine with differentiable rendering using ","explanation":"## Why This Is Asked\nAssesses ability to design a stereo-to-3D pose system with real-time constraints, weak supervision, and differentiable refinement—critical for robotics/AR on large platforms.\n\n## Key Concepts\n- Stereo geometry and epipolar constraints\n- Lightweight backbones and differentiable rendering\n- Parametric articulated tool models\n- Temporal fusion with uncertainty (UKF)\n- Weak supervision from motion and segmentation cues\n- Edge-device latency targets\n\n## Code Example\n```javascript\n// Skeleton: pose_init from stereo + kinematic model\nfunction initPose(stereoLeft, stereoRight, toolModel) { /* ... */ }\n```\n\n## Follow-up Questions\n- How would you validate occlusion scenarios and motion blur?\n- How would you extend to N cameras and address calibration drift?\n- Which ablations reveal the contribution of differentiable rendering?","diagram":"flowchart TD\n  AcquireStereo[Acquire stereo frames] --> FeatureExtract[Feature extraction]\n  FeatureExtract --> InitPose[Init pose via kinematic model]\n  InitPose --> Refine[Differentiable rendering refinement]\n  Refine --> Fuse[Temporal fusion & uncertainty]\n  Fuse --> Output[Output poses]","difficulty":"advanced","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Slack","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T05:41:54.990Z","createdAt":"2026-01-18T05:41:54.991Z"},{"id":"q-3777","question":"Design a real-time 3D scene reconstruction system for a warehouse robot using a single RGB camera. Build a voxel-based TSDF map that separates static infrastructure from moving objects (people, pallets) and fuses frames with a dynamic mask. Specify voxel resolution, dynamic handling method (per-voxel dynamic score), pose estimation for fusion, latency target (20-30 ms/frame), and an evaluation plan including per-voxel error and dynamic object drift metrics?","answer":"Use a TSDF voxel grid at 0.08 m resolution, inventorying both static and dynamic regions. Estimate camera pose with lightweight visual odometry and fuse only voxels labeled static by a dynamic mask, r","explanation":"## Why This Is Asked\n\nTests system-level thinking for 3D reconstruction under dynamic scenes and edge-runtime constraints, plus evaluation rigor.\n\n## Key Concepts\n\n- Dense 3D representation (TSDF) with dynamic masking\n- Lightweight pose estimation and masked fusion\n- Dynamic voxel confidence and decay for occlusions\n- Edge latency targets and robust evaluation metrics\n\n## Code Example\n\n```javascript\n// Pseudocode: update TSDF with dynamic masking\nfunction updateVoxelTSDF(voxel, depth, pose, isDynamic) {\n  if (isDynamic) return; // skip static updates for dynamic voxels until confirmed\n  const sdf = projectDepthToVoxel(depth, pose, voxel.coord);\n  voxel.tsdf = (voxel.tsdf * voxel.weight + sdf) / (voxel.weight + 1);\n  voxel.weight += 1;\n}\n```\n\n## Follow-up Questions\n\n- How would you integrate semantic priors to improve dynamic masking in cluttered warehouses?\n- What ablation model would you run to measure the impact of dynamic masking on map quality and drift over 5 seconds?","diagram":"flowchart TD\n  Camera[RGB Camera] --> PoseEst[Lightweight VO / Pose]\n  PoseEst --> Fusion[Dynamic Masked Fusion]\n  Fusion --> Map[Voxel TSDF Map]\n  Map --> Evaluation[Metrics: per-voxel RMSE, dynamic drift]","difficulty":"advanced","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","Meta","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T09:31:25.945Z","createdAt":"2026-01-18T09:31:25.945Z"},{"id":"q-3835","question":"Design a real-time monocular perception pipeline for a Tesla/PayPal-style ADAS/detection task: detect pedestrians and cyclists in adverse weather (rain, fog) using only a single RGB camera on edge hardware. Outline the architecture (backbone, detector head), how monocular depth cues are integrated, a weather-robust data strategy (augmentation, synthetic data, self-supervised tasks), latency targets, and an evaluation plan across IoU, depth accuracy, and safety-critical false negatives?","answer":"Use a lightweight backbone (MobileNetV3) with a single-shot detector head for 2D boxes and classes, plus a monocular depth head trained with self-supervised photometric and temporal losses. Fuse depth","explanation":"## Why This Is Asked\\n\\nEvaluate design of depth-aware perception with a single RGB camera under adverse weather and edge constraints. Focus on architecture choices, data strategy, and safety-focused evaluation beyond 2D detection.\\n\\n## Key Concepts\\n- Monocular depth estimation\\n- Weather-robust augmentation\\n- Self-supervised and temporal losses\\n- Edge latency and fusion\\n\\n## Code Example\\n```javascript\n// Implementation sketch\nfunction fuseDetAndDepth(boxes, depthMap) {\n  return boxes.map(b => ({...b, depth: depthMap.sample(b.cx, b.cy)}))\n}\n```\n\\n## Follow-up Questions\\n- How would you quantify risk of depth misestimation under fog?\\n- How would you validate domain adaptation from synthetic to real rain?","diagram":null,"difficulty":"advanced","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["PayPal","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T11:30:13.248Z","createdAt":"2026-01-18T11:30:13.248Z"},{"id":"q-4001","question":"On a fixed RGB camera watching a conveyor belt, design a beginner-friendly CV pipeline to detect when an item is rotated away from upright by more than 15 degrees. Use only a single RGB stream and lightweight cues: color histograms, edge density, simple optical flow, and optionally a tiny classifier trained on a small labeled set. Outline data flow, thresholding strategy, and a minimal evaluation plan?","answer":"Propose a fixed-camera belt monitor with a 3-step feature fusion: (1) ROI crop per item and HSV color histogram; (2) edge density from grayscale to gauge shape integrity; (3) short-window dense optica","explanation":"## Why This Is Asked\nNew angle: orientation reliability on a belt using simple cues and a tiny classifier. Encourages feature engineering, thresholding, and practical eval in a manufacturing context relevant to automations used by major tech companies.\n\n## Key Concepts\n- Lightweight feature fusion: color histograms, edge density, optical flow\n- Small classifier and thresholding to decide rotation\n- ROI extraction and a short temporal window for motion cues\n\n## Code Example\n```python\n# pseudo feature extraction (illustrative)\nvec = [hist_value, edge_density, flow_mean]\n# clf trained to output upright vs rotated probability\nprob = clf.predict_proba([vec])[0,1]\nflag = prob > THRESH\n```\n\n## Follow-up Questions\n- How would you handle multiple items on the belt?\n- How would you adapt thresholds for lighting changes?","diagram":"flowchart TD\n  Camera[Fixed RGB Camera] --> ROI[ROI Extraction]\n  ROI --> Features[Feature Extraction]\n  Features --> Classifier[Lightweight Classifier]\n  Classifier --> Decision[Rotation Flag]","difficulty":"beginner","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Square","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T19:22:54.440Z","createdAt":"2026-01-18T19:22:54.440Z"},{"id":"q-4044","question":"Scenario: A fixed RGB camera watches a 3-row shelf in a convenience store. Each row should contain items from a category: snacks, drinks, or stationery. Design a beginner-friendly CV pipeline to detect when an item is placed in the wrong row (e.g., a drink on the snacks row) using only a single RGB stream. Use lightweight cues (color histograms, edge density, simple optical flow) and optionally a tiny classifier trained on a small labeled set. Outline data flow, thresholding strategy, and a minimal evaluation plan?","answer":"Segment the shelf into three ROIs corresponding to snacks, drinks, and stationery. For each ROI, compute a lightweight feature vector consisting of HSV color histograms, edge density, and optical flow magnitude. Use a tiny classifier or prototype-based matching with minimal labeled data to detect category violations. Apply temporal smoothing across frames to reduce false positives.","explanation":"## Why This Is Asked\nTests practical, beginner-friendly computer vision on edge hardware with a realistic misplacement task and minimal labeled data.\n\n## Key Concepts\n- ROI segmentation for fixed layouts\n- Lightweight features: HSV histogram, edge density, optical flow magnitude\n- Tiny classifier or prototype-based decision with small labeled data\n- Thresholded, temporal aggregation to reduce noise\n\n## Code Example\n```javascript\n// Pseudo-feature extraction sketch\nfunction extractFeatures(imgROI) {\n  // compute histogram, edge density, flow magnitude\n  // return [hist, edges, flow]\n}\n```\n\n## Follow-up Considerations\n- How would you handle lighting variations across different times of day?\n- What strategies could you use to reduce false positives when items are partially occluded?\n- How might you extend this system to handle additional categories or shelf configurations?","diagram":"flowchart TD\n  A[Video Frame] --> B[ROI Per Row]\n  B --> C[Feature Extraction]\n  C --> D[Classification/Similarity]\n  D --> E[Misplacement Score]\n  E --> F[Alert]","difficulty":"beginner","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Microsoft","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T06:08:14.568Z","createdAt":"2026-01-18T21:27:40.341Z"},{"id":"q-4069","question":"Context: A fixed rooftop RGB camera watches a campus bike lane. Design a beginner-friendly CV pipeline to detect cyclists and estimate their real-time speed using only RGB frames. Include a lightweight detector, a simple calibration-based mapping from pixel motion to m/s, a temporal filter, latency targets, and a minimal evaluation plan?","answer":"Use a lightweight cyclist detector (MobileNetV2-SSD) per frame, then track the bike centroid with a small Kalman filter. Calibrate using known camera height to map pixel motion to real distance via scale factor, compute frame-to-frame speed, and apply temporal smoothing.","explanation":"## Why This Is Asked\n\nAssesses ability to design an end-to-end CV pipeline under real-time constraints, focusing on detection, tracking, and monocular speed estimation without depth sensors.\n\n## Key Concepts\n\n- Lightweight detection\n- Centroid tracking with Kalman filter\n- Camera height-based scale and frame-to-frame speed\n- Edge latency and simple evaluation plan\n\n## Code Example\n\n```javascript\n// Minimal speed estimate sketch\nfunction estimateSpeed(bikeCenters, dt, scale){\n  const dx = bikeCenters[1].x - bikeCenters[0].x;\n  const dy = bikeCenters[1].y - bikeCenters[0].y;\n  const delta = Math","diagram":null,"difficulty":"beginner","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","IBM","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T05:54:34.160Z","createdAt":"2026-01-18T22:37:04.651Z"},{"id":"q-4159","question":"Design a real-time on-device video segmentation system for AR on a mobile phone with a single RGB camera that can learn new object masks from 10–20 labeled frames using active learning and self-supervised pretraining. Target latency <40 ms/frame and memory under 400 MB. Describe architecture (shared encoder + compact decoder), online adaptation (entropy-based frame selection, pseudo-labels, limited fine-tuning), data flow, and evaluation metrics (IoU, mask stability, adaptation speed)?","answer":"Leverage MobileOne encoder + compact decoder for per-frame masks, plus a temporal refinement module guided by optical flow to stabilize outputs. Online adaptation uses entropy-based frame selection on","explanation":"## Why This Is Asked\nThis question probes on-device learning for segmentation under tight latency and memory constraints, a practical pain point for AR workflows at scale.\n\n## Key Concepts\n- On-device few-shot learning\n- Self-supervised pretraining and pseudo-labeling\n- Temporal coherence via optical flow\n- Memory and latency budgeting on mobile GPUs\n\n## Code Example\n```javascript\n// Pseudo-code for on-device online update\nfunction onlineUpdate(frame, labelMask) {\n  // extract features\n  // compute loss on pseudo-labels\n  // update small head only\n}\n```\n\n## Follow-up Questions\n- How would you prevent drift during online fine-tuning?\n- What ablations would you run to measure the impact of the temporal refinement module?","diagram":"flowchart TD\n  A[Input frame] --> B[Shared encoder]\n  B --> C[Mask head]\n  B --> D[Temporal refine (optical flow)]\n  C --> E[Mask output]\n  D --> E","difficulty":"intermediate","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T05:48:44.842Z","createdAt":"2026-01-19T05:48:44.842Z"},{"id":"q-4225","question":"Design an edge-device semantic segmentation system for a mobile robot with a single RGB camera that must rapidly adapt to new indoor/outdoor environments using only a tiny labeled seed (<50 frames) and no cloud access. Propose a lightweight model, a self-supervised domain adaptation loop (temporal consistency + augmentations), and a concrete evaluation plan across IoU, boundary accuracy, and inference latency?","answer":"Deploy a lightweight encoder–decoder (MobileNetV3 backbone with a tiny SegFormer head) running ~18–22 FPS on the edge. Use a teacher–student EMA setup for self-supervised adaptation: temporal consiste","explanation":"## Why This Is Asked\nTests on-device adaptation with minimal labeled data, no cloud, and robust performance across diverse environments. It emphasizes practical design choices, latency constraints, and a solid evaluation plan.\n\n## Key Concepts\n- Lightweight segmentation architectures for edge\n- Teacher–student EMA for self-supervised domain adaptation\n- Temporal consistency and augmentation strategies\n- Seed supervision and online adaptation loop\n\n## Code Example\n\n```javascript\n// EMA update for teacher parameters (pseudo)\nlet teacherParams = initialTeacher;\nfunction updateTeacher(studentParams, alpha) {\n  return alpha * teacherParams + (1 - alpha) * studentParams;\n}\n```\n\n## Follow-up Questions\n- How would you handle sudden scene shifts (e.g., lighting, weather) without labeled data?\n- What metrics would you monitor in production to detect model drift on-device?","diagram":"flowchart TD\n  A[Video stream] --> B[Edge Encoder]\n  B --> C[Segmentation Head]\n  C --> D[Prediction Cache]\n  D --> E[Adaptation Trigger]\n  E --> F[Robot Action]","difficulty":"intermediate","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","LinkedIn","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T09:14:18.687Z","createdAt":"2026-01-19T09:14:18.687Z"},{"id":"q-4555","question":"Design a real-time monocular depth and 6-DoF pose estimation pipeline for a robotic pick-and-place task using a single RGB camera in cluttered, occluded environments. Requirements: sub-40 ms latency on an edge GPU, self-supervised training for depth and pose, temporal filtering to handle occlusions, and a robust evaluation plan including depth accuracy, pose drift under occlusion, and failure modes?","answer":"Utilize a lightweight backbone (EfficientNet-Lite) with dedicated monocular depth and 6-DoF pose estimation heads. Train using self-supervised learning with view synthesis loss and temporal consistency constraints, incorporating synthetic occlusion augmentation. Fuse predictions with an Extended Kalman Filter for temporal smoothing and implement a differentiable refinement loop using rendered feedback to maintain sub-40 ms latency on edge GPU hardware.","explanation":"## Why This Is Asked\nTests ability to design a real-time monocular depth and 6-DoF pose estimation pipeline that performs robustly under occlusion conditions on edge hardware, requiring self-supervised training approaches and iterative refinement capabilities.\n\n## Key Concepts\n- Dual-head architecture: monocular depth estimation and 6-DoF pose regression with lightweight backbone\n- Latency-aware design: EfficientNet-Lite backbone optimized for edge GPU inference under 40 ms\n- Self-supervised training: view synthesis loss, temporal consistency constraints, synthetic occlusion augmentation\n- Temporal filtering: Extended Kalman Filter/Unscented Kalman Filter for stability during motion and occlusion events\n- Differentiable refinement: rendered feedback loop for iterative pose optimization\n\n## Code Example\n\n```python\n# Pseudocode: EKF update with rendered feedback\nstate_pred = predict(state)\npos_ref = render(state_\n```","diagram":"flowchart TD\n  A[Camera] --> B[Backbone+Depth Head]\n  B --> C[Pose Head]\n  B --> D[Depth Head]\n  C --> E[Temporal Filter (EKF/UKF)]\n  D --> E\n  E --> F[Renderer Refinement]\n  F --> G[Outputs]","difficulty":"advanced","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google","LinkedIn"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T06:09:37.444Z","createdAt":"2026-01-19T23:45:05.384Z"},{"id":"q-456","question":"How would you design a real-time object detection system for a social media platform that processes 10M images/day with 99.9% accuracy and <100ms latency?","answer":"Implement a distributed pipeline with GPU clusters for inference, model quantization using TensorRT/ONNX, edge caching for popular content, and ensemble models. Incorporate batch processing, model versioning, and comprehensive monitoring to achieve the target performance metrics.","explanation":"## Architecture\n- **Inference Layer**: GPU clusters with model quantization and TensorRT optimization\n- **Caching Layer**: Redis for frequent detections and CDN edge caching for popular content\n- **Load Balancing**: Kubernetes with auto-scaling based on queue depth and latency thresholds\n- **Monitoring**: Real-time accuracy metrics and comprehensive latency tracking\n\n## Optimization Strategies\n- **Model Compression**: TensorRT optimization with 8-bit quantization for faster inference\n- **Batch Processing**: Dynamic batching based on system load to maximize GPU utilization\n- **Fallback Mechanism**: CPU inference when GPU resources are unavailable\n- **Model Ensemble**: Combine YOLOv8 with ResNet to achieve 99.9% accuracy\n\n## Production Considerations\n- **Model Versioning**: Implement A/B testing and gradual rollouts for model updates\n- **Scalability**: Horizontal scaling of inference nodes to handle peak loads\n- **Reliability**: Implement circuit breakers and retry mechanisms for fault tolerance","diagram":"flowchart TD\n  A[Image Upload] --> B[Preprocessing]\n  B --> C[GPU Inference]\n  C --> D[Postprocessing]\n  D --> E[Cache Check]\n  E --> F[Result Storage]\n  G[Load Balancer] --> C\n  H[Monitor] --> I[Alert System]\n  J[Model Registry] --> C","difficulty":"advanced","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Microsoft","Salesforce","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-09T08:56:19.980Z","createdAt":"2025-12-26 12:51:05"},{"id":"q-4582","question":"Design a production-ready monocular 3D object detector for robotic pick-and-place on a new warehouse line. The system must maintain accurate 6-DoF poses for a fixed set of tools under changing intrinsics, lens distortion, and illumination, using only a single RGB camera and streaming data. Propose a real-time adaptation loop with no labeled data, specify data flow, losses (temporal reprojection, photometric consistency, geometric self-supervision), latency targets, and evaluation plan for drift and false negatives?","answer":"A fixed 6-DoF detection head operates in parallel with a lightweight RGB backbone to predict tool poses. Implement an online self-supervised adaptation loop utilizing temporal reprojection loss from visual odometry, photometric consistency across consecutive frames, and geometric self-supervision to maintain accuracy under varying intrinsics and illumination conditions.","explanation":"## Why This Is Asked\nThis question evaluates real-world robustness by requiring self-supervision mechanisms to handle non-stationary intrinsics and lighting—common challenges in deployed computer vision systems.\n\n## Key Concepts\n- Online self-supervision with temporal and photometric consistency\n- Intrinsics drift management through lightweight calibration\n- Latency constraints for edge devices and evaluation under occlusion\n\n## Code Example\n```javascript\nfunction loss(frame_t, frame_t1, pose_t, pose_t1) {\n  const L_reproj = reprojectionLoss(frame_t, frame_t1, pose_t, pose_t1);\n  const L_photo","diagram":"flowchart TD\n  A[Frame] --> B[Backbone]\n  B --> C[6-DoF Head]\n  C --> D[Online self-supervision]\n  D --> E[EMA update]\n  E --> F[Latency target]\n","difficulty":"advanced","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Hugging Face"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T05:55:52.430Z","createdAt":"2026-01-20T02:34:51.333Z"},{"id":"q-4836","question":"Edge CV task: A warehouse robot with a single RGB camera (Jetson-class) must detect a fixed set of 60 SKUs and flag Unknown items for human review in real time. Design a compact on-device detector with an Open-Set component and a memory module for rapid few-shot updates. Outline architecture (backbone, embedding head, novelty scorer), data strategy (synthetic + real), latency targets (<40 ms/frame), and evaluation (per-class mAP, unknown recall/precision)?","answer":"Propose a two-head detector: backbone MobileNetV3-Large; detection head for class scores; embedding head (128-d) for SKU prototypes. Novelty scorer uses cosine distance to 60 prototypes with a learnab","explanation":"## Why This Is Asked\nTests open-set recognition, on-device learning, and latency budgeting in a realistic warehouse scenario.\n\n## Key Concepts\n- Open-set detection with embedding-based novelty scoring\n- Lightweight on-device memory for rapid updates\n- Synthetic + real data with domain randomization\n\n## Code Example\n```javascript\nfunction cosine(a,b){ let na=Math.hypot(...a); let nb=Math.hypot(...b); let dot=a.reduce((s,v,i)=>s+v*b[i],0); return dot/(na*nb); }\nfunction isUnknown(feature, prototypes, thresh){ return Math.max(...prototypes.map(p=>cosine(feature,p))) < thresh; }\n```\n\n## Follow-up Questions\n- How would you handle skewed SKU distribution over time?\n- What failure modes would you test for during deployment? ","diagram":"flowchart TD\n  A[Input RGB frame] --> B[Backbone]\n  B --> C[Detection Head]\n  B --> D[Embedding Head]\n  C --> E[Known SKU Detections]\n  D --> F[Novelty Scorer]\n  F --> G[Unknown Flag]\n  E & G --> H[NMS & Post-Processing]\n  H --> I[Output with Unknown flag]","difficulty":"advanced","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","LinkedIn","Lyft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T15:42:38.137Z","createdAt":"2026-01-20T15:42:38.137Z"},{"id":"q-487","question":"Design a real-time object detection system for DoorDash delivery vehicles that must identify packages, license plates, and traffic signs in varying weather conditions. How would you handle model optimization for edge deployment and ensure 99% accuracy?","answer":"Use YOLOv8 with custom-trained heads for each object class. Implement TensorRT optimization for NVIDIA Jetson edge devices. Use multi-scale training with weather augmentation (rain, fog, night). Deploy with model ensemble techniques and confidence thresholding to maintain 99% accuracy.","explanation":"## Architecture\n- **Detection Pipeline**: Multi-task CNN with shared backbone and separate heads\n- **Edge Optimization**: TensorRT INT8 quantization, model pruning, batch size 1\n- **Weather Handling**: Domain adaptation, synthetic data generation\n\n## Implementation\n```python\n# Model optimization example\nimport torch\nfrom torch2trt import torch2trt\n\nmodel = YOLOv8(num_classes=3)\nmodel.load_state_dict(torch.load('best.pth'))\nmodel.cuda().eval()\n\n# TensorRT conversion\nx = torch.ones((1, 3, 640, 640)).cuda()\nmodel_trt = torch2trt(model, [x], fp16_mode=True)\n```\n\n## Performance\n- **Inference**: <50ms per frame\n- **Accuracy**: 99%+ on validation set\n- **Memory**: <2GB on edge device","diagram":"flowchart TD\n  A[Camera Feed] --> B[Preprocessing]\n  B --> C[YOLOv8 Detection]\n  C --> D[TensorRT Inference]\n  D --> E[NMS Filtering]\n  E --> F[Temporal Smoothing]\n  F --> G[Confidence Check]\n  G --> H{Accuracy > 99%?}\n  H -->|Yes| I[Output Results]\n  H -->|No| J[Cloud Fallback]","difficulty":"advanced","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-01T06:41:06.125Z","createdAt":"2025-12-26 12:51:06"},{"id":"q-4963","question":"Design a real-time, on-device monocular inventory locator for a warehouse drone using a single RGB camera. The system should localize items in 3D against a catalog, handle new SKUs weekly, and run <40 ms/frame on an embedded GPU. Outline architecture (backbone, depth proxy, ego-motion, mapping), data strategy (synthetic+real, domain randomization), online SKU updates, and evaluation plan including 3D localization accuracy and update latency?","answer":"Implement an edge-optimized monocular SLAM pipeline for a warehouse drone using a single RGB camera to achieve real-time 3D item localization against a product catalog. The system must support weekly SKU updates while maintaining sub-40ms/frame processing on embedded GPU hardware. Architecture includes: (1) Efficient backbone (MobileNetV3/EfficientNet) for feature extraction, (2) Lightweight depth proxy network for metric scale estimation, (3) Visual-inertial odometry for ego-motion tracking, (4) Differentiable mapping layer for 3D item pose estimation. Data strategy combines synthetic warehouse environments with domain randomization and limited real-world fine-tuning. Online SKU updates use few-shot learning with catalog embeddings and continual learning techniques. Evaluation framework measures 3D localization accuracy (ADD metrics), update latency, and drift over extended flight paths.","explanation":"## Why This Is Asked\nThis scenario tests practical computer vision constraints in logistics applications, specifically real-time 3D localization from monocular input on edge devices with rapid catalog updates.\n\n## Key Concepts\n- Monocular SLAM on embedded systems\n- Lightweight depth estimation\n- Online learning for SKU updates\n- Synthetic-to-real domain adaptation\n- Real-time performance optimization\n\n## Code Example\n```python\n# Pseudo depth proxy update\ndef depth_proxy(frame, prev_depth):\n    features = backbone(frame)\n    depth = depth_head(features)\n    return temporal_smooth(depth, prev_depth)\n```","diagram":null,"difficulty":"intermediate","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-21T06:06:57.864Z","createdAt":"2026-01-20T21:42:58.388Z"},{"id":"q-5067","question":"Design a privacy-preserving, real-time on-device face-detection and blurring pipeline for live video in a social app using a single RGB camera, running on mobile GPUs with <60 ms/frame, ensuring raw faces never leave the device, while enabling privacy-friendly analytics. Outline architecture, latency, guarantees, and evaluation?","answer":"Propose a lightweight detector (MobileNetV3/EfficientNet-lite) achieving ~60 ms/frame on mobile GPU, followed by a blur mask sized to the face bounding box. All processing stays on-device; use per-fac","explanation":"## Why This Is Asked\nEdge privacy is critical for live video in social apps. A real-time, on-device solution avoids transmitting raw faces while still enabling analytics, a practical, high-stakes requirement.\n\n## Key Concepts\n- On-device inference with mobile-optimized backbones\n- Privacy guarantees: no raw faces leave device; analytics with differential privacy\n- Adaptive blur strength by face size and context\n- End-to-end latency targets and privacy risk assessment\n\n## Code Example\n```python\n# Skeleton: on-device face blur pipeline\ndef process_frame(frame):\n    boxes = detect_faces(frame)\n    mask = blur_masks(frame, boxes)\n    output = apply_mask(frame, mask)\n    return output\n```\n\n## Follow-up Questions\n- How would you extend to multi-camera or depth-sensing inputs for better localization?\n- How would you validate privacy guarantees and quantify potential leakage in analytics?","diagram":"flowchart TD\n  A[Input Frame] --> B[Face Detector]\n  B --> C[Blur Module]\n  C --> D[Output Frame]\n  D --> E[Privacy Analytics]","difficulty":"intermediate","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["PayPal","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T04:26:44.583Z","createdAt":"2026-01-21T04:26:44.583Z"},{"id":"q-5084","question":"Design a real-time, edge-friendly pipeline for a single RGB camera to identify, segment, and estimate 6-DoF pose for up to 6 hand tools (screwdriver, wrench, pliers, hammer, etc.) in cluttered, variable lighting. Tools may be unseen at train time, so enable zero-/few-shot adaptation using a compact 3D prior and a differentiable renderer for pose refinement. Outline architecture, data strategy, losses, latency, and evaluation?","answer":"Edge pipeline: a lightweight detector (MobileNetV3+FPN) outputs 2D tool proposals; lift to 6-DoF with a CAD prior via 2D-3D correspondences; refine with a differentiable renderer using silhouette and ","explanation":"## Why This Is Asked\n\nTests real-time, edge CV capabilities with occlusion, clutter, and generalization to unseen tools. Requires integration of 2D detection, 3D reasoning, and differentiable rendering under tight latency.\n\n## Key Concepts\n\n- Lightweight detection and 2D-to-3D lifting\n- Compact 3D priors and differentiable rendering for pose refinement\n- Zero-/few-shot adaptation with shape priors and adapters\n- Temporal fusion to handle occlusion\n\n## Code Example\n\n```javascript\n// Skeleton: map 2D proposals to 3D pose with CAD prior and refine\nfunction refinePose(proposals2D, cadPrior) {\n  // compute 2D-3D correspondences\n  // run differentiable renderer and reprojection loss\n  // output refined 6-DoF pose\n}\n```\n\n## Follow-up Questions\n\n- How would you measure latency and accuracy on edge hardware under varying illuminations?\n- What additional priors or data augmentation would improve unseen tool generalization?","diagram":"flowchart TD\n  A[Camera] --> B[Detector]\n  B --> C[2D-3D Mapping]\n  C --> D[Differentiable Renderer Refinement]\n  D --> E[Temporal Filter]\n  E --> F[Control/Action]","difficulty":"advanced","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","NVIDIA","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T05:45:39.186Z","createdAt":"2026-01-21T05:45:39.186Z"},{"id":"q-5157","question":"Design a monocular 6-DoF pose estimation system for a cluttered manufacturing cell: given a single RGB camera feed of a known set of parts on a workbench, with a small labeled pose dataset per part and a large unlabeled video stream, estimate each part's 6-DoF pose in real time, handle symmetry and occlusion, and support online few-shot adaptation for new items. Outline architecture, data strategy, losses, online adaptation, and evaluation with timing constraints?","answer":"Use a two-stage monocular 6‑DoF estimator: a lightweight backbone (MobileNetV3) plus a 6‑DoF head predicting quaternion and translation, followed by a differentiable renderer-based refinement. Train o","explanation":"## Why This Is Asked\nTests ability to design a practical monocular 6-DoF pose estimator that handles symmetry, occlusion, and online adaptation in a real-world manufacturing setting. Emphasizes differentiable refinement, data strategies, and latency planning.\n\n## Key Concepts\n- Monocular 6-DoF pose estimation\n- Differentiable rendering for pose refinement\n- Symmetry handling and occlusion robustness\n- Synthetic data + domain randomization\n- Online adaptation with few-shot updates\n- Real-time latency targets (embedded GPU)\n\n## Code Example\n```javascript\nclass PoseHead {\n  constructor(featDim) {\n    this.fcRot = linear(featDim, 4); // quaternion\n    this.fcTrans = linear(featDim, 3);\n  }\n  forward(feat) {\n    const q = normalize(this.fcRot(feat), 2);\n    const t = this.fcTrans(feat);\n    return { quat: q, trans: t };\n  }\n}\n```\n\n## Follow-up Questions\n- How would you validate symmetry handling for objects with known symmetry? \n- How would you mitigate catastrophic forgetting during online adaptation?\n","diagram":"flowchart TD\n  A(Input RGB) --> B(Backbone + Feature Pyramid)\n  B --> C(Pose Head: quat + trans)\n  C --> D(Differentiable Renderer Refine)\n  D --> E(Output Pose)\n  E --> F(Evaluation & Latency Check)","difficulty":"intermediate","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","IBM","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T08:49:02.553Z","createdAt":"2026-01-21T08:49:02.553Z"},{"id":"q-517","question":"Design a real-time object detection system for cryptocurrency trading terminals that must detect and classify multiple monitor types, trading interfaces, and unauthorized screen recording devices with <100ms latency. How would you optimize YOLOv8 for this specific use case?","answer":"Optimize YOLOv8-nano with TensorRT quantization, custom dataset of trading interfaces, and multi-scale feature fusion. Implement temporal consistency with optical flow tracking, use knowledge distillation from larger models, and deploy with batch inference pipelines to achieve sub-100ms latency.","explanation":"## Architecture Overview\n- **Model Selection**: YOLOv8-nano optimized for speed, custom-trained on comprehensive trading UI datasets\n- **Optimization Pipeline**: TensorRT FP16 quantization with custom CUDA kernels for hardware acceleration\n- **Real-time Processing**: Intelligent frame skipping combined with motion detection for idle periods\n\n## Technical Implementation\n```python\n# Custom trading interface detector\nclass TradingDetector:\n    def __init__(self):\n        self.model = YOLO('yolov8n_trading.pt')\n        self.tracker = DeepSort(max_age=30)\n    \n    def detect_frame(self, frame):\n        results = self.model(frame, conf=0.7)\n        return self.tracker.update(results)","diagram":"flowchart TD\n  A[Camera Feed] --> B[Frame Preprocessing]\n  B --> C[YOLOv8-nano Inference]\n  C --> D[Object Tracking]\n  D --> E[Classification Layer]\n  E --> F[Alert System]\n  F --> G[Security Dashboard]\n  C --> H[Performance Monitor]\n  H --> I[Model Retraining]","difficulty":"advanced","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Coinbase","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-09T08:38:40.181Z","createdAt":"2025-12-26 12:51:06"},{"id":"q-5239","question":"Design a real-time, edge-optimized 3D near-miss detector from a single RGB camera for urban rideshare scenarios. The system should output a 3D bounding box plus a safety risk score within 25 ms/frame, handle rain/glare, maintain privacy (no identity leakage), and generalize to new urban layouts. Outline architecture, data strategy, and evaluation plan?","answer":"Implement a real-time edge detector for near-miss events from one RGB stream. Use a compact backbone (EfficientNet-Lite + FPN) with ConvLSTM, a monocular depth proxy from motion/ego cues to yield 3D b","explanation":"## Why This Is Asked\n\nEdge, privacy, and safety-critical decisions require a tight loop from perception to prognosis under latency constraints.\n\n## Key Concepts\n\n- 3D perception from monocular cues with temporal context\n- Edge latency, model compression, and memory constraints\n- Privacy preservation in CV models\n- Domain randomization and self-supervised depth signals\n\n## Code Example\n\n```javascript\nfunction clamp(n, a, b){return Math.max(a, Math.min(b, n))}\n```\n\n## Follow-up Questions\n\n- How would you validate robustness to rain, glare, and occlusion, and what metrics matter for safety-critical NFN?\n- Which ablations isolate the temporal module’s contribution to near-miss detection?","diagram":null,"difficulty":"advanced","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T11:53:30.646Z","createdAt":"2026-01-21T11:53:30.646Z"},{"id":"q-5292","question":"Context: A fixed rooftop RGB camera overlooks a street at dusk. Design a beginner-friendly CV pipeline to estimate each pedestrian's height using only monocular cues from the RGB stream. Rely on a simple ground-plane-based height estimate using pixel extent and a calibrated focal length, with optional shadows. Outline data flow, scale calibration, thresholding, and a minimal evaluation plan?","answer":"Detect pedestrian with a lightweight detector; estimate height via pixel height mapped to real height using ground plane and focal length: height ≈ (pixelHeight × knownDistance) / focalLength. If shad","explanation":"## Why This Is Asked\n\nTests ability to extract metric height from a single RGB camera using simple geometry and cues, a realistic constraint on edge devices. Encourages thinking about calibration, error propagation, and lightweight refinement.\n\n## Key Concepts\n\n- Single-view geometry and ground-plane scaling\n- Pixel-to-real-world conversion using focal length\n- Shadow cues as a bias/auxiliary signal\n- Lightweight detectors and tiny classifiers for pruning\n- Evaluation metrics and latency targets\n\n## Code Example\n\n```javascript\n// estimateHeight(pixelHeight, focalLength, groundDistance)\n// height ≈ (pixelHeight * groundDistance) / focalLength\nfunction estimateHeight(pixelHeight, focalLength, groundDistance) {\n  return (pixelHeight * groundDistance) / focalLength;\n}\n```\n\n## Follow-up Questions\n\n- How would you handle unknown ground plane tilt or camera calibration drift?\n- How would you extend this to handle occlusions or multiple pedestrians in a frame?","diagram":"flowchart TD\n  A[Detect pedestrian] --> B[Compute horizon and scale from calibration]\n  B --> C[Estimate height from pixel height]\n  C --> D[Shadow-based bias (optional)]\n  D --> E[Optional lightweight classifier]\n  E --> F[Output height with uncertainty]","difficulty":"beginner","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Tesla","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T15:12:51.230Z","createdAt":"2026-01-21T15:12:51.231Z"},{"id":"q-5349","question":"Design a real-time edge vision system for a factory line: detect, localize, and classify new SKUs from a single RGB camera. New SKUs arrive weekly; support few-shot adaptation from 5–10 labeled examples per SKU with on-device latency under 40 ms. Describe architecture, data strategy (synthetic+real), losses, adaptation workflow, and evaluation metrics?","answer":"Propose a lightweight anchor-free detector using a MobileNetV3 backbone plus a tiny 3D pose head and a prototypical network for fast 5–10 example SKU adaptation. Train with synthetic+real data and sel","explanation":"## Why This Is Asked\nAssesses practical few-shot, edge CV with real-world constraints.\n\n## Key Concepts\n- Few-shot adaptation, prototypical networks, on-device inference\n- Synthetic+real data, self-supervised previews, domain shift\n- 3D pose estimation for precise localization\n\n## Code Example\n```javascript\n// Proto distance for few-shot adaptation\nfunction protoDistance(x, prototypes){ /* compute cosine dist to class prototypes */ }\n```\n\n## Follow-up Questions\n- How would you measure on-device adaptation latency in a real deployment?\n- What failure modes would you expect with highly similar SKUs, and how would you mitigate them?","diagram":"flowchart TD\n  A[Input RGB] --> B[Backbone+Detector]\n  B --> C{SKU adaptation?}\n  C -->|Yes| D[Few-shot prototype adaptation]\n  C -->|No| E[Inference]\n  D --> E","difficulty":"intermediate","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Bloomberg","Coinbase"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T19:04:14.668Z","createdAt":"2026-01-21T19:04:14.668Z"},{"id":"q-5421","question":"Design an on-device video moderation system for a social app running on mid-range mobile hardware. The system must recognize evolving policy violations with 5–10 labeled examples per new category, run at 25–40 ms per frame on an NPU, and support rapid adaptation with synthetic-plus-real data. Ensure privacy by on-device processing and secure federated updates. Outline architecture, data strategy, open-set handling, and evaluation?","answer":"Edge design uses MobileNetV3-L as backbone, a lightweight 3-frame temporal module, and dual heads: a classifier and an open-set detector using Mahalanobis distance on features. Adaptation via 5–10-shot prototypical networks with synthetic-plus-real data, domain randomization, and secure federated updates. Privacy preserved through on-device processing and encrypted model aggregation.","explanation":"## Why This Is Asked\nAssess end-to-end edge ML design for evolving categories with strict latency, privacy, and adaptation constraints.\n\n## Key Concepts\n- On-device inference latency and memory budgets\n- Few-shot adaptation (5–10 examples)\n- Open-set recognition and robust filtering\n- Synthetic data + domain randomization\n- Privacy via secure federated learning\n\n## Code Example\n```python\n# Pseudo-code: open-set scoring with Mahalanobis distance\nimport numpy as np\n\ndef open_set_score(feat, means, cov_inv):\n    dists = np.array([np.dot((feat - m).T, cov_inv).dot(feat - m) for m in means])\n    return dists\n```","diagram":"flowchart TD\n  A[Input Video] --> B[Preprocessing]\n  B --> C[Backbone + Temporal Module]\n  C --> D[Open-Set Detector & Classifier]\n  D --> E[Frame Decision]\n","difficulty":"intermediate","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","Meta","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T06:05:20.855Z","createdAt":"2026-01-21T21:42:48.627Z"},{"id":"q-545","question":"How would you detect if an image contains a face using basic computer vision techniques?","answer":"To detect if an image contains a face using basic computer vision techniques, apply Haar cascade classifiers that scan the image using trained rectangular filters to identify facial features like eyes and nose. Alternatively, use HOG feature extraction combined with SVM classifiers to distinguish face patterns from background regions.","explanation":"## Face Detection Methods\n\n### Haar Cascade Approach\n- Uses integral images for fast feature computation\n- Trains on positive/negative face samples\n- Detects facial features through rectangular filters\n\n### Implementation Steps\n- Convert image to grayscale\n- Apply histogram equalization\n- Load pre-trained cascade classifier\n- Use detectMultiScale() with proper parameters\n\n### Common Parameters\n```python\nfaces = face_cascade.detectMultiScale(\n    gray_image,\n    scaleFactor=1.1,\n    minNeighbors=5,\n    minSize=(30, 30)\n)\n```\n\n### Trade-offs\n- Fast but less accurate than deep learning\n- Works well for frontal faces\n- Sensitive to lighting conditions","diagram":"flowchart TD\n  A[Input Image] --> B[Grayscale Conversion]\n  B --> C[Histogram Equalization]\n  C --> D[Haar Cascade Detection]\n  D --> E[Face Bounding Boxes]\n  E --> F[Output Results]","difficulty":"beginner","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","NVIDIA","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":["haar cascades","hog features","svm","opencv","detectmultiscale","integral images"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2026-01-07T03:43:05.970Z","createdAt":"2025-12-26 12:51:07"},{"id":"q-5500","question":"Design a monocular vision pipeline for a warehouse robot to detect, segment, and estimate 6-DoF pose of bottle-cap objects on a cluttered shelf from a single RGB camera. New cap designs arrive weekly; support 5–10-shot adaptation with on-device latency under 50 ms per frame. Describe architecture, data strategy, losses, adaptation workflow, and evaluation metrics?","answer":"Two-stage approach: (1) fast ROI detector with a lightweight backbone to localize caps; (2) a 6-DoF pose head (MLP) predicting rotation and translation, trained with differentiable renderer supervision to handle occlusions and ensure geometric consistency.","explanation":"## Why This Is Asked\n\nAssesses real-world edge deployment capabilities: monocular vision, few-shot adaptation, occlusion handling, and latency guarantees in cluttered warehouse environments.\n\n## Key Concepts\n\n- Two-stage pipelines (detection + pose estimation)\n- Differentiable rendering for supervision\n- Few-shot adaptation (5–10 examples) via meta-learning\n- Temporal smoothing to stabilize pose across frames\n\n## Code Example\n\n```javascript\n// Pseudo-loss combination for pose estimation\nconst poseLoss = L2(predPose, truePose);\nconst maskLoss = BCE(predMask, trueMask);\nconst reprojLoss = reproj","diagram":null,"difficulty":"intermediate","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Salesforce","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T05:08:50.632Z","createdAt":"2026-01-22T02:43:25.205Z"},{"id":"q-5692","question":"Design a monocular vision pipeline for a moving rideshare vehicle to detect, track, and estimate 3D pose and short-term trajectories of vulnerable road users (pedestrians, cyclists, motorcyclists) from a single front camera. New object types (e-scooters) appear weekly; support 5-shot on-device adaptation with sub-40 ms per frame on embedded GPU. Describe architecture, data strategy, losses, adaptation workflow, and evaluation metrics?","answer":"Propose a monocular, edge-optimized pipeline that detects, tracks, and estimates 3D pose and short-term trajectories of vulnerable road users from a front-facing camera. Enable 5-shot on-device adapta","explanation":"## Why This Is Asked\nTests end-to-end CV system design under real-time edge constraints, with safe, urban-focused objects and rapid adaptation to new classes.\n\n## Key Concepts\n- Monocular 3D pose estimation\n- Multi-object tracking with trajectory prediction\n- Edge latency constraints and model compression\n- Few-shot on-device adaptation\n\n## Code Example\n```python\n# Pseudo-loss components for joint detection, pose, and trajectory\nloss_total = w_det * L_det + w_pose * L_pose + w_traj * L_traj\n```\n\n## Follow-up Questions\n- How would you evaluate failure modes under rain/fog conditions?\n- What ablations would you run to justify the 5-shot adaptation strategy?","diagram":"flowchart TD\n  A[Input: Monocular video] --> B[2D Detection & ID tracking]\n  B --> C[3D Pose Estimation]\n  C --> D[Trajectory Prediction (short-term)]\n  D --> E[Output: Detections, Poses, Trajectories, Tracks]","difficulty":"intermediate","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T11:40:55.101Z","createdAt":"2026-01-22T11:40:55.101Z"},{"id":"q-570","question":"How would you design a real-time object detection system for Airbnb's property listing photos that can identify amenities and safety violations while processing 10,000 images per hour?","answer":"I'd design a distributed object detection pipeline using Redis queues for image processing, YOLOv8 for real-time detection, and GPU-accelerated batch inference. The system would employ model ensembling for accuracy, TensorRT optimization for performance, and intelligent result caching.","explanation":"## Architecture\n- **Ingestion**: S3 event triggers Lambda functions to queue images for processing\n- **Processing**: GPU workers execute YOLOv8 models with TensorRT optimization for maximum throughput\n- **Storage**: Detection results and metadata stored in PostgreSQL with vector embeddings for similarity search\n- **API Layer**: FastAPI endpoints with Redis caching for low-latency responses\n\n## Performance Optimization\n- **Batch Processing**: Process 32 images per GPU batch to maximize utilization\n- **Model Selection**: YOLOv8-large achieves 45% mAP at 80 FPS for high accuracy requirements\n- **Auto-scaling**: Dynamic worker scaling based on queue depth and processing latency metrics\n\n## Design Trade-offs\n- **Accuracy vs Speed**: Medium models provide optimal balance with 35% mAP at 120 FPS\n- **Cost Efficiency**: Spot instances reduce GPU costs by 70% with checkpoint recovery\n- **Latency Targets**: P99 processing time under 2 seconds for real-time user experience","diagram":"flowchart TD\n  A[S3 Upload] --> B[Lambda Trigger]\n  B --> C[Redis Queue]\n  C --> D[GPU Workers]\n  D --> E[YOLOv8 Detection]\n  E --> F[PostgreSQL]\n  F --> G[API Cache]\n  G --> H[Client]","difficulty":"advanced","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Instacart"],"eli5":null,"relevanceScore":null,"voiceKeywords":["distributed pipeline","redis queue","yolov8","batch inference","tensorrt","model ensemble"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2026-01-08T11:41:33.371Z","createdAt":"2025-12-27T01:12:04.929Z"},{"id":"q-5763","question":"Design a real-time cross-camera 3D scene understanding module for a rideshare vehicle using a multi-camera rig (front + side) to detect, segment, and estimate 3D pose of vulnerable road users and new weekly object types (e-scooters, delivery robots). Achieve 5-shot on-device adaptation with latency <50 ms per frame. Describe architecture, data strategy, losses, adaptation workflow, and evaluation metrics?","answer":"Propose a shared backbone with per-camera heads (front/side), a lightweight cross-camera fusion module leveraging ego-motion, and a temporal refinement stage to estimate 6-DoF pose and segmentation fo","explanation":"## Why This Is Asked\nTests cross-camera fusion, temporal consistency, few-shot on-device adaptation, and strict latency under real-world urban load.\n\n## Key Concepts\n- Cross-camera feature fusion with ego-motion\n- 3D pose and semantic segmentation in real time\n- On-device few-shot adaptation for new classes\n- Latency and memory constraints on edge GPUs\n\n## Code Example\n```javascript\nfunction fuse(frontFeat, sideFeat, egoMotion) {\n  // Simplified fusion sketch: align sides, combine features, apply temporal refinement\n  const aligned = align(frontFeat, sideFeat, egoMotion);\n  return refine(aligned);\n}\n```\n\n## Follow-up Questions\n- How would you handle severe occlusion or rapid ego-motion jitter?\n- What privacy-preserving tweaks would you add for passenger data?","diagram":"flowchart TD\n  Rig[Camera Rig] --> Backbone[Shared Backbone]\n  Backbone --> FrontHead[Front Camera Head]\n  Backbone --> SideHead[Side Camera Head]\n  FrontHead --> Fusion[Cross-Camera Fusion & Temporal Refinement]\n  SideHead --> Fusion\n  Fusion --> Output[Predictions: 3D Pose + Segmentation + Identities]","difficulty":"intermediate","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T15:45:19.138Z","createdAt":"2026-01-22T15:45:19.138Z"},{"id":"q-5798","question":"In a retail store, design a privacy-preserving edge CV pipeline that detects, segments, and localizes new grocery SKUs on shelf faces from a single RGB stream. SKUs arrive weekly; support 5-shot on-device adaptation with latency under 40 ms per frame on an embedded GPU. Outline architecture, data strategy (synthetic+real), losses, adaptation workflow, and evaluation metrics; include privacy constraints (no raw frames exit device) and low-bandwidth sync to cloud?","answer":"Two-tier edge model: a lightweight detector (EffDet-lite) with a prototype-based adapter bank that updates from 5 labeled examples. Use meta-learning (ProtoNet/MAML) for rapid adaptation; enforce priv","explanation":"## Why This Is Asked\nRetail SKUs change weekly and privacy constraints prevent raw footage leaving devices. Edge adaptation with tiny budgets tests practicality in stores.\n\n## Key Concepts\n- Edge inference and latency budgets\n- Few-shot adaptation and prototype-based learning\n- Privacy-preserving ML with on-device processing\n- Synthetic+real data strategies and domain shift handling\n\n## Code Example\n```python\n# Example adaptation snippet\ndef adapt_prototypes(support_images, labels):\n    prototypes = build_prototypes(support_images, labels)\n    return prototypes\n```\n\n## Follow-up Questions\n- How would you ensure privacy while still enabling cloud-side analytics?\n- How would you evaluate robustness to lighting changes and occlusion?","diagram":null,"difficulty":"advanced","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Instacart"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T17:06:56.000Z","createdAt":"2026-01-22T17:06:56.000Z"},{"id":"q-5814","question":"Design a monocular vision system for AR glasses to simultaneously detect a handheld object, estimate its 6-DoF pose, and predict contact points with the user's hand during natural manipulation, using a single RGB camera. New object geometries arrive weekly; support 5-shot on-device adaptation with latency under 40 ms per frame. Describe architecture, data strategy, losses, adaptation workflow, and evaluation metrics?","answer":"Propose a streaming monocular system that jointly estimates 6-DoF object pose and hand-object contact maps from a single RGB feed, with a fast on-device adapter for new geometries (5-shot). Include a ","explanation":"## Why This Is Asked\nTests integration of hand tracking, object pose, and contact prediction on constrained hardware, in a realistic AR scenario with weekly new objects and tight latency.\n\n## Key Concepts\n- Joint hand-object pose estimation from monocular RGB\n- On-device few-shot adaptation for new geometries\n- Contact heatmaps and temporal consistency\n- Synthetic+real data with domain randomization and self-supervision\n\n## Code Example\n```javascript\nfunction totalLoss(poseEst, contactMap, groundTruth) {\n  const poseLoss = poseEst.loss(groundTruth.pose);\n  const contactLoss = heatmapLoss(contactMap, groundTruth.contact);\n  return poseLoss + 0.5 * contactLoss;\n}\n```\n\n## Follow-up Questions\n- How would you handle highly reflective objects that fail standard shading cues?\n- What metrics and instrumentation would you use to quantify perceived AR latency and user comfort?","diagram":"flowchart TD\n  A[Input RGB] --> B[Hand Pose Estimation]\n  A --> C[Object Pose Estimation]\n  B --> D[Contact Map]\n  C --> D\n  D --> E[Temporal Tracking]","difficulty":"advanced","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Oracle","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T17:46:37.973Z","createdAt":"2026-01-22T17:46:37.973Z"},{"id":"q-5922","question":"Design a real-time edge CV system that detects humans and tools, estimates 6-DoF pose for both, and predicts safe handoff gestures to coordinate with a collaborative robot. New tools arrive weekly; support 5-shot on-device adaptation with <50 ms/frame. Describe architecture, data strategy, losses, adaptation workflow, and evaluation metrics?","answer":"Real-time edge computer vision system that detects humans and tools, estimates 6-DoF pose for both entities, and predicts safe handoff gestures to coordinate with a collaborative robot operating in a shared workspace. The system must accommodate new tools introduced weekly and support 5-shot on-device adaptation while maintaining sub-50 millisecond processing latency per frame.","explanation":"## Why This Is Asked\nThis question tests your ability to design safety-critical computer vision systems for human-robot collaboration in industrial environments. It evaluates your understanding of edge computing constraints, few-shot learning, and multi-task learning approaches under strict latency requirements.\n\n## Key Concepts\n- Edge inference under 50 ms/frame processing constraint\n- Multi-task learning architecture: object detection, 6-DoF pose estimation, and gesture recognition\n- 6-DoF pose estimation using Perspective-n-Point (PnP) algorithm from 2D keypoint detections\n- 5-shot on-device adaptation using lightweight adapter modules or efficient fine-tuning strategies\n- Real-time performance optimization for edge deployment\n- Evaluation metrics: pose estimation error, gesture classification accuracy, and handoff success rate\n\n## Code Example\n```javascript\n// Lightweight adapter for per-tool few-shot adaptation (pseudo-code)\nfunction adaptModel(baseModel, supportShots) {\n  // Implement efficient adapter layers for rapid tool adaptation\n  return adaptedModel;\n}\n```\n\n## Follow-up Questions\n- How would you handle occlusion scenarios and completely unseen tool categories?\n- What optimization techniques would ensure consistent sub-50ms performance across different hardware configurations?\n- How would you implement continuous learning without catastrophic forgetting of previously learned tools?","diagram":null,"difficulty":"intermediate","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Cloudflare","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T05:51:57.207Z","createdAt":"2026-01-22T22:31:46.668Z"},{"id":"q-5992","question":"In a mobile AR app, design a real-time, RGB-only system that reconstructs a metric 3D room layout and tracks dynamic objects (chairs, tables, people) with per-frame latency under 30 ms on a modern mobile GPU. The system must support 5-shot adaptation to new furniture models arriving monthly, using on-device learning. Describe architecture, data strategy (synthetic+real, CAD assets), losses (photometric, depth/normal, temporal), adaptation workflow, and evaluation metrics (depth/mesh accuracy, pose stability, AR alignment)?","answer":"Implement a real-time RGB SLAM and scene reconstruction system featuring a lightweight EfficientNet-Lite backbone with depth prediction head, fused with TSDF voxel grid processing for metric reconstruction. Deploy a parallel dynamic object tracking stream that utilizes instance segmentation masks and a motion-aware transformer to output per-object pose and shape parameters. The static scene employs TSDF fusion for accurate metric reconstruction, while dynamic objects are tracked through optical flow combined with learned motion priors. For 5-shot adaptation, freeze the backbone and fine-tune only object-specific heads using synthetic CAD renders blended with real frames via domain randomization. The system processes RGB input at 30 FPS on mobile GPU through optimized batch inference and memory-efficient voxel updates.","explanation":"## Why This Is Asked\nThis question evaluates end-to-end design capabilities for monocular 3D reconstruction and dynamic object tracking on mobile devices, including rapid adaptation to new assets using minimal labeled examples. It tests architectural trade-offs (backbone selection, fusion strategies, dynamic object handling), data strategy formulation, and comprehensive evaluation methodologies.\n\n## Key Concepts\n- Real-time monocular 3D reconstruction with TSDF fusion\n- Dynamic object segmentation and tracking in video streams\n- On-device few-shot adaptation using CAD synthetic data\n- Multi-loss optimization strategies for depth and temporal consistency\n- Mobile GPU optimization for sub-30ms latency requirements","diagram":"flowchart TD\n  A[RGB Frame] --> B[Feature Extractor]\n  B --> C[Depth Head]\n  C --> D[TSDF Fusion]\n  A --> E[Dynamic Mask/MOT]\n  D --> F[Mesh Output]\n  E --> F","difficulty":"advanced","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T04:52:23.282Z","createdAt":"2026-01-23T02:48:01.807Z"},{"id":"q-6316","question":"Design a monocular edge vision system for a mobile warehouse robot to identify, segment, and estimate 6-DoF pose of tools (wrenches, pliers) on a cluttered workbench, while reconstructing a compact 3D scene for grasp planning. New tool geometries appear weekly; support 3-shot on-device adaptation with latency <50 ms per frame on embedded GPU. Describe architecture, data strategy, losses, adaptation workflow, and evaluation metrics?","answer":"Outline a lightweight multi-task head sharing a CNN backbone (e.g., efficient ConvNeXt) that outputs tool segmentation, 6-DoF pose (via PnP with a learned depth proxy), and a coarse 3D scene map for g","explanation":"## Why This Is Asked\nTests ability to design an edge-optimized, multi-task CV system with few-shot adaptation in a logistics setting.\n\n## Key Concepts\n- Edge-friendly multi-task learning\n- Few-shot on-device adaptation\n- 6-DoF pose from monocular data + depth proxy\n- Synthetic-real data mix with domain randomization\n- Grasp-aware scene reconstruction\n\n## Code Example\n```javascript\n// Adapter sketch for quick task-specific fine-tuning\nclass Adapter{ constructor(d){ this.d=d } forward(x){ return x; } }\n```\n\n## Follow-up Questions\n- How would you handle severe occlusion or fast tool rearrangement?\n- What ablations would you run to justify 3-shot adaptation versus fine-tuning per item?","diagram":null,"difficulty":"intermediate","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T19:03:46.305Z","createdAt":"2026-01-23T19:03:46.305Z"},{"id":"q-6498","question":"Design a beginner-friendly CV pipeline for a fixed overhead RGB camera monitoring a 6-slot hardware shelf to flag mislabelled or swapped items by category (e.g., sensors vs actuators). Use only RGB cues and a tiny classifier trained on 3–5 examples per SKU, map detections to shelf slots, handle occlusions, and keep per-frame latency under 50 ms. Describe architecture, data strategy, adaptation workflow, and evaluation metrics?","answer":"Implement a fixed overhead RGB pipeline for a 6-slot shelf. Map slots, detect items with lightweight color/texture cues, classify SKU with a tiny CNN or SVM trained on 3–5 examples, and compare agains","explanation":"## Why This Is Asked\nTests a beginner-friendly yet practical shelf-scanning solution, focusing on slot mapping, lightweight features, and few-shot adaptation with real-time constraints.\n\n## Key Concepts\n- Grid-slot mapping and spatial association\n- Lightweight RGB features (color/texture) + tiny classifier\n- Occlusion handling with simple temporal consistency\n- Few-shot adaptation (3–5 examples per SKU) and on-device latency\n- Evaluation: per-slot accuracy, mislabel rate, frame-time budget\n\n## Code Example\n```javascript\n// Pseudo: map frame to 6 slots and assign topN SKU predictions\nfunction detectShelf(frame, catalog, slots){\n  // 1) extract regions per slot\n  // 2) compute lightweight features per region\n  // 3) classify with small model (3-5 examples per SKU)\n  // 4) compare with catalog; flag swaps\n  // 5) return slot states\n}\n```\n\n## Follow-up Questions\n- How would you handle weekly new SKUs with on-device adaptation without re-training?\n- How would you calibrate for different lighting conditions to maintain accuracy?","diagram":"flowchart TD\n  A[Overhead RGB Frame] --> B[Slot Grid Mapping]\n  B --> C[Region Feature Extraction]\n  C --> D[Tiny Classifier]\n  D --> E[Slot Catalog Comparison]\n  E --> F[Flag Mislabel/Swap]\n  F --> G[Temporal Association & Latency Check]","difficulty":"beginner","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Oracle","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T05:34:41.469Z","createdAt":"2026-01-24T05:34:41.470Z"},{"id":"q-6703","question":"Design a real-time monocular pipeline to detect and localize monthly-changing warehouse glyph tags on shelves from a mobile robot's RGB camera, estimating their 6-DoF pose. Tags change weekly; support 3-shot on-device adaptation with under 50 ms per frame. Describe architecture, data strategy, losses, adaptation workflow, and evaluation metrics?","answer":"Two-stage monocular pipeline: (1) a fast glyph detector to localize tag regions; (2) a lightweight pose net that outputs 6-DoF via 2D-3D correspondences refined with PnP+RANSAC. Adaptation: 3-shot on-","explanation":"## Why This Is Asked\nThis probes real-time monocular pose estimation with dynamic symbols in a mobile-robot warehouse setting, plus on-device few-shot adaptation.\n\n## Key Concepts\n- Monocular 6-DoF pose from changing glyphs\n- On-device 3-shot adaptation via meta-learning\n- Synthetic+real data and differentiable supervision\n- PnP+RANSAC refinement; latency constraints\n\n## Code Example\n```python\n# Pseudo skeleton: two-stage pose pipeline\nimport torch\nclass PoseHead(nn.Module):\n    def forward(self, crop):\n        # predict 2D-3D correspondences and solve PnP\n        pass\n```\n\n## Follow-up Questions\n- How would you evaluate robustness to occlusion?\n- How would you extend to multiple glyphs per frame?","diagram":null,"difficulty":"intermediate","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","LinkedIn"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T14:02:16.625Z","createdAt":"2026-01-24T14:02:16.625Z"},{"id":"q-6711","question":"You have a fixed downward-facing RGB camera monitoring a supermarket shelf. Design a beginner-friendly CV pipeline to estimate stock levels by counting visible items per product type in real time, handling partial occlusions and variable lighting. Use lightweight cues (color histograms, edges) and optionally a tiny classifier trained on a small labeled set. Outline data flow, processing steps, and evaluation metrics?","answer":"Approach: crop shelf ROI; use color/edge-based segmentation to generate item blobs; count blobs per product type; refine with a tiny 2-layer CNN on blob crops trained with ~20 examples per type; apply","explanation":"## Why This Is Asked\nTests a practical, beginner-friendly CV pipeline for real-time counting under occlusion and lighting changes. Emphasizes fallbacks to simple cues and a tiny classifier, plus lightweight tracking for stability.\n\n## Key Concepts\n- ROI extraction and scene normalization\n- Blob-based counting from simple cues\n- Lightweight classifier for type refinement\n- Frame-to-frame tracking (Kalman)\n\n## Code Example\n```javascript\n// Pseudocode: basic blob counting and simple classifier usage\nfunction countItems(frameROI, types) {\n  const blobs = segmentByColorAndEdges(frameROI);\n  const counts = {};\n  for (const t of types) counts[t] = 0;\n  for (const b of blobs) {\n    const type = classifyBlob(b, t); // tiny CNN or heuristic\n    if (type) counts[type] += 1;\n  }\n  return counts;\n}\n```\n\n## Follow-up Questions\n- How would you handle new product types without retraining the classifier?\n- What are failure modes with reflective packaging or transparent wrappers?","diagram":"flowchart TD\nROI[Shelf ROI] -->Seg[SimpleSegmentation by color/edges]\nSeg --> BlobCount[Blob counting per type]\nBlobCount --> Classify[Tiny CNN on blob crops]\nClassify --> Counts[Frame counts]\nCounts --> Kalman[Temporal smoothing]\nKalman --> Eval[Evaluation & thresholds]","difficulty":"beginner","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Google","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T14:29:00.852Z","createdAt":"2026-01-24T14:29:00.852Z"},{"id":"q-6775","question":"Design a single-RGB-camera, edge-accelerated system that accepts a natural language instruction (e.g., 'find the blue bottle on the middle shelf') and returns a pixel-precise segmentation mask plus a 6-DoF pose for the matched object in cluttered shelves. New object descriptions appear weekly; support 5-shot on-device adaptation with latency under 40 ms per frame. Describe architecture, data strategy, losses, adaptation workflow, and evaluation metrics?","answer":"Architecture uses a compact ViT backbone with a vision-language head that embeds the NL instruction and fuses it with per-object visual features. A lightweight 5-shot adapter enables on-device fine-tu","explanation":"## Why This Is Asked\n\nTests the ability to fuse NL grounding with CV in real-time edge settings and to enable 5-shot adaptation for weekly new objects.\n\n## Key Concepts\n\n- Vision-Language grounding on-device\n- 5-shot adapters for on-device fine-tuning\n- 6-DoF pose from RGB with PnP and depth cues\n- Real-time latency and evaluation metrics\n\n## Code Example\n\n```javascript\n// Pseudo-pipeline sketch\nfunction fuse(imageFeatures, nlEmbedding){ /* ... */ }\n```\n\n## Follow-up Questions\n\n- How would you scale to 1k NL queries and objects?\n- How would you disambiguate conflicting NL instructions in clutter?","diagram":"flowchart TD\n  NL[NL Instruction] --> V[Vision-Language Encoder]\n  V --> C[Candidate Objects & Localization]\n  C --> S[Segmentation Mask]\n  C --> P[6-DoF Pose]\n  Adapter --> Adapt[5-shot On-device Adapter]","difficulty":"advanced","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","Lyft","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T17:01:48.994Z","createdAt":"2026-01-24T17:01:48.994Z"},{"id":"q-6931","question":"Design a real-time monocular 3D scene understanding module for a drone that must detect, segment, and estimate 3D pose for dynamic obstacles (e.g., birds, suspended loads, other drones) while flying through urban canyons at 15 m/s using a single RGB camera. The system must support 3-shot adaptation to novel obstacle types with under 20 ms per frame on an edge device. Describe architecture, temporal fusion, losses, adaptation workflow, and evaluation metrics?","answer":"Two-branch, edge-optimized monocular pipeline: a real-time detector and segmentation backbone paired with a lightweight depth and pose estimation head, integrated with a short-term temporal tracker. Three-shot adaptation is achieved through meta-learning using MAML for rapid novel obstacle generalization. Temporal fusion employs Kalman filtering with motion models to ensure stable tracking during high-speed flight. The multi-task loss function combines detection (focal loss), segmentation (Dice loss), depth estimation (scale-invariant loss), and pose estimation (geometric loss) components. Adaptation workflow: pre-train on diverse obstacle datasets, meta-train with task sampling strategies, then perform three-shot fine-tuning on novel obstacle classes. Evaluation metrics include 2D Average Precision, segmentation Intersection over Union, depth Root Mean Square Error, 3D pose angular and translation error, and processing latency (<20ms per frame).","explanation":"## Why This Is Asked\nThis question evaluates the ability to design real-time, edge-efficient 3D perception systems with few-shot learning capabilities for dynamic obstacle detection in autonomous drone navigation.\n\n## Key Concepts\n- Monocular depth and pose estimation for fast edge inference\n- Temporal fusion techniques for tracking stability\n- Meta-learning approaches for three-shot adaptation\n- Comprehensive evaluation metrics spanning 2D detection, 3D estimation, and latency constraints\n\n## Code Example\n```javascript\nfunction forward(input){\n  // detector + segmentation head\n  // depth + pose estimation\n  // temporal fusion\n  return output;\n}\n```","diagram":"flowchart TD\n  A[RGB Frame] --> B[Detector+Seg Head]\n  B --> C[Depth/Pose Head]\n  C --> D[Temporal Fusion]\n  D --> E[Adaptation Module]\n  E --> F[Output: 3D Poses + SegMasks]","difficulty":"advanced","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Apple","Hugging Face"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T05:51:12.272Z","createdAt":"2026-01-24T23:31:06.853Z"},{"id":"q-6988","question":"Design a beginner CV pipeline for a fixed RGB camera over a grocery shelf to count visible instances of a target item (e.g., a cereal box) in each frame. Handle partial occlusion and varying lighting using only color histograms, simple texture features, contour-based detection, and an optional tiny classifier trained on a small labeled set. Describe data flow, thresholds, and evaluation?","answer":"Implement: 1) predefine target color ranges in HSV and extract color histograms; 2) run edge/texture (LBP) to filter; 3) find blobs with contour area thresholds and cluster overlaps; 4) train a tiny c","explanation":"## Why This Is Asked\nBeginner-friendly count-from-a-single-camera problem focusing on simple cues (color, texture) and lightweight classification, common in retail robotics.\n\n## Key Concepts\n- HSV color histograms for color-based candidate generation\n- Local Binary Pattern texture discrimination\n- Contour-based blob detection and non-maximum suppression\n- Tiny classifier (e.g., logistic regression) on small labeled sets\n- Evaluation: frame-level count error, latency\n\n## Code Example\n```javascript\n// Pseudo-code: pipeline skeleton\nfunction processFrame(frame) {\n  const hsv = toHSV(frame);\n  const colorMask = inColorRange(hsv, targetRange);\n  const edges = computeLBP(frame);\n  const candidates = findBlobs(colorMask, edges);\n  const filtered = classifyCandidates(candidates);\n  const finalDet = nonMaxSuppression(filtered);\n  return count(finalDet);\n}\n```\n\n## Follow-up Questions\n- How would you extend to multiple target items with shared color ranges?\n- How would lighting changes be handled if colors drift over time?","diagram":"flowchart TD\n  A[Input Frame] --> B[HSV Color Extraction]\n  B --> C[Color Thresholding]\n  C --> D[LBP Texture Filter]\n  D --> E[Contours/Blobs]\n  E --> F[Tiny Classifier]\n  F --> G[Non-Max Suppression]\n  G --> H[Count Detections]","difficulty":"beginner","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T04:25:41.790Z","createdAt":"2026-01-25T04:25:41.790Z"},{"id":"q-7268","question":"Design a monocular edge CV pipeline that, from a single RGB camera mounted on a robotic gripper, detects and localizes deformable packaging (e.g., plastic pouches) on a cluttered shelf, and estimates their 6-DoF pose including occlusion-aware segmentation. New packaging designs arrive monthly; support 5-shot on-device adaptation with under 60 ms per frame. Describe architecture, data strategy (synthetic+real with domain randomization), losses, adaptation workflow, and evaluation metrics?","answer":"Propose an end-to-end edge pipeline: a lightweight CNN backbone feeding a joint 6-DoF pose head and an occlusion-aware mask head, plus a refinement module for deformable packaging. Use synthetic data ","explanation":"## Why This Is Asked\nRealistic test of designing robust, real-time perception for deformable objects with on-device adaptation and occlusion handling.\n\n## Key Concepts\n- Deformable-object 6-DoF pose estimation\n- Occlusion-aware segmentation\n- On-device 5-shot adaptation\n- Synthetic+real data with domain randomization\n- Edge latency targets and refinement loops\n\n## Code Example\n```javascript\n// Pseudo-loss aggregation for joint pose and mask\nL_pose = ADD_S(predict_pose, gt_pose)\nL_mask = BCE(predict_mask, gt_mask) + Dice(predict_mask, gt_mask)\nL_total = w1*L_pose + w2*L_mask + w_refine*L_refine\n```\n\n## Follow-up Questions\n- How would you validate 5-shot adaptation effectiveness across monthly design changes?\n- What debugging steps would you use for occlusion-heavy scene failures?","diagram":null,"difficulty":"intermediate","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","LinkedIn"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T15:53:51.233Z","createdAt":"2026-01-25T15:53:51.234Z"},{"id":"q-7288","question":"Design a real-time monocular vision system for a data-center service robot that detects, tracks, and estimates 6-DoF pose of server components (PSUs, drives, cables) from a moving RGB camera, and classifies whether a connector is engaged or disengaged. Weekly new hardware variants appear; support 5-shot on-device adaptation with sub-60 ms per frame on an edge GPU. Describe architecture, data strategy (synthetic + real), losses, adaptation workflow, and evaluation metrics?","answer":"Propose a lightweight detector + 6-DoF pose head with temporal tracking. Use synthetic data with domain randomization plus a small real per-variant fine-tuning set. Freeze backbone; adapt pose+state h","explanation":"## Why This Is Asked\n\nAssesses ability to craft a robust, on-device CV pipeline for dynamic data-center robotics, handling frequent hardware variants and few-shot adaptation while meeting strict latency.\n\n## Key Concepts\n\n- On-device fine-tuning\n- 6-DoF pose estimation\n- Temporal tracking\n- Synthetic data + domain randomization\n- On-variant few-shot adaptation\n\n## Code Example\n\n```javascript\n// Skeleton: detector + pose head forward pass\nfunction forwardPass(frame) {\n  const det = detector(frame);\n  const pose = poseHead(det.bboxes, frame);\n  return {det, pose};\n}\n```\n\n## Follow-up Questions\n\n- How would you handle severe occlusion from dense cabling?\n- What metrics would you use to validate adaptation stability across variants?","diagram":"flowchart TD\n  A[Input RGB frame] --> B[Detector & Pose Head]\n  B --> C[Tracker]\n  A --> D[State Classifier]\n  C --> E[Output]\n  D --> E","difficulty":"advanced","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Hashicorp","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T16:56:31.717Z","createdAt":"2026-01-25T16:56:31.717Z"},{"id":"q-7299","question":"Scenario: A grocery fulfillment robot uses a fixed top-down RGB camera to verify item placement and label orientation for reliable grasping. Design a beginner-friendly pipeline that combines lightweight cues (color histogram, edge density, texture) with a tiny classifier to detect correct vs misoriented items, and supports 3-5 shot on-device adaptation with under 50 ms per frame. Describe architecture, data strategy, losses, adaptation workflow, and evaluation metrics?","answer":"Use a lightweight top-down pipeline: a small CNN (5-7 layers) processes ROI crops; fuse with fast classical cues (color histogram, edge density, texture) in a tiny decision head to judge correct vs mi","explanation":"## Why This Is Asked\nTests the ability to combine learned features with classical cues under tight on-device latency, in a realistic retail robotics setting.\n\n## Key Concepts\n- Lightweight CNN + classical feature fusion\n- On-device few-shot adaptation (3-5 shots)\n- Latency budgeting (<=50 ms/frame)\n- Robust evaluation: per-item accuracy, misorientation metrics, throughput\n\n## Code Example\n```javascript\n// Pseudo-code sketch of forward pass\nfunction forward(frame) {\n  const rois = detectROIs(frame);\n  for (const roi of rois) {\n    const cnnFeat = smallCNN(roi);\n    const cues = extractCues(roi); // hist, edges, texture\n    const fused = fuse(cnnFeat, cues);\n    const label = orientationHead(fused);\n    if (label === 'misoriented') alert(roi);\n  }\n}\n```\n\n## Follow-up Questions\n- How would you handle label drift with new item designs?\n- What ablation studies would you run to verify cue contributions?","diagram":"flowchart TD\n  A[Input frame] --> B[ROI detection]\n  B --> C[CNN features]\n  C --> D[Classical cues fusion]\n  D --> E[Orientation classifier]\n  E --> F[Decision: correct / misoriented]","difficulty":"beginner","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","Robinhood","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T17:29:08.003Z","createdAt":"2026-01-25T17:29:08.003Z"},{"id":"q-7339","question":"Design a real-time edge vision pipeline for a live video call platform aimed at Twitter and Zoom to detect and estimate 6-DoF head pose and gaze for up to 8 participants from a single frontal RGB camera, plus micro-expressions to infer engagement. Include on-device 5-shot adaptation for new demographics, privacy-preserving processing (deterministic obfuscation before cloud streaming), and per-frame latency under 30 ms. Describe architecture, data strategy, losses, adaptation workflow, and evaluation metrics?","answer":"Edge-first multi-person vision for live calls: run a lightweight head-pose and gaze estimator per participant from a single front camera, plus micro-expr cues via a compact 3D facial model. On-device ","explanation":"## Why This Is Asked\nTests real-time multi-person CV at edge with privacy constraints, demographic adaptation, and micro-expression signals—relevant to social/video platforms.\n\n## Key Concepts\n- Edge-accelerated multi-person pipelines\n- 3D facial modeling for pose/gaze\n- Micro-expression signals\n- On-device adaptation (5-shot)\n- Privacy-preserving streaming\n\n## Code Example\n```javascript\n// Placeholder: example glue code for per-frame orchestration\n```\n\n## Follow-up Questions\n- How would you measure privacy impact and latency trade-offs in production?","diagram":"flowchart TD\n  A[Input: video frame] --> B[Face detection + ROI extraction]\n  B --> C[On-device head pose & gaze estimator]\n  C --> D[Micro-expression timeline model]\n  D --> E[Privacy module: obfuscation & DP]\n  E --> F[Cloud/UI streaming]\n  F --> G[Model adaptation loop (5-shot)]","difficulty":"advanced","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Twitter","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T19:08:10.053Z","createdAt":"2026-01-25T19:08:10.053Z"},{"id":"q-7374","question":"Design a real-time monocular edge pipeline for indoor scene understanding to enable AR furniture placement in Airbnb listings. From a single RGB video stream, detect and segment furniture (chair/table/sofa), estimate 6-DoF pose, and recover a coarse 3D shape for each item as the user walks through a room. Weekly-new models require 5-shot on-device adaptation with latency <50 ms/frame on embedded GPU. Describe architecture, data strategy (synthetic+real), losses, adaptation workflow, and evaluation metrics?","answer":"Architect a lightweight monocular edge system: a shared backbone (MobileNetV3+), a fast instance segmentation head (lite Mask R-CNN), a 6-DoF pose head, and a cuboid-based shape decoder. Weekly new mo","explanation":"## Why This Is Asked\nContextualizes real-world AR in rental spaces; emphasizes on-device adaptation and latency.\n\n## Key Concepts\n- Monocular 3D pose with shape priors\n- Few-shot adaptation on edge\n- Synthetic+real data and domain randomization\n- Real-time, memory-constrained inference\n\n## Code Example\n```javascript\nfunction poseAddS(posePred, poseGT) {\n  // compute ADD-S distance between predicted and ground-truth models\n  return distance(posePred.points, poseGT.points);\n}\n```\n\n## Follow-up Questions\n- Why use cuboid priors vs full meshes?\n- How would you measure AR placement drift over time?\n- what changes for 60 FPS on mid-range devices?","diagram":"flowchart TD\n  A[Input video] --> B[Backbone]\n  B --> C[Segmentation Head]\n  B --> D[6-DoF Pose Head]\n  B --> E[Shape Decoder]\n  C --> F[Masks]\n  D --> G[Pose 6-DoF]\n  E --> H[Shape priors]\n  F --> I[AR overlay]\n  G --> I\n  I --> J[Evaluation]","difficulty":"intermediate","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Discord"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T20:40:06.178Z","createdAt":"2026-01-25T20:40:06.178Z"},{"id":"q-7392","question":"Scenario: A fixed RGB camera overlooks a warehouse shelf stocked with clear, transparent plastic bottles. Design a beginner-friendly CV pipeline to detect, localize, and count each bottle in view, despite reflections and partial occlusions. Use only a single RGB stream and lightweight cues (color histogram, edge density, simple optical flow) and optionally a tiny classifier trained on a small labeled set. Outline architecture, data strategy, losses, adaptation workflow for new bottle designs, and evaluation metrics?","answer":"Two-stage lightweight pipeline: (1) candidate blob proposals from color-normalized edge-density maps to roughly segment bottle regions; (2) a tiny classifier/regressor requiring 5-10 examples per new design to refine proposals and filter false positives from reflections. Data strategy: collect diverse lighting/angle scenarios, augment with synthetic glare artifacts, and maintain a small adaptation set per bottle variant. Losses: binary cross-entropy for detection, smooth L1 for bounding box regression, and contrastive loss for few-shot adaptation. Adaptation workflow: freeze backbone, fine-tune classification head on new design samples with online learning. Evaluation metrics: precision/recall at IoU>0.5, count error (MAE), and inference latency (<30ms target).","explanation":"## Why This Is Asked\n\nTests ability to design practical, on-device CV with challenging reflective objects using only RGB. Focus on fallbacks for glare, simple cues, and few-shot adaptation.\n\n## Key Concepts\n\n- Single RGB robustness with reflections\n- Lightweight cues and modular design\n- Few-shot adaptation for new bottle designs\n- On-device latency considerations\n\n## Code Example\n\n```javascript\n// Pseudocode for edge-density blob proposals\nfunction proposalBlobs(img) {\n  const g = grayscale(img);\n  const edges = sobel(g);\n  const density = normalize(abs(edges));\n  return findBlobs(density > threshold);\n}\n```","diagram":"flowchart TD\n  A[Camera Input] --> B[Edge-density + color normalization]\n  B --> C[Blob Proposals]\n  C --> D[Tiny Classifier]\n  D --> E[Center Estimation & Count]","difficulty":"beginner","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Databricks","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T06:53:42.422Z","createdAt":"2026-01-25T21:29:57.116Z"},{"id":"q-7420","question":"Given a fixed overhead RGB camera overlooking a store aisle, design a beginner-friendly CV pipeline to estimate real-time checkout queue length and predicted wait time using only a single RGB stream. Use lightweight background subtraction, blob tracking, and privacy-preserving steps (e.g., face blur). Outline data flow, thresholding, and an evaluation plan?","answer":"Design a lightweight single-RGB pipeline using background subtraction to extract moving blobs, filter by size and aspect ratio, assign blobs to predefined ROI regions per checkout lane, track with a simple Kalman filter for temporal consistency, count unique IDs per ROI for queue length, and estimate wait time based on historical throughput data. Include privacy-preserving face blur and optional region-based anonymization.","explanation":"## Why This Is Asked\nTests ability to design a privacy-preserving, low-latency CV system with simple components for real-time queue analytics.\n\n## Key Concepts\n- Background subtraction, blob tracking, simple data association\n- ROI-based counting, privacy preservation, latency budgeting\n- Lightweight evaluation: frame-accurate counts vs ground truth\n\n## Code Example\n```javascript\n// Pseudo: subtract background, detect blobs, track with Kalman, count per ROI\n```\n\n## Follow-up Questions\n- How would you adapt if lighting changes\n- How to quantify confidence in queue length estimates","diagram":"flowchart TD\n  A[RGB Frame] --> B[Background Subtraction]\n  B --> C[Blob Detections]\n  C --> D[Tracking (Kalman)]\n  D --> E[Queue Length Estimation]\n  E --> F[Privacy Masking]","difficulty":"beginner","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T06:35:05.981Z","createdAt":"2026-01-25T22:33:28.590Z"},{"id":"q-7464","question":"Design an on-device monocular CV pipeline for a warehouse drone to detect, segment, and estimate the 6-DoF pose of reflective metal caps on a fast-moving belt; weekly new cap geometries; support 1–3 shot adaptation with sub-40 ms per frame; include on-device intrinsic self-calibration, an uncertainty-aware pose head, and a lightweight quality check for human handoff. Describe architecture, losses, data strategy, adaptation workflow, and evaluation?","answer":"Edge-first design: Efficient backbone, detection+segmentation head, and a 6-DoF pose head predicting translation+rotation with per-frame uncertainty. Refine with a lightweight PnP-ICP. On-device intrinsic self-calibration uses a calibration pattern or natural features with bundle adjustment. Losses combine detection (focal), segmentation (Dice+CrossEntropy), pose (Huber for translation, geodesic for rotation), uncertainty (negative log-likelihood), and calibration loss. Data strategy: synthetic rendering of cap geometries with domain randomization, complemented by real-world calibration sequences. Adaptation workflow: weekly, capture 1–3 reference shots of new caps, fine-tune detection/segmentation heads while freezing backbone, update pose head with uncertainty regularization. Quality check monitors pose uncertainty, reprojection error, and segmentation confidence—flagging low-confidence frames for human review. Evaluation: latency profiling (<40ms), pose accuracy (ADD-S for caps), uncertainty calibration (reliability diagrams), and adaptation efficiency (few-shot convergence).","explanation":"## Why This Is Asked\nTests ability to design real-time edge CV with uncertainty and on-device adaptation for changing parts.\n\n## Key Concepts\n- Edge AI and latency constraints\n- Uncertainty estimation for poses\n- Self-calibration of intrinsics\n- 1–3 shot adaptation with synthetic+real\n- PnP-ICP refinement\n\n## Code Example\n```javascript\n// Pseudocode for adaptation step\nfunction adapt(model, examples){ /* ... */ }\n```\n\n## Follow-up Questions\n- How would you validate uncertainty calibration in deployment?\n- What failure modes trigger offload to cloud or human-in-the-loop?","diagram":"flowchart TD\n  Drone[Drone Edge] --> Detect[Detect/Segment]\n  Detect --> Pose[6-DoF Pose Head]\n  Pose --> Refine[PnP-ICP Refine]\n  Refine --> Uni[Uncertainty Head]\n  Uni --> Handoff{Quality Check}\n  Handoff --> Cloud[Offload/Monitoring]\n","difficulty":"advanced","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Meta","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T05:38:25.946Z","createdAt":"2026-01-26T02:34:18.237Z"},{"id":"q-7617","question":"Design a real-time edge vision system that fuses RGB, depth, and proprioceptive priors to detect, localize, and estimate 6-DoF pose of textureless, small fasteners (e.g., screws) on a cluttered conveyor while a robotic gripper reaches to pick them. Weekly new fastener designs appear; support 5-shot on-device adaptation with sub-40 ms per frame on an edge GPU. Describe architecture, data strategy (synthetic+real), losses (photometric, geometric, pose prior), adaptation workflow, and evaluation metrics?","answer":"Propose a multi-stream fusion approach using stereo RGB + depth, with a lightweight 6-DoF pose head and differentiable rendering for pose refinement. Enable 5-shot adaptation via meta-learning and on-","explanation":"## Why This Is Asked\n\nAssess ability to design practical, latency-bounded multi-sensor systems for object pose under weekly design changes.\n\n## Key Concepts\n\n- Sensor fusion and multi-stream architectures\n- 6-DoF pose estimation for textureless objects\n- On-device adaptation and meta-learning workflows\n- Differentiable rendering or geometry-based losses\n- Synthetic+real data strategy and evaluation\n\n## Code Example\n\n```javascript\n// Implementation code here\n```\n\n## Follow-up Questions\n\n- How would you handle occlusion and partial visibility?\n- What ablations would you run to validate each loss term?\n","diagram":"flowchart TD\n  RGB+Depth --> FeatureExtraction\n  FeatureExtraction --> PoseHead\n  PoseHead --> PoseRefinement\n  PoseRefinement --> Losses\n  Losses --> Adaptation\n  Adaptation --> Evaluation","difficulty":"advanced","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Plaid","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T09:59:38.337Z","createdAt":"2026-01-26T09:59:38.337Z"},{"id":"q-7667","question":"Design a real-time, cross-view 3D object detector and pose estimator to identify and track weekly-updated 3D tokens (e.g., security badges or fasteners) on cluttered shelves using two synchronized RGB cameras (depth optional). Require a 5-shot on-device adaptation workflow with latency under 50 ms per frame. Describe architecture, data strategy, losses, adaptation workflow, and evaluation metrics, including temporal consistency and privacy constraints?","answer":"Two-view detector with a shared 3D head and lightweight cross-view fusion. Use a 5-shot on-device adapter that updates final layers per weekly token set with a single gradient step. Losses: Focal for ","explanation":"## Why This Is Asked\n\nTests ability to design multi-view 3D perception with few-shot adaptation while meeting strict latency and privacy constraints. The scenario probes cross-view fusion, on-device learning, and robust evaluation across occlusions and motion.\n\n## Key Concepts\n\n- Cross-view fusion\n- 6-DoF pose with PnP\n- Few-shot on-device adaptation\n- Temporal consistency\n- Privacy-by-design\n\n## Code Example\n\n```javascript\n// Pseudocode: simple pose residual loss between predicted and target 3D landmarks\nfunction poseLoss(pred, target) {\n  const loss = meanSquaredError(pred.landmarks, target.landmarks);\n  return loss;\n}\n```\n\n## Follow-up Questions\n\n- How to handle severe occlusion and repeated tokens?\n- How would you validate real-time latency on heterogeneous edge devices?","diagram":"flowchart TD\n  A[Input: two RGB images] --> B[Shared 3D head]\n  B --> C{Fusion}\n  C --> D[Detec/ localize tokens]\n  D --> E[Pose Estimation]\n  E --> F[Tracking]","difficulty":"advanced","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","PayPal","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T11:55:13.278Z","createdAt":"2026-01-26T11:55:13.279Z"},{"id":"q-7720","question":"Scenario: A fixed overhead RGB camera watches a cluttered lab bench. Design a beginner-friendly CV pipeline to detect whether a specific tool (e.g., a hex wrench) is correctly seated in its labeled slot. Use only a single RGB stream and lightweight cues (color histograms, edge density, simple background subtraction) and optionally a tiny classifier trained on a small labeled set. Outline data flow, thresholds, and evaluation?","answer":"Proposed approach: locate the labeled slot ROI with lightweight template matching; subtract background to confirm object presence; extract features per ROI: HSV color histogram (H,S channels), edge de","explanation":"## Why This Is Asked\nTests from-scratch practical CV design with classic features on CPU, in cluttered bench setting.\n\n## Key Concepts\n- ROI localization\n- Lightweight features (HSV hist, edge density)\n- Simple classifier and temporal smoothing\n- Data collection under lighting variation\n\n## Code Example\n```javascript\n// Pseudo-code outline for ROI feature extraction\n```\n\n## Follow-up Questions\n- How would you scale to multiple tool slots?\n- How would you handle changing backgrounds or reflections?","diagram":null,"difficulty":"beginner","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Tesla","Two Sigma","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T15:02:48.303Z","createdAt":"2026-01-26T15:02:48.303Z"},{"id":"q-7793","question":"Design a real-time monocular vision system that reconstructs 3D geometry and detects/poses a weekly-updated class of desk objects (mugs, notebooks, pens) from a single RGB camera mounted on a laptop. New object types appear weekly; enable 5-shot on-device adaptation with under 60 ms per frame. Describe architecture, data strategy, losses, adaptation workflow, and evaluation metrics?","answer":"Propose a lightweight monocular 3D scene parser: MobileNetV3 backbone with a depth head and an object-aware 6-DoF pose decoder. Train on synthetic+real using domain randomization; render-consistency a","explanation":"## Why This Is Asked\n\nAssess ability to design real-time edge CV with dynamic object sets and ultra-low latency, plus on-device adaptation and robust pose estimation.\n\n## Key Concepts\n\n- Monocular 3D reconstruction\n- Few-shot on-device adaptation via adapters\n- Synthetic+real training and differentiable rendering\n- Occlusion-robust pose estimation and depth hints\n- Latency budgeting and evaluation metrics\n\n## Code Example\n\n```javascript\n// Pseudocode: adapter-based 5-shot fine-tuning on device\nclass Adapter{ /* lightweight FiLM-style modules */ }\nfunction refinePose(detections, adapters){ /* iterative pose refinement using reprojection error */ }\n```\n\n## Follow-up Questions\n\n- How would you handle extreme occlusion or glossy surfaces?\n- How would you validate performance as new object types accumulate weekly? \n","diagram":null,"difficulty":"intermediate","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","MongoDB","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T17:57:49.009Z","createdAt":"2026-01-26T17:57:49.009Z"},{"id":"q-7871","question":"Design a real-time monocular CV system for a drone-based turbine inspection that detects, localizes, and estimates 6-DoF pose of structural bolts and fasteners on turbine blades as they rotate in wind. New bolt geometries appear; support 2-shot on-device adaptation with under 60 ms per frame on an embedded GPU. Describe architecture, data strategy (synthetic+real), losses, adaptation workflow, and evaluation metrics?","answer":"Use a lightweight backbone (MobileNetV3+FPN) with a 6-DoF pose head; train with 2D detections, 3D keypoints, and PnP-based pose losses. For adaptation, deploy a tiny adapter module that performs 2-sho","explanation":"## Why This Is Asked\n\nAssesses capability to design edge-friendly 3D pose systems for dynamic outdoor scenes, handling wind-induced blur, occlusion, and rapid adaptation to new bolt geometries with minimal on-device training.\n\n## Key Concepts\n\n- Edge latency and memory constraints on embedded GPUs\n- Monocular 3D pose estimation (6-DoF) from moving cameras\n- Few-shot on-device adaptation (2-shot)\n- Synthetic-to-real transfer with domain randomization\n- Robustness to motion blur and lighting variation\n\n## Code Example\n\n```javascript\nfunction fineTuneAdapter(model, supportImages, supportLabels) {\n  // tiny adapter fine-tune on-device\n  for (let i = 0; i < 5; i++) {\n    const batch = getBatch(supportImages, supportLabels);\n    const loss = model.computeLoss(batch);\n    model.backward(loss);\n  }\n  return model;\n}\n```\n\n## Follow-up Questions\n\n- How would you measure generalization to new turbine models or bolt types?\n- What guardrails prevent false positives in flight-critical decisions?","diagram":"flowchart TD\n  A[Drone] --> B[Monocular Camera]\n  B --> C[Detector]\n  C --> D[6-DoF Pose Head]\n  D --> E[On-device Adaptation Module]","difficulty":"advanced","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","Slack","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T20:58:46.605Z","createdAt":"2026-01-26T20:58:46.605Z"},{"id":"q-7925","question":"Design an edge CV system for a warehouse robot with a single RGB camera to detect pallets, estimate 6‑DoF pose, and emit a 128‑D embedding. Store embedding, pose, and metadata in MongoDB Atlas Vector Search; downstream Oracle ERP consumes via a CDC‑like pipeline. Pallet designs evolve weekly; enable 5–10 shot on‑device adaptation. Outline architecture, data model, adaptation workflow, and metrics?","answer":"Implement a lightweight on-device detector (EfficientDet-lite or MobileViT) to locate pallets and estimate 6-DoF pose using Perspective-n-Point (PnP) with known pallet geometry. Extract a 128-D embedding, then store the embedding, pose, and metadata in MongoDB Atlas Vector Search. Implement a Change Data Capture (CDC)-like pipeline to synchronize data to the downstream Oracle ERP system. Enable 5-10 shot on-device adaptation for new pallet designs through fine-tuning or few-shot learning techniques.","explanation":"## Why This Is Asked\n\nThis question tests real-world edge computer vision design with cross-system dataflow: vector search in MongoDB and ERP integration in Oracle, plus hardware constraints and weekly design updates.\n\n## Key Concepts\n\n- Edge inference under strict latency constraints\n- 6-DoF pose estimation with known geometry using PnP\n- Embedding management and MongoDB Atlas Vector Search\n- CDC-style synchronization to Oracle ERP\n- On-device 5-10 shot adaptation for evolving pallet designs\n\n## Code Example\n\n```javascript\n// PnP pose estimation sketch (OpenCV.js)\nfunction estimatePose(imagePoints, objectPoints, cameraMatrix, distCoeffs) {\n  // SolvePnP returns rotation and translation vectors\n  const success = cv.solvePnP(\n    objectPoints,    // 3D points in object coordinate system\n    imagePoints,     // 2D points in image coordinate system\n    cameraMatrix,    // Camera intrinsic parameters\n    distCoeffs,      // Distortion coefficients\n    rvec,            // Output rotation vector\n    tvec             // Output translation vector\n  );\n  \n  if (success) {\n    // Convert rotation vector to rotation matrix\n    const R = cv.Rodrigues(rvec);\n    return { rotation: R, translation: tvec };\n  }\n  return null;\n}\n```","diagram":"flowchart TD\n  A[Camera] --> B[Edge Inference]\n  B --> C[Embedding + Pose]\n  C --> D[Mongo Atlas Vector Search]\n  C --> E[Local Cache]\n  D --> F[Oracle ERP via CDC]\n  E --> F\n","difficulty":"intermediate","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T06:02:37.719Z","createdAt":"2026-01-26T23:41:28.418Z"},{"id":"q-7991","question":"In a **Zoom-like video call**, design a beginner CV pipeline to segment a person from the background using a fixed RGB webcam, applying privacy-preserving blur or replacement. Use lightweight cues (skin color, motion, edges) plus a tiny classifier trained on **5–10 examples** to refine the mask. On-device latency **<60 ms/frame**. Describe architecture, data strategy, losses, adaptation workflow, and evaluation metrics?","answer":"Propose a lightweight segmentation head (MobileNetV2 backbone, small decoder) producing a person mask, refined with an edge-aware loss and temporal smoothing. Use a simple skin-color prior and motion ","explanation":"## Why This Is Asked\n\nPrivacy-preserving background segmentation for video calls is a real, beginner-friendly CV task that balances simple cues with a practical deployment constraint.\n\n## Key Concepts\n\n- Lightweight segmentation and edge-aware losses\n- On-device few-shot adaptation and linear probing\n- Temporal smoothing and privacy-focused evaluation\n\n## Code Example\n\n```javascript\n// Pseudo-structure\nfunction buildModel(){ /* MobileNetV2 + small decoder */ }\n```\n\n## Follow-up Questions\n\n- How would you extend to multiple people?\n- How would you handle varying lighting and backgrounds?\n","diagram":"flowchart TD\n  Input[Frame] --> Priors[SkinColor+Motion Priors]\n  Priors --> Mask[Mask Head]\n  Mask --> Ref[Refine with Edge+Temporal]\n  Ref --> Output[Blur/Replace Background]\n","difficulty":"beginner","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["NVIDIA","Salesforce","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T04:28:24.865Z","createdAt":"2026-01-27T04:28:24.865Z"},{"id":"q-8080","question":"Design a real-time monocular CV system for on-device streaming analysis: from a single RGB stream, jointly perform instance segmentation of people and vehicles, estimate their 3D pose relative to the camera, and predict per-frame motion blur to guide bitrate allocation for adaptive streaming. New content styles appear weekly; support 5-shot on-device adaptation with latency under 40 ms per frame on a mobile GPU. Describe architecture, data strategy, losses, adaptation workflow, and evaluation metrics?","answer":"Ship a single-pass multi-task network with a shared backbone (EfficientNet-Lite + FPN) and three heads: instance segmentation, 3D pose per instance, and per-frame motion-blur prediction. Train with sy","explanation":"## Why This Is Asked\nTests a production-minded, latency-constrained, multi-task CV design for streaming platforms and edge devices, emphasizing on-device adaptation to new content styles.\n\n## Key Concepts\n- Multi-task learning with a shared backbone\n- Lightweight heads for segmentation, 3D pose, and blur estimation\n- On-device few-shot adaptation (5-shot)\n- Latency budgeting and profiling\n- Evaluation combining segmentation, pose similarity, and blur accuracy\n\n## Code Example\n```javascript\n// Skeleton outline for multi-task head sharing backbone\nclass MultiTaskModel {\n  constructor(backbone) { /* ... */ }\n  forward(x) { /* features -> seg, pose, blur */ }\n}\n```\n\n## Follow-up Questions\n- How would you validate 5-shot adaptation in a live stream?\n- What data augmentations help generalize to new content styles?\n","diagram":"flowchart TD\n A[Input RGB] --> B[Backbone + FPN]\n B --> C[SegHead]\n B --> D[PoseHead]\n B --> E[BlurHead]","difficulty":"advanced","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Netflix","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T08:54:32.743Z","createdAt":"2026-01-27T08:54:32.743Z"},{"id":"q-8182","question":"Design an on-device, real-time hazard cue detector for a car-mounted monocular camera that outputs a calibrated probability of imminent danger from non-typical cues (e.g., pedestrians signaling, brake-light patterns, cyclist hand signals) in urban driving. The system must adapt to new cues with 5-shot on-device learning and run under 70 ms per frame on an embedded GPU. Describe architecture, data strategy, losses, adaptation workflow, and evaluation metrics?","answer":"Two-tower on-device design: a fast backbone (MobileNetV3/RepVGG-lite) with a compact cue head that outputs a hazard probability plus an uncertainty estimate. Data: real urban clips supplemented by syn","explanation":"## Why This Is Asked\n\nThis question probes real-world on-device CV, calibrated uncertainty, and few-shot adaptation for safety-critical cues in autonomous driving.\n\n## Key Concepts\n\n- On-device real-time inference\n- Calibrated uncertainty (NLL, ensembles, or temperature scaling)\n- 5-shot on-device adaptation with adapters\n- Domain-randomized synthetic data + real data\n- Evaluation under distribution shift and latency constraints\n\n## Code Example\n\n```javascript\n// pseudocode outline\nclass HazardModel { /* ... */ }\n```\n\n## Follow-up Questions\n\n- How would you measure calibration under drift? \n- How would you defend against adversarial cues? ","diagram":"flowchart TD\n  A[Input frame] --> B[Backbone]\n  B --> C[CueHead: hazard prob + uncertainty]\n  C --> D{Decision: trigger warning?}\n  D -->|Yes| E[Alert/Stop]\n  D -->|No| F[Continue]\n  F --> G[5-shot Adaptation Module]\n  G --> B","difficulty":"advanced","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Netflix","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T13:31:04.266Z","createdAt":"2026-01-27T13:31:04.266Z"},{"id":"q-8319","question":"Design an edge CV system to monitor a wall-mounted financial dashboard: using a single RGB camera, detect and localize chart regions on the display, classify chart type (bar, line, pie), and reconstruct an approximate data curve for the main series. New layouts appear weekly; support 3-shot on-device adaptation with latency under 60 ms per frame. Describe architecture, data strategy (synthetic+real), losses, adaptation workflow, and evaluation metrics?","answer":"Propose a two-branch edge model: a fast detector (lightweight CNN) for chart regions and a lean chart-serializer (tiny transformer) to classify type and reconstruct curves. Use synthetic chart renderi","explanation":"## Why This Is Asked\nThis task blends practical CV at the edge with domain adaptation for evolving visuals, mirroring real-world finance displays.\n\n## Key Concepts\n- Edge latency and model design for single RGB input\n- Few-shot/continual adaptation (3-shot)\n- Multi-task losses: detection, classification, regression\n- Synthetic data generation and real data finetuning\n- Evaluation: region IoU, type accuracy, curve RMSE, per-frame latency\n\n## Code Example\n```javascript\nfunction totalLoss(pred, target){\n  return bboxLoss(pred.bbox, target.bbox) + typeLoss(pred.type, target.type) + curveLoss(pred.curve, target.curve);\n}\n```\n\n## Follow-up Questions\n- How would you handle layout drift over longer periods?\n- What ablation studies would you run to justify the architecture choices?","diagram":"flowchart TD\n  A[Input RGB frame] --> B[Chart region detector]\n  B --> C[Chart type classifier]\n  B --> D[Curve estimator]\n  C --> E[Output: type, region]\n  D --> E\n  E --> F[Display-ready data]","difficulty":"intermediate","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T19:43:08.367Z","createdAt":"2026-01-27T19:43:08.368Z"},{"id":"q-8383","question":"You have a fixed RGB webcam overlooking a cluttered kitchen counter. Design a beginner-friendly CV pipeline to detect and count distinct dishware (plates, bowls, mugs) and estimate their 3D pose relative to the counter plane from a single image. New dish types appear weekly; support 5-shot on-device adaptation with under 60 ms per frame. Describe architecture, data strategy, losses, adaptation workflow, and evaluation metrics?","answer":"Implement a lightweight detector (Tiny-YOLO-lite) to localize dishware; estimate 3D pose using planar constraints with the counter plane via homography and PnP in a single view. For new dish types, enable 5-shot on-device adaptation through meta-learning with prototypical networks, achieving rapid fine-tuning under 60ms per frame via model quantization and hardware acceleration.","explanation":"## Why This Is Asked\nThis tests designing an on-device, low-latency computer vision pipeline that leverages planar priors and supports few-shot adaptation for emerging classes.\n\n## Key Concepts\n- Single-view 3D pose estimation with plane priors\n- Lightweight detection architectures and on-device fine-tuning\n- Synthetic and real data strategies, loss function design, latency constraints\n\n## Code Example\n```javascript\n// Architecture implementation placeholder\n```\n\n## Follow-up Questions\n- How would you handle occlusions and cluttered kitchen counters?\n- What ablation studies demonstrate the importance of the planar prior?","diagram":null,"difficulty":"beginner","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","OpenAI","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-28T05:12:41.542Z","createdAt":"2026-01-27T22:30:34.559Z"},{"id":"q-274","question":"How would you implement a hybrid CNN architecture combining ResNet residual connections with EfficientNet compound scaling for production image classification?","answer":"Implement a hybrid CNN by integrating ResNet residual connections with EfficientNet's compound scaling methodology. This involves using ResNet blocks enhanced with channel attention mechanisms, applying EfficientNet's compound scaling formula φ = α^β · γ^φ to balance depth, width, and resolution, and optimizing the model with mixed precision training for production deployment.","explanation":"## Concept\nA hybrid CNN architecture combines ResNet's residual connections (identity shortcuts) with EfficientNet's compound scaling approach, which systematically balances network depth, width, and resolution using a fixed compound coefficient φ.\n\n## Implementation\n```python\n# Hybrid block with residual + efficient scaling\nclass HybridBlock(nn.Module):\n    def __init__(self, in_ch, out_ch, stride=1):\n        super().__init__()\n        # EfficientNet MBConv with residual\n        self.mbconv = MBConv(in_ch, out_ch, stride)\n        self.shortcut = nn.Identity() if stride == 1 else nn.Conv2d(in_ch, out_ch, 1)\n        \n    def forward(self, x):\n        return self.mbconv(x) + self.shortcut(x)\n```\n\n## Production Optimization\n- Mixed precision training for memory efficiency\n- Channel attention for improved feature representation\n- Compound scaling ensures optimal resource utilization","diagram":"flowchart TD\n    A[Input Image] --> B[Stem Conv 3x3]\n    B --> C[Hybrid Block 1: MBConv + Residual]\n    C --> D[Hybrid Block 2: MBConv + Residual]\n    D --> E[SE Block: Channel Attention]\n    E --> F[Hybrid Block 3: Downsample]\n    F --> G[Global Average Pool]\n    G --> H[Fully Connected]\n    H --> I[Softmax]\n    \n    C --> C1[Identity Shortcut]\n    C1 --> C2[Add]\n    D --> C2\n    E --> C2","difficulty":"intermediate","tags":["cnn","resnet","efficientnet"],"channel":"computer-vision","subChannel":"image-classification","sourceUrl":"https://arxiv.org/abs/1905.11946","videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google","Meta","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-08T11:22:10.106Z","createdAt":"2025-12-26 12:51:07"},{"id":"q-253","question":"How does YOLO implement real-time object detection using grid-based prediction and what are the key components of its architecture?","answer":"YOLO divides image into S×S grid, each cell predicts B bounding boxes with confidence scores, class probabilities, and uses anchor boxes for better localization.","explanation":"## Interview Context\nThis question assesses understanding of modern object detection architectures and trade-offs between speed and accuracy. YOLO's unified approach is fundamental for real-time applications.\n\n## Technical Details\n\n### Core Architecture\n- **Backbone**: Darknet feature extractor (Darknet-53 for YOLOv3)\n- **Grid System**: S×S grid where each cell predicts B bounding boxes\n- **Output Tensor**: [S, S, B*(5+C)] where 5 = (x, y, w, h, confidence), C = class count\n\n### Key Components\n- **Anchor Boxes**: Pre-defined aspect ratios to improve bounding box prediction\n- **IoU Calculation**: Intersection over Union for confidence scoring\n- **Non-Maximum Suppression**: Removes redundant detections above IoU threshold\n- **Multi-Scale Predictions**: Feature maps at different scales for various object sizes\n\n### Loss Function\n```\nLoss = λ_coord * MSE(bbox) + λ_noobj * MSE(confidence) + MSE(class)\n```\n\n### Training Process\n- Single-stage training end-to-end\n- Uses mean squared error for bounding box regression\n- Cross-entropy for classification\n\n## Performance Comparison\n- **YOLOv1**: 45 FPS on Titan X (63.4% mAP)\n- **Faster R-CNN**: 7 FPS (73.2% mAP)\n- **YOLOv3**: 30 FPS (57.9% mAP)\n\n## Follow-up Questions\n1. How does YOLOv5 improve upon YOLOv3's architecture and training methodology?\n2. What are the trade-offs between YOLO and two-stage detectors like Faster R-CNN?\n3. How would you optimize YOLO for edge devices with limited computational resources?","diagram":"graph TD\n    A[Input Image 448×448] --> B[Backbone Network DarkNet-53]\n    B --> C[Feature Maps]\n    C --> D[Detection Head]\n    D --> E[Grid S×S]\n    E --> F[Each Cell Predicts]\n    F --> G[B Bounding Boxes]\n    F --> H[Confidence Scores]\n    F --> I[Class Probabilities]\n    G --> J[Non-Max Suppression]\n    H --> J\n    I --> J\n    J --> K[Final Detections]","difficulty":"beginner","tags":["yolo","rcnn","detr"],"channel":"computer-vision","subChannel":"object-detection","sourceUrl":null,"videos":null,"companies":["Amazon","Google","Meta","Microsoft","NVIDIA","Tesla"],"eli5":"Imagine you have a big chocolate chip cookie and you want to find all the chocolate chips really fast! You cut the cookie into tiny squares, like a checkerboard. Each little square has a special job - it looks for chocolate chips that might be hiding inside it. Each square also draws a box around any chips it finds and says how sure it is that it's really a chip. Some squares might think they see a chip when it's just a crumb, so they have to be very confident! The computer learns to do this super quickly by practicing with lots of cookies, so it can find all the chocolate chips in one quick look instead of searching slowly all over the cookie.","relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-26T16:38:46.236Z","createdAt":"2025-12-26 12:51:07"},{"id":"q-200","question":"How does U-Net's skip connection architecture enable precise medical image segmentation?","answer":"U-Net employs a contracting encoder to extract contextual features and an expanding decoder with skip connections that preserve spatial details, enabling precise pixel-wise segmentation.","explanation":"## Why Asked\nTests understanding of advanced CNN architectures for medical imaging and computer vision segmentation tasks.\n\n## Key Concepts\n- Encoder-decoder architecture\n- Skip connections for feature preservation\n- Contracting and expanding paths\n- Pixel-wise segmentation\n\n## Code Example\n```\ndef unet_block(x, skip):\n    x = Conv2D(64, 3, padding='same')(x)\n    x = concatenate([x, skip])\n    return x\n```\n\n## Follow-up Questions\n- How does U-Net handle class imbalance?\n- What are alternatives to skip connections?\n- How does it compare to FCN?","diagram":"flowchart TD\n  A[Input Image] --> B[Encoder Path]\n  B --> B1[Bottleneck]\n  B1 --> C[Decoder Path]\n  C --> D[Segmentation Output]\n  B -.->|Skip Connections| C","difficulty":"beginner","tags":["unet","mask-rcnn","sam"],"channel":"computer-vision","subChannel":"segmentation","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":"https://www.youtube.com/watch?v=HS3Q_90hnDg"},"companies":["Amazon","Google","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":["u-net","skip connections","encoder","decoder","spatial details","segmentation"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2025-12-30T01:46:54.144Z","createdAt":"2025-12-26 12:51:06"},{"id":"q-228","question":"How would you optimize a real-time medical image segmentation pipeline using SAM with 100ms latency constraint on edge devices?","answer":"Use SAM's lightweight encoder with quantized ViT-B, implement prompt caching, and apply tensorRT optimization for sub-100ms inference.","explanation":"## Concept Overview\nReal-time medical segmentation requires balancing accuracy with strict latency constraints. SAM (Segment Anything Model) provides zero-shot segmentation but needs optimization for edge deployment.\n\n## Implementation Details\n- **Model Optimization**: Use SAM-ViT-B (lightweight) with INT8 quantization\n- **Prompt Engineering**: Implement prompt caching for similar anatomical regions\n- **Hardware Acceleration**: Deploy with TensorRT on NVIDIA Jetson or CoreML on Apple Silicon\n- **Batch Processing**: Process multiple slices in parallel when available\n\n## Code Example\n```python\n# Optimized SAM inference pipeline\nimport torch\nfrom segment_anything import sam_model_registry\n\nclass OptimizedSAM:\n    def __init__(self):\n        self.sam = sam_model_registry['vit_b'](checkpoint='sam_vit_b.pth')\n        self.sam.eval()\n        self.sam.cuda()\n        # Enable TensorRT optimization\n        self.sam = torch.compile(self.sam, mode='max-autotune')\n    \n    def segment_with_cache(self, image, prompt):\n        # Check prompt cache first\n        cache_key = hash(prompt.tobytes())\n        if cache_key in self.prompt_cache:\n            return self.prompt_cache[cache_key]\n        \n        masks = self.sam.predict(image, prompt)\n        self.prompt_cache[cache_key] = masks\n        return masks\n```\n\n## Common Pitfalls\n- **Memory Overhead**: Prompt caching can consume significant memory on edge devices\n- **Quantization Loss**: INT8 quantization may reduce fine-grained segmentation accuracy\n- **Prompt Sensitivity**: Medical images require precise prompt placement for accurate results\n- **Hardware Variability**: Different edge devices have varying compute capabilities","diagram":"graph TD[Input Medical Image] --> A[Preprocessing: Resize/Normalize]\nA --> B[Prompt Detection: Anatomical Region]\nB --> C{Prompt Cache Hit?}\nC -->|Yes| D[Return Cached Mask]\nC -->|No| E[SAM Encoder: ViT-B Lightweight]\nE --> F[Prompt-Guided Decoder]\nF --> G[Post-processing: Refine Boundaries]\nG --> H[Cache Result]\nH --> I[Output Segmentation Mask]\nD --> I\n\nsubgraph Edge Device Optimization\n    J[TensorRT Engine] --> K[INT8 Quantization]\n    K --> L[Memory Pool Management]\nend\n\nE -.-> J\nF -.-> J","difficulty":"advanced","tags":["unet","mask-rcnn","sam"],"channel":"computer-vision","subChannel":"segmentation","sourceUrl":null,"videos":null,"companies":["Apple","Google","Meta","Microsoft","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-26T12:40:51.586Z","createdAt":"2025-12-26 12:51:07"}],"subChannels":["general","image-classification","object-detection","segmentation"],"companies":["Adobe","Airbnb","Amazon","Anthropic","Apple","Bloomberg","Citadel","Cloudflare","Coinbase","Databricks","Discord","DoorDash","Goldman Sachs","Google","Hashicorp","Hugging Face","IBM","Instacart","LinkedIn","Lyft","Meta","Microsoft","MongoDB","NVIDIA","Netflix","OpenAI","Oracle","PayPal","Plaid","Robinhood","Salesforce","Scale Ai","Slack","Snap","Snowflake","Square","Tesla","Twitter","Two Sigma","Uber","Zoom"],"stats":{"total":85,"beginner":23,"intermediate":23,"advanced":39,"newThisWeek":38}}