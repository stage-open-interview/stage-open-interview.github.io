{"questions":[{"id":"q-1018","question":"You’re building a mobile camera app that auto-captures frames from a live video feed. Describe a practical, beginner-friendly pipeline to decide whether a frame is usable by applying two simple checks: (1) sharpness via variance of Laplacian, (2) exposure via histogram-based brightness. Explain how thresholds would be chosen, how you'd adapt them across lighting, and provide a minimal code snippet illustrating the core checks?","answer":"Compute sharpness as the variance of the Laplacian on grayscale and assess exposure with 2nd and 98th percentile brightness. Require low>=20 and high<=235, and sharpness>100. Adapt thresholds with a 3","explanation":"## Why This Is Asked\n\nTests ability to design a lightweight, production-friendly CV heuristic using simple, well-known metrics, and to reason about robustness to lighting without heavy models.\n\n## Key Concepts\n\n- Variance of Laplacian as a sharpness proxy\n- Histogram-based exposure sensing (2nd/98th percentile)\n- Thresholding plus simple adaptation for devices\n\n- Trade-offs between false positives/negatives\n\n## Code Example\n\n```python\nimport cv2, numpy as np\ndef frame_ok(frame, sharp_th=100.0, low_thr=20, high_thr=235):\n    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n    sharp = cv2.Laplacian(gray, cv2.CV_64F).var()\n    lo, hi = np.percentile(gray, (2, 98))\n    exp_ok = (lo >= low_thr) and (hi <= high_thr)\n    return (sharp > sharp_th) and exp_ok\n```\n\n## Follow-up Questions\n\n- How would you calibrate thresholds across cameras?\n- How would you extend to color balance and motion blur?\n","diagram":"flowchart TD\nFrame[Frame In] --> Sharp[Compute Laplacian Variance]\nSharp --> Decide1{Sharpness > 100?}\nDecide1 -- Yes --> Exp[Compute Exposure]\nExp --> Decide2{Exposure OK?}\nDecide2 -- Yes --> Accept[Accept Frame]\nDecide2 -- No --> Reject[Reject Frame]\nDecide1 -- No --> Reject","difficulty":"beginner","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","LinkedIn","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T19:32:04.521Z","createdAt":"2026-01-12T19:32:04.521Z"},{"id":"q-1093","question":"Design a privacy-preserving, real-time hand-gesture recognition system for video calls on consumer laptops (720p camera) that distinguishes a small set of gestures (hand-raise, thumbs-up, peace) without exposing facial details. Must run on-device at 30–60 FPS, handle lighting/occlusion, and support federated fine-tuning with differential privacy. Outline architecture, data strategy, and evaluation plan?","answer":"Two-stage on-device design: a lightweight hand ROI extractor (tiny CNN such as MobileNetV3) plus a temporal classifier (1D conv or efficient Transformer) operating on a short frame window. Train with ","explanation":"## Why This Is Asked\nThis question probes practical on-device CV with privacy constraints, latency pressure, and data privacy.\n\n## Key Concepts\n- Lightweight models, ROI extraction, temporal reasoning\n- Quantization, pruning, on-device inference\n- Federated learning, differential privacy, privacy budgets\n- Evaluation: latency, memory, F1/precision-recall under occlusion\n\n## Code Example\n```javascript\n// Pseudocode: frame window prep for on-device gesture model\nfunction prepareWindow(frames) {\n  const ROI = detectHandROI(frames[0]);\n  const window = frames.slice(-16).map(f => crop(f, ROI));\n  return stack(window);\n}\n```\n\n## Follow-up Questions\n- How would you measure and mitigate drift when lighting changes across devices?\n- How would you handle new gestures without retraining all devices?","diagram":null,"difficulty":"intermediate","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","Tesla","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T22:25:53.986Z","createdAt":"2026-01-12T22:25:53.986Z"},{"id":"q-1108","question":"Design a real-time, on-device hand pose/gesture system for air-drawing in a video-conferencing app. From a monocular 1080p60 stream, infer 2D/3D hand pose with <40ms latency on a CPU, robust to occlusion and varied skin tones, and support at least 5 gestures (draw, erase, next, previous, pointer). Outline data needs, model architecture, latency optimizations, temporal consistency, and evaluation plan?","answer":"Proposed approach: deploy a lightweight 2D keypoint detector plus a compact 3D lifting head on-device, with a short temporal filter (Kalman/temporal conv). Use INT8 quantization and ONNX Runtime for C","explanation":"## Why This Is Asked\nThis question probes on-device real-time hand pose recognition, latency budgeting, robustness to occlusion and skin tone, and interface with gesture-based controls for conferencing.\n\n## Key Concepts\n- On-device real-time inference\n- 2D keypoint + 3D lifting\n- Temporal smoothing for stability\n- Occlusion and bias robustness\n- Quantization and runtime deployment\n\n## Code Example\n```javascript\nfunction inferHandPose(frame) {\n  const kps2d = detector2D(frame);\n  const pose3D = liftTo3D(kps2d);\n  return smoothPose(pose3D);\n}\n```\n\n## Follow-up Questions\n- How would you handle multiple hands and long occlusions?\n- How would you validate latency and diagnose spikes in production?\n","diagram":"flowchart TD\nA[Capture 1080p60 frame] --> B[2D hand-keypoint detector]\nB --> C[3D pose lifting]\nC --> D[Temporal smoothing]\nD --> E[Gesture classifier]\nE --> F[Action: air-draw / next / prev]","difficulty":"advanced","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Plaid","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T23:17:25.742Z","createdAt":"2026-01-12T23:17:25.742Z"},{"id":"q-1270","question":"Design a real-time monocular 3D detector for an assembly line that estimates 6-DoF pose of tools from a single RGB camera, achieving sub-50ms per frame on embedded hardware. Use a lightweight backbone with self-supervised pretraining plus a small labeled set; recover pose via differentiable PnP from 2D-3D correspondences with a temporal filter and reprojection losses. Monitor drift with streaming per-frame errors?","answer":"Propose a lightweight monocular 3D detector on edge hardware that predicts 2D-3D keypoint correspondences and a scale per tool, then solves 6-DoF via differentiable PnP. Use self-supervised pretrainin","explanation":"## Why This Is Asked\nIndustrial CV on the edge demands real-time 3D understanding from monocular inputs with limited labels. The design must handle latency, occlusion, and drift in long shifts.\n\n## Key Concepts\n- Monocular 3D pose estimation\n- Differentiable PnP\n- Self-supervised learning\n- Temporal filtering\n- Embedded/edge deployment\n\n## Code Example\n```python\n# Simple differentiable PnP pose recovery sketch\nimport torch\n\ndef pose_from_keypoints(pts2d, pts3d, K):\n    # Placeholder for differentiable PnP optimization\n    # Return rotation R and translation t\n    R = torch.eye(3)\n    t = torch.zeros(3)\n    return R, t\n```\n\n## Follow-up Questions\n- How to robustly handle occlusions and outliers in keypoint matches?\n- What evaluation protocol would reliably detect drift over 24h? ","diagram":null,"difficulty":"intermediate","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Hashicorp","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T07:35:09.692Z","createdAt":"2026-01-13T07:35:09.692Z"},{"id":"q-1417","question":"Given a single RGB camera mounted on an autonomous delivery drone operating in urban environments, design a real-time system to detect and track pedestrians and other vulnerable actors at 30 fps under nighttime and rain conditions. Propose data strategy (synthetic rain + real), model backbone, temporal fusion and latency targets, and safety/failover mechanisms?","answer":"Use a lightweight detector such as EfficientDet-D3 with a Kalman-filter-based tracker and a short-term memory for temporal fusion; train on rain-augmented synthetic data plus a small labeled real-worl","explanation":"## Why This Is Asked\nDesigning perception for aerial platforms under adverse weather tests data strategy, real-time constraints, and safety guarantees in production-like settings.\n\n## Key Concepts\n- Single-view detection under rain/night; robustness to weather\n- Temporal fusion and multi-object tracking in a streaming context\n- Domain adaptation: synthetic rain + real rainy data\n- Edge latency budgets and model backbones\n- Safety/failover and occlusion handling\n\n## Code Example\n```python\n# Pseudo-code: simple Kalman-based track update\nclass Track:\n    def update(self, measurement):\n        self.state = self.kf.update(self.state, measurement)\n```\n\n## Follow-up Questions\n- How would you validate performance across rain intensities and lighting?\n- How would you quantify and mitigate false positives in crowded scenes?\n","diagram":"flowchart TD\n  A[Input: video stream] --> B[Preprocessing]\n  B --> C[Detection & Tracking]\n  C --> D[Temporal Fusion & Association]\n  D --> E[Output: Trajectories & Alerts]","difficulty":"advanced","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Cloudflare","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T15:56:01.086Z","createdAt":"2026-01-13T15:56:01.086Z"},{"id":"q-1435","question":"For aerial inspection of solar farms, design a CV system to detect and grade micro-cracks on solar panels from drone video with limited labeled data. Specify an architecture that detects tiny defects, data-augmentation strategies (synthetic crack overlays, texture randomization), domain adaptation, and an edge-friendly output (box, segmentation mask, and severity score). Include evaluation protocol and latency targets?","answer":"I’d use a two-stage detector with a high-res backbone (EfficientNetV2-S + FPN) and a light segmentation head. Train with synthetic crack overlays on panel textures plus self-supervised pretraining on ","explanation":"## Why This Is Asked\nTests ability to design CV solutions under data scarcity, with tiny defect detection, multi-task outputs, and edge deployment constraints.\n\n## Key Concepts\n- Tiny defect detection in high-res imagery\n- Synthetic data and SSL for labels-scarce regimes\n- Domain adaptation and edge-model optimization\n- Multi-task outputs: bbox, mask, severity\n\n## Code Example\n```python\n# Pseudo training loop outline\nfor batch in dataloader:\n  imgs, boxes, masks, sev = batch\n  feats = backbone(imgs)\n  pred_boxes, pred_masks = heads(feats)\n  loss = focal_loss(pred_boxes, boxes) + bce_loss(pred_masks, masks) + mse_loss(sev, severity)\n  loss.backward()\n  opt.step()\n```\n\n## Follow-up Questions\n- How would you quantify model calibration for severity predictions?\n- What ablation would you run to isolate the impact of synthetic data?","diagram":"flowchart TD\n  A[Drone Frame] --> B[Preprocess] \n  B --> C[SSL Pretraining] \n  C --> D[Detector (backbone + heads)] \n  D --> E[Postprocess: NMS + severity] \n  E --> F[Edge Deployment]","difficulty":"advanced","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","MongoDB","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T16:56:34.735Z","createdAt":"2026-01-13T16:56:34.735Z"},{"id":"q-1861","question":"You're given an overhead RGB image of a table with scattered coins. Design a beginner-friendly pipeline to count the number of coins and estimate their approximate denomination from a single image. Include preprocessing, circle/contour detection, radius-based grouping to separate coin types, handling shadows, and a minimal code sketch using OpenCV to detect circular shapes?","answer":"Preprocess: grayscale, CLAHE, and blur to reduce noise. Use HoughCircles (dp=1.2, minDist=20, minRadius=10, maxRadius=60) to detect coin-like circles. Merge duplicates by proximity, bucket by radius i","explanation":"## Why This Is Asked\nTests ability to design a practical CV pipeline using lightweight, non-ML methods for a common task.\n\n## Key Concepts\n- Circle detection with Hough transform\n- Radius-based grouping for coin types\n- Noise/shadow handling with blur and histogram equalization\n- Validation with synthetic data\n\n## Code Example\n```javascript\n// OpenCV.js-like pseudocode\nlet src = cv.imread('table.jpg');\ncv.cvtColor(src, src, cv.COLOR_RGBA2GRAY);\ncv.equalizeHist(src, src);\ncv.GaussianBlur(src, new cv.Size(9,9), 2);\nlet circles = new cv.Mat();\ncv.HoughCircles(src, circles, cv.HOUGH_GRADIENT, 1.2, 20, 50, 30, 10, 60);\n```\n\n## Follow-up Questions\n- How would you address perspective distortions?\n- How would you validate robustness with lighting changes?\n","diagram":"flowchart TD\n  A[Input image] --> B[Preprocess]\n  B --> C[Circle detection]\n  C --> D[Filter duplicates]\n  D --> E[Group by radius]\n  E --> F[Count & report]","difficulty":"beginner","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Instacart","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T14:48:10.414Z","createdAt":"2026-01-14T14:48:10.414Z"},{"id":"q-1945","question":"Design a real-time anomaly-detection pipeline for a single RGB camera monitoring an industrial warehouse. Propose a memory-augmented autoencoder approach that runs on an edge device at 15–20 FPS, uses frame-wise reconstruction error plus optical-flow residuals, maintains a fixed-size normal-pattern memory, and includes a drift-adaptation strategy with minimal labeling?","answer":"Implement a memory-augmented autoencoder (MAE) with a lightweight encoder-decoder and a fixed memory bank of normal latent prototypes. For each frame, retrieve the nearest memory vectors, fuse them wi","explanation":"## Why This Is Asked\\n\\nTests practical CV design under edge constraints, handling concept drift with minimal labels, and fusing appearance with motion cues.\\n\\n## Key Concepts\\n- Memory-augmented autoencoder for normal-pattern modeling\\n- Fixed-size memory bank and online updates\\n- Frame reconstruction error + optical-flow residuals for anomaly scoring\\n- Edge deployment with INT8/quantized backbones\\n\\n## Code Example\\n```javascript\\n// Skeleton: MAE forward pass and anomaly scoring\\nfunction MAEForward(frame, model, memory) {\\n  // encode, memory lookup, decode, compute error\\n}\\n```\\n\\n## Follow-up Questions\\n- How would you quantify drift and decide when to refresh memory?\\n- How would you evaluate latency vs. accuracy trade-offs on Jetson?","diagram":null,"difficulty":"intermediate","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","Hugging Face"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T17:58:29.262Z","createdAt":"2026-01-14T17:58:29.262Z"},{"id":"q-2131","question":"In a factory setting, design a real-time 2-view RGB tool-pose tracking system that achieves sub-60ms per frame on edge hardware. Use two synchronized cameras, a lightweight backbone, and a differentiable PnP with 2D-3D correspondences; include cross-view fusion, a temporal filter, and a self-supervised pretraining strategy with synthetic data. How would you validate drift and occlusion resilience?","answer":"Propose a dual-RGB edge pipeline: detect 2D keypoints on each view with a small CNN (e.g., MobileNetV3) and fuse via a learned cross-view encoder; compute 6-DoF tool pose with differentiable PnP + rob","explanation":"## Why This Is Asked\nTests multi-view real-time pose estimation under strict latency, with practical constraints on edge hardware, including cross-view fusion, differentiable PnP, and self-supervised learning.\n\n## Key Concepts\n- multi-view fusion\n- differentiable PnP and RANSAC\n- edge latency optimization\n- self-supervised pretraining with synthetic data\n- drift and occlusion resilience\n\n## Code Example\n```javascript\n// Skeleton: data flow for two views\nfunction processFrame(view1, view2, intrinsics, extrinsics){\n  // 1) detect 2D keypoints in both views\n  // 2) establish cross-view correspondences\n  // 3) triangulate to 3D points\n  // 4) solve PnP for 6-DoF pose\n  // 5) apply temporal filter for stability\n}\n```\n\n## Follow-up Questions\n- How would you evaluate drift over long streaming sequences?\n- How would you handle calibration drift or a camera failure during inference?","diagram":"flowchart TD\n  A[Two RGB cameras] --> B[2D keypoint detectors]\n  B --> C[Cross-view fusion encoder]\n  C --> D[Triangulation to 3D]\n  D --> E[Differentiable PnP pose]\n  E --> F[Temporal fusion (Kalman)]","difficulty":"advanced","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","OpenAI","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T04:14:43.759Z","createdAt":"2026-01-15T04:14:43.759Z"},{"id":"q-2242","question":"In a warehouse setting, design an edge-friendly CV pipeline that counts and localizes pallets from a single moving RGB camera mounted on a forklift, using self-supervised depth cues from motion and a lightweight 3D detector to output per-pallet 3D bounding boxes with uncertainty at 30 Hz. Address occlusion, dynamic workers, and domain shift between day and night?","answer":"I'd deploy a lightweight 2D pallet detector (EfficientDet-D3) to generate proposals, estimate depth from ego-motion via structure-from-motion, and use a differentiable 3D box head for per-pallet boxes","explanation":"## Why This Is Asked\nAssesses end-to-end design for real-world inventory sensing under occlusion, speed, and lighting variation, emphasizing self-supervised depth, differentiable 3D reasoning, and uncertainty estimation on edge hardware.\n\n## Key Concepts\n- Lightweight 2D detector (EfficientDet)\n- Self-supervised depth from ego-motion\n- Differentiable 3D box regression\n- Uncertainty via MC dropout and aleatoric depth\n- Kalman tracking + data association\n- Edge deployment (latency, memory)\n\n## Code Example\n```javascript\n// Pseudocode\nfunction pipeline(frame, state){\n  proposals = detector(frame)\n  depth = depthFromMotion(frame, state)\n  boxes = reg3D(proposals, depth)\n  tracks = kalmanTrack(boxes, state)\n  return tracks\n}\n```\n\n## Follow-up Questions\n- How would you evaluate robustness to motion blur and occlusion?\n- How would you adapt the system to different pallet shapes/sizes or new warehouses without labeled data?","diagram":null,"difficulty":"advanced","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T09:00:25.390Z","createdAt":"2026-01-15T09:00:25.390Z"},{"id":"q-2333","question":"Design a beginner-friendly pipeline to detect and count red boxes moving on a conveyor using a single RGB camera in real time. Include HSV color space selection, dual-range red thresholds, noise removal with morphology, contour filtering by area/shape, and a simple line-crossing tracker. Provide a minimal OpenCV.js code snippet for core steps?","answer":"Use HSV segmentation for red with two ranges, apply Gaussian blur and morphology to reduce noise, detect contours, filter by area and aspect ratio to identify boxes, compute centroids, and count when ","explanation":"## Why This Is Asked\n\nThis task yields a concrete, end-to-end CV workflow suitable for entry-level projects that run in real time on modest hardware. It tests practical skills in color segmentation, noise handling, contour reasoning, and a simple, robust counting strategy.\n\n## Key Concepts\n\n- HSV color space and red wrap-around handling\n- Morphology for noise reduction\n- Contour area and aspect-ratio filtering\n- Centroid computation and line-crossing tracking\n- Lightweight calibration for varying lighting\n\n## Code Example\n\n```javascript\n// OpenCV.js core steps for red box detection\nfunction detectRedBoxes(src) {\n  // convert to HSV, threshold red in two ranges, combine masks\n  // blur and apply morphology\n  // find contours, filter by area/shape\n  // compute centroids and detect crossing against a line\n}\n```\n\n## Follow-up Questions\n\n- How would you handle occlusion or partial boxes?\n- How would you validate with limited labeled data and choose metrics?","diagram":null,"difficulty":"beginner","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Google","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T13:08:46.849Z","createdAt":"2026-01-15T13:08:46.849Z"},{"id":"q-2377","question":"Scenario: fixed overhead RGB camera watches a single shoebox on a shelf. Design a beginner-friendly, non-deep-learning pipeline to decide whether the box is upright within ±10 degrees using contour-based detection. Explain preprocessing, edge/shape heuristics, thresholds, and how to handle perspective distortion and occlusion. Include a minimal Python OpenCV snippet that outputs 'upright' or 'tilted'?","answer":"Convert to grayscale and blur (3x3). Run Canny to get edges, then find contours and keep the largest near-rectangular contour via minAreaRect and a 4-corner check. Normalize the angle to [-90,90], tak","explanation":"## Why This Is Asked\nTests building a robust, non-ML CV heuristic that handles real-world perspective and noise.\n\n## Key Concepts\n- Contour detection and minAreaRect for orientation\n- Edge extraction and thresholding for robustness\n- Angle normalization and aspect-ratio checks\n- Frame-consistency to reject jitter\n\n## Code Example\n```python\nimport cv2\nimport numpy as np\n\ndef upright_status(img):\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    blur = cv2.GaussianBlur(gray, (3,3), 0)\n    edges = cv2.Canny(blur, 50, 150)\n    cnts, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    if not cnts:\n        return 'tilted'\n    c = max(cnts, key=cv2.contourArea)\n    rect = cv2.minAreaRect(c)\n    angle = abs(rect[2])\n    if angle > 45:\n        angle = 90 - angle\n    w, h = rect[1]\n    aspect = w / h if h else 0\n    if angle <= 10 and 0.8 <= aspect <= 1.2:\n        return 'upright'\n    return 'tilted'\n```\n\n## Follow-up Questions\n- How would you scale this to multiple boxes in a scene? \n- How would you handle strong perspective distortion or occlusion while keeping it beginner-friendly?","diagram":"flowchart TD\n  A[Acquire image] --> B[Preprocess]\n  B --> C[Edge detect]\n  C --> D[Find contours]\n  D --> E[Select largest near-rect]{Is near-rect?}\n  E -->|Yes| F[MinAreaRect and angle]\n  F --> G[Compute upright status]\n  G --> H[Output Upright/Tilted]","difficulty":"beginner","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Oracle","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T15:42:44.654Z","createdAt":"2026-01-15T15:42:44.654Z"},{"id":"q-2546","question":"Design a real-time monocular hand pose and gesture recognition system for AR UI on a battery-constrained headset. Propose a lightweight architecture (e.g., two-branch network with 2D heatmaps and a temporal encoder), latency target <25 ms per frame on edge hardware, occlusion handling, and 3D hand pose estimation—describe data strategy, loss terms, and evaluation?","answer":"Two-branch lightweight model: Branch A generates 2D hand keypoint heatmaps using a MobileNetV3-like backbone; Branch B encodes short-term temporal context. 3D pose is recovered through a differentiable hand model with a PnP solver, achieving <25ms latency via model quantization and efficient temporal fusion. Occlusion handling incorporates visibility prediction and temporal smoothing, while training combines synthetic and real data with domain adaptation.","explanation":"## Why This Is Asked\nTests practical hand tracking for AR UI with tight latency, occlusion handling, and edge deployment, aligning with high-performance computer vision needs at tech leaders.\n\n## Key Concepts\n- Lightweight backbones for 2D heatmaps\n- Temporal encoders (GRU/Transformer) for stability\n- Differentiable hand model with PnP for 3D pose\n- Occlusion handling via visibility prediction and temporal smoothing\n- Synthetic + real data with domain adaptation; real-time evaluation metrics MPJPE, PCK3D\n\n## Code Example\n```python\n# Pseudocode: fuse 2D heatmaps with temporal features to estimate 3D hand pose\nheatmaps = branch_a(image)  # 2D keypoint detection\ntemporal_features = branch_b(heatmaps_sequence)  # temporal context\npose_3d = hand_model_solver(heatmaps, temporal_features)  # 3D pose recovery\n```","diagram":"flowchart TD\n  A[Input: monocular video] --> B[Backbone: 2D heatmaps]\n  A --> C[Temporal encoder]\n  B --> D[3D pose via differentiable hand model]\n  C --> D\n  D --> E[Occlusion vis predictor + smoothing]\n  E --> F[Gesture decoding -> UI events]","difficulty":"intermediate","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","NVIDIA","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T05:30:31.054Z","createdAt":"2026-01-15T22:33:53.343Z"},{"id":"q-2778","question":"Design a compact on-device system to detect and track pedestrians in urban traffic and estimate their 3D pose (6-DoF) from a monocular video feed. Constraints: sub-40 ms per frame on an embedded GPU, a lightweight backbone with self-supervised pretraining on unlabeled video plus a small labeled set, and robustness to occlusion, motion blur, and adverse weather. Describe architecture, data strategy, losses, and evaluation methodology?","answer":"Design a compact on-device system: a 2D keypoint detector + 3D pose head (MobileNetV3-scale) with temporal fusion. Reconstruct 6‑DoF pedestrian pose from 2D joints with differentiable PnP and a short ","explanation":"## Why This Is Asked\nAssesses on-device CV design, latency budgeting, and robust 3D human pose estimation for safety-critical driving.\n\n## Key Concepts\n- On-device real-time inference with a lightweight backbone\n- 2D-to-3D pose reconstruction and Kalman filtering\n- Self-supervised pretraining from unlabeled video\n- Robustness to occlusion and adverse weather\n- Quantization and latency accounting\n\n## Code Example\n```javascript\n// Pseudocode for pipeline\nclass PedestrianPoseEstimator {\n  constructor() { /* load detector, 3D head, PnP module, tracker */ }\n  forward(frame) {\n    const keypoints2D = this.detector.detect(frame)\n    const pose3D = this.poseHead.estimate(keypoints2D)\n    const pose6DoF = differentiablePnP(pose3D, this.cameraIntrinsics)\n    return this.tracker.update(pose6DoF)\n  }\n}\n```\n\n## Follow-up Questions\n- How would you extend this to multi-pedestrian tracking with identity preservation?\n- What failure modes are most critical in heavy rain and how would you mitigate them?","diagram":"flowchart TD\n  A[Input video frame] --> B[2D keypoint detector]\n  B --> C[3D pose head]\n  C --> D[Differentiable PnP]\n  D --> E[Kalman tracker]\n  E --> F[On-device inference output]\n  F --> G[Evaluation]\n","difficulty":"advanced","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","Lyft","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T11:36:45.771Z","createdAt":"2026-01-16T11:36:45.771Z"},{"id":"q-3001","question":"Design a real-time multi-view RGB-D fusion system to estimate 6-DoF tool poses and hand interactions on a 4-camera rig. Use a two-branch head: per-view 2D heatmaps plus a depth-voxel fusion module feeding a lightweight 3D pose regressor. Train with synthetic data and self-supervised cross-view consistency; monitor drift via reprojection error and pose-trajectory. Latency <40 ms?","answer":"Propose a 4-view RGB-D fusion with a dual-branch head: per-view 2D heatmaps and a depth-voxel fusion module feeding a lightweight 3D pose regressor. Use differentiable fusion, cross-view consistency l","explanation":"## Why This Is Asked\n\nTests multi-view fusion, edge latency, and robust data association in manufacturing/UD scenarios. Probes training with synthetic data and drift control under strict latency budgets.\n\n## Key Concepts\n\n- Multi-view fusion across RGB-D cameras\n- Edge latency and model compression\n- Self-supervised and synthetic data strategies\n- Cross-view consistency and drift monitoring\n- Data association across views\n\n## Code Example\n\n```javascript\n// High-level fusion outline\nfunction fuseViews(viewFeatures, depthMaps){\n  // voxel pooling and fusion\n  return fused3D;\n}\n```\n\n## Follow-up Questions\n\n- How would you handle unseen tools during inference?\n- How would you evaluate latency vs accuracy on an embedded GPU?","diagram":"flowchart TD\n  A[Input: 4 RGB-D streams] --> B[Per-view 2D heatmaps]\n  B --> C[Depth-voxel fusion]\n  C --> D[Lightweight 3D pose regressor]\n  D --> E[Tool pose + hand interaction outputs]\n  E --> F[Metrics: latency, drift, accuracy]","difficulty":"advanced","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","DoorDash","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T20:41:00.651Z","createdAt":"2026-01-16T20:41:00.651Z"},{"id":"q-3027","question":"Scenario: A fixed RGB camera watches a horizontal conveyor belt in a packaging line. Bottles pass one by one; you must count each item and verify the cap is present and seated using only lightweight CV techniques (no deep nets). Propose a beginner-friendly pipeline: (1) background subtraction to segment bottles, (2) contour-based separation and simple tracking to avoid double counting, (3) HSV-based cap presence check, (4) a temporal filter to smooth counts across frames. Explain thresholds, occlusion handling, and provide a minimal Python/OpenCV outline?","answer":"Approach uses a Gaussian Mixture Model (GMM) or MOG2 background subtraction to segment bottles, filters by aspect ratio and area constraints, samples the cap Region of Interest (ROI) in HSV color space to detect cap presence when hue and saturation values fall within calibrated thresholds, and implements a per-bottle tracker to prevent double counting combined with temporal smoothing for robust counting.","explanation":"## Why This Is Asked\n\nThis question tests practical computer vision skills: robust object counting using lightweight methods, occlusion handling strategies, and threshold tuning in real-world industrial applications. The constraint of avoiding deep networks emphasizes the importance of classical CV techniques.\n\n## Key Concepts\n\n- Background subtraction for moving object detection\n- Contour filtering and simple object tracking\n- HSV color space thresholding for component verification\n- Temporal counting logic and line-crossing detection\n- Occlusion handling and threshold robustness\n- Real-time performance considerations\n\n## Code Example\n\n```python\nimport cv2\nimport numpy as np\n\n# Initialize background subtractor\nbackSub = cv2.createBackgroundSubtractorMOG2(detectShadows=True)\n\nframe = cv2.imread('frame.png')\n# Background subtraction to segment bottles\nfg_mask = backSub.apply(frame)\n# Morphological operations to clean mask\nkernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\nfg_mask = cv2.morphologyEx(fg_mask, cv2.MORPH_OPEN, kernel)\n\n# Find and filter contours\ncontours, _ = cv2.findContours(fg_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\nfor contour in contours:\n    area = cv2.contourArea(contour)\n    x, y, w, h = cv2.boundingRect(contour)\n    aspect_ratio = float(w) / h\n    \n    # Filter by area and aspect ratio for bottles\n    if area > 500 and 0.3 < aspect_ratio < 0.7:\n        # Extract cap ROI and check in HSV space\n        cap_roi = frame[y:y+h//4, x:x+w]\n        hsv_cap = cv2.cvtColor(cap_roi, cv2.COLOR_BGR2HSV)\n        # Apply HSV thresholds for cap detection\n        cap_mask = cv2.inRange(hsv_cap, (h_low, s_low, v_low), (h_high, s_high, v_high))\n        cap_present = np.sum(cap_mask) > threshold\n```\n\n## Follow-up Questions\n\n- How would you handle varying lighting conditions across different shifts?\n- What strategies would you implement for overlapping bottles during high throughput?\n- How could you extend this system to detect bottle fill levels?\n- What performance optimizations would you consider for real-time processing?","diagram":"flowchart TD\n  Start[Start] --> Seg[Background Subtraction & Contour Detection]\n  Seg --> Cap[Cap Presence Check (HSV)]\n  Cap --> Count[Temporal Count & Line Crossing]\n  Count --> End[Total Count]\n","difficulty":"beginner","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Oracle","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T05:43:56.300Z","createdAt":"2026-01-16T21:43:48.972Z"},{"id":"q-3066","question":"On a fast, fixed conveyor, design a real-time monocular 6-DoF pose estimator for a robotic gripper picking tools with glossy surfaces and changing lighting. Target <=25 ms per frame on embedded edge hardware; use a lightweight backbone (MobileNetV3+FPN), differentiable PnP from 2D-3D correspondences, and a short temporal fusion. Describe data strategy, losses (reprojection, pose, temporal consistency), and how you monitor drift and recover from failures?","answer":"Implement a lightweight backbone using MobileNetV3+FPN to predict 2D keypoints and estimate 6-DoF pose through a differentiable PnP layer. Train with synthetic domain randomization to handle lighting variations and glossy surface reflections, complemented by real-world fine-tuning. Optimize for embedded deployment via quantization and TensorRT to achieve ≤25ms per frame. Apply short temporal fusion (3-5 frames) using an EKF to smooth pose estimates and detect drift. Monitor reprojection error and keypoint confidence, triggering recovery when thresholds are exceeded. Loss functions combine reprojection error, pose regularization, and temporal consistency terms.","explanation":"## Why This Is Asked\nAssess real-time monocular pose estimation with edge constraints and data strategy under appearance changes.\n\n## Key Concepts\n- Monocular 6-DoF pose with differentiable PnP\n- Temporal fusion and drift monitoring\n- Edge hardware optimization (quantization, TensorRT)\n- Domain randomization for glossy surfaces\n\n## Code Example\n```javascript\n// placeholder snippet showing a differentiable PnP integration\nfunction differentiablePnP(keypoints2D, points3D, intrinsics) {\n  // implement PnP with gradient flow\n  return estimatePose(keypoints2D, points3D, intrinsics);\n}\n```\n\n## Follow-up Questions\n- How do you handle occlusions in keypoint detection?\n- What recovery strategies work best for tracking failures?\n- How do you balance accuracy vs. speed on embedded hardware?","diagram":null,"difficulty":"advanced","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","Microsoft","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T05:18:31.011Z","createdAt":"2026-01-16T23:36:16.161Z"},{"id":"q-3102","question":"Design a privacy-preserving real-time perception pipeline for monocular video on an embedded device that detects cars, pedestrians, and other scene objects while obfuscating faces and license plates in intermediate features. Outline architecture (edge detector, privacy module, differentiable de-identification loss), latency target <50 ms/frame, and evaluation combining object mAP with privacy leakage metrics?","answer":"Edge-first detector processes frames on-device; faces and license plates are passed through a learnable de-identification module that replaces sensitive regions with anonymized tokens before any transmission or logging. The architecture consists of: (1) a lightweight backbone (e.g., MobileNetV3) for feature extraction, (2) a privacy module that applies differentiable obfuscation to face/license plate regions, and (3) detection heads for cars, pedestrians, and other objects. The de-identification loss combines adversarial privacy loss with reconstruction constraints, ensuring sensitive information cannot be recovered while preserving contextual features for detection. Latency optimization includes model quantization, early-exit strategies, and hardware-aware pruning to achieve <50ms/frame on embedded platforms.","explanation":"## Why This Is Asked\n\nAssesses the ability to architect privacy-aware computer vision systems under strict edge constraints while balancing accuracy and privacy guarantees. Requires concrete architecture, differentiable privacy terms, and rigorous evaluation plans.\n\n## Key Concepts\n\n- Privacy-preserving CV and de-identification modules\n- Differentiable privacy loss and threat-model-aware evaluation\n- Edge/on-device inference and latency budgeting\n- Trade-offs between detection accuracy and privacy guarantees\n\n## Code Example\n\n```javascript\n// Lightweight total loss combining detection accuracy and privacy\nfunction computeTotalLoss(detectionPred, privacyPred, targets) {\n    const detectionLoss = computeDetectionLoss(detectionPred, targets);\n    const privacyLoss = computePrivacyLoss(privacyPred, targets.sensitiveRegions);\n    \n    // Weighted combination with privacy budget constraint\n    return detectionLoss + lambdaPrivacy * privacyLoss;\n}\n\n// Differentiable de-identification module\nclass PrivacyModule {\n    obfuscate(features, sensitiveRegions) {\n        // Apply learnable obfuscation to sensitive regions\n        return features.map((feat, i) => \n            sensitiveRegions[i] ? this.anonymize(feat) : feat\n        );\n    }\n}\n```","diagram":"flowchart TD\n  A[Frame] --> B[Edge Detector]\n  B --> C[Privacy Module]\n  C --> D[Object Detector]\n  D --> E[Outputs]","difficulty":"advanced","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Netflix","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T03:46:01.354Z","createdAt":"2026-01-17T02:21:34.230Z"},{"id":"q-3228","question":"Design a real-time on-edge computer vision pipeline to identify grocery items on a fast-moving conveyor using a single RGB camera. The catalog resides in a MongoDB collection; new items arrive weekly. Achieve <40 ms/frame inference on embedded GPUs, handle unknown items with open-set recognition, and support rapid adaptation with a small labeled set plus self-supervised pretraining. Outline architecture, data flow, and evaluation?","answer":"On-edge detector with a lightweight backbone (EfficientNet-Lite) running under 40 ms/frame; two-stage: fast region proposal then compact classifier with an open-set head. Catalog in MongoDB with per-i","explanation":"## Why This Is Asked\\n\\nAdvanced edge inference with open-set handling is realistic for on-site processing and catalog integration. It tests both CV skills and data engineering with a MongoDB-backed catalog, plus incremental learning in a streaming setting.\\n\\n## Key Concepts\\n\\n- Edge latency and hardware constraints\\n- Open-set recognition and calibration\\n- Self-supervised pretraining and incremental learning\\n- Catalog indexing and embedding storage in MongoDB\\n- Evaluation in a streaming, production-like scenario\\n\\n## Code Example\\n\\n```python\\ndef infer(frame, model):\\n  logits = model(frame)\\n  return softmax(logits)\\n```\\n\\n## Follow-up Questions\\n\\n- How would you onboard new items without downtime?\\n- How would you monitor model drift and rollback strategies?","diagram":null,"difficulty":"advanced","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T07:39:42.261Z","createdAt":"2026-01-17T07:39:42.261Z"},{"id":"q-3291","question":"Design a real-time monocular 6-DoF object tracker for a single RGB camera that can track up to 4 tools on an assembly-line in clutter and variable lighting. Propose a lightweight backbone (MobileNetV3) for initial pose, a differentiable renderer-based refinement (e.g., PyTorch3D) with reprojection and silhouette losses, and a streaming UKF to fuse measurements and handle occlusions. Explain data strategy, losses, latency targets (60 FPS), and evaluation protocol?","answer":"A practical answer would describe a two-stage pipeline: 1) an anchor-free detector using MobileNetV3 for fast 2D-keypoint-into-3D hypotheses, 2) a differentiable renderer (PyTorch3D) to refine 6-DoF w","explanation":"## Why This Is Asked\nTests monocular 3D tracking, differentiable rendering, and real-time fusion under occlusion. It probes design choices for embedded constraints and data strategies across synthetic-real domains.\n\n## Key Concepts\n- Real-time monocular 6-DoF tracking\n- Lightweight backbone, differentiable renderer\n- 2D reprojection and silhouette losses\n- Temporal fusion (UKF) and occlusion handling\n- Data augmentation and domain randomization\n\n## Code Example\n```javascript\n// Pseudocode for UKF pose update\nfunction ukfPredict(state, dt){ /* ... */ }\n```\n\n## Follow-up Questions\n- How would you validate drift and set reinitialization thresholds?\n- What ablation studies would you run to justify the renderer-based refinement?","diagram":"flowchart TD\n  A[RGB Frame] --> B[2D Detector: MobileNetV3]\n  B --> C[2D-3D Hypotheses]\n  C --> D[Differentiable Renderer Refinement]\n  D --> E[Temporal Fusion: UKF]\n  E --> F[Tracked Poses]","difficulty":"advanced","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Instacart","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T10:32:11.309Z","createdAt":"2026-01-17T10:32:11.309Z"},{"id":"q-3439","question":"Design a robust multi-view CV pipeline to detect tampering in high-volume financial documents scanned from different angles; propose a light 2D-3D fusion model using a differentiable renderer to verify cross-view consistency, augmented with synthetic tamper data and a small real set, plus a temporal consistency module to tolerate scan noise. Include latency targets and evaluation protocol?","answer":"Proposed solution: a dual-branch CV system that ingests multi-view scans of each document. Branch A uses a lightweight backbone to extract 2D features; Branch B uses a differentiable renderer to enfor","explanation":"## Why This Is Asked\nIn finance, document integrity is critical. This question probes multi-view reasoning, 2D-3D fusion, differentiable rendering, and temporal stability under latency constraints, all on edge hardware.\n\n## Key Concepts\n- Multi-view fusion with differentiable rendering\n- Synthetic data with domain randomization\n- Lightweight backbones for edge latency\n- Temporal consistency (Kalman/UKF) across frames\n\n## Code Example\n```javascript\nfunction fuse(viewFeatures, tamperTemplate) {\n  // pseudo: cross-view attention with a 3D consistency loss\n  return tamperScore;\n}\n```\n\n## Follow-up Questions\n- How would you quantify cross-view inconsistency and its impact on false positives?\n- What ablation study would you run to justify the differentiable renderer component?","diagram":"flowchart TD\n  A[Multi-view Scans] --> B[2D Feature Branch]\n  A --> C[3D Consistency Renderer]\n  B --> D[Fusion & Tamper Scoring]\n  C --> D\n  D --> E[Temporal Filter & Evaluation]","difficulty":"advanced","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T16:36:46.160Z","createdAt":"2026-01-17T16:36:46.160Z"},{"id":"q-3574","question":"Design a real-time 3D object tracker for autonomous warehouse robots that fuses stereo cameras, a lightweight LiDAR, and an IMU to track up to 6 moving pallets in clutter with occlusions. Propose a sensor fusion architecture (learned fusion head plus model-based tracker), data association (JPDA or MHT), and a latency target under 40 ms per frame on edge hardware. Include evaluation and failure handling?","answer":"An on-board multi-sensor fusion tracker employing a two-stage approach: a lightweight CNN-based fusion head that fuses stereo depth, LiDAR echoes, and IMU data to produce per-object 3D state estimates, followed by a model-based tracker (UKF/EKF) for temporal consistency. The fusion head processes synchronized sensor streams at 15-20 Hz, outputting 6-DOF pose and velocity for each detected pallet. For data association under occlusion, implement Joint Probabilistic Data Association (JPDA) with gating based on Mahalanobis distance and motion prediction confidence. The architecture targets <40ms latency per frame on edge hardware (NVIDIA Jetson/Intel NUC) through model quantization (INT8), tensor optimization, and parallel processing pipelines. Track management includes initialization/deletion logic, occlusion handling using last-known-state prediction, and drift detection via residual monitoring. Evaluation uses CLEAR MOT metrics on warehouse datasets with synthetic occlusions, measuring tracking accuracy, ID switches, and computational efficiency. Failure handling incorporates sensor fault detection, fallback to single-sensor tracking, and graceful degradation when association confidence drops below thresholds.","explanation":"## Why This Is Asked\nAssesses multi-sensor fusion design with real-time constraints in a practical logistics setting.\n\n## Key Concepts\n- Multi-sensor fusion (stereo, LiDAR, IMU)\n- Data association under occlusion (JPDA/MHT)\n- Real-time edge latency and lightweight models\n- Track management and drift handling\n\n## Code Example\n```javascript\n// Pseudo: simple UKF update step with JPDA associations\nfunction updateUKF(state, measurements, associations) {\n  // Predict step using motion model\n  const predicted = predictState(state);\n  \n  // JPDA-weighted measurement update\n  const innovation = computeInnovation(predicted, measurements);\n  const kalmanGain = computeKalmanGain(predicted.covariance, innovation.covariance);\n  \n  // Apply JPDA-weighted update\n  const updatedState = applyUpdate(state, innovation, kalmanGain, associations.weights);\n  \n  return updatedState;\n}\n```","diagram":null,"difficulty":"advanced","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Meta","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T05:42:46.884Z","createdAt":"2026-01-17T21:38:52.560Z"},{"id":"q-3576","question":"You have a fixed RGB camera overlooking a supermarket shelf. Design a beginner-friendly pipeline to detect when two items occlude each other in the frame to help picker guidance. Use only a single RGB stream, lightweight cues (color histograms, edge density, simple optical flow), and optionally a tiny classifier trained on a small labeled set. Outline data flow, thresholding strategy, and a minimal evaluation plan?","answer":"Proposed approach: compute color histogram dissimilarity in candidate regions to separate items, track edge density to detect split boundaries, and apply a lightweight optical flow check to verify independent motion between overlapping objects. The pipeline processes the RGB stream through three parallel feature extractors, combines their confidence scores using adaptive thresholds, and outputs occlusion flags for picker guidance.","explanation":"## Why This Is Asked\n\nThis question probes practical computer vision reasoning for occlusion handling with a single RGB stream, touching on feature design, lightweight modeling, and evaluation—common requirements in industrial applications.\n\n## Key Concepts\n\n- Occlusion detection using color, edges, and motion cues\n- Lightweight modeling and thresholding strategies\n- Edge device deployment and latency considerations\n- Evaluation metrics: precision/recall for occlusion flags\n\n## Code Example\n\n```python\n# Minimal feature extraction sketch\nimport cv2\nframe = cv2.imread('frame.jpg')\n# color histogram computation\nhist = cv2.calcHist([frame], [0,1,2], None, [8,8,8], [0,256,0,256,0,256])\n# edge density detection\nedges = cv2.Canny(cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY), 50, 150)\n# optical flow for motion verification\n# (implementation depends on consecutive frames)\n```","diagram":"flowchart TD\n  A[Acquire frame] --> B[Compute color-hist diff] \n  B --> C[Edge-density check] \n  C --> D[Compute optical flow] \n  D --> E[Occlusion flag?] \n  E --> F[Optional small classifier] ","difficulty":"beginner","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Google","Instacart"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T05:42:17.970Z","createdAt":"2026-01-17T22:27:42.129Z"},{"id":"q-3635","question":"Scenario: a fixed RGB camera watches a conveyor; detect a missing screw by comparing live frame to a reference image using a lightweight template matching approach (normalized cross-correlation). Describe a real-time pipeline on a consumer CPU at 24 FPS: ROI selection, lighting normalization, pass/fail thresholding, and a simple occlusion guard. Include a minimal OpenCV NCC-based snippet and a plan for evaluation?","answer":"Leverage normalized cross-correlation (NCC) on a fixed 60x60 pixel ROI centered on the screw location, working in grayscale for computational efficiency. Apply per-frame histogram equalization to mitigate lighting variations across the conveyor. Use cv2.matchTemplate(refROI, currROI, cv2.TM_CCORR_NORMED) to compute similarity scores, with a pass/fail threshold typically set around 0.7-0.8. Implement a simple occlusion guard by rejecting frames with excessive global intensity changes or when the correlation score drops abruptly across consecutive frames, indicating potential obstructions rather than missing screws.","explanation":"## Why This Is Asked\nThis question evaluates practical computer vision engineering skills for real-time quality control systems, specifically testing the ability to design efficient template matching solutions under resource constraints.\n\n## Key Concepts\n- Normalized cross-correlation for template matching\n- ROI selection and computational optimization\n- Lighting normalization techniques\n- Real-time processing constraints (24 FPS on consumer CPU)\n- Temporal filtering for occlusion detection\n- Threshold tuning for pass/fail classification\n\n## Code Example\n```python\nimport cv2\nimport numpy as np\n\n# Load reference template\nref_template = cv2.imread('reference_screw.png', cv2.IMREAD_GRAYSCALE)\n\n# Process current frame\nframe = cv2.imread('current_frame.png', cv2.IMREAD_GRAYSCALE)\n\n# Define fixed ROI (60x60 pixels around screw location)\ny, x, h, w = 50, 30, 60, 60\nroi = frame[y:y+h, x:x+w]\n\n# Lighting normalization\nroi_normalized = cv2.equalizeHist(roi)\n\n# Template matching using NCC\nresult = cv2.matchTemplate(roi_normalized, ref_template, cv2.TM_CCORR_NORMED)\n_, max_score, _, _ = cv2.minMaxLoc(result)\n\n# Pass/fail determination\nthreshold = 0.75\nis_present = max_score > threshold\n```\n\n## Evaluation Plan\n- Collect dataset with known ground truth (present/missing screws)\n- Test across various lighting conditions and conveyor speeds\n- Measure precision/recall and processing latency\n- Optimize threshold values using ROC analysis\n- Validate 24 FPS performance on target consumer CPU hardware","diagram":"flowchart TD\n  Start[Start]\n  ROI[Define ROI around screw]\n  NCC[Compute NCC with ref]\n  Decision{Score >= thr?}\n  Start --> ROI --> NCC --> Decision\n  Decision --> Pass[Pass]\n  Decision --> Fail[Fail]","difficulty":"beginner","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Meta","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T04:02:03.003Z","createdAt":"2026-01-18T02:35:59.615Z"},{"id":"q-3692","question":"With two synchronized RGB cameras observing a factory belt, design a real-time system to detect and estimate 6-DoF poses of articulated tools (e.g., screwdrivers, wrenches) as they move. Use stereo cues, a lightweight backbone, weak supervision (motion, silhouette), and differentiable rendering for pose refinement. Target sub-60 ms per frame on edge hardware, include uncertainty estimates and a practical evaluation protocol?","answer":"Propose a stereo-based pipeline: lightweight backbone (MobileNetV3), epipolar-guided feature matching, and a parametric 6-DoF tool model to initialize pose. Refine with differentiable rendering using ","explanation":"## Why This Is Asked\nAssesses ability to design a stereo-to-3D pose system with real-time constraints, weak supervision, and differentiable refinement—critical for robotics/AR on large platforms.\n\n## Key Concepts\n- Stereo geometry and epipolar constraints\n- Lightweight backbones and differentiable rendering\n- Parametric articulated tool models\n- Temporal fusion with uncertainty (UKF)\n- Weak supervision from motion and segmentation cues\n- Edge-device latency targets\n\n## Code Example\n```javascript\n// Skeleton: pose_init from stereo + kinematic model\nfunction initPose(stereoLeft, stereoRight, toolModel) { /* ... */ }\n```\n\n## Follow-up Questions\n- How would you validate occlusion scenarios and motion blur?\n- How would you extend to N cameras and address calibration drift?\n- Which ablations reveal the contribution of differentiable rendering?","diagram":"flowchart TD\n  AcquireStereo[Acquire stereo frames] --> FeatureExtract[Feature extraction]\n  FeatureExtract --> InitPose[Init pose via kinematic model]\n  InitPose --> Refine[Differentiable rendering refinement]\n  Refine --> Fuse[Temporal fusion & uncertainty]\n  Fuse --> Output[Output poses]","difficulty":"advanced","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Slack","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T05:41:54.990Z","createdAt":"2026-01-18T05:41:54.991Z"},{"id":"q-3777","question":"Design a real-time 3D scene reconstruction system for a warehouse robot using a single RGB camera. Build a voxel-based TSDF map that separates static infrastructure from moving objects (people, pallets) and fuses frames with a dynamic mask. Specify voxel resolution, dynamic handling method (per-voxel dynamic score), pose estimation for fusion, latency target (20-30 ms/frame), and an evaluation plan including per-voxel error and dynamic object drift metrics?","answer":"Use a TSDF voxel grid at 0.08 m resolution, inventorying both static and dynamic regions. Estimate camera pose with lightweight visual odometry and fuse only voxels labeled static by a dynamic mask, r","explanation":"## Why This Is Asked\n\nTests system-level thinking for 3D reconstruction under dynamic scenes and edge-runtime constraints, plus evaluation rigor.\n\n## Key Concepts\n\n- Dense 3D representation (TSDF) with dynamic masking\n- Lightweight pose estimation and masked fusion\n- Dynamic voxel confidence and decay for occlusions\n- Edge latency targets and robust evaluation metrics\n\n## Code Example\n\n```javascript\n// Pseudocode: update TSDF with dynamic masking\nfunction updateVoxelTSDF(voxel, depth, pose, isDynamic) {\n  if (isDynamic) return; // skip static updates for dynamic voxels until confirmed\n  const sdf = projectDepthToVoxel(depth, pose, voxel.coord);\n  voxel.tsdf = (voxel.tsdf * voxel.weight + sdf) / (voxel.weight + 1);\n  voxel.weight += 1;\n}\n```\n\n## Follow-up Questions\n\n- How would you integrate semantic priors to improve dynamic masking in cluttered warehouses?\n- What ablation model would you run to measure the impact of dynamic masking on map quality and drift over 5 seconds?","diagram":"flowchart TD\n  Camera[RGB Camera] --> PoseEst[Lightweight VO / Pose]\n  PoseEst --> Fusion[Dynamic Masked Fusion]\n  Fusion --> Map[Voxel TSDF Map]\n  Map --> Evaluation[Metrics: per-voxel RMSE, dynamic drift]","difficulty":"advanced","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","Meta","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T09:31:25.945Z","createdAt":"2026-01-18T09:31:25.945Z"},{"id":"q-3835","question":"Design a real-time monocular perception pipeline for a Tesla/PayPal-style ADAS/detection task: detect pedestrians and cyclists in adverse weather (rain, fog) using only a single RGB camera on edge hardware. Outline the architecture (backbone, detector head), how monocular depth cues are integrated, a weather-robust data strategy (augmentation, synthetic data, self-supervised tasks), latency targets, and an evaluation plan across IoU, depth accuracy, and safety-critical false negatives?","answer":"Use a lightweight backbone (MobileNetV3) with a single-shot detector head for 2D boxes and classes, plus a monocular depth head trained with self-supervised photometric and temporal losses. Fuse depth","explanation":"## Why This Is Asked\\n\\nEvaluate design of depth-aware perception with a single RGB camera under adverse weather and edge constraints. Focus on architecture choices, data strategy, and safety-focused evaluation beyond 2D detection.\\n\\n## Key Concepts\\n- Monocular depth estimation\\n- Weather-robust augmentation\\n- Self-supervised and temporal losses\\n- Edge latency and fusion\\n\\n## Code Example\\n```javascript\n// Implementation sketch\nfunction fuseDetAndDepth(boxes, depthMap) {\n  return boxes.map(b => ({...b, depth: depthMap.sample(b.cx, b.cy)}))\n}\n```\n\\n## Follow-up Questions\\n- How would you quantify risk of depth misestimation under fog?\\n- How would you validate domain adaptation from synthetic to real rain?","diagram":null,"difficulty":"advanced","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["PayPal","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T11:30:13.248Z","createdAt":"2026-01-18T11:30:13.248Z"},{"id":"q-4001","question":"On a fixed RGB camera watching a conveyor belt, design a beginner-friendly CV pipeline to detect when an item is rotated away from upright by more than 15 degrees. Use only a single RGB stream and lightweight cues: color histograms, edge density, simple optical flow, and optionally a tiny classifier trained on a small labeled set. Outline data flow, thresholding strategy, and a minimal evaluation plan?","answer":"Propose a fixed-camera belt monitor with a 3-step feature fusion: (1) ROI crop per item and HSV color histogram; (2) edge density from grayscale to gauge shape integrity; (3) short-window dense optica","explanation":"## Why This Is Asked\nNew angle: orientation reliability on a belt using simple cues and a tiny classifier. Encourages feature engineering, thresholding, and practical eval in a manufacturing context relevant to automations used by major tech companies.\n\n## Key Concepts\n- Lightweight feature fusion: color histograms, edge density, optical flow\n- Small classifier and thresholding to decide rotation\n- ROI extraction and a short temporal window for motion cues\n\n## Code Example\n```python\n# pseudo feature extraction (illustrative)\nvec = [hist_value, edge_density, flow_mean]\n# clf trained to output upright vs rotated probability\nprob = clf.predict_proba([vec])[0,1]\nflag = prob > THRESH\n```\n\n## Follow-up Questions\n- How would you handle multiple items on the belt?\n- How would you adapt thresholds for lighting changes?","diagram":"flowchart TD\n  Camera[Fixed RGB Camera] --> ROI[ROI Extraction]\n  ROI --> Features[Feature Extraction]\n  Features --> Classifier[Lightweight Classifier]\n  Classifier --> Decision[Rotation Flag]","difficulty":"beginner","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Square","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T19:22:54.440Z","createdAt":"2026-01-18T19:22:54.440Z"},{"id":"q-4044","question":"Scenario: A fixed RGB camera watches a 3-row shelf in a convenience store. Each row should contain items from a category: snacks, drinks, or stationery. Design a beginner-friendly CV pipeline to detect when an item is placed in the wrong row (e.g., a drink on the snacks row) using only a single RGB stream. Use lightweight cues (color histograms, edge density, simple optical flow) and optionally a tiny classifier trained on a small labeled set. Outline data flow, thresholding strategy, and a minimal evaluation plan?","answer":"Segment the shelf into three ROIs corresponding to snacks, drinks, and stationery. For each ROI, compute a lightweight feature vector consisting of HSV color histograms, edge density, and optical flow magnitude. Use a tiny classifier or prototype-based matching with minimal labeled data to detect category violations. Apply temporal smoothing across frames to reduce false positives.","explanation":"## Why This Is Asked\nTests practical, beginner-friendly computer vision on edge hardware with a realistic misplacement task and minimal labeled data.\n\n## Key Concepts\n- ROI segmentation for fixed layouts\n- Lightweight features: HSV histogram, edge density, optical flow magnitude\n- Tiny classifier or prototype-based decision with small labeled data\n- Thresholded, temporal aggregation to reduce noise\n\n## Code Example\n```javascript\n// Pseudo-feature extraction sketch\nfunction extractFeatures(imgROI) {\n  // compute histogram, edge density, flow magnitude\n  // return [hist, edges, flow]\n}\n```\n\n## Follow-up Considerations\n- How would you handle lighting variations across different times of day?\n- What strategies could you use to reduce false positives when items are partially occluded?\n- How might you extend this system to handle additional categories or shelf configurations?","diagram":"flowchart TD\n  A[Video Frame] --> B[ROI Per Row]\n  B --> C[Feature Extraction]\n  C --> D[Classification/Similarity]\n  D --> E[Misplacement Score]\n  E --> F[Alert]","difficulty":"beginner","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Microsoft","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-19T06:08:14.568Z","createdAt":"2026-01-18T21:27:40.341Z"},{"id":"q-4069","question":"Context: A fixed rooftop RGB camera watches a campus bike lane. Design a beginner-friendly CV pipeline to detect cyclists and estimate their real-time speed using only RGB frames. Include a lightweight detector, a simple calibration-based mapping from pixel motion to m/s, a temporal filter, latency targets, and a minimal evaluation plan?","answer":"Use a lightweight cyclist detector (MobileNetV2-SSD) per frame, then track the bike centroid with a small Kalman filter. Calibrate using known camera height to map pixel motion to real distance via scale factor, compute frame-to-frame speed, and apply temporal smoothing.","explanation":"## Why This Is Asked\n\nAssesses ability to design an end-to-end CV pipeline under real-time constraints, focusing on detection, tracking, and monocular speed estimation without depth sensors.\n\n## Key Concepts\n\n- Lightweight detection\n- Centroid tracking with Kalman filter\n- Camera height-based scale and frame-to-frame speed\n- Edge latency and simple evaluation plan\n\n## Code Example\n\n```javascript\n// Minimal speed estimate sketch\nfunction estimateSpeed(bikeCenters, dt, scale){\n  const dx = bikeCenters[1].x - bikeCenters[0].x;\n  const dy = bikeCenters[1].y - bikeCenters[0].y;\n  const delta = Math","diagram":null,"difficulty":"beginner","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","IBM","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-19T05:54:34.160Z","createdAt":"2026-01-18T22:37:04.651Z"},{"id":"q-4159","question":"Design a real-time on-device video segmentation system for AR on a mobile phone with a single RGB camera that can learn new object masks from 10–20 labeled frames using active learning and self-supervised pretraining. Target latency <40 ms/frame and memory under 400 MB. Describe architecture (shared encoder + compact decoder), online adaptation (entropy-based frame selection, pseudo-labels, limited fine-tuning), data flow, and evaluation metrics (IoU, mask stability, adaptation speed)?","answer":"Leverage MobileOne encoder + compact decoder for per-frame masks, plus a temporal refinement module guided by optical flow to stabilize outputs. Online adaptation uses entropy-based frame selection on","explanation":"## Why This Is Asked\nThis question probes on-device learning for segmentation under tight latency and memory constraints, a practical pain point for AR workflows at scale.\n\n## Key Concepts\n- On-device few-shot learning\n- Self-supervised pretraining and pseudo-labeling\n- Temporal coherence via optical flow\n- Memory and latency budgeting on mobile GPUs\n\n## Code Example\n```javascript\n// Pseudo-code for on-device online update\nfunction onlineUpdate(frame, labelMask) {\n  // extract features\n  // compute loss on pseudo-labels\n  // update small head only\n}\n```\n\n## Follow-up Questions\n- How would you prevent drift during online fine-tuning?\n- What ablations would you run to measure the impact of the temporal refinement module?","diagram":"flowchart TD\n  A[Input frame] --> B[Shared encoder]\n  B --> C[Mask head]\n  B --> D[Temporal refine (optical flow)]\n  C --> E[Mask output]\n  D --> E","difficulty":"intermediate","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-19T05:48:44.842Z","createdAt":"2026-01-19T05:48:44.842Z"},{"id":"q-4225","question":"Design an edge-device semantic segmentation system for a mobile robot with a single RGB camera that must rapidly adapt to new indoor/outdoor environments using only a tiny labeled seed (<50 frames) and no cloud access. Propose a lightweight model, a self-supervised domain adaptation loop (temporal consistency + augmentations), and a concrete evaluation plan across IoU, boundary accuracy, and inference latency?","answer":"Deploy a lightweight encoder–decoder (MobileNetV3 backbone with a tiny SegFormer head) running ~18–22 FPS on the edge. Use a teacher–student EMA setup for self-supervised adaptation: temporal consiste","explanation":"## Why This Is Asked\nTests on-device adaptation with minimal labeled data, no cloud, and robust performance across diverse environments. It emphasizes practical design choices, latency constraints, and a solid evaluation plan.\n\n## Key Concepts\n- Lightweight segmentation architectures for edge\n- Teacher–student EMA for self-supervised domain adaptation\n- Temporal consistency and augmentation strategies\n- Seed supervision and online adaptation loop\n\n## Code Example\n\n```javascript\n// EMA update for teacher parameters (pseudo)\nlet teacherParams = initialTeacher;\nfunction updateTeacher(studentParams, alpha) {\n  return alpha * teacherParams + (1 - alpha) * studentParams;\n}\n```\n\n## Follow-up Questions\n- How would you handle sudden scene shifts (e.g., lighting, weather) without labeled data?\n- What metrics would you monitor in production to detect model drift on-device?","diagram":"flowchart TD\n  A[Video stream] --> B[Edge Encoder]\n  B --> C[Segmentation Head]\n  C --> D[Prediction Cache]\n  D --> E[Adaptation Trigger]\n  E --> F[Robot Action]","difficulty":"intermediate","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","LinkedIn","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-19T09:14:18.687Z","createdAt":"2026-01-19T09:14:18.687Z"},{"id":"q-4555","question":"Design a real-time monocular depth and 6-DoF pose estimation pipeline for a robotic pick-and-place task using a single RGB camera in cluttered, occluded environments. Requirements: sub-40 ms latency on an edge GPU, self-supervised training for depth and pose, temporal filtering to handle occlusions, and a robust evaluation plan including depth accuracy, pose drift under occlusion, and failure modes?","answer":"Utilize a lightweight backbone (EfficientNet-Lite) with dedicated monocular depth and 6-DoF pose estimation heads. Train using self-supervised learning with view synthesis loss and temporal consistency constraints, incorporating synthetic occlusion augmentation. Fuse predictions with an Extended Kalman Filter for temporal smoothing and implement a differentiable refinement loop using rendered feedback to maintain sub-40 ms latency on edge GPU hardware.","explanation":"## Why This Is Asked\nTests ability to design a real-time monocular depth and 6-DoF pose estimation pipeline that performs robustly under occlusion conditions on edge hardware, requiring self-supervised training approaches and iterative refinement capabilities.\n\n## Key Concepts\n- Dual-head architecture: monocular depth estimation and 6-DoF pose regression with lightweight backbone\n- Latency-aware design: EfficientNet-Lite backbone optimized for edge GPU inference under 40 ms\n- Self-supervised training: view synthesis loss, temporal consistency constraints, synthetic occlusion augmentation\n- Temporal filtering: Extended Kalman Filter/Unscented Kalman Filter for stability during motion and occlusion events\n- Differentiable refinement: rendered feedback loop for iterative pose optimization\n\n## Code Example\n\n```python\n# Pseudocode: EKF update with rendered feedback\nstate_pred = predict(state)\npos_ref = render(state_\n```","diagram":"flowchart TD\n  A[Camera] --> B[Backbone+Depth Head]\n  B --> C[Pose Head]\n  B --> D[Depth Head]\n  C --> E[Temporal Filter (EKF/UKF)]\n  D --> E\n  E --> F[Renderer Refinement]\n  F --> G[Outputs]","difficulty":"advanced","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google","LinkedIn"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T06:09:37.444Z","createdAt":"2026-01-19T23:45:05.384Z"},{"id":"q-456","question":"How would you design a real-time object detection system for a social media platform that processes 10M images/day with 99.9% accuracy and <100ms latency?","answer":"Implement a distributed pipeline with GPU clusters for inference, model quantization using TensorRT/ONNX, edge caching for popular content, and ensemble models. Incorporate batch processing, model versioning, and comprehensive monitoring to achieve the target performance metrics.","explanation":"## Architecture\n- **Inference Layer**: GPU clusters with model quantization and TensorRT optimization\n- **Caching Layer**: Redis for frequent detections and CDN edge caching for popular content\n- **Load Balancing**: Kubernetes with auto-scaling based on queue depth and latency thresholds\n- **Monitoring**: Real-time accuracy metrics and comprehensive latency tracking\n\n## Optimization Strategies\n- **Model Compression**: TensorRT optimization with 8-bit quantization for faster inference\n- **Batch Processing**: Dynamic batching based on system load to maximize GPU utilization\n- **Fallback Mechanism**: CPU inference when GPU resources are unavailable\n- **Model Ensemble**: Combine YOLOv8 with ResNet to achieve 99.9% accuracy\n\n## Production Considerations\n- **Model Versioning**: Implement A/B testing and gradual rollouts for model updates\n- **Scalability**: Horizontal scaling of inference nodes to handle peak loads\n- **Reliability**: Implement circuit breakers and retry mechanisms for fault tolerance","diagram":"flowchart TD\n  A[Image Upload] --> B[Preprocessing]\n  B --> C[GPU Inference]\n  C --> D[Postprocessing]\n  D --> E[Cache Check]\n  E --> F[Result Storage]\n  G[Load Balancer] --> C\n  H[Monitor] --> I[Alert System]\n  J[Model Registry] --> C","difficulty":"advanced","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Microsoft","Salesforce","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-09T08:56:19.980Z","createdAt":"2025-12-26 12:51:05"},{"id":"q-4582","question":"Design a production-ready monocular 3D object detector for robotic pick-and-place on a new warehouse line. The system must maintain accurate 6-DoF poses for a fixed set of tools under changing intrinsics, lens distortion, and illumination, using only a single RGB camera and streaming data. Propose a real-time adaptation loop with no labeled data, specify data flow, losses (temporal reprojection, photometric consistency, geometric self-supervision), latency targets, and evaluation plan for drift and false negatives?","answer":"A fixed 6-DoF detection head operates in parallel with a lightweight RGB backbone to predict tool poses. Implement an online self-supervised adaptation loop utilizing temporal reprojection loss from visual odometry, photometric consistency across consecutive frames, and geometric self-supervision to maintain accuracy under varying intrinsics and illumination conditions.","explanation":"## Why This Is Asked\nThis question evaluates real-world robustness by requiring self-supervision mechanisms to handle non-stationary intrinsics and lighting—common challenges in deployed computer vision systems.\n\n## Key Concepts\n- Online self-supervision with temporal and photometric consistency\n- Intrinsics drift management through lightweight calibration\n- Latency constraints for edge devices and evaluation under occlusion\n\n## Code Example\n```javascript\nfunction loss(frame_t, frame_t1, pose_t, pose_t1) {\n  const L_reproj = reprojectionLoss(frame_t, frame_t1, pose_t, pose_t1);\n  const L_photo","diagram":"flowchart TD\n  A[Frame] --> B[Backbone]\n  B --> C[6-DoF Head]\n  C --> D[Online self-supervision]\n  D --> E[EMA update]\n  E --> F[Latency target]\n","difficulty":"advanced","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Hugging Face"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T05:55:52.430Z","createdAt":"2026-01-20T02:34:51.333Z"},{"id":"q-4836","question":"Edge CV task: A warehouse robot with a single RGB camera (Jetson-class) must detect a fixed set of 60 SKUs and flag Unknown items for human review in real time. Design a compact on-device detector with an Open-Set component and a memory module for rapid few-shot updates. Outline architecture (backbone, embedding head, novelty scorer), data strategy (synthetic + real), latency targets (<40 ms/frame), and evaluation (per-class mAP, unknown recall/precision)?","answer":"Propose a two-head detector: backbone MobileNetV3-Large; detection head for class scores; embedding head (128-d) for SKU prototypes. Novelty scorer uses cosine distance to 60 prototypes with a learnab","explanation":"## Why This Is Asked\nTests open-set recognition, on-device learning, and latency budgeting in a realistic warehouse scenario.\n\n## Key Concepts\n- Open-set detection with embedding-based novelty scoring\n- Lightweight on-device memory for rapid updates\n- Synthetic + real data with domain randomization\n\n## Code Example\n```javascript\nfunction cosine(a,b){ let na=Math.hypot(...a); let nb=Math.hypot(...b); let dot=a.reduce((s,v,i)=>s+v*b[i],0); return dot/(na*nb); }\nfunction isUnknown(feature, prototypes, thresh){ return Math.max(...prototypes.map(p=>cosine(feature,p))) < thresh; }\n```\n\n## Follow-up Questions\n- How would you handle skewed SKU distribution over time?\n- What failure modes would you test for during deployment? ","diagram":"flowchart TD\n  A[Input RGB frame] --> B[Backbone]\n  B --> C[Detection Head]\n  B --> D[Embedding Head]\n  C --> E[Known SKU Detections]\n  D --> F[Novelty Scorer]\n  F --> G[Unknown Flag]\n  E & G --> H[NMS & Post-Processing]\n  H --> I[Output with Unknown flag]","difficulty":"advanced","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","LinkedIn","Lyft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T15:42:38.137Z","createdAt":"2026-01-20T15:42:38.137Z"},{"id":"q-487","question":"Design a real-time object detection system for DoorDash delivery vehicles that must identify packages, license plates, and traffic signs in varying weather conditions. How would you handle model optimization for edge deployment and ensure 99% accuracy?","answer":"Use YOLOv8 with custom-trained heads for each object class. Implement TensorRT optimization for NVIDIA Jetson edge devices. Use multi-scale training with weather augmentation (rain, fog, night). Deploy with model ensemble techniques and confidence thresholding to maintain 99% accuracy.","explanation":"## Architecture\n- **Detection Pipeline**: Multi-task CNN with shared backbone and separate heads\n- **Edge Optimization**: TensorRT INT8 quantization, model pruning, batch size 1\n- **Weather Handling**: Domain adaptation, synthetic data generation\n\n## Implementation\n```python\n# Model optimization example\nimport torch\nfrom torch2trt import torch2trt\n\nmodel = YOLOv8(num_classes=3)\nmodel.load_state_dict(torch.load('best.pth'))\nmodel.cuda().eval()\n\n# TensorRT conversion\nx = torch.ones((1, 3, 640, 640)).cuda()\nmodel_trt = torch2trt(model, [x], fp16_mode=True)\n```\n\n## Performance\n- **Inference**: <50ms per frame\n- **Accuracy**: 99%+ on validation set\n- **Memory**: <2GB on edge device","diagram":"flowchart TD\n  A[Camera Feed] --> B[Preprocessing]\n  B --> C[YOLOv8 Detection]\n  C --> D[TensorRT Inference]\n  D --> E[NMS Filtering]\n  E --> F[Temporal Smoothing]\n  F --> G[Confidence Check]\n  G --> H{Accuracy > 99%?}\n  H -->|Yes| I[Output Results]\n  H -->|No| J[Cloud Fallback]","difficulty":"advanced","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-01T06:41:06.125Z","createdAt":"2025-12-26 12:51:06"},{"id":"q-517","question":"Design a real-time object detection system for cryptocurrency trading terminals that must detect and classify multiple monitor types, trading interfaces, and unauthorized screen recording devices with <100ms latency. How would you optimize YOLOv8 for this specific use case?","answer":"Optimize YOLOv8-nano with TensorRT quantization, custom dataset of trading interfaces, and multi-scale feature fusion. Implement temporal consistency with optical flow tracking, use knowledge distillation from larger models, and deploy with batch inference pipelines to achieve sub-100ms latency.","explanation":"## Architecture Overview\n- **Model Selection**: YOLOv8-nano optimized for speed, custom-trained on comprehensive trading UI datasets\n- **Optimization Pipeline**: TensorRT FP16 quantization with custom CUDA kernels for hardware acceleration\n- **Real-time Processing**: Intelligent frame skipping combined with motion detection for idle periods\n\n## Technical Implementation\n```python\n# Custom trading interface detector\nclass TradingDetector:\n    def __init__(self):\n        self.model = YOLO('yolov8n_trading.pt')\n        self.tracker = DeepSort(max_age=30)\n    \n    def detect_frame(self, frame):\n        results = self.model(frame, conf=0.7)\n        return self.tracker.update(results)","diagram":"flowchart TD\n  A[Camera Feed] --> B[Frame Preprocessing]\n  B --> C[YOLOv8-nano Inference]\n  C --> D[Object Tracking]\n  D --> E[Classification Layer]\n  E --> F[Alert System]\n  F --> G[Security Dashboard]\n  C --> H[Performance Monitor]\n  H --> I[Model Retraining]","difficulty":"advanced","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Coinbase","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-09T08:38:40.181Z","createdAt":"2025-12-26 12:51:06"},{"id":"q-545","question":"How would you detect if an image contains a face using basic computer vision techniques?","answer":"To detect if an image contains a face using basic computer vision techniques, apply Haar cascade classifiers that scan the image using trained rectangular filters to identify facial features like eyes and nose. Alternatively, use HOG feature extraction combined with SVM classifiers to distinguish face patterns from background regions.","explanation":"## Face Detection Methods\n\n### Haar Cascade Approach\n- Uses integral images for fast feature computation\n- Trains on positive/negative face samples\n- Detects facial features through rectangular filters\n\n### Implementation Steps\n- Convert image to grayscale\n- Apply histogram equalization\n- Load pre-trained cascade classifier\n- Use detectMultiScale() with proper parameters\n\n### Common Parameters\n```python\nfaces = face_cascade.detectMultiScale(\n    gray_image,\n    scaleFactor=1.1,\n    minNeighbors=5,\n    minSize=(30, 30)\n)\n```\n\n### Trade-offs\n- Fast but less accurate than deep learning\n- Works well for frontal faces\n- Sensitive to lighting conditions","diagram":"flowchart TD\n  A[Input Image] --> B[Grayscale Conversion]\n  B --> C[Histogram Equalization]\n  C --> D[Haar Cascade Detection]\n  D --> E[Face Bounding Boxes]\n  E --> F[Output Results]","difficulty":"beginner","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","NVIDIA","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":["haar cascades","hog features","svm","opencv","detectmultiscale","integral images"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2026-01-07T03:43:05.970Z","createdAt":"2025-12-26 12:51:07"},{"id":"q-570","question":"How would you design a real-time object detection system for Airbnb's property listing photos that can identify amenities and safety violations while processing 10,000 images per hour?","answer":"I'd design a distributed object detection pipeline using Redis queues for image processing, YOLOv8 for real-time detection, and GPU-accelerated batch inference. The system would employ model ensembling for accuracy, TensorRT optimization for performance, and intelligent result caching.","explanation":"## Architecture\n- **Ingestion**: S3 event triggers Lambda functions to queue images for processing\n- **Processing**: GPU workers execute YOLOv8 models with TensorRT optimization for maximum throughput\n- **Storage**: Detection results and metadata stored in PostgreSQL with vector embeddings for similarity search\n- **API Layer**: FastAPI endpoints with Redis caching for low-latency responses\n\n## Performance Optimization\n- **Batch Processing**: Process 32 images per GPU batch to maximize utilization\n- **Model Selection**: YOLOv8-large achieves 45% mAP at 80 FPS for high accuracy requirements\n- **Auto-scaling**: Dynamic worker scaling based on queue depth and processing latency metrics\n\n## Design Trade-offs\n- **Accuracy vs Speed**: Medium models provide optimal balance with 35% mAP at 120 FPS\n- **Cost Efficiency**: Spot instances reduce GPU costs by 70% with checkpoint recovery\n- **Latency Targets**: P99 processing time under 2 seconds for real-time user experience","diagram":"flowchart TD\n  A[S3 Upload] --> B[Lambda Trigger]\n  B --> C[Redis Queue]\n  C --> D[GPU Workers]\n  D --> E[YOLOv8 Detection]\n  E --> F[PostgreSQL]\n  F --> G[API Cache]\n  G --> H[Client]","difficulty":"advanced","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Instacart"],"eli5":null,"relevanceScore":null,"voiceKeywords":["distributed pipeline","redis queue","yolov8","batch inference","tensorrt","model ensemble"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2026-01-08T11:41:33.371Z","createdAt":"2025-12-27T01:12:04.929Z"},{"id":"q-274","question":"How would you implement a hybrid CNN architecture combining ResNet residual connections with EfficientNet compound scaling for production image classification?","answer":"Implement a hybrid CNN by integrating ResNet residual connections with EfficientNet's compound scaling methodology. This involves using ResNet blocks enhanced with channel attention mechanisms, applying EfficientNet's compound scaling formula φ = α^β · γ^φ to balance depth, width, and resolution, and optimizing the model with mixed precision training for production deployment.","explanation":"## Concept\nA hybrid CNN architecture combines ResNet's residual connections (identity shortcuts) with EfficientNet's compound scaling approach, which systematically balances network depth, width, and resolution using a fixed compound coefficient φ.\n\n## Implementation\n```python\n# Hybrid block with residual + efficient scaling\nclass HybridBlock(nn.Module):\n    def __init__(self, in_ch, out_ch, stride=1):\n        super().__init__()\n        # EfficientNet MBConv with residual\n        self.mbconv = MBConv(in_ch, out_ch, stride)\n        self.shortcut = nn.Identity() if stride == 1 else nn.Conv2d(in_ch, out_ch, 1)\n        \n    def forward(self, x):\n        return self.mbconv(x) + self.shortcut(x)\n```\n\n## Production Optimization\n- Mixed precision training for memory efficiency\n- Channel attention for improved feature representation\n- Compound scaling ensures optimal resource utilization","diagram":"flowchart TD\n    A[Input Image] --> B[Stem Conv 3x3]\n    B --> C[Hybrid Block 1: MBConv + Residual]\n    C --> D[Hybrid Block 2: MBConv + Residual]\n    D --> E[SE Block: Channel Attention]\n    E --> F[Hybrid Block 3: Downsample]\n    F --> G[Global Average Pool]\n    G --> H[Fully Connected]\n    H --> I[Softmax]\n    \n    C --> C1[Identity Shortcut]\n    C1 --> C2[Add]\n    D --> C2\n    E --> C2","difficulty":"intermediate","tags":["cnn","resnet","efficientnet"],"channel":"computer-vision","subChannel":"image-classification","sourceUrl":"https://arxiv.org/abs/1905.11946","videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google","Meta","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-08T11:22:10.106Z","createdAt":"2025-12-26 12:51:07"},{"id":"q-253","question":"How does YOLO implement real-time object detection using grid-based prediction and what are the key components of its architecture?","answer":"YOLO divides image into S×S grid, each cell predicts B bounding boxes with confidence scores, class probabilities, and uses anchor boxes for better localization.","explanation":"## Interview Context\nThis question assesses understanding of modern object detection architectures and trade-offs between speed and accuracy. YOLO's unified approach is fundamental for real-time applications.\n\n## Technical Details\n\n### Core Architecture\n- **Backbone**: Darknet feature extractor (Darknet-53 for YOLOv3)\n- **Grid System**: S×S grid where each cell predicts B bounding boxes\n- **Output Tensor**: [S, S, B*(5+C)] where 5 = (x, y, w, h, confidence), C = class count\n\n### Key Components\n- **Anchor Boxes**: Pre-defined aspect ratios to improve bounding box prediction\n- **IoU Calculation**: Intersection over Union for confidence scoring\n- **Non-Maximum Suppression**: Removes redundant detections above IoU threshold\n- **Multi-Scale Predictions**: Feature maps at different scales for various object sizes\n\n### Loss Function\n```\nLoss = λ_coord * MSE(bbox) + λ_noobj * MSE(confidence) + MSE(class)\n```\n\n### Training Process\n- Single-stage training end-to-end\n- Uses mean squared error for bounding box regression\n- Cross-entropy for classification\n\n## Performance Comparison\n- **YOLOv1**: 45 FPS on Titan X (63.4% mAP)\n- **Faster R-CNN**: 7 FPS (73.2% mAP)\n- **YOLOv3**: 30 FPS (57.9% mAP)\n\n## Follow-up Questions\n1. How does YOLOv5 improve upon YOLOv3's architecture and training methodology?\n2. What are the trade-offs between YOLO and two-stage detectors like Faster R-CNN?\n3. How would you optimize YOLO for edge devices with limited computational resources?","diagram":"graph TD\n    A[Input Image 448×448] --> B[Backbone Network DarkNet-53]\n    B --> C[Feature Maps]\n    C --> D[Detection Head]\n    D --> E[Grid S×S]\n    E --> F[Each Cell Predicts]\n    F --> G[B Bounding Boxes]\n    F --> H[Confidence Scores]\n    F --> I[Class Probabilities]\n    G --> J[Non-Max Suppression]\n    H --> J\n    I --> J\n    J --> K[Final Detections]","difficulty":"beginner","tags":["yolo","rcnn","detr"],"channel":"computer-vision","subChannel":"object-detection","sourceUrl":null,"videos":null,"companies":["Amazon","Google","Meta","Microsoft","NVIDIA","Tesla"],"eli5":"Imagine you have a big chocolate chip cookie and you want to find all the chocolate chips really fast! You cut the cookie into tiny squares, like a checkerboard. Each little square has a special job - it looks for chocolate chips that might be hiding inside it. Each square also draws a box around any chips it finds and says how sure it is that it's really a chip. Some squares might think they see a chip when it's just a crumb, so they have to be very confident! The computer learns to do this super quickly by practicing with lots of cookies, so it can find all the chocolate chips in one quick look instead of searching slowly all over the cookie.","relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-26T16:38:46.236Z","createdAt":"2025-12-26 12:51:07"},{"id":"q-200","question":"How does U-Net's skip connection architecture enable precise medical image segmentation?","answer":"U-Net employs a contracting encoder to extract contextual features and an expanding decoder with skip connections that preserve spatial details, enabling precise pixel-wise segmentation.","explanation":"## Why Asked\nTests understanding of advanced CNN architectures for medical imaging and computer vision segmentation tasks.\n\n## Key Concepts\n- Encoder-decoder architecture\n- Skip connections for feature preservation\n- Contracting and expanding paths\n- Pixel-wise segmentation\n\n## Code Example\n```\ndef unet_block(x, skip):\n    x = Conv2D(64, 3, padding='same')(x)\n    x = concatenate([x, skip])\n    return x\n```\n\n## Follow-up Questions\n- How does U-Net handle class imbalance?\n- What are alternatives to skip connections?\n- How does it compare to FCN?","diagram":"flowchart TD\n  A[Input Image] --> B[Encoder Path]\n  B --> B1[Bottleneck]\n  B1 --> C[Decoder Path]\n  C --> D[Segmentation Output]\n  B -.->|Skip Connections| C","difficulty":"beginner","tags":["unet","mask-rcnn","sam"],"channel":"computer-vision","subChannel":"segmentation","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":"https://www.youtube.com/watch?v=HS3Q_90hnDg"},"companies":["Amazon","Google","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":["u-net","skip connections","encoder","decoder","spatial details","segmentation"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2025-12-30T01:46:54.144Z","createdAt":"2025-12-26 12:51:06"},{"id":"q-228","question":"How would you optimize a real-time medical image segmentation pipeline using SAM with 100ms latency constraint on edge devices?","answer":"Use SAM's lightweight encoder with quantized ViT-B, implement prompt caching, and apply tensorRT optimization for sub-100ms inference.","explanation":"## Concept Overview\nReal-time medical segmentation requires balancing accuracy with strict latency constraints. SAM (Segment Anything Model) provides zero-shot segmentation but needs optimization for edge deployment.\n\n## Implementation Details\n- **Model Optimization**: Use SAM-ViT-B (lightweight) with INT8 quantization\n- **Prompt Engineering**: Implement prompt caching for similar anatomical regions\n- **Hardware Acceleration**: Deploy with TensorRT on NVIDIA Jetson or CoreML on Apple Silicon\n- **Batch Processing**: Process multiple slices in parallel when available\n\n## Code Example\n```python\n# Optimized SAM inference pipeline\nimport torch\nfrom segment_anything import sam_model_registry\n\nclass OptimizedSAM:\n    def __init__(self):\n        self.sam = sam_model_registry['vit_b'](checkpoint='sam_vit_b.pth')\n        self.sam.eval()\n        self.sam.cuda()\n        # Enable TensorRT optimization\n        self.sam = torch.compile(self.sam, mode='max-autotune')\n    \n    def segment_with_cache(self, image, prompt):\n        # Check prompt cache first\n        cache_key = hash(prompt.tobytes())\n        if cache_key in self.prompt_cache:\n            return self.prompt_cache[cache_key]\n        \n        masks = self.sam.predict(image, prompt)\n        self.prompt_cache[cache_key] = masks\n        return masks\n```\n\n## Common Pitfalls\n- **Memory Overhead**: Prompt caching can consume significant memory on edge devices\n- **Quantization Loss**: INT8 quantization may reduce fine-grained segmentation accuracy\n- **Prompt Sensitivity**: Medical images require precise prompt placement for accurate results\n- **Hardware Variability**: Different edge devices have varying compute capabilities","diagram":"graph TD[Input Medical Image] --> A[Preprocessing: Resize/Normalize]\nA --> B[Prompt Detection: Anatomical Region]\nB --> C{Prompt Cache Hit?}\nC -->|Yes| D[Return Cached Mask]\nC -->|No| E[SAM Encoder: ViT-B Lightweight]\nE --> F[Prompt-Guided Decoder]\nF --> G[Post-processing: Refine Boundaries]\nG --> H[Cache Result]\nH --> I[Output Segmentation Mask]\nD --> I\n\nsubgraph Edge Device Optimization\n    J[TensorRT Engine] --> K[INT8 Quantization]\n    K --> L[Memory Pool Management]\nend\n\nE -.-> J\nF -.-> J","difficulty":"advanced","tags":["unet","mask-rcnn","sam"],"channel":"computer-vision","subChannel":"segmentation","sourceUrl":null,"videos":null,"companies":["Apple","Google","Meta","Microsoft","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-26T12:40:51.586Z","createdAt":"2025-12-26 12:51:07"}],"subChannels":["general","image-classification","object-detection","segmentation"],"companies":["Adobe","Airbnb","Amazon","Anthropic","Apple","Bloomberg","Citadel","Cloudflare","Coinbase","Databricks","Discord","DoorDash","Goldman Sachs","Google","Hashicorp","Hugging Face","IBM","Instacart","LinkedIn","Lyft","Meta","Microsoft","MongoDB","NVIDIA","Netflix","OpenAI","Oracle","PayPal","Plaid","Robinhood","Salesforce","Scale Ai","Slack","Snap","Snowflake","Square","Tesla","Twitter","Two Sigma","Uber","Zoom"],"stats":{"total":44,"beginner":13,"intermediate":7,"advanced":24,"newThisWeek":30}}