{"questions":[{"id":"q-643","question":"How would you design an indexing strategy for a time-series database that handles both recent data queries and long-term historical analysis, considering the trade-offs between write performance and query efficiency?","answer":"I would implement a tiered indexing strategy combining time-based partitioning with adaptive index types: recent data would utilize high-performance write-optimized indexes, while historical data would employ compressed, space-efficient indexes for long-term storage and analytical queries.","explanation":"For a time-series database, I'd design a multi-tiered indexing approach that dynamically adapts to data age and access patterns. Recent data (last 30 days) would use a composite index on timestamp and primary key with minimal overhead structure, implementing LSM tree or skip list architectures to maximize write throughput while maintaining fast query performance. This tier prioritizes write efficiency since recent data represents a small but frequently accessed portion of the total dataset.\n\nFor medium-term data (1-12 months), I'd transition to more space-efficient indexing using compressed bitmap indexes or time-series specific structures that balance storage optimization with acceptable query performance. This tier would leverage data compression techniques and adaptive sampling to reduce storage footprint while maintaining reasonable analytical capabilities.\n\nFor historical data (12+ months), I'd implement highly compressed, columnar-based indexing optimized for analytical workloads, potentially utilizing data summarization and aggregation strategies to dramatically reduce storage requirements while supporting large-scale historical analysis queries.","diagram":null,"difficulty":"intermediate","tags":["time-series","index-optimization","partitioning","performance-tuning"],"channel":"database","subChannel":"database","sourceUrl":null,"videos":null,"companies":[],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"lastUpdated":"2026-01-08T11:46:00.042Z","createdAt":"2026-01-07T14:03:16.585Z"},{"id":"q-651","question":"How would you design an indexing strategy for a table with 10 million rows that has frequent read queries with multiple WHERE conditions, occasional bulk inserts, and needs to support both exact match and range queries on different columns?","answer":"Create a composite index for the most common query patterns, include selective columns first, use covering indexes to avoid lookups, and balance read performance with write overhead by monitoring inde","explanation":"For this scenario, I'd start by analyzing the query patterns to identify the most frequent and selective WHERE conditions. The primary strategy would be creating a composite index that matches the most common query pattern, with the most selective column first to maximize index effectiveness. For example, if queries frequently filter by status (high selectivity) and date_range (range queries), I'd create a composite index on (status, created_at) to support both exact matches and range scans efficiently.\n\nTo handle the bulk insert performance concern, I'd implement a careful indexing approach: create essential indexes before data loading, drop non-critical indexes during bulk operations, and rebuild them afterward. I'd also consider using partial indexes for frequently queried subsets of data, and include additional columns in the composite index to create covering indexes that eliminate the need for table lookups. The key is monitoring index usage statistics to identify unused indexes that add write overhead without read benefits.\n\nFor the mixed query requirements, I'd employ multiple index types: B-tree indexes for equality and range queries, hash indexes for pure equality lookups if supported, and potentially bitmap indexes for low-cardinality columns in analytical scenarios. The strategy would involve testing different index combinations with EXPLAIN ANALYZE to verify query plans, ensuring the optimizer chooses the most efficient access paths while keeping the total index size manageable to avoid excessive storage and maintenance costs.","diagram":null,"difficulty":"intermediate","tags":["database-indexing","query-optimization","performance-tuning","composite-indexes","bulk-operations"],"channel":"database","subChannel":"database","sourceUrl":null,"videos":null,"companies":[],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"lastUpdated":"2026-01-09T03:41:16.931Z","createdAt":"2026-01-09T03:41:16.931Z"},{"id":"q-630","question":"Explain the difference between a B-tree index and a hash index, and when would you choose one over the other?","answer":"B-tree indexes support range queries and ordered data access, while hash indexes only support exact equality lookups. Choose B-tree for range queries and sorting, hash for exact match operations.","explanation":"B-tree indexes are the most common type of database index, organizing data in a balanced tree structure that allows for efficient searching, insertion, and deletion. They maintain data in sorted order, making them ideal for range queries (>, <, BETWEEN), ORDER BY operations, and prefix searches. B-trees have logarithmic time complexity O(log n) for most operations and work well with both equality and range predicates.\n\nHash indexes, on the other hand, use a hash function to map keys to bucket locations, providing O(1) average time complexity for exact equality matches. They excel at point queries using = operators but cannot handle range queries or ordering since the data is not stored in any particular sequence. Hash indexes are typically memory-based and perform best when the selectivity is high (few matching rows per key).\n\nIn practice, B-tree indexes are the default choice for most scenarios due to their versatility. Hash indexes are preferred for high-volume OLTP systems with frequent exact lookups, such as user authentication tables or cache key-value stores. For example, a user table queried by email would benefit from a B-tree index (supporting LIKE searches), while a session table queried by exact session ID would be better served by a hash index.","diagram":null,"difficulty":"intermediate","tags":["database-indexing","b-tree","hash-index","query-optimization"],"channel":"database","subChannel":"index-types","sourceUrl":null,"videos":null,"companies":["Google","Amazon","Microsoft","Meta","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"lastUpdated":"2026-01-02T03:42:16.443Z","createdAt":"2026-01-02T03:42:16.443Z"},{"id":"da-125","question":"Explain database indexing and when should you use it?","answer":"Database indexes are data structures that improve query speed by maintaining sorted references to data locations, trading increased write overhead for faster read operations.","explanation":"**How Indexes Work**:\n- Create sorted data structures (B-tree, Hash) that point to actual data\n- Enable efficient data location without full table scans\n- Trade write performance for read speed improvements\n\n**When to Use**:\n- Frequently queried columns\n- WHERE clause conditions\n- JOIN operations\n- ORDER BY and GROUP BY columns\n\n**When NOT to Use**:\n- Small tables (< 100 rows)\n- Frequently updated columns\n- Low cardinality columns\n- Write-heavy workloads","diagram":"graph TD\n    Query[SQL Query] --> Index[(Database Index)]\n    Index --> DataPoint[Data Location Pointer]\n    DataPoint --> TableData[(Table Data)]\n    WriteOp[Write Operation] --> IndexUpdate[Update Index]\n    IndexUpdate --> TableUpdate[Update Table]\n    style Index fill:#4ade80\n    style TableData fill:#fbbf24","difficulty":"intermediate","tags":["sql","indexing"],"channel":"database","subChannel":"indexing","sourceUrl":null,"videos":{"longVideo":"https://www.youtube.com/watch?v=BIlFTFrEFOI","shortVideo":"https://www.youtube.com/watch?v=BHwzDmr6d7s"},"companies":["Amazon","Google","Meta","Microsoft","Netflix"],"eli5":"Imagine you have a big box of LEGOs mixed together. If you want to find all the red pieces, you'd have to dig through everything! But what if you had a special magic book that tells you exactly where each red LEGO is? You could find them super fast! A database index is like that magic book. It keeps a quick list of where things are stored, so the computer doesn't have to search through everything. Use it when you need to find things quickly, like finding your favorite toys in a huge toy box. The only downside is that when you add new toys, you also have to update your magic book - that takes a little extra time. But it's totally worth it when you want to find things fast!","relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"lastUpdated":"2025-12-25T13:11:04.397Z","createdAt":"2025-12-26 12:51:06"},{"id":"db-1","question":"Explain the differences between Clustered and Non-Clustered Indexes, including their performance implications, storage characteristics, and when to choose each type in database design?","answer":"Clustered indexes physically reorder data (1 per table) - ideal for range queries and primary keys. Non-clustered indexes store pointers to data (many per table) - better for selective lookups and covering queries. Clustered improves sequential reads but slows INSERT/UPDATE due to page splits. Non-clustered adds storage overhead but speeds up specific query patterns without affecting data order.","explanation":"## Storage Architecture\n**Clustered**: Data pages ordered by index key. Table IS the index.\n**Non-Clustered**: B-tree structure with leaf nodes pointing to data rows (or clustered index keys).\n\n## Performance Trade-offs\n- **Range queries**: Clustered excels (contiguous data)\n- **Point lookups**: Non-clustered optimal (direct navigation)\n- **INSERT/UPDATE**: Clustered slower (page reorganization)\n- **Storage**: Non-clustered requires additional space\n\n## Use Cases\n**Choose Clustered**: Primary keys, date ranges, sequential access patterns\n**Choose Non-Clustered**: Foreign keys, search columns, filtering predicates\n\n## Edge Cases\n- Tables without clustered indexes use heap storage\n- Multiple non-clustered indexes can reference same clustered key\n- Covering indexes eliminate key lookups entirely","diagram":"flowchart TD\n  A[Query] --> B{Index Type?}\n  B -->|Clustered| C[Direct data access]\n  B -->|Non-Clustered| D[Pointer lookup] --> C\n  C --> E[Result]","difficulty":"beginner","tags":["sql","indexing","perf"],"channel":"database","subChannel":"indexing","sourceUrl":null,"videos":null,"companies":null,"eli5":"Imagine your toys are in a big toy box! A clustered index is like putting all your cars together, then all your dolls, then all your blocks - everything is sorted in one order. You can only have one way to sort everything! A non-clustered index is like having a separate notebook that says 'cars are on page 3, dolls are on page 7' - you can have many notebooks for different ways to find things! Use clustered when you always look for things in the same order (like finding books by their number). Use non-clustered when you need to find things in many different ways (like finding toys by color, size, or type)!","relevanceScore":79,"voiceKeywords":null,"voiceSuitable":false,"lastUpdated":"2025-12-27T06:26:44.376Z","createdAt":"2025-12-26 12:51:05"},{"id":"q-170","question":"When would you choose a composite index over multiple single-column indexes in a relational database?","answer":"Composite indexes are preferable when queries consistently filter on multiple columns together, as they enable more efficient data retrieval than separate single-column indexes.","explanation":"Composite indexes excel when queries frequently filter or sort on multiple columns simultaneously. By storing data in a specific column order, they allow the database to satisfy query conditions through a single index lookup rather than multiple index scans. For instance, a composite index on (last_name, first_name) efficiently handles queries like `WHERE last_name = 'Smith' AND first_name = 'John'`. However, composite indexes incur higher write overhead and should be implemented judiciously based on actual query patterns and performance requirements.","diagram":"graph TD\n    A[Query: WHERE last_name='Smith' AND first_name='John'] --> B{Index Strategy}\n    B --> C[Single Column Indexes]\n    B --> D[Composite Index last_name,first_name]\n    C --> E[2 Index Scans + Merge]\n    D --> F[1 Index Lookup]\n    E --> G[Higher I/O Cost]\n    F --> H[Lower I/O Cost]","difficulty":"intermediate","tags":["index","optimization"],"channel":"database","subChannel":"indexing","sourceUrl":null,"videos":{"longVideo":"https://www.youtube.com/watch?v=oebtXK16WuU"},"companies":["Amazon","Goldman Sachs","Google","Meta","Microsoft"],"eli5":"Imagine you're looking for a specific toy in a giant toy box. If you have separate lists for 'red toys' and 'car toys', you'd have to check both lists and then search through all the toys to find the red cars. But if you have one special list that says 'red cars' together, you can find them instantly! A composite index is like that special combined list - it helps the database find things much faster when you're looking for something that matches multiple rules at once, like finding all the red car toys instead of searching through all red toys AND all car toys separately.","relevanceScore":null,"voiceKeywords":["composite index","single-column indexes","query performance","filtering","database optimization"],"voiceSuitable":true,"lastUpdated":"2026-01-08T11:26:38.999Z","createdAt":"2025-12-26 12:51:06"},{"id":"q-288","question":"What is the main difference between B-tree and hash index in terms of range query performance?","answer":"B-trees provide O(log n) range scans with sorted data traversal; hash indexes offer O(1) equality lookups but degrade to O(n) for ranges and require collision handling strategies like chaining or open addressing.","explanation":"## Why Asked\nTests practical understanding of index selection based on query patterns and performance characteristics, including collision scenarios.\n\n## Performance Comparison\n**B-tree Index:**\n- Range queries: O(log n + k) where k = result count\n- Equality lookups: O(log n)\n- Memory overhead: 20-30% additional storage\n- Example: `SELECT * FROM employees WHERE salary BETWEEN 50000 AND 80000` efficiently scans adjacent leaf nodes\n\n**Hash Index:**\n- Equality lookups: O(1) average case\n- Range queries: O(n) full table scan required\n- Memory overhead: 15-25% additional storage\n- Example: `SELECT * FROM employees WHERE email = 'john@company.com'` direct bucket access\n\n## Collision Handling\nHash indexes use two main collision strategies:\n1. **Separate Chaining**: Stores colliding keys in linked lists from the same bucket\n2. **Open Addressing**: Probes alternative slots using linear/quadratic probing\n\nPerformance degrades when load factor exceeds 0.7, turning O(1) lookups into O(n) chain traversals. B-trees maintain consistent performance regardless of data distribution.\n\n## Code Example\n```sql\n-- B-tree for range queries (salary ranges, date ranges)\nCREATE INDEX idx_salary_btree ON employees(salary);\n-- Hash for exact matches (email lookups, user IDs)\nCREATE INDEX idx_email_hash ON employees USING HASH(email);\n-- Monitor hash index collision impact\nSELECT schemaname, tablename, indexname, \n       pg_stat_get_numscans(pg_stat_get_index_relid(indexrelid)) as scans\nFROM pg_stat_user_indexes;\n```\n\n## Follow-up Questions\nHow would you handle hash index rebalancing with high insert rates?\nWhat hybrid indexing strategy combines both B-tree and hash benefits?\nHow does partitioning affect index choice for billion-row tables?","diagram":"flowchart TD\n  A[Query Type] --> B{Equality?}\n  B -->|Yes| C[Hash Index: O(1)]\n  B -->|No| D[Range/Sort: B-tree]","difficulty":"beginner","tags":["btree","hash-index","composite"],"channel":"database","subChannel":"indexing","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"lastUpdated":"2025-12-28T02:04:14.459Z","createdAt":"2025-12-26 12:51:07"},{"id":"q-365","question":"You're designing a real-time analytics system for Discord that processes millions of message events per minute. Your PostgreSQL database is experiencing severe write contention on the message_events table. How would you design a partitioning strategy using declarative partitioning, and what specific index optimizations would you implement to handle both time-series queries and user-based lookups efficiently?","answer":"Implement declarative partitioning with time-based partitions (hourly for high-traffic periods, daily for standard load) on the message_events table. Create BRIN indexes on timestamp columns for efficient range queries, B-tree indexes on user_id for fast user-based lookups, and composite indexes (user_id, timestamp) to optimize common query patterns that filter by both user and time ranges.","explanation":"## Why This Is Asked\nDiscord processes millions of message events per minute, making database write contention a critical scalability challenge. This question assesses understanding of PostgreSQL's advanced partitioning capabilities and index optimization strategies for high-throughput, time-series workloads.\n\n## Expected Answer\nStrong candidates will discuss:\n- Declarative partitioning strategy (hourly partitions during peak load, daily partitions for normal periods)\n- BRIN indexes on timestamp columns for space-efficient range scans\n- B-tree indexes on user_id for rapid user-specific queries\n- Composite (user_id, timestamp) indexes for common analytical query patterns\n- Partition pruning and constraint exclusion optimization\n- Automated partition creation and maintenance workflows\n\n## Code Example\n```sql\nCREATE TABLE message_events (\n    id BIGSERIAL,\n    user_id BIGINT NOT NULL,\n    event_type VARCHAR(50),\n    payload JSONB,\n    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()\n) PARTITION BY RANGE (created_at);\n\n-- Create hourly partitions for recent data\nCREATE TABLE message_events_2024_01_01_h00 PARTITION OF message_events\n    FOR VALUES FROM ('2024-01-01 00:00:00') TO ('2024-01-01 01:00:00');\n\n-- Indexes\nCREATE INDEX CONCURRENTLY idx_message_events_user_id ON message_events (user_id);\nCREATE INDEX CONCURRENTLY idx_message_events_timestamp_brin ON message_events USING BRIN (created_at);\nCREATE INDEX CONCURRENTLY idx_message_events_user_time ON message_events (user_id, created_at);\n```","diagram":"flowchart TD\n    A[Message Event Ingestion] --> B[Partition Router]\n    B --> C{Timestamp Range}\n    C -->|00:00-01:00| D[Partition 2024_01_01_00]\n    C -->|01:00-02:00| E[Partition 2024_01_01_01]\n    C -->|02:00-03:00| F[Partition 2024_01_01_02]\n    D --> G[BRIN Index on Timestamp]\n    E --> H[BRIN Index on Timestamp]\n    F --> I[BRIN Index on Timestamp]\n    G --> J[B-tree Index on user_id]\n    H --> K[B-tree Index on user_id]\n    I --> L[B-tree Index on user_id]\n    J --> M[Query Engine]\n    K --> M\n    L --> M\n    M --> N[Partition Pruning]\n    N --> O[Result Set]","difficulty":"advanced","tags":["joins","indexes","normalization","postgres"],"channel":"database","subChannel":"indexing","sourceUrl":null,"videos":{"shortVideo":"https://www.youtube.com/watch?v=-qNSXK7s7_w","longVideo":"https://www.youtube.com/watch?v=niOq5zorv-g"},"companies":["Amazon","Discord","Google","Netflix","Palantir","Stripe","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":["partitioning","brin index","b-tree index","write contention","time-series","composite index"],"voiceSuitable":true,"lastUpdated":"2025-12-31T06:42:15.110Z","createdAt":"2025-12-26 12:51:05"},{"id":"q-409","question":"You're designing a database for an e-commerce platform with frequent queries on (user_id, order_date) and (product_id, category). How would you choose between B-tree and hash indexes, and what composite index strategy would optimize both query patterns?","answer":"Use B-tree composite indexes: (user_id, order_date) for range queries and (product_id, category) for exact matches. Hash indexes are unsuitable for order_date range queries because they store data in hash buckets without logical ordering, forcing full table scans for any date range operations.","explanation":"## Why This Is Asked\nTests practical index selection knowledge and understanding of real-world query optimization trade-offs that TCS engineers face daily.\n\n## Expected Answer\nStrong candidate explains B-tree handles range queries (order_date) and equality, while hash only supports equality. They'll recommend two composite indexes matching query patterns, discuss index ordering, and mention covering indexes to avoid table scans.\n\n## Index Selection Deep Dive\n**B-tree vs Hash for Range Queries:**\nHash indexes distribute values across buckets using hash functions, destroying any logical ordering. For queries like `WHERE order_date BETWEEN '2024-01-01' AND '2024-01-31'`, hash indexes must scan every bucket (O(n) complexity), while B-trees efficiently traverse adjacent leaf nodes (O(log n + k) complexity).\n\n**Composite Index Strategy:**\n- `(user_id, order_date)`: Enables efficient user order history with date filtering\n- `(product_id, category)`: Optimizes product category lookups and inventory queries\n\n**Covering Index Benefits:**\nA covering index like `(user_id, order_date, status, total)` can satisfy queries without accessing the table, reducing I/O by 60-80% for common reporting queries.\n\n## Code Example\n```sql\n-- Primary composite indexes\nCREATE INDEX idx_user_orders ON orders(user_id, order_date);\nCREATE INDEX idx_product_category ON products(product_id, category);\n\n-- Covering index for order summaries (avoids table lookups)\nCREATE INDEX idx_order_summary ON orders(user_id, order_date, status, total) \nINCLUDE (customer_name);\n\n-- Hash index would fail here (cannot optimize range):\n-- EXPLAIN ANALYZE SELECT * FROM orders \n-- WHERE order_date >= '2024-01-01' AND order_date <= '2024-01-31';\n```\n\n## Performance Impact\n- B-tree range queries: 10-100x faster than hash indexes\n- Index size: B-trees use 20-30% more storage but provide comprehensive optimization\n- Write overhead: B-trees have 15-25% slower inserts due to tree rebalancing\n\n## Follow-up Questions\n- How would you handle queries that only filter on the second column?\n- When would you consider a hash index instead?\n- How do you monitor index effectiveness in production?","diagram":"flowchart TD\n    A[Query Analysis] --> B{Range Queries?}\n    B -->|Yes| C[B-tree Index]\n    B -->|No| D{Equality Only?}\n    D -->|Yes| E[Hash Index]\n    D -->|No| C\n    C --> F[Composite Strategy]\n    E --> F\n    F --> G[Index Ordering]\n    G --> H[Covering Indexes]\n    H --> I[Performance Monitoring]","difficulty":"intermediate","tags":["btree","hash-index","composite"],"channel":"database","subChannel":"indexing","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":"https://www.youtube.com/watch?v=BHCSL_ZifI0"},"companies":["Gitlab","Tcs","Tempus"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"lastUpdated":"2025-12-28T02:24:47.473Z","createdAt":"2025-12-26 12:51:05"},{"id":"q-420","question":"You're designing a user database for a chat application with 10M users. When would you choose a B-tree index over a hash index for the 'email' column, and what are the performance implications for login queries, user search, and profile updates?","answer":"B-tree indexes are preferable for email columns due to support for range queries, prefix searches, and better concurrent access, despite hash indexes being ~20% faster for exact matches. B-trees provide 100-500ms login queries, efficient user searches with prefix patterns, and profile updates with minimal write amplification, while hash indexes would require full rebuilds during updates and couldn't handle common search patterns.","explanation":"## Interview Context\nThis question evaluates database indexing strategy for large-scale user management, testing understanding of query patterns, performance trade-offs, and real-world operational requirements.\n\n## Technical Deep Dive\n### Performance Metrics by Operation Type\n**Login Queries (exact email match)**: B-tree: 100-500ms, Hash: 80-400ms. B-tree's slight overhead is offset by MVCC compatibility and better lock management under 1000 concurrent logins.\n\n**User Search (prefix/wildcard)**: B-tree: 200-800ms with index, Hash: unavailable (full table scan: 2-10s). Common patterns like 'user%@domain.com' or partial searches require B-tree's range capabilities.\n\n**Profile Updates**: B-tree: 50-150ms with incremental updates, Hash: 200-500ms requiring periodic full rebuilds. B-tree maintains structure during high-volume updates (1000+ TPS).\n\n### Scalability Considerations\nAt 10M users, B-tree indexes use ~2-3GB memory vs hash's ~1.5-2GB, but B-tree's read performance degrades gracefully (logarithmic) while hash performance can collapse with high update rates. For chat applications needing both authentication and search functionality, B-tree provides the optimal balance of read efficiency, write performance, and query flexibility.\n\n### Edge Cases and Optimization\nConsider composite B-tree indexes on (email, last_login) for frequent queries and partial indexes for active users only, further reducing memory usage while maintaining performance.","diagram":"flowchart TD\n  A[Query: WHERE email = 'user@domain.com'] --> B{Index Type?}\n  B -->|Hash Index| C[O(1) Direct Lookup]\n  B -->|B-tree Index| D[O(log n) Tree Traversal]\n  E[Query: WHERE email LIKE 'user%'] --> F{Supported?}\n  F -->|Hash Index| G[❌ Full Table Scan]\n  F -->|B-tree Index| H[✅ Range Scan]\n  I[Query: ORDER BY email] --> J{Can Use Index?}\n  J -->|Hash Index| K[❌ Separate Sort]\n  J -->|B-tree Index| L[✅ Ordered Retrieval]","difficulty":"beginner","tags":["btree","hash-index","composite"],"channel":"database","subChannel":"indexing","sourceUrl":null,"videos":{"shortVideo":"https://www.youtube.com/watch?v=6fnmXX8RK0s","longVideo":"https://www.youtube.com/watch?v=a1Z40OC553Y"},"companies":["Amazon","Apple","Google","Meta","Microsoft","Netflix"],"eli5":"Imagine you have a giant box of 10 million toy cars and need to find specific ones quickly. A B-tree is like organizing them by color in rainbow order - you can find all red cars, or all cars from 'red' to 'blue', and even cars that start with 'r'. A hash index is like putting each car in a magic box that instantly gives you the exact car you ask for, but only if you know the exact name. For finding friends by email, the rainbow order helps when you want to find everyone whose email starts with 'john' or all emails between 'a' and 'm'. The magic box is faster when you know the exact email, but gets confused if you ask for partial matches. When kids update their profiles, the rainbow order stays neat while the magic box needs to be completely reorganized!","relevanceScore":null,"voiceKeywords":["b-tree index","hash index","range queries","concurrency","mvcc","covering indexes"],"voiceSuitable":true,"lastUpdated":"2025-12-29T08:05:59.955Z","createdAt":"2025-12-26 12:51:05"},{"id":"q-489","question":"You're designing a database for LinkedIn's feed system. Posts can be queried by user_id, created_at, and engagement_score. How would you optimize the indexing strategy for high-throughput reads and writes?","answer":"Use a composite B-tree index on (user_id, created_at DESC) for primary feed queries, complemented by a separate index on engagement_score for trending posts retrieval. For exact user_id lookups, consider a hash index for optimal performance. Implement covering indexes to avoid table scans and reduce I/O operations.","explanation":"## Index Selection\n- B-tree indexes excel at range queries with created_at ordering requirements\n- Hash indexes provide optimal performance for exact match scenarios\n- Composite indexes efficiently handle multi-column query patterns\n\n## Performance Considerations\n- Index cardinality directly influences query planner optimization\n- Write amplification occurs with multiple index maintenance\n- Balance memory consumption against query performance requirements\n\n## Scaling Strategies\n- Partition indexes by user_id ranges for improved distribution\n- Leverage covering indexes to eliminate unnecessary table scans\n- Monitor index fragmentation and schedule periodic rebuilds","diagram":"flowchart TD\n  A[Query Request] --> B{Query Type}\n  B -->|Feed by User| C[Composite Index user_id+created_at]\n  B -->|Trending Posts| D[Index engagement_score]\n  B -->|User Lookup| E[Hash Index user_id]\n  C --> F[Ordered Results]\n  D --> F\n  E --> F","difficulty":"advanced","tags":["btree","hash-index","composite"],"channel":"database","subChannel":"indexing","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","LinkedIn","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"lastUpdated":"2026-01-08T11:58:39.899Z","createdAt":"2025-12-26 12:51:06"},{"id":"q-572","question":"You're designing a database for a high-frequency trading system. When would you choose a B-tree index over a hash index for composite queries on (symbol, timestamp, price)? What are the specific performance implications?","answer":"Choose a B-tree index for composite queries on (symbol, timestamp, price) when you need range operations, ordered results, or prefix searches. B-tree efficiently handles timestamp ranges and ordered access patterns typical in trading systems, whereas hash indexes only support exact equality matches, making them unsuitable for timestamp-based range queries.","explanation":"## Index Selection Criteria\n\n- **B-tree advantages**: Range queries, ordered results, composite indexes, prefix searches\n- **Hash advantages**: Exact equality matches, faster point lookups  \n- **Trade-offs**: B-tree uses more space and has slower inserts; hash indexes are limited to equality operations\n\n## Performance Implications\n\n```sql\n-- B-tree efficiently handles:\nSELECT * FROM trades WHERE symbol='NVDA' AND timestamp BETWEEN '09:30' AND '16:00' ORDER BY price;\n\n-- Hash index cannot optimize:\nSELECT * FROM trades WHERE symbol='NVDA' AND timestamp > '09:30';\n```\n\n## Composite Index Strategy\n\n- Leading column should have high cardinality and be frequently filtered\n- For trading queries: index on (symbol, timestamp) optimizes symbol-specific time ranges\n- Additional price column supports ordering within the symbol-time window","diagram":"flowchart TD\n  A[Query Analysis] --> B{Range Queries?}\n  B -->|Yes| C[B-tree Index]\n  B -->|No| D{Equality Only?}\n  D -->|Yes| E[Hash Index]\n  D -->|No| C\n  C --> F[Ordered Access]\n  E --> G[Point Lookups]","difficulty":"advanced","tags":["btree","hash-index","composite"],"channel":"database","subChannel":"indexing","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"lastUpdated":"2026-01-08T11:53:27.862Z","createdAt":"2025-12-27T01:12:32.096Z"},{"id":"q-599","question":"When would you choose a composite index over multiple single-column indexes, and what are the trade-offs?","answer":"Composite indexes are optimal for queries that filter on multiple columns in a specific order, while single-column indexes work better for independent column queries.","explanation":"Composite indexes (multi-column indexes) are most effective when your queries frequently filter on multiple columns together. The index column order is crucial - it should match your WHERE clause conditions from most selective to least selective. For example, if you often query `WHERE user_id = ? AND created_at > ?`, a composite index on `(user_id, created_at)` would be highly efficient. The main trade-offs are: 1) Composite indexes consume more storage space than single indexes, 2) They're less flexible - they only help queries that use the leftmost columns in the correct order, 3) They can slow down INSERT/UPDATE operations more due to the larger index structure. Single-column indexes are better when columns are queried independently or when you need maximum flexibility. Real-world example: An e-commerce orders table might use a composite index on `(customer_id, order_date, status)` for customer order history queries, while maintaining separate indexes on `order_date` for reporting and `status` for order management queries.","diagram":null,"difficulty":"intermediate","tags":["indexing","performance","query-optimization","database-design"],"channel":"database","subChannel":"indexing-strategies","sourceUrl":null,"videos":null,"companies":["Amazon","Google","Meta","Microsoft","Netflix","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"lastUpdated":"2025-12-27T10:40:52.487Z","createdAt":"2025-12-27T10:40:52.487Z"},{"id":"q-618","question":"Explain the difference between clustered and non-clustered indexes and when you would choose each type. Provide a specific example scenario.","answer":"Clustered indexes sort and store data rows physically in the table, while non-clustered indexes store pointers to the data rows. Choose clustered for range queries and non-clustered for selective look","explanation":"Clustered indexes determine the physical order of data in a table, making them ideal for range queries and ordered result sets. Since there can only be one clustered index per table, it's typically placed on the most frequently accessed column like a primary key or date column. For example, in an orders table, a clustered index on order_date would efficiently retrieve all orders from a specific time period.\n\nNon-clustered indexes create a separate data structure that contains key values and pointers to the actual data rows. You can have multiple non-clustered indexes per table, making them suitable for selective lookups and covering different query patterns. For instance, in a users table, you might have non-clustered indexes on email, username, and last_name columns to support various search operations.\n\nThe choice depends on query patterns: use clustered indexes for columns frequently used in range scans, ORDER BY clauses, and JOIN operations. Use non-clustered indexes for columns used in WHERE clauses with high selectivity, or when you need to support multiple different access patterns without changing the physical data order.","diagram":null,"difficulty":"intermediate","tags":["database","indexing","performance","sql"],"channel":"database","subChannel":"indexing-strategies","sourceUrl":null,"videos":null,"companies":["Google","Microsoft","Amazon","Meta","Apple"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"lastUpdated":"2025-12-30T01:37:17.690Z","createdAt":"2025-12-30T01:37:17.690Z"},{"id":"da-129","question":"What is the main difference between SQL and NoSQL databases in terms of data structure?","answer":"SQL uses relational tables with predefined schemas enforcing data consistency, while NoSQL uses flexible data models (documents, key-values, graphs, columns) that adapt to evolving data structures.","explanation":"## SQL vs NoSQL Data Structure\n\n**SQL Databases (Relational):**\n- Store data in structured tables with rows and columns\n- Require predefined schemas that enforce data integrity and consistency\n- Support complex joins and ACID transactions\n- **Choose SQL when:** You need strong data consistency, complex relationships between entities, or structured reporting\n- **Examples:** Financial systems (banking applications), inventory management, customer relationship management\n- **Popular databases:** MySQL, PostgreSQL, Oracle, SQL Server\n\n**NoSQL Databases (Non-relational):**\n- Store data in flexible formats without rigid schemas:\n  - **Document stores** (JSON-like documents) - MongoDB, CouchDB\n  - **Key-value pairs** - Redis, DynamoDB\n  - **Column-family** - Cassandra, HBase\n  - **Graph databases** - Neo4j, Amazon Neptune\n- **Choose NoSQL when:** You need horizontal scaling, rapid development iterations, or handling unstructured/semi-structured data\n- **Examples:** Social media feeds, IoT sensor data, real-time analytics, content management systems\n- **Benefits:** Better performance for large-scale distributed systems, easier schema evolution, and handling diverse data types","diagram":"graph TD\n    A[Database Types] --> B[SQL Databases]\n    A --> C[NoSQL Databases]\n    \n    B --> D[Fixed Schema]\n    B --> E[Tables with Rows/Columns]\n    B --> F[ACID Compliance]\n    \n    C --> G[Flexible Schema]\n    C --> H[Multiple Data Models]\n    \n    H --> I[Document Store]\n    H --> J[Key-Value]\n    H --> K[Column-Family]\n    H --> L[Graph]\n    \n    I --> M[MongoDB]\n    J --> N[Redis]\n    K --> O[Cassandra]\n    L --> P[Neo4j]","difficulty":"beginner","tags":["nosql","mongodb"],"channel":"database","subChannel":"nosql","sourceUrl":null,"videos":{"shortVideo":"https://www.youtube.com/watch?v=eVApl3kzTB0","longVideo":"https://www.youtube.com/watch?v=uD3p_rZPBUQ"},"companies":["Amazon","Google","Microsoft","Netflix","Uber"],"eli5":"Imagine you're organizing your toys! SQL is like having special boxes with fixed spots - each box has places for exactly 3 cars, 2 dolls, and 5 blocks. Every toy must go in its correct spot! NoSQL is like a big play area where you can dump toys however you want - sometimes you make a pile of cars, other times you mix dolls and blocks together. You can even add new toy types anytime! SQL wants everything organized the same way, while NoSQL lets you play freely with your toys however you like.","relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"lastUpdated":"2025-12-28T02:05:20.616Z","createdAt":"2025-12-26 12:51:06"},{"id":"q-242","question":"How do MongoDB's document structure and SQL's table rows differ in handling user data with varying attributes, and what are the performance implications for common user operations?","answer":"MongoDB stores flexible JSON-like documents allowing varying schemas per user, ideal for profiles with optional fields. SQL uses fixed table rows with predefined columns, ensuring consistency but requiring schema migrations for new attributes. MongoDB excels at read-heavy user operations with embedded data, while SQL offers stronger ACID guarantees for transactional user updates.","explanation":"## Schema Flexibility\nMongoDB documents can have different structures:\n```javascript\n// User with social media links\n{\n  _id: ObjectId,\n  name: \"John\",\n  social: { twitter: \"@john\", linkedin: \"john-doe\" }\n}\n// User without social media\n{\n  _id: ObjectId,\n  name: \"Jane\"\n}\n```\nSQL requires NULL values or separate tables for optional fields.\n\n## Query Patterns\nMongoDB: `db.users.findOne({\"social.twitter\": \"@john\"})`\nSQL: `SELECT * FROM users WHERE twitter IS NOT NULL`\n\n## Performance Trade-offs\n- MongoDB: Faster reads for user profiles, slower complex joins\n- SQL: Better for analytical queries across user data, stronger consistency\n\n## When to Choose\nMongoDB: User profiles, content management, rapid iteration\nSQL: Financial data, user transactions, reporting systems","diagram":"graph TD\n    A[Application] --> B[MongoDB Collection]\n    A --> C[SQL Database]\n    \n    B --> D[Document 1<br/>name: 'John'<br/>email: 'john@ex.com'<br/>profile.age: 30]\n    B --> E[Document 2<br/>name: 'Jane'<br/>email: 'jane@ex.com'<br/>profile.age: 25<br/>profile.interests: ['coding']]\n    \n    C --> F[Users Table<br/>id | name | email | age]\n    C --> G[Interests Table<br/>user_id | interest]\n    \n    D --> H[Flexible Schema<br/>Nested Data]\n    E --> H\n    F --> I[Rigid Schema<br/>Normalized Data]\n    G --> I","difficulty":"beginner","tags":["mongodb","dynamodb","cassandra","redis"],"channel":"database","subChannel":"nosql","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":71,"voiceKeywords":null,"voiceSuitable":false,"lastUpdated":"2025-12-27T06:26:09.616Z","createdAt":"2025-12-26 12:51:07"},{"id":"q-331","question":"You're designing a multi-region e-commerce platform using DynamoDB. Your product catalog needs to support 10M items with eventual consistency across regions, but you must handle hot partitioning during flash sales. How would you design your partition key strategy and what trade-offs would you make between read performance and write throughput?","answer":"Use composite partition keys with sharding (product_id#hash) and adaptive capacity for hot items, trading some read latency for write scalability.","explanation":"## Why This Is Asked\nHashiCorp needs engineers who understand distributed database design at scale, especially for high-traffic scenarios. This tests knowledge of DynamoDB's partitioning model, hot key mitigation, and multi-region consistency trade-offs.\n\n## Expected Answer\nStrong candidates would discuss:\n- Composite partition keys with random suffixes to distribute load\n- Adaptive capacity for hot items during flash sales\n- Trade-offs between eventual consistency and read-after-write\n- Use of DynamoDB Accelerator (DAX) for read performance\n- Consideration of global tables vs. application-level replication\n\n## Code Example\n```typescript\n// Partition key strategy with sharding\nconst generatePartitionKey = (productId: string, shardCount: number = 10) => {\n  const hash = productId.split('').reduce((acc, char) => acc + char.charCodeAt(0), 0);\n  const shardId = hash % shardCount;\n  return `${productId}#shard${shardId}`;\n};\n\n// Item structure for hot item handling\ninterface CatalogItem {\n  PK: string; // product_id#shardX\n  SK: string; // METADATA\n  productId: string;\n  shardId: number;\n  // ... other attributes\n  hotItemFlag: boolean; // for adaptive capacity\n}\n```\n\n## Follow-up Questions\n- How would you handle schema evolution when adding new product categories?\n- What monitoring would you implement to detect hot partitioning before it impacts performance?\n- How would you design your backup and disaster recovery strategy?","diagram":"flowchart TD\n  A[Product Request] --> B[Generate Partition Key]\n  B --> C[product_id + hash]\n  C --> D{Hot Item?}\n  D -->|Yes| E[Enable Adaptive Capacity]\n  D -->|No| F[Standard Write]\n  E --> G[Write to DynamoDB]\n  F --> G\n  G --> H[Replicate to Global Tables]\n  H --> I[Read via DAX Cache]","difficulty":"advanced","tags":["mongodb","dynamodb","cassandra","redis"],"channel":"database","subChannel":"nosql","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","Meta","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":["dynamodb","partition key","sharding","eventual consistency","hot partitioning","adaptive capacity"],"voiceSuitable":true,"lastUpdated":"2025-12-27T04:58:34.068Z","createdAt":"2025-12-26 12:51:04"},{"id":"q-268","question":"How would you optimize a time-series analytics query that scans 100M+ rows across multiple date partitions in PostgreSQL when the WHERE clause cannot be pruned effectively due to complex temporal conditions?","answer":"Implement composite partitioning by date + user_id with BRIN indexes, create materialized views for common aggregations, use columnar storage via cstore_fdw, add query plan hints with SET enable_seqscan=off, and leverage parallel query with max_parallel_workers_per_gather=4. Combine this with partition-wise joins and incremental materialized view refresh for optimal performance.","explanation":"## Interview Context\nThis question assesses your ability to optimize large-scale PostgreSQL queries with real-world constraints like real-time dashboard requirements and inefficient WHERE clauses.\n\n## Technical Approach\n- **Composite Partitioning**: Date + user_id partitions reduce scan scope while maintaining query flexibility\n- **BRIN Indexes**: Block Range Indexes perfect for time-series data with natural ordering\n- **Materialized Views**: Pre-computed aggregations refreshed incrementally to avoid full rescans\n- **Columnar Storage**: TimescaleDB's compression reduces I/O for analytical workloads\n- **Parallel Execution**: Leverage multiple CPU cores for concurrent partition processing\n\n## Implementation Details\n```sql\n-- Create time-series table with TimescaleDB\nCREATE TABLE metrics (\n  timestamp TIMESTAMPTZ NOT NULL,\n  user_id BIGINT NOT NULL,\n  value NUMERIC NOT NULL\n) PARTITION BY RANGE (timestamp);\n\n-- Create BRIN index for efficient range scans\nCREATE INDEX idx_metrics_brin ON metrics USING BRIN (timestamp);\n\n-- Materialized view for common aggregations\nCREATE MATERIALIZED VIEW daily_metrics AS\nSELECT date_trunc('day', timestamp) as day,\n       user_id, AVG(value), COUNT(*)\nFROM metrics GROUP BY day, user_id;\n```\n\n## Follow-up Questions\n1. How would you handle partition pruning when the WHERE clause doesn't include the partition key?\n2. What metrics would you monitor to determine if your optimization is effective?\n3. How would you design the schema to handle both real-time and historical query patterns?","diagram":"flowchart TD\n    A[Query Request] --> B{Partition Strategy}\n    B -->|Composite| C[Time + Hash Partitioning]\n    B -->|Single Key| D[Range Partitioning Only]\n    \n    C --> E{Query Pattern}\n    E -->|Analytical| F[Materialized Views]\n    E -->|Real-time| G[Direct Table Scan]\n    \n    D --> H[All Partitions Scanned]\n    H --> I[Performance Degradation]\n    \n    F --> J[Pre-computed Aggregations]\n    G --> K[Parallel Execution]\n    J --> L[Fast Analytics]\n    K --> L","difficulty":"advanced","tags":["explain","query-plan","partitioning"],"channel":"database","subChannel":"query-optimization","sourceUrl":null,"videos":null,"companies":["Amazon","Google","Meta","Microsoft","Netflix"],"eli5":"Imagine you have a huge toy box with 100 million LEGOs scattered all over your room! You need to find all the red LEGOs you played with last week, but they're mixed with everything else. Instead of digging through the whole pile every time, you put toys in small boxes labeled by date and by which friend played with them. Now when you look for red LEGOs, you only check a few boxes! You also make special picture books showing your favorite toy combinations, so you don't have to rebuild them each time. You ask your friends to help search different boxes at the same time, like a team treasure hunt! This way, finding your toys becomes super fast and fun instead of taking forever!","relevanceScore":null,"voiceKeywords":["composite partitioning","brin indexes","materialized views","columnar storage","parallel query","partition-wise joins"],"voiceSuitable":true,"lastUpdated":"2025-12-27T04:58:51.805Z","createdAt":"2025-12-26 12:51:07"},{"id":"q-303","question":"How would you optimize a slow PostgreSQL query that joins 5 tables with millions of rows?","answer":"I would optimize the PostgreSQL query through a systematic approach: first analyze the execution plan using EXPLAIN ANALYZE to identify bottlenecks, then add appropriate indexes on foreign keys and frequently used join columns, and finally consider query rewriting techniques such as subquery optimization or join reordering.","explanation":"## Why Asked\nTests practical database optimization skills and understanding of PostgreSQL performance tuning in complex scenarios.\n\n## Key Concepts\nIndexing strategies, query execution plans, join algorithms, database statistics, and performance monitoring.\n\n## Code Example\n```sql\n-- Add composite index for frequent joins\nCREATE INDEX idx_orders_customer_date \nON orders(customer_id, order_date);\n\n-- Analyze query performance\nEXPLAIN ANALYZE SELECT * FROM orders o\nJOIN customers c ON o.customer_id = c.id\nWHERE o.order_date > '2023-01-01';\n```\n\n## Follow-up Questions\nWhat types of indexes would you use for different query patterns? How do you handle index bloat? When would you consider partitioning instead of indexing?","diagram":"flowchart TD\n  A[Slow Query] --> B[EXPLAIN ANALYZE]\n  B --> C[Identify Bottlenecks]\n  C --> D[Add Indexes]\n  D --> E[Rewrite Query]\n  E --> F[Test Performance]","difficulty":"advanced","tags":["joins","indexes","normalization","postgres"],"channel":"database","subChannel":"query-optimization","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"lastUpdated":"2025-12-30T01:44:52.447Z","createdAt":"2025-12-26 12:51:04"},{"id":"q-343","question":"You have a PostgreSQL table with 100M rows partitioned by date. A query filtering on a specific date range is still slow. What would you check in the EXPLAIN plan and how would you optimize it?","answer":"Check partition pruning effectiveness, index utilization patterns, and expensive sort operations. Create composite indexes on (date, filtered_columns) and evaluate clustering strategies for optimal data access.","explanation":"## Why This Is Asked\nTests practical query optimization skills, understanding of partitioning benefits, and ability to diagnose performance issues in large datasets.\n\n## Expected Answer\nStrong candidates would mention: 1) Verify partition pruning is working, 2) Check if indexes are being used vs sequential scans, 3) Look for expensive sorts or hash aggregates, 4) Consider composite indexes covering the WHERE clause, 5) Evaluate if the partition key is optimal for the query pattern.\n\n## Code Example\n```sql\n-- Check partition pruning\nEXPLAIN (ANALYZE, BUFFERS) SELECT * FROM events \nWHERE event_date BETWEEN '2024-01-01' AND '2024-01-31'\nAND status = 'completed';\n\n-- Add composite index\nCREATE INDEX CONCURRENTLY idx_events_date_status \nON events (event_date, status);\n\n-- Consider clustering\nCLUSTER events USING idx_events_date_status;\n```","diagram":"flowchart TD\n  A[Slow Query] --> B[Check EXPLAIN Plan]\n  B --> C{Partition Pruning?}\n  C -->|No| D[Fix Partition Key]\n  C -->|Yes| E{Index Usage?}\n  E -->|No| F[Add Composite Index]\n  E -->|Yes| G{Expensive Sorts?}\n  G -->|Yes| H[Cluster/Reorder]\n  G -->|No| I[Check Statistics]\n  D --> J[Optimized Query]\n  F --> J\n  H --> J\n  I --> J","difficulty":"intermediate","tags":["explain","query-plan","partitioning"],"channel":"database","subChannel":"query-optimization","sourceUrl":null,"videos":{"shortVideo":"https://www.youtube.com/watch?v=n2Fluyr3lbc","longVideo":"https://www.youtube.com/watch?v=sitUYx2EfhY"},"companies":["Affirm","Amazon","Google","Jane Street","Meta","Microsoft","Netflix","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"lastUpdated":"2026-01-01T06:40:35.027Z","createdAt":"2025-12-26 12:51:05"},{"id":"q-380","question":"You're optimizing a query that's slow due to a large time-series table. The query filters by timestamp range and device_id. How would you analyze the query plan and what partitioning strategy would you recommend?","answer":"Analyze the EXPLAIN plan to identify full table scans, then implement timestamp-based partitioning with a composite index on (timestamp, device_id).","explanation":"## Why This Is Asked\nTesla deals with massive time-series data from vehicles. This tests practical query optimization skills and understanding of how partitioning affects performance at scale.\n\n## Expected Answer\nA strong candidate would: 1) Use EXPLAIN ANALYZE to identify the bottleneck, 2) Notice the query isn't using the index effectively, 3) Recommend partitioning by timestamp ranges (daily/weekly), 4) Suggest a composite index, 5) Explain how partition pruning reduces scanned data.\n\n## Code Example\n```sql\n-- Analyze current performance\nEXPLAIN ANALYZE SELECT * FROM telemetry \nWHERE timestamp BETWEEN '2024-01-01' AND '2024-01-07'\n  AND device_id = 'vehicle_123';\n\n-- Create partitioned table\nCREATE TABLE telemetry (\n    id SERIAL,\n    device_id VARCHAR(50),\n    timestamp TIMESTAMP,\n    metrics JSONB,\n    PRIMARY KEY (id, timestamp)\n) PARTITION BY RANGE (timestamp);\n\n-- Create daily partitions\nCREATE TABLE telemetry_2024_01_01 PARTITION OF telemetry\n    FOR VALUES FROM ('2024-01-01') TO ('2024-01-02');\n\n-- Composite index for efficient filtering\nCREATE INDEX idx_telemetry_timestamp_device \n    ON telemetry (timestamp, device_id);\n```\n\n## Key Insights\n- Partition pruning reduces scanned data dramatically\n- Composite index order matters: timestamp first for partitioning, then device_id\n- Consider query patterns when choosing partition granularity","diagram":"flowchart TD\n  A[Query: SELECT * FROM telemetry] --> B[EXPLAIN ANALYZE]\n  B --> C{Full Table Scan?}\n  C -->|Yes| D[Identify Bottleneck]\n  C -->|No| E[Check Index Usage]\n  D --> F[Create Timestamp Partitions]\n  E --> F\n  F --> G[Add Composite Index]\n  G --> H[Partition Pruning Applied]\n  H --> I[Reduced I/O & Faster Query]","difficulty":"intermediate","tags":["explain","query-plan","partitioning"],"channel":"database","subChannel":"query-optimization","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hrt","Stripe","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"lastUpdated":"2026-01-08T11:40:16.800Z","createdAt":"2025-12-26 12:51:04"},{"id":"q-436","question":"You have a 100M row orders table with slow queries. The query plan shows sequential scans despite indexes on customer_id and order_date. How would you diagnose and fix this performance issue?","answer":"Begin by running EXPLAIN ANALYZE to identify why sequential scans are occurring despite existing indexes. If the planner continues choosing sequential scans, consider partitioning the table by date ranges or customer_id, and create composite indexes for common filter combinations. Additionally, analyze and update table statistics to ensure the query optimizer has accurate cardinality information.","explanation":"## Diagnosis\n- Run EXPLAIN ANALYZE to identify bottlenecks and understand execution plan choices\n- Check index selectivity and cardinality to determine why indexes aren't being used\n- Verify query patterns match the index column order and structure\n\n## Solutions\n- **Partitioning**: Implement range partitioning by order_date or hash partitioning by customer_id to reduce scan scope\n- **Indexing**: Create composite indexes (customer_id, order_date) for common filter combinations\n- **Statistics**: Run ANALYZE to update planner statistics and improve optimization decisions\n- **Configuration**: Increase work_mem for larger sort operations and complex queries\n\n## Trade-offs\n- Partitioning significantly improves query performance but adds operational complexity\n- Additional indexes increase write overhead but substantially improve read performance\n- Statistics maintenance requires regular updates for optimal query planning","diagram":"flowchart TD\n  A[Slow Query] --> B[EXPLAIN ANALYZE]\n  B --> C{Sequential Scan?}\n  C -->|Yes| D[Check Index Usage]\n  C -->|No| E[Optimize Join Order]\n  D --> F[Low Selectivity?]\n  F -->|Yes| G[Add Composite Index]\n  F -->|No| H[Consider Partitioning]\n  G --> I[Monitor Performance]\n  H --> I","difficulty":"advanced","tags":["explain","query-plan","partitioning"],"channel":"database","subChannel":"query-optimization","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Salesforce","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"lastUpdated":"2026-01-08T11:43:30.551Z","createdAt":"2025-12-26 12:51:05"},{"id":"q-546","question":"You're analyzing a slow query on a partitioned table. The EXPLAIN plan shows a full table scan instead of partition pruning. What could cause this and how would you fix it?","answer":"The issue likely occurs when the WHERE clause doesn't properly reference the partition key or uses non-sargable expressions that prevent the optimizer from identifying which partitions to scan. Ensure your query filters the partition column using exact matches or range predicates without applying functions or implicit conversions to it.","explanation":"## Common Causes\n- WHERE clause doesn't include the partition key\n- Functions applied to the partition column\n- Implicit data type conversions\n- Outdated table statistics\n- Parameter sniffing issues\n\n## Solutions\n- Rewrite queries to include the partition key in filter conditions\n- Remove functions from the partition column in WHERE clauses\n- Explicitly cast data types to avoid implicit conversions\n- Update statistics regularly with ANALYZE or UPDATE STATISTICS\n- Use OPTIMIZE FOR hints or local variables to mitigate parameter sniffing\n\n## Verification\n```sql\nEXPLAIN SELECT * FROM orders\nWHERE order_date >= '2024-01-01'\nAND customer_id = 123\n```","diagram":"flowchart TD\n  A[Query Analysis] --> B{Partition Key in WHERE?}\n  B -->|No| C[Add partition key filter]\n  B -->|Yes| D{Functions on column?}\n  D -->|Yes| E[Remove functions]\n  D -->|No| F[Check statistics]\n  F --> G[Update stats if needed]\n  G --> H[Verify partition pruning]","difficulty":"intermediate","tags":["explain","query-plan","partitioning"],"channel":"database","subChannel":"query-optimization","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Microsoft","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":["partition pruning","where clause","sargable","partition key","explain plan"],"voiceSuitable":true,"lastUpdated":"2026-01-08T11:54:54.500Z","createdAt":"2025-12-26 12:51:07"},{"id":"da-156","question":"What are the key differences between DELETE and TRUNCATE commands in SQL, including their impact on identity columns, foreign key constraints, and performance characteristics?","answer":"DELETE removes rows individually with logging, can use WHERE clauses, fires triggers, and respects foreign key constraints. TRUNCATE deallocates data pages instantly, resets identity columns, bypasses triggers, and requires table-level locks. TRUNCATE is faster but less flexible, while DELETE offers granular control with higher overhead.","explanation":"## Core Differences\n\n**DELETE**: Row-by-row removal with transaction logging, supports WHERE clauses, fires triggers, maintains identity values\n**TRUNCATE**: Bulk deallocation of data pages, resets identity columns, minimal logging, no WHERE clause support\n\n## Performance Impact\n\n- **DELETE**: Slower due to individual row logging and trigger execution\n- **TRUNCATE**: 10-100x faster by deallocating entire data pages\n\n## Constraint Handling\n\n- **DELETE**: Respects foreign key constraints, cascades properly\n- **TRUNCATE**: Cannot be used with foreign key references (requires disable/drop)\n\n## Identity Column Behavior\n\n```sql\n-- DELETE preserves identity\nDELETE FROM users WHERE id > 100;\n-- Next INSERT continues from last identity\n\n-- TRUNCATE resets identity\nTRUNCATE TABLE users;\n-- Next INSERT starts from seed value (usually 1)\n```\n\n## Use Cases\n\n**DELETE**: Selective data removal, audit trail maintenance, trigger-dependent operations\n**TRUNCATE**: Complete table reset, staging table cleanup, bulk data refresh scenarios\n\n## Database Variations\n\n- **PostgreSQL**: TRUNCATE supports CASCADE option\n- **SQL Server**: TRUNCATE cannot be used on tables with indexed views\n- **MySQL**: TRUNCATE resets AUTO_INCREMENT, DELETE does not\n\n## Locking Mechanisms\n\n- **DELETE**: Row-level locks, escalates to table/page locks\n- **TRUNCATE**: Exclusive table lock (X), but shorter duration","diagram":"graph TD\n    A[SQL Data Removal] --> B[DELETE]\n    A --> C[TRUNCATE]\n    B --> D[Row-by-row removal]\n    B --> E[WHERE clause supported]\n    B --> F[Slower, logged]\n    B --> G[Can rollback]\n    C --> H[Bulk removal]\n    C --> I[No WHERE clause]\n    C --> J[Faster, minimal log]\n    C --> K[Cannot rollback]\n    C --> L[Resets auto-increment]","difficulty":"beginner","tags":["sql","indexing"],"channel":"database","subChannel":"sql","sourceUrl":null,"videos":null,"companies":["Amazon","Google","Microsoft","Netflix","Oracle","Snowflake"],"eli5":"Imagine you have a big box of LEGO blocks! DELETE is like picking out one block at a time - you can be super picky and only remove the red ones if you want. But it takes time because you do it one by one. TRUNCATE is like dumping the whole box upside down - WHOOSH! All the blocks fall out at once. It's super fast, but you can't choose which blocks stay - they all go! So DELETE is careful and picky, while TRUNCATE is fast and takes everything!","relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"lastUpdated":"2025-12-25T16:38:58.155Z","createdAt":"2025-12-26 12:51:06"},{"id":"q-172","question":"Design a database failover strategy for a high-traffic e-commerce platform using primary-replica PostgreSQL. How would you ensure zero-downtime failover while maintaining data consistency during peak traffic of 10,000 requests/second?","answer":"Implement a comprehensive failover strategy using PgBouncer for connection pooling with automatic failover detection, continuous health checks every 5 seconds with 3-second timeouts, and write operation queuing with eventual consistency guarantees during transition periods.","explanation":"Interview Context: This SRE question evaluates system design expertise in database resilience and high availability architecture for mission-critical e-commerce platforms.\n\nTechnical Requirements:\n- Primary-replica PostgreSQL configuration with synchronous replication\n- PgBouncer connection pool with transaction pooling mode\n- Health checks every 5 seconds with 3-second timeout thresholds\n- Automatic failover completion within 30 seconds\n- Write operations queue management during failover transitions\n\nNon-Functional Requirements & Calculations:\n- Availability target: 99.99% (52.6 minutes maximum annual downtime)\n- Recovery Time Objective (RTO): <30 seconds\n- Recovery Point Objective (RPO): <1 second\n- Connection pool sizing: 100 connections (10% of peak traffic capacity)\n- Failover trigger threshold: 3 consecutive failed health checks\n- Write queue capacity limits to prevent memory exhaustion","diagram":"graph TD\n    A[Application] --> B[Connection Pool]\n    B --> C[Primary DB]\n    B --> D[Replica DB]\n    E[Chaos Engine] -->|Block Connection| C\n    B -->|Failover| D\n    F[Write Queue] -->|Queue During Failover| B\n    G[Monitoring] -->|Track Metrics| A\n    G -->|Track Metrics| B\n    G -->|Track Metrics| C\n    G -->|Track Metrics| D","difficulty":"intermediate","tags":["chaos","resilience"],"channel":"database","subChannel":"sql","sourceUrl":null,"videos":{"longVideo":"https://www.youtube.com/watch?v=4W-wiXkfoH8","shortVideo":"https://www.youtube.com/watch?v=4W-wiXkfoH8"},"companies":["Amazon","Bloomberg","Microsoft","Netflix","Uber"],"eli5":"Imagine you have two toy boxes - your main box and a backup box. You always play with your main box first, but if it gets lost, you need to use your backup box without losing any toys! To test this, we pretend your main toy box suddenly disappears. We watch to see if you automatically start using your backup box instead. We also make sure any new toys you were about to put in the main box get saved safely somewhere else first. This way, we know you'll never lose your toys, even if your favorite box goes missing!","relevanceScore":null,"voiceKeywords":["primary-replica","postgresql","connection pooling","automatic failover","health checks","zero-downtime","eventual consistency"],"voiceSuitable":true,"lastUpdated":"2026-01-08T11:26:47.311Z","createdAt":"2025-12-26 12:51:06"},{"id":"q-458","question":"You have a PostgreSQL database with orders (10M rows) and customers (1M rows). A query joining these tables is slow. How would you optimize it?","answer":"To optimize the slow PostgreSQL query joining orders (10M rows) and customers (1M rows), add indexes on the customer_id foreign key in the orders table and analyze the query execution plan with EXPLAIN ANALYZE to identify specific bottlenecks like missing indexes or inefficient join operations.","explanation":"## Key Optimization Strategies\n\n- **Indexing**: Create composite indexes on join columns and frequently filtered columns\n- **Query Analysis**: Use EXPLAIN ANALYZE to identify full table scans and expensive operations\n- **Join Strategy**: Ensure proper join order and consider hash joins for large datasets\n- **Partitioning**: Implement table partitioning by date or customer ranges for better performance\n\n## PostgreSQL Specific Techniques\n\n```sql\n-- Create composite index for join and filter\nCREATE INDEX idx_orders_customer_date \nON orders(customer_id, order_date);\n\n-- Analyze query performance\nEXPLAIN ANALYZE SELECT o.*, c.name \nFROM orders o JOIN customers c \nON o.customer_id = c.id \nWHERE o.order_date > '2023-01-01';\n```\n\n## Trade-offs\n\n- **Storage**: Indexes increase storage requirements\n- **Write Performance**: More indexes slow down INSERT/UPDATE operations\n- **Maintenance**: Regular VACUUM and ANALYZE needed for optimal performance","diagram":"flowchart TD\n  A[Slow Query] --> B[EXPLAIN ANALYZE]\n  B --> C[Identify Bottlenecks]\n  C --> D[Add Indexes]\n  D --> E[Optimize Join Order]\n  E --> F[Consider Partitioning]\n  F --> G[Monitor Performance]","difficulty":"intermediate","tags":["joins","indexes","normalization","postgres"],"channel":"database","subChannel":"sql","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Tesla","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"lastUpdated":"2026-01-06T04:04:25.605Z","createdAt":"2025-12-26 12:51:05"},{"id":"da-128","question":"You have a banking system where users can transfer money between accounts. Design a transaction to handle a transfer of $500 from Account A (balance: $1000) to Account B (balance: $200). What happens if the system crashes after debiting Account A but before crediting Account B? How would you ensure data consistency?","answer":"If the system crashes after debiting Account A but before crediting Account B, Account A would lose $500 while Account B doesn't receive it, creating an inconsistent state. To ensure data consistency, use a database transaction that atomically debits Account A and credits Account B, so both operations either complete together or roll back together if the system fails.","explanation":"## Database Transaction for Money Transfer\n\nThis scenario illustrates the critical importance of **ACID properties** in database transactions:\n\n### The Problem\nWithout proper transaction handling:\n1. Debit $500 from Account A (balance becomes $500)\n2. **System crashes here**\n3. Credit to Account B never happens\n4. **Result: $500 disappears from the system**\n\n### The Solution: ACID Transaction\n\n```sql\nBEGIN TRANSACTION;\n\n-- Check sufficient funds\nSELECT balance FROM accounts WHERE id = 'A' FOR UPDATE;\n\n-- Perform both operations atomically\nUPDATE accounts SET balance = balance - 500 WHERE id = 'A';\nUPDATE accounts SET balance = balance + 500 WHERE id = 'B';\n\nCOMMIT;\n```\n\n### ACID Properties Explained\n\n- **Atomicity**: Both operations succeed together or fail together\n- **Consistency**: Total money in system remains constant\n- **Isolation**: Other transactions can't see intermediate states\n- **Durability**: Once committed, changes survive system crashes\n\n### Additional Safeguards\n\n1. **Deadlock Prevention**: Always acquire locks in consistent order (e.g., by account ID)\n2. **Timeout Handling**: Set transaction timeouts to prevent indefinite locks\n3. **Retry Logic**: Implement exponential backoff for transient failures\n4. **Audit Trail**: Log all transaction attempts for reconciliation","diagram":"graph TD\n    A[Start Transaction] --> B[Lock Account A]\n    B --> C[Check Balance >= $500]\n    C -->|Yes| D[Debit $500 from Account A]\n    C -->|No| E[Rollback - Insufficient Funds]\n    D --> F[Credit $500 to Account B]\n    F --> G[Commit Transaction]\n    G --> H[Release Locks]\n    \n    D -->|System Crash| I[Automatic Rollback]\n    F -->|System Crash| I\n    I --> J[Both Accounts Restored]\n    \n    E --> K[Transaction Failed]\n    \n    style A fill:#e1f5fe\n    style G fill:#c8e6c9\n    style I fill:#ffcdd2\n    style E fill:#ffcdd2","difficulty":"intermediate","tags":["acid","transactions"],"channel":"database","subChannel":"transactions","sourceUrl":null,"videos":{"shortVideo":"https://www.youtube.com/watch?v=GAe5oB742dw","longVideo":"https://www.youtube.com/watch?v=pomxJOFVcQs"},"companies":["Amazon","Goldman Sachs","Google","Microsoft","Stripe"],"eli5":"Imagine you're trading cookies with your friend. You have 10 cookies and want to give 5 to your friend who has 2 cookies. You take 5 cookies from your jar first, but before putting them in your friend's jar, the lights go out! Now you only have 5 cookies left, but your friend still has only 2. The cookies are lost in the dark! To fix this, we use a magic box called a transaction. It's like having a grown-up watch the trade. If the lights go out midway, the magic box automatically puts everything back exactly where it started - you get your 5 cookies back, and your friend still has their 2. Either the whole trade finishes perfectly, or nothing changes at all. No cookies ever get lost in the dark!","relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"lastUpdated":"2026-01-07T13:09:41.449Z","createdAt":"2025-12-26 12:51:06"},{"id":"da-134","question":"You have a banking system where Account A transfers $100 to Account B, but during the transaction, Account B gets deleted by another process. The transfer uses READ COMMITTED isolation. What happens to the $100, and how would you prevent data inconsistency?","answer":"Money disappears into deleted account. Use SELECT FOR UPDATE or SERIALIZABLE isolation to prevent phantom reads and ensure referential integrity.","explanation":"## Transaction Isolation and Phantom Reads\n\nThis scenario demonstrates a **phantom read** problem in READ COMMITTED isolation:\n\n### What Happens:\n1. **Transaction T1** (transfer): Reads Account A balance, debits $100\n2. **Transaction T2** (deletion): Deletes Account B \n3. **Transaction T1**: Attempts to credit Account B - but it no longer exists\n4. **Result**: $100 vanishes from the system\n\n### Why READ COMMITTED Fails:\n- Only prevents **dirty reads** and **non-repeatable reads**\n- Does **NOT** prevent **phantom reads** (rows appearing/disappearing)\n- Account B's existence isn't locked during the transfer\n\n### Solutions:\n\n#### 1. Row-Level Locking\n```sql\nBEGIN;\nSELECT balance FROM accounts WHERE id = 'B' FOR UPDATE;\n-- This locks Account B, preventing deletion\nUPDATE accounts SET balance = balance - 100 WHERE id = 'A';\nUPDATE accounts SET balance = balance + 100 WHERE id = 'B';\nCOMMIT;\n```\n\n#### 2. SERIALIZABLE Isolation\n```sql\nSET TRANSACTION ISOLATION LEVEL SERIALIZABLE;\n-- Prevents all anomalies including phantom reads\n```\n\n#### 3. Application-Level Validation\n```sql\nBEGIN;\nIF NOT EXISTS (SELECT 1 FROM accounts WHERE id = 'B') THEN\n    ROLLBACK;\nEND IF;\n-- Proceed with transfer\nCOMMIT;\n```\n\n### Best Practice:\nUse **SELECT FOR UPDATE** on target accounts before any transfer to ensure atomicity and prevent phantom deletions.","diagram":"graph TD\n    A[Transaction T1: Transfer $100] --> B[Read Account A: $500]\n    A --> C[Read Account B: $200]\n    D[Transaction T2: Delete Account B] --> E[DELETE FROM accounts WHERE id='B']\n    B --> F[UPDATE Account A: $400]\n    C --> G[UPDATE Account B: ???]\n    E --> H[Account B Deleted]\n    G --> I[ERROR: Account B not found]\n    F --> J[Money Lost: $100 vanished]\n    \n    K[Solution: SELECT FOR UPDATE] --> L[Lock Account B]\n    L --> M[Prevent Deletion]\n    M --> N[Safe Transfer]","difficulty":"advanced","tags":["acid","transactions"],"channel":"database","subChannel":"transactions","sourceUrl":null,"videos":{"longVideo":"https://www.youtube.com/watch?v=tYG0akP7H-M"},"companies":["Amazon","Goldman Sachs","Google","Microsoft","Stripe"],"eli5":"Imagine you're giving $100 from your piggy bank to your friend Sarah. But just as you hand the money, Sarah disappears into a magic portal! Your money just vanishes because she's gone. That's what happens when Account B gets deleted during the transfer - the $100 goes poof! To prevent this, you could first grab Sarah's hand and promise you won't let go until she takes the money (that's like SELECT FOR UPDATE). Or you could make everyone stand in a line where no one can disappear until all trades are finished (that's SERIALIZABLE). This way, money never gets lost!","relevanceScore":null,"voiceKeywords":["transaction","isolation level","read committed","serializable","select for update","referential integrity","phantom reads"],"voiceSuitable":true,"lastUpdated":"2025-12-27T05:50:53.813Z","createdAt":"2025-12-26 12:51:06"},{"id":"da-170","question":"You're building a banking system where users can transfer money between accounts. How would you design the transaction handling to ensure no money is lost or created during transfers, especially when the system crashes mid-transfer?","answer":"Use ACID transactions with BEGIN, UPDATE accounts, COMMIT/ROLLBACK. Ensure atomicity by debiting source and crediting destination in single transaction.","explanation":"## Transaction Design for Money Transfers\n\n### Key Requirements\n- **Atomicity**: Both debit and credit must succeed or fail together\n- **Consistency**: Total money in system remains constant\n- **Isolation**: Concurrent transfers don't interfere\n- **Durability**: Completed transfers survive system crashes\n\n### Implementation Strategy\n\n```sql\nBEGIN TRANSACTION;\n\n-- Check sufficient funds\nSELECT balance FROM accounts WHERE id = :source_id FOR UPDATE;\n\n-- Debit source account\nUPDATE accounts \nSET balance = balance - :amount \nWHERE id = :source_id AND balance >= :amount;\n\n-- Credit destination account  \nUPDATE accounts \nSET balance = balance + :amount \nWHERE id = :dest_id;\n\n-- Verify both operations succeeded\nIF (rowcount_source = 1 AND rowcount_dest = 1) THEN\n    COMMIT;\nELSE\n    ROLLBACK;\nEND IF;\n```\n\n### Crash Recovery\n- **Before COMMIT**: Transaction is rolled back on restart\n- **After COMMIT**: Changes are durable due to write-ahead logging\n- **During COMMIT**: Database ensures atomic completion\n\n### Concurrency Control\n- Use row-level locks with `FOR UPDATE`\n- Implement deadlock detection and retry logic\n- Consider isolation levels (READ COMMITTED vs SERIALIZABLE)\n\n### Monitoring\n- Track transaction success/failure rates\n- Monitor lock contention and deadlock frequency\n- Audit trail for all financial transactions","diagram":"graph TD\n    A[Client Initiates Transfer] --> B[BEGIN TRANSACTION]\n    B --> C[LOCK Source Account]\n    C --> D[CHECK Balance >= Amount]\n    D --> E{Sufficient Funds?}\n    E -->|No| F[ROLLBACK - Return Error]\n    E -->|Yes| G[DEBIT Source Account]\n    G --> H[LOCK Destination Account]\n    H --> I[CREDIT Destination Account]\n    I --> J{Both Updates Successful?}\n    J -->|No| K[ROLLBACK - Return Error]\n    J -->|Yes| L[COMMIT Transaction]\n    L --> M[Release Locks]\n    M --> N[Return Success]\n    \n    O[System Crash] --> P{Crash Timing}\n    P -->|Before COMMIT| Q[Automatic ROLLBACK on Restart]\n    P -->|After COMMIT| R[Changes Preserved via WAL]\n    P -->|During COMMIT| S[Database Ensures Atomic Completion]","difficulty":"intermediate","tags":["acid","transactions"],"channel":"database","subChannel":"transactions","sourceUrl":null,"videos":{"longVideo":"https://www.youtube.com/watch?v=AcqtAEzuoj0"},"companies":["Amazon","Goldman Sachs","Google","Microsoft","Stripe"],"eli5":"Imagine you're trading toys with your friend. You give them your red car, and they give you their blue ball. But what if your mom calls you away in the middle? You don't want to lose your car AND not get the ball!\n\nSo you put both toys in a special magic box. The box only opens if BOTH toys are inside. If something goes wrong, the box closes and keeps everything safe - no toys lost!\n\nBank transfers work the same way. The money goes from your account to your friend's account inside a magic box. If the computer crashes, the box closes and puts the money back where it started. Nobody loses money and nobody gets extra money. It's all or nothing - just like toy trading should be!","relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"lastUpdated":"2025-12-21T13:16:36.003Z","createdAt":"2025-12-26 12:51:06"},{"id":"da-172","question":"In a distributed database system, how would you implement a two-phase commit protocol to ensure atomicity across multiple nodes, and what are the key failure scenarios and recovery mechanisms you must handle?","answer":"Implement coordinator managing prepare/commit phases with transaction logging. Handle node failures via timeout detection, force rollback on prepare failures, and use recovery logs to resolve uncertain states. Consider three-phase commit for enhanced fault tolerance.","explanation":"## Two-Phase Commit Implementation\n\n**Core Protocol:**\n- Coordinator sends prepare request to all participants\n- Participants write prepare log and acknowledge readiness\n- Coordinator collects responses, sends commit/abort\n- Participants finalize and write commit log\n\n**Failure Handling:**\n- **Node Crash:** Recovery log determines transaction state\n- **Network Partition:** Timeout triggers abort for uncertain states\n- **Coordinator Failure:** Participants block until recovery\n\n**Enhanced Approaches:**\n- **Three-Phase Commit:** Adds pre-commit phase to reduce blocking\n- **Paxos/Raft:** Consensus algorithms for coordinator election\n- **Timeout Strategies:** Exponential backoff with jitter\n\n**Performance Considerations:**\n- **Latency:** 2-3 round trips per transaction\n- **Throughput:** Limited by slowest participant\n- **Recovery Cost:** Log scanning and state reconstruction\n\n**Real-world Applications:**\n- **XA Protocol:** Standard for distributed transactions\n- **Google Spanner:** Uses TrueTime for global consistency\n- **CockroachDB:** Implements distributed ACID via Raft","diagram":"graph TD\n    A[Coordinator] -->|Prepare Request| B[Participant 1]\n    A -->|Prepare Request| C[Participant 2]\n    A -->|Prepare Request| D[Participant 3]\n    B -->|Vote-Commit| A\n    C -->|Vote-Commit| A\n    D -->|Vote-Commit| A\n    A -->|Global Commit| B\n    A -->|Global Commit| C\n    A -->|Global Commit| D\n    B -->|Ack| A\n    C -->|Ack| A\n    D -->|Ack| A\n    E[Failure Scenario] --> F[Coordinator Crash]\n    E --> G[Participant Timeout]\n    E --> H[Network Partition]","difficulty":"advanced","tags":["acid","transactions"],"channel":"database","subChannel":"transactions","sourceUrl":null,"videos":null,"companies":null,"eli5":"Imagine you and your friends are building a giant LEGO castle together. Before anyone adds their piece, everyone raises their hand to say 'I'm ready!' That's the first phase - checking if everyone can participate. Once all hands are up, you all say 'GO!' and everyone adds their piece at the same time. If someone's hand doesn't go up, or if they get distracted and don't add their piece when you say 'GO,' you have to start over. You also need a plan for when friends get called away for dinner mid-game or when someone can't hear you from across the playground. The two-phase commit is just like making sure everyone is ready before doing something important together!","relevanceScore":78,"voiceKeywords":null,"voiceSuitable":false,"lastUpdated":"2025-12-27T06:25:55.019Z","createdAt":"2025-12-26 12:51:06"},{"id":"db-2","question":"How do ACID properties ensure data integrity in a banking transaction where $100 is transferred from Account A to Account B?","answer":"Atomicity ensures all-or-nothing execution, Consistency maintains valid states, Isolation prevents interference, and Durability guarantees persistence.","explanation":"**Banking Transfer Scenario**: Account A transfers $100 to Account B\n\n• **Atomicity**: Either both the debit from Account A and the credit to Account B succeed, or both fail and rollback completely\n• **Consistency**: The database maintains a valid state—total money remains constant, and account balances never go negative\n• **Isolation**: Concurrent transactions see consistent snapshots—if Account C checks Account A's balance during the transfer, they see either the before or after state, never a partial state\n• **Durability**: Once the transaction commits, changes persist even through system crashes via Write-Ahead Logging","diagram":"graph TD\n    Start[Transfer $100: A → B] --> Atomic{Atomicity Check}\n    Atomic -->|Success| Consistent[Consistency Validation]\n    Atomic -->|Failure| Rollback[Complete Rollback]\n    Consistent --> Isolated[Isolation Control]\n    Isolated --> Durable[Durability Commit]\n    Durable --> Complete[Transaction Complete]\n    Rollback --> Failed[Transaction Failed]","difficulty":"intermediate","tags":["acid","transactions","theory"],"channel":"database","subChannel":"transactions","sourceUrl":null,"videos":{"longVideo":"https://www.youtube.com/watch?v=pomxJOFVcQs"},"companies":["Amazon","Goldman Sachs","Google","PayPal","Stripe"],"eli5":"Imagine you're moving your favorite toy from box A to box B. Atomicity means either the whole move happens or nothing at all - you can't leave the toy floating in between! Consistency is like making sure both boxes still follow the rules (no box gets too heavy or empty). Isolation is like having a private room where no one else can mess with your toys while you're moving them. Durability is like taking a picture to remember exactly where each toy ended up, even if the lights go out. Your $100 transfer works the same way - it either moves completely or not at all, keeping both accounts safe and correct!","relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"lastUpdated":"2026-01-08T11:31:31.585Z","createdAt":"2025-12-26 12:51:05"},{"id":"q-190","question":"What is the difference between READ COMMITTED and REPEATABLE READ isolation levels in database transactions, and how does MVCC implementation affect their behavior?","answer":"READ COMMITTED sees only committed data at query time, allowing non-repeatable reads. REPEATABLE READ uses MVCC snapshots to guarantee consistent reads within a transaction, preventing non-repeatable reads but still allowing phantom reads in most implementations like MySQL InnoDB.","explanation":"## Key Differences\n\n**READ COMMITTED**: Each query sees a fresh snapshot of committed data\n**REPEATABLE READ**: Single snapshot for entire transaction duration\n\n## MVCC Implementation\n\n- **PostgreSQL**: Both levels use MVCC, but REPEATABLE READ creates transaction-wide snapshot\n- **MySQL InnoDB**: REPEATABLE READ prevents most phantom reads through next-key locking\n- **SQL Server**: REPEATABLE READ uses range locks to prevent phantom reads\n\n## Performance Trade-offs\n\n- **READ COMMITTED**: Lower memory usage, better for long-running transactions\n- **REPEATABLE READ**: Higher memory for snapshot maintenance, potential lock contention\n\n## Real-world Scenarios\n\n**Banking**: REPEATABLE READ prevents balance changes during statement generation\n**Analytics**: READ COMMITTED preferred for real-time reporting with minimal locking\n\n## Edge Cases\n\n- **Hot rows**: REPEATABLE READ may cause lock escalation\n- **Long transactions**: Snapshot maintenance overhead increases with time\n- **Replication lag**: READ COMMITTED may show inconsistent data across replicas","diagram":"flowchart TD\n    A[Transaction Starts] --> B{Isolation Level}\n    B -->|READ COMMITTED| C[Query 1: Reads Committed Data]\n    B -->|REPEATABLE READ| D[Query 1: Takes Snapshot]\n    C --> E[Other Transaction Commits]\n    D --> E\n    E --> F{Query 2}\n    F -->|READ COMMITTED| G[Sees New Committed Data]\n    F -->|REPEATABLE READ| H[Sees Same Snapshot Data]\n    G --> I[Non-repeatable Read Possible]\n    H --> J[Consistent Read Guaranteed]","difficulty":"beginner","tags":["acid","isolation-levels","mvcc"],"channel":"database","subChannel":"transactions","sourceUrl":null,"videos":null,"companies":["Amazon","Databricks","Google","Microsoft","Oracle","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":["read committed","repeatable read","mvcc","non-repeatable reads","phantom reads","snapshots","innodb"],"voiceSuitable":true,"lastUpdated":"2025-12-27T04:58:18.873Z","createdAt":"2025-12-26 12:51:07"},{"id":"q-317","question":"Explain how MVCC (Multi-Version Concurrency Control) works and how it prevents lost updates in a database system?","answer":"MVCC creates multiple versions of data rows, allowing reads to proceed without blocking writes and preventing lost updates through version comparison.","explanation":"## Why Asked\nInterview context at Microsoft and similar companies tests understanding of database concurrency mechanisms and how they handle simultaneous operations.\n\n## Key Concepts\n- Snapshot isolation\n- Version chains\n- Transaction visibility\n- Write-write conflicts\n\n## Code Example\n```\n-- Transaction 1\nBEGIN TRANSACTION ISOLATION LEVEL REPEATABLE READ;\nUPDATE accounts SET balance = balance - 100 WHERE id = 1;\n\n-- Transaction 2 (concurrent)\nBEGIN TRANSACTION ISOLATION LEVEL REPEATABLE READ;\nUPDATE accounts SET balance = balance + 100 WHERE id = 2;\n\n-- MVCC prevents lost updates\n```\n\n## Follow-up","diagram":"flowchart TD\n  A[Start Transaction] --> B[Create Snapshot]\n  B --> C[Read from Version Chain]\n  C --> D[Write New Version]\n  D --> E[Commit Check]\n  E --> F[End]","difficulty":"intermediate","tags":["acid","isolation-levels","mvcc"],"channel":"database","subChannel":"transactions","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":"https://www.youtube.com/watch?v=pomxJOFVcQs"},"companies":["Microsoft","Plaid","Warner Bros"],"eli5":null,"relevanceScore":null,"voiceKeywords":["mvcc","multi-version concurrency control","lost updates","version comparison","row versioning","read consistency","non-blocking"],"voiceSuitable":true,"lastUpdated":"2025-12-30T01:45:52.909Z","createdAt":"2025-12-26 12:51:05"},{"id":"q-353","question":"You're building a collaborative design tool where multiple users can edit the same document simultaneously. How would you use database transactions and isolation levels to prevent conflicts while maintaining good performance?","answer":"Use MVCC with READ COMMITTED isolation, implement optimistic locking with version columns, and handle conflicts with retry logic.","explanation":"## Why This Is Asked\n\nTests understanding of concurrent access patterns, transaction isolation, and performance trade-offs - critical for Canva's collaborative editing features.\n\n## Expected Answer\n\nStrong candidates discuss: MVCC benefits, READ COMMITTED vs SERIALIZABLE trade-offs, optimistic vs pessimistic locking, conflict resolution strategies, and how to balance consistency with performance.\n\n## Code Example\n\n```typescript\n// Optimistic locking with version check\nasync function updateDocument(docId: string, changes: any, expectedVersion: number) {\n  const result = await db.transaction(async (tx) => {\n    const doc = await tx.query.documents.findFirst({\n      where: { id: docId, version: expectedVersion }\n    });\n    \n    if (!doc) {\n      throw new Error('Document not found or version mismatch');\n    }\n    \n    return await tx.update(documents)\n      .set({ ...changes, version: expectedVersion + 1 })\n      .where({ id: docId });\n  });\n  \n  return result;\n}\n\n// Conflict resolution with retry\nasync function updateWithRetry(docId: string, changes: any, maxRetries = 3) {\n  for (let attempt = 1; attempt <= maxRetries; attempt++) {\n    try {\n      const doc = await getDocument(docId);\n      return await updateDocument(docId, changes, doc.version);\n    } catch (error) {\n      if (attempt === maxRetries) throw error;\n      await delay(Math.pow(2, attempt) * 100); // Exponential backoff\n    }\n  }\n}\n```","diagram":"flowchart TD\n  A[User starts edit] --> B[Read document with version]\n  B --> C[Make changes locally]\n  C --> D[Attempt update with version check]\n  D --> E{Version matches?}\n  E -->|Yes| F[Update successful]\n  E -->|No| G[Conflict detected]\n  G --> H[Refresh and retry]\n  F --> I[End]\n  H --> B","difficulty":"beginner","tags":["acid","isolation-levels","mvcc"],"channel":"database","subChannel":"transactions","sourceUrl":null,"videos":{"shortVideo":"https://www.youtube.com/watch?v=pomxJOFVcQs","longVideo":"https://www.youtube.com/watch?v=qcInj-XW1Vc"},"companies":["Adobe","Amazon","Canva","Epic Games","Google","Meta","Microsoft","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"lastUpdated":"2025-12-30T01:45:07.615Z","createdAt":"2025-12-26 12:51:05"},{"id":"q-397","question":"In a high-transaction payment system using PostgreSQL, how would you design a transaction isolation strategy to prevent lost updates while maintaining high concurrency for account transfers?","answer":"Use SERIALIZABLE isolation with explicit row-level locking and retry logic for account transfers to prevent lost updates while maintaining concurrency.","explanation":"## Why This Is Asked\nPayPal needs to ensure financial transaction integrity under high load. This tests understanding of ACID properties, isolation levels, and practical database design for financial systems.\n\n## Expected Answer\nStrong candidates will discuss:\n- SERIALIZABLE vs REPEATABLE READ trade-offs\n- Explicit row-level locking with SELECT FOR UPDATE\n- Deadlock detection and retry mechanisms\n- Connection pooling and transaction timeout handling\n- Monitoring for serialization failures\n\n## Code Example\n```typescript\nasync function transferFunds(fromId: number, toId: number, amount: number) {\n  const maxRetries = 3;\n  \n  for (let attempt = 1; attempt <= maxRetries; attempt++) {\n    const tx = await db.transaction({ isolationLevel: 'serializable' });\n    \n    try {\n      // Lock both accounts to prevent concurrent modifications\n      const [fromAccount, toAccount] = await Promise.all([\n        tx.query('SELECT balance FROM accounts WHERE id = $1 FOR UPDATE', [fromId]),\n        tx.query('SELECT balance FROM accounts WHERE id = $1 FOR UPDATE', [toId])\n      ]);\n      \n      if (fromAccount.balance < amount) {\n        await tx.rollback();\n        throw new Error('Insufficient funds');\n      }\n      \n      await Promise.all([\n        tx.query('UPDATE accounts SET balance = balance - $1 WHERE id = $2', [amount, fromId]),\n        tx.query('UPDATE accounts SET balance = balance + $1 WHERE id = $2', [amount, toId])\n      ]);\n      \n      await tx.commit();\n      return;\n    } catch (error) {\n      await tx.rollback();\n      if (error.code === '40001' && attempt < maxRetries) {\n        // Serialization failure, retry with exponential backoff\n        await new Promise(resolve => setTimeout(resolve, Math.pow(2, attempt) * 100));\n        continue;\n      }\n      throw error;\n    }\n  }\n}\n```\n\n## Follow-up Questions\n- How would you handle distributed transactions across multiple databases?\n- What monitoring metrics would you track for transaction performance?\n- How would you optimize this for thousands of concurrent transfers?","diagram":"flowchart TD\n  A[Client Request Transfer] --> B[Begin Serializable Transaction]\n  B --> C[SELECT FROM Account FOR UPDATE]\n  C --> D[SELECT TO Account FOR UPDATE]\n  D --> E{Sufficient Balance?}\n  E -->|No| F[Rollback & Return Error]\n  E -->|Yes| G[UPDATE FROM Account Balance]\n  G --> H[UPDATE TO Account Balance]\n  H --> I{Serialization Failure?}\n  I -->|Yes| J[Rollback & Retry]\n  I -->|No| K[Commit Transaction]\n  J --> B\n  F --> L[End]\n  K --> L","difficulty":"advanced","tags":["acid","isolation-levels","mvcc"],"channel":"database","subChannel":"transactions","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google","Netflix","PayPal","Square","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":["serializable","transaction isolation","row-level locking","lost updates","concurrency","retry logic","postgresql"],"voiceSuitable":true,"lastUpdated":"2025-12-27T05:46:03.685Z","createdAt":"2025-12-26 12:51:04"},{"id":"q-428","question":"You're building a booking system for Airbnb where multiple users can reserve the same property simultaneously. How would you design the transaction handling to prevent double bookings while maintaining high availability?","answer":"Use SERIALIZABLE isolation with optimistic concurrency control. Implement row-level locks on property availability tables, use MVCC snapshot reads for checking availability, and apply application-leve","explanation":"## Problem Context\nAirbnb's booking system faces race conditions where multiple guests can book the same property simultaneously, leading to overbookings and customer dissatisfaction.\n\n## Technical Solution\n- **Database Design**: Separate availability calendar table with row-level locks\n- **Isolation Level**: SERIALIZABLE for critical booking operations\n- **Concurrency Pattern**: Optimistic locking with version columns\n- **Performance Strategy**: Read replicas for availability checks, write-through caching\n\n## Implementation Details\n- Use PostgreSQL's SELECT FOR UPDATE to lock specific date ranges\n- Implement retry logic with exponential backoff for serialization failures\n- Cache availability data with 5-minute TTL, invalidate on bookings\n- Use distributed transactions across booking and payment services\n\n## Trade-offs Considered\n- SERIALIZABLE ensures consistency but reduces throughput\n- Row-level locks prevent deadlocks better than table locks\n- Caching improves read performance but adds complexity","diagram":"flowchart TD\n  A[Guest Initiates Booking] --> B[Check Availability via Read Replica]\n  B --> C{Property Available?}\n  C -->|Yes| D[Begin Serializable Transaction]\n  C -->|No| E[Return Unavailable]\n  D --> F[SELECT FOR UPDATE on Property Dates]\n  F --> G[Re-check Availability]\n  G --> H{Still Available?}\n  H -->|Yes| I[Create Booking Record]\n  H -->|No| J[Rollback & Retry]\n  I --> K[Update Availability Calendar]\n  K --> L[Invalidate Cache]\n  L --> M[Commit Transaction]\n  M --> N[Send Confirmation]","difficulty":"intermediate","tags":["acid","isolation-levels","mvcc"],"channel":"database","subChannel":"transactions","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Amazon","Google","Meta","Microsoft","Netflix","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":["serializable isolation","optimistic concurrency","row-level locks","mvcc","high availability","transaction handling","double booking"],"voiceSuitable":true,"lastUpdated":"2025-12-27T04:55:16.138Z","createdAt":"2025-12-26 12:51:05"},{"id":"q-519","question":"You're designing a high-frequency trading system where transactions must see consistent data snapshots. How would you implement MVCC to handle concurrent reads while preventing write skew anomalies, and what isolation level would you choose?","answer":"Implement MVCC with tuple versioning using transaction IDs and visibility rules. Choose REPEATABLE READ or SERIALIZABLE isolation. Use snapshot timestamps, row-level version chains, and predicate locking.","explanation":"## MVCC Implementation\n\n- **Version Storage**: Each row stores xmin (creator) and xmax (deleter) transaction IDs\n- **Visibility Rules**: Transaction sees rows where xmin ≤ current_txid AND xmax is null/uncommitted\n- **Snapshot Management**: Track active transaction IDs at snapshot creation time\n\n## Isolation Strategy\n\n- **REPEATABLE READ**: Prevents non-repeatable reads, allows write skew\n- **SERIALIZABLE**: Full protection using SIREAD locks and conflict detection\n- **Write Skew Prevention**: Predicate locking on WHERE clauses or true serializable isolation\n\n## Performance Considerations\n\n- **Index Management**: Maintain separate visibility information for indexes to avoid full table scans\n- **Vacuum Processing**: Regular cleanup of dead tuples to prevent table bloat and maintain query performance\n- **Memory Overhead**: Balance snapshot retention time with memory usage for optimal throughput","diagram":"flowchart TD\n  A[Transaction T1 Starts] --> B[Creates Snapshot S1]\n  C[Transaction T2 Starts] --> D[Creates Snapshot S2]\n  B --> E[Reads Row v1]\n  D --> F[Reads Row v1]\n  E --> G[Updates Row to v2]\n  F --> H[Attempts Update]\n  G --> I[T1 Commits v2]\n  H --> J[Detects Conflict]\n  J --> K[T2 Rolls Back]","difficulty":"advanced","tags":["acid","isolation-levels","mvcc"],"channel":"database","subChannel":"transactions","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","Snap","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":["mvcc","tuple versioning","transaction ids","visibility rules","repeatable read","serializable","snapshot timestamps","row-level version chains","predicate lock"],"voiceSuitable":true,"lastUpdated":"2025-12-29T08:38:28.890Z","createdAt":"2025-12-26 12:51:06"}],"subChannels":["database","index-types","indexing","indexing-strategies","nosql","query-optimization","sql","transactions"],"companies":["Adobe","Affirm","Airbnb","Amazon","Anthropic","Apple","Bloomberg","Canva","Databricks","Discord","Epic Games","Gitlab","Goldman Sachs","Google","Hashicorp","Hrt","IBM","Jane Street","LinkedIn","Lyft","Meta","Microsoft","NVIDIA","Netflix","Oracle","Palantir","PayPal","Plaid","Salesforce","Slack","Snap","Snowflake","Square","Stripe","Tcs","Tempus","Tesla","Uber","Warner Bros"],"stats":{"total":37,"beginner":8,"intermediate":18,"advanced":11,"newThisWeek":2}}