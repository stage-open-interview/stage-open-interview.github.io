{"questions":[{"id":"q-1193","question":"You have an array A of n positive integers and a threshold T. For a given window length L, define ok(L) as: there exists a subarray of length L with sum <= T. Design an O(n) check for ok(L) using a sliding window, then outline how to find the maximum L with binary search over [1..n], and analyze total time and space. Include edge-case handling and practical optimizations?","answer":"Explain ok(L) in O(n) using a sliding window: compute the first L-sum, then slide by subtracting A[i-L] and adding A[i], stopping if a window sum <= T exists. Binary search L in [1..n] for the max L; ","explanation":"## Why This Is Asked\n\nThis question blends sliding-window technique with binary search to analyze a threshold problem, testing practical algorithm design and reasoning about complexity.\n\n## Key Concepts\n\n- Sliding window for fixed-length subarray sums\n- Binary search over the answer space\n- Time/space trade-offs and edge-case handling\n- Practical optimizations (early exit, optional prefix sums)\n\n## Code Example\n\n```javascript\nfunction maxLWithSumAtMost(A, T) {\n  const n = A.length;\n  const ok = (L) => {\n    let sum = 0;\n    for (let i = 0; i < L; i++) sum += A[i];\n    if (sum <= T) return true;\n    for (let i = L; i < n; i++) {\n      sum += A[i] - A[i - L];\n      if (sum <= T) return true;\n    }\n    return false;\n  };\n  let lo = 1, hi = n, ans = 0;\n  while (lo <= hi) {\n    const mid = (lo + hi) >> 1;\n    if (ok(mid)) { ans = mid; lo = mid + 1; } else { hi = mid - 1; }\n  }\n  return ans;\n}\n```\n\n## Follow-up Questions\n\n- How would the solution change if elements could be negative?\n- What are the memory/speed trade-offs if you precompute prefix sums?\n- How would you adapt for multiple thresholds T1..Tk in parallel?","diagram":null,"difficulty":"beginner","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Snowflake","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T04:42:46.129Z","createdAt":"2026-01-13T04:42:46.129Z"},{"id":"q-1254","question":"You're given a DAG G=(V,E) with N nodes and M edges. Edges can be inserted online in batches of size B. Design a dynamic transitive-closure using bitsets to answer reachability queries in O(1). Provide initialization, amortized per-batch update time, and memory. Include two practical optimizations and compare to recomputing the closure after each batch?","answer":"Adopt a reachability bitset per node reach[u] over all nodes, with a topological order. For a batch of insertions, process edges in topo order; if v not in reach[u], do reach[u] |= reach[v] and push t","explanation":"## Why This Is Asked\n\nAssesses dynamic graph reasoning, bitset optimizations, and amortized analysis in a practical DAG setting.\n\n## Key Concepts\n\n- Bitset reachability per node\n- Topological ordering reuse\n- Incremental updates and fixed-point propagation\n- Amortized cost and memory trade-offs\n\n## Code Example\n\n```javascript\n// Pseudocode for dynamic reachability with bitsets\nfor edge (u,v) in batch in topo order {\n  if (!(reach[u] has v)) {\n    reach[u] |= reach[v]\n    // propagate if new bits were added\n  }\n}\n```\n\n## Follow-up Questions\n\n- How would deletions affect the structure?\n- Analyze worst-case batch patterns that maximize work.","diagram":null,"difficulty":"intermediate","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Square","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T06:47:01.803Z","createdAt":"2026-01-13T06:47:01.803Z"},{"id":"q-2159","question":"Scenario: In a weighted directed graph with nonnegative weights, you must maintain approximate betweenness centrality for all nodes under batches of edge insertions of size B. Propose a concrete, implementable scheme that (i) initializes estimates, (ii) updates after each batch with amortized cost, (iii) bounds memory, and (iv) provides guaranteed error bounds ε. Include two practical optimizations and compare to re-running Brandes' algorithm after every batch?","answer":"Maintain approximate betweenness via dynamic sampling of source–target pairs. Precompute a fixed set of sample shortest paths; store their node visit counts. After a batch of B insertions, only paths ","explanation":"## Why This Is Asked\nTests understanding of dynamic graph algorithms, sampling for approximations, and amortized analysis in batch-update settings.\n\n## Key Concepts\n- Dynamic graphs with batch updates\n- Approximate centrality with ε guarantees\n- Sampling-based path tracking and incremental updates\n- Trade-offs: time, memory, and rebuild frequency\n\n## Code Example\n```javascript\n// Sketch: updateBatch(graph, batch, samples, eps) { /* adjust sample counts for new edges */ }\n```\n\n## Follow-up Questions\n- How to bound error growth with insertions?\n- How to select number of samples s and batch size B for a target ε?","diagram":null,"difficulty":"intermediate","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","Snowflake","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T05:39:09.336Z","createdAt":"2026-01-15T05:39:09.337Z"},{"id":"q-678","question":"In a directed acyclic graph with N nodes and M edges, all edge costs are nonnegative. Compute the minimum-cost path from S to T. Costs can decrease online; design a strategy to maintain shortest paths with updates, aiming for sublinear re-computation on average. Provide initial complexity and amortized update complexity, plus memory usage and practical optimizations?","answer":"Initial: run a topological DP in O(N+M) to obtain dist to all nodes. Online decreases trigger relaxations: if w(u,v) decreases and dist[u]+w(u,v) < dist[v], update dist[v] and propagate to successors ","explanation":"## Why This Is Asked\n\nTests online update handling, DAG shortest paths, and amortized analysis.\n\n## Key Concepts\n\n- DAG shortest paths via topological order\n- Dynamic relaxations under decreasing edge weights\n- Amortized analysis and worst-case bounds\n\n## Code Example\n\n```javascript\nfunction updateShortestPaths(n, adj, dist, s, t, u, v, wOld, wNew){\n  // assume we know wOld and wNew with a decrease; perform relaxation\n  if(dist[u] + wNew < dist[v]){\n    dist[v] = dist[u] + wNew;\n    // push v to a queue and relax its outgoing edges\n  }\n  // ... full propagation would run until convergence\n}\n```\n\n## Follow-up Questions\n\n- How would you adapt for non-DAG graphs?\n- How does update order affect performance in practice?","diagram":"flowchart TD\n  A[Initial topological DP] --> B[Edge decrease detected]\n  B --> C[Relax dist[v]]\n  C --> D[Propagate changes]\n  D --> E[Converged]","difficulty":"advanced","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","Meta","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-11T15:56:43.115Z","createdAt":"2026-01-11T15:56:43.115Z"},{"id":"q-690","question":"Design a data structure to support two online operations on an integer array A of length N: 1) rangeAdd(l, r, delta) adds delta to A[i] for l <= i <= r, 2) queryMaxSubarray() returns the maximum subarray sum of the current A. Provide a structure that supports both operations in O(log N) time, describe what to store per node, how to merge children, and how to apply a lazy add. Include correctness and complexity considerations?","answer":"Use a segment tree with lazy propagation. Each node stores: total sum, max prefix, max suffix, and max subarray. To add v to a range, lazily update the node: adjust sum, pref, suff, and propagate lazi","explanation":"## Why This Is Asked\nTests ability to combine range updates with non-linear objective (max subarray) and shows knowledge of segment trees, lazy propagation, and invariants under updates.\n\n## Key Concepts\n- Segment tree with lazy propagation\n- Node values: sum, max prefix, max suffix, max subarray\n- Merge operation correctness\n- Complexity analysis: O(log N) per op, O(N) memory\n- Edge cases: all negative arrays, large deltas, non-overlapping ranges\n\n## Code Example\n```javascript\nclass Node { constructor(sum=0,pref=-Infinity,suff=-Infinity,best=-Infinity){ this.sum=sum; this.pref=pref; this.suff=suff; this.best=best; } }\n```\n\n## Follow-up Questions\n- How would you adapt if rangeAdd is replaced with rangeMultiply?\n- How would you extend to support range assign and query across multiple segments?","diagram":null,"difficulty":"intermediate","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-11T16:23:52.242Z","createdAt":"2026-01-11T16:23:52.242Z"},{"id":"q-700","question":"You're building a real-time analytics dashboard that shows the top-k most frequent event types from a high-volume log stream (e.g., clicks, errors). Each event has a string type. Design a data structure and algorithm to maintain the current top-k frequencies with online increments, aiming for roughly O(log k) update time and O(n) memory. Explain how you handle ties and memory growth, and compare with a naive approach that re-sorts after every insert?","answer":"Use a hash map to count frequencies and a min-heap of size k to track current top-k. On event type t: increment freq[t]; if t is in heap, update its priority; else if heap size < k, insert (freq[t], t","explanation":"## Why This Is Asked\nThis probes real-time top-k maintenance and amortized analysis using simple DS.\n\n## Key Concepts\n- HashMap for counts\n- Min-heap for top-k\n- Tie-breaking rules\n- Memory-use trade-offs\n\n## Code Example\n```javascript\nfunction update(freq, heap, k, t) {\n  freq[t] = (freq[t] || 0) + 1;\n  if (heap.has(t)) adjust(heap, t);\n  else if (heap.size < k) heap.push([freq[t], t]);\n  else if (freq[t] > heap.min()[0]) heap.replace([freq[t], t]);\n}\n```\n\n## Follow-up Questions\n- How would you handle deletions or aging counts?\n- How would you test correctness with sequences simulating bursts?","diagram":null,"difficulty":"beginner","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-11T17:19:43.376Z","createdAt":"2026-01-11T17:19:43.376Z"},{"id":"q-704","question":"Scenario: a directed graph with nonnegative weights and a fixed source S. Each batch updates up to B edges (increases or decreases). Propose a practical dynamic data structure to maintain exact distances from S to all nodes and answer distance queries S→T in polylog time, with sublinear amortized update time. Compare to rerunning Dijkstra after every batch; include expected bounds, memory usage, and practical heuristics?","answer":"Maintain an exact SSSP with a dynamic SPT plus a limited local reoptimization. For a batch of up to B edge-weight changes, re-evaluate only the affected subtree via a replacement-path frontier and rew","explanation":"## Why This Is Asked\nAssesses skill in dynamic graph algorithms, not just static shortest paths; requires designing practical online maintenance with bounds.\n\n## Key Concepts\n- Dynamic shortest paths\n- Replacement paths and locality\n- Batch updates and amortized analysis\n- Potentials for nonnegative weights\n- SPT maintenance\n\n## Code Example\n```javascript\n// Skeleton: dynamic SSSP maintenance interface\nclass DynamicSSSP {\n  constructor(graph, s) { /* ... */ }\n  batchUpdate(changes) { /* changes: Array<{u,v,wDelta}> */ }\n  distanceTo(v) { /* return dist from s to v */ }\n}\n```\n\n## Follow-up Questions\n- How to adapt when B ≈ M or updates are adversarial?\n- How would you extend to multiple sources and dynamic graph rebuilds?","diagram":null,"difficulty":"advanced","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Salesforce","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-11T18:27:46.453Z","createdAt":"2026-01-11T18:27:46.453Z"},{"id":"q-709","question":"Given a directed graph with nonnegative weights, a fixed source S, and a stream of online edge weight updates (both increases and decreases), design a dynamic SSSP data structure that maintains exact distances dist(S, v) for all v after each update. Aim for sublinear amortized update time per edge change; specify initial preprocessing, worst-case vs amortized bounds, memory usage, and practical optimizations. Provide a plan for applying this in a traffic-graph scenario with frequent but localized updates?","answer":"Maintain dist from S to all nodes with a dynamic SSSP structure: run full Dijkstra once to initialize (O(M log N)). For each edge weight update (u, v, δ), re-relax only nodes affected by the change us","explanation":"## Why This Is Asked\nDynamic maintenance of shortest paths under online updates is critical in production traffic graphs. Candidates must reason about amortized analysis, locality, and practical heuristics beyond static re-computation.\n\n## Key Concepts\n- Dynamic shortest paths\n- Amortized analysis\n- Localized relaxation\n- Priority queues and delta updates\n- Trade-offs: exactness vs approximate\n\n## Code Example\n```javascript\n// Pseudo approach sketch: re-relaxation function\nfunction updateEdge(u,v,delta){\n  // update weight\n  // if newDist improves dist[v], push to PQ and propagate\n}\n```\n\n## Follow-up Questions\n- How would you handle multiple concurrent updates efficiently?\n- What metrics would you monitor in production?","diagram":null,"difficulty":"intermediate","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["OpenAI","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-11T19:15:05.608Z","createdAt":"2026-01-11T19:15:05.608Z"},{"id":"q-721","question":"Given a fixed directed graph with nonnegative weights and a single source S, handle a batch of edge-weight decreases (no insertions/deletions). Design a dynamic algorithm to update the exact S→v distances after each batch with sublinear amortized per-edge cost. Specify data structures, provide an amortized bound, and discuss memory and practical optimizations for real-time traffic networks?","answer":"Use dist[] and a min-heap. For each decreased edge (u,v) to w', if dist[u]+w' < dist[v], set dist[v] = dist[u]+w' and push v; repeatedly pop and relax neighbors until the queue empties. This confines ","explanation":"## Why This Is Asked\nDynamic SSSP with weight-decreases mirrors real-time route/latency updates. This tests practical algorithm design, data-structure choices, and amortized reasoning for production graphs.\n\n## Key Concepts\n- Dynamic shortest paths under monotone updates\n- Dijkstra-like propagation with lazy updates\n- Priority queues vs bucketed approaches for integer weights\n- Amortized analysis based on updated edges and affected nodes\n- Memory footprint: O(N+M)\n\n## Code Example\n```javascript\n// Pseudocode for batch update\nfunction updateBatch(changes) {\n  for (const {u,v,w} of changes) {\n    if (dist[u] + w < dist[v]) {\n      dist[v] = dist[u] + w;\n      pq.push(v);\n    }\n  }\n  while (!pq.isEmpty()) {\n    const x = pq.pop();\n    for (const e of adj[x]) {\n      if (dist[x] + e.w < dist[e.v]) {\n        dist[e.v] = dist[x] + e.w; pq.push(e.v);\n      }\n    }\n  }\n}\n```\n\n## Follow-up Questions\n- How would you adapt for batches that include edge insertions/deletions?\n- Compare binary-heap vs Dial's bucket approach for integer weights in [0, W].","diagram":"flowchart TD\n  S[Source]\n  D[Decrease batch]\n  P[Relax candidates]\n  Q[Queue updates]\n  E[Edge relaxations]\n  S --> D\n  D --> P\n  P --> Q\n  Q --> E\n  E --> P","difficulty":"intermediate","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","NVIDIA","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-11T20:23:01.260Z","createdAt":"2026-01-11T20:23:01.260Z"},{"id":"q-732","question":"Scenario: A data stream yields integers. At each time step, a new value enters a sliding window of fixed size W, and the oldest value leaves. Design a solution to maintain the top-2 most frequent values in the current window with fast updates. Compare a naive O(W) recompute to an augmented structure using a frequency map and a max-heap with lazy deletions. Provide update and query complexities and memory usage?","answer":"Naive: on each slide recompute the top-2 by scanning the W elements, O(W) per step. Improved: maintain a frequency map plus a max-heap with lazy deletions. Updates are O(log D) where D is distinct val","explanation":"## Why This Is Asked\n\nTests familiarity with sliding-window problems and practical complexity trade-offs between recomputation and incremental data structures.\n\n## Key Concepts\n\n- Sliding window\n- Frequency counting\n- Hash map and max-heap\n- Lazy deletion\n\n## Code Example\n\n```javascript\nclass TopKWindow {\n  constructor(W){ this.W=W; this.freq=new Map(); this.window=[]; this.maxHeap=[]; }\n  // ... implement add, slide, and getTopK using lazy deletions\n}\n```\n\n## Follow-up Questions\n\n- How would the approach scale for larger k than 2?\n- How would you adapt for non-integer or large-value domains?","diagram":null,"difficulty":"beginner","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Twitter","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-11T21:19:58.623Z","createdAt":"2026-01-11T21:19:58.623Z"},{"id":"q-740","question":"Scenario: An edge CDN collects response times in milliseconds for every request. Design a beginner-friendly online algorithm to maintain the median latency as new times arrive, using only inserts. Explain the data structure, update steps, and time/space bounds, assuming up to 1e6 entries?","answer":"Use two heaps: a max-heap for the lower half and a min-heap for the upper half. On each new latency x, push to the appropriate heap and rebalance so sizes differ by at most 1. Median is the top of the","explanation":"## Why This Is Asked\n\nTests ability to design a stable online statistic with clear time/space guarantees using simple data structures.\n\n## Key Concepts\n\n- streaming algorithms\n- two-heap median maintenance\n- amortized vs worst-case costs\n- space efficiency\n\n## Code Example\n\n```javascript\n// production-ready sketch of two-heap median tracker\nclass BinaryHeap {\n  constructor(compare){ this.data=[]; this.compare=compare; }\n  size(){ return this.data.length; }\n  top(){ return this.data[0]; }\n  push(x){ this.data.push(x); this._siftUp(this.data.length-1); }\n  pop(){ const top=this.data[0]; const end=this.data.pop(); if (this.data.length){ this.data[0]=end; this._siftDown(0); } return top; }\n  _siftUp(i){ const x=this.data[i]; while(i>0){ const p=(i-1)>>1; if (this.compare(x, this.data[p])>=0) break; this.data[i]=this.data[p]; i=p; } this.data[i]=x; }\n  _siftDown(i){ const n=this.data.length; const x=this.data[i]; while(true){ let l=2*i+1, r=l+1, smallest=i; if (l<n && this.compare(this.data[l], this.data[smallest])<0) smallest=l; if (r<n && this.compare(this.data[r], this.data[smallest])<0) smallest=r; if (smallest===i) break; this.data[i]=this.data[smallest]; i=smallest; } this.data[i]=x; }\n}\n\nclass MedianTracker {\n  constructor(){ this.lower=new BinaryHeap((a,b)=> b-a); this.upper=new BinaryHeap((a,b)=> a-b); }\n  insert(x){ if (this.lower.size()===0 || x <= this.lower.top()) this.lower.push(x); else this.upper.push(x);\n    if (this.lower.size() > this.upper.size()+1) this.upper.push(this.lower.pop());\n    else if (this.upper.size() > this.lower.size()) this.lower.push(this.upper.pop());\n  }\n  median(){ if (this.lower.size() === this.upper.size()) return (this.lower.top() + this.upper.top())/2; return this.lower.top(); }\n}\n```\n\n## Follow-up Questions\n\n- How would you adapt for deletions or outliers removal?\n- How would you validate correctness with unit tests and random streams?","diagram":"flowchart TD\n  S[Stream of latencies] --> I[Insert into appropriate heap]\n  I --> R[Rebalance if needed]\n  R --> M[Median available via tops]\n  M --> Q[Query median in O(1)]","difficulty":"beginner","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Coinbase","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-11T22:21:49.038Z","createdAt":"2026-01-11T22:21:49.038Z"},{"id":"q-742","question":"In a DAG with N nodes and M edges, nonnegative edge weights. You maintain shortest-path distances from source S to a fixed set of target nodes {T1,...,Tk}. Edge weights can only decrease over time due to updates. After a batch of updates, you should update only the target distances that can improve, avoiding full re-relaxation. Propose a practical algorithm that lazily propagates decreases using the DAG’s topological order, such that total work across updates is sublinear on average. Provide update and query steps, concrete time bounds, and memory usage, plus optimizations?","answer":"Maintain dist from S to all nodes in topological order. On a batch of weight decreases (u->v with newW <= oldW): if dist[v] > dist[u] + newW, update and push v. Pop nodes in increasing topo index, rel","explanation":"## Why This Is Asked\nTests ability to exploit DAG structure for incremental shortest paths with monotone updates and selective recomputation.\n\n## Key Concepts\n- DAG topological order\n- Monotone updates and lazy propagation\n- Selective relaxation focused on targets\n- Amortized analysis of edge relaxations\n\n## Code Example\n```javascript\n// Pseudocode for batch decreases\nfunction applyDecreases(batch) {\n  const queue = [];\n  for (const e of batch) {\n    if (dist[e.v] > dist[e.u] + e.newW) {\n      dist[e.v] = dist[e.u] + e.newW;\n      queue.push(e.v);\n    }\n  }\n  queue.sort((a,b)=>topo[a]-topo[b]);\n  while (queue.length) {\n    const x = queue.shift();\n    for (const y of adj[x]) {\n      const w = weight(x,y);\n      if (dist[y] > dist[x] + w) {\n        dist[y] = dist[x] + w;\n        queue.push(y);\n      }\n    }\n  }\n}\n``` \n\n## Follow-up Questions\n- How would you handle dynamic target set changes efficiently?\n- What edge cases degrade the amortized bound and how could you mitigate them?","diagram":"flowchart TD\n  A[Batch Update] --> B[Relax Affected Nodes]\n  B --> C[Distance Stable]\n  C --> D[Answer Queries for Tk]\n  A --> E[Push Victims to Queue]\n  E --> B","difficulty":"advanced","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Microsoft","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-11T23:17:59.814Z","createdAt":"2026-01-11T23:17:59.814Z"},{"id":"q-755","question":"You're maintaining real-time travel times in a citywide road network modeled as a weighted directed graph with nonnegative costs. Costs can only decrease as new data arrives. Design an incremental algorithm to keep the shortest-path distances from a fixed hub S to all nodes up-to-date after each edge-cost decrease, aiming for sublinear amortized update work. Provide initial SSSP complexity, amortized per-decrease update, memory usage, and practical optimization strategies?","answer":"Compute initial SSSP with Dijkstra: O(M log N). For a decreased edge (u,v) with w' < w, if dist[u] + w' < dist[v], update dist[v] and propagate relaxations through the graph using a PQ. Amortized work","explanation":"## Why This Is Asked\n\nTests ability to extend classic shortest-paths to dynamic updates with a concrete, real-world constraint (only decreases) and to analyze practical performance in terms of amortized work and memory.\n\n## Key Concepts\n\n- Dynamic single-source shortest paths with only edge decreases\n- Amortized analysis and potential-based arguments\n- Priority-queue relaxation propagation\n- Memory-time trade-offs and localized updates\n- Practical optimizations: skip non-on-SPT edges, batch updates, integer weights with Dial's algorithm\n\n## Code Example\n\n```javascript\nclass DynamicSSSP {\n  constructor(graph, source) {\n    this.graph = graph; // adjacency: {u: [{v, w}, ...]}\n    this.n = Object.keys(graph).length;\n    this.dist = Array(this.n).fill(Infinity);\n    this.dist[source] = 0;\n    this.pq = new MinHeap(); // custom min-heap with push/pop\n    this.pq.push([0, source]);\n    while (!this.pq.isEmpty()) {\n      const [d, u] = this.pq.pop();\n      if (d !== this.dist[u]) continue;\n      for (const {v, w} of this.graph[u]) {\n        if (this.dist[v] > d + w) {\n          this.dist[v] = d + w;\n          this.pq.push([this.dist[v], v]);\n        }\n      }\n    }\n  }\n  decreaseEdge(u, v, newW) {\n    // update edge weight in graph[u] item\n    // assume newW <= oldW\n    // attempt relaxation\n    if (this.dist[u] + newW < this.dist[v]) {\n      this.dist[v] = this.dist[u] + newW;\n      this.pq.push([this.dist[v], v]);\n      while (!this.pq.isEmpty()) {\n        const [d, x] = this.pq.pop();\n        if (d !== this.dist[x]) continue;\n        for (const {to, w} of this.graph[x] || []) {\n          if (this.dist[to] > d + w) {\n            this.dist[to] = d + w;\n            this.pq.push([this.dist[to], to]);\n          }\n        }\n      }\n    }\n  }\n}\n```\n\n## Follow-up Questions\n\n- How would you adapt if many edges frequently decrease in a short window?\n- How would you extend to multiple sources or all-pairs distances, and what would the complexity trade-offs look like?","diagram":"flowchart TD\n  S[Hub S] --> E[Edge decrease event]\n  E --> R{Relaxation possible?}\n  R -->|Yes| U[Update dist and propagate via Dijkstra]\n  R -->|No| End[Done]","difficulty":"intermediate","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Google","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T01:33:06.200Z","createdAt":"2026-01-12T01:33:06.200Z"},{"id":"q-766","question":"## Prompt\n\nIn a dynamic directed graph G=(V,E) with nonnegative weights, edge latencies only decrease in batches. Design a practical, production-ready algorithm to maintain a (1+ε)-approximate SSSP tree from a source S under these updates, enabling distance queries dist(S,v) in O(log|V|) time. Target sublinear amortized update in |E| for batch updates, and linear space. Explain data structures, update bounds, and how you bound cascade effects?","answer":"Use a landmark-based dynamic SSSP: pick k landmarks; store dist(S,ℓ) and dist(ℓ,v) for all v; approximate dist(S,v) ≈ minℓ dist(S,ℓ)+dist(ℓ,v). On a batch of decreases, relax only paths affected by up","explanation":"## Why This Is Asked\n\nTests ability to reason about dynamic approximate shortest paths with real-world batch updates. Challenges include cascade propagation, landmark selection, and trade-offs between accuracy and update time. This probes knowledge of dynamic data structures and practical heuristics for production systems.\n\n## Key Concepts\n\n- Dynamic shortest paths with updates\n- (1+ε)-approximation via landmarks\n- Amortized analysis under batch updates\n- Space-time trade-offs in DS design\n\n## Code Example\n\n```javascript\nclass LandmarkSSSP {\n  constructor(S, landmarks) {\n    this.S = S;\n    this.landmarks = landmarks;\n  }\n  updateBatch(changes) {\n    // relax locally; recompute affected landmarks lazily\n  }\n  dist(v) {\n    // return min over landmarks dist(S,ℓ)+dist(ℓ,v)\n    return 0;\n  }\n}\n```\n\n## Follow-up Questions\n\n- How to select landmarks adaptively\n- How to bound worst-case cascades on adversarial batches","diagram":"flowchart TD\n  S(Source) --> L1[Landmark1]\n  S --> L2[Landmark2]\n  L1 --> V1[NodeA]\n  L2 --> V1","difficulty":"advanced","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Meta","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T03:51:35.496Z","createdAt":"2026-01-12T03:51:35.496Z"},{"id":"q-770","question":"Given a directed graph G=(V,E) with |V|=N and |E|=M, nonnegative weights, support online operations: 1) decreaseWeight(u,v,delta) with delta>0, 2) queryShortestPath(S,T) returning current shortest path length. Updates and queries are interleaved. Propose a data-structure and algorithm that achieves sublinear amortized reprocessing per update, justify amortized bounds, and discuss space and practical optimizations for massive graphs (N up to 1e6, M up to 1e7)?","answer":"Maintain a two-layer dynamic SSSP: a backbone sparse graph that preserves most shortest paths, plus a frontier-based lazy relaxation region. On decreaseWeight(u,v,delta), only relax from nodes in the ","explanation":"## Why This Is Asked\nTests ability to design dynamic shortest paths with interleaved updates and queries at scale, focusing on amortized analysis and engineering practicality.\n\n## Key Concepts\n- Dynamic graphs and amortized analysis\n- Localized frontier-based relaxations\n- Lazy evaluation and batching for latency control\n\n## Code Example\n```python\nclass DynamicSSSP:\n    def __init__(self, graph, s):\n        self.graph = graph\n        self.dist = [float('inf')] * graph.n\n        self.dist[s] = 0\n        self.cache = {}\n    def decrease(self, u, v, delta):\n        w = delta\n        if self.dist[u] + w < self.dist[v]:\n            self.dist[v] = self.dist[u] + w\n            self._relax_frontier({v})\n    def query(self, t):\n        return self.dist[t]\n    def _relax_frontier(self, frontier):\n        # localized Dijkstra updates\n        pass\n```\n\n## Follow-up Questions\n- How would you extend to edge deletions and negative cycles?\n- How would you empirically validate the amortized bounds on a real-world, large-scale graph?","diagram":"flowchart TD\n  A[Decrease edge weight] --> B[Frontier recomputation check]\n  B --> C{Cache valid?}\n  C -->|Yes| D[Answer query from cache]\n  C -->|No| E[Run localized Dijkstra from frontier]\n  E --> F[Update caches and distances]\n  F --> G[Return to queries]","difficulty":"advanced","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Lyft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T04:49:05.095Z","createdAt":"2026-01-12T04:49:05.095Z"},{"id":"q-779","question":"You're building a real-time analytics component for a fintech platform. You must maintain the number of distinct values in the most recent W events in a streaming fashion. Implement two operations: append(v) to push a new event value, and distinctCount() to return the number of unique values among the last W events. Assume values are integers up to 1e9 and W can be large. Provide a simple approach with its time/memory costs, then describe an amortized-constant-time solution using a hashmap plus a circular buffer, and discuss edge cases (e.g., large W, many duplicates). How would you implement and analyze it?","answer":"Option 1: recompute distinct on each append: O(W) time, O(W) space. Option 2: use a circular buffer of size W and a hashmap counts[value]→freq. On append: evict oldest value, decrement its count; add ","explanation":"## Why This Is Asked\n\nTests sliding-window knowledge, practical data structures, and amortized reasoning relevant to real-time analytics.\n\n## Key Concepts\n\n- Sliding window\n- Amortized analysis\n- Hashmap counters\n- Circular buffer\n\n## Code Example\n\n```python\nclass DistinctWindow:\n    def __init__(self, W):\n        self.W = W\n        self.buf = [None]*W\n        self.left = 0\n        self.right = 0\n        self.size = 0\n        self.counts = {}\n\n    def append(self, v):\n        if self.size == self.W:\n            old = self.buf[self.left]\n            self.buf[self.left] = None\n            self.left = (self.left + 1) % self.W\n            self.size -= 1\n            if old is not None:\n                c = self.counts[old]\n                if c == 1:\n                    del self.counts[old]\n                else:\n                    self.counts[old] = c - 1\n        self.buf[self.right] = v\n        self.right = (self.right + 1) % self.W\n        self.size += 1\n        self.counts[v] = self.counts.get(v, 0) + 1\n\n    def distinctCount(self):\n        return len(self.counts)\n```\n\n## Follow-up Questions\n\n- How does performance change if W is very large but few unique values exist? Any memory optimizations?\n- How would you adapt to a dynamic window size W' that changes over time without recomputing from scratch?","diagram":null,"difficulty":"beginner","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","LinkedIn","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T05:31:28.017Z","createdAt":"2026-01-12T05:31:28.017Z"},{"id":"q-787","question":"You maintain N players with integer scores in the range 0..10000. You must support two online operations: 1) update(i, s) — set player i's score to s; 2) countLE(X) — return how many players have score <= X. Propose a data structure and algorithm to support both in O(log V) time per operation, where V=10001, and analyze space usage. Include initialization and a brief correctness sketch?","answer":"Fenwick tree (BIT) over the value domain 0..10000. Initialize by counting frequencies of each score; update(i, newVal) does bit.add(oldVal, -1) and bit.add(newVal, +1); countLE(X) = bit.sum(X). Comple","explanation":"## Why This Is Asked\nTests ability to map domain-specific queries to a standard data structure and reason about time/space.\n\n## Key Concepts\n- Fenwick Tree (BIT)\n- Prefix sums\n- Coordinate/value-domain handling\n\n## Code Example\n```javascript\nclass Fenwick {\n  constructor(n){ this.n=n; this.f=new Array(n+1).fill(0); }\n  add(i, delta){ for(i+=1; i<=this.n; i+= i&-i) this.f[i]+=delta; }\n  sum(i){ let s=0; for(i+=1; i>0; i-= i&-i) s+=this.f[i]; return s; }\n}\n```\n\n## Follow-up Questions\n- How would you adapt for dynamic value ranges or non-integer scores?\n- How do you handle ties or range queries like count in (a,b]?","diagram":null,"difficulty":"beginner","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Cloudflare","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T06:39:27.526Z","createdAt":"2026-01-12T06:39:27.527Z"},{"id":"q-796","question":"Given an array A of length N with integers in range [0, R). You implement a function to count distinct values by inserting each A[i] into a hash set, then return its size. 1) What is the time and space complexity in terms of N and D (distinct values)? 2) If R is small, propose a memory-efficient alternative and analyze its tradeoffs?","answer":"With a hash set: expected O(N) time; O(D) space where D is distinct values; inserts amortize O(1). Worst-case O(N) time if hash collisions. If R is small, use a bitset/boolean array of size R for O(1)","explanation":"## Why This Is Asked\nDesign and analyze a simple counting distinct values task, contrasting hash-based and range-based approaches. It tests understanding of time vs space tradeoffs and edge cases of hash collisions and bounded value domains.\n\n## Key Concepts\n- Hash-set complexity\n- Distinct values D\n- Bitset vs hash-based counts\n- Space-time tradeoffs\n\n## Code Example\n```javascript\nfunction countDistinct(arr, R){\n  const seen = new Set();\n  for(const x of arr){ seen.add(x); }\n  return seen.size;\n}\n```\n\n## Follow-up Questions\n- How would you adapt for streaming data with sliding windows?\n- What about false positives with probabilistic structures like Bloom filters?","diagram":null,"difficulty":"beginner","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Netflix","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T07:30:21.355Z","createdAt":"2026-01-12T07:30:21.355Z"},{"id":"q-803","question":"In a directed graph G=(V,E) with N nodes, M edges and nonnegative weights, a fixed source S, and an SSSP tree T. Edges can change weight online (increase or decrease), but no edges are added or removed. Propose a concrete, implementable strategy to maintain the SSSP efficiently, including data structures, update rules, and expected time bounds. Provide preprocessing, amortized per-update, and memory usage, plus practical optimizations and a concrete scenario where it shines (e.g., streaming latency updates)?","answer":"Maintain a single-source shortest-path tree (SSSP) from source S. For weight decreases on tree edges, perform localized relaxation using bounded Dijkstra that visits only the affected frontier. For weight increases or non-tree edges, employ a hierarchical approach: first determine if the edge creates a shorter alternative path, and if so, execute localized re-relaxation from the affected subtree.\n\n**Data Structures:**\n- Fibonacci heap for priority queue (O(1) insert, O(log n) delete-min)\n- Adjacency list with edge weight tracking\n- Parent array for SSSP tree maintenance\n- Distance array with lazy updates\n- Affected node set to track nodes requiring relaxation\n\n**Preprocessing:** O(M + N log N) to compute initial SSSP using Dijkstra's algorithm\n\n**Update Rules:**\n- Weight decrease: If edge belongs to SSSP tree, push affected subtree nodes to priority queue with updated distances; otherwise, simply check if it creates a shorter path\n- Weight increase: If edge belongs to SSSP tree, mark affected subtree and run bounded Dijkstra from frontier nodes; otherwise, no action required\n\n**Time Bounds:**\n- Amortized per-update: O(log N + K) where K is the number of affected nodes (typically much smaller than N)\n- Worst-case: O(N log N) for pathological updates affecting the entire tree\n- Memory usage: O(N + M) for graph storage plus O(N) for auxiliary structures\n\n**Practical Optimizations:**\n- Use multi-level buckets for improved constant factors\n- Batch multiple updates before reprocessing\n- Maintain distance bounds to prune unnecessary relaxations\n- Cache frequently accessed edge weights\n\n**Concrete Scenario:** Real-time network routing with streaming latency updates, where edge weights represent network delays that change frequently but only affect localized regions of the network.","explanation":"## Why This Is Asked\nDesigning dynamic SSSP algorithms is essential for real-time network applications. This question tests understanding of worst-case versus amortized guarantees, appropriate data structure selection, and practical implementation constraints.\n\n## Key Concepts\n- Dynamic shortest paths with nonnegative edge weights\n- Localized re-relaxation and bounded Dijkstra execution\n- Amortized analysis and worst-case performance bounds\n- Memory-efficient data structures and practical optimizations\n- Hierarchical update strategies for different edge types\n\n## Code Example\n```javascript\n// Dynamic SSSP maintenance implementation\nclass DynamicSSSP {\n  constructor(graph, source) {\n    this.graph = graph;\n    this.source = source;\n    this.distances = new Array(graph.N).fill(Infinity);\n    this.parents = new Array(graph.N).fill(-1);\n    this.heap = new FibonacciHeap();\n    this.affectedNodes = new Set();\n    \n    // Initialize with Dijkstra\n    this.initializeSSSP();\n  }\n  \n  updateEdge(u, v, newWeight) {\n    const oldWeight = this.graph.getWeight(u, v);\n    this.graph.setWeight(u, v, newWeight);\n    \n    if (newWeight < oldWeight) {\n      this.handleWeightDecrease(u, v, newWeight);\n    } else {\n      this.handleWeightIncrease(u, v, newWeight);\n    }\n  }\n  \n  handleWeightDecrease(u, v, newWeight) {\n    if (this.parents[v] === u) {\n      // Tree edge weight decreased\n      this.propagateDecrease(v);\n    } else if (this.distances[u] + newWeight < this.distances[v]) {\n      // Non-tree edge creates shorter path\n      this.updateDistance(v, this.distances[u] + newWeight, u);\n    }\n  }\n  \n  handleWeightIncrease(u, v, newWeight) {\n    if (this.parents[v] === u) {\n      // Tree edge weight increased - need re-relaxation\n      this.affectedNodes.add(v);\n      this.boundedDijkstra([v]);\n    }\n    // Non-tree edge weight increase requires no action\n  }\n  \n  boundedDijkstra(startNodes) {\n    const queue = new FibonacciHeap();\n    \n    for (const node of startNodes) {\n      queue.insert(node, this.distances[node]);\n    }\n    \n    while (!queue.isEmpty()) {\n      const { node: u, distance: du } = queue.extractMin();\n      \n      if (du > this.distances[u]) continue;\n      \n      for (const [v, weight] of this.graph.neighbors(u)) {\n        const newDist = du + weight;\n        if (newDist < this.distances[v]) {\n          this.updateDistance(v, newDist, u);\n          queue.insert(v, newDist);\n        }\n      }\n    }\n  }\n  \n  updateDistance(node, newDistance, parent) {\n    this.distances[node] = newDistance;\n    this.parents[node] = parent;\n    this.affectedNodes.add(node);\n  }\n  \n  propagateDecrease(startNode) {\n    const queue = new FibonacciHeap();\n    queue.insert(startNode, this.distances[startNode]);\n    \n    while (!queue.isEmpty()) {\n      const { node: u } = queue.extractMin();\n      \n      for (const [v, weight] of this.graph.neighbors(u)) {\n        if (this.parents[v] === u) {\n          const newDist = this.distances[u] + weight;\n          if (newDist < this.distances[v]) {\n            this.updateDistance(v, newDist, u);\n            queue.insert(v, newDist);\n          }\n        }\n      }\n    }\n  }\n  \n  initializeSSSP() {\n    // Standard Dijkstra implementation\n    this.distances[this.source] = 0;\n    const heap = new FibonacciHeap();\n    heap.insert(this.source, 0);\n    \n    while (!heap.isEmpty()) {\n      const { node: u, distance: du } = heap.extractMin();\n      \n      if (du > this.distances[u]) continue;\n      \n      for (const [v, weight] of this.graph.neighbors(u)) {\n        const newDist = du + weight;\n        if (newDist < this.distances[v]) {\n          this.distances[v] = newDist;\n          this.parents[v] = u;\n          heap.insert(v, newDist);\n        }\n      }\n    }\n  }\n}\n```\n\nThis implementation demonstrates the core principles of dynamic SSSP maintenance with efficient localized updates and amortized performance guarantees.","diagram":null,"difficulty":"advanced","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","LinkedIn"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T12:44:57.161Z","createdAt":"2026-01-12T08:34:14.620Z"},{"id":"q-808","question":"Dynamic path counting in a DAG: maintain the number of S→T paths of length at most L under online edge insertions and deletions. Propose a data structure and amortized update time bound in terms of N, M, L and #updates; discuss memory usage and how to handle large L and modulo arithmetic in practice?","answer":"Maintain a layer-based DP: c[v][d] = number of S→v paths of exact length d for d = 0..L. Initial fill by DP in topological order. On an edge insertion/deletion, compute the delta for c[v][d] caused by","explanation":"## Why This Is Asked\n\nEvaluates practicality of dynamic programming under online graph updates, requiring precise amortized reasoning and data-structure design rather than surface knowledge.\n\n## Key Concepts\n\n- DAG properties and topological order\n- Path counting DP with an extra length dimension\n- Incremental updates via delta propagation\n- Frontier-limited recomputation and memory-time tradeoffs\n\n## Code Example\n\n```python\n# Pseudocode for update propagation (edge (u->v) updated)\ndef update_edge(u, v, delta_sign):\n    for d in range(1, L+1):\n        delta = delta_sign * paths_from_S_to[u][d-1]\n        if delta:\n            paths_to[v][d] += delta\n            enqueue_successors(v, d, delta)\n```\n\n## Follow-up Questions\n\n- How would you adapt when L varies per query or when edges have weights affecting path counts differently?\n- What are the bottlenecks for extremely large L or dense DAGs, and how would you mitigate them?","diagram":"flowchart TD\n  A[Start] --> B[Receive edge update]\n  B --> C[Compute deltas on layer counts]\n  C --> D[Propagate to successors up to L]\n  D --> E[Return updated counts to queries]","difficulty":"intermediate","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Snowflake","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T09:33:58.407Z","createdAt":"2026-01-12T09:33:58.407Z"},{"id":"q-817","question":"Design a dynamic, multi-source shortest-path maintenance scheme for a directed graph with nonnegative weights. A fixed set of K hub nodes H must always have exact shortest-path distances to all nodes. Edges can be inserted or weights decreased online, in batches of size at most B. Provide initial preprocessing and a full-update algorithm, with (i) initial time, (ii) amortized update time per batch, and (iii) memory usage. Include two practical optimizations and compare to recomputing from scratch after each batch?","answer":"Multi-source incremental Dijkstra. Precompute d(h,v) for all h∈H with K runs: O(K E log V). For a batch of up to B updates, relax via affected edges using a shared min-heap, propagating improvements o","explanation":"## Why This Is Asked\nTests dynamic graphs, amortized analysis, and practical heuristics for keeping multiple SSSP trees up to date.\n\n## Key Concepts\n- Dynamic SSSP with monotone updates\n- Multi-source labeling\n- Amortized analysis and batch processing\n- Space-time trade-offs\n\n## Code Example\n```javascript\n// Pseudocode: multi-source incremental relaxations\nlet D = new Map(); // D[h] -> dist array\nfor (let h of H) D[h] = dijkstra(G, h);\nfunction updateBatch(batch){\n  let PQ = new MinHeap();\n  for (let e of batch){ /* decrease weights or insert */ }\n  // relaxations\n  while(!PQ.isEmpty()){ let {d,u} = PQ.pop(); if (d> D[h][u]) continue; ... }\n}\n```\n\n## Follow-up Questions\n- How do you bound X in worst-case, and how would you adapt if H grows?\n- How would you extend to approximate distances with guarantees?","diagram":null,"difficulty":"advanced","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","PayPal","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T10:26:55.970Z","createdAt":"2026-01-12T10:26:55.970Z"},{"id":"q-827","question":"Design a dynamic distance labeling scheme for an undirected weighted graph that supports edge insertions and deletions online. Use a fixed hub set H to enable dist(u,v) queries via dist(u,v)=min_h dist(u,h)+dist(h,v) only if H covers all shortest paths. Explain maintenance of hub distances under updates, and bound update/query times and memory usage. Provide two optimizations and a comparison to recomputing from scratch?","answer":"Approach: hub labeling with a fixed hub set H. Each node v stores dist(v,h) for all h in H; dist(u,v) = min_h dist(u,h)+dist(h,v) if H covers all shortest paths. On updates, recompute only hubs touche","explanation":"## Why This Is Asked\nThis probes dynamic graph labeling, balancing query time against update cost, a practical constraint in large-scale networks where frequent edits occur.\n\n## Key Concepts\n- Hub labeling and cover properties\n- Dynamic SSSP maintenance for limited regions\n- Trade-offs: hub count, space, and amortized updates\n\n## Code Example\n```javascript\n// Pseudocode sketch for updates\nfunction updateEdge(u, v, w, graph, labels, H){\n  // identify affected hubs and propagate changes using bounded-radius search\n}\n```\n\n## Follow-up Questions\n- How do you pick and update H to adapt to changing graphs? \n- How would you extend to directed graphs with non-symmetric distances?","diagram":"flowchart TD\n  A[Node] --> B[Hub]\n  B --> C[Distance to hub]\n  A --> D[Query dist to E]\n  D --> E[Compute min over hubs]","difficulty":"intermediate","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Meta","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T11:23:08.719Z","createdAt":"2026-01-12T11:23:08.719Z"},{"id":"q-831","question":"In an undirected weighted graph G=(V,E) with nonnegative weights, design a (1+ε)-approximate distance oracle based on a fixed landmark set L (|L|=k). Edges are only inserted online in batches of size B; no deletions. After each batch, specify: (i) preprocessing time and space to build distances from every landmark, (ii) amortized update time per batch to update the oracle, (iii) query time for dist(u,v), (iv) total memory, (v) two practical optimizations, (vi) a comparison to rebuilding all-pairs distances after each batch. Provide concrete asymptotics in terms of n=|V|, m=|E|, k, ε, B?","answer":"Use k landmarks. Precompute Dijkstra from each landmark: time O(k m log n), space O(k n). Query dist(u,v) = minℓ (d(u,ℓ)+d(ℓ,v)) in O(k). For a batch of B edge insertions, update via incremental relax","explanation":"## Why This Is Asked\nTests designing a practical distance oracle under incremental graph updates, balancing preprocessing, update, and query costs, plus real-world trade-offs when batch sizes are small.\n\n## Key Concepts\n- (1+ε) distance oracle with landmarks\n- Incremental insertions only\n- Multi-source Dijkstra from landmarks\n- Amortized per-batch analysis\n- Pruning and reuse of work across batches\n\n## Code Example\n```javascript\nfunction rebuildFromLandmarks(graph, landmarks) {\n  // placeholder: run Dijkstra from each landmark\n}\n```\n\n## Follow-up Questions\n- How would the approach adapt if deletions appear?\n- How does ε influence practical update costs and memory?","diagram":null,"difficulty":"intermediate","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","Microsoft","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T12:43:15.513Z","createdAt":"2026-01-12T12:43:15.513Z"}],"subChannels":["general"],"companies":["Airbnb","Apple","Bloomberg","Citadel","Cloudflare","Coinbase","Databricks","Discord","DoorDash","Goldman Sachs","Google","Instacart","LinkedIn","Lyft","Meta","Microsoft","NVIDIA","Netflix","OpenAI","Oracle","PayPal","Robinhood","Salesforce","Snap","Snowflake","Square","Tesla","Twitter","Two Sigma","Uber","Zoom"],"stats":{"total":23,"beginner":7,"intermediate":9,"advanced":7,"newThisWeek":23}}