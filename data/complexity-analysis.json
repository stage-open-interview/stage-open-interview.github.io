{"questions":[{"id":"q-1193","question":"You have an array A of n positive integers and a threshold T. For a given window length L, define ok(L) as: there exists a subarray of length L with sum <= T. Design an O(n) check for ok(L) using a sliding window, then outline how to find the maximum L with binary search over [1..n], and analyze total time and space. Include edge-case handling and practical optimizations?","answer":"Explain ok(L) in O(n) using a sliding window: compute the first L-sum, then slide by subtracting A[i-L] and adding A[i], stopping if a window sum <= T exists. Binary search L in [1..n] for the max L; ","explanation":"## Why This Is Asked\n\nThis question blends sliding-window technique with binary search to analyze a threshold problem, testing practical algorithm design and reasoning about complexity.\n\n## Key Concepts\n\n- Sliding window for fixed-length subarray sums\n- Binary search over the answer space\n- Time/space trade-offs and edge-case handling\n- Practical optimizations (early exit, optional prefix sums)\n\n## Code Example\n\n```javascript\nfunction maxLWithSumAtMost(A, T) {\n  const n = A.length;\n  const ok = (L) => {\n    let sum = 0;\n    for (let i = 0; i < L; i++) sum += A[i];\n    if (sum <= T) return true;\n    for (let i = L; i < n; i++) {\n      sum += A[i] - A[i - L];\n      if (sum <= T) return true;\n    }\n    return false;\n  };\n  let lo = 1, hi = n, ans = 0;\n  while (lo <= hi) {\n    const mid = (lo + hi) >> 1;\n    if (ok(mid)) { ans = mid; lo = mid + 1; } else { hi = mid - 1; }\n  }\n  return ans;\n}\n```\n\n## Follow-up Questions\n\n- How would the solution change if elements could be negative?\n- What are the memory/speed trade-offs if you precompute prefix sums?\n- How would you adapt for multiple thresholds T1..Tk in parallel?","diagram":null,"difficulty":"beginner","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Snowflake","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T04:42:46.129Z","createdAt":"2026-01-13T04:42:46.129Z"},{"id":"q-1254","question":"You're given a DAG G=(V,E) with N nodes and M edges. Edges can be inserted online in batches of size B. Design a dynamic transitive-closure using bitsets to answer reachability queries in O(1). Provide initialization, amortized per-batch update time, and memory. Include two practical optimizations and compare to recomputing the closure after each batch?","answer":"Adopt a reachability bitset per node reach[u] over all nodes, with a topological order. For a batch of insertions, process edges in topo order; if v not in reach[u], do reach[u] |= reach[v] and push t","explanation":"## Why This Is Asked\n\nAssesses dynamic graph reasoning, bitset optimizations, and amortized analysis in a practical DAG setting.\n\n## Key Concepts\n\n- Bitset reachability per node\n- Topological ordering reuse\n- Incremental updates and fixed-point propagation\n- Amortized cost and memory trade-offs\n\n## Code Example\n\n```javascript\n// Pseudocode for dynamic reachability with bitsets\nfor edge (u,v) in batch in topo order {\n  if (!(reach[u] has v)) {\n    reach[u] |= reach[v]\n    // propagate if new bits were added\n  }\n}\n```\n\n## Follow-up Questions\n\n- How would deletions affect the structure?\n- Analyze worst-case batch patterns that maximize work.","diagram":null,"difficulty":"intermediate","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Square","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T06:47:01.803Z","createdAt":"2026-01-13T06:47:01.803Z"},{"id":"q-2159","question":"Scenario: In a weighted directed graph with nonnegative weights, you must maintain approximate betweenness centrality for all nodes under batches of edge insertions of size B. Propose a concrete, implementable scheme that (i) initializes estimates, (ii) updates after each batch with amortized cost, (iii) bounds memory, and (iv) provides guaranteed error bounds ε. Include two practical optimizations and compare to re-running Brandes' algorithm after every batch?","answer":"Maintain approximate betweenness via dynamic sampling of source–target pairs. Precompute a fixed set of sample shortest paths; store their node visit counts. After a batch of B insertions, only paths ","explanation":"## Why This Is Asked\nTests understanding of dynamic graph algorithms, sampling for approximations, and amortized analysis in batch-update settings.\n\n## Key Concepts\n- Dynamic graphs with batch updates\n- Approximate centrality with ε guarantees\n- Sampling-based path tracking and incremental updates\n- Trade-offs: time, memory, and rebuild frequency\n\n## Code Example\n```javascript\n// Sketch: updateBatch(graph, batch, samples, eps) { /* adjust sample counts for new edges */ }\n```\n\n## Follow-up Questions\n- How to bound error growth with insertions?\n- How to select number of samples s and batch size B for a target ε?","diagram":null,"difficulty":"intermediate","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","Snowflake","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T05:39:09.336Z","createdAt":"2026-01-15T05:39:09.337Z"},{"id":"q-2497","question":"Maintain exact single-source shortest paths from a fixed hub h under online edge insertions (batches). Propose a practical scheme that (i) initializes with Dijkstra, (ii) after each batch re-relaxes only affected nodes using a min-heap, (iii) bounds memory, and (iv) gives amortized update time. Include two optimizations and compare to rerunning Dijkstra after every batch. Provide asymptotics in n, m, B?","answer":"Core approach: maintain exact distances from hub h. After a batch, re-relax only nodes whose paths can improve. Initialize with Dijkstra from h. Use a min-heap to propagate decreases until fixed. Init","explanation":"## Why This Is Asked\nTests the ability to design incremental single-source shortest paths with batch updates and to reason about amortized work and practical data structures.\n\n## Key Concepts\n- Incremental SSSP with batch edge insertions\n- Localized relaxation and pruning\n- Priority queues and potential bucketed variants (Dial's)\n- Amortized analysis and space efficiency\n\n## Code Example\n```javascript\nfunction updateBatch(batchEdges){\n  for (const [u,v,w] of batchEdges){\n    // ensure undirected view if needed\n    adj[u].push([v,w]);\n    adj[v].push([u,w]);\n  }\n  for (const [u,v,w] of batchEdges){\n    if (dist[v] > dist[u] + w){\n      dist[v] = dist[u] + w;\n      pq.push([dist[v], v]);\n    }\n    if (dist[u] > dist[v] + w){\n      dist[u] = dist[v] + w;\n      pq.push([dist[u], u]);\n    }\n  }\n  while (!pq.isEmpty()){\n    const [d, x] = pq.pop();\n    if (d !== dist[x]) continue;\n    for (const [y,w] of adj[x]){\n      if (dist[y] > dist[x] + w){\n        dist[y] = dist[x] + w;\n        pq.push([dist[y], y]);\n      }\n    }\n  }\n}\n```","diagram":null,"difficulty":"advanced","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Oracle","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T20:40:07.034Z","createdAt":"2026-01-15T20:40:07.034Z"},{"id":"q-2635","question":"Scenario: A real-time analytics service ingests event messages with a 32-bit integer key. Events arrive in batches of size B every second, and the system must report the number of distinct keys seen so far (cardinality) with a configurable error ε. Propose a practical streaming algorithm and data structure (no deletions) to maintain an ε-approximate cardinality. After each batch, specify (i) per-element insertion time and total batch time, (ii) space usage, (iii) query time for the current estimate, (iv) two optimizations, and (v) how this compares to re-scanning all seen events after every batch. Use asymptotics in n (seen events), B, and ε?","answer":"Use HyperLogLog with m = ceil(1.04/ε^2) registers. For each key, hash and update a register; per-element cost O(1). Batch of B costs O(B). Memory O(m). Query O(1) to estimate cardinality. Optimization","explanation":"## Why This Is Asked\\nTests practical understanding of streaming cardinality with limited precision, a common real-time metric.\\n\\n## Key Concepts\\n- HyperLogLog, probabilistic counting, amortized analysis, batch processing.\\n- Trade-offs: memory vs. accuracy, update vs. query costs.\\n\\n## Code Example\\n```javascript\\n// sketch: update(hll, x) { hll.add(hash(x)); }\\n```\\n\\n## Follow-up Questions\\n- How would you choose ε given memory limits? \\n- How do deletions or windowed cardinality change the approach?","diagram":null,"difficulty":"beginner","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","PayPal","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T04:17:01.951Z","createdAt":"2026-01-16T04:17:01.951Z"},{"id":"q-2655","question":"Design a dynamic (1+ε)-spanner for an undirected weighted graph under online edge insertions in batches of size B. Build a (k,ε)-spanner with O(n^{1+1/k}) edges. After each batch, update only edges that improve distances beyond ε; amortized O(B log n). Query dist via the spanner in O(1). Memory O(n^{1+1/k}). Optimizations: lazy rebuild; reuse tight edges. Compare to rebuilding: faster updates; worst-case near O(n^{1+1/k}) per batch?","answer":"Propose a dynamic (1+ε)-spanner for an undirected weighted graph under online edge insertions in batches of size B. Build a (k,ε)-spanner with O(n^{1+1/k}) edges. After each batch, update only edges t","explanation":"## Why This Is Asked\nTests ability to synthesize dynamic graph sparsification with real-world batch updates and explain trade-offs of maintaining approximate structures.\n\n## Key Concepts\n- Dynamic spanners\n- Batch update amortization\n- Distance queries over sparse representations\n- Memory vs accuracy trade-offs\n\n## Code Example\n```javascript\n// Pseudocode: batch update for spanner maintenance\nfunction updateBatch(G, batchEdges, k, eps){\n  // add edges, check potential shortcuts, prune by eps\n  // update spanner edges selectively\n}\n```\n\n## Follow-up Questions\n- How to extend to edge deletions?\n- How does skewed batch size affect amortized cost?","diagram":null,"difficulty":"advanced","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Robinhood","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T05:41:04.220Z","createdAt":"2026-01-16T05:41:04.220Z"},{"id":"q-2726","question":"Scenario: A weighted directed graph with nonnegative weights tracks network latencies. Edges can be inserted or have their weights decreased in online batches of size B. After each batch, you must maintain exact distances from a fixed source S to all nodes. Propose a practical hybrid scheme that combines (i) an incremental update pass limited to affected nodes, (ii) periodic full re-computation every T batches, and (iii) a lazy rebuild trigger when incremental cost exceeds a threshold. Provide initial preprocessing, amortized update time per batch, memory bounds, two optimizations, and a comparison to re-running Dijkstra from scratch after every batch?","answer":"Hybrid incremental+rebuild: keep dist[v] from S and a frontier. For each batch of B updates, enqueue affected endpoints and relax until no improvement. After T batches, run a full Dijkstra to rebuild ","explanation":"## Why This Is Asked\nDynamic shortest-path maintenance with batching tests practical trade-offs between local updates and periodic rebuilds. It blends online incremental work with amortized guarantees and requires careful parameterization.\n\n## Key Concepts\n- Incremental relaxations focused on affected region\n- Periodic full rebuild to bound drift\n- Lazy thresholds to trigger rebuilds\n\n## Code Example\n```javascript\n// Pseudocode sketch for batch update\n```\n\n## Follow-up Questions\n- How would you adapt this for multiple sources S1..Sk?\n- How would you choose T given m, n, and B?","diagram":"flowchart TD\n  A[Batch Update (size B)] --> B[Frontier enqueue]\n  B --> C[Relax distances]\n  C --> D{Rebuild needed?}\n  D --> E[Yes: full Dijkstra]\n  D --> F[No: serve queries]\n","difficulty":"intermediate","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Meta","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T09:39:47.374Z","createdAt":"2026-01-16T09:39:47.374Z"},{"id":"q-3064","question":"In a directed graph with nonnegative weights, updates arrive in batches of B edge insertions or weight decreases. After each batch, maintain P = { (u,v) | dist(u,v) ≤ D } for a fixed threshold D. Propose a practical scheme that (i) precomputes a compact structure, (ii) updates P after each batch with amortized time, (iii) bounds memory, and (iv) provides two optimizations. Compare to recomputing all-pairs distances up to D after every batch?","answer":"Use a landmark-based threshold maintenance scheme. Select L of size k; for each s in L, run Dijkstra truncated at D to compute dist(s,•). Store ds[v]. After a batch, re-relax only nodes reachable from updated edges, leveraging the landmark distances to bound recomputation.","explanation":"## Why This Is Asked\nTests ability to design dynamic, approximation-friendly data structures for thresholded distances under batched updates. It probes practical trade-offs between accuracy, time, and memory, beyond standard APSP or exact single-source maintenance.\n\n## Key Concepts\n- Dynamic graphs with batched updates\n- Thresholded reachability dist(u,v) ≤ D\n- Landmark-based distance sketches\n- Amortized analysis and lazy propagation\n\n## Code Example\n```python\n# skeleton for batch update (pseudo)\ndef update_batch(graph, batch, D, landmarks):\n    for s in landmarks:\n        relax_truncated(gra\n```","diagram":"flowchart TD\n  A[Update Batch] --> B[Identify Affected Edges]\n  B --> C[Update Landmark Distances]\n  C --> D[Compute P]\n  D --> E[Optional Rebuild Trigger]","difficulty":"intermediate","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Goldman Sachs","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T05:18:43.450Z","createdAt":"2026-01-16T23:34:35.046Z"},{"id":"q-3179","question":"You maintain a growing list of N tasks with integer priorities. Operations arrive in batches of size B, appending new tasks. After each batch you must surface the current minimum priority quickly. Compare two schemes: (A) rebuild a heap from scratch after every batch; (B) insert the B new tasks into a single heap. Provide total time and per-operation costs, and derive the breakeven batch size B* in terms of N. Include memory usage considerations?","answer":"Scheme A total time approximately (N^2)/(2B); amortized per insertion approximately N/(2B). Scheme B total time approximately N log N; amortized approximately log N. Breakeven when N log N ≈ N^2/(2B) ","explanation":"## Why This Is Asked\nThis tests practical amortized reasoning on common data-structure choices in streaming-like workloads.\n\n## Key Concepts\n- Amortized analysis\n- Heap rebuild vs incremental insert\n- Breakeven batch size and memory bound\n\n## Code Example\n```javascript\nfunction rebuildHeap(arr){ /* build min-heap from scratch */ }\n```\n\n## Follow-up Questions\n- How would you adapt if you also needed delete-min in O(log N)?\n- How would varying batch sizes affect the breakeven point?","diagram":"flowchart TD\n  A[Batch] --> B[Scheme A: Rebuild]\n  A --> C[Scheme B: Incr Inserts]\n  B --> D[Compare costs]\n  C --> D","difficulty":"beginner","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Slack","Tesla","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T05:40:30.032Z","createdAt":"2026-01-17T05:40:30.032Z"},{"id":"q-3314","question":"Scenario: A streaming service receives integers in batches of size B. After each batch, output the median of all values seen so far. Propose a practical scheme that (i) uses two heaps to balance lower/upper halves, (ii) inserts each new value with O(log n) cost, (iii) bounds memory, (iv) offers two optimizations, and (v) compares to recomputing from scratch after every batch. Give asymptotics in terms of n and B?","answer":"Two-heap approach: max-heap for lower half and min-heap for upper half; keep size diff ≤ 1. For each value, push to the appropriate heap and rebalance. After n elements, median is the top of the large","explanation":"## Why This Is Asked\nThis checks understanding of streaming data structures and complexity trade-offs for maintaining order statistics under batch updates.\n\n## Key Concepts\n- Running median via two heaps\n- Per-element insert cost and batch amortization\n- Memory growth with total elements\n\n## Code Example\n```javascript\nclass MedianMaintainer {\n  constructor() {\n    this.low = new MaxHeap();\n    this.high = new MinHeap();\n  }\n  add(x) {\n    if (this.low.isEmpty() || x <= this.low.peek()) this.low.push(x);\n    else this.high.push(x);\n    // rebalance\n    if (this.low.size() > this.high.size() + 1) this.high.push(this.low.pop());\n    else if (this.high.size() > this.low.size()) this.low.push(this.high.pop());\n  }\n  median() {\n    if (this.low.size() === this.high.size()) return (this.low.peek() + this.high.peek()) / 2;\n    return this.low.peek();\n  }\n}\n```\n\n## Follow-up Questions\n- How does varying batch size B affect latency and memory?\n- How would you extend to handle sliding windows or duplicates efficiently?","diagram":null,"difficulty":"beginner","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Microsoft","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T11:26:41.053Z","createdAt":"2026-01-17T11:26:41.053Z"},{"id":"q-3412","question":"Write a function countPairs(n) with:\n for i from 1 to n:\n   for j from 1 to floor(n/i):\n     // O(1) work\nWhat is the overall time complexity and space usage? Provide the tight bound, discuss best/worst-case, and compare to a memory-efficient variant that avoids the inner loop entirely?","answer":"T(n) = sum_{i=1}^{n} floor(n/i) = n log n + O(n), hence Theta(n log n). Space is O(1) beyond few counters. Best and worst are the same asymptotic. A memory-efficient variant computes the same count vi","explanation":"## Why This Is Asked\nTests ability to derive nested-loop complexity where inner bound shrinks with i, revealing a harmonic-sum growth.\n\n## Key Concepts\n- Harmonic series: sum floor(n/i) ~ n log n\n- Theta vs O, space O(1)\n- Closed-form approximations vs exact sums\n\n## Code Example\n```javascript\nfunction countPairs(n){\n  let count=0;\n  for(let i=1;i<=n;i++){\n    for(let j=1;j<=Math.floor(n/i);j++){\n      count++;\n    }\n  }\n  return count;\n}\n```\n\n## Follow-up Questions\n- How would you prove the Theta(n log n) bound?\n- How does changing inner bound to floor(n/i^2) affect complexity?\n- What are practical implications for large n?","diagram":null,"difficulty":"beginner","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T15:30:27.134Z","createdAt":"2026-01-17T15:30:27.134Z"},{"id":"q-3504","question":"Scenario: Streaming terms from a document corpus arrive in batches of B. Estimate the number of distinct terms seen so far using a probabilistic counter. Design an implementable plan with initial preprocessing, per-batch updates, memory bounds, two optimizations, and a comparison to exact counting. Include formulas in terms of B, U (distinct terms), and target error ε?","answer":"Use a HyperLogLog sketch to estimate the number of distinct terms in a stream arriving in batches of B. Initialize with p so that m = 2^p registers; memory ~ m. For each batch, insert B items into a p","explanation":"## Why This Is Asked\nEvaluates practical understanding of streaming algorithms, amortized costs, and accuracy vs memory. It also tests ability to present concrete formulas and compare approximate vs exact techniques.\n\n## Key Concepts\n- HyperLogLog, sketch mergeability\n- Batch processing and amortized analysis\n- Memory vs accuracy trade-offs\n\n## Code Example\n```javascript\nclass HLL { constructor(p) { } add(x) { } merge(other) { } estimate() { return 0; } }\n```\n\n## Follow-up Questions\n- How to pick p for target ε?\n- How would deletions or time decay affect the approach?","diagram":null,"difficulty":"beginner","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Hugging Face","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T19:23:01.756Z","createdAt":"2026-01-17T19:23:01.757Z"},{"id":"q-3589","question":"You maintain a weighted undirected graph G=(V,E) with nonnegative weights. Edges arrive online in batches of size B. You need to answer s shortest-path queries dist(u_i,v_i) for i=1..s after each batch, within a (1+ε) approximation, reusing work across all pairs. Propose a single shared data structure (i) initialization to support all s pairs, (ii) amortized per-batch update cost when a batch arrives, (iii) per-query time, (iv) memory, (v) two practical optimizations, and (vi) a comparison to rebuilding from scratch per batch. Express asymptotics in n=|V|, m=|E|, s, B, ε?","answer":"Use a shared hub-based scheme: select k = O((1/ε) log n) hubs H and maintain dist(h,v) for all h ∈ H through incremental updates after each batch. Answer dist(u,v) as min_{h ∈ H} [dist(u,h) + dist(h,v)]. Upon batch arrival, recompute affected hub distances using Dijkstra's algorithm from each hub, achieving O(k·(m + n log n)) amortized update cost. Each query answers in O(k) time by scanning all hubs. Memory usage is O(k·n) for storing hub-to-vertex distances. Two practical optimizations: (1) maintain only distances to vertices within radius R from each hub, pruning distant vertices; (2) use incremental Dijkstra with priority queue reuse across batches. Compared to rebuilding from scratch per batch (O(s·(m + n log n)) total), this shared structure reduces total cost by factor O(s/k) while maintaining (1+ε) approximation guarantee.","explanation":"## Why This Is Asked\nExplores practical dynamic approximate all-pairs shortest paths (APSP) with shared structure; tests understanding of trade-offs between update costs and query time in batch-dynamic settings.\n\n## Key Concepts\n- Batch-dynamic graphs with nonnegative edge weights\n- Hub-based distance sketches for approximate APSP\n- Amortized analysis across multiple query pairs\n- Trade-offs between approximation accuracy ε and computational efficiency\n- Incremental shortest path algorithms\n\n## Code Example\n```javascript\n// Hub-based distance approximation: dist(u,v) ≈ min_h∈H [dist(u,h) + dist(h,v)]\n\nclass HubBasedAPSP {\n  constructor(graph, epsilon) {\n    this.k = Math.ceil((1/epsilon) * Math.log(graph.n));\n    this.hubs = this.selectHubs(graph);\n    this.distances = new Map(); // hub -> vertex distances\n  }\n\n  updateBatch(edges) {\n    for (const hub of this.hubs) {\n      this.distances.set(hub, this.incrementalDijkstra(hub, edges));\n    }\n  }\n\n  query(u, v) {\n    let minDist = Infinity;\n    for (const hub of this.hubs) {\n      const dist = this.distances.get(hub).get(u) + \n                   this.distances.get(hub).get(v);\n      minDist = Math.min(minDist, dist);\n    }\n    return minDist;\n  }\n}\n```","diagram":"flowchart TD\n  S[Start] --> U[Update batch B]\n  U --> Q[Answer s queries via hubs]\n  Q --> R[Optional prune/rebuild hubs]\n  R --> S","difficulty":"advanced","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","Plaid","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T05:32:59.572Z","createdAt":"2026-01-17T22:37:49.256Z"},{"id":"q-3629","question":"In a directed graph with N nodes and M edges, edges arrive online in batches of size B. After each batch, estimate, with high probability, the number of nodes reachable from a fixed set S within at most D hops, within a (1±ε) factor. Propose a streaming sampling scheme: (i) pick k random walks of length D starting from S or uniformly; (ii) specify update rules per batch; (iii) derive variance/error bounds and how to set k for a target failure prob; (iv) amortized time and memory; (v) compare to rerunning a bounded-depth multi-source BFS after each batch. Provide asymptotics in n,m,k,B,ε,D?","answer":"Use k independent random walks of length D seeded from S. Each batch extends walks through the new edges; maintain lastVisit[v] as the last batch id that touched v. The estimator of Reach_D(S) is the number of distinct nodes visited across all walks, scaled by N/|S|k. Update rules: for each new edge (u,v) in batch, if lastVisit[u] equals current batch id, propagate walk to v with probability 1/k, updating lastVisit[v]. Variance bounds: Var(estimator) ≤ N²/k, so setting k = O(1/(ε²δ)) achieves (1±ε) approximation with failure probability δ. Amortized time per batch: O(B·k), memory: O(N + k·D). Compared to bounded-depth multi-source BFS (O(B·|S|·D) time, O(N) memory per batch), random walks achieve O(|S|/k) factor speedup for large |S| with comparable memory.","explanation":"## Why This Is Asked\nTests dynamic, randomized complexity reasoning for streaming graph data; pushes beyond exact APSP-like or single-source updates to practical sampling-based approximations.\n\n## Key Concepts\n- Dynamic sampling, Monte Carlo guarantees, batch updates, memory/time trade-offs, reachability estimation.\n\n## Code Example\n```javascript\n// Pseudocode sketch: maintain lastVisit as IntArray[N]\n// and frontier queues for k walks; advance via new edges only\nfunction updateBatch(edges, batchId) {\n  for (let [u,v] of edges) {\n    if (lastVisit[u] === batchId) {\n      if (Math.random() < 1/k) {\n        lastVisit[v] = batchId;\n        visitedNodes.add(v);\n      }\n    }\n  }\n}\n```\n\n## Follow-up Questions\n- How would you adapt if S changes over time?\n- How to handle high out-degree bursts?\n- What if edges can be deleted as well as added?","diagram":null,"difficulty":"advanced","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Discord","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T05:07:32.976Z","createdAt":"2026-01-18T02:31:31.639Z"},{"id":"q-3881","question":"Scenario: In a weighted undirected graph G=(V,E) with nonnegative weights, edges arrive in online batches of size B. From a fixed root r, maintain a (1+ε)-approximate shortest path tree (SPT) to all nodes after each batch. Propose a practical scheme that (i) initializes the SPT, (ii) updates after a batch with amortized time, (iii) bounds memory, (iv) offers two optimizations, and (v) compares to rebuilding the SPT from scratch after every batch. Provide asymptotics in n=|V|, m=|E|, B, ε?","answer":"Initialize the SPT with a single Dijkstra run from r. For each batch, relax only edges touching affected vertices and propagate improvements along the current tree until changes fall below ε·dist(v). ","explanation":"## Why This Is Asked\nTests ability to design dynamic approximate SPTs under batched updates, balancing accuracy and update cost, a realistic concern for large-scale graphs in production.\n\n## Key Concepts\n- (1+ε)-approximate SPT maintenance\n- Batch-driven relaxations with local propagation\n- Periodic full rebuild to bound drift\n- Time/memory trade-offs; practical optimizations\n\n## Code Example\n```javascript\n// Pseudo-code sketch\ninitializeSPT(r);\nfor each batch B:\n  relaxEdges(B);\n  if (batchIndex % T === 0) rebuildSPTFromScratch();\n```\n\n## Follow-up Questions\n- How to choose T to optimize long-term cost?\n- How would you adapt to edge deletions or negative-weight adjustments?\n","diagram":null,"difficulty":"advanced","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","NVIDIA","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T13:44:44.560Z","createdAt":"2026-01-18T13:44:44.560Z"},{"id":"q-3905","question":"Scenario: A ride-hailing network graph G=(V,E) with nonnegative weights receives edge insertions in batches of size B. You must maintain an ε-approximation to the top-r eigenvector centrality for all nodes after each batch, with update time sublinear in n. Propose a concrete scheme: (i) initial computation, (ii) update after batch using low-rank ΔA and a few subspace iterations, (iii) how to answer centralities fast, (iv) memory bounds, (v) two optimizations, (vi) comparison to recomputing from scratch after every batch. Express asymptotics in n, m, B, r, ε?","answer":"Maintain a rank-r approximation X of the top eigenvectors of A. After each batch ΔA, compute a low-rank update on the touched subgraph and perform a few subspace iterations to refresh X. Centralities ","explanation":"## Why This Is Asked\n\nTests ability to design incremental spectral methods for dynamic graphs and reason about trade-offs between update time, accuracy, and memory.\n\n## Key Concepts\n\n- Eigenvector centrality; Rayleigh quotient\n- Low-rank updates; batch dynamics\n- Subspace iteration; spectral gap\n- Complexity trade-offs; memory vs accuracy\n\n## Code Example\n\n```javascript\n// simplified incremental update\nfunction updateCentrality(X, A, DeltaA, touched) {\n  const Y = project(DeltaA, X);\n  X = orthonormalize(X.add(DeltaA.dot(X)));\n  return X;\n}\n```\n\n## Follow-up Questions\n\n- How does the spectral gap affect ε guarantees?\n- How would you extend to dynamic node insertions?","diagram":"flowchart TD\n  A[Batch ΔA] --> B[Low-rank projection]\n  B --> C[Subspace iterations on X]\n  C --> D[Updated X (r-dim)]\n  D --> E[Answer centrality via Rayleigh quotient]\n  E --> F[Queries O(1)]","difficulty":"advanced","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Plaid","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T14:37:04.049Z","createdAt":"2026-01-18T14:37:04.049Z"},{"id":"q-3920","question":"In a directed weighted graph G=(V,E) with nonnegative weights, edges are inserted online in batches of size B. After each batch you must maintain approximate eccentricities ecc(v)=max_u dist(v,u) for all v within a (1+ε) factor. Propose a practical scheme combining (i) a landmark-based distance substructure from a fixed set L (|L|=k), (ii) selective exact updates for nodes affected by the batch, and (iii) a periodic rebuild. Provide preprocessing, per-batch update time, memory, and a comparison to recomputing all-pairs eccentricities from scratch after each batch. Express asymptotics in n=|V|, m=|E|, k, B, ε?","answer":"Use a two-tier approach: precompute dist(.,ℓ) for k landmarks ℓ∈L; after a batch of B insertions, identify affected nodes (endpoints plus neighborhood up to depth d) and run bounded Dijkstra to tighte","explanation":"## Why This Is Asked\nTests practical dynamic distance maintenance with provable bounds and batch updates.\n\n## Key Concepts\nDynamic graphs, landmarks, incremental shortest paths, amortized analysis, rebuild strategies.\n\n## Code Example\n```javascript\n// placeholder\n```\n\n## Follow-up Questions\n- How would you adapt if edge weights change? \n- How to pick k and T for balanced latency and memory?","diagram":null,"difficulty":"advanced","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T15:36:44.940Z","createdAt":"2026-01-18T15:36:44.941Z"},{"id":"q-3971","question":"Scenario: A DAG G=(V,E) with a fixed source S. Each edge e=(u,v) contributes 1 path along that edge; the number of distinct S→v paths is computed by DP over a topological order. Edges can be inserted online in batches of size B. Propose a practical hybrid scheme that maintains exact path counts to all v after each batch using (i) an incremental update pass limited to affected nodes, (ii) a periodic full recomputation every T batches, and (iii) a lazy rebuild trigger when incremental cost exceeds a threshold. Include initial preprocessing, amortized update time per batch, memory bounds, two optimizations, and a comparison to re-running the full DP after every batch?","answer":"Propose maintaining S→v path counts in a DAG by DP over a topological order. Precompute counts with counts[S]=1. After each batch of B edge insertions, propagate deltas through the affected region in ","explanation":"## Why This Is Asked\nTests dynamic programming under streaming updates and amortized analysis, a common real-world concern in routing, scheduling, and graph analytics at scale.\n\n## Key Concepts\n- Dynamic DP on DAGs, topological order maintenance\n- Batch update amortization, threshold-based lazy rebuild\n- Trade-offs: incremental vs full recomputation, memory bounds\n\n## Code Example\n```javascript\n// Pseudo: update path counts on batch insertions\nfunction updateBatchCounts(G, batch) {\n  // G: DAG with topo order; counts[v] stored\n  let touched = new Set();\n  for (let e of batch) {\n    // e: {u,v}\n    touched.add(e.v); touched.add(e.u);\n  }\n  // propagate in topo order from minIndex(touched)\n  // ... detailed implementation depends on graph structure\n}\n```\n\n## Follow-up Questions\n- How would you handle large path counts exceeding 64-bit and avoid overflow?\n- How to optimally choose B and T given dynamic update patterns?\n- Could this extend to counting modulo a large prime to preserve exactness with bounded memory?","diagram":"flowchart TD\n  A[Batch Insertions] --> B[Affected Region]\n  B --> C[Incremental Update]\n  C --> D{Delta > Threshold?}\n  D -->|Yes| E[Lazy Rebuild / Full DP]\n  D -->|No| F[Wait for next batch]\n  E --> G[Periodic Full Rebuild Every T Batches]\n  G --> H[Update Counts]\n  H --> I[Ready for Next Batch]","difficulty":"intermediate","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","NVIDIA","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T17:39:31.122Z","createdAt":"2026-01-18T17:39:31.122Z"},{"id":"q-4046","question":"Scenario: A log analytics service processes a high-velocity stream of user IDs and needs the number of unique users in the last W events (sliding window). Design two practical strategies to maintain that count as IDs arrive: (A) recompute from scratch for every window; (B) maintain a hash-map of counts and update incrementally. Provide asymptotics for time and space in terms of N, W, and batch size B, and compare to a full recompute after each batch?","answer":"Naive: O((N−W+1)W) time, O(W) memory; recomputes each window. Incremental: O(1) average per ID, O(W) memory, total O(N) time; use a hash map of counts by ID; on slide, decrement leaving ID and remove ","explanation":"## Why This Is Asked\nTests practical time-space reasoning for streaming window problems; contrasts naive recomputation with incremental maintenance under amortized analysis. \n\n## Key Concepts\n- Sliding window distinct count; hash-map counts; amortized O(1) updates; space O(W) (or O(U) where U is unique in window).\n\n## Code Example\n```javascript\n// Naive\nfunction distinctLastW(arr, W) {\n  const window = arr.slice(-W);\n  return new Set(window).size;\n}\n\n// Incremental (sketch)\nfunction distinctLastWIncremental(stream, W) {\n  const counts = new Map();\n  let distinct = 0;\n  // first W\n  // then slide: update counts for leaving and entering IDs\n  return distinct;\n}\n```\n\n## Follow-up Questions\n- How would you adapt for variable window sizes or out-of-order arrivals?\n- What data structure tweaks reduce hash overhead in high-cardinality streams?","diagram":"flowchart TD\n  A[New ID arrives] --> B{Window full?}\n  B -- Yes --> C[Decrement leaving ID count]\n  B -- No --> D[No removal yet]\n  C --> E[Increment entering ID count]\n  D --> E\n  E --> F[Report distinct count]","difficulty":"beginner","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Bloomberg","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T21:28:49.518Z","createdAt":"2026-01-18T21:28:49.518Z"},{"id":"q-4223","question":"You have n users as undirected nodes. Edges arrive in batches of size B, each batch adding friendships. After each batch, report the number of connected components in the graph. Propose a practical incremental scheme using a union-find (disjoint-set) structure with path compression and union by rank. Provide amortized per-edge cost, per-batch cost, and memory in terms of n, m, B, alpha(n). Also discuss when a rebuild might be preferable?","answer":"Initialize a DSU with n nodes and components=C=n. For each batch of B edges, run union(u,v) for every edge; if a union merges two sets, C--. After the batch, report C. Amortized per-edge cost is alpha","explanation":"## Why This Is Asked\nTests incremental connectivity intuition, DSU basics, and amortized analysis for real-world batch updates.\n\n## Key Concepts\n- Union-Find with path compression and union by rank\n- Alpha(n)\n- Batch vs online update trade-offs\n\n## Code Example\n```javascript\nclass DSU {\n  constructor(n){\n    this.parent = Array.from({length:n}, (_,i)=>i);\n    this.rank = new Array(n).fill(0);\n    this.components = n;\n  }\n  find(x){ return this.parent[x]===x? x: this.find(this.parent[x]); }\n  union(a,b){ let ra=this.find(a), rb=this.find(b); if(ra===rb) return false; if(this.rank[ra]<this.rank[rb])[ra,rb]=[rb,ra]; this.parent[rb]=ra; if(this.rank[ra]===this.rank[rb]) this.rank[ra]++; this.components--; return true; }\n}\n```\n\n## Follow-up Questions\n- How would you adapt for directed graphs and counting weak components?\n- What heuristics decide when to trigger a rebuild versus continuing online?","diagram":"flowchart TD\n  A[Batch arrives] --> B{New nodes?}\n  B -- Yes --> C[Init DSU entries for new nodes]\n  B -- No --> D[Union edges in batch]\n  D --> E[Update components]\n  E --> F[Report total components]","difficulty":"beginner","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-19T09:13:22.138Z","createdAt":"2026-01-19T09:13:22.138Z"},{"id":"q-4296","question":"Count inversions in an array of length n. Compare the naive approach O(n^2) with a merge-sort based approach that runs in O(n log n). Explain which inputs favor each, and discuss space usage and stability implications. End with ?","answer":"Classic: naive inversion count checks all pairs, giving O(n^2) time. The merge-sort based method counts inversions during merging, achieving O(n log n) time and O(n) extra space. For large n or random","explanation":"## Why This Is Asked\nThis tests basic algorithmic analysis and trade-offs between brute force and divide-and-conquer techniques.\n\n## Key Concepts\n- Time complexity and space usage\n- Inversions problem and two standard approaches\n- Trade-offs with input size and data order\n\n## Code Example\n```javascript\nfunction countInversionsNaive(a){\n  let c=0;\n  for(let i=0;i<a.length;i++){\n    for(let j=i+1;j<a.length;j++){\n      if(a[i]>a[j]) c++;\n    }\n  }\n  return c;\n}\nfunction countInversionsMergeSort(a){\n  function sortCount(arr){\n    if(arr.length<=1) return {array: arr, inv:0};\n    const mid = Math.floor(arr.length/2);\n    const left = sortCount(arr.slice(0,mid));\n    const right = sortCount(arr.slice(mid));\n    let i=0,j=0, merged=[], inv= left.inv + right.inv;\n    while(i<left.array.length && j<right.array.length){\n      if(left.array[i] <= right.array[j]) merged.push(left.array[i++]);\n      else { merged.push(right.array[j++]); inv += left.array.length - i; }\n    }\n    return {array: merged.concat(left.array.slice(i)).concat(right.array.slice(j)), inv: inv};\n  }\n  return sortCount(a).inv;\n}\n```\n\n## Follow-up Questions\n- How would you extend the merge-count to return the actual inversion pairs?\n- What changes if the input is already sorted or nearly sorted?","diagram":null,"difficulty":"beginner","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-19T11:51:09.110Z","createdAt":"2026-01-19T11:51:09.110Z"},{"id":"q-4352","question":"You have an integer array a of length n. You receive online batches of B point updates (set a[i] = v). After each batch you must answer: how many elements in a exceed a fixed threshold T? Propose a practical data structure and amortized analysis for updates and queries, including preprocessing time, memory, and a comparison to recomputing counts from scratch after every batch. Assume B ≤ n and T is fixed?","answer":"Use sqrt-decomposition: partition a into blocks of size s ≈ sqrt(n) and store for each block cnt_gt = # of elements > T. Preprocessing: O(n) time, O(n) space. On update a[i] = v, adjust cnt_gt of its ","explanation":"## Why This Is Asked\nNew angle: practical data-structure design for batch updates.\n\n## Key Concepts\n- Amortized analysis\n- sqrt decomposition\n- fixed-threshold counters\n\n## Code Example\n```javascript\n// Pseudocode illustrating update and query\n```\n\n## Follow-up Questions\n- How would you adapt for varying thresholds without rebuilding?\n- What if B is large relative to n?","diagram":null,"difficulty":"beginner","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Apple","Databricks"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-19T15:38:59.062Z","createdAt":"2026-01-19T15:38:59.062Z"},{"id":"q-4459","question":"You receive a real-time stream of integers in [0, M-1], arriving in batches of size B. After each batch, determine whether value x has appeared at least t times so far. Propose a Count-Min Sketch based solution: set width w and depth d in terms of ε and δ to bound errors, compute update time per batch, per-query time, memory, and compare to exact counting. Assume B, t, ε, δ fixed?","answer":"Use Count-Min Sketch with width w = ceil(e/ε) and depth d = ceil(ln(1/δ)). For each element, update all d counters; a batch of B elements costs O(Bd). To answer freq(x), take the minimum across the d ","explanation":"## Why This Is Asked\nStreaming frequency sketches are common in prod telemetry. This question probes knowledge of CMS, accuracy guarantees, batch processing, and amortized space-time trade-offs.\n\n## Key Concepts\n- Count-Min Sketch (CMS)\n- ε, δ error bounds\n- Update/query complexity for batches\n- Space complexity O(wd)\n- Trade-offs vs exact counting\n\n## Code Example\n```javascript\n// CMS skeleton\nconst w = Math.ceil(Math.E / eps);\nconst d = Math.ceil(Math.log(1/delta));\nconst table = Array.from({length: d}, () => Array(w).fill(0));\n\nfunction update(x) { for (let i = 0; i < d; i++) table[i][hash_i(i, x) % w]++; }\n\nfunction query(x) { let m = Infinity; for (let i = 0; i < d; i++) m = Math.min(m, table[i][hash_i(i, x) % w]); return m; }\n```\n\n## Follow-up Questions\n- How would deletions or non-monotone streams affect CMS choices?\n- How would you monitor actual error in production without degrading performance?","diagram":null,"difficulty":"beginner","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","DoorDash","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-19T19:43:56.248Z","createdAt":"2026-01-19T19:43:56.248Z"},{"id":"q-4598","question":"Dynamic SSSP under batch edge insertions: A directed graph with n nodes; edges are added online in batches of size B. After each batch, maintain approximate distances dist(s,v) from a fixed source s within a (1+ε) factor for all v. Propose a practical scheme using (i) a fixed landmark set L with precomputed dist(s,l) and dist(l,v), (ii) selective exact relaxations for nodes affected by the batch, and (iii) periodic rebuild. Provide preprocessing, per-batch update time, memory, and a comparison to recomputing SSSP from scratch?","answer":"Leverage k landmarks; precompute dist(s,l) for l in L and dist(l,v) via a one-time multi-source run. For each batch, run relaxations only from nodes touched by new edges, updating dist(s,v) via d(s,u)","explanation":"## Why This Is Asked\nTests practical design of dynamic shortest paths with batch updates, balancing accuracy and speed.\n\n## Key Concepts\n- Incremental relaxations; landmark sketches; periodic rebuilds; amortized cost.\n- Trade-offs: landmark count k, rebuild threshold, sparse updates.\n- Correctness: maintains (1+ε) approximation under non-deleting edges.\n\n## Code Example\n```javascript\n// Pseudo: maintain distS[v], distL[l][v], update on batch\n```\n\n## Follow-up Questions\n- How would you extend to multiple sources or undirected graphs?\n- What about negative edges, or dynamic edge deletions?","diagram":"flowchart TD\n  A[Batch arrive] --> B[Relax touched nodes]\n  B --> C{Improve dist?}\n  C -->|Yes| D[Update dist(s,v) via landmarks]\n  C -->|No| E[Batch end]\n  D --> F[Periodic rebuild?]\n  F --> G[Full SSSP refresh]\n  E --> A","difficulty":"advanced","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T04:11:57.938Z","createdAt":"2026-01-20T04:11:57.941Z"},{"id":"q-4702","question":"Real-time logs: integers in [0, M-1] arrive in batches of size B. After each batch, determine how many distinct values have appeared at least t times in total so far. Propose a practical solution using approximate counting, discuss update/query costs, and memory, and compare to recomputing counts from scratch. Express costs in terms of M, B, t, and number of processed batches?","answer":"Use a Count-Min Sketch CMS with width w=ceil(e/ε) and depth d=ceil(ln(1/δ)) to track per-value counts. Maintain a hash-set F of values whose CMS-estimate ≥ t; F grows only when a new value crosses t. ","explanation":"## Why This Is Asked\nTests ability to design streaming, memory‑efficient counting with provable guarantees beyond exact recounts.\n\n## Key Concepts\n- Count-Min Sketch for non-negative counts; parameter trade-offs ε, δ\n- Thresholded tracking of crossing values; lazy enumeration\n- Amortized batch updates vs full re-scan\n\n## Code Example\n```javascript\nclass CMS { constructor(w, d) { /* init */ } update(x, delta=1) { /* per-hash row */ } estimate(x) { /* min over rows */ } }\n```\n\n## Follow-up Questions\n- How to choose ε, δ for a target error?  \n- How would you extend for deletions or windowed counts?  \n- What's the effect of hash collisions on |F| accuracy?","diagram":"flowchart TD\n  Start --> CMS_Update\n  CMS_Update --> Threshold_Check\n  Threshold_Check --> Update_F\n  Update_F --> Return_Count","difficulty":"beginner","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Instacart","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T08:53:43.396Z","createdAt":"2026-01-20T08:53:43.396Z"},{"id":"q-4715","question":"Dynamic graph: edges are inserted in batches of size B in a weighted directed graph. After each batch, design a practical incremental algorithm to maintain approximate betweenness centrality for all nodes within a (1+ε) factor. Use sampling with s sources and incremental SP-tree updates; specify preprocessing time, per-batch update time, memory, and compare to recomputing BC from scratch?","answer":"Use a sampling-based approximate betweenness centrality with dynamic updates. Sample s sources; maintain weighted shortest-path trees from each source. On a batch of B edge insertions, identify affect","explanation":"## Why This Is Asked\nTests ability to combine sampling, incremental shortest-path maintenance, and BC error bounds in a dynamic setting.\n\n## Key Concepts\n- Betweenness centrality, sampling-based approximations, incremental shortest paths, dynamic graphs, error guarantees.\n\n## Code Example\n```javascript\nfunction init(n, m, s) {\n  // init sampling and data structures\n}\n```\n\n## Follow-up Questions\n- How to choose s to meet a target ε?\n- How to parallelize updates across sources and scale with high-degree batches?\n","diagram":null,"difficulty":"advanced","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","Oracle","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T09:43:16.276Z","createdAt":"2026-01-20T09:43:16.276Z"},{"id":"q-4830","question":"You receive a stream of integers in [0, M-1] in batches of B. Maintain a sliding window of the most recent N elements. After each batch, report the current number of distinct values in the window. Propose a practical data structure (e.g., hashmap + queue) to support batch inserts, updates, and a correct count, and derive update time, memory, and a comparison to recomputing from scratch?","answer":"Use a queue for the last N values and a hashmap freq to count occurrences in the window. For each batch, enqueue B items and increment freq; while queue.size() > N, dequeue oldest and decrement freq, ","explanation":"## Why This Is Asked\nTests ability to design a simple real-time data structure for sliding-window distinct counting, including correctness, amortized analysis, and memory trade-offs.\n\n## Key Concepts\n- Sliding window maintenance\n- Hash map counters\n- Amortized analysis\n- Streaming memory trade-offs\n\n## Code Example\n```python\nfrom collections import deque, Counter\nclass SlidingDistinct:\n    def __init__(self, N):\n        self.N = N\n        self.q = deque()\n        self.c = Counter()\n    def batch(self, arr):\n        for x in arr:\n            self.q.append(x)\n            self.c[x] += 1\n        while len(self.q) > self.N:\n            y = self.q.popleft()\n            self.c[y] -= 1\n            if self.c[y] == 0:\n                del self.c[y]\n```\n\n## Follow-up Questions\n- How would you adapt to dynamic N?\n- What if value domain is large or unbounded; how would you bound memory?\n","diagram":null,"difficulty":"beginner","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","MongoDB","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T15:11:26.018Z","createdAt":"2026-01-20T15:11:26.018Z"},{"id":"q-678","question":"In a directed acyclic graph with N nodes and M edges, all edge costs are nonnegative. Compute the minimum-cost path from S to T. Costs can decrease online; design a strategy to maintain shortest paths with updates, aiming for sublinear re-computation on average. Provide initial complexity and amortized update complexity, plus memory usage and practical optimizations?","answer":"Initial: run a topological DP in O(N+M) to obtain dist to all nodes. Online decreases trigger relaxations: if w(u,v) decreases and dist[u]+w(u,v) < dist[v], update dist[v] and propagate to successors ","explanation":"## Why This Is Asked\n\nTests online update handling, DAG shortest paths, and amortized analysis.\n\n## Key Concepts\n\n- DAG shortest paths via topological order\n- Dynamic relaxations under decreasing edge weights\n- Amortized analysis and worst-case bounds\n\n## Code Example\n\n```javascript\nfunction updateShortestPaths(n, adj, dist, s, t, u, v, wOld, wNew){\n  // assume we know wOld and wNew with a decrease; perform relaxation\n  if(dist[u] + wNew < dist[v]){\n    dist[v] = dist[u] + wNew;\n    // push v to a queue and relax its outgoing edges\n  }\n  // ... full propagation would run until convergence\n}\n```\n\n## Follow-up Questions\n\n- How would you adapt for non-DAG graphs?\n- How does update order affect performance in practice?","diagram":"flowchart TD\n  A[Initial topological DP] --> B[Edge decrease detected]\n  B --> C[Relax dist[v]]\n  C --> D[Propagate changes]\n  D --> E[Converged]","difficulty":"advanced","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","Meta","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T15:56:43.115Z","createdAt":"2026-01-11T15:56:43.115Z"},{"id":"q-690","question":"Design a data structure to support two online operations on an integer array A of length N: 1) rangeAdd(l, r, delta) adds delta to A[i] for l <= i <= r, 2) queryMaxSubarray() returns the maximum subarray sum of the current A. Provide a structure that supports both operations in O(log N) time, describe what to store per node, how to merge children, and how to apply a lazy add. Include correctness and complexity considerations?","answer":"Use a segment tree with lazy propagation. Each node stores: total sum, max prefix, max suffix, and max subarray. To add v to a range, lazily update the node: adjust sum, pref, suff, and propagate lazi","explanation":"## Why This Is Asked\nTests ability to combine range updates with non-linear objective (max subarray) and shows knowledge of segment trees, lazy propagation, and invariants under updates.\n\n## Key Concepts\n- Segment tree with lazy propagation\n- Node values: sum, max prefix, max suffix, max subarray\n- Merge operation correctness\n- Complexity analysis: O(log N) per op, O(N) memory\n- Edge cases: all negative arrays, large deltas, non-overlapping ranges\n\n## Code Example\n```javascript\nclass Node { constructor(sum=0,pref=-Infinity,suff=-Infinity,best=-Infinity){ this.sum=sum; this.pref=pref; this.suff=suff; this.best=best; } }\n```\n\n## Follow-up Questions\n- How would you adapt if rangeAdd is replaced with rangeMultiply?\n- How would you extend to support range assign and query across multiple segments?","diagram":null,"difficulty":"intermediate","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T16:23:52.242Z","createdAt":"2026-01-11T16:23:52.242Z"},{"id":"q-700","question":"You're building a real-time analytics dashboard that shows the top-k most frequent event types from a high-volume log stream (e.g., clicks, errors). Each event has a string type. Design a data structure and algorithm to maintain the current top-k frequencies with online increments, aiming for roughly O(log k) update time and O(n) memory. Explain how you handle ties and memory growth, and compare with a naive approach that re-sorts after every insert?","answer":"Use a hash map to count frequencies and a min-heap of size k to track current top-k. On event type t: increment freq[t]; if t is in heap, update its priority; else if heap size < k, insert (freq[t], t","explanation":"## Why This Is Asked\nThis probes real-time top-k maintenance and amortized analysis using simple DS.\n\n## Key Concepts\n- HashMap for counts\n- Min-heap for top-k\n- Tie-breaking rules\n- Memory-use trade-offs\n\n## Code Example\n```javascript\nfunction update(freq, heap, k, t) {\n  freq[t] = (freq[t] || 0) + 1;\n  if (heap.has(t)) adjust(heap, t);\n  else if (heap.size < k) heap.push([freq[t], t]);\n  else if (freq[t] > heap.min()[0]) heap.replace([freq[t], t]);\n}\n```\n\n## Follow-up Questions\n- How would you handle deletions or aging counts?\n- How would you test correctness with sequences simulating bursts?","diagram":null,"difficulty":"beginner","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T17:19:43.376Z","createdAt":"2026-01-11T17:19:43.376Z"},{"id":"q-704","question":"Scenario: a directed graph with nonnegative weights and a fixed source S. Each batch updates up to B edges (increases or decreases). Propose a practical dynamic data structure to maintain exact distances from S to all nodes and answer distance queries S→T in polylog time, with sublinear amortized update time. Compare to rerunning Dijkstra after every batch; include expected bounds, memory usage, and practical heuristics?","answer":"Maintain an exact SSSP with a dynamic SPT plus a limited local reoptimization. For a batch of up to B edge-weight changes, re-evaluate only the affected subtree via a replacement-path frontier and rew","explanation":"## Why This Is Asked\nAssesses skill in dynamic graph algorithms, not just static shortest paths; requires designing practical online maintenance with bounds.\n\n## Key Concepts\n- Dynamic shortest paths\n- Replacement paths and locality\n- Batch updates and amortized analysis\n- Potentials for nonnegative weights\n- SPT maintenance\n\n## Code Example\n```javascript\n// Skeleton: dynamic SSSP maintenance interface\nclass DynamicSSSP {\n  constructor(graph, s) { /* ... */ }\n  batchUpdate(changes) { /* changes: Array<{u,v,wDelta}> */ }\n  distanceTo(v) { /* return dist from s to v */ }\n}\n```\n\n## Follow-up Questions\n- How to adapt when B ≈ M or updates are adversarial?\n- How would you extend to multiple sources and dynamic graph rebuilds?","diagram":null,"difficulty":"advanced","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Salesforce","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T18:27:46.453Z","createdAt":"2026-01-11T18:27:46.453Z"},{"id":"q-709","question":"Given a directed graph with nonnegative weights, a fixed source S, and a stream of online edge weight updates (both increases and decreases), design a dynamic SSSP data structure that maintains exact distances dist(S, v) for all v after each update. Aim for sublinear amortized update time per edge change; specify initial preprocessing, worst-case vs amortized bounds, memory usage, and practical optimizations. Provide a plan for applying this in a traffic-graph scenario with frequent but localized updates?","answer":"Maintain dist from S to all nodes with a dynamic SSSP structure: run full Dijkstra once to initialize (O(M log N)). For each edge weight update (u, v, δ), re-relax only nodes affected by the change us","explanation":"## Why This Is Asked\nDynamic maintenance of shortest paths under online updates is critical in production traffic graphs. Candidates must reason about amortized analysis, locality, and practical heuristics beyond static re-computation.\n\n## Key Concepts\n- Dynamic shortest paths\n- Amortized analysis\n- Localized relaxation\n- Priority queues and delta updates\n- Trade-offs: exactness vs approximate\n\n## Code Example\n```javascript\n// Pseudo approach sketch: re-relaxation function\nfunction updateEdge(u,v,delta){\n  // update weight\n  // if newDist improves dist[v], push to PQ and propagate\n}\n```\n\n## Follow-up Questions\n- How would you handle multiple concurrent updates efficiently?\n- What metrics would you monitor in production?","diagram":null,"difficulty":"intermediate","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["OpenAI","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T19:15:05.608Z","createdAt":"2026-01-11T19:15:05.608Z"},{"id":"q-721","question":"Given a fixed directed graph with nonnegative weights and a single source S, handle a batch of edge-weight decreases (no insertions/deletions). Design a dynamic algorithm to update the exact S→v distances after each batch with sublinear amortized per-edge cost. Specify data structures, provide an amortized bound, and discuss memory and practical optimizations for real-time traffic networks?","answer":"Use dist[] and a min-heap. For each decreased edge (u,v) to w', if dist[u]+w' < dist[v], set dist[v] = dist[u]+w' and push v; repeatedly pop and relax neighbors until the queue empties. This confines ","explanation":"## Why This Is Asked\nDynamic SSSP with weight-decreases mirrors real-time route/latency updates. This tests practical algorithm design, data-structure choices, and amortized reasoning for production graphs.\n\n## Key Concepts\n- Dynamic shortest paths under monotone updates\n- Dijkstra-like propagation with lazy updates\n- Priority queues vs bucketed approaches for integer weights\n- Amortized analysis based on updated edges and affected nodes\n- Memory footprint: O(N+M)\n\n## Code Example\n```javascript\n// Pseudocode for batch update\nfunction updateBatch(changes) {\n  for (const {u,v,w} of changes) {\n    if (dist[u] + w < dist[v]) {\n      dist[v] = dist[u] + w;\n      pq.push(v);\n    }\n  }\n  while (!pq.isEmpty()) {\n    const x = pq.pop();\n    for (const e of adj[x]) {\n      if (dist[x] + e.w < dist[e.v]) {\n        dist[e.v] = dist[x] + e.w; pq.push(e.v);\n      }\n    }\n  }\n}\n```\n\n## Follow-up Questions\n- How would you adapt for batches that include edge insertions/deletions?\n- Compare binary-heap vs Dial's bucket approach for integer weights in [0, W].","diagram":"flowchart TD\n  S[Source]\n  D[Decrease batch]\n  P[Relax candidates]\n  Q[Queue updates]\n  E[Edge relaxations]\n  S --> D\n  D --> P\n  P --> Q\n  Q --> E\n  E --> P","difficulty":"intermediate","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","NVIDIA","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T20:23:01.260Z","createdAt":"2026-01-11T20:23:01.260Z"},{"id":"q-732","question":"Scenario: A data stream yields integers. At each time step, a new value enters a sliding window of fixed size W, and the oldest value leaves. Design a solution to maintain the top-2 most frequent values in the current window with fast updates. Compare a naive O(W) recompute to an augmented structure using a frequency map and a max-heap with lazy deletions. Provide update and query complexities and memory usage?","answer":"Naive: on each slide recompute the top-2 by scanning the W elements, O(W) per step. Improved: maintain a frequency map plus a max-heap with lazy deletions. Updates are O(log D) where D is distinct val","explanation":"## Why This Is Asked\n\nTests familiarity with sliding-window problems and practical complexity trade-offs between recomputation and incremental data structures.\n\n## Key Concepts\n\n- Sliding window\n- Frequency counting\n- Hash map and max-heap\n- Lazy deletion\n\n## Code Example\n\n```javascript\nclass TopKWindow {\n  constructor(W){ this.W=W; this.freq=new Map(); this.window=[]; this.maxHeap=[]; }\n  // ... implement add, slide, and getTopK using lazy deletions\n}\n```\n\n## Follow-up Questions\n\n- How would the approach scale for larger k than 2?\n- How would you adapt for non-integer or large-value domains?","diagram":null,"difficulty":"beginner","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Twitter","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T21:19:58.623Z","createdAt":"2026-01-11T21:19:58.623Z"},{"id":"q-740","question":"Scenario: An edge CDN collects response times in milliseconds for every request. Design a beginner-friendly online algorithm to maintain the median latency as new times arrive, using only inserts. Explain the data structure, update steps, and time/space bounds, assuming up to 1e6 entries?","answer":"Use two heaps: a max-heap for the lower half and a min-heap for the upper half. On each new latency x, push to the appropriate heap and rebalance so sizes differ by at most 1. Median is the top of the","explanation":"## Why This Is Asked\n\nTests ability to design a stable online statistic with clear time/space guarantees using simple data structures.\n\n## Key Concepts\n\n- streaming algorithms\n- two-heap median maintenance\n- amortized vs worst-case costs\n- space efficiency\n\n## Code Example\n\n```javascript\n// production-ready sketch of two-heap median tracker\nclass BinaryHeap {\n  constructor(compare){ this.data=[]; this.compare=compare; }\n  size(){ return this.data.length; }\n  top(){ return this.data[0]; }\n  push(x){ this.data.push(x); this._siftUp(this.data.length-1); }\n  pop(){ const top=this.data[0]; const end=this.data.pop(); if (this.data.length){ this.data[0]=end; this._siftDown(0); } return top; }\n  _siftUp(i){ const x=this.data[i]; while(i>0){ const p=(i-1)>>1; if (this.compare(x, this.data[p])>=0) break; this.data[i]=this.data[p]; i=p; } this.data[i]=x; }\n  _siftDown(i){ const n=this.data.length; const x=this.data[i]; while(true){ let l=2*i+1, r=l+1, smallest=i; if (l<n && this.compare(this.data[l], this.data[smallest])<0) smallest=l; if (r<n && this.compare(this.data[r], this.data[smallest])<0) smallest=r; if (smallest===i) break; this.data[i]=this.data[smallest]; i=smallest; } this.data[i]=x; }\n}\n\nclass MedianTracker {\n  constructor(){ this.lower=new BinaryHeap((a,b)=> b-a); this.upper=new BinaryHeap((a,b)=> a-b); }\n  insert(x){ if (this.lower.size()===0 || x <= this.lower.top()) this.lower.push(x); else this.upper.push(x);\n    if (this.lower.size() > this.upper.size()+1) this.upper.push(this.lower.pop());\n    else if (this.upper.size() > this.lower.size()) this.lower.push(this.upper.pop());\n  }\n  median(){ if (this.lower.size() === this.upper.size()) return (this.lower.top() + this.upper.top())/2; return this.lower.top(); }\n}\n```\n\n## Follow-up Questions\n\n- How would you adapt for deletions or outliers removal?\n- How would you validate correctness with unit tests and random streams?","diagram":"flowchart TD\n  S[Stream of latencies] --> I[Insert into appropriate heap]\n  I --> R[Rebalance if needed]\n  R --> M[Median available via tops]\n  M --> Q[Query median in O(1)]","difficulty":"beginner","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Coinbase","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T22:21:49.038Z","createdAt":"2026-01-11T22:21:49.038Z"},{"id":"q-742","question":"In a DAG with N nodes and M edges, nonnegative edge weights. You maintain shortest-path distances from source S to a fixed set of target nodes {T1,...,Tk}. Edge weights can only decrease over time due to updates. After a batch of updates, you should update only the target distances that can improve, avoiding full re-relaxation. Propose a practical algorithm that lazily propagates decreases using the DAG’s topological order, such that total work across updates is sublinear on average. Provide update and query steps, concrete time bounds, and memory usage, plus optimizations?","answer":"Maintain dist from S to all nodes in topological order. On a batch of weight decreases (u->v with newW <= oldW): if dist[v] > dist[u] + newW, update and push v. Pop nodes in increasing topo index, rel","explanation":"## Why This Is Asked\nTests ability to exploit DAG structure for incremental shortest paths with monotone updates and selective recomputation.\n\n## Key Concepts\n- DAG topological order\n- Monotone updates and lazy propagation\n- Selective relaxation focused on targets\n- Amortized analysis of edge relaxations\n\n## Code Example\n```javascript\n// Pseudocode for batch decreases\nfunction applyDecreases(batch) {\n  const queue = [];\n  for (const e of batch) {\n    if (dist[e.v] > dist[e.u] + e.newW) {\n      dist[e.v] = dist[e.u] + e.newW;\n      queue.push(e.v);\n    }\n  }\n  queue.sort((a,b)=>topo[a]-topo[b]);\n  while (queue.length) {\n    const x = queue.shift();\n    for (const y of adj[x]) {\n      const w = weight(x,y);\n      if (dist[y] > dist[x] + w) {\n        dist[y] = dist[x] + w;\n        queue.push(y);\n      }\n    }\n  }\n}\n``` \n\n## Follow-up Questions\n- How would you handle dynamic target set changes efficiently?\n- What edge cases degrade the amortized bound and how could you mitigate them?","diagram":"flowchart TD\n  A[Batch Update] --> B[Relax Affected Nodes]\n  B --> C[Distance Stable]\n  C --> D[Answer Queries for Tk]\n  A --> E[Push Victims to Queue]\n  E --> B","difficulty":"advanced","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Microsoft","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T23:17:59.814Z","createdAt":"2026-01-11T23:17:59.814Z"},{"id":"q-755","question":"You're maintaining real-time travel times in a citywide road network modeled as a weighted directed graph with nonnegative costs. Costs can only decrease as new data arrives. Design an incremental algorithm to keep the shortest-path distances from a fixed hub S to all nodes up-to-date after each edge-cost decrease, aiming for sublinear amortized update work. Provide initial SSSP complexity, amortized per-decrease update, memory usage, and practical optimization strategies?","answer":"Compute initial SSSP with Dijkstra: O(M log N). For a decreased edge (u,v) with w' < w, if dist[u] + w' < dist[v], update dist[v] and propagate relaxations through the graph using a PQ. Amortized work","explanation":"## Why This Is Asked\n\nTests ability to extend classic shortest-paths to dynamic updates with a concrete, real-world constraint (only decreases) and to analyze practical performance in terms of amortized work and memory.\n\n## Key Concepts\n\n- Dynamic single-source shortest paths with only edge decreases\n- Amortized analysis and potential-based arguments\n- Priority-queue relaxation propagation\n- Memory-time trade-offs and localized updates\n- Practical optimizations: skip non-on-SPT edges, batch updates, integer weights with Dial's algorithm\n\n## Code Example\n\n```javascript\nclass DynamicSSSP {\n  constructor(graph, source) {\n    this.graph = graph; // adjacency: {u: [{v, w}, ...]}\n    this.n = Object.keys(graph).length;\n    this.dist = Array(this.n).fill(Infinity);\n    this.dist[source] = 0;\n    this.pq = new MinHeap(); // custom min-heap with push/pop\n    this.pq.push([0, source]);\n    while (!this.pq.isEmpty()) {\n      const [d, u] = this.pq.pop();\n      if (d !== this.dist[u]) continue;\n      for (const {v, w} of this.graph[u]) {\n        if (this.dist[v] > d + w) {\n          this.dist[v] = d + w;\n          this.pq.push([this.dist[v], v]);\n        }\n      }\n    }\n  }\n  decreaseEdge(u, v, newW) {\n    // update edge weight in graph[u] item\n    // assume newW <= oldW\n    // attempt relaxation\n    if (this.dist[u] + newW < this.dist[v]) {\n      this.dist[v] = this.dist[u] + newW;\n      this.pq.push([this.dist[v], v]);\n      while (!this.pq.isEmpty()) {\n        const [d, x] = this.pq.pop();\n        if (d !== this.dist[x]) continue;\n        for (const {to, w} of this.graph[x] || []) {\n          if (this.dist[to] > d + w) {\n            this.dist[to] = d + w;\n            this.pq.push([this.dist[to], to]);\n          }\n        }\n      }\n    }\n  }\n}\n```\n\n## Follow-up Questions\n\n- How would you adapt if many edges frequently decrease in a short window?\n- How would you extend to multiple sources or all-pairs distances, and what would the complexity trade-offs look like?","diagram":"flowchart TD\n  S[Hub S] --> E[Edge decrease event]\n  E --> R{Relaxation possible?}\n  R -->|Yes| U[Update dist and propagate via Dijkstra]\n  R -->|No| End[Done]","difficulty":"intermediate","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Google","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T01:33:06.200Z","createdAt":"2026-01-12T01:33:06.200Z"},{"id":"q-766","question":"## Prompt\n\nIn a dynamic directed graph G=(V,E) with nonnegative weights, edge latencies only decrease in batches. Design a practical, production-ready algorithm to maintain a (1+ε)-approximate SSSP tree from a source S under these updates, enabling distance queries dist(S,v) in O(log|V|) time. Target sublinear amortized update in |E| for batch updates, and linear space. Explain data structures, update bounds, and how you bound cascade effects?","answer":"Use a landmark-based dynamic SSSP: pick k landmarks; store dist(S,ℓ) and dist(ℓ,v) for all v; approximate dist(S,v) ≈ minℓ dist(S,ℓ)+dist(ℓ,v). On a batch of decreases, relax only paths affected by up","explanation":"## Why This Is Asked\n\nTests ability to reason about dynamic approximate shortest paths with real-world batch updates. Challenges include cascade propagation, landmark selection, and trade-offs between accuracy and update time. This probes knowledge of dynamic data structures and practical heuristics for production systems.\n\n## Key Concepts\n\n- Dynamic shortest paths with updates\n- (1+ε)-approximation via landmarks\n- Amortized analysis under batch updates\n- Space-time trade-offs in DS design\n\n## Code Example\n\n```javascript\nclass LandmarkSSSP {\n  constructor(S, landmarks) {\n    this.S = S;\n    this.landmarks = landmarks;\n  }\n  updateBatch(changes) {\n    // relax locally; recompute affected landmarks lazily\n  }\n  dist(v) {\n    // return min over landmarks dist(S,ℓ)+dist(ℓ,v)\n    return 0;\n  }\n}\n```\n\n## Follow-up Questions\n\n- How to select landmarks adaptively\n- How to bound worst-case cascades on adversarial batches","diagram":"flowchart TD\n  S(Source) --> L1[Landmark1]\n  S --> L2[Landmark2]\n  L1 --> V1[NodeA]\n  L2 --> V1","difficulty":"advanced","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Meta","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T03:51:35.496Z","createdAt":"2026-01-12T03:51:35.496Z"},{"id":"q-770","question":"Given a directed graph G=(V,E) with |V|=N and |E|=M, nonnegative weights, support online operations: 1) decreaseWeight(u,v,delta) with delta>0, 2) queryShortestPath(S,T) returning current shortest path length. Updates and queries are interleaved. Propose a data-structure and algorithm that achieves sublinear amortized reprocessing per update, justify amortized bounds, and discuss space and practical optimizations for massive graphs (N up to 1e6, M up to 1e7)?","answer":"Maintain a two-layer dynamic SSSP: a backbone sparse graph that preserves most shortest paths, plus a frontier-based lazy relaxation region. On decreaseWeight(u,v,delta), only relax from nodes in the ","explanation":"## Why This Is Asked\nTests ability to design dynamic shortest paths with interleaved updates and queries at scale, focusing on amortized analysis and engineering practicality.\n\n## Key Concepts\n- Dynamic graphs and amortized analysis\n- Localized frontier-based relaxations\n- Lazy evaluation and batching for latency control\n\n## Code Example\n```python\nclass DynamicSSSP:\n    def __init__(self, graph, s):\n        self.graph = graph\n        self.dist = [float('inf')] * graph.n\n        self.dist[s] = 0\n        self.cache = {}\n    def decrease(self, u, v, delta):\n        w = delta\n        if self.dist[u] + w < self.dist[v]:\n            self.dist[v] = self.dist[u] + w\n            self._relax_frontier({v})\n    def query(self, t):\n        return self.dist[t]\n    def _relax_frontier(self, frontier):\n        # localized Dijkstra updates\n        pass\n```\n\n## Follow-up Questions\n- How would you extend to edge deletions and negative cycles?\n- How would you empirically validate the amortized bounds on a real-world, large-scale graph?","diagram":"flowchart TD\n  A[Decrease edge weight] --> B[Frontier recomputation check]\n  B --> C{Cache valid?}\n  C -->|Yes| D[Answer query from cache]\n  C -->|No| E[Run localized Dijkstra from frontier]\n  E --> F[Update caches and distances]\n  F --> G[Return to queries]","difficulty":"advanced","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Lyft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T04:49:05.095Z","createdAt":"2026-01-12T04:49:05.095Z"},{"id":"q-779","question":"You're building a real-time analytics component for a fintech platform. You must maintain the number of distinct values in the most recent W events in a streaming fashion. Implement two operations: append(v) to push a new event value, and distinctCount() to return the number of unique values among the last W events. Assume values are integers up to 1e9 and W can be large. Provide a simple approach with its time/memory costs, then describe an amortized-constant-time solution using a hashmap plus a circular buffer, and discuss edge cases (e.g., large W, many duplicates). How would you implement and analyze it?","answer":"Option 1: recompute distinct on each append: O(W) time, O(W) space. Option 2: use a circular buffer of size W and a hashmap counts[value]→freq. On append: evict oldest value, decrement its count; add ","explanation":"## Why This Is Asked\n\nTests sliding-window knowledge, practical data structures, and amortized reasoning relevant to real-time analytics.\n\n## Key Concepts\n\n- Sliding window\n- Amortized analysis\n- Hashmap counters\n- Circular buffer\n\n## Code Example\n\n```python\nclass DistinctWindow:\n    def __init__(self, W):\n        self.W = W\n        self.buf = [None]*W\n        self.left = 0\n        self.right = 0\n        self.size = 0\n        self.counts = {}\n\n    def append(self, v):\n        if self.size == self.W:\n            old = self.buf[self.left]\n            self.buf[self.left] = None\n            self.left = (self.left + 1) % self.W\n            self.size -= 1\n            if old is not None:\n                c = self.counts[old]\n                if c == 1:\n                    del self.counts[old]\n                else:\n                    self.counts[old] = c - 1\n        self.buf[self.right] = v\n        self.right = (self.right + 1) % self.W\n        self.size += 1\n        self.counts[v] = self.counts.get(v, 0) + 1\n\n    def distinctCount(self):\n        return len(self.counts)\n```\n\n## Follow-up Questions\n\n- How does performance change if W is very large but few unique values exist? Any memory optimizations?\n- How would you adapt to a dynamic window size W' that changes over time without recomputing from scratch?","diagram":null,"difficulty":"beginner","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","LinkedIn","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T05:31:28.017Z","createdAt":"2026-01-12T05:31:28.017Z"},{"id":"q-787","question":"You maintain N players with integer scores in the range 0..10000. You must support two online operations: 1) update(i, s) — set player i's score to s; 2) countLE(X) — return how many players have score <= X. Propose a data structure and algorithm to support both in O(log V) time per operation, where V=10001, and analyze space usage. Include initialization and a brief correctness sketch?","answer":"Fenwick tree (BIT) over the value domain 0..10000. Initialize by counting frequencies of each score; update(i, newVal) does bit.add(oldVal, -1) and bit.add(newVal, +1); countLE(X) = bit.sum(X). Comple","explanation":"## Why This Is Asked\nTests ability to map domain-specific queries to a standard data structure and reason about time/space.\n\n## Key Concepts\n- Fenwick Tree (BIT)\n- Prefix sums\n- Coordinate/value-domain handling\n\n## Code Example\n```javascript\nclass Fenwick {\n  constructor(n){ this.n=n; this.f=new Array(n+1).fill(0); }\n  add(i, delta){ for(i+=1; i<=this.n; i+= i&-i) this.f[i]+=delta; }\n  sum(i){ let s=0; for(i+=1; i>0; i-= i&-i) s+=this.f[i]; return s; }\n}\n```\n\n## Follow-up Questions\n- How would you adapt for dynamic value ranges or non-integer scores?\n- How do you handle ties or range queries like count in (a,b]?","diagram":null,"difficulty":"beginner","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Cloudflare","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T06:39:27.526Z","createdAt":"2026-01-12T06:39:27.527Z"},{"id":"q-796","question":"Given an array A of length N with integers in range [0, R). You implement a function to count distinct values by inserting each A[i] into a hash set, then return its size. 1) What is the time and space complexity in terms of N and D (distinct values)? 2) If R is small, propose a memory-efficient alternative and analyze its tradeoffs?","answer":"With a hash set: expected O(N) time; O(D) space where D is distinct values; inserts amortize O(1). Worst-case O(N) time if hash collisions. If R is small, use a bitset/boolean array of size R for O(1)","explanation":"## Why This Is Asked\nDesign and analyze a simple counting distinct values task, contrasting hash-based and range-based approaches. It tests understanding of time vs space tradeoffs and edge cases of hash collisions and bounded value domains.\n\n## Key Concepts\n- Hash-set complexity\n- Distinct values D\n- Bitset vs hash-based counts\n- Space-time tradeoffs\n\n## Code Example\n```javascript\nfunction countDistinct(arr, R){\n  const seen = new Set();\n  for(const x of arr){ seen.add(x); }\n  return seen.size;\n}\n```\n\n## Follow-up Questions\n- How would you adapt for streaming data with sliding windows?\n- What about false positives with probabilistic structures like Bloom filters?","diagram":null,"difficulty":"beginner","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Netflix","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T07:30:21.355Z","createdAt":"2026-01-12T07:30:21.355Z"},{"id":"q-803","question":"In a directed graph G=(V,E) with N nodes, M edges and nonnegative weights, a fixed source S, and an SSSP tree T. Edges can change weight online (increase or decrease), but no edges are added or removed. Propose a concrete, implementable strategy to maintain the SSSP efficiently, including data structures, update rules, and expected time bounds. Provide preprocessing, amortized per-update, and memory usage, plus practical optimizations and a concrete scenario where it shines (e.g., streaming latency updates)?","answer":"Maintain a single-source shortest-path tree (SSSP) from source S. For weight decreases on tree edges, perform localized relaxation using bounded Dijkstra that visits only the affected frontier. For weight increases or non-tree edges, employ a hierarchical approach: first determine if the edge creates a shorter alternative path, and if so, execute localized re-relaxation from the affected subtree.\n\n**Data Structures:**\n- Fibonacci heap for priority queue (O(1) insert, O(log n) delete-min)\n- Adjacency list with edge weight tracking\n- Parent array for SSSP tree maintenance\n- Distance array with lazy updates\n- Affected node set to track nodes requiring relaxation\n\n**Preprocessing:** O(M + N log N) to compute initial SSSP using Dijkstra's algorithm\n\n**Update Rules:**\n- Weight decrease: If edge belongs to SSSP tree, push affected subtree nodes to priority queue with updated distances; otherwise, simply check if it creates a shorter path\n- Weight increase: If edge belongs to SSSP tree, mark affected subtree and run bounded Dijkstra from frontier nodes; otherwise, no action required\n\n**Time Bounds:**\n- Amortized per-update: O(log N + K) where K is the number of affected nodes (typically much smaller than N)\n- Worst-case: O(N log N) for pathological updates affecting the entire tree\n- Memory usage: O(N + M) for graph storage plus O(N) for auxiliary structures\n\n**Practical Optimizations:**\n- Use multi-level buckets for improved constant factors\n- Batch multiple updates before reprocessing\n- Maintain distance bounds to prune unnecessary relaxations\n- Cache frequently accessed edge weights\n\n**Concrete Scenario:** Real-time network routing with streaming latency updates, where edge weights represent network delays that change frequently but only affect localized regions of the network.","explanation":"## Why This Is Asked\nDesigning dynamic SSSP algorithms is essential for real-time network applications. This question tests understanding of worst-case versus amortized guarantees, appropriate data structure selection, and practical implementation constraints.\n\n## Key Concepts\n- Dynamic shortest paths with nonnegative edge weights\n- Localized re-relaxation and bounded Dijkstra execution\n- Amortized analysis and worst-case performance bounds\n- Memory-efficient data structures and practical optimizations\n- Hierarchical update strategies for different edge types\n\n## Code Example\n```javascript\n// Dynamic SSSP maintenance implementation\nclass DynamicSSSP {\n  constructor(graph, source) {\n    this.graph = graph;\n    this.source = source;\n    this.distances = new Array(graph.N).fill(Infinity);\n    this.parents = new Array(graph.N).fill(-1);\n    this.heap = new FibonacciHeap();\n    this.affectedNodes = new Set();\n    \n    // Initialize with Dijkstra\n    this.initializeSSSP();\n  }\n  \n  updateEdge(u, v, newWeight) {\n    const oldWeight = this.graph.getWeight(u, v);\n    this.graph.setWeight(u, v, newWeight);\n    \n    if (newWeight < oldWeight) {\n      this.handleWeightDecrease(u, v, newWeight);\n    } else {\n      this.handleWeightIncrease(u, v, newWeight);\n    }\n  }\n  \n  handleWeightDecrease(u, v, newWeight) {\n    if (this.parents[v] === u) {\n      // Tree edge weight decreased\n      this.propagateDecrease(v);\n    } else if (this.distances[u] + newWeight < this.distances[v]) {\n      // Non-tree edge creates shorter path\n      this.updateDistance(v, this.distances[u] + newWeight, u);\n    }\n  }\n  \n  handleWeightIncrease(u, v, newWeight) {\n    if (this.parents[v] === u) {\n      // Tree edge weight increased - need re-relaxation\n      this.affectedNodes.add(v);\n      this.boundedDijkstra([v]);\n    }\n    // Non-tree edge weight increase requires no action\n  }\n  \n  boundedDijkstra(startNodes) {\n    const queue = new FibonacciHeap();\n    \n    for (const node of startNodes) {\n      queue.insert(node, this.distances[node]);\n    }\n    \n    while (!queue.isEmpty()) {\n      const { node: u, distance: du } = queue.extractMin();\n      \n      if (du > this.distances[u]) continue;\n      \n      for (const [v, weight] of this.graph.neighbors(u)) {\n        const newDist = du + weight;\n        if (newDist < this.distances[v]) {\n          this.updateDistance(v, newDist, u);\n          queue.insert(v, newDist);\n        }\n      }\n    }\n  }\n  \n  updateDistance(node, newDistance, parent) {\n    this.distances[node] = newDistance;\n    this.parents[node] = parent;\n    this.affectedNodes.add(node);\n  }\n  \n  propagateDecrease(startNode) {\n    const queue = new FibonacciHeap();\n    queue.insert(startNode, this.distances[startNode]);\n    \n    while (!queue.isEmpty()) {\n      const { node: u } = queue.extractMin();\n      \n      for (const [v, weight] of this.graph.neighbors(u)) {\n        if (this.parents[v] === u) {\n          const newDist = this.distances[u] + weight;\n          if (newDist < this.distances[v]) {\n            this.updateDistance(v, newDist, u);\n            queue.insert(v, newDist);\n          }\n        }\n      }\n    }\n  }\n  \n  initializeSSSP() {\n    // Standard Dijkstra implementation\n    this.distances[this.source] = 0;\n    const heap = new FibonacciHeap();\n    heap.insert(this.source, 0);\n    \n    while (!heap.isEmpty()) {\n      const { node: u, distance: du } = heap.extractMin();\n      \n      if (du > this.distances[u]) continue;\n      \n      for (const [v, weight] of this.graph.neighbors(u)) {\n        const newDist = du + weight;\n        if (newDist < this.distances[v]) {\n          this.distances[v] = newDist;\n          this.parents[v] = u;\n          heap.insert(v, newDist);\n        }\n      }\n    }\n  }\n}\n```\n\nThis implementation demonstrates the core principles of dynamic SSSP maintenance with efficient localized updates and amortized performance guarantees.","diagram":null,"difficulty":"advanced","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","LinkedIn"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T12:44:57.161Z","createdAt":"2026-01-12T08:34:14.620Z"},{"id":"q-808","question":"Dynamic path counting in a DAG: maintain the number of S→T paths of length at most L under online edge insertions and deletions. Propose a data structure and amortized update time bound in terms of N, M, L and #updates; discuss memory usage and how to handle large L and modulo arithmetic in practice?","answer":"Maintain a layer-based DP: c[v][d] = number of S→v paths of exact length d for d = 0..L. Initial fill by DP in topological order. On an edge insertion/deletion, compute the delta for c[v][d] caused by","explanation":"## Why This Is Asked\n\nEvaluates practicality of dynamic programming under online graph updates, requiring precise amortized reasoning and data-structure design rather than surface knowledge.\n\n## Key Concepts\n\n- DAG properties and topological order\n- Path counting DP with an extra length dimension\n- Incremental updates via delta propagation\n- Frontier-limited recomputation and memory-time tradeoffs\n\n## Code Example\n\n```python\n# Pseudocode for update propagation (edge (u->v) updated)\ndef update_edge(u, v, delta_sign):\n    for d in range(1, L+1):\n        delta = delta_sign * paths_from_S_to[u][d-1]\n        if delta:\n            paths_to[v][d] += delta\n            enqueue_successors(v, d, delta)\n```\n\n## Follow-up Questions\n\n- How would you adapt when L varies per query or when edges have weights affecting path counts differently?\n- What are the bottlenecks for extremely large L or dense DAGs, and how would you mitigate them?","diagram":"flowchart TD\n  A[Start] --> B[Receive edge update]\n  B --> C[Compute deltas on layer counts]\n  C --> D[Propagate to successors up to L]\n  D --> E[Return updated counts to queries]","difficulty":"intermediate","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Snowflake","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T09:33:58.407Z","createdAt":"2026-01-12T09:33:58.407Z"},{"id":"q-817","question":"Design a dynamic, multi-source shortest-path maintenance scheme for a directed graph with nonnegative weights. A fixed set of K hub nodes H must always have exact shortest-path distances to all nodes. Edges can be inserted or weights decreased online, in batches of size at most B. Provide initial preprocessing and a full-update algorithm, with (i) initial time, (ii) amortized update time per batch, and (iii) memory usage. Include two practical optimizations and compare to recomputing from scratch after each batch?","answer":"Multi-source incremental Dijkstra. Precompute d(h,v) for all h∈H with K runs: O(K E log V). For a batch of up to B updates, relax via affected edges using a shared min-heap, propagating improvements o","explanation":"## Why This Is Asked\nTests dynamic graphs, amortized analysis, and practical heuristics for keeping multiple SSSP trees up to date.\n\n## Key Concepts\n- Dynamic SSSP with monotone updates\n- Multi-source labeling\n- Amortized analysis and batch processing\n- Space-time trade-offs\n\n## Code Example\n```javascript\n// Pseudocode: multi-source incremental relaxations\nlet D = new Map(); // D[h] -> dist array\nfor (let h of H) D[h] = dijkstra(G, h);\nfunction updateBatch(batch){\n  let PQ = new MinHeap();\n  for (let e of batch){ /* decrease weights or insert */ }\n  // relaxations\n  while(!PQ.isEmpty()){ let {d,u} = PQ.pop(); if (d> D[h][u]) continue; ... }\n}\n```\n\n## Follow-up Questions\n- How do you bound X in worst-case, and how would you adapt if H grows?\n- How would you extend to approximate distances with guarantees?","diagram":null,"difficulty":"advanced","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","PayPal","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T10:26:55.970Z","createdAt":"2026-01-12T10:26:55.970Z"},{"id":"q-827","question":"Design a dynamic distance labeling scheme for an undirected weighted graph that supports edge insertions and deletions online. Use a fixed hub set H to enable dist(u,v) queries via dist(u,v)=min_h dist(u,h)+dist(h,v) only if H covers all shortest paths. Explain maintenance of hub distances under updates, and bound update/query times and memory usage. Provide two optimizations and a comparison to recomputing from scratch?","answer":"Approach: hub labeling with a fixed hub set H. Each node v stores dist(v,h) for all h in H; dist(u,v) = min_h dist(u,h)+dist(h,v) if H covers all shortest paths. On updates, recompute only hubs touche","explanation":"## Why This Is Asked\nThis probes dynamic graph labeling, balancing query time against update cost, a practical constraint in large-scale networks where frequent edits occur.\n\n## Key Concepts\n- Hub labeling and cover properties\n- Dynamic SSSP maintenance for limited regions\n- Trade-offs: hub count, space, and amortized updates\n\n## Code Example\n```javascript\n// Pseudocode sketch for updates\nfunction updateEdge(u, v, w, graph, labels, H){\n  // identify affected hubs and propagate changes using bounded-radius search\n}\n```\n\n## Follow-up Questions\n- How do you pick and update H to adapt to changing graphs? \n- How would you extend to directed graphs with non-symmetric distances?","diagram":"flowchart TD\n  A[Node] --> B[Hub]\n  B --> C[Distance to hub]\n  A --> D[Query dist to E]\n  D --> E[Compute min over hubs]","difficulty":"intermediate","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Meta","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T11:23:08.719Z","createdAt":"2026-01-12T11:23:08.719Z"},{"id":"q-831","question":"In an undirected weighted graph G=(V,E) with nonnegative weights, design a (1+ε)-approximate distance oracle based on a fixed landmark set L (|L|=k). Edges are only inserted online in batches of size B; no deletions. After each batch, specify: (i) preprocessing time and space to build distances from every landmark, (ii) amortized update time per batch to update the oracle, (iii) query time for dist(u,v), (iv) total memory, (v) two practical optimizations, (vi) a comparison to rebuilding all-pairs distances after each batch. Provide concrete asymptotics in terms of n=|V|, m=|E|, k, ε, B?","answer":"Use k landmarks. Precompute Dijkstra from each landmark: time O(k m log n), space O(k n). Query dist(u,v) = minℓ (d(u,ℓ)+d(ℓ,v)) in O(k). For a batch of B edge insertions, update via incremental relax","explanation":"## Why This Is Asked\nTests designing a practical distance oracle under incremental graph updates, balancing preprocessing, update, and query costs, plus real-world trade-offs when batch sizes are small.\n\n## Key Concepts\n- (1+ε) distance oracle with landmarks\n- Incremental insertions only\n- Multi-source Dijkstra from landmarks\n- Amortized per-batch analysis\n- Pruning and reuse of work across batches\n\n## Code Example\n```javascript\nfunction rebuildFromLandmarks(graph, landmarks) {\n  // placeholder: run Dijkstra from each landmark\n}\n```\n\n## Follow-up Questions\n- How would the approach adapt if deletions appear?\n- How does ε influence practical update costs and memory?","diagram":null,"difficulty":"intermediate","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","Microsoft","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T12:43:15.513Z","createdAt":"2026-01-12T12:43:15.513Z"}],"subChannels":["general"],"companies":["Airbnb","Amazon","Anthropic","Apple","Bloomberg","Citadel","Cloudflare","Coinbase","Databricks","Discord","DoorDash","Goldman Sachs","Google","Hashicorp","Hugging Face","IBM","Instacart","LinkedIn","Lyft","Meta","Microsoft","MongoDB","NVIDIA","Netflix","OpenAI","Oracle","PayPal","Plaid","Robinhood","Salesforce","Scale Ai","Slack","Snap","Snowflake","Square","Stripe","Tesla","Twitter","Two Sigma","Uber","Zoom"],"stats":{"total":47,"beginner":19,"intermediate":12,"advanced":16,"newThisWeek":25}}