{"questions":[{"id":"q-1193","question":"You have an array A of n positive integers and a threshold T. For a given window length L, define ok(L) as: there exists a subarray of length L with sum <= T. Design an O(n) check for ok(L) using a sliding window, then outline how to find the maximum L with binary search over [1..n], and analyze total time and space. Include edge-case handling and practical optimizations?","answer":"Explain ok(L) in O(n) using a sliding window: compute the first L-sum, then slide by subtracting A[i-L] and adding A[i], stopping if a window sum <= T exists. Binary search L in [1..n] for the max L; ","explanation":"## Why This Is Asked\n\nThis question blends sliding-window technique with binary search to analyze a threshold problem, testing practical algorithm design and reasoning about complexity.\n\n## Key Concepts\n\n- Sliding window for fixed-length subarray sums\n- Binary search over the answer space\n- Time/space trade-offs and edge-case handling\n- Practical optimizations (early exit, optional prefix sums)\n\n## Code Example\n\n```javascript\nfunction maxLWithSumAtMost(A, T) {\n  const n = A.length;\n  const ok = (L) => {\n    let sum = 0;\n    for (let i = 0; i < L; i++) sum += A[i];\n    if (sum <= T) return true;\n    for (let i = L; i < n; i++) {\n      sum += A[i] - A[i - L];\n      if (sum <= T) return true;\n    }\n    return false;\n  };\n  let lo = 1, hi = n, ans = 0;\n  while (lo <= hi) {\n    const mid = (lo + hi) >> 1;\n    if (ok(mid)) { ans = mid; lo = mid + 1; } else { hi = mid - 1; }\n  }\n  return ans;\n}\n```\n\n## Follow-up Questions\n\n- How would the solution change if elements could be negative?\n- What are the memory/speed trade-offs if you precompute prefix sums?\n- How would you adapt for multiple thresholds T1..Tk in parallel?","diagram":null,"difficulty":"beginner","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Snowflake","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T04:42:46.129Z","createdAt":"2026-01-13T04:42:46.129Z"},{"id":"q-1254","question":"You're given a DAG G=(V,E) with N nodes and M edges. Edges can be inserted online in batches of size B. Design a dynamic transitive-closure using bitsets to answer reachability queries in O(1). Provide initialization, amortized per-batch update time, and memory. Include two practical optimizations and compare to recomputing the closure after each batch?","answer":"Adopt a reachability bitset per node reach[u] over all nodes, with a topological order. For a batch of insertions, process edges in topo order; if v not in reach[u], do reach[u] |= reach[v] and push t","explanation":"## Why This Is Asked\n\nAssesses dynamic graph reasoning, bitset optimizations, and amortized analysis in a practical DAG setting.\n\n## Key Concepts\n\n- Bitset reachability per node\n- Topological ordering reuse\n- Incremental updates and fixed-point propagation\n- Amortized cost and memory trade-offs\n\n## Code Example\n\n```javascript\n// Pseudocode for dynamic reachability with bitsets\nfor edge (u,v) in batch in topo order {\n  if (!(reach[u] has v)) {\n    reach[u] |= reach[v]\n    // propagate if new bits were added\n  }\n}\n```\n\n## Follow-up Questions\n\n- How would deletions affect the structure?\n- Analyze worst-case batch patterns that maximize work.","diagram":null,"difficulty":"intermediate","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Square","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T06:47:01.803Z","createdAt":"2026-01-13T06:47:01.803Z"},{"id":"q-2159","question":"Scenario: In a weighted directed graph with nonnegative weights, you must maintain approximate betweenness centrality for all nodes under batches of edge insertions of size B. Propose a concrete, implementable scheme that (i) initializes estimates, (ii) updates after each batch with amortized cost, (iii) bounds memory, and (iv) provides guaranteed error bounds ε. Include two practical optimizations and compare to re-running Brandes' algorithm after every batch?","answer":"Maintain approximate betweenness via dynamic sampling of source–target pairs. Precompute a fixed set of sample shortest paths; store their node visit counts. After a batch of B insertions, only paths ","explanation":"## Why This Is Asked\nTests understanding of dynamic graph algorithms, sampling for approximations, and amortized analysis in batch-update settings.\n\n## Key Concepts\n- Dynamic graphs with batch updates\n- Approximate centrality with ε guarantees\n- Sampling-based path tracking and incremental updates\n- Trade-offs: time, memory, and rebuild frequency\n\n## Code Example\n```javascript\n// Sketch: updateBatch(graph, batch, samples, eps) { /* adjust sample counts for new edges */ }\n```\n\n## Follow-up Questions\n- How to bound error growth with insertions?\n- How to select number of samples s and batch size B for a target ε?","diagram":null,"difficulty":"intermediate","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","Snowflake","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T05:39:09.336Z","createdAt":"2026-01-15T05:39:09.337Z"},{"id":"q-2497","question":"Maintain exact single-source shortest paths from a fixed hub h under online edge insertions (batches). Propose a practical scheme that (i) initializes with Dijkstra, (ii) after each batch re-relaxes only affected nodes using a min-heap, (iii) bounds memory, and (iv) gives amortized update time. Include two optimizations and compare to rerunning Dijkstra after every batch. Provide asymptotics in n, m, B?","answer":"Core approach: maintain exact distances from hub h. After a batch, re-relax only nodes whose paths can improve. Initialize with Dijkstra from h. Use a min-heap to propagate decreases until fixed. Init","explanation":"## Why This Is Asked\nTests the ability to design incremental single-source shortest paths with batch updates and to reason about amortized work and practical data structures.\n\n## Key Concepts\n- Incremental SSSP with batch edge insertions\n- Localized relaxation and pruning\n- Priority queues and potential bucketed variants (Dial's)\n- Amortized analysis and space efficiency\n\n## Code Example\n```javascript\nfunction updateBatch(batchEdges){\n  for (const [u,v,w] of batchEdges){\n    // ensure undirected view if needed\n    adj[u].push([v,w]);\n    adj[v].push([u,w]);\n  }\n  for (const [u,v,w] of batchEdges){\n    if (dist[v] > dist[u] + w){\n      dist[v] = dist[u] + w;\n      pq.push([dist[v], v]);\n    }\n    if (dist[u] > dist[v] + w){\n      dist[u] = dist[v] + w;\n      pq.push([dist[u], u]);\n    }\n  }\n  while (!pq.isEmpty()){\n    const [d, x] = pq.pop();\n    if (d !== dist[x]) continue;\n    for (const [y,w] of adj[x]){\n      if (dist[y] > dist[x] + w){\n        dist[y] = dist[x] + w;\n        pq.push([dist[y], y]);\n      }\n    }\n  }\n}\n```","diagram":null,"difficulty":"advanced","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Oracle","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T20:40:07.034Z","createdAt":"2026-01-15T20:40:07.034Z"},{"id":"q-2635","question":"Scenario: A real-time analytics service ingests event messages with a 32-bit integer key. Events arrive in batches of size B every second, and the system must report the number of distinct keys seen so far (cardinality) with a configurable error ε. Propose a practical streaming algorithm and data structure (no deletions) to maintain an ε-approximate cardinality. After each batch, specify (i) per-element insertion time and total batch time, (ii) space usage, (iii) query time for the current estimate, (iv) two optimizations, and (v) how this compares to re-scanning all seen events after every batch. Use asymptotics in n (seen events), B, and ε?","answer":"Use HyperLogLog with m = ceil(1.04/ε^2) registers. For each key, hash and update a register; per-element cost O(1). Batch of B costs O(B). Memory O(m). Query O(1) to estimate cardinality. Optimization","explanation":"## Why This Is Asked\\nTests practical understanding of streaming cardinality with limited precision, a common real-time metric.\\n\\n## Key Concepts\\n- HyperLogLog, probabilistic counting, amortized analysis, batch processing.\\n- Trade-offs: memory vs. accuracy, update vs. query costs.\\n\\n## Code Example\\n```javascript\\n// sketch: update(hll, x) { hll.add(hash(x)); }\\n```\\n\\n## Follow-up Questions\\n- How would you choose ε given memory limits? \\n- How do deletions or windowed cardinality change the approach?","diagram":null,"difficulty":"beginner","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","PayPal","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T04:17:01.951Z","createdAt":"2026-01-16T04:17:01.951Z"},{"id":"q-2655","question":"Design a dynamic (1+ε)-spanner for an undirected weighted graph under online edge insertions in batches of size B. Build a (k,ε)-spanner with O(n^{1+1/k}) edges. After each batch, update only edges that improve distances beyond ε; amortized O(B log n). Query dist via the spanner in O(1). Memory O(n^{1+1/k}). Optimizations: lazy rebuild; reuse tight edges. Compare to rebuilding: faster updates; worst-case near O(n^{1+1/k}) per batch?","answer":"Propose a dynamic (1+ε)-spanner for an undirected weighted graph under online edge insertions in batches of size B. Build a (k,ε)-spanner with O(n^{1+1/k}) edges. After each batch, update only edges t","explanation":"## Why This Is Asked\nTests ability to synthesize dynamic graph sparsification with real-world batch updates and explain trade-offs of maintaining approximate structures.\n\n## Key Concepts\n- Dynamic spanners\n- Batch update amortization\n- Distance queries over sparse representations\n- Memory vs accuracy trade-offs\n\n## Code Example\n```javascript\n// Pseudocode: batch update for spanner maintenance\nfunction updateBatch(G, batchEdges, k, eps){\n  // add edges, check potential shortcuts, prune by eps\n  // update spanner edges selectively\n}\n```\n\n## Follow-up Questions\n- How to extend to edge deletions?\n- How does skewed batch size affect amortized cost?","diagram":null,"difficulty":"advanced","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Robinhood","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T05:41:04.220Z","createdAt":"2026-01-16T05:41:04.220Z"},{"id":"q-2726","question":"Scenario: A weighted directed graph with nonnegative weights tracks network latencies. Edges can be inserted or have their weights decreased in online batches of size B. After each batch, you must maintain exact distances from a fixed source S to all nodes. Propose a practical hybrid scheme that combines (i) an incremental update pass limited to affected nodes, (ii) periodic full re-computation every T batches, and (iii) a lazy rebuild trigger when incremental cost exceeds a threshold. Provide initial preprocessing, amortized update time per batch, memory bounds, two optimizations, and a comparison to re-running Dijkstra from scratch after every batch?","answer":"Hybrid incremental+rebuild: keep dist[v] from S and a frontier. For each batch of B updates, enqueue affected endpoints and relax until no improvement. After T batches, run a full Dijkstra to rebuild ","explanation":"## Why This Is Asked\nDynamic shortest-path maintenance with batching tests practical trade-offs between local updates and periodic rebuilds. It blends online incremental work with amortized guarantees and requires careful parameterization.\n\n## Key Concepts\n- Incremental relaxations focused on affected region\n- Periodic full rebuild to bound drift\n- Lazy thresholds to trigger rebuilds\n\n## Code Example\n```javascript\n// Pseudocode sketch for batch update\n```\n\n## Follow-up Questions\n- How would you adapt this for multiple sources S1..Sk?\n- How would you choose T given m, n, and B?","diagram":"flowchart TD\n  A[Batch Update (size B)] --> B[Frontier enqueue]\n  B --> C[Relax distances]\n  C --> D{Rebuild needed?}\n  D --> E[Yes: full Dijkstra]\n  D --> F[No: serve queries]\n","difficulty":"intermediate","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Meta","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T09:39:47.374Z","createdAt":"2026-01-16T09:39:47.374Z"},{"id":"q-3064","question":"In a directed graph with nonnegative weights, updates arrive in batches of B edge insertions or weight decreases. After each batch, maintain P = { (u,v) | dist(u,v) ≤ D } for a fixed threshold D. Propose a practical scheme that (i) precomputes a compact structure, (ii) updates P after each batch with amortized time, (iii) bounds memory, and (iv) provides two optimizations. Compare to recomputing all-pairs distances up to D after every batch?","answer":"Use a landmark-based threshold maintenance scheme. Select L of size k; for each s in L, run Dijkstra truncated at D to compute dist(s,•). Store ds[v]. After a batch, re-relax only nodes reachable from updated edges, leveraging the landmark distances to bound recomputation.","explanation":"## Why This Is Asked\nTests ability to design dynamic, approximation-friendly data structures for thresholded distances under batched updates. It probes practical trade-offs between accuracy, time, and memory, beyond standard APSP or exact single-source maintenance.\n\n## Key Concepts\n- Dynamic graphs with batched updates\n- Thresholded reachability dist(u,v) ≤ D\n- Landmark-based distance sketches\n- Amortized analysis and lazy propagation\n\n## Code Example\n```python\n# skeleton for batch update (pseudo)\ndef update_batch(graph, batch, D, landmarks):\n    for s in landmarks:\n        relax_truncated(gra\n```","diagram":"flowchart TD\n  A[Update Batch] --> B[Identify Affected Edges]\n  B --> C[Update Landmark Distances]\n  C --> D[Compute P]\n  D --> E[Optional Rebuild Trigger]","difficulty":"intermediate","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Goldman Sachs","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T05:18:43.450Z","createdAt":"2026-01-16T23:34:35.046Z"},{"id":"q-3179","question":"You maintain a growing list of N tasks with integer priorities. Operations arrive in batches of size B, appending new tasks. After each batch you must surface the current minimum priority quickly. Compare two schemes: (A) rebuild a heap from scratch after every batch; (B) insert the B new tasks into a single heap. Provide total time and per-operation costs, and derive the breakeven batch size B* in terms of N. Include memory usage considerations?","answer":"Scheme A total time approximately (N^2)/(2B); amortized per insertion approximately N/(2B). Scheme B total time approximately N log N; amortized approximately log N. Breakeven when N log N ≈ N^2/(2B) ","explanation":"## Why This Is Asked\nThis tests practical amortized reasoning on common data-structure choices in streaming-like workloads.\n\n## Key Concepts\n- Amortized analysis\n- Heap rebuild vs incremental insert\n- Breakeven batch size and memory bound\n\n## Code Example\n```javascript\nfunction rebuildHeap(arr){ /* build min-heap from scratch */ }\n```\n\n## Follow-up Questions\n- How would you adapt if you also needed delete-min in O(log N)?\n- How would varying batch sizes affect the breakeven point?","diagram":"flowchart TD\n  A[Batch] --> B[Scheme A: Rebuild]\n  A --> C[Scheme B: Incr Inserts]\n  B --> D[Compare costs]\n  C --> D","difficulty":"beginner","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Slack","Tesla","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T05:40:30.032Z","createdAt":"2026-01-17T05:40:30.032Z"},{"id":"q-3314","question":"Scenario: A streaming service receives integers in batches of size B. After each batch, output the median of all values seen so far. Propose a practical scheme that (i) uses two heaps to balance lower/upper halves, (ii) inserts each new value with O(log n) cost, (iii) bounds memory, (iv) offers two optimizations, and (v) compares to recomputing from scratch after every batch. Give asymptotics in terms of n and B?","answer":"Two-heap approach: max-heap for lower half and min-heap for upper half; keep size diff ≤ 1. For each value, push to the appropriate heap and rebalance. After n elements, median is the top of the large","explanation":"## Why This Is Asked\nThis checks understanding of streaming data structures and complexity trade-offs for maintaining order statistics under batch updates.\n\n## Key Concepts\n- Running median via two heaps\n- Per-element insert cost and batch amortization\n- Memory growth with total elements\n\n## Code Example\n```javascript\nclass MedianMaintainer {\n  constructor() {\n    this.low = new MaxHeap();\n    this.high = new MinHeap();\n  }\n  add(x) {\n    if (this.low.isEmpty() || x <= this.low.peek()) this.low.push(x);\n    else this.high.push(x);\n    // rebalance\n    if (this.low.size() > this.high.size() + 1) this.high.push(this.low.pop());\n    else if (this.high.size() > this.low.size()) this.low.push(this.high.pop());\n  }\n  median() {\n    if (this.low.size() === this.high.size()) return (this.low.peek() + this.high.peek()) / 2;\n    return this.low.peek();\n  }\n}\n```\n\n## Follow-up Questions\n- How does varying batch size B affect latency and memory?\n- How would you extend to handle sliding windows or duplicates efficiently?","diagram":null,"difficulty":"beginner","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Microsoft","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T11:26:41.053Z","createdAt":"2026-01-17T11:26:41.053Z"},{"id":"q-3412","question":"Write a function countPairs(n) with:\n for i from 1 to n:\n   for j from 1 to floor(n/i):\n     // O(1) work\nWhat is the overall time complexity and space usage? Provide the tight bound, discuss best/worst-case, and compare to a memory-efficient variant that avoids the inner loop entirely?","answer":"T(n) = sum_{i=1}^{n} floor(n/i) = n log n + O(n), hence Theta(n log n). Space is O(1) beyond few counters. Best and worst are the same asymptotic. A memory-efficient variant computes the same count vi","explanation":"## Why This Is Asked\nTests ability to derive nested-loop complexity where inner bound shrinks with i, revealing a harmonic-sum growth.\n\n## Key Concepts\n- Harmonic series: sum floor(n/i) ~ n log n\n- Theta vs O, space O(1)\n- Closed-form approximations vs exact sums\n\n## Code Example\n```javascript\nfunction countPairs(n){\n  let count=0;\n  for(let i=1;i<=n;i++){\n    for(let j=1;j<=Math.floor(n/i);j++){\n      count++;\n    }\n  }\n  return count;\n}\n```\n\n## Follow-up Questions\n- How would you prove the Theta(n log n) bound?\n- How does changing inner bound to floor(n/i^2) affect complexity?\n- What are practical implications for large n?","diagram":null,"difficulty":"beginner","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T15:30:27.134Z","createdAt":"2026-01-17T15:30:27.134Z"},{"id":"q-3504","question":"Scenario: Streaming terms from a document corpus arrive in batches of B. Estimate the number of distinct terms seen so far using a probabilistic counter. Design an implementable plan with initial preprocessing, per-batch updates, memory bounds, two optimizations, and a comparison to exact counting. Include formulas in terms of B, U (distinct terms), and target error ε?","answer":"Use a HyperLogLog sketch to estimate the number of distinct terms in a stream arriving in batches of B. Initialize with p so that m = 2^p registers; memory ~ m. For each batch, insert B items into a p","explanation":"## Why This Is Asked\nEvaluates practical understanding of streaming algorithms, amortized costs, and accuracy vs memory. It also tests ability to present concrete formulas and compare approximate vs exact techniques.\n\n## Key Concepts\n- HyperLogLog, sketch mergeability\n- Batch processing and amortized analysis\n- Memory vs accuracy trade-offs\n\n## Code Example\n```javascript\nclass HLL { constructor(p) { } add(x) { } merge(other) { } estimate() { return 0; } }\n```\n\n## Follow-up Questions\n- How to pick p for target ε?\n- How would deletions or time decay affect the approach?","diagram":null,"difficulty":"beginner","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Hugging Face","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T19:23:01.756Z","createdAt":"2026-01-17T19:23:01.757Z"},{"id":"q-3589","question":"You maintain a weighted undirected graph G=(V,E) with nonnegative weights. Edges arrive online in batches of size B. You need to answer s shortest-path queries dist(u_i,v_i) for i=1..s after each batch, within a (1+ε) approximation, reusing work across all pairs. Propose a single shared data structure (i) initialization to support all s pairs, (ii) amortized per-batch update cost when a batch arrives, (iii) per-query time, (iv) memory, (v) two practical optimizations, and (vi) a comparison to rebuilding from scratch per batch. Express asymptotics in n=|V|, m=|E|, s, B, ε?","answer":"Use a shared hub-based scheme: select k = O((1/ε) log n) hubs H and maintain dist(h,v) for all h ∈ H through incremental updates after each batch. Answer dist(u,v) as min_{h ∈ H} [dist(u,h) + dist(h,v)]. Upon batch arrival, recompute affected hub distances using Dijkstra's algorithm from each hub, achieving O(k·(m + n log n)) amortized update cost. Each query answers in O(k) time by scanning all hubs. Memory usage is O(k·n) for storing hub-to-vertex distances. Two practical optimizations: (1) maintain only distances to vertices within radius R from each hub, pruning distant vertices; (2) use incremental Dijkstra with priority queue reuse across batches. Compared to rebuilding from scratch per batch (O(s·(m + n log n)) total), this shared structure reduces total cost by factor O(s/k) while maintaining (1+ε) approximation guarantee.","explanation":"## Why This Is Asked\nExplores practical dynamic approximate all-pairs shortest paths (APSP) with shared structure; tests understanding of trade-offs between update costs and query time in batch-dynamic settings.\n\n## Key Concepts\n- Batch-dynamic graphs with nonnegative edge weights\n- Hub-based distance sketches for approximate APSP\n- Amortized analysis across multiple query pairs\n- Trade-offs between approximation accuracy ε and computational efficiency\n- Incremental shortest path algorithms\n\n## Code Example\n```javascript\n// Hub-based distance approximation: dist(u,v) ≈ min_h∈H [dist(u,h) + dist(h,v)]\n\nclass HubBasedAPSP {\n  constructor(graph, epsilon) {\n    this.k = Math.ceil((1/epsilon) * Math.log(graph.n));\n    this.hubs = this.selectHubs(graph);\n    this.distances = new Map(); // hub -> vertex distances\n  }\n\n  updateBatch(edges) {\n    for (const hub of this.hubs) {\n      this.distances.set(hub, this.incrementalDijkstra(hub, edges));\n    }\n  }\n\n  query(u, v) {\n    let minDist = Infinity;\n    for (const hub of this.hubs) {\n      const dist = this.distances.get(hub).get(u) + \n                   this.distances.get(hub).get(v);\n      minDist = Math.min(minDist, dist);\n    }\n    return minDist;\n  }\n}\n```","diagram":"flowchart TD\n  S[Start] --> U[Update batch B]\n  U --> Q[Answer s queries via hubs]\n  Q --> R[Optional prune/rebuild hubs]\n  R --> S","difficulty":"advanced","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","Plaid","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T05:32:59.572Z","createdAt":"2026-01-17T22:37:49.256Z"},{"id":"q-3629","question":"In a directed graph with N nodes and M edges, edges arrive online in batches of size B. After each batch, estimate, with high probability, the number of nodes reachable from a fixed set S within at most D hops, within a (1±ε) factor. Propose a streaming sampling scheme: (i) pick k random walks of length D starting from S or uniformly; (ii) specify update rules per batch; (iii) derive variance/error bounds and how to set k for a target failure prob; (iv) amortized time and memory; (v) compare to rerunning a bounded-depth multi-source BFS after each batch. Provide asymptotics in n,m,k,B,ε,D?","answer":"Use k independent random walks of length D seeded from S. Each batch extends walks through the new edges; maintain lastVisit[v] as the last batch id that touched v. The estimator of Reach_D(S) is the number of distinct nodes visited across all walks, scaled by N/|S|k. Update rules: for each new edge (u,v) in batch, if lastVisit[u] equals current batch id, propagate walk to v with probability 1/k, updating lastVisit[v]. Variance bounds: Var(estimator) ≤ N²/k, so setting k = O(1/(ε²δ)) achieves (1±ε) approximation with failure probability δ. Amortized time per batch: O(B·k), memory: O(N + k·D). Compared to bounded-depth multi-source BFS (O(B·|S|·D) time, O(N) memory per batch), random walks achieve O(|S|/k) factor speedup for large |S| with comparable memory.","explanation":"## Why This Is Asked\nTests dynamic, randomized complexity reasoning for streaming graph data; pushes beyond exact APSP-like or single-source updates to practical sampling-based approximations.\n\n## Key Concepts\n- Dynamic sampling, Monte Carlo guarantees, batch updates, memory/time trade-offs, reachability estimation.\n\n## Code Example\n```javascript\n// Pseudocode sketch: maintain lastVisit as IntArray[N]\n// and frontier queues for k walks; advance via new edges only\nfunction updateBatch(edges, batchId) {\n  for (let [u,v] of edges) {\n    if (lastVisit[u] === batchId) {\n      if (Math.random() < 1/k) {\n        lastVisit[v] = batchId;\n        visitedNodes.add(v);\n      }\n    }\n  }\n}\n```\n\n## Follow-up Questions\n- How would you adapt if S changes over time?\n- How to handle high out-degree bursts?\n- What if edges can be deleted as well as added?","diagram":null,"difficulty":"advanced","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Discord","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T05:07:32.976Z","createdAt":"2026-01-18T02:31:31.639Z"},{"id":"q-3881","question":"Scenario: In a weighted undirected graph G=(V,E) with nonnegative weights, edges arrive in online batches of size B. From a fixed root r, maintain a (1+ε)-approximate shortest path tree (SPT) to all nodes after each batch. Propose a practical scheme that (i) initializes the SPT, (ii) updates after a batch with amortized time, (iii) bounds memory, (iv) offers two optimizations, and (v) compares to rebuilding the SPT from scratch after every batch. Provide asymptotics in n=|V|, m=|E|, B, ε?","answer":"Initialize the SPT with a single Dijkstra run from r. For each batch, relax only edges touching affected vertices and propagate improvements along the current tree until changes fall below ε·dist(v). ","explanation":"## Why This Is Asked\nTests ability to design dynamic approximate SPTs under batched updates, balancing accuracy and update cost, a realistic concern for large-scale graphs in production.\n\n## Key Concepts\n- (1+ε)-approximate SPT maintenance\n- Batch-driven relaxations with local propagation\n- Periodic full rebuild to bound drift\n- Time/memory trade-offs; practical optimizations\n\n## Code Example\n```javascript\n// Pseudo-code sketch\ninitializeSPT(r);\nfor each batch B:\n  relaxEdges(B);\n  if (batchIndex % T === 0) rebuildSPTFromScratch();\n```\n\n## Follow-up Questions\n- How to choose T to optimize long-term cost?\n- How would you adapt to edge deletions or negative-weight adjustments?\n","diagram":null,"difficulty":"advanced","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","NVIDIA","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T13:44:44.560Z","createdAt":"2026-01-18T13:44:44.560Z"},{"id":"q-3905","question":"Scenario: A ride-hailing network graph G=(V,E) with nonnegative weights receives edge insertions in batches of size B. You must maintain an ε-approximation to the top-r eigenvector centrality for all nodes after each batch, with update time sublinear in n. Propose a concrete scheme: (i) initial computation, (ii) update after batch using low-rank ΔA and a few subspace iterations, (iii) how to answer centralities fast, (iv) memory bounds, (v) two optimizations, (vi) comparison to recomputing from scratch after every batch. Express asymptotics in n, m, B, r, ε?","answer":"Maintain a rank-r approximation X of the top eigenvectors of A. After each batch ΔA, compute a low-rank update on the touched subgraph and perform a few subspace iterations to refresh X. Centralities ","explanation":"## Why This Is Asked\n\nTests ability to design incremental spectral methods for dynamic graphs and reason about trade-offs between update time, accuracy, and memory.\n\n## Key Concepts\n\n- Eigenvector centrality; Rayleigh quotient\n- Low-rank updates; batch dynamics\n- Subspace iteration; spectral gap\n- Complexity trade-offs; memory vs accuracy\n\n## Code Example\n\n```javascript\n// simplified incremental update\nfunction updateCentrality(X, A, DeltaA, touched) {\n  const Y = project(DeltaA, X);\n  X = orthonormalize(X.add(DeltaA.dot(X)));\n  return X;\n}\n```\n\n## Follow-up Questions\n\n- How does the spectral gap affect ε guarantees?\n- How would you extend to dynamic node insertions?","diagram":"flowchart TD\n  A[Batch ΔA] --> B[Low-rank projection]\n  B --> C[Subspace iterations on X]\n  C --> D[Updated X (r-dim)]\n  D --> E[Answer centrality via Rayleigh quotient]\n  E --> F[Queries O(1)]","difficulty":"advanced","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Plaid","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T14:37:04.049Z","createdAt":"2026-01-18T14:37:04.049Z"},{"id":"q-3920","question":"In a directed weighted graph G=(V,E) with nonnegative weights, edges are inserted online in batches of size B. After each batch you must maintain approximate eccentricities ecc(v)=max_u dist(v,u) for all v within a (1+ε) factor. Propose a practical scheme combining (i) a landmark-based distance substructure from a fixed set L (|L|=k), (ii) selective exact updates for nodes affected by the batch, and (iii) a periodic rebuild. Provide preprocessing, per-batch update time, memory, and a comparison to recomputing all-pairs eccentricities from scratch after each batch. Express asymptotics in n=|V|, m=|E|, k, B, ε?","answer":"Use a two-tier approach: precompute dist(.,ℓ) for k landmarks ℓ∈L; after a batch of B insertions, identify affected nodes (endpoints plus neighborhood up to depth d) and run bounded Dijkstra to tighte","explanation":"## Why This Is Asked\nTests practical dynamic distance maintenance with provable bounds and batch updates.\n\n## Key Concepts\nDynamic graphs, landmarks, incremental shortest paths, amortized analysis, rebuild strategies.\n\n## Code Example\n```javascript\n// placeholder\n```\n\n## Follow-up Questions\n- How would you adapt if edge weights change? \n- How to pick k and T for balanced latency and memory?","diagram":null,"difficulty":"advanced","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T15:36:44.940Z","createdAt":"2026-01-18T15:36:44.941Z"},{"id":"q-3971","question":"Scenario: A DAG G=(V,E) with a fixed source S. Each edge e=(u,v) contributes 1 path along that edge; the number of distinct S→v paths is computed by DP over a topological order. Edges can be inserted online in batches of size B. Propose a practical hybrid scheme that maintains exact path counts to all v after each batch using (i) an incremental update pass limited to affected nodes, (ii) a periodic full recomputation every T batches, and (iii) a lazy rebuild trigger when incremental cost exceeds a threshold. Include initial preprocessing, amortized update time per batch, memory bounds, two optimizations, and a comparison to re-running the full DP after every batch?","answer":"Propose maintaining S→v path counts in a DAG by DP over a topological order. Precompute counts with counts[S]=1. After each batch of B edge insertions, propagate deltas through the affected region in ","explanation":"## Why This Is Asked\nTests dynamic programming under streaming updates and amortized analysis, a common real-world concern in routing, scheduling, and graph analytics at scale.\n\n## Key Concepts\n- Dynamic DP on DAGs, topological order maintenance\n- Batch update amortization, threshold-based lazy rebuild\n- Trade-offs: incremental vs full recomputation, memory bounds\n\n## Code Example\n```javascript\n// Pseudo: update path counts on batch insertions\nfunction updateBatchCounts(G, batch) {\n  // G: DAG with topo order; counts[v] stored\n  let touched = new Set();\n  for (let e of batch) {\n    // e: {u,v}\n    touched.add(e.v); touched.add(e.u);\n  }\n  // propagate in topo order from minIndex(touched)\n  // ... detailed implementation depends on graph structure\n}\n```\n\n## Follow-up Questions\n- How would you handle large path counts exceeding 64-bit and avoid overflow?\n- How to optimally choose B and T given dynamic update patterns?\n- Could this extend to counting modulo a large prime to preserve exactness with bounded memory?","diagram":"flowchart TD\n  A[Batch Insertions] --> B[Affected Region]\n  B --> C[Incremental Update]\n  C --> D{Delta > Threshold?}\n  D -->|Yes| E[Lazy Rebuild / Full DP]\n  D -->|No| F[Wait for next batch]\n  E --> G[Periodic Full Rebuild Every T Batches]\n  G --> H[Update Counts]\n  H --> I[Ready for Next Batch]","difficulty":"intermediate","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","NVIDIA","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T17:39:31.122Z","createdAt":"2026-01-18T17:39:31.122Z"},{"id":"q-4046","question":"Scenario: A log analytics service processes a high-velocity stream of user IDs and needs the number of unique users in the last W events (sliding window). Design two practical strategies to maintain that count as IDs arrive: (A) recompute from scratch for every window; (B) maintain a hash-map of counts and update incrementally. Provide asymptotics for time and space in terms of N, W, and batch size B, and compare to a full recompute after each batch?","answer":"Naive: O((N−W+1)W) time, O(W) memory; recomputes each window. Incremental: O(1) average per ID, O(W) memory, total O(N) time; use a hash map of counts by ID; on slide, decrement leaving ID and remove ","explanation":"## Why This Is Asked\nTests practical time-space reasoning for streaming window problems; contrasts naive recomputation with incremental maintenance under amortized analysis. \n\n## Key Concepts\n- Sliding window distinct count; hash-map counts; amortized O(1) updates; space O(W) (or O(U) where U is unique in window).\n\n## Code Example\n```javascript\n// Naive\nfunction distinctLastW(arr, W) {\n  const window = arr.slice(-W);\n  return new Set(window).size;\n}\n\n// Incremental (sketch)\nfunction distinctLastWIncremental(stream, W) {\n  const counts = new Map();\n  let distinct = 0;\n  // first W\n  // then slide: update counts for leaving and entering IDs\n  return distinct;\n}\n```\n\n## Follow-up Questions\n- How would you adapt for variable window sizes or out-of-order arrivals?\n- What data structure tweaks reduce hash overhead in high-cardinality streams?","diagram":"flowchart TD\n  A[New ID arrives] --> B{Window full?}\n  B -- Yes --> C[Decrement leaving ID count]\n  B -- No --> D[No removal yet]\n  C --> E[Increment entering ID count]\n  D --> E\n  E --> F[Report distinct count]","difficulty":"beginner","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Bloomberg","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T21:28:49.518Z","createdAt":"2026-01-18T21:28:49.518Z"},{"id":"q-4223","question":"You have n users as undirected nodes. Edges arrive in batches of size B, each batch adding friendships. After each batch, report the number of connected components in the graph. Propose a practical incremental scheme using a union-find (disjoint-set) structure with path compression and union by rank. Provide amortized per-edge cost, per-batch cost, and memory in terms of n, m, B, alpha(n). Also discuss when a rebuild might be preferable?","answer":"Initialize a DSU with n nodes and components=C=n. For each batch of B edges, run union(u,v) for every edge; if a union merges two sets, C--. After the batch, report C. Amortized per-edge cost is alpha","explanation":"## Why This Is Asked\nTests incremental connectivity intuition, DSU basics, and amortized analysis for real-world batch updates.\n\n## Key Concepts\n- Union-Find with path compression and union by rank\n- Alpha(n)\n- Batch vs online update trade-offs\n\n## Code Example\n```javascript\nclass DSU {\n  constructor(n){\n    this.parent = Array.from({length:n}, (_,i)=>i);\n    this.rank = new Array(n).fill(0);\n    this.components = n;\n  }\n  find(x){ return this.parent[x]===x? x: this.find(this.parent[x]); }\n  union(a,b){ let ra=this.find(a), rb=this.find(b); if(ra===rb) return false; if(this.rank[ra]<this.rank[rb])[ra,rb]=[rb,ra]; this.parent[rb]=ra; if(this.rank[ra]===this.rank[rb]) this.rank[ra]++; this.components--; return true; }\n}\n```\n\n## Follow-up Questions\n- How would you adapt for directed graphs and counting weak components?\n- What heuristics decide when to trigger a rebuild versus continuing online?","diagram":"flowchart TD\n  A[Batch arrives] --> B{New nodes?}\n  B -- Yes --> C[Init DSU entries for new nodes]\n  B -- No --> D[Union edges in batch]\n  D --> E[Update components]\n  E --> F[Report total components]","difficulty":"beginner","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T09:13:22.138Z","createdAt":"2026-01-19T09:13:22.138Z"},{"id":"q-4296","question":"Count inversions in an array of length n. Compare the naive approach O(n^2) with a merge-sort based approach that runs in O(n log n). Explain which inputs favor each, and discuss space usage and stability implications. End with ?","answer":"Classic: naive inversion count checks all pairs, giving O(n^2) time. The merge-sort based method counts inversions during merging, achieving O(n log n) time and O(n) extra space. For large n or random","explanation":"## Why This Is Asked\nThis tests basic algorithmic analysis and trade-offs between brute force and divide-and-conquer techniques.\n\n## Key Concepts\n- Time complexity and space usage\n- Inversions problem and two standard approaches\n- Trade-offs with input size and data order\n\n## Code Example\n```javascript\nfunction countInversionsNaive(a){\n  let c=0;\n  for(let i=0;i<a.length;i++){\n    for(let j=i+1;j<a.length;j++){\n      if(a[i]>a[j]) c++;\n    }\n  }\n  return c;\n}\nfunction countInversionsMergeSort(a){\n  function sortCount(arr){\n    if(arr.length<=1) return {array: arr, inv:0};\n    const mid = Math.floor(arr.length/2);\n    const left = sortCount(arr.slice(0,mid));\n    const right = sortCount(arr.slice(mid));\n    let i=0,j=0, merged=[], inv= left.inv + right.inv;\n    while(i<left.array.length && j<right.array.length){\n      if(left.array[i] <= right.array[j]) merged.push(left.array[i++]);\n      else { merged.push(right.array[j++]); inv += left.array.length - i; }\n    }\n    return {array: merged.concat(left.array.slice(i)).concat(right.array.slice(j)), inv: inv};\n  }\n  return sortCount(a).inv;\n}\n```\n\n## Follow-up Questions\n- How would you extend the merge-count to return the actual inversion pairs?\n- What changes if the input is already sorted or nearly sorted?","diagram":null,"difficulty":"beginner","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T11:51:09.110Z","createdAt":"2026-01-19T11:51:09.110Z"},{"id":"q-4352","question":"You have an integer array a of length n. You receive online batches of B point updates (set a[i] = v). After each batch you must answer: how many elements in a exceed a fixed threshold T? Propose a practical data structure and amortized analysis for updates and queries, including preprocessing time, memory, and a comparison to recomputing counts from scratch after every batch. Assume B ≤ n and T is fixed?","answer":"Use sqrt-decomposition: partition a into blocks of size s ≈ sqrt(n) and store for each block cnt_gt = # of elements > T. Preprocessing: O(n) time, O(n) space. On update a[i] = v, adjust cnt_gt of its ","explanation":"## Why This Is Asked\nNew angle: practical data-structure design for batch updates.\n\n## Key Concepts\n- Amortized analysis\n- sqrt decomposition\n- fixed-threshold counters\n\n## Code Example\n```javascript\n// Pseudocode illustrating update and query\n```\n\n## Follow-up Questions\n- How would you adapt for varying thresholds without rebuilding?\n- What if B is large relative to n?","diagram":null,"difficulty":"beginner","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Apple","Databricks"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T15:38:59.062Z","createdAt":"2026-01-19T15:38:59.062Z"},{"id":"q-4459","question":"You receive a real-time stream of integers in [0, M-1], arriving in batches of size B. After each batch, determine whether value x has appeared at least t times so far. Propose a Count-Min Sketch based solution: set width w and depth d in terms of ε and δ to bound errors, compute update time per batch, per-query time, memory, and compare to exact counting. Assume B, t, ε, δ fixed?","answer":"Use Count-Min Sketch with width w = ceil(e/ε) and depth d = ceil(ln(1/δ)). For each element, update all d counters; a batch of B elements costs O(Bd). To answer freq(x), take the minimum across the d ","explanation":"## Why This Is Asked\nStreaming frequency sketches are common in prod telemetry. This question probes knowledge of CMS, accuracy guarantees, batch processing, and amortized space-time trade-offs.\n\n## Key Concepts\n- Count-Min Sketch (CMS)\n- ε, δ error bounds\n- Update/query complexity for batches\n- Space complexity O(wd)\n- Trade-offs vs exact counting\n\n## Code Example\n```javascript\n// CMS skeleton\nconst w = Math.ceil(Math.E / eps);\nconst d = Math.ceil(Math.log(1/delta));\nconst table = Array.from({length: d}, () => Array(w).fill(0));\n\nfunction update(x) { for (let i = 0; i < d; i++) table[i][hash_i(i, x) % w]++; }\n\nfunction query(x) { let m = Infinity; for (let i = 0; i < d; i++) m = Math.min(m, table[i][hash_i(i, x) % w]); return m; }\n```\n\n## Follow-up Questions\n- How would deletions or non-monotone streams affect CMS choices?\n- How would you monitor actual error in production without degrading performance?","diagram":null,"difficulty":"beginner","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","DoorDash","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T19:43:56.248Z","createdAt":"2026-01-19T19:43:56.248Z"},{"id":"q-4598","question":"Dynamic SSSP under batch edge insertions: A directed graph with n nodes; edges are added online in batches of size B. After each batch, maintain approximate distances dist(s,v) from a fixed source s within a (1+ε) factor for all v. Propose a practical scheme using (i) a fixed landmark set L with precomputed dist(s,l) and dist(l,v), (ii) selective exact relaxations for nodes affected by the batch, and (iii) periodic rebuild. Provide preprocessing, per-batch update time, memory, and a comparison to recomputing SSSP from scratch?","answer":"Leverage k landmarks; precompute dist(s,l) for l in L and dist(l,v) via a one-time multi-source run. For each batch, run relaxations only from nodes touched by new edges, updating dist(s,v) via d(s,u)","explanation":"## Why This Is Asked\nTests practical design of dynamic shortest paths with batch updates, balancing accuracy and speed.\n\n## Key Concepts\n- Incremental relaxations; landmark sketches; periodic rebuilds; amortized cost.\n- Trade-offs: landmark count k, rebuild threshold, sparse updates.\n- Correctness: maintains (1+ε) approximation under non-deleting edges.\n\n## Code Example\n```javascript\n// Pseudo: maintain distS[v], distL[l][v], update on batch\n```\n\n## Follow-up Questions\n- How would you extend to multiple sources or undirected graphs?\n- What about negative edges, or dynamic edge deletions?","diagram":"flowchart TD\n  A[Batch arrive] --> B[Relax touched nodes]\n  B --> C{Improve dist?}\n  C -->|Yes| D[Update dist(s,v) via landmarks]\n  C -->|No| E[Batch end]\n  D --> F[Periodic rebuild?]\n  F --> G[Full SSSP refresh]\n  E --> A","difficulty":"advanced","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T04:11:57.938Z","createdAt":"2026-01-20T04:11:57.941Z"},{"id":"q-4702","question":"Real-time logs: integers in [0, M-1] arrive in batches of size B. After each batch, determine how many distinct values have appeared at least t times in total so far. Propose a practical solution using approximate counting, discuss update/query costs, and memory, and compare to recomputing counts from scratch. Express costs in terms of M, B, t, and number of processed batches?","answer":"Use a Count-Min Sketch CMS with width w=ceil(e/ε) and depth d=ceil(ln(1/δ)) to track per-value counts. Maintain a hash-set F of values whose CMS-estimate ≥ t; F grows only when a new value crosses t. ","explanation":"## Why This Is Asked\nTests ability to design streaming, memory‑efficient counting with provable guarantees beyond exact recounts.\n\n## Key Concepts\n- Count-Min Sketch for non-negative counts; parameter trade-offs ε, δ\n- Thresholded tracking of crossing values; lazy enumeration\n- Amortized batch updates vs full re-scan\n\n## Code Example\n```javascript\nclass CMS { constructor(w, d) { /* init */ } update(x, delta=1) { /* per-hash row */ } estimate(x) { /* min over rows */ } }\n```\n\n## Follow-up Questions\n- How to choose ε, δ for a target error?  \n- How would you extend for deletions or windowed counts?  \n- What's the effect of hash collisions on |F| accuracy?","diagram":"flowchart TD\n  Start --> CMS_Update\n  CMS_Update --> Threshold_Check\n  Threshold_Check --> Update_F\n  Update_F --> Return_Count","difficulty":"beginner","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Instacart","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T08:53:43.396Z","createdAt":"2026-01-20T08:53:43.396Z"},{"id":"q-4715","question":"Dynamic graph: edges are inserted in batches of size B in a weighted directed graph. After each batch, design a practical incremental algorithm to maintain approximate betweenness centrality for all nodes within a (1+ε) factor. Use sampling with s sources and incremental SP-tree updates; specify preprocessing time, per-batch update time, memory, and compare to recomputing BC from scratch?","answer":"Use a sampling-based approximate betweenness centrality with dynamic updates. Sample s sources; maintain weighted shortest-path trees from each source. On a batch of B edge insertions, identify affect","explanation":"## Why This Is Asked\nTests ability to combine sampling, incremental shortest-path maintenance, and BC error bounds in a dynamic setting.\n\n## Key Concepts\n- Betweenness centrality, sampling-based approximations, incremental shortest paths, dynamic graphs, error guarantees.\n\n## Code Example\n```javascript\nfunction init(n, m, s) {\n  // init sampling and data structures\n}\n```\n\n## Follow-up Questions\n- How to choose s to meet a target ε?\n- How to parallelize updates across sources and scale with high-degree batches?\n","diagram":null,"difficulty":"advanced","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","Oracle","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T09:43:16.276Z","createdAt":"2026-01-20T09:43:16.276Z"},{"id":"q-4830","question":"You receive a stream of integers in [0, M-1] in batches of B. Maintain a sliding window of the most recent N elements. After each batch, report the current number of distinct values in the window. Propose a practical data structure (e.g., hashmap + queue) to support batch inserts, updates, and a correct count, and derive update time, memory, and a comparison to recomputing from scratch?","answer":"Use a queue for the last N values and a hashmap freq to count occurrences in the window. For each batch, enqueue B items and increment freq; while queue.size() > N, dequeue oldest and decrement freq, ","explanation":"## Why This Is Asked\nTests ability to design a simple real-time data structure for sliding-window distinct counting, including correctness, amortized analysis, and memory trade-offs.\n\n## Key Concepts\n- Sliding window maintenance\n- Hash map counters\n- Amortized analysis\n- Streaming memory trade-offs\n\n## Code Example\n```python\nfrom collections import deque, Counter\nclass SlidingDistinct:\n    def __init__(self, N):\n        self.N = N\n        self.q = deque()\n        self.c = Counter()\n    def batch(self, arr):\n        for x in arr:\n            self.q.append(x)\n            self.c[x] += 1\n        while len(self.q) > self.N:\n            y = self.q.popleft()\n            self.c[y] -= 1\n            if self.c[y] == 0:\n                del self.c[y]\n```\n\n## Follow-up Questions\n- How would you adapt to dynamic N?\n- What if value domain is large or unbounded; how would you bound memory?\n","diagram":null,"difficulty":"beginner","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","MongoDB","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T15:11:26.018Z","createdAt":"2026-01-20T15:11:26.018Z"},{"id":"q-4910","question":"Maintain a dynamic topological order for a directed graph as edges are inserted in batches of size B. After each batch, if the graph remains acyclic, output a valid topological order; if a cycle appears, provide a short certificate. Propose a practical scheme that (i) uses the current order and only reorders affected nodes, (ii) triggers a full topo recomputation after a threshold of work, and (iii) includes two optimizations and a complexity/space comparison to recomputing from scratch after every batch?","answer":"Keep a live topological order P. For each batch, process edges; if pos[u] < pos[v], ignore. If not, perform a localized reordering that moves u and its affected successors forward along P, stopping wh","explanation":"## Why This Is Asked\nTests practical dynamic graph skills: incremental order maintenance with cycles, batch budgeting.\n\n## Key Concepts\n- Dynamic topological order\n- Local vs global rebuild\n- Amortized analysis with rebuild thresholds\n- Cycle certificates and correctness proofs\n\n## Code Example\n```javascript\n// sketch: determine if edge u->v is safe; otherwise lazily reorder\nfunction insertEdgeAndUpdate(order, pos, edges, u, v, budget) {\n  if (pos[u] < pos[v]) return {acyclic: true};\n  // perform limited reordering within budget\n  // ... omitted for brevity\n}\n```\n\n## Follow-up Questions\n- How to adapt for mixed insertions/deletions?\n- What are worst-case guidelines for the threshold to guarantee amortized bounds?","diagram":null,"difficulty":"intermediate","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","Meta","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T19:00:01.946Z","createdAt":"2026-01-20T19:00:01.946Z"},{"id":"q-5042","question":"Maintain a directed graph G=(V,E) with nonnegative weights. Edges arrive online in batches of size B (no deletions). After each batch, design a (1+ε)-approximate SSSP oracle from a fixed source s to all nodes. State (i) preprocessing time to initialize distances from s, (ii) amortized update time per batch to update the oracle, (iii) query time dist(s,v) for any v, (iv) total memory, (v) two practical optimizations, and (vi) a comparison to re-running Dijkstra from s after every batch. Give asymptotics in n, m, B, ε?","answer":"Use a landmark-based SSSP approach: select k landmarks including the source s, and store distances from s and from each landmark to all nodes. Preprocess by running Dijkstra from k+1 roots. For each batch of B edges, relax the edges and propagate improvements through the landmark network. This yields a (1+ε)-approximation with appropriate landmark selection.","explanation":"## Why This Is Asked\nExplores dynamic shortest paths with batch updates and amortized analysis, common in routing and networks.\n\n## Key Concepts\n- Dynamic graphs with batch updates\n- Landmark/labeling SSSP techniques\n- Trade-offs between preprocessing, updates, and queries\n- Amortization vs worst-case analysis\n- Memory budgeting considerations\n\n## Code Example\n```javascript\n// Draft skeleton for dynamic SSSP with landmarks\nclass DynamicSSSP {\n  constructor(n, s, k) {\n    this.n = n;\n    this.s = s;\n    this.k = k;\n    // Initialize data structures\n  }\n  \n  batchInsert(edges) {\n    // Relax affected edges and update landmark distances\n    // Propagate distance improvements efficiently\n  }\n}\n```","diagram":null,"difficulty":"advanced","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Cloudflare","Lyft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-21T05:09:19.517Z","createdAt":"2026-01-21T02:48:00.052Z"},{"id":"q-5073","question":"Dynamic reachability indexing for DAGs: edges added in batches of size B; after each batch, build a (1+epsilon)-approximate reachability oracle using a dynamic two-hop labeling with k hubs. For every node u, store F(u) = {h in H | h reaches u} and for every node v, store B(v) = {h in H | v reaches h}. Reach(u,v) iff F(u) intersect B(v) is non-empty. After a batch, recompute labels only for nodes affected by new edges. Provide choices: hub selection, guarantees, update time, query time, memory, two optimizations, and a comparison to full recomputation. Use n=|V|, m=|E|, k, epsilon, B?","answer":"Use a dynamic two-hop labeling with k hubs. Maintain for every node u a forward label F(u) = {hub h | h can reach u} and for every node v a backward label B(v) = {hub h | v can reach h}. Reach(u,v) if","explanation":"## Why This Is Asked\nTests dynamic reachability indexing, DAG invariants, and trade-offs between update and query costs. It probes practical design choices for incremental graphs in build-system-style pipelines.\n\n## Key Concepts\n- Two-hop labeling, hub selection, approximate reachability, batch updates, amortized analysis.\n\n## Code Example\n```javascript\n// Pseudo: update labels for affected nodes; query reach(u,v) via intersection\n```\n\n## Follow-up Questions\n- How would you adapt if deletions were allowed? \n- What failure modes exist for randomized hub selection?","diagram":"flowchart TD\n  A[Edge batch] --> B[Identify affected nodes]\n  B --> C[Update F(u), B(v) for affected nodes]\n  C --> D[Query reach(u,v) via F(u) ∩ B(v)]\n  D --> E[Optional rebuild triggers]\n","difficulty":"advanced","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Hashicorp","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T04:33:16.166Z","createdAt":"2026-01-21T04:33:16.166Z"},{"id":"q-5121","question":"In a streaming setting, integers arrive in batches of size B; after each batch you must report the k-th smallest value seen so far, with k small (e.g., ≤100). Propose a practical online algorithm using a max-heap of size k to maintain the k smallest elements, and analyze update time per batch, query time, and memory, plus a comparison to a baseline that stores all seen numbers?","answer":"Maintain a max-heap of the k smallest seen so far. For every value x in a batch of B, if heap size < k push x; else if x < heap.max, pop the max and push x. After the batch, the k-th smallest is heap.","explanation":"## Why This Is Asked\n\nTests ability to design a streaming solution with tight memory and concrete complexity. Using a max-heap of size k yields exact k-th order statistics with minimal memory, suitable for beginner to intermediate candidates.\n\n## Key Concepts\n\n- Order statistics in streams\n- Max-heap to track k smallest elements\n- Time: per-element O(log k), per-batch O(B log k)\n- Space: O(k) versus naive O(n)\n\n## Code Example\n\n```python\nimport heapq\n\ndef kth_smallest_in_batches(k, batches):\n    # maintain a max-heap via negation\n    heap = []  # store negatives to simulate max-heap\n    results = []\n    for batch in batches:\n        for x in batch:\n            if len(heap) < k:\n                heapq.heappush(heap, -x)\n            else:\n                if x < -heap[0]:\n                    heapq.heapreplace(heap, -x)\n        results.append(-heap[0])  # current k-th smallest\n    return results\n```\n\n## Follow-up Questions\n\n- How would you adapt if k grows over time or batches vary in size B?\n- What happens if values repeat—how to handle ties correctly?","diagram":"flowchart TD\n  A[Batch arrival] --> B[For each x in batch]\n  B --> C{heap size < k}\n  C -->|Yes| D[Push x]\n  C -->|No| E[x < -heap[0]?]\n  E -->|Yes| F[Replace max with x]\n  E -->|No| G[Ignore x]\n  D --> H[Next x]\n  F --> H\n  H --> B\n  H --> I[After batch: k-th smallest = -heap[0]]","difficulty":"beginner","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T07:09:51.050Z","createdAt":"2026-01-21T07:09:51.050Z"},{"id":"q-5146","question":"Scenario: A streaming analytics service maintains a dynamic multiset of item scores. Online batches of B updates arrive (insertions and deletions). After each batch you must report the sum of the k largest scores. Propose a data structure and algorithm to support batch updates and a top-k-sum query with amortized O(log N) per update, describe how to maintain two balanced BSTs (TopK and Rest), how elements move between them, and provide space bounds. Compare to recomputing the top-k sum from scratch after every batch?","answer":"Maintain two ordered multisets: TopK with the k largest values and Rest with the remainder, plus a running sumTopK. For each insert x: if TopK.size < k, put in TopK and sumTopK+=x; else if x > min(Top","explanation":"## Why This Is Asked\nTests ability to design an online data-structure solution for order-statistics with updates. It emphasizes maintainable invariants, amortized costs, and careful handling of deletions.\n\n## Key Concepts\n- Dynamic order statistics via two balanced BSTs or heaps\n- Maintaining a running top-k sum\n- Correct handling of duplicates and batch updates\n- Amortized vs worst-case costs and space bounds\n\n## Code Example\n```javascript\nclass TopKSums {\n  constructor(k) { this.k = k; /* II: implement with two multisets and a sum */ }\n  insert(x) { /* ... */ }\n  delete(x) { /* ... */ }\n  topKSum() { /* ... */ return this.sumTopK; }\n}\n```\n\n## Follow-up Questions\n- How to handle changing k over time?\n- How does the structure perform under adversarial updates vs random updates?","diagram":"flowchart TD\n  A[Batch updates arrive] --> B[Determine if update affects TopK]\n  B --> C[Rebalance TopK/Rest if needed]\n  C --> D[Update sumTopK]\n  D --> E[Return sumTopK for batch]","difficulty":"intermediate","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Discord","Hugging Face"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T08:07:09.463Z","createdAt":"2026-01-21T08:07:09.463Z"},{"id":"q-5166","question":"In a real-time event stream, each event has a category in [1..C]. Events arrive in batches of size B. After each batch, return the current top-3 categories by estimated frequency. Propose a practical online algorithm using Misra-Gries with parameter s; derive update time per batch and memory, provide error guarantees and a comparison to exact counting?","answer":"Use Misra-Gries with s counters; keep at most s−1 items. For each event in the batch: if in counters, inc; else if free slot, insert with 1; else dec all counters by 1. After batch, report top-3 by co","explanation":"## Why This Is Asked\n\nThis question probes understanding of streaming frequency estimation with bounded memory, a common real-world requirement in analytics dashboards.\n\n## Key Concepts\n\n- Misra-Gries algorithm for frequent items\n- Online/update-time vs. memory trade-offs\n- Error bounds and how they scale with N and s\n\n## Code Example\n\n```javascript\n// Pseudo\nclass MG {\n  constructor(s){\n    this.s = s;\n    this.counters = new Map();\n  }\n  add(x){\n    if (this.counters.has(x)) { this.counters.set(x, this.counters.get(x)+1); return; }\n    if (this.counters.size < this.s-1) {\n      this.counters.set(x, 1);\n      return;\n    }\n    for (let k of Array.from(this.counters.keys())) {\n      this.counters.set(k, this.counters.get(k)-1);\n      if (this.counters.get(k) === 0) this.counters.delete(k);\n    }\n  }\n  topK(k){\n    // return top k by counts\n    return [...this.counters.entries()].sort((a,b)=>b[1]-a[1]).slice(0,k);\n  }\n}\n```\n\n## Follow-up Questions\n\n- How to choose s for a desired error?\n- What if there are ties in top-3?","diagram":null,"difficulty":"beginner","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Square","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T08:58:27.401Z","createdAt":"2026-01-21T08:58:27.402Z"},{"id":"q-5182","question":"You process a stream of user IDs arriving in batches of size B. After each batch, determine if the number of distinct IDs seen so far is at least D. Propose a HyperLogLog-based solution: choose parameters to achieve relative error ε with high probability, express update time per batch, query time, memory in terms of ε, δ, B, and D. Include a concrete example of parameter choices and how the threshold decision is made?","answer":"Use HyperLogLog with m = ceil((1.04/ε)^2) registers and run k independent sketches (k=3) for a probabilistic bound δ via median. After each batch, update all sketches for B elements (O(B) time), query","explanation":"## Why This Is Asked\nAssesses practical streaming estimation under tight memory, a common real-world need.\n\n## Key Concepts\n- Cardinality estimation, HyperLogLog registers, error ~1.04/√m\n- Median-of-independents to bound δ\n- Amortized batch updates, cost vs. exact counting\n\n## Code Example\n```javascript\nclass HyperLogLog { constructor(m){...} add(x){...} estimate(){...} }\n```\n\n## Follow-up Questions\n- How would you handle deletions or sliding windows?\n- How to choose ε and δ for a fixed memory budget?","diagram":"flowchart TD\nA[Batch arrives] --> B[Hash & update HLL sketches]\nB --> C[Compute estimates]\nC --> D{Compare to D threshold}\nD --> E[Output decision]","difficulty":"beginner","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","OpenAI","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T10:00:24.970Z","createdAt":"2026-01-21T10:00:24.970Z"},{"id":"q-5339","question":"You process latency samples of database operations arriving in batches of size B. After each batch, you must report the approximate 50th, 90th, and 99th percentile latency over all seen samples. Propose a practical online quantile data structure (e.g., GK sketch or TDigest) with parameter choices to bound error ≤ε with probability ≥1-δ. Provide update time per batch, per-query time, and memory in terms ε, δ, B, and L. Compare to recomputing after every batch?","answer":"Use a GK sketch or TDigest to maintain online quantiles. For each sample, update in O(log(1/ε)) time with memory O(1/ε). Batch cost is O(B log(1/ε)); querying p-th percentile is O(1). Memory is O(1/ε)","explanation":"## Why This Is Asked\nLatency quantiles are critical for SLOs in modern data stores. A streaming monitor must provide fast, bounded estimates while data grows unbounded. GK sketch or TDigest fits: small memory, controllable error.\n\n## Key Concepts\n- Online quantile estimation\n- Error bounds ε and confidence δ\n- Trade-offs between per-sample update cost and batch query cost\n\n## Code Example\n```javascript\nclass GKSketch {\n  constructor(eps) {\n    this.eps = eps;\n    // compact representation\n  }\n  add(x) { /* update */ }\n  query(q) { /* return estimate for percentile q */ }\n}\n```\n\n## Follow-up Questions\n- How would you handle drift or varying data distributions?\n- How would you extend to multiple concurrent streams or sliding windows?","diagram":null,"difficulty":"beginner","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T17:34:43.443Z","createdAt":"2026-01-21T17:34:43.444Z"},{"id":"q-5634","question":"Advanced complexity-analysis: You maintain a directed graph G=(V,E) with n=|V|, m=|E|. Edges are online-inserted in batches of size B (no deletions). After each batch you must answer whether there exists a path from any source in S to any target in T, where |S|=|T|=k. Propose a practical incremental reachability structure that (i) preprocesses efficiently, (ii) updates amortized per batch, (iii) answers reachability queries in O(1) time, (iv) uses near-linear space, (v) includes two optimizations, and (vi) compares to recomputing transitive closure after every batch. Provide asymptotics in terms of n,m,k,B?","answer":"Preprocess collapses G into its SCC DAG and builds a reachability bitset per SCC. On batch insertions, only touched regions propagate along the topo order, updating bitsets. Query(s,t) checks if s’s S","explanation":"## Why This Is Asked\nTests incremental graph algorithms and amortized bounds under online edge insertions with concrete batch sizes. Requires a practical structure, not just theory.\n\n## Key Concepts\n- SCC condensation, transitive closure with bitsets, topological order, batch amortization, cache-friendly data layouts.\n\n## Code Example\n```javascript\n// Pseudocode sketch for batch update\nfor edge in batch:\n  updateSCC(edge)\n  propagate(edge)\n```\n\n## Follow-up Questions\n- How would you handle queries when the graph has many small SCCs vs few large SCCs?\n- What are failure modes if batches create long cascades of updates?","diagram":null,"difficulty":"advanced","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Discord","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T09:07:39.655Z","createdAt":"2026-01-22T09:07:39.655Z"},{"id":"q-5715","question":"Beginner challenge: A real-time stream of user IDs (strings) arrives in batches of size B. After each batch, estimate the total number of distinct IDs seen so far with a relative error ε. Propose a practical online data structure and provide the per-batch update time, per-query time, and memory usage, along with how error scales with ε. Assume no deletions and that IDs are hashed to avoid duplicates?","answer":"Use HyperLogLog (HLL) cardinality estimator. Set m = ceil(1.04/ε^2) registers. For each ID in a batch, hash to 64 bits and update the corresponding register; update time is O(B). After batch, count vi","explanation":"## Why This Is Asked\nThis tests practical streaming cardinality with fixed memory and simple math for error guarantees.\n\n## Key Concepts\n- HyperLogLog, relative error ε, memory ~ m\n- Hashing to 64-bit space; register updates\n- Mergeability for multi-stream fusion\n\n## Code Example\n```javascript\nclass HyperLogLog {\n  constructor(p) { /* precision p -> m = 2^p */ }\n  add(id) { /* hash and update register */ }\n  estimate() { /* return count */ }\n}\n```\n\n## Follow-up Questions\n- How to merge two HLLs in constant time? \n- What if ε=0.01 requires how many registers and memory?\n","diagram":null,"difficulty":"beginner","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","LinkedIn","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T13:11:33.049Z","createdAt":"2026-01-22T13:11:33.049Z"},{"id":"q-5793","question":"You manage a live 2D plane with axis-aligned rectangles representing ad slots. Rectangles arrive online in batches of size B; they never disappear. After each batch, determine if there exists any uncovered point. Propose a practical incremental rectangle-union structure that (i) preprocesses quickly, (ii) updates amortized per batch, (iii) answers 'gap exists' in O(1), (iv) uses near-linear space, (v) includes two optimizations, and (vi) compares to recomputing the union from scratch after every batch. Provide asymptotics in terms of N total rectangles and plane bounds?","answer":"Propose an incremental rectangle-union (IRU) that keeps a segment-tree over x, with each node storing union of y-intervals for covered stripes. On batch insert of B rectangles, merge intervals bottom-","explanation":"## Why This Is Asked\nTests ability to design a scalable geometric data structure with batch updates and a real decision problem (gap existence). \n\n## Key Concepts\n- Dynamic rectangle unions, plane sweep, amortized updates, space efficiency.\n- Trade-offs between exact area maintenance and lazy recomputation.\n\n## Code Example\n```javascript\nfunction insertRectangles(rects) { /* placeholder */ }\n```\n\n## Follow-up Questions\n- How would you extend to non-uniform plane bounds? \n- What if deletions were allowed?","diagram":null,"difficulty":"advanced","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Instacart","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T17:00:53.875Z","createdAt":"2026-01-22T17:00:53.875Z"},{"id":"q-5907","question":"You're modeling a real-time collaboration network. The undirected graph has n nodes; edges arrive in batches of size B. After each batch, report the current number of connected components. Propose a DSU-based solution with path compression and union by size, analyze preprocessing time and memory, per-batch update time for B unions, total memory, and compare to recomputing CC from scratch after every batch. Express asymptotics in terms of n, m, B, and α(n)?","answer":"Maintain a Disjoint Set Union (DSU) structure over n nodes. Initialize components = n. For each batch of B edges (u,v), if find(u) ≠ find(v) then union the sets and decrement components. After processing each batch, report the current component count. Time complexity: O(n) for initialization, O(B·α(n)) per batch for B union operations, O(m·α(n)) total for processing all m edges. Memory usage: O(n) for parent and size arrays.","explanation":"## Why This Is Asked\n\nThis question tests understanding of dynamic connectivity under incremental edge insertions and practical amortized analysis in graph algorithms.\n\n## Key Concepts\n\n- Disjoint Set Union (DSU) with path compression and union by size\n- Amortized α(n) (inverse Ackermann function) for near-constant operations\n- Batch processing efficiency vs full recomputation approaches\n\n## Code Example\n\n```javascript\nclass DSU {\n  constructor(n) {\n    this.parent = Array.from({length: n}, (_, i) => i);\n    this.size = new Array(n).fill(1);\n    this.components = n;\n  }\n  \n  find(a) {\n    if (this.parent[a] !== a) {\n      this.parent[a] = this.find(this.parent[a]);\n    }\n    return this.parent[a];\n  }\n  \n  union(a, b) {\n    const rootA = this.find(a);\n    const rootB = this.find(b);\n    if (rootA === rootB) return false;\n    \n    if (this.size[rootA] < this.size[rootB]) {\n      [rootA, rootB] = [rootB, rootA];\n    }\n    this.parent[rootB] = rootA;\n    this.size[rootA] += this.size[rootB];\n    this.components--;\n    return true;\n  }\n}\n\nfunction processBatches(n, batches) {\n  const dsu = new DSU(n);\n  const results = [];\n  \n  for (const batch of batches) {\n    for (const [u, v] of batch) {\n      dsu.union(u, v);\n    }\n    results.push(dsu.components);\n  }\n  \n  return results;\n}\n```\n\n## Analysis\n\n**Preprocessing:** O(n) time to initialize DSU arrays, O(n) memory for parent and size arrays.\n\n**Per-batch updates:** O(B·α(n)) time for B union operations with path compression and union by size.\n\n**Total complexity:** O(n + m·α(n)) time, O(n) memory.\n\n**Comparison to recomputation:** Full DFS/BFS after each batch would cost O(n + m) per batch, totaling O((n + m)·(m/B)) time—significantly worse for large graphs with many small batches.\n\nThe DSU approach leverages amortized near-constant operations, making it ideal for dynamic connectivity scenarios with incremental updates.","diagram":"flowchart TD\n  A[Init DSU(n)] --> B[Process batch of B edges]\n  B --> C{Union reduces components?}\n  C -->|Yes| D[components--] --> E[Report components]\n  C -->|No| E","difficulty":"beginner","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Snowflake","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T05:55:34.822Z","createdAt":"2026-01-22T21:47:09.549Z"},{"id":"q-5988","question":"You receive a real-time stream of user IDs in batches of size B; after each batch, estimate the number of distinct IDs seen so far with relative error ε and confidence 1−δ. Design a HyperLogLog-based solution: choose m in terms of ε, δ; specify hashing, per-item update time, memory, and how to merge across batches. Compare to exact counting in time and space?","answer":"Use HyperLogLog with m = ceil(1.04/ε²) registers. Hash each ID to 64 bits, place the leading-zero position into the corresponding register, and keep the maximum. Update time per item is O(1); per batch O(B). Memory is O(m) = O(1/ε²). To merge across batches, take element-wise maximum of registers. Compared to exact counting, HyperLogLog reduces space from O(N) to O(1/ε²) while maintaining O(1) per-item time.","explanation":"## Why This Is Asked\nTests practical streaming analytics: estimate distinct counts with small memory.\n\n## Key Concepts\n- Distinct counting with probabilistic sketches\n- HyperLogLog parameterization: m ~ 1.04/ε², mergeability, hash quality\n- Trade-offs: accuracy vs memory, merging vs reinitializing\n\n## Code Example\n```javascript\nfunction add(log, x){ const h = hash64(x); const r = leadingZeros(h); log[Math.floor(h%log.length)] = Math.max(log[Math.floor(h%log.length)], r); }\n```\n\n## Follow-up Questions\n- How would you handle deletes or time-based expirations?\n- How to calibrate ε, δ for a 1e6-user system?","diagram":"flowchart TD\n  A[Start] --> B[Hash ID to 64 bits]\n  B --> C[Update register]\n  C --> D[Batch complete: compute estimate]\n  D --> E[Merge if needed]\n  E --> F[Return estimate]","difficulty":"beginner","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Microsoft","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T04:53:25.018Z","createdAt":"2026-01-23T02:44:44.551Z"},{"id":"q-6047","question":"Given a directed graph G=(V,E) with n=|V|, edges inserted online in batches of size B (no deletions), after each batch determine if G contains a directed cycle. Propose a practical incremental cycle-detection structure: (i) preprocessing time, (ii) amortized update time per batch, (iii) cycle-detection query time, (iv) memory usage, (v) two optimizations, and (vi) a comparison to re-running a topological sort after every batch. Provide asymptotics in terms of n, m, B?","answer":"Propose incremental cycle-detection using a dynamic topological order maintained by an order-maintenance structure. For each batch of B edge insertions, fix only the affected region via local rerootin","explanation":"## Why This Is Asked\nIncremental cycle detection in streaming graphs is a realistic bottleneck in live systems like build systems or streaming dependency graphs. It tests dynamic data structures and amortized analysis beyond static recomputation.\n\n## Key Concepts\n- Dynamic graphs and incremental updates\n- Dynamic topological ordering\n- Order-maintenance data structures\n- Amortized vs worst-case guarantees\n- Trade-offs: preprocessing, update, query costs\n\n## Code Example\n```javascript\nclass DynCycleDetector {\n  constructor(n) { this.n=n; /* init */ }\n  insertEdge(u,v){ /* update dynamic order; return true if cycle created */ }\n  hasCycle(){ /* return boolean */ }\n}\n```\n\n## Follow-up Questions\n- How would you extend to handle multiple batches concurrently?\n- How does adversarial insertion affect guarantees?\n- What are practical benchmarks to validate the amortized bounds?","diagram":null,"difficulty":"advanced","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Tesla","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T06:53:14.781Z","createdAt":"2026-01-23T06:53:14.781Z"},{"id":"q-6129","question":"You maintain an undirected graph with n nodes and m edges; edges are inserted in batches of size B (no deletions). Each edge has a weight w. After each batch, output the total weight of the current minimum spanning forest (MSF). Propose a practical incremental MSF data structure with: (i) fast preprocessing, (ii) amortized per-batch updates, (iii) O(1) MSF weight query, (iv) near-linear space, (v) two optimizations, (vi) a comparison to rebuilding MSF from scratch after every batch. Provide asymptotics in n, m, B?","answer":"Propose an incremental MSF using Kruskal with a DSU for components. Keep the current MSF and, on a batch of B edges, sort them by weight and iteratively add edges that connect different components, up","explanation":"## Why This Is Asked\nTests understanding of dynamic graphs, offline/online tradeoffs, and amortized analysis in a realistic setting where edge insertions occur in batches.\n\n## Key Concepts\n- Incremental MST updates with DSU\n- Kruskal's algorithm adapted to batches\n- Amortized bounds and practical optimizations\n- Space efficiency over re-running from scratch\n\n## Code Example\n```python\nclass DSU:\n    def __init__(self,n): self.p=list(range(n)); self.r=[0]*n\n    def find(self(x):\n        while self.p[x]!=x:\n            self.p[x]=self.p[self.p[x]]; x=self.p[x]\n        return x\n    def union(self,a,b):\n        x,y=self.find(a),self.find(b)\n        if x==y: return False\n        if self.r[x]<self.r[y]: x,y=y,x\n        self.p[y]=x\n        if self.r[x]==self.r[y]: self.r[x]+=1\n        return True\n```\n\n## Follow-up Questions\n- How would you extend to support edge weight updates?\n- What if batches can be adversarially large; how adjust? ","diagram":null,"difficulty":"advanced","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","Netflix","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T10:03:18.904Z","createdAt":"2026-01-23T10:03:18.904Z"},{"id":"q-6520","question":"Maintain a permutation a[1..n]. You receive online batches of B point updates (i -> v) while preserving the permutation. After each batch, output the total number of inversions in a. Propose a practical data structure and amortized analysis for updates and queries, including preprocessing time, memory, and a comparison to recomputing inversions from scratch after every batch?","answer":"Partition indices into blocks of size s≈√n. Each block stores its elements, a sorted copy, and intra-block inversions. Inversions = sum intra-block plus cross-block counts via binary searches between ","explanation":"## Why This Is Asked\nTests dynamic inversion maintenance under batch updates with realistic constraints; requires a concrete data-structure design and amortized analysis.\n\n## Key Concepts\n- dynamic data structures\n- sqrt decomposition\n- inversion counting\n- cross-block queries\n- amortized analysis\n\n## Code Example\n```javascript\n// Data layout sketch\nconst n = a.length;\nconst s = Math.floor(Math.sqrt(n));\nlet blocks = [];\n// build blocks with values, sorted copies, and intra inversions\n```\n\n## Follow-up Questions\n- How to relax the permutation requirement? \n- Can this approach handle duplicates or non-uniform block sizes?","diagram":"flowchart TD\n  A[Batch Update] --> B{Block Affected}\n  B --> C[Update Block Data]\n  B --> D[Recompute intra inversions]\n  C --> E[Update cross-block counts]\n  D --> F[Compute Total Inversions]\n  E --> F","difficulty":"intermediate","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Twitter","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T06:48:22.217Z","createdAt":"2026-01-24T06:48:22.218Z"},{"id":"q-6589","question":"Maintain a growing string S that only accepts batch appends (no deletions). After each batch of B characters, output the total number of distinct substrings of S. Propose a practical structure (suffix automaton) and an amortized analysis: per-character cost, per-batch reporting, space usage, two optimizations, and a comparison to rebuilding from scratch. Provide asymptotics in terms of total length N and batch size B?","answer":"Build and maintain a suffix automaton (SAM). Each extend adds new substrings by delta = len(cur) - len(link(cur)). A batch of B appends costs O(B) amortized; total O(N) over all appends. Space is O(N)","explanation":"## Why This Is Asked\nMaintaining the number of distinct substrings under incremental appends tests knowledge of suffix automata, amortized analysis, and practical update strategies. It highlights how to avoid recomputation and reason about state growth.\n\n## Key Concepts\n- Suffix Automaton (SAM) for distinct substrings\n- Online extensions and delta = len(cur) - len(link(cur))\n- Amortized analysis over batches\n- Space linear in string length\n\n## Code Example\n```javascript\nclass SAMNode {\n  constructor() {\n    this.next = {};\n    this.link = -1;\n    this.len = 0;\n  }\n}\nclass SuffixAutomaton {\n  constructor() {\n    this.st = [new SAMNode()];\n    this.last = 0;\n    this.size = 1;\n  }\n  extend(c){\n    const st = this.st; const cur = this.size++;\n    st.push(new SAMNode()); st[cur].len = st[this.last].len + 1;\n    let p = this.last;\n    while (p >= 0 && !(c in st[p].next)) { st[p].next[c] = cur; p = st[p].link; }\n    if (p === -1) { st[cur].link = 0; }\n    else {\n      const q = st[p].next[c];\n      if (st[p].len + 1 === st[q].len) {\n        st[cur].link = q;\n      } else {\n        const clone = this.size++;\n        st.push(Object.assign(new SAMNode(), st[q]));\n        st[clone].len = st[p].len + 1;\n        while (p >= 0 && st[p].next[c] === q) { st[p].next[c] = clone; p = st[p].link; }\n        st[q].link = st[cur].link = clone;\n      }\n    }\n    this.last = cur;\n  }\n}\n```\n\n## Follow-up Questions\n- How would you handle a large alphabet or non-text streams efficiently?\n- How does the approach adapt if deletions are allowed?\n- What are practical bottlenecks in real-world languages?","diagram":null,"difficulty":"advanced","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T09:30:04.095Z","createdAt":"2026-01-24T09:30:04.097Z"},{"id":"q-6614","question":"You're given a DAG on n nodes. Edges are added only (no deletions) in online batches of size B. After each batch, output the total number of ordered pairs (u,v) such that v is reachable from u, counting (u,u). Propose a practical incremental transitive-closure scheme with: (i) preprocessing, (ii) amortized per-batch update time, (iii) O(1) query for the total reachable pairs, (iv) memory bounds, (v) two optimizations, (vi) a comparison to rebuilding reachability from scratch after every batch. Provide asymptotics in n and B?","answer":"Maintain reach[u] as a bitset of nodes reachable from u (including itself). Initialize by topological order; compute reach via DP. On adding edge (a,b): if reach[a][b] is already 1, skip. Otherwise fo","explanation":"## Why This Is Asked\nTests incremental reachability and transitive-closure techniques under batched edge insertions in DAGs, a common analytics scenario.\n\n## Key Concepts\n- Incremental transitive closure\n- Bitset optimization (O(n^2 / w))\n- Amortized analysis across batches\n\n## Code Example\n```javascript\n// Pseudocode for batch edge insertion and propagation\n```\n\n## Follow-up Questions\n- How would you adapt this for nondeterministic edge additions where deletions occur?\n- What are the practical memory/latency trade-offs for m ~ n^2 edges?","diagram":null,"difficulty":"intermediate","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Two Sigma","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T10:29:34.836Z","createdAt":"2026-01-24T10:29:34.838Z"},{"id":"q-6651","question":"Maintain a directed graph with n nodes; edges are added in online batches of size B (no deletions). After each batch, output the number of nodes reachable from a fixed source S. Propose a practical incremental scheme: (i) a fast local propagation pass, (ii) a periodic full recomputation every T batches, (iii) a lazy rebuild trigger when incremental cost exceeds a threshold. Include preprocessing, amortized update time per batch, memory bounds, two optimizations, and a comparison to re-running DFS from S after every batch?","answer":"Incrementally maintain a reachable[S] bitset. On each batch, for every new edge (u,v): if reachable[u] and not reachable[v], mark v reachable and BFS/DFS through its out-neighbors, updating count. Amo","explanation":"Why this is asked: tests understanding of incremental graph algorithms and amortized analysis in a streaming-like setting. Key ideas: propagate reachability only when needed; track frontier; cost bounds. Implementation notes: use adjacency lists, visited flags, and a queue. Optimizations trade off per-batch latency against occasional full rebuild. Edge cases: isolated nodes; multiple paths; cycles.","diagram":null,"difficulty":"intermediate","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T11:41:59.162Z","createdAt":"2026-01-24T11:41:59.162Z"},{"id":"q-6675","question":"You are given an undirected simple graph G=(V,E) with n=|V| and m=|E|. Edges arrive online in batches of size B (B ≤ m) with no deletions. After each batch, output the current number of triangles in G. Propose a practical incremental triangle-counting structure that (i) preprocesses efficiently, (ii) updates amortized per batch, (iii) returns the triangle count in O(1) after each batch, (iv) uses near-linear space, (v) includes two optimizations, and (vi) shows how it compares to recomputing triangles from scratch after every batch. Provide asymptotics in terms of n,m,B?","answer":"Maintain an oriented graph via degree-based orientation. For each vertex u, store forward neighbors N^+(u). When inserting edge (u,v), orient from lower degree (or tie-break by id) to higher; new tria","explanation":"## Why This Is Asked\nTests dynamic triangle counting with tight amortized analysis, batching, and memory trade-offs.\n\n## Key Concepts\n- Dynamic graphs, incremental triangles\n- Degree-based edge orientation, forward neighbor sets\n- Amortized analysis, batch processing, space efficiency\n\n## Code Example\n```javascript\n// high-level sketch\nconst Nplus = Array.from({length: n}, ()=> new Set());\nconst deg = new Int32Array(n);\nlet triangles = 0;\nfunction addEdge(u,v){\n  // decide orientation\n  if (deg[u] > deg[v] || (deg[u]===deg[v] && u>v)) [u,v] = [v,u];\n  // count new triangles\n  let t = 0;\n  for (let w of Nplus[u]) if (Nplus[v].has(w)) t++;\n  triangles += t;\n  Nplus[u].add(v);\n  deg[u]++; deg[v]++;\n  // potentially rebalance orientation lazily\n}\n```\n\n## Follow-up Questions\n- How does the method adapt if deletions are allowed?\n- What are practical bottlenecks on real graphs (power-law networks)?","diagram":"flowchart TD\n  A[Input n,m,B] --> B[Orient edges by degree]\n  B --> C{For edge (u,v)}\n  C --> D[Compute t = |N^+(u) ∩ N^+(v)|]\n  D --> E[triangles += t]\n  E --> F[Add edge to N^+(u)]\n  F --> G[Query: triangles in O(1)]","difficulty":"advanced","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","IBM","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T13:14:21.671Z","createdAt":"2026-01-24T13:14:21.671Z"},{"id":"q-6691","question":"You have a dynamic set P of n 2D points added in batches of B. After each batch, compute the size of the Pareto frontier (points not dominated by any other: a dominates b if x_a ≥ x_b and y_a ≥ y_b with at least one strict). Propose a practical data structure to maintain the frontier with amortized update time per batch, O(1) frontier-size queries, near-linear space, and two optimizations. Compare to rebuilding after every batch. Provide asymptotics in n and B?","answer":"An efficient approach uses a frontier stored as an ordered map by x, with y strictly decreasing as x grows. For each batch of B points, sort by x, prune intra-batch dominations, then merge with the fr","explanation":"## Why This Is Asked\nTests ability to design incremental geometry data structures, reason about dominance, and assess batch-update trade-offs.\n\n## Key Concepts\n- Pareto frontier maintenance\n- Batch incremental updates\n- Monotone frontier properties (x increasing, y decreasing)\n- Trade-offs: update time vs rebuild cost\n\n## Code Example\n```javascript\n// Pseudocode sketch: merge batch into frontier\nfunction insertBatch(frontier, batch){\n  batch.sort((a,b)=> a.x-b.x);\n  // prune intra-batch dominated\n  let filtered=[]; let maxY=-Infinity;\n  for(let p of batch){ if(p.y>maxY){ filtered.push(p); maxY=p.y; } }\n  // merge with frontier (frontier: sorted by x, y strictly decreasing)\n  let i=0,j=0; let newFrontier=[];\n  while(i<frontier.length || j<filtered.length){\n    let a = frontier[i], b = filtered[j];\n    if(!a) { newFrontier.push(...filtered.slice(j)); break; }\n    if(!b) { newFrontier.push(...frontier.slice(i)); break; }\n    if(a.x < b.x){ newFrontier.push(a); i++; }\n    else if(b.x < a.x){ newFrontier.push(b); j++; }\n    else { // equal x, keep larger y\n      if(a.y > b.y){ newFrontier.push(a); i++; j++; }\n      else { newFrontier.push(b); i++; j++; }\n    }\n  }\n  // optional prune to maintain y-decreasing property\n  return pruneFrontier(newFrontier);\n}\n```","diagram":null,"difficulty":"advanced","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Snowflake","Square","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T13:41:22.264Z","createdAt":"2026-01-24T13:41:22.265Z"},{"id":"q-678","question":"In a directed acyclic graph with N nodes and M edges, all edge costs are nonnegative. Compute the minimum-cost path from S to T. Costs can decrease online; design a strategy to maintain shortest paths with updates, aiming for sublinear re-computation on average. Provide initial complexity and amortized update complexity, plus memory usage and practical optimizations?","answer":"Initial: run a topological DP in O(N+M) to obtain dist to all nodes. Online decreases trigger relaxations: if w(u,v) decreases and dist[u]+w(u,v) < dist[v], update dist[v] and propagate to successors ","explanation":"## Why This Is Asked\n\nTests online update handling, DAG shortest paths, and amortized analysis.\n\n## Key Concepts\n\n- DAG shortest paths via topological order\n- Dynamic relaxations under decreasing edge weights\n- Amortized analysis and worst-case bounds\n\n## Code Example\n\n```javascript\nfunction updateShortestPaths(n, adj, dist, s, t, u, v, wOld, wNew){\n  // assume we know wOld and wNew with a decrease; perform relaxation\n  if(dist[u] + wNew < dist[v]){\n    dist[v] = dist[u] + wNew;\n    // push v to a queue and relax its outgoing edges\n  }\n  // ... full propagation would run until convergence\n}\n```\n\n## Follow-up Questions\n\n- How would you adapt for non-DAG graphs?\n- How does update order affect performance in practice?","diagram":"flowchart TD\n  A[Initial topological DP] --> B[Edge decrease detected]\n  B --> C[Relax dist[v]]\n  C --> D[Propagate changes]\n  D --> E[Converged]","difficulty":"advanced","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","Meta","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T15:56:43.115Z","createdAt":"2026-01-11T15:56:43.115Z"},{"id":"q-6799","question":"Real-time status code monitoring: codes in 0..M-1 arrive in batches of size B. After each batch, determine if any code has appeared at least t times so far. Propose a beginner-friendly approximate method using a fixed-size hash bucket array of size D. Map code c to bucket b = hash(c) mod D and increment buckets[b] for every c in the batch. If any buckets[b] ≥ t, answer YES; otherwise NO. Compare to exact counting in terms of memory and time?","answer":"Two-tier approach: maintain D buckets. For each code in the batch, bucket = hash(code) mod D; buckets[bucket]++. Track maxBucket. After batch, YES if maxBucket ≥ t, else NO. Complexity: update O(B), q","explanation":"## Why This Is Asked\nTests understanding of space-time trade-offs in streaming analysis. Introduces hashing-based approximation and how it affects accuracy, latency, and memory.\n\n## Key Concepts\n- Hash bucketing for compact counts\n- Amortized batch updates\n- Probability of collision vs. accuracy\n- Space-time trade-offs vs. exact counting\n\n## Code Example\n```javascript\nclass BucketCounter {\n  constructor(D) {\n    this.D = D;\n    this.buckets = new Array(D).fill(0);\n    this.max = 0;\n  }\n  updateBatch(batch, hashFn) {\n    for (const c of batch) {\n      const b = hashFn(c) % this.D;\n      this.buckets[b] += 1;\n      if (this.buckets[b] > this.max) this.max = this.buckets[b];\n    }\n  }\n  query(t) { return this.max >= t; }\n}\n```\n\n## Follow-up Questions\n- How would you bound false positives and choose D for a target ε? \n- What changes if batches vary in size or if t scales over time?","diagram":null,"difficulty":"beginner","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Citadel","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T17:52:55.459Z","createdAt":"2026-01-24T17:52:55.459Z"},{"id":"q-6821","question":"You have a DAG G=(V,E) with n=|V|, m=|E|. Edges arrive online in batches of size B (no deletions). After each batch you must determine whether an insertion creates a cycle, and if acyclic, maintain a valid topological order. Propose a practical incremental topological-order maintenance structure that (i) preprocesses efficiently, (ii) updates in amortized O(B log n) time, (iii) detects cycles in O(B) per batch, (iv) uses near-linear space, (v) includes two optimizations, and (vi) compares to recomputing from scratch after every batch. Provide asymptotics in terms of n, m, B?","answer":"Approach: incremental topological ordering. Maintain a label pos(v) for each node. For edge u→v, if pos(u) < pos(v) nothing; otherwise trigger reordering via a bounded DFS from v and backward from u, ","explanation":"## Why This Is Asked\nReal systems add edges over time; maintaining a topological order under batched insertions tests dynamic graph mastery, practical tuning, and worst‑case guarantees.\n\n## Key Concepts\n- Incremental topological ordering; cycle detection; batch amortized analysis; data‑structure tradeoffs.\n\n## Code Example\n```javascript\n// Incremental topological update skeleton\nfunction updateBatch(graph, batch){\n  // graph maintains pos[v]\n  // check order and adjust frontier accordingly\n}\n```\n\n## Follow-up Questions\n- How would you handle edge deletions? \n- Can you adapt to parallel batch processing while preserving correctness?","diagram":"flowchart TD\n  A[Batch Edge] --> B{Cycle?}\n  B -- Yes --> C[Report cycle]\n  B -- No --> D[Update order]\n  D --> E[Next batch]","difficulty":"advanced","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T18:54:14.589Z","createdAt":"2026-01-24T18:54:14.589Z"},{"id":"q-690","question":"Design a data structure to support two online operations on an integer array A of length N: 1) rangeAdd(l, r, delta) adds delta to A[i] for l <= i <= r, 2) queryMaxSubarray() returns the maximum subarray sum of the current A. Provide a structure that supports both operations in O(log N) time, describe what to store per node, how to merge children, and how to apply a lazy add. Include correctness and complexity considerations?","answer":"Use a segment tree with lazy propagation. Each node stores: total sum, max prefix, max suffix, and max subarray. To add v to a range, lazily update the node: adjust sum, pref, suff, and propagate lazi","explanation":"## Why This Is Asked\nTests ability to combine range updates with non-linear objective (max subarray) and shows knowledge of segment trees, lazy propagation, and invariants under updates.\n\n## Key Concepts\n- Segment tree with lazy propagation\n- Node values: sum, max prefix, max suffix, max subarray\n- Merge operation correctness\n- Complexity analysis: O(log N) per op, O(N) memory\n- Edge cases: all negative arrays, large deltas, non-overlapping ranges\n\n## Code Example\n```javascript\nclass Node { constructor(sum=0,pref=-Infinity,suff=-Infinity,best=-Infinity){ this.sum=sum; this.pref=pref; this.suff=suff; this.best=best; } }\n```\n\n## Follow-up Questions\n- How would you adapt if rangeAdd is replaced with rangeMultiply?\n- How would you extend to support range assign and query across multiple segments?","diagram":null,"difficulty":"intermediate","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T16:23:52.242Z","createdAt":"2026-01-11T16:23:52.242Z"},{"id":"q-700","question":"You're building a real-time analytics dashboard that shows the top-k most frequent event types from a high-volume log stream (e.g., clicks, errors). Each event has a string type. Design a data structure and algorithm to maintain the current top-k frequencies with online increments, aiming for roughly O(log k) update time and O(n) memory. Explain how you handle ties and memory growth, and compare with a naive approach that re-sorts after every insert?","answer":"Use a hash map to count frequencies and a min-heap of size k to track current top-k. On event type t: increment freq[t]; if t is in heap, update its priority; else if heap size < k, insert (freq[t], t","explanation":"## Why This Is Asked\nThis probes real-time top-k maintenance and amortized analysis using simple DS.\n\n## Key Concepts\n- HashMap for counts\n- Min-heap for top-k\n- Tie-breaking rules\n- Memory-use trade-offs\n\n## Code Example\n```javascript\nfunction update(freq, heap, k, t) {\n  freq[t] = (freq[t] || 0) + 1;\n  if (heap.has(t)) adjust(heap, t);\n  else if (heap.size < k) heap.push([freq[t], t]);\n  else if (freq[t] > heap.min()[0]) heap.replace([freq[t], t]);\n}\n```\n\n## Follow-up Questions\n- How would you handle deletions or aging counts?\n- How would you test correctness with sequences simulating bursts?","diagram":null,"difficulty":"beginner","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T17:19:43.376Z","createdAt":"2026-01-11T17:19:43.376Z"},{"id":"q-7013","question":"Real-time analytics: a stream of integers a in [0, M-1] arrives in batches of size B. After each batch, determine the current top-K most frequent values seen so far, using a sketch-based method that uses O(K) counters. Propose an algorithm (e.g., Misra-Gries), derive update time per batch, query time, and memory; discuss how to choose K relative to M and accuracy, and compare to exact counting?","answer":"Misra–Gries heavy hitters with K counters. For each x in batch: if x in the map, inc its counter; else if fewer than K entries, insert x with 1; else dec all K counters by 1 and drop zeros. After batc","explanation":"## Why This Is Asked\nAnalyzes streaming-frequency estimation with bounded memory and batch processing.\n\n## Key Concepts\n- Misra-Gries heavy hitters; counter trimming; candidate set.\n- Trade-offs: accuracy vs memory; exact counting as baseline.\n- Batch processing implications on update costs.\n\n## Code Example\n```javascript\n// Misra-Gries skeleton\nclass MG {\n  constructor(K){ this.K=K; this.map=new Map(); }\n  add(x){ if(this.map.has(x)) this.map.set(x,this.map.get(x)+1); else if(this.map.size<this.K) this.map.set(x,1); else { for(let [k,c] of this.map){ this.map.set(k,c-1); } /* remove zeros */ for(let [k,c] of Array.from(this.map)) if(this.map.get(k)<=0) this.map.delete(k); } }\n  }\n}\n```","diagram":null,"difficulty":"beginner","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Square","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T05:45:46.207Z","createdAt":"2026-01-25T05:45:46.208Z"},{"id":"q-704","question":"Scenario: a directed graph with nonnegative weights and a fixed source S. Each batch updates up to B edges (increases or decreases). Propose a practical dynamic data structure to maintain exact distances from S to all nodes and answer distance queries S→T in polylog time, with sublinear amortized update time. Compare to rerunning Dijkstra after every batch; include expected bounds, memory usage, and practical heuristics?","answer":"Maintain an exact SSSP with a dynamic SPT plus a limited local reoptimization. For a batch of up to B edge-weight changes, re-evaluate only the affected subtree via a replacement-path frontier and rew","explanation":"## Why This Is Asked\nAssesses skill in dynamic graph algorithms, not just static shortest paths; requires designing practical online maintenance with bounds.\n\n## Key Concepts\n- Dynamic shortest paths\n- Replacement paths and locality\n- Batch updates and amortized analysis\n- Potentials for nonnegative weights\n- SPT maintenance\n\n## Code Example\n```javascript\n// Skeleton: dynamic SSSP maintenance interface\nclass DynamicSSSP {\n  constructor(graph, s) { /* ... */ }\n  batchUpdate(changes) { /* changes: Array<{u,v,wDelta}> */ }\n  distanceTo(v) { /* return dist from s to v */ }\n}\n```\n\n## Follow-up Questions\n- How to adapt when B ≈ M or updates are adversarial?\n- How would you extend to multiple sources and dynamic graph rebuilds?","diagram":null,"difficulty":"advanced","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Salesforce","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T18:27:46.453Z","createdAt":"2026-01-11T18:27:46.453Z"},{"id":"q-709","question":"Given a directed graph with nonnegative weights, a fixed source S, and a stream of online edge weight updates (both increases and decreases), design a dynamic SSSP data structure that maintains exact distances dist(S, v) for all v after each update. Aim for sublinear amortized update time per edge change; specify initial preprocessing, worst-case vs amortized bounds, memory usage, and practical optimizations. Provide a plan for applying this in a traffic-graph scenario with frequent but localized updates?","answer":"Maintain dist from S to all nodes with a dynamic SSSP structure: run full Dijkstra once to initialize (O(M log N)). For each edge weight update (u, v, δ), re-relax only nodes affected by the change us","explanation":"## Why This Is Asked\nDynamic maintenance of shortest paths under online updates is critical in production traffic graphs. Candidates must reason about amortized analysis, locality, and practical heuristics beyond static re-computation.\n\n## Key Concepts\n- Dynamic shortest paths\n- Amortized analysis\n- Localized relaxation\n- Priority queues and delta updates\n- Trade-offs: exactness vs approximate\n\n## Code Example\n```javascript\n// Pseudo approach sketch: re-relaxation function\nfunction updateEdge(u,v,delta){\n  // update weight\n  // if newDist improves dist[v], push to PQ and propagate\n}\n```\n\n## Follow-up Questions\n- How would you handle multiple concurrent updates efficiently?\n- What metrics would you monitor in production?","diagram":null,"difficulty":"intermediate","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["OpenAI","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T19:15:05.608Z","createdAt":"2026-01-11T19:15:05.608Z"},{"id":"q-7161","question":"You're building a social-network analytics service. The directed graph G=(V,E) has n nodes and m edges. Edges arrive online in batches of size B (no deletions). After each batch, design a practical incremental approximate betweenness centrality system using a fixed landmark set of size s to bound error ε with probability 1-δ. The system should preprocess, update amortized per batch, answer top-k queries quickly, use near-linear space, include two optimizations, and compare to recomputing Brandes after each batch. Provide asymptotics in n,m,B,s,ε,δ?","answer":"Use s landmark shortest-path samples to approximate BC. Precompute distances from each landmark to all nodes (O(s(n log n + m))). After a batch of B edges, relax only affected regions per landmark (am","explanation":"## Why This Is Asked\nTests ability to design scalable approximate graph metrics under online updates; bridges sampling theory with dynamic algorithms.\n\n## Key Concepts\n- Landmark-based approximations\n- Incremental shortest-path relaxations\n- Trade-offs: accuracy ε vs. update cost δ\n\n## Code Example\n```javascript\n// Pseudocode: initialize landmarks, compute dist from each landmark\nfor each l in landmarks:\n  dist[l] = Dijkstra(l, G)\n// On batch update\nfor edge in batch:\n  updateGraph(edge)\n  for l in landmarks:\n    relaxPathsFrom(l, affectedSubgraph)\n// Top-k query uses aggregated scores\n```\n\n## Follow-up Questions\n- How to adapt s and δ in response to observed error? \n- How do deletions or edge weight changes affect guarantees and cost?","diagram":"flowchart TD\n  A[Batch arrives] --> B[Update landmarks]\n  B --> C[Recompute/adjust BC estimates]\n  C --> D[Execute top-k query]\n  D --> E[Return results]","difficulty":"advanced","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Robinhood","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T11:39:42.480Z","createdAt":"2026-01-25T11:39:42.480Z"},{"id":"q-721","question":"Given a fixed directed graph with nonnegative weights and a single source S, handle a batch of edge-weight decreases (no insertions/deletions). Design a dynamic algorithm to update the exact S→v distances after each batch with sublinear amortized per-edge cost. Specify data structures, provide an amortized bound, and discuss memory and practical optimizations for real-time traffic networks?","answer":"Use dist[] and a min-heap. For each decreased edge (u,v) to w', if dist[u]+w' < dist[v], set dist[v] = dist[u]+w' and push v; repeatedly pop and relax neighbors until the queue empties. This confines ","explanation":"## Why This Is Asked\nDynamic SSSP with weight-decreases mirrors real-time route/latency updates. This tests practical algorithm design, data-structure choices, and amortized reasoning for production graphs.\n\n## Key Concepts\n- Dynamic shortest paths under monotone updates\n- Dijkstra-like propagation with lazy updates\n- Priority queues vs bucketed approaches for integer weights\n- Amortized analysis based on updated edges and affected nodes\n- Memory footprint: O(N+M)\n\n## Code Example\n```javascript\n// Pseudocode for batch update\nfunction updateBatch(changes) {\n  for (const {u,v,w} of changes) {\n    if (dist[u] + w < dist[v]) {\n      dist[v] = dist[u] + w;\n      pq.push(v);\n    }\n  }\n  while (!pq.isEmpty()) {\n    const x = pq.pop();\n    for (const e of adj[x]) {\n      if (dist[x] + e.w < dist[e.v]) {\n        dist[e.v] = dist[x] + e.w; pq.push(e.v);\n      }\n    }\n  }\n}\n```\n\n## Follow-up Questions\n- How would you adapt for batches that include edge insertions/deletions?\n- Compare binary-heap vs Dial's bucket approach for integer weights in [0, W].","diagram":"flowchart TD\n  S[Source]\n  D[Decrease batch]\n  P[Relax candidates]\n  Q[Queue updates]\n  E[Edge relaxations]\n  S --> D\n  D --> P\n  P --> Q\n  Q --> E\n  E --> P","difficulty":"intermediate","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","NVIDIA","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T20:23:01.260Z","createdAt":"2026-01-11T20:23:01.260Z"},{"id":"q-7220","question":"You're building a real-time analytics service that tracks two independent user-activity streams, A and B, arriving in batches of size B. After each batch, estimate the Jaccard similarity J = |A ∩ B| / |A ∪ B| for all time seen so far. Propose a practical solution using MinHash or related sketches, specify update time per batch, query time, memory, and compare to exact intersection?","answer":"Use a MinHash sketch with d independent hashes per stream. For each element x in the batch, compute d hash values and update the signature minima for A and B. After each batch, estimate J as the fract","explanation":"## Why This Is Asked\nTests ability to reason about streaming similarity with limited memory, a practical beginner-level challenge that still touches core complexity trade-offs.\n\n## Key Concepts\n- MinHash sketches for Jaccard similarity\n- Batch processing costs vs per-element costs\n- Trade-offs: d (accuracy) vs memory, impact of collisions\n- Exact vs approximate counting implications\n\n## Code Example\n```javascript\n// Pseudo: initialize sketches for A and B\nclass MinHash {\n  constructor(d, maxHash) { this.d = d; this.sig = Array(d).fill(Infinity); this.maxHash = maxHash; }\n  add(x) { for (let i=0; i<this.d; i++) { const h = hash_i(x, i) % this.maxHash; if (h < this.sig[i]) this.sig[i] = h; } }\n  resembles(other) { let matches = 0; for (let i=0; i<this.d; i++) if (this.sig[i] === other.sig[i]) matches++; return matches / this.d; }\n}\n```\n\n## Follow-up Questions\n- How would you choose d given target accuracy ε?\n- How handle skewed data or duplicates in batches?\n- How would you extend to weighted or multi-set similarity?","diagram":"flowchart TD\n  A[Batch arrives] --> B[Compute MinHash for A and B]\n  B --> C[Compare signatures -> Jaccard estimate]\n  C --> D[Report update]\n  D --> E[Repeat on next batch]","difficulty":"beginner","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Lyft","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T13:58:36.074Z","createdAt":"2026-01-25T13:58:36.074Z"},{"id":"q-7256","question":"Design an incremental data structure to track the number of distinct substrings of a string S as characters are appended in online batches of size B. S starts empty; at each batch, you append B characters (given in order). After each batch, output the count of distinct substrings of the current S. Propose a practical approach (e.g., suffix automaton with batch-aware updates), specify preprocessing, per-batch update time, total space, two optimizations, and compare to rebuilding from scratch after each batch. Provide asymptotics in terms of N=|S|, σ, and B?","answer":"Maintain a suffix automaton (SAM) of S. Each new char adds at most two states; increase in distinct substrings equals maxlen[new] - maxlen[link[new]]. For a batch of B appends, process in order; amort","explanation":"## Why This Is Asked\nTests incremental data-structure design for a streaming string problem and probes SAM knowledge, batch amortization, and trade-offs vs rebuild.\n\n## Key Concepts\n- Suffix automaton incremental updates\n- Distinct substrings maintenance\n- Batch amortization and space efficiency\n\n## Code Example\n\n```javascript\n// pseudo\nclass SAM { /*...*/ }\n```\n\n## Follow-up Questions\n- How does the bound change with alphabet size?\n- How would you handle deletions or edits?\n","diagram":"flowchart TD\n  S[Current string S] --> SAM[SAM state machine]\n  SAM --> Dist[Distinct substrings count]\n  Batch[Batch of B] --> Update[Insert B chars into SAM]\n  Update --> Dist","difficulty":"advanced","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T15:36:52.518Z","createdAt":"2026-01-25T15:36:52.518Z"},{"id":"q-7318","question":"An undirected graph with n nodes starts with no edges. Edges arrive online in batches of size B. After each batch, determine the number of connected components. Propose a DSU (Union-Find) solution with path compression and union by rank; specify update time per batch, per-edge cost, total memory, and compare to rebuilding components from scratch after every batch. Express asymptotics in n, m, B, and α(n)?","answer":"Use a Disjoint Set Union (union-find) with path compression and union by rank. Initialize components = n. For each edge (u,v) in the batch, if find(u) != find(v) { union(u,v); components-- }. Amortize","explanation":"## Why This Is Asked\nTests ability to model dynamic connectivity with incremental updates and reason about amortized costs using a classic data structure.\n\n## Key Concepts\n- Disjoint Set Union (Union-Find)\n- Path compression and union by rank\n- Amortized α(n) time per operation\n- Batch processing vs full rebuild costs\n\n## Code Example\n```javascript\n// initialize\nfunction makeSet(n){/*...*/}\nlet components = n;\nfor (const [u,v] of batch){\n  if (find(u) !== find(v)) { union(u,v); components--; }\n}\n```\n\n## Follow-up Questions\n- How to handle edge deletions? \n- What changes if batches vary in size or arrive continuously?","diagram":null,"difficulty":"beginner","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Goldman Sachs","LinkedIn"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T18:40:19.462Z","createdAt":"2026-01-25T18:40:19.462Z"},{"id":"q-732","question":"Scenario: A data stream yields integers. At each time step, a new value enters a sliding window of fixed size W, and the oldest value leaves. Design a solution to maintain the top-2 most frequent values in the current window with fast updates. Compare a naive O(W) recompute to an augmented structure using a frequency map and a max-heap with lazy deletions. Provide update and query complexities and memory usage?","answer":"Naive: on each slide recompute the top-2 by scanning the W elements, O(W) per step. Improved: maintain a frequency map plus a max-heap with lazy deletions. Updates are O(log D) where D is distinct val","explanation":"## Why This Is Asked\n\nTests familiarity with sliding-window problems and practical complexity trade-offs between recomputation and incremental data structures.\n\n## Key Concepts\n\n- Sliding window\n- Frequency counting\n- Hash map and max-heap\n- Lazy deletion\n\n## Code Example\n\n```javascript\nclass TopKWindow {\n  constructor(W){ this.W=W; this.freq=new Map(); this.window=[]; this.maxHeap=[]; }\n  // ... implement add, slide, and getTopK using lazy deletions\n}\n```\n\n## Follow-up Questions\n\n- How would the approach scale for larger k than 2?\n- How would you adapt for non-integer or large-value domains?","diagram":null,"difficulty":"beginner","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Twitter","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T21:19:58.623Z","createdAt":"2026-01-11T21:19:58.623Z"},{"id":"q-7349","question":"Maintain an array a[1..n] with online batches of B range-assign updates (l, r, c) setting a[i] = c for i in [l, r]. After each batch, output the number of color segments in a (maximal consecutive equal values). Propose a practical interval-merge data structure (e.g., an ordered map of disjoint segments) and an amortized analysis of per-batch work, including preprocessing, memory, and a comparison to recomputing from scratch after every batch?","answer":"Use a run-length encoded structure: store disjoint segments [l, r, val] ordered by l. For a batch (l, r, c): split segments at l and r+1, remove covered parts, insert [l, r, c], then merge with adjace","explanation":"## Why This Is Asked\nTests designing online interval updates with a global property (segment count) under range-assigns; requires boundary splitting, merging logic, and amortized reasoning.\n\n## Key Concepts\n- Run-length encoding, ordered interval map, boundary splits and merges\n- Amortized analysis based on number of touched segments per batch\n- Trade-offs vs full recomputation and alternative data structures (e.g., segment trees)\n\n## Code Example\n```javascript\nclass Segments {\n  constructor(n, init) {\n    this.segs = [{l:1, r:n, v:init}];\n  }\n  apply(l, r, c) {\n    // split at l and r+1, erase overlaps, insert [l,r,c], then merge with neighbors\n  }\n  count() { return this.segs.length; }\n}\n```\n\n## Follow-up Questions\n- How would you handle updates with random colors and many small batches? \n- How does performance change if many updates touch the same underlying values?\n","diagram":null,"difficulty":"intermediate","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T19:40:08.171Z","createdAt":"2026-01-25T19:40:08.171Z"},{"id":"q-740","question":"Scenario: An edge CDN collects response times in milliseconds for every request. Design a beginner-friendly online algorithm to maintain the median latency as new times arrive, using only inserts. Explain the data structure, update steps, and time/space bounds, assuming up to 1e6 entries?","answer":"Use two heaps: a max-heap for the lower half and a min-heap for the upper half. On each new latency x, push to the appropriate heap and rebalance so sizes differ by at most 1. Median is the top of the","explanation":"## Why This Is Asked\n\nTests ability to design a stable online statistic with clear time/space guarantees using simple data structures.\n\n## Key Concepts\n\n- streaming algorithms\n- two-heap median maintenance\n- amortized vs worst-case costs\n- space efficiency\n\n## Code Example\n\n```javascript\n// production-ready sketch of two-heap median tracker\nclass BinaryHeap {\n  constructor(compare){ this.data=[]; this.compare=compare; }\n  size(){ return this.data.length; }\n  top(){ return this.data[0]; }\n  push(x){ this.data.push(x); this._siftUp(this.data.length-1); }\n  pop(){ const top=this.data[0]; const end=this.data.pop(); if (this.data.length){ this.data[0]=end; this._siftDown(0); } return top; }\n  _siftUp(i){ const x=this.data[i]; while(i>0){ const p=(i-1)>>1; if (this.compare(x, this.data[p])>=0) break; this.data[i]=this.data[p]; i=p; } this.data[i]=x; }\n  _siftDown(i){ const n=this.data.length; const x=this.data[i]; while(true){ let l=2*i+1, r=l+1, smallest=i; if (l<n && this.compare(this.data[l], this.data[smallest])<0) smallest=l; if (r<n && this.compare(this.data[r], this.data[smallest])<0) smallest=r; if (smallest===i) break; this.data[i]=this.data[smallest]; i=smallest; } this.data[i]=x; }\n}\n\nclass MedianTracker {\n  constructor(){ this.lower=new BinaryHeap((a,b)=> b-a); this.upper=new BinaryHeap((a,b)=> a-b); }\n  insert(x){ if (this.lower.size()===0 || x <= this.lower.top()) this.lower.push(x); else this.upper.push(x);\n    if (this.lower.size() > this.upper.size()+1) this.upper.push(this.lower.pop());\n    else if (this.upper.size() > this.lower.size()) this.lower.push(this.upper.pop());\n  }\n  median(){ if (this.lower.size() === this.upper.size()) return (this.lower.top() + this.upper.top())/2; return this.lower.top(); }\n}\n```\n\n## Follow-up Questions\n\n- How would you adapt for deletions or outliers removal?\n- How would you validate correctness with unit tests and random streams?","diagram":"flowchart TD\n  S[Stream of latencies] --> I[Insert into appropriate heap]\n  I --> R[Rebalance if needed]\n  R --> M[Median available via tops]\n  M --> Q[Query median in O(1)]","difficulty":"beginner","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Coinbase","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T22:21:49.038Z","createdAt":"2026-01-11T22:21:49.038Z"},{"id":"q-742","question":"In a DAG with N nodes and M edges, nonnegative edge weights. You maintain shortest-path distances from source S to a fixed set of target nodes {T1,...,Tk}. Edge weights can only decrease over time due to updates. After a batch of updates, you should update only the target distances that can improve, avoiding full re-relaxation. Propose a practical algorithm that lazily propagates decreases using the DAG’s topological order, such that total work across updates is sublinear on average. Provide update and query steps, concrete time bounds, and memory usage, plus optimizations?","answer":"Maintain dist from S to all nodes in topological order. On a batch of weight decreases (u->v with newW <= oldW): if dist[v] > dist[u] + newW, update and push v. Pop nodes in increasing topo index, rel","explanation":"## Why This Is Asked\nTests ability to exploit DAG structure for incremental shortest paths with monotone updates and selective recomputation.\n\n## Key Concepts\n- DAG topological order\n- Monotone updates and lazy propagation\n- Selective relaxation focused on targets\n- Amortized analysis of edge relaxations\n\n## Code Example\n```javascript\n// Pseudocode for batch decreases\nfunction applyDecreases(batch) {\n  const queue = [];\n  for (const e of batch) {\n    if (dist[e.v] > dist[e.u] + e.newW) {\n      dist[e.v] = dist[e.u] + e.newW;\n      queue.push(e.v);\n    }\n  }\n  queue.sort((a,b)=>topo[a]-topo[b]);\n  while (queue.length) {\n    const x = queue.shift();\n    for (const y of adj[x]) {\n      const w = weight(x,y);\n      if (dist[y] > dist[x] + w) {\n        dist[y] = dist[x] + w;\n        queue.push(y);\n      }\n    }\n  }\n}\n``` \n\n## Follow-up Questions\n- How would you handle dynamic target set changes efficiently?\n- What edge cases degrade the amortized bound and how could you mitigate them?","diagram":"flowchart TD\n  A[Batch Update] --> B[Relax Affected Nodes]\n  B --> C[Distance Stable]\n  C --> D[Answer Queries for Tk]\n  A --> E[Push Victims to Queue]\n  E --> B","difficulty":"advanced","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Microsoft","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T23:17:59.814Z","createdAt":"2026-01-11T23:17:59.814Z"},{"id":"q-755","question":"You're maintaining real-time travel times in a citywide road network modeled as a weighted directed graph with nonnegative costs. Costs can only decrease as new data arrives. Design an incremental algorithm to keep the shortest-path distances from a fixed hub S to all nodes up-to-date after each edge-cost decrease, aiming for sublinear amortized update work. Provide initial SSSP complexity, amortized per-decrease update, memory usage, and practical optimization strategies?","answer":"Compute initial SSSP with Dijkstra: O(M log N). For a decreased edge (u,v) with w' < w, if dist[u] + w' < dist[v], update dist[v] and propagate relaxations through the graph using a PQ. Amortized work","explanation":"## Why This Is Asked\n\nTests ability to extend classic shortest-paths to dynamic updates with a concrete, real-world constraint (only decreases) and to analyze practical performance in terms of amortized work and memory.\n\n## Key Concepts\n\n- Dynamic single-source shortest paths with only edge decreases\n- Amortized analysis and potential-based arguments\n- Priority-queue relaxation propagation\n- Memory-time trade-offs and localized updates\n- Practical optimizations: skip non-on-SPT edges, batch updates, integer weights with Dial's algorithm\n\n## Code Example\n\n```javascript\nclass DynamicSSSP {\n  constructor(graph, source) {\n    this.graph = graph; // adjacency: {u: [{v, w}, ...]}\n    this.n = Object.keys(graph).length;\n    this.dist = Array(this.n).fill(Infinity);\n    this.dist[source] = 0;\n    this.pq = new MinHeap(); // custom min-heap with push/pop\n    this.pq.push([0, source]);\n    while (!this.pq.isEmpty()) {\n      const [d, u] = this.pq.pop();\n      if (d !== this.dist[u]) continue;\n      for (const {v, w} of this.graph[u]) {\n        if (this.dist[v] > d + w) {\n          this.dist[v] = d + w;\n          this.pq.push([this.dist[v], v]);\n        }\n      }\n    }\n  }\n  decreaseEdge(u, v, newW) {\n    // update edge weight in graph[u] item\n    // assume newW <= oldW\n    // attempt relaxation\n    if (this.dist[u] + newW < this.dist[v]) {\n      this.dist[v] = this.dist[u] + newW;\n      this.pq.push([this.dist[v], v]);\n      while (!this.pq.isEmpty()) {\n        const [d, x] = this.pq.pop();\n        if (d !== this.dist[x]) continue;\n        for (const {to, w} of this.graph[x] || []) {\n          if (this.dist[to] > d + w) {\n            this.dist[to] = d + w;\n            this.pq.push([this.dist[to], to]);\n          }\n        }\n      }\n    }\n  }\n}\n```\n\n## Follow-up Questions\n\n- How would you adapt if many edges frequently decrease in a short window?\n- How would you extend to multiple sources or all-pairs distances, and what would the complexity trade-offs look like?","diagram":"flowchart TD\n  S[Hub S] --> E[Edge decrease event]\n  E --> R{Relaxation possible?}\n  R -->|Yes| U[Update dist and propagate via Dijkstra]\n  R -->|No| End[Done]","difficulty":"intermediate","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Google","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T01:33:06.200Z","createdAt":"2026-01-12T01:33:06.200Z"},{"id":"q-766","question":"## Prompt\n\nIn a dynamic directed graph G=(V,E) with nonnegative weights, edge latencies only decrease in batches. Design a practical, production-ready algorithm to maintain a (1+ε)-approximate SSSP tree from a source S under these updates, enabling distance queries dist(S,v) in O(log|V|) time. Target sublinear amortized update in |E| for batch updates, and linear space. Explain data structures, update bounds, and how you bound cascade effects?","answer":"Use a landmark-based dynamic SSSP: pick k landmarks; store dist(S,ℓ) and dist(ℓ,v) for all v; approximate dist(S,v) ≈ minℓ dist(S,ℓ)+dist(ℓ,v). On a batch of decreases, relax only paths affected by up","explanation":"## Why This Is Asked\n\nTests ability to reason about dynamic approximate shortest paths with real-world batch updates. Challenges include cascade propagation, landmark selection, and trade-offs between accuracy and update time. This probes knowledge of dynamic data structures and practical heuristics for production systems.\n\n## Key Concepts\n\n- Dynamic shortest paths with updates\n- (1+ε)-approximation via landmarks\n- Amortized analysis under batch updates\n- Space-time trade-offs in DS design\n\n## Code Example\n\n```javascript\nclass LandmarkSSSP {\n  constructor(S, landmarks) {\n    this.S = S;\n    this.landmarks = landmarks;\n  }\n  updateBatch(changes) {\n    // relax locally; recompute affected landmarks lazily\n  }\n  dist(v) {\n    // return min over landmarks dist(S,ℓ)+dist(ℓ,v)\n    return 0;\n  }\n}\n```\n\n## Follow-up Questions\n\n- How to select landmarks adaptively\n- How to bound worst-case cascades on adversarial batches","diagram":"flowchart TD\n  S(Source) --> L1[Landmark1]\n  S --> L2[Landmark2]\n  L1 --> V1[NodeA]\n  L2 --> V1","difficulty":"advanced","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Meta","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T03:51:35.496Z","createdAt":"2026-01-12T03:51:35.496Z"},{"id":"q-7671","question":"Implement a dynamic array that starts at capacity 4 and doubles on resize; after inserting N elements, what is the amortized cost per insertion, and how would a 1.5x growth factor change the bound, including cache implications?","answer":"Amortized cost per insertion is O(1). With growth factor b, total copies ≤ N · b/(b−1). For b=2, amortized ≤ 2. For b=1.5, amortized ≤ 3. Larger b reduces the number of reallocations, but can hurt cac","explanation":"## Why This Is Asked\n\nAssess understanding of amortized analysis as applied to a ubiquitous data structure.\n\n## Key Concepts\n\n- Amortized analysis\n- Growth factor and geometric series\n- Allocation overhead vs cache locality\n\n## Code Example\n\n```javascript\nfunction totalCopies(N, b){\n  return N * (b / (b - 1));\n}\n```\n\n## Follow-up Questions\n\n- How does starting capacity affect the bound?\n- What changes if growth factor varies adaptively based on N?","diagram":"flowchart TD\n  A[Insert N elements] --> B[Check capacity]\n  B --> C{Resize needed?}\n  C -- Yes --> D[Allocate new buffer (size ≈ b × old)]\n  C -- No --> E[Append element]\n  D --> E","difficulty":"beginner","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Goldman Sachs","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T12:00:24.997Z","createdAt":"2026-01-26T12:00:24.998Z"},{"id":"q-770","question":"Given a directed graph G=(V,E) with |V|=N and |E|=M, nonnegative weights, support online operations: 1) decreaseWeight(u,v,delta) with delta>0, 2) queryShortestPath(S,T) returning current shortest path length. Updates and queries are interleaved. Propose a data-structure and algorithm that achieves sublinear amortized reprocessing per update, justify amortized bounds, and discuss space and practical optimizations for massive graphs (N up to 1e6, M up to 1e7)?","answer":"Maintain a two-layer dynamic SSSP: a backbone sparse graph that preserves most shortest paths, plus a frontier-based lazy relaxation region. On decreaseWeight(u,v,delta), only relax from nodes in the ","explanation":"## Why This Is Asked\nTests ability to design dynamic shortest paths with interleaved updates and queries at scale, focusing on amortized analysis and engineering practicality.\n\n## Key Concepts\n- Dynamic graphs and amortized analysis\n- Localized frontier-based relaxations\n- Lazy evaluation and batching for latency control\n\n## Code Example\n```python\nclass DynamicSSSP:\n    def __init__(self, graph, s):\n        self.graph = graph\n        self.dist = [float('inf')] * graph.n\n        self.dist[s] = 0\n        self.cache = {}\n    def decrease(self, u, v, delta):\n        w = delta\n        if self.dist[u] + w < self.dist[v]:\n            self.dist[v] = self.dist[u] + w\n            self._relax_frontier({v})\n    def query(self, t):\n        return self.dist[t]\n    def _relax_frontier(self, frontier):\n        # localized Dijkstra updates\n        pass\n```\n\n## Follow-up Questions\n- How would you extend to edge deletions and negative cycles?\n- How would you empirically validate the amortized bounds on a real-world, large-scale graph?","diagram":"flowchart TD\n  A[Decrease edge weight] --> B[Frontier recomputation check]\n  B --> C{Cache valid?}\n  C -->|Yes| D[Answer query from cache]\n  C -->|No| E[Run localized Dijkstra from frontier]\n  E --> F[Update caches and distances]\n  F --> G[Return to queries]","difficulty":"advanced","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Lyft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T04:49:05.095Z","createdAt":"2026-01-12T04:49:05.095Z"},{"id":"q-7734","question":"You're processing a real-time web log of user IDs arriving in batches of size B from a universe of M users. After each batch, estimate the total number of distinct users seen so far with 5% relative error at 95% confidence. Propose a practical HyperLogLog-based solution: choose the number of registers s, derive the expected relative error as a function of s, state update time per batch, total memory, and how to adapt when B is large or the user space M grows. Compare to exact counting and mention potential edge cases?","answer":"Use HyperLogLog: select s = 2^p registers to meet 5% error (error ≈ 1.04/√s). For 5% error, p ≈ 9 (s = 512). Per-element update is O(1); per-batch cost is O(B). Memory ~ s registers (a few KB). Exact ","explanation":"## Why This Is Asked\nTests understanding of streaming distinct counting, memory-time tradeoffs, and practical sketch use in real-time systems.\n\n## Key Concepts\n- HyperLogLog and register-based cardinality estimation\n- Relative error as a function of registers\n- Batch processing and per-element O(1) updates\n- Memory implications for large universes\n- Edge cases: recency requirements, skew, resets\n\n## Code Example\n```javascript\n// Pseudo: add(x) updates HLL registers; estimate() returns cardinality\nfunction hllAdd(hll, x){ /* hash to [0,1) and update register */ }\nfunction hllEstimate(hll){ /* compute and return estimate */ }\n```\n\n## Follow-up Questions\n- How would you implement a sliding window distinct count with HyperLogLog?\n- How would you combine multiple HLL sketches from parallel streams for aggregation?","diagram":"flowchart TD\n  A[Batch arrives] --> B[Hash elements into registers]\n  B --> C[Update HLL state]\n  C --> D[Compute estimate]\n  D --> E[Compare to target error and report]","difficulty":"beginner","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Netflix","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T15:43:37.027Z","createdAt":"2026-01-26T15:43:37.028Z"},{"id":"q-7773","question":"In a real-time stream of user IDs arriving in batches of size B, after each batch report the number of distinct IDs seen in the last W events. Propose a practical approach using a counting Bloom filter plus bucketized sliding window. Describe data structures, per-batch work, memory usage, and error bounds, and compare to recomputing exactly?","answer":"Propose a sliding-window distinct counter using a counting Bloom filter plus bucketization. Maintain a ring of last W/B buckets; insert B IDs of the newest bucket and decrement counts for the oldest b","explanation":"## Why This Is Asked\nTests ability to reason about streaming data, sliding windows, and approximate data structures under memory constraints.\n\n## Key Concepts\n- Sliding window distinct counts\n- Counting Bloom filter with deletions\n- Bucketized aging (W events total)\n- Amortized per-batch cost O(B)\n\n## Code Example\n```javascript\n// Pseudo sketch for bucketed sliding window with counting Bloom filter\nclass SlidingDistinct {\n  constructor(W, B, m){ /* init */ }\n  addBatch(batch){ /* insert IDs, drop old bucket */ }\n  estimate(){ /* return distinct estimate */ }\n}\n```\n\n## Follow-up Questions\n- How does bucket size B affect error and memory?\n- How would non-uniform ID distributions impact false positives?","diagram":null,"difficulty":"beginner","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","NVIDIA","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T17:12:44.827Z","createdAt":"2026-01-26T17:12:44.828Z"},{"id":"q-779","question":"You're building a real-time analytics component for a fintech platform. You must maintain the number of distinct values in the most recent W events in a streaming fashion. Implement two operations: append(v) to push a new event value, and distinctCount() to return the number of unique values among the last W events. Assume values are integers up to 1e9 and W can be large. Provide a simple approach with its time/memory costs, then describe an amortized-constant-time solution using a hashmap plus a circular buffer, and discuss edge cases (e.g., large W, many duplicates). How would you implement and analyze it?","answer":"Option 1: recompute distinct on each append: O(W) time, O(W) space. Option 2: use a circular buffer of size W and a hashmap counts[value]→freq. On append: evict oldest value, decrement its count; add ","explanation":"## Why This Is Asked\n\nTests sliding-window knowledge, practical data structures, and amortized reasoning relevant to real-time analytics.\n\n## Key Concepts\n\n- Sliding window\n- Amortized analysis\n- Hashmap counters\n- Circular buffer\n\n## Code Example\n\n```python\nclass DistinctWindow:\n    def __init__(self, W):\n        self.W = W\n        self.buf = [None]*W\n        self.left = 0\n        self.right = 0\n        self.size = 0\n        self.counts = {}\n\n    def append(self, v):\n        if self.size == self.W:\n            old = self.buf[self.left]\n            self.buf[self.left] = None\n            self.left = (self.left + 1) % self.W\n            self.size -= 1\n            if old is not None:\n                c = self.counts[old]\n                if c == 1:\n                    del self.counts[old]\n                else:\n                    self.counts[old] = c - 1\n        self.buf[self.right] = v\n        self.right = (self.right + 1) % self.W\n        self.size += 1\n        self.counts[v] = self.counts.get(v, 0) + 1\n\n    def distinctCount(self):\n        return len(self.counts)\n```\n\n## Follow-up Questions\n\n- How does performance change if W is very large but few unique values exist? Any memory optimizations?\n- How would you adapt to a dynamic window size W' that changes over time without recomputing from scratch?","diagram":null,"difficulty":"beginner","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","LinkedIn","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T05:31:28.017Z","createdAt":"2026-01-12T05:31:28.017Z"},{"id":"q-7864","question":"You’re receiving a real-time stream of user IDs in batches of size B. After each batch, determine whether the number of distinct IDs seen in the most recent N entries exceeds a threshold T. Propose a practical, beginner-friendly algorithm with bounded memory, show update and query costs, and compare to exact sliding-window cardinality. Include a concrete data-structure choice and outline trade-offs?","answer":"Use a bucketed HyperLogLog solution. Split N into K buckets of size N/K. Each batch inserts IDs into the active bucket’s HLL (O(1) per item). On bucket rotation, finalize that bucket and merge into a ","explanation":"## Why This Is Asked\nEvaluates practical streaming cardinality with sliding windows, balancing accuracy, memory, and speed using beginner-friendly probabilistic structures.\n\n## Key Concepts\n- Sliding-window cardinality\n- HyperLogLog (HLL) accuracy, m registers\n- Bucketed window to bound memory\n- Trade-offs: error vs memory vs update cost\n\n## Code Example\n```javascript\n// Pseudocode sketch\nclass BucketedHLL {\n  constructor(K, m) { /* initialize K x m registers */ }\n  add(id) { /* hash to active bucket and update HLL */ }\n  rotate() { /* finalize bucket, merge into union */ }\n  estimate() { /* return union estimate across buckets */ }\n}\n```\n\n## Follow-up Questions\n- How to adjust K for different N or data skew?\n- How does reset/expire affect accuracy and latency?","diagram":null,"difficulty":"beginner","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Goldman Sachs"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T20:51:22.005Z","createdAt":"2026-01-26T20:51:22.005Z"},{"id":"q-787","question":"You maintain N players with integer scores in the range 0..10000. You must support two online operations: 1) update(i, s) — set player i's score to s; 2) countLE(X) — return how many players have score <= X. Propose a data structure and algorithm to support both in O(log V) time per operation, where V=10001, and analyze space usage. Include initialization and a brief correctness sketch?","answer":"Fenwick tree (BIT) over the value domain 0..10000. Initialize by counting frequencies of each score; update(i, newVal) does bit.add(oldVal, -1) and bit.add(newVal, +1); countLE(X) = bit.sum(X). Comple","explanation":"## Why This Is Asked\nTests ability to map domain-specific queries to a standard data structure and reason about time/space.\n\n## Key Concepts\n- Fenwick Tree (BIT)\n- Prefix sums\n- Coordinate/value-domain handling\n\n## Code Example\n```javascript\nclass Fenwick {\n  constructor(n){ this.n=n; this.f=new Array(n+1).fill(0); }\n  add(i, delta){ for(i+=1; i<=this.n; i+= i&-i) this.f[i]+=delta; }\n  sum(i){ let s=0; for(i+=1; i>0; i-= i&-i) s+=this.f[i]; return s; }\n}\n```\n\n## Follow-up Questions\n- How would you adapt for dynamic value ranges or non-integer scores?\n- How do you handle ties or range queries like count in (a,b]?","diagram":null,"difficulty":"beginner","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Cloudflare","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T06:39:27.526Z","createdAt":"2026-01-12T06:39:27.527Z"},{"id":"q-796","question":"Given an array A of length N with integers in range [0, R). You implement a function to count distinct values by inserting each A[i] into a hash set, then return its size. 1) What is the time and space complexity in terms of N and D (distinct values)? 2) If R is small, propose a memory-efficient alternative and analyze its tradeoffs?","answer":"With a hash set: expected O(N) time; O(D) space where D is distinct values; inserts amortize O(1). Worst-case O(N) time if hash collisions. If R is small, use a bitset/boolean array of size R for O(1)","explanation":"## Why This Is Asked\nDesign and analyze a simple counting distinct values task, contrasting hash-based and range-based approaches. It tests understanding of time vs space tradeoffs and edge cases of hash collisions and bounded value domains.\n\n## Key Concepts\n- Hash-set complexity\n- Distinct values D\n- Bitset vs hash-based counts\n- Space-time tradeoffs\n\n## Code Example\n```javascript\nfunction countDistinct(arr, R){\n  const seen = new Set();\n  for(const x of arr){ seen.add(x); }\n  return seen.size;\n}\n```\n\n## Follow-up Questions\n- How would you adapt for streaming data with sliding windows?\n- What about false positives with probabilistic structures like Bloom filters?","diagram":null,"difficulty":"beginner","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Netflix","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T07:30:21.355Z","createdAt":"2026-01-12T07:30:21.355Z"},{"id":"q-803","question":"In a directed graph G=(V,E) with N nodes, M edges and nonnegative weights, a fixed source S, and an SSSP tree T. Edges can change weight online (increase or decrease), but no edges are added or removed. Propose a concrete, implementable strategy to maintain the SSSP efficiently, including data structures, update rules, and expected time bounds. Provide preprocessing, amortized per-update, and memory usage, plus practical optimizations and a concrete scenario where it shines (e.g., streaming latency updates)?","answer":"Maintain a single-source shortest-path tree (SSSP) from source S. For weight decreases on tree edges, perform localized relaxation using bounded Dijkstra that visits only the affected frontier. For weight increases or non-tree edges, employ a hierarchical approach: first determine if the edge creates a shorter alternative path, and if so, execute localized re-relaxation from the affected subtree.\n\n**Data Structures:**\n- Fibonacci heap for priority queue (O(1) insert, O(log n) delete-min)\n- Adjacency list with edge weight tracking\n- Parent array for SSSP tree maintenance\n- Distance array with lazy updates\n- Affected node set to track nodes requiring relaxation\n\n**Preprocessing:** O(M + N log N) to compute initial SSSP using Dijkstra's algorithm\n\n**Update Rules:**\n- Weight decrease: If edge belongs to SSSP tree, push affected subtree nodes to priority queue with updated distances; otherwise, simply check if it creates a shorter path\n- Weight increase: If edge belongs to SSSP tree, mark affected subtree and run bounded Dijkstra from frontier nodes; otherwise, no action required\n\n**Time Bounds:**\n- Amortized per-update: O(log N + K) where K is the number of affected nodes (typically much smaller than N)\n- Worst-case: O(N log N) for pathological updates affecting the entire tree\n- Memory usage: O(N + M) for graph storage plus O(N) for auxiliary structures\n\n**Practical Optimizations:**\n- Use multi-level buckets for improved constant factors\n- Batch multiple updates before reprocessing\n- Maintain distance bounds to prune unnecessary relaxations\n- Cache frequently accessed edge weights\n\n**Concrete Scenario:** Real-time network routing with streaming latency updates, where edge weights represent network delays that change frequently but only affect localized regions of the network.","explanation":"## Why This Is Asked\nDesigning dynamic SSSP algorithms is essential for real-time network applications. This question tests understanding of worst-case versus amortized guarantees, appropriate data structure selection, and practical implementation constraints.\n\n## Key Concepts\n- Dynamic shortest paths with nonnegative edge weights\n- Localized re-relaxation and bounded Dijkstra execution\n- Amortized analysis and worst-case performance bounds\n- Memory-efficient data structures and practical optimizations\n- Hierarchical update strategies for different edge types\n\n## Code Example\n```javascript\n// Dynamic SSSP maintenance implementation\nclass DynamicSSSP {\n  constructor(graph, source) {\n    this.graph = graph;\n    this.source = source;\n    this.distances = new Array(graph.N).fill(Infinity);\n    this.parents = new Array(graph.N).fill(-1);\n    this.heap = new FibonacciHeap();\n    this.affectedNodes = new Set();\n    \n    // Initialize with Dijkstra\n    this.initializeSSSP();\n  }\n  \n  updateEdge(u, v, newWeight) {\n    const oldWeight = this.graph.getWeight(u, v);\n    this.graph.setWeight(u, v, newWeight);\n    \n    if (newWeight < oldWeight) {\n      this.handleWeightDecrease(u, v, newWeight);\n    } else {\n      this.handleWeightIncrease(u, v, newWeight);\n    }\n  }\n  \n  handleWeightDecrease(u, v, newWeight) {\n    if (this.parents[v] === u) {\n      // Tree edge weight decreased\n      this.propagateDecrease(v);\n    } else if (this.distances[u] + newWeight < this.distances[v]) {\n      // Non-tree edge creates shorter path\n      this.updateDistance(v, this.distances[u] + newWeight, u);\n    }\n  }\n  \n  handleWeightIncrease(u, v, newWeight) {\n    if (this.parents[v] === u) {\n      // Tree edge weight increased - need re-relaxation\n      this.affectedNodes.add(v);\n      this.boundedDijkstra([v]);\n    }\n    // Non-tree edge weight increase requires no action\n  }\n  \n  boundedDijkstra(startNodes) {\n    const queue = new FibonacciHeap();\n    \n    for (const node of startNodes) {\n      queue.insert(node, this.distances[node]);\n    }\n    \n    while (!queue.isEmpty()) {\n      const { node: u, distance: du } = queue.extractMin();\n      \n      if (du > this.distances[u]) continue;\n      \n      for (const [v, weight] of this.graph.neighbors(u)) {\n        const newDist = du + weight;\n        if (newDist < this.distances[v]) {\n          this.updateDistance(v, newDist, u);\n          queue.insert(v, newDist);\n        }\n      }\n    }\n  }\n  \n  updateDistance(node, newDistance, parent) {\n    this.distances[node] = newDistance;\n    this.parents[node] = parent;\n    this.affectedNodes.add(node);\n  }\n  \n  propagateDecrease(startNode) {\n    const queue = new FibonacciHeap();\n    queue.insert(startNode, this.distances[startNode]);\n    \n    while (!queue.isEmpty()) {\n      const { node: u } = queue.extractMin();\n      \n      for (const [v, weight] of this.graph.neighbors(u)) {\n        if (this.parents[v] === u) {\n          const newDist = this.distances[u] + weight;\n          if (newDist < this.distances[v]) {\n            this.updateDistance(v, newDist, u);\n            queue.insert(v, newDist);\n          }\n        }\n      }\n    }\n  }\n  \n  initializeSSSP() {\n    // Standard Dijkstra implementation\n    this.distances[this.source] = 0;\n    const heap = new FibonacciHeap();\n    heap.insert(this.source, 0);\n    \n    while (!heap.isEmpty()) {\n      const { node: u, distance: du } = heap.extractMin();\n      \n      if (du > this.distances[u]) continue;\n      \n      for (const [v, weight] of this.graph.neighbors(u)) {\n        const newDist = du + weight;\n        if (newDist < this.distances[v]) {\n          this.distances[v] = newDist;\n          this.parents[v] = u;\n          heap.insert(v, newDist);\n        }\n      }\n    }\n  }\n}\n```\n\nThis implementation demonstrates the core principles of dynamic SSSP maintenance with efficient localized updates and amortized performance guarantees.","diagram":null,"difficulty":"advanced","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","LinkedIn"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T12:44:57.161Z","createdAt":"2026-01-12T08:34:14.620Z"},{"id":"q-8044","question":"Edge insertions only: You have a directed weighted graph G=(V,E) with nonnegative weights. Edges are online-inserted in batches of size B; no deletions. After each batch, design a practical incremental SSSP structure that (i) preprocesses in O(n log n + m) time, (ii) updates in amortized O(B log n) time, (iii) answers dist(s, v) in O(1) time for all v, (iv) uses O(n+m) space, (v) includes two optimizations, (vi) compares to recomputing Dijkstra after every batch. Provide asymptotics in terms of n, m, B?","answer":"Incremental SSSP with nonnegative weights. Keep dist[s..] from source s; insertions relax edges (u,v,w); if dist[u]+w < dist[v], update dist[v] and push to a bucket/min-heap. Propagate along the front","explanation":"## Why This Is Asked\nModels incremental graph updates relevant to real-time routing and CDN health checks.\n\n## Key Concepts\n- Incremental updates with batch insertions\n- SSSP under dynamic graphs\n- Trade-offs between batch size and update cost\n\n## Code Example\n\n```javascript\n// incremental SSSP sketch\nfunction incrementalSSSP(n, adj, dist, s) {\n  // placeholder for batch update handling\n}\n```\n\n## Follow-up Questions\n- How to handle negative weights or weight updates?\n- What if batches include deletions?","diagram":"flowchart TD\n  Start([Start]) --> Batch[Process Batch]\n  Batch --> Relax[Relax edges]\n  Relax --> Done{All dist stable?}\n  Done --> End([End])\n  Done --> Batch","difficulty":"advanced","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","DoorDash","Lyft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T07:09:41.837Z","createdAt":"2026-01-27T07:09:41.837Z"},{"id":"q-808","question":"Dynamic path counting in a DAG: maintain the number of S→T paths of length at most L under online edge insertions and deletions. Propose a data structure and amortized update time bound in terms of N, M, L and #updates; discuss memory usage and how to handle large L and modulo arithmetic in practice?","answer":"Maintain a layer-based DP: c[v][d] = number of S→v paths of exact length d for d = 0..L. Initial fill by DP in topological order. On an edge insertion/deletion, compute the delta for c[v][d] caused by","explanation":"## Why This Is Asked\n\nEvaluates practicality of dynamic programming under online graph updates, requiring precise amortized reasoning and data-structure design rather than surface knowledge.\n\n## Key Concepts\n\n- DAG properties and topological order\n- Path counting DP with an extra length dimension\n- Incremental updates via delta propagation\n- Frontier-limited recomputation and memory-time tradeoffs\n\n## Code Example\n\n```python\n# Pseudocode for update propagation (edge (u->v) updated)\ndef update_edge(u, v, delta_sign):\n    for d in range(1, L+1):\n        delta = delta_sign * paths_from_S_to[u][d-1]\n        if delta:\n            paths_to[v][d] += delta\n            enqueue_successors(v, d, delta)\n```\n\n## Follow-up Questions\n\n- How would you adapt when L varies per query or when edges have weights affecting path counts differently?\n- What are the bottlenecks for extremely large L or dense DAGs, and how would you mitigate them?","diagram":"flowchart TD\n  A[Start] --> B[Receive edge update]\n  B --> C[Compute deltas on layer counts]\n  C --> D[Propagate to successors up to L]\n  D --> E[Return updated counts to queries]","difficulty":"intermediate","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Snowflake","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T09:33:58.407Z","createdAt":"2026-01-12T09:33:58.407Z"},{"id":"q-8105","question":"You maintain a dynamic set of closed intervals on a line. Initially empty. In online batches of B updates, each update adds a new interval [l, r] (l ≤ r). After each batch, output the total length of the union of all intervals. Propose a practical data structure with amortized guarantees, and analyze preprocessing, memory, and per-batch costs, plus a comparison to recomputing the union from scratch after every batch?","answer":"A practical approach uses an ordered map of disjoint intervals by start. On inserting [l,r], locate overlaps, merge them into a single [L,R], and adjust a running total length accordingly. Complexity ","explanation":"## Why This Is Asked\nTests dynamic interval maintenance, a practical data structure choice, and amortized analysis to compare incremental updates with full recomputation.\n\n## Key Concepts\n- Interval merging with ordered maps/trees\n- Running total maintenance\n- Amortized analysis across batches\n- Space: O(m) where m is number of disjoint intervals\n\n## Code Example\n```javascript\nclass IntervalSet {\n  constructor(){ this.intervals = new Map(); /* keyed by start; maintains non-overlapping intervals */ }\n  add(l, r){ /* find overlaps, merge, update totalLength */ }\n  length(){ /* return totalLength */ }\n}\n```\n\n## Follow-up Questions\n- How would deletions or endpoint adjustments affect the data structure?\n- What happens in the worst-case adversarial insertion order, and can you bound the total number of merges over many batches?","diagram":"flowchart TD\n  A[Batch Input] --> B[Find overlaps]\n  B --> C[Merge intervals]\n  C --> D[Update total length]\n  D --> E[Output length]","difficulty":"intermediate","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Tesla","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T09:54:21.296Z","createdAt":"2026-01-27T09:54:21.296Z"},{"id":"q-8138","question":"Maintain a dynamic union length of intervals in [0,1]. You start with an empty set. You receive online batches of B interval insertions (l, r) with 0 ≤ l ≤ r ≤ 1. After each batch, output the total measure of the union of all intervals. Propose a practical data structure, its preprocessing, update amortized cost per batch, and how its complexity scales with n total intervals, B, and coordinate size. Compare to rebuilding the union after each batch?","answer":"Use a dynamic segment tree over [0,1] with a cover-count per node. Each interval insertion increments counts on the covered nodes; the union length is the total length of segments with count>0, mainta","explanation":"## Why This Is Asked\nThis question tests practical amortized analysis and data-structure choices for dynamic interval union maintenance in streaming contexts, balancing update speed, memory, and query latency.\n\n## Key Concepts\n- Dynamic segment trees for range coverage\n- Amortized analysis with batch inserts\n- Coordinate compression vs sparse representations\n- Trade-offs: memory vs speed\n\n## Code Example\n```javascript\nclass Node {\n  constructor(){ this.cover=0; this.left=null; this.right=null; this.len=0; }\n}\nfunction update(node, l, r, ql, qr){ /* maintain cover counts and length */ }\n```\n\n## Follow-up Questions\n- How would you extend to intervals on arbitrary ranges or multi-dimensional boxes?\n- What are worst-case scenarios that force log U factor growth?","diagram":null,"difficulty":"advanced","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","MongoDB","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T11:07:29.055Z","createdAt":"2026-01-27T11:07:29.055Z"},{"id":"q-817","question":"Design a dynamic, multi-source shortest-path maintenance scheme for a directed graph with nonnegative weights. A fixed set of K hub nodes H must always have exact shortest-path distances to all nodes. Edges can be inserted or weights decreased online, in batches of size at most B. Provide initial preprocessing and a full-update algorithm, with (i) initial time, (ii) amortized update time per batch, and (iii) memory usage. Include two practical optimizations and compare to recomputing from scratch after each batch?","answer":"Multi-source incremental Dijkstra. Precompute d(h,v) for all h∈H with K runs: O(K E log V). For a batch of up to B updates, relax via affected edges using a shared min-heap, propagating improvements o","explanation":"## Why This Is Asked\nTests dynamic graphs, amortized analysis, and practical heuristics for keeping multiple SSSP trees up to date.\n\n## Key Concepts\n- Dynamic SSSP with monotone updates\n- Multi-source labeling\n- Amortized analysis and batch processing\n- Space-time trade-offs\n\n## Code Example\n```javascript\n// Pseudocode: multi-source incremental relaxations\nlet D = new Map(); // D[h] -> dist array\nfor (let h of H) D[h] = dijkstra(G, h);\nfunction updateBatch(batch){\n  let PQ = new MinHeap();\n  for (let e of batch){ /* decrease weights or insert */ }\n  // relaxations\n  while(!PQ.isEmpty()){ let {d,u} = PQ.pop(); if (d> D[h][u]) continue; ... }\n}\n```\n\n## Follow-up Questions\n- How do you bound X in worst-case, and how would you adapt if H grows?\n- How would you extend to approximate distances with guarantees?","diagram":null,"difficulty":"advanced","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","PayPal","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T10:26:55.970Z","createdAt":"2026-01-12T10:26:55.970Z"},{"id":"q-8261","question":"New angle: distinct counting in streaming batches. You receive batches of B integers from [0, M-1]. After each batch, design a practical online algorithm to estimate the total number of distinct values seen so far within 1±ε with probability at least 1-δ. Propose a sketch-based solution (e.g., HyperLogLog), specify sketch size in terms of ε and δ, per-item update time, batch query time, memory, and compare to an exact hash-set approach. Include asymptotics?","answer":"Use a HyperLogLog sketch with m = ceil((1.04/ε)^2) registers of 6 bits. On each value, hash to 64 bits, index i = top log2(m) bits, rho = position of first 1 in the remainder; M[i] = max(M[i], rho). A","explanation":"## Why This Is Asked\nTests ability to design streaming algorithms with probabilistic guarantees and compare practical trade-offs to exact counting.\n\n## Key Concepts\n- HyperLogLog sketch for distinct counting\n- Trade-off: memory vs. error ε and confidence δ\n- Per-item update O(1); query O(1); batch considerations\n\n## Code Example\n```javascript\nfunction hllAdd(registers, x) {\n  const h = hash64(x);\n  const m = registers.length;\n  const idx = h >>> (64 - Math.log2(m));\n  const rho = leadingZeros((h << Math.log2(m)) | (h >>> (64 - Math.log2(m))) ) + 1;\n  registers[idx] = Math.max(registers[idx], rho);\n}\n```\n\n## Follow-up Questions\n- How would deletions be supported?\n- How to choose ε, δ in constrained memory?","diagram":null,"difficulty":"beginner","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T17:10:30.668Z","createdAt":"2026-01-27T17:10:30.668Z"},{"id":"q-827","question":"Design a dynamic distance labeling scheme for an undirected weighted graph that supports edge insertions and deletions online. Use a fixed hub set H to enable dist(u,v) queries via dist(u,v)=min_h dist(u,h)+dist(h,v) only if H covers all shortest paths. Explain maintenance of hub distances under updates, and bound update/query times and memory usage. Provide two optimizations and a comparison to recomputing from scratch?","answer":"Approach: hub labeling with a fixed hub set H. Each node v stores dist(v,h) for all h in H; dist(u,v) = min_h dist(u,h)+dist(h,v) if H covers all shortest paths. On updates, recompute only hubs touche","explanation":"## Why This Is Asked\nThis probes dynamic graph labeling, balancing query time against update cost, a practical constraint in large-scale networks where frequent edits occur.\n\n## Key Concepts\n- Hub labeling and cover properties\n- Dynamic SSSP maintenance for limited regions\n- Trade-offs: hub count, space, and amortized updates\n\n## Code Example\n```javascript\n// Pseudocode sketch for updates\nfunction updateEdge(u, v, w, graph, labels, H){\n  // identify affected hubs and propagate changes using bounded-radius search\n}\n```\n\n## Follow-up Questions\n- How do you pick and update H to adapt to changing graphs? \n- How would you extend to directed graphs with non-symmetric distances?","diagram":"flowchart TD\n  A[Node] --> B[Hub]\n  B --> C[Distance to hub]\n  A --> D[Query dist to E]\n  D --> E[Compute min over hubs]","difficulty":"intermediate","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Meta","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T11:23:08.719Z","createdAt":"2026-01-12T11:23:08.719Z"},{"id":"q-831","question":"In an undirected weighted graph G=(V,E) with nonnegative weights, design a (1+ε)-approximate distance oracle based on a fixed landmark set L (|L|=k). Edges are only inserted online in batches of size B; no deletions. After each batch, specify: (i) preprocessing time and space to build distances from every landmark, (ii) amortized update time per batch to update the oracle, (iii) query time for dist(u,v), (iv) total memory, (v) two practical optimizations, (vi) a comparison to rebuilding all-pairs distances after each batch. Provide concrete asymptotics in terms of n=|V|, m=|E|, k, ε, B?","answer":"Use k landmarks. Precompute Dijkstra from each landmark: time O(k m log n), space O(k n). Query dist(u,v) = minℓ (d(u,ℓ)+d(ℓ,v)) in O(k). For a batch of B edge insertions, update via incremental relax","explanation":"## Why This Is Asked\nTests designing a practical distance oracle under incremental graph updates, balancing preprocessing, update, and query costs, plus real-world trade-offs when batch sizes are small.\n\n## Key Concepts\n- (1+ε) distance oracle with landmarks\n- Incremental insertions only\n- Multi-source Dijkstra from landmarks\n- Amortized per-batch analysis\n- Pruning and reuse of work across batches\n\n## Code Example\n```javascript\nfunction rebuildFromLandmarks(graph, landmarks) {\n  // placeholder: run Dijkstra from each landmark\n}\n```\n\n## Follow-up Questions\n- How would the approach adapt if deletions appear?\n- How does ε influence practical update costs and memory?","diagram":null,"difficulty":"intermediate","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","Microsoft","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T12:43:15.513Z","createdAt":"2026-01-12T12:43:15.513Z"},{"id":"q-8326","question":"Dynamic 2D convex hull with batched insertions: Points are added to a set P in batches of size B. After each batch, we must (a) report the area of Conv(P) and (b) answer whether a fixed query point q lies inside Conv(P). Propose a practical incremental structure that (i) preprocesses efficiently, (ii) updates amortized per batch, (iii) yields hull-area in O(1) and membership test in O(log h), (iv) uses near-linear space, (v) includes two optimizations, (vi) compares to rebuilding after each batch. Give asymptotics in n=|P|, B?","answer":"Maintain the hull with two monotone chains (upper/lower). On each batch, sort B points by x, merge into the existing hull in amortized O((B+h) log h) time, effectively O(B log n). Update hull area inc","explanation":"## Why This Is Asked\nTests dynamic geometry reasoning, batched updates, and amortized analysis; pushes practical design choices beyond static hulls.\n\n## Key Concepts\n- Dynamic convex hull; batch updates; amortized costs; area maintenance; point-in-hull queries; space efficiency.\n\n## Code Example\n```javascript\nclass DynamicHull {\n  constructor(points) { /* build initial hull */ }\n  insertBatch(batch) { /* merge batch into hull, update area */ }\n  hullArea() { /* return precomputed area in O(1) */ }\n  contains(p) { /* O(log h) test on hull */ }\n}\n```\n\n## Follow-up Questions\n- How do collinear points affect amortized bounds? \n- How would deletions be integrated without breaking invariants?","diagram":"flowchart TD\n  A[Batch Insert] --> B[Merge into Hull]\n  B --> C[Update Area]\n  C --> D[Answer Contains q]\n  D --> E[Return Hull Area]","difficulty":"advanced","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Hugging Face","Lyft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T19:54:53.195Z","createdAt":"2026-01-27T19:54:53.195Z"},{"id":"q-8403","question":"Maintain a dynamic 2D point set P in the plane. Points are appended in online batches of size B (i -> (x_i,y_i)); never deleted. After each batch, report the area of the convex hull of all points seen so far. Propose a practical batched rebuild data structure: (i) a base hull H for old points, (ii) a buffer U for the new batch, (iii) a rebuild threshold after T batches, with amortized update and query times. Include preprocessing, memory, two optimizations, and a comparison to rebuilding from scratch after every batch?","answer":"Two-layer hull architecture: maintain a base convex hull H for all previously processed points and a buffer U storing the current batch of size B. After each batch, compute the convex hull area by merging U with H using an incremental hull algorithm. Rebuild the entire hull only when the buffer size exceeds a threshold T×B, then clear U. This approach provides O(B log B) amortized update time and O(|H| + |U|) query time.","explanation":"## Why This Is Asked\nTests understanding of dynamic geometric data structures and amortized analysis through batched rebuilding strategies.\n\n## Key Concepts\n- Convex hull maintenance (monotone chain, Graham scan)\n- Amortized time complexity analysis\n- Batch processing and rebuild thresholds\n- Memory management for dynamic point sets\n\n## Code Example\n```javascript\nfunction rebuildHull(points) {\n  // Sort points lexicographically and compute convex hull\n  points.sort((a, b) => a.x !== b.x ? a.x - b.x : a.y - b.y);\n  // Build hull using monotone chain algorithm\n  return hull;\n}\n```\n\n## Follow-up","diagram":null,"difficulty":"intermediate","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Robinhood","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-28T06:04:11.400Z","createdAt":"2026-01-27T23:02:51.039Z"},{"id":"q-8445","question":"Dynamic 2D convex hull with batch insertions: Points arrive online in batches of size B; no deletions. After each batch, determine whether a query point p lies inside the current hull. Propose a practical incremental data structure that (i) preprocesses near-linearly, (ii) updates hull amortized O(B log n), (iii) answers point-in-hull in O(log n) (or O(1) with tradeoffs), (iv) uses near-linear space, (v) includes two optimizations, and (vi) compares to recomputing hull after every batch. What are the asymptotics in terms of n, B, and two-dimensional hull size h?","answer":"Maintain hull H as two monotone chains (upper and lower). For each batch of B points, compute the convex hull Hb of the batch in O(B log B) time using Graham scan; then merge Hb into H by tangent-finding binary search in O((|H| + B) log |H|) time, which amortizes to O(B log n) over n total points. Point-in-hull queries are answered via binary search on the monotone chains in O(log h) time, where h = |H| is the current hull size. The data structure uses O(n) space total.","explanation":"## Why This Is Asked\nThis question probes incremental computational geometry techniques, batch amortized analysis, and practical trade-offs for maintaining 2D convex hulls under streaming insertions. It tests understanding of how to bound update costs while supporting fast queries with near-linear space.\n\n## Key Concepts\n- Incremental convex hull maintenance using monotone chain decomposition\n- Batch processing with amortized cost analysis\n- Point-in-hull queries via binary search on monotone chains\n- Space-efficient hull representation and practical optimizations\n\n## Optimizations\n1. **Lazy merging**: Defer non-critical merges until query time to reduce overhead\n2. **Hierarchical batching**: Maintain multiple batch sizes to balance update and query costs\n\n## Asymptotic Analysis\n- Preprocessing: O(n log n) to initialize hull structure\n- Update per batch: O(B log n) amortized over n points\n- Query: O(log h) time, where h is current hull size (≤ n)\n- Space: O(n) total\n- Comparison to recomputation: Batch approach saves factor of B/log B vs O(n log n) per batch","diagram":"flowchart TD\n  Start((Start)) --> A[Batch arrives: compute Hb for batch]\n  A --> B[Merge Hb into H to form new hull]\n  B --> C[Answer p-in-hull via tangent binary search]\n  C --> D[Apply two optimizations (e.g., lazy rebuild, prune interiors)]\n  D --> End((Done))","difficulty":"advanced","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-28T05:50:12.606Z","createdAt":"2026-01-28T02:40:59.627Z"},{"id":"q-8475","question":"You are given a string S initially empty. Online, you receive batches of B characters to append to the end. After each batch, compute the total number of distinct substrings of S. Propose a practical data structure and amortized analysis for updates and queries, including preprocessing time, memory, and a comparison to rebuilding from scratch after every batch. Assume B ≤ 2e5 and total length up to 1e6?","answer":"Implement a suffix automaton to track distinct substrings as S grows. On each appended character, extend the SAM; the number of new substrings is len(cur) - len(link(cur)) where cur is the new last st","explanation":"## Why This Is Asked\nTests dynamic string processing, amortized analysis, and practical data structures for online updates.\n\n## Key Concepts\n- Suffix automaton for incremental substring counting\n- amortized per-character extension bounds\n- memory proportional to string length\n\n## Code Example\n```javascript\nclass SAM { constructor(){ this.next=[{}]; this.link=[-1]; this.len=[0]; this.last=0; } extend(ch){ /* standard SAM extend: create cur, set len, link, clone if needed */ } }\n```\n\n## Follow-up Questions\n- How would deletions affect the structure?\n- Compare with a suffix array approach for batch rebuilds.","diagram":"flowchart TD\n  A[Append batch] --> B[Extend SAM]\n  B --> C[Compute new substrings per char]\n  C --> D[Update totalDistinct]\n  D --> E[Respond with totalDistinct]","difficulty":"advanced","tags":["complexity-analysis"],"channel":"complexity-analysis","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-28T04:27:08.929Z","createdAt":"2026-01-28T04:27:08.929Z"}],"subChannels":["general"],"companies":["Adobe","Airbnb","Amazon","Anthropic","Apple","Bloomberg","Citadel","Cloudflare","Coinbase","Databricks","Discord","DoorDash","Goldman Sachs","Google","Hashicorp","Hugging Face","IBM","Instacart","LinkedIn","Lyft","Meta","Microsoft","MongoDB","NVIDIA","Netflix","OpenAI","Oracle","PayPal","Plaid","Robinhood","Salesforce","Scale Ai","Slack","Snap","Snowflake","Square","Stripe","Tesla","Twitter","Two Sigma","Uber","Zoom"],"stats":{"total":88,"beginner":35,"intermediate":20,"advanced":33,"newThisWeek":38}}