{"questions":[{"id":"q-199","question":"When deploying LLM inference with vLLM and Triton Inference Server, how do you handle request batching across multiple GPU nodes while maintaining sub-100ms latency for individual requests?","answer":"Use dynamic batching with Triton's ensemble scheduler, vLLM's PagedAttention, and request routing based on queue depth and model load.","explanation":"## Concept Overview\nDistributed LLM inference requires intelligent request batching to maximize GPU utilization while meeting latency SLAs. The challenge is balancing throughput with individual request response times.\n\n## Implementation Details\n- **Triton Ensemble Scheduler**: Coordinates multiple model instances with different batch sizes\n- **vLLM PagedAttention**: Manages memory efficiently across requests\n- **Dynamic Batching**: Groups requests based on arrival time and sequence length\n- **Load-aware Routing**: Distributes requests based on current GPU utilization\n\n## Code Example\n```python\n# Triton config for dynamic batching\nname: \"llm_vllm\"\nplatform: \"python_vllm\"\nmax_batch_size: 32\ndynamic_batching {\n  max_queue_delay_microseconds: 5000\n  preferred_batch_size: [4, 8, 16]\n}\n```\n\n## Common Pitfalls\n- Fixed batch sizes causing latency spikes\n- Memory fragmentation with static allocation\n- Poor request routing leading to GPU imbalance\n- Ignoring sequence length variance in batching logic","diagram":"flowchart LR\n    A[Client Request] --> B[Load Balancer]\n    B --> C{Queue Depth}\n    C -->|Low| D[Single GPU]\n    C -->|High| E[Multi-GPU Batch]\n    E --> F[Triton Ensemble]\n    F --> G[vLLM PagedAttention]\n    G --> H[GPU Cluster]\n    H --> I[Response Aggregation]\n    I --> J[Client Response]","difficulty":"advanced","tags":["vllm","tgi","triton","onnx"],"channel":"llm-ops","subChannel":"deployment","sourceUrl":null,"videos":null,"companies":["Amazon","Google","Meta","Microsoft","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-25T12:52:32.776Z","createdAt":"2025-12-26 12:51:06"},{"id":"q-1045","question":"You're running a beginner LLM inference API used by multiple clients. Design a low-cost, safe plan to implement per-tenant model versioning and zero-downtime hot-swapping. Include routing to the correct version, how you deploy new versions, rollback strategy, and observability to verify no regressions before full rollout?","answer":"Implement per-tenant routing via a header that maps to immutable model blobs; deploy new versions with a canary gate and two concurrent pools; route tenants gradually and switch over only after health","explanation":"## Why This Is Asked\n\nAssess ability to plan multitenant versioned deployments, zero-downtime swaps, rollback, and observability.\n\n## Key Concepts\n\n- Multitenancy with per-tenant version routing\n- Immutable model blobs and canary deployments\n- Safe rollback and health checks\n- Per-tenant observability and version tagging\n\n## Code Example\n\n```python\n# Minimal routing sketch\nfrom fastapi import FastAPI, Header, HTTPException\n\napp = FastAPI()\n\ndef load_model(path):\n    pass  # placeholder\n\nmodels = {\n  'v1': load_model('s3://bucket/model_v1.pt'),\n  'v2': load_model('s3://bucket/model_v2.pt'),\n}\n\n@app.post('/infer')\nasync def infer(payload: dict, x_version: str = Header(..., alias='X-Model-Version')):\n    model = models.get(x_version)\n    if not model:\n        raise HTTPException(400, 'Unknown model version')\n    return model.infer(payload)\n```\n\n## Follow-up Questions\n\n- How would you test rollback under simulated failure?\n- How would you track per-tenant version distribution and latency?","diagram":null,"difficulty":"beginner","tags":["llm-ops"],"channel":"llm-ops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T20:31:58.942Z","createdAt":"2026-01-12T20:31:58.942Z"},{"id":"q-1060","question":"You're delivering a privacy-preserving, regionally scoped, multi-tenant LLM assistant used by teams across geographies. Describe an end-to-end design that ensures tenant isolation, data residency, and per-tenant policy enforcement while meeting sub-200ms latency for common prompts. Include routing, key management, redaction, auditability, and deletion workflows, plus how you'd validate compliance across regions?","answer":"Route prompts to region-resident pools by tenant residency, using per-tenant encryption keys and strict redaction before forwarding. Isolate tenants with separate model pools and containers, maintain ","explanation":"## Why This Is Asked\n\nTests ability to design cross-region, privacy-first systems with strict tenant isolation. Covers routing, KMS key management, redaction, auditability, and deletion workflows.\n\n## Key Concepts\n\n- Data residency and tenancy\n- Per-tenant policy enforcement\n- Tamper-evident logging and deletion workflows\n- Latency optimization with regional caches\n\n## Code Example\n\n```yaml\npolicy:\n  - tenant_id: \"tenantA\"\n    region: \"us-east-1\"\n    redact_prompts: true\n    retention_days: 365\n```\n\n## Follow-up Questions\n\n- How would you verify deletion guarantees across regions?\n- How do you monitor audit log integrity in production?\n","diagram":"flowchart TD\n  A[Request] --> B[Policy Router]\n  B --> C[Region Router]\n  C --> D[Model Pool]\n  D --> E[Response]","difficulty":"intermediate","tags":["llm-ops"],"channel":"llm-ops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","NVIDIA","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T21:21:41.776Z","createdAt":"2026-01-12T21:21:41.776Z"},{"id":"q-1147","question":"You're operating a multi-tenant, multi-region LLM inference service for regulated financial firms. Design an architecture that enforces per-tenant data residency and per-tenant model pools, plus dynamic model-version rollout with feature flags and safe rollback. Describe policy-driven routing, on-the-fly prompt sanitization, observability, and incident response. Include concrete data-plane components and a rollout sequence for a new model version?","answer":"Implement regional data stores and tenant-scoped vaults to enforce residency; assign per-tenant model pools; policy-driven router binds tenant, region, and model version; enable canary rollouts via fe","explanation":"## Why This Is Asked\n\nThis question probes practical capability to enforce data residency and isolated model environments while executing safe, observable model-version rollouts in regulated contexts.\n\n## Key Concepts\n\n- Data residency and tenant isolation\n- Per-tenant model pools\n- Canary rollout and feature flags\n- Prompt sanitization at ingress\n- Observability and incident response\n\n## Code Example\n\n```javascript\n// Pseudo routing decision\nfunction route(req){\n  const tenant = req.tenant\n  const region = req.region\n  const version = featureFlag(tenant, 'new-version') ? 'v2' : 'v1'\n  return { tenant, region, version }\n}\n```\n\n## Follow-up Questions\n\n- How would you design rollback criteria and metrics for a canary rollout?\n- What changes would you make to support regulatory audits and data access controls?","diagram":"flowchart TD\n  A[Ingress] --> B[Policy Engine]\n  B --> C{Tenant/Region Rules}\n  C -->|Region| D[Regional Data Store]\n  C -->|Tenant| E[Model Pool]\n  B --> F[Canary Controller]\n  F --> G[Model Version Canary]\n  G --> H[Inference Service]\n  H --> I[Telemetry]","difficulty":"advanced","tags":["llm-ops"],"channel":"llm-ops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Goldman Sachs","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T01:32:43.862Z","createdAt":"2026-01-13T01:32:43.862Z"},{"id":"q-1263","question":"You operate a global LLM inference service across three regions and must upgrade models with zero downtime. Describe a concrete plan for rolling upgrades with canaries, traffic-splitting, warm-up, health checks, and fast rollback, ensuring SLA adherence and minimal cold-start impact during the switch?","answer":"Leverage region-specific model pools with dual versions, progressive canaries (e.g., 5/20/75%), and a gate that halts on SLA breach. Pre-warm new shards, perform rolling upgrades, and roll back automa","explanation":"## Why This Is Asked\n\nTests the ability to plan zero-downtime upgrades, canary rollouts, and safe rollback across regions while maintaining SLA.\n\n## Key Concepts\n\n- Canary deployments and progressive traffic shifting\n- Multi-region model pools and a shared registry\n- Warm-up strategies and cache/pool readiness\n- Health checks tied to latency budgets and error rates\n- Safe rollback triggers and observability\n\n## Code Example\n\n```javascript\n// Pseudo health-check gate for upgrade\nconst metrics = { latencyP99: 120, errorRate: 0.01 };\nif (metrics.latencyP99 > 200 || metrics.errorRate > 0.02) {\n  // trigger rollback\n}\n```\n\n## Follow-up Questions\n\n- How would you validate canary stability under burst traffic?\n- How would you ensure fair SLA guarantees for tenants with tighter latency budgets?","diagram":null,"difficulty":"intermediate","tags":["llm-ops"],"channel":"llm-ops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Slack","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T07:27:44.095Z","createdAt":"2026-01-13T07:27:44.095Z"},{"id":"q-1640","question":"You're launching a beginner-friendly LLM chat in a mobile-first app. To balance latency and privacy, design an edge-first routing: small prompts stay on-device; larger or sensitive prompts go to the cloud. Describe concrete components, a simple policy rule, data flow, and how you'd test it for privacy and latency before release?","answer":"Implement edge-first routing: if prompt <= 128 tokens and contains no PII, run on-device via TensorFlow Lite; else route to cloud. Key parts: client SDK, edge inference runner, policy module, cloud en","explanation":"## Why This Is Asked\n\nTests practical decision-making for edge/cloud routing and privacy in a beginner context.\n\n## Key Concepts\n\n- Edge inference\n- Simple policy\n- PII detection\n- Latency testing\n\n## Code Example\n\n```javascript\nfunction routePrompt(p) {\n  const isSmall = p.tokens <= 128;\n  const hasPII = /PII_REGEX/.test(p.text);\n  if (isSmall && !hasPII) return edgeInfer(p);\n  return cloudInfer(p);\n}\n```\n\n## Follow-up Questions\n\n- How would you adapt the policy for offline mode changes?\n- What metrics would you log to monitor privacy and latency?","diagram":null,"difficulty":"beginner","tags":["llm-ops"],"channel":"llm-ops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Apple","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T04:25:24.070Z","createdAt":"2026-01-14T04:25:24.070Z"},{"id":"q-1695","question":"Design a compliant, auditable LLM inference pipeline for a regulated financial platform that must deliver per-transaction data lineage, prompt provenance, and immutable end-to-end logging with 24-month retention while keeping latency under 150 ms end-to-end. Describe data flows, encryption, storage, and how you test for data leakage and drift?","answer":"Route through per-tenant gateway enforcing strict policy, attach lineage: tenant_id, txn_id, prompt_digest, model_version; stream logs to an append-only store with S3 Object Lock and KMS, plus a hash ","explanation":"## Why This Is Asked\n\nTests ability to design auditable, low-latency LLM infra for regulated domains. It probes data lineage, provenance, immutability, retention, and leakage/drift controls.\n\n## Key Concepts\n\n- Per-tenant isolation and policy enforcement\n- Immutable, tamper-evident logging with provenance\n- End-to-end latency budgeting and micro-batching\n- Leakage testing and drift detection\n\n## Code Example\n\n```javascript\n// Pseudo log entry for audit\nconst log = {\n  tenantId: 'T123',\n  txnId: 'TXN456',\n  promptDigest: 'abc123',\n  modelVersion: 'v2.1',\n  timestamp: Date.now()\n};\n```\n\n## Follow-up Questions\n\n- How would you test retention using legal holds and egress audits?\n- How would you integrate this with your CI/CD pipelines?","diagram":"flowchart TD\n  Client --> Gateway\n  Gateway --> InferenceService\n  InferenceService --> Logs\n  InferenceService --> Metrics","difficulty":"advanced","tags":["llm-ops"],"channel":"llm-ops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Oracle","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T07:04:13.141Z","createdAt":"2026-01-14T07:04:13.141Z"},{"id":"q-1858","question":"Design a beginner-friendly chat moderation assistant for a live-stream app with edge-first routing: short prompts stay on-device; longer or risk-prone prompts go to cloud. Specify the minimal architecture, a concrete routing rule (token threshold and risk score), data flow with redaction, and a practical test plan to validate latency (<200ms on-device, <600ms cloud) and privacy?","answer":"On-device for prompts up to 40 tokens; cloud for longer or high-risk prompts. Routing: if token_count <= 40 AND risk_score < 0.2, stay local; else route to cloud. Redact PII before cloud transmission.","explanation":"## Why This Is Asked\nTests ability to design edge-cloud LLM workflows with simple, tangible policies.\n\n## Key Concepts\n- Edge vs cloud inference\n- Lightweight risk scoring\n- Data redaction and privacy\n- Latency budgets and observability\n\n## Code Example\n```javascript\nfunction shouldRouteOnDevice(tokens, risk) {\n  return tokens <= 40 && risk < 0.2;\n}\n```\n\n## Follow-up Questions\n- How would you validate privacy in production without impacting user experience?\n- What changes if prompts vary greatly in length across users?","diagram":"flowchart TD\n  A[User Prompt] --> B{Token <=40 & Risk<0.2?}\n  B -- Yes --> C[On-device Inference]\n  B -- No --> D[Cloud Inference]\n  C --> E[Response]\n  D --> E","difficulty":"beginner","tags":["llm-ops"],"channel":"llm-ops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Databricks"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T14:45:11.271Z","createdAt":"2026-01-14T14:45:11.271Z"},{"id":"q-2003","question":"Design a tenancy-aware LLM inference layer where 60 regional tenants share a single GPU pool but must enforce per-tenant data isolation, prompt sanitization, and per-tenant model versioning. When burst traffic occurs, guarantee 95th percentile latency under 300 ms while preventing cross-tenant data leakage. Describe data flows, policy evaluation, routing, and rollback?","answer":"Design a tenancy-aware policy engine with per-tenant model pools, routing, and prompt sanitization. Each request passes through: tenant policy lookup, prompt scrubber (PII/PHI), model selector with ve","explanation":"## Why This Is Asked\nTests ability to design policy-driven, privacy-preserving routing for multi-tenant LLMs with latency guarantees. Requires thinking about isolation, versioning, and observability.\n\n## Key Concepts\n- Per-tenant policy engine and model pools\n- Real-time prompt sanitization and leakage checks\n- Latency budgeting and burst handling\n- Data privacy: encryption, residency, and access controls\n- Observability and drift detection\n\n## Code Example\n```javascript\n// Pseudo policy evaluation sketch\nfunction evaluatePolicy(req, tenantStore){\n  const t = tenantStore.get(req.tenantId);\n  if(!t) throw new Error('Unknown tenant');\n  return { allow: t.allowed, model: t.modelVersion };\n}\n```\n\n## Follow-up Questions\n- How would you test for cross-tenant leakage under load?\n- How would you handle model version migrations without cold starts?","diagram":"flowchart TD\n  A[Client Request] --> B[Tenant Policy Lookup]\n  B --> C{Policy OK?}\n  C -->|Yes| D[Prompt Sanitization]\n  C -->|No| E[Reject]\n  D --> F[Model Routing & Version Pinning]\n  F --> G[Inference Engine]\n  G --> H[Response]\n  H --> I[Telemetry & Drift Checks]","difficulty":"intermediate","tags":["llm-ops"],"channel":"llm-ops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google","IBM"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T20:39:47.133Z","createdAt":"2026-01-14T20:39:47.133Z"},{"id":"q-2146","question":"Design a hybrid edge-cloud LLM inference system where tenants move between corporate networks and remote locations. How would you enforce per-tenant data locality, latency budgets, model versioning, and graceful failover while keeping observability clear and simple?","answer":"Implement a policy-driven router: edge inference for latency-sensitive tenants with data residency pinned to allowed regions, spillover to cloud for bursts. Maintain per-tenant model pools and version","explanation":"## Why This Is Asked\nTests real-world hybrid edge-cloud serving with data locality, SLA management, and per-tenant isolation under fluctuating network conditions.\n\n## Key Concepts\n- Data locality and residency per tenant\n- Edge vs cloud routing and burst handling\n- Per-tenant model pools and versioning\n- Observability, fault tolerance, and rollback strategies\n\n## Code Example\n```javascript\n// Pseudo policy evaluator\nfunction routeRequest(req, policyStore){\n  const p = policyStore.get(req.tenantId);\n  if(p.locality === 'edge' && req.latencyBudget >= EDGE_LATENCY){\n    return 'edge';\n  }\n  return 'cloud';\n}\n```\n\n## Follow-up Questions\n- How would you validate locality guarantees under variable networks?\n- How would you automate canary rollouts and safe rollback per tenant?","diagram":null,"difficulty":"intermediate","tags":["llm-ops"],"channel":"llm-ops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Google","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T04:26:04.952Z","createdAt":"2026-01-15T04:26:04.952Z"},{"id":"q-2190","question":"Design a lightweight PII redaction gate that runs before prompts reach the model in a multi-tenant LLM-ops pipeline. Describe concrete redaction rules, per-tenant policy config, data locality considerations, and how you'd validate latency and redaction accuracy with a minimal test suite?","answer":"Pre-infer redaction gate: apply per-tenant regex rules for emails, phone numbers, SSNs; replace with [REDACTED], preserving token structure. Store policies in a config service (tenant_id → rules). Red","explanation":"## Why This Is Asked\nThis question probes practical privacy controls in LLM ops, focusing on implementable redaction, per-tenant policy configuration, and measurable latency impact.\n\n## Key Concepts\n- PII detection and redaction via regex rules\n- Per-tenant policy configuration with hot-reload\n- Data locality: redact before network transfer or on-device when feasible\n- Latency budgeting and minimal test coverage for accuracy\n\n## Code Example\n```javascript\n// Example redaction skeleton\nconst defaultPIIPatterns = {\n  emails: /\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b/g,\n  phones: /(\\+?\\d{1,3}[-.\\s]?)?(\\(?\\d{3}\\)?|\\d{3})[-.\\s]?\\d{3}[-.\\s]?\\d{4}/g,\n  ssn: /\\b\\d{3}-\\d{2}-\\d{4}\\b/g\n};\n\nfunction redactPrompt(prompt, tenantRules) {\n  const patterns = Object.assign({}, defaultPIIPatterns, tenantRules?.patterns);\n  let redacted = prompt;\n  for (const key in patterns) {\n    redacted = redacted.replace(patterns[key], '[REDACTED]');\n  }\n  return redacted;\n}\n```\n\n## Follow-up Questions\n- How would you test per-tenant policy reloading without affecting live requests?\n- How would you handle false positives that degrade user experience?","diagram":null,"difficulty":"beginner","tags":["llm-ops"],"channel":"llm-ops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Goldman Sachs","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T06:57:24.667Z","createdAt":"2026-01-15T06:57:24.667Z"},{"id":"q-2201","question":"Design a per-tenant, order-preserving prompt queue for a multi-tenant LLM chat backend. Ensure prompts from different tenants can be processed in parallel, but each tenant's prompts are served strictly in arrival order. Describe data models, queueing strategy, and a minimal Python asyncio prototype showing enqueue, dispatch, and a mock model call with per-tenant isolation and timeouts?","answer":"Per-tenant FIFO queues and a central dispatcher. Each tenant_id maps to its own asyncio.Queue; a single scheduler pulls from all tenants in a round-robin with prioritization by arrival time, dispatchi","explanation":"## Why This Is Asked\nTests understanding of concurrency, isolation, and ordering in a multi-tenant LLM-ops backend with a simple, real-world pattern.\n\n## Key Concepts\n- Per-tenant FIFO queues for strict ordering\n- Central dispatcher enabling cross-tenant parallelism\n- Data isolation and fairness across tenants\n- Timeouts, dead-letter handling, and retriable prompts\n\n## Code Example\n```python\nimport asyncio\nfrom collections import defaultdict\n\nclass TenantQueueManager:\n    def __init__(self, model_call, timeout=1.0):\n        self.queues = defaultdict(asyncio.Queue)\n        self.model_call = model_call\n        self.timeout = timeout\n        self.active = True\n\n    async def enqueue(self, tenant_id, prompt):\n        await self.queues[tenant_id].put(prompt)\n\n    async def dispatch(self):\n        while self.active:\n            for tenant_id, q in list(self.queues.items()):\n                if not q.empty():\n                    prompt = await asyncio.wait_for(q.get(), timeout=self.timeout)\n                    asyncio.create_task(self.model_call(tenant_id, prompt))\n            await asyncio.sleep(0.01)\n\nasync def mock_model_call(tenant_id, prompt):\n    await asyncio.sleep(0.05)  # simulate model latency\n    return f\"{tenant_id}:{prompt}\"\n```\n\n## Follow-up Questions\n- How would you adjust the design to handle bursty tenants without starving others?\n- How would you test ordering guarantees under simulated failures and retries?","diagram":"flowchart TD\n  A[Incoming Prompt] --> B[Tenant ID Router]\n  B --> C{Tenant FIFO Queues}\n  C --> D[Dispatcher]\n  D --> E[Model Pool]\n  E --> F[Response]","difficulty":"beginner","tags":["llm-ops"],"channel":"llm-ops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Robinhood","Salesforce","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T07:33:15.318Z","createdAt":"2026-01-15T07:33:15.318Z"},{"id":"q-2309","question":"In a multi-tenant LLM-ops gateway used by Oracle and Apple, design a real-time policy-driven guardrail that prevents tenant data leakage via prompt injection, enforces per-tenant data locality and content policies, and streams a tamper-evident audit log. Describe data models, policy evaluation flow, latency targets (<50 ms per prompt), and a minimal Rust/protobuf prototype snippet for policy evaluation?","answer":"Implement per-tenant policy bundles stored in an encrypted policy store; a policy engine evaluates prompts with redaction rules, provenance checks, and leakage prevention. Route to tenant-specific poo","explanation":"## Why This Is Asked\nThis tests governance, real-time decisioning, and cross-tenant safety in production.\n\n## Key Concepts\n- Per-tenant policy stores\n- Prompt provenance and redaction\n- Data locality routing\n- Tamper-evident auditing\n\n## Code Example\n```protobuf\nsyntax = \"proto3\";\nmessage PolicyDecision {\n  string tenant_id = 1;\n  enum Decision { ALLOW = 0; DENY = 1; WARN = 2; }\n  Decision decision = 2;\n  string reason = 3;\n  int64 ts = 4;\n}\n```\n\n## Follow-up Questions\n- How would you test latency under burst?\n- Describe failure modes and fail-open/closed decisions.","diagram":"flowchart TD\n  InboundPrompt[Inbound Prompt] --> PolicyEngine[Policy Engine]\n  PolicyEngine --> Decision{Decision}\n  Decision --> Route[Route to Tenant Pool]\n  Decision --> Block[Block Request]\n  Decision --> Quarantine[Quarantine for Review]","difficulty":"advanced","tags":["llm-ops"],"channel":"llm-ops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T11:38:41.792Z","createdAt":"2026-01-15T11:38:41.792Z"},{"id":"q-2339","question":"In a multi-tenant LLM-ops platform that supports dynamic model tiering (tiny/fast vs large/accurate) with per-tenant QoS budgets, how would you design the model selection, routing, and accounting to meet SLAs while minimizing cross-tenant warmups? Include the data model, API surface, and a minimal config snippet showing how tenants express budgets and tier preferences?","answer":"Use a per-tenant policy store and two pools (small/fast, large/accurate). The router picks the cheapest tier that meets latency targets, enforces per-tenant in-flight limits with token buckets, and pr","explanation":"## Why This Is Asked\nCovers real-world needs for dynamic model tiering, strict isolation, and per-tenant SLAs under burst traffic.\n\n## Key Concepts\n- Per-tenant QoS budgets and latency targets\n- Dynamic tiering and dedicated pools\n- Backpressure, pre-warming, and autoscaling for SLA adherence\n\n## Code Example\n```javascript\n// Minimal per-tenant policy model\ntype TenantPolicy = {\n  tenantId: string;\n  tier: 'tiny'|'large';\n  latencyTargetMs: number;\n  maxInFlight: number;\n}\n```\n\n## Follow-up Questions\n- How would you test fairness during bursts?\n- How would you extend to add a new model tier with different pricing?","diagram":"flowchart TD\n  A[Tenant Request] --> B[Tier Router]\n  B --> C[Small Pool]\n  B --> D[Large Pool]\n  C --> E[Inference]\n  D --> E","difficulty":"advanced","tags":["llm-ops"],"channel":"llm-ops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","MongoDB","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T13:13:36.569Z","createdAt":"2026-01-15T13:13:36.569Z"},{"id":"q-2407","question":"Design a per-brand, hot-swappable steering layer between gateway and models. JSON-based policies enforce safety and tone per brand; compute a per-prompt risk score in under 20 ms; policy versions are immutable with a blue/green rollout for rollback; describe data models, policy evaluation flow, rollback strategy, and a minimal Rust/WASM prototype for policy evaluation?","answer":"Design a per-brand, hot-swappable steering layer between gateway and models. JSON-based policies enforce safety and tone per brand; compute a per-prompt risk score in under 20 ms; policy versions are ","explanation":"## Why This Is Asked\n\nAssess ability to implement low-latency, per-tenant policy gating with live hot-swaps, immutability guarantees, and rollback strategies in a production LLM-ops gateway.\n\n## Key Concepts\n\n- Per-brand policy isolation and versioning\n- Immutable policy versions with blue/green rollout\n- Fast policy evaluation (sub-20 ms) and robust auditing\n- WASM/Rust prototype for safe, sandboxed evaluation\n\n## Code Example\n\n```rust\n// Minimal WASM-friendly policy evaluator API\npub fn eval(input: &str, policy_json: &str) -> bool {\n    // naive policy: blocklist example\n    if policy_json.contains(\"blocked\") {\n        return !input.contains(\"blocked\");\n    }\n    true\n}\n```\n\n## Follow-up Questions\n\n- How would you monitor policy drift and trigger automatic rollbacks?\n- What metrics would you collect to ensure low false negatives with high safety?","diagram":null,"difficulty":"advanced","tags":["llm-ops"],"channel":"llm-ops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Instacart","Lyft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T16:57:02.661Z","createdAt":"2026-01-15T16:57:02.662Z"},{"id":"q-467","question":"You're deploying a LLM inference service that must handle 10,000 RPS with <100ms latency. How would you design the architecture to balance cost, performance, and reliability?","answer":"I'd design a horizontally scalable architecture with GPU instances behind a load balancer, using request batching and model parallelism to handle 10,000 RPS while maintaining <100ms latency, with Redis caching and auto-scaling to optimize cost and reliability.","explanation":"## Architecture Design\n- **GPU Autoscaling**: Scale based on GPU utilization and request queue depth\n- **Request Batching**: Group similar requests to maximize GPU throughput\n- **Model Parallelism**: Split large models across multiple GPU instances\n- **Caching Layer**: Redis for prompt/response caching with TTL\n- **Load Balancing**: Health checks and circuit breakers for reliability\n\n## Performance Optimization\n```python\n# Request batching implementation\nbatch_size = 32\nmax_wait_time = 50  # ms\n\nasync def process_batch(requests):\n    # Batch inference logic\n    return await model.generate_batch(requests)\n```\n\n## Monitoring & Scaling\n- **Metrics**: GPU utilization, request latency, queue length, error rates\n- **Scaling Triggers**: GPU >80%, queue >1000, latency >100ms\n- **Cost Optimization**: Spot instances for non-critical workloads","diagram":"flowchart TD\n  A[Client Request] --> B[Load Balancer]\n  B --> C{Cache Hit?}\n  C -->|Yes| D[Return Cached]\n  C -->|No| E[Request Queue]\n  E --> F[Batch Processor]\n  F --> G[GPU Cluster]\n  G --> H[Response Cache]\n  H --> I[Client Response]\n  J[Monitoring] --> K[Autoscaler]\n  K --> G","difficulty":"intermediate","tags":["llm-ops"],"channel":"llm-ops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Microsoft","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":["gpu autoscaling","request batching","model parallelism","redis caching","load balancer","latency"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2026-01-06T04:04:29.761Z","createdAt":"2025-12-26 12:51:05"},{"id":"q-497","question":"How would you design a distributed inference serving system for LLMs that handles 100K RPS with sub-100ms latency while managing GPU memory fragmentation and ensuring high availability?","answer":"Implement a multi-tier architecture with intelligent request routing, model parallelism, and dynamic batching. Utilize GPU memory pooling, KV cache optimization, and auto-scaling with comprehensive health checks. Deploy across multiple availability zones for high availability.","explanation":"## Architecture\n- **Request Router**: Load balancer with health checks and circuit breakers\n- **Inference Nodes**: GPU pods with model parallelism and dynamic batching\n- **Cache Layer**: Redis for KV cache and model weights\n- **Monitoring**: Prometheus metrics for latency, throughput, and GPU utilization\n\n## Key Components\n- **Memory Management**: GPU memory pooling with fragmentation mitigation\n- **Auto-scaling**: Horizontal pod autoscaling based on queue depth\n- **Fault Tolerance**: Multi-AZ deployment with failover mechanisms\n\n## Performance Optimizations\n- **Batch Processing**: Dynamic batching for optimal GPU utilization","diagram":"flowchart TD\n  A[Client Request] --> B[Load Balancer]\n  B --> C[Request Router]\n  C --> D[Inference Node 1]\n  C --> E[Inference Node 2]\n  C --> F[Inference Node N]\n  D --> G[GPU Memory Pool]\n  E --> H[GPU Memory Pool]\n  F --> I[GPU Memory Pool]\n  G --> J[KV Cache]\n  H --> K[KV Cache]\n  I --> L[KV Cache]\n  J --> M[Response]\n  K --> M\n  L --> M","difficulty":"advanced","tags":["llm-ops"],"channel":"llm-ops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-08T11:59:23.877Z","createdAt":"2025-12-26 12:51:06"},{"id":"q-528","question":"You're deploying a LLM inference service that must handle 1000 concurrent requests with <500ms latency. Your current setup uses a single GPU with vLLM. How would you architect the system to meet these requirements?","answer":"Implement horizontal scaling with a load balancer and GPU auto-scaling. Leverage vLLM's tensor parallelism across multiple GPUs, configure max_batch_size=32, enable continuous batching, and add Redis for request queuing.","explanation":"## Architecture\n- **Load Balancer**: NGINX with round-robin request distribution\n- **Inference Nodes**: Multiple vLLM instances utilizing tensor parallelism\n- **Queue System**: Redis for request buffering during traffic spikes\n- **Monitoring**: Prometheus + Grafana for comprehensive GPU metrics\n\n## Key Optimizations\n- **Continuous Batching**: Process requests as they arrive to minimize latency\n- **Tensor Parallelism**: Distribute model computation across multiple GPUs\n- **Auto-scaling**: Kubernetes HPA based on GPU utilization metrics\n- **Cache Strategy**: KV cache reuse for similar prompts to improve throughput\n\n## Trade-offs\n- **Cost vs Latency**: Additional GPUs reduce latency but increase operational costs\n- **Complexity**: Horizontal scaling introduces architectural overhead but ensures scalability","diagram":"flowchart TD\n  A[Client Request] --> B[Load Balancer]\n  B --> C[Redis Queue]\n  C --> D[vLLM Node 1]\n  C --> E[vLLM Node 2]\n  C --> F[vLLM Node N]\n  D --> G[GPU Pool 1]\n  E --> H[GPU Pool 2]\n  F --> I[GPU Pool N]\n  G --> J[Response]\n  H --> J\n  I --> J\n  K[Monitoring] --> L[Auto-scaler]\n  L --> C","difficulty":"intermediate","tags":["llm-ops"],"channel":"llm-ops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Hugging Face","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":["horizontal scaling","load balancer","gpu auto-scaling","vllm","tensor parallelism","continuous batching","redis","request queuing"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2026-01-09T08:42:48.273Z","createdAt":"2025-12-26 12:51:06"},{"id":"q-554","question":"You're deploying a multi-tenant LLM inference service that must handle 10,000 concurrent requests with sub-100ms latency. How would you design the request routing, model loading strategy, and autoscaling to meet these SLAs while optimizing GPU utilization?","answer":"Implement request batching with dynamic batch sizes, use TensorRT-LLM with model parallelism across GPU clusters, employ two-level routing with fast-path for cached embeddings, and leverage KEDA autoscaling with GPU utilization and queue length metrics while adding cost optimization through spot instance mixing and model quantization.","explanation":"## Architecture\n- **Request Router**: NGINX + custom Go service for request classification and load balancing\n- **Model Loading**: TensorRT-LLM with continuous batching, in-flight batching, and model parallelism\n- **Autoscaling**: KEDA with GPU utilization (80% threshold) and queue length (>100 requests) metrics\n\n## Key Strategies\n- **Model Caching**: Hot models pre-loaded on dedicated GPU nodes, cold models on-demand loading with 30s SLA\n- **Batch Optimization**: Dynamic batching (1-32 requests) based on real-time latency requirements and queue depth\n- **Resource Management**: GPU sharing with MPS for smaller models (<7B parameters), dedicated GPUs for larger models\n\n## Monitoring & Observability\n- **SLA Tracking**: Prometheus metrics for P99 latency <100ms, error rate <0.1%, and throughput monitoring\n- **GPU Metrics**: NVIDIA DCGM exporter for utilization, memory usage, and temperature\n- **Business Metrics**: Request classification accuracy, cache hit rates (>85%), and cost per token\n\n## Cost Optimization\n- **Instance Strategy**: 70% on-demand + 30% spot instances with automatic failover\n- **Model Optimization**: INT8 quantization for 40% memory reduction with minimal accuracy loss\n- **Scaling Policies**: Right-sizing based on daily traffic patterns (peak/off-peak GPU allocation)\n- **Caching Strategy**: Redis cluster for frequent prompts, reducing inference costs by 25%","diagram":"flowchart TD\n  A[Client Request] --> B[Load Balancer]\n  B --> C[Request Router]\n  C --> D{Model Cached?}\n  D -->|Yes| E[Fast Path GPU Pool]\n  D -->|No| F[Cold Start Queue]\n  E --> G[TensorRT-LLM Engine]\n  F --> H[Model Loader]\n  H --> G\n  G --> I[Batch Processor]\n  I --> J[Response Aggregator]\n  J --> K[Client Response]\n  L[Monitor] --> M[Autoscaler]\n  M --> N[GPU Pool Scaling]","difficulty":"advanced","tags":["llm-ops"],"channel":"llm-ops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","IBM"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-28T02:18:53.349Z","createdAt":"2025-12-26 12:51:07"},{"id":"q-581","question":"How would you design a production LLM inference pipeline that handles 10K RPS with sub-200ms latency while managing GPU memory fragmentation and cold start issues?","answer":"Implement a multi-tier architecture with request batching, model parallelism, and GPU memory pooling. Use NVIDIA Triton for model serving with dynamic batching, implement KV cache optimization, and set up pre-warmed GPU instances with memory pre-allocation to eliminate cold starts.","explanation":"## Architecture Overview\n- **Request Layer**: Load balancer with intelligent request queuing and dynamic batching\n- **Inference Layer**: GPU clusters with distributed model parallelization\n- **Optimization Layer**: Advanced KV cache management and GPU memory pooling\n\n## Key Components\n- **Dynamic Batching**: Aggregate requests in real-time to maximize GPU utilization\n- **Model Parallelism**: Distribute large models across multiple GPU instances\n- **Memory Management**: Pre-allocated GPU memory pools to prevent fragmentation\n\n## Performance Strategies\n- **Warm Standby**: Maintain pre-loaded models to eliminate cold start latency\n- **Request Routing**: Intelligent load distribution based on GPU availability and model complexity","diagram":"flowchart TD\n  A[Load Balancer] --> B[Request Queue]\n  B --> C[Dynamic Batching]\n  C --> D[GPU Cluster 1]\n  C --> E[GPU Cluster 2]\n  D --> F[KV Cache Manager]\n  E --> G[KV Cache Manager]\n  F --> H[Response Aggregator]\n  G --> H\n  H --> I[Client]","difficulty":"advanced","tags":["llm-ops"],"channel":"llm-ops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-08T11:52:10.090Z","createdAt":"2025-12-27T01:13:47.367Z"},{"id":"q-849","question":"You operate a dual-tenant LLM inference service for sensitive internal docs and external users. Design a policy-driven routing and isolation architecture that guarantees tenant data separation, per-tenant model pools, on-the-fly prompt sanitization, and strict latency budgets under burst traffic. Include observability, data handling, and fail-open/closed strategies?","answer":"Propose a per-tenant policy engine with tenant-scoped pools, prompt sanitization, and data isolation via separate worker processes and memory arenas. Route to a sanitized pipeline or full model based ","explanation":"## Why This Is Asked\nInterview context explanation.\n\n## Key Concepts\n\n- Policy engine\n- Tenant isolation\n- Prompt sanitization\n- Confidential computing\n- SLA management\n- Observability\n- Audit\n\n## Code Example\n\n```javascript\n// Pseudo routing kernel\nfunction routeRequest(req, policyStore, pools) {\n  const tenant = req.tenant\n  const policy = policyStore[tenant]\n  const path = policy.requiresSanitization ? 'sanitized' : 'full'\n  return pools[path].assign(req)\n}\n```\n\n## Follow-up Questions\n\n- How would you test data isolation and sanitization coverage? How would you ensure SLA adherence under burst traffic?","diagram":"flowchart TD\n  A[Client] --> B[Policy Engine]\n  B --> C{Tenant Policy}\n  C -- External/Sanitized --> D[Sanitized Path]\n  C -- Internal/Full --> E[Full Path]\n  D --> F[Model Sandbox]\n  E --> F\n  F --> G[Return]","difficulty":"advanced","tags":["llm-ops"],"channel":"llm-ops","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Google","Instacart"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T13:30:46.947Z","createdAt":"2026-01-12T13:30:46.947Z"},{"id":"q-252","question":"What are the key techniques and trade-offs for optimizing large language models in production, including quantization strategies and their impact on performance?","answer":"Production optimization combines quantization (8-bit/4-bit), pruning, and distillation. Static quantization offers 2-4x speedup with minimal accuracy loss (<2%), while dynamic quantization provides better compatibility but higher latency. Quantization-aware training preserves accuracy for sub-8-bit models, and GPTQ/AWQ achieve 3-5x memory reduction with careful calibration.","explanation":"## Model Optimization Strategies\n\n**Quantization Methods:**\n- **Static (Post-Training):** Convert weights offline, ideal for deployment consistency\n- **Dynamic:** Runtime quantization, better hardware compatibility, higher latency\n- **Quantization-Aware Training (QAT):** Simulate quantization during training, preserve accuracy\n- **GPTQ/AWQ:** Advanced algorithms for extreme compression (4-bit, 3-bit)\n\n**Performance Trade-offs:**\n- **Memory:** 4-bit quantization reduces memory by 8x vs FP16\n- **Speed:** INT8 inference 2-4x faster on optimized hardware\n- **Accuracy:** LLMs typically tolerate 8-bit with <1% degradation\n\n**Implementation Examples:**\n```python\n# BitsAndBytes 4-bit quantization\nfrom transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"meta-llama/Llama-2-7b\",\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.float16\n)\n```\n\n**Production Considerations:**\n- **Hardware Support:** NVIDIA GPUs with Tensor Cores, Apple Neural Engine\n- **Batch Size Impact:** Larger batches improve quantization efficiency\n- **Calibration Data:** Representative dataset crucial for static quantization\n- **Mixed Precision:** Combine FP16 attention with INT8 matrices\n\n**Edge Cases:**\n- Small models (<1B params) may not benefit from aggressive quantization\n- MoE models require careful per-expert quantization\n- Training vs inference: different optimization strategies needed","diagram":"graph TD\n    A[Original Model<br/>32-bit Weights] --> B[Quantization Process]\n    B --> C[Quantized Model<br/>8-bit Weights]\n    B --> D[Scale Factors]\n    B --> E[Zero Points]\n    C --> F[Smaller Memory Footprint]\n    C --> G[Faster Inference]\n    D --> H[Dequantization Layer]\n    E --> H\n    H --> I[Original Precision Output]","difficulty":"beginner","tags":["quantization","pruning","distillation"],"channel":"llm-ops","subChannel":"optimization","sourceUrl":null,"videos":null,"companies":["Amazon","Apple","Google","Meta","Microsoft","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-26T16:38:38.111Z","createdAt":"2025-12-26 12:51:07"},{"id":"q-422","question":"You're building a production LLM service handling 10K requests/second with transformer models experiencing memory spikes during multi-head attention. How would you optimize memory usage while maintaining throughput and latency requirements?","answer":"Implement flash attention-2 with memory-efficient attention computation, use 8-bit quantization for weights and KV cache, apply dynamic batching with padding optimization, and implement gradient checkpointing. Configure tensor parallelism across GPUs with memory pooling strategies, use paged attention for KV cache management, and set up continuous batching with request scheduling to maintain 50ms P99 latency.","explanation":"## Memory Optimization Techniques\n\n**Flash Attention-2** reduces memory from O(N²) to O(N) by computing attention in blocks without materializing the full attention matrix. For sequence length 4096, this cuts memory usage from ~67GB to ~2GB.\n\n**Quantization Strategy**: Use 8-bit quantization for model weights (4x memory reduction) and 4-bit for KV cache. Implement mixed-precision with FP16 for attention scores and INT8 for weights.\n\n## Architecture Patterns\n\n**Tensor Parallelism**: Split attention heads across GPUs. For 96-head model on 8 GPUs, each handles 12 heads, reducing per-GPU memory by 8x.\n\n**Paged Attention**: Implement KV cache with 2KB pages, allowing selective eviction and reducing fragmentation by 30-40%.\n\n## Performance Optimization\n\n**Dynamic Batching**: Group requests with similar sequence lengths. Use padding masks and implement continuous batching to achieve 85% GPU utilization.\n\n**Memory Pooling**: Pre-allocate memory pools for tensors, reducing allocation overhead by 60% and preventing memory spikes.\n\n## Real-world Implementation\n\n```python\n# Flash Attention with memory pooling\nclass OptimizedAttention(nn.Module):\n    def __init__(self, heads, dim):\n        self.flash_attn = FlashAttention2()\n        self.kv_cache = PagedKVCache(page_size=2048)\n    \n    def forward(self, x):\n        return self.flash_attn(\n            x, use_memory_pool=True,\n            kv_cache=self.kv_cache\n        )\n```\n\n**Monitoring**: Track GPU memory utilization, attention computation time, and cache hit rates. Set alerts for memory usage >80% to trigger auto-scaling.","diagram":"flowchart TD\n  A[Client Request] --> B[Load Balancer]\n  B --> C{Sequence Length}\n  C -->|Short| D[Flash Attention]\n  C -->|Long| E[Grouped Query Attention]\n  D --> F[Head Pruning]\n  E --> F\n  F --> G[KV Cache Check]\n  G -->|Hit| H[Direct Output]\n  G -->|Miss| I[Trie Tokenizer]\n  I --> J[Batch Processing]\n  J --> K[Memory Pool]\n  K --> L[GPU Compute]\n  L --> M[Response]","difficulty":"advanced","tags":["transformer","attention","tokenization"],"channel":"llm-ops","subChannel":"optimization","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-24T06:28:14.733Z","createdAt":"2025-12-26 12:51:05"}],"subChannels":["deployment","general","optimization"],"companies":["Adobe","Airbnb","Amazon","Apple","Cloudflare","Databricks","Discord","DoorDash","Goldman Sachs","Google","Hugging Face","IBM","Instacart","LinkedIn","Lyft","Meta","Microsoft","MongoDB","NVIDIA","Netflix","Oracle","PayPal","Plaid","Robinhood","Salesforce","Slack","Snap","Square","Tesla","Zoom"],"stats":{"total":23,"beginner":6,"intermediate":6,"advanced":11,"newThisWeek":15}}