{"questions":[{"id":"aws-ml-specialty-modeling-1768148573552-0","question":"A retailer uses AWS SageMaker to train a regression model to forecast daily product demand. To evaluate model performance, you want a metric that heavily penalizes large errors and aligns with business impact on inventory decisions. Which metric should you choose?","answer":"[{\"id\":\"a\",\"text\":\"MAE\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"RMSE\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"R^2\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"MAPE\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct answer is **RMSE** because it amplifies larger errors, which more closely reflects the cost of forecasting errors in inventory planning.\n\n## Why Other Options Are Wrong\n- **MAE** is less sensitive to large errors and may underrepresent the cost of big stock-forecast mismatches.\n- **MAPE** can be undefined when actual demand is zero and can bias evaluation when scale varies across items.\n- **R^2** measures explained variance rather than absolute forecast error magnitude, making it a poor proxy for forecasting accuracy in operational settings.\n\n## Key Concepts\n- Regression evaluation metrics\n- RMSE vs MAE and R^2\n- Aligning metrics with business costs\n\n## Real-World Application\nIn production, compare model revisions on RMSE over a holdout period, translate RMSE differences into inventory impact (stockouts/overstock), and pick the version with the lowest RMSE that also meets business constraints.","diagram":null,"difficulty":"intermediate","tags":["SageMaker","Modeling","Kubernetes","Terraform","certification-mcq","domain-weight-36"],"channel":"aws-ml-specialty","subChannel":"modeling","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T16:22:53.553Z","createdAt":"2026-01-11 16:22:53"},{"id":"aws-ml-specialty-modeling-1768148573552-1","question":"In a fraud detection model with highly imbalanced classes, you want to evaluate model performance in a way that emphasizes catching positives and remains informative under imbalance. Which metric is most appropriate to monitor for threshold-insensitive performance on the positive class?","answer":"[{\"id\":\"a\",\"text\":\"Accuracy\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"F1-Score\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"ROC-AUC\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"PR-AUC\",\"isCorrect\":true}]","explanation":"## Correct Answer\nThe correct answer is **PR-AUC** because precision-recall curves are more informative than ROC in highly imbalanced settings and PR-AUC focuses on the positive class performance across thresholds.\n\n## Why Other Options Are Wrong\n- **Accuracy** can be misleading when the positive class is rare.\n- **F1-Score** is threshold-dependent and may not reflect performance across all decision thresholds.\n- **ROC-AUC** can be overly optimistic on imbalanced data since it considers true negative rate as well.\n\n## Key Concepts\n- Class imbalance handling in evaluation\n- Precision-recall vs ROC curves\n- Threshold-insensitive vs threshold-dependent metrics\n\n## Real-World Application\nIn production, monitor PR-AUC on a holdout fraud dataset and tune the decision threshold based on business costs of false positives and false negatives.","diagram":null,"difficulty":"intermediate","tags":["SageMaker","Modeling","EKS","Terraform","certification-mcq","domain-weight-36"],"channel":"aws-ml-specialty","subChannel":"modeling","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T16:22:53.899Z","createdAt":"2026-01-11 16:22:54"},{"id":"aws-ml-specialty-modeling-1768148573552-2","question":"You deploy a model as a SageMaker real-time endpoint to serve predictions with variable traffic. To balance latency and cost, you want to automatically scale the endpoint, not scale to zero. Which approach is correct?","answer":"[{\"id\":\"a\",\"text\":\"Configure Application Auto Scaling on the SageMaker endpoint with a target tracking policy using InvocationsPerInstance, setting min=1 and max=20.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Keep a single instance and rely on manual scaling.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Configure the endpoint to scale to zero during idle periods.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use Batch Transform for all inferences.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption **A** is correct: use Application Auto Scaling with a target tracking policy based on InvocationsPerInstance, with a sensible min and max, to maintain latency while controlling costs. Real-time endpoints cannot scale to zero.\n\n## Why Other Options Are Wrong\n- **B**: Manual scaling is not responsive to traffic variation and defeats auto-scaling benefits.\n- **C**: Real-time endpoints do not support scale-to-zero; at least one instance is required.\n- **D**: Batch Transform is not suitable for real-time low-latency inferences.\n\n## Key Concepts\n- SageMaker real-time endpoints vs batch transform\n- Endpoint auto-scaling and InvocationsPerInstance metric\n- Min/Max capacity sizing and latency considerations\n\n## Real-World Application\nImplement a scalable real-time endpoint by enabling auto-scaling policies that react to traffic, ensuring consistent latency during peak hours while avoiding over-provisioning during off-peak times.","diagram":null,"difficulty":"intermediate","tags":["SageMaker","Kubernetes","Terraform","EKS","certification-mcq","domain-weight-36"],"channel":"aws-ml-specialty","subChannel":"modeling","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T16:22:54.249Z","createdAt":"2026-01-11 16:22:54"}],"subChannels":["modeling"],"companies":[],"stats":{"total":3,"beginner":0,"intermediate":3,"advanced":0,"newThisWeek":3}}