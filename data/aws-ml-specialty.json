{"questions":[{"id":"aws-ml-specialty-data-engineering-1768216892678-0","question":"To ensure reproducible ML training datasets when ingesting batch and streaming data into an S3-based data lake, which approach best provides dataset versioning and provenance?","answer":"[{\"id\":\"a\",\"text\":\"Use separate S3 prefixes for each dataset version (e.g., s3://bucket/data/v1, v2) and maintain Glue Data Catalog partitions aligned to versions.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Use a single prefix and rely on file metadata timestamps for versioning.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Store a version number in DynamoDB and generate synthetic copies.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Encrypt data with a KMS key; encryption provides versioning.\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nA. Versioned S3 prefixes with Glue Catalog partitions provide immutable datasets and clear provenance for training runs, enabling reproducible pipelines by pinning to a specific version.\n\n## Why Other Options Are Wrong\n\n- B: Relying on file timestamps does not guarantee reproducibility.\n- C: Storing a version in DynamoDB does not version the dataset or ensure lineage.\n- D: Encryption with KMS does not provide versioning or provenance.\n\n## Key Concepts\n\n- Data versioning in S3 and Glue partitions\n- Data provenance and reproducibility in ML pipelines\n- Data Catalog synchronization with versioned data\n\n## Real-World Application\n\n- In production, pin pipeline inputs to a specific version path (e.g., v3) to guarantee identical training data across runs and enable audits.","diagram":null,"difficulty":"intermediate","tags":["AWS","S3","Glue","Data Lake","Data Catalog","ML Pro provenance","certification-mcq","domain-weight-20"],"channel":"aws-ml-specialty","subChannel":"data-engineering","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T11:21:32.679Z","createdAt":"2026-01-12 11:21:32"},{"id":"aws-ml-specialty-data-engineering-1768216892678-1","question":"In a SageMaker ML pipeline, you want to automatically detect data quality issues in incoming training data and fail the pipeline if quality falls below a threshold?","answer":"[{\"id\":\"a\",\"text\":\"Add a SageMaker Processing step that runs a data quality script (e.g., checks for missing values, distribution checks) and returns a pass/fail flag.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Rely on the ingestion step to proceed regardless of data quality issues.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Visualize data quality in QuickSight and manually approve before training.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Trigger a Lambda job to post a notification but do not block training.\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nA. A SageMaker Processing step that runs a data quality script and returns a pass/fail aligns with automated quality gating in the pipeline.\n\n## Why Other Options Are Wrong\n\n- B allows bad data to proceed, defeating automated quality control.\n- C is a visualization tool; it does not automatically fail or block pipeline steps.\n- D only notifies and does not prevent a training run with poor data quality.\n\n## Key Concepts\n\n- SageMaker Pipelines\n- Processing steps for data validation\n- Automated gating based on quality metrics\n\n## Real-World Application\n\n- Integrate with Step Functions or SageMaker Pipeline conditional logic to automatically stop and alert when data quality thresholds are breached.","diagram":null,"difficulty":"intermediate","tags":["AWS","SageMaker","SageMaker Pipelines","Data Quality","Processing","certification-mcq","domain-weight-20"],"channel":"aws-ml-specialty","subChannel":"data-engineering","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T11:21:33.054Z","createdAt":"2026-01-12 11:21:33"},{"id":"aws-ml-specialty-data-engineering-1768216892678-2","question":"You must mask PII in training data before ingestion into modeling while preserving data relationships and enabling auditability. Which AWS capability best supports this requirement?","answer":"[{\"id\":\"a\",\"text\":\"Use AWS Lake Formation column-level permissions and data masking to redact PII during access for training jobs.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Encrypt data at rest using SSE-KMS; encryption hides content but does not mask or preserve relationships.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Rely on IAM role restrictions to prevent access to PII data during training.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Train on raw data in SageMaker without masking; gives full data access to model trainer.\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nA. Lake Formation column-level permissions and data masking enable masking of PII during access for training while preserving data relationships for audits.\n\n## Why Other Options Are Wrong\n\n- B Encryption hides content but does not mask data for analytic relationships or auditing.\n- C IAM restrictions help control access but do not perform data masking for training workflows.\n- D Training on raw data violates masking and auditability requirements.\n\n## Key Concepts\n\n- Lake Formation masking and column-level security\n- Data masking vs encryption\n- Auditing data access in ML pipelines\n\n## Real-World Application\n\n- Implement masking policies in Lake Formation, then run SageMaker training jobs that read masked data, enabling compliance reporting.","diagram":null,"difficulty":"intermediate","tags":["AWS","Lake Formation","Data Masking","Compliance","SageMaker","certification-mcq","domain-weight-20"],"channel":"aws-ml-specialty","subChannel":"data-engineering","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T11:21:33.442Z","createdAt":"2026-01-12 11:21:33"},{"id":"aws-ml-specialty-exploratory-analysis-1768181482134-0","question":"Given a large CSV dataset stored in S3 with daily partitions, you want to compute descriptive statistics (count, min, max, mean, percentiles) without loading all data into memory. Which approach is most appropriate in AWS for scalable EDA?","answer":"[{\"id\":\"a\",\"text\":\"Use Amazon QuickSight to automatically generate statistics from the data in S3\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Load the entire dataset into a SageMaker notebook and compute statistics there\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use AWS Athena with SQL aggregate functions including approx_percentile to compute statistics directly on S3 data\",\"isCorrect\":true},{\"id\":\"d\",\"text\":\"Use AWS Glue Elastic Views to materialize precomputed summaries on S3\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption C is correct because Athena lets you run serverless SQL queries directly on S3 data, enabling descriptive statistics without loading data into memory. Using functions like AVG, MIN, MAX, COUNT, and APPROX_PERCENTILE allows scalable EDA over partitioned datasets.\n\n## Why Other Options Are Wrong\n- Option A is incorrect because QuickSight is a BI visualization tool and not ideal for computing granular descriptive statistics across raw partitions without pre-aggregation.\n- Option B is incorrect due to memory and compute constraints when loading the full dataset into a notebook for large datasets.\n- Option D is incorrect as Glue Elastic Views focuses on virtualized data views rather than on-demand statistical summaries for EDA.\n\n## Key Concepts\n- Serverless analytics with Athena on S3\n- Descriptive statistics: count, min, max, mean, percentiles\n- Approximate percentile functions for scalable quantiles\n- Partitioned data handling in S3\n\n## Real-World Application\nData scientists can perform quick, scalable EDA by querying partitioned data in S3 with Athena, obtaining summary statistics without a heavy data movement or in-memory processing step, enabling faster iterations before deeper modeling.","diagram":null,"difficulty":"intermediate","tags":["AmazonS3","AmazonAthena","AWSGlue","certification-mcq","domain-weight-24"],"channel":"aws-ml-specialty","subChannel":"exploratory-analysis","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T01:31:22.136Z","createdAt":"2026-01-12 01:31:22"},{"id":"aws-ml-specialty-exploratory-analysis-1768181482134-1","question":"During initial data profiling for a dataset in S3, you need automated profiling metrics including missing values, data types, and unique values per column. Which AWS service best provides this with minimal setup?","answer":"[{\"id\":\"a\",\"text\":\"Use AWS Glue DataBrew Data Profiling to automatically generate data quality metrics\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Use Amazon Athena to manually query missing value counts per column\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use Amazon QuickSight to automatically detect missing values in visuals\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Load into a SageMaker notebook and inspect for missing values manually\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A is correct because AWS Glue DataBrew includes a profiling feature that automatically reports missing values, data types, distributions, and other quality metrics with minimal setup.\n\n## Why Other Options Are Wrong\n- Option B is incorrect as Athena would require writing multiple queries per column to gather missing value counts, which is manual and slower for large schemas.\n- Option C is incorrect because QuickSight focuses on visualization and discovery, not automated per-column profiling metrics.\n- Option D is incorrect due to manual effort and lack of automated profiling features.\n\n## Key Concepts\n- Data profiling and quality metrics\n- AWS Glue DataBrew profiler capabilities\n- Schema and distribution basics for EDA readiness\n\n## Real-World Application\nBefore modeling, a data engineer profiles the dataset to understand data quality, guiding cleaning steps and ensuring columns with high missingness or inconsistent types are handled appropriately.","diagram":null,"difficulty":"intermediate","tags":["AWSGlue","DataBrew","AmazonS3","certification-mcq","domain-weight-24"],"channel":"aws-ml-specialty","subChannel":"exploratory-analysis","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T01:31:22.615Z","createdAt":"2026-01-12 01:31:22"},{"id":"aws-ml-specialty-exploratory-analysis-1768181482134-2","question":"You have a large Parquet dataset in S3 with hundreds of numeric features. You want to compute a full pairwise correlation matrix efficiently for downstream modeling. Which approach is most scalable on AWS?","answer":"[{\"id\":\"a\",\"text\":\"Use Amazon Athena to compute pairwise correlations across all feature pairs using the corr function\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Load the data into a SageMaker notebook and compute correlations with pandas\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use Amazon EMR with Spark to compute a correlation matrix in distributed fashion\",\"isCorrect\":true},{\"id\":\"d\",\"text\":\"Use Amazon QuickSight to automatically generate a correlation matrix\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption C is correct because a distributed Spark job on EMR scales to hundreds of numeric features and large datasets, allowing efficient computation of a full correlation matrix across features.\n\n## Why Other Options Are Wrong\n- Option A is less reliable for large feature sets due to potential combinatorial explosion and performance constraints in SQL-native corr across all pairs.\n- Option B would require loading massive data into a notebook, which is impractical for memory and compute limits.\n- Option D is not designed to compute a full correlation matrix; QuickSight focuses on visualization and analytics, not distributed computation of all pairwise correlations.\n\n## Key Concepts\n- Distributed computation of statistics\n- Spark DataFrame corr and matrix operations\n- Scalability with EMR clusters\n\n## Real-World Application\nFor projects with high-dimensional feature spaces, a Spark-based correlation computation on EMR provides performance and scalability advantages, enabling timely EDA insights for feature selection and model planning.","diagram":null,"difficulty":"intermediate","tags":["AmazonS3","AmazonEMR","ApacheSpark","certification-mcq","domain-weight-24"],"channel":"aws-ml-specialty","subChannel":"exploratory-analysis","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T01:31:23.110Z","createdAt":"2026-01-12 01:31:23"},{"id":"q-876","question":"You're deploying a SageMaker real-time endpoint for a model expected to see bursty, unpredictable traffic. Propose a concrete autoscaling setup using AWS Application Auto Scaling that keeps latency under a target while never scaling to zero. Specify min and max instances, the metric and target value (latency or invocations), the policy type, and cooldowns; discuss validation steps?","answer":"Configure a target-tracking policy on the endpoint with min 1, max 20 instances. Use SageMakerEndpointLatency (p95) as the predefined metric with target value 0.25s; set ScaleOutCooldown 300s and Scal","explanation":"## Why This Is Asked\n\nAssesses practical autoscaling setup for real-time endpoints, focusing on latency control, non-zero minimum, and stable scaling behavior.\n\n## Key Concepts\n\n- SageMaker real-time endpoints\n- AWS Application Auto Scaling\n- Target tracking vs step scaling\n- Latency vs concurrency metrics\n- Cooldown and stability\n\n## Code Example\n\n```javascript\n{\n  \"PolicyName\": \"EndpointLatencyTargetTracking\",\n  \"PolicyType\": \"TargetTrackingScaling\",\n  \"TargetTrackingScalingPolicyConfiguration\": {\n    \"TargetValue\": 0.25,\n    \"PredefinedMetricSpecification\": {\n      \"PredefinedMetricType\": \"SageMakerEndpointLatency\"\n    },\n    \"ScaleOutCooldown\": 300,\n    \"ScaleInCooldown\": 600\n  }\n}\n```\n\n## Follow-up Questions\n\n- How would you validate the setup under burst traffic?\n- How would you prevent over-scaling in steady-state?","diagram":null,"difficulty":"beginner","tags":["aws-ml-specialty"],"channel":"aws-ml-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Lyft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:56:45.598Z","createdAt":"2026-01-12T13:56:45.598Z"},{"id":"aws-ml-specialty-modeling-1768148573552-0","question":"A retailer uses AWS SageMaker to train a regression model to forecast daily product demand. To evaluate model performance, you want a metric that heavily penalizes large errors and aligns with business impact on inventory decisions. Which metric should you choose?","answer":"[{\"id\":\"a\",\"text\":\"MAE\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"RMSE\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"R^2\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"MAPE\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct answer is **RMSE** because it amplifies larger errors, which more closely reflects the cost of forecasting errors in inventory planning.\n\n## Why Other Options Are Wrong\n- **MAE** is less sensitive to large errors and may underrepresent the cost of big stock-forecast mismatches.\n- **MAPE** can be undefined when actual demand is zero and can bias evaluation when scale varies across items.\n- **R^2** measures explained variance rather than absolute forecast error magnitude, making it a poor proxy for forecasting accuracy in operational settings.\n\n## Key Concepts\n- Regression evaluation metrics\n- RMSE vs MAE and R^2\n- Aligning metrics with business costs\n\n## Real-World Application\nIn production, compare model revisions on RMSE over a holdout period, translate RMSE differences into inventory impact (stockouts/overstock), and pick the version with the lowest RMSE that also meets business constraints.","diagram":null,"difficulty":"intermediate","tags":["SageMaker","Modeling","Kubernetes","Terraform","certification-mcq","domain-weight-36"],"channel":"aws-ml-specialty","subChannel":"modeling","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T16:22:53.553Z","createdAt":"2026-01-11 16:22:53"},{"id":"aws-ml-specialty-modeling-1768148573552-1","question":"In a fraud detection model with highly imbalanced classes, you want to evaluate model performance in a way that emphasizes catching positives and remains informative under imbalance. Which metric is most appropriate to monitor for threshold-insensitive performance on the positive class?","answer":"[{\"id\":\"a\",\"text\":\"Accuracy\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"F1-Score\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"ROC-AUC\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"PR-AUC\",\"isCorrect\":true}]","explanation":"## Correct Answer\nThe correct answer is **PR-AUC** because precision-recall curves are more informative than ROC in highly imbalanced settings and PR-AUC focuses on the positive class performance across thresholds.\n\n## Why Other Options Are Wrong\n- **Accuracy** can be misleading when the positive class is rare.\n- **F1-Score** is threshold-dependent and may not reflect performance across all decision thresholds.\n- **ROC-AUC** can be overly optimistic on imbalanced data since it considers true negative rate as well.\n\n## Key Concepts\n- Class imbalance handling in evaluation\n- Precision-recall vs ROC curves\n- Threshold-insensitive vs threshold-dependent metrics\n\n## Real-World Application\nIn production, monitor PR-AUC on a holdout fraud dataset and tune the decision threshold based on business costs of false positives and false negatives.","diagram":null,"difficulty":"intermediate","tags":["SageMaker","Modeling","EKS","Terraform","certification-mcq","domain-weight-36"],"channel":"aws-ml-specialty","subChannel":"modeling","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T16:22:53.899Z","createdAt":"2026-01-11 16:22:54"},{"id":"aws-ml-specialty-modeling-1768148573552-2","question":"You deploy a model as a SageMaker real-time endpoint to serve predictions with variable traffic. To balance latency and cost, you want to automatically scale the endpoint, not scale to zero. Which approach is correct?","answer":"[{\"id\":\"a\",\"text\":\"Configure Application Auto Scaling on the SageMaker endpoint with a target tracking policy using InvocationsPerInstance, setting min=1 and max=20.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Keep a single instance and rely on manual scaling.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Configure the endpoint to scale to zero during idle periods.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use Batch Transform for all inferences.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption **A** is correct: use Application Auto Scaling with a target tracking policy based on InvocationsPerInstance, with a sensible min and max, to maintain latency while controlling costs. Real-time endpoints cannot scale to zero.\n\n## Why Other Options Are Wrong\n- **B**: Manual scaling is not responsive to traffic variation and defeats auto-scaling benefits.\n- **C**: Real-time endpoints do not support scale-to-zero; at least one instance is required.\n- **D**: Batch Transform is not suitable for real-time low-latency inferences.\n\n## Key Concepts\n- SageMaker real-time endpoints vs batch transform\n- Endpoint auto-scaling and InvocationsPerInstance metric\n- Min/Max capacity sizing and latency considerations\n\n## Real-World Application\nImplement a scalable real-time endpoint by enabling auto-scaling policies that react to traffic, ensuring consistent latency during peak hours while avoiding over-provisioning during off-peak times.","diagram":null,"difficulty":"intermediate","tags":["SageMaker","Kubernetes","Terraform","EKS","certification-mcq","domain-weight-36"],"channel":"aws-ml-specialty","subChannel":"modeling","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T16:22:54.249Z","createdAt":"2026-01-11 16:22:54"}],"subChannels":["data-engineering","exploratory-analysis","general","modeling"],"companies":["Bloomberg","Lyft"],"stats":{"total":10,"beginner":1,"intermediate":9,"advanced":0,"newThisWeek":10}}