{"questions":[{"id":"q-1228","question":"Design a drift-aware continuous training and multi-region deployment workflow for a fraud-detection model, using SageMaker Model Monitor, Pipelines, and Model Registry. Explain how you detect data and feature drift (PSI/KS against baselines), retrain triggers, versioning, canary validation, rollback, and how cross-region consistency is maintained?","answer":"Implement drift-aware continuous training and multi-region deployment with SageMaker Model Monitor, Pipelines, and Model Registry. Detect data and feature drift using PSI/KS against baselines; retrain","explanation":"## Why This Is Asked\nAssesses practical MLOps skills: drift detection, versioned deployment, cross-region consistency, and safe canary releases in a real-world, regulated context.\n\n## Key Concepts\n- Drift detection with PSI/KS against baselines\n- SageMaker Model Monitor and Pipelines integration\n- Model Registry versioning and promoted stages\n- Canary validation and rollback strategies\n\n## Code Example\n```python\n# Pseudo: define a drift check step in SageMaker Pipelines\npipeline_step = DriftCheckStep(..., drift_check_config={ 'DataDrift': {'Threshold': 0.2}, 'FeatureDrift': {'Threshold': 0.1} })\n```\n\n## Follow-up Questions\n- How would you determine Canary rollout percentages across regions?\n- What metrics would you surface in CloudWatch and SageMaker Model Monitor dashboards to detect drift early?","diagram":"flowchart TD\n  A[Data Ingest] --> B[Drift Check with PSI/KS]\n  B --> C{Drift > Threshold}\n  C -- Yes --> D[Trigger Retrain]\n  D --> E[Register New Version]\n  E --> F[Canary Rollout (Region A)]\n  F --> G[Monitor Metrics]\n  G -- Stable --> H[Full Rollout]\n  G -- Drift --> I[Rollback to Previous Version]","difficulty":"advanced","tags":["aws-ml-specialty"],"channel":"aws-ml-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Bloomberg","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T05:38:55.228Z","createdAt":"2026-01-13T05:38:55.228Z"},{"id":"q-1297","question":"You're deploying a multilingual sentiment-analysis model for a global customer-support chatbot. To minimize downtime when updating language adapters, design a SageMaker-based deployment with per-language variants, Model Registry, and canary rollouts that preserve latency SLAs and isolate traffic. Describe autoscaling, traffic routing, validation, and rollback criteria with concrete values?","answer":"Use per-language EndpointVariants and register adapters in Model Registry. Deploy a canary that shifts 20% of traffic to the new language adapter while 80% stays on the baseline. Scale per-region with","explanation":"## Why This Is Asked\n\nTests practical use of SageMaker features to handle multilingual adapters with zero-downtime updates and strict latency SLAs.\n\n## Key Concepts\n\n- SageMaker Model Registry and Endpoint Variants\n- Canary deployments and per-language traffic routing\n- Drift and latency validation; per-language observability\n- Rollback and promotion criteria; cross-region considerations\n\n## Code Example\n\n```javascript\n// Example: pseudo-configure per-language variants and canary rollout\nconst variants = [\n  { Language: 'en', VariantName: 'prod-en', ModelName: 'sentiment-en', TrafficSplit: 0.8 },\n  { Language: 'en', VariantName: 'canary-en', ModelName: 'sentiment-en-v2', TrafficSplit: 0.2 }\n  // ...additional languages\n];\n// Register models, create endpoint config, and set alarms for drift/latency\n```\n\n## Follow-up Questions\n\n- How would you monitor and react to per-language data drift in real time?\n- How would you handle adding a brand-new language with zero downtime across regions?","diagram":"flowchart TD\n  A[Language Adapter Update] --> B[Route Canary Traffic]\n  B --> C[Monitor Latency & Drift]\n  C --> D{OK?}\n  D -->|Yes| E[Promote to Baseline]\n  D -->|No| F[Rollback & Retry]\n  E --> G[Continue Serving]\n  F --> G","difficulty":"intermediate","tags":["aws-ml-specialty"],"channel":"aws-ml-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T08:43:55.886Z","createdAt":"2026-01-13T08:43:55.886Z"},{"id":"q-1324","question":"In a two-region SageMaker real-time inference setup for fraud detection, data drift is likely between regions and latency targets are strict. Outline a concrete canary deployment with per-region endpoint configs, Drift Detection thresholds, and Feature Store versioning/replication. Include traffic split, rollback criteria, validation plan, and monitoring strategy?","answer":"Route 10% of new-endpoint traffic to Region A and Region B, 90% to the stable version. Enable Drift Detection with feature-level thresholds (e.g., z-score > 3) on key features; trigger automatic rollb","explanation":"## Why This Is Asked\nTests ability to design multi-region canary rollout with drift detection and data governance.\n\n## Key Concepts\n- SageMaker Real-time Endpoints and canary deployments\n- Drift Detection jobs and thresholds\n- Feature Store versioning and cross-region replication\n- Latency SLAs and monitoring/alerts\n\n## Code Example\n```javascript\n// Example using AWS SDK for drift detection setup (pseudo)\nconst detector = new SageMakerDriftDetector({ ... });\n```\n\n## Follow-up Questions\n- How would you validate drift thresholds across regions?\n- How would you handle failed rollback and hotfix deployment?","diagram":"flowchart TD\n  A[Client Request] --> B[Region A Endpoint]\n  A --> C[Region B Endpoint]\n  B --> D[Canary Router]\n  C --> D\n  D --> E[Monitoring & Rollback]","difficulty":"advanced","tags":["aws-ml-specialty"],"channel":"aws-ml-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","LinkedIn","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T11:35:13.099Z","createdAt":"2026-01-13T11:35:13.100Z"},{"id":"q-1378","question":"You’re building a SageMaker ML workflow that validates incoming data in a processing step before training. Design a minimal Processing Job using Python to check (i) all required features exist, (ii) numeric columns have ≤5% missing values, (iii) categoricals are within allowed sets. How would you trigger it from a SageMaker Pipeline, store results, and surface metrics? Include concrete resource choices?","answer":"Design a SageMaker Processing Job (ScriptProcessor) that loads input data from S3, checks: (i) all required features exist, (ii) numeric columns have ≤5% missing values, (iii) categoricals are within ","explanation":"## Why This Is Asked\nTests practical data-validation wiring in a real-world pipeline, not just model inference.\n\n## Key Concepts\n- SageMaker Processing, ScriptProcessor, Pandas data validation\n- SageMaker Pipeline integration, ProcessingStep, S3 artifacts\n- CloudWatch metrics for validation outcomes\n\n## Code Example\n```python\n# Pseudocode for validation in a SageMaker Processing job\nimport pandas as pd\nimport json\n# load data from S3, validate, write report\n```\n\n## Follow-up Questions\n- How would you handle missing required features differently for streaming data?\n- How would you version the validation logic in the registry?","diagram":null,"difficulty":"beginner","tags":["aws-ml-specialty"],"channel":"aws-ml-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Instacart"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T14:40:23.676Z","createdAt":"2026-01-13T14:40:23.676Z"},{"id":"q-1457","question":"You're building a real-time risk-scoring model for a multi-region e-commerce platform. The model consumes streaming events, uses SageMaker Feature Store for features, and is deployed as a real-time endpoint with cross-region routing. Describe a concrete end-to-end deployment and monitoring design that ensures deterministic latency, supports feature versioning, detects data drift, and handles canary rollouts with rollback triggers; include governance, cost controls, and testing strategy?","answer":"Deploy two SageMaker real-time endpoints in us-east-1 and eu-west-1 behind Route 53 latency routing. Use Feature Store versioned feature groups (v1/v2) for online features and run Model Monitor drift ","explanation":"## Why This Is Asked\nTests ability to design cross-region, latency-sensitive inference with feature versioning and drift monitoring, plus controlled rollouts and cost management.\n\n## Key Concepts\n- SageMaker real-time endpoints\n- Cross-region routing (Route 53 / Global Accelerator)\n- Feature Store versioning\n- Model Monitor drift detection\n- Canary/blue-green deployment\n- Application Auto Scaling for latency targets\n- Cost governance and observability\n\n## Code Example\n```yaml\nResources:\n  EndpointConfigA:\n    Type: AWS::SageMaker::EndpointConfig\n    Properties:\n      ProductionVariants:\n        - VariantName: v1\n          ModelName: my-model-v1\n          InitialInstanceCount: 2\n          InstanceType: ml.m5.large\n```\n\n## Follow-up Questions\n- How would you validate latency targets before production rollout?\n- What triggers a rollback and how would you automate it across regions?","diagram":null,"difficulty":"advanced","tags":["aws-ml-specialty"],"channel":"aws-ml-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T17:52:56.069Z","createdAt":"2026-01-13T17:52:56.069Z"},{"id":"q-1564","question":"You're deploying a global churn-prediction model for a SaaS app that requires real-time scoring in-app and nightly analytics reports, while complying with data residency rules. Propose an end-to-end AWS pattern using SageMaker real-time endpoints for live inference, Batch Transform for nightly analytics, per-region Feature Store isolation, and a governance framework with Model Registry versioning, drift detection, automated rollback, and cost controls. Include testing and validation steps?","answer":"Deploy regional real-time endpoints behind regional VPCs with per-region Feature Store isolation; run nightly Batch Transform jobs for analytics. Use SageMaker Model Registry for versioning with automated approval workflows, implement Clarify drift detection with automated alerts and rollback triggers, and establish canary rollouts with traffic shifting. Apply cost controls through endpoint autoscaling, instance right-sizing, and scheduled scaling for non-production hours.","explanation":"## Why This Is Asked\n\nAssesses ability to design multi-region, residency-compliant ML deployments combining real-time and batch workloads, governance, and automated rollback. Tests practical use of SageMaker Endpoint Variants, Model Registry, and drift tooling.\n\n## Key Concepts\n\n- Regional real-time endpoints with VPC isolation\n- Per-region Feature Store governance\n- Batch Transform for analytics\n- Model Registry versioning\n- Drift detection with Clarify\n- Canary rollouts and automated rollback\n- Cost controls and autoscaling\n\n## Code Example\n\n```javascript\n// CDK deployment pattern for regional ML infrastructure\nconst modelPackage = new sagemaker.CfnModelPackage(this, 'ModelPackage', {\n  modelPackageName: modelVersion.name,\n  approvalStatus: 'Approved',\n  sourceAccount: process.env.AWS_ACCOUNT_ID\n});\n\n// Real-time endpoint with autoscaling\nconst endpoint = new sagemaker.CfnEndpoint(this, 'Endpoint', {\n  endpointConfigName: endpointConfig.endpointConfigName\n});\n```","diagram":"flowchart TD\n  A[Live Inference] --> B[Regional Endpoint A]\n  A --> C[Regional Endpoint B]\n  D[Batch Transform] --> E[Analytics Data Lake]\n  F[Model Registry] --> G[Canary Rollout]\n  H[Drift Monitors] --> F","difficulty":"intermediate","tags":["aws-ml-specialty"],"channel":"aws-ml-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T04:57:28.836Z","createdAt":"2026-01-13T21:50:00.812Z"},{"id":"q-1572","question":"Design a multi-tenant, per-tenant inference service on SageMaker for a financial risk model where each client has isolated data, separate feature store namespace, and per-tenant model version, yet share a common endpoint. Describe the architecture, how you isolate data and billing, how you route requests by tenant_id, how you handle feature/version drift, and how you implement canary rollouts and rollback?","answer":"Design a multi-tenant SageMaker real-time endpoint behind API Gateway. Each client utilizes a unique Feature Store namespace and per-tenant model version in the SageMaker Model Registry; isolation is achieved through IAM boundaries, VPC endpoints, and Secrets Manager. API Gateway routes requests by tenant_id to the shared endpoint, which loads tenant-specific features and models. Billing is tracked through CloudWatch metrics and cost allocation tags per tenant. Feature drift is monitored with SageMaker Model Monitor, while version drift is handled through automated model registry validation. Canary rollouts use weighted traffic shifting in API Gateway, with rollback via previous model version restoration.","explanation":"## Why This Is Asked\n\nTests ability to design secure multi-tenant ML inference on AWS with data isolation and governance.\n\n## Key Concepts\n\n- SageMaker Model Registry and per-tenant versions\n- Feature Store namespaces and isolation\n- IAM boundaries, VPC endpoints, Secrets Manager\n- API Gateway routing by tenant_id\n- Canary releases, rollback, drift detection\n\n## Code Example\n\n```javascript\n// pseudo-Infra snippet: outline of resources\n```\n\n## Follow-up Questions\n\n- How would you test tenant isolation and cost accounting?\n- How would you monitor latency per-tenant and detect leakage?","diagram":null,"difficulty":"advanced","tags":["aws-ml-specialty"],"channel":"aws-ml-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Hugging Face","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T06:12:25.191Z","createdAt":"2026-01-13T22:36:03.835Z"},{"id":"q-1618","question":"You're deploying a predictive maintenance model to 10,000 industrial edge devices using SageMaker Edge Manager. Outline a phased OTA rollout with canary groups, offline devices, and automated rollback triggers based on telemetry. Specify packaging and signing process, versioned Edge Manifest in S3, device-grouping strategy, and how you monitor model accuracy, latency, and update success; discuss cost controls and governance?","answer":"I'll outline a comprehensive phased OTA rollout strategy for deploying a predictive maintenance model to 10,000 industrial edge devices using SageMaker Edge Manager. The approach includes canary testing, offline device handling, and telemetry-driven rollback mechanisms.\n\n**Packaging and Signing Process:**\nPackage the model artifacts into a compressed tar.gz file with metadata, then sign using AWS KMS with a dedicated edge signing key. The signed package includes model weights, inference runtime, configuration files, and checksum verification.\n\n**Versioned Edge Manifest in S3:**\nPublish a versioned Edge Manifest to S3 containing deployment specifications, device compatibility matrix, and rollback configuration. Each manifest version is immutable and includes digital signatures for integrity verification.\n\n**Device Grouping Strategy:**\nSegment devices into logical groups: Canary (1% devices), Phase 1 (10% high-priority), Phase 2 (30% critical infrastructure), and Phase 3 (remaining 59%). Groups are defined by device type, network connectivity, criticality, and geographic location.\n\n**Phased Rollout with Canary Groups:**\nBegin with canary deployment to 100 devices, monitor for 24 hours, then proceed through phases. Each phase includes automated health checks and requires manual approval before progression.\n\n**Offline Device Handling:**\nImplement store-and-forward mechanism for offline devices. Devices cache update packages locally and apply when connectivity restores. Use exponential backoff for retry attempts.\n\n**Automated Rollback Triggers:**\nConfigure rollback triggers based on: model accuracy degradation (>5% drop), inference latency increase (>200ms), error rate spike (>3%), memory usage threshold (>80%), and telemetry anomalies.\n\n**Monitoring Strategy:**\nTrack model accuracy through validation datasets, monitor inference latency at the edge, and measure update success rates. Use CloudWatch metrics and device-level telemetry for real-time visibility.\n\n**Cost Controls:**\nImplement lifecycle policies for old model versions, use S3 Intelligent Tiering for storage optimization, and schedule updates during off-peak hours to reduce data transfer costs.\n\n**Governance:**\nEstablish role-based access control, require multi-person approval for production deployments, and maintain audit trails through AWS CloudTrail.","explanation":"## Why This Is Asked\nThis question tests practical edge deployment, OTA rollout, and telemetry-driven governance for real-world industrial settings.\n\n## Key Concepts\n- SageMaker Edge Manager OTA workflow\n- Canary and phased rollouts with rollback triggers\n- Telemetry-driven monitoring (latency, errors, drift)\n- Edge manifest versioning and cryptographic signing\n- Cost governance and access control\n\n## Code Example\n```json\n{\n  \"EdgeVersion\": \"v1.2.3\",\n  \"ModelArtifactsS3Key\": \"s3://bucket/models/maintenance/v1.2.3/model.tar.gz\",\n  \"SignKMSKeyId\": \"alias/edge-signing-key\",\n  \"RolloutPlan\": {\n    \"CanaryGroup\": {\n      \"DeviceCount\": 100,\n      \"MonitoringDuration\": \"24h\",\n      \"ApprovalRequired\": true\n    },\n    \"Phases\": [\n      {\n        \"Name\": \"Phase1\",\n        \"DevicePercentage\": 10,\n        \"DeviceTypes\": [\"high-priority\"]\n      },\n      {\n        \"Name\": \"Phase2\", \n        \"DevicePercentage\": 30,\n        \"DeviceTypes\": [\"critical-infrastructure\"]\n      },\n      {\n        \"Name\": \"Phase3\",\n        \"DevicePercentage\": 59,\n        \"DeviceTypes\": [\"standard\"]\n      }\n    ]\n  },\n  \"RollbackTriggers\": {\n    \"AccuracyThreshold\": 0.95,\n    \"LatencyThresholdMs\": 200,\n    \"ErrorRateThreshold\": 0.03,\n    \"MemoryThresholdPercent\": 80\n  }\n}\n```","diagram":"flowchart TD\n  A[Edge Deployment] --> B[Device Groups]\n  B --> C[OTA Manifest]\n  C --> D[Delivery & Install]\n  D --> E[Telemetry & Monitoring]\n  E --> F[Rollback Gate]\n  F --> G[Artifact Pruning]","difficulty":"advanced","tags":["aws-ml-specialty"],"channel":"aws-ml-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","IBM"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T05:28:38.285Z","createdAt":"2026-01-14T02:43:57.722Z"},{"id":"q-1690","question":"You're deploying a SageMaker real-time endpoint for a financial risk model that ingests customer PII from EU and US users. Propose a compliant deployment pattern that enforces data locality (EU data stays EU), supports active-active regional endpoints, and provides GA-ready drift and privacy controls. Include resource layout, data flow, encryption, IAM/KMS, auditing, canaries, and cost controls?","answer":"Deploy two SageMaker real-time endpoints in eu-west-1 and us-east-1. Data stays in-region (S3 with regional KMS keys; Feature Store per region; no cross-region PII copies). Route users to nearest endp","explanation":"## Why This Is Asked\nTests data locality, governance, and multi-region serving under real-world privacy constraints.\n\n## Key Concepts\n- Data locality and encryption in AWS\n- Multi-region SageMaker endpoints and Route 53 routing\n- Canary deployments and drift monitoring\n- IAM/KMS controls and auditing\n\n## Code Example\n```javascript\n// CloudFormation/CDK-like snippet (pseudo)\nconst eu = { region: 'eu-west-1' };\nconst us = { region: 'us-east-1' };\n// Define endpoints, regional buckets, KMS keys, and Route53 routing\n```\n\n## Follow-up Questions\n- How would you test regional failover latency?\n- How would you enforce data-residency with automated alerts if cross-region transfer occurs?","diagram":null,"difficulty":"intermediate","tags":["aws-ml-specialty"],"channel":"aws-ml-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Robinhood","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T07:00:22.870Z","createdAt":"2026-01-14T07:00:22.870Z"},{"id":"q-1725","question":"Design an end-to-end, multi-account, multi-region real-time fraud-detection pipeline on AWS. The model is deployed as SageMaker endpoints in two regions with cross-region routing. Provide concrete choices for endpoint configuration, SageMaker Feature Store versioning, canary rollout, drift detection, monitoring, autoscaling, governance, and cost controls; include validation steps and rollback criteria?","answer":"Propose a two-region, two-account SageMaker setup with endpoint replicas, Feature Store versioned per model, and canary rollout with 20% traffic to a new version behind a traffic router. Use SageMaker","explanation":"## Why This Is Asked\nEvaluates cross-account, multi-region design, governance, and operational readiness for real-time ML at scale.\n\n## Key Concepts\n- SageMaker endpoint configuration and autoscaling\n- Feature Store versioning and data lineage\n- Cross-account IAM access and resource sharing\n- Drift and data-quality monitoring with SageMaker Model Monitor\n- Canary rollouts, rollback triggers, and traffic shaping\n- Cost controls, guardrails, and compliance requirements\n\n## Code Example\n```bash\n# Example commands (illustrative)\naws sagemaker create-endpoint-config --endpoint-config-name fraud-endpoint --production-variants file://variants.json\naws sagemaker put-model-package-group-policy --model-package-group-name FraudGroup --policy file://policy.json\n```\n\n## Follow-up Questions\n- How would you validate drift triggers with simulated data? \n- How do you coordinate feature-store migrations across regions without downtime? \n","diagram":"flowchart TD\n  A[Model Registry] --> B[Feature Store Versioning]\n  B --> C[Region 1 Endpoint]\n  B --> D[Region 2 Endpoint]\n  C --> E[Traffic Router]\n  D --> E\n  E --> F[Model Monitor & Alarms]\n  F --> G[Remediation / Rollback]\n  G --> H[Cost & Governance]","difficulty":"advanced","tags":["aws-ml-specialty"],"channel":"aws-ml-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Salesforce","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T08:43:49.439Z","createdAt":"2026-01-14T08:43:49.439Z"},{"id":"q-1761","question":"You're building a privacy-preserving, multi-tenant real-time inference platform on AWS where each tenant's data must stay isolated (data locality + encryption) and costs are allocated per tenant. Propose an architecture using SageMaker Endpoints behind PrivateLink, per-tenant Feature Store versions, and a tenant-scoped Model Registry with canary rollouts. Explain how you validate latency, monitor drift, trigger retraining via SageMaker Pipelines, and enforce governance and per-tenant cost controls?","answer":"Architecture: per-tenant SageMaker Endpoints in a VPC with PrivateLink, isolated Feature Store versions, and a tenant-scoped Model Registry. Use canary rollouts with traffic shifting, monitor latency,","explanation":"## Why This Is Asked\nTests ability to design privacy-conscious, multi-tenant ML deployments on AWS with governance and operational controls.\n\n## Key Concepts\n- Isolation: VPC PrivateLink, IAM boundaries\n- Per-tenant versioning: Feature Store, Model Registry\n- Canary deployments and rollback\n- Drift and bias monitoring with SageMaker Clarify\n- Cost governance and auditing\n\n## Code Example\n```python\n# Pseudocode: tenant-scoped endpoint setup (illustrative)\nfrom aws_cdk import aws_sagemaker as sagemaker\n# ... construct per-tenant endpoint, feature store, and registry\n```\n\n## Follow-up Questions\n- How would you enforce data locality across regions while allowing cross-tenant analytics?\n- What metrics and alarms would you apply to detect drift fast enough for per-tenant retraining?","diagram":"flowchart TD\n  A[Tenant isolation] --> B[PrivateLink endpoints]\n  B --> C[Feature Store per tenant]\n  C --> D[Model Registry per tenant]\n  D --> E[Canary rollout]\n  E --> F[Monitoring & drift detection]\n  F --> G[SageMaker Pipelines retraining]\n  G --> H[Governance & cost controls]","difficulty":"intermediate","tags":["aws-ml-specialty"],"channel":"aws-ml-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Meta","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T09:47:18.638Z","createdAt":"2026-01-14T09:47:18.639Z"},{"id":"q-1868","question":"Design a hybrid on-prem plus AWS inference workflow for a regulated financial service where customer data must never leave the on-prem site, but model updates are deployed from SageMaker. Propose an architecture using SageMaker Edge Manager for edge endpoints, PrivateLink to AWS backends, and a regulated Model Registry with per-tenant access controls. Include latency targets, canary rollouts to edge devices, drift detection, retraining triggers, and auditability?","answer":"Implement SageMaker Edge Manager to deploy a compact inference model to on‑prem gateways so data never leaves the site. Retrain centrally in AWS and push updates via PrivateLink. Use a per‑tenant Mode","explanation":"## Why This Is Asked\nTests hybrid edge-cloud design in regulated environments, focusing data locality, secure model updates, and governance.\n\n## Key Concepts\n- SageMaker Edge Manager\n- PrivateLink\n- Canary rollouts\n- Drift detection with Clarify\n- SageMaker Pipelines for retraining\n- CloudTrail auditing\n\n## Code Example\n```javascript\n// Pseudocode: canary rollout trigger\nconst canaryConfig = { fraction: 0.1, metrics: { driftThreshold: 0.05 } };\ntriggerCanary(modelArn, canaryConfig);\n```\n\n## Follow-up Questions\n- How would you verify privacy compliance across tenants?\n- How would you test edge latency under network partition?","diagram":"flowchart TD\n  A(On-prem gateway) --> B[SageMaker Edge Manager]\n  B --> C[PrivateLink to AWS backends]\n  C --> D[Central retraining in AWS]\n  D --> E[Push updates to edge]\n  E --> F{Canary rollout}\n  F --> G[Production edge endpoints]\n  F --> H[Canary edge endpoints]\n  G --> I[Drift detection & retraining trigger]","difficulty":"advanced","tags":["aws-ml-specialty"],"channel":"aws-ml-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Microsoft","Square","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T14:56:22.744Z","createdAt":"2026-01-14T14:56:22.744Z"},{"id":"q-1887","question":"Design a beginner-friendly SageMaker multi-model endpoint setup that serves two small text classifiers from a single endpoint. Route requests by a tenant_id included in the JSON input, ensuring models load on demand, monitor latency with CloudWatch, and implement a simple canary switch to compare model A vs B for a subset of tenants before full rollout. Include basic file structure, IAM roles, and a minimal test plan?","answer":"Leverage SageMaker Multi-Model Endpoint (one host, two models). Upload models to S3 with separate model artifacts, deploy a shared container, and implement a simple routing layer in the inference scri","explanation":"## Why This Is Asked\nTests practical use of SageMaker Multi-Model Endpoints, dynamic model loading, and tenant-aware routing without multiple endpoints.\n\n## Key Concepts\n- SageMaker Multi-Model Endpoint, dynamic model loading\n- JSON payload routing by tenant_id\n- Canary rollout, rollback criteria\n- CloudWatch metrics for latency/throughput, tagging for cost\n\n## Code Example\n```python\n# routing sketch\nimport json\nMODEL_MAP = {'tenant1': 'modelA', 'tenant2': 'modelB'}\n\ndef handle_inference(event):\n    payload = json.loads(event['body'])\n    tenant = payload.get('tenant_id', 'default')\n    model_key = MODEL_MAP.get(tenant, 'default')\n    # load and run model_key, return result\n```\n\n## Follow-up Questions\n- How would you test cold-start effects and memory constraints?\n- How would you automate canary promotion with CloudWatch alarms?","diagram":"flowchart TD\n  A[Receive Request] --> B{Tenant_ID in payload}\n  B --> C[Route to Model A or B]\n  C --> D[Load Model if needed]\n  D --> E[Run Inference]\n  E --> F[Return Response]","difficulty":"beginner","tags":["aws-ml-specialty"],"channel":"aws-ml-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Discord","IBM"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T15:47:11.239Z","createdAt":"2026-01-14T15:47:11.239Z"},{"id":"q-1913","question":"You’re deploying a beginner-friendly real-time sentiment moderation model for a global social app on SageMaker. End-user data must stay in one region and be routed through PrivateLink. Propose a concrete deployment: a single SageMaker endpoint behind PrivateLink, basic drift and bias checks with SageMaker Clarify, and a versioned Feature Store for user interactions, plus a simple canary and rollback plan. Include latency targets, retraining triggers, and governance basics?","answer":"Use a single SageMaker endpoint (ml.m5.large) in a single region behind a PrivateLink VPC endpoint. Enable SageMaker Clarify on-inference for bias and drift checks. Store user interaction features in ","explanation":"## Why This Is Asked\nTests ability to design a simple, privacy-conscious real-time inference with bias/drift checks and feature store versioning using AWS ML services; beginner-friendly yet demonstrates practical constraints like data locality, PrivateLink, and canary rollout.\n\n## Key Concepts\n- SageMaker Endpoint + PrivateLink\n- SageMaker Clarify on-inference\n- Feature Store versioning\n- Canary deployment and rollback\n- Drift and bias checks\n- Governance (IAM, KMS, data locality)\n\n## Code Example\n```javascript\n// Pseudo-configuration for Clarify and canary\nconst cfg = {\n  clarify: { biasCheck: true, driftCheck: true },\n  featureStoreVersioning: true\n}\n```\n\n## Follow-up Questions\n- What metrics would you monitor to ensure drift and bias are under control in Clarify?\n- Describe a rollback decision path after a canary deployment if latency degrades.","diagram":null,"difficulty":"beginner","tags":["aws-ml-specialty"],"channel":"aws-ml-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Snap","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T16:58:08.881Z","createdAt":"2026-01-14T16:58:08.881Z"},{"id":"q-2039","question":"You're deploying a beginner-friendly text classification service with SageMaker Serverless Inference for a low-traffic social app. Data must remain in-region, and endpoint credentials must rotate every 90 days. Propose a concrete setup: packaging and artifact storage, a single variant serverless endpoint, monthly drift-driven retraining triggers, a rollback plan, and basic monitoring/alerts for latency and errors?","answer":"Deploy a serverless endpoint configuration in SageMaker with versioned model artifacts stored in S3, implement a single production variant for text classification, establish monthly drift detection triggers via SageMaker Pipelines for automated retraining, create a rollback strategy using previous model versions, and configure CloudWatch monitoring with alerts for latency metrics and error rates.","explanation":"## Why This Is Asked\nTests familiarity with serverless inference architectures, model lifecycle management, and data residency requirements for beginner-level MLOps roles. It also evaluates understanding of basic operational patterns including automated retraining, rollback procedures, and monitoring fundamentals.\n\n## Key Concepts\n- SageMaker Serverless Inference fundamentals and configuration\n- Model packaging strategies, S3 artifact storage, and versioned model registries\n- Endpoint management with single-variant deployment and rollback mechanisms\n- CloudWatch monitoring for latency tracking and error rate alerting\n- Data residency compliance, IAM role management, and Secrets Manager for credential rotation\n- Automated drift detection and pipeline-triggered retraining workflows\n\n## Code Example\n```python\n# Serverless endpoint deployment with monitoring setup\nimport boto3\nimport json\nfrom datetime import datetime\n\nsagemaker = boto3.client('sagemaker')\ncloudwatch = boto3.client('cloudwatch')\n\ndef create_serverless_endpoint():\n    # Create model with versioned S3 artifacts\n    model_name = f\"text-classifier-{datetime.now().strftime('%Y-%m-%d')}\"\n    \n    sagemaker.create_model(\n        ModelName=model_name,\n        PrimaryContainer={\n            'Image': '763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-inference:1.12-cpu-py38',\n            'ModelDataUrl': 's3://my-model-bucket/artifacts/text-classifier-v1.tar.gz',\n            'Environment': {\n                'SAGEMAKER_PROGRAM': 'inference.py',\n                'SAGEMAKER_SUBMIT_DIRECTORY': '/opt/ml/model/code'\n            }\n        },\n        ExecutionRoleArn='arn:aws:iam::123456789012:role/SageMakerExecutionRole'\n    )\n    \n    # Configure serverless endpoint\n    endpoint_config_name = f\"text-classifier-config-{datetime.now().strftime('%Y-%m-%d')}\"\n    \n    sagemaker.create_endpoint_config(\n        EndpointConfigName=endpoint_config_name,\n        ProductionVariants=[{\n            'VariantName': 'AllTraffic',\n            'ModelName': model_name,\n            'ServerlessConfig': {\n                'MemorySizeInMB': 2048,\n                'MaxConcurrency': 10\n            }\n        }]\n    )\n    \n    # Create endpoint\n    endpoint_name = 'text-classifier-endpoint'\n    sagemaker.create_endpoint(\n        EndpointName=endpoint_name,\n        EndpointConfigName=endpoint_config_name\n    )\n    \n    return endpoint_name\n\ndef setup_monitoring(endpoint_name):\n    # Create CloudWatch alarms for latency and errors\n    cloudwatch.put_metric_alarm(\n        AlarmName=f'{endpoint_name}-high-latency',\n        MetricName='InvocationLatency',\n        Namespace='AWS/SageMaker',\n        Statistic='Average',\n        Period=300,\n        EvaluationPeriods=2,\n        Threshold=5000,  # 5 seconds\n        ComparisonOperator='GreaterThanThreshold',\n        Dimensions=[{'Name': 'EndpointName', 'Value': endpoint_name}]\n    )\n    \n    cloudwatch.put_metric_alarm(\n        AlarmName=f'{endpoint_name}-high-errors',\n        MetricName='Invocation4XXErrors',\n        Namespace='AWS/SageMaker',\n        Statistic='Sum',\n        Period=300,\n        EvaluationPeriods=2,\n        Threshold=10,  # 10 errors in 5 minutes\n        ComparisonOperator='GreaterThanThreshold',\n        Dimensions=[{'Name': 'EndpointName', 'Value': endpoint_name}]\n    )\n\n# Execute deployment\nendpoint = create_serverless_endpoint()\nsetup_monitoring(endpoint)\n```","diagram":"flowchart TD\n  A[Client Request] --> B[Serverless Inference Endpoint]\n  B --> C[Model Artifact v1.x]\n  C --> D[CloudWatch Monitoring]\n  D --> E{Drift/Rollback Triggers}\n  E --> F[Rollback to Previous Version]","difficulty":"beginner","tags":["aws-ml-specialty"],"channel":"aws-ml-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T04:48:55.988Z","createdAt":"2026-01-14T21:45:07.847Z"},{"id":"q-2052","question":"Design an automated rollout/rollback strategy for a multi-tenant real-time SageMaker inference platform with per-tenant Feature Store variants and cross-region canaries. How would you implement retraining triggers, drift validation, and per-tenant cost controls? Include concrete thresholds, duration, and rollback criteria?","answer":"You're managing a multi-tenant real-time inference platform on SageMaker. Design an automated rollout/rollback strategy using SageMaker Pipelines for retraining orchestration, per-tenant Feature Store variants for data isolation, and cross-region canary deployments for risk mitigation. Implement drift validation through Model Monitor with specific thresholds (AUC drop >0.05, latency increase >20%, error rate >5%), configure retraining triggers based on data drift or performance degradation, and enforce per-tenant cost controls via budget alerts and auto-scaling limits. Establish concrete rollback criteria including canary failure rates, regional latency SLA breaches, and cost threshold violations.","explanation":"## Why This Is Asked\nTests ability to design scalable, automated ML lifecycle management for multi-tenant production workloads, including drift-driven retraining, per-tenant isolation, cross-region canaries, and cost governance.\n\n## Key Concepts\n- SageMaker Pipelines for retraining orchestration\n- Model Monitor drift detection with specific thresholds\n- Per-tenant Feature Store variants and PrivateLink isolation\n- Canary deployments across regions with rollback criteria\n- Cost governance per tenant\n\n## Code Example\n```javascript\nfunction shouldRollback(current, next, thresholds) {\n  const aucD","diagram":"flowchart TD\n  A[Rollout] --> B[Canary in Region 1]\n  B --> C{Drift or latency triggers?}\n  C -- Yes --> D[Rollback to previous version]\n  C -- No --> E[Promote to Region 2 and full rollout]","difficulty":"intermediate","tags":["aws-ml-specialty"],"channel":"aws-ml-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Meta","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T05:52:14.622Z","createdAt":"2026-01-14T22:39:38.964Z"},{"id":"q-2078","question":"Design a multi-tenant, privacy-preserving inference path on AWS that returns per-prediction explanations without leaking tenant data. Propose using SageMaker Endpoints behind PrivateLink, SageMaker Clarify explainability, per-tenant Feature Store versions, and a tenant-scoped Model Registry with canaries. Include drift detection, retraining triggers, and auditability; specify latency targets and rollback criteria?","answer":"Architect a multi-tenant, privacy-preserving inference path on AWS that returns per-prediction explanations without leaking tenant data. Deploy SageMaker Endpoints behind PrivateLink for network isolation, integrate SageMaker Clarify for per-prediction SHAP explanations, implement per-tenant Feature Store versions for data segregation, and establish a tenant-scoped Model Registry with canary deployments. Include automated drift detection with retraining triggers, comprehensive audit logging, and define latency targets (<100ms P99) with rollback criteria (error rate >5% or latency >200ms).","explanation":"## Why This Is Asked\nTests ability to combine explainability, privacy, and governance in a real multi-tenant AWS ML stack, plus concrete latency and rollback criteria.\n\n## Key Concepts\n- SageMaker Endpoints + PrivateLink for isolation\n- SageMaker Clarify for per-prediction explanations\n- Feature Store versions per tenant\n- Tenant-scoped Model Registry with canaries\n- Drift detection, retraining triggers, audit logs, rollback strategy\n\n## Code Example\n```python\n# Pseudo-config: Clarify explainer tied to a per-tenant endpoint\nfrom sagemaker.explainability import ClarifyExplainer\nclarify = ClarifyExplainer(\n    endpoint_name=f\"tenant-{tenant_id}-endpoint\",\n    explainability_config={\n        \"shap\": {\"baseline\": \"median\"},\n        \"output_path\": f\"s3://tenant-{tenant_id}-explanations/\"\n    }\n)\n```","diagram":null,"difficulty":"intermediate","tags":["aws-ml-specialty"],"channel":"aws-ml-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","Plaid","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T05:32:14.995Z","createdAt":"2026-01-14T23:30:54.991Z"},{"id":"q-2096","question":"Design an end-to-end deployment for a real-time anomaly detection pipeline across three data-residency regions with strict data sovereignty. Raw data must stay on-prem; only aggregated signals may traverse to AWS. Propose an architecture using SageMaker Endpoints in each region, PrivateLink to on-prem data sources, and a Global data-plane that routes latency-critical inferences. Include canary rollouts, drift detection, automated retraining, and rollback criteria?","answer":"Deploy regional SageMaker Endpoints across three data-residency regions, establishing PrivateLink connections to on-premises data sources to maintain strict data sovereignty. Route latency-critical inferences through AWS Global Accelerator for optimal performance. Implement canary deployments using versioned endpoints with weighted traffic splitting, configure drift detection metrics to trigger automated retraining pipelines, and establish rollback criteria based on performance thresholds and error rates.","explanation":"## Why This Is Asked\nTests multi-region, data sovereignty, and automation in ML infrastructure. It covers PrivateLink, cross-region routing, drift detection, and canary-controlled model updates.\n\n## Key Concepts\n- Data locality with PrivateLink\n- Regional SageMaker Endpoints and Global routing\n- Drift detection and automated retraining\n- Canary deployments and Model Registry governance\n\n## Code Example\n```python\n# pseudo: trigger retraining on drift metric threshold\nif drift_metric > threshold:\n    run_pipeline('retrain')\n```\n\n## Follow-up Questions\n- How would you monitor data residency violations?","diagram":"flowchart TD\n  A[On-Prem Data] -->|PrivateLink| B[Regional SageMaker Endpoint A]\n  A -->|PrivateLink| C[Regional SageMaker Endpoint B]\n  A -->|PrivateLink| D[Regional SageMaker Endpoint C]\n  B --> E[Drift Detector]\n  C --> E\n  D --> E\n  E --> F[SageMaker Pipelines Retrain]\n  F --> G[Model Registry Canary]\n  G --> H[Traffic Router]\n  H --> I[Real-time Inference]\n","difficulty":"advanced","tags":["aws-ml-specialty"],"channel":"aws-ml-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Meta","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T05:02:11.642Z","createdAt":"2026-01-15T02:12:27.097Z"},{"id":"q-2151","question":"Design a data-quality and feature-drift monitoring plan for a real-time fraud-detection inference service deployed as SageMaker Endpoints across four Regions using PrivateLink. Outline detection of shifts in feature distributions before inference, retraining triggers via SageMaker Pipelines, and canary rollouts with rollback criteria. Include data provenance, access control, and governance?","answer":"Baseline feature distributions and data quality checks per region; monitor drift with KL divergence and population drift thresholds. Use SageMaker Feature Store for per-region features, Model Monitor ","explanation":"## Why This Is Asked\nAssesses practical drift-detection, feature governance, and automated retraining in a multi-region SageMaker setup.\n\n## Key Concepts\n- Data quality and feature-drift monitoring\n- SageMaker Feature Store and Model Monitor\n- SageMaker Pipelines and canary rollouts\n- PrivateLink, governance, data provenance\n\n## Code Example\n```python\nimport numpy as np\ndef kl(p, q):\n    p = p / p.sum()\n    q = q / q.sum()\n    return float(np.sum(np.where((p>0)&(q>0), p * np.log(p/q), 0)))\n```\n\n## Follow-up Questions\n- How would you validate drift thresholds across regions with different data volumes?\n- What metrics define a successful retraining canary rollout?","diagram":"flowchart TD\n  DataSource --> FeatureStore\n  FeatureStore --> ModelMonitor\n  ModelMonitor --> Pipelines\n  Pipelines --> Canary --> Production\n  Production --> Rollback","difficulty":"intermediate","tags":["aws-ml-specialty"],"channel":"aws-ml-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","MongoDB","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T05:34:01.463Z","createdAt":"2026-01-15T05:34:01.465Z"},{"id":"q-876","question":"You're deploying a SageMaker real-time endpoint for a model expected to see bursty, unpredictable traffic. Propose a concrete autoscaling setup using AWS Application Auto Scaling that keeps latency under a target while never scaling to zero. Specify min and max instances, the metric and target value (latency or invocations), the policy type, and cooldowns; discuss validation steps?","answer":"Configure a target-tracking policy on the endpoint with min 1, max 20 instances. Use SageMakerEndpointLatency (p95) as the predefined metric with target value 0.25s; set ScaleOutCooldown 300s and Scal","explanation":"## Why This Is Asked\n\nAssesses practical autoscaling setup for real-time endpoints, focusing on latency control, non-zero minimum, and stable scaling behavior.\n\n## Key Concepts\n\n- SageMaker real-time endpoints\n- AWS Application Auto Scaling\n- Target tracking vs step scaling\n- Latency vs concurrency metrics\n- Cooldown and stability\n\n## Code Example\n\n```javascript\n{\n  \"PolicyName\": \"EndpointLatencyTargetTracking\",\n  \"PolicyType\": \"TargetTrackingScaling\",\n  \"TargetTrackingScalingPolicyConfiguration\": {\n    \"TargetValue\": 0.25,\n    \"PredefinedMetricSpecification\": {\n      \"PredefinedMetricType\": \"SageMakerEndpointLatency\"\n    },\n    \"ScaleOutCooldown\": 300,\n    \"ScaleInCooldown\": 600\n  }\n}\n```\n\n## Follow-up Questions\n\n- How would you validate the setup under burst traffic?\n- How would you prevent over-scaling in steady-state?","diagram":null,"difficulty":"beginner","tags":["aws-ml-specialty"],"channel":"aws-ml-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Lyft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T13:56:45.598Z","createdAt":"2026-01-12T13:56:45.598Z"},{"id":"q-896","question":"You run a SageMaker real-time endpoint serving a risk-scoring model for payments. After a drift alert, outline a canary deployment plan using endpoint variants and the Model Registry to shift 20% of traffic to a new version while preserving latency and safety. Describe how you automate metric validation (latency, error rate, and drift), rollback triggers, and guardrails, and how you promote a stable canary to baseline?","answer":"Design a canary deployment with two endpoint variants (Canary 0.2, Baseline 0.8) via a new EndpointConfig and Model Registry version. Automate with Step Functions to monitor p95 latency <180 ms, error","explanation":"## Why This Is Asked\nAssesses practical real-time deployment skills, canary traffic shifts, and automated rollback using SageMaker features.\n\n## Key Concepts\n- Endpoint variants and traffic shifting\n- Model Registry versioning\n- Automated validation windows and rollback triggers\n- Drift detection and monitoring integration\n\n## Code Example\n```python\n# Example using boto3\nimport boto3\nsm = boto3.client('sagemaker')\nsm.update_endpoint(\n  EndpointName='risk-endpoint',\n  DesiredWeightsAndVariants=[\n    {'VariantName': 'Canary', 'DesiredWeight': 0.2},\n    {'VariantName': 'Baseline', 'DesiredWeight': 0.8}\n  ]\n)\n```\n\n## Follow-up Questions\n- How would you scale back if latency spikes occur?  \n- What monitoring alerts would you configure and why?","diagram":"flowchart TD\n  A[New Model Version in Model Registry] --> B[Create EndpointConfig with Canary + Baseline]\n  B --> C[Update Endpoint Weights (Canary 0.2, Baseline 0.8)]\n  C --> D[CloudWatch + SageMaker Drift Monitoring]\n  D --> E{Stable for 15 min?}\n  E -->|Yes| F[Promote Canary to Baseline]\n  E -->|No| G[Rollback to Baseline]\n  G --> H[Notify Stakeholders]","difficulty":"intermediate","tags":["aws-ml-specialty"],"channel":"aws-ml-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","PayPal","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T14:36:26.853Z","createdAt":"2026-01-12T14:36:26.853Z"},{"id":"q-969","question":"In a production AWS ML pipeline, you must serve multiple fraud-detection models across two regions using a SageMaker Multi-Model Endpoint (MME). Propose a concrete deployment and autoscaling strategy that keeps p95 latency under 200 ms during peak, prevents cold starts, and optimizes memory by loading only active models. Describe per-model versioning with SageMaker Model Registry, traffic routing, canary validation, rollback triggers, cost implications, and cross-region consistency?","answer":"Proposed: use a SageMaker Multi-Model Endpoint (MME) with memory budgets per model and on-demand loading to fit bursts. Scale the endpoint via Application Auto Scaling on latency (p95 target 200 ms), ","explanation":"## Why This Is Asked\n\nThis question tests practical, scale-aware deployment of MME with versioning, cross-region consistency, and robust rollback.\n\n## Key Concepts\n\n- SageMaker Multi-Model Endpoint\n- Application Auto Scaling with latency targets\n- SageMaker Model Registry for versioning\n- Canary deployment and traffic routing\n- Drift monitoring and rollback\n\n## Code Example\n\n```javascript\n# Python-like pseudocode for boto3 usage\nimport boto3\nautoscaler = boto3.client('application-autoscaling')\nautoscaler.register_scalable_target(\n  ServiceNamespace='sagemaker',\n  ResourceId='endpoint/MMEEndpoint',\n  ScalableDimension='sagemaker:variant:DesiredInstanceCount',\n  MinCapacity=2,\n  MaxCapacity=8\n)\n```\n\n## Follow-up Questions\n\n- How would you budget memory per model to avoid OOM across models in MME?\n- How would you ensure cross-region parity during canary rollouts?","diagram":null,"difficulty":"advanced","tags":["aws-ml-specialty"],"channel":"aws-ml-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Netflix","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T17:33:28.367Z","createdAt":"2026-01-12T17:33:28.367Z"}],"subChannels":["general"],"companies":["Adobe","Amazon","Bloomberg","Cloudflare","Coinbase","Discord","Goldman Sachs","Google","Hugging Face","IBM","Instacart","LinkedIn","Lyft","Meta","Microsoft","MongoDB","Netflix","PayPal","Plaid","Robinhood","Salesforce","Slack","Snap","Snowflake","Square","Stripe","Tesla","Twitter","Two Sigma","Uber","Zoom"],"stats":{"total":22,"beginner":5,"intermediate":8,"advanced":9,"newThisWeek":22}}