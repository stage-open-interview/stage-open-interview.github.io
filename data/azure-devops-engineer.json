{"questions":[{"id":"azure-devops-engineer-design-build-release-1768163011870-0","question":"You’re designing a CI/CD pipeline for a microservices app deployed to AKS. You need to deploy to dev, test, staging, and production with environment-specific approvals and automated checks, and production should be blocked if tests or gates fail in earlier stages. Which Azure Pipelines approach best meets these requirements?","answer":"[{\"id\":\"a\",\"text\":\"Create separate classic release pipelines for each environment and chain approvals between them.\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Create a single multi-stage YAML pipeline with environments for each stage and configure approvals and checks on the relevant environments.\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Place all steps in a single script in the build pipeline and promote artifacts manually to production.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Deploy the same deployment job to all environments in parallel to minimize total duration.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct because a multi-stage YAML pipeline can define distinct environments (dev, test, staging, prod) with per-environment approvals and checks. This enables gating so that production deployments only proceed if prior stages pass and required gates (tests, security scans) succeed. \n\n## Why Other Options Are Wrong\n- Option A: Using separate classic release pipelines increases operational overhead and makes gating harder to enforce consistently across environments. \n- Option C: A single build script with manual promotion lacks automated, auditable governance and gating across multiple environments. \n- Option D: Deploying the same job in parallel bypasses environment-specific approvals and checks, undermining governance.\n\n## Key Concepts\n- Multi-stage YAML pipelines\n- Environments and approvals in Azure Pipelines\n- Release gates and deployment approvals\n\n## Real-World Application\nThis pattern provides a single source of truth for release flow, enabling repeatable, auditable deployments with governance across environments and automated checks before production.","diagram":null,"difficulty":"intermediate","tags":["AzurePipelines","Kubernetes","AKS","Helm","Terraform","CI/CD","EKS","AWS","certification-mcq","domain-weight-40"],"channel":"azure-devops-engineer","subChannel":"design-build-release","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T20:23:31.871Z","createdAt":"2026-01-11 20:23:32"},{"id":"azure-devops-engineer-design-build-release-1768163011870-1","question":"In a large monorepo, you want to speed builds by caching dependencies across runs. Which Azure Pipelines feature should you use?","answer":"[{\"id\":\"a\",\"text\":\"Publish build artifacts and download them in subsequent jobs.\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use the Cache task to store dependencies (by a key) and restore them in subsequent runs.\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Disable caching to avoid stale dependencies.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Move to self-hosted runners only.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct because the Cache task stores dependencies (for example npm install, pip install) between runs, using a key that typically depends on relevant files. This reduces build times by avoiding re-installing unchanged dependencies. \n\n## Why Other Options Are Wrong\n- Option A describes artifacts caching for transfer, not dependency caching, which may not significantly reduce build times. \n- Option C contradicts the goal of speeding builds. \n- Option D can help performance in some cases but does not address dependency caching directly and adds maintenance overhead.\n\n## Key Concepts\n- Azure Pipelines Cache task\n- Dependency caching and cache keys\n\n## Real-World Application\nConfigure your cache key to include dependency manifest files (e.g., package.json, requirements.txt) so caches refresh when dependencies change, keeping builds fast and correct.","diagram":null,"difficulty":"intermediate","tags":["AzurePipelines","Kubernetes","AKS","Terraform","CI/CD","Caching","EKS","AWS","certification-mcq","domain-weight-40"],"channel":"azure-devops-engineer","subChannel":"design-build-release","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T20:23:32.380Z","createdAt":"2026-01-11 20:23:32"},{"id":"azure-devops-engineer-design-build-release-1768163011870-2","question":"Deploying to AKS with Helm, you want to ensure container images are scanned for vulnerabilities before deployment. Which approach should you add to the pipeline?","answer":"[{\"id\":\"a\",\"text\":\"Rely on Kubernetes admission controllers to block vulnerable images.\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Add a container image scanning step in the pipeline (for example using Trivy) and fail the run on critical CVEs.\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Perform security checks only after deployment.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Manually review images in a separate process.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct because integrating an automated image scanner (like Trivy) in the CI/CD pipeline prevents deploying vulnerable images and can fail the run on critical CVEs, providing fast feedback. \n\n## Why Other Options Are Wrong\n- Option A relies on cluster-level controls which may be bypassed or misconfigured and may not provide fast feedback in CI/CD. \n- Option C delays vulnerability detection until after deployment, increasing risk. \n- Option D introduces manual steps that slow delivery and reduce consistency.\n\n## Key Concepts\n- Container image security scanning\n- CI/CD gates and automated fail-on-detection\n- Tools like Trivy, Clair, or Aqua\n\n## Real-World Application\nAutomated pre-deployment scanning is a best practice for ensuring image security in Kubernetes environments, enabling teams to enforce compliance before any production impact.","diagram":null,"difficulty":"intermediate","tags":["AzurePipelines","Kubernetes","AKS","Helm","Terraform","Trivy","CI/CD","AWS","EKS","certification-mcq","domain-weight-40"],"channel":"azure-devops-engineer","subChannel":"design-build-release","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T20:23:32.889Z","createdAt":"2026-01-11 20:23:32"},{"id":"azure-devops-engineer-design-source-control-1768202940302-0","question":"In an Azure DevOps project, you manage a single repository with branches main, release/v1.0, and feature/*. You want to ensure that PRs merging into main cannot complete unless a Build Validation policy has completed successfully for the associated pipeline. What is the recommended configuration to enforce this?","answer":"[{\"id\":\"a\",\"text\":\"Rely on manual code reviews and document a build step in the PR description\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Configure a Build Validation policy on the main branch that references the YAML pipeline and require a successful build before PR completion\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Enforce a client-side pre-commit hook to run builds before allowing pushes to main\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Create a separate validation branch and merge it into main after a successful build\",\"isCorrect\":false}]","explanation":"## Correct Answer\nConfigure a Build Validation policy on the main branch that references the YAML pipeline and require a successful build before PR completion. This gates merges with automated CI results, ensuring code quality and consistency.\n\n## Why Other Options Are Wrong\n- Relying on manual reviews (A) does not guarantee a successful build and can miss failures.\n- Client-side pre-commit hooks (C) are not enforceable across all contributors and environments.\n- A separate validation branch (D) adds latency and complexity without providing the same enforceable gate as a PR-based Build Validation policy.\n\n## Key Concepts\n- Branch policies in Azure DevOps\n- Build Validation policy integration with YAML pipelines\n- PR gating and CI/CD quality gates\n\n## Real-World Application\nApply Build Validation on main for all release-ready PRs, and link the policy to the exact pipeline used in your CI/CD process to prevent merges when builds fail.\n","diagram":null,"difficulty":"intermediate","tags":["Terraform","Kubernetes","AWS CodeCommit","Azure Repos","certification-mcq","domain-weight-15"],"channel":"azure-devops-engineer","subChannel":"design-source-control","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T07:29:00.304Z","createdAt":"2026-01-12 07:29:00"},{"id":"azure-devops-engineer-design-source-control-1768202940302-1","question":"Your organization uses a mono-repo and several teams share components. To reduce repository bloat and enable independent development of shared modules, you consider Git submodules. Which statement best describes the trade-offs of using submodules in a collaborative Azure DevOps environment?","answer":"[{\"id\":\"a\",\"text\":\"Submodules decouple histories and allow independent versioning, but cloning and syncing require extra explicit commands and careful coordination\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Submodules automatically fetch and update nested repositories during clone, reducing setup effort for new contributors\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Submodules merge automatically into the parent repository following commits in the child repositories\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Submodules duplicate the entire history of the child repos inside the parent repo, increasing size and complexity\",\"isCorrect\":false}]","explanation":"## Correct Answer\nSubmodules decouple histories and allow independent versioning, but cloning and syncing require extra explicit commands and careful coordination. This trade-off is central to deciding whether to adopt submodules in a team environment.\n\n## Why Other Options Are Wrong\n- Submodules do not auto-fetch or auto-update (B); they require explicit commands to initialize and update.\n- Submodules do not merge automatically into the parent (C); they remain separate repos referenced by the parent.\n- Submodules do not duplicate full histories into the parent (D); they reference separate repos, so the parent’s size is not inflated by full histories.\n\n## Key Concepts\n- Git submodules vs. subtrees\n- Initialization and update workflows\n- Impact on CI/CD and onboarding\n\n## Real-World Application\nUse submodules only when components must evolve independently with clear ownership; provide automation scripts to initialize/update submodules in CI pipelines and document the workflow for contributors.\n","diagram":null,"difficulty":"intermediate","tags":["Terraform","Kubernetes","AWS CodeCommit","Azure Repos","certification-mcq","domain-weight-15"],"channel":"azure-devops-engineer","subChannel":"design-source-control","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T07:29:00.926Z","createdAt":"2026-01-12 07:29:01"},{"id":"azure-devops-engineer-design-source-control-1768202940302-2","question":"A release branch pattern release/v2.x is used and Azure Boards is integrated with Azure Repos. You want to enforce that every PR targeting release/v2.x is linked to at least one Azure Boards work item and that the PR title contains the associated work item ID. Which combination best achieves this governance in Azure DevOps?","answer":"[{\"id\":\"a\",\"text\":\"Enable the Work item linking policy on the target branch and implement a PR title convention that includes the work item ID\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Enable only the Build Validation policy on the target branch\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Require minimum two reviewers and disable work item linking\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Rely on repository-level policies and rely on post-merge audits to verify linking\",\"isCorrect\":false}]","explanation":"## Correct Answer\nEnable the Work item linking policy on the target branch to ensure PRs are associated with at least one Azure Boards work item, and implement a PR title convention that includes the work item ID to promote traceability in reviews and dashboards.\n\n## Why Other Options Are Wrong\n- Build Validation alone (B) does not guarantee work item linkage.\n- Requiring two reviewers without linking (C) does not enforce traceability to work items.\n- Relying on post-merge audits (D) leaks governance to after-the-fact checks and misses opportunities for early quality gates.\n\n## Key Concepts\n- Azure DevOps branch policies: Work item linking\n- PR title/description governance\n- End-to-end traceability with Azure Boards\n\n## Real-World Application\nApply Work item linking on the release/* target branch and enforce a naming convention in PR titles to ensure each PR is tied to a work item from Azure Boards, facilitating traceability in audits and reporting.\n","diagram":null,"difficulty":"intermediate","tags":["Terraform","Kubernetes","AWS CodeCommit","Azure Repos","certification-mcq","domain-weight-15"],"channel":"azure-devops-engineer","subChannel":"design-source-control","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T07:29:01.442Z","createdAt":"2026-01-12 07:29:01"}],"subChannels":["design-build-release","design-source-control"],"companies":[],"stats":{"total":6,"beginner":0,"intermediate":6,"advanced":0,"newThisWeek":6}}