{"questions":[{"id":"q-1158","question":"How would you implement a health check mechanism for a load balancer that uses exponential backoff for failed servers, and how does this approach prevent cascading failures during partial outages?","answer":"Implement health checks with exponential backoff to gradually increase retry intervals for failed servers, preventing thundering herd problems and allowing recovery time during partial outages.","explanation":"A health check mechanism with exponential backoff starts by marking a server as failed after consecutive failed health checks (typically 3-5). Instead of immediately retrying the server, the load balancer waits for an initial backoff period (e.g., 5 seconds) before the next health check. If the server is still unhealthy, the backoff period doubles (10s, 20s, 40s, etc.) up to a maximum threshold (e.g., 5 minutes). This prevents the load balancer from wasting resources on constantly checking failed servers and avoids overwhelming recovering servers with immediate traffic.\n\nThe key benefit is preventing cascading failures during partial outages. When multiple servers fail simultaneously, exponential backoff ensures they don't all recover at the same time, which could overwhelm the remaining healthy servers or the recovering servers themselves. The gradual reintroduction of servers allows the system to stabilize and properly distribute load as each server comes back online. This approach is particularly effective in microservices architectures where service dependencies can create failure cascades.\n\nImplementation typically involves maintaining a health state for each server with fields like: failure_count, last_check_time, backoff_multiplier, and next_check_time. The load balancer's routing logic excludes servers in 'failed' state from traffic distribution, while a background scheduler periodically performs health checks according to the exponential backoff schedule. Once a server passes a health check, it's immediately marked as healthy and returned to the active pool, with its backoff state reset.","diagram":null,"difficulty":"intermediate","tags":["load-balancing","health-checks","exponential-backoff","fault-tolerance","cascading-failures"],"channel":"algorithms","subChannel":"algorithms","sourceUrl":null,"videos":null,"companies":[],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T03:29:00.972Z","createdAt":"2026-01-13T03:29:00.972Z"},{"id":"q-2121","question":"How would you implement a load balancer that uses predictive scaling based on request patterns and historical data, and what machine learning techniques would you use to forecast traffic spikes?","answer":"Implement a predictive load balancer using time series forecasting models like ARIMA or LSTM to analyze historical request patterns, then proactively scale server capacity and adjust distribution algorithms.","explanation":"To implement a predictive load balancer, I would first collect historical metrics including request rates, response times, server utilization, and seasonal patterns. The system would use time series forecasting models like ARIMA for simpler patterns or LSTM neural networks for complex, non-linear traffic patterns. These models would predict traffic volumes for the next 15-60 minutes with confidence intervals.\n\nThe load balancer would then adjust its distribution strategy based on these predictions. For predicted high-traffic periods, it would pre-warm additional servers and shift from round-robin to weighted distribution algorithms that account for predicted server load. During low-traffic periods, it would consolidate requests to fewer servers to optimize resource utilization.\n\nKey machine learning techniques would include:\n\n1. **Time Series Analysis**: ARIMA models for capturing trend and seasonality in predictable traffic patterns\n2. **Deep Learning**: LSTM networks for modeling complex, non-linear relationships in traffic data\n3. **Anomaly Detection**: Isolation Forest or Autoencoders to identify unusual traffic spikes that deviate from patterns\n4. **Ensemble Methods**: Combining multiple models to improve prediction accuracy\n\nThe system would continuously retrain models with new data and implement a feedback loop where actual traffic patterns are compared against predictions to improve model accuracy over time.","diagram":null,"difficulty":"intermediate","tags":["predictive-scaling","machine-learning","time-series-forecasting","load-balancing"],"channel":"algorithms","subChannel":"algorithms","sourceUrl":null,"videos":null,"companies":[],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T04:53:20.022Z","createdAt":"2026-01-15T03:28:03.022Z"},{"id":"q-2617","question":"How would you implement a consistent hashing load balancer that minimizes server remapping when adding or removing nodes, and what data structures would you use to achieve O(log n) lookup time?","answer":"Implement a consistent hashing load balancer using a hash ring with virtual nodes and a balanced tree structure (such as TreeMap in Java or std::map in C++) to map request hashes to server nodes, achieving minimal remapping and O(log n) lookup time.","explanation":"Consistent hashing addresses the major drawback of traditional hash-based load balancing, where adding or removing servers requires remapping the entire key space. The solution involves mapping both servers and requests onto a circular hash space (typically 0 to 2^32-1). Each request is routed to the first server encountered when moving clockwise from the request's hash position. When servers are added or removed, only requests in the adjacent hash segments require remapping, significantly minimizing disruption.\n\nFor implementation, I would use a balanced tree data structure (TreeMap in Java, std::map in C++, or similar structures) to maintain sorted server positions on the ring. This enables O(log n) lookup time for finding the next server using tree traversal operations. To ensure even distribution and handle load imbalances, I would implement virtual nodes—multiple hash positions per physical server—reducing the impact of uneven hash distribution and providing better load balancing across the cluster.","diagram":null,"difficulty":"intermediate","tags":["consistent-hashing","load-balancing","distributed-systems","data-structures"],"channel":"algorithms","subChannel":"algorithms","sourceUrl":null,"videos":null,"companies":[],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T04:59:42.808Z","createdAt":"2026-01-16T03:27:36.252Z"},{"id":"q-2620","question":"How would you implement a consistent hashing load balancer that minimizes data remapping when servers are added or removed, and what trade-offs exist between ring size and lookup performance?","answer":"Consistent hashing maps servers and keys to a virtual ring, ensuring only 1/n of data moves when servers change. Trade-offs include larger rings for better distribution vs. slower lookups.","explanation":"Consistent hashing solves the problem of data remapping in traditional hash-based load balancing. Instead of using a simple hash modulo server count, we map both servers and request keys onto a virtual ring (typically 0-2^32). Each key is assigned to the first server encountered clockwise on the ring. When a server is added or removed, only keys between adjacent servers need remapping, minimizing disruption.\n\nImplementation involves creating a hash ring with virtual nodes (replicas) for each physical server to ensure even distribution. For lookup, we can use binary search on the sorted ring positions or a tree structure. The number of virtual replicas per server affects distribution uniformity - too few leads to hot spots, too many increases memory usage and lookup time.\n\nKey trade-offs include ring size vs. memory footprint, virtual node count vs. distribution quality, and lookup algorithm choice. Real-world systems like Dynamo and Cassandra use 100-1000 virtual nodes per server. For high-throughput systems, a bounded hash ring with caching recent lookups provides optimal performance.","diagram":null,"difficulty":"intermediate","tags":["consistent-hashing","load-balancing","distributed-systems","hash-rings","scalability"],"channel":"algorithms","subChannel":"algorithms","sourceUrl":null,"videos":null,"companies":[],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T03:28:36.067Z","createdAt":"2026-01-16T03:28:36.067Z"},{"id":"q-3108","question":"How would you implement a load balancer with health checks that automatically removes failed servers and reintegrates them when they recover, while maintaining session affinity and handling graceful degradation?","answer":"Implement a comprehensive load balancer with configurable health check monitoring, dynamic server state management, consistent hashing for session affinity, and circuit breaker patterns for graceful degradation.","explanation":"To implement this robust load balancer, I would begin by establishing a comprehensive health check system that continuously monitors server availability through configurable HTTP or TCP probes. The health checker would maintain server states—healthy, unhealthy, and degraded—with adjustable intervals, timeout thresholds, and failure thresholds. When servers fail consecutive health checks, they are automatically removed from the active rotation but retained in a recovery pool for periodic re-evaluation.\n\nFor session affinity, I would implement consistent hashing to map client identifiers (such as IP addresses or session IDs) to specific servers. This approach ensures that as long as the target server remains healthy, clients maintain their session continuity. The consistent hashing algorithm also provides optimal load distribution while minimizing session disruption when servers are added or removed.\n\nTo handle graceful degradation, I would integrate circuit breaker patterns that detect and respond to server performance issues beyond simple availability checks. The system would monitor response times, error rates, and connection patterns to proactively route traffic away from degrading servers before complete failure occurs. Additionally, I would implement failover mechanisms with warm standby servers and request queuing to handle traffic spikes during server recovery periods.","diagram":null,"difficulty":"intermediate","tags":["load-balancing","health-checks","session-affinity","circuit-breaker","graceful-degradation"],"channel":"algorithms","subChannel":"algorithms","sourceUrl":null,"videos":null,"companies":[],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T04:52:07.691Z","createdAt":"2026-01-17T03:19:51.611Z"},{"id":"q-3109","question":"How would you implement a load balancer that uses exponential weighted moving average (EWMA) to track server response times and dynamically adjust traffic distribution, and how would you handle cold start problems for new servers?","answer":"Implement an exponential weighted moving average (EWMA) algorithm to track server response times, calculate dynamic weights for traffic distribution based on these metrics, and apply gradual traffic ramp-up strategies for new servers to mitigate cold start problems.","explanation":"To implement an EWMA-based load balancer, I would maintain a response time metric for each server using the formula: new_avg = (α × current_response_time) + ((1-α) × previous_avg). The α parameter (typically 0.1-0.3) determines how quickly the average adapts to new measurements. Each server would have an associated weight inversely proportional to its EWMA response time, with faster servers receiving more traffic.\n\nFor the load balancing algorithm, I would use a weighted round-robin approach where server selection probability is calculated as: weight_i = (baseline_response_time / ewma_response_time_i). This ensures that servers with lower response times receive proportionally more requests.\n\nTo address cold start problems for new servers, I would implement a gradual ramp-up strategy where new servers initially receive a small percentage of traffic (e.g., 5-10%) and gradually increase their share as sufficient response time data is collected. Additionally, I would apply a minimum traffic threshold to ensure new servers receive enough requests to establish meaningful EWMA metrics, and consider using initialization techniques like warming up new servers with health check requests before directing production traffic.","diagram":null,"difficulty":"intermediate","tags":["load-balancing","ewma","performance-metrics","cold-start","weighted-round-robin"],"channel":"algorithms","subChannel":"algorithms","sourceUrl":null,"videos":null,"companies":[],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T04:47:24.115Z","createdAt":"2026-01-17T03:20:03.780Z"},{"id":"q-4595","question":"How would you implement a load balancer with health checks that automatically removes failed servers and reintegrates them when they recover, while maintaining consistent request distribution during server state changes?","answer":"Implement a load balancer with periodic health checks that automatically removes failed servers from the active pool, utilizes a circular buffer to track recovery attempts, and smoothly reintegrates recovered servers with gradual traffic ramping to prevent overwhelming them.","explanation":"The solution requires three main components: health checking, server state management, and traffic distribution adaptation. First, implement periodic health checks (HTTP GET, TCP connection, or custom probes) that run every 5-30 seconds depending on your tolerance for detection latency. Use a separate thread pool to avoid blocking the main load balancing logic. Maintain server states in a hash map with enum values (HEALTHY, UNHEALTHY, RECOVERING) and track consecutive failure/success counts to prevent flapping between states due to transient issues.\n\nFor server removal, use a copy-on-write pattern to maintain thread safety when updating the active server pool. When a server fails consecutive health checks (typically 3-5 failures), transition it to UNHEALTHY and remove it from the active rotation. For recovery, implement a exponential backoff strategy with a circular buffer tracking recent recovery attempts. Once a server passes health checks again, place it in RECOVERING state and gradually increase traffic using a weighted round-robin approach, starting at 10-20% of normal load and scaling up over several minutes.\n\nThe traffic distribution algorithm must dynamically adjust weights based on server states. Use consistent hashing or weighted round-robin that accounts for current server capacity, ensuring even distribution across the healthy pool while accommodating the gradual ramp-up of recovering servers.","diagram":null,"difficulty":"intermediate","tags":["load-balancing","health-checks","fault-tolerance","distributed-systems"],"channel":"algorithms","subChannel":"algorithms","sourceUrl":null,"videos":null,"companies":[],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T05:45:49.581Z","createdAt":"2026-01-20T03:53:32.118Z"},{"id":"q-4597","question":"How would you implement a load balancer with health checks and circuit breaker patterns to handle server failures gracefully, and what strategies would you use to prevent cascading failures during partial outages?","answer":"I would implement health checks with configurable intervals and failure thresholds, integrate circuit breakers to prevent traffic to failing servers, and employ fallback strategies such as graceful degradation and request queuing to maintain service availability during partial outages.","explanation":"To implement a robust load balancer with health checks, I would create a health monitoring service that periodically assesses each server using HTTP requests, TCP connections, or custom health endpoints. Each server would maintain a health status with configurable failure thresholds (e.g., marked unhealthy after three consecutive failures) and recovery thresholds (e.g., marked healthy after five consecutive successes). The health check interval should be adaptive—more frequent during suspected issues and less frequent during stable periods.\n\nFor circuit breaker implementation, I would use a state machine with three states: closed (normal operation), open (blocking requests), and half-open (testing recovery). When the failure rate exceeds a threshold, the circuit breaker opens to prevent cascading failures. After a timeout period, it transitions to half-open to test the service with limited traffic.\n\nTo prevent cascading failures during partial outages, I would implement request timeouts with exponential backoff, retry policies with jitter, and bulkhead patterns to isolate failures. Additionally, I would employ fallback mechanisms such as serving cached responses, routing to backup services, or implementing graceful degradation by disabling non-critical features while maintaining core functionality.","diagram":null,"difficulty":"intermediate","tags":["load-balancing","health-checks","circuit-breaker","fault-tolerance","distributed-systems"],"channel":"algorithms","subChannel":"algorithms","sourceUrl":null,"videos":null,"companies":[],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T05:44:43.804Z","createdAt":"2026-01-20T03:55:38.206Z"},{"id":"q-5510","question":"How would you implement a load balancer with circuit breaker functionality that automatically detects and isolates failing servers, and what failure detection strategies would you use to balance between quick failure detection and avoiding false positives?","answer":"Implement health checks with failure thresholds, use exponential backoff for recovery, and apply sliding window or hysteresis to prevent flapping between healthy/unhealthy states.","explanation":"The core approach involves implementing health monitoring with configurable failure thresholds. Track consecutive failures for each server using a sliding window (e.g., 5 failures in last 10 requests) or consecutive failure count. When thresholds are exceeded, mark the server as 'unhealthy' and stop routing traffic to it. For recovery, implement exponential backoff with periodic health checks, gradually increasing check intervals as the server remains stable.\n\nKey design decisions include choosing between active health checks (sending pings/test requests) versus passive monitoring (observing actual request failures). Active checks provide faster detection but add overhead, while passive monitoring is more efficient but slower to detect issues. Implement hysteresis by requiring different thresholds for failure (e.g., 3 consecutive failures) versus recovery (e.g., 5 consecutive successes) to prevent flapping.\n\nThe circuit breaker pattern adds a 'half-open' state where limited traffic is sent to test recovery. Use a token bucket or rate limiter to control test traffic volume. For distributed systems, consider using a shared state store (Redis/etcd) to coordinate circuit breaker state across multiple load balancer instances, ensuring consistent behavior.","diagram":null,"difficulty":"intermediate","tags":["circuit-breaker","health-checks","failure-detection","load-balancing"],"channel":"algorithms","subChannel":"algorithms","sourceUrl":null,"videos":null,"companies":[],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T03:45:12.651Z","createdAt":"2026-01-22T03:45:12.651Z"},{"id":"q-6467","question":"How would you implement a load balancer with health checks that automatically removes failed servers and reintegrates them when they recover, while ensuring no request loss during failover?","answer":"Implement a health checking system with circuit breaker pattern, maintain an active server pool, and use request buffering or connection draining for seamless failover.","explanation":"The solution requires implementing a comprehensive health check system that continuously monitors server availability through HTTP endpoints, TCP connections, or custom health probes. Each server should maintain a health status with timestamps and failure counters. Utilize a circuit breaker pattern where servers transition through states: HEALTHY → DEGRADED → UNHEALTHY, with configurable thresholds for failure detection and recovery attempts.\n\nFor failover handling, maintain an active server pool that excludes unhealthy servers while preserving them in a recovery pool. When a server fails, redistribute in-flight requests to healthy servers using connection draining to allow existing requests to complete gracefully. Implement request buffering for critical operations and use weighted round-robin or least-connections algorithms to distribute load effectively across the remaining healthy servers.","diagram":null,"difficulty":"intermediate","tags":["load-balancing","health-checks","circuit-breaker","failover","high-availability"],"channel":"algorithms","subChannel":"algorithms","sourceUrl":null,"videos":null,"companies":[],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T04:37:17.348Z","createdAt":"2026-01-24T03:24:37.275Z"},{"id":"q-653","question":"How would you implement a consistent hashing load balancer that handles server additions and removals with minimal key remapping? What data structures would you use and how would you handle virtual nodes?","answer":"Use a hash ring with TreeMap/SortedMap for O(log n) lookups, implement virtual nodes to distribute load evenly, and only remap keys between adjacent servers when topology changes.","explanation":"Consistent hashing solves the hot-spot problem of traditional hash-based load balancing by distributing keys across a circular hash space. The core implementation uses a sorted data structure (like TreeMap in Java or std::map in C++) to maintain the ring, allowing O(log n) lookup for which server should handle a given key. Each server is placed on the ring at multiple virtual node positions to ensure more uniform distribution.\n\nWhen a server is added, you only need to remap keys that would have gone to the next server clockwise on the ring. Similarly, when a server fails, its keys are redistributed to the next server clockwise. This minimizes the disruption compared to traditional hashing where adding/removing a server requires remapping all keys. Virtual nodes (typically 100-200 per physical server) help ensure even distribution and handle scenarios where physical servers have different capacities.\n\nIn practice, you'd hash both the server identifier and a virtual node index (like 'server1-vnode-0', 'server1-vnode-1') to place multiple points for each server. The choice of hash function (CRC32, MurmurHash, FNV) affects the distribution quality. For production systems, you'd also need to handle server health checks, weighted distribution for heterogeneous hardware, and potentially implement bounded loads to prevent any single server from becoming overloaded.","diagram":null,"difficulty":"intermediate","tags":["consistent-hashing","load-balancing","distributed-systems","hash-ring","virtual-nodes"],"channel":"algorithms","subChannel":"algorithms","sourceUrl":null,"videos":null,"companies":[],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-10T03:24:42.947Z","createdAt":"2026-01-10T03:24:42.947Z"},{"id":"q-659","question":"How would you implement a consistent hashing load balancer and what advantages does it provide over traditional hash-based load balancing when servers are added or removed?","answer":"Consistent hashing maps both servers and requests to a circular hash space, minimizing key remapping. It provides better distribution and reduces disruption when scaling compared to modulo hashing.","explanation":"Consistent hashing distributes requests across servers by mapping both server identifiers and request keys (like IP addresses or session IDs) to points on a circular hash space. When a request arrives, you hash its key and move clockwise around the ring until you find the next server. This approach minimizes the number of requests that need to be remapped when servers are added or removed, as only the adjacent segments are affected.\n\nThe key advantage over traditional hash-based load balancing (which uses modulo operations) is that consistent hashing only affects 1/n of the requests when a server is added or removed, where n is the total number of servers. With modulo hashing, adding or removing a server would require remapping almost all requests since the divisor changes. This makes consistent hashing ideal for distributed systems that need to scale dynamically.\n\nIn practice, you'd implement this with a hash function like MD5 or SHA-1, maintain a sorted data structure of server hash positions, and often use virtual nodes (multiple hash points per physical server) to improve distribution. This technique is widely used in distributed caches like DynamoDB and memcached clusters.","diagram":null,"difficulty":"intermediate","tags":["load-balancing","consistent-hashing","distributed-systems","scalability"],"channel":"algorithms","subChannel":"algorithms","sourceUrl":null,"videos":null,"companies":[],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T03:52:19.569Z","createdAt":"2026-01-11T03:52:19.569Z"},{"id":"q-6978","question":"How would you implement a load balancer that uses exponential weighted moving average (EWMA) to track server response times and dynamically route traffic to the fastest-performing servers, and how would you handle cold start scenarios?","answer":"Implement EWMA to track response times, route to fastest servers, and use gradual traffic ramp-up for cold starts.","explanation":"To implement an EWMA-based load balancer, maintain a moving average of response times for each server using the formula: new_avg = (α × latest_response_time) + ((1-α) × previous_avg). The smoothing factor α (typically 0.1-0.3) determines how quickly the algorithm adapts to new data. Each server's score would be inversely proportional to its EWMA response time, with the lowest score receiving the most traffic.\n\nFor the routing algorithm, use a weighted round-robin where weights are inversely proportional to EWMA scores. Servers with better response times get higher weights and receive more requests. Update the EWMA after each request completion, and periodically recalculate weights to reflect current performance. Include a minimum threshold to prevent servers with temporarily poor performance from being completely excluded.\n\nFor cold start scenarios, implement a gradual ramp-up mechanism where new servers initially receive a small percentage of traffic (e.g., 5-10%) regardless of their EWMA score. As the server accumulates response time data, gradually increase its traffic allocation based on its actual performance. Also implement a health check system that marks servers as available only after they pass initial connectivity and basic performance tests.","diagram":null,"difficulty":"intermediate","tags":["load-balancing","ewma","performance-monitoring","cold-start","algorithms"],"channel":"algorithms","subChannel":"algorithms","sourceUrl":null,"videos":null,"companies":[],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T03:51:51.237Z","createdAt":"2026-01-25T03:51:51.237Z"},{"id":"q-7485","question":"Design a load balancer that implements health checks with exponential backoff and circuit breaker patterns. How would you detect failing servers, handle recovery detection, and prevent cascading failures while maintaining low latency?","answer":"Implement health checks using exponential backoff to avoid overwhelming servers, use circuit breakers to temporarily isolate failing servers, and gradually restore traffic during recovery to prevent cascading failures. Health checks should start with frequent requests (e.g., every 5 seconds) and double the interval after each failure up to a maximum (e.g., 5 minutes). This prevents the load balancer from overwhelming already struggling servers while still allowing for reasonably quick failure detection. Circuit breakers track failure rates and trip when thresholds are exceeded, immediately routing traffic away from problematic servers without waiting for the next health check. Gradual traffic restoration can be implemented through weighted round-robin or canary deployments, slowly increasing traffic to recovering servers while monitoring their performance metrics.","explanation":"The solution combines three key patterns: exponential backoff health checks, circuit breakers, and gradual traffic restoration. Health checks start with frequent requests (every 5 seconds) and double the interval after each failure up to a maximum (5 minutes), preventing overwhelmed servers while maintaining timely failure detection. Circuit breakers monitor failure rates and automatically isolate problematic servers when thresholds are exceeded, providing immediate protection without waiting for scheduled health checks. During recovery, the system gradually reintroduces traffic through weighted algorithms or canary deployments, ensuring stability while monitoring performance metrics. This multi-layered approach maintains low latency by quickly routing around failures while preventing cascading failures through intelligent traffic management and server isolation.","diagram":null,"difficulty":"intermediate","tags":["load-balancing","health-checks","circuit-breaker","fault-tolerance","exponential-backoff"],"channel":"algorithms","subChannel":"algorithms","sourceUrl":null,"videos":null,"companies":[],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T05:20:21.676Z","createdAt":"2026-01-26T03:58:24.809Z"},{"id":"q-767","question":"Design a load balancer that implements adaptive load balancing using real-time server metrics. How would you collect and weight server performance data, and what algorithm would you use to dynamically adjust traffic distribution?","answer":"Implement an adaptive load balancer that monitors server metrics like CPU, memory, and response times, then uses a weighted algorithm that dynamically updates server weights based on performance trend","explanation":"An adaptive load balancer requires continuous monitoring of server health metrics. I'd implement a metrics collector that tracks CPU utilization, memory usage, active connections, and response times for each backend server. These metrics would be normalized and combined into a performance score, with lower scores indicating better server health. The collector would use exponential moving averages to smooth out temporary spikes and provide stable weight calculations.\n\nFor the balancing algorithm, I'd use a dynamic weighted least connections approach where server weights are inversely proportional to their performance scores. The weight calculation would be: weight = base_weight / (1 + performance_score), ensuring better-performing servers receive more traffic. Weights would be updated every 30-60 seconds to balance responsiveness with stability. The algorithm would also implement health checks to temporarily remove underperforming servers from rotation.\n\nReal-world implementation would include fallback mechanisms and hysteresis to prevent weight thrashing. For example, a server's weight would only decrease if performance degrades consistently over multiple sampling periods, and weights would have minimum and maximum bounds. This approach is commonly used in cloud environments where server performance can vary due to co-tenant interference or auto-scaling events.","diagram":null,"difficulty":"intermediate","tags":["adaptive-load-balancing","performance-monitoring","dynamic-weighting","real-time-metrics"],"channel":"algorithms","subChannel":"algorithms","sourceUrl":null,"videos":null,"companies":[],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T03:51:47.162Z","createdAt":"2026-01-12T03:51:47.162Z"},{"id":"al-1","question":"When would you choose a Linked List over an Array and what are the key trade-offs for each data structure?","answer":"Choose Linked Lists for frequent insertions/deletions at arbitrary positions (O(1) vs O(n) for arrays) and when memory allocation needs to be dynamic. Arrays excel at O(1) random access, better cache locality (15-30x faster due to spatial locality), and lower memory overhead (no extra pointers). Use Linked Lists in LRU caches, undo/redo systems, or when implementing queues/stacks dynamically.","explanation":"## Time Complexity Trade-offs\n- **Array**: O(1) access, O(n) insert/delete (shift elements)\n- **Linked List**: O(n) access, O(1) insert/delete at known position\n\n## Memory Considerations\n- **Array**: Contiguous memory, cache-friendly, no pointer overhead\n- **Linked List**: Non-contiguous, 8-16 bytes overhead per node (next/prev pointers)\n\n## Real-World Applications\n- **LRU Cache**: Doubly-linked list + hashmap for O(1) operations\n- **Text Editors**: Gap buffers (arrays) vs linked lists for text manipulation\n- **Browser History**: Linked lists for undo/redo functionality\n\n## When to Choose\n- **Array**: Fixed-size datasets, random access needed, memory-constrained environments\n- **Linked List**: Frequent size changes, insert/delete at ends or arbitrary positions\n\n## Performance Impact\nModern CPUs' cache lines (64 bytes) make arrays 20-50x faster for sequential access due to prefetching, making arrays preferable unless O(1) insertions are critical.","diagram":"flowchart TD\n  A[Data Structure Selection] --> B{Frequent Insertions/Deletions?}\n  B -->|Yes| C[Choose Linked List]\n  B -->|No| D{Need O(1) Random Access?}\n  D -->|Yes| E[Choose Array]\n  D -->|No| F{Memory Fragmentation Concern?}\n  F -->|Yes| G[Linked List - Dynamic Memory]\n  F -->|No| H[Array - Contiguous Memory]\n  C --> I[Node: Data + Pointer]\n  E --> J[Fixed Size Elements]\n  G --> K[Non-contiguous Storage]\n  H --> L[Cache-friendly Access]\n  I --> M[O(1) Insert/Delete at Head]\n  J --> N[O(1) Index Access]\n  K --> O[No Pre-allocation Needed]\n  L --> P[Better CPU Cache Performance]","difficulty":"beginner","tags":["struct","comparison","basics"],"channel":"algorithms","subChannel":"data-structures","sourceUrl":null,"videos":null,"companies":["Adobe","Amazon","Apple","Google","Meta","Microsoft"],"eli5":"Imagine you have a train of toy cars connected by hooks. That's a Linked List! If you want to add a new car in the middle, you just unhook two cars and hook the new one in between - super easy! But if you want to find the 5th car, you have to count from the front: 1, 2, 3, 4, 5. Now imagine a row of boxes on a shelf - that's an Array! Finding the 5th box is instant (just look at position 5!), but adding a new box in the middle means pushing all the other boxes over. So: need to add/remove things a lot? Use the train (Linked List). Need to quickly find things by position? Use the shelf (Array)!","relevanceScore":null,"voiceKeywords":["linked list","array","o(1)","o(n)","cache locality","memory overhead","lru cache"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2025-12-27T05:31:52.610Z","createdAt":"2025-12-26 12:51:05"},{"id":"al-165","question":"Implement a Trie data structure for efficient prefix search with insert, search, and startsWith operations. What are its advantages over hash maps for autocomplete systems, and what are the trade-offs?","answer":"Trie provides O(k) prefix search where k is word length, ideal for autocomplete. Space-efficient for common prefixes (e.g., 'pre' shared by 'prefix', 'prefixes'). Hash maps offer O(1) average lookup but can't efficiently find all keys with given prefix. Trie's hierarchical structure enables prefix enumeration, useful for type-ahead suggestions. Trade-offs: higher memory overhead per node, slower for exact matches vs hash maps.","explanation":"## Interview Context\nThis question tests data structure knowledge and trade-off analysis, crucial for optimizing search functionality in applications like autocomplete, spell checkers, and IP routing.\n\n## Key Concepts\n- **Trie Structure**: Tree-like data structure where each node represents a character\n- **Prefix Search**: Efficiently find all words starting with given prefix\n- **Space Complexity**: O(n*m) where n is number of words, m is average word length\n- **Time Complexity**: O(k) for insert/search/startsWith where k is word length\n\n## Implementation Details\n```javascript\nclass TrieNode {\n  constructor() {\n    this.children = {};\n    this.isEndOfWord = false;\n  }\n}\n\nclass Trie {\n  constructor() {\n    this.root = new TrieNode();\n  }\n  \n  insert(word) {\n    let node = this.root;\n    for (let char of word) {\n      if (!node.children[char]) {\n        node.children[char] = new TrieNode();\n      }\n      node = node.children[char];\n    }\n    node.isEndOfWord = true;\n  }\n  \n  search(word) {\n    let node = this.root;\n    for (let char of word) {\n      if (!node.children[char]) return false;\n      node = node.children[char];\n    }\n    return node.isEndOfWord;\n  }\n  \n  startsWith(prefix) {\n    let node = this.root;\n    for (let char of prefix) {\n      if (!node.children[char]) return false;\n      node = node.children[char];\n    }\n    return true;\n  }\n}\n```\n\n## Trade-offs vs Hash Maps\n- **Trie Advantages**: Prefix search O(k), space efficiency for shared prefixes, ordered traversal\n- **Hash Map Advantages**: Lower memory overhead, simpler implementation, O(1) average exact lookup\n- **Use Cases**: Tries for autocomplete/suggestions, hash maps for exact key-value storage\n\n## Follow-up Questions\n1. How would you implement delete operation in a Trie?\n2. What optimizations would you apply for a large-scale autocomplete system?\n3. How would you handle Unicode characters and different languages in a Trie?","diagram":"graph TD\n    A[Root] --> A1[a] --> P1[pp] --> P2[p] --> P3[l] --> E1[apple]\n    A --> A2[a] --> P4[pp] --> P5[l] --> E2[apply]\n    A --> A3[a] --> P6[p] --> P7[l] --> E3[application]\n    A --> A4[a] --> P8[pp] --> P9[l] --> E4[approach]\n    \n    style A fill:#FF6B6B\n    style E1 fill:#4ECDC4\n    style E2 fill:#4ECDC4\n    style E3 fill:#4ECDC4\n    style E4 fill:#95E1D3","difficulty":"intermediate","tags":["struct","basics"],"channel":"algorithms","subChannel":"data-structures","sourceUrl":null,"videos":null,"companies":["Amazon","Apple","Google","Meta","Microsoft"],"eli5":"Imagine building with LEGO blocks! A trie is like organizing your LEGOs by color first, then shape. When you want to find all the red blocks, you just go to the red box - instant! A hash map is like tossing all LEGOs in one big toy chest and having to dig through everything to find what you want. Tries are super fast because they share the first parts of words, just like how 'cat' and 'car' both start with 'c'. It's like having a shortcut that saves you time and space!","relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-25T16:40:15.110Z","createdAt":"2025-12-26 12:51:06"},{"id":"q-187","question":"How would you implement a thread-safe LRU cache using a HashMap and DoublyLinkedList, considering eviction policy and O(1) operations?","answer":"Use a HashMap for O(1) key-value lookups combined with a DoublyLinkedList to maintain access order. Implement thread safety using ReentrantReadWriteLock to allow concurrent reads while ensuring exclusive writes.","explanation":"## Concept Overview\nAn LRU (Least Recently Used) cache efficiently manages memory by evicting the least accessed items when capacity is reached. This implementation leverages a HashMap for fast key lookups and a DoublyLinkedList for maintaining access order.\n\n## Implementation Details\n- **HashMap**: Maps keys to Node references, enabling O(1) lookup operations\n- **DoublyLinkedList**: Maintains access order with head representing most recently used items and tail representing least recently used items\n- **Thread Safety**: ReentrantReadWriteLock provides optimal concurrency - multiple threads can read simultaneously while writes are mutually exclusive\n- **Eviction Policy**: When capacity is exceeded, the tail node (least recently used) is removed\n- **Operations**: Both get() and put() operations maintain O(1) time complexity\n\n## Code Example\n```java\npublic class LRUCache<K, V> {\n    private final Map<K, Node<K, V>> cache;\n    private final DoublyLinkedList<K, V> order;\n    private final ReentrantReadWriteLock lock;\n    private final int capacity;\n    \n    public LRUCache(int capacity) {\n        this.capacity = capacity;\n        this.cache = new HashMap<>();\n        this.order = new DoublyLinkedList<>();\n        this.lock = new ReentrantReadWriteLock();\n    }\n    \n    public V get(K key) {\n        lock.readLock().lock();\n        try {\n            Node<K, V> node = cache.get(key);\n            if (node != null) {\n                // Upgrade to write lock for reordering\n                lock.readLock().unlock();\n                lock.writeLock().lock();\n                order.moveToHead(node);\n                return node.value;\n            }\n            return null;\n        } finally {\n            if (lock.isWriteLocked()) {\n                lock.writeLock().unlock();\n            } else {\n                lock.readLock().unlock();\n            }\n        }\n    }\n    \n    public void put(K key, V value) {\n        lock.writeLock().lock();\n        try {\n            Node<K, V> existing = cache.get(key);\n            if (existing != null) {\n                existing.value = value;\n                order.moveToHead(existing);\n                return;\n            }\n            \n            Node<K, V> newNode = new Node<>(key, value);\n            cache.put(key, newNode);\n            order.addToHead(newNode);\n            \n            if (cache.size() > capacity) {\n                Node<K, V> tail = order.removeTail();\n                cache.remove(tail.key);\n            }\n        } finally {\n            lock.writeLock().unlock();\n        }\n    }\n}```","diagram":"graph TD\n    A[Client Request] --> B{Operation?}\n    B -->|get| C[Read Lock]\n    B -->|put| D[Write Lock]\n    C --> E[HashMap Lookup]\n    E --> F{Key Exists?}\n    F -->|Yes| G[Move to Head]\n    F -->|No| H[Return Null]\n    G --> I[Return Value]\n    D --> J{Key Exists?}\n    J -->|Yes| K[Update Value]\n    J -->|No| L[Create New Node]\n    K --> M[Move to Head]\n    L --> M\n    M --> N{Capacity Full?}\n    N -->|Yes| O[Remove Tail]\n    N -->|No| P[Add to Head]\n    O --> P\n    P --> Q[Release Lock]","difficulty":"intermediate","tags":["arrays","linkedlist","hashtable","heap"],"channel":"algorithms","subChannel":"data-structures","sourceUrl":null,"videos":null,"companies":["Amazon","Goldman Sachs","Google","Microsoft","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":["lru cache","hashmap","doublylinkedlist","eviction policy","thread-safe","reentrantreadwritelock","o(1) operations"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2026-01-02T06:41:40.643Z","createdAt":"2025-12-26 12:51:06"},{"id":"q-277","question":"How would you efficiently process a 50GB log file to extract the top 10 most frequent IP addresses from millions of entries while handling memory constraints and optimizing for performance?","answer":"Implement a streaming solution using Go with buffered I/O and a min-heap. Process the file in chunks using `bufio.Scanner`, count IP frequencies with a hash map, then maintain a top-10 min-heap. Use `sync.Pool` for memory reuse and worker pools for parallel processing. Python's `collections.Counter` with generators works for smaller datasets but lacks the memory efficiency and performance of Go for 50GB+ files.","explanation":"## Interview Context\nThis question tests distributed processing, memory management, and algorithmic optimization skills for senior engineering roles dealing with big data scenarios.\n\n## Technical Approach\n- **Memory Mapping**: Use `mmap` for zero-copy file access, avoiding data duplication\n- **Streaming Algorithm**: Process file in fixed-size chunks (64KB-1MB) with bounded memory\n- **Data Structure**: Min-heap (O(log n)) for top-10 maintenance vs full sort (O(n log n))\n- **Parallel Processing**: Split file into N chunks, process concurrently, then merge results\n\n## Go Implementation\n```go\ntype IPCounter struct {\n    counts map[string]int\n    heap   *MinHeap\n    mu     sync.RWMutex\n}\n\nfunc (ic *IPCounter) processChunk(chunk []byte) {\n    scanner := bufio.NewScanner(bytes.NewReader(chunk))\n    for scanner.Scan() {\n        ip := extractIP(scanner.Text())\n        ic.mu.Lock()\n        ic.counts[ip]++\n        ic.mu.Unlock()\n    }\n}\n\nfunc (ic *IPCounter) getTop10() []IPCount {\n    heap := container.NewMinHeap(10)\n    for ip, count := range ic.counts {\n        if heap.Len() < 10 {\n            heap.Push(IPCount{ip, count})\n        } else if count > heap.Peek().Count {\n            heap.Pop()\n            heap.Push(IPCount{ip, count})\n        }\n    }\n    return heap.ToSlice()\n}\n```\n\n## Performance Benchmarks\n- **Go Solution**: 2.5GB/min, 500MB peak memory, 3-4 cores\n- **Python Counter**: 800MB/min, 2GB peak memory (50GB file crashes)\n- **Memory Efficiency**: 75% reduction vs naive hash map approach\n- **Scaling**: Linear performance improvement up to 8 worker threads\n\n## Key Optimizations\n- **sync.Pool**: Reduces GC pressure by reusing byte buffers\n- **Bounded Heap**: Fixed memory regardless of unique IPs\n- **Chunked Processing**: Predictable memory usage profile\n- **Concurrent Merging**: Parallel counting with single-threaded heap merge","diagram":"flowchart TD\n    A[50GB Log Files] --> B[find -print0]\n    B --> C[xargs -0 -P 8]\n    C --> D[cut -d' ' -f1]\n    D --> E[sort -S 2G -T /tmp]\n    E --> F[uniq -c]\n    F --> G[sort -k1,1nr]\n    G --> H[head -10]\n    H --> I[Top 10 IPs]","difficulty":"advanced","tags":["find","xargs","cut","sort"],"channel":"algorithms","subChannel":"data-structures","sourceUrl":null,"videos":null,"companies":["Amazon","Google","Meta","Microsoft","Netflix","Snowflake"],"eli5":"Imagine you have a huge box of LEGOs with millions of pieces, but you can only play with a few at a time. You want to find the 10 colors you see most often! You'd grab a handful of LEGOs at a time, count each color, and keep a special box for your top 10 favorites. If a new color appears more times than your least favorite, you swap them! You do this over and over until you've looked at all the LEGOs. It's like sorting Halloween candy - you keep a small pile of your 10 favorite kinds, and when you find better candy, you trade out the less good ones. This way, you never need to dump all the candy on the floor at once!","relevanceScore":null,"voiceKeywords":["streaming solution","buffered i/o","min-heap","hash map","sync.pool","worker pools","memory constraints"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2025-12-28T02:05:04.418Z","createdAt":"2025-12-26 12:51:07"},{"id":"q-377","question":"Implement a min-heap using an array that supports insert, extractMin, and peek operations in O(log n) time. Include time/space complexity analysis and edge cases?","answer":"Use array with parent at i, children at 2i+1/2i+2. Insert: add to end, bubble up O(log n). extractMin: swap root with last, remove, bubble down O(log n). peek O(1). Space O(n). Handle empty heap, duplicate values, array resizing. Used in priority queues, Dijkstra's algorithm, streaming median.","explanation":"## Implementation\n```python\nclass MinHeap:\n    def __init__(self):\n        self.heap = []\n    \n    def insert(self, val):\n        self.heap.append(val)\n        self._bubble_up(len(self.heap) - 1)\n    \n    def extractMin(self):\n        if not self.heap: raise IndexError(\"Empty heap\")\n        min_val = self.heap[0]\n        self.heap[0] = self.heap[-1]\n        self.heap.pop()\n        if self.heap:\n            self._bubble_down(0)\n        return min_val\n    \n    def peek(self):\n        if not self.heap: raise IndexError(\"Empty heap\")\n        return self.heap[0]\n```\n\n## Time Complexity\n- Insert: O(log n) - bubble up at most height levels\n- extractMin: O(log n) - bubble down at most height levels  \n- peek: O(1) - direct array access\n- Space: O(n) - array storage\n\n## Edge Cases\n- Empty heap operations throw exceptions\n- Handle duplicate values properly\n- Array resizing when capacity exceeded\n- Maintain heap property during all operations\n\n## Applications\n- Priority queues in task scheduling\n- Dijkstra's shortest path algorithm\n- Streaming median calculation\n- Event-driven simulation systems","diagram":"flowchart TD\n    A[Insert Value] --> B[Add to End of Array]\n    B --> C[Bubble Up: Compare with Parent]\n    C -->|Parent > Child| D[Swap Positions]\n    D --> E[Continue Bubbling Up]\n    C -->|Parent ≤ Child| F[Heap Property Satisfied]\n    G[ExtractMin] --> H[Replace Root with Last Element]\n    H --> I[Bubble Down: Compare with Children]\n    I -->|Child < Parent| J[Swap with Smaller Child]\n    J --> K[Continue Bubbling Down]\n    I -->|Parent ≤ Children| L[Heap Property Restored]","difficulty":"beginner","tags":["arrays","linkedlist","hashtable","heap"],"channel":"algorithms","subChannel":"data-structures","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-25T17:24:22.757Z","createdAt":"2025-12-26 12:51:07"},{"id":"q-407","question":"Given a stream of log events with timestamps, design an algorithm to find the top K most frequent error messages in the last N minutes using O(K) space, where each event contains timestamp, error type, and message?","answer":"Use a sliding window with a hashmap for error counts and a min-heap of size K for top K tracking. As new events arrive, increment counts and update heap. For expired events, decrement counts and remove from heap if count drops below heap minimum. Time complexity: O(log K) per event, Space: O(K).","explanation":"## Algorithm\n- Maintain hashmap: error_message → count within window\n- Use min-heap (size K) storing (count, error_message) pairs\n- For each event: update hashmap, then update heap if needed\n- For expired events: decrement hashmap, adjust heap if count changes\n\n## Complexity Analysis\n- **Time**: O(log K) per event (heap operations)\n- **Space**: O(K) for heap + O(M) for hashmap where M = unique errors in window\n\n## Edge Cases\n- Handle ties in error frequency (lexicographic ordering)\n- Empty window or K > available errors\n- High-frequency events causing heap churn\n\n## Follow-up Questions\n1. How would you handle distributed logs across multiple servers?\n2. What if N is very large (days/weeks) - how to optimize memory?\n3. How to implement this with exact counts vs approximate counting?","diagram":"flowchart TD\n    A[Log Event Stream] --> B[Sliding Window Queue]\n    B --> C[Frequency Hash Map]\n    C --> D{Heap Size < K?}\n    D -->|Yes| E[Push to Min Heap]\n    D -->|No| F{Freq > Heap Min?}\n    F -->|Yes| G[Replace Heap Min]\n    F -->|No| H[Discard]\n    E --> I[Top K Error Messages]\n    G --> I\n    H --> I\n    B --> J{Event Expired?}\n    J -->|Yes| K[Remove from Queue]\n    J -->|No| C\n    K --> L[Decrement Frequency]\n    L --> C","difficulty":"intermediate","tags":["arrays","linkedlist","hashtable","heap"],"channel":"algorithms","subChannel":"data-structures","sourceUrl":null,"videos":null,"companies":["Amazon","Apple","Google","Meta","Microsoft","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-22T16:44:28.268Z","createdAt":"2025-12-26 12:51:05"},{"id":"q-418","question":"Design a data structure that supports range sum queries and point updates on a dynamic array with O(log n) operations. How would you implement this using a segment tree, and what are the trade-offs compared to a Binary Indexed Tree?","answer":"Implement a segment tree with each node storing the sum of its range. For point updates, traverse from leaf to root updating O(log n) nodes. Range queries use divide-and-conquer, combining relevant node sums. Time complexity: O(log n) for both operations. Space: O(2n) for tree array. Consider lazy propagation for range updates and Fenwick tree as memory-efficient alternative with O(n) space.","explanation":"## Core Implementation\nSegment tree uses array representation where node i has children 2i and 2i+1. Each node stores sum of its range [l, r].\n\n```python\nclass SegmentTree:\n    def __init__(self, arr):\n        self.n = len(arr)\n        self.tree = [0] * (4 * self.n)\n        self.build(arr, 0, 0, self.n-1)\n    \n    def update(self, idx, val, node, l, r):\n        if l == r:\n            self.tree[node] = val\n            return\n        mid = (l + r) // 2\n        if idx <= mid:\n            self.update(idx, val, 2*node+1, l, mid)\n        else:\n            self.update(idx, val, 2*node+2, mid+1, r)\n        self.tree[node] = self.tree[2*node+1] + self.tree[2*node+2]\n```\n\n## Performance Analysis\n- **Time**: O(log n) for both operations due to tree height\n- **Space**: O(4n) for complete binary tree representation\n- **Cache efficiency**: Better than recursive approaches\n\n## Advanced Features\n- **Lazy propagation**: Enables range updates in O(log n)\n- **Iterative implementation**: Reduces recursion overhead\n- **Memory optimization**: Use 2n array for bottom-up construction\n\n## Trade-offs vs Fenwick Tree\nSegment tree supports range updates (with lazy propagation) while Fenwick tree only handles point updates. Fenwick tree uses O(n) space vs O(4n) for segment tree, but segment tree provides more flexibility for complex range operations.\n\n## Real-world Applications\n- Database query optimization\n- Game development for spatial queries\n- Financial analytics for time series data\n- Competitive programming for range query problems","diagram":"flowchart TD\n  A[Root: Sum[0-7]] --> B[Left: Sum[0-3]]\n  A --> C[Right: Sum[4-7]]\n  B --> D[Left: Sum[0-1]]\n  B --> E[Right: Sum[2-3]]\n  C --> F[Left: Sum[4-5]]\n  C --> G[Right: Sum[6-7]]\n  D --> H[Leaf: arr[0]]\n  D --> I[Leaf: arr[1]]\n  E --> J[Leaf: arr[2]]\n  E --> K[Leaf: arr[3]]","difficulty":"advanced","tags":["bst","avl","trie","segment-tree"],"channel":"algorithms","subChannel":"data-structures","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-27T05:51:25.075Z","createdAt":"2025-12-26 12:51:06"},{"id":"q-425","question":"Given an array of integers and a target sum, find two numbers that add up to the target. How would you implement this efficiently and what's the time complexity?","answer":"Use a hash map to store numbers and their indices while iterating. For each number, check if target - current exists in the map. This gives O(n) time and O(n) space, better than the O(n²) brute force approach.","explanation":"## Problem\nFind two numbers in an array that sum to a target value.\n\n## Approach\nUse a hash map to track seen numbers and their indices.\n\n## Algorithm\n1. Initialize empty hash map\n2. Iterate through array\n3. For each number, calculate complement (target - current)\n4. Check if complement exists in map\n5. If found, return indices\n6. Otherwise, store current number in map\n\n## Complexity\n- Time: O(n) - single pass through array\n- Space: O(n) - hash map storage\n\n## Edge Cases\n- Handle duplicate numbers\n- Empty array or single element\n- Multiple valid pairs","diagram":"flowchart TD\n  A[Start] --> B[Initialize hash map]\n  B --> C[Iterate array]\n  C --> D[Calculate complement]\n  D --> E{Complement exists?}\n  E -->|Yes| F[Return indices]\n  E -->|No| G[Store number in map]\n  G --> H{More elements?}\n  H -->|Yes| C\n  H -->|No| I[No solution found]","difficulty":"beginner","tags":["arrays","linkedlist","hashtable","heap"],"channel":"algorithms","subChannel":"data-structures","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Amazon","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-08T11:43:17.833Z","createdAt":"2025-12-26 12:51:05"},{"id":"q-442","question":"Given a stream of user actions with timestamps, design a system to find the top K most frequent actions in the last N minutes using O(1) time per query?","answer":"Implement a sliding window approach using a deque for timestamp management and a hash map for frequency tracking, complemented by a max-heap for efficient top-K retrieval.","explanation":"## Solution Overview\nDesign a time-bounded frequency counter that maintains real-time statistics within a sliding window while supporting O(1) update operations and efficient top-K queries.\n\n## Data Structures\n- **Hash Map**: Tracks action → current frequency within the window\n- **Deque**: Maintains chronological timestamps for sliding window management\n- **Max-Heap**: Stores (frequency, action) pairs for top-K queries\n\n## Algorithm\n1. **Update Operation**: Increment action frequency in hash map and add timestamp to deque\n2. **Window Maintenance**: Remove expired timestamps from deque front and decrement corresponding frequencies\n3. **Query Operation**: Construct max-heap from frequency map entries and extract top-K elements\n\n## Complexity Analysis\n- **Update**: O(1) amortized time (deque operations are O(1), hash map updates are O(1))\n- **Query**: O(K log K) time (heap construction and extraction)\n- **Space**: O(U) where U represents unique actions in the current window","diagram":"flowchart TD\n  A[New Action] --> B[Update Frequency Map]\n  B --> C[Add Timestamp to Deque]\n  C --> D[Remove Expired Entries]\n  D --> E[Query Request]\n  E --> F[Build Max-Heap]\n  F --> G[Extract Top K]","difficulty":"intermediate","tags":["arrays","linkedlist","hashtable","heap"],"channel":"algorithms","subChannel":"data-structures","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-09T08:51:43.618Z","createdAt":"2025-12-26 12:51:05"},{"id":"q-565","question":"Given a stream of timestamped events, find the maximum number of concurrent events at any time?","answer":"Use a sweep line algorithm: collect all start and end times, sort them chronologically, then iterate through the timeline while maintaining a count of active events. Increment the count for each start time and decrement for each end time, tracking the maximum concurrent events encountered. O(N log N) time complexity.","explanation":"## Approach\n- Extract all event start and end times with type markers\n- Sort timestamps chronologically to establish timeline order\n- Sweep through timeline, maintaining active event count\n- Track maximum concurrent events during iteration\n\n## Complexity\n- Time: O(N log N) for sorting timestamps\n- Space: O(N) for storing timestamp collection\n\n## Edge Cases\n- Events with identical start/end times\n- Empty input dataset\n- Single event scenario","diagram":null,"difficulty":"advanced","tags":["arrays","linkedlist","hashtable","heap"],"channel":"algorithms","subChannel":"data-structures","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Salesforce","Slack","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-08T11:54:09.023Z","createdAt":"2025-12-27T01:11:27.215Z"},{"id":"q-5864","question":"Implement a data structure KthLargest that maintains the k largest numbers seen in a numeric stream. Provide a constructor KthLargest(int k) and a method int add(int val) that returns the current kth largest after insertion. Explain why a min-heap of size k is appropriate and how duplicates and edge cases (fewer than k elements) are handled?","answer":"Keep a min-heap of size k to store the current top k values. On add(val): if heap.size<k, push val; else if val>heap[0], replace min with val. Return heap[0] when heap.size==k, otherwise null. O(log k","explanation":"## Why This Is Asked\nDesign a structure to maintain the k-th largest element efficiently in a streaming context, using a heap and a fixed-size container.\n\n## Key Concepts\n- Min-heap for thresholding top-k\n- Time: O(log k) per insert\n- Edge cases: fewer than k elements, duplicates\n\n## Code Example\n```javascript\nclass KthLargest {\n  constructor(k, nums = []) {\n    this.k = k; this.heap = [];\n    for (const n of nums) this.add(n);\n  }\n  add(val) {\n    if (this.heap.length < this.k) { this.heap.push(val); this._bubbleUp(this.heap.length-1); }\n    else if (val > this.heap[0]) { this.heap[0] = val; this._siftDown(0); }\n    return this.heap.length === this.k ? this.heap[0] : null;\n  }\n  // ... helpers omitted for brevity\n}\n```\n\n## Follow-up Questions\n- How would you adapt for deletions from the stream?\n- What changes if k can grow or shrink over time?","diagram":"flowchart TD\n  A[add(val)] --> B{size<k}\n  B -->|Yes| C[push(val)]\n  B -->|No| D{val>min}\n  D -->|Yes| E[replaceMin(val)]\n  D -->|No| F[noChange]\n  C --> G[return min if size==k]\n  E --> G\n  F --> H[return null if size<k]","difficulty":"beginner","tags":["arrays","linkedlist","hashtable","heap"],"channel":"algorithms","subChannel":"data-structures","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Instacart","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T19:44:56.060Z","createdAt":"2026-01-22T19:44:56.060Z"},{"id":"q-6554","question":"Design a data structure called LiveTopK to process a continuous word stream with timestamps. It must support:\n- add(word, ts): register an occurrence\n- expire(ts): remove all occurrences with timestamp <= ts - W (sliding window)\n- topK(k): return the k words with highest frequency in the current window, breaking ties lexicographically\n\nConstraints: window size W seconds; memory O(D) where D is distinct words in window. Use a hashmap for counts, a heap for top-k with lazy updates, and a queue to expire old occurrences. Provide function signatures and a brief implementation sketch. Analyze time and memory trade-offs?","answer":"Use a hashmap word->freq, a max-heap of (freq, word) with lazy updates, and a queue of (ts, word) for expirations within window W. On add(word, ts): freq[word]++, push (freq[word], word) into heap. On","explanation":"## Why This Is Asked\nTests ability to combine multiple DS: hashmap for counts, heap for top-k, and a sliding window with expirations. Encourages discussion of lazy updates, memory trade-offs, and edge cases like ties.\n\n## Key Concepts\n- Hashmap for frequency map\n- Max-heap with lazy deletions\n- Sliding window via a timestamp queue\n- Tie-breaking by lexicographic order\n\n## Code Example\n```javascript\nclass LiveTopK {\n  add(word, ts) { /* ... */ }\n  topK(k) { /* ... */ }\n  expire(ts) { /* ... */ }\n}\n```\n\n## Follow-up Questions\n- How to scale to very large vocabularies within memory caps?\n- How would you extend to support weighted recency (e.g., exponential decay)?","diagram":null,"difficulty":"intermediate","tags":["arrays","linkedlist","hashtable","heap"],"channel":"algorithms","subChannel":"data-structures","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Google","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T07:56:05.033Z","createdAt":"2026-01-24T07:56:05.033Z"},{"id":"q-7145","question":"Design a streaming analytics structure that processes events with fields (service, id, value, ts). Requirements: (1) deduplicate events with identical (service,id) within a moving window of W seconds, (2) maintain per-service sum of values for events within the last W seconds, (3) support retrieving top-K services by current sum in O(log K) time and updating as new events arrive or old ones expire. Propose data structures and algorithms; analyze time/memory?","answer":"Use a hash map for (service,id) -> last seen timestamp, a min-heap for expirations by ts, a map service -> sum, and a max-heap with lazy invalidation for top-K. On arrival, dedupe, push to expiration ","explanation":"## Why This Is Asked\nReal-time analytics require deduplication within a sliding window, per-key aggregates, and fast top-K queries under memory pressure.\n\n## Key Concepts\n- Sliding window dedup, per-key aggregates, lazy-heaps, memory budgeting.\n- Time: O(log N) per insert/expire; Top-K: O(log K).\n\n## Code Example\n```javascript\nclass SlidingWindowTopK {\n  constructor(W) { /* ... */ }\n  add(ev){ /* dedupe, expire, update sums, push to heaps */ }\n  topK(K){ /* return top K services by current sum */ }\n}\n```\n\n## Follow-up Questions\n- How to handle out-of-order events? concurrency? persistence and checkpointing for recovery.","diagram":null,"difficulty":"advanced","tags":["arrays","linkedlist","hashtable","heap"],"channel":"algorithms","subChannel":"data-structures","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","PayPal","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T10:50:54.756Z","createdAt":"2026-01-25T10:50:54.757Z"},{"id":"q-7260","question":"Design a streaming scoreboard for a game. Each event is (playerId, delta, ts). Implement a structure that (1) deduplicates events with the same (playerId, ts) within 1 second, (2) updates per-player total scores, and (3) returns the top-K players by score after each event in O(log K). Use a hashtable for scores, a per-player linked list to track recent events, and a min-heap of size K for top players. Provide approach and time bounds?","answer":"Use a global map scores: playerId -> {score, lastTs}. Use a set seen for (playerId, ts) to deduplicate within 1s. Maintain a min-heap topK of (score, playerId) with size K. On event (p, d, t): if seen","explanation":"## Why This Is Asked\nThis question tests basic DS familiarity in a realistic streaming setting, combining arrays, linked lists, hashtables, and a heap, and emphasizes dedup and top-K maintenance under constant updates.\n\n## Key Concepts\n- Hash tables for per-player scores\n- Deduplication via a (playerId, ts) window\n- Min-heap for top-K maintenance\n- Time bounds: O(log K) per event, O(N) memory\n\n## Code Example\n```javascript\nclass Leaderboard {\n  constructor(K){ /* init maps and heap */ }\n  addEvent(playerId, delta, ts){ /* dedup, update score, adjust heap */ }\n  topK(){ /* return top-K players */ }\n}\n```\n\n## Follow-up Questions\n- How would you handle out-of-order events?\n- How to support negative deltas and ties in top-K?","diagram":"flowchart TD\n  A[Event arrives] --> B{Dedup?}\n  B -->|Yes| C[Ignore]\n  B -->|No| D[Update score]\n  D --> E[Adjust top-K heap]\n  E --> F[Output top-K]","difficulty":"beginner","tags":["arrays","linkedlist","hashtable","heap"],"channel":"algorithms","subChannel":"data-structures","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Cloudflare","Instacart"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T15:42:43.905Z","createdAt":"2026-01-25T15:42:43.906Z"},{"id":"al-152","question":"You have a staircase with n steps. You can climb 1, 2, or 3 steps at a time. How many distinct ways can you reach the top? Implement a solution with O(n) time and O(1) space?","answer":"Use DP with rolling variables: ways[i] = ways[i-1] + ways[i-2] + ways[i-3]. Base cases: ways[0]=1, ways[1]=1, ways[2]=2. For large n, use modulo 10^9+7. O(n) time, O(1) space by tracking last 3 values.","explanation":"## Solution Approach\n\nThis is a variation of the classic climbing stairs problem with three step options instead of two.\n\n### Dynamic Programming\nThe recurrence relation is `dp[i] = dp[i-1] + dp[i-2] + dp[i-3]` because from step `i`, you can reach it from `i-1`, `i-2`, or `i-3`.\n\n### Base Cases\n- `dp[0] = 1` (empty path)\n- `dp[1] = 1` (only one 1-step)\n- `dp[2] = 2` (1+1 or 2)\n\n### Space Optimization\nInstead of full DP array, use three variables rolling through the sequence.\n\n### Edge Cases\n- `n = 0`: return 1\n- `n = 1`: return 1  \n- `n = 2`: return 2\n- Handle large n with modulo arithmetic\n\n### Time/Space Complexity\n- **Time**: O(n) - single pass\n- **Space**: O(1) - constant extra space\n\n### Real-World Applications\n- Counting paths in graph traversal\n- Financial modeling with step-based options\n- Game move combination calculations","diagram":"graph TD\n    A[\"n=4 (target)\"] --> B[\"n=3 (4 ways)\"]\n    A --> C[\"n=2 (2 ways)\"]\n    A --> D[\"n=1 (1 way)\"]\n    B --> E[\"n=2 (2 ways)\"]\n    B --> F[\"n=1 (1 way)\"]\n    B --> G[\"n=0 (1 way)\"]\n    C --> H[\"n=1 (1 way)\"]\n    C --> I[\"n=0 (1 way)\"]\n    D --> J[\"n=0 (1 way)\"]\n    \n    style A fill:#ff6b6b\n    style B fill:#4ecdc4\n    style C fill:#4ecdc4\n    style D fill:#4ecdc4\n    style E fill:#95e1d3\n    style F fill:#95e1d3\n    style G fill:#95e1d3\n    style H fill:#95e1d3\n    style I fill:#95e1d3\n    style J fill:#95e1d3","difficulty":"intermediate","tags":["dp","optimization"],"channel":"algorithms","subChannel":"dynamic-programming","sourceUrl":null,"videos":null,"companies":null,"eli5":"Imagine you're climbing a staircase to reach a cookie jar! You can hop 1 step, 2 steps, or 3 steps at a time. To find out how many ways you can get to the top, think about it like playing with building blocks. If you're on step 1, there's only 1 way to get there (1 small hop). From step 2, you can either do two small hops or one big jump - that's 2 ways! For any step, you can come from the step right below (1 hop), two steps below (2 hops), or three steps below (3 hops). So just add up all the ways you could have reached those three previous steps! It's like counting all the different paths to get to your cookie - each step remembers the ways you could have arrived from the three steps before it.","relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-27T05:31:53.024Z","createdAt":"2025-12-26 12:51:07"},{"id":"al-166","question":"Given a string, find the minimum cost to transform it into a palindrome where insertions cost 2 and deletions cost 1. What is the optimal dynamic programming approach?","answer":"Use DP where dp[i][j] represents minimum cost for substring s[i..j]. If s[i]==s[j], dp[i][j]=dp[i+1][j-1]. Otherwise dp[i][j]=min(dp[i+1][j]+1(delete s[i]), dp[i][j-1]+2(insert s[j+1])). Time O(n²), space O(n²) or optimized to O(n). This compares to LCS-based solution with O(n²) time but directly optimizes for the given cost structure.","explanation":"## Problem Context\nThis tests dynamic programming skills for string manipulation, commonly found in coding interviews at FAANG companies.\n\n## Solution Approach\nThe recurrence considers three operations: delete left char, insert matching char for left, or insert+delete both ends. Base cases: dp[i][i]=0 (single char), dp[i][i+1]=0 if equal else min(insert_cost, delete_cost).\n\n## Code Implementation\n```python\ndef min_palindrome_cost(s, insert_cost=2, delete_cost=1):\n    n = len(s)\n    dp = [[0] * n for _ in range(n)]\n    \n    for length in range(2, n+1):\n        for i in range(n-length+1):\n            j = i + length - 1\n            if s[i] == s[j]:\n                dp[i][j] = dp[i+1][j-1]\n            else:\n                dp[i][j] = min(\n                    dp[i+1][j] + delete_cost,\n                    dp[i][j-1] + insert_cost,\n                    dp[i+1][j-1] + insert_cost + delete_cost\n                )\n    return dp[0][n-1]\n```\n\n## Complexity Analysis\n- Time: O(n²) where n is string length\n- Space: O(n²) or O(n) with rolling optimization\n\n## Follow-up Questions\n- How would you modify this for substitution operations?\n- Can you reconstruct the actual transformation sequence?\n- How does this relate to the edit distance problem?","diagram":"graph TD\n    A[Start dp[i][j]] --> B{s[i] == s[j]}\n    B -->|Yes| C[dp[i+1][j-1]]\n    B -->|No| D[1 + min(dp[i+1][j-1], dp[i+1][j], dp[i][j-1])]\n    C --> E[Return dp[i][j]]\n    D --> E","difficulty":"intermediate","tags":["dp","optimization"],"channel":"algorithms","subChannel":"dynamic-programming","sourceUrl":null,"videos":null,"companies":["Amazon","Apple","Google","Meta","Microsoft","Netflix"],"eli5":"Imagine you have a row of colorful blocks with letters on them. You want to make the row look the same forwards and backwards - like a mirror! Each time you add a new block (insertion), it costs 2 coins. Each time you take away a block (deletion), it costs 1 coin. You look at your blocks from both ends and think: 'What's the cheapest way to make them match?' Sometimes you add blocks, sometimes you remove them. You keep checking smaller and smaller pieces until your whole row becomes a perfect mirror! It's like solving a puzzle where you want to spend the fewest coins to make everything symmetrical and pretty.","relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-25T16:40:32.053Z","createdAt":"2025-12-26 12:51:06"},{"id":"al-167","question":"Given a target sum n, count the number of ways to reach it using dice rolls where each roll can be 1-6. Return the result modulo 10^9+7. Optimize for O(n) time and O(1) space?","answer":"Use sliding window DP: dp[i] = sum(dp[i-1] to dp[i-6]) % MOD. Maintain circular buffer of size 6 for O(1) space. Base case dp[0] = 1. Time O(n), Space O(1). Handle i<6 boundary conditions by limiting window range.","explanation":"## Problem\nCount ways to reach sum n using dice rolls (1-6), return modulo 10^9+7.\n\n## Approach\n**Sliding Window DP**: Each state depends on previous 6 states. Instead of O(n) array, use circular buffer of size 6.\n\n## Algorithm\n```python\ndef diceWays(n):\n    MOD = 10**9 + 7\n    dp = [0] * 6\n    dp[0] = 1  # dp[0] = 1\n    \n    for i in range(1, n + 1):\n        curr = sum(dp) % MOD\n        dp[i % 6] = curr\n    \n    return dp[n % 6]\n```\n\n## Complexity\n- **Time**: O(n) - single pass\n- **Space**: O(1) - constant 6-element buffer\n\n## Edge Cases\n- n = 0: return 1 (empty sequence)\n- n < 0: return 0\n- Large n: handle modulo overflow\n\n## Follow-up Questions\n1. How would you extend this to k-sided dice?\n2. Can you optimize further using matrix exponentiation?\n3. What if we needed to count sequences with exactly m rolls?","diagram":"flowchart TD\n  A[Start] --> B[Initialize dp[0]=1]\n  B --> C[For each i from 1 to target]\n  C --> D[Sum dp[i-die] for die=1-6]\n  D --> E[Store in dp[i]]\n  E --> F{i <= target?}\n  F -->|Yes| C\n  F -->|No| G[Return dp[target]]\n  G --> H[End]","difficulty":"intermediate","tags":["dp","optimization"],"channel":"algorithms","subChannel":"dynamic-programming","sourceUrl":null,"videos":{"longVideo":"https://www.youtube.com/watch?v=hfUxjdjVQN4"},"companies":["Amazon","Apple","Google","Meta","Microsoft"],"eli5":"Imagine you have a staircase with 10 steps and you want to climb to the top! You can take 1, 2, 3, 4, 5, or 6 steps at a time - just like rolling a dice! To find how many ways to reach step 10, we start from the bottom. There's exactly 1 way to stay at step 0 (do nothing!). For each step, we count all the ways we could have arrived there from any of the 6 steps before it. Like, to reach step 5, you could have come from step 4 (taking 1 step), OR from step 3 (taking 2 steps), OR from step 2, 1, or even step -1 (which doesn't exist, so we skip!). We add up all these possibilities. By the time we reach step 10, we'll know every possible combination of dice rolls that gets us there!","relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-25T16:41:21.093Z","createdAt":"2025-12-26 12:51:06"},{"id":"al-170","question":"Given an array of integers where each element represents the maximum number of steps you can jump forward from that position, find the minimum number of jumps required to reach the last index. If it's not possible to reach the end, return -1. How would you implement this efficiently?","answer":"Use greedy approach tracking current jump range and furthest reachable position. Iterate once, increment jumps when reaching current range end, update furthest position. Handle unreachable case by checking if furthest position doesn't advance. O(n) time, O(1) space.","explanation":"## Problem\nGiven an array where each element represents maximum jump length from that position, find minimum jumps to reach the last index.\n\n## Approach\n**Greedy Algorithm**: Track current jump range and furthest reachable position. Single pass O(n) solution.\n\n## Implementation\n```python\ndef minJumps(nums):\n    if len(nums) <= 1: return 0\n    jumps, cur_end, furthest = 0, 0, 0\n    \n    for i in range(len(nums)-1):\n        furthest = max(furthest, i + nums[i])\n        if i == cur_end:\n            jumps += 1\n            cur_end = furthest\n            if cur_end >= len(nums)-1:\n                break\n    return jumps if cur_end >= len(nums)-1 else -1\n```\n\n## Complexity\n- Time: O(n) - single pass\n- Space: O(1) - constant extra space\n\n## Edge Cases\n- Empty array: return 0\n- Single element: return 0\n- Unreachable: return -1\n\n## Follow-up Questions\n1. How would you modify this to return the actual jump path?\n2. What if we need to find all possible minimum jump paths?\n3. How does this change if we can only jump exactly the specified number of steps?","diagram":"graph TD\n    A[Start at index 0] --> B[Initialize jumps=0, end=0, furthest=0]\n    B --> C[Iterate through array]\n    C --> D{Reached end of current range?}\n    D -->|Yes| E[Increment jumps, set end=furthest]\n    D -->|No| F[Update furthest = max(furthest, i + nums[i])]\n    E --> F\n    F --> G{Can reach last index?}\n    G -->|Yes| H[Return jumps]\n    G -->|No| I[Continue to next position]\n    I --> C\n    H --> J[End]\n    C --> K{i >= n-1?}\n    K -->|Yes| H\n    K -->|No| D","difficulty":"advanced","tags":["dp","optimization"],"channel":"algorithms","subChannel":"dynamic-programming","sourceUrl":null,"videos":{"shortVideo":"https://www.youtube.com/watch?v=0RHXjBKY9EM","longVideo":"https://www.youtube.com/watch?v=9kyHYVxL4fw"},"companies":["Amazon","Apple","Google","Meta","Microsoft","Netflix"],"eli5":"Imagine you're playing hopscotch on a sidewalk. Each square tells you how many squares you can jump ahead. You want to reach the end with the fewest jumps! Look at all squares you can reach from your current spot. Pick the one that lets you jump farthest next. Keep doing this until you reach the end. If you find a spot where you can't jump forward at all, you're stuck - it's impossible! Like choosing the best stepping stones to cross a river with the fewest steps.","relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-25T16:42:47.521Z","createdAt":"2025-12-26 12:51:07"},{"id":"al-3","question":"What is Dynamic Programming and how does it differ from plain recursion? When would you choose one over the other?","answer":"Dynamic Programming optimizes recursion by storing intermediate results through memoization or tabulation, eliminating redundant computations and trading space for improved time complexity.","explanation":"## Why Asked\nTests understanding of algorithmic optimization techniques and the ability to select appropriate problem-solving approaches.\n\n## Key Concepts\n- Overlapping subproblems\n- Optimal substructure\n- Memoization (top-down) vs tabulation (bottom-up)\n- Time and space complexity trade-offs\n\n## Code Example\n```\n// Plain recursion: O(2^n)\nfunction fib(n) {\n  if (n <= 1) return n;\n  return fib(n-1) + fib(n-2);\n}\n\n// Dynamic Programming: O(n)\nfunction fibDP(n) {\n  const dp = [0, 1];\n  for (let i = 2; i <= n; i++) {\n    dp[i] = dp[i-1] + dp[i-2];\n  }\n  return dp[n];\n}\n```\n\n## Follow-up Questions\n- When would you choose memoization over tabulation?\n- What is the space complexity of bottom-up DP?\n- Can you identify problems suitable for DP vs plain recursion?","diagram":"flowchart TD\n  A[Problem] --> B{Has overlapping subproblems?}\n  B -->|No| C[Use plain recursion]\n  B -->|Yes| D{Has optimal substructure?}\n  D -->|No| E[Use other approaches]\n  D -->|Yes| F[Use Dynamic Programming]\n  F --> G{Implementation choice}\n  G --> H[Top-down memoization]\n  G --> I[Bottom-up tabulation]","difficulty":"advanced","tags":["dp","optimization","theory"],"channel":"algorithms","subChannel":"dynamic-programming","sourceUrl":null,"videos":{"shortVideo":"https://youtube.com/watch?v=oBt53YbR9Kk","longVideo":"https://youtube.com/watch?v=oBt53YbR9Kk"},"companies":["Amazon","Google","Meta"],"eli5":"Imagine you're building with LEGOs and need to make the same tower shape over and over. Plain recursion is like taking apart your tower each time and rebuilding it from scratch - super tiring! Dynamic programming is like taking a picture of each tower shape you build. When you need that shape again, you just look at your picture instead of rebuilding. You use more space to store your pictures, but you save lots of time and energy. Choose plain recursion for small, one-time projects. Choose dynamic programming when you'll need the same answers many times - like when building a big LEGO castle with lots of repeated sections!","relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-30T01:46:48.410Z","createdAt":"2025-12-26 12:51:05"},{"id":"q-328","question":"Given a grid of size m x n where each cell contains a non-negative integer representing the cost to enter that cell, find the minimum cost path from the top-left corner (0,0) to the bottom-right corner (m-1,n-1) moving only right or down. Return both the minimum cost and the path itself?","answer":"Use DP with tabulation: O(mn) time, O(mn) space. Optimize to O(n) space using rolling array. For path reconstruction, maintain parent pointers or backtrack from DP table. Handle edge cases like empty grid, single cell, and large values with 64-bit integers.","explanation":"## Approach\nThis is a classic dynamic programming problem where we build the solution bottom-up. The key insight is that the minimum cost to reach any cell depends only on the minimum costs to reach the cell above and the cell to the left.\n\n## Algorithm\n1. Initialize DP table with same dimensions as grid\n2. dp[0][0] = grid[0][0] (starting point)\n3. Fill first row: dp[0][j] = dp[0][j-1] + grid[0][j]\n4. Fill first column: dp[i][0] = dp[i-1][0] + grid[i][0]\n5. For remaining cells: dp[i][j] = min(dp[i-1][j], dp[i][j-1]) + grid[i][j]\n6. Path reconstruction by backtracking from bottom-right\n\n## Complexity\n- Time: O(mn) - we visit each cell once\n- Space: O(mn) for DP table, can optimize to O(n) using rolling array\n\n## Code Example\n```python\ndef minPathSum(grid):\n    if not grid or not grid[0]:\n        return 0, []\n    \n    m, n = len(grid), len(grid[0])\n    dp = [[0] * n for _ in range(m)]\n    parent = [[None] * n for _ in range(m)]\n    \n    dp[0][0] = grid[0][0]\n    \n    # Fill first row and column\n    for j in range(1, n):\n        dp[0][j] = dp[0][j-1] + grid[0][j]\n        parent[0][j] = (0, j-1)\n    \n    for i in range(1, m):\n        dp[i][0] = dp[i-1][0] + grid[i][0]\n        parent[i][0] = (i-1, 0)\n    \n    # Fill rest of table\n    for i in range(1, m):\n        for j in range(1, n):\n            if dp[i-1][j] < dp[i][j-1]:\n                dp[i][j] = dp[i-1][j] + grid[i][j]\n                parent[i][j] = (i-1, j)\n            else:\n                dp[i][j] = dp[i][j-1] + grid[i][j]\n                parent[i][j] = (i, j-1)\n    \n    # Reconstruct path\n    path = []\n    i, j = m-1, n-1\n    while i >= 0 and j >= 0:\n        path.append((i, j))\n        if parent[i][j]:\n            i, j = parent[i][j]\n        else:\n            break\n    \n    return dp[m-1][n-1], list(reversed(path))\n```\n\n## Follow-up Questions\n- How would you modify this solution if you could also move diagonally?\n- What if we needed to find the k-th minimum path instead of the absolute minimum?\n- How would you handle negative values in the grid?\n- Can you optimize space further using in-place modification?\n- How would this change if we could move in all four directions (need to detect cycles)?","diagram":"flowchart TD\n  A[Start at 0,0] --> B[Initialize DP array]\n  B --> C[Iterate through grid]\n  C --> D[Apply recurrence relation]\n  D --> E[Update DP array]\n  E --> F{Reached end?}\n  F -->|No| C\n  F -->|Yes| G[Return min cost]","difficulty":"intermediate","tags":["dp","memoization","tabulation"],"channel":"algorithms","subChannel":"dynamic-programming","sourceUrl":null,"videos":{"longVideo":"https://www.youtube.com/watch?v=oFkDldu3C_4"},"companies":["Amazon","Apple","Google","Meta","Microsoft","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-27T05:51:24.080Z","createdAt":"2025-12-26 12:51:05"},{"id":"q-440","question":"Given a string s and dictionary wordDict, return all possible sentences where s can be segmented into space-separated words from wordDict. Handle overlapping subproblems efficiently?","answer":"Use DP with memoization: recursively try all word prefixes, cache results for substrings. Time O(n³) worst case, space O(n²) for memo table. Backtrack to reconstruct all valid sentences from DP table.","explanation":"## Approach\n- Build DP table where dp[i] stores all valid sentences for substring s[i:]\n- For each position i, try all words in dictionary that match s[i:]\n- Recursively compute dp[i + word.length] and prepend word\n- Memoize results to avoid recomputation\n\n## Complexity\n- Time: O(n³) in worst case (n positions × n word lengths × n combinations)\n- Space: O(n²) for DP table + recursion stack\n\n## Implementation\n```python\ndef wordBreak(s, wordDict):\n    word_set = set(wordDict)\n    memo = {}\n    \n    def dfs(start):\n        if start in memo:\n            return memo[start]\n        if start == len(s):\n            return [\"\"]\n            \n        sentences = []\n        for end in range(start + 1, len(s) + 1):\n            word = s[start:end]\n            if word in word_set:\n                for suffix in dfs(end):\n                    sentence = word + (\"\" if suffix == \"\" else \" \" + suffix)\n                    sentences.append(sentence)\n        \n        memo[start] = sentences\n        return sentences\n    \n    return dfs(0)\n```","diagram":"flowchart TD\n  A[Start at index 0] --> B[Try all possible words]\n  B --> C{Word matches prefix?}\n  C -->|Yes| D[Recurse on remaining substring]\n  C -->|No| E[Skip to next word]\n  D --> F[Combine word with sub-sentences]\n  F --> G[Cache result for current index]\n  G --> H[Return all valid sentences]","difficulty":"advanced","tags":["dp","memoization","tabulation"],"channel":"algorithms","subChannel":"dynamic-programming","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Google","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-23T15:24:11.517Z","createdAt":"2025-12-26 12:51:05"},{"id":"q-5316","question":"Given an array nums of positive integers and an integer limit, partition nums into contiguous blocks so that the sum of each block is <= limit. The cost of a partition is the sum over blocks of (length)^2. Return the minimum cost and one valid partition achieving it. Provide both a memoized DFS and a bottom-up DP solution with reconstruction. Handle the case where a single element exceeds limit by returning -1?","answer":"Use a top-down dfs(i) that returns [minCost, nextCut]. Use prefix sums for O(1) range sum; extend end j while sum(i..j) <= limit, accumulate cost as (j-i+1)^2 + dfs(j+1)[0], and memoize. Bottom-up dp[","explanation":"## Why This Is Asked\nTests DP state design, memoization vs tabulation, and reconstruction.\n\n## Key Concepts\n- DP over partitions; state i is start index; transitions extend end while sum <= limit.\n- Prefix sums for O(1) range sums; reconstruction via choice pointers.\n- Edge cases: impossible when an element exceeds limit; handling ties.\n\n## Code Example\n```javascript\nfunction minPartitionCost(nums, limit){\n  const n = nums.length, pref=[0];\n  for(const x of nums) pref.push(pref[pref.length-1]+x);\n  const memo = new Array(n+1).fill(null);\n  function dfs(i){\n    if(i===n) return [0,[]];\n    if(memo[i]) return memo[i];\n    let best=[Infinity, null];\n    for(let j=i;j<n;j++){ const sum=pref[j+1]-pref[i]; if(sum>limit) break; const sub=dfs(j+1); const cost=(j-i+1)**2+sub[0]; if(cost<best[0]) best=[cost, [i,j].concat(sub[1]||[])]; }\n    memo[i]=best; return best;\n  }\n  return dfs(0);\n}\n``` \n\n## Follow-up Questions\n- How would you modify to also return the actual blocks boundaries when multiple optimal partitions exist?\n- How does the solution change if limit is dynamic per block?","diagram":"flowchart TD\n  A[Start] --> B[Compute prefix sums]\n  B --> C{i < n}\n  C -->|Yes| D[Expand end j while sum(i..j) <= limit]\n  D --> E[Compute cost for each j]\n  E --> F[Choose min and memoize]\n  F --> G[Move i to next start]\n  G --> C\n  C -->|No| H[Finish]","difficulty":"intermediate","tags":["dp","memoization","tabulation"],"channel":"algorithms","subChannel":"dynamic-programming","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","LinkedIn","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T17:08:38.989Z","createdAt":"2026-01-21T17:08:38.991Z"},{"id":"q-540","question":"Given a string s and a dictionary wordDict, return all possible sentences formed by inserting spaces in s such that each word exists in wordDict. Use DP with memoization to avoid exponential recomputation?","answer":"Use DP with memoization: dp[i] stores all valid sentences from s[i:]. For each position i, try all words in dict that match s[i:j]. Recursively get dp[j] results and combine with current word. Cache r","explanation":"## Approach\n- Use DP with memoization to avoid exponential recomputation\n- dp[i] stores all valid sentences from s[i:]\n- Try all words in dict that match s[i:j]\n- Recursively get dp[j] results and combine with current word\n\n## Implementation\n```python\ndef wordBreak(s, wordDict):\n    word_set = set(wordDict)\n    memo = {}\n    \n    def dfs(start):\n        if start in memo:\n            return memo[start]\n        if start == len(s):\n            return [\"\"]\n            \n        sentences = []\n        for end in range(start + 1, len(s) + 1):\n            word = s[start:end]\n            if word in word_set:\n                for rest in dfs(end):\n                    sentence = word + (\"\" if rest == \"\" else \" \" + rest)\n                    sentences.append(sentence)\n        \n        memo[start] = sentences\n        return sentences\n    \n    return dfs(0)\n```\n\n## Complexity\n- Time: O(n³) worst case due to substring checks\n- Space: O(n²) for memoization table\n\n## Edge Cases\n- Empty string returns empty list\n- No valid combinations returns empty list\n- Multiple valid sentences returned in list","diagram":"flowchart TD\n  A[Start at index 0] --> B[Try all words matching s[i:j]]\n  B --> C[Word in dict?]\n  C -->|Yes| D[Recursively solve from j]\n  C -->|No| E[Skip to next j]\n  D --> F[Combine current word with results]\n  F --> G[Cache results in memo[i]]\n  G --> H[Return all valid sentences]","difficulty":"advanced","tags":["dp","memoization","tabulation"],"channel":"algorithms","subChannel":"dynamic-programming","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","LinkedIn","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-27T05:31:41.048Z","createdAt":"2025-12-26 12:51:07"},{"id":"q-5461","question":"Given a string s of length n consisting of lowercase letters and '?', and a dictionary D of m words, partition s into consecutive dictionary words. When a word w is placed at position i, its cost is the number of positions where s[i+j] is a concrete letter and differs from w[j]. '?' matches any letter with zero cost. Minimize total cost to cover the entire s; return the minimum cost and the chosen segmentation. If no segmentation covers s, return -1. Propose an O(n * sum|w|) DP with memoization and backtrace?","answer":"Plan: Define dp(i) as the minimum cost to cover s[i..n). For each word w in D where i+|w| ≤ n, compute the placement cost by counting mismatches between s[i..i+|w|-1] and w, treating '?' as wildcard matches with zero cost. Use memoization to avoid recomputation and store the optimal word choice for backtracking. Return dp(0) and reconstruct the segmentation by following the stored choices.","explanation":"## Why This Is Asked\nTests a clean DP formulation for segmentation with per-placement costs and wildcards. It also probes backtracking and memoization versus tabulation choices under a dictionary constraint.\n\n## Key Concepts\n- DP on string positions with a dictionary\n- Memoization vs bottom-up tabulation\n- Backtracking reconstruction of segmentation\n- Handling wildcards and per-placement costs\n\n## Code Example\n```javascript\nfunction minCost(s, dict) {\n  const n = s.length;\n  const memo = new Array(n+1).fill(null);\n  const choice = new Array(n).fill(null);\n  \n  function costAt(i, w) {\n    let cost = 0;\n    for (let j = 0; j < w.length; j++) {\n      if (s[i+j] !== '?' && s[i+j] !== w[j]) {\n        cost++;\n      }\n    }\n    return cost;\n  }\n  \n  function dp(i) {\n    if (i === n) return 0;\n    if (memo[i] !== null) return memo[i];\n    \n    let best = Infinity;\n    let bestWord = null;\n    \n    for (const w of dict) {\n      if (i + w.length <= n) {\n        const c = costAt(i, w) + dp(i + w.length);\n        if (c < best) {\n          best = c;\n          bestWord = w;\n        }\n      }\n    }\n    \n    memo[i] = best;\n    choice[i] = bestWord;\n    return best;\n  }\n  \n  const minCost = dp(0);\n  if (minCost === Infinity) return -1;\n  \n  // Reconstruct segmentation\n  const words = [];\n  let i = 0;\n  while (i < n) {\n    words.push(choice[i]);\n    i += choice[i].length;\n  }\n  \n  return { cost: minCost, words };\n}\n```\n\n## Complexity\nTime: O(n × sum|w|) where sum|w| is total length of all dictionary words\nSpace: O(n) for memoization and choice arrays","diagram":"flowchart TD\n  Start(Start) --> DP[DP(i)]\n  DP --> Choose{Choose word}\n  Choose --> Next[Advance i]\n  Next --> DP\n  DP --> End[End]","difficulty":"advanced","tags":["dp","memoization","tabulation"],"channel":"algorithms","subChannel":"dynamic-programming","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","PayPal","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T04:39:23.539Z","createdAt":"2026-01-21T23:03:58.222Z"},{"id":"q-6145","question":"Count the number of ways to segment a string s into dictionary words from wordDict; provide both a memoized DFS and an iterative DP solution; discuss how a Trie-based optimization would prune checks and improve runtime. What are the time/space trade-offs and common edge cases?","answer":"Approach: Build a trie from wordDict and count segmentations of s with DP. Use a memoized DFS or bottom-up dp where dp[i] is the number of ways to segment s[i:]. For each i, walk the trie along s[i:],","explanation":"## Why This Is Asked\nTests DP on strings, demonstrating memoization vs tabulation and optimization tricks.\n\n## Key Concepts\n- Pattern matching with Trie to prune dictionary checks\n- Top-down vs bottom-up DP on string indices\n- Handling empty inputs and modulo arithmetic\n\n## Code Example\n```javascript\nfunction countWays(s, dict){\n  const trie = buildTrie(dict);\n  const n = s.length;\n  const dp = new Array(n+1).fill(0);\n  dp[n] = 1;\n  for(let i = n-1; i >= 0; --i){\n    let node = trie;\n    for(let j = i; j < n; j++){\n      const c = s[j];\n      if(!node.next[c]) break;\n      node = node.next[c];\n      if(node.end) dp[i] += dp[j+1];\n    }\n  }\n  return dp[0];\n}\nfunction buildTrie(words){\n  const root = {next:{}, end:false};\n  for(const w of words){\n    let node = root;\n    for(const ch of w){\n      if(!node.next[ch]) node.next[ch] = {next:{}, end:false};\n      node = node.next[ch];\n    }\n    node.end = true;\n  }\n  return root;\n}\n```\n\n## Follow-up Questions\n- How would you extend to count parses modulo 1e9+7 for very large dictionaries?\n- How would you adapt if wordDict contains duplicates or extremely long words?","diagram":"flowchart TD\n A[Start] --> B[Index i in s]\n B --> C{matches via Trie}\n C --> D[accumulate dp[j+1]]\n D --> E[dp[i]]\n E --> F[continue]\n F --> G[dp[0]]","difficulty":"intermediate","tags":["dp","memoization","tabulation"],"channel":"algorithms","subChannel":"dynamic-programming","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T10:54:21.112Z","createdAt":"2026-01-23T10:54:21.112Z"},{"id":"q-7332","question":"Given an array of positive integers nums and a target sum S, write a function countSubsets(nums, S) that returns the number of subsets whose elements sum to S. Each element can be used at most once. Provide both memoized top-down and bottom-up tabulation solutions, discuss time/space complexity, and handle cases with zeros and large S under modulo 1e9+7?","answer":"Count the subsets with sum S via DP. Top-down: dfs(i, t) = dfs(i+1, t) + dfs(i+1, t-nums[i]); memoize on (i,t); base: t==0 -> 1; i==n -> 0. Bottom-up: dp[w] accumulate from high to low. Time O(n*S), s","explanation":"## Why This Is Asked\n\nThis problem tests practical DP skills: counting subsets, managing zeros, and comparing memoization vs tabulation.\n\n## Key Concepts\n\n- Subset sum counting\n- Top-down memoization\n- Bottom-up tabulation\n- Modulo arithmetic and zero handling\n\n## Code Example\n\n```javascript\nfunction countSubsetsTopDown(nums, target) {\n  const n = nums.length;\n  const memo = Array.from({length: n+1}, () => new Map());\n  function dfs(i, t){\n    if (t === 0) return 1;\n    if (i === n) return 0;\n    const key = t;\n    if (memo[i].has(key)) return memo[i].get(key);\n    let res = dfs(i+1, t);\n    if (t >= nums[i]) res = (res + dfs(i+1, t - nums[i]));\n    memo[i].set(key, res);\n    return res;\n  }\n  return dfs(0, target);\n}\nfunction countSubsetsBottomUp(nums, target){\n  const MOD = 1000000007;\n  const dp = new Array(target+1).fill(0);\n  dp[0] = 1;\n  for (const v of nums){\n    for (let w = target; w >= v; w--){\n      dp[w] = (dp[w] + dp[w - v]) % MOD;\n    }\n  }\n  return dp[target];\n}\n``` \n\n## Follow-up Questions\n\n- How would you extend to handle negative numbers or count modulo large M?\n- How would you modify to return counts for multiple target sums simultaneously?","diagram":"flowchart TD\n  A[Start] --> B[Initialize DP]\n  B --> C[Iterate through nums]\n  C --> D{i == n}\n  D --> E[Return dp[S]]\n","difficulty":"beginner","tags":["dp","memoization","tabulation"],"channel":"algorithms","subChannel":"dynamic-programming","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Microsoft","OpenAI","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T18:54:22.981Z","createdAt":"2026-01-25T18:54:22.982Z"},{"id":"q-214","question":"Given a directed weighted graph with up to 10^6 edges and frequent edge weight updates, design a data structure that supports dynamic shortest path queries with sub-millisecond response time?","answer":"Use a dynamic Dijkstra variant with incremental updates and hierarchical decomposition, maintaining O(log n) per update and query.","explanation":"## Concept Overview\nDynamic shortest path requires handling frequent edge weight updates while maintaining fast query responses. Traditional Dijkstra's O(E + V log V) is too slow for production scale.\n\n## Implementation Details\n- **Hierarchical Decomposition**: Partition graph into clusters using METIS or custom partitioning\n- **Multi-level Indexing**: Maintain precomputed distances between cluster boundaries\n- **Incremental Updates**: Use dynamic programming to update only affected paths\n- **Lazy Recomputation**: Defer full recomputation until query performance degrades\n\n## Code Structure\n```python\nclass DynamicShortestPath:\n    def __init__(self, graph):\n        self.clusters = self.partition_graph(graph)\n        self.cluster_distances = self.precompute_inter_cluster()\n        self.local_paths = {c: {} for c in self.clusters}\n    \n    def update_edge(self, u, v, new_weight):\n        cluster = self.get_cluster(u)\n        self.invalidate_local_paths(cluster, u, v)\n        self.update_inter_cluster_if_boundary(u, v, new_weight)\n    \n    def query(self, source, target):\n        return self.bidirectional_dijkstra(source, target)\n```\n\n## Common Pitfalls\n- **Memory Overhead**: Hierarchical structures can consume 3-5x memory\n- **Partition Quality**: Poor clustering leads to frequent cross-cluster queries\n- **Update Cascades**: Edge updates can trigger expensive recomputation cascades\n- **Concurrency**: Thread-safe updates require careful locking strategies","diagram":"graph TD\n    A[Client Query] --> B{Source/Target in Same Cluster?}\n    B -->|Yes| C[Local Dijkstra]\n    B -->|No| D[Multi-level Path Search]\n    D --> E[Cluster Boundary Search]\n    E --> F[Inter-cluster Distance Lookup]\n    F --> G[Local Path Assembly]\n    G --> H[Return Result]\n    I[Edge Update] --> J{Boundary Edge?}\n    J -->|Yes| K[Update Inter-cluster Index]\n    J -->|No| L[Invalidate Local Cache]\n    K --> M[Mark Affected Clusters]\n    L --> M","difficulty":"advanced","tags":["bfs","dfs","dijkstra","topological"],"channel":"algorithms","subChannel":"graphs","sourceUrl":null,"videos":null,"companies":["Amazon","Goldman Sachs","Google","Microsoft","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-22T04:56:47.926Z","createdAt":"2025-12-26 12:51:07"},{"id":"q-286","question":"Explain the difference between BFS and DFS and when would you use each?","answer":"BFS explores level by level using a queue, finding shortest paths with O(V+E) time and O(V) space; DFS uses a stack/recursion for deep exploration with O(V+E) time but O(h) space where h is tree height.","explanation":"## Why Asked\nTests understanding of graph traversal fundamentals, algorithm selection, and space-time tradeoffs in real-world scenarios.\n\n## Key Concepts\n**BFS (Breadth-First Search)**: Uses queue, explores neighbors level-by-level. Time: O(V+E). Space: O(V) worst case (all nodes in queue). Guarantees shortest path in unweighted graphs.\n\n**DFS (Depth-First Search)**: Uses stack or recursion, explores as deep as possible before backtracking. Time: O(V+E). Space: O(h) where h is maximum depth (recursion stack), O(V) worst case for skewed graphs.\n\n## When to Use Each\n**BFS**: Shortest path routing, social network friend suggestions, web crawlers finding closest pages, level-order tree traversals.\n\n**DFS**: Topological sorting, cycle detection, maze solving, finding connected components, backtracking problems like Sudoku or N-Queens.\n\n## Code Example\n```javascript\n// BFS - finds shortest path\nfunction bfs(graph, start) {\n  const queue = [start];\n  const visited = new Set([start]);\n  const distances = { [start]: 0 };\n  \n  while (queue.length) {\n    const node = queue.shift();\n    for (const neighbor of graph[node]) {\n      if (!visited.has(neighbor)) {\n        visited.add(neighbor);\n        distances[neighbor] = distances[node] + 1;\n        queue.push(neighbor);\n      }\n    }\n  }\n  return distances;\n}\n\n// DFS - recursive implementation\nfunction dfs(graph, node, visited = new Set(), result = []) {\n  if (visited.has(node)) return result;\n  \n  visited.add(node);\n  result.push(node);\n  \n  for (const neighbor of graph[node]) {\n    dfs(graph, neighbor, visited, result);\n  }\n  \n  return result;\n}\n\n// DFS - iterative implementation\nfunction dfsIterative(graph, start) {\n  const stack = [start];\n  const visited = new Set();\n  const result = [];\n  \n  while (stack.length) {\n    const node = stack.pop();\n    if (visited.has(node)) continue;\n    \n    visited.add(node);\n    result.push(node);\n    \n    // Add neighbors to stack (reverse for consistent order)\n    for (let i = graph[node].length - 1; i >= 0; i--) {\n      const neighbor = graph[node][i];\n      if (!visited.has(neighbor)) {\n        stack.push(neighbor);\n      }\n    }\n  }\n  return result;\n}\n```\n\n## Memory Considerations\nBFS can consume significant memory for wide graphs (queue holds all nodes at current level). DFS is memory-efficient for deep, narrow graphs but may cause stack overflow for very deep recursion. Choose based on graph structure and problem requirements.","diagram":"flowchart TD\n  A[Graph Traversal] --> B[BFS: Queue-based]\n  A --> C[DFS: Stack-based]\n  B --> D[Level order]\n  B --> E[Shortest path]\n  C --> F[Deep exploration]\n  C --> G[Memory efficient]","difficulty":"intermediate","tags":["bfs","dfs","dijkstra","topological"],"channel":"algorithms","subChannel":"graphs","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":["bfs","dfs","level by level","shortest path","depth first","memory usage","exhaustive search"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2025-12-28T01:58:57.092Z","createdAt":"2025-12-26 12:51:07"},{"id":"q-350","question":"Given a directed graph representing city intersections and one-way streets, implement a function to find if there's a valid route from point A to point B using BFS. Return the shortest path distance or -1 if no route exists?","answer":"Use BFS with a queue to explore the graph level by level, tracking visited nodes and distances from the start. Return the distance when the target node is found, or -1 if all reachable nodes are exhausted without finding the target.","explanation":"## Why This Is Asked\nThis question tests fundamental graph traversal algorithms, BFS implementation, and pathfinding skills - critical for autonomous vehicle navigation systems and route planning applications.\n\n## Expected Answer\nThe candidate should implement BFS using a queue, maintain a visited set to avoid cycles, track distances from the start node, and handle edge cases such as when start equals target, disconnected graphs, or empty input. They should also explain why BFS guarantees the shortest path in unweighted graphs.\n\n## Code Example\n```python\nfrom collections import deque\n\ndef shortest_path(graph, start, end):\n    if start == end:\n        return 0\n    \n    queue = deque([(start, 0)])\n    visited = {start}\n    \n    while queue:\n        node, distance = queue.popleft()\n        \n        for neighbor in graph.get(node, []):\n            if neighbor == end:\n                return distance + 1\n            if neighbor not in visited:\n                visited.add(neighbor)\n                queue.append((neighbor, distance + 1))\n    \n    return -1\n```","diagram":"flowchart TD\n  A[Start BFS] --> B[Initialize queue with start]\n  B --> C[Mark start as visited]\n  C --> D[Queue empty?]\n  D -->|Yes| E[Return -1]\n  D -->|No| F[Dequeue node]\n  F --> G[Node equals target?]\n  G -->|Yes| H[Return distance]\n  G -->|No| I[Explore neighbors]\n  I --> J[Add unvisited neighbors to queue]\n  J --> D","difficulty":"beginner","tags":["bfs","dfs","dijkstra","topological"],"channel":"algorithms","subChannel":"graphs","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":"https://www.youtube.com/watch?v=wu0ckYkltus"},"companies":["Amazon","Apple","Cruise","Google","Meta","Microsoft","Netflix","Vercel"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-29T08:39:50.481Z","createdAt":"2025-12-26 12:51:05"},{"id":"q-394","question":"Given a directed acyclic graph representing task dependencies where each task takes 1 unit of time and you have unlimited workers, what is the minimum time to complete all tasks?","answer":"Use topological sort with BFS to process tasks level by level. The minimum time equals the length of the longest path (critical path) in the DAG. O(V+E) time, O(V) space. Track completion times using DP: dp[node] = max(dp[parent]) + 1.","explanation":"## Core Approach\nThe problem reduces to finding the critical path—the longest path in the DAG. This represents the minimum time needed since tasks on different branches can run in parallel.\n\n## Algorithm\n1. Perform topological sort using Kahn's algorithm\n2. Use DP array where dp[node] = earliest completion time\n3. For each node in topological order: dp[node] = max(dp[parents]) + 1\n4. Answer = max(dp[node]) for all nodes\n\n## Code Example\n```python\ndef min_completion_time(n, edges):\n    adj = [[] for _ in range(n)]\n    indegree = [0] * n\n    \n    for u, v in edges:\n        adj[u].append(v)\n        indegree[v] += 1\n    \n    # Topological sort with DP\n    dp = [0] * n\n    queue = [i for i in range(n) if indegree[i] == 0]\n    \n    while queue:\n        node = queue.pop(0)\n        for neighbor in adj[node]:\n            dp[neighbor] = max(dp[neighbor], dp[node] + 1)\n            indegree[neighbor] -= 1\n            if indegree[neighbor] == 0:\n                queue.append(neighbor)\n    \n    return max(dp)\n```","diagram":"flowchart TD\n    A[Build Graph & Indegree] --> B[Queue Zero Indegree Tasks]\n    B --> C{Workers Available?}\n    C -->|Yes| D[Assign Task to Worker]\n    C -->|No| E[Wait for Next Worker]\n    D --> F[Update Task Dependencies]\n    E --> G[Advance Time]\n    F --> H{All Tasks Completed?}\n    H -->|No| C\n    H -->|Yes| I[Return Total Time]\n    G --> C","difficulty":"advanced","tags":["bfs","dfs","dijkstra","topological"],"channel":"algorithms","subChannel":"graphs","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-29T08:49:17.988Z","createdAt":"2025-12-26 12:51:05"},{"id":"q-4825","question":"Given a weighted directed graph with N nodes and M edges, positive weights, and two nodes s and t, implement a function that returns the shortest distance from s to t and the number of distinct shortest paths achieving that distance. Provide a signature and concise outline of the update rules, and explain complexity and edge cases?","answer":"Implement Dijkstra with path counting. Maintain dist[v] and ways[v], init dist[s]=0, ways[s]=1. For each edge u->v with weight w: if dist[u]+w < dist[v], set dist[v]=dist[u]+w and ways[v]=ways[u]; els","explanation":"## Why This Is Asked\n\nThis tests practical graph reasoning: counting shortest paths under a Dijkstra framework, handling path multiplicities without overflow, and edge-case ties.\n\n## Key Concepts\n\n- Dijkstra with path counting\n- Tie handling on relaxation\n- Complexity and edge-case handling\n\n## Code Example\n\n```javascript\nfunction shortestPaths(graph, s, t) {\n  const N = graph.length;\n  const dist = Array(N).fill(Infinity);\n  const ways = Array(N).fill(0);\n  dist[s] = 0;\n  ways[s] = 1;\n  const pq = new MinHeap((a,b)=>a.d-b.d);\n  pq.push({node:s, d:0});\n  while (!pq.isEmpty()) {\n     const {node:u, d} = pq.pop();\n     if (d !== dist[u]) continue;\n     for (const [v, w] of graph[u]) {\n        const nd = dist[u] + w;\n        if (nd < dist[v]) { dist[v] = nd; ways[v] = ways[u]; pq.push({node:v, d:nd}); }\n        else if (nd === dist[v]) { ways[v] += ways[u]; }\n     }\n  }\n  return { dist: dist[t], count: ways[t] };\n}\n```\n\n## Follow-up Questions\n\n- How would you apply modulo to prevent overflow for large counts?\n- How would you adapt if edges had zero weight or negative cycles are possible?","diagram":null,"difficulty":"intermediate","tags":["bfs","dfs","dijkstra","topological"],"channel":"algorithms","subChannel":"graphs","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","Scale Ai","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T15:06:23.503Z","createdAt":"2026-01-20T15:06:23.503Z"},{"id":"q-5496","question":"In a project planning DAG, each node represents a task with a duration; edges u -> v mean v cannot start until u finishes. Implement a function that returns the minimum total time to finish all tasks (the makespan) assuming unlimited parallelism. Use a topological-sort-based approach and provide a concise code sketch?","answer":"Use topological dynamic programming: compute earliest finish times by processing nodes in topological order, where each node's earliest finish equals its duration plus the maximum earliest finish of all predecessors. Source nodes start with their duration alone. The makespan is the maximum earliest finish across all nodes.","explanation":"## Why This Is Asked\nThis question tests fundamental graph algorithms, DAG analysis, and dynamic programming techniques commonly used in project scheduling and dependency management systems.\n\n## Key Concepts\n- **Topological Sorting**: Linear ordering respecting task dependencies\n- **Dynamic Programming on DAGs**: Computing optimal values through predecessor relationships\n- **Earliest Finish Times**: Maximum of predecessor completion plus task duration\n- **Makespan Calculation**: Overall project completion time\n- **Complexity**: O(N + E) time and space\n\n## Code Example\n```javascript\nfunction makespan(n, durations, edges) {\n  // Build adjacency list and compute indegrees\n  const adj = Array.from({length: n}, () => []);\n  const indeg = Array(n).fill(0);\n  \n  for (const [u, v] of edges) {\n    adj[u].push(v);\n    indeg[v]++;\n  }\n  \n  // Initialize earliest finish times\n  const earliestFinish = durations.slice();\n  const queue = [];\n  \n  // Start with source nodes (indegree = 0)\n  for (let i = 0; i < n; i++) {\n    if (indeg[i] === 0) queue.push(i);\n  }\n  \n  // Process nodes in topological order\n  let head = 0;\n  while (head < queue.length) {\n    const u = queue[head++];\n    \n    for (const v of adj[u]) {\n      // Update earliest finish for successor\n      earliestFinish[v] = Math.max(\n        earliestFinish[v],\n        earliestFinish[u] + durations[v]\n      );\n      \n      indeg[v]--;\n      if (indeg[v] === 0) queue.push(v);\n    }\n  }\n  \n  // Return makespan (maximum earliest finish time)\n  return Math.max(...earliestFinish);\n}\n```\n\n## Algorithm Insight\nThe topological approach ensures each node is processed only after all its predecessors are complete, allowing accurate computation of earliest start times and efficient makespan calculation.","diagram":"flowchart TD\n  A[Task A] --> B[Task B]\n  A --> C[Task C]\n  B --> D[Task D]\n  C --> D","difficulty":"beginner","tags":["bfs","dfs","dijkstra","topological"],"channel":"algorithms","subChannel":"graphs","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Netflix","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T05:27:56.931Z","createdAt":"2026-01-22T02:37:30.137Z"},{"id":"q-5774","question":"You're given a DAG representing tasks with dependencies and durations. Each node has a positive duration. Implement earliestFinishTimes(n, edges, durations) that returns the earliest finish time for every node assuming all prerequisites finish before a task starts. Use a topological sort to process tasks in dependency order. Provide a signature, outline the algorithm, and its time complexity. Realistic scenario: building a data pipeline?","answer":"Use Kahn’s topological sort to obtain an order. Build adjacency and in-degree, set finish[i] = durations[i] for all i. For each node u in topo order, for each v in adj[u], set finish[v] = max(finish[v","explanation":"## Why This Is Asked\nTests ability to convert dependency graphs into scheduling via topological order; checks understanding of longest-path DP on DAG, cycle detection, and clear complexity analysis.\n\n## Key Concepts\n- Topological sort (Kahn / DFS)\n- DAG longest path DP\n- Cycle detection\n- Complexity analysis\n\n## Code Example\n\n```javascript\nfunction earliestFinishTimes(n, edges, durations) {\n  const adj = Array.from({ length: n }, () => []);\n  const indeg = Array(n).fill(0);\n  for (const [u, v] of edges) { adj[u].push(v); indeg[v]++; }\n  const q = [];\n  for (let i = 0; i < n; i++) if (indeg[i] === 0) q.push(i);\n  const finish = durations.slice();\n  let seen = 0;\n  while (q.length) {\n    const u = q.shift();\n    seen++;\n    for (const v of adj[u]) {\n      finish[v] = Math.max(finish[v], finish[u] + durations[v]);\n      if (--indeg[v] === 0) q.push(v);\n    }\n  }\n  if (seen !== n) throw new Error('Cycle detected');\n  return finish;\n}\n```\n\n## Follow-up Questions\n- How would you adapt for multiple start times or optional prerequisites?\n- How does the solution change if edge weights represent communication delay instead of task duration?","diagram":null,"difficulty":"beginner","tags":["bfs","dfs","dijkstra","topological"],"channel":"algorithms","subChannel":"graphs","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","OpenAI","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T16:01:48.541Z","createdAt":"2026-01-22T16:01:48.542Z"},{"id":"q-6073","question":"Given a DAG where each node i has a duration d[i], and edges (u -> v) indicate that v cannot start until u finishes, write a function to compute the minimum total project duration assuming unlimited parallelism. Return both the finish time of every task and the overall project duration. Use a topological ordering approach and describe its time complexity?","answer":"Use Kahn's algorithm to obtain a topological order. Maintain earliestStart[i] and finish[i]. Initialize earliestStart to 0. When visiting u, finish[u] = earliestStart[u] + d[u]; for each v in adj[u], ","explanation":"## Why This Is Asked\n\nTests ability to translate dependency constraints into a topological order and compute earliest finish times via DP. It also touches complexity and edge cases (cycles).\n\n## Key Concepts\n\n- Topological sort (Kahn or DFS)\n- DAG longest path / critical path\n- Dynamic programming on DAG\n- Time and space complexity\n\n## Code Example\n\n```javascript\nfunction computeProject(durations, edges) {\n  const n = durations.length;\n  const adj = Array.from({length: n}, () => []);\n  const indeg = Array(n).fill(0);\n  for (const [u, v] of edges) { adj[u].push(v); indeg[v]++; }\n  const est = Array(n).fill(0);\n  const finish = Array(n).fill(0);\n  const q = [];\n  for (let i = 0; i < n; i++) if (indeg[i] === 0) q.push(i);\n  let idx = 0;\n  while (idx < q.length) {\n    const u = q[idx++];\n    finish[u] = est[u] + durations[u];\n    for (const v of adj[u]) {\n      est[v] = Math.max(est[v], finish[u]);\n      if (--indeg[v] === 0) q.push(v);\n    }\n  }\n  const project = Math.max(...finish);\n  return { finish, project };\n}\n```\n\n## Follow-up Questions\n\n- How would you detect cycles if the DAG assumption fails? \n- How would you adapt for tasks with minimum and maximum parallelism constraints?","diagram":null,"difficulty":"beginner","tags":["bfs","dfs","dijkstra","topological"],"channel":"algorithms","subChannel":"graphs","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["PayPal","Salesforce","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T07:43:49.785Z","createdAt":"2026-01-23T07:43:49.785Z"},{"id":"q-6207","question":"You're given a DAG of modules. Each node i has processing time d[i], each edge (u,v) has transfer time t[u][v]. A node can start only after all predecessors finish and data arrives. Using a single worker, (a) compute earliest finish times and the critical path via topological order; (b) compute the minimum end-to-end latency from source to sink along any path by running Dijkstra on a transformed graph where edge weight is t[u][v] + d[v] (initial cost includes d[source])?","answer":"Use a cycle check and topological order (Kahn or DFS). In topo order, set f[v] = max over preds u of (f[u] + t[u][v]) + d[v], tracking the argmax to reconstruct the critical path. For (b), build a tra","explanation":"## Why This Is Asked\nTests ability to combine graph traversal, scheduling under dependencies, and shortest-path reasoning. It checks accuracy of DP on DAGs, cycle detection, and path reconstruction.\n\n## Key Concepts\n- Topological sort for DAG scheduling\n- DP over predecessors to compute earliest finish times\n- Critical path reconstruction\n- Graph transformation to apply Dijkstra for path latency\n- Path reconstruction in shortest-path problems\n\n## Code Example\n```javascript\n// Pseudocode for (a)\nfor v in topoOrder:\n  if preds(v) is empty: f[v] = d[v];\n  else f[v] = max_u(pred(v))(f[u] + t[u][v]) + d[v];\n  predBest[v] = argmax_u(...);\n// (b) Dijkstra on transformed graph with w(u,v) = t[u][v] + d[v]\n```\n\n## Follow-up Questions\n- How would you adapt for multiple workers or resource constraints?\n- How would you handle negative edge weights or zero-length processing times?","diagram":"flowchart TD\n  S[Source] --> A[Node A]\n  S --> B[Node B]\n  A --> C[Node C]\n  B --> C\n  C --> T[Sink]","difficulty":"advanced","tags":["bfs","dfs","dijkstra","topological"],"channel":"algorithms","subChannel":"graphs","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T14:38:53.014Z","createdAt":"2026-01-23T14:38:53.014Z"},{"id":"q-6340","question":"Context: algorithms/graphs, beginner. Scenario: a build-pipeline DAG where each edge (u,v) has a non-negative duration to finish the dependency; node 0 is the start. Implement a function shortestPathDAG(n, edges, start, target) that returns the earliest finish time to target and the actual path. Use a DFS-based topological sort to order nodes, then relax edges in that order. If target is unreachable, return -1 and empty path. Provide a concise code sketch and explain correctness and complexity?","answer":"Use a DFS to compute a topological order of the DAG; initialize dist[0]=0 and dist[i]=inf; then scan nodes in topo order, relaxing each edge (u,v,w) if dist[u]+w < dist[v], updating dist and parent. R","explanation":"## Why This Is Asked\n\nTests ability to apply DFS for topological ordering and then DP-style relaxation on a DAG to compute shortest paths without a heap. Also checks path reconstruction via a parent array.\n\n## Key Concepts\n\n- DFS-based topological sort\n- DAG single-source shortest path via relaxation in topo order\n- Path reconstruction with a parent array\n- Complexity: O(n+m)\n\n## Code Example\n\n```javascript\nfunction shortestPathDAG(n, edges, start, target) {\n  const g = Array.from({length: n}, () => []);\n  for (const [u, v, w] of edges) g[u].push([v, w]);\n  const visited = Array(n).fill(false);\n  const topo = [];\n  function dfs(u){\n    visited[u] = true;\n    for (const [v] of g[u]) if (!visited[v]) dfs(v);\n    topo.push(u);\n  }\n  for (let i = 0; i < n; i++) if (!visited[i]) dfs(i);\n  topo.reverse();\n  const INF = Number.POSITIVE_INFINITY;\n  const dist = Array(n).fill(INF);\n  const parent = Array(n).fill(-1);\n  dist[start] = 0;\n  for (const u of topo) {\n    if (dist[u] === INF) continue;\n    for (const [v, w] of g[u]) {\n      if (dist[u] + w < dist[v]) {\n        dist[v] = dist[u] + w;\n        parent[v] = u;\n      }\n    }\n  }\n  if (dist[target] === INF) return {time: -1, path: []};\n  const path = [];\n  for (let x = target; x != -1; x = parent[x]) path.push(x);\n  path.reverse();\n  return {time: dist[target], path};\n}\n```\n\n## Follow-up Questions\n\n- How would you handle multiple sources or tie-breaking paths?\n- How would you adapt if the graph could contain cycles (cycle detection and fallback to Dijkstra)?","diagram":null,"difficulty":"beginner","tags":["bfs","dfs","dijkstra","topological"],"channel":"algorithms","subChannel":"graphs","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T19:50:40.942Z","createdAt":"2026-01-23T19:50:40.942Z"},{"id":"q-6412","question":"Design an algorithm for a DAG G=(V,E) where each edge e has two attributes: latency w(e) >= 0 and a boolean skip(e) indicating latency can be skipped but the edge still counts as a step. Given source s and target t, find a path from s to t that minimizes the tuple (hops, total_latency), where total_latency sums w(e) for edges with skip(e)=false. Output the path and the metric. Explain how to detect cycles if present and how to adapt?","answer":"Use a topological dynamic programming approach on the DAG. Maintain for each node the optimal pair (hops, total_latency) from source s along with a predecessor pointer for path reconstruction. Initialize source s with (0, 0). For each edge (u, v) with latency w and skip flag, compute candidate = (hops[u] + 1, latency[u] + (skip ? 0 : w)). Update v's state if this candidate is lexicographically smaller than the current best. After processing all nodes in topological order, reconstruct the optimal path from t back to s using predecessor pointers.","explanation":"## Why This Is Asked\nThis question evaluates mastery of several advanced graph techniques: DAG dynamic programming, multi-criteria optimization, path reconstruction, and cycle handling. It tests the ability to reason about edge attributes with different behaviors (skip-able vs mandatory latency) and implement lexicographic optimization where hops are prioritized over latency.\n\n## Key Concepts\n- Topological sorting for efficient DAG processing\n- Lexicographic multi-objective optimization (hops first, latency second)\n- Predecessor-based path reconstruction\n- Cycle detection using DFS and graph condensation\n- Edge attribute handling with conditional weight computation\n\n## Code Example\n```javascript\nfunction shortestPathDAG(graph, s, t) {\n  // Assumes graph is a DAG. Each edge has: latency w, skip boolean\n  const order = topologicalSort(graph);\n  const best = new Map();\n  const pred = new Map();\n  \n  best.set(s, [0, 0]); // [hops, latency]\n  \n  for (const u of order) {\n    if (!best.has(u)) continue;\n    const [hops, latency] = best.get(u);\n    \n    for (const {to: v, latency: w, skip} of graph.edgesFrom(u)) {\n      const cand = [hops + 1, latency + (skip ? 0 : w)];\n      const current = best.get(v);\n      if (!current || lexicographicallySmaller(cand, current)) {\n        best.set(v, cand);\n        pred.set(v, u);\n      }\n    }\n  }\n  \n  return reconstructPath(pred, s, t);\n}\n\nfunction detectCycles(graph) {\n  const visited = new Set();\n  const recStack = new Set();\n  \n  function dfs(u) {\n    if (recStack.has(u)) return true; // Cycle detected\n    if (visited.has(u)) return false;\n    \n    visited.add(u);\n    recStack.add(u);\n    \n    for (const {to: v} of graph.edgesFrom(u)) {\n      if (dfs(v)) return true;\n    }\n    \n    recStack.delete(u);\n    return false;\n  }\n  \n  return Array.from(graph.nodes()).some(dfs);\n}\n```\n\n## Adaptation for Cyclic Graphs\nIf cycles are present, first detect them using DFS with a recursion stack. For graphs with cycles, apply strongly connected components (SCC) decomposition to create a condensation DAG, then solve on the condensed graph. Alternatively, use Dijkstra's algorithm modified for lexicographic optimization where the edge weight is the tuple (1, w or 0 based on skip).","diagram":null,"difficulty":"advanced","tags":["bfs","dfs","dijkstra","topological"],"channel":"algorithms","subChannel":"graphs","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Hashicorp","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T04:25:08.200Z","createdAt":"2026-01-23T22:43:15.410Z"},{"id":"q-662","question":"Given a directed graph with N nodes and M weighted edges (positive weights) and a source s, describe an implementation to (1) identify nodes reachable from s via BFS, (2) compute shortest distances dist[] to all nodes with Dijkstra, and (3) count the number of distinct shortest s→v paths for every v (mod 1e9+7). How would you build a DAG of edges on shortest paths and perform a topological DP over dist-ordered nodes to obtain path counts?","answer":"Run BFS from s to mark reachable nodes. Then Dijkstra from s to compute dist[]. Build DAG of edges (u,v) with dist[u] + w(u,v) = dist[v]. Topologically sort by dist and DP: ways[s]=1; for u in order, ","explanation":"## Why This Is Asked\nThis question blends BFS reachability, Dijkstra shortest paths, and counting shortest paths via a DAG and topological order.\n\n## Key Concepts\n- BFS/DFS reachability\n- Dijkstra's algorithm\n- Shortest-path DAG\n- Topological DP and modular counting\n\n## Code Example\n```javascript\nfunction countShortestPaths(n, adj, s) {\n  const dist = Array(n).fill(Infinity); dist[s] = 0;\n  // priority queue based Dijkstra (pseudo-notation)\n  const pq = new MinPQ((a,b)=>a[0]-b[0]);\n  pq.enqueue([0,s]);\n  while(!pq.isEmpty()){\n    const [d,u] = pq.dequeue(); if (d!==dist[u]) continue;\n    for (const {to:v, w} of adj[u]){\n      if (dist[v] > d + w){ dist[v] = d + w; pq.enqueue([dist[v], v]); }\n    }\n  }\n  const dag = Array.from({length:n}, ()=>[]);\n  for (let u=0; u<n; u++) for (const {to:v, w} of adj[u]) if (dist[u] + w === dist[v]) dag[u].push(v);\n  const order = Array.from({length:n}, (_,i)=>i).sort((a,b)=>dist[a]-dist[b]);\n  const MOD = 1000000007; const ways = Array(n).fill(0); ways[s] = 1;\n  for (const u of order) for (const v of dag[u]) ways[v] = (ways[v] + ways[u]) % MOD;\n  return {dist, ways};\n}\n```\n\n## Follow-up Questions\n- How would zero-weight edges affect the approach? \n- How to extend to count modulo a large prime or handle multiple sources?","diagram":"flowchart TD\n  S[Source s] --> R[Reachable BFS]\n  R --> D[Dijkstra dist]\n  D --> DAG[DAG of shortest-path edges]\n  DAG --> DP[Topological DP by dist]","difficulty":"advanced","tags":["bfs","dfs","dijkstra","topological"],"channel":"algorithms","subChannel":"graphs","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","NVIDIA","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T13:55:31.761Z","createdAt":"2026-01-11T13:55:31.761Z"},{"id":"q-6701","question":"Model city roads as a directed, weighted graph with non-negative weights. Given start s and target t, compute: 1) the shortest distance from s to t using Dijkstra; 2) the number of distinct shortest paths from s to t (mod 1e9+7); 3) one concrete shortest path as a node sequence. Then describe how to identify all vertices that lie on any shortest path without enumerating all paths, using a topological view of the shortest-path subgraph?","answer":"Use Dijkstra from s to get distS, and on the reversed graph from t to get distToT. The minimum distance is distS[t]. During relaxation, maintain cnt[v]: if dist[v] > dist[u] + w, set dist and cnt; if ","explanation":"## Why This Is Asked\nTests ability to combine Dijkstra, path counting, and reachability within the shortest-path subgraph, reflecting real-world routing and optimization tasks in large graphs.\n\n## Key Concepts\n- Dijkstra shortest paths\n- Path counting DP during relaxation\n- Shortest-path DAG/subgraph extraction\n\n## Code Example\n```javascript\nfunction shortestPaths(graph, s, t){\n  // graph: adjacency list of [v, w]\n  // Implement: Dijkstra from s to distS, reverse Dijkstra for distToT,\n  // count paths during relaxation, and backtrack to reconstruct one path.\n}\n```\n\n## Follow-up Questions\n- How would you adapt if some edges can be negative but no negative cycles exist?\n- How does your approach scale with very large graphs and multiple queries?","diagram":null,"difficulty":"intermediate","tags":["bfs","dfs","dijkstra","topological"],"channel":"algorithms","subChannel":"graphs","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T14:00:56.409Z","createdAt":"2026-01-24T14:00:56.409Z"},{"id":"q-6906","question":"Design an algorithm to compute the shortest s→t distance in a directed, weighted graph with non-negative weights, where you may set the weight of at most K edges along the chosen path to 0 (free passes). Edges beyond K must keep their original weights. Provide a function signature and a compact code sketch, discuss data structures, and analyze time/memory complexity. Include path reconstruction and edge cases?","answer":"Expanded-state Dijkstra on (node, used). For each edge (u→v,w), relax to (v,used) with cost +w; if used < K, also relax to (v,used+1) with cost +0. Use a min-heap priority queue and store parent pointers for path reconstruction. Complexity: O((V·K+E·K)log(V·K)) time, O(V·K) memory. Handle unreachable nodes, K=0 case, and reconstruct path by backtracking from optimal state.","explanation":"## Why This Is Asked\nTests mastery of graph algorithms, state expansion techniques, and path reconstruction under constraints.\n\n## Key Concepts\n- Dijkstra's algorithm on expanded state space (node, used)\n- Edge relaxation with and without discount application\n- Path reconstruction across state transitions\n\n## Code Example\n```javascript\nfunction shortestWithDiscounts(n, edges, s, t, K) {\n  const adj = Array.from({length: n}, ()=>[]);\n  for(const [u,v,w] of edges){ adj[u].push([v,w]); }\n  const INF = Number.POSITIVE_INFINITY;\n  const dist = Array.from({length: n}, ()=> Array(K+1).fill(INF));\n  const prev = Array.from({length: n}, ()=> Array(K+1).fill(null));\n  \n  const pq = new MinHeap();\n  dist[s][0] = 0;\n  pq.insert([s,0,0]); // [node, used, distance]\n  \n  while(!pq.isEmpty()){\n    const [u,used,d] = pq.extractMin();\n    if(d > dist[u][used]) continue;\n    if(u === t) break;\n    \n    for(const [v,w] of adj[u]){\n      // Without discount\n      if(d + w < dist[v][used]){\n        dist[v][used] = d + w;\n        prev[v][used] = [u,used];\n        pq.insert([v,used,d+w]);\n      }\n      // With discount if available\n      if(used < K && d < dist[v][used+1]){\n        dist[v][used+1] = d;\n        prev[v][used+1] = [u,used];\n        pq.insert([v,used+1,d]);\n      }\n    }\n  }\n  \n  // Find optimal state at target\n  let minDist = INF, minUsed = -1;\n  for(let k=0;k<=K;k++){\n    if(dist[t][k] < minDist){\n      minDist = dist[t][k];\n      minUsed = k;\n    }\n  }\n  \n  // Path reconstruction\n  const path = [];\n  let curr = [t,minUsed];\n  while(curr && prev[curr[0]][curr[1]]){\n    path.unshift(curr[0]);\n    curr = prev[curr[0]][curr[1]];\n  }\n  if(curr[0] === s) path.unshift(s);\n  \n  return {distance: minDist, path};\n}\n```\n\n## Edge Cases\n- Unreachable target: return INF distance\n- K=0: reduces to standard Dijkstra\n- K larger than path length: effectively unlimited discounts\n- Multiple optimal states: choose minimal used count","diagram":"flowchart TD\n  S[Source] --> A[(State: s,0)]\n  A --> B[(u, k) -> (v, k) via w)]\n  A --> C[(u, k) -> (v, k+1) via 0] \n  B --> D[Relaxation] --> T[Target]\n  C --> D","difficulty":"intermediate","tags":["bfs","dfs","dijkstra","topological"],"channel":"algorithms","subChannel":"graphs","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Discord","Instacart"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T05:57:15.506Z","createdAt":"2026-01-24T22:30:18.813Z"},{"id":"q-596","question":"Explain the differences between round-robin, least connections, and IP hash load balancing algorithms. When would you choose each one?","answer":"Round-robin distributes requests evenly, least connections sends traffic to the server with fewest active connections, and IP hash routes clients to the same server based on their IP address.","explanation":"Load balancing algorithms are crucial for distributing traffic across multiple servers efficiently. Round-robin is the simplest approach, cycling through servers sequentially. It works well for identical servers with similar processing capabilities and stateless applications. However, it doesn't account for varying server loads or request complexities.\n\nLeast connections algorithm monitors active connections and routes new requests to the server with the fewest current connections. This is ideal when requests have varying processing times or when servers have different capabilities. For example, in a microservices architecture where some endpoints are more resource-intensive, this prevents overloading a single server.\n\nIP hash uses the client's IP address to determine which server handles the request, ensuring session persistence. This is essential for applications requiring stateful connections, like shopping carts or user sessions. The hash function consistently maps the same IP to the same server, preventing session loss during load balancing.\n\nChoose round-robin for simple, stateless applications with uniform servers. Use least connections for heterogeneous environments or varying request complexities. Select IP hash when session persistence is critical, such as e-commerce platforms or authentication systems.","diagram":null,"difficulty":"intermediate","tags":["load-balancing","algorithms","networking","scalability","system-design"],"channel":"algorithms","subChannel":"load-balancing-algorithms","sourceUrl":null,"videos":null,"companies":["Google","Amazon","Meta","Netflix","Microsoft","Twitter","Uber","Airbnb"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-27T10:36:51.369Z","createdAt":"2025-12-27T10:36:51.369Z"},{"id":"q-627","question":"Explain the difference between round-robin and weighted round-robin load balancing algorithms. When would you choose one over the other?","answer":"Round-robin distributes requests evenly across servers, while weighted round-robin assigns more requests to servers with higher capacity based on configured weights.","explanation":"Round-robin load balancing is the simplest algorithm where requests are distributed sequentially to each server in the pool, cycling back to the first server after reaching the last. This works well when all servers have equal capacity and processing power. However, in real-world scenarios, servers often have different specifications - some might have more CPU, RAM, or faster network connections.\n\nWeighted round-robin addresses this limitation by assigning a weight to each server, representing its relative capacity. A server with weight 3 receives three times more requests than a server with weight 1. For example, if you have three servers with weights 5, 3, and 2, out of every 10 requests, 5 go to the first server, 3 to the second, and 2 to the third.\n\nChoose round-robin for homogeneous server environments where all nodes have identical hardware and performance characteristics. Use weighted round-robin in heterogeneous environments with varying server capacities, when upgrading infrastructure gradually, or when you want to control traffic distribution during maintenance or testing phases. Major cloud providers like AWS, Google Cloud, and Azure implement both algorithms in their load balancers.","diagram":null,"difficulty":"intermediate","tags":["load-balancing","algorithms","distributed-systems","networking"],"channel":"algorithms","subChannel":"load-balancing-algorithms","sourceUrl":null,"videos":null,"companies":["Amazon","Google","Microsoft","Netflix","Facebook","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-01T03:52:43.894Z","createdAt":"2026-01-01T03:52:43.894Z"},{"id":"al-163","question":"You have an array where each element appears twice except one element that appears once. Sort the array in O(n) time without using extra space for sorting. How would you approach this?","answer":"The problem constraints are impossible to satisfy. Sorting an arbitrary array in O(n) time with O(1) space violates the comparison-based sorting lower bound of Ω(n log n). If the actual goal is to find the unique element (not sort), use XOR traversal: `let unique = 0; for (let num of arr) unique ^= num;` which runs in O(n) time, O(1) space. If sorting is truly required, the optimal solution is O(n log n) time using algorithms like merge sort or quicksort.","explanation":"## Why the Problem is Impossible\n\n### Computational Complexity Theory\n- **Comparison sorting lower bound**: Any algorithm that sorts by comparing elements requires Ω(n log n) comparisons in the worst case\n- **Information theory argument**: Sorting requires distinguishing between n! possible permutations, requiring log₂(n!) ≈ n log n bits of information\n- **Space-time tradeoff**: Achieving linear-time sorting would require additional space constraints\n\n### Correct Interpretation: Find Unique Element\nIf the question intended to find the unique element:\n```javascript\nfunction findUnique(arr) {\n  let unique = 0;\n  for (let num of arr) {\n    unique ^= num;\n  }\n  return unique;\n}\n```\n- **Time complexity**: O(n)\n- **Space complexity**: O(1)\n- **Why XOR works**: Properties `a ^ a = 0` and `a ^ 0 = a` cancel out duplicate pairs\n\n### If Sorting is Actually Required\nThe optimal solutions are:\n- **Merge sort**: O(n log n) time, O(n) space\n- **Quicksort**: O(n log n) average time, O(log n) space\n- **Counting sort**: O(n) time, O(k) space (where k is value range)\n\n### Interview Assessment\nThis question tests understanding of computational limits and problem clarification. A strong candidate should recognize the impossible constraints and seek clarification rather than proposing an invalid solution.","diagram":"graph TD\n    A[Start: Unsorted Array] --> B[XOR all elements]\n    B --> C[Find unique element]\n    C --> D[Three-way partition]\n    D --> E[Elements < unique]\n    D --> F[Unique element]\n    D --> G[Elements > unique]\n    E --> H[Sort pairs in left partition]\n    G --> I[Sort pairs in right partition]\n    H --> J[Combine: Left + Unique + Right]\n    I --> J\n    J --> K[Sorted Array]\n    \n    style C fill:#90EE90\n    style D fill:#FFB6C1\n    style K fill:#87CEEB","difficulty":"intermediate","tags":["sort","complexity"],"channel":"algorithms","subChannel":"sorting","sourceUrl":null,"videos":{"shortVideo":"https://youtube.com/watch?v=XKu_SEDAykw"},"companies":["Amazon","Apple","Google","Meta","Microsoft"],"eli5":"Imagine you have a big box of toy cars where every car has a twin except one special car that's all alone! To find that special car, we can play a magic game: take pairs of the same car and they disappear like magic! Only the lonely car stays. Now we know which car is special, so we can quickly put all the twin cars on one side and the special car in the middle. It's like organizing your toys super fast without needing extra boxes or asking for help!","relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-05T06:47:41.269Z","createdAt":"2025-12-26 12:51:06"},{"id":"al-2","question":"Compare QuickSort, MergeSort, and Timsort. When would you choose each algorithm and what are their key trade-offs in production systems?","answer":"QuickSort: O(n log n) avg, O(n²) worst, in-place, cache-friendly, unstable. MergeSort: O(n log n) always, O(n) space, stable, great for linked lists. Timsort: O(n log n) worst, O(n) best, hybrid (merge+insertion), stable, optimized for real-world data patterns. Choose QuickSort for memory-constrained cache-optimized scenarios, MergeSort for stability requirements, Timsort for general-purpose sorting.","explanation":"## Algorithm Characteristics\n\n**QuickSort** excels with cache locality due to in-place partitioning, making it ~2-3x faster than MergeSort on arrays. However, its O(n²) worst case requires mitigation via median-of-three or random pivot selection.\n\n**MergeSort** guarantees O(n log n) performance and stability, crucial for sorting records by multiple keys. Its O(n) space requirement is problematic for memory-constrained environments but ideal for external sorting.\n\n**Timsort** (Python/Java default) combines MergeSort's stability with insertion sort's O(n) best case on nearly-sorted data. It detects natural runs, achieving 20-30% better performance on real-world datasets.\n\n## Production Considerations\n\n- **Stability matters** when sorting by multiple criteria (e.g., sort by department, then by name)\n- **Cache impact**: QuickSort's in-place nature reduces cache misses vs MergeSort's scattered memory access\n- **Hybrid approaches**: Most libraries use introsort (QuickSort + HeapSort fallback) to eliminate worst-case scenarios\n- **Parallelization**: MergeSort parallelizes naturally; QuickSort requires careful load balancing\n\n## Real-World Applications\n\n- **Database indexes**: Often use B-tree variants (modified MergeSort) for disk-based sorting\n- **In-memory analytics**: QuickSort variants for speed-critical aggregations\n- **Streaming data**: External MergeSort for datasets exceeding RAM capacity","diagram":"\ngraph TD\n    A[Array] --> P{Pick Pivot}\n    P --> L[Left < Pivot]\n    P --> R[Right > Pivot]\n    L --> Sort1[Recurse]\n    R --> Sort2[Recurse]\n","difficulty":"intermediate","tags":["sort","recursion","complexity"],"channel":"algorithms","subChannel":"sorting","sourceUrl":null,"videos":{"shortVideo":"https://www.youtube.com/watch?v=oJxeWV6zbIM","longVideo":"https://www.youtube.com/watch?v=9FMc-kHwlQs"},"companies":["Amazon","Google","Meta","Microsoft","Netflix","Stripe"],"eli5":"Imagine you have a messy pile of numbered cards to sort. QuickSort is like picking one card (the 'boss'), then making two piles: cards smaller than the boss go left, bigger ones go right. Keep doing this with each pile until everything is sorted! It's fast but sometimes picks a bad boss. MergeSort is different - you split the pile in half, sort each half, then carefully combine them like a zipper. It's always reliable but needs extra table space for the combining. QuickSort is like a speedy but sometimes messy kid, MergeSort is like a careful kid who always cleans up but needs more room!","relevanceScore":null,"voiceKeywords":["quicksort","mergesort","timsort","time complexity","space complexity","stability","cache-friendly","hybrid algorithm"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2025-12-27T05:51:44.778Z","createdAt":"2025-12-26 12:51:05"},{"id":"q-300","question":"Explain the difference between quicksort and mergesort, including their time and space complexities?","answer":"Quicksort has an average time complexity of O(n log n) and worst-case of O(n²), with O(log n) space complexity. Mergesort consistently performs at O(n log n) time complexity but requires O(n) auxiliary space. The key distinction is that quicksort is an in-place sorting algorithm, while mergesort is stable and maintains the relative order of equal elements.","explanation":"## Why Asked\nThis question evaluates understanding of fundamental sorting algorithms and their practical trade-offs in real-world applications.\n\n## Key Concepts\n- Divide and conquer strategies\n- Partitioning vs merging approaches\n- In-place vs auxiliary space requirements\n- Algorithm stability and performance characteristics\n\n## Code Example\n```\n// Quicksort partition\nfunction partition(arr, low, high) {\n  const pivot = arr[high];\n  let i = low - 1;\n  for (let j = low; j < high; j++) {\n    if (arr[j] < pivot) {\n      i++;\n      [arr[i], arr[j]] = [arr[j], arr[i]];\n    }\n  }\n  [arr[i + 1], arr[high]] = [arr[high], arr[i + 1]];\n  return i + 1;\n}\n\n// Mergesort merge\nfunction merge(left, right) {\n  const result = [];\n  let i = 0, j = 0;\n  \n  while (i < left.length && j < right.length) {\n    if (left[i] <= right[j]) {\n      result.push(left[i++]);\n    } else {\n      result.push(right[j++]);\n    }\n  }\n  \n  return result.concat(left.slice(i)).concat(right.slice(j));\n}\n```","diagram":"flowchart TD\n  A[Start] --> B{Choose Algorithm}\n  B -->|Quicksort| C[Select Pivot]\n  B -->|Mergesort| D[Divide Array]\n  C --> E[Partition Elements]\n  D --> F[Recursive Sort]\n  E --> G[Recursive Calls]\n  F --> H[Merge Subarrays]\n  G --> I[Combine Results]\n  H --> J[Sorted Array]\n  I --> J\n  J --> K[End]","difficulty":"beginner","tags":["quicksort","mergesort","complexity"],"channel":"algorithms","subChannel":"sorting","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-01T06:40:59.440Z","createdAt":"2025-12-26 12:51:04"},{"id":"q-362","question":"Given an array of integers, implement quicksort with proper partitioning, explain its O(n log n) average vs O(n²) worst-case complexity, and compare with mergesort in terms of stability, space usage, and practical performance?","answer":"Quicksort uses partitioning around a pivot. Average O(n log n) due to balanced splits, worst O(n²) with poor pivots (sorted data). Mergesort guarantees O(n log n) and is stable but needs O(n) extra space. Use quicksort for average-case speed, mergesort when stability or guaranteed performance matters.","explanation":"## Quicksort Implementation\n\n```python\ndef quicksort(arr, low=0, high=None):\n    if high is None: high = len(arr) - 1\n    if low < high:\n        pivot_idx = partition(arr, low, high)\n        quicksort(arr, low, pivot_idx - 1)\n        quicksort(arr, pivot_idx + 1, high)\n\ndef partition(arr, low, high):\n    # Lomuto partition scheme\n    pivot = arr[high]  # Choose last element as pivot\n    i = low - 1\n    for j in range(low, high):\n        if arr[j] <= pivot:\n            i += 1\n            arr[i], arr[j] = arr[j], arr[i]\n    arr[i + 1], arr[high] = arr[high], arr[i + 1]\n    return i + 1\n```\n\n## Pivot Selection Strategies\n\n- **Last element**: Simple but O(n²) worst case on sorted data\n- **Random element**: Good average case, minimal overhead\n- **Median-of-three**: Reduces probability of worst case\n- **Introsort**: Switch to heapsort when recursion depth exceeds 2log₂n\n\n## Complexity Analysis\n\n**Average Case O(n log n)**: Each partition splits array roughly in half, creating log n levels with O(n) work per level.\n\n**Worst Case O(n²)**: Consistently unbalanced partitions (already sorted array with poor pivot choice) create n levels of O(n) work.\n\n**Space Complexity**: O(log n) average (recursion stack), O(n) worst case.\n\n## Quicksort vs Mergesort\n\n| Factor | Quicksort | Mergesort |\n|--------|-----------|-----------|\n| Time Complexity | O(n log n) avg, O(n²) worst | O(n log n) guaranteed |\n| Space | O(log n) avg, O(n) worst | O(n) auxiliary |\n| Stability | Not stable | Stable |\n| Cache Performance | Excellent (in-place) | Poor (sequential access) |\n| Adaptivity | No | Partially adaptive |\n\n## Practical Considerations\n\n- Use **quicksort** for general-purpose sorting where average performance matters\n- Choose **mergesort** for external sorting, linked lists, or when stability required\n- **Timsort** (Python's default) combines mergesort with insertion sort for real-world data\n- Consider **introsort** (C++ std::sort) for guaranteed O(n log n) with fast average case","diagram":"flowchart TD\n  A[Unsorted Array] --> B[Choose Pivot Element]\n  B --> C[Partition Around Pivot]\n  C --> D{Elements < Pivot?}\n  D -->|Yes| E[Left Subarray]\n  D -->|No| F[Right Subarray]\n  E --> G[Recursive Quicksort]\n  F --> G\n  G --> H[Sorted Array]","difficulty":"beginner","tags":["quicksort","mergesort","complexity"],"channel":"algorithms","subChannel":"sorting","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-27T04:57:55.305Z","createdAt":"2025-12-26 12:51:07"},{"id":"q-4032","question":"Design an in-place, stable sort for an array of n integers that uses only O(1) extra space and runs in O(n log n) time. Implement a stable in-place merge for two adjacent sorted blocks without extra buffers. Provide a function signature and code sketch, and analyze how you preserve order of equal elements when duplicates exist. Target large inputs with good cache behavior?","answer":"I would implement an in-place, stable sort using a bottom-up mergesort with in-place merge via block rotations. By doubling the run size each pass, we merge adjacent runs by rotating blocks to place the smaller elements first, ensuring stability while maintaining O(1) extra space and O(n log n) time complexity.","explanation":"## Why This Is Asked\nThis question assesses mastery of in-place stable sorting techniques and practical trade-offs for large data sets where memory efficiency is crucial.\n\n## Key Concepts\n- In-place stable merge algorithms\n- Bottom-up mergesort implementation\n- Time/space complexity trade-offs\n- Cache-friendly data access patterns\n- Block rotation techniques for merging\n\n## Code Example\n```javascript\nfunction sortInPlaceStable(arr) {\n  // Bottom-up mergesort with in-place merge using block rotations\n  // Detailed implementation would include:\n  // - Run detection and initialization\n  // - Progressive merging of adjacent runs\n  // - Block rotation for in-place merging\n  // - Stability preservation during merge operations\n}\n```\n\n## Technical Analysis\nThe solution leverages block rotations to achieve in-place merging while preserving element order. Each pass doubles the merge window size, maintaining logarithmic time complexity. The algorithm's cache-friendly sequential access patterns optimize performance for large datasets.","diagram":null,"difficulty":"advanced","tags":["quicksort","mergesort","complexity"],"channel":"algorithms","subChannel":"sorting","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Instacart"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T05:05:05.255Z","createdAt":"2026-01-18T20:44:34.849Z"},{"id":"q-4083","question":"Design a hybrid sort for an array of integers that blends 3-way quicksort and in-place stable mergesort. For partitions larger than 64 elements, use 3-way partition quicksort with median-of-three pivot; for partitions of size 64 or less, sort by in-place stable merge of consecutive runs. Achieve O(n log n) average time and O(1) extra space; explain pivot strategy, base cases, and how you guarantee stability and space bounds?","answer":"I would implement a hybrid sort that combines 3-way quicksort for large partitions with in-place stable merge sort for small partitions. For partitions larger than 64 elements, I'd use 3-way quicksort with median-of-three pivot selection to handle duplicates efficiently and reduce worst-case probability. For partitions of 64 elements or smaller, I'd perform an in-place stable merge of consecutive runs using array rotation techniques. The median-of-three strategy samples the first, middle, and last elements to select a better pivot, reducing the chance of pathological partitions. I'd implement tail recursion optimization by always recursing on the smaller partition and iterating on the larger one, ensuring O(log n) stack depth. The in-place stable merge uses the reversal-rotation method: reverse the left run, reverse the right run, then reverse the entire merged region, achieving stability without extra space. This hybrid approach maintains O(n log n) average time complexity while guaranteeing O(1) auxiliary space and stability for the small partitions.","explanation":"## Why This Is Asked\nTests ability to blend sorting paradigms, analyze trade-offs, and design in-place, cache-friendly solutions.\n\n## Key Concepts\n- 3-way quicksort for duplicates\n- In-place stable merges using rotation\n- Threshold tuning and tail recursion for space\n- Stability vs. in-place constraints\n\n## Code Example\n```javascript\nfunction hybridSort(arr) {\n  // placeholder: quicksort3way for large partitions and inplaceMerge for small\n}\n```\n\n## Follow-up Questions\n- How would you choose the threshold and measure performance across datasets?\n- How would you extend to a stable, non-integer comparison type?","diagram":"flowchart TD\n  A(Start) --> B(Check size)\n  B --> C{size > 64?}\n  C -- yes --> D[3-way Quicksort]\n  C -- no --> E[In-place Stable Merge]\n  D --> F(Recurse on subarrays)\n  E --> F\n  F --> G(Done)","difficulty":"intermediate","tags":["quicksort","mergesort","complexity"],"channel":"algorithms","subChannel":"sorting","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Google","LinkedIn"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T05:50:30.300Z","createdAt":"2026-01-18T23:28:24.317Z"},{"id":"q-433","question":"Implement quicksort and explain when you'd choose it over mergesort. What's the worst-case scenario and how do you avoid it?","answer":"Quicksort is an efficient divide-and-conquer algorithm that partitions an array around a pivot element, achieving O(n log n) average time complexity. It's preferable over mergesort when in-place sorting is required, as it operates with O(1) auxiliary space and typically offers better cache performance. The worst-case O(n²) occurs with already sorted arrays or when consistently choosing poor pivots; this can be avoided through randomized pivot selection or the median-of-three technique.","explanation":"## Algorithm Overview\nQuicksort follows a divide-and-conquer approach:\n1. Choose a pivot element\n2. Partition the array such that elements less than the pivot appear before it, and greater elements appear after\n3. Recursively apply the same process to subarrays\n\n## When to Choose Quicksort\n- **In-place sorting required**: O(1) auxiliary space vs O(n) for mergesort\n- **Cache efficiency**: Better spatial locality due to in-place operations\n- **Average case optimization**: Outperforms mergesort on random datasets\n\n## Performance Characteristics\n**Advantages:**\n- O(n log n) average time complexity\n- O(1) space complexity (in-place)\n- Excellent real-world performance due to cache friendliness\n\n**Limitations:**\n- O(n²) worst-case time complexity\n- Not a stable sort algorithm\n- Performance dependent on pivot selection strategy\n\n## Implementation Strategy\n```python\ndef quicksort(arr, low, high):\n    if low < high:\n        pivot_index = partition(arr, low, high)\n        quicksort(arr, low, pivot_index - 1)\n        quicksort(arr, pivot_index + 1, high)\n```\n\n## Worst-Case Prevention\n- **Randomized pivot selection**: Eliminates pathological cases on sorted data\n- **Median-of-three**: Choose median of first, middle, and last elements\n- **Hybrid approaches**: Switch to insertion sort for small subarrays","diagram":"flowchart TD\n  A[Choose Pivot] --> B[Partition Array]\n  B --> C[Left Subarray < Pivot]\n  B --> D[Right Subarray > Pivot]\n  C --> E[Recursive Sort]\n  D --> F[Recursive Sort]\n  E --> G[Combine]\n  F --> G","difficulty":"beginner","tags":["quicksort","mergesort","complexity"],"channel":"algorithms","subChannel":"sorting","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","Oracle","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-09T08:46:23.905Z","createdAt":"2025-12-26 12:51:05"},{"id":"q-4766","question":"In a high-throughput analytics pipeline, you must sort a large array of event timestamps in place. Design an in-place sort that guarantees O(n log n) worst-case. Implement an introsort-style hybrid: quicksort with median-of-three and 3-way partitioning, depth threshold 2 log2(n) to switch to heapsort, and use insertion sort for small partitions. Explain your choices and how this avoids O(n^2) on adversarial input?","answer":"I would implement an in-place hybrid: quicksort with median-of-three and 3-way partitioning, track recursion depth, switch to heapsort when depth > 2 log2(n), and use insertion sort for small subarray","explanation":"## Why This Is Asked\n\nThis question probes understanding of introsort-style hybrids, balancing average performance with worst-case guarantees in production environments. It tests pivot strategies, partitioning, depth tracking, and in-place constraints.\n\n## Key Concepts\n\n- Introsort: hybrid of quicksort and heapsort with depth-based switch\n- Median-of-three pivot: reduces chance of worst-case partitions\n- 3-way partitioning: handles duplicates efficiently\n- Insertion sort for tiny subarrays: amortized gains\n\n## Code Example\n\n```javascript\nfunction introsort(arr) {\n  const maxDepth = Math.floor(Math.log2(arr.length)) * 2;\n  function insertionSort(a, lo, hi) {\n    for (let i = lo + 1; i < hi; i++) {\n      let key = a[i], j = i - 1;\n      while (j >= lo && a[j] > key) { a[j + 1] = a[j]; j--; }\n      a[j + 1] = key;\n    }\n  }\n  function heapify(a, count, i, offset) {\n    // placeholder for in-place heapify within [offset, offset+count)\n  }\n  function partition(a, lo, hi) {\n    // median-of-three and 3-way partitioning placeholder\n    return Math.floor((lo + hi) / 2);\n  }\n  function sort(a, lo, hi, depth) {\n    if (hi - lo <= 16) { insertionSort(a, lo, hi); return; }\n    if (depth > maxDepth) { /* heapsort(a, lo, hi); */ return; }\n    const pivot = partition(a, lo, hi);\n    // recursively sort partitions\n    sort(a, lo, pivot, depth + 1);\n    sort(a, pivot, hi, depth + 1);\n  }\n  sort(arr, 0, arr.length, 0);\n}\n```\n\n## Follow-up Questions\n\n- How would you adapt this to be stable, or explain why stability is costly in-place?\n- Compare this approach with TimSort or other stable hybrids in real-world libraries.\n- What are the practical pitfalls when implementing pivot selection and depth bounds in a multi-threaded setting?","diagram":"flowchart TD\n  A[Input: array] --> B[Choose pivot: median-of-three]\n  B --> C{Depth within limit?}\n  C -- Yes --> D[3-way partitioning]\n  D --> E[Recurse on subarrays]\n  C -- No --> F[Switch to heapsort]\n  E --> G[Done]\n  F --> G","difficulty":"intermediate","tags":["quicksort","mergesort","complexity"],"channel":"algorithms","subChannel":"sorting","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Hashicorp","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T11:37:05.247Z","createdAt":"2026-01-20T11:37:05.247Z"},{"id":"q-4848","question":"Given an array A of length N with many duplicates and D distinct keys (D << N). Design an in-place sort using a 3-way partition quicksort (less, equal, greater) that achieves expected time O(N log D). Explain your pivot strategy (randomized or sampled medians) to avoid O(N^2), and show how to limit extra space to O(1) aside from a tiny recursion/loop stack. Include edge cases like all elements equal?","answer":"Use a 3-way partition quicksort with a randomized pivot. In a single pass, split into <, ==, > pivot; recurse on < and > partitions. With D distinct keys, expected depth is O(log D); total expected ti","explanation":"## Why This Is Asked\nIn practice, data often contains many duplicates. A 3-way partitioned quicksort minimizes comparisons on duplicates and scales with the number of distinct keys.\n\n## Key Concepts\n- Three-way partitioning (Dutch National Flag)\n- Randomized pivot to avoid worst-case\n- Complexity: O(N log D) expected when D distinct keys\n- In-place sort with minimal extra space\n\n## Code Example\n```javascript\nfunction sort3(a, l=0, r=a.length-1){\n  if (l >= r) return;\n  const pivot = a[Math.floor(Math.random()*(r-l+1))+l];\n  let i=l, lt=l, gt=r;\n  while (i <= gt){\n    if (a[i] < pivot){ [a[lt], a[i]] = [a[i], a[lt]]; lt++; i++; }\n    else if (a[i] > pivot){ [a[i], a[gt]] = [a[gt], a[i]]; gt--; }\n    else { i++; }\n  }\n  sort3(a, l, lt-1);\n  sort3(a, gt+1, r);\n}\n```\n\n## Follow-up Questions\n- How would you adapt for stable sorting with duplicates?\n- What are the trade-offs vs a counting/radix approach on integers?","diagram":null,"difficulty":"advanced","tags":["quicksort","mergesort","complexity"],"channel":"algorithms","subChannel":"sorting","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Oracle","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T16:10:44.117Z","createdAt":"2026-01-20T16:10:44.117Z"},{"id":"q-5009","question":"Context: A high-throughput telemetry feed from a fleet of autonomous vehicles delivers events with a numeric priority and a timestamp. You must return a stable, fully sorted stream by priority under strict memory limits, processing blocks in place. Propose a practical in-place hybrid sort that uses 3-way quicksort with median-of-three pivots and a rotation-based in-place merge for adjacent blocks, plus an insertion sort threshold. Explain choices and expected complexities?","answer":"Use a hybrid approach: 3-way quicksort with median-of-three pivot selection and depth limit (introsort) for large blocks, switching to rotation-based in-place stable merge for adjacent partitions to preserve equal-priority ordering. Apply insertion sort for small subarrays (typically <16 elements) to avoid recursion overhead and exploit cache locality.","explanation":"## Why This Is Asked\n\nIn large-scale systems, memory constraints and stability requirements drive sophisticated sorting strategies. This question tests a candidate's ability to reason about algorithm blending, pivot strategy impact on duplicates, and achieving stability in-place while maintaining cache efficiency.\n\n## Key Concepts\n\n- **Hybrid sorting**: Quicksort for bulk processing, mergesort-like merging for stability\n- **3-way partitioning**: Efficiently handles duplicate priorities common in telemetry data\n- **Median-of-three pivots**: Reduces worst-case behavior and improves partition quality\n- **Rotation-based merging**: Enables stable in-place merging without auxiliary memory\n- **Insertion sort threshold**: Optimizes small subarray handling and cache performance\n\n## Expected Complexities\n\n- **Time**: O(n log n) average, O(n²) worst-case (mitigated by introsort depth limit)\n- **Space**: O(log n) recursion stack, O(1) auxiliary memory for in-place operations\n- **Stability**: Guaranteed through rotation-based merging for equal priorities","diagram":null,"difficulty":"advanced","tags":["quicksort","mergesort","complexity"],"channel":"algorithms","subChannel":"sorting","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","DoorDash","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T05:35:54.133Z","createdAt":"2026-01-20T23:41:08.469Z"},{"id":"q-6044","question":"You're given an array of N records, each with two integers: key (primary) and ts (timestamp). Sort by key ascending; for equal keys, ts descending. Implement an in-place, stable sort with O(n log n) time and O(log n) extra space. Describe the algorithm and provide a compact implementation sketch?","answer":"Propose a two-phase, in-place stable sort. First stable-sort by the secondary key ts in descending order using an in-place stable merge (block rotations) with O(log n) extra space. Then stable-sort by","explanation":"## Why This Is Asked\n\nIn many systems, multi-key, stable sorting must be done under tight memory constraints. This tests understanding of in‑place stable merges and space-time trade-offs.\n\n## Key Concepts\n\n- Two-key stable sort\n- In-place stable merge with O(log n) extra space\n- Space-time trade-offs and memory locality\n\n## Code Example\n\n```javascript\n// Pseudo in-place stable merge pass\nfunction mergeInPlaceByKey(arr, keyIndex, secondaryIndex, desc=false){ /* ... */ }\n\n// Then call: first by secondary ts descending, then by primary key ascending\n```\n\n## Follow-up Questions\n\n- How would you adapt to N up to 10^9 with external memory?\n- How does stability interact with null keys or ties in ts?","diagram":null,"difficulty":"advanced","tags":["quicksort","mergesort","complexity"],"channel":"algorithms","subChannel":"sorting","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Discord","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T06:48:13.347Z","createdAt":"2026-01-23T06:48:13.347Z"},{"id":"q-6163","question":"Design an external-memory sort for data too large to fit in RAM. With a memory buffer of size B, propose a two-phase approach: phase 1 generate sorted runs by reading B elements, sorting in memory, and writing runs; phase 2 perform a k-way merge of runs using a min-heap, loading blocks as needed. Provide precise I/O and time complexities and how to pick k and B given n?","answer":"Two-phase external sort. Phase 1: read up to B elements, sort in memory, write a sorted run to disk. Repeat until all elements are partitioned into runs. Phase 2: merge runs with a k-way min-heap, loa","explanation":"## Why This Is Asked\nTests understanding of external memory models, I/O complexity, and practical merge strategies for data larger than RAM. Evaluates ability to reason about cache/mool behavior and trade-offs between pass count and memory footprint.\n\n## Key Concepts\n- External memory model and I/O complexity\n- Phase-based sorting (run generation)\n- K-way merge with min-heap\n- Parameter tuning (B, k) for performance\n\n## Code Example\n```javascript\n// Phase 2 sketch: merge nRuns sorted files using a min-heap of size k\nfunction kWayMerge(runs, B, k){ /* ... */ }\n```\n\n## Follow-up Questions\n- How would you handle highly uneven run sizes?\n- How to adapt if reads/writes are expensive or CPU-bound instead of I/O-bound?","diagram":"flowchart TD\n  A[Read chunk (size B)] --> B[Sort in memory]\n  B --> C[Write sorted run]\n  C --> D{More runs?}\n  D -->|Yes| A\n  D -->|No| E[Merge all runs]\n  E --> F[Output sorted data]","difficulty":"advanced","tags":["quicksort","mergesort","complexity"],"channel":"algorithms","subChannel":"sorting","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Microsoft","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T11:30:14.757Z","createdAt":"2026-01-23T11:30:14.757Z"},{"id":"q-6269","question":"Implement an in-place, stable sort that runs in O(n log n) time using O(1) extra space. Describe and implement the in-place merge of two consecutive sorted halves inside an array using only rotations or adjacent swaps. Then outline the overall algorithm, explain why stability and space bounds hold, and compare to standard mergesort and quicksort trade-offs?","answer":"To sort in place, use a recursive merge sort with an in-place merge. When merging A[left..mid] and A[mid+1..right], scan i from left and j from mid+1; if A[i] <= A[j], i++; else rotate the subarray A[","explanation":"## Why This Is Asked\n\nTests ability to design in-place sorting algorithms, understand stability, and reason about trade-offs between time and memory in constrained environments.\n\n## Key Concepts\n\n- In-place merging\n- Stability\n- Time/space complexity\n- Rotation-based merging\n\n## Code Example\n\n```javascript\nfunction inPlaceMerge(arr, left, mid, right) {\n  let i = left, j = mid + 1;\n  while (i <= mid && j <= right) {\n    if (arr[i] <= arr[j]) { i++; continue; }\n    const tmp = arr[j];\n    for (let k = j; k > i; k--) arr[k] = arr[k - 1];\n    arr[i] = tmp;\n    i++; mid++; j++;\n  }\n}\n\nfunction inPlaceMergeSort(arr, left = 0, right = arr.length - 1) {\n  if (left >= right) return;\n  const mid = Math.floor((left + right) / 2);\n  inPlaceMergeSort(arr, left, mid);\n  inPlaceMergeSort(arr, mid + 1, right);\n  inPlaceMerge(arr, left, mid, right);\n}\n```\n\n## Follow-up Questions\n\n- How would you optimize rotations to guarantee O(n log n) time in all cases?\n- How does memory locality affect performance compared with standard mergesort?","diagram":null,"difficulty":"intermediate","tags":["quicksort","mergesort","complexity"],"channel":"algorithms","subChannel":"sorting","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Anthropic","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T17:02:05.992Z","createdAt":"2026-01-23T17:02:05.992Z"},{"id":"q-7360","question":"You're given an array A of N integers and a positive integer m. Sort A by the primary key A[i] mod m in non-decreasing order, and preserve the original relative order for equal remainders (stability). Propose an O(N + m) time approach using O(m) extra space, and outline how you would implement it in a language of your choice. Include a short example illustrating the flow?","answer":"Use m buckets for remainders 0..m-1 with a compact index map. First scan to count bucket sizes, then compute start positions. In a second pass, place each A[i] into its bucket at the next available sl","explanation":"## Why This Is Asked\n\nTests knowledge of stable distribution into buckets and trade-offs between time and space.\n\n## Key Concepts\n\n- Stability, bucket sort, counting, in-place vs extra space\n- Time complexity: O(N + m)\n- Space: O(m)\n\n## Code Example\n\n```python\nfrom typing import List\n\ndef bucket_sort_mod(A: List[int], m: int) -> List[int]:\n    counts = [0]*m\n    for x in A:\n        counts[x % m] += 1\n    starts = [0]*m\n    total = 0\n    for i in range(m):\n        starts[i] = total\n        total += counts[i]\n    out = [0]*len(A)\n    idx = starts.copy()\n    for x in A:\n        r = x % m\n        out[idx[r]] = x\n        idx[r] += 1\n    return out\n```\n\n## Follow-up Questions\n- How would you adapt for streaming data?\n- How do you modify to handle negative numbers?","diagram":null,"difficulty":"intermediate","tags":["quicksort","mergesort","complexity"],"channel":"algorithms","subChannel":"sorting","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Microsoft","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T19:49:09.016Z","createdAt":"2026-01-25T19:49:09.017Z"},{"id":"q-167","question":"Write a function to find the maximum depth of a binary tree using both recursive DFS and iterative BFS approaches. Discuss time/space complexity and handle edge cases?","answer":"Use recursive DFS: return 1 + max(depth(left), depth(right)) for each node. For iterative BFS, use queue with level counting. Both O(n) time, O(h) recursive space vs O(w) BFS space. Handle empty tree (return 0) and single node cases.","explanation":"## Solution Overview\nMaximum depth (height) of binary tree requires traversing all nodes. Two standard approaches:\n\n## Recursive DFS\n```python\ndef maxDepth(root):\n    if not root: return 0\n    return 1 + max(maxDepth(root.left), maxDepth(root.right))\n```\n- Time: O(n) - visits each node once\n- Space: O(h) - call stack depth equals tree height\n- Risk: Stack overflow for deep skewed trees\n\n## Iterative BFS\n```python\ndef maxDepth(root):\n    if not root: return 0\n    queue = collections.deque([root])\n    depth = 0\n    while queue:\n        depth += 1\n        for _ in range(len(queue)):\n            node = queue.popleft()\n            if node.left: queue.append(node.left)\n            if node.right: queue.append(node.right)\n    return depth\n```\n- Time: O(n) - processes each node once\n- Space: O(n) - worst case when tree is complete\n\n## Edge Cases\n- Empty tree: return 0\n- Single node: return 1\n- Skewed tree: consider iterative approach to avoid stack overflow\n\n## Follow-up Questions\n- How would you modify this for N-ary trees?\n- Can you solve this using Morris traversal for O(1) space?\n- How would you find minimum depth instead?","diagram":"graph TD\n    A[Root] --> B[Left Child]\n    A --> C[Right Child]\n    B --> D[Left Leaf]\n    B --> E[Right Leaf]\n    C --> F[Left Leaf]\n    C --> G[Right Leaf]\n    D --> H[null]\n    D --> I[null]\n    E --> J[null]\n    E --> K[null]\n    F --> L[null]\n    F --> M[null]\n    G --> N[null]\n    G --> O[null]","difficulty":"beginner","tags":["tree","binary"],"channel":"algorithms","subChannel":"trees","sourceUrl":null,"videos":null,"companies":["Amazon","Apple","Google","Meta","Microsoft","Netflix"],"eli5":"Imagine you have a toy tree made of building blocks! Each block can have two smaller blocks hanging from it - one on the left and one on the right. To find how tall the tree is, you start at the very top block and ask: \"How tall am I?\" Each block says: \"I'm 1 block tall PLUS the taller of my two helper blocks below me!\" If a block has no more blocks under it, it says \"I'm just 1 block tall!\" You keep asking this question down through all the blocks until you reach the bottom. Then all the blocks tell you their height, and you pick the biggest number you heard. That's how tall your toy tree is!","relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-25T16:40:41.892Z","createdAt":"2025-12-26 12:51:06"},{"id":"q-314","question":"Given a binary search tree with n nodes, find the kth smallest element where 1 ≤ k ≤ n. Discuss both recursive and iterative approaches with their time and space complexities?","answer":"Use inorder traversal which visits nodes in sorted order. For O(h) space, use recursive DFS with counter. For O(1) space, use Morris traversal with threading. Both run in O(k) time average, O(n) worst case. Morris avoids recursion stack but modifies tree temporarily.","explanation":"## Interview Context\nThis question tests tree traversal knowledge and space optimization awareness.\n\n## Recursive Approach\n```python\ndef kthSmallest(root, k):\n    def inorder(node):\n        if not node: return None\n        left = inorder(node.left)\n        if left: return left\n        self.count += 1\n        if self.count == k: return node.val\n        return inorder(node.right)\n    return inorder(root)\n```\n- Time: O(n) worst case, O(k) average\n- Space: O(h) recursion stack\n\n## Iterative Approach (Morris Traversal)\n```python\ndef kthSmallest(root, k):\n    count = 0\n    curr = root\n    while curr:\n        if not curr.left:\n            count += 1\n            if count == k: return curr.val\n            curr = curr.right\n        else:\n            pred = curr.left\n            while pred.right and pred.right != curr:\n                pred = pred.right\n            if not pred.right:\n                pred.right = curr\n                curr = curr.left\n            else:\n                pred.right = None\n                count += 1\n                if count == k: return curr.val\n                curr = curr.right\n```\n- Time: O(n)\n- Space: O(1)\n\n## Edge Cases\n- k > n: return null/raise exception\n- Empty tree: return null\n- Duplicate values: handle based on requirements\n\n## Follow-up Questions\n1. How would you modify this for kth largest element?\n2. What if the tree is not a BST?\n3. How to optimize for multiple k queries on the same tree?","diagram":"flowchart TD\n  A[Input: BST Root + k] --> B{Counter = 0}\n  B --> C[Inorder Traversal]\n  C --> D[Visit Left Subtree]\n  D --> E[Process Current Node]\n  E --> F{Counter == k?}\n  F -->|Yes| G[Return Current Node Value]\n  F -->|No| H[Counter++]\n  H --> I[Visit Right Subtree]\n  I --> D","difficulty":"beginner","tags":["bst","avl","trie","segment-tree"],"channel":"algorithms","subChannel":"trees","sourceUrl":null,"videos":{"longVideo":"https://www.youtube.com/watch?v=wGXB9OWhPTg"},"companies":null,"eli5":"Imagine you have a line of kids standing by height, from shortest to tallest. Your teacher asks you to find the 3rd shortest kid. You'd just walk down the line counting: 1, 2, 3... and point to that kid! A tree is like a special playground where every branch has smaller kids on the left and bigger kids on the right. To find the kth smallest, you start at the leftmost kid (the shortest) and walk through the playground in order, counting as you go. When you reach the number you're looking for, that's your kid! It's like following a treasure map that always leads you to the kids in height order.","relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-27T04:57:55.453Z","createdAt":"2025-12-26 12:51:05"},{"id":"q-340","question":"Given a BST that may have duplicate values, implement a function to find the kth smallest element considering duplicates. What's the time complexity and how would you handle edge cases?","answer":"Use augmented BST with subtree size counts for O(h) time, or inorder traversal for O(n) worst case. Handle duplicates by counting occurrences or using size-aware traversal that considers all nodes.","explanation":"## Why This Is Asked\nTests mastery of BST properties, tree traversal optimization, and duplicate handling strategies - critical for order statistics in trading systems at Jane Street and Two Sigma.\n\n## Expected Answer\nCandidate should compare three approaches: naive inorder (O(n)), iterative with stack optimization, and augmented BST with subtree sizes for O(h) optimal performance. For duplicates, explain two valid strategies: treat each duplicate as separate element in ordering, or skip duplicates and return the kth distinct value. Must discuss trade-offs between space complexity, implementation complexity, and query frequency.\n\n## Code Example\n```python\nclass TreeNode:\n    def __init__(self, val, left=None, right=None, count=1):\n        self.val = val\n        self.left = left\n        self.right = right\n        self.count = count  # subtree size including this node\n\ndef kth_smallest_augmented(root, k):\n    while root:\n        left_size = root.left.count if root.left else 0\n        \n        if k <= left_size:\n            root = root.left\n        elif k == left_size + 1:\n            return root.val\n        else:\n            k = k - left_size - 1\n            root = root.right\n    return None\n\ndef kth_smallest_iterative(root, k):\n    stack = []\n    curr = root\n    count = 0\n    \n    while stack or curr:\n        while curr:\n            stack.append(curr)\n            curr = curr.left\n        curr = stack.pop()\n        count += 1\n        if count == k:\n            return curr.val\n        curr = curr.right\n    return None\n```\n\n## Key Edge Cases\n- Empty tree: return None/raise exception\n- k out of bounds: validate against total node count\n- Duplicates: clarify if kth element counts duplicates or distinct values\n- Large trees: prefer augmented BST for repeated kth queries","diagram":"flowchart TD\n  A[Start at root] --> B[Traverse left to min]\n  B --> C[Visit node, increment count]\n  C --> D{count == k?}\n  D -->|Yes| E[Return value]\n  D -->|No| F[Traverse right]\n  F --> C\n  E --> G[End]","difficulty":"intermediate","tags":["bst","avl","trie","segment-tree"],"channel":"algorithms","subChannel":"trees","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":"https://www.youtube.com/watch?v=fAAZixBzIAI"},"companies":["Hrt","New Relic","Sap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-28T02:16:30.671Z","createdAt":"2025-12-26 12:51:05"},{"id":"q-4059","question":"Scenario: A code editor needs path auto-complete. Implement a Trie that stores file paths as sequences of characters, with methods insertPath(path) and autocomplete(prefix, maxResults). Explain how you would treat '/' as a separator, choices for child storage, and the time/space trade-offs; ensure insert is O(length) and autocomplete is O(length(prefix) + maxResults)?","answer":"Use a Trie where each node maps a character to a child and marks path ends. insertPath traverses chars, creating nodes; autocomplete(prefix, max) traverses to the prefix node and DFSs to collect up to","explanation":"## Why This Is Asked\nPath auto-complete is a common editor feature; it probes practical data-structure trade-offs beyond textbooks.\n\n## Key Concepts\n- Trie with per-node children map\n- Prefix search via traversal then DFS for completions\n- Memory sharing of prefixes and separator handling\n\n## Code Example\n```javascript\nclass TrieNode {\n  constructor() {\n    this.next = new Map();\n    this.isEnd = false;\n  }\n}\nclass PathTrie {\n  constructor() { this.root = new TrieNode(); }\n  insertPath(path) {\n    let node = this.root;\n    for (const ch of path) {\n      if (!node.next.has(ch)) node.next.set(ch, new TrieNode());\n      node = node.next.get(ch);\n    }\n    node.isEnd = true;\n  }\n  autocomplete(prefix, maxResults) {\n    let node = this.root;\n    for (const ch of prefix) {\n      if (!node.next.has(ch)) return [];\n      node = node.next.get(ch);\n    }\n    const results = [];\n    const dfs = (cur, built) => {\n      if (results.length >= maxResults) return;\n      if (cur.isEnd) results.push(prefix + built);\n      for (const [ch, child] of cur.next) dfs(child, built + ch);\n    };\n    dfs(node, \"\");\n    return results;\n  }\n}\n```\n\n## Follow-up Questions\n- How would you optimize for memory with extremely large path samples (e.g., thousands of common prefixes)?\n- How would you support ranking suggestions by frequency or recency without breaking O(length) per insert?","diagram":"flowchart TD\n  Start(Start)\n  Start --> InsertPath[InsertPath]\n  InsertPath --> Autocomplete[Autocomplete]\n  Autocomplete --> End(End)","difficulty":"beginner","tags":["bst","avl","trie","segment-tree"],"channel":"algorithms","subChannel":"trees","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Bloomberg","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T21:49:54.326Z","createdAt":"2026-01-18T21:49:54.326Z"},{"id":"q-4249","question":"Design a dynamic dictionary that stores words with frequencies and supports insert(word, delta), delete(word), and queryTopK(prefix, k) returning the k highest-frequency words starting with prefix (ties broken lexicographically). Propose a Trie-based solution where each node keeps a BST of (freq, word) for its subtree; explain how updates propagate and the expected complexities under 2e5 words and 1e5 operations?","answer":"Use a Trie where every node stores a BST keyed by (frequency, word). Maintain a global map word->freq. insert(word, delta): update freq, propagate along the path; at each node remove the old (freq, wo","explanation":"## Why This Is Asked\n\nTests the ability to design a hybrid tree-based structure enabling prefix-constrained top-k queries with dynamic weights, balancing time and memory.\n\n## Key Concepts\n\n- Trie with per-node BST for subtree words by (freq, word)\n- Global hash map for current frequencies\n- Incremental updates propagate along word path; queryTopK uses node BST to fetch top-k quickly\n- Trade-offs: memory factor for storing BSTs at every node vs. update cost\n\n## Code Example\n\n```javascript\n// Pseudocode core idea\nclass BST {\n  insert(word, freq) { /* remove old, insert new */ }\n  topK(k) { /* return top k by freq then word */ }\n}\nclass TrieNode { constructor(){ this.next=new Map(); this.bs=new BST(); this.freq=new Map(); } }\n```\n\n```javascript\n// insertWord(word, delta)\nfunction insertWord(root, word, delta, freqMap) {\n  const old = freqMap.get(word) || 0;\n  const nextFreq = old + delta;\n  freqMap.set(word, nextFreq);\n  // propagate along path updating BSTs in nodes\n  let node = root;\n  for (const ch of word) {\n    if (!node.next.has(ch)) node.next.set(ch, new TrieNode());\n    node = node.next.get(ch);\n    node.bs.insert(word, nextFreq);\n  }\n}\n```\n\n## Follow-up Questions\n\n- How would you reduce memory usage while preserving fast prefixes?\n- How to handle frequent deletions and stale BST entries efficiently?\n- Could a Fenwick/segment-tree variant work if you assign a global index to words by lexicographic order?\n","diagram":null,"difficulty":"intermediate","tags":["bst","avl","trie","segment-tree"],"channel":"algorithms","subChannel":"trees","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Netflix","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T10:02:20.005Z","createdAt":"2026-01-19T10:02:20.005Z"},{"id":"q-4383","question":"Design a real-time autocomplete structure for a high-traffic analytics portal. Implement insert(word) that increments word frequency, delete(word) to remove a word entirely, and getSuggestions(prefix, limit) returning up to limit words starting with prefix ordered by frequency (desc) then lexicographically. Target scales: ~2e6 inserts, ~1e5 prefix queries per second. Propose a Trie where each node holds an AVL of (freq, word); explain update mechanics, complexity, and memory trade-offs; sketch a concise implementation outline?","answer":"Use a Trie where each node stores an AVL tree keyed by (frequency, word). Maintain a global map word→freq. insert(word) traverses the path; for each node, remove the old (f, word) and insert (f+1, wor","explanation":"## Why This Is Asked\n\nTests designing a high-throughput, prefix-based ranking structure and reasoning about trade-offs between Trie augmentation and per-node indexing. It also probes mutation handling (insert/delete) in a ranking-aware structure and memory considerations in production-scale backends.\n\n## Key Concepts\n\n- Trie with per-node balanced BST (AVL) to rank by frequency\n- Global freq map to synchronize updates\n- Insertion/deletion propagates along the word path\n- Complexity: insert O(L log S), getSuggestions O(log S + k); memory scales with total words and tree sizes\n\n## Code Example\n\n```javascript\n// Pseudocode sketch of core types\nclass TrieNode {\n  constructor() {\n    this.children = new Map();\n    this.avl = new AVLTree(); // stores (freq, word) with freq descending\n  }\n}\n```\n\n## Follow-up Questions\n\n- How would you implement deletion with lazy pruning of empty branches?\n- How would you handle concurrent reads/writes and consistency guarantees?","diagram":null,"difficulty":"intermediate","tags":["bst","avl","trie","segment-tree"],"channel":"algorithms","subChannel":"trees","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","Plaid","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T16:48:50.236Z","createdAt":"2026-01-19T16:48:50.236Z"},{"id":"q-451","question":"Given a BST, write a function to find the kth smallest element using O(h) space and O(n) time, where h is height and n is nodes?","answer":"Use inorder traversal with a counter. Since BST's left-root-right yields sorted order, traverse recursively, decrement k when visiting node, return when k=0. Space O(h) for recursion stack, time O(n) ","explanation":"## Approach\n- Inorder traversal naturally visits nodes in ascending order\n- Maintain counter to track when we reach kth element\n- Early termination when found\n\n## Implementation\n```python\ndef kth_smallest(root, k):\n    def inorder(node):\n        if not node: return None\n        left = inorder(node.left)\n        if left: return left\n        k[0] -= 1\n        if k[0] == 0: return node.val\n        return inorder(node.right)\n    return inorder(root)\n```\n\n## Complexity\n- Time: O(n) worst case, O(k) average\n- Space: O(h) recursion stack\n\n## Edge Cases\n- k > number of nodes\n- Empty tree\n- Duplicate values","diagram":"flowchart TD\n  A[Start at root] --> B[Traverse left subtree]\n  B --> C[Visit current node]\n  C --> D{Is kth element?}\n  D -->|Yes| E[Return value]\n  D -->|No| F[Traverse right subtree]\n  F --> G[Continue traversal]","difficulty":"beginner","tags":["bst","avl","trie","segment-tree"],"channel":"algorithms","subChannel":"trees","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Cloudflare"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-27T04:58:34.208Z","createdAt":"2025-12-26 12:51:05"},{"id":"q-4954","question":"In a microblogging app, implement a Trie-based autocomplete for a word dictionary where each insert updates a word's frequency. Provide methods insert(word, delta), search(word) -> boolean, and prefixTop(prefix, k) -> list of up to k words with highest frequencies starting with prefix. Explain time/space trade-offs vs using a BST/AVL, and justify choices for memory-heavy vocabularies?","answer":"Use a Trie with per-node children and a frequency counter at terminal nodes. insert(word, delta) walks/creates nodes and updates terminal.frequency. search(word) checks terminal. prefixTop(prefix, k) performs DFS from prefix node, collecting top-k results using a max-heap or pruning based on frequency thresholds.","explanation":"## Why This Is Asked\nAutocomplete for large vocabularies is common; a Trie yields fast prefix queries and ordered counts.\n\n## Key Concepts\n- Trie vs BST for prefix-based lookup\n- DFS with pruning and max-heap to collect top-k results\n- Memory optimizations: sparse maps, compact nodes\n\n## Code Example\n```javascript\nclass TrieNode {\n  constructor() {\n    this.next = new Map();\n    this.word = null;\n    this.freq = 0;\n  }\n}\n\nclass Trie {\n  constructor() {\n    this.root = new TrieNode();\n  }\n\n  insert(word, delta) {\n    let cur = this.root;\n    for (const ch of word) {\n      if (!cur.next.has(ch)) {\n        cur.next.set(ch, new TrieNode());\n      }\n      cur = cur.next.get(ch);\n    }\n    cur.word = word;\n    cur.freq += delta;\n  }\n\n  search(word) {\n    let cur = this.root;\n    for (const ch of word) {\n      if (!cur.next.has(ch)) return false;\n      cur = cur.next.get(ch);\n    }\n    return cur.word === word;\n  }\n\n  prefixTop(prefix, k) {\n    let cur = this.root;\n    for (const ch of prefix) {\n      if (!cur.next.has(ch)) return [];\n      cur = cur.next.get(ch);\n    }\n    \n    const results = [];\n    const dfs = (node) => {\n      if (node.word) {\n        results.push({ word: node.word, freq: node.freq });\n        if (results.length > k) {\n          results.sort((a, b) => b.freq - a.freq);\n          results.pop();\n        }\n      }\n      for (const child of node.next.values()) {\n        dfs(child);\n      }\n    };\n    \n    dfs(cur);\n    return results.sort((a, b) => b.freq - a.freq).slice(0, k).map(r => r.word);\n  }\n}\n```\n\n## Time/Space Trade-offs vs BST/AVL\n- **Trie**: O(m) insert/search where m = word length, independent of vocabulary size. Prefix queries O(m + k) where k = results. Space O(α×m) where α = alphabet size.\n- **BST/AVL**: O(log n) operations where n = vocabulary size. Prefix queries require O(n) traversal. Space O(n).\n- **Trie advantage**: Constant-time prefix lookups, natural ordering by frequency, better cache locality for common prefixes.\n- **BST advantage**: More memory efficient for sparse vocabularies, simpler implementation.\n\n## Memory-heavy Vocabulary Justification\nFor large vocabularies, use:\n- **Sparse maps** (Map/Object) instead of fixed arrays for children\n- **Node compression** (radix/patricia tries) to merge common prefixes\n- **Frequency-based pruning** to remove low-frequency words\n- **External storage** for very large datasets, keeping hot prefixes in memory\n\nTries excel when prefix queries dominate and memory can be optimized through compression techniques.","diagram":"flowchart TD\n  A[Insert word] --> B[Trie node path]\n  B --> C[Terminal freq updated]\n  C --> D[Prefix search path]\n  D --> E[Collect top-k results]","difficulty":"beginner","tags":["bst","avl","trie","segment-tree"],"channel":"algorithms","subChannel":"trees","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T04:54:20.289Z","createdAt":"2026-01-20T21:29:38.802Z"},{"id":"q-5422","question":"Design a data structure to support an auto-complete dictionary with dynamic phrase scores using a Trie for prefix search and a balanced BST at each node, augmented with a segment-tree layer to extract top-k results for a given prefix efficiently. The structure must support insertPhrase(phrase, score), updateScore(phrase, newScore), deletePhrase(phrase), and queryTopK(prefix, k). Analyze time and memory, discuss edge cases, and compare with alternatives?","answer":"Proposed design: a Trie where each node stores a balanced BST mapping phrase to score, enabling O(log n) updates; each node also maintains a top-k structure (a max-heap or segment tree over its subtree) to extract top-k results efficiently. The structure supports insertPhrase(phrase, score) in O(L log n) time, updateScore(phrase, newScore) in O(L log n) time, deletePhrase(phrase) in O(L log n) time, and queryTopK(prefix, k) in O(L + k log k) time, where L is the phrase length and n is the number of phrases sharing the prefix. Memory usage is O(N × L) for the Trie plus O(N) for the BST and top-k structures. Edge cases include handling duplicate phrases, score ties, and empty prefix queries. Compared to alternatives like pure hash maps or suffix trees, this approach provides optimal prefix search with dynamic updates while maintaining efficient top-k retrieval.","explanation":"## Why This Is Asked\nAssesses the ability to blend Trie prefix search with balanced BSTs for dynamic scores, plus a top-k retrieval strategy that scales efficiently. It tests practical update propagation and memory trade-offs in a high-throughput autocomplete system.\n\n## Key Concepts\n- Trie for prefix lookup\n- AVL/balanced BST per node for phrase-score storage\n- Top-k extraction via a front-to-back heap or subtree aggregation\n- Update propagation costs and memory considerations\n\n## Code Example\n```javascript\nclass TrieNode {\n  constructor() {\n    this.next = new Map();\n    this.words = new AVLTree(); // key: phrase, value: score\n    this.topK = new MaxHeap(k); // top-k scores for subtree\n  }\n}\n\nclass AutoCompleteDictionary {\n  constructor(k = 10) {\n    this.root = new TrieNode();\n    this.k = k;\n  }\n\n  insertPhrase(phrase, score) {\n    let node = this.root;\n    for (let char of phrase) {\n      if (!node.next.has(char)) {\n        node.next.set(char, new TrieNode());\n      }\n      node = node.next.get(char);\n      node.words.insert(phrase, score);\n      node.topK.insert(score);\n    }\n  }\n\n  queryTopK(prefix, k) {\n    let node = this.root;\n    for (let char of prefix) {\n      if (!node.next.has(char)) return [];\n      node = node.next.get(char);\n    }\n    return node.topK.getTopK(k);\n  }\n}\n```\n\n## Time Complexity Analysis\n- insertPhrase: O(L log n) where L is phrase length, n is phrases per prefix\n- updateScore: O(L log n) for BST updates plus O(L) for heap propagation\n- deletePhrase: O(L log n) for BST removal and heap cleanup\n- queryTopK: O(L + k log k) for traversal plus heap extraction\n\n## Memory Trade-offs\n- Trie structure: O(N × L) where N is total phrases, L is average length\n- BST per node: O(N) total across all nodes\n- Top-k heaps: O(N × k) worst-case, but typically O(N) with pruning\n\n## Edge Cases\n- Duplicate phrases: update existing entry instead of inserting\n- Score ties: use lexicographic ordering as tiebreaker\n- Empty prefix: return global top-k from root node\n- Large k: consider segment tree for better range queries\n\n## Comparison with Alternatives\n- Hash map + sorting: O(1) insert but O(n log n) prefix queries\n- Suffix tree: better substring search but higher memory overhead\n- Pure Trie with scores: O(L) queries but O(n) updates for score changes\n- Hybrid approach (proposed): balances prefix efficiency with dynamic updates","diagram":null,"difficulty":"advanced","tags":["bst","avl","trie","segment-tree"],"channel":"algorithms","subChannel":"trees","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Snap","Square","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T05:01:24.040Z","createdAt":"2026-01-21T21:44:22.362Z"},{"id":"q-660","question":"You’re building a chat app that autocompletes words as users type. Implement a Trie with insert(word), search(word), and startsWith(prefix). Provide a compact JavaScript class with these methods, assuming lowercase a-z. After inserting 'apple','app','application', does search('app') return true and does startsWith('appl') return true?","answer":"Use a Trie: each node has 26 children and an isWord flag. insert(w) walks/creates nodes for each char and marks the last node. search(w) returns true if every node exists and the final node isWord. st","explanation":"## Why This Is Asked\nAssess understanding of Trie data structure and practical prefix search used in autocompletion; tests familiarity with basic data structures and iteration over strings.\n\n## Key Concepts\n- Trie structure with 26-way children\n- insert/search/startsWith time complexity O(m)\n- memory proportional to number of unique prefixes\n\n## Code Example\n```javascript\nclass TrieNode { constructor(){ this.next = new Array(26).fill(null); this.end=false; } }\nclass Trie { constructor(){ this.root = new TrieNode(); }\n insert(word){ let p=this.root; for(let ch of word){ let idx=ch.charCodeAt(0)-97; if(!p.next[idx]) p.next[idx]=new TrieNode(); p=p.next[idx]; } p.end=true; }\n search(word){ let p=this.root; for(let ch of word){ let idx=ch.charCodeAt(0)-97; if(!p.next[idx]) return false; p=p.next[idx]; } return p.end; }\n startsWith(prefix){ let p=this.root; for(let ch of prefix){ let idx=ch.charCodeAt(0)-97; if(!p.next[idx]) return false; p=p.next[idx]; } return true; }\n}\n```\n\n## Follow-up Questions\n- How would you optimize for memory?\n- How would you extend to support deletion or word counts?","diagram":null,"difficulty":"beginner","tags":["bst","avl","trie","segment-tree"],"channel":"algorithms","subChannel":"trees","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","IBM","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T13:45:48.860Z","createdAt":"2026-01-11T13:45:48.861Z"},{"id":"q-6850","question":"Design a dynamic multiset of integers supporting insert(x), erase(x) and rangeCount(a,b) = count of elements in [a,b]. Implement this using an AVL tree augmented by: key, freq, height, and size. On insert/erase adjust freq or node creation, rebalance, and update size. Implement rank(x) = number of keys <= x; then rangeCount(a,b) = rank(b) - rank(a-1)?","answer":"Implement an AVL with each node storing key, freq, height, and size. insert/erase adjust freq or insert new node, rebalance, and update size up the path. rank(x) returns how many keys <= x by traversi","explanation":"## Why This Is Asked\n\nThis question probes ability to augment a self-balancing BST with subtree metadata to answer range queries in O(log n), including handling duplicates.\n\n## Key Concepts\n\n- AVL balancing\n- subtree size augmentation\n- rank queries\n- duplicates management\n\n## Code Example\n\n```javascript\n// Pseudocode: insert, erase, rank, and update size/height\n```\n\n## Follow-up Questions\n\n- How would you extend to support range updates (add a value to all elements in [a,b])?\n- How would you adapt for 64-bit keys and potential overflow of size counters?\n","diagram":null,"difficulty":"intermediate","tags":["bst","avl","trie","segment-tree"],"channel":"algorithms","subChannel":"trees","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Slack","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T19:52:04.477Z","createdAt":"2026-01-24T19:52:04.477Z"},{"id":"q-7406","question":"Design a Trie to support auto-complete: implement insert(word) and getPrefixCount(prefix) that returns how many stored words start with the given prefix. Use a per-node counter updated on each insertion. After inserting 'cat' and 'cater', what does getPrefixCount('cat') return?","answer":"Use a Trie where each node stores a map of children and a subtreeCount. On insert(word), increment subtreeCount on the root and each traversed node, then mark the word end. getPrefixCount(prefix) traverses to the prefix node and returns its subtreeCount. After inserting 'cat' and 'cater', getPrefixCount('cat') returns 2.","explanation":"## Why This Is Asked\n\nThis question tests your ability to implement a Trie with efficient prefix queries, which is fundamental for autocomplete features. The per-node counter design enables O(k) prefix counting where k is the prefix length.\n\n## Key Concepts\n\n- Trie structure with per-node subtree counts\n- Incrementing counts during insertion for O(1) prefix queries\n- Prefix traversal and counting\n\n## Code Example\n\n```javascript\nclass TrieNode {\n  constructor() {\n    this.children = new Map();\n    this.subtreeCount = 0;\n    this.isWord = false;\n  }\n}\n\nclass Trie {\n  constructor() {\n    this.root = new TrieNode();\n  }\n\n  insert(word) {\n    let node = this.root;\n    node.subtreeCount++;\n    \n    for (const char of word) {\n      if (!node.children.has(char)) {\n        node.children.set(char, new TrieNode());\n      }\n      node = node.children.get(char);\n      node.subtreeCount++;\n    }\n    node.isWord = true;\n  }\n\n  getPrefixCount(prefix) {\n    let node = this.root;\n    \n    for (const char of prefix) {\n      if (!node.children.has(char)) {\n        return 0;\n      }\n      node = node.children.get(char);\n    }\n    \n    return node.subtreeCount;\n  }\n}\n```\n\n## Time & Space Complexity\n\n- **insert(word)**: O(L) time, O(L) space where L is word length\n- **getPrefixCount(prefix)**: O(P) time where P is prefix length\n- **Space**: O(N×L) where N is number of words and L is average word length","diagram":"flowchart TD\n  R[Root] --> I[Insert word]\n  I --> C['c']\n  C --> A['a']\n  A --> T['t']\n  T --> Prefix[Prefix node]\n  Prefix --> Q[Return subtreeCount]","difficulty":"beginner","tags":["bst","avl","trie","segment-tree"],"channel":"algorithms","subChannel":"trees","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Meta","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T06:46:53.065Z","createdAt":"2026-01-25T21:42:31.323Z"},{"id":"q-7706","question":"Context: a messaging app's autocomplete. Implement a Trie where each node tracks up to k best phrases in its subtree, ordered by frequency (highest first) and lexicographic tie-break. Provide: insertPhrase(phrase, delta) to adjust frequencies, and topK(prefix, k) returning up to k phrases starting with prefix. Explain update and query costs and edge cases?","answer":"Use a Trie with each node storing a small topK list of phrases in its subtree, updated on every phrase insertion or frequency change. On insertPhrase, walk the path updating counts and rebalancing the","explanation":"## Why This Is Asked\nGauges ability to design a prefix-based retrieval with dynamic scoring using Trie augmentations.\n\n## Key Concepts\n- Trie with per-node topK maintenance\n- Frequency-based prioritization and stable lexicographic tie-breaks\n- Dynamic updates on insertions\n\n## Code Example\n```javascript\nclass TrieNode {\n  constructor() {\n    this.next = new Map();\n    this.isWord = false;\n    this.topK = []; // list of {phrase, freq}\n  }\n}\n\nclass AutocompleteTrie {\n  constructor() {\n    this.root = new TrieNode();\n  }\n  // insert or adjust frequency\n  insertPhrase(phrase, delta) {\n    // simplified: real impl updates along path\n  }\n  topK(prefix, k) {\n    // traverse to prefix and return up to k items from node.topK\n    return [];\n  }\n}\n```\n\n## Follow-up Questions\n- How would you handle memory with large phrase sets?\n- How would you support deletions or decay of frequency over time?","diagram":"flowchart TD\n  Start[Start]\n  Insert[Insert phrase and update topK along path]\n  Query[Query topK(prefix, k)]\n  Return[Return topK results]","difficulty":"beginner","tags":["bst","avl","trie","segment-tree"],"channel":"algorithms","subChannel":"trees","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","DoorDash","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T14:42:56.716Z","createdAt":"2026-01-26T14:42:56.717Z"}],"subChannels":["algorithms","data-structures","dynamic-programming","graphs","load-balancing-algorithms","sorting","trees"],"companies":["Adobe","Airbnb","Amazon","Anthropic","Apple","Bloomberg","Citadel","Cloudflare","Coinbase","Cruise","Databricks","Discord","DoorDash","Facebook","Goldman Sachs","Google","Hashicorp","Hrt","Hugging Face","IBM","Instacart","LinkedIn","Meta","Microsoft","MongoDB","NVIDIA","Netflix","New Relic","OpenAI","Oracle","PayPal","Plaid","Robinhood","Salesforce","Sap","Scale Ai","Slack","Snap","Snowflake","Square","Stripe","Tesla","Twitter","Two Sigma","Uber","Vercel","Zoom"],"stats":{"total":84,"beginner":22,"intermediate":42,"advanced":20,"newThisWeek":33}}