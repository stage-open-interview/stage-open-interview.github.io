{"questions":[{"id":"q-1158","question":"How would you implement a health check mechanism for a load balancer that uses exponential backoff for failed servers, and how does this approach prevent cascading failures during partial outages?","answer":"Implement health checks with exponential backoff to gradually increase retry intervals for failed servers, preventing thundering herd problems and allowing recovery time during partial outages.","explanation":"A health check mechanism with exponential backoff starts by marking a server as failed after consecutive failed health checks (typically 3-5). Instead of immediately retrying the server, the load balancer waits for an initial backoff period (e.g., 5 seconds) before the next health check. If the server is still unhealthy, the backoff period doubles (10s, 20s, 40s, etc.) up to a maximum threshold (e.g., 5 minutes). This prevents the load balancer from wasting resources on constantly checking failed servers and avoids overwhelming recovering servers with immediate traffic.\n\nThe key benefit is preventing cascading failures during partial outages. When multiple servers fail simultaneously, exponential backoff ensures they don't all recover at the same time, which could overwhelm the remaining healthy servers or the recovering servers themselves. The gradual reintroduction of servers allows the system to stabilize and properly distribute load as each server comes back online. This approach is particularly effective in microservices architectures where service dependencies can create failure cascades.\n\nImplementation typically involves maintaining a health state for each server with fields like: failure_count, last_check_time, backoff_multiplier, and next_check_time. The load balancer's routing logic excludes servers in 'failed' state from traffic distribution, while a background scheduler periodically performs health checks according to the exponential backoff schedule. Once a server passes a health check, it's immediately marked as healthy and returned to the active pool, with its backoff state reset.","diagram":null,"difficulty":"intermediate","tags":["load-balancing","health-checks","exponential-backoff","fault-tolerance","cascading-failures"],"channel":"algorithms","subChannel":"algorithms","sourceUrl":null,"videos":null,"companies":[],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T03:29:00.972Z","createdAt":"2026-01-13T03:29:00.972Z"},{"id":"q-2121","question":"How would you implement a load balancer that uses predictive scaling based on request patterns and historical data, and what machine learning techniques would you use to forecast traffic spikes?","answer":"Implement a predictive load balancer using time series forecasting models like ARIMA or LSTM to analyze historical request patterns, then proactively scale server capacity and adjust distribution algorithms.","explanation":"To implement a predictive load balancer, I would first collect historical metrics including request rates, response times, server utilization, and seasonal patterns. The system would use time series forecasting models like ARIMA for simpler patterns or LSTM neural networks for complex, non-linear traffic patterns. These models would predict traffic volumes for the next 15-60 minutes with confidence intervals.\n\nThe load balancer would then adjust its distribution strategy based on these predictions. For predicted high-traffic periods, it would pre-warm additional servers and shift from round-robin to weighted distribution algorithms that account for predicted server load. During low-traffic periods, it would consolidate requests to fewer servers to optimize resource utilization.\n\nKey machine learning techniques would include:\n\n1. **Time Series Analysis**: ARIMA models for capturing trend and seasonality in predictable traffic patterns\n2. **Deep Learning**: LSTM networks for modeling complex, non-linear relationships in traffic data\n3. **Anomaly Detection**: Isolation Forest or Autoencoders to identify unusual traffic spikes that deviate from patterns\n4. **Ensemble Methods**: Combining multiple models to improve prediction accuracy\n\nThe system would continuously retrain models with new data and implement a feedback loop where actual traffic patterns are compared against predictions to improve model accuracy over time.","diagram":null,"difficulty":"intermediate","tags":["predictive-scaling","machine-learning","time-series-forecasting","load-balancing"],"channel":"algorithms","subChannel":"algorithms","sourceUrl":null,"videos":null,"companies":[],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T04:53:20.022Z","createdAt":"2026-01-15T03:28:03.022Z"},{"id":"q-653","question":"How would you implement a consistent hashing load balancer that handles server additions and removals with minimal key remapping? What data structures would you use and how would you handle virtual nodes?","answer":"Use a hash ring with TreeMap/SortedMap for O(log n) lookups, implement virtual nodes to distribute load evenly, and only remap keys between adjacent servers when topology changes.","explanation":"Consistent hashing solves the hot-spot problem of traditional hash-based load balancing by distributing keys across a circular hash space. The core implementation uses a sorted data structure (like TreeMap in Java or std::map in C++) to maintain the ring, allowing O(log n) lookup for which server should handle a given key. Each server is placed on the ring at multiple virtual node positions to ensure more uniform distribution.\n\nWhen a server is added, you only need to remap keys that would have gone to the next server clockwise on the ring. Similarly, when a server fails, its keys are redistributed to the next server clockwise. This minimizes the disruption compared to traditional hashing where adding/removing a server requires remapping all keys. Virtual nodes (typically 100-200 per physical server) help ensure even distribution and handle scenarios where physical servers have different capacities.\n\nIn practice, you'd hash both the server identifier and a virtual node index (like 'server1-vnode-0', 'server1-vnode-1') to place multiple points for each server. The choice of hash function (CRC32, MurmurHash, FNV) affects the distribution quality. For production systems, you'd also need to handle server health checks, weighted distribution for heterogeneous hardware, and potentially implement bounded loads to prevent any single server from becoming overloaded.","diagram":null,"difficulty":"intermediate","tags":["consistent-hashing","load-balancing","distributed-systems","hash-ring","virtual-nodes"],"channel":"algorithms","subChannel":"algorithms","sourceUrl":null,"videos":null,"companies":[],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-10T03:24:42.947Z","createdAt":"2026-01-10T03:24:42.947Z"},{"id":"q-659","question":"How would you implement a consistent hashing load balancer and what advantages does it provide over traditional hash-based load balancing when servers are added or removed?","answer":"Consistent hashing maps both servers and requests to a circular hash space, minimizing key remapping. It provides better distribution and reduces disruption when scaling compared to modulo hashing.","explanation":"Consistent hashing distributes requests across servers by mapping both server identifiers and request keys (like IP addresses or session IDs) to points on a circular hash space. When a request arrives, you hash its key and move clockwise around the ring until you find the next server. This approach minimizes the number of requests that need to be remapped when servers are added or removed, as only the adjacent segments are affected.\n\nThe key advantage over traditional hash-based load balancing (which uses modulo operations) is that consistent hashing only affects 1/n of the requests when a server is added or removed, where n is the total number of servers. With modulo hashing, adding or removing a server would require remapping almost all requests since the divisor changes. This makes consistent hashing ideal for distributed systems that need to scale dynamically.\n\nIn practice, you'd implement this with a hash function like MD5 or SHA-1, maintain a sorted data structure of server hash positions, and often use virtual nodes (multiple hash points per physical server) to improve distribution. This technique is widely used in distributed caches like DynamoDB and memcached clusters.","diagram":null,"difficulty":"intermediate","tags":["load-balancing","consistent-hashing","distributed-systems","scalability"],"channel":"algorithms","subChannel":"algorithms","sourceUrl":null,"videos":null,"companies":[],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-11T03:52:19.569Z","createdAt":"2026-01-11T03:52:19.569Z"},{"id":"q-767","question":"Design a load balancer that implements adaptive load balancing using real-time server metrics. How would you collect and weight server performance data, and what algorithm would you use to dynamically adjust traffic distribution?","answer":"Implement an adaptive load balancer that monitors server metrics like CPU, memory, and response times, then uses a weighted algorithm that dynamically updates server weights based on performance trend","explanation":"An adaptive load balancer requires continuous monitoring of server health metrics. I'd implement a metrics collector that tracks CPU utilization, memory usage, active connections, and response times for each backend server. These metrics would be normalized and combined into a performance score, with lower scores indicating better server health. The collector would use exponential moving averages to smooth out temporary spikes and provide stable weight calculations.\n\nFor the balancing algorithm, I'd use a dynamic weighted least connections approach where server weights are inversely proportional to their performance scores. The weight calculation would be: weight = base_weight / (1 + performance_score), ensuring better-performing servers receive more traffic. Weights would be updated every 30-60 seconds to balance responsiveness with stability. The algorithm would also implement health checks to temporarily remove underperforming servers from rotation.\n\nReal-world implementation would include fallback mechanisms and hysteresis to prevent weight thrashing. For example, a server's weight would only decrease if performance degrades consistently over multiple sampling periods, and weights would have minimum and maximum bounds. This approach is commonly used in cloud environments where server performance can vary due to co-tenant interference or auto-scaling events.","diagram":null,"difficulty":"intermediate","tags":["adaptive-load-balancing","performance-monitoring","dynamic-weighting","real-time-metrics"],"channel":"algorithms","subChannel":"algorithms","sourceUrl":null,"videos":null,"companies":[],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T03:51:47.162Z","createdAt":"2026-01-12T03:51:47.162Z"},{"id":"al-1","question":"When would you choose a Linked List over an Array and what are the key trade-offs for each data structure?","answer":"Choose Linked Lists for frequent insertions/deletions at arbitrary positions (O(1) vs O(n) for arrays) and when memory allocation needs to be dynamic. Arrays excel at O(1) random access, better cache locality (15-30x faster due to spatial locality), and lower memory overhead (no extra pointers). Use Linked Lists in LRU caches, undo/redo systems, or when implementing queues/stacks dynamically.","explanation":"## Time Complexity Trade-offs\n- **Array**: O(1) access, O(n) insert/delete (shift elements)\n- **Linked List**: O(n) access, O(1) insert/delete at known position\n\n## Memory Considerations\n- **Array**: Contiguous memory, cache-friendly, no pointer overhead\n- **Linked List**: Non-contiguous, 8-16 bytes overhead per node (next/prev pointers)\n\n## Real-World Applications\n- **LRU Cache**: Doubly-linked list + hashmap for O(1) operations\n- **Text Editors**: Gap buffers (arrays) vs linked lists for text manipulation\n- **Browser History**: Linked lists for undo/redo functionality\n\n## When to Choose\n- **Array**: Fixed-size datasets, random access needed, memory-constrained environments\n- **Linked List**: Frequent size changes, insert/delete at ends or arbitrary positions\n\n## Performance Impact\nModern CPUs' cache lines (64 bytes) make arrays 20-50x faster for sequential access due to prefetching, making arrays preferable unless O(1) insertions are critical.","diagram":"flowchart TD\n  A[Data Structure Selection] --> B{Frequent Insertions/Deletions?}\n  B -->|Yes| C[Choose Linked List]\n  B -->|No| D{Need O(1) Random Access?}\n  D -->|Yes| E[Choose Array]\n  D -->|No| F{Memory Fragmentation Concern?}\n  F -->|Yes| G[Linked List - Dynamic Memory]\n  F -->|No| H[Array - Contiguous Memory]\n  C --> I[Node: Data + Pointer]\n  E --> J[Fixed Size Elements]\n  G --> K[Non-contiguous Storage]\n  H --> L[Cache-friendly Access]\n  I --> M[O(1) Insert/Delete at Head]\n  J --> N[O(1) Index Access]\n  K --> O[No Pre-allocation Needed]\n  L --> P[Better CPU Cache Performance]","difficulty":"beginner","tags":["struct","comparison","basics"],"channel":"algorithms","subChannel":"data-structures","sourceUrl":null,"videos":null,"companies":["Adobe","Amazon","Apple","Google","Meta","Microsoft"],"eli5":"Imagine you have a train of toy cars connected by hooks. That's a Linked List! If you want to add a new car in the middle, you just unhook two cars and hook the new one in between - super easy! But if you want to find the 5th car, you have to count from the front: 1, 2, 3, 4, 5. Now imagine a row of boxes on a shelf - that's an Array! Finding the 5th box is instant (just look at position 5!), but adding a new box in the middle means pushing all the other boxes over. So: need to add/remove things a lot? Use the train (Linked List). Need to quickly find things by position? Use the shelf (Array)!","relevanceScore":null,"voiceKeywords":["linked list","array","o(1)","o(n)","cache locality","memory overhead","lru cache"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2025-12-27T05:31:52.610Z","createdAt":"2025-12-26 12:51:05"},{"id":"al-165","question":"Implement a Trie data structure for efficient prefix search with insert, search, and startsWith operations. What are its advantages over hash maps for autocomplete systems, and what are the trade-offs?","answer":"Trie provides O(k) prefix search where k is word length, ideal for autocomplete. Space-efficient for common prefixes (e.g., 'pre' shared by 'prefix', 'prefixes'). Hash maps offer O(1) average lookup but can't efficiently find all keys with given prefix. Trie's hierarchical structure enables prefix enumeration, useful for type-ahead suggestions. Trade-offs: higher memory overhead per node, slower for exact matches vs hash maps.","explanation":"## Interview Context\nThis question tests data structure knowledge and trade-off analysis, crucial for optimizing search functionality in applications like autocomplete, spell checkers, and IP routing.\n\n## Key Concepts\n- **Trie Structure**: Tree-like data structure where each node represents a character\n- **Prefix Search**: Efficiently find all words starting with given prefix\n- **Space Complexity**: O(n*m) where n is number of words, m is average word length\n- **Time Complexity**: O(k) for insert/search/startsWith where k is word length\n\n## Implementation Details\n```javascript\nclass TrieNode {\n  constructor() {\n    this.children = {};\n    this.isEndOfWord = false;\n  }\n}\n\nclass Trie {\n  constructor() {\n    this.root = new TrieNode();\n  }\n  \n  insert(word) {\n    let node = this.root;\n    for (let char of word) {\n      if (!node.children[char]) {\n        node.children[char] = new TrieNode();\n      }\n      node = node.children[char];\n    }\n    node.isEndOfWord = true;\n  }\n  \n  search(word) {\n    let node = this.root;\n    for (let char of word) {\n      if (!node.children[char]) return false;\n      node = node.children[char];\n    }\n    return node.isEndOfWord;\n  }\n  \n  startsWith(prefix) {\n    let node = this.root;\n    for (let char of prefix) {\n      if (!node.children[char]) return false;\n      node = node.children[char];\n    }\n    return true;\n  }\n}\n```\n\n## Trade-offs vs Hash Maps\n- **Trie Advantages**: Prefix search O(k), space efficiency for shared prefixes, ordered traversal\n- **Hash Map Advantages**: Lower memory overhead, simpler implementation, O(1) average exact lookup\n- **Use Cases**: Tries for autocomplete/suggestions, hash maps for exact key-value storage\n\n## Follow-up Questions\n1. How would you implement delete operation in a Trie?\n2. What optimizations would you apply for a large-scale autocomplete system?\n3. How would you handle Unicode characters and different languages in a Trie?","diagram":"graph TD\n    A[Root] --> A1[a] --> P1[pp] --> P2[p] --> P3[l] --> E1[apple]\n    A --> A2[a] --> P4[pp] --> P5[l] --> E2[apply]\n    A --> A3[a] --> P6[p] --> P7[l] --> E3[application]\n    A --> A4[a] --> P8[pp] --> P9[l] --> E4[approach]\n    \n    style A fill:#FF6B6B\n    style E1 fill:#4ECDC4\n    style E2 fill:#4ECDC4\n    style E3 fill:#4ECDC4\n    style E4 fill:#95E1D3","difficulty":"intermediate","tags":["struct","basics"],"channel":"algorithms","subChannel":"data-structures","sourceUrl":null,"videos":null,"companies":["Amazon","Apple","Google","Meta","Microsoft"],"eli5":"Imagine building with LEGO blocks! A trie is like organizing your LEGOs by color first, then shape. When you want to find all the red blocks, you just go to the red box - instant! A hash map is like tossing all LEGOs in one big toy chest and having to dig through everything to find what you want. Tries are super fast because they share the first parts of words, just like how 'cat' and 'car' both start with 'c'. It's like having a shortcut that saves you time and space!","relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-25T16:40:15.110Z","createdAt":"2025-12-26 12:51:06"},{"id":"q-187","question":"How would you implement a thread-safe LRU cache using a HashMap and DoublyLinkedList, considering eviction policy and O(1) operations?","answer":"Use a HashMap for O(1) key-value lookups combined with a DoublyLinkedList to maintain access order. Implement thread safety using ReentrantReadWriteLock to allow concurrent reads while ensuring exclusive writes.","explanation":"## Concept Overview\nAn LRU (Least Recently Used) cache efficiently manages memory by evicting the least accessed items when capacity is reached. This implementation leverages a HashMap for fast key lookups and a DoublyLinkedList for maintaining access order.\n\n## Implementation Details\n- **HashMap**: Maps keys to Node references, enabling O(1) lookup operations\n- **DoublyLinkedList**: Maintains access order with head representing most recently used items and tail representing least recently used items\n- **Thread Safety**: ReentrantReadWriteLock provides optimal concurrency - multiple threads can read simultaneously while writes are mutually exclusive\n- **Eviction Policy**: When capacity is exceeded, the tail node (least recently used) is removed\n- **Operations**: Both get() and put() operations maintain O(1) time complexity\n\n## Code Example\n```java\npublic class LRUCache<K, V> {\n    private final Map<K, Node<K, V>> cache;\n    private final DoublyLinkedList<K, V> order;\n    private final ReentrantReadWriteLock lock;\n    private final int capacity;\n    \n    public LRUCache(int capacity) {\n        this.capacity = capacity;\n        this.cache = new HashMap<>();\n        this.order = new DoublyLinkedList<>();\n        this.lock = new ReentrantReadWriteLock();\n    }\n    \n    public V get(K key) {\n        lock.readLock().lock();\n        try {\n            Node<K, V> node = cache.get(key);\n            if (node != null) {\n                // Upgrade to write lock for reordering\n                lock.readLock().unlock();\n                lock.writeLock().lock();\n                order.moveToHead(node);\n                return node.value;\n            }\n            return null;\n        } finally {\n            if (lock.isWriteLocked()) {\n                lock.writeLock().unlock();\n            } else {\n                lock.readLock().unlock();\n            }\n        }\n    }\n    \n    public void put(K key, V value) {\n        lock.writeLock().lock();\n        try {\n            Node<K, V> existing = cache.get(key);\n            if (existing != null) {\n                existing.value = value;\n                order.moveToHead(existing);\n                return;\n            }\n            \n            Node<K, V> newNode = new Node<>(key, value);\n            cache.put(key, newNode);\n            order.addToHead(newNode);\n            \n            if (cache.size() > capacity) {\n                Node<K, V> tail = order.removeTail();\n                cache.remove(tail.key);\n            }\n        } finally {\n            lock.writeLock().unlock();\n        }\n    }\n}```","diagram":"graph TD\n    A[Client Request] --> B{Operation?}\n    B -->|get| C[Read Lock]\n    B -->|put| D[Write Lock]\n    C --> E[HashMap Lookup]\n    E --> F{Key Exists?}\n    F -->|Yes| G[Move to Head]\n    F -->|No| H[Return Null]\n    G --> I[Return Value]\n    D --> J{Key Exists?}\n    J -->|Yes| K[Update Value]\n    J -->|No| L[Create New Node]\n    K --> M[Move to Head]\n    L --> M\n    M --> N{Capacity Full?}\n    N -->|Yes| O[Remove Tail]\n    N -->|No| P[Add to Head]\n    O --> P\n    P --> Q[Release Lock]","difficulty":"intermediate","tags":["arrays","linkedlist","hashtable","heap"],"channel":"algorithms","subChannel":"data-structures","sourceUrl":null,"videos":null,"companies":["Amazon","Goldman Sachs","Google","Microsoft","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":["lru cache","hashmap","doublylinkedlist","eviction policy","thread-safe","reentrantreadwritelock","o(1) operations"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2026-01-02T06:41:40.643Z","createdAt":"2025-12-26 12:51:06"},{"id":"q-277","question":"How would you efficiently process a 50GB log file to extract the top 10 most frequent IP addresses from millions of entries while handling memory constraints and optimizing for performance?","answer":"Implement a streaming solution using Go with buffered I/O and a min-heap. Process the file in chunks using `bufio.Scanner`, count IP frequencies with a hash map, then maintain a top-10 min-heap. Use `sync.Pool` for memory reuse and worker pools for parallel processing. Python's `collections.Counter` with generators works for smaller datasets but lacks the memory efficiency and performance of Go for 50GB+ files.","explanation":"## Interview Context\nThis question tests distributed processing, memory management, and algorithmic optimization skills for senior engineering roles dealing with big data scenarios.\n\n## Technical Approach\n- **Memory Mapping**: Use `mmap` for zero-copy file access, avoiding data duplication\n- **Streaming Algorithm**: Process file in fixed-size chunks (64KB-1MB) with bounded memory\n- **Data Structure**: Min-heap (O(log n)) for top-10 maintenance vs full sort (O(n log n))\n- **Parallel Processing**: Split file into N chunks, process concurrently, then merge results\n\n## Go Implementation\n```go\ntype IPCounter struct {\n    counts map[string]int\n    heap   *MinHeap\n    mu     sync.RWMutex\n}\n\nfunc (ic *IPCounter) processChunk(chunk []byte) {\n    scanner := bufio.NewScanner(bytes.NewReader(chunk))\n    for scanner.Scan() {\n        ip := extractIP(scanner.Text())\n        ic.mu.Lock()\n        ic.counts[ip]++\n        ic.mu.Unlock()\n    }\n}\n\nfunc (ic *IPCounter) getTop10() []IPCount {\n    heap := container.NewMinHeap(10)\n    for ip, count := range ic.counts {\n        if heap.Len() < 10 {\n            heap.Push(IPCount{ip, count})\n        } else if count > heap.Peek().Count {\n            heap.Pop()\n            heap.Push(IPCount{ip, count})\n        }\n    }\n    return heap.ToSlice()\n}\n```\n\n## Performance Benchmarks\n- **Go Solution**: 2.5GB/min, 500MB peak memory, 3-4 cores\n- **Python Counter**: 800MB/min, 2GB peak memory (50GB file crashes)\n- **Memory Efficiency**: 75% reduction vs naive hash map approach\n- **Scaling**: Linear performance improvement up to 8 worker threads\n\n## Key Optimizations\n- **sync.Pool**: Reduces GC pressure by reusing byte buffers\n- **Bounded Heap**: Fixed memory regardless of unique IPs\n- **Chunked Processing**: Predictable memory usage profile\n- **Concurrent Merging**: Parallel counting with single-threaded heap merge","diagram":"flowchart TD\n    A[50GB Log Files] --> B[find -print0]\n    B --> C[xargs -0 -P 8]\n    C --> D[cut -d' ' -f1]\n    D --> E[sort -S 2G -T /tmp]\n    E --> F[uniq -c]\n    F --> G[sort -k1,1nr]\n    G --> H[head -10]\n    H --> I[Top 10 IPs]","difficulty":"advanced","tags":["find","xargs","cut","sort"],"channel":"algorithms","subChannel":"data-structures","sourceUrl":null,"videos":null,"companies":["Amazon","Google","Meta","Microsoft","Netflix","Snowflake"],"eli5":"Imagine you have a huge box of LEGOs with millions of pieces, but you can only play with a few at a time. You want to find the 10 colors you see most often! You'd grab a handful of LEGOs at a time, count each color, and keep a special box for your top 10 favorites. If a new color appears more times than your least favorite, you swap them! You do this over and over until you've looked at all the LEGOs. It's like sorting Halloween candy - you keep a small pile of your 10 favorite kinds, and when you find better candy, you trade out the less good ones. This way, you never need to dump all the candy on the floor at once!","relevanceScore":null,"voiceKeywords":["streaming solution","buffered i/o","min-heap","hash map","sync.pool","worker pools","memory constraints"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2025-12-28T02:05:04.418Z","createdAt":"2025-12-26 12:51:07"},{"id":"q-377","question":"Implement a min-heap using an array that supports insert, extractMin, and peek operations in O(log n) time. Include time/space complexity analysis and edge cases?","answer":"Use array with parent at i, children at 2i+1/2i+2. Insert: add to end, bubble up O(log n). extractMin: swap root with last, remove, bubble down O(log n). peek O(1). Space O(n). Handle empty heap, duplicate values, array resizing. Used in priority queues, Dijkstra's algorithm, streaming median.","explanation":"## Implementation\n```python\nclass MinHeap:\n    def __init__(self):\n        self.heap = []\n    \n    def insert(self, val):\n        self.heap.append(val)\n        self._bubble_up(len(self.heap) - 1)\n    \n    def extractMin(self):\n        if not self.heap: raise IndexError(\"Empty heap\")\n        min_val = self.heap[0]\n        self.heap[0] = self.heap[-1]\n        self.heap.pop()\n        if self.heap:\n            self._bubble_down(0)\n        return min_val\n    \n    def peek(self):\n        if not self.heap: raise IndexError(\"Empty heap\")\n        return self.heap[0]\n```\n\n## Time Complexity\n- Insert: O(log n) - bubble up at most height levels\n- extractMin: O(log n) - bubble down at most height levels  \n- peek: O(1) - direct array access\n- Space: O(n) - array storage\n\n## Edge Cases\n- Empty heap operations throw exceptions\n- Handle duplicate values properly\n- Array resizing when capacity exceeded\n- Maintain heap property during all operations\n\n## Applications\n- Priority queues in task scheduling\n- Dijkstra's shortest path algorithm\n- Streaming median calculation\n- Event-driven simulation systems","diagram":"flowchart TD\n    A[Insert Value] --> B[Add to End of Array]\n    B --> C[Bubble Up: Compare with Parent]\n    C -->|Parent > Child| D[Swap Positions]\n    D --> E[Continue Bubbling Up]\n    C -->|Parent ≤ Child| F[Heap Property Satisfied]\n    G[ExtractMin] --> H[Replace Root with Last Element]\n    H --> I[Bubble Down: Compare with Children]\n    I -->|Child < Parent| J[Swap with Smaller Child]\n    J --> K[Continue Bubbling Down]\n    I -->|Parent ≤ Children| L[Heap Property Restored]","difficulty":"beginner","tags":["arrays","linkedlist","hashtable","heap"],"channel":"algorithms","subChannel":"data-structures","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-25T17:24:22.757Z","createdAt":"2025-12-26 12:51:07"},{"id":"q-407","question":"Given a stream of log events with timestamps, design an algorithm to find the top K most frequent error messages in the last N minutes using O(K) space, where each event contains timestamp, error type, and message?","answer":"Use a sliding window with a hashmap for error counts and a min-heap of size K for top K tracking. As new events arrive, increment counts and update heap. For expired events, decrement counts and remove from heap if count drops below heap minimum. Time complexity: O(log K) per event, Space: O(K).","explanation":"## Algorithm\n- Maintain hashmap: error_message → count within window\n- Use min-heap (size K) storing (count, error_message) pairs\n- For each event: update hashmap, then update heap if needed\n- For expired events: decrement hashmap, adjust heap if count changes\n\n## Complexity Analysis\n- **Time**: O(log K) per event (heap operations)\n- **Space**: O(K) for heap + O(M) for hashmap where M = unique errors in window\n\n## Edge Cases\n- Handle ties in error frequency (lexicographic ordering)\n- Empty window or K > available errors\n- High-frequency events causing heap churn\n\n## Follow-up Questions\n1. How would you handle distributed logs across multiple servers?\n2. What if N is very large (days/weeks) - how to optimize memory?\n3. How to implement this with exact counts vs approximate counting?","diagram":"flowchart TD\n    A[Log Event Stream] --> B[Sliding Window Queue]\n    B --> C[Frequency Hash Map]\n    C --> D{Heap Size < K?}\n    D -->|Yes| E[Push to Min Heap]\n    D -->|No| F{Freq > Heap Min?}\n    F -->|Yes| G[Replace Heap Min]\n    F -->|No| H[Discard]\n    E --> I[Top K Error Messages]\n    G --> I\n    H --> I\n    B --> J{Event Expired?}\n    J -->|Yes| K[Remove from Queue]\n    J -->|No| C\n    K --> L[Decrement Frequency]\n    L --> C","difficulty":"intermediate","tags":["arrays","linkedlist","hashtable","heap"],"channel":"algorithms","subChannel":"data-structures","sourceUrl":null,"videos":null,"companies":["Amazon","Apple","Google","Meta","Microsoft","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-22T16:44:28.268Z","createdAt":"2025-12-26 12:51:05"},{"id":"q-418","question":"Design a data structure that supports range sum queries and point updates on a dynamic array with O(log n) operations. How would you implement this using a segment tree, and what are the trade-offs compared to a Binary Indexed Tree?","answer":"Implement a segment tree with each node storing the sum of its range. For point updates, traverse from leaf to root updating O(log n) nodes. Range queries use divide-and-conquer, combining relevant node sums. Time complexity: O(log n) for both operations. Space: O(2n) for tree array. Consider lazy propagation for range updates and Fenwick tree as memory-efficient alternative with O(n) space.","explanation":"## Core Implementation\nSegment tree uses array representation where node i has children 2i and 2i+1. Each node stores sum of its range [l, r].\n\n```python\nclass SegmentTree:\n    def __init__(self, arr):\n        self.n = len(arr)\n        self.tree = [0] * (4 * self.n)\n        self.build(arr, 0, 0, self.n-1)\n    \n    def update(self, idx, val, node, l, r):\n        if l == r:\n            self.tree[node] = val\n            return\n        mid = (l + r) // 2\n        if idx <= mid:\n            self.update(idx, val, 2*node+1, l, mid)\n        else:\n            self.update(idx, val, 2*node+2, mid+1, r)\n        self.tree[node] = self.tree[2*node+1] + self.tree[2*node+2]\n```\n\n## Performance Analysis\n- **Time**: O(log n) for both operations due to tree height\n- **Space**: O(4n) for complete binary tree representation\n- **Cache efficiency**: Better than recursive approaches\n\n## Advanced Features\n- **Lazy propagation**: Enables range updates in O(log n)\n- **Iterative implementation**: Reduces recursion overhead\n- **Memory optimization**: Use 2n array for bottom-up construction\n\n## Trade-offs vs Fenwick Tree\nSegment tree supports range updates (with lazy propagation) while Fenwick tree only handles point updates. Fenwick tree uses O(n) space vs O(4n) for segment tree, but segment tree provides more flexibility for complex range operations.\n\n## Real-world Applications\n- Database query optimization\n- Game development for spatial queries\n- Financial analytics for time series data\n- Competitive programming for range query problems","diagram":"flowchart TD\n  A[Root: Sum[0-7]] --> B[Left: Sum[0-3]]\n  A --> C[Right: Sum[4-7]]\n  B --> D[Left: Sum[0-1]]\n  B --> E[Right: Sum[2-3]]\n  C --> F[Left: Sum[4-5]]\n  C --> G[Right: Sum[6-7]]\n  D --> H[Leaf: arr[0]]\n  D --> I[Leaf: arr[1]]\n  E --> J[Leaf: arr[2]]\n  E --> K[Leaf: arr[3]]","difficulty":"advanced","tags":["bst","avl","trie","segment-tree"],"channel":"algorithms","subChannel":"data-structures","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-27T05:51:25.075Z","createdAt":"2025-12-26 12:51:06"},{"id":"q-425","question":"Given an array of integers and a target sum, find two numbers that add up to the target. How would you implement this efficiently and what's the time complexity?","answer":"Use a hash map to store numbers and their indices while iterating. For each number, check if target - current exists in the map. This gives O(n) time and O(n) space, better than the O(n²) brute force approach.","explanation":"## Problem\nFind two numbers in an array that sum to a target value.\n\n## Approach\nUse a hash map to track seen numbers and their indices.\n\n## Algorithm\n1. Initialize empty hash map\n2. Iterate through array\n3. For each number, calculate complement (target - current)\n4. Check if complement exists in map\n5. If found, return indices\n6. Otherwise, store current number in map\n\n## Complexity\n- Time: O(n) - single pass through array\n- Space: O(n) - hash map storage\n\n## Edge Cases\n- Handle duplicate numbers\n- Empty array or single element\n- Multiple valid pairs","diagram":"flowchart TD\n  A[Start] --> B[Initialize hash map]\n  B --> C[Iterate array]\n  C --> D[Calculate complement]\n  D --> E{Complement exists?}\n  E -->|Yes| F[Return indices]\n  E -->|No| G[Store number in map]\n  G --> H{More elements?}\n  H -->|Yes| C\n  H -->|No| I[No solution found]","difficulty":"beginner","tags":["arrays","linkedlist","hashtable","heap"],"channel":"algorithms","subChannel":"data-structures","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Amazon","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-08T11:43:17.833Z","createdAt":"2025-12-26 12:51:05"},{"id":"q-442","question":"Given a stream of user actions with timestamps, design a system to find the top K most frequent actions in the last N minutes using O(1) time per query?","answer":"Implement a sliding window approach using a deque for timestamp management and a hash map for frequency tracking, complemented by a max-heap for efficient top-K retrieval.","explanation":"## Solution Overview\nDesign a time-bounded frequency counter that maintains real-time statistics within a sliding window while supporting O(1) update operations and efficient top-K queries.\n\n## Data Structures\n- **Hash Map**: Tracks action → current frequency within the window\n- **Deque**: Maintains chronological timestamps for sliding window management\n- **Max-Heap**: Stores (frequency, action) pairs for top-K queries\n\n## Algorithm\n1. **Update Operation**: Increment action frequency in hash map and add timestamp to deque\n2. **Window Maintenance**: Remove expired timestamps from deque front and decrement corresponding frequencies\n3. **Query Operation**: Construct max-heap from frequency map entries and extract top-K elements\n\n## Complexity Analysis\n- **Update**: O(1) amortized time (deque operations are O(1), hash map updates are O(1))\n- **Query**: O(K log K) time (heap construction and extraction)\n- **Space**: O(U) where U represents unique actions in the current window","diagram":"flowchart TD\n  A[New Action] --> B[Update Frequency Map]\n  B --> C[Add Timestamp to Deque]\n  C --> D[Remove Expired Entries]\n  D --> E[Query Request]\n  E --> F[Build Max-Heap]\n  F --> G[Extract Top K]","difficulty":"intermediate","tags":["arrays","linkedlist","hashtable","heap"],"channel":"algorithms","subChannel":"data-structures","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-09T08:51:43.618Z","createdAt":"2025-12-26 12:51:05"},{"id":"q-565","question":"Given a stream of timestamped events, find the maximum number of concurrent events at any time?","answer":"Use a sweep line algorithm: collect all start and end times, sort them chronologically, then iterate through the timeline while maintaining a count of active events. Increment the count for each start time and decrement for each end time, tracking the maximum concurrent events encountered. O(N log N) time complexity.","explanation":"## Approach\n- Extract all event start and end times with type markers\n- Sort timestamps chronologically to establish timeline order\n- Sweep through timeline, maintaining active event count\n- Track maximum concurrent events during iteration\n\n## Complexity\n- Time: O(N log N) for sorting timestamps\n- Space: O(N) for storing timestamp collection\n\n## Edge Cases\n- Events with identical start/end times\n- Empty input dataset\n- Single event scenario","diagram":null,"difficulty":"advanced","tags":["arrays","linkedlist","hashtable","heap"],"channel":"algorithms","subChannel":"data-structures","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Salesforce","Slack","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-08T11:54:09.023Z","createdAt":"2025-12-27T01:11:27.215Z"},{"id":"al-152","question":"You have a staircase with n steps. You can climb 1, 2, or 3 steps at a time. How many distinct ways can you reach the top? Implement a solution with O(n) time and O(1) space?","answer":"Use DP with rolling variables: ways[i] = ways[i-1] + ways[i-2] + ways[i-3]. Base cases: ways[0]=1, ways[1]=1, ways[2]=2. For large n, use modulo 10^9+7. O(n) time, O(1) space by tracking last 3 values.","explanation":"## Solution Approach\n\nThis is a variation of the classic climbing stairs problem with three step options instead of two.\n\n### Dynamic Programming\nThe recurrence relation is `dp[i] = dp[i-1] + dp[i-2] + dp[i-3]` because from step `i`, you can reach it from `i-1`, `i-2`, or `i-3`.\n\n### Base Cases\n- `dp[0] = 1` (empty path)\n- `dp[1] = 1` (only one 1-step)\n- `dp[2] = 2` (1+1 or 2)\n\n### Space Optimization\nInstead of full DP array, use three variables rolling through the sequence.\n\n### Edge Cases\n- `n = 0`: return 1\n- `n = 1`: return 1  \n- `n = 2`: return 2\n- Handle large n with modulo arithmetic\n\n### Time/Space Complexity\n- **Time**: O(n) - single pass\n- **Space**: O(1) - constant extra space\n\n### Real-World Applications\n- Counting paths in graph traversal\n- Financial modeling with step-based options\n- Game move combination calculations","diagram":"graph TD\n    A[\"n=4 (target)\"] --> B[\"n=3 (4 ways)\"]\n    A --> C[\"n=2 (2 ways)\"]\n    A --> D[\"n=1 (1 way)\"]\n    B --> E[\"n=2 (2 ways)\"]\n    B --> F[\"n=1 (1 way)\"]\n    B --> G[\"n=0 (1 way)\"]\n    C --> H[\"n=1 (1 way)\"]\n    C --> I[\"n=0 (1 way)\"]\n    D --> J[\"n=0 (1 way)\"]\n    \n    style A fill:#ff6b6b\n    style B fill:#4ecdc4\n    style C fill:#4ecdc4\n    style D fill:#4ecdc4\n    style E fill:#95e1d3\n    style F fill:#95e1d3\n    style G fill:#95e1d3\n    style H fill:#95e1d3\n    style I fill:#95e1d3\n    style J fill:#95e1d3","difficulty":"intermediate","tags":["dp","optimization"],"channel":"algorithms","subChannel":"dynamic-programming","sourceUrl":null,"videos":null,"companies":null,"eli5":"Imagine you're climbing a staircase to reach a cookie jar! You can hop 1 step, 2 steps, or 3 steps at a time. To find out how many ways you can get to the top, think about it like playing with building blocks. If you're on step 1, there's only 1 way to get there (1 small hop). From step 2, you can either do two small hops or one big jump - that's 2 ways! For any step, you can come from the step right below (1 hop), two steps below (2 hops), or three steps below (3 hops). So just add up all the ways you could have reached those three previous steps! It's like counting all the different paths to get to your cookie - each step remembers the ways you could have arrived from the three steps before it.","relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-27T05:31:53.024Z","createdAt":"2025-12-26 12:51:07"},{"id":"al-166","question":"Given a string, find the minimum cost to transform it into a palindrome where insertions cost 2 and deletions cost 1. What is the optimal dynamic programming approach?","answer":"Use DP where dp[i][j] represents minimum cost for substring s[i..j]. If s[i]==s[j], dp[i][j]=dp[i+1][j-1]. Otherwise dp[i][j]=min(dp[i+1][j]+1(delete s[i]), dp[i][j-1]+2(insert s[j+1])). Time O(n²), space O(n²) or optimized to O(n). This compares to LCS-based solution with O(n²) time but directly optimizes for the given cost structure.","explanation":"## Problem Context\nThis tests dynamic programming skills for string manipulation, commonly found in coding interviews at FAANG companies.\n\n## Solution Approach\nThe recurrence considers three operations: delete left char, insert matching char for left, or insert+delete both ends. Base cases: dp[i][i]=0 (single char), dp[i][i+1]=0 if equal else min(insert_cost, delete_cost).\n\n## Code Implementation\n```python\ndef min_palindrome_cost(s, insert_cost=2, delete_cost=1):\n    n = len(s)\n    dp = [[0] * n for _ in range(n)]\n    \n    for length in range(2, n+1):\n        for i in range(n-length+1):\n            j = i + length - 1\n            if s[i] == s[j]:\n                dp[i][j] = dp[i+1][j-1]\n            else:\n                dp[i][j] = min(\n                    dp[i+1][j] + delete_cost,\n                    dp[i][j-1] + insert_cost,\n                    dp[i+1][j-1] + insert_cost + delete_cost\n                )\n    return dp[0][n-1]\n```\n\n## Complexity Analysis\n- Time: O(n²) where n is string length\n- Space: O(n²) or O(n) with rolling optimization\n\n## Follow-up Questions\n- How would you modify this for substitution operations?\n- Can you reconstruct the actual transformation sequence?\n- How does this relate to the edit distance problem?","diagram":"graph TD\n    A[Start dp[i][j]] --> B{s[i] == s[j]}\n    B -->|Yes| C[dp[i+1][j-1]]\n    B -->|No| D[1 + min(dp[i+1][j-1], dp[i+1][j], dp[i][j-1])]\n    C --> E[Return dp[i][j]]\n    D --> E","difficulty":"intermediate","tags":["dp","optimization"],"channel":"algorithms","subChannel":"dynamic-programming","sourceUrl":null,"videos":null,"companies":["Amazon","Apple","Google","Meta","Microsoft","Netflix"],"eli5":"Imagine you have a row of colorful blocks with letters on them. You want to make the row look the same forwards and backwards - like a mirror! Each time you add a new block (insertion), it costs 2 coins. Each time you take away a block (deletion), it costs 1 coin. You look at your blocks from both ends and think: 'What's the cheapest way to make them match?' Sometimes you add blocks, sometimes you remove them. You keep checking smaller and smaller pieces until your whole row becomes a perfect mirror! It's like solving a puzzle where you want to spend the fewest coins to make everything symmetrical and pretty.","relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-25T16:40:32.053Z","createdAt":"2025-12-26 12:51:06"},{"id":"al-167","question":"Given a target sum n, count the number of ways to reach it using dice rolls where each roll can be 1-6. Return the result modulo 10^9+7. Optimize for O(n) time and O(1) space?","answer":"Use sliding window DP: dp[i] = sum(dp[i-1] to dp[i-6]) % MOD. Maintain circular buffer of size 6 for O(1) space. Base case dp[0] = 1. Time O(n), Space O(1). Handle i<6 boundary conditions by limiting window range.","explanation":"## Problem\nCount ways to reach sum n using dice rolls (1-6), return modulo 10^9+7.\n\n## Approach\n**Sliding Window DP**: Each state depends on previous 6 states. Instead of O(n) array, use circular buffer of size 6.\n\n## Algorithm\n```python\ndef diceWays(n):\n    MOD = 10**9 + 7\n    dp = [0] * 6\n    dp[0] = 1  # dp[0] = 1\n    \n    for i in range(1, n + 1):\n        curr = sum(dp) % MOD\n        dp[i % 6] = curr\n    \n    return dp[n % 6]\n```\n\n## Complexity\n- **Time**: O(n) - single pass\n- **Space**: O(1) - constant 6-element buffer\n\n## Edge Cases\n- n = 0: return 1 (empty sequence)\n- n < 0: return 0\n- Large n: handle modulo overflow\n\n## Follow-up Questions\n1. How would you extend this to k-sided dice?\n2. Can you optimize further using matrix exponentiation?\n3. What if we needed to count sequences with exactly m rolls?","diagram":"flowchart TD\n  A[Start] --> B[Initialize dp[0]=1]\n  B --> C[For each i from 1 to target]\n  C --> D[Sum dp[i-die] for die=1-6]\n  D --> E[Store in dp[i]]\n  E --> F{i <= target?}\n  F -->|Yes| C\n  F -->|No| G[Return dp[target]]\n  G --> H[End]","difficulty":"intermediate","tags":["dp","optimization"],"channel":"algorithms","subChannel":"dynamic-programming","sourceUrl":null,"videos":{"longVideo":"https://www.youtube.com/watch?v=hfUxjdjVQN4"},"companies":["Amazon","Apple","Google","Meta","Microsoft"],"eli5":"Imagine you have a staircase with 10 steps and you want to climb to the top! You can take 1, 2, 3, 4, 5, or 6 steps at a time - just like rolling a dice! To find how many ways to reach step 10, we start from the bottom. There's exactly 1 way to stay at step 0 (do nothing!). For each step, we count all the ways we could have arrived there from any of the 6 steps before it. Like, to reach step 5, you could have come from step 4 (taking 1 step), OR from step 3 (taking 2 steps), OR from step 2, 1, or even step -1 (which doesn't exist, so we skip!). We add up all these possibilities. By the time we reach step 10, we'll know every possible combination of dice rolls that gets us there!","relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-25T16:41:21.093Z","createdAt":"2025-12-26 12:51:06"},{"id":"al-170","question":"Given an array of integers where each element represents the maximum number of steps you can jump forward from that position, find the minimum number of jumps required to reach the last index. If it's not possible to reach the end, return -1. How would you implement this efficiently?","answer":"Use greedy approach tracking current jump range and furthest reachable position. Iterate once, increment jumps when reaching current range end, update furthest position. Handle unreachable case by checking if furthest position doesn't advance. O(n) time, O(1) space.","explanation":"## Problem\nGiven an array where each element represents maximum jump length from that position, find minimum jumps to reach the last index.\n\n## Approach\n**Greedy Algorithm**: Track current jump range and furthest reachable position. Single pass O(n) solution.\n\n## Implementation\n```python\ndef minJumps(nums):\n    if len(nums) <= 1: return 0\n    jumps, cur_end, furthest = 0, 0, 0\n    \n    for i in range(len(nums)-1):\n        furthest = max(furthest, i + nums[i])\n        if i == cur_end:\n            jumps += 1\n            cur_end = furthest\n            if cur_end >= len(nums)-1:\n                break\n    return jumps if cur_end >= len(nums)-1 else -1\n```\n\n## Complexity\n- Time: O(n) - single pass\n- Space: O(1) - constant extra space\n\n## Edge Cases\n- Empty array: return 0\n- Single element: return 0\n- Unreachable: return -1\n\n## Follow-up Questions\n1. How would you modify this to return the actual jump path?\n2. What if we need to find all possible minimum jump paths?\n3. How does this change if we can only jump exactly the specified number of steps?","diagram":"graph TD\n    A[Start at index 0] --> B[Initialize jumps=0, end=0, furthest=0]\n    B --> C[Iterate through array]\n    C --> D{Reached end of current range?}\n    D -->|Yes| E[Increment jumps, set end=furthest]\n    D -->|No| F[Update furthest = max(furthest, i + nums[i])]\n    E --> F\n    F --> G{Can reach last index?}\n    G -->|Yes| H[Return jumps]\n    G -->|No| I[Continue to next position]\n    I --> C\n    H --> J[End]\n    C --> K{i >= n-1?}\n    K -->|Yes| H\n    K -->|No| D","difficulty":"advanced","tags":["dp","optimization"],"channel":"algorithms","subChannel":"dynamic-programming","sourceUrl":null,"videos":{"shortVideo":"https://www.youtube.com/watch?v=0RHXjBKY9EM","longVideo":"https://www.youtube.com/watch?v=9kyHYVxL4fw"},"companies":["Amazon","Apple","Google","Meta","Microsoft","Netflix"],"eli5":"Imagine you're playing hopscotch on a sidewalk. Each square tells you how many squares you can jump ahead. You want to reach the end with the fewest jumps! Look at all squares you can reach from your current spot. Pick the one that lets you jump farthest next. Keep doing this until you reach the end. If you find a spot where you can't jump forward at all, you're stuck - it's impossible! Like choosing the best stepping stones to cross a river with the fewest steps.","relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-25T16:42:47.521Z","createdAt":"2025-12-26 12:51:07"},{"id":"al-3","question":"What is Dynamic Programming and how does it differ from plain recursion? When would you choose one over the other?","answer":"Dynamic Programming optimizes recursion by storing intermediate results through memoization or tabulation, eliminating redundant computations and trading space for improved time complexity.","explanation":"## Why Asked\nTests understanding of algorithmic optimization techniques and the ability to select appropriate problem-solving approaches.\n\n## Key Concepts\n- Overlapping subproblems\n- Optimal substructure\n- Memoization (top-down) vs tabulation (bottom-up)\n- Time and space complexity trade-offs\n\n## Code Example\n```\n// Plain recursion: O(2^n)\nfunction fib(n) {\n  if (n <= 1) return n;\n  return fib(n-1) + fib(n-2);\n}\n\n// Dynamic Programming: O(n)\nfunction fibDP(n) {\n  const dp = [0, 1];\n  for (let i = 2; i <= n; i++) {\n    dp[i] = dp[i-1] + dp[i-2];\n  }\n  return dp[n];\n}\n```\n\n## Follow-up Questions\n- When would you choose memoization over tabulation?\n- What is the space complexity of bottom-up DP?\n- Can you identify problems suitable for DP vs plain recursion?","diagram":"flowchart TD\n  A[Problem] --> B{Has overlapping subproblems?}\n  B -->|No| C[Use plain recursion]\n  B -->|Yes| D{Has optimal substructure?}\n  D -->|No| E[Use other approaches]\n  D -->|Yes| F[Use Dynamic Programming]\n  F --> G{Implementation choice}\n  G --> H[Top-down memoization]\n  G --> I[Bottom-up tabulation]","difficulty":"advanced","tags":["dp","optimization","theory"],"channel":"algorithms","subChannel":"dynamic-programming","sourceUrl":null,"videos":{"shortVideo":"https://youtube.com/watch?v=oBt53YbR9Kk","longVideo":"https://youtube.com/watch?v=oBt53YbR9Kk"},"companies":["Amazon","Google","Meta"],"eli5":"Imagine you're building with LEGOs and need to make the same tower shape over and over. Plain recursion is like taking apart your tower each time and rebuilding it from scratch - super tiring! Dynamic programming is like taking a picture of each tower shape you build. When you need that shape again, you just look at your picture instead of rebuilding. You use more space to store your pictures, but you save lots of time and energy. Choose plain recursion for small, one-time projects. Choose dynamic programming when you'll need the same answers many times - like when building a big LEGO castle with lots of repeated sections!","relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-30T01:46:48.410Z","createdAt":"2025-12-26 12:51:05"},{"id":"q-328","question":"Given a grid of size m x n where each cell contains a non-negative integer representing the cost to enter that cell, find the minimum cost path from the top-left corner (0,0) to the bottom-right corner (m-1,n-1) moving only right or down. Return both the minimum cost and the path itself?","answer":"Use DP with tabulation: O(mn) time, O(mn) space. Optimize to O(n) space using rolling array. For path reconstruction, maintain parent pointers or backtrack from DP table. Handle edge cases like empty grid, single cell, and large values with 64-bit integers.","explanation":"## Approach\nThis is a classic dynamic programming problem where we build the solution bottom-up. The key insight is that the minimum cost to reach any cell depends only on the minimum costs to reach the cell above and the cell to the left.\n\n## Algorithm\n1. Initialize DP table with same dimensions as grid\n2. dp[0][0] = grid[0][0] (starting point)\n3. Fill first row: dp[0][j] = dp[0][j-1] + grid[0][j]\n4. Fill first column: dp[i][0] = dp[i-1][0] + grid[i][0]\n5. For remaining cells: dp[i][j] = min(dp[i-1][j], dp[i][j-1]) + grid[i][j]\n6. Path reconstruction by backtracking from bottom-right\n\n## Complexity\n- Time: O(mn) - we visit each cell once\n- Space: O(mn) for DP table, can optimize to O(n) using rolling array\n\n## Code Example\n```python\ndef minPathSum(grid):\n    if not grid or not grid[0]:\n        return 0, []\n    \n    m, n = len(grid), len(grid[0])\n    dp = [[0] * n for _ in range(m)]\n    parent = [[None] * n for _ in range(m)]\n    \n    dp[0][0] = grid[0][0]\n    \n    # Fill first row and column\n    for j in range(1, n):\n        dp[0][j] = dp[0][j-1] + grid[0][j]\n        parent[0][j] = (0, j-1)\n    \n    for i in range(1, m):\n        dp[i][0] = dp[i-1][0] + grid[i][0]\n        parent[i][0] = (i-1, 0)\n    \n    # Fill rest of table\n    for i in range(1, m):\n        for j in range(1, n):\n            if dp[i-1][j] < dp[i][j-1]:\n                dp[i][j] = dp[i-1][j] + grid[i][j]\n                parent[i][j] = (i-1, j)\n            else:\n                dp[i][j] = dp[i][j-1] + grid[i][j]\n                parent[i][j] = (i, j-1)\n    \n    # Reconstruct path\n    path = []\n    i, j = m-1, n-1\n    while i >= 0 and j >= 0:\n        path.append((i, j))\n        if parent[i][j]:\n            i, j = parent[i][j]\n        else:\n            break\n    \n    return dp[m-1][n-1], list(reversed(path))\n```\n\n## Follow-up Questions\n- How would you modify this solution if you could also move diagonally?\n- What if we needed to find the k-th minimum path instead of the absolute minimum?\n- How would you handle negative values in the grid?\n- Can you optimize space further using in-place modification?\n- How would this change if we could move in all four directions (need to detect cycles)?","diagram":"flowchart TD\n  A[Start at 0,0] --> B[Initialize DP array]\n  B --> C[Iterate through grid]\n  C --> D[Apply recurrence relation]\n  D --> E[Update DP array]\n  E --> F{Reached end?}\n  F -->|No| C\n  F -->|Yes| G[Return min cost]","difficulty":"intermediate","tags":["dp","memoization","tabulation"],"channel":"algorithms","subChannel":"dynamic-programming","sourceUrl":null,"videos":{"longVideo":"https://www.youtube.com/watch?v=oFkDldu3C_4"},"companies":["Amazon","Apple","Google","Meta","Microsoft","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-27T05:51:24.080Z","createdAt":"2025-12-26 12:51:05"},{"id":"q-440","question":"Given a string s and dictionary wordDict, return all possible sentences where s can be segmented into space-separated words from wordDict. Handle overlapping subproblems efficiently?","answer":"Use DP with memoization: recursively try all word prefixes, cache results for substrings. Time O(n³) worst case, space O(n²) for memo table. Backtrack to reconstruct all valid sentences from DP table.","explanation":"## Approach\n- Build DP table where dp[i] stores all valid sentences for substring s[i:]\n- For each position i, try all words in dictionary that match s[i:]\n- Recursively compute dp[i + word.length] and prepend word\n- Memoize results to avoid recomputation\n\n## Complexity\n- Time: O(n³) in worst case (n positions × n word lengths × n combinations)\n- Space: O(n²) for DP table + recursion stack\n\n## Implementation\n```python\ndef wordBreak(s, wordDict):\n    word_set = set(wordDict)\n    memo = {}\n    \n    def dfs(start):\n        if start in memo:\n            return memo[start]\n        if start == len(s):\n            return [\"\"]\n            \n        sentences = []\n        for end in range(start + 1, len(s) + 1):\n            word = s[start:end]\n            if word in word_set:\n                for suffix in dfs(end):\n                    sentence = word + (\"\" if suffix == \"\" else \" \" + suffix)\n                    sentences.append(sentence)\n        \n        memo[start] = sentences\n        return sentences\n    \n    return dfs(0)\n```","diagram":"flowchart TD\n  A[Start at index 0] --> B[Try all possible words]\n  B --> C{Word matches prefix?}\n  C -->|Yes| D[Recurse on remaining substring]\n  C -->|No| E[Skip to next word]\n  D --> F[Combine word with sub-sentences]\n  F --> G[Cache result for current index]\n  G --> H[Return all valid sentences]","difficulty":"advanced","tags":["dp","memoization","tabulation"],"channel":"algorithms","subChannel":"dynamic-programming","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Google","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-23T15:24:11.517Z","createdAt":"2025-12-26 12:51:05"},{"id":"q-540","question":"Given a string s and a dictionary wordDict, return all possible sentences formed by inserting spaces in s such that each word exists in wordDict. Use DP with memoization to avoid exponential recomputation?","answer":"Use DP with memoization: dp[i] stores all valid sentences from s[i:]. For each position i, try all words in dict that match s[i:j]. Recursively get dp[j] results and combine with current word. Cache r","explanation":"## Approach\n- Use DP with memoization to avoid exponential recomputation\n- dp[i] stores all valid sentences from s[i:]\n- Try all words in dict that match s[i:j]\n- Recursively get dp[j] results and combine with current word\n\n## Implementation\n```python\ndef wordBreak(s, wordDict):\n    word_set = set(wordDict)\n    memo = {}\n    \n    def dfs(start):\n        if start in memo:\n            return memo[start]\n        if start == len(s):\n            return [\"\"]\n            \n        sentences = []\n        for end in range(start + 1, len(s) + 1):\n            word = s[start:end]\n            if word in word_set:\n                for rest in dfs(end):\n                    sentence = word + (\"\" if rest == \"\" else \" \" + rest)\n                    sentences.append(sentence)\n        \n        memo[start] = sentences\n        return sentences\n    \n    return dfs(0)\n```\n\n## Complexity\n- Time: O(n³) worst case due to substring checks\n- Space: O(n²) for memoization table\n\n## Edge Cases\n- Empty string returns empty list\n- No valid combinations returns empty list\n- Multiple valid sentences returned in list","diagram":"flowchart TD\n  A[Start at index 0] --> B[Try all words matching s[i:j]]\n  B --> C[Word in dict?]\n  C -->|Yes| D[Recursively solve from j]\n  C -->|No| E[Skip to next j]\n  D --> F[Combine current word with results]\n  F --> G[Cache results in memo[i]]\n  G --> H[Return all valid sentences]","difficulty":"advanced","tags":["dp","memoization","tabulation"],"channel":"algorithms","subChannel":"dynamic-programming","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","LinkedIn","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-27T05:31:41.048Z","createdAt":"2025-12-26 12:51:07"},{"id":"q-214","question":"Given a directed weighted graph with up to 10^6 edges and frequent edge weight updates, design a data structure that supports dynamic shortest path queries with sub-millisecond response time?","answer":"Use a dynamic Dijkstra variant with incremental updates and hierarchical decomposition, maintaining O(log n) per update and query.","explanation":"## Concept Overview\nDynamic shortest path requires handling frequent edge weight updates while maintaining fast query responses. Traditional Dijkstra's O(E + V log V) is too slow for production scale.\n\n## Implementation Details\n- **Hierarchical Decomposition**: Partition graph into clusters using METIS or custom partitioning\n- **Multi-level Indexing**: Maintain precomputed distances between cluster boundaries\n- **Incremental Updates**: Use dynamic programming to update only affected paths\n- **Lazy Recomputation**: Defer full recomputation until query performance degrades\n\n## Code Structure\n```python\nclass DynamicShortestPath:\n    def __init__(self, graph):\n        self.clusters = self.partition_graph(graph)\n        self.cluster_distances = self.precompute_inter_cluster()\n        self.local_paths = {c: {} for c in self.clusters}\n    \n    def update_edge(self, u, v, new_weight):\n        cluster = self.get_cluster(u)\n        self.invalidate_local_paths(cluster, u, v)\n        self.update_inter_cluster_if_boundary(u, v, new_weight)\n    \n    def query(self, source, target):\n        return self.bidirectional_dijkstra(source, target)\n```\n\n## Common Pitfalls\n- **Memory Overhead**: Hierarchical structures can consume 3-5x memory\n- **Partition Quality**: Poor clustering leads to frequent cross-cluster queries\n- **Update Cascades**: Edge updates can trigger expensive recomputation cascades\n- **Concurrency**: Thread-safe updates require careful locking strategies","diagram":"graph TD\n    A[Client Query] --> B{Source/Target in Same Cluster?}\n    B -->|Yes| C[Local Dijkstra]\n    B -->|No| D[Multi-level Path Search]\n    D --> E[Cluster Boundary Search]\n    E --> F[Inter-cluster Distance Lookup]\n    F --> G[Local Path Assembly]\n    G --> H[Return Result]\n    I[Edge Update] --> J{Boundary Edge?}\n    J -->|Yes| K[Update Inter-cluster Index]\n    J -->|No| L[Invalidate Local Cache]\n    K --> M[Mark Affected Clusters]\n    L --> M","difficulty":"advanced","tags":["bfs","dfs","dijkstra","topological"],"channel":"algorithms","subChannel":"graphs","sourceUrl":null,"videos":null,"companies":["Amazon","Goldman Sachs","Google","Microsoft","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-22T04:56:47.926Z","createdAt":"2025-12-26 12:51:07"},{"id":"q-286","question":"Explain the difference between BFS and DFS and when would you use each?","answer":"BFS explores level by level using a queue, finding shortest paths with O(V+E) time and O(V) space; DFS uses a stack/recursion for deep exploration with O(V+E) time but O(h) space where h is tree height.","explanation":"## Why Asked\nTests understanding of graph traversal fundamentals, algorithm selection, and space-time tradeoffs in real-world scenarios.\n\n## Key Concepts\n**BFS (Breadth-First Search)**: Uses queue, explores neighbors level-by-level. Time: O(V+E). Space: O(V) worst case (all nodes in queue). Guarantees shortest path in unweighted graphs.\n\n**DFS (Depth-First Search)**: Uses stack or recursion, explores as deep as possible before backtracking. Time: O(V+E). Space: O(h) where h is maximum depth (recursion stack), O(V) worst case for skewed graphs.\n\n## When to Use Each\n**BFS**: Shortest path routing, social network friend suggestions, web crawlers finding closest pages, level-order tree traversals.\n\n**DFS**: Topological sorting, cycle detection, maze solving, finding connected components, backtracking problems like Sudoku or N-Queens.\n\n## Code Example\n```javascript\n// BFS - finds shortest path\nfunction bfs(graph, start) {\n  const queue = [start];\n  const visited = new Set([start]);\n  const distances = { [start]: 0 };\n  \n  while (queue.length) {\n    const node = queue.shift();\n    for (const neighbor of graph[node]) {\n      if (!visited.has(neighbor)) {\n        visited.add(neighbor);\n        distances[neighbor] = distances[node] + 1;\n        queue.push(neighbor);\n      }\n    }\n  }\n  return distances;\n}\n\n// DFS - recursive implementation\nfunction dfs(graph, node, visited = new Set(), result = []) {\n  if (visited.has(node)) return result;\n  \n  visited.add(node);\n  result.push(node);\n  \n  for (const neighbor of graph[node]) {\n    dfs(graph, neighbor, visited, result);\n  }\n  \n  return result;\n}\n\n// DFS - iterative implementation\nfunction dfsIterative(graph, start) {\n  const stack = [start];\n  const visited = new Set();\n  const result = [];\n  \n  while (stack.length) {\n    const node = stack.pop();\n    if (visited.has(node)) continue;\n    \n    visited.add(node);\n    result.push(node);\n    \n    // Add neighbors to stack (reverse for consistent order)\n    for (let i = graph[node].length - 1; i >= 0; i--) {\n      const neighbor = graph[node][i];\n      if (!visited.has(neighbor)) {\n        stack.push(neighbor);\n      }\n    }\n  }\n  return result;\n}\n```\n\n## Memory Considerations\nBFS can consume significant memory for wide graphs (queue holds all nodes at current level). DFS is memory-efficient for deep, narrow graphs but may cause stack overflow for very deep recursion. Choose based on graph structure and problem requirements.","diagram":"flowchart TD\n  A[Graph Traversal] --> B[BFS: Queue-based]\n  A --> C[DFS: Stack-based]\n  B --> D[Level order]\n  B --> E[Shortest path]\n  C --> F[Deep exploration]\n  C --> G[Memory efficient]","difficulty":"intermediate","tags":["bfs","dfs","dijkstra","topological"],"channel":"algorithms","subChannel":"graphs","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":["bfs","dfs","level by level","shortest path","depth first","memory usage","exhaustive search"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2025-12-28T01:58:57.092Z","createdAt":"2025-12-26 12:51:07"},{"id":"q-350","question":"Given a directed graph representing city intersections and one-way streets, implement a function to find if there's a valid route from point A to point B using BFS. Return the shortest path distance or -1 if no route exists?","answer":"Use BFS with a queue to explore the graph level by level, tracking visited nodes and distances from the start. Return the distance when the target node is found, or -1 if all reachable nodes are exhausted without finding the target.","explanation":"## Why This Is Asked\nThis question tests fundamental graph traversal algorithms, BFS implementation, and pathfinding skills - critical for autonomous vehicle navigation systems and route planning applications.\n\n## Expected Answer\nThe candidate should implement BFS using a queue, maintain a visited set to avoid cycles, track distances from the start node, and handle edge cases such as when start equals target, disconnected graphs, or empty input. They should also explain why BFS guarantees the shortest path in unweighted graphs.\n\n## Code Example\n```python\nfrom collections import deque\n\ndef shortest_path(graph, start, end):\n    if start == end:\n        return 0\n    \n    queue = deque([(start, 0)])\n    visited = {start}\n    \n    while queue:\n        node, distance = queue.popleft()\n        \n        for neighbor in graph.get(node, []):\n            if neighbor == end:\n                return distance + 1\n            if neighbor not in visited:\n                visited.add(neighbor)\n                queue.append((neighbor, distance + 1))\n    \n    return -1\n```","diagram":"flowchart TD\n  A[Start BFS] --> B[Initialize queue with start]\n  B --> C[Mark start as visited]\n  C --> D[Queue empty?]\n  D -->|Yes| E[Return -1]\n  D -->|No| F[Dequeue node]\n  F --> G[Node equals target?]\n  G -->|Yes| H[Return distance]\n  G -->|No| I[Explore neighbors]\n  I --> J[Add unvisited neighbors to queue]\n  J --> D","difficulty":"beginner","tags":["bfs","dfs","dijkstra","topological"],"channel":"algorithms","subChannel":"graphs","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":"https://www.youtube.com/watch?v=wu0ckYkltus"},"companies":["Amazon","Apple","Cruise","Google","Meta","Microsoft","Netflix","Vercel"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-29T08:39:50.481Z","createdAt":"2025-12-26 12:51:05"},{"id":"q-394","question":"Given a directed acyclic graph representing task dependencies where each task takes 1 unit of time and you have unlimited workers, what is the minimum time to complete all tasks?","answer":"Use topological sort with BFS to process tasks level by level. The minimum time equals the length of the longest path (critical path) in the DAG. O(V+E) time, O(V) space. Track completion times using DP: dp[node] = max(dp[parent]) + 1.","explanation":"## Core Approach\nThe problem reduces to finding the critical path—the longest path in the DAG. This represents the minimum time needed since tasks on different branches can run in parallel.\n\n## Algorithm\n1. Perform topological sort using Kahn's algorithm\n2. Use DP array where dp[node] = earliest completion time\n3. For each node in topological order: dp[node] = max(dp[parents]) + 1\n4. Answer = max(dp[node]) for all nodes\n\n## Code Example\n```python\ndef min_completion_time(n, edges):\n    adj = [[] for _ in range(n)]\n    indegree = [0] * n\n    \n    for u, v in edges:\n        adj[u].append(v)\n        indegree[v] += 1\n    \n    # Topological sort with DP\n    dp = [0] * n\n    queue = [i for i in range(n) if indegree[i] == 0]\n    \n    while queue:\n        node = queue.pop(0)\n        for neighbor in adj[node]:\n            dp[neighbor] = max(dp[neighbor], dp[node] + 1)\n            indegree[neighbor] -= 1\n            if indegree[neighbor] == 0:\n                queue.append(neighbor)\n    \n    return max(dp)\n```","diagram":"flowchart TD\n    A[Build Graph & Indegree] --> B[Queue Zero Indegree Tasks]\n    B --> C{Workers Available?}\n    C -->|Yes| D[Assign Task to Worker]\n    C -->|No| E[Wait for Next Worker]\n    D --> F[Update Task Dependencies]\n    E --> G[Advance Time]\n    F --> H{All Tasks Completed?}\n    H -->|No| C\n    H -->|Yes| I[Return Total Time]\n    G --> C","difficulty":"advanced","tags":["bfs","dfs","dijkstra","topological"],"channel":"algorithms","subChannel":"graphs","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-29T08:49:17.988Z","createdAt":"2025-12-26 12:51:05"},{"id":"q-662","question":"Given a directed graph with N nodes and M weighted edges (positive weights) and a source s, describe an implementation to (1) identify nodes reachable from s via BFS, (2) compute shortest distances dist[] to all nodes with Dijkstra, and (3) count the number of distinct shortest s→v paths for every v (mod 1e9+7). How would you build a DAG of edges on shortest paths and perform a topological DP over dist-ordered nodes to obtain path counts?","answer":"Run BFS from s to mark reachable nodes. Then Dijkstra from s to compute dist[]. Build DAG of edges (u,v) with dist[u] + w(u,v) = dist[v]. Topologically sort by dist and DP: ways[s]=1; for u in order, ","explanation":"## Why This Is Asked\nThis question blends BFS reachability, Dijkstra shortest paths, and counting shortest paths via a DAG and topological order.\n\n## Key Concepts\n- BFS/DFS reachability\n- Dijkstra's algorithm\n- Shortest-path DAG\n- Topological DP and modular counting\n\n## Code Example\n```javascript\nfunction countShortestPaths(n, adj, s) {\n  const dist = Array(n).fill(Infinity); dist[s] = 0;\n  // priority queue based Dijkstra (pseudo-notation)\n  const pq = new MinPQ((a,b)=>a[0]-b[0]);\n  pq.enqueue([0,s]);\n  while(!pq.isEmpty()){\n    const [d,u] = pq.dequeue(); if (d!==dist[u]) continue;\n    for (const {to:v, w} of adj[u]){\n      if (dist[v] > d + w){ dist[v] = d + w; pq.enqueue([dist[v], v]); }\n    }\n  }\n  const dag = Array.from({length:n}, ()=>[]);\n  for (let u=0; u<n; u++) for (const {to:v, w} of adj[u]) if (dist[u] + w === dist[v]) dag[u].push(v);\n  const order = Array.from({length:n}, (_,i)=>i).sort((a,b)=>dist[a]-dist[b]);\n  const MOD = 1000000007; const ways = Array(n).fill(0); ways[s] = 1;\n  for (const u of order) for (const v of dag[u]) ways[v] = (ways[v] + ways[u]) % MOD;\n  return {dist, ways};\n}\n```\n\n## Follow-up Questions\n- How would zero-weight edges affect the approach? \n- How to extend to count modulo a large prime or handle multiple sources?","diagram":"flowchart TD\n  S[Source s] --> R[Reachable BFS]\n  R --> D[Dijkstra dist]\n  D --> DAG[DAG of shortest-path edges]\n  DAG --> DP[Topological DP by dist]","difficulty":"advanced","tags":["bfs","dfs","dijkstra","topological"],"channel":"algorithms","subChannel":"graphs","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","NVIDIA","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-11T13:55:31.761Z","createdAt":"2026-01-11T13:55:31.761Z"},{"id":"q-596","question":"Explain the differences between round-robin, least connections, and IP hash load balancing algorithms. When would you choose each one?","answer":"Round-robin distributes requests evenly, least connections sends traffic to the server with fewest active connections, and IP hash routes clients to the same server based on their IP address.","explanation":"Load balancing algorithms are crucial for distributing traffic across multiple servers efficiently. Round-robin is the simplest approach, cycling through servers sequentially. It works well for identical servers with similar processing capabilities and stateless applications. However, it doesn't account for varying server loads or request complexities.\n\nLeast connections algorithm monitors active connections and routes new requests to the server with the fewest current connections. This is ideal when requests have varying processing times or when servers have different capabilities. For example, in a microservices architecture where some endpoints are more resource-intensive, this prevents overloading a single server.\n\nIP hash uses the client's IP address to determine which server handles the request, ensuring session persistence. This is essential for applications requiring stateful connections, like shopping carts or user sessions. The hash function consistently maps the same IP to the same server, preventing session loss during load balancing.\n\nChoose round-robin for simple, stateless applications with uniform servers. Use least connections for heterogeneous environments or varying request complexities. Select IP hash when session persistence is critical, such as e-commerce platforms or authentication systems.","diagram":null,"difficulty":"intermediate","tags":["load-balancing","algorithms","networking","scalability","system-design"],"channel":"algorithms","subChannel":"load-balancing-algorithms","sourceUrl":null,"videos":null,"companies":["Google","Amazon","Meta","Netflix","Microsoft","Twitter","Uber","Airbnb"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-27T10:36:51.369Z","createdAt":"2025-12-27T10:36:51.369Z"},{"id":"q-627","question":"Explain the difference between round-robin and weighted round-robin load balancing algorithms. When would you choose one over the other?","answer":"Round-robin distributes requests evenly across servers, while weighted round-robin assigns more requests to servers with higher capacity based on configured weights.","explanation":"Round-robin load balancing is the simplest algorithm where requests are distributed sequentially to each server in the pool, cycling back to the first server after reaching the last. This works well when all servers have equal capacity and processing power. However, in real-world scenarios, servers often have different specifications - some might have more CPU, RAM, or faster network connections.\n\nWeighted round-robin addresses this limitation by assigning a weight to each server, representing its relative capacity. A server with weight 3 receives three times more requests than a server with weight 1. For example, if you have three servers with weights 5, 3, and 2, out of every 10 requests, 5 go to the first server, 3 to the second, and 2 to the third.\n\nChoose round-robin for homogeneous server environments where all nodes have identical hardware and performance characteristics. Use weighted round-robin in heterogeneous environments with varying server capacities, when upgrading infrastructure gradually, or when you want to control traffic distribution during maintenance or testing phases. Major cloud providers like AWS, Google Cloud, and Azure implement both algorithms in their load balancers.","diagram":null,"difficulty":"intermediate","tags":["load-balancing","algorithms","distributed-systems","networking"],"channel":"algorithms","subChannel":"load-balancing-algorithms","sourceUrl":null,"videos":null,"companies":["Amazon","Google","Microsoft","Netflix","Facebook","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-01T03:52:43.894Z","createdAt":"2026-01-01T03:52:43.894Z"},{"id":"al-163","question":"You have an array where each element appears twice except one element that appears once. Sort the array in O(n) time without using extra space for sorting. How would you approach this?","answer":"The problem constraints are impossible to satisfy. Sorting an arbitrary array in O(n) time with O(1) space violates the comparison-based sorting lower bound of Ω(n log n). If the actual goal is to find the unique element (not sort), use XOR traversal: `let unique = 0; for (let num of arr) unique ^= num;` which runs in O(n) time, O(1) space. If sorting is truly required, the optimal solution is O(n log n) time using algorithms like merge sort or quicksort.","explanation":"## Why the Problem is Impossible\n\n### Computational Complexity Theory\n- **Comparison sorting lower bound**: Any algorithm that sorts by comparing elements requires Ω(n log n) comparisons in the worst case\n- **Information theory argument**: Sorting requires distinguishing between n! possible permutations, requiring log₂(n!) ≈ n log n bits of information\n- **Space-time tradeoff**: Achieving linear-time sorting would require additional space constraints\n\n### Correct Interpretation: Find Unique Element\nIf the question intended to find the unique element:\n```javascript\nfunction findUnique(arr) {\n  let unique = 0;\n  for (let num of arr) {\n    unique ^= num;\n  }\n  return unique;\n}\n```\n- **Time complexity**: O(n)\n- **Space complexity**: O(1)\n- **Why XOR works**: Properties `a ^ a = 0` and `a ^ 0 = a` cancel out duplicate pairs\n\n### If Sorting is Actually Required\nThe optimal solutions are:\n- **Merge sort**: O(n log n) time, O(n) space\n- **Quicksort**: O(n log n) average time, O(log n) space\n- **Counting sort**: O(n) time, O(k) space (where k is value range)\n\n### Interview Assessment\nThis question tests understanding of computational limits and problem clarification. A strong candidate should recognize the impossible constraints and seek clarification rather than proposing an invalid solution.","diagram":"graph TD\n    A[Start: Unsorted Array] --> B[XOR all elements]\n    B --> C[Find unique element]\n    C --> D[Three-way partition]\n    D --> E[Elements < unique]\n    D --> F[Unique element]\n    D --> G[Elements > unique]\n    E --> H[Sort pairs in left partition]\n    G --> I[Sort pairs in right partition]\n    H --> J[Combine: Left + Unique + Right]\n    I --> J\n    J --> K[Sorted Array]\n    \n    style C fill:#90EE90\n    style D fill:#FFB6C1\n    style K fill:#87CEEB","difficulty":"intermediate","tags":["sort","complexity"],"channel":"algorithms","subChannel":"sorting","sourceUrl":null,"videos":{"shortVideo":"https://youtube.com/watch?v=XKu_SEDAykw"},"companies":["Amazon","Apple","Google","Meta","Microsoft"],"eli5":"Imagine you have a big box of toy cars where every car has a twin except one special car that's all alone! To find that special car, we can play a magic game: take pairs of the same car and they disappear like magic! Only the lonely car stays. Now we know which car is special, so we can quickly put all the twin cars on one side and the special car in the middle. It's like organizing your toys super fast without needing extra boxes or asking for help!","relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-05T06:47:41.269Z","createdAt":"2025-12-26 12:51:06"},{"id":"al-2","question":"Compare QuickSort, MergeSort, and Timsort. When would you choose each algorithm and what are their key trade-offs in production systems?","answer":"QuickSort: O(n log n) avg, O(n²) worst, in-place, cache-friendly, unstable. MergeSort: O(n log n) always, O(n) space, stable, great for linked lists. Timsort: O(n log n) worst, O(n) best, hybrid (merge+insertion), stable, optimized for real-world data patterns. Choose QuickSort for memory-constrained cache-optimized scenarios, MergeSort for stability requirements, Timsort for general-purpose sorting.","explanation":"## Algorithm Characteristics\n\n**QuickSort** excels with cache locality due to in-place partitioning, making it ~2-3x faster than MergeSort on arrays. However, its O(n²) worst case requires mitigation via median-of-three or random pivot selection.\n\n**MergeSort** guarantees O(n log n) performance and stability, crucial for sorting records by multiple keys. Its O(n) space requirement is problematic for memory-constrained environments but ideal for external sorting.\n\n**Timsort** (Python/Java default) combines MergeSort's stability with insertion sort's O(n) best case on nearly-sorted data. It detects natural runs, achieving 20-30% better performance on real-world datasets.\n\n## Production Considerations\n\n- **Stability matters** when sorting by multiple criteria (e.g., sort by department, then by name)\n- **Cache impact**: QuickSort's in-place nature reduces cache misses vs MergeSort's scattered memory access\n- **Hybrid approaches**: Most libraries use introsort (QuickSort + HeapSort fallback) to eliminate worst-case scenarios\n- **Parallelization**: MergeSort parallelizes naturally; QuickSort requires careful load balancing\n\n## Real-World Applications\n\n- **Database indexes**: Often use B-tree variants (modified MergeSort) for disk-based sorting\n- **In-memory analytics**: QuickSort variants for speed-critical aggregations\n- **Streaming data**: External MergeSort for datasets exceeding RAM capacity","diagram":"\ngraph TD\n    A[Array] --> P{Pick Pivot}\n    P --> L[Left < Pivot]\n    P --> R[Right > Pivot]\n    L --> Sort1[Recurse]\n    R --> Sort2[Recurse]\n","difficulty":"intermediate","tags":["sort","recursion","complexity"],"channel":"algorithms","subChannel":"sorting","sourceUrl":null,"videos":{"shortVideo":"https://www.youtube.com/watch?v=oJxeWV6zbIM","longVideo":"https://www.youtube.com/watch?v=9FMc-kHwlQs"},"companies":["Amazon","Google","Meta","Microsoft","Netflix","Stripe"],"eli5":"Imagine you have a messy pile of numbered cards to sort. QuickSort is like picking one card (the 'boss'), then making two piles: cards smaller than the boss go left, bigger ones go right. Keep doing this with each pile until everything is sorted! It's fast but sometimes picks a bad boss. MergeSort is different - you split the pile in half, sort each half, then carefully combine them like a zipper. It's always reliable but needs extra table space for the combining. QuickSort is like a speedy but sometimes messy kid, MergeSort is like a careful kid who always cleans up but needs more room!","relevanceScore":null,"voiceKeywords":["quicksort","mergesort","timsort","time complexity","space complexity","stability","cache-friendly","hybrid algorithm"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2025-12-27T05:51:44.778Z","createdAt":"2025-12-26 12:51:05"},{"id":"q-300","question":"Explain the difference between quicksort and mergesort, including their time and space complexities?","answer":"Quicksort has an average time complexity of O(n log n) and worst-case of O(n²), with O(log n) space complexity. Mergesort consistently performs at O(n log n) time complexity but requires O(n) auxiliary space. The key distinction is that quicksort is an in-place sorting algorithm, while mergesort is stable and maintains the relative order of equal elements.","explanation":"## Why Asked\nThis question evaluates understanding of fundamental sorting algorithms and their practical trade-offs in real-world applications.\n\n## Key Concepts\n- Divide and conquer strategies\n- Partitioning vs merging approaches\n- In-place vs auxiliary space requirements\n- Algorithm stability and performance characteristics\n\n## Code Example\n```\n// Quicksort partition\nfunction partition(arr, low, high) {\n  const pivot = arr[high];\n  let i = low - 1;\n  for (let j = low; j < high; j++) {\n    if (arr[j] < pivot) {\n      i++;\n      [arr[i], arr[j]] = [arr[j], arr[i]];\n    }\n  }\n  [arr[i + 1], arr[high]] = [arr[high], arr[i + 1]];\n  return i + 1;\n}\n\n// Mergesort merge\nfunction merge(left, right) {\n  const result = [];\n  let i = 0, j = 0;\n  \n  while (i < left.length && j < right.length) {\n    if (left[i] <= right[j]) {\n      result.push(left[i++]);\n    } else {\n      result.push(right[j++]);\n    }\n  }\n  \n  return result.concat(left.slice(i)).concat(right.slice(j));\n}\n```","diagram":"flowchart TD\n  A[Start] --> B{Choose Algorithm}\n  B -->|Quicksort| C[Select Pivot]\n  B -->|Mergesort| D[Divide Array]\n  C --> E[Partition Elements]\n  D --> F[Recursive Sort]\n  E --> G[Recursive Calls]\n  F --> H[Merge Subarrays]\n  G --> I[Combine Results]\n  H --> J[Sorted Array]\n  I --> J\n  J --> K[End]","difficulty":"beginner","tags":["quicksort","mergesort","complexity"],"channel":"algorithms","subChannel":"sorting","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-01T06:40:59.440Z","createdAt":"2025-12-26 12:51:04"},{"id":"q-362","question":"Given an array of integers, implement quicksort with proper partitioning, explain its O(n log n) average vs O(n²) worst-case complexity, and compare with mergesort in terms of stability, space usage, and practical performance?","answer":"Quicksort uses partitioning around a pivot. Average O(n log n) due to balanced splits, worst O(n²) with poor pivots (sorted data). Mergesort guarantees O(n log n) and is stable but needs O(n) extra space. Use quicksort for average-case speed, mergesort when stability or guaranteed performance matters.","explanation":"## Quicksort Implementation\n\n```python\ndef quicksort(arr, low=0, high=None):\n    if high is None: high = len(arr) - 1\n    if low < high:\n        pivot_idx = partition(arr, low, high)\n        quicksort(arr, low, pivot_idx - 1)\n        quicksort(arr, pivot_idx + 1, high)\n\ndef partition(arr, low, high):\n    # Lomuto partition scheme\n    pivot = arr[high]  # Choose last element as pivot\n    i = low - 1\n    for j in range(low, high):\n        if arr[j] <= pivot:\n            i += 1\n            arr[i], arr[j] = arr[j], arr[i]\n    arr[i + 1], arr[high] = arr[high], arr[i + 1]\n    return i + 1\n```\n\n## Pivot Selection Strategies\n\n- **Last element**: Simple but O(n²) worst case on sorted data\n- **Random element**: Good average case, minimal overhead\n- **Median-of-three**: Reduces probability of worst case\n- **Introsort**: Switch to heapsort when recursion depth exceeds 2log₂n\n\n## Complexity Analysis\n\n**Average Case O(n log n)**: Each partition splits array roughly in half, creating log n levels with O(n) work per level.\n\n**Worst Case O(n²)**: Consistently unbalanced partitions (already sorted array with poor pivot choice) create n levels of O(n) work.\n\n**Space Complexity**: O(log n) average (recursion stack), O(n) worst case.\n\n## Quicksort vs Mergesort\n\n| Factor | Quicksort | Mergesort |\n|--------|-----------|-----------|\n| Time Complexity | O(n log n) avg, O(n²) worst | O(n log n) guaranteed |\n| Space | O(log n) avg, O(n) worst | O(n) auxiliary |\n| Stability | Not stable | Stable |\n| Cache Performance | Excellent (in-place) | Poor (sequential access) |\n| Adaptivity | No | Partially adaptive |\n\n## Practical Considerations\n\n- Use **quicksort** for general-purpose sorting where average performance matters\n- Choose **mergesort** for external sorting, linked lists, or when stability required\n- **Timsort** (Python's default) combines mergesort with insertion sort for real-world data\n- Consider **introsort** (C++ std::sort) for guaranteed O(n log n) with fast average case","diagram":"flowchart TD\n  A[Unsorted Array] --> B[Choose Pivot Element]\n  B --> C[Partition Around Pivot]\n  C --> D{Elements < Pivot?}\n  D -->|Yes| E[Left Subarray]\n  D -->|No| F[Right Subarray]\n  E --> G[Recursive Quicksort]\n  F --> G\n  G --> H[Sorted Array]","difficulty":"beginner","tags":["quicksort","mergesort","complexity"],"channel":"algorithms","subChannel":"sorting","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-27T04:57:55.305Z","createdAt":"2025-12-26 12:51:07"},{"id":"q-433","question":"Implement quicksort and explain when you'd choose it over mergesort. What's the worst-case scenario and how do you avoid it?","answer":"Quicksort is an efficient divide-and-conquer algorithm that partitions an array around a pivot element, achieving O(n log n) average time complexity. It's preferable over mergesort when in-place sorting is required, as it operates with O(1) auxiliary space and typically offers better cache performance. The worst-case O(n²) occurs with already sorted arrays or when consistently choosing poor pivots; this can be avoided through randomized pivot selection or the median-of-three technique.","explanation":"## Algorithm Overview\nQuicksort follows a divide-and-conquer approach:\n1. Choose a pivot element\n2. Partition the array such that elements less than the pivot appear before it, and greater elements appear after\n3. Recursively apply the same process to subarrays\n\n## When to Choose Quicksort\n- **In-place sorting required**: O(1) auxiliary space vs O(n) for mergesort\n- **Cache efficiency**: Better spatial locality due to in-place operations\n- **Average case optimization**: Outperforms mergesort on random datasets\n\n## Performance Characteristics\n**Advantages:**\n- O(n log n) average time complexity\n- O(1) space complexity (in-place)\n- Excellent real-world performance due to cache friendliness\n\n**Limitations:**\n- O(n²) worst-case time complexity\n- Not a stable sort algorithm\n- Performance dependent on pivot selection strategy\n\n## Implementation Strategy\n```python\ndef quicksort(arr, low, high):\n    if low < high:\n        pivot_index = partition(arr, low, high)\n        quicksort(arr, low, pivot_index - 1)\n        quicksort(arr, pivot_index + 1, high)\n```\n\n## Worst-Case Prevention\n- **Randomized pivot selection**: Eliminates pathological cases on sorted data\n- **Median-of-three**: Choose median of first, middle, and last elements\n- **Hybrid approaches**: Switch to insertion sort for small subarrays","diagram":"flowchart TD\n  A[Choose Pivot] --> B[Partition Array]\n  B --> C[Left Subarray < Pivot]\n  B --> D[Right Subarray > Pivot]\n  C --> E[Recursive Sort]\n  D --> F[Recursive Sort]\n  E --> G[Combine]\n  F --> G","difficulty":"beginner","tags":["quicksort","mergesort","complexity"],"channel":"algorithms","subChannel":"sorting","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","Oracle","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-09T08:46:23.905Z","createdAt":"2025-12-26 12:51:05"},{"id":"q-167","question":"Write a function to find the maximum depth of a binary tree using both recursive DFS and iterative BFS approaches. Discuss time/space complexity and handle edge cases?","answer":"Use recursive DFS: return 1 + max(depth(left), depth(right)) for each node. For iterative BFS, use queue with level counting. Both O(n) time, O(h) recursive space vs O(w) BFS space. Handle empty tree (return 0) and single node cases.","explanation":"## Solution Overview\nMaximum depth (height) of binary tree requires traversing all nodes. Two standard approaches:\n\n## Recursive DFS\n```python\ndef maxDepth(root):\n    if not root: return 0\n    return 1 + max(maxDepth(root.left), maxDepth(root.right))\n```\n- Time: O(n) - visits each node once\n- Space: O(h) - call stack depth equals tree height\n- Risk: Stack overflow for deep skewed trees\n\n## Iterative BFS\n```python\ndef maxDepth(root):\n    if not root: return 0\n    queue = collections.deque([root])\n    depth = 0\n    while queue:\n        depth += 1\n        for _ in range(len(queue)):\n            node = queue.popleft()\n            if node.left: queue.append(node.left)\n            if node.right: queue.append(node.right)\n    return depth\n```\n- Time: O(n) - processes each node once\n- Space: O(n) - worst case when tree is complete\n\n## Edge Cases\n- Empty tree: return 0\n- Single node: return 1\n- Skewed tree: consider iterative approach to avoid stack overflow\n\n## Follow-up Questions\n- How would you modify this for N-ary trees?\n- Can you solve this using Morris traversal for O(1) space?\n- How would you find minimum depth instead?","diagram":"graph TD\n    A[Root] --> B[Left Child]\n    A --> C[Right Child]\n    B --> D[Left Leaf]\n    B --> E[Right Leaf]\n    C --> F[Left Leaf]\n    C --> G[Right Leaf]\n    D --> H[null]\n    D --> I[null]\n    E --> J[null]\n    E --> K[null]\n    F --> L[null]\n    F --> M[null]\n    G --> N[null]\n    G --> O[null]","difficulty":"beginner","tags":["tree","binary"],"channel":"algorithms","subChannel":"trees","sourceUrl":null,"videos":null,"companies":["Amazon","Apple","Google","Meta","Microsoft","Netflix"],"eli5":"Imagine you have a toy tree made of building blocks! Each block can have two smaller blocks hanging from it - one on the left and one on the right. To find how tall the tree is, you start at the very top block and ask: \"How tall am I?\" Each block says: \"I'm 1 block tall PLUS the taller of my two helper blocks below me!\" If a block has no more blocks under it, it says \"I'm just 1 block tall!\" You keep asking this question down through all the blocks until you reach the bottom. Then all the blocks tell you their height, and you pick the biggest number you heard. That's how tall your toy tree is!","relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-25T16:40:41.892Z","createdAt":"2025-12-26 12:51:06"},{"id":"q-314","question":"Given a binary search tree with n nodes, find the kth smallest element where 1 ≤ k ≤ n. Discuss both recursive and iterative approaches with their time and space complexities?","answer":"Use inorder traversal which visits nodes in sorted order. For O(h) space, use recursive DFS with counter. For O(1) space, use Morris traversal with threading. Both run in O(k) time average, O(n) worst case. Morris avoids recursion stack but modifies tree temporarily.","explanation":"## Interview Context\nThis question tests tree traversal knowledge and space optimization awareness.\n\n## Recursive Approach\n```python\ndef kthSmallest(root, k):\n    def inorder(node):\n        if not node: return None\n        left = inorder(node.left)\n        if left: return left\n        self.count += 1\n        if self.count == k: return node.val\n        return inorder(node.right)\n    return inorder(root)\n```\n- Time: O(n) worst case, O(k) average\n- Space: O(h) recursion stack\n\n## Iterative Approach (Morris Traversal)\n```python\ndef kthSmallest(root, k):\n    count = 0\n    curr = root\n    while curr:\n        if not curr.left:\n            count += 1\n            if count == k: return curr.val\n            curr = curr.right\n        else:\n            pred = curr.left\n            while pred.right and pred.right != curr:\n                pred = pred.right\n            if not pred.right:\n                pred.right = curr\n                curr = curr.left\n            else:\n                pred.right = None\n                count += 1\n                if count == k: return curr.val\n                curr = curr.right\n```\n- Time: O(n)\n- Space: O(1)\n\n## Edge Cases\n- k > n: return null/raise exception\n- Empty tree: return null\n- Duplicate values: handle based on requirements\n\n## Follow-up Questions\n1. How would you modify this for kth largest element?\n2. What if the tree is not a BST?\n3. How to optimize for multiple k queries on the same tree?","diagram":"flowchart TD\n  A[Input: BST Root + k] --> B{Counter = 0}\n  B --> C[Inorder Traversal]\n  C --> D[Visit Left Subtree]\n  D --> E[Process Current Node]\n  E --> F{Counter == k?}\n  F -->|Yes| G[Return Current Node Value]\n  F -->|No| H[Counter++]\n  H --> I[Visit Right Subtree]\n  I --> D","difficulty":"beginner","tags":["bst","avl","trie","segment-tree"],"channel":"algorithms","subChannel":"trees","sourceUrl":null,"videos":{"longVideo":"https://www.youtube.com/watch?v=wGXB9OWhPTg"},"companies":null,"eli5":"Imagine you have a line of kids standing by height, from shortest to tallest. Your teacher asks you to find the 3rd shortest kid. You'd just walk down the line counting: 1, 2, 3... and point to that kid! A tree is like a special playground where every branch has smaller kids on the left and bigger kids on the right. To find the kth smallest, you start at the leftmost kid (the shortest) and walk through the playground in order, counting as you go. When you reach the number you're looking for, that's your kid! It's like following a treasure map that always leads you to the kids in height order.","relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-27T04:57:55.453Z","createdAt":"2025-12-26 12:51:05"},{"id":"q-340","question":"Given a BST that may have duplicate values, implement a function to find the kth smallest element considering duplicates. What's the time complexity and how would you handle edge cases?","answer":"Use augmented BST with subtree size counts for O(h) time, or inorder traversal for O(n) worst case. Handle duplicates by counting occurrences or using size-aware traversal that considers all nodes.","explanation":"## Why This Is Asked\nTests mastery of BST properties, tree traversal optimization, and duplicate handling strategies - critical for order statistics in trading systems at Jane Street and Two Sigma.\n\n## Expected Answer\nCandidate should compare three approaches: naive inorder (O(n)), iterative with stack optimization, and augmented BST with subtree sizes for O(h) optimal performance. For duplicates, explain two valid strategies: treat each duplicate as separate element in ordering, or skip duplicates and return the kth distinct value. Must discuss trade-offs between space complexity, implementation complexity, and query frequency.\n\n## Code Example\n```python\nclass TreeNode:\n    def __init__(self, val, left=None, right=None, count=1):\n        self.val = val\n        self.left = left\n        self.right = right\n        self.count = count  # subtree size including this node\n\ndef kth_smallest_augmented(root, k):\n    while root:\n        left_size = root.left.count if root.left else 0\n        \n        if k <= left_size:\n            root = root.left\n        elif k == left_size + 1:\n            return root.val\n        else:\n            k = k - left_size - 1\n            root = root.right\n    return None\n\ndef kth_smallest_iterative(root, k):\n    stack = []\n    curr = root\n    count = 0\n    \n    while stack or curr:\n        while curr:\n            stack.append(curr)\n            curr = curr.left\n        curr = stack.pop()\n        count += 1\n        if count == k:\n            return curr.val\n        curr = curr.right\n    return None\n```\n\n## Key Edge Cases\n- Empty tree: return None/raise exception\n- k out of bounds: validate against total node count\n- Duplicates: clarify if kth element counts duplicates or distinct values\n- Large trees: prefer augmented BST for repeated kth queries","diagram":"flowchart TD\n  A[Start at root] --> B[Traverse left to min]\n  B --> C[Visit node, increment count]\n  C --> D{count == k?}\n  D -->|Yes| E[Return value]\n  D -->|No| F[Traverse right]\n  F --> C\n  E --> G[End]","difficulty":"intermediate","tags":["bst","avl","trie","segment-tree"],"channel":"algorithms","subChannel":"trees","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":"https://www.youtube.com/watch?v=fAAZixBzIAI"},"companies":["Hrt","New Relic","Sap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-28T02:16:30.671Z","createdAt":"2025-12-26 12:51:05"},{"id":"q-451","question":"Given a BST, write a function to find the kth smallest element using O(h) space and O(n) time, where h is height and n is nodes?","answer":"Use inorder traversal with a counter. Since BST's left-root-right yields sorted order, traverse recursively, decrement k when visiting node, return when k=0. Space O(h) for recursion stack, time O(n) ","explanation":"## Approach\n- Inorder traversal naturally visits nodes in ascending order\n- Maintain counter to track when we reach kth element\n- Early termination when found\n\n## Implementation\n```python\ndef kth_smallest(root, k):\n    def inorder(node):\n        if not node: return None\n        left = inorder(node.left)\n        if left: return left\n        k[0] -= 1\n        if k[0] == 0: return node.val\n        return inorder(node.right)\n    return inorder(root)\n```\n\n## Complexity\n- Time: O(n) worst case, O(k) average\n- Space: O(h) recursion stack\n\n## Edge Cases\n- k > number of nodes\n- Empty tree\n- Duplicate values","diagram":"flowchart TD\n  A[Start at root] --> B[Traverse left subtree]\n  B --> C[Visit current node]\n  C --> D{Is kth element?}\n  D -->|Yes| E[Return value]\n  D -->|No| F[Traverse right subtree]\n  F --> G[Continue traversal]","difficulty":"beginner","tags":["bst","avl","trie","segment-tree"],"channel":"algorithms","subChannel":"trees","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Cloudflare"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-27T04:58:34.208Z","createdAt":"2025-12-26 12:51:05"},{"id":"q-660","question":"You’re building a chat app that autocompletes words as users type. Implement a Trie with insert(word), search(word), and startsWith(prefix). Provide a compact JavaScript class with these methods, assuming lowercase a-z. After inserting 'apple','app','application', does search('app') return true and does startsWith('appl') return true?","answer":"Use a Trie: each node has 26 children and an isWord flag. insert(w) walks/creates nodes for each char and marks the last node. search(w) returns true if every node exists and the final node isWord. st","explanation":"## Why This Is Asked\nAssess understanding of Trie data structure and practical prefix search used in autocompletion; tests familiarity with basic data structures and iteration over strings.\n\n## Key Concepts\n- Trie structure with 26-way children\n- insert/search/startsWith time complexity O(m)\n- memory proportional to number of unique prefixes\n\n## Code Example\n```javascript\nclass TrieNode { constructor(){ this.next = new Array(26).fill(null); this.end=false; } }\nclass Trie { constructor(){ this.root = new TrieNode(); }\n insert(word){ let p=this.root; for(let ch of word){ let idx=ch.charCodeAt(0)-97; if(!p.next[idx]) p.next[idx]=new TrieNode(); p=p.next[idx]; } p.end=true; }\n search(word){ let p=this.root; for(let ch of word){ let idx=ch.charCodeAt(0)-97; if(!p.next[idx]) return false; p=p.next[idx]; } return p.end; }\n startsWith(prefix){ let p=this.root; for(let ch of prefix){ let idx=ch.charCodeAt(0)-97; if(!p.next[idx]) return false; p=p.next[idx]; } return true; }\n}\n```\n\n## Follow-up Questions\n- How would you optimize for memory?\n- How would you extend to support deletion or word counts?","diagram":null,"difficulty":"beginner","tags":["bst","avl","trie","segment-tree"],"channel":"algorithms","subChannel":"trees","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","IBM","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-11T13:45:48.860Z","createdAt":"2026-01-11T13:45:48.861Z"}],"subChannels":["algorithms","data-structures","dynamic-programming","graphs","load-balancing-algorithms","sorting","trees"],"companies":["Adobe","Airbnb","Amazon","Anthropic","Apple","Cloudflare","Cruise","Facebook","Goldman Sachs","Google","Hashicorp","Hrt","IBM","LinkedIn","Meta","Microsoft","NVIDIA","Netflix","New Relic","OpenAI","Oracle","Robinhood","Salesforce","Sap","Slack","Snowflake","Square","Stripe","Twitter","Two Sigma","Uber","Vercel"],"stats":{"total":40,"beginner":11,"intermediate":19,"advanced":10,"newThisWeek":7}}